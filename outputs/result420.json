[{"title": "Self-Boosting Large Language Models with  Synthetic Preference Data", "link_suffix": "/forum?id=7visV100Ms", "link": "https://openreview.net/forum?id=7visV100Ms", "pdf_link": "https://openreview.net/pdf?id=7visV100Ms", "keywords": "preference optimization, synthetic data, LLM alignment", "abstract": "Through alignment with human preferences, Large Language Models (LLMs) have advanced significantly in generating honest, harmless, and helpful responses. However, collecting high-quality preference data is a resource-intensive and creativity-demanding process, especially for the continual improvement of LLMs. We introduce SynPO, a self-boosting paradigm that leverages synthetic preference data for model alignment. SynPO employs an iterative mechanism wherein a self-prompt generator creates diverse prompts, and a response improver refines model responses progressively. This approach trains LLMs to autonomously learn the generative rewards for their own outputs and  eliminates the need for large-scale annotation of prompts and human preferences. After four SynPO iterations, Llama3-8B and Mistral-7B show significant enhancements in instruction-following abilities, achieving over 22.1% win rate improvements on AlpacaEval 2.0 and ArenaHard. Simultaneously, SynPO improves the general performance of LLMs on various tasks, validated by a 3.2 to 5.0 average score increase on the well-recognized Open LLM leaderboard.", "title_embedding_index": 20950, "title_abs_embedding_index": 20975}, {"title": "Influential Language Data Selection via Gradient Trajectory Pursuit", "link_suffix": "/forum?id=che9LCwPQM", "link": "https://openreview.net/forum?id=che9LCwPQM", "pdf_link": "https://openreview.net/pdf?id=che9LCwPQM", "keywords": "Language data selection; LLM", "abstract": "Curating a desirable dataset for training has been the core of building highly capable large language models (Touvron et al., 2023; Achiam et al., 2023; Team et al., 2024). Gradient influence scores (Pruthi et al., 2020; Xia et al., 2024) have been shown to be correlated with model performance and are commonly used as the criterion for data selection. However, existing methods are built upon either individual sample rankings or inefficient matching process, leading to suboptimal performance or scaling up issues. In this paper, we propose Gradient Trajectory Pursuit (GTP), an algorithm that performs pursuit of gradient trajectories via jointly selecting data points under an L0-norm regularized objective. The proposed algorithm highlights: (1) joint selection instead of independent top-k selection, which automatically de-duplicates samples; (2) higher efficiency with compressive sampling processes, which can be further sped up using a distributed framework. In the experiments, we demonstrate the algorithm in both in-domain and target-domain selection benchmarks and show that it outperforms top-k selection and competitive algorithms consistently, for example, our algorithm chooses as low as 0.5% data to achieve full performance on the targeted instruction tuning tasks.", "title_embedding_index": 20951, "title_abs_embedding_index": 20976}, {"title": "A Research on Result Interpretability of Medical AI Based on Large Language Model", "link_suffix": "/forum?id=Hb3x52Jliq", "link": "https://openreview.net/forum?id=Hb3x52Jliq", "pdf_link": "https://openreview.net/pdf?id=Hb3x52Jliq", "keywords": "XAI, AI Agent, Medical AI, Amyloidosis, LLM, AIGC", "abstract": "Explainability is one of the important challenges facing the application of medical AI. The existing AI explainability research is more of a kind of process explainability study. Drawing on the behavioral habits of human beings to communicate on a certain topic, this paper proposes a definition of result interpretability for medical AI, divides explainable medical AI research into three phases: data explainability, process explainability and result interpretability, and argues that once an AI model reaches a certain result interpretability metric, we can accept its conclusions and apply it to the clinic without having to wait until human beings fully understand the operation and decision-making mechanism of the AI model before using it. In this regard, we propose the c oncept of interpretative integrity. Further, we propose an architecture for result-interpretable medical AI system based on AI-Agent and build a result-interpretable system around risk prediction AI model for amyloidosis, which enables professional interpretation of the result of the risk prediction model for amyloidosis disease through a large language model and supports professional Q&A with clinicians. The implementation of the system enhances clinicians' professional acceptance of medical AI models, and provides a more feasible realization path for the large-scale application of AI-assisted diagnosis.", "title_embedding_index": 20952, "title_abs_embedding_index": 20977}, {"title": "Towards Human-like Virtual Beings: Simulating Human Behavior in 3D Scenes", "link_suffix": "/forum?id=yc38vnXhTh", "link": "https://openreview.net/forum?id=yc38vnXhTh", "pdf_link": "https://openreview.net/pdf?id=yc38vnXhTh", "keywords": "Agent AI, 3D Humanoid, Large Language Model, Deep Learning", "abstract": "Building autonomous agents that can replicate human behavior in the realistic 3D world is a key step toward artificial general intelligence. This requires agents to be holistic goal achievers and to naturally adapt to environmental dynamics. In this work, we introduce ACTOR, an agent capable of performing high-level, long-horizon, abstract goals in 3D households, guided by its internal value similar to those of humans. ACTOR operates in a perceive-plan-act cycle, extending the ungrounded, scene-agnostic LLM controller with deliberate goal decomposition and decision-making through actively searching the behavior space, generating activity choices based on a hierarchical prior, and evaluating these choices using customizable value functions to determine the subsequent steps. Furthermore, we introduce BehaviorHub, a large-scale human behavior simulation dataset in scene-aware, complicated tasks. Considering the unaffordable acquisition of human-authored 3D human behavior data, we construct BehaviorHub by exploring the commonsense knowledge of LLMs learned from large corpora, and automatically aligning motion resources with 3D scene for knowledgeable generation. Extensive experiments on our established benchmark demonstrate that the proposed architecture leads to effective behavior planning and simulation. BehaviorHub also proves beneficial for downstream task development. Our code and dataset will be publicly released.", "title_embedding_index": 20953, "title_abs_embedding_index": 20978}, {"title": "Deciphering Cross-Modal Alignment in Large Vision-Language Models with Modality Integration Rate", "link_suffix": "/forum?id=uQEsLZU15E", "link": "https://openreview.net/forum?id=uQEsLZU15E", "pdf_link": "https://openreview.net/pdf?id=uQEsLZU15E", "keywords": "Large Vision-Language Models, Cross-Modal Alignment", "abstract": "We present the Modality Integration Rate (MIR), an effective, robust, and generalized metric to indicate the multi-modal pre-training quality of Large Vision Language Models (LVLMs). Large-scale pre-training plays a critical role in building capable LVLMs, while evaluating its training quality without the costly supervised fine-tuning stage is under-explored. Loss, perplexity, and in-context evaluation results are commonly used pre-training metrics for Large Language Models (LLMs), while we observed that these metrics are less indicative when aligning a well-trained LLM with a new modality. Due to the lack of proper metrics, the research of LVLMs in the critical pre-training stage is hindered greatly, including the training data choice, efficient module design, etc.In this paper, we propose evaluating the pre-training quality from the inter-modal distribution distance perspective and present MIR, the Modality Integration Rate, which is 1) Effective to represent the pre-training quality and show a positive relation with the benchmark performance after supervised fine-tuning. 2) Robust toward different training/evaluation data. 3) Generalize across training configurations and architecture choices.We conducted a series of pre-training experiments to explore the effectiveness of MIR and observed satisfactory results that MIR is indicative about training data selection, training strategies schedule, and model architecture design to get better pre-training results. \nWe hope MIR could be a helpful metric for building capable LVLMs and inspire the following research about modality alignment in different areas.", "title_embedding_index": 20954, "title_abs_embedding_index": 20979}, {"title": "Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark", "link_suffix": "/forum?id=g90RNzs8wX", "link": "https://openreview.net/forum?id=g90RNzs8wX", "pdf_link": "https://openreview.net/pdf?id=g90RNzs8wX", "keywords": "Graph out-of-distribution detection; graph anomaly detection; benchmark", "abstract": "To build safe and reliable graph machine learning systems, unsupervised graph-level anomaly detection (GLAD) and unsupervised graph-level out-of-distribution (OOD) detection (GLOD) have received significant attention in recent years. Though these two lines of research share the same objective, they have been studied independently in the community due to distinct evaluation setups, creating a gap that hinders the application and evaluation of methods from one to the other. To bridge the gap, in this work, we present a Unified Benchmark for unsupervised Graph-level OOD and anomaly Detection (UB-GOLD), a comprehensive evaluation framework that unifies GLAD and GLOD under the concept of generalized graph-level OOD detection. Our benchmark encompasses 35 datasets spanning four practical anomaly and OOD detection scenarios, facilitating the comparison of 18 representative GLAD/GLOD methods. We conduct multi-dimensional analyses to explore the effectiveness, generalizability, robustness, and efficiency of existing methods, shedding light on their strengths and limitations. Furthermore, we provide an open-source codebase of UB-GOLD to foster reproducible research and outline potential directions for future investigations based on our insights.", "title_embedding_index": 20955, "title_abs_embedding_index": 20980}, {"title": "Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting", "link_suffix": "/forum?id=ix2yRWarPn", "link": "https://openreview.net/forum?id=ix2yRWarPn", "pdf_link": "https://openreview.net/pdf?id=ix2yRWarPn", "keywords": "articulated object modeling", "abstract": "Building interactable replicas of articulated objects is a key challenge in computer vision. Existing methods often fail to effectively integrate information across different object states, limiting the accuracy of part-mesh reconstruction and part dynamics modeling, particularly for complex multi-part articulated objects. We introduce ArtGS, a novel approach that leverages 3D Gaussians as a flexible and efficient representation to address these issues. Our method incorporates canonical Gaussians with coarse-to-fine initialization and updates for aligning articulated part information across different object states, and employs a skinning-inspired part dynamics modeling module to improve both part-mesh reconstruction and articulation learning. Extensive experiments on both synthetic and real-world datasets, including a new benchmark for complex multi-part objects, demonstrate that ArtGS achieves state-of-the-art performance in joint parameter estimation and part mesh reconstruction. Our approach significantly improves reconstruction quality and efficiency, especially for multi-part articulated objects. Additionally, we provide comprehensive analyses of our design choices, validating the effectiveness of each component to highlight potential areas for future improvement.", "title_embedding_index": 20956, "title_abs_embedding_index": 20981}, {"title": "DiFSD: Ego-Centric Fully Sparse Paradigm with Uncertainty Denoising and Iterative Refinement for Efficient Self-Driving", "link_suffix": "/forum?id=zKoUV1wHRJ", "link": "https://openreview.net/forum?id=zKoUV1wHRJ", "pdf_link": "https://openreview.net/pdf?id=zKoUV1wHRJ", "keywords": "Autonomous Driving, End-to-End Fully Sparse Paradigm, Iterative Refinement, Uncertainty Denoising", "abstract": "Current end-to-end autonomous driving methods resort to unifying modular designs for various tasks (e.g. perception, prediction and planning). Although optimized in a planning-oriented spirit with a fully differentiable framework, existing end-to-end driving systems without ego-centric designs still suffer from unsatisfactory performance and inferior efficiency, owing to the rasterized scene representation learning and redundant information transmission. In this paper, we revisit the human driving behavior and propose an ego-centric fully sparse paradigm, named DiFSD, for end-to-end self-driving. Specifically, DiFSD mainly consists of sparse perception, hierarchical interaction and iterative motion planner. The sparse perception module performs detection, tracking and online mapping based on sparse representation of the driving scene. The hierarchical interaction module aims to select the Closest In-Path Vehicle / Stationary (CIPV / CIPS) from coarse to fine, benefiting from an additional geometric prior. As for the iterative motion planner, both selected interactive agents and ego-vehicle are considered for joint motion prediction, where the output multi-modal ego-trajectories are optimized in an iterative fashion. Besides, both position-level motion diffusion and trajectory-level planning denoising are introduced for uncertainty modeling, thus facilitating the training stability and convergence of the whole framework. Extensive experiments conducted on nuScenes dataset demonstrate the superior planning performance and great efficiency of DiFSD, which significantly reduces the average L2 error by 66% and collision rate by 77% than UniAD while achieves 8.2x faster running efficiency.", "title_embedding_index": 20957, "title_abs_embedding_index": 20982}, {"title": "NeMal: Never ending Marine Learning - Unleashing the Power of Controllable Image Synthesis for Promoting Marine Visual Understanding", "link_suffix": "/forum?id=rztZ2QfSfJ", "link": "https://openreview.net/forum?id=rztZ2QfSfJ", "pdf_link": "https://openreview.net/pdf?id=rztZ2QfSfJ", "keywords": "never ending marine learning, controllable image synthesis, foundation models, vision language analysis", "abstract": "The relentless pursuit of marine learning is required by the essential need to understand and protect the complex marine ecosystems that cover over 70% of the surface of our planet. Due to the specific underwater/marine environments, the data collection and labeling are expensive and labor-intensive, also limited to user groups with special equipment. Existing marine visual learning just optimizes models from a small set of marine data with human labels, which cannot fit the essence of ongoing marine exploration. In this work, we propose NeMal, a \\underline{N}ever-\\underline{e}nding \\underline{Ma}rine \\underline{L}earning system that harnesses controllable image synthesis and efficient foundation models to perform never-ending marine visual synthesis and understanding. Based on NeMal, we produce MarineSynth, which is the first large-scale marine synthetic dataset to date, featuring more than 4 million unique text prompts and corresponding text-to-image outputs with pseudo labels from text prompts or foundation models. The experiments on downstream classification, segmentation, and vision-language understanding tasks demonstrate the promise of utilizing synthetic data to promote marine visual understanding, significantly reducing human efforts in both data collection and labeling.", "title_embedding_index": 20958, "title_abs_embedding_index": 20983}, {"title": "Segment, Associate, and Classify: Decoupled Audio-Visual Segmentation Framework", "link_suffix": "/forum?id=8VnS320esG", "link": "https://openreview.net/forum?id=8VnS320esG", "pdf_link": "https://openreview.net/pdf?id=8VnS320esG", "keywords": "Audio-visual segmentation, audio-visual semantic segmentation, image segmentation", "abstract": "The audio-visual segmentation task aims to segment sounding objects associated with the corresponding audio in visual data. Unlike conventional supervised approaches, this paper presents a method that does not require ground-truth audio-visual masks during training. The proposed framework consists of three decoupled stages: (1) segmenting category and audio-agnostic objects solely from an input image, (2) associating input audio and segmented object masks to obtain the corresponding mask to the audio, and (3) classifying the object mask. We leverage the pretrained segmentation and vision-language foundation models in the segmentation and classification stages, respectively, and the audio-mask association module in the second stage is trained without relying on ground-truth correspondence between audio and object masks via a multiple-instance contrastive learning scheme. In the association module, we propose object mask representation to incorporate the local and global information of object masks and training framework to enhance the segmentation performance on the multi-source audio inputs. Our approach significantly outperforms previous unsupervised and weakly-supervised audio-visual source localization and segmentation methods. Furthermore, our approach achieves a comparable performance to the supervised audio-visual semantic segmentation baseline.", "title_embedding_index": 20959, "title_abs_embedding_index": 20984}, {"title": "MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization", "link_suffix": "/forum?id=x1Okv4kbVR", "link": "https://openreview.net/forum?id=x1Okv4kbVR", "pdf_link": "https://openreview.net/pdf?id=x1Okv4kbVR", "keywords": "weak-to-strong alignment, preference optimization", "abstract": "As large language models (LLMs) are rapidly advancing and achieving near-human capabilities, aligning them with human values is becoming more urgent. In scenarios where LLMs outperform humans, we face a weak-to-strong alignment problem where we need to effectively align strong student LLMs through weak supervision generated by weak teachers. Existing alignment methods mainly focus on strong-to-weak alignment and self-alignment settings, and it is impractical to adapt them to the much harder weak-to-strong alignment setting. To fill this gap, we propose a multi-agent contrastive preference optimization (MACPO) framework. MACPO facilitates weak teachers and strong students to learn from each other by iteratively reinforcing unfamiliar positive behaviors while penalizing familiar negative ones. To get this, we devise a mutual positive behavior augmentation strategy to encourage weak teachers and strong students to learn from each other\u2019s positive behavior and further provide higher quality positive behavior for the next iteration. Additionally, we propose a hard negative behavior construction strategy to induce weak teachers and strong students to generate familiar negative behavior by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human judgments, demonstrate that MACPO simultaneously improves alignment performance of strong students and weak teachers. Moreover, as the number of weak teachers increases, MACPO achieves better weak-to-strong alignment performance through more iteration optimization rounds.", "title_embedding_index": 20960, "title_abs_embedding_index": 20985}, {"title": "Biology Instructions: A Dataset and Benchmark for Multi-Omics Sequence Understanding Capability of Large Language Models", "link_suffix": "/forum?id=P4KzPJlnFk", "link": "https://openreview.net/forum?id=P4KzPJlnFk", "pdf_link": "https://openreview.net/pdf?id=P4KzPJlnFk", "keywords": "Instruction benchmark, Multi-omics, AI for Biology", "abstract": "Large language models have already demonstrated their formidable capabilities in general domains, ushering in a revolutionary transformation. However, exploring and exploiting the extensive knowledge of these models to comprehend multi-omics biology remains underexplored. To fill this research gap, we first introduce Biology-Instructions, the first large-scale multi-omics biological sequences-related instruction-tuning dataset including DNA, RNA, proteins, and multi-molecules, designed to bridge the gap between large language models (LLMs) and complex biological sequences-related tasks. This dataset can enhance the versatility of LLMs by integrating diverse biological sequenced-related tasks with advanced reasoning capabilities, maintaining conversational fluency. Additionally, we reveal significant performance limitations in even state-of-the-art LLMs on biological sequence-related multi-omics tasks without specialized pre-training and instruction-tuning. We further develop a strong baseline called ChatMultiOmics with a novel three-stage training pipeline, demonstrating the powerful ability to understand biology by using Biology-Instructions. Biology-Instructions and ChatMultiOmics are publicly available and crucial resources for enabling more effective integration of LLMs with multi-omics sequence analysis.", "title_embedding_index": 20961, "title_abs_embedding_index": 20986}, {"title": "KA-GAT: Kolmogorov\u2013Arnold based Graph Attention Networks", "link_suffix": "/forum?id=zf777Odl6J", "link": "https://openreview.net/forum?id=zf777Odl6J", "pdf_link": "https://openreview.net/pdf?id=zf777Odl6J", "keywords": "Graph Neural Networks, Kolmogorov-Arnold Networks, Graph Attention Networks, Multi-head Attention Mechanism, Model Interpretability", "abstract": "Graph Neural Networks (GNNs) excel at processing graph-structured data but often struggle with complex, high-dimensional features and nonlinear relationships. To address these limitations, we propose KA-GAT, a novel model that integrates Kolmogorov-Arnold Networks (KANs) with Graph Attention Networks (GATs). KA-GAT leverages KANs to decompose and reconstruct features, enhancing the model's ability to handle complex data. The multi-head attention mechanism further improves flexibility and interpretability by dynamically focusing on different graph components. Experimental results on benchmark datasets, including Cora and Citeseer, demonstrate that KA-GAT outperforms traditional GNN models such as GCN and GAT in accuracy, precision, and F1-score. These findings underscore KA-GAT's suitability for tasks involving complex graph structures and high-dimensional features, contributing a novel architecture, enhanced interpretability, and robust experimental validation to the field.", "title_embedding_index": 20962, "title_abs_embedding_index": 20987}, {"title": "Leveraging Semantic and Positional Uncertainty for Trajectory Prediction", "link_suffix": "/forum?id=WTmZS5GU0E", "link": "https://openreview.net/forum?id=WTmZS5GU0E", "pdf_link": "https://openreview.net/pdf?id=WTmZS5GU0E", "keywords": "Uncertainty, Trajectory Prediction", "abstract": "Given a time horizon with historical movement data and environmental context, trajectory prediction aims to forecast the future motion of dynamic entities, such as vehicles and pedestrians. A key challenge in this task arises from the dynamic and noisy nature of real-time maps. This noise primarily stems from two resources: (1) positional errors due to sensor inaccuracies or environmental occlusions, and (2) cognitive errors resulting from incorrect scene understanding. \nIn an attempt to solve this problem, we propose a new framework that estimates two kinds of uncertainty, \\ie, positional uncertainty and semantic uncertainty simultaneously, and explicitly incorporates both uncertainties into the trajectory prediction process. \nIn particular, we introduce a dual-head structure to independently perform semantic prediction twice and positional prediction twice, and further extract the prediction variance as the uncertainty indicator in an end-to-end manner. The uncertainty is then directly concatenated with the semantic and positional predictions to enhance the trajectory estimation.\nTo validate the effectiveness of our uncertainty-aware approach, we evaluate it on the real-world driving dataset, \\ie, nuScenes. \nExtensive experiments on 3 mapping estimation and 2 trajectory approaches show that the proposed method (1) effectively captures map noise through both positional and semantic uncertainties, and (2) seamlessly integrates and enhances existing trajectory prediction methods on multiple evaluation metrics, \\ie, minADE, minFDE, and MR.", "title_embedding_index": 20963, "title_abs_embedding_index": 20988}, {"title": "SYMPOL: Symbolic Tree-Based On-Policy Reinforcement Learning", "link_suffix": "/forum?id=qpXctF2aLZ", "link": "https://openreview.net/forum?id=qpXctF2aLZ", "pdf_link": "https://openreview.net/pdf?id=qpXctF2aLZ", "keywords": "Symbolic Reinforcement Learning, Interpretable Reinforcement Learning, Decision Trees, Proximal Policy Optimization", "abstract": "Reinforcement learning (RL) has seen significant success across various domains, but its adoption is often limited by the black-box nature of neural network policies, making them difficult to interpret. In contrast, symbolic policies allow representing decision-making strategies in a compact and interpretable way. However, learning symbolic policies directly within on-policy methods remains challenging. In this paper, we introduce SYMPOL, a novel method for SYMbolic tree-based on-POLicy RL. SYMPOL employs a tree-based model integrated with a policy gradient method, enabling the agent to learn and adapt its actions while maintaining a high level of interpretability. We evaluate SYMPOL on a set of benchmark RL tasks, demonstrating its superiority over alternative tree-based RL approaches in terms of performance and interpretability. To the best of our knowledge, this is the first method, that allows a gradient-based end-to-end learning of interpretable, axis-aligned decision trees on-policy. Therefore, SYMPOL can become the foundation for a new class of interpretable RL based on decision trees.", "title_embedding_index": 20964, "title_abs_embedding_index": 20989}, {"title": "IMPLICIT VARIATIONAL REJECTION SAMPLING", "link_suffix": "/forum?id=GDZeeCZ3MM", "link": "https://openreview.net/forum?id=GDZeeCZ3MM", "pdf_link": "https://openreview.net/pdf?id=GDZeeCZ3MM", "keywords": "Varaitional inference, reject sampling, implict distribution", "abstract": "Variational Inference (VI) is a cornerstone technique in Bayesian machine learning, employed to approximate complex posterior distributions. However, traditional VI methods often rely on mean-field assumptions, which may inadequately capture the true posterior's complexity. To address this limitation, recent advancements have utilized neural networks to model implicit distributions, thereby offering increased flexibility. Despite this, the practical constraints of neural network architectures can still result in inaccuracies in posterior approximations. In this work, we introduce a novel method called Implicit Variational Rejection Sampling (IVRS), which integrates implicit distributions with rejection sampling to enhance the approximation of the posterior distribution. Our method employs neural networks to construct implicit proposal distributions and utilizes rejection sampling with a meticulously designed acceptance probability function. A discriminator network is employed to estimate the density ratio between the implicit proposal and the true posterior, thereby refining the approximation. We propose the Implicit Resampling Evidence Lower Bound (IR-ELBO) as a metric to characterize the quality of the resampled distribution, enabling the derivation of a tighter variational lower bound. Experimental results demonstrate that our method outperforms traditional variational inference techniques in terms of both accuracy and efficiency, leading to significant improvements in inference performance. This work not only showcases the effective combination of implicit distributions and rejection sampling but also offers a novel perspective and methodology for advancing variational inference.", "title_embedding_index": 20965, "title_abs_embedding_index": 20990}, {"title": "Deep Koopman-layered Model with Universal Property Based on Toeplitz Matrices", "link_suffix": "/forum?id=IZbthMfqad", "link": "https://openreview.net/forum?id=IZbthMfqad", "pdf_link": "https://openreview.net/pdf?id=IZbthMfqad", "keywords": "Koopman operator, Toeplitz matrix, nonautonomous dynamical system", "abstract": "We propose deep Koopman-layered models with learnable parameters in the form of Toeplitz matrices for analyzing the dynamics of time-series data.\nThe proposed model has both theoretical solidness and flexibility.\nBy virtue of the universal property of Toeplitz\nmatrices and the reproducing property underlined in the model, we can show its universality and the generalization property.\nIn addition, the flexibility of the proposed model enables the model to fit time-series data coming from nonautonomous dynamical systems.\nWhen training the model, we apply Krylov subspace methods for efficient computations.\nIn addition, the proposed model can be regarded as a neural ODE-based model.\nIn this sense, the proposed model establishes a new connection among Koopman operators, neural ODEs, and numerical linear algebraic methods.", "title_embedding_index": 20966, "title_abs_embedding_index": 20991}, {"title": "DUAL-TASK VAE FOR NODE-LEVEL DATA AUGMENTATION", "link_suffix": "/forum?id=XWb6dPuhmC", "link": "https://openreview.net/forum?id=XWb6dPuhmC", "pdf_link": "https://openreview.net/pdf?id=XWb6dPuhmC", "keywords": "VAE, GNNs, Graph Data Augmentation, Node Classification, Dual-task Training, Loss Weight Adjustment", "abstract": "Graph Neural Networks (GNNs) are adept at handling graph-structured data but are often limited by the scarcity of high-quality labeled data. To address this limitation, our study presents a Variational Autoencoder (VAE)-based approach for node-level data augmentation, which enriches the original graph data by creating new representations in the latent space. We adopt a two-stage framework: a VAE learns latent representations in the first stage, while a Graph Attention Network (GAT) performs node classification in the second stage. Our experiments on the Cora dataset indicate that integrating raw data with latent representations (Raw+NR), coupled with dual-task training and loss weight adjustment, markedly improves model performance. The optimized model achieves a peak accuracy of 0.886 and an F1 score of 0.873, approaching state-of-the-art performance with a simpler model. This study offers valuable strategies for enhancing graph data augmentation and GNN training, which could contribute to advancing research in the field.", "title_embedding_index": 20967, "title_abs_embedding_index": 20992}, {"title": "EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models", "link_suffix": "/forum?id=xtlMtbVfWu", "link": "https://openreview.net/forum?id=xtlMtbVfWu", "pdf_link": "https://openreview.net/pdf?id=xtlMtbVfWu", "keywords": "Distributed Training, Large Language Models, Local SGD, Training Acceleration", "abstract": "Distributed training methods are crucial for large language models (LLMs). However, existing distributed training methods often suffer from communication bottlenecks, stragglers, and limited elasticity, particularly in heterogeneous or large-scale environments. Local SGD methods have been proposed to address these issues, but their effectiveness remains constrained to small-scale training due to the lack of robust distributed strategies and concerns over efficiency and stability. To tackle these issues, we propose EDiT, an innovative Efficient Distributed Training method that combines a tailored Local SGD approach with advanced distributed techniques to enhance large-scale training efficiency, and employ a pseudo gradient penalty strategy to ensure training stability and improve performance. Additionally, we introduce A-EDiT, a fully asynchronous variant of EDiT that accommodates heterogeneous clusters. Building on EDiT/A-EDiT, we conduct a series of experiments to validate large-scale asynchronous training for LLMs, accompanied by comprehensive analyses. Experimental results demonstrate the superior performance of EDiT/A-EDiT in terms of convergence, generalization, acceleration, scalability, and stability, establishing them as robust solutions for distributed LLM training in diverse computational ecosystems.", "title_embedding_index": 20968, "title_abs_embedding_index": 20993}, {"title": "Private Wasserstein Distance", "link_suffix": "/forum?id=O7wTfBLSFn", "link": "https://openreview.net/forum?id=O7wTfBLSFn", "pdf_link": "https://openreview.net/pdf?id=O7wTfBLSFn", "keywords": "data valuation, data marketplace, optimal transport, Wasserstein distance", "abstract": "Wasserstein distance is a key metric for quantifying data divergence from a distributional perspective. However, its application in privacy-sensitive environments, where direct sharing of raw data is prohibited, presents significant challenges. Existing approaches, such as Differential Privacy and Federated Optimization, have been employed to estimate the Wasserstein distance under such constraints. However, these methods often fall short when both accuracy and security are required. In this study, we explore the inherent triangular properties within the Wasserstein space, leading to a novel solution named $\\texttt{TriangleWad}$. This approach facilitates the fast computation of the Wasserstein distance between datasets stored across different entities, ensuring that raw data remain completely hidden. TriangleWad not only strengthens resistance to potential attacks but also preserves high estimation accuracy.\nThrough extensive experiments across various tasks involving both image and text data, we demonstrate its superior performance and significant potential for real-world applications.", "title_embedding_index": 20969, "title_abs_embedding_index": 20994}, {"title": "IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model", "link_suffix": "/forum?id=N5YTixK4F1", "link": "https://openreview.net/forum?id=N5YTixK4F1", "pdf_link": "https://openreview.net/pdf?id=N5YTixK4F1", "keywords": "large vision-language model; ID recognition", "abstract": "The rapid advancement of Large Vision-Language models (LVLMs) has demonstrated a spectrum of emergent capabilities. Nevertheless, current models only focus on the visual content of a single scenario, while their ability to associate instances across different scenes has not yet been explored, which is essential for understanding complex visual content, such as movies with multiple characters and intricate plots. Towards movie understanding, a critical initial step for LVLMs is to unleash the potential of character identities memory and recognition across multiple visual scenarios. To achieve the goal, we propose visual instruction tuning with ID reference and develop an ID-Aware Large Vision-Language Model, IDA-VLM. Furthermore, our research introduces a novel benchmark MM-ID, to examine LVLMs on instance IDs memory and recognition across four dimensions: matching, location, question-answering, and captioning. Our findings highlight the limitations of existing LVLMs in recognizing and associating instance identities with ID reference. This paper paves the way for future artificial intelligence systems to possess multi-identity visual inputs, thereby facilitating the comprehension of complex visual narratives like movies.", "title_embedding_index": 20970, "title_abs_embedding_index": 20995}, {"title": "Never Forget the Basics: In-distribution Knowledge Retention for Continual Test-time Adaptation in Human Motion Prediction", "link_suffix": "/forum?id=nc0XGK40dn", "link": "https://openreview.net/forum?id=nc0XGK40dn", "pdf_link": "https://openreview.net/pdf?id=nc0XGK40dn", "keywords": "Human Pose Prediction, Domain Adaptation, Graph Representation Learning, Graph Out-of-distribution Detection", "abstract": "This paper presents a novel approach to addressing the underexplored challenge of human pose prediction in dynamic target domains that simultaneously contain in-distribution (ID) and out-of-distribution (OOD) data. Existing test-time adaptation (TTA) techniques predominantly focus on OOD data, neglecting the fact that ID data, which closely resembles the training distribution, is often encountered during real-world deployment, leading to significant degradation in ID performance. To address this, we introduce In-Distribution Knowledge Retention (IDKR), a continual TTA framework designed to preserve critical knowledge about ID data while adapting to unseen OOD sequences. Our method introduces an ID-informative subgraph learning strategy that leverages the structural characteristics of human skeletal data to compute a structural graph Fisher Information Matrix (SG-FIM). Unlike prior work, IDKR simultaneously considers both node and edge features in the skeletal graph, with edge features, representing the invariant bone lengths between parent-child joint pairs, being essential for maintaining structural consistency across poses. These edge features are key to extracting reliable SG-FIM parameters, enabling the model to retain parameters critical for ID performance while selectively updating those needed for OOD adaptation. Extensive experiments on multiple benchmark datasets demonstrate that IDKR consistently outperforms state-of-the-art methods, particularly in scenarios involving mixed ID and OOD data, setting a new standard for robust human pose prediction in dynamic environments.", "title_embedding_index": 20971, "title_abs_embedding_index": 20996}, {"title": "Gaussian-Based Instance-Adaptive Intensity Modeling for Point-Supervised Facial Expression Spotting", "link_suffix": "/forum?id=daD6uGMeLs", "link": "https://openreview.net/forum?id=daD6uGMeLs", "pdf_link": "https://openreview.net/pdf?id=daD6uGMeLs", "keywords": "micro-expression spotting, semi-supervised learning, soft pseudo-labeling", "abstract": "Point-supervised facial expression spotting (P-FES) aims to localize facial expression instances in untrimmed videos, requiring only a single timestamp label for each instance during training. To address label sparsity, hard pseudo-labeling is often employed to propagate point labels to unlabeled frames; however, this approach can lead to confusion when distinguishing between neutral and expression frames with various intensities, which can negatively impact model performance. In this paper, we propose a two-branch framework for P-FES that incorporates a Gaussian-based instance-adaptive Intensity Modeling (GIM) module for soft pseudo-labeling. GIM models the expression intensity distribution for each instance. Specifically, we detect the pseudo-apex frame around each point label, estimate the duration, and construct a Gaussian distribution for each expression instance. We then assign soft pseudo-labels to pseudo-expression frames as intensity values based on the Gaussian distribution. Additionally, we introduce an Intensity-Aware Contrastive (IAC) loss to enhance discriminative feature learning and suppress neutral noise by contrasting neutral frames with expression frames of various intensities. Extensive experiments on the SAMM and CAS(ME)$^2$ datasets demonstrate the effectiveness of our proposed framework.", "title_embedding_index": 20972, "title_abs_embedding_index": 20997}, {"title": "Learning Spatial-Semantic Features for Robust Video Object Segmentation", "link_suffix": "/forum?id=EM93t94zEi", "link": "https://openreview.net/forum?id=EM93t94zEi", "pdf_link": "https://openreview.net/pdf?id=EM93t94zEi", "keywords": "Video Object Segmentation, Spatial-Semantic Feature, Long-Term, Discriminative Object Queries", "abstract": "Tracking and segmenting multiple similar objects with distinct or complex parts in long-term videos is particularly challenging due to the ambiguity in identifying target components and the confusion caused by occlusion, background clutter, and changes in appearance or environment over time. In this paper, we propose a robust video object segmentation framework that learns spatial-semantic features and discriminative object queries to address the above issues. Specifically, we construct a spatial-semantic block comprising a semantic embedding component and a spatial dependency modeling part for associating global semantic features and local spatial features, providing a comprehensive target representation. In addition, we develop a masked cross-attention module to generate object queries that focus on the most discriminative parts of target objects during query propagation, alleviating noise accumulation to ensure effective long-term query propagation. The experimental results show that the proposed method sets new state-of-the-art performance on multiple data sets, including the DAVIS2017 test (\\textbf{87.8%}), YoutubeVOS 2019 (\\textbf{88.1%}), MOSE val (\\textbf{74.0%}), and LVOS test (\\textbf{73.0%}), which demonstrate the effectiveness and generalization capacity of the proposed method. We will make all the source code and trained models publicly available.", "title_embedding_index": 20973, "title_abs_embedding_index": 20998}, {"title": "Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause Analysis", "link_suffix": "/forum?id=X7dQuJqs8c", "link": "https://openreview.net/forum?id=X7dQuJqs8c", "pdf_link": "https://openreview.net/pdf?id=X7dQuJqs8c", "keywords": "Root Cause Analysis, Multi-Agent System, Standard Operation Procedure", "abstract": "In the realm of microservices architecture, the occurrence of frequent incidents necessitates the employment of Root Cause Analysis (RCA) for swift issue resolution. It is common that a serious incident can take several domain experts hours to identify the root cause. Consequently, a contemporary trend involves harnessing Large Language Models (LLMs) as automated agents for RCA. Though the recent ReAct framework aligns well with the Site Reliability Engineers (SREs) for its thought-action-observation paradigm, its hallucinations often lead to irrelevant actions and directly affect subsequent results. Additionally, the complex and variable clues of the incident can overwhelm the model one step further. To confront these challenges, we propose Flow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced LLM-based multi-agent system. By explicitly summarizing the diagnosis steps of SREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA process towards the correct trajectory. To facilitate the rational and effective utilization of SOPs, we design an SOP-centric framework called SOP flow. SOP flow contains a series of tools, including one for finding relevant SOPs for incidents, another for automatically generating SOPs for incidents without relevant ones, and a tool for converting SOPs into code. This significantly alleviates the hallucination issues of ReAct in RCA tasks. We also design multiple auxiliary agents to assist the main agent by removing useless noise, narrowing the search space, and informing the main agent whether the RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our Flow-of-Action method achieves 64.01%, meeting the accuracy requirements for RCA in real-world systems.", "title_embedding_index": 20974, "title_abs_embedding_index": 20999}]