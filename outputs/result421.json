[
    {
        "title": "PromptSFL: Improving Visual Prompt Tuning For Split Federated Learning",
        "link_suffix": "/forum?id=Lh30EOD4CT",
        "link": "https://openreview.net/forum?id=Lh30EOD4CT",
        "pdf_link": "https://openreview.net/pdf?id=Lh30EOD4CT",
        "keywords": "Federated Learning, Fine Tuning, Prompts",
        "abstract": "Conflict arises due to the disparity between the substantial resource demands of pre-trained models and the limited available resources of federated learning (FL) participants. Split learning presents a viable approach for adapting pre-trained models to FL, involving the allocation of a small portion of the pre-trained model to clients while deploying the remaining part on a server. Moreover, the application of Visual Prompt Tuning (VPT) to pre-trained models has shown state-of-the-art performances in parameter-efficient fine-tuning methods. However, VPT exhibits unsatisfactory performance in split federated learning (SFL) compared to its performance in centralized learning. In this paper, we first identify that VPT falls short of expectations in SFL due to the insufficient generalization capability of clients. To address this issue, we propose PromptSFL, which aligns the feature spaces of prompts between clients and the server to adapt VPT for SFL. PromptSFL transmits the final prompts in clients, termed skip prompts, to the first prompts in the server, enabling clients to extract more common features from the server. Additionally, we introduce a linear layer to map the prompts from clients to the feature space in the server during this skipping process, preventing the prompts of clients from overfitting to local datasets. Moreover, to enhance the convergence speed of SFL, PromptSFL employs an adaptive learning rate for clients. Extensive experiments demonstrate the effectiveness and efficiency of PromptSFL."
    },
    {
        "title": "NeuroLifting: Neural Inference on Markov Random Fields at Scale",
        "link_suffix": "/forum?id=ZyCuQxyPJK",
        "link": "https://openreview.net/forum?id=ZyCuQxyPJK",
        "pdf_link": "https://openreview.net/pdf?id=ZyCuQxyPJK",
        "keywords": "Markov Random Fields, unsupervised learning, discrete optimization",
        "abstract": "Inference in large-scale Markov Random Fields (MRFs) is a critical yet challenging task, traditionally approached through approximate methods like belief propagation and mean field, or exact methods such as the Toulbar2 solver. These strategies often fail to strike an optimal balance between efficiency and solution quality, particularly as the problem scale increases. This paper introduces NeuroLifting, a novel technique that leverages Graph Neural Networks (GNNs) to reparameterize decision variables in MRFs, facilitating the use of standard gradient descent optimization. By extending traditional lifting techniques into a non-parametric neural network framework, NeuroLifting benefits from the smooth loss landscape of neural networks, enabling efficient and parallelizable optimization. Empirical results demonstrate that, on moderate scales, NeuroLifting performs very close to the exact solver Toulbar2 in terms of solution quality, significantly surpassing existing approximate methods. Notably, on large-scale MRFs, NeuroLifting delivers superior solution quality against all baselines, as well as exhibiting linear computational complexity growth. This work presents a significant advancement in MRF inference, offering a scalable and effective solution for large-scale problems."
    },
    {
        "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
        "link_suffix": "/forum?id=SfZpk8CV9l",
        "link": "https://openreview.net/forum?id=SfZpk8CV9l",
        "pdf_link": "https://openreview.net/pdf?id=SfZpk8CV9l",
        "keywords": "Multimodal Large Language Model",
        "abstract": "Recent advancements in multimodal foundation models have yielded significant progress in vision-language understanding. Initial attempts have also explored the potential of multimodal large language models (MLLMs) for visual content generation. However, existing works have insufficiently addressed the varying granularity demands of different image generation tasks within a unified MLLM paradigm \u2014 from the diversity required in text-to-image generation to the precise controllability needed in image manipulation. In this work, we propose PUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA unifies multi-granular visual features as both inputs and outputs of MLLMs, elegantly addressing the different granularity requirements of various image generation tasks within a unified MLLM framework. Following multimodal pretraining and task-specific instruction tuning, PUMA demonstrates proficiency in a wide range of multimodal tasks, including image understanding, diverse text-to-image generation, editing, inpainting, colorization, and conditional generation. This work represents a significant step towards a truly unified MLLM capable of adapting to the granularity demands of various visual tasks."
    },
    {
        "title": "Efficient Open-world Test Time Adaptation of Vision Language Models",
        "link_suffix": "/forum?id=lF9QXpfNHm",
        "link": "https://openreview.net/forum?id=lF9QXpfNHm",
        "pdf_link": "https://openreview.net/pdf?id=lF9QXpfNHm",
        "keywords": "Test Time Adaptation, Vision Language Models, Robust learning, Domain Adaptation, Open World learning",
        "abstract": "In dynamic real-world settings, models must adapt to changing data distributions, a challenge known as Test Time Adaptation (TTA). Open-world classification, where a model must distinguish between known and unknown classes, further complicates TTA. We introduce ROSITA, a novel method for Open World Single Image Test Time Adaptation using Vision-Language Models (VLMs). ROSITA leverages feature banks and a novel contrastive loss to improve the separation of known and unknown classes, enabling efficient adaptation to domain shifts while equipping the model to reject unknown classes. Our approach sets a new benchmark for this problem, validated through extensive experiments across diverse real-world test environments. Our code is anonymously released at \\url{https://github.com/anon-tta/ROSITA.git}"
    },
    {
        "title": "Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training",
        "link_suffix": "/forum?id=aD2uwhLbnA",
        "link": "https://openreview.net/forum?id=aD2uwhLbnA",
        "pdf_link": "https://openreview.net/pdf?id=aD2uwhLbnA",
        "keywords": "Sharpness-Aware Minimization, Implicit Bias, Training Dynamics",
        "abstract": "Sharpness-Aware Minimization (SAM) has substantially improved the generalization of neural networks under various settings.\nDespite the success, its effectiveness remains poorly understood.\nIn this work, we discover an intriguing phenomenon in the training dynamics of SAM, shedding lights on understanding its implicit bias towards flatter minima over Stochastic Gradient Descent (SGD).\nSpecifically, we find thatSAM efficiently selects flatter minima late in training.\nRemarkably, even a few epochs of SAM applied at the end of training yield nearly the same generalization and solution sharpness as full SAM training.\nSubsequently, we delve deeper into the underlying mechanism behind this phenomenon.\nTheoretically, we identify two phases in the learning dynamics after applying SAM late in training: i) SAM first escapes the minimum found by SGD exponentially fast; and ii) then rapidly converges to a flatter minimum within the same valley.\nFurthermore, we empirically investigate the role of SAM during the early training phase.\nWe conjecture that the optimization method chosen in the late phase is more crucial in shaping the final solution's properties.\nBased on this viewpoint, we extend our findings from SAM to Adversarial Training.\nWe provide source code in supplementary materials and will release checkpoints in future."
    },
    {
        "title": "Zero-Shot Image Compression with Diffusion-Based Posterior Sampling",
        "link_suffix": "/forum?id=qi7udwV66M",
        "link": "https://openreview.net/forum?id=qi7udwV66M",
        "pdf_link": "https://openreview.net/pdf?id=qi7udwV66M",
        "keywords": "zeroshot, compression, diffusion models, posterior sampling",
        "abstract": "Diffusion models dominate the field of image generation, however they have yet to make major breakthroughs in the field of image compression. Indeed, while pre-trained diffusion models have been successfully adapted to a wide variety of downstream tasks, \nexisting work in diffusion-based image compression require task specific model training, which can be both cumbersome and limiting. This work addresses this gap by harnessing the image prior learned by existing pre-trained diffusion models for solving the task of lossy image compression. This enables the use of the wide variety of publicly-available models, and avoids the need for training or fine-tuning. Our method, PSC (Posterior Sampling-based Compression), utilizes zero-shot diffusion-based posterior samplers. It does so through a novel sequential process inspired by the active acquisition technique \"Adasense\" to accumulate informative measurements of the image. This strategy minimizes uncertainty in the reconstructed image and allows for construction of an image-adaptive transform coordinated between both the encoder and decoder. PSC offers a progressive compression scheme that is both practical and simple to implement. Despite minimal tuning, and a simple quantization and entropy coding, PSC achieves competitive results compared to established methods, paving the way for further exploration of pre-trained diffusion models and posterior samplers for image compression."
    },
    {
        "title": "Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object Detection",
        "link_suffix": "/forum?id=cqWD2dpDHW",
        "link": "https://openreview.net/forum?id=cqWD2dpDHW",
        "pdf_link": "https://openreview.net/pdf?id=cqWD2dpDHW",
        "keywords": "Unsupervised 3D Object Detection, Uncertainty Learning",
        "abstract": "Unsupervised 3D object detection aims to identify objects of interest from unlabeled raw data, such as LiDAR points. Recent approaches usually adopt pseudo 3D bounding boxes (3D bboxes) from clustering algorithm to initialize the model training. However, pseudo bboxes inevitably contain noise, and such inaccuracies accumulate to the final model, compromising the performance. Therefore, in an attempt to mitigate the negative impact of inaccurate pseudo bboxes, we introduce a new uncertainty-aware framework for unsupervised 3D object detection, dubbed UA3D. In particular, our method consists of two phases: uncertainty estimation and uncertainty regularization. (1) In the uncertainty estimation phase, we incorporate an extra auxiliary detection branch alongside the original primary detector. The prediction disparity between the primary and auxiliary detectors could reflect fine-grained uncertainty at the box coordinate level. (2) Based on the assessed uncertainty, we adaptively adjust the weight of every 3D bbox coordinate via uncertainty regularization, refining the training process on pseudo bboxes. For pseudo bbox coordinate with high uncertainty, we assign a relatively low loss weight. Extensive experiments verify that the proposed method is robust against the noisy pseudo bboxes, yielding substantial improvements on nuScenes and Lyft compared to existing approaches, with increases of +6.9% AP_BEV and +2.5% AP_3D on nuScenes, and +4.1% AP_BEV and +2.0% AP_3D on Lyft. The anonymous code and checkpoints are athttps://anonymous.4open.science/r/CBC6/."
    },
    {
        "title": "CAPGen: An Environment-Adaptive Generator of Adversarial Patches",
        "link_suffix": "/forum?id=pqxSDbX8XT",
        "link": "https://openreview.net/forum?id=pqxSDbX8XT",
        "pdf_link": "https://openreview.net/pdf?id=pqxSDbX8XT",
        "keywords": "Physical attack, Adversarial patch, Environment consistency",
        "abstract": "Adversarial patches, often used to provide physical stealth protection for critical assets and assess perception algorithm robustness, usually neglect the need for visual harmony with the background environment, making them easily noticeable. Moreover, existing methods primarily concentrate on improving attack performance, disregarding the intricate dynamics of adversarial patch elements. In this work, we introduce the Camouflaged Adversarial Pattern Generator (CAPGen), a novel approach that leverages specific base colors from the surrounding environment to produce patches that seamlessly blend with their background for superior visual stealthiness while maintaining robust adversarial performance. We delve into the influence of both patterns (i.e., color-agnostic texture information) and colors on the effectiveness of attacks facilitated by patches, discovering that patterns exert a more pronounced effect on performance than colors. Based on these findings, we propose a rapid generation strategy for adversarial patches. This involves updating the colors of high-performance adversarial patches to align with those of the new environment, ensuring visual stealthiness without compromising adversarial impact. This paper is the first to comprehensively examine the roles played by patterns and colors in the context of adversarial patches."
    },
    {
        "title": "BiAssemble: Learning Collaborative Affordance for Bimanual Geometric Assembly",
        "link_suffix": "/forum?id=9xsXEj2ile",
        "link": "https://openreview.net/forum?id=9xsXEj2ile",
        "pdf_link": "https://openreview.net/pdf?id=9xsXEj2ile",
        "keywords": "Bimanual Manipulation, Robotics, Shape Assembly",
        "abstract": "Shape assembly, the process of combining parts into a complete whole, is a crucial skill for robots with broad real-world applications. Among the various assembly tasks, geometric assembly\u2014where broken parts are reassembled into their original form (e.g., reconstructing a shattered bowl)\u2014is particularly challenging. This requires the robot to recognize geometric cues for grasping, assembly, and subsequent bimanual collaborative manipulation on varied fragments. In this paper, we exploit the geometric generalization of point-level affordance, learning affordance aware of bimanual collaboration in geometric assembly with long-horizon action sequences. To address the evaluation ambiguity caused by geometry diversity  of broken parts, we introduce a real-world benchmark featuring geometric variety and global reproducibility. Extensive experiments demonstrate the superiority of our approach over both previous affordance-based and imitation-based methods."
    },
    {
        "title": "Croc: Pretraining Large Multimodal Models with Cross-Modal Comprehension",
        "link_suffix": "/forum?id=FyVuLQNHVi",
        "link": "https://openreview.net/forum?id=FyVuLQNHVi",
        "pdf_link": "https://openreview.net/pdf?id=FyVuLQNHVi",
        "keywords": "Visual Language Models; Pretraining",
        "abstract": "Recent advancements in Large Language Models (LLMs) have catalyzed the development of Large Multimodal Models (LMMs). However, existing research primarily focuses on tuning language and image instructions, ignoring the critical pretraining phase where models learn to process textual and visual modalities jointly. This paper proposes a new pretraining paradigm for LMMs to enhance the visual comprehension capabilities of LLMs by introducing a novel cross-modal comprehension stage. Specifically, we design a dynamically learnable prompt token pool and employ the Hungarian algorithm to replace part of the original visual tokens with the most relevant prompt tokens. Then, we conceptualize visual tokens as analogous to a \"foreign language\" for the LLMs and propose a mixed attention mechanism with bidirectional visual attention and unidirectional textual attention to comprehensively enhance the understanding of visual tokens. Meanwhile, we integrate a detailed caption generation task, leveraging rich descriptions to further facilitate LLMs in understanding visual semantic information. After pretraining on 1.5 million publicly accessible data, we present a new foundation model called Croc. Experimental results demonstrate that Croc achieves new state-of-the-art performance on massive vision-language benchmarks. To support reproducibility and facilitate further research, we will release the training code and pre-trained model weights."
    },
    {
        "title": "CoSeC-LCD: Controllable Self-Contrastive Latent Consistency Distillation for Better and Faster Human Animation Generation",
        "link_suffix": "/forum?id=DKYd9tlraw",
        "link": "https://openreview.net/forum?id=DKYd9tlraw",
        "pdf_link": "https://openreview.net/pdf?id=DKYd9tlraw",
        "keywords": "Consistency Distillation; Controllable and Consistent Generation",
        "abstract": "Generating pose-driven and reference-consistent human animation has significant practical applications, yet it remains a prominent research challenge, facing substantial obstacles. A major issue with widely adopted diffusion-based methods is their slow generation speed, which is primarily due to multi-step iterative denoising processes. To tackle this challenge, we take the pioneering step of proposing the ReferenceLCM architecture, which utilizes latent consistency models (LCM) to facilitate accelerated generation. Additionally, to address hallucinations in fine-grained control, we introduce the Controllable Self-Contrastive Latent Consistency Distillation (CoSeC-LCD) regularization method. Our approach introduces a novel perspective by categorizing tasks into various classes and employing contrastive learning to capture underlying patterns. Building on this insight, we implement a hierarchical optimization strategy that significantly enhances animation quality across both spatial and temporal aspects. Comprehensive qualitative and quantitative experiments reveal that our method achieves results comparable to, or even surpassing, many state-of-the-art approaches, enabling high-fidelity human animation generation within just 2-4 inference steps."
    },
    {
        "title": "CrayonRobo: Toward Generic Robot Manipulation via Crayon Visual Prompting",
        "link_suffix": "/forum?id=Aqfwhna1D7",
        "link": "https://openreview.net/forum?id=Aqfwhna1D7",
        "pdf_link": "https://openreview.net/pdf?id=Aqfwhna1D7",
        "keywords": "Robotic manipulation",
        "abstract": "In robotic manipulation, there are several ways to convey the task goal, including language conditions, goal images, and goal videos. However, natural language can be ambiguous, and images or videos can be over-specified. To address this issue, we propose an innovative approach using a straightforward and practical representation: crayon visual prompts, which explicitly indicate both low-level actions and high-level planning.\nSpecifically, for each atomic step, our method allows drawing simple yet expressive 2D visual prompts on RGB images to represent the required actions, i.e., end-effector pose and moving direction. We devise a training strategy that enables the model to comprehend each color prompt and predict the contact pose along with the movement direction in SE(3) space. Furthermore, we design an interaction strategy that leverages the predicted movement direction to form a trajectory connecting the sequence of atomic steps, thereby completing the long-horizon task.\nThrough introducing simple human drawn prompts or automatically generated alternatives, we enable the model to explicitly understand its task objective and boost its generalization ability on unseen tasks by providing model-understandable crayon visual prompts.\nWe evaluate our method in both simulation and real-world environments, demonstrating its promising performance."
    },
    {
        "title": "DC-DPM: A Divide-and-Conquer Approach for Diffusion Reverse Process",
        "link_suffix": "/forum?id=VbAxCwV2e3",
        "link": "https://openreview.net/forum?id=VbAxCwV2e3",
        "pdf_link": "https://openreview.net/pdf?id=VbAxCwV2e3",
        "keywords": "Diffusion Model; Reverse Process Transition Kernel; Divide-and-Conquer",
        "abstract": "Diffusion models have achieved great success in generative tasks. However, previous approaches typically approximate the reversed transition kernel with a Gaussian distribution. This approximation can diverge from real scenarios, necessitating multiple iterative steps for high-quality sample generation and limiting the real-time inference performance of diffusion models.\nIn this paper, we propose a \\textbf{D}ivide-and-\\textbf{C}onquer strategy to improve the traditional single Gaussian transition kernel representation in each denoising step of \\textbf{D}iffusion \\textbf{P}robabilistic \\textbf{M}odels (DC-DPM), thus enhancing generation quality particularly over a limited number of timesteps. By dividing the data into clusters, our DC-DPM learns specific kernels for each partition. We design two merging strategies for these cluster-specific kernels along with corresponding training and sampling methods.\nWe provide theoretical proof of DC-DPM's convergence to the true data distribution from a novel perspective. Experimental results demonstrate the superior generation quality of our method compared to the traditional single Gaussian kernel. Furthermore, our DC-DPM can synergize with previous kernel optimization methods, enhancing their generation quality, especially with a small number of timesteps."
    },
    {
        "title": "Boosting the visual interpretability of CLIP via adversarial fine-tuning",
        "link_suffix": "/forum?id=khuIvzxPRp",
        "link": "https://openreview.net/forum?id=khuIvzxPRp",
        "pdf_link": "https://openreview.net/pdf?id=khuIvzxPRp",
        "keywords": "interpretability, vision-language models, CLIP",
        "abstract": "CLIP has achieved great success in visual representation learning and is becoming an important plug-in component for many large multi-modal models like LLaVA and DALL-E. However, the lack of interpretability caused by the intricate image encoder architecture and training process restrict its wider use in high-stake decision making applications. In this work, we propose an unsupervised adversarial fine-tuning (AFT) with norm-regularization to enhance the visual interpretability of CLIP. We provide theoretical analysis showing that AFT has implicit regularization that enforces the image encoder to encode the input features sparsely, directing the network's focus towards meaningful features. Evaluations by both feature attribution techniques and network dissection offer convincing evidence that the visual interpretability of CLIP has significant improvements. With AFT, the image encoder priorities pertinent input features, and the neuron within the encoder exhibit better alignment with human-understandable concepts. Moreover, these effects are generalizable to out-of-distribution datasets and can be transferred to downstream tasks. Additionally, AFT enhances the visual interpretability of derived large vision-language models that incorporate the pre-trained CLIP an integral component. The code of this work will be made publicly available."
    },
    {
        "title": "Mitigating Copy Bias in In-Context Learning through Neuron Pruning",
        "link_suffix": "/forum?id=Hs1UTIOwKr",
        "link": "https://openreview.net/forum?id=Hs1UTIOwKr",
        "pdf_link": "https://openreview.net/pdf?id=Hs1UTIOwKr",
        "keywords": "large language models, in-context learning, interpretability, llms",
        "abstract": "Large language models (LLMs) have demonstrated impressive few-shot in-context learning (ICL) abilities. Still, we show that they are sometimes prone to a `copying bias', where they copy answers from provided examples instead of learning the underlying patterns. In this work, we propose a novel and simple method to mitigate such copying bias.  First, we create a synthetic task and use the Integrated Gradients method to identify neurons that prioritize copying over generalization. We demonstrate that pruning these neurons consistently improves performance across a diverse set of ICL tasks. We also show that our method is applicable across various LLM architectures, including Transformers and State-Space Models, without requiring modifications.In our analysis, we adopt a task-recognition perspective on ICL and examine task vectors (Hendel et al., 2023) induced by the model. We find that pruning enhances the quality of these vectors, suggesting that the pruned neurons previously hindered effective task recognition."
    },
    {
        "title": "DiffGS: Repurposing Image Diffusion Models for Scalable Gaussian Splatting Generation",
        "link_suffix": "/forum?id=eajZpoQkGK",
        "link": "https://openreview.net/forum?id=eajZpoQkGK",
        "pdf_link": "https://openreview.net/pdf?id=eajZpoQkGK",
        "keywords": "3D generation, diffusion models, 3D Gaussian Splatting",
        "abstract": "Recent advancements in 3D content generation from text or a single image struggle with limited high-quality 3D datasets and inconsistency from 2D multi-view generation. We introduce DiffGS, a novel 3D generative framework that natively generates 3D Gaussians by taming large-scale text-to-image diffusion models. It differs from previous 3D generative models by effectively utilizing web-scale 2D priors while maintaining 3D consistency in a unified model. To bootstrap the training, a lightweight reconstruction model is proposed to instantly produce multi-view Gaussian grids for scalable dataset curation. In conjunction with the regular diffusion loss on these grids, a 3D rendering loss is introduced to facilitate 3D coherence across arbitrary views. The compatibility with image diffusion models enables seamless adaptions of numerous techniques for image generation to the 3D realm. Extensive experiments reveal the superiority of DiffGS in text- and image-conditioned generation tasks and downstream applications. Thorough ablation studies validate the efficacy of each critical design choice and provide insights into the underlying mechanism. Code and models will be publicly available."
    },
    {
        "title": "Overcoming Catastrophic Forgetting: A Novel Fine-Tuning Method",
        "link_suffix": "/forum?id=F8BPhZ5nmU",
        "link": "https://openreview.net/forum?id=F8BPhZ5nmU",
        "pdf_link": "https://openreview.net/pdf?id=F8BPhZ5nmU",
        "keywords": "lifelong learning",
        "abstract": "Despite remarkable advances in Large Language Models (LLMs), a persistent challenge remains: the potential for these models to acquire erroneous or outdated information from their training data. Direct fine-tuning with data containing new knowledge can be ineffective due to conflicts between old and new knowledge. This paper proposes a novel fine-tuning paradigm called Delicate Fine-Tuning (DFT ) that leverages parametric arithmetic to pinpoint the location of knowledge and update only the minimal set of relevant parameters. Experimental results on two publicly available datasets demonstrate that our proposed DFT significantly improves the knowledge updating performance of full fine-tuning, consistently outperforming existing baselines in most cases."
    },
    {
        "title": "Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining",
        "link_suffix": "/forum?id=T1OvCSFaum",
        "link": "https://openreview.net/forum?id=T1OvCSFaum",
        "pdf_link": "https://openreview.net/pdf?id=T1OvCSFaum",
        "keywords": "reinforcement learning, offline reinforcement learning, world model",
        "abstract": "A significant aspiration of offline reinforcement learning (RL) is to develop a generalist agent with high capabilities from large and heterogeneous datasets. However, prior approaches that scale offline RL either rely heavily on expert trajectories or struggle to generalize to diverse unseen tasks. Inspired by the excellent generalization of world model in conditional video generation, we explore the potential of image observation-based world model for scaling offline RL and enhancing generalization on novel tasks. In this paper, we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based RL agent pretrained on multiple Atari games with 6 billion tokens data to learn general-purpose representation and decision-making ability. Our method jointly optimizes a world-action model through a shared transformer backbone, which stabilize temporal difference learning with large models during pretraining. Moreover, we propose a provably efficient and parallelizable planning algorithm to compensate for the Q-value estimation error and thus search out better policies. Experimental results indicate that our largest agent, with 150 million parameters, achieves 78.9% human-level performance on pretrained games using only 10% subsampled offline data, outperforming existing state-of-the-art large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales favorably with model capacity and can sample-efficiently transfer to novel games using only 5k offline fine-tuning data (approximately 4 trajectories) per game, demonstrating superior generalization."
    },
    {
        "title": "Deep Incomplete Multi-view Learning via Cyclic Permutation of VAEs",
        "link_suffix": "/forum?id=s4MwstmB8o",
        "link": "https://openreview.net/forum?id=s4MwstmB8o",
        "pdf_link": "https://openreview.net/pdf?id=s4MwstmB8o",
        "keywords": "Multi-View Learning, Representation Learning, Multimodal VAEs, Generative Models",
        "abstract": "Multi-View Representation Learning (MVRL) aims to derive a unified representation from multi-view data by leveraging shared and complementary information across views. However, when views are irregularly missing, the incomplete data can lead to representations that lack sufficiency and consistency. To address this, we propose Multi-View Permutation of Variational Auto-Encoders (MVP), which excavates invariant relationships between views in incomplete data. MVP establishes inter-view correspondences in the latent space of Variational Auto-Encoders, enabling the inference of missing views and the aggregation of more sufficient information. To derive a valid Evidence Lower Bound (ELBO) for learning, we apply permutations to randomly reorder variables for cross-view generation and then partition them by views to maintain invariant meanings under permutations. Additionally, we enhance consistency by introducing an informational prior with cyclic permutations of posteriors, which turns the regularization term into a similarity measure across distributions. We demonstrate the effectiveness of our approach on seven diverse datasets with varying missing ratios, achieving superior performance in multi-view clustering and generation tasks."
    },
    {
        "title": "X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention",
        "link_suffix": "/forum?id=ML8FH4s5Ts",
        "link": "https://openreview.net/forum?id=ML8FH4s5Ts",
        "pdf_link": "https://openreview.net/pdf?id=ML8FH4s5Ts",
        "keywords": "Portrait Animation, Head Avatar, Conditional Video Generation",
        "abstract": "We propose X-NeMo, a novel zero-shot diffusion-based portrait animation pipeline that animates a static portrait using facial movements from a driving video of a different individual. Our work first identifies the root causes of the limitations in prior approaches, such as identity leakage and difficulty in capturing subtle and extreme expressions. To address these challenges, we introduce a fully end-to-end training framework that distills a 1D identity-agnostic latent motion descriptor from driving image, effectively controlling motion through cross-attention during image generation. Our implicit motion descriptor captures expressive facial motion in fine detail, learned end-to-end from a diverse video dataset without reliance on any pre-trained motion detectors.  We further disentangle motion latents from identity cues with enhanced expressiveness by supervising their learning with a dual GAN decoder, alongside spatial and color augmentations. By embedding the driving motion into a 1D latent vector and controlling motion via cross-attention instead of additive spatial guidance, our design effectively eliminates the transmission of spatial-aligned structural clues from the driving condition to the diffusion backbone, substantially mitigating identity leakage. Extensive experiments demonstrate that X-NeMo surpasses state-of-the-art baselines, producing highly expressive animations with superior identity resemblance. Our code and models will be available for research."
    },
    {
        "title": "LIME-Eval: Rethinking Low-light Image Enhancement Evaluation via Object Detection",
        "link_suffix": "/forum?id=hYU0P4Wlj9",
        "link": "https://openreview.net/forum?id=hYU0P4Wlj9",
        "pdf_link": "https://openreview.net/pdf?id=hYU0P4Wlj9",
        "keywords": "Low-light enhancement, Image quality assessment",
        "abstract": "Due to the nature of enhancement--the absence of paired ground-truth information, high-level vision tasks have been recently employed to evaluate the performance of low-light image enhancement.  A widely-used manner is to see how accurately an object detector trained on enhanced low-light images by different candidates can perform with respect to annotated semantic labels. In this paper, we first demonstrate that the mentioned approach is generally prone to overfitting, and thus diminishes its measurement reliability. In search of a proper evaluation metric, we propose LIME-Bench, the first online benchmark platform designed to collect human preferences for low-light enhancement, providing a valuable dataset for validating the correlation between human perception and automated evaluation metrics. We then customize LIME-Eval, a novel evaluation framework that utilizes detectors pre-trained on standard-lighting datasets without object annotations, to judge the quality of enhanced images. By adopting an energy-based strategy to assess the accuracy of output confidence maps, our LIME-Eval can simultaneously bypass biases associated with retraining detectors and circumvent the reliance on annotations for dim images. Comprehensive experiments are provided to reveal the effectiveness of our LIME-Eval. Our code will be made publicly available."
    },
    {
        "title": "Spectral Truncation Kernels: Noncommutativity inC\u2217-algebraic Kernel Machines",
        "link_suffix": "/forum?id=5GZuEZDmUE",
        "link": "https://openreview.net/forum?id=5GZuEZDmUE",
        "pdf_link": "https://openreview.net/pdf?id=5GZuEZDmUE",
        "keywords": "kernel methods, positive definite kernel, spectral truncation",
        "abstract": "$C^*$-algebra-valued kernels could pave the way for the next generation of kernel machines. To further our fundamental understanding of learning with $C^*$-algebraic kernels, we propose a new class of positive definite kernels based on the spectral truncation. We focus on kernels whose inputs and outputs are vectors or functions and generalize typical kernels by introducing the noncommutativity of the products appearing in the kernels. The noncommutativity induces interactions along the data function domain. We show that it is a governing factor leading to performance enhancement: we can balance the representation power and the model complexity. We also propose a deep learning perspective to increase the representation capacity of spectral truncation kernels. The flexibility of the proposed class of kernels allows us to go beyond previous commutative kernels, addressing two of the foremost issues regarding learning in vector-valued RKHSs, namely the choice of the kernel and the computational cost."
    },
    {
        "title": "Real3D: Towards Scaling Up Large Reconstruction Models with Real-World Images",
        "link_suffix": "/forum?id=Ffuw2ryqpz",
        "link": "https://openreview.net/forum?id=Ffuw2ryqpz",
        "pdf_link": "https://openreview.net/pdf?id=Ffuw2ryqpz",
        "keywords": "Large Reconstruction Model, Single-view Reconstruction, In-the-wild Data, Model Self-Training",
        "abstract": "The default strategy for training single-view Large Reconstruction Models (LRMs) follows the fully supervised route using large-scale datasets of synthetic 3D assets or multi-view captures. Although these resources simplify the training procedure, they are hard to scale up beyond the existing datasets and they are not necessarily representative of the real distribution of object shapes. To address these limitations, in this paper, we introduce Real3D, the first LRM system that can be trained using single-view real-world images. Real3D introduces a novel self-training framework that can benefit from both the existing synthetic data and diverse single-view real images. We propose two unsupervised losses that allow us to supervise LRMs at the pixel- and semantic-level, even for training examples without ground-truth 3D or novel views. To further improve performance and scale up the image data, we develop an automatic data curation approach to collect high-quality examples from in-the-wild images. Our experiments show that Real3D consistently outperforms prior work in four diverse evaluation settings that include real and synthetic data, as well as both in-domain and out-of-domain shapes. We will make our code, models and data available upon publication."
    },
    {
        "title": "VOVTrack: Exploring the Potentiality in Videos for Open-Vocabulary Object Tracking",
        "link_suffix": "/forum?id=3vxfFFP3q5",
        "link": "https://openreview.net/forum?id=3vxfFFP3q5",
        "pdf_link": "https://openreview.net/pdf?id=3vxfFFP3q5",
        "keywords": "Object Tracking, Open-Vocabulary",
        "abstract": "Open-vocabulary multi-object tracking (OVMOT) represents a critical new challenge involving the detection and tracking of diverse object categories in videos, encompassing both seen categories (base classes) and unseen categories (novel classes). This issue amalgamates the complexities of open-vocabulary object detection (OVD) and multi-object tracking (MOT). Existing approaches to OVMOT often merge OVD and MOT methodologies as separate modules, predominantly focusing on the problem through an image-centric lens. In this paper, we propose OVTracker, a novel method that integrates object states relevant to MOT and video-centric training to address this challenge from a video object tracking standpoint. First, we consider the tracking-related state of the objects during tracking and propose a new prompt-guided attention mechanism for more accurate localization and classification (detection) of the time-varying objects. Subsequently,\nwe leverage raw video data without annotations by formulating a self-supervised object similarity learning technique to facilitate temporal object association (tracking). Experimental results underscore that OVTracker outperforms existing methods, establishing itself as a state-of-the-art solution for open-vocabulary tracking tasks."
    },
    {
        "title": "Improving Group Connectivity for Generalization of Federated Deep Learning",
        "link_suffix": "/forum?id=v3W9tdTGx5",
        "link": "https://openreview.net/forum?id=v3W9tdTGx5",
        "pdf_link": "https://openreview.net/pdf?id=v3W9tdTGx5",
        "keywords": "Deep learning, federated learning, generalization",
        "abstract": "Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental \"connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term \"connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propose FedGuCci(+), improving group connectivity for better generalization. It is shown that our methods can boost the generalization of FL under client heterogeneity across various tasks (4 CV datasets and 6 NLP datasets) and model architectures (e.g., ViTs and PLMs)."
    }
]