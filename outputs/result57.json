[
    {
        "title": "Direct Preference Optimization With Unobserved Preference Heterogeneity",
        "link_suffix": "/forum?id=NQZNNUsutn",
        "link": "https://openreview.net/forum?id=NQZNNUsutn",
        "pdf_link": "https://openreview.net/pdf?id=NQZNNUsutn",
        "keywords": "RLHF, LLM Alignment, Preference Aggregation",
        "abstract": "RLHF has emerged as a pivotal step in aligning language models with human objectives and values. It typically involves learning a reward model from human preference data and then using reinforcement learning to update the generative model accordingly. Conversely, Direct Preference Optimization (DPO) directly optimizes the generative model with preference data, skipping reinforcement learning. However, both RLHF and DPO assume uniform preferences, overlooking the reality of diverse human annotators. This paper presents a new method to align generative models with varied human preferences. We propose an Expectation-Maximization adaptation to DPO, generating a mixture of models based on latent preference types of the annotators. We then introduce a min-max regret ensemble learning model to produce a single generative method to minimize worst-case regret among annotator subgroups with similar latent factors. Our algorithms leverage the simplicity of DPO while accommodating diverse preferences. Experimental results validate the effectiveness of our approach in producing equitable generative policies."
    },
    {
        "title": "Efficient Bayesian DNN Compression through Sparse Quantized Sub-distributions",
        "link_suffix": "/forum?id=u48BF5O7oL",
        "link": "https://openreview.net/forum?id=u48BF5O7oL",
        "pdf_link": "https://openreview.net/pdf?id=u48BF5O7oL",
        "keywords": "Bayesian Deep Neural Networks, Quantization, Pruning, Variational Inference",
        "abstract": "This paper presents a novel method that simultaneously achieves model pruning and low-bit quantization through Bayesian variational inference to effectively compress deep neural networks (DNNs) while suffering minimal performance degradation. \nUnlike previous approaches that treat pruning and quantization as separate, sequential tasks, our method explores a unified optimization space, enabling more efficient compression. \nBy leveraging a spike-and-slab prior combined with Gaussian Mixture Models (GMM), we can achieve both network sparsity and low-bit representation. Experiments on CIFAR-10, CIFAR-100, and SQuAD datasets demonstrate that our approach achieves compression rates of up to 32x with less than a $1.3\\%$ accuracy loss on the CIFAR datasets and a 1.66 point decrease in F1 score on SQuAD. Additionally, we show that the Bayesian model average of neural networks can further mitigate the impact of quantization noise, leading to more robust compressed models. Our method outperforms existing techniques in both compression efficiency and accuracy retention, offering a promising solution for compressing DNNs."
    },
    {
        "title": "Physics-Informed Deep B-Spline Networks",
        "link_suffix": "/forum?id=rCvdAVQpAe",
        "link": "https://openreview.net/forum?id=rCvdAVQpAe",
        "pdf_link": "https://openreview.net/pdf?id=rCvdAVQpAe",
        "keywords": "Physics-informed machine learning, B-splines, Partial differential equations (PDEs)",
        "abstract": "Physics-informed machine learning provides an approach to combing data and governing physics laws for solving complex partial differential equations (PDEs). However, efficiently solving PDEs with varying parameters and changing initial conditions and boundary conditions (ICBCs) remains an open challenge. We propose a hybrid framework that uses a neural network to learn B-spline control points to approximate solutions to PDEs with varying system and ICBC parameters. The proposed network can be trained efficiently as one can directly specify ICBCs without imposing losses, calculate physics-informed loss functions through analytical formulas, and requires only learning the weights of B-spline functions as opposed to both weights and basis as in traditional neural operator learning methods. We show theoretical guarantees that the proposed B-spline networks are universal approximators of arbitrary dimensional PDEs under certain conditions. We also demonstrate in experiments that the proposed B-spline network can solve problems with discontinuous ICBCs and outperforms existing methods, and is able to learn solutions of 3D heat equations with diverse initial conditions."
    },
    {
        "title": "Graph Distributional Analytics: Enhancing GNN Explainability through Scalable Embedding and Distribution Analysis",
        "link_suffix": "/forum?id=Fzz8acgC6X",
        "link": "https://openreview.net/forum?id=Fzz8acgC6X",
        "pdf_link": "https://openreview.net/pdf?id=Fzz8acgC6X",
        "keywords": "Graph Neural Networks, Explainability, Graph Distributional Analytics, Weisfeiler-Leman graph kernel, Graph embeddings, Distributional analysis, Out-of-distribution data, Model transparency, Structural features, Machine learning, Graph classification, Scalable methods, GNN interpretability, Model robustness",
        "abstract": "Graph Neural Networks (GNNs) have achieved significant success in processing graph-structured data but often lack interpretability, limiting their practical applicability. We introduce the Graph Distributional Analytics (GDA) framework, leveraging novel combinations of scalable techniques to enhance GNN explainability. The integration of Weisfeiler-Leman (WL) graph kernels with distributional distance analysis enables GDA to efficiently quantify graph data distributions, while capturing global structural complexities without significant computational costs. GDA creates high-dimensional embeddings employing WL kernels, measures the distribution of distances from measures of categorical central tendency, and assigns distribution scores to quantify each graph's deviation from this vector We evaluate GDA on the ENZYMES, ogbg-ppa, and MalNet-Tiny datasets. Our experiments demonstrate GDA not only accurately characterizes graph distributions but also outperforms baseline methods in identifying specific structural features responsible for misclassifications. This comprehensive analysis provides deeper insights into how training data distributions affect model performance, particularly with out-of-distribution (OOD) data. By revealing the underlying structural causes of GNN predictions through a novel synergy of established techniques, GDA enhances transparency and offers a practical tool for practitioners to build more interpretable and robust graph-based models. Our framework's scalability, efficiency, and ability to integrate with various embedding methods make it a valuable addition to the suite of tools available for GNN analysis."
    },
    {
        "title": "Generalized Smooth Stochastic Variational Inequalities:  Almost Sure Convergence and Convergence Rates",
        "link_suffix": "/forum?id=LVmafig6Tk",
        "link": "https://openreview.net/forum?id=LVmafig6Tk",
        "pdf_link": "https://openreview.net/pdf?id=LVmafig6Tk",
        "keywords": "stochastic variational inequalities, generalized smoothness, clipping",
        "abstract": "This paper focuses on solving a stochastic variational inequality (SVI) problem under relaxed smoothness assumption for a class of structured non-monotone operators. The SVI problem has attracted significant interest in the machine learning community due to its immediate application to adversarial training and multi-agent reinforcement learning. In many such applications, the resulting operators do not satisfy the smoothness assumption. To address this issue, we focus on the generalized smoothness assumption and consider two well-known stochastic methods with clipping, namely, projection and Korpelevich. For these clipped methods, we provide the first almost-sure convergence results without making any assumptions on the boundedness of either the stochastic operator or the stochastic samples. Furthermore, we provide the first in-expectation convergence rate results for these methods under a relaxed smoothness assumption."
    },
    {
        "title": "Improve Vision Language Model Chain-of-thought Reasoning",
        "link_suffix": "/forum?id=XgYZT35N76",
        "link": "https://openreview.net/forum?id=XgYZT35N76",
        "pdf_link": "https://openreview.net/pdf?id=XgYZT35N76",
        "keywords": "Vision Language Model, Chain-of-thought Reasoning",
        "abstract": "Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. \nHowever, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we first evaluate the CoT abilities of existing VLMs and show that training on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality by constructing positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, based on the comparisons with annotated short answers. We then use the Direct Preference Optimization algorithm on this pairwise data to refine the model’s reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs."
    },
    {
        "title": "Binary Reward Labeling: Bridging Offline Preference and Reward-Based Reinforcement Learning",
        "link_suffix": "/forum?id=1Ffzgglq2I",
        "link": "https://openreview.net/forum?id=1Ffzgglq2I",
        "pdf_link": "https://openreview.net/pdf?id=1Ffzgglq2I",
        "keywords": "Preference based reinforcement learning; Offline reinforcement learning",
        "abstract": "Offline reinforcement learning has become one of the most practical RL settings. However, most existing works on offline RL focus on the standard setting with scalar reward feedback. It remains unknown how to universally transfer the existing rich understanding of offline RL from the reward-based to the preference-based setting. In this work, we propose a general framework to bridge this gap. Our key insight is transforming preference feedback to scalar rewards via binary reward labeling (BRL), and then any reward-based offline RL algorithms can be applied to the dataset with the reward labels. The information loss during the feedback signal transition is minimized with binary reward labeling in the practical learning scenarios. We theoretically show the connection between several recent PBRL techniques and our framework combined with specific offline RL algorithms. By combining reward labeling with different algorithms, our framework can lead to new and potentially more efficient offline PBRL algorithms. We empirically test our framework on preference datasets based on the standard D4RL benchmark. When combined with a variety of efficient reward-based offline RL algorithms, the learning result achieved under our framework is comparable to training the same algorithm on the dataset with actual rewards in many cases and better than the recent PBRL baselines in most cases."
    },
    {
        "title": "AI Derivation and Exploration of Antibiotic Class Spaces",
        "link_suffix": "/forum?id=IZiKBis0AA",
        "link": "https://openreview.net/forum?id=IZiKBis0AA",
        "pdf_link": "https://openreview.net/pdf?id=IZiKBis0AA",
        "keywords": "fragment-based drug design, antibiotic resistance, pharmacokinetics, hybrid antibiotics, in silico analysis, retrosynthesis, chemical space exploration, machine learning, antibiotic discovery, protein targets",
        "abstract": "This paper presents a novel approach to fragment-based antibiotic drug design design. We introduce a tool called FILTER, which uses chemical structure data, pathway information, and protein targets to predict pharmacokinetic properties of existing and novel drugs. We report on three distinct experiments utilizing FILTER. The first experiment is an in silico analysis that recreates the historical discovery of penicillin derivatives, validating our approach against known outcomes. The second experiment explores the combination of functional groups from different antibiotic classes to create molecules with multiple mechanisms of action. We refer to this approach as hybridization as all synthesized molecules are composed of fragments from both classes. Our final experiment is forward-looking as it explores new chemical spaces to build a library of promising compounds for further antibiotic development. Throughout all these experiments, FILTER serves as an indispensable oracle, predicting physical properties and potential therapeutic efficacy of the new molecular architectures, aiming to accelerate the drug development process and address the challenge of antibiotic resistance. Our approach represents an ongoing, significant shift from traditional drug discovery methods, emphasizing the role of innovative technologies in combating the urgent global threat of antimicrobial resistance."
    },
    {
        "title": "BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Tumor Bone Metastasis Diagnosis and Prognosis",
        "link_suffix": "/forum?id=YH4M1Tbxfz",
        "link": "https://openreview.net/forum?id=YH4M1Tbxfz",
        "pdf_link": "https://openreview.net/pdf?id=YH4M1Tbxfz",
        "keywords": "Medical Dataset, Breast Tumor Bone Metastasis, Diagnosis, Prognosis, Sparse CT reconstruction, CT, X-ray, AI for Science",
        "abstract": "Breast tumor bone metastasis (BTBM) affects women’s health globally, calling for the development of effective solutions for its diagnosis and prognosis. While the deep learning has exhibited impressive capacities across various healthcare domains, their applicability to managing BTBM diseases is consistently hindered by the lack of an open, large-scale, deep learning-ready dataset. As such, we introduce the Bone Metastasis (BoneMet) dataset, the first large-scale, publicly available, high-resolution medical resource specifically targeting BTBM for disease diagnosis, prognosis, and treatment management. It offers over 50 terabytes of multi-modal medical data, including 2D X-ray images, 3D CT scans, and detailed biological data (e.g., medical records and bone quantitative analysis), collected from thousands of mice spanning from 2019 to 2024. Our BoneMet dataset is well-organized into six components, i.e., Rotation-X-Ray, Recon-CT, Seg-CT, Regist-CT, RoI-CT, and MiceMediRec. Thanks to its extensive data samples and our tireless efforts of image processing, organization and data labeling, BoneMet can be readily adopted to build versatile, large-scale AI models for managing BTBM diseases, which have been validated by our extensive experiments via various deep learning solutions. To facilitate its easy access and wide dissemination, we have created the BoneMet package, providing three APIs that enable researchers to (i)flexibly process and download the BoneMet data filtered by specific time frames;and (ii) develop and train large-scale AI models for precise BTBM diagnosis and prognosis. The BoneMet dataset is officially available on Hugging Face Datasets athttps://huggingface.co/datasets/BoneMet/BoneMet. The BoneMet package is available on the Python Package Index (PyPI) athttps://pypi.org/project/BoneMet. Code and tutorials are available athttps://github.com/BoneMet/BoneMet."
    },
    {
        "title": "Time After Time: Scalable Effect Estimation for Interventions on When and What to do",
        "link_suffix": "/forum?id=5yDS32hKJc",
        "link": "https://openreview.net/forum?id=5yDS32hKJc",
        "pdf_link": "https://openreview.net/pdf?id=5yDS32hKJc",
        "keywords": "effect estimation, treatment times, irregular times, sequential decision making",
        "abstract": "Decision support in fields such as healthcare and finance requires reasoning about treatment timing. Artificial Intelligence holds great potential for supporting such decisions by estimating the causal effect of policies such as medication regimens, or resource allocation schedules. However, existing methods for effect estimation are limited in their ability to handle \\emph{irregular time}. While treatments and observations in data are often irregularly spaced across the timeline, existing techniques either discretize time, do not scale gracefully to large models, or disregard the effect of treatment time.We present a solution for effect estimation of sequential treatment times called Earliest Disagreement Q-Evaluation (EDQ). The method is based on Dynamic Programming and is compatible with flexible sequence models, such as transformers. It provides accurate estimates under the assumptions of ignorability, overlap, and no-instantaneous effects. We validate the approach through experiments on a survival time prediction task."
    },
    {
        "title": "One Communication Round is All It Needs for Federated Fine-Tuning Foundation Models",
        "link_suffix": "/forum?id=kH5nNlgT52",
        "link": "https://openreview.net/forum?id=kH5nNlgT52",
        "pdf_link": "https://openreview.net/pdf?id=kH5nNlgT52",
        "keywords": "Foundation Models, Federated Fine-Tuning, One-shot Federated Learning",
        "abstract": "The recent advancement of large foundation models (FMs) has increased the demand for fine-tuning these models on large-scale and cross-domain datasets. To address this, federated fine-tuning has emerged as a solution, allowing models to be fine-tuned on distributed datasets across multiple devices while ensuring data privacy. However, the substantial parameter size of FMs and the multi-round communication required by traditional federated fine-tuning algorithms result in prohibitively high communication costs, challenging the practicality of federated fine-tuning. In this paper, we are the first to reveal, both theoretically and empirically, that the traditional multi-round aggregation algorithms may not be necessary for federated fine-tuning large FMs. Our experiments reveal that a single round of communication (i.e., one-shot federated fine-tuning) yields a global model performance comparable to that achieved through multiple rounds of communication.\nThrough rigorous mathematical and empirical analyses, we demonstrate that large FMs, due to their extensive parameter sizes and pre-training on general tasks, achieve significantly lower training loss in one-shot federated fine-tuning compared to smaller models.\nOur extensive experiments show that one-shot federated fine-tuning not only reduces communication costs but also enables asynchronous aggregation, enhances privacy, and maintains performance consistency with multi-round federated fine-tuning for models larger than 1 billion parameters, on text generation and text-to-image generation tasks. Our findings have the potential to revolutionize federated fine-tuning in practice, enhancing efficiency, reducing costs, and expanding accessibility for large-scale models. This breakthrough paves the way for broader adoption and application of federated fine-tuning across various domains."
    },
    {
        "title": "AERO: Softmax-Only LLMs for Efficient Private Inference",
        "link_suffix": "/forum?id=CPBdBmnkA5",
        "link": "https://openreview.net/forum?id=CPBdBmnkA5",
        "pdf_link": "https://openreview.net/pdf?id=CPBdBmnkA5",
        "keywords": "Private inference, LLMs, Architectural Optimization, Entropy Regularization",
        "abstract": "The pervasiveness of proprietary language models has raised privacy concerns for users' sensitive data, emphasizing the need for private inference (PI), where inference is performed directly on encrypted inputs. However, current PI methods face prohibitively higher communication and latency overheads,  primarily due to nonlinear operations. In this paper, we present a comprehensive analysis to understand the role of nonlinearities in transformer-based decoder-only language models.  We introduce AERO, a four-step architectural optimization framework that refines the existing LLM architecture for efficient PI by systematically removing nonlinearities such as LayerNorm and GELU and reducing FLOPs counts. For the {\\em first time}, we propose a Softmax-only architecture with significantly fewer FLOPs tailored for efficient PI.  Furthermore, we devise a novel entropy regularization technique to improve the performance of Softmax-only models. AERO achieves up to 4.23$\\times$ communication and 1.94$\\times$ latency reduction. We validate the effectiveness of AERO by benchmarking it against the state-of-the-art."
    },
    {
        "title": "Ego-centric Learning of Communicative World Models for  Autonomous Driving",
        "link_suffix": "/forum?id=7orD38wzdi",
        "link": "https://openreview.net/forum?id=7orD38wzdi",
        "pdf_link": "https://openreview.net/pdf?id=7orD38wzdi",
        "keywords": "World Model, Reinforcement Learning, Autonomous Driving, Distributed Learning",
        "abstract": "We study multi-agent reinforcement learning (MARL) for tasks in complex high-dimensional environments, such as autonomous driving. \nMARL is known to suffer from thepartial observabilityandnon-stationarityissues. To tackle these challenges, information sharing is often employed, which however faces major hurdles in practice, including overwhelming communication overhead and scalability concerns. Based on the key observation that world model encodes high-dimensional inputs to low-dimensional latent representation with a  small memory footprint, we developCALL,  {C}ommunic{a}tive Wor{l}d Mode{l}, for ego-centric MARL, where  1) each agent \nfirst learns its world model that encodes its state and intention into low-dimensional latent representation which can be  shared with other agents of interest via lightweight communication; and 2) each agent carries out ego-centric learning while exploiting lightweight information sharing to enrich  her world model learning and improve prediction for better planning. We characterize the gain on the prediction accuracy from the information sharing and its impact on performance  gap. Extensive experiments are carried out on the challenging local trajectory planning tasks in the CARLA platform to demonstrate the performance gains of  usingCALL."
    },
    {
        "title": "Phase-aware Training Schedule Simplifies Learning in Flow-Based Generative Models",
        "link_suffix": "/forum?id=SEvJfuCtPY",
        "link": "https://openreview.net/forum?id=SEvJfuCtPY",
        "pdf_link": "https://openreview.net/pdf?id=SEvJfuCtPY",
        "keywords": "diffusion models, phase transitions, flow-based generative model, high-dimensional gaussian mixtures, denoising autoencoders, training schedules",
        "abstract": "We analyze the training of a two-layer autoencoder used to parameterize a flow-based generative model for sampling from a high-dimensional Gaussian mixture. Building on the work of Cui et al. (2024), we find that the phase where the high-level features are learnt during training disappears as the dimension goes to infinity without an appropriate time schedule. We introduce a time dilation that solves this problem. This enables us to characterize the learnt velocity field, finding a first phase where the high-level feature (asymmetry between modes) is learnt and a second phase where the low-level feature (distribution of each mode) is learnt. We find that the autoencoder representing the velocity field learns to simplify by estimating only the parameters relevant to the feature for each phase. Turning to real data, we propose a method that, for a given feature, finds intervals of time where training improves accuracy the most on that feature, and we provide an experiment on MNIST validating this approach."
    },
    {
        "title": "From Overconnectivity to Sparsity: Emulating Synaptic Pruning with Long Connections",
        "link_suffix": "/forum?id=qMUtej58Pc",
        "link": "https://openreview.net/forum?id=qMUtej58Pc",
        "pdf_link": "https://openreview.net/pdf?id=qMUtej58Pc",
        "keywords": "machine learning architectures, sparsity, residual connections, redundancy, long connections, pruning, synaptic pruning",
        "abstract": "During brain development, an excess number of synapses are initially created, which are progressively eliminated through a process known as synaptic pruning. This procedure is activity-dependent, shaped by the brain's experiences. While creating an overabundance of synaptic connections only to later remove many might appear inefficient, research suggests that pruned networks demonstrate significant efficiency and robustness. Inspired by this biological process, we propose a neural network architecture utilizing long connections instead of traditional short residual connections. When long connections neural networks (LCNs) are trained with gradient descent, information is naturally \"pushed\" down to the first few layers, leading to a sparse network. Even more surprising is that this simple architectural modification leads to networks that exhibit behaviors similar to biological brain networks, namely: early overconnectivity to later sparsity,\nenhanced robustness to noise, efficiency in low-data settings and longer training times. Specifically, starting with a traditional neural network architecture with initial depth $d$ and $k$ connections, long connections are added from all layers to the last layer and summed up. During LCN training, 30-80% of the top layers become effective identity mappings as all relevant information is concentrated in the bottom layers. Pruning the top layers results in a refined network with a reduced depth $d'$ and final connections $k'$, achieving significant efficiencies without any loss in performance compared to residual baselines. We apply this architecture to various classification tasks and show that, in all experiments, the network converges to utilizing only a subset of the initially defined pre-training connections, and the amount of compression is dependent on the task complexity."
    },
    {
        "title": "Deep Learning Alternatives Of The Kolmogorov Superposition Theorem",
        "link_suffix": "/forum?id=SyVPiehSbg",
        "link": "https://openreview.net/forum?id=SyVPiehSbg",
        "pdf_link": "https://openreview.net/pdf?id=SyVPiehSbg",
        "keywords": "Kolmogorov-Arnold Representation Theorem, Function Approximation, Physics Informed Neural Networks, AI4Science",
        "abstract": "This paper explores alternative formulations of the Kolmogorov Superposition Theorem (KST) as a foundation for neural network design. The original KST formulation, while mathematically elegant, presents practical challenges due to its limited insight into the structure of inner and outer functions and the large number of unknown variables it introduces. Kolmogorov-Arnold Networks (KANs) leverage KST for function approximation, but they have faced scrutiny due to mixed results compared to traditional multilayer perceptrons (MLPs) and practical limitations imposed by the original KST formulation. To address these issues, we introduce ActNet, a scalable deep learning model that builds on the KST and overcomes some of the drawbacks of Kolmogorov's original formulation. We evaluate ActNet in the context of Physics-Informed Neural Networks (PINNs), a framework well-suited for leveraging KST's strengths in low-dimensional function approximation, particularly for simulating partial differential equations (PDEs). In this challenging setting, where models must learn latent functions without direct measurements, ActNet consistently outperforms KANs across multiple benchmarks and is competitive against the current best MLP-based approaches. These results present ActNet as a promising new direction for KST-based deep learning applications, particularly in scientific computing and PDE simulation tasks."
    },
    {
        "title": "Vision Language Models are In-Context Value Learners",
        "link_suffix": "/forum?id=friHAl5ofG",
        "link": "https://openreview.net/forum?id=friHAl5ofG",
        "pdf_link": "https://openreview.net/pdf?id=friHAl5ofG",
        "keywords": "robot learning, vision-language model, value estimation, manipulation",
        "abstract": "Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve. However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize. To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress. Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions. Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks. Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos. The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning."
    },
    {
        "title": "Fast Fractional Natural Gradient Descent using Learnable Spectral Factorizations",
        "link_suffix": "/forum?id=yBLBls6ryd",
        "link": "https://openreview.net/forum?id=yBLBls6ryd",
        "pdf_link": "https://openreview.net/pdf?id=yBLBls6ryd",
        "keywords": "natural gradient, Riemannian optimization, positive-definite manifold, Kronecker-facotrized, Shampoo",
        "abstract": "Many popular optimization methods can be united through fractional natural gradient descent (FNGD), which pre-conditions the gradient with a fractional power of the inverse Fisher:\n    RMSprop and Adam(W) estimate a diagonal Fisher matrix and apply a square root before inversion; other methods like K-FAC and Shampoo employ matrix-valued Fisher estimates and apply the inverse and inverse square root, respectively.\n    Recently, the question of how fractional power affects optimization has moved into focus, e.g. offering trade-offs between convergence and generalization.\n    Gaining deeper insights into this phenomenon would require going beyond diagonal estimations and using cheap and flexible matrix-valued Fisher estimators capable of applying any fractional power; however, existing methods are limited by their expensive matrix fraction computation.\n    To address this, we propose a Riemannian framework to learn eigen-factorized Fisher estimations on the fly, allowing for the cheap application ofarbitraryfractional powers.\n    Our approach does not require matrix decompositions and, therefore, is stable in half precision.\n    We show our framework's efficacy on positive-definite matrix optimization problems and demonstrate its efficiency and flexibility for training neural nets."
    },
    {
        "title": "GRAIN: Exact Graph Reconstruction from Gradients",
        "link_suffix": "/forum?id=7bAjVh3CG3",
        "link": "https://openreview.net/forum?id=7bAjVh3CG3",
        "pdf_link": "https://openreview.net/pdf?id=7bAjVh3CG3",
        "keywords": "gradient leakage, gradient inversion, graph neural networks, federated learning, graph convolutional networks, gnn, gcn, attack, privacy, reconstruction",
        "abstract": "Federated learning allows multiple parties to train collaboratively while only Federated learning allows multiple parties to train collaboratively while only sharing gradient updates. However, recent work has shown that it is possible to exactly reconstruct private data such as text and images from gradients for both fully connected and transformer layers in the honest-but-curious setting. In this work, we present GRAIN, the first exact reconstruction attack on graph-structured data that recovers both the structure of the graph and the associated node features. Concretely, we focus on Graph Convolutional Networks (GCN), a powerful framework for learning on graphs. Our method first utilizes the low-rank structure of GCN layer updates to efficiently reconstruct and filter building blocks, which are subgraphs of the input graph. These building blocks are then joined to complete the input graph. Our experimental evaluation on molecular datasets shows that GRAIN can perfectly reconstruct up to 70% of all molecules, compared to at most 20% correctly positioned nodes and 32% recovered node features for the baseline."
    },
    {
        "title": "ALLaM: Large Language Models for Arabic and English",
        "link_suffix": "/forum?id=MscdsFVZrN",
        "link": "https://openreview.net/forum?id=MscdsFVZrN",
        "pdf_link": "https://openreview.net/pdf?id=MscdsFVZrN",
        "keywords": "Large Language Model, English, Arabic, Second Language Acquisition",
        "abstract": "In this work, we present ALLaM: Arabic Large Language Model, a series of large language models to support the ecosystem of Arabic Language Technologies (ALT). ALLaM is carefully trained, considering the values of language alignment and transferability of knowledge at scale. The models are based on an autoregressive decoder-only architecture and are pretrained on a mixture of Arabic and English texts. We illustrate how the second-language acquisition via vocabulary expansion can help steer a language model towards a new language without any major catastrophic forgetting in English. Furthermore, we highlight the effectiveness of using translation data and the process of knowledge encoding within the language model's latent space. Finally, we show that effective alignment with human preferences can significantly enhance the performance of a large language model (LLM) compared to less aligned models of a larger scale. Our methodology enables us to achieve state-of-the-art performance in various Arabic benchmarks, including MMLU Arabic, ACVA, and Arabic Exams. Our aligned models improve both in Arabic and English from its base aligned models."
    },
    {
        "title": "Execution-guided within-prompt search for programming-by-example",
        "link_suffix": "/forum?id=PY56Wur7S0",
        "link": "https://openreview.net/forum?id=PY56Wur7S0",
        "pdf_link": "https://openreview.net/pdf?id=PY56Wur7S0",
        "keywords": "Programming-by-example, program synthesis, large language models",
        "abstract": "Soundness is an important property in programming-by-example (PBE) as any learned program is expected to be correct for at least the examples that were part of the problem statement. \nThis allows synthesizers to perform a search over a domain-specific language (DSL) that terminates when any sound program is found.\nLarge language models (LLMs) can generate code from examples without being limited to a DSL, but they lack soundness guarantees (generated code is not even guaranteed to execute) and the concept of search (samples are independent).\nIn this paper, we use an LLM as a policy that generates lines of code and then join these lines of code to let the LLM implicitly estimate the value of each of these lines in its next iteration.\nWe further guide the policy and value estimation by executing each line and annotating it with its results on the given examples. \nThis allows us to search for programs within a prompt until a sound program is found by letting the policy reason in both the syntactic (code) and semantic (execution) space.\nWe evaluate this approach on five benchmarks across different domains, such as string transformations, list transformations, and arbitrary Python programming problems, showing that within-prompt search and execution allows us to sample better programs more consistently.\nAdditionally, our experiments indicate that the model does behave like a policy and value."
    },
    {
        "title": "Efficient molecular conformer generation with SO(3) averaged flow-matching and reflow",
        "link_suffix": "/forum?id=Jj4XIKX4TJ",
        "link": "https://openreview.net/forum?id=Jj4XIKX4TJ",
        "pdf_link": "https://openreview.net/pdf?id=Jj4XIKX4TJ",
        "keywords": "Flow-matching, few-shot generation, equivariance, small molecules",
        "abstract": "Molecular conformer generation is a critical task in computational chemistry and drug discovery. Diverse generative deep learning methods have been proposed and shown to outperform traditional cheminformatics tools. State-of-the-art models leverage neural transport, employing denoising diffusion or flow-matching to generate or refine atomic point clouds from a prior distribution. Still, sampling with existing models requires significant computational expense. In this work, we build upon flow-matching and propose two mechanisms for accelerating training and inference of 3D molecular conformer generation. For fast training, we introduce the SO(3)-Averaged Flow, which we show to converge faster and generate better conformer ensembles compared to flow-matching and Kabsch alignment-based optimal transport flow. For fast inference, we further show that reflow methods and distillation of these models enable few-steps or even one-step molecular conformer generation with high quality. Using these two techniques, we demonstrate a model that can match the performance of strong transformer baselines with only a fraction of the number of parameters and generation steps. The training techniques proposed in this work lay the foundation for highly efficient molecular conformer generation with generative deep learning model."
    },
    {
        "title": "TC-Bench: Benchmarking Temporal Compositionality in Conditional Video Generation",
        "link_suffix": "/forum?id=xSOl0s1u77",
        "link": "https://openreview.net/forum?id=xSOl0s1u77",
        "pdf_link": "https://openreview.net/pdf?id=xSOl0s1u77",
        "keywords": "Video Generation Benchmark; Text-to-Video Generation; Compositional Video Generation",
        "abstract": "Video generation has many unique challenges beyond those of image generation. The temporal dimension introduces extensive possible variations across frames, over which consistency and continuity may be violated. In this study, we move beyond evaluating simple actions and argue that generated videos should incorporate the emergence of new concepts and their relation transitions like in real-world videos as time progresses. To assess the \\textbf{T}emporal \\textbf{C}ompositionality of video generation models, we propose TC-Bench, a benchmark of meticulously crafted text prompts, corresponding ground truth videos, and robust evaluation metrics. The prompts articulate the initial and final states of scenes, effectively reducing ambiguities for frame development and simplifying the assessment of transition completion. In addition, by collecting aligned real-world videos corresponding to the prompts, we expand TC-Bench's applicability from text-conditional models to image-conditional ones that can perform generative frame interpolation. We also develop new metrics to measure the completeness of component transitions in generated videos, which demonstrate significantly higher correlations with human judgments than existing metrics. Our comprehensive experimental results reveal that most video generators achieve less than ～20% of the compositional changes, highlighting enormous space for future improvement. Our analysis indicates that current video generation models struggle to interpret descriptions of compositional changes and dynamically map varied semantics across different time steps."
    },
    {
        "title": "Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups",
        "link_suffix": "/forum?id=7PLpiVdnUC",
        "link": "https://openreview.net/forum?id=7PLpiVdnUC",
        "pdf_link": "https://openreview.net/pdf?id=7PLpiVdnUC",
        "keywords": "Canonicalization, Equivariance, Invariance, Lie algebra, Partial Differential Equations, Neural Operator, PINN, Neural PDE solver, Lie point symmetries, Frames, Frame Averaging",
        "abstract": "The quest for robust and generalizable machine learning models has driven recent interest in exploiting symmetries through equivariant neural networks. In the context of PDE solvers, recent works have shown that Lie point symmetries can be a useful inductive bias for Physics-Informed Neural Networks (PINNs) through data and loss augmentation. Despite this, directly enforcing equivariance within the model architecture for these problems remains elusive. This is because many PDEs admit non-compact symmetry groups, oftentimes not studied beyond their infinitesimal generators, making them incompatible with most existing equivariant architectures. In this work, we propose Lie aLgebrA Canonicalization (LieLAC), a novel approach that exploits only the action of infinitesimal generators of the symmetry group, circumventing the need for knowledge of the full group structure. To achieve this, we address existing theoretical issues in the canonicalization literature, establishing connections with frame averaging in the case of continuous non-compact groups. Operating within the framework of canonicalization, LieLAC can easily be integrated with unconstrained pre-trained models, transforming inputs to a canonical form before feeding them into the existing model, effectively aligning the input for model inference according to allowed symmetries. LieLAC utilizes standard Lie group descent schemes, achieving equivariance in pre-trained models. Finally, we showcase LieLAC's efficacy on tasks of invariant image classification and Lie point symmetry equivariant neural PDE solvers using pre-trained models."
    },
    {
        "title": "Gaussian Masked Autoencoders",
        "link_suffix": "/forum?id=BoRmf8wDZ7",
        "link": "https://openreview.net/forum?id=BoRmf8wDZ7",
        "pdf_link": "https://openreview.net/pdf?id=BoRmf8wDZ7",
        "keywords": "Representation learning, Gaussian Splatting",
        "abstract": "This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While mainstream self-supervised learning frameworks such as MAE operate on low-level pixels, the image synthesis community has evolved to use latent, mid-level representations for better generative visual data modeling. Our approach, named GMAE, aims to reconcile these two and get the benefits of both worlds. Like MAE, it reconstructs the image end-to-end in the pixel space; however, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities (e.g figure-ground segmentation, image layering, edge detection, etc) while preserving the high self-supervised representation quality from MAE. Notably, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data."
    }
]