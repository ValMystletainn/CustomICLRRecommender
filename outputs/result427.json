[
    {
        "title": "ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks",
        "link_suffix": "/forum?id=APojAzJQiq",
        "link": "https://openreview.net/forum?id=APojAzJQiq",
        "pdf_link": "https://openreview.net/pdf?id=APojAzJQiq",
        "keywords": "Physics Informed Neural Networks, Multi-task learning, Conflicting gradients",
        "abstract": "The loss functions of many learning problems contain multiple additive terms that can disagree and yield conflicting update directions. For Physics-Informed Neural Networks (PINNs), loss terms on initial/boundary conditions and physics equations are particularly interesting as they are well-established as highly difficult tasks. To improve learning the challenging multi-objective task posed by PINNs, we propose the ConFIG method, which provides conflict-free updates by ensuring a positive dot product between the final update and each loss-specific gradient. It also maintains consistent optimization rates for all loss terms and dynamically adjusts gradient magnitudes based on conflict levels. We additionally leverage momentum to accelerate optimizations by alternating the back-propagation of different loss terms. We provide a mathematical proof showing the convergence of the ConFIG method, and it is evaluated across a range of challenging PINN scenarios. ConFIG consistently shows superior performance and runtime compared to baseline methods. We also test the proposed method in a classic multi-task benchmark, where the ConFIG method likewise exhibits a highly promising performance. Source code will be made available upon acceptance."
    },
    {
        "title": "Reflective Gaussian Splatting",
        "link_suffix": "/forum?id=xPxHQHDH2u",
        "link": "https://openreview.net/forum?id=xPxHQHDH2u",
        "pdf_link": "https://openreview.net/pdf?id=xPxHQHDH2u",
        "keywords": "Gaussain-Splatting, Physically based Rendering, Deferred-Rendering, Inter-Reflection",
        "abstract": "Novel view synthesis has experienced significant advancements owing to increasingly capable NeRF- and 3DGS-based methods. However, reflective object reconstruction remains challenging, lacking a proper solution to achieve real-time, high-quality rendering while accommodating inter-reflection. To fill this gap, we introduce a Reflective Gaussian splatting (Ref-Gaussian) framework characterized with two components: (I) Physically based deferred rendering that empowers the rendering equation with pixel-level material properties via formulating split-sum approximation; (II) Gaussian-grounded inter-reflection that realizes the desired inter-reflection function within a Gaussian splatting paradigm for the first time. To enhance geometry modeling, we further introduce material-aware normal propagation and an initial per-Gaussian shading stage, along with 2D Gaussian primitives. Extensive experiments on standard datasets demonstrate that Ref-Gaussian surpasses existing approaches in terms of quantitative metrics, visual quality, and compute efficiency. Further, we illustrate that Ref-Gaussian supports more applications such as relighting and editing."
    },
    {
        "title": "Data Center Cooling System Optimization Using Offline Reinforcement Learning",
        "link_suffix": "/forum?id=W8xukd70cU",
        "link": "https://openreview.net/forum?id=W8xukd70cU",
        "pdf_link": "https://openreview.net/pdf?id=W8xukd70cU",
        "keywords": "Offline Reinforcement learning, data center optimization, cooling system, energy saving",
        "abstract": "The recent advances in information technology and artificial intelligence have fueled a rapid expansion of the data center (DC) industry worldwide, accompanied by an immense appetite for electricity to power the DCs. In a typical DC, around 30-40% of the energy is spent on the cooling system rather than on computer servers, posing a pressing need for developing new energy-saving optimization technologies for DC cooling systems. However, optimizing such real-world industrial systems faces numerous challenges, including but not limited to a lack of reliable simulation environments, limited historical data, and stringent safety and control robustness requirements. In this work, we present a novel physics-informed offline reinforcement learning (RL) framework for energy efficiency optimization of DC cooling systems. The proposed framework models the complex dynamical patterns and physical dependencies inside a server room using a purposely designed graph neural network architecture that is compliant with the fundamental time-reversal symmetry. Because of its well-behaved and generalizable state-action representations, the model enables sample-efficient and robust latent space offline policy learning using limited real-world operational data. Our framework has been successfully deployed and verified in a large-scale production DC for closed-loop control of its air-cooling units (ACUs). We conducted a total of 1300 hours of short and long-term experiments in the production DC environment. The results show that our method achieves 14-18% energy savings in the DC cooling system, without any violation of the safety or operational constraints. We have also conducted a comprehensive evaluation of our approach in a real-world DC testbed environment. Our results have demonstrated the significant potential of offline RL in solving a broad range of data-limited, safety-critical real-world industrial control problems."
    },
    {
        "title": "A Primal-Dual Approach for Dynamic Pricing of Sequentially Displayed Complementary Items under Sale Constraints",
        "link_suffix": "/forum?id=HLxWF7xqiK",
        "link": "https://openreview.net/forum?id=HLxWF7xqiK",
        "pdf_link": "https://openreview.net/pdf?id=HLxWF7xqiK",
        "keywords": "dynamic pricing, primal-dual",
        "abstract": "We address the challenging problem of dynamically pricing complementary items that are sequentially displayed to customers. An illustrative example is the online sale of flight tickets, where customers navigate through multiple web pages. Initially, they view the ticket cost, followed by ancillary expenses such as insurance and additional luggage fees. Coherent pricing policies for complementary items are essential because optimizing the pricing of each item individually is ineffective. Our scenario also involves a sales constraint, which specifies a minimum number of items to sell, and uncertainty regarding customer demand curves. To tackle this problem, we originally formulate it as a Markov decision process with constraints. Leveraging online learning tools, we design a primal-dual online optimization algorithm. We empirically evaluate our approach using synthetic settings randomly generated from real-world data, covering various configurations from stationary to non-stationary, and compare its performance in terms of constraints violation and regret against well-known baselines optimizing each state singularly."
    },
    {
        "title": "DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimizaiton",
        "link_suffix": "/forum?id=HE5JmwniHm",
        "link": "https://openreview.net/forum?id=HE5JmwniHm",
        "pdf_link": "https://openreview.net/pdf?id=HE5JmwniHm",
        "keywords": "multiple kernel clustering; multi-view clustering; late fusion MVC",
        "abstract": "Recent advancements in multiple kernel clustering (MKC) have highlighted the effectiveness of late fusion strategies, particularly in enhancing computational efficiency to near-linear complexity while achieving promising clustering performance. However, existing methods encounter three significant limitations: (1) reliance on fixed base partition matrices that do not adaptively optimize during the clustering process, thereby constraining their performance to the inherent representational capabilities of these matrices; (2) a focus on adjusting kernel weights to explore inter-view consistency and complementarity, which often neglects the intrinsic high-order correlations among views, thereby limiting the extraction of comprehensive multiple kernel information; (3) a lack of adaptive mechanisms to accommodate varying distributions within the data, which limits robustness and generalization. To address these challenges, this paper proposes a novel algorithm termed Dynamic Late Fusion Multiple Kernel Clustering with Robust {Tensor Learning via min-max optimization (DLEFT-MKC), which effectively overcomes the representational bottleneck of base partition matrices and facilitates the learning of meaningful high-order cross-view information. Specifically, it is the first to incorporate a min-max optimization paradigm into tensor-based MKC, enhancing algorithm robustness and generalization. Additionally, it dynamically reconstructs decision layers to enhance representation capabilities and subsequently stacks the reconstructed representations for tensor learning that promotes the capture of high-order associations and cluster structures across views, ultimately yielding consensus clustering partitions. To solve the resultant optimization problem, we innovatively design a strategy that combines reduced gradient descent with the alternating direction method of multipliers, ensuring convergence to local optima while maintaining high computational efficiency. Extensive experimental results across various benchmark datasets validate the superior effectiveness and efficiency of the proposed DLEFT-MKC."
    },
    {
        "title": "COMET: Benchmark for Comprehensive Biological Multi-omics Evaluation Tasks and Language Models",
        "link_suffix": "/forum?id=C81bqFCmMf",
        "link": "https://openreview.net/forum?id=C81bqFCmMf",
        "pdf_link": "https://openreview.net/pdf?id=C81bqFCmMf",
        "keywords": "Multi-omics Benchmark, AI for Biology, Language Models",
        "abstract": "As key elements within the central dogma, DNA, RNA, and proteins play crucial roles in maintaining life by guaranteeing accurate genetic expression and implementation. Although research on these molecules has profoundly impacted fields like medicine, agriculture, and industry, the diversity of machine learning approaches\u2014from traditional statistical methods to deep learning models and large language models\u2014poses challenges for researchers in choosing the most suitable models for specific tasks, especially for cross-omics and multi-omics tasks due to the lack of comprehensive benchmarks. To address this, we introduce the first comprehensive multi-omics benchmark COMET (Benchmark for BiologicalCOmprehensiveMulti-omicsEvaluationTasks and Language Models), designed to evaluate models across single-omics, cross-omics, and multi-omics tasks. First, we curate and develop a diverse collection of downstream tasks and datasets covering key structural and functional aspects in DNA, RNA, and proteins, including tasks that span multiple omics levels. Then, we evaluate existing foundational language models for DNA, RNA, and proteins, as well as the newly proposed multi-omics method, offering valuable insights into their performance in integrating and analyzing data from different biological modalities. This benchmark aims to define critical issues in multi-omics research and guide future directions, ultimately promoting advancements in understanding biological processes through integrated and different omics data analysis."
    },
    {
        "title": "Adversarial-Guided Diffusion for Robust and High-Fidelity Multimodal LLM Attacks",
        "link_suffix": "/forum?id=UXNprzZmvZ",
        "link": "https://openreview.net/forum?id=UXNprzZmvZ",
        "pdf_link": "https://openreview.net/pdf?id=UXNprzZmvZ",
        "keywords": "Adversarial Attack, multimodal large language models",
        "abstract": "Recent diffusion-based adversarial attack methods have shown promising results in generating natural adversarial images. However, these methods often lack fidelity by inducing significant distortion on the original image with even small perturbations on the latent representation. In this paper, we propose Adversarial-Guided Diffusion (AGD), a novel diffusion-based generative adversarial attack framework, which introduces adversarial noise during the reverse sampling of conditional diffusion models. \nAGD uses editing-friendly inversion sampling to faithfully reconstruct images without significantly distorting them through gradients on the latent representation. In addition, AGD enhances latent representations by intelligently choosing sampling steps, thereby injecting adversarial semantics more smoothly. Extensive experiments demonstrate that our method outperforms state-of-the-art methods in both the effectiveness of generating adversarial images for targeted attacks on multimodal large language models (MLLMs) and image quality, successfully misleading the MLLM's responses. We argue that the security concerns surrounding the adversarial robustness of MLLMs deserve increased attention from the research community."
    },
    {
        "title": "Bias Mitigation in Graph Diffusion Models",
        "link_suffix": "/forum?id=CSj72Rr2PB",
        "link": "https://openreview.net/forum?id=CSj72Rr2PB",
        "pdf_link": "https://openreview.net/pdf?id=CSj72Rr2PB",
        "keywords": "Diffusion models, Graph learning, Bias analysis",
        "abstract": "Most existing graph generative diffusion models suffer from significant exposure bias during graph sampling. We observe that the forward diffusion\u2019s maximum perturbation distribution in most models deviates from the standard normal distribution, while reverse sampling consistently starts from a standard normal distribution. This mismatch results in a reverse starting bias, which, together with the exposure bias, degrades generation quality. The exposure bias typically accumulates and propagates throughout the sampling process. In this paper, we effectively address both biases. To mitigate reverse starting bias, we employ a newly designed Langevin sampling algorithm to align with the forward maximum perturbation distribution, establishing a new reverse starting point. To address the exposure bias, we introduce a fraction correction mechanism based on a newly defined score difference. Our approach, which requires no network modifications, is validated across multiple models, datasets, and tasks, achieving state-of-the-art results."
    },
    {
        "title": "ACDC: Autoregressive Coherent Multimodal Generation using Diffusion Correction",
        "link_suffix": "/forum?id=Zp51wHvoot",
        "link": "https://openreview.net/forum?id=Zp51wHvoot",
        "pdf_link": "https://openreview.net/pdf?id=Zp51wHvoot",
        "keywords": "Large Multimodal Models, Autoregressive Models, Diffusion Models",
        "abstract": "Autoregressive models (ARMs) and diffusion models (DMs) represent two leading paradigms in generative modeling, each excelling in distinct areas: ARMs in global context modeling and long-sequence generation, and DMs in generating high-quality local contexts, especially for continuous data such as images and short videos. However, ARMs often suffer from exponential error accumulation over long sequences, leading to physically implausible results, while DMs are limited by their local context generation capabilities. In this work, we introduce Autoregressive Coherent multimodal generation with Diffusion Correction (ACDC), a zero-shot approach that combines the strengths of both ARMs and DMs at the inference stage without the need for additional fine-tuning. ACDC leverages ARMs for global context generation and memory-conditioned DMs for local correction, ensuring high-quality outputs by correcting artifacts in generated multimodal tokens. In particular, we propose a memory module based on large language models (LLMs) that dynamically adjusts the conditioning texts for the DMs, preserving crucial global context information. Our experiments on multimodal tasks, including coherent multi-frame story generation and autoregressive video generation, demonstrate that ACDC effectively mitigates the accumulation of errors and significantly enhances the quality of generated outputs, achieving superior performance while remaining agnostic to specific ARM and DM architectures."
    },
    {
        "title": "The Utility and Complexity of In- and Out-of-Distribution Machine Unlearning",
        "link_suffix": "/forum?id=HVFMooKrHX",
        "link": "https://openreview.net/forum?id=HVFMooKrHX",
        "pdf_link": "https://openreview.net/pdf?id=HVFMooKrHX",
        "keywords": "machine unlearning, differential privacy, optimization, theory",
        "abstract": "Machine unlearning, the process of selectively removing data from trained models, is increasingly crucial for addressing privacy concerns and knowledge gaps post-deployment. Despite this importance, existing approaches are often heuristic and lack formal guarantees. In this paper, we analyze the fundamental utility, time, and space complexity trade-offs of approximate unlearning, providing rigorous certification analogous to differential privacy. For in-distribution data, we show that a surprisingly simple and general procedure\u2014empirical risk minimization with output perturbation\u2014achieves tight unlearning-utility-complexity trade-offs, addressing a previous theoretical gap on the separation from unlearning ``for free\" via differential privacy. However, such techniques fail out-of-distribution, where unlearning time complexity can exceed that of retraining, even for a single sample. To address this, we propose a new robust and noisy gradient descent variant that provably amortizes unlearning time complexity without compromising utility."
    },
    {
        "title": "Deep Complex Spatio-Spectral Networks with Complex Visual Inputs",
        "link_suffix": "/forum?id=9hmDl8fFDs",
        "link": "https://openreview.net/forum?id=9hmDl8fFDs",
        "pdf_link": "https://openreview.net/pdf?id=9hmDl8fFDs",
        "keywords": "Deep Complex Newtworks, Complex-valued color transformation",
        "abstract": "Complex-valued neural networks have attracted growing attention for their ability to handle complex-valued data with enhanced representational capacity. However, their potential in computer vision remains relatively untapped. \nIn this paper, we introduce Deep Complex Spatio-Spectral Network (DCSNet), a fully complex-valued token-based, end-to-end neural network designed for foreground extraction tasks. Additionally, our DCSNet encoder can be used for image classification.\nWe also propose an invertible real-to-complex (R2C) transform, which generates two complex-valued input channels, complex intensity, and complex hue, while producing complex-valued images with distinct real and imaginary components.\nDCSNet operates in both spatial and spectral domains by leveraging complex-valued inputs and complex Fourier transform.\nAs a result, the complex-valued representation is maintained throughout DCSNet, and we avoid the information loss typically associated with Real$\\leftrightarrow$Complex transformations.\n Extensive experiments show that DCSNet surpasses existing complex-valued methods across various tasks on both real and complex-valued data and achieves competitive performance compared to state-of-the-art real-valued methods, establishing a robust framework for handling both data types effectively."
    },
    {
        "title": "SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints",
        "link_suffix": "/forum?id=m8Rk3HLGFx",
        "link": "https://openreview.net/forum?id=m8Rk3HLGFx",
        "pdf_link": "https://openreview.net/pdf?id=m8Rk3HLGFx",
        "keywords": "Video Generation, Diffusion Model",
        "abstract": "Recent advancements in video diffusion models demonstrate remarkable capabilities in simulating real-world dynamics and 3D consistency. This progress motivates us to explore the potential of these models to maintain dynamic consistency across diverse viewpoints, a feature highly sought after in applications like virtual filming. Unlike existing methods focused on multi-view generation of single objects for 4D reconstruction, our interest lies in generating open-world videos from arbitrary viewpoints, incorporating six degrees of freedom (6 DoF) camera poses.\nTo achieve this, we propose a plug-and-play module that enhances a pre-trained text-to-video model for multi-camera video generation, ensuring consistent content across different viewpoints. Specifically, we introduce a multi-view synchronization module designed to maintain appearance and geometry consistency across these viewpoints. Given the scarcity of high-quality training data, we also propose a progressive training scheme that leverages multi-camera images and monocular videos as a supplement to Unreal Engine-rendered multi-camera videos. This comprehensive approach significantly benefits our model.\nExperimental results demonstrate the superiority of our proposed method over existing competitors and several baselines. Furthermore, our method enables intriguing extensions, such as re-rendering a video from multiple novel viewpoints."
    },
    {
        "title": "Learning Diverse Bimanual Dexterous Manipulation Skills from Human Demonstrations",
        "link_suffix": "/forum?id=8yEoTBceap",
        "link": "https://openreview.net/forum?id=8yEoTBceap",
        "pdf_link": "https://openreview.net/pdf?id=8yEoTBceap",
        "keywords": "bimanual dexterous manipulation, reinforcement learning",
        "abstract": "Bimanual dexterous manipulation is a critical yet underexplored area in robotics. Its high-dimensional action space and inherent task complexity present significant challenges for policy learning, and the limited task diversity in existing benchmarks hinders general-purpose skill development. Existing approaches largely depend on reinforcement learning, often constrained by intricately designed reward functions tailored to a narrow set of tasks. In this work, we present a novel approach for efficiently learning diverse bimanual dexterous skills from abundant human demonstrations. Specifically, we introduce BiDexHD, a framework that unifies task construction from existing bimanual datasets and employs teacher-student policy learning to address all tasks. The teacher learns state-based policies using a general two-stage reward function across tasks with shared behaviors, while the student distills the learned multi-task policies into a vision-based policy. With BiDexHD, scalable learning of numerous bimanual dexterous skills from auto-constructed tasks becomes feasible, offering promising advances toward universal bimanual dexterous manipulation. Our empirical evaluation on the TACO dataset, spanning 141 tasks across six categories, demonstrates a task fulfillment rate of 74.59% on trained tasks and 51.07% on unseen tasks, showcasing the effectiveness and competitive zero-shot generalization capabilities of BiDexHD. For videos and more information, visit our project page."
    },
    {
        "title": "Convergence of Sharpness-Aware Minimization Algorithms using Increasing Batch Size and Decaying Learning Rate",
        "link_suffix": "/forum?id=IcMfCFPdd2",
        "link": "https://openreview.net/forum?id=IcMfCFPdd2",
        "pdf_link": "https://openreview.net/pdf?id=IcMfCFPdd2",
        "keywords": "batch size, deep learning, gap guided sharpness-aware minimization, learning rate, sharpness-aware minimization",
        "abstract": "The sharpness-aware minimization (SAM) algorithm and its variants, including gap guided SAM (GSAM), have been successful at improving the generalization capability of deep neural network models by finding flat local minima of the empirical loss in training. Meanwhile, it has been shown theoretically and practically that increasing the batch size or decaying the learning rate avoids sharp local minima of the empirical loss. In this paper, we consider the GSAM algorithm with increasing batch sizes or decaying learning rates, such as cosine annealing or linear learning rate, and theoretically show its convergence. Moreover, we numerically compare SAM (GSAM) with and without an increasing batch size and conclude that using an increasing batch size or decaying learning rate finds flatter local minima than using a constant batch size and learning rate."
    },
    {
        "title": "CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models",
        "link_suffix": "/forum?id=E77uvbOTtp",
        "link": "https://openreview.net/forum?id=E77uvbOTtp",
        "pdf_link": "https://openreview.net/pdf?id=E77uvbOTtp",
        "keywords": "Diffusion models, Manifold, Classifier-free guidance",
        "abstract": "Classifier-free guidance (CFG) is a fundamental tool in modern diffusion models for text-guided generation. Although effective, CFG has notable drawbacks. For instance, DDIM with CFG lacks invertibility, complicating image editing; furthermore, high guidance scales, essential for high-quality outputs, frequently result in issues like mode collapse. Contrary to the widespread belief that these are inherent limitations of diffusion models, this paper reveals that the problems actually stem from the off-manifold phenomenon associated with CFG, rather than the diffusion models themselves. More specifically, inspired by the recent advancements of diffusion model-based inverse problem solvers (DIS),  we reformulate text-guidance as an inverse problem with a text-conditioned score matching loss and develop CFG++, a novel approach that tackles the off-manifold challenges inherent in traditional CFG. CFG++ features a surprisingly simple fix to CFG, yet it offers significant improvements, including better sample quality for text-to-image generation, invertibility, smaller guidance scales,  reduced etc. Furthermore, CFG++ enables seamless interpolation between unconditional and conditional sampling at lower guidance scales, consistently outperforming traditional CFG at all scales. Moreover, CFG++ can be easily integrated into the high-order diffusion solvers and naturally extends to distilled diffusion models. Experimental results confirm that our method significantly enhances performance in text-to-image generation, DDIM inversion, editing, and solving inverse problems, suggesting a wide-ranging impact and potential applications in various fields that utilize text guidance. Project Page:https://cfgpp-diffusion.github.io/anon"
    },
    {
        "title": "Increasing Both Batch Size and Learning Rate Accelerates Stochastic Gradient Descent",
        "link_suffix": "/forum?id=l2odw7OiNw",
        "link": "https://openreview.net/forum?id=l2odw7OiNw",
        "pdf_link": "https://openreview.net/pdf?id=l2odw7OiNw",
        "keywords": "acceleration, batch size, deep learning, empirical risk minimization, learning rate, mini-batch SGD",
        "abstract": "The performance of mini-batch stochastic gradient descent (SGD) strongly depends on setting the batch size and learning rate to minimize the empirical loss in training the deep neural network.\nIn this paper, we present theoretical analyses of mini-batch SGD with four schedulers: \n(i) constant batch size and decaying learning rate scheduler,\n(ii) increasing batch size and decaying learning rate scheduler, \n(iii) increasing batch size and increasing learning rate scheduler,\nand \n(iv) increasing batch size and warm-up decaying learning rate scheduler. \nWe show that mini-batch SGD using scheduler (i) does not always minimize the expectation of the full gradient norm of the empirical loss, whereas it does using any of schedulers (ii), (iii), and (iv).\nFurthermore, schedulers (iii) and (iv) accelerate mini-batch SGD. \nThe paper also provides numerical results of supporting analyses showing that using scheduler (iii) or (iv) minimizes the full gradient norm of the empirical loss faster than using scheduler (i) or (ii)."
    },
    {
        "title": "Learn to Synthesize Compact Datasets by Matching Effects",
        "link_suffix": "/forum?id=z3vplLsIve",
        "link": "https://openreview.net/forum?id=z3vplLsIve",
        "pdf_link": "https://openreview.net/pdf?id=z3vplLsIve",
        "keywords": "Deep Learning, Dataset Distillation",
        "abstract": "The emerging field of data distillation aims to compress large datasets by aligning synthetic and real data representations to create a highly informative dataset. The optimization objectives of data distillation focus on aligning representations by using process alignment methods such as trajectory and gradient matching. However, this approach is limited by the strict alignment of intermediate quantities between synthetic and real data and the mismatch between their optimization trajectories. To address these limitations, a new data distillation method called effect alignment is proposed, which aims to only push for the consistency of endpoint training results. The approach uses classification tasks to estimate the impact of replacing real training samples with synthetic data, which helps to learn a synthetic dataset that can replace the real dataset and achieve effect alignment. The method is efficient and does not require costly mechanisms, and satisfactory results have been achieved through experiments."
    },
    {
        "title": "TDDBench: A Benchmark for Training data detection",
        "link_suffix": "/forum?id=hpeyWG1PP6",
        "link": "https://openreview.net/forum?id=hpeyWG1PP6",
        "pdf_link": "https://openreview.net/pdf?id=hpeyWG1PP6",
        "keywords": "Training data detection; Benchmark; Copyright certification",
        "abstract": "Training Data Detection (TDD) is a task aimed at determining whether a specific data instance is used to train a  machine learning model. In the computer security literature, TDD is also referred to as Membership Inference Attack (MIA). Given its potential to assess the risks of training data breaches, ensure copyright authentication, and verify model unlearning, TDD has garnered significant attention in recent years, leading to the development of numerous methods. Despite these advancements, there is no comprehensive benchmark to thoroughly evaluate the effectiveness of TDD methods.\nIn this work, we introduce TDDBench, which consists of 13 datasets spanning three data modalities: image, tabular, and text. We benchmark 21 different TDD methods across four detection paradigms and evaluate their performance from five perspectives: average detection performance, best detection performance, memory consumption, and computational efficiency in both time and memory. With TDDBench, researchers can identify bottlenecks and areas for improvement in TDD algorithms, while practitioners can make informed trade-offs between effectiveness and efficiency when selecting TDD algorithms for specific use cases. Our large-scale benchmarking also reveals the generally unsatisfactory performance of TDD algorithms across different datasets. To enhance accessibility and reproducibility, we open-source TDDBench for the research community."
    },
    {
        "title": "Mind's Eye: Image Recognition by EEG via Multimodal Similarity-Keeping Contrastive Learning",
        "link_suffix": "/forum?id=KO09K3rBSr",
        "link": "https://openreview.net/forum?id=KO09K3rBSr",
        "pdf_link": "https://openreview.net/pdf?id=KO09K3rBSr",
        "keywords": "EEG, contrastive learning, brain-computer interface",
        "abstract": "Decoding images from non-invasive electroencephalographic (EEG) signals has been a grand challenge in understanding how the human brain process visual information in real-world scenarios. To cope with the issues of signal-to-noise ratio and nonstationarity, this paper introduces a MUltimodal Similarity-keeping contrastivE learning (MUSE) framework for zero-shot EEG-based image classification. We develop a series of multivariate time-series encoders tailored for EEG signals and assess the efficacy of regularized contrastive EEG-Image pretraining using an extensive visual EEG dataset. Our method achieves state-of-the-art performance, with a top-1 accuracy of 19.3% and a top-5 accuracy of 48.8% in 200-way zero-shot image classification. Furthermore, we visualize neural patterns via model interpretation, shedding light on the visual processing dynamics in the human brain."
    },
    {
        "title": "Compositional Video Generation as Flow Equalization",
        "link_suffix": "/forum?id=qTWDpbF47t",
        "link": "https://openreview.net/forum?id=qTWDpbF47t",
        "pdf_link": "https://openreview.net/pdf?id=qTWDpbF47t",
        "keywords": "Video Generation; Compositionality",
        "abstract": "Large-scale Text-to-Video (T2V) diffusion models have recently demonstrated unprecedented capability to transform natural language descriptions into stunning and photorealistic videos. Despite these promising results, a significant challenge remains: these models struggle to fully grasp complex compositional interactions between multiple concepts and actions. This issue arises when some words dominantly influence the final video, overshadowing other concepts.To tackle this problem, we introduce \\textbf{Vico}, a generic framework for compositional video generation that explicitly ensures all concepts are represented properly. At its core, Vico analyzes how input tokens influence the generated video, and adjusts the model to prevent any single concept from dominating. Specifically, Vico extracts attention weights from all layers to build a spatial-temporal attention graph, and then estimates the influence as the \\emph{max-flow} from the source text token to the video target token. Although the direct computation of attention flow in diffusion models is typically infeasible, we devise an efficient approximation based on subgraph flows and employ a fast and vectorized implementation, which in turn makes the flow computation manageable and differentiable. By updating the noisy latent to balance these flows, Vico captures complex interactions and consequently produces videos that closely adhere to textual descriptions. We apply our method to multiple diffusion-based video models for compositional T2V and video editing. Empirical results demonstrate that our framework significantly enhances the compositional richness and accuracy of the generated videos."
    },
    {
        "title": "NECOMIMI: Neural-Cognitive Multimodal EEG-informed Image Generation with Diffusion Models",
        "link_suffix": "/forum?id=ZLZs2QG7vz",
        "link": "https://openreview.net/forum?id=ZLZs2QG7vz",
        "pdf_link": "https://openreview.net/pdf?id=ZLZs2QG7vz",
        "keywords": "EEG, diffusion model, EEG to image, brain-computer interface, image reconstruction",
        "abstract": "NECOMIMI (NEural-COgnitive MultImodal EEG-Informed Image Generation with Diffusion Models) introduces a novel framework for generating images directly from EEG signals using advanced diffusion models. Unlike previous works that focused solely on EEG-image classification through contrastive learning, NECOMIMI extends this task to image generation. The proposed NERV EEG encoder demonstrates state-of-the-art (SoTA) performance across multiple zero-shot classification tasks, including 2-way, 4-way, and 200-way, and achieves top results in our newly proposed Category-based Assessment Table (CAT) Score, which evaluates the quality of EEG-generated images based on semantic concepts. A key discovery of this work is that the model tends to generate abstract or generalized images, such as landscapes, rather than specific objects, highlighting the inherent challenges of translating noisy and low-resolution EEG data into detailed visual outputs. Additionally, we introduce the CAT Score as a new metric tailored for EEG-to-image evaluation and establish a benchmark on the ThingsEEG dataset. This study underscores the potential of EEG-to-image generation while revealing the complexities and challenges that remain in bridging neural activity with visual representation."
    },
    {
        "title": "Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models",
        "link_suffix": "/forum?id=rdSVgnLHQB",
        "link": "https://openreview.net/forum?id=rdSVgnLHQB",
        "pdf_link": "https://openreview.net/pdf?id=rdSVgnLHQB",
        "keywords": "Diffusion probabilistic models, Image generation",
        "abstract": "Diffusion probabilistic models have achieved remarkable success in generative tasks across diverse data types. While recent studies have explored alternative degradation processes beyond Gaussian noise, this paper bridges two key diffusion paradigms: hot diffusion, which relies entirely on noise, and cold diffusion, which uses only blurring without noise. We argue that hot diffusion fails to exploit the strong correlation between high-frequency image detail and low-frequency structures, leading to random behaviors in the early steps of generation. Conversely, while cold diffusion leverages image correlations for prediction, it neglects the role of noise (randomness) in shaping the data manifold, resulting in out-of-manifold issues and partially explaining its performance drop. To integrate both strengths, we propose Warm Diffusion, a unified Blur-Noise Mixture Diffusion Model (BNMD), to control blurring and noise jointly. Our divide-and-conquer strategy exploits the spectral dependency in images, simplifying score model estimation by disentangling the denoising and deblurring processes. We further analyze the Blur-to-Noise Ratio (BNR) using spectral analysis to investigate the trade-off between model learning dynamics and changes in the data manifold. Extensive experiments across benchmarks validate the effectiveness of our approach for image generation."
    },
    {
        "title": "Decision Rules are in the Pixels: Towards Pixel-level Evaluation of Saliency-based XAI Models",
        "link_suffix": "/forum?id=mKGXdsq7fD",
        "link": "https://openreview.net/forum?id=mKGXdsq7fD",
        "pdf_link": "https://openreview.net/pdf?id=mKGXdsq7fD",
        "keywords": "post-hoc XAI, evaluation, pixel attributions, shapley value, feature deletion",
        "abstract": "The intricate and opaque nature of deep neural networks (DNNs) makes it difficult to decipher how they make decisions. Explainable artificial intelligence (XAI) has emerged as a promising remedy to this conundrum. However, verifying the correctness of XAI methods remains challenging, due to the absence of universally accepted ground-truth explanations. In this study, we focus on assessing the correctness of saliency-based XAI models applied to DNN-based image classifiers at the pixel level. The proposed evaluation protocol departs significantly from previous human-centric correctness assessment at the semantically meaningful object part level, which may not correspond to the actual decision rules derived by classifiers. A crucial step in our approach involves introducing a spatially localized shortcut, a form of decision rule that DNN-based classifiers tend to adopt preferentially, without disrupting original image patterns and decision rules therein. After verifying the shortcut as the dominant decision rule, we estimate the Shapley value for each pixel within the shortcut area to generate the ground-truth explanation map, assuming that pixels outside this area have null contributions. We quantitatively evaluate fourteen saliency-based XAI methods for classifiers utilizing convolutional neural networks and vision Transformers, trained on perturbed CIFAR-10, CIFAR-100, and ImageNet datasets, respectively. Comprehensive experimental results show that existing saliency-based XAI models struggle to offer accurate pixel-level attributions, casting doubt on the recent progress in saliency-based XAI."
    },
    {
        "title": "ZeroDiff: Solidified Visual-semantic Correlation in Zero-Shot Learning",
        "link_suffix": "/forum?id=wy9FRV8O5s",
        "link": "https://openreview.net/forum?id=wy9FRV8O5s",
        "pdf_link": "https://openreview.net/pdf?id=wy9FRV8O5s",
        "keywords": "Zero-shot Learning, Generative Model, Diffusion Mechanism, Effective Learning",
        "abstract": "Zero-shot Learning (ZSL) aims to enable classifiers to identify unseen classes. This is typically achieved by generating visual features for unseen classes based on learned visual-semantic correlations from seen classes. However, most current generative approaches heavily rely on having a sufficient number of samples from seen classes. Our study reveals that a scarcity of seen class samples results in a marked decrease in performance across many generative ZSL techniques.  We argue, quantify, and empirically demonstrate that this decline is largely attributable to spurious visual-semantic correlations. To address this issue, we introduce ZeroDiff, an innovative generative framework for ZSL that incorporates diffusion mechanisms and contrastive representations to enhance visual-semantic correlations. ZeroDiff comprises three key components: (1) Diffusion augmentation, which naturally transforms limited data into an expanded set of noised data to mitigate generative model overfitting; (2) Supervised-contrastive (SC)-based representations that dynamically characterize each limited sample to support visual feature generation; and (3) Multiple feature discriminators employing a Wasserstein-distance-based mutual learning approach, evaluating generated features from various perspectives, including pre-defined semantics, SC-based representations, and the diffusion process. Extensive experiments on three popular ZSL benchmarks demonstrate that ZeroDiff not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. The code will be released upon acceptance."
    },
    {
        "title": "Kolmogorov-Arnold Networks with Variable Function Basis",
        "link_suffix": "/forum?id=IqaQZ1Jdky",
        "link": "https://openreview.net/forum?id=IqaQZ1Jdky",
        "pdf_link": "https://openreview.net/pdf?id=IqaQZ1Jdky",
        "keywords": "Kolmogorov Arnold Networks; variety function basis; interpretability; Weierstrass Approximation Theorem; Bernstein polynomial; multivarious time series forecasting; image for classification; learn the correct univariate functions.",
        "abstract": "\\begin{abstract}\nNeural networks exhibit exceptional performance in processing complex data, yet their internal structures remain largely unexplored. The emergence of Kolmogorov-Arnold Networks (KANs) represents a significant departure from traditional Multi-Layer Perceptrons (MLPs). In contrast to MLPs, KANs replace fixed activation functions at nodes (neurons\") with learnable activation functions on edges (weights\"), enhancing both accuracy and interpretability.\nAs data evolves, the demand for models that are both flexible and robust minimizing the influence of input data variability continues to grow. Addressing this need, we propose a general framework for KANs utilizing a \\underline{\\textbf{V}}ariety \\underline{\\textbf{B}}ernstei\\underline{\\textbf{n}} Polynomial Function Basis for \\underline{\\textbf{K}}olmogorov-\\underline{\\textbf{A}}rnold \\underline{\\textbf{N}}etworks (VBn-KAN). This framework leverages the Weierstrass approximation theorem to extend function basis within KANs in theory, specifically selecting Bernstein polynomials ($B_n$) for their robustness, assured by the uniform convergence proposition. Additionally, to enhance flexibility, we implement techniques to vary the function basis $B_n$ when handling diverse datasets. Comprehensive experiments across three fields: multivariate time series forecasting, computer vision, and function approximation\u2014demonstrate that our method outperforms conventional approaches and other variants of KANs.\n\\end{abstract}"
    }
]