[
    {
        "title": "StyleShot: A snapshot on any style",
        "link_suffix": "/forum?id=Qy3UwW4OJ9",
        "link": "https://openreview.net/forum?id=Qy3UwW4OJ9",
        "pdf_link": "https://openreview.net/pdf?id=Qy3UwW4OJ9",
        "keywords": "diffusion model",
        "abstract": "In this paper, we show that, a good style representation is crucial and sufficient for generalized style transfer without test-time tuning.\nWe achieve this through constructing a style-aware encoder and a well-organized style dataset called StyleGallery.\nWith dedicated design for style learning, this style-aware encoder is trained to extract expressive style representation with decoupling training strategy, and StyleGallery enables the generalization ability.\nWe further employ a content-fusion encoder to enhance image-driven style transfer.\nWe highlight that, our approach, named StyleShot, is simple yet effective in mimicking various desired styles, i.e., 3D, flat, abstract or even fine-grained styles, without test-time tuning. Rigorous experiments validate that, StyleShot achieves superior performance across a wide range of styles compared to existing state-of-the-art methods."
    },
    {
        "title": "Faceshot: Bring any Character into Life",
        "link_suffix": "/forum?id=oJA1GUqRww",
        "link": "https://openreview.net/forum?id=oJA1GUqRww",
        "pdf_link": "https://openreview.net/pdf?id=oJA1GUqRww",
        "keywords": "diffusion model",
        "abstract": "Portrait animation generates dynamic, realistic videos by mimicking facial expressions from a driven video.\nHowever, existing landmark-based methods are constrained by facial landmark detection and motion transfer limitations, resulting in suboptimal performance. In this paper, we present \\emph{FaceShot}, a novel training-free framework designed to animate any character from any driven video, human or non-human, with unprecedented robustness and stability.\nWe achieve this by offering precise and robust landmark results from an appearance-guided landmark matching module and a relative motion transfer module.\nTogether, these components harness the robust semantic correspondences of latent diffusion models to deliver landmarks across a wide range of character types, all without requiring fine-tuning or retraining.\nWith this powerful generalization capability, FaceShot can significantly extend the application of portrait animation by breaking the limitation of landmark detection for any character and driven video.\nFurthermore, FaceShot is compatible with any landmark-driven animation model, enhancing the realism and consistency of animations while significantly improving overall performance.\nExtensive experiments on our newly constructed character benchmark CABench confirm that FaceShot consistently surpasses state-of-the-art approaches across any character domain, setting a new standard for open-domain portrait animation. \nOur code will be made publicly available."
    },
    {
        "title": "IgGM: A Generative Model for Functional Antibody and Nanobody Design",
        "link_suffix": "/forum?id=zmmfsJpYcq",
        "link": "https://openreview.net/forum?id=zmmfsJpYcq",
        "pdf_link": "https://openreview.net/pdf?id=zmmfsJpYcq",
        "keywords": "de novo antibody design, complex structure prediction, protein design",
        "abstract": "Immunoglobulins are crucial proteins produced by the immune system to identify and bind to foreign substances, playing an essential role in shielding organisms from infections and diseases. Designing specific antibodies opens new pathways for disease treatment. With the rise of deep learning, AI-driven drug design has become possible, leading to several methods for antibody design. However, many of these approaches require additional conditions that differ from real-world scenarios, making it challenging to incorporate them into existing antibody design processes. Here, we introduce IgGM, a generative model for the de novo design of immunoglobulins with functional specificity. IgGM produces antibody sequences and structures simultaneously for a given antigen, consisting of three core components: a pre-trained language model for extracting sequence features, a feature learning module for identifying pertinent features, and a prediction module that outputs designed antibody sequences and the predicted complete antibody-antigen complex structure. IgGM has shown effectiveness in both predicting structures and designing novel antibodies and nanobodies, making it relevant in various practical scenarios of antibody and nanobody design."
    },
    {
        "title": "LayerShuffle: Enhancing Robustness in Vision Transformers  by Randomizing Layer Execution Order",
        "link_suffix": "/forum?id=pK3oe2bubc",
        "link": "https://openreview.net/forum?id=pK3oe2bubc",
        "pdf_link": "https://openreview.net/pdf?id=pK3oe2bubc",
        "keywords": "Vision Transformer; Distributed Vision Transformer; Decentralization; AI Robustness; Merged Models; Scalable AI; Edge AI; Random Execution Order",
        "abstract": "Due to their architecture and how they are trained, artificial neural networks are typically not robust toward pruning,  replacing, or shuffling layers at test time. However, such properties would be desirable for different applications, such as distributed neural network architectures where the order of execution cannot be guaranteed or parts of the network can fail during inference. In this work, we address these issues through a number of training approaches for vision transformers whose most important component is randomizing the execution order of attention modules at training time. With our proposed approaches, vision transformers are capable to adapt to arbitrary layer execution orders at test time assuming one tolerates a reduction (about 20%) in accuracy at the same model size. We analyse the feature representations of our trained models as well as how each layer contributes to the models prediction based on its position during inference. Our analysis shows that layers learn to contribute differently based on their position in the network. Importantly,  trained models can also be randomly merged with each other resulting in functional (\"Frankenstein\") models without loss of performance compared to the source models. Finally, we layer-prune our models at test time and find that their performance declines gracefully."
    },
    {
        "title": "Gradient-Free Analytical Fisher Information of Diffused Distributions",
        "link_suffix": "/forum?id=aisfb733DW",
        "link": "https://openreview.net/forum?id=aisfb733DW",
        "pdf_link": "https://openreview.net/pdf?id=aisfb733DW",
        "keywords": "diffusion models; Fisher Information; analytical formulation",
        "abstract": "Diffusion models (DMs) have demonstrated powerful distributional modeling capabilities through matching the first-order score of diffused distributions.\nRecent advancements have explored incorporating the second-order Fisher information, defined as the negative Hessian of log-density, into various downstream tasks and theoretical analysis of DMs.\nHowever, current practices often overlook the inherent structure of diffused distributions, accessing Fisher information via applying auto-differentiation to the learned score network. \nThis approach, while straightforward, leaves theoretical properties unexplored and is time-consuming. \nIn this paper, we derive the analytical formulation of Fisher information (AFI) by applying consecutive differentials to the diffused distributions.\nAs a result, AFI takes a gradient-free form of a weighted sum (or integral) of outer-products of the score and initial data.\nBased on this formulation, we propose two algorithmic variants of AFI for distinct scenarios.\nWhen evaluating the AFI\u2019s trace, we introduce a parameterized network to learn the trace.\nWhen AFI is applied as a linear operator, we present a training-free method that simplifies it into several inner-product calculations.\nFurthermore, we provide theoretical guarantees for both algorithms regarding convergence analysis and approximation error bounds.\nAdditionally, we leverage AFI to establish the first general theorem for the optimal transport property of the diffusion ODE deduced map.\nExperiments in likelihood evaluation and adjoint optimization demonstrate the superior accuracy and reduced time-cost of the proposed algorithms."
    },
    {
        "title": "Refining CLIP's Spatial Awareness: A Visual-Centric Perspective",
        "link_suffix": "/forum?id=38No4B8sx6",
        "link": "https://openreview.net/forum?id=38No4B8sx6",
        "pdf_link": "https://openreview.net/pdf?id=38No4B8sx6",
        "keywords": "Self-distillation; CLIP; Open-vocabulary dense prediction",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) excels in global alignment with language but exhibits limited sensitivity to spatial information, leading to strong performance in zero-shot classification tasks but underperformance in tasks requiring precise spatial understanding. Recent approaches have introduced Region-Language Alignment (RLA) to enhance CLIP's performance in dense multimodal tasks by aligning regional visual representations with corresponding text inputs. However, we find that CLIP ViTs fine-tuned with RLA suffer from notable loss in spatial awareness, which is crucial for dense prediction tasks. To address this, we propose the Spatial Correlation Distillation (SCD) framework, which preserves CLIP's inherent spatial structure and mitigates above degradation. To further enhance spatial correlations, we introduce a lightweight Refiner that extracts refined correlations directly from CLIP before feeding them into SCD, based on an intriguring finding that CLIP naturally capture high-quality dense features. Together, these components form a robust distillation framework that enables CLIP ViTs to integrate both visual-language and visual-centric improvements, achieving state-of-the-art results across various open-vocabulary dense prediction benchmarks."
    },
    {
        "title": "Massively Parallel Environments for Large-Scale Combinatorial Optimizations Using Reinforcement Learning",
        "link_suffix": "/forum?id=CJEBFNBLhO",
        "link": "https://openreview.net/forum?id=CJEBFNBLhO",
        "pdf_link": "https://openreview.net/pdf?id=CJEBFNBLhO",
        "keywords": "Combinatorial Optimizations, Massively Parallel Environments, Reinforcement learning, distribution-wise approach",
        "abstract": "Most combinatorial optimization (CO) problems are NP-hard and difficult to find high-quality solutions. Reinforcement learning (RL) is a promising technique due to its powerful search capability; however, sampling speed is a common bottleneck. Current benchmark works only provide instance-wise approaches, while our work cover both instance-wise and distribution-wise approaches, especially in large-scale CO problems. In this paper, we build 24 GPU-based massively parallel environments for 12 CO problems, i.e., each problem has two environments; and use them to train RL-based approaches. We reproduce benchmark RL algorithms, including instance-wise and distribution-wise approaches especially in large-scale CO problems, on both synthetic datasets and real-world datasets. Take the graph maxcut problem as an example. The sampling speed is improved by at least two orders over conventional implementations, and the scale (i.e., number of nodes) of trained problems in a distribution-wise approach is up to thousands of nodes, i.e., improved by one order. The objective value obtained by inference (100 $\\sim$ 200 seconds) in the distribution-wise scenario is almost the same as the state-of-the-art (SOTA) solver Gurobi (running for 1 hour), and better than the SOTA RL-based approach. The code is available at:https://github.com/OpenAfterReview."
    },
    {
        "title": "Do Deep Neural Network Solutions Form a Star Domain?",
        "link_suffix": "/forum?id=QjO0fUlVYK",
        "link": "https://openreview.net/forum?id=QjO0fUlVYK",
        "pdf_link": "https://openreview.net/pdf?id=QjO0fUlVYK",
        "keywords": "mode connectivity, loss landscapes, neural networks, parameter space, star domain",
        "abstract": "It has recently been conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that a linear path can connect two independent solutions with low loss, given the weights of one of the models are appropriately permuted. However, current methods to test this theory often require very wide networks to succeed. In this work, we conjecture that more generally, the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demonstrate better uncertainty estimates on Bayesian Model Averaging over the obtained star domain. Further, we demonstrate star models as potential substitutes for model ensembles."
    },
    {
        "title": "Prompt-Guided Distillation from Multimodal Large Language Models to Task-specific Models for Multimodal Sentiment Analysis",
        "link_suffix": "/forum?id=BzVJOqwBka",
        "link": "https://openreview.net/forum?id=BzVJOqwBka",
        "pdf_link": "https://openreview.net/pdf?id=BzVJOqwBka",
        "keywords": "Multimodal Sentimen Analysis, Representation Learning, Multimodal Large Language Model, Knowledge Distillation",
        "abstract": "Multimodal Sentiment Analysis (MSA) has made some progress with the advent of Multimodal Large Language Models (MLLMs). However, the scalability and the closed-source nature of some MLLMs imposes challenges for efficient application in the real-word. In this study, we explore an innovative pathway to infuse the capabilities of general MLLMs into task-specific small models for MSA. We introduce the Prompt-Guided Multimodal Framework (PGMF), a refined teacher-student framework designed to transfer knowledge from powerful, general MLLMs to smaller, efficient models. The PGMF-Teacher utilizes MLLM-generated prompts and a tailored conditional alignment module to achieve better MSA, while the PGMF-Student distills this expertise to predict independently of MLLMs' guidance. Extensive evaluations on two popular MSA datasets including SIMS and MOSI demonstrate that compared to previous task-specific small models, PGMF-Teacher achieves state-of-the-art performance with the help of MLLMs' prompts, while PGMF-Student achieve competitive results with fewer parameters and without relying on MLLMs' prompts. The proposed framework offers a novel way to equip task-specific small models with the capability of MLLMs."
    },
    {
        "title": "Information Theoretic Text-to-Image Alignment",
        "link_suffix": "/forum?id=Ugs2W5XFFo",
        "link": "https://openreview.net/forum?id=Ugs2W5XFFo",
        "pdf_link": "https://openreview.net/pdf?id=Ugs2W5XFFo",
        "keywords": "Diffusion model, Text-image alignment, Mutual information",
        "abstract": "Diffusion models for Text-to-Image (T2I) conditional generation have recently achieved\ntremendous success. Yet, aligning these models with user\u2019s intentions still involves a\nlaborious trial-and-error process, and this challenging alignment problem has attracted\nconsiderable attention from the research community. In this work, instead of relying on\nfine-grained linguistic analyses of prompts, human annotation, or auxiliary vision-language\nmodels, we use Mutual Information (MI) to guide model alignment. In brief, our method\nuses self-supervised fine-tuning and relies on a point-wise MI estimation between prompts\nand images to create a synthetic fine-tuning set for improving model alignment. Our\nanalysis indicates that our method is superior to the state-of-the-art, yet it only requires\nthe pre-trained denoising network of the T2I model itself to estimate MI, and a simple\nfine-tuning strategy that improves alignment while maintaining image quality."
    },
    {
        "title": "Continual LLaVA: Continual Instruction Tuning in Large Vision-Language Models",
        "link_suffix": "/forum?id=rwmwFnmjAX",
        "link": "https://openreview.net/forum?id=rwmwFnmjAX",
        "pdf_link": "https://openreview.net/pdf?id=rwmwFnmjAX",
        "keywords": "Large Vision-Language Models, Instruction Tuning, Continual Learning",
        "abstract": "Instruction tuning constitutes a prevalent technique for tailoring Large Vision Language Models (LVLMs) to meet individual task requirements. To date, most of the existing approaches are confined to single-task adaptation, whereas the requirements in real-world scenarios are inherently varied and continually evolving. Thus an ideal LVLM should sustain continual instruction tuning in the face of stream-task distributions (i.e., different domains, emerging capabilities, and new datasets) while minimizing the forgetting of previously acquired knowledge. To achieve this, we propose a new benchmark for COntinuAl inStruction Tuning on LVLMs (COAST), which encompasses the aforementioned domain-incremental, capability-incremental, and dataset-incremental configurations. In terms of methodology, we propose Continual LLaVA, a rehearsal-free method tailored for continual instruction tuning in LVLMs. To circumvent the additional overhead associated with experience replay, we freeze LVLMs and construct the dual increment embeddings for each input instruction to facilitate parameter-efficient tuning. Specifically, the increment embeddings can be decomposed into two principal components: 1) intrinsic increment embeddings to encode task-specific characteristics. To achieve this, we set up a low-rank pool containing candidate embeddings, from which we select the relevant ones based on their similarity with the user instructions; 2) contextual increment embeddings to investigate the inter-dependencies across tasks. In this regard, the low-rank embeddings chosen in the previous tasks are aggregated via learnable weighted sum to provide complementary hints. Extensive experiments indicate that the proposed Continual LLaVA outperforms previous methods by significantly reducing the forgetting during the continual instruction tuning process."
    },
    {
        "title": "Generalizing Dynamics Modeling Easier from Representation Perspective",
        "link_suffix": "/forum?id=i1BTP8wFYM",
        "link": "https://openreview.net/forum?id=i1BTP8wFYM",
        "pdf_link": "https://openreview.net/pdf?id=i1BTP8wFYM",
        "keywords": "Dynamics Modeling, Ordinary Differential Equations, Pre-trained Language Models",
        "abstract": "Learning system dynamics from observations is a critical problem in many applications over various real-world complex systems, e.g., climate, ecology, and fluid systems. Recently, the neural-based dynamics modeling method has become the prevalent solution, where its basic idea is to embed the original states of objects into a latent space before learning the dynamics using neural-based methods such as neural Ordinary Differential Equations (ODE). Given observations from different complex systems, the existing dynamics modeling methods offer a specific model for each observation, resulting in poor generalization. Inspired by the great success of pre-trained models, we raise a question: whether we can conduct a generalized Pre-trained Dynamic EncoDER (PDEDER), which, for various complex systems, can embed their original states into a latent space, where the dynamics can be easier captured. To conduct this generalized PDEDER, we collect 153 sets of real-world and synthetic observations from 24 complex systems. Inspired by the success of time series forecasting using Pre-trained Language Models (PLM), we can employ any PLM and further update it over these dynamic observations by tokenization techniques to achieve the generalized PDEDER. Given any future dynamic observation, we can fine-tune PDEDERwith any specific dynamics modeling method. We evaluate PDEDER on 18 dynamic systems by long/short-term forecasting under both in-domain and cross-domain settings and the empirical results indicate the effectiveness of PDEDER."
    },
    {
        "title": "Semantix: An Energy-guided Sampler for Semantic Style Transfer",
        "link_suffix": "/forum?id=si37wk8U5D",
        "link": "https://openreview.net/forum?id=si37wk8U5D",
        "pdf_link": "https://openreview.net/pdf?id=si37wk8U5D",
        "keywords": "style transfer, diffusion model, energy guidance",
        "abstract": "Recent advances in style and appearance transfer are impressive, but most methods isolate global style and local appearance transfer, neglecting semantic correspondence. Additionally, image and video tasks are typically handled in isolation, with little focus on integrating them for video transfer. To address these limitations, we introduce a novel task,Semantic Style Transfer, which involves transferring style and appearance features from a reference image to a target visual content based on semantic correspondence. We subsequently propose a training-free method,Semantix, an energy-guided sampler designed for Semantic Style Transfer that simultaneously guides both style and appearance transfer based on semantic understanding capacity of pre-trained diffusion models. Additionally, as a sampler,Semantixcan be seamlessly applied to both image and video models, enabling semantic style transfer to be generic across various visual media. Specifically, once inverting both reference and context images or videos to noise space by SDEs,Semantixutilizes a meticulously crafted energy function to guide the sampling process, including three key components:Style Feature Guidance,Spatial Feature GuidanceandSemantic Distanceas a regularisation term. Experimental results demonstrate thatSemantixnot only effectively accomplishes the task of semantic style transfer across images and videos, but also surpasses existing state-of-the-art solutions in both fields."
    },
    {
        "title": "Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation",
        "link_suffix": "/forum?id=kqSmedTcgb",
        "link": "https://openreview.net/forum?id=kqSmedTcgb",
        "pdf_link": "https://openreview.net/pdf?id=kqSmedTcgb",
        "keywords": "Text-augmented, Multimodal LLM, Chemical reaction condition recommendation",
        "abstract": "High-throughput reaction condition (RC) screening is fundamental to chemical synthesis. However, current RC screening suffers from laborious and costly trial-and-error workflows. Traditional computer-aided synthesis planning (CASP) tools fail to find suitable RCs due to data sparsity and inadequate reaction representations. Nowadays, large language models (LLMs) are capable of tackling chemistry-related problems, such as molecule design, and chemical logic Q&A tasks. However, LLMs have not yet achieved accurate predictions of chemical reaction conditions. Here, we present Chemma-RC, a text-augmented multimodal LLM that learns a unified reaction representation from SMILES, reaction graphs, and textual corpus for chemical reaction condition recommendation (RCR). We construct a 1.2 million pair-wised Q&A instruction dataset to train Chemma-RC and design a projection module for modality alignment. Our experimental results demonstrate that Chemma-RC achieves state-of-the-art performance on two open benchmark datasets and exhibits strong generalization capabilities on out-of-domain (OOD) and High-Throughput Experimentation (HTE) datasets. Chemma-RC has the potential to accelerate high-throughput condition screening in chemical synthesis."
    },
    {
        "title": "GaussianAnything: Interactive Point Cloud Latent Diffusion for 3D Generation",
        "link_suffix": "/forum?id=P4DbTSDQFu",
        "link": "https://openreview.net/forum?id=P4DbTSDQFu",
        "pdf_link": "https://openreview.net/pdf?id=P4DbTSDQFu",
        "keywords": "3D Generative Models, Gaussian Splatting, Latent Diffusion Models",
        "abstract": "Recent advancements in diffusion models and large-scale datasets have revolutionized image and video generation, with increasing focus on 3D content generation. While existing methods show promise, they face challenges in input formats, latent space structures, and output representations. This paper introduces a novel 3D generation framework that addresses these issues, enabling scalable and high-quality 3D generation with an interactive Point Cloud-structured Latent space. Our approach utilizes a VAE with multi-view posed RGB-D-N renderings as input, features a unique latent space design that preserves 3D shape information, and incorporates a cascaded latent diffusion model for improved shape-texture disentanglement. The proposed method, GaussianAnything, supports multi-modal conditional 3D generation, allowing for point cloud, caption, and single/multi-view image inputs. Experimental results demonstrate superior performance on various datasets, advancing the state-of-the-art in 3D content generation."
    },
    {
        "title": "Does equivariance matter at scale?",
        "link_suffix": "/forum?id=iIWeyfGTof",
        "link": "https://openreview.net/forum?id=iIWeyfGTof",
        "pdf_link": "https://openreview.net/pdf?id=iIWeyfGTof",
        "keywords": "Geometric deep learning, equivariance, neural scaling laws",
        "abstract": "Given large data sets and sufficient compute, is it beneficial to design neural architectures for the structure and symmetries of each problem? Or is it more efficient to learn them from data? We study empirically how equivariant and non-equivariant networks scale with compute and training samples. Focusing on a benchmark problem of rigid-body interactions and on general-purpose transformer architectures, we perform a series of experiments, varying the model size, training steps, and dataset size. We find evidence for three conclusions. First, equivariance improves data efficiency, but training non-equivariant models with data augmentation can close this gap given sufficient epochs. Second, scaling with compute follows a power law, with equivariant models outperforming non-equivariant ones at each tested compute budget. Finally, the optimal allocation of a compute budget onto model size and training duration differs between equivariant and non-equivariant models."
    },
    {
        "title": "OpenDAS: Open-Vocabulary Domain Adaptation for Segmentation",
        "link_suffix": "/forum?id=wazvIr0Sw0",
        "link": "https://openreview.net/forum?id=wazvIr0Sw0",
        "pdf_link": "https://openreview.net/pdf?id=wazvIr0Sw0",
        "keywords": "Computer Vision, Vision-Language Models, Domain Adaptation, Open-Vocabulary Segmentation, Prompt Tuning",
        "abstract": "Recently, Vision-Language Models (VLMs) have advanced segmentation techniques by shifting from the traditional segmentation of a closed-set of predefined object classes to open-vocabulary segmentation (OVS), allowing users to segment novel classes and concepts unseen during training of the segmentation model. However, this flexibility comes with a trade-off: fully-supervised closed-set methods still outperform OVS methods on base classes, that is on classes on which they have been explicitly trained. This is due to the lack of pixel-aligned training masks for VLMs (which are trained on image-caption pairs), and the absence of domain-specific knowledge, such as autonomous driving. Therefore, we propose the task of open-vocabulary domain adaptation to infuse domain-specific knowledge into VLMs while preserving their open-vocabulary nature. By doing so, we achieve improved performance in base and novel classes. Existing VLM adaptation methods improve performance on base (training) queries, but fail to fully preserve the open-set capabilities of VLMs on novel queries. To address this shortcoming, we combine parameter-efficient prompt tuning with a triplet-loss-based training strategy that uses auxiliary negative queries. Notably, our approach is the only parameter-efficient method that consistently surpasses the original VLM on novel classes. Our adapted VLMs can seamlessly be integrated into existing OVS pipelines, e.g., improving OVSeg by +6.0% mIoU on ADE20K for open-vocabulary 2D segmentation, and OpenMask3D by +4.1% AP on ScanNet++ Offices for open-vocabulary 3D instance segmentation without other changes."
    },
    {
        "title": "Interpretable and Adaptive Graph Contrastive Learning with Information Sharing for Biomedical Link Prediction",
        "link_suffix": "/forum?id=GlgD9o9bl4",
        "link": "https://openreview.net/forum?id=GlgD9o9bl4",
        "pdf_link": "https://openreview.net/pdf?id=GlgD9o9bl4",
        "keywords": "Drug Discovery, Biomedical Link Prediction, Interpretability, Molecular Graph",
        "abstract": "The identification of unobserved links in drug-related biomedical networks is essential for various drug discovery applications, which is also beneficial for both disease diagnosis and treatment through exploring the underlying molecular mechanisms. However, existing solutions face significant challenges due to three main limitations: (1) lack of interpretability to provide comprehensive and reliable insights, (2) insufficient robustness and flexibility in cold-start scenarios, and (3) inadequate interaction and sharing of multi-view information. In light of this, we propose DrugXAS, an interpretable and adaptive cross-view contrastive learning framework with information sharing for biomedical link prediction. Specifically, DrugXAS has three distinctive characteristics for addressing these challenges. To solve the first problem, we propose an attention-aware augmentation scheme to provide understandable explanations of intrinsic mechanisms. To deal with the second challenge, we propose an adaptive graph updater and neighborhood sampler, which select proper neighbors according to the feedbacks from the model to improve aggregation ability. To tackle the third issue, an information sharing module with diffusion loss is proposed to incorporate chemical structures into heterogeneous relational semantics and facilitate the contrast process. Empirically, extensive experiments on seven benchmark datasets involving multi-type tasks demonstrate that the proposed DrugXAS outperforms the state-of-the-art methods in terms of precision, robustness, and interpretability. The source code of DrugXAS is available athttps://anonymous.4open.science/r/DrugXAS-8EC7."
    },
    {
        "title": "MindFlayer: Efficient Asynchronous Parallel SGD in the Presence of Heterogeneous and Random Worker Compute Times",
        "link_suffix": "/forum?id=lcAmiLDEQ4",
        "link": "https://openreview.net/forum?id=lcAmiLDEQ4",
        "pdf_link": "https://openreview.net/pdf?id=lcAmiLDEQ4",
        "keywords": "asynchronous optimization, parallel optimization, federated learning, distributed learning, nonconvex optimization, asynchronous methods, Random Compute Heterogeneity, time complexity, heterogeneous clients, stragglers",
        "abstract": "We study the problem of minimizing the expectation of smooth nonconvex functions with the help of several parallel workers whose role is to compute stochastic gradients. In particular, we focus on the challenging situation where the workers' compute times are arbitrarily heterogeneous and random. In the simpler regime characterized by arbitrarily heterogeneous but deterministic compute times, Tyurin and Richt'{a}rik (NeurIPS 2023) recently designed the first theoretically optimal asynchronous SGD method, called Rennala SGD, in terms of a novel complexity notion called time complexity. The starting point of our work is the observation that Rennala SGD can have arbitrarily bad performance in the presence of random compute times -- a setting it was not designed to handle. To advance our understanding of stochastic optimization in this challenging regime, we propose a new asynchronous SGD method, for which we coin the name MindFlayer SGD. Our theory and empirical results demonstrate the superiority of MindFlayer SGD over existing baselines, including Rennala SGD, in cases when the noise is heavy tailed."
    },
    {
        "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies",
        "link_suffix": "/forum?id=b1CVu9l5GO",
        "link": "https://openreview.net/forum?id=b1CVu9l5GO",
        "pdf_link": "https://openreview.net/pdf?id=b1CVu9l5GO",
        "keywords": "Vision Language Model, Robot Learning",
        "abstract": "Although large vision-language-action (VLA) models pretrained on extensive robot datasets offer promising generalist policies for robotic learning, they still struggle with spatial-temporal dynamics in interactive robotics, making them less effective in handling complex tasks, such as manipulation. In this work, we introduce visual trace prompting, a simple yet effective approach to facilitate VLA models\u2019 spatial-temporal awareness for action prediction by encoding state-action trajectories visually. We develop a new TraceVLA model by finetuning\nOpenVLA on our own collected dataset of 150K robot manipulation trajectories using visual trace prompting. Evaluations of TraceVLA across 137 configurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate state-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and 3.5x on real-robot tasks and exhibiting robust generalization across diverse embodiments and scenarios. To further validate the effectiveness and generality of our method, we present a compact VLA model based on 4B Phi-3-Vision, pretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B OpenVLA baseline while significantly improving inference efficiency."
    },
    {
        "title": "LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression",
        "link_suffix": "/forum?id=PpYy0dR3Qw",
        "link": "https://openreview.net/forum?id=PpYy0dR3Qw",
        "pdf_link": "https://openreview.net/pdf?id=PpYy0dR3Qw",
        "keywords": "distributed optimization, local training, compression, communication-efficient algorithm, federated learning",
        "abstract": "In $D$istributed optimization and $L$earning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of $Lo$cal training, which reduces the communication frequency, and $Co$mpression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogeneous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms."
    },
    {
        "title": "CP-Guard+: A New Paradigm for Malicious Agent Detection and Defense in Collaborative Perception",
        "link_suffix": "/forum?id=9MNzHTSDgh",
        "link": "https://openreview.net/forum?id=9MNzHTSDgh",
        "pdf_link": "https://openreview.net/pdf?id=9MNzHTSDgh",
        "keywords": "Collaborative perception, security, defense, malicious agent detection",
        "abstract": "Collaborative perception (CP) is a promising method for safe connected and autonomous driving, which enables multiple connected and autonomous vehicles (CAVs) to share sensing information with each other to enhance perception performance. For example, occluded objects can be detected, and the sensing range can be extended. However, compared with single-agent perception, the openness of a CP system makes it more vulnerable to malicious agents and attackers, who can inject malicious information to mislead the perception of an ego CAV, resulting in severe risks for the safety of autonomous driving systems. To mitigate the vulnerability of CP systems, we first propose a new paradigm for malicious agent detection that effectively identifies malicious agents at the feature level without requiring verification of final perception results, significantly reducing computational overhead. Building on this paradigm, we introduce CP-GuardBench, the first comprehensive dataset provided to train and evaluate various malicious agent detection methods for CP systems. Furthermore, we develop a robust defense method called CP-Guard+, which enhances the margin between the representations of benign and malicious features through a carefully designed mixed contrastive training strategy. Finally, we conduct extensive experiments on both CP-GuardBench and V2X-Sim, and the results demonstrate the superiority of CP-Guard+."
    },
    {
        "title": "LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models",
        "link_suffix": "/forum?id=3b9SKkRAKw",
        "link": "https://openreview.net/forum?id=3b9SKkRAKw",
        "pdf_link": "https://openreview.net/pdf?id=3b9SKkRAKw",
        "keywords": "data synthesis, diffusion models, cardiac MRI, lung nodule CT, segmentation",
        "abstract": "Patient data from real-world clinical practice often suffers from data scarcity and long-tail imbalances, leading to biased outcomes or algorithmic unfairness. This study addresses these challenges by generating lesion-containing image-segmentation pairs from lesion-free images. Previous efforts in medical imaging synthesis have struggled with separating lesion information from background, resulting in low-quality backgrounds and limited control over the synthetic output. Inspired by diffusion-based image inpainting, we propose LeFusion, a lesion-focused diffusion model. By redesigning the diffusion learning objectives to focus on lesion areas, we simplify the learning process and improve control over the output while preserving high-fidelity backgrounds by integrating forward-diffused background contexts into the reverse diffusion process. Additionally, we tackle two major challenges in lesion texture synthesis: 1) multi-peak and 2) multi-class lesions. We introduce two effective strategies: histogram-based texture control and multi-channel decomposition, enabling the controlled generation of high-quality lesions in difficult scenarios. Furthermore, we incorporate lesion mask diffusion, allowing control over lesion size, location, and boundary, thus increasing lesion diversity. Validated on 3D cardiac lesion MRI and lung nodule CT datasets, LeFusion-generated data significantly improves the performance of state-of-the-art segmentation models, including nnUNet and SwinUNETR."
    },
    {
        "title": "DRUPI: Dataset Reduction Using Privileged Information",
        "link_suffix": "/forum?id=rCno6eYdXk",
        "link": "https://openreview.net/forum?id=rCno6eYdXk",
        "pdf_link": "https://openreview.net/pdf?id=rCno6eYdXk",
        "keywords": "Dataset Reduction, Efficient AI, Privileged Information",
        "abstract": "Dataset reduction (DR) seeks to select or distill samples from large datasets into smaller subsets while preserving performance on target tasks. Existing methods primarily focus on pruning or synthesizing data in the same format as the original dataset, typically the input data and corresponding labels. However, in DR settings, we find it is possible to synthesize more information beyond the data-label pair as an additional learning target to facilitate model training. In this paper, we introduce Dataset Reduction Using Privileged Information (DRUPI), which enriches DR by synthesizing privileged information alongside the reduced dataset. This privileged information can take the form of feature labels or attention labels, providing auxiliary supervision to improve model learning. Our findings reveal that effective feature labels must balance between being overly discriminative and excessively diverse, with a moderate level proving optimal for improving the reduced dataset\u2019s efficacy. Extensive experiments on ImageNet, CIFAR-10/100, and Tiny ImageNet demonstrate that DRUPI integrates seamlessly with existing dataset reduction methods, offering significant performance gains.Code is included in the supplementary material and will be released."
    },
    {
        "title": "Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Models",
        "link_suffix": "/forum?id=UvMSKonce8",
        "link": "https://openreview.net/forum?id=UvMSKonce8",
        "pdf_link": "https://openreview.net/pdf?id=UvMSKonce8",
        "keywords": "Interpretability, Efficient-AI, Transformers",
        "abstract": "The debate between self-interpretable models and post-hoc explanations for black-box models is central to Explainable AI (XAI). Self-interpretable models, such as concept-based networks, offer insights by connecting decisions to human-understandable concepts but often struggle with performance and scalability. Conversely, post-hoc methods like Shapley values, while theoretically robust, are computationally expensive and resource-intensive. To bridge the gap between these two lines of research, we propose a novel method that combines their strengths, providing theoretically guaranteed self-interpretability for black-box models without compromising prediction accuracy. Specifically, we introduce a parameter-efficient pipeline, AutoGnothi, which integrates a small side network into the black-box model, allowing it to generate Shapley value explanations without changing the original network parameters. This side-tuning approach significantly reduces memory, training, and inference costs, outperforming traditional parameter-efficient methods, where full fine-tuning serves as the optimal baseline. AutoGnothi enables the black-box model to predict and explain its predictions with minimal overhead. Extensive experiments show that AutoGnothi offers accurate explanations for both vision and language tasks, delivering superior computational efficiency with comparable interpretability."
    }
]