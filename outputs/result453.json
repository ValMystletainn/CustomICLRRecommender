[
    {
        "title": "Discovering Long-Term Effects on Parameter Efficient Fine-tuning",
        "link_suffix": "/forum?id=n2VZtv8tqL",
        "link": "https://openreview.net/forum?id=n2VZtv8tqL",
        "pdf_link": "https://openreview.net/pdf?id=n2VZtv8tqL",
        "keywords": "Bio-inspried, PEFT, transfer learning",
        "abstract": "Pre-trained Artificial Neural Networks (ANNs) demonstrate robust pattern recognition abilities, closely mirroring the functionality of Biological Neural Networks (BNNs). We are particularly intrigued by these models' capacity for acquiring new knowledge through fine-tuning, such, Parameter-efficient Fine-tuning (PEFT). Given that both ANNs and BNNs propagate information layer-by-layer, a useful analogy can be drawn: ANN weights correspond to synapses in BNNs, while features (latent variables or activations) parallel the neurotransmitters released by neurons. Building upon this clue, we delve deeper into exploring the connections between feature adjustment and weight adjustment, resulting in our proposed method Synapses & Neurons (SAN) that learns scaling matrices for features and propagates their effects towards posterior weight matrices. Our approach draws strong inspiration from well-known neuroscience phenomena - Long-term Potentiation (LTP) and Long-term Depression (LTD), which also reveal the relationship between synapse development and neurotransmitter release levels. We conducted extensive comparisons of PEFT on 26 datasets using attention-based networks as well as convolution-based networks, leading to significant improvements compared to other tuning methods, +8.5% over fully-finetune, +7% over Visual Prompt Tuning, and +3.2% over Low-Rank Adapter."
    },
    {
        "title": "Progressive Parameter Efficient Transfer Learning for Semantic Segmentation",
        "link_suffix": "/forum?id=YNbLUGDAX5",
        "link": "https://openreview.net/forum?id=YNbLUGDAX5",
        "pdf_link": "https://openreview.net/pdf?id=YNbLUGDAX5",
        "keywords": "Parameter Efficient Transfer Learning, Semantic Segmentation",
        "abstract": "Parameter Efficient Transfer Learning (PETL) excels in downstream classification fine-tuning with minimal computational overhead, demonstrating its potential within the pre-train and fine-tune paradigm. However, recent PETL methods consistently struggle when fine-tuning for semantic segmentation tasks, limiting their broader applicability. In this paper, we identify that fine-tuning for semantic segmentation requires larger parameter adjustments due to shifts in semantic perception granularity. Current PETL approaches are unable to effectively accommodate these shifts, leading to significant performance degradation. To address this, we introduce ProPETL, a novel approach that incorporates an additional midstream adaptation to progressively align pre-trained models for segmentation tasks. Through this process, ProPETL achieves state-of-the-art performance on most segmentation benchmarks and, for the first time, surpasses full fine-tuning on the challenging COCO-Stuff10k dataset. Furthermore, ProPETL demonstrates strong generalization across various pre-trained models and scenarios, highlighting its effectiveness and versatility for broader adoption in segmentation tasks."
    },
    {
        "title": "CROSS-CHANNEL ACTIVATION FUNCTION WITH PASS-THROUGH RATIO CONTROL",
        "link_suffix": "/forum?id=7UTsVPcHZa",
        "link": "https://openreview.net/forum?id=7UTsVPcHZa",
        "pdf_link": "https://openreview.net/pdf?id=7UTsVPcHZa",
        "keywords": "Activation functions, Simplex projection, Convolutional Neural Network, Pass-through ratio",
        "abstract": "In convolutional neural networks (CNNs), activation layers process features from convolutional layers, which have multiple output channels. Conventional activation functions like ReLU handle these multi-channel features independently, ignoring spatial and cross-channel dependencies. This hard-thresholding approach can lead to information loss by eliminating negative features and disrupting the connection within input features. To address this issue, we propose a novel activation function that considers mutual relations across multiple channels. Our activation layer processes tuples across channels as single inputs, ensuring that output tuples remain in the same projection space, with their $\\ell_1$ norms bounded by a learnable parameter. This parameter controls the pass-through ratio, which is the proportion of input data allowed to pass through the activation layer, offering a significant advantage over ReLU. Our approach demonstrated superior accuracy in classification tasks on common benchmarks and domain-specific datasets for CNN-based models. The proposed activation layer outperformed ReLU and other common layers in both clean and noisy data scenarios, as confirmed by statistical tests. Our results highlight the effectiveness of this activation function in maintaining feature integrity and improving model performance."
    },
    {
        "title": "Dynamic Low-Rank Sparse Adaptation for Large Language Models",
        "link_suffix": "/forum?id=oXh0939Zzq",
        "link": "https://openreview.net/forum?id=oXh0939Zzq",
        "pdf_link": "https://openreview.net/pdf?id=oXh0939Zzq",
        "keywords": "Large Language Models; Network Sparsity; Low-Rank Adaptation",
        "abstract": "Despite the efficacy of network sparsity in alleviating the deployment strain of Large Language Models (LLMs), it endures significant performance degradation. Applying Low-Rank Adaptation (LoRA) to fine-tune the sparse LLMs offers an intuitive approach to counter this predicament, while it holds shortcomings include: 1) The inability to integrate LoRA weights into sparse LLMs post-training, and 2) Insufficient performance recovery at high sparsity ratios. In this paper, we introduces dynamic $\\textbf{Lo}$w-rank $\\textbf{S}$parse $\\textbf{A}$daptation $\\textbf{(LoSA)}$, a novel method that seamlessly integrates low-rank adaptation into LLM sparsity within a unified framework, thereby enhancing the performance of sparse LLMs without increasing the inference latency. In particular, LoSA dynamically sparsifies the LoRA outcomes based on the corresponding sparse weights during fine-tuning, thus guaranteeing that the LoRA module can be integrated into the sparse LLMs post-training. Besides, to achieve the optimal sparse model architecture, LoSA leverages Representation Mutual Information (RMI) as an indicator to determine the importance of layers, thereby dynamically determining the optimal layer-wise sparsity rates during fine-tuning. Predicated on this, LoSA adjusts the rank of the LoRA module based on the variability in layer-wise reconstruction errors, allocating an appropriate fine-tuning for each layer to reduce the output discrepancies between dense and sparse LLMs. Extensive experiments tell that LoSA can efficiently boost the efficacy of sparse LLMs within a few hours, without introducing any additional inferential burden. For example, LoSA reduced the perplexity of sparse LLaMA-2-7B by $\\textbf{68.73}$$\\downarrow$ and increased zero-shot accuracy by $\\textbf{16.32}$%$\\uparrow$, achieving a $\\textbf{2.60$\\times$}$ speedup on CPU and $\\textbf{2.23$\\times$}$ speedup on GPU, requiring only $\\textbf{45 minutes}$ of fine-tuning on $\\textbf{a single}$ NVIDIA A100 80GB GPU. Code is available in the supplementary material."
    },
    {
        "title": "Semantic Score Distillation Sampling for Compositional Text-to-3D Generation",
        "link_suffix": "/forum?id=sOAQY8hrAu",
        "link": "https://openreview.net/forum?id=sOAQY8hrAu",
        "pdf_link": "https://openreview.net/pdf?id=sOAQY8hrAu",
        "keywords": "Text-to-3D Generation, Score Distillation Sampling",
        "abstract": "Generating high-quality 3D assets from textual descriptions remains a pivotal challenge in computer graphics and vision research. Due to the scarcity of 3D data, state-of-the-art approaches utilize pre-trained 2D diffusion priors, optimized through Score Distillation Sampling (SDS). Despite progress, crafting complex 3D scenes featuring multiple objects or intricate interactions is still difficult. To tackle this, recent methods have incorporated box or layout guidance. However, these layout-guided compositional methods often struggle to provide fine-grained control, as they are generally coarse and lack expressiveness. To overcome these challenges, we introduce a novel SDS approach, Semantic Score Distillation Sampling (SemanticSDS), designed to effectively improve the expressiveness and accuracy of compositional text-to-3D generation. Our approach integrates new semantic embeddings that maintain consistency across different rendering views and clearly differentiate between various objects and parts. These embeddings are transformed into a semantic map, which directs a region-specific SDS process, enabling precise optimization and compositional generation. By leveraging explicit semantic guidance, our method unlocks the compositional capabilities of existing pre-trained diffusion models, thereby achieving superior quality in 3D content generation, particularly for complex objects and scenes. Experimental results demonstrate that our SemanticSDS framework is highly effective for generating state-of-the-art complex 3D content."
    },
    {
        "title": "Generalizable Transferability Estimation of Foundation Vision Models via Implicit Learning",
        "link_suffix": "/forum?id=v8GuB74YRA",
        "link": "https://openreview.net/forum?id=v8GuB74YRA",
        "pdf_link": "https://openreview.net/pdf?id=v8GuB74YRA",
        "keywords": "Transferability Estimation, Transfer Learning",
        "abstract": "Transferability estimation aims to identify the most suitable model from a collection of pre-trained models for specific downstream tasks, playing a crucial role in the success of the pre-training and fine-tuning paradigm. However, the recent proliferation of pre-trained models with diverse architectures and training strategies poses significant challenges for transferability estimation due to discrepancies in intrinsic model characteristics, making it difficult for existing methods to accurately simulate embedding space evolution within feasible computational limits. To address these challenges, we propose an Implicit Transferability Modeling (ITM) paradigm that incorporates an implicit modeling strategy for the intrinsic properties of pre-trained models, enabling more accurate transferability estimation. ITM employs a Divide-and-Conquer Adaptation (DCA) process to efficiently model the transfer process, reducing both learning complexity and computational cost. Additionally, we introduce a Pseudo-Clustering-based Optimization (PCO) strategy that eliminates the need for extensive fine-tuning, enabling effective estimation without intensive retraining. Our method significantly outperforms state-of-the-art approaches, achieving notable improvements across ten widely used benchmarks and demonstrating its effectiveness and generalizability in enabling accurate and efficient model selection for downstream tasks."
    },
    {
        "title": "Towards Efficient Automatic Self-Pruning of Large Language Models",
        "link_suffix": "/forum?id=Iv4NCR9wzg",
        "link": "https://openreview.net/forum?id=Iv4NCR9wzg",
        "pdf_link": "https://openreview.net/pdf?id=Iv4NCR9wzg",
        "keywords": "Large Language Models; Structured Pruning; Evolutionary Algorithm",
        "abstract": "Despite exceptional capabilities, Large Language Models (LLMs) still face deployment challenges due to their enormous size. \nPost-training structured pruning is a promising solution that prunes LLMs without the need for retraining, reducing computational overhead, and it is hardware-deployment friendly.\nHowever, the training-free nature of post-training structured pruning leads to significant performance degradation. \nWe argue that the key to mitigating this issue lies in accurately determining the pruning rate for each layer. \nMeanwhile, we find that LLMs may have prior knowledge about their own redundancy. \nBased on this insight, we introduce $\\textbf{Self-Pruner}$ an end-to-end automatic self-pruning framework for LLMs, which efficiently search layer-wise pruning rates.\nSpecifically, $\\textbf{Self-Pruner}$ leverages LLMs to autonomously execute the entire evolutionary search process to search for pruning rate configurations. \nIn this process, LLMs are used to generate populations, select parent solutions from the current population, and perform crossover and mutation operations to produce offspring solutions. \nIn this way, LLMs automatically generate and evaluate a large number of candidate solutions, effectively converging to find the pruning rate configurations with minimal human intervention.\nExtensive experiments demonstrate $\\textbf{Self-Pruner}$'s better performance compared to existing state-of-the-art methods. \nNotably, $\\textbf{Self-Pruner}$ prunes LLaMA-2-70B to 49B level with only 0.80% drop in accuracy across seven commonsense reasoning tasks, achieving a 1.39$\\times$ speedup on NVIDIA A100 80GB GPU. Further pruning to 35B level resulted in only a 3.80% decrease in accuracy while obtaining a 1.70$\\times$ speedup. Code is available in the supplementary material."
    },
    {
        "title": "Mitigating Gradient Interference for Efficient Sparse Fine-Tuning of Large Language Models",
        "link_suffix": "/forum?id=53MDeiZ9mC",
        "link": "https://openreview.net/forum?id=53MDeiZ9mC",
        "pdf_link": "https://openreview.net/pdf?id=53MDeiZ9mC",
        "keywords": "Large language models, Sparse",
        "abstract": "Large Language Model (LLM) sparsification plays a crucial role in model compression. \nAmong various methods, training-free approaches are highly efficient but often result in accuracy loss, while full fine-tuning requires substantial computational resources. \nRecent works have begun exploring sparse Parameter-Efficient Fine-Tuning (PEFT) methods, but lack theoretical guidance.\nThis study presents the first comprehensive theoretical framework for efficient sparse fine-tuning, addressing a critical gap in the literature. \nSpecifically, we identify gradient conflict as the primary issue in PEFT sparse methods, wherein masked pretrained weights and corresponding PEFT weights exhibit competing optimization objectives during fine-tuning, potentially compromising model performance.\nWe theoretically model this phenomenon and identify three key factors influencing the efficacy of fine-tuning in sparsified LLMs: (1) error introduced by weight norms, (2) error composition from PEFT structures, and (3) error accumulation during fine-tuning.\nLeveraging these theoretical insights, we propose a novel iterative sparse fine-tuning scheme that systematically addresses each identified factor. \nWe implement an iterative process alternating between sparsity and fine-tuning to mitigate accumulated error in single turn of finetuning. \nWe employ pooling instead of low-rank decomposition to reduce error composition from PEFT structures. \nWe apply normalization to PEFT modules during fine-tuning, constraining error values by limiting weight norms while preserving representational capacity. \nAdditionally, we utilize Centered Kernel Alignment based information similarity assessment for adaptive allocation of layer-level sparsity and PEFT parameter quantities, addressing layer-specific redundancy.\nEmpirical evaluation on a 50% sparse LLaMA-2 7B model demonstrates the superiority of our approach, achieving lossless compression."
    },
    {
        "title": "A Coefficient Makes SVRG Effective",
        "link_suffix": "/forum?id=twtTLZnG0B",
        "link": "https://openreview.net/forum?id=twtTLZnG0B",
        "pdf_link": "https://openreview.net/pdf?id=twtTLZnG0B",
        "keywords": "Optimization; Variance Reduction; SGD",
        "abstract": "Stochastic Variance Reduced Gradient (SVRG), introduced by Johnson & Zhang (2013), is a theoretically compelling optimization method. However, as Defazio & Bottou (2019) highlight, its effectiveness in deep learning is yet to be proven. In this work, we demonstrate the potential of SVRG in optimizing real-world neural networks. Our empirical analysis finds that, for deeper neural networks, the strength of the variance reduction term in SVRG should be smaller and decrease as training progresses. Inspired by this, we introduce a multiplicative coefficient $\\alpha$ to control the strength and adjust it through a linear decay schedule. We name our method $\\alpha$-SVRG. Our results show $\\alpha$-SVRG better optimizes models, consistently reducing training loss compared to the baseline and standard SVRG across various model architectures and multiple image classification datasets. We hope our findings encourage further exploration into variance reduction techniques in deep learning."
    },
    {
        "title": "DiffPC: Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement",
        "link_suffix": "/forum?id=RL7PycCtAO",
        "link": "https://openreview.net/forum?id=RL7PycCtAO",
        "pdf_link": "https://openreview.net/pdf?id=RL7PycCtAO",
        "keywords": "lossy image compression, diffusion model",
        "abstract": "Reconstructing high-quality images under low bitrates conditions presents a challenge, and previous methods have made this task feasible by leveraging the priors of diffusion models.  However, the effective exploration of pre-trained latent diffusion models and semantic information integration in image compression tasks still needs further study. To address this issue, we introduce Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement (DiffPC), a two-stage image compression framework based on stable diffusion. DiffPC efficiently encodes low-level image information, enabling the highly realistic reconstruction of the original image by leveraging high-level semantic features and the prior knowledge inherent in diffusion models. Specifically, DiffPC utilizes a multi-feature compressor to represent crucial low-level information with minimal bitrates and employs pre-embedding to acquire more robust hybrid semantics, thereby providing additional context for the decoding end. Furthermore, we have devised a control module tailored for image compression tasks, ensuring structural and textural consistency in reconstruction even at low bitrates and preventing decoding collapses induced by condition leakage. Extensive experiments demonstrate that our method achieves state-of-the-art perceptual fidelity and surpasses previous perceptual image compression methods by a significant margin in statistical fidelity."
    },
    {
        "title": "MambaVC: Exploring Selective State Spaces for Learned Visual Compression",
        "link_suffix": "/forum?id=KgJwbsfN7G",
        "link": "https://openreview.net/forum?id=KgJwbsfN7G",
        "pdf_link": "https://openreview.net/pdf?id=KgJwbsfN7G",
        "keywords": "Selective State Spaces; Learned Visual Compression",
        "abstract": "Learned visual compression is an important and active task in multimedia. Existing approaches have explored various CNN- and Transformer-based designs to model content distribution and eliminate redundancy, where balancing efficacy (i.e., rate-distortion trade-off) and efficiency remains a challenge. Recently, state-space models (SSMs) have shown promise due to their long-range modeling capacity and efficiency. Inspired by this, we take the first step to explore SSMs for visual compression. We introduce MambaVC, a simple, strong and efficient compression network based on SSM. MambaVC develops a visual state space (VSS) block with a 2D selective scanning (2DSS) module as the nonlinear activation function after each downsampling, which helps to capture informative global contexts and enhances compression. On compression benchmark datasets, MambaVC achieves superior rate-distortion performance with lower computational and memory overheads. Specifically, it outperforms CNN and Transformer variants by 7.2% and 15.2% on Kodak, respectively, while reducing computation by 42% and 24%, and saving 12% and 71% of memory. MambaVC shows even greater improvements with high-resolution images, highlighting its potential and scalability in real-world applications. We also provide a comprehensive comparison of different network designs, underscoring MambaVC's advantages. Code is available athttps://anonymous.4open.science/r/MambaVC-408and will be open-sourced."
    },
    {
        "title": "A Decade's Battle on Dataset Bias: Are We There Yet?",
        "link_suffix": "/forum?id=SctfBCLmWo",
        "link": "https://openreview.net/forum?id=SctfBCLmWo",
        "pdf_link": "https://openreview.net/pdf?id=SctfBCLmWo",
        "keywords": "Vision datasets, Dataset bias, Deep learning",
        "abstract": "We revisit the ``dataset classification'' experiment suggested by Torralba & Efros (2011) a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be explained by memorization. We hope our discovery will inspire the community to rethink issues involving dataset bias."
    },
    {
        "title": "Deep Neural Networks without Normalization",
        "link_suffix": "/forum?id=nmRY3BAll4",
        "link": "https://openreview.net/forum?id=nmRY3BAll4",
        "pdf_link": "https://openreview.net/pdf?id=nmRY3BAll4",
        "keywords": "Normalization, Deep Neural Networks",
        "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. In this work, we demonstrate that we can achieve strong performance without them, using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation:\n$\\mathrm{DyT}(\\mathbf {x}) = \\tanh(\\alpha \\mathbf {x}),$\nas a drop-in replacement to normalization layers (e.g., layer normalization).\nDyT is directly inspired by the simple observation that  normalization layers produce tanh-like, S-shaped curves for their input-output mappings.\nWith DyT, networks without normalization layers could match or exceed the performance of their normalization counterparts, while keeping all other training hyperparameters intact. \nExperiments across diverse settings validate this, ranging from recognition to generation, ConvNets to LLMs, and supervised to self-supervised learning. \nOur findings challenge the conventional understanding that normalization layers are indispensable, and provide new insights into their workings."
    },
    {
        "title": "Consistency Flow Matching: Defining Straight Flows with Velocity Consistency",
        "link_suffix": "/forum?id=bS76qaGbel",
        "link": "https://openreview.net/forum?id=bS76qaGbel",
        "pdf_link": "https://openreview.net/pdf?id=bS76qaGbel",
        "keywords": "Flow Matching, Generative Models",
        "abstract": "Flow matching (FM) is a general framework for defining probability paths via Ordinary Differential Equations (ODEs) to transform between noise and data samples. Recent approaches attempt to straighten these flow trajectories to generate high-quality samples with fewer function evaluations, typically through iterative rectification methods or optimal transport solutions. In this paper, we introduce Consistency Flow Matching (Consistency-FM), a novel FM method that explicitly enforces self-consistency in the velocity field. Consistency-FM directly defines straight flows starting from different times to the same endpoint, imposing constraints on their velocity values. Additionally, we propose a multi-segment training approach for Consistency-FM to enhance expressiveness, achieving a better trade-off between sampling quality and speed. Extensive experiments demonstrate that our Consistency-FM significantly improves training efficiency by converging 4.4x faster than consistency models and 1.7x faster than rectified flow models while achieving better generation quality."
    },
    {
        "title": "Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning",
        "link_suffix": "/forum?id=RYrJqz44p4",
        "link": "https://openreview.net/forum?id=RYrJqz44p4",
        "pdf_link": "https://openreview.net/pdf?id=RYrJqz44p4",
        "keywords": "parameter efficient fine-tuning, low-rank adaptation, task-specific directions",
        "abstract": "Large language models demonstrate impressive performance on downstream tasks, yet requiring extensive resource consumption when fully fine-tuning all parameters. To mitigate this, Parameter Efficient Fine-Tuning (PEFT) strategies, such as LoRA, have been developed. \nIn this paper, we delve into the concept of task-specific directions (TSDs)\u2014critical for transitioning large models from pretrained states to task-specific enhancements in PEFT. We propose a framework to clearly define these directions and explore their properties, and practical utilization challenges. We then introduce a novel approach, LoRA-Dash, which aims to maximize the impact of TSDs during the fine-tuning process, thereby enhancing model performance on targeted tasks. Extensive experiments have conclusively demonstrated the effectiveness of LoRA-Dash, and in-depth analyses further reveal the underlying mechanisms of LoRA-Dash."
    },
    {
        "title": "Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation",
        "link_suffix": "/forum?id=D4xztKoz0Y",
        "link": "https://openreview.net/forum?id=D4xztKoz0Y",
        "pdf_link": "https://openreview.net/pdf?id=D4xztKoz0Y",
        "keywords": "category-level object pose estimation, spherical representations, shape-independence, correspondence prediction",
        "abstract": "Category-level object pose estimation aims to determine the pose and size of novel objects in specific categories. Existing correspondence-based approaches typically adopt point-based representations to establish the correspondences between primitive observed points and normalized object coordinates. However, due to the inherent shape-dependence of canonical coordinates, these methods suffer from semantic incoherence across diverse object shapes. To resolve this issue, we innovatively leverage the sphere as a shared proxy shape of objects to learn shape-independent transformation via spherical representations. Based on this insight, we introduce a novel architecture called SpherePose, which yields precise correspondence prediction through three core designs. Firstly, We endow the point-wise feature extraction with $\\mathrm{SO(3)$-invariance, which facilitates robust mapping between camera coordinate space and object coordinate space regardless of rotation transformation. Secondly, the spherical attention mechanism is designed to propagate and integrate features among spherical anchors from a comprehensive perspective, thus mitigating the interference of noise and incomplete point cloud. Lastly, a hyperbolic correspondence loss function is designed to distinguish subtle distinctions, which can promote the precision of correspondence prediction. Experimental results on CAMERA25, REAL275 and HouseCat6D benchmarks demonstrate the superior performance of our method, verifying the effectiveness of spherical representations and architectural innovations."
    },
    {
        "title": "SparseVLM: Visual Token Sparsification for Efficient Vision Language Models Inference",
        "link_suffix": "/forum?id=1xG3MN1RRW",
        "link": "https://openreview.net/forum?id=1xG3MN1RRW",
        "pdf_link": "https://openreview.net/pdf?id=1xG3MN1RRW",
        "keywords": "Sparsification, Vision Language Model, Efficiency",
        "abstract": "In vision-language models (VLMs), visual tokens usually consume a significant amount of computational overhead, despite their sparser information density compared to text tokens. To address this, most existing methods learn a network to prune redundant visual tokens and require additional training data. Differently, we propose an efficient training-free token optimization mechanism dubbed SparseVLM without extra parameters or fine-tuning costs. Concretely, given that visual tokens complement text tokens in VLMs for linguistic reasoning, we select visual-relevant text tokens to rate the significance of vision tokens within the self-attention matrix extracted from the VLMs. Then we progressively prune irrelevant tokens. To maximize sparsity while retaining essential information, we introduce a rank-based strategy to adaptively determine the sparsification ratio for each layer, alongside a token recycling method that compresses pruned tokens into more compact representations. Experimental results show that our SparseVLM improves the efficiency of various VLMs across a range of image and video understanding tasks. In particular, LLaVA equipped with SparseVLM reduces 61% $\\sim$ 67% FLOPs with a compression ratio of 78% while maintaining 93% of the accuracy."
    },
    {
        "title": "Spread them Apart: Towards Robust Watermarking of Generated Content",
        "link_suffix": "/forum?id=9XEBFywIW7",
        "link": "https://openreview.net/forum?id=9XEBFywIW7",
        "pdf_link": "https://openreview.net/pdf?id=9XEBFywIW7",
        "keywords": "data watermarking, ai safety, generative content",
        "abstract": "Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license.  In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks."
    },
    {
        "title": "Prune at the Clients, Not the Server: Accelerated Sparse Training in Federated Learning",
        "link_suffix": "/forum?id=Pv6fwGPgrA",
        "link": "https://openreview.net/forum?id=Pv6fwGPgrA",
        "pdf_link": "https://openreview.net/pdf?id=Pv6fwGPgrA",
        "keywords": "Federated Learning, Compression, Sparsity, Pruning, Communication Efficiency, Local Training",
        "abstract": "In the recent paradigm of Federated Learning (FL), multiple clients train a shared model while keeping their local data private. Resource constraints of clients and communication costs pose major problems for training large models in FL. On the one hand, addressing the resource limitations of the clients, sparse training has proven to be a powerful tool in the centralized setting. On the other hand, communication costs in FL can be addressed by local training, where each client takes multiple gradient steps on its local data. Recent work has shown that local training can provably achieve the optimal accelerated communication complexity [Mishchenko et al., 2022]. Hence, one would like an accelerated sparse training algorithm. In this work we show that naive integration of sparse training and acceleration on the server fails, and how to fix it by letting the clients perform these tasks appropriately. We introduce Sparse-ProxSkip, our method developed for the nonconvex setting, inspired by RandProx [Condat and Richt\u00e1rik, 2022], which provably combines sparse training and acceleration in the convex setting. We demonstrate the good performance of Sparse-ProxSkip in extensive experiments."
    },
    {
        "title": "FedComLoc: Communication-Efficient Distributed Training of Sparse and Quantized Models",
        "link_suffix": "/forum?id=0jmFRA64Vw",
        "link": "https://openreview.net/forum?id=0jmFRA64Vw",
        "pdf_link": "https://openreview.net/pdf?id=0jmFRA64Vw",
        "keywords": "Federated Learning, Compression, Sparsity, Quantization, Communication Efficiency, Local Training",
        "abstract": "Federated Learning (FL) has garnered increasing attention due to its unique characteristic of allowing heterogeneous clients to process their private data locally and interact with a central server, while being respectful of privacy. A critical bottleneck in FL is the communication cost. A pivotal strategy to mitigate this burden is Local Training, which involves running multiple local stochastic gradient descent iterations between communication phases. Our work is inspired by the innovative Scaffnew algorithm, which has considerably advanced the reduction of communication complexity in FL. We introduce FedComLoc (Federated Compressed and Local Training), integrating practical and effective compression into Scaffnew to further enhance communication efficiency. Extensive experiments, using the popular Top-K compressor and quantization, demonstrate its prowess in substantially reducing communication overheads in heterogeneous settings."
    },
    {
        "title": "Cohort Squeeze: Beyond a Single Communication Round per Cohort in Cross-Device Federated Learning",
        "link_suffix": "/forum?id=3emaMXjdkF",
        "link": "https://openreview.net/forum?id=3emaMXjdkF",
        "pdf_link": "https://openreview.net/pdf?id=3emaMXjdkF",
        "keywords": "stochastic proximal point methods, federated learning, cross-device setting, arbitrary sampling",
        "abstract": "Virtually all federated learning (FL) methods, including FedAvg, operate in the following manner: i) an orchestrating server sends the current model parameters to a cohort of clients selected via certain rule, ii) these clients then independently perform a local training procedure (e.g., via SGD or Adam) using their own training data, and iii) the resulting models are shipped to the server for aggregation. This process is repeated until a model of suitable quality is found. A notable feature of these methods is that each cohort is involved in a single communication round with the server only. In this work we challenge this algorithmic design primitive and investigate whether it is possible to \u201csqueeze more juice\u201d out of each cohort than what is possible in a single communication round. Surprisingly, we find that this is indeed the case, and our approach leads to up to 74% reduction in the total communication cost needed to train a FL model in the cross-device setting. Our method is based on a novel variant of the stochastic proximal point method (SPPM-AS) which supports a large collection of client sampling procedures some of which lead to further gains when compared to classical client selection approaches."
    },
    {
        "title": "SuperCorrect: Supervising and Correcting Language Models with Error-Driven Insights",
        "link_suffix": "/forum?id=PyjZO7oSw2",
        "link": "https://openreview.net/forum?id=PyjZO7oSw2",
        "pdf_link": "https://openreview.net/pdf?id=PyjZO7oSw2",
        "keywords": "Large Language Models, LLM Reasoning",
        "abstract": "Large language models (LLMs) like GPT-4, PaLM, and LLaMA have shown significant improvements in various reasoning tasks. However, smaller models such as Llama-3-8B and DeepSeekMath-Base still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps.\nTo overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models."
    },
    {
        "title": "Beyond Sequence: Impact of Geometric Context for RNA Property Prediction",
        "link_suffix": "/forum?id=9htTvHkUhh",
        "link": "https://openreview.net/forum?id=9htTvHkUhh",
        "pdf_link": "https://openreview.net/pdf?id=9htTvHkUhh",
        "keywords": "Geometric deep learning, Graph Neural Networks, GNNs, RNA property prediction, explicit structural information, curated datasets, benchmark models, noise and robustness, partial labeling, OOD generalization",
        "abstract": "Accurate prediction of RNA properties, such as stability and interactions, is crucial for advancing our understanding of biological processes and developing RNA-based therapeutics. RNA structures can be represented as 1D sequences, 2D topological graphs, or 3D all-atom models, each offering different insights into its function. Existing works predominantly focus on 1D sequence-based models, which overlook the geometric context provided by 2D and 3D geometries. This study presents the first systematic evaluation of incorporating explicit 2D and 3D geometric information into RNA property prediction, considering not only performance but also real-world challenges such as limited data availability, partial labeling, sequencing noise, and computational efficiency. To this end, we introduce a newly curated set of RNA datasets with enhanced 2D and 3D structural annotations, providing a resource for model evaluation on RNA data. Our findings reveal that models with explicit geometry encoding generally outperform sequence-based models, with an average prediction RMSE reduction of around 12% across all various RNA tasks and excelling in low-data and partial labeling regimes, underscoring the value of explicitly incorporating geometric context. On the other hand, geometry-unaware sequence-based models are more robust under sequencing noise but often require around 2-5x training data to match the performance of geometry-aware models. Our study offers further insights into the trade-offs between different RNA representations in practical applications and addresses a significant gap in evaluating deep learning models for RNA tasks."
    },
    {
        "title": "DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from One Demo",
        "link_suffix": "/forum?id=8oFvUBvF1u",
        "link": "https://openreview.net/forum?id=8oFvUBvF1u",
        "pdf_link": "https://openreview.net/pdf?id=8oFvUBvF1u",
        "keywords": "robotics, correspondence, computer vision, 3D vision",
        "abstract": "Dense 3D correspondence can enhance robotic manipulation by enabling the generalization of spatial, functional, and dynamic information from one object to an unseen counterpart. Compared to shape correspondence, semantic correspondence is more effective in generalizing across different object categories. To this end, we present DenseMatcher, a method capable of computing 3D correspondences between in-the-wild objects that share similar structures. DenseMatcher first computes vertex features by projecting multiview 2D features onto meshes and refining them with a 3D network, and subsequently finds dense correspondences with the obtained features using functional map. In addition, we craft the first 3D matching dataset that contains colored object meshes across diverse categories. In our experiments, we show that DenseMatcher significantly outperforms prior 3D matching baselines by 43.5%. We demonstrate the downstream effectiveness of DenseMatcher in (i) robotic manipulation, where it achieves cross-instance and cross-category generalization on long-horizon complex manipulation tasks from observing only one demo; (ii) zero-shot color mapping between digital assets, where appearance can be transferred between different objects with relatable geometry. More details and demonstrations can be found athttp://densematcher.github.io."
    },
    {
        "title": "Enhancing Prototype-Based Federated Learning with Structured Sparse Prototypes",
        "link_suffix": "/forum?id=LilItwL2br",
        "link": "https://openreview.net/forum?id=LilItwL2br",
        "pdf_link": "https://openreview.net/pdf?id=LilItwL2br",
        "keywords": "federated learning, prototype-based federated learning, distributed machine learning, structured sparsity",
        "abstract": "Prototype-Based Federated Learning (PBFL) has gained attention for its communication efficiency, privacy preservation, and personalization capabilities in resource-constrained environments. Despite these advantages, PBFL methods face challenges, including high communication costs for high-dimensional prototypes and numerous classes, privacy concerns during aggregation, and uniform knowledge distillation in heterogeneous data settings.\nTo address these issues, we introduce three novel methods, each targeting a specific PBFL stage: 1) Class-wise Prototype Sparsification (CPS) reduces communication costs by creating structured sparse prototypes, where each prototype utilizes only a subset of representation layer dimensions. 2) Privacy-Preserving Prototype Aggregation (PPA) enhances privacy by eliminating the transmission of client class distribution information when aggregating local prototypes. 3) Class-Proportional Knowledge Distillation (CPKD) improves personalization by modulating the distillation strength for each class based on clients' local data distributions.\nWe integrate these three methods into two foundational PBFL approaches and conduct experimental evaluations. The results demonstrate that this integration achieves up to 10\u00d7 and 4\u00d7 reductions in communication costs while outperforming the original and most communication-efficient approaches evaluated, respectively."
    }
]