[
    {
        "title": "f-Divergence Policy Optimization in Fully Decentralized Cooperative MARL",
        "link_suffix": "/forum?id=1olDGAXncb",
        "link": "https://openreview.net/forum?id=1olDGAXncb",
        "pdf_link": "https://openreview.net/pdf?id=1olDGAXncb",
        "keywords": "multi-agent, reinforcement learning, fully decentralized learning, policy optimization, convergence, independent learning",
        "abstract": "Independent learning is a straightforward solution for fully decentralized learning in cooperative multi-agent reinforcement learning (MARL). The study of independent learning has a history of decades, and the representatives, such as independent Q-learning and independent PPO, can obtain good performance in some benchmarks. However, most independent learning algorithms lack convergence guarantees or theoretical support. In this paper, we propose a general formulation of independent policy optimization, $f$-divergence policy optimization. We show the generality of such a formulation and analyze its limitation. Based on this formulation, we further propose a novel independent learning algorithm, TVPO, that theoretically guarantees convergence. Empirically, we show that TVPO outperforms state-of-the-art fully decentralized learning methods in three popular cooperative MARL benchmarks, which verifies the efficacy of TVPO."
    },
    {
        "title": "How Feature Learning Can Improve Neural Scaling Laws",
        "link_suffix": "/forum?id=dEypApI1MZ",
        "link": "https://openreview.net/forum?id=dEypApI1MZ",
        "pdf_link": "https://openreview.net/pdf?id=dEypApI1MZ",
        "keywords": "neural scaling laws, feature learning, kernel methods, linear networks, mean field theory",
        "abstract": "We develop a simple solvable model of neural scaling laws beyond the kernel limit. Theoretical analysis of this model predicts the performance scaling predictions with model size, training time and total amount of available data. From the scaling analysis we identify three relevant regimes: hard tasks, easy tasks, and super easy tasks. For easy and super-easy target functions, which are in the Hilbert space (RKHS) of the initial infinite-width neural tangent kernel (NTK), there is no change in the scaling exponents between feature learning models and models in the kernel regime. For hard tasks, which we define as tasks outside of the RKHS of the initial NTK, we show analytically and empirically that feature learning can improve the scaling with training time and compute, approximately doubling the exponent for very hard tasks. This leads to a new compute optimal scaling law for hard tasks in the feature learning regime. We support our finding that feature learning improves the scaling law for hard tasks with experiments of nonlinear MLPs fitting functions with power-law Fourier spectra on the circle and CNNs learning vision tasks."
    },
    {
        "title": "Agent-G: An Agentic Framework for Graph Retrieval Augmented Generation",
        "link_suffix": "/forum?id=g2C947jjjQ",
        "link": "https://openreview.net/forum?id=g2C947jjjQ",
        "pdf_link": "https://openreview.net/pdf?id=g2C947jjjQ",
        "keywords": "Graph Retrieval-Augmented Generation, Large Language Models, LLM Agents",
        "abstract": "Given two knowledge sources, one containing unstructured documents and the other comprising structured graph knowledge bases, how can we effectively retrieve the relevant information to answer user questions?\nWhile Retrieval-Augmented Generation (RAG) retrieves documents to assist the large language model (LLM) in question answering,\nGraph RAG (GRAG) uses graph knowledge bases as an additional knowledge source.\nHowever, there are many questions that require information from both sources, which complicates the scenario and makes hybrid retrieval essential.\nThe goal is to effectively leverage both sources to provide better answers to the questions.\nTherefore, we propose Agent-G, a unified framework for GRAG, composed of an agent, a retriever bank, and a critic module.\nAgent-G has the following advantages:Agentic, it automatically improves the agent's action with self-reflection,Adaptive, it solves questions that require hybrid knowledge source with a single unified framework,Interpretable, it justifies decision making and reduces hallucinations, andEffective, it adapts to different GRAG settings and outperforms all baselines.The experiments are conducted on two real-world GRAG benchmarks, namely STaRK and CRAG.\nIn STaRK, Agent-G shows relative improvements in Hit@1 of 47% in STaRK-MAG and 55% in TaRK-Prime.\nIn CRAG, Agent-G increases accuracy by 35% while reducing hallucination by 11%, both relatively."
    },
    {
        "title": "Keys to Robust Edits: From Theoretical Insights to Practical Advances",
        "link_suffix": "/forum?id=a69zct3BkY",
        "link": "https://openreview.net/forum?id=a69zct3BkY",
        "pdf_link": "https://openreview.net/pdf?id=a69zct3BkY",
        "keywords": "model editing",
        "abstract": "Large language models (LLMs) have revolutionized knowledge storage and retrieval, but face challenges with conflicting and outdated information. Knowledge editing techniques have been proposed to address these issues, yet they struggle with robustness tests involving long contexts, paraphrased subjects, and continuous edits. This work investigates the cause of these failures in locate-and-edit methods, offering theoretical insights into their key-value modeling and deriving mathematical bounds for robust and specific edits, leading to a novel 'group discussion' conceptual model for locate-and-edit methods. Empirical analysis reveals that keys used by current methods fail to meet robustness and specificity requirements. To address this, we propose a Robust Edit Pathway (REP) that disentangles editing keys from LLMs' inner representations. Evaluations on LLaMA2-7B and Mistral-7B using the CounterFact dataset show that REP significantly improves robustness across various metrics, both in-domain and out-of-domain, with minimal trade-offs in success rate and locality. Our findings advance the development of reliable and flexible knowledge updating in LLMs."
    },
    {
        "title": "STAMP: Scalable Task- And Model-agnostic Collaborative Perception",
        "link_suffix": "/forum?id=8NdNniulYE",
        "link": "https://openreview.net/forum?id=8NdNniulYE",
        "pdf_link": "https://openreview.net/pdf?id=8NdNniulYE",
        "keywords": "Autonomous Driving, Collaborative Perception, Domain Adaptation",
        "abstract": "Perception is a crucial component of autonomous driving systems. However, single-agent setups often face limitations due to sensor constraints, especially under challenging conditions like severe occlusion, adverse weather, and long-range object detection. Multi-agent collaborative perception (CP) offers a promising solution that enables communication and information sharing between connected vehicles. Yet, the heterogeneity among agents\u2014in terms of sensors, models, and tasks\u2014significantly hinders effective and efficient cross-agent collaboration. To address these challenges, we propose STAMP, a scalable task- and model-agnostic collaborative perception framework tailored for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific domains and a shared protocol domain, facilitating efficient feature sharing and fusion while minimizing computational overhead. Moreover, our approach enhances scalability, preserves model security, and accommodates a diverse range of agents. Extensive experiments on both simulated (OPV2V) and real-world (V2V4Real) datasets demonstrate that STAMP achieves comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As the first-of-its-kind task- and model-agnostic collaborative perception framework, STAMP aims to advance research in scalable and secure mobility systems, bringing us closer to Level 5 autonomy. Our project page is at \\href{https://jocular-manatee-91cad0.netlify.app/}{https://jocular-manatee-91cad0.netlify.app}and the code is available at \\href{https://anonymous.4open.science/r/STAMP-id}{https://anonymous.4open.science/r/STAMP}."
    },
    {
        "title": "360-InpaintR: Reference-Guided 3D Inpainting for Unbounded Scenes",
        "link_suffix": "/forum?id=AMVLOv30Qg",
        "link": "https://openreview.net/forum?id=AMVLOv30Qg",
        "pdf_link": "https://openreview.net/pdf?id=AMVLOv30Qg",
        "keywords": "NeRF inpainting, 3D Gaussian splatting, Reference-based inpainting, Unbounded scene",
        "abstract": "This paper introduces 360-InpaintR, the first reference-based 360\u00b0 inpainting method for 3D Gaussian Splatting (3DGS) scenes, particularly designed for unbounded environments. Our method leverages multi-view information and introduces an improved unseen mask generation technique to address the challenges of view consistency and geometric plausibility in 360\u00b0 scenes. We effectively integrate reference-guided 3D inpainting with diffusion priors to ensure consistent results across diverse viewpoints. To facilitate research in this area, we present a new 360\u00b0 inpainting dataset and capture protocol, enabling high-quality novel view synthesis and quantitative evaluations of modified scenes. Experimental results demonstrate that 360-InpaintR performs favorably against existing methods in both quantitative metrics and qualitative assessments, particularly in complex scenes with large view variations."
    },
    {
        "title": "Energy-Based Conceptual Diffusion Model",
        "link_suffix": "/forum?id=BV84FICIAM",
        "link": "https://openreview.net/forum?id=BV84FICIAM",
        "pdf_link": "https://openreview.net/pdf?id=BV84FICIAM",
        "keywords": "Interpretability, Concepts, Diffusion Model, Energy-Based Model, Generative Model",
        "abstract": "Diffusion models have shown impressive sample generation capabilities across various domains. However, current methods are still lacking in human-understandable explanations and interpretable control: (1) they do not provide a probabilistic framework for systematic interpretation. For example, when tasked with generating an image of a \"Nighthawk\", they cannot quantify the probability of specific concepts (e.g., \"black bill\" and \"brown crown\" usually seen in Nighthawks) or verify whether the generated concepts align with the instruction. This limits explanations of the generative process; (2) they do not naturally support control mechanisms based on concept probabilities, such as correcting errors (e.g., correcting \"black crown\" to \"brown crown\" in a generated \"Nighthawk\" image) or performing imputations using these concepts, therefore falling short in interpretable editing capabilities. To address these limitations, we propose Energy-based Conceptual Diffusion Models (ECDMs). ECDMs integrate diffusion models and Concept Bottleneck Models (CBMs) within the framework of Energy-Based Models to provide unified interpretations. Unlike conventional CBMs, which are typically discriminative, our approach extends CBMs to the generative process. ECDMs use a set of energy networks and pretrained diffusion models to define the joint energy estimation of the input instructions, concept vectors, and generated images. This unified framework enables concept-based generation, interpretation, debugging, intervention, and imputation through conditional probabilities derived from energy estimates. Our experiments on various real-world datasets demonstrate that ECDMs offer both strong generative performance and rich concept-based interpretability."
    },
    {
        "title": "VLMGuard: Defending VLMs against Malicious Prompts via Unlabeled Data",
        "link_suffix": "/forum?id=JwoCs9O3QL",
        "link": "https://openreview.net/forum?id=JwoCs9O3QL",
        "pdf_link": "https://openreview.net/pdf?id=JwoCs9O3QL",
        "keywords": "Malicious prompt detection",
        "abstract": "Vision-language models (VLMs) are essential for contextual understanding of both visual and textual information. However, their vulnerability to adversarially manipulated inputs presents significant risks, leading to compromised outputs and raising concerns about the reliability in VLM-integrated applications.  Detecting these malicious prompts is thus crucial for maintaining trust in VLM generations. A major challenge in developing a safeguarding prompt classifier is the lack of a large amount of labeled benign and malicious data.  To\n address the issue, we introduce VLMGuard, a novel learning framework that leverages the unlabeled user prompts in the wild for malicious prompt detection. These unlabeled prompts, which naturally arise when VLMs are deployed in the open world, consist of both benign and malicious information. To harness the unlabeled data, we present an automated maliciousness estimation score for distinguishing between benign and malicious samples within this unlabeled mixture, thereby enabling the training of a binary prompt classifier on top. Notably, our framework does not require extra human annotations, offering strong flexibility and practicality for real-world applications. Extensive experiment shows VLMGuard achieves superior detection results, significantly outperforming state-of-the-art methods.  Disclaimer: This paper may contain offensive examples; reader discretion is advised."
    },
    {
        "title": "Constructing Confidence Intervals for Average Treatment Effects from Multiple Observational Datasets",
        "link_suffix": "/forum?id=BHFs80Jf5V",
        "link": "https://openreview.net/forum?id=BHFs80Jf5V",
        "pdf_link": "https://openreview.net/pdf?id=BHFs80Jf5V",
        "keywords": "Causality machine learning, Average treatment effects, Confidence intervals, Prediction-powered inference",
        "abstract": "Estimating confidence intervals (CIs) of the average treatment effects (ATE) from patient records is crucial to assess the effectiveness and safety of drugs. However, patient records typically come from different hospitals, thus raising the question of how multiple observational datasets can be effectively combined for this purpose. In our paper, we propose a new method that estimates the ATE from multiple observational datasets and provides valid CIs. Our method makes little assumptions about the observational datasets and is thus widely applicable in medical practice. The key idea of our method is that we leverage prediction-powered inferences and thereby essentially `shrink' the CIs so that we offer more precise uncertainty quantification as compared to na{\"i}ve approaches. We further prove the unbiasedness of our method and the validity of our CIs. We confirm our theoretical results through various numerical experiments. Finally, we provide an extension of our method for constructing CIs from combinations of experimental and observational datasets."
    },
    {
        "title": "ElastoGen: 4D Generaetive Elastodynamics",
        "link_suffix": "/forum?id=j50c2tkQUu",
        "link": "https://openreview.net/forum?id=j50c2tkQUu",
        "pdf_link": "https://openreview.net/pdf?id=j50c2tkQUu",
        "keywords": "generative model, machine learning, neural network architectures",
        "abstract": "We present ElastoGen, a knowledge-driven model that generates physically accurate and coherent 4D elastodynamics. Instead of relying on petabyte-scale data-driven learning, ElastoGen leverages the principles of physics-in-the-loop and learns from established physical knowledge, such as partial differential equations and their numerical solutions. The core idea of ElastoGen is converting the global differential operator, corresponding to the nonlinear elastodynamic equations, into iterative local convolution-like operations, which naturally fit modern neural networks. Each network module is specifically designed to support this goal rather than functioning as a black box. As a result, ElastoGen is exceptionally lightweight in terms of both training requirements and network scale. Additionally, due to its alignment with physical procedures, ElastoGen efficiently generates accurate dynamics for a wide range of hyperelastic materials and can be easily integrated with upstream and downstream deep modules to enable end-to-end 4D generation."
    },
    {
        "title": "InnateCoder: Learning Programmatic Options with Foundation Models",
        "link_suffix": "/forum?id=QiUitwJDKI",
        "link": "https://openreview.net/forum?id=QiUitwJDKI",
        "pdf_link": "https://openreview.net/pdf?id=QiUitwJDKI",
        "keywords": "programmatic policies, reinforcement learning, options",
        "abstract": "Outside of transfer learning settings, reinforcement learning agents start their learning process from a clean slate. As a result, such agents have to go through a slow process to learn even the most obvious skills required to solve a problem. In this paper, we present InnateCoder, a system that leverages human knowledge encoded in foundation models to provide programmatic policies that encode \"innate skills\" in the form of temporally extended actions, or options. In contrast to existing approaches to learning options, InnateCoder learns them from the general human knowledge encoded in foundation models in a zero-shot setting, and not from the knowledge the agent gains by interacting with the environment. Then, InnateCoder searches for a programmatic policy by combining the programs encoding these options into a larger and more complex program. We hypothesized that InnateCoder's scheme of learning and using options could improve the sampling efficiency of current methods for synthesizing programmatic policies. We evaluated our hypothesis in MicroRTS and Karel the Robot, two challenging domains. Empirical results support our hypothesis, since they show that InnateCoder is more sample efficient than versions of the system that do not use options or learn the options from experience. The policies InnateCoder learns are competitive and often outperform current state-of-the-art agents in both domains."
    },
    {
        "title": "Learning to Help in Multi-Class Settings",
        "link_suffix": "/forum?id=NCgTbt2j1F",
        "link": "https://openreview.net/forum?id=NCgTbt2j1F",
        "pdf_link": "https://openreview.net/pdf?id=NCgTbt2j1F",
        "keywords": "learning with abstention, learning with reject option, learning to defer, classification",
        "abstract": "Deploying complex machine learning models on resource-constrained devices is challenging due to limited computational power, memory, and model retrainability. To address these limitations, a hybrid system can be established by augmenting the local model with a server-side model, where samples are selectively deferred by arejectorand then sent to the server for processing. The hybrid system enables efficient use of computational resources while minimizing the overhead associated with server usage. The recently proposed Learning to Help (L2H) model proposed training a server model given a fixed local (client) model. This differs from the Learning to Defer (L2D) framework which trains the client for a fixed (expert) server. In both L2D and L2H, the training includes learning a rejector at the client to determine when to query the server. In this work, we extend the L2H model from binary to multi-class classification problems and demonstrate its applicability in a number of different scenarios of practical interest in which access to the server may be limited by cost, availability, or policy. We derive a stage-switching surrogate loss function that is differentiable, convex, and consistent with the Bayes rule corresponding to the 0-1 loss for the L2H model. Experiments show that our proposed methods offer an efficient and practical solution for multi-class classification in resource-constrained environments."
    },
    {
        "title": "Stand on Two Shoulders: Dynamically Merging Tokens from General and Medical Experts",
        "link_suffix": "/forum?id=33P4evE2ej",
        "link": "https://openreview.net/forum?id=33P4evE2ej",
        "pdf_link": "https://openreview.net/pdf?id=33P4evE2ej",
        "keywords": "Visual Adaptation, Medical Representation Learning",
        "abstract": "In the realm of medical image analysis, the transferability of pre-trained Vision Transformers (ViTs) to specialized medical tasks remains a significant challenge. Previous approaches focus on adapting a single model, by introducing specialized learnable layers to the pre-trained model. However, a single model optimized for general tasks underperforms in domain-specific applications, while one medical models limited by their fundamental inferior capabilities, is not robust enough in real-world adaptation. To address this, we introduce the DynaMer Adapter, a novel architecture designed to enable Dynamically Merge tokens from general and medical pre-trained models, enhancing the adaptability of ViTs for medical imaging tasks. DynaMer incorporates a Gated Mixture-of-Expert (MoE) Adapter, ensuring that the model ingeniously prioritizes relevant features for specific medical tasks. Additionally, we incorporate a layer-wise skipping router within the architecture, designed to adjust the number of input tokens efficiently, thereby optimizing inference time without compromising on model accuracy. Extensive evaluations on the Medical Visual Task Adaptation Benchmark (Med-VTAB) demonstrate that DynaMer achieves state-of-the-art performance, particularly excelling in patient out-of-distribution settings and tasks with only few samples."
    },
    {
        "title": "MIBench: A Comprehensive Benchmark for Model Inversion Attack and Defense",
        "link_suffix": "/forum?id=QWjpjisCjs",
        "link": "https://openreview.net/forum?id=QWjpjisCjs",
        "pdf_link": "https://openreview.net/pdf?id=QWjpjisCjs",
        "keywords": "Benchmark, Privacy Leakage, Model Inversion",
        "abstract": "Model Inversion (MI) attacks aim at leveraging the output information of target models to reconstruct privacy-sensitive training data, raising widespread concerns on privacy threats of Deep Neural Networks (DNNs). Unfortunately, in tandem with the rapid evolution of MI attacks, the lack of a comprehensive, aligned, and reliable benchmark has emerged as a formidable challenge. This deficiency leads to inadequate comparisons between different attack methods and inconsistent experimental setups. In this paper, we introduce the first practical benchmark for model inversion attacks and defenses to address this critical gap, which is named \"\\textit{MIBench}\". This benchmark serves as an extensible and reproducible modular-based toolbox and currently integrates a total of 16 state-of-the-art attack and defense methods. Moreover, we furnish a suite of assessment tools encompassing 9 commonly used evaluation protocols to facilitate standardized and fair evaluation and analysis. Capitalizing on this foundation, we conduct extensive experiments from multiple perspectives to holistically compare and analyze the performance of various methods across different scenarios, which overcomes the misalignment issues and discrepancy prevalent in previous works. Based on the collected attack methods and defense strategies, we analyze the impact of target resolution, defense robustness, model predictive power, model architectures, transferability and loss function. Our hope is that this \\textit{MIBench} could provide a unified, practical and extensible toolbox and is widely utilized by researchers in the field to rigorously test and compare their novel methods, ensuring equitable evaluations and thereby propelling further advancements in the future development."
    },
    {
        "title": "Implicit Bayesian Markov Decision Process for Resource Efficient Decisions in Drug Discovery",
        "link_suffix": "/forum?id=CaNp8ALCRT",
        "link": "https://openreview.net/forum?id=CaNp8ALCRT",
        "pdf_link": "https://openreview.net/pdf?id=CaNp8ALCRT",
        "keywords": "Bayesian Markov Decision Process, ensemble approach, similarity-based metric, sequential decision making",
        "abstract": "In drug discovery, researchers make sequential decisions to schedule experiments, aiming to maximize the probability of success towards drug candidates while simultaneously minimizing expected costs. However, such tasks pose significant challenges due to complex trade-offs between uncertainty reduction and allocation of constrained resources in a high-dimensional state-action space. Traditional methods based on simple rule-based heuristics or domain expertise often result in either inefficient resource utilization due to risk aversion or missed opportunities due to reckless decisions. To address these challenges, we developed an Implicit Bayesian Markov Decision Process (IB-MDP) algorithm that constructs an explicit MDP model of the environment\u2019s dynamics by integrating historical data through a similarity-based metric and enables effective planning by simulating future states and actions. To enhance the robustness of the decision-making process, the IB-MDP also incorporates an ensemble approach that recommends maximum likelihood actions to effectively balance the dual objectives of reducing state uncertainty and optimizing expected costs. Our experimental results demonstrate that the IB-MDP algorithm offers significant improvements over traditional rule-based methods by identifying optimal decisions that ensure more efficient use of resources in drug discovery."
    },
    {
        "title": "Metalic: Meta-Learning In-Context with Protein Language Models",
        "link_suffix": "/forum?id=TUKt7ag0qq",
        "link": "https://openreview.net/forum?id=TUKt7ag0qq",
        "pdf_link": "https://openreview.net/pdf?id=TUKt7ag0qq",
        "keywords": "Meta-Learning, In-context Learning, Tuning, Protein, Fitness Prediction, Property Prediction, Protein Language Model, PLM, Large Language Model",
        "abstract": "Predicting the biophysical and functional properties of proteins is essential for in silico protein design. Machine learning has emerged as a promising technique for such prediction tasks. However, the relative scarcity of in vitro annotations means that these models often have little, or no, specific data on the desired fitness prediction task. As a result of limited data, protein language models (PLMs) are typically trained on general protein sequence modeling tasks, and then fine-tuned, or applied zero-shot, to protein fitness prediction. When no task data is available, the models make strong assumptions about the correlation between the protein sequence likelihood and fitness scores. In contrast, we propose meta-learning over a distribution of standard fitness prediction tasks, and demonstrate positive transfer to unseen fitness prediction tasks. Our method, called Metalic (Meta-Learning In-Context), uses in-context learning and fine-tuning, when data is available, to adapt to new tasks. Crucially, fine-tuning enables considerable generalization, even though it is not accounted for during meta-training. Our fine-tuned models achieve strong results with 18 times fewer parameters than state-of-the-art models. Moreover, our method sets a new state-of-the-art in low-data settings on ProteinGym, an established fitness-prediction benchmark. Due to data scarcity, we believe meta-learning will play a pivotal role in advancing protein engineering."
    },
    {
        "title": "Mexa: Multilingual Evaluation of English-Centric LLMs via  Cross-Lingual Alignment",
        "link_suffix": "/forum?id=hsMkpzr9Oy",
        "link": "https://openreview.net/forum?id=hsMkpzr9Oy",
        "pdf_link": "https://openreview.net/pdf?id=hsMkpzr9Oy",
        "keywords": "multilingual, evaluation, low-resource languages, large language models",
        "abstract": "English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages.\nWe introduce Mexa, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. Mexa leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate task performance in other languages.\nWe conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models.\nOur results show that Mexa, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets.\nThis suggests that Mexa is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. HuggingFace Leaderboard: [anonymized URL], GitHub Code: [anonymized URL]."
    },
    {
        "title": "MoE-Pruner: Pruning Mixture-of-Experts Large Language Model using the Hints from Its Router",
        "link_suffix": "/forum?id=hB6jYbvypa",
        "link": "https://openreview.net/forum?id=hB6jYbvypa",
        "pdf_link": "https://openreview.net/pdf?id=hB6jYbvypa",
        "keywords": "mixture-of-experts, large language models, sparsity, network pruning, knowledge distillation",
        "abstract": "Mixture-of-Experts (MoE) architectures face challenges such as high memory consumption and redundancy in experts. Pruning MoE can reduce network weights while maintaining model performance. Motivated by the recent observation of emergent large magnitude features in Large Language Models (LLM) and MoE routing policy, we propose MoE-Pruner, a method that prunes weights with the smallest magnitudes multiplied by the corresponding input activations and router weights, on each output neuron. Our pruning method is one-shot, requiring no retraining or weight updates. We evaluate our method on Mixtral-8x7B and Mixtral-8x22B across multiple language benchmarks. Experimental results show that our method significantly outperforms state-of-the-art LLM pruning methods. Furthermore, our pruned MoE models can benefit from a pretrained teacher model through expert-wise knowledge distillation, improving performance post-pruning. Experimental results demonstrate that the Mixtral-8x7B model with 50% sparsity maintains 99% of the performance of the original model after the expert-wise knowledge distillation."
    },
    {
        "title": "Can We Ignore Labels in Out of Distribution Detection?",
        "link_suffix": "/forum?id=falBlwUsIH",
        "link": "https://openreview.net/forum?id=falBlwUsIH",
        "pdf_link": "https://openreview.net/pdf?id=falBlwUsIH",
        "keywords": "Out of Distribution, Uncertainty, Self Supervised, Unsupervised",
        "abstract": "Out-of-distribution (OOD) detection methods have recently become more prominent, serving as a core element in safety-critical autonomous systems. One major purpose of OOD detection is to reject invalid inputs that could lead to unpredictable errors and compromise safety. Due to the cost of labeled data, recent works have investigated the feasibility of self-supervised learning (SSL) OOD detection, unlabled OOD detection, and zero shot OOD detection. In this work, we identify a set of conditions for a theoretical guarantee of failure in unlabeled OOD detection algorithms from an information-theoretic perspective. These conditions are present in all OOD tasks dealing with real world data: I) we provide theoretical proof of unlabeled OOD detection failure when there exists zero mutual information between the learning objective and the in-distribution labels, a.k.a. \u2018label blindness\u2019, II) we define a new OOD task \u2013 Adjacent OOD detection \u2013 that tests for label blindness and accounts for a previously ignored safety gap in all OOD detection benchmarks, and III) we perform experiments demonstrating that existing unlabeled OOD methods fail under conditions suggested by our label blindness theory and analyze the implications for future research in unlabeled OOD methods."
    },
    {
        "title": "Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning",
        "link_suffix": "/forum?id=dCPF1wlqj8",
        "link": "https://openreview.net/forum?id=dCPF1wlqj8",
        "pdf_link": "https://openreview.net/pdf?id=dCPF1wlqj8",
        "keywords": "Large langauge model for reasoning, planning, code-aided reasoning",
        "abstract": "Despite the remarkable success of large language models (LLMs) on traditional natural language processing tasks, their planning ability remains a critical bottleneck in tackling complex multi-step reasoning tasks. Existing approaches mainly rely on prompting or task-specific fine-tuning, often suffering from weak robustness and cross-task generalization. To address the limitation, we introduce CodePlan, a scalable paradigm that empowers LLMs to generate and follow code-form plans---pseudocode that outlines high-level, structured reasoning processes. By leveraging the structured and versatile nature of code, CodePlan effectively captures the rich semantics and control flows inherent to sophisticated reasoning. Importantly, CodePlan allows the automatic extraction of code-form plans from massive, wide-ranging text corpora without the need for curated, task-specific datasets. This enables it to scale up efficiently and improve reasoning capabilities across diverse scenarios. To train CodePlan, we construct a large-scale dataset of 2M examples that integrate code-form plans with standard prompt-response pairs from existing corpora. With minimal computation overhead during both training and inference, CodePlan achieves a 25.1% relative improvement compared with directly generating responses, averaged across 13 challenging multi-step reasoning benchmarks, spanning mathematical reasoning, symbolic reasoning, instruction-following, multi-hop QA, and decision-making tasks. Further analysis reveals CodePlan's increasing performance gains on more complex reasoning tasks, as well as significant data efficiency thanks to its generalization ability."
    },
    {
        "title": "OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition",
        "link_suffix": "/forum?id=DLDuVbxORA",
        "link": "https://openreview.net/forum?id=DLDuVbxORA",
        "pdf_link": "https://openreview.net/pdf?id=DLDuVbxORA",
        "keywords": "network pruning, low-rank, compression, sparsification, large language models, outlier features",
        "abstract": "The recent paradigm shift to large-scale foundation models has brought about a new era for deep learning that, while has found great success in practice, has also been plagued by prohibitively expensive costs in terms of high memory consumption and compute. To mitigate these issues, there has been a concerted effort in post-hoc neural network pruning techniques that do not require costly retraining. Despite the considerable progress being made, existing methods often exhibit a steady drop in model performance as the compression increases. In this paper, we present a novel approach to compressing large transformers, coined OATS, that compresses the model weights by approximating each weight as the sum of a sparse matrix and a low-rank matrix. Prior to the decomposition, the weights are first scaled by the second moment of their input embeddings, so as to ensure the preservation of  outlier features recently observed in large transformer models. Without retraining, OATS achieves state-of-the-art performance when compressing large language models, such as Llama-3 and Phi-3, and vision transformers, such as Google's ViT and DINOv2, by up to $60\\%$, all while speeding up the model's inference on a CPU by up to $1.37\\times$ compared to prior pruning methods."
    },
    {
        "title": "Diffusion Models Are Real-Time Game Engines",
        "link_suffix": "/forum?id=P8pqeEkn1H",
        "link": "https://openreview.net/forum?id=P8pqeEkn1H",
        "pdf_link": "https://openreview.net/pdf?id=P8pqeEkn1H",
        "keywords": "Diffusion Models, Neural Game Engines, Real-Time Simulation, Reinforcement Learning, Auto-Regressive Generation, Video Game Simulation, Interactive World Simulation",
        "abstract": "We present GameNGen, the first game engine powered entirely by a neural model that also enables real-time interaction with a complex environment over long trajectories at high quality. When trained on the classic game DOOM, GameNGen extracts gameplay and uses it to generate a playable environment that can interactively simulate new trajectories. GameNGen runs at 20 frames per second on a single TPU and remains stable over extended multi-minute play sessions. Next frame prediction achieves a PSNR of 29.4, comparable to lossy JPEG compression. Human raters are only slightly better than random chance at distinguishing short clips of the game from clips of the simulation, even after 5 minutes of auto-regressive generation. GameNGen is trained in two phases: (1) an RL-agent learns to play the game and the training sessions are recorded, and (2) a diffusion model is trained to produce the next frame, conditioned on the sequence of past frames and actions. Conditioning augmentations help ensure stable auto-regressive generation over long trajectories, and decoder fine-tuning improves the fidelity of visual details and text."
    },
    {
        "title": "Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models",
        "link_suffix": "/forum?id=Mri9WIfxSm",
        "link": "https://openreview.net/forum?id=Mri9WIfxSm",
        "pdf_link": "https://openreview.net/pdf?id=Mri9WIfxSm",
        "keywords": "Discrete Diffusion, CTMCs, Ratio Matching, Score Entropy, Cross Entropy, Language Models",
        "abstract": "While continuous diffusion models excel in modeling continuous distributions, their application to categorical data has been less effective. Recent work has shown that ratio-matching throughscore-entropywithin a continuous-time discrete Markov chain (CTMC) framework serves as a competitive alternative to autoregressive models in language modeling.\nTo enhance this framework, we first introduce three new theorems concerning the KL divergence between the data and learned distribution. Our results serve as the discrete counterpart to those established for continuous diffusion models and allow us to derive an improved upper bound of the perplexity. Second, we empirically show that ratio-matching performed by minimizing thedenoising cross-entropybetween the clean and corrupted data enables models to outperform those utilizing score-entropy with up to 10% lower perplexity/generative-perplexity, and 15% faster training steps.\n To further support our findings, we introduce and evaluate a novel CTMC transition-rate matrix that allows prediction refinement, and derive the analytic expression for its matrix exponential which facilitates the computation of conditional ratios thus enabling efficient training and generation."
    },
    {
        "title": "Efficient Vision-Language Models by Summarizing Visual Tokens into Compact Registers",
        "link_suffix": "/forum?id=tNxr38vfYR",
        "link": "https://openreview.net/forum?id=tNxr38vfYR",
        "pdf_link": "https://openreview.net/pdf?id=tNxr38vfYR",
        "keywords": "Vision-language Models",
        "abstract": "Recent advancements in vision-language models have expanded their potential for real-world applications, enabling these models to perform complex reasoning on images. \nHowever, in the widely used fully autoregressive pipeline like LLaVA, where projected visual tokens are prepended to textual tokens, the visual tokens often number in the hundreds or thousands, making them much longer than the input prompt. This large quantity of visual tokens introduces significant computational overhead, slowing down training and inference. \nIn this paper, we propose Visual Compact Token Registers (Victor), a method that reduces the number of visual tokens by summarizing them into a smaller set of register tokens. Victor adds a few learnable register tokens after the visual tokens and summarizes the visual information into these registers using the first few layers in the language tower. After these few layers, all visual tokens are discarded, significantly improving computational efficiency for both training and inference. Notably, our method is easy to implement and requires a small number of new trainable parameters with minimal impact on model performance.\nIn our experiment, with merely $8$ visual registers\u2014about $1%$ of the original tokens\u2014Victor shows less than a $4%$ performance drop while reducing total training time by $43%$ and boosting inference throughput by $3.36\\times$."
    },
    {
        "title": "Managing Diffuse Risks in the Safe Deployment of Untrusted Large Language Models",
        "link_suffix": "/forum?id=keu6sxrPWn",
        "link": "https://openreview.net/forum?id=keu6sxrPWn",
        "pdf_link": "https://openreview.net/pdf?id=keu6sxrPWn",
        "keywords": "AI Safety, alignment, language model deployment",
        "abstract": "As large language models (LLMs) grow more powerful, they also become more difficult to trust. They could be either aligned with human intentions, or exhibit \"subversive misalignment\" -- introducing subtle errors that bypass safety checks. Although individual errors may not immediately cause harm, each increases the risk of an eventual safety failure. With this uncertainty, model deployment often grapples with the tradeoff between ensuring safety and harnessing the capabilities of untrusted models. In this work, we introduce the ``Diffuse Risk Management'' problem, aiming to balance the average-case safety and usefulness in the deployment of untrusted models over a large sequence of tasks. We approach this problem by developing a two-level framework: the single-task level (micro-protocol) and the whole-scenario level (macro-protocol). At the single-task level, we develop various \\textit{micro}-protocols that use a less capable, but extensively tested (trusted) model to harness and monitor the untrusted model. At the whole-scenario level, we find an optimal \\textit{macro}-protocol that uses an adaptive estimate of the untrusted model's risk to choose between micro-protocols. To evaluate the robustness of our method, we follow \\textit{control evaluations} in a code generation testbed, which involves a red team attempting to generate subtly backdoored code with an LLM whose deployment is safeguarded by a blue team. Experiment results show that our approach retains 99.6% usefulness of the untrusted model while ensuring near-perfect safety, significantly outperforming existing deployment methods. Our approach also demonstrates robustness when the trusted and untrusted models have a large capability gap. Our findings demonstrate the promise of managing diffuse risks in the deployment of increasingly capable but untrusted LLMs."
    }
]