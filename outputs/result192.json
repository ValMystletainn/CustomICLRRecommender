[{"title": "Data Attribution for Multitask Learning", "link_suffix": "/forum?id=77zLqGGowO", "link": "https://openreview.net/forum?id=77zLqGGowO", "pdf_link": "https://openreview.net/pdf?id=77zLqGGowO", "keywords": "Data Attribution, Influence Functions, Multitask Learning, Interpretability", "abstract": "Data attribution quantifies the influence of individual training data points on machine learning models, aiding in their interpretation and improvement. While prior work has primarily focused on single-task learning (STL), this work extends data attribution to multitask learning (MTL). Data attribution in MTL presents new opportunities for interpreting and improving MTL models while also introducing unique technical challenges. On the opportunity side, data attribution in MTL offers a natural way to efficiently measure task relatedness, a key factor that impacts the effectiveness of MTL. However, the shared and task-specific parameters in MTL models present challenges that require specialized data attribution methods. In this paper, we propose the MultiTask Influence Function (MTIF), a novel data attribution method tailored for MTL. MTIF leverages the structure of MTL models to efficiently estimate the impact of removing data points or excluding tasks on the predictions of specific target tasks, providing both data-level and task-level influence analysis. Extensive experiments on both linear and neural network models show that MTIF effectively approximates leave-one-out and leave-one-task-out effects. Moreover, MTIF facilitates fine-grained data selection, consistently improving model performance in MTL, and provides interpretable insights into task relatedness. Our work establishes a novel connection between data attribution and MTL, offering an efficient and scalable solution for measuring task relatedness and enhancing MTL models.", "title_embedding_index": 9550, "title_abs_embedding_index": 9575}, {"title": "Provable unlearning in topic modeling and downstream tasks", "link_suffix": "/forum?id=dh78yRFVK9", "link": "https://openreview.net/forum?id=dh78yRFVK9", "pdf_link": "https://openreview.net/pdf?id=dh78yRFVK9", "keywords": "Machine unlearning, topic models, language models", "abstract": "Machine unlearning algorithms are increasingly important as legal concerns arise around the provenance of training data, but verifying the success of unlearning is often difficult. Provable guarantees for unlearning are often limited to supervised learning settings. In this paper, we provide the first theoretical guarantees for unlearning in the pre-training and fine-tuning paradigm by studying topic models, simple bag-of-words language models that can be adapted to solve downstream tasks like retrieval and classification. First, we design a provably effective unlearning algorithm for topic models that incurs a computational overhead independent of the size of the original dataset. Our analysis additionally quantifies the deletion capacity of the model -- i.e., the number of examples that can be unlearned without incurring a significant cost in model performance. Finally, we formally extend our analyses to account for adaptation to a given downstream task. In particular, we design an efficient algorithm to perform unlearning after fine-tuning the topic model via a linear head. Notably, we show that it is easier to unlearn pre-training data from models that have been fine-tuned to a particular task, and one can unlearn this data without modifying the base model.", "title_embedding_index": 9551, "title_abs_embedding_index": 9576}, {"title": "Dynamic Self-Distillation via Previous Mini-batches for Fine-tuning Small Language Models", "link_suffix": "/forum?id=Wv9Gl1bFbc", "link": "https://openreview.net/forum?id=Wv9Gl1bFbc", "pdf_link": "https://openreview.net/pdf?id=Wv9Gl1bFbc", "keywords": "Knowledge Distillation, Self-Distillation, Pre-trained Language Models, Large Language Models, Small Language Models, NLP, Fine-tuning", "abstract": "Knowledge Distillation (KD) has become a widely adopted approach for compressing large language models (LLMs) to reduce computational costs and memory footprint. However, the availability of complex teacher models is a prerequisite for running most KD pipelines. Thus, the traditional KD procedure can be unachievable or budget-unfriendly, particularly when relying on commercial LLMs like GPT4. In this regard, Self Distillation (SelfD) emerges as an advisable alternative, enabling student models to learn without teachers' guidance. Nonetheless, existing SelfD approaches for LMs often involve architectural modifications, assuming the models are open-source, which may not always be practical. In this work, we introduce a model-agnostic and task-agnostic method named dynamic SelfD from the previous mini-batch (DynSDPB), which realizes current iterations\u2019 distillation from the last ones\u2019 generated logits. Additionally, to address prediction inaccuracies during the early iterations, we dynamically adjust the distillation influence and temperature values to enhance the adaptability of fine-tuning. Furthermore, we propose Vocabulary Map Matching (VMM), aiming to address output inconsistency for auto-regressive LLMs. Last but not least, DynSDPB facilitates the seamless integration of existing self-correction and self-training techniques for small language models (SLMs). We apply DynSDPB to both encoder-only LMs (e.g., BERT model families) and decoder-only LMs (e.g., LLaMA model families), validating its effectiveness across natural language understanding (NLU) and natural language generation (NLG) benchmarks.", "title_embedding_index": 9552, "title_abs_embedding_index": 9577}, {"title": "Mitigating Multimodal Hallucinations via Gradient-based Self-Reflection", "link_suffix": "/forum?id=zgXGNXkC0F", "link": "https://openreview.net/forum?id=zgXGNXkC0F", "pdf_link": "https://openreview.net/pdf?id=zgXGNXkC0F", "keywords": "Hallucinations, MLLMs, Gradient-based Analysis", "abstract": "Hallucination in Multimodal Large Language Models (MLLMs) occurs when inaccurate text-visual alignments are generated, posing a major challenge for reliable model output. Previous studies have identified three primary biases as major causes of hallucinations: text-visual bias (over-reliance on text over visual details), co-occurrence bias (misleading object correlations), and long-term bias (increased hallucinations in later stages of long sequences). Existing hallucination mitigation methods often rely on visual grounding, which requires additional resources such as scoring systems using another MLLM, and still fail to fully address all biases, particularly co-occurrence bias in visual inputs. We propose Gradient-based Influence-Aware Contrastive Decoding (GACD) to explicitly and jointly balance these biases, thereby mitigating hallucinations. To quantify these biases at the individual sample level, we introduce `token influence'. Since biases are rooted in the training data and become embedded in pre-trained MLLMs, we derive token influence through self-reflection by calculating the gradients from output predictions to input tokens. Notably, GACD is the first approach capable of fully addressing co-occurrence bias without relying on extra resources or any form of tuning. Extensive experiments demonstrate GACD's effectiveness in reducing hallucinations and improving MLLM performance, achieving new state-of-the-art results while providing insights into the visual perception capabilities of these models.", "title_embedding_index": 9553, "title_abs_embedding_index": 9578}, {"title": "A Critical Look At Tokenwise Reward-Guided Text Generation", "link_suffix": "/forum?id=KMWGzQi7Qy", "link": "https://openreview.net/forum?id=KMWGzQi7Qy", "pdf_link": "https://openreview.net/pdf?id=KMWGzQi7Qy", "keywords": "LLM, RLHF, Alignment, Model Efficiency, Reward Models, Sampling", "abstract": "Large language models (LLMs) can be improved by aligning with human preferences through fine-tuning---the so-called reinforcement learning from human feedback (RLHF). However, the cost of fine-tuning an LLM is prohibitive for many users. Due to their ability to bypass LLM fine-tuning, prediction-time tokenwise reward-guided text generation (RGTG) methods have recently been proposed. They use a reward model trained on full sequences to score partial sequences during decoding in a bid to steer the generation towards sequences with high rewards. However, these methods have so far been only heuristically motivated and poorly analyzed. In this work, we show that reward models trained on full sequences are not compatible with scoring partial sequences. To alleviate this issue, we propose to train a Bradley-Terry reward model on partial sequences explicitly, and autoregressively sample from the implied tokenwise policy during decoding time. We study the properties of this reward model and the resulting policy: We show that this policy is proportional to the ratio of two distinct RLHF policies. Our simple approach outperforms previous RGTG methods and performs similarly to strong offline baselines without large-scale LLM finetuning.", "title_embedding_index": 9554, "title_abs_embedding_index": 9579}, {"title": "Point-based Instance Completion with Scene Constraints", "link_suffix": "/forum?id=llSiIJosDj", "link": "https://openreview.net/forum?id=llSiIJosDj", "pdf_link": "https://openreview.net/pdf?id=llSiIJosDj", "keywords": "instance scene completion, point cloud completion", "abstract": "Recent point-based object completion methods have demonstrated the ability to accurately recover the missing geometry of partially observed objects. However, these approaches are not well-suited for completing objects within a scene as they do not consider known scene constraints (e.g., other observed surfaces) in their completions and further expect the partial input to be in a canonical coordinate system which does not hold for objects within scenes. While instance scene completion methods have been proposed for completing objects within a scene, they lag behind point-based object completion methods in terms of object completion quality and still do not consider known scene constraints during completion. To overcome these limitations, we propose a point cloud based instance completion model that can robustly complete objects at arbitrary scales and pose in the scene. To enable reasoning at the scene level, we introduce a sparse set of scene constraints represented as point clouds and integrate them into our completion model via a cross-attention mechanism. To evaluate the instance scene completion task on indoor scenes, we further build a new synthetic dataset called ScanWCF, which contains labeled partial scans as well as aligned ground truth scene completions that are watertight and collision free. Through several experiments, we demonstrate that our method achieves improved fidelity to partial scans, higher completion quality, and greater plausibility over existing state-of-the-art methods. The dataset and the code will be publicly available.", "title_embedding_index": 9555, "title_abs_embedding_index": 9580}, {"title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG", "link_suffix": "/forum?id=hPk6IZXo9V", "link": "https://openreview.net/forum?id=hPk6IZXo9V", "pdf_link": "https://openreview.net/pdf?id=hPk6IZXo9V", "keywords": "Brain decoding, Semantic reconstruction, Transfer learning, Deep learning, Intracranial EEG, Natural Language Processing", "abstract": "Decoding continuous language from neural signals remains a significant challenge in the intersection of neuroscience and artificial intelligence. We introduce Neuro2Semantic, a novel framework that reconstructs the semantic content of perceived speech from intracranial EEG (iEEG) recordings. Our approach consists of two phases: first, an LSTM-based adapter aligns neural signals with pre-trained text embeddings; second, a corrector module generates continuous, natural text directly from these aligned embeddings. This flexible method overcomes the limitations of previous decoding approaches and enables unconstrained text generation. Neuro2Semantic achieves remarkable performance with as little as 30 minutes of neural data, significantly outperforming a recent state-of-the-art method in low-data settings. These results highlight the potential for practical applications in brain-computer interfaces and neural decoding technologies.", "title_embedding_index": 9556, "title_abs_embedding_index": 9581}, {"title": "Core Tokensets for Data-efficient Sequential Training of Transformers", "link_suffix": "/forum?id=gLkmW0cfcI", "link": "https://openreview.net/forum?id=gLkmW0cfcI", "pdf_link": "https://openreview.net/pdf?id=gLkmW0cfcI", "keywords": "Transformers, Coresets, Sequential Learning", "abstract": "Deep networks are frequently tuned to novel tasks and continue learning from ongoing data streams. Such sequential training requires consolidation of new and past information, a challenge predominantly addressed by retaining the most important data points - formally known as coresets. Traditionally, these coresets consist of entire samples, such as images or sentences. However, recent transformer architectures operate on tokens, leading to the famous assertion that an image is worth 16x16 words. Intuitively, not all of these tokens are equally informative or memorable. Going beyond coresets, we thus propose to construct a deeper-level data summary on the level of tokens. Ours, respectively named core tokensets, both select the most informative data points and leverage feature attribution to store only their most relevant features. We demonstrate that core tokensets yield significant performance retention in incremental image classification, open-ended visual question answering, and continual image captioning with significantly reduced memory. In fact, we empirically find that a core tokenset of 1% of the data performs comparably to at least a twice as large and up to 10 times larger coreset.", "title_embedding_index": 9557, "title_abs_embedding_index": 9582}, {"title": "FACTS: A Factored State-Space Framework for World Modelling", "link_suffix": "/forum?id=dmCGjPFVhF", "link": "https://openreview.net/forum?id=dmCGjPFVhF", "pdf_link": "https://openreview.net/pdf?id=dmCGjPFVhF", "keywords": "spatial-temporal modelling, world modelling, multivariate time-series forecasting, object-centric representation learning, unsupervised learning", "abstract": "World modelling is essential for understanding and predicting the dynamics of complex systems by learning both spatial and temporal dependencies. However, current frameworks, such as Transformers and selective state-space models like Mambas, exhibit limitations in efficiently encoding spatial and temporal structures, particularly in scenarios requiring long-term high-dimensional sequence modelling. To address these issues, we propose a novel recurrent framework, the FACTored State-space (FACTS) model, for spatio-temporal world modelling. The FACTS framework constructs a graph-structured memory with a routing mechanism that learns permutable memory representations, ensuring invariance to input permutations while adapting through selective state-space propagation. Furthermore, FACTS can be linearised to support parallel computation of high-dimensional sequences. We empirically evaluate FACTS across diverse tasks, including multivariate time series forecasting and object-centric world modelling, demonstrating that it consistently outperforms or matches specialised state-of-the-art models, despite its general-purpose world modelling design.", "title_embedding_index": 9558, "title_abs_embedding_index": 9583}, {"title": "Phantom: General Trigger Attacks on Retrieval Augmented Language Generation", "link_suffix": "/forum?id=BHIsVV4G7q", "link": "https://openreview.net/forum?id=BHIsVV4G7q", "pdf_link": "https://openreview.net/pdf?id=BHIsVV4G7q", "keywords": "Large Language Models, AI Security, AI Safety, RAG, Poisoning Attacks", "abstract": "Retrieval Augmented Generation (RAG) expands the capabilities of modern large language models (LLMs), by anchoring, adapting, and personalizing their responses to the most relevant knowledge sources. It is particularly useful in chatbot applications, allowing developers to customize LLM output without expensive retraining.\nDespite their significant utility in various applications, RAG systems present new security risks. In this work, we propose new attack vectors that allow an adversary to inject a single malicious document into a RAG system's knowledge base, and mount a backdoor poisoning attack.\nWe design Phantom, a general two-stage optimization framework against RAG systems, that crafts a malicious poisoned document leading to an integrity violation in the model's output.\nFirst, the document is constructed to be retrieved only when a specific trigger sequence of tokens appears in the victim's queries. \nSecond, the document is further optimized with crafted adversarial text that induces various adversarial objectives on the LLM output, including refusal to answer, reputation damage, privacy violations, and harmful behaviors.\nWe demonstrate our attacks on multiple LLM architectures, including Gemma, Vicuna, and Llama, and show that they transfer to GPT-3.5 Turbo and GPT-4. Finally, we successfully conducted a Phantom attack on NVIDIA's black-box production RAG system, \"Chat with RTX\".", "title_embedding_index": 9559, "title_abs_embedding_index": 9584}, {"title": "Batched Bayesian optimization with correlated candidate uncertainties", "link_suffix": "/forum?id=fzJtylzsKO", "link": "https://openreview.net/forum?id=fzJtylzsKO", "pdf_link": "https://openreview.net/pdf?id=fzJtylzsKO", "keywords": "Bayesian optimization, molecular design, molecular discovery, uncertainty, model-guided optimization", "abstract": "Batched Bayesian optimization (BO) can accelerate molecular design by efficiently identifying top-performing compounds from a large chemical library. Existing acquisition strategies for batch design in BO aim to balance exploration and exploitation. This often involves optimizing non-additive batch acquisition functions, necessitating approximation via myopic construction and/or diversity heuristics. In this work, we propose an acquisition strategy for discrete optimization that is motivated by pure exploitation, qPO (multipoint Probability of Optimality). qPO maximizes the probability that the batch includes the true optimum, which is expressible as the sum over individual acquisition scores and thereby circumvents the combinatorial challenge of optimizing a batch acquisition function. We differentiate the proposed strategy from parallel Thompson sampling and discuss how it implicitly captures diversity. Finally, we apply our method to the model-guided exploration of large chemical libraries and provide empirical evidence that it performs better than or on par with state-of-the-art methods in batched Bayesian optimization.", "title_embedding_index": 9560, "title_abs_embedding_index": 9585}, {"title": "Benchmarking a well-calibrated measure of weight similarity of deep neural network models", "link_suffix": "/forum?id=izDiFGXn9B", "link": "https://openreview.net/forum?id=izDiFGXn9B", "pdf_link": "https://openreview.net/pdf?id=izDiFGXn9B", "keywords": "deep neural network, weights similarity, model interpretation, computater vision", "abstract": "Deep learning approaches have revolutionized artificial intelligence, but model opacity and fragility remain significant challenges. The reason for these challenges, we believe, is a knowledge gap at the heart of the field --- the lack of well-calibrated metrics quantifying the similarity of the internal representations of models obtained using different architectures, training strategies, different checkpoints, or under different random initializations.  While several metrics have been proposed, they are poorly calibrated and susceptible to manipulations and confounding factors, as well as being computationally intensive when probed with a large and diverse set of test samples. We report here an integration of chain normalization of weights and centered kernel alignment that, by focusing on weight similarity instead of activation similarity, overcomes most of the limitations of existing metrics. Our approach is sample-agnostic, symmetric in weight space, computationally efficient, and well-calibrated.", "title_embedding_index": 9561, "title_abs_embedding_index": 9586}, {"title": "Tackling the Generative learning trilemma through VAE and GMM-controlled latent space class expansion", "link_suffix": "/forum?id=KNXFYBrSWH", "link": "https://openreview.net/forum?id=KNXFYBrSWH", "pdf_link": "https://openreview.net/pdf?id=KNXFYBrSWH", "keywords": "data augmentation, classifier, variational auto-encoder, gaussian mixture, latent space representation", "abstract": "Achieving efficient data augmentation (DA) in time series classification is not a trivial task due to the high complexity of temporal data. Generative models, such as GANs (Generative Adversarial Networks), diffusion models, and Variational Autoencoders (VAEs), are powerful techniques to address the generative learning trilemma of producing (1) high-quality samples, (2) fast sampling, and (3) diversity. These methods vary in their ability to address the trilemma. Diffusion models allows for high diversity and high quality samples, while GAN allows for high quality samples and fast sampling, and VAE for high diversity and fast sampling. In this paper, we introduce a novel generative method, ASCENSION (VAE and GMM-controlled latent space class expansion), that retains the strengths of VAE in terms of diversity and fast sampling, while enabling controlled and quantifiable exploration of uncharted regions in the latent space. This approach not only enhances classification performance but also yields higher quality (more realistic) samples. ASCENSION leverages the probabilistic nature of the VAE's latent space to represent classes as Gaussian mixture models (GMMs). By modifying this mixture, we enable precise manipulation of class probability densities and boundaries. To ensure intra-class compactness and maximize inter-class separation, we apply clustering constraints. Empirical evaluations on the UCR benchmark (102 datasets) show that ASCENSION outperforms state-of-the-art DA methods, achieving an average classification accuracy improvement of approximately $7$% and excelling in all aspects of the generative learning trilemma.", "title_embedding_index": 9562, "title_abs_embedding_index": 9587}, {"title": "Noise-Robust Preference Losses for Deep Regression Models", "link_suffix": "/forum?id=w5pErXbwQl", "link": "https://openreview.net/forum?id=w5pErXbwQl", "pdf_link": "https://openreview.net/pdf?id=w5pErXbwQl", "keywords": "Regression, Robustness, Alignment", "abstract": "Deep regression models are widely employed for tasks such as pricing and forecasting. In industrial applications, it is common for analysts to adjust model outputs before they are deployed in commercial products. These adjustments, which we name \"analyst influences\", not only ensure the quality of the final products but also provide training data to improve model performance over time. However, due to the huge volumes of data, analyst influences can be applied broadly and can lack precision, hindering training effectiveness. To resolve the issue, we propose a novel framework Preference Learning from Analyst Influence which creates a weighted loss function that explicitly accounts for the relative quality levels of the training samples in comparison to model outputs. This approach effectively mitigates the impact of coarse training instances. Our extensive experiments on real-world data drawn from airline revenue management demonstrate that the proposed framework not only enhances pricing stability but also improves alignment with analyst influences compared to baselines.", "title_embedding_index": 9563, "title_abs_embedding_index": 9588}, {"title": "Towards Neural Scaling Laws for Foundation Models on Temporal Graphs", "link_suffix": "/forum?id=pIT0P1UASS", "link": "https://openreview.net/forum?id=pIT0P1UASS", "pdf_link": "https://openreview.net/pdf?id=pIT0P1UASS", "keywords": "Temporal graph learning, foundation model, graph neural networks, neural scaling law", "abstract": "The field of temporal graph learning aims to learn from evolving network data to forecast future interactions. Given a collection of observed temporal graphs, is it possible to predict the evolution of an unseen network from the same domain?\nTo answer this question, we first present the Temporal Graph Scaling (TGS) dataset, a large collection of temporal graphs consisting of eighty-four ERC20 token transaction networks collected from 2017 to 2023. Next, we evaluate the transferability of Temporal Graph Neural Networks (TGNNs) for the temporal graph property prediction task by pre-training on a collection of up to sixty-four token transaction networks and then evaluating the downstream performance on twenty unseen token networks. We find that the neural scaling law observed in NLP and Computer Vision also applies in temporal graph learning, where pre-training on a greater number of networks leads to improved downstream performance. To the best of our knowledge, this is the first empirical demonstration of the transferability of temporal graph learning. On downstream token networks, the largest pre-trained model outperforms single model TGNNs on thirteen unseen test networks. Therefore, we believe that this is a promising first step towards building foundation models for temporal graphs. We provide the implementation of TGS athttps://anonymous.4open.science/r/ScalingTGNs.", "title_embedding_index": 9564, "title_abs_embedding_index": 9589}, {"title": "Multi-Scale Image Diffusion Transformers: Explainability Leads to Faster Training", "link_suffix": "/forum?id=leBbjaUxut", "link": "https://openreview.net/forum?id=leBbjaUxut", "pdf_link": "https://openreview.net/pdf?id=leBbjaUxut", "keywords": "Diffusion Models, Vision Transformers, Generative Images, Explainable AI, Training Efficiency", "abstract": "Diffusion models have significantly advanced image synthesis but often face high computational demands and slow convergence rates during training. To tackle these challenges, we propose the Multi-Scale Diffusion Transformer (MDiT), which incorporates heterogeneous, asymmetric, scale-specific transformer blocks to reintroduce explicit inductive structural biases into diffusion transformers (DiTs). Using explainable AI techniques, we demonstrate that DiTs inherently learn these biases, exhibiting distinct encode-decode behaviors, effectively functioning as semantic autoencoders. Our optimized MDiT architecture leverages this understanding to achieve a $\\ge 3\\times$ increase in convergence speed on FFHQ-256x256 and ImageNet-256x256, culminating in a $7\\times$ training speedup on ImageNet compared with state-of-the-art models. This acceleration significantly reduces the computational requirements for training, measured in FLOPs, enabling more efficient resource use and enhancing performance on  smaller datasets. Additionally, we develop a variance matching regularization technique to correct sample variance discrepancies which can occur in latent diffusion models, enhancing image contrast and vibrancy, and further accelerating convergence.", "title_embedding_index": 9565, "title_abs_embedding_index": 9590}, {"title": "Evaluating multiple models using labeled and unlabeled data", "link_suffix": "/forum?id=HvkXPQhQvv", "link": "https://openreview.net/forum?id=HvkXPQhQvv", "pdf_link": "https://openreview.net/pdf?id=HvkXPQhQvv", "keywords": "classifier evaluation, semi-supervised learning, unlabeled data", "abstract": "It remains difficult to evaluate machine learning classifiers in the absence of a large, labeled dataset. While labeled data can be prohibitively expensive or impossible to obtain, unlabeled data is plentiful.\nHere, we introduce Semi-Supervised Model Evaluation (SSME), a method that uses both labeled and unlabeled data to evaluate machine learning classifiers. SSME is the first evaluation method to take advantage of the fact that: (i) there are frequently multiple classifiers for the same task,  (ii) continuous classifier scores are often available for all classes, and (iii) unlabeled data is often far more plentiful than labeled data. \nThe key idea is to use a semi-supervised mixture model to estimate the joint distribution of ground truth labels and classifier predictions.\nWe can then use this model to estimate any metric that is a function of classifier scores and ground truth labels (e.g., accuracy or expected calibration error). \nWe present experiments in four domains where obtaining large labeled datasets is often impractical: (1) healthcare, (2) content moderation, (3) molecular property prediction, and (4) image annotation. Our results demonstrate that SSME estimates performance more accurately than do competing methods, reducing error by 5.1x relative to using labeled data alone and 2.4x relative to the next best competing method. SSME also improves accuracy when evaluating performance across subsets of the test distribution (e.g., specific demographic subgroups) and when evaluating the performance of large language models.", "title_embedding_index": 9566, "title_abs_embedding_index": 9591}, {"title": "FairCoT: Enhancing Fairness in Diffusion Models via Chain of Thought Reasoning of Multimodal Language Models", "link_suffix": "/forum?id=WGWoRZb0pT", "link": "https://openreview.net/forum?id=WGWoRZb0pT", "pdf_link": "https://openreview.net/pdf?id=WGWoRZb0pT", "keywords": "diffusion models;fairness; bias; chain of thought; text to image; multimodal LLMs", "abstract": "In the domain of text-to-image generative models, biases inherent in training datasets often propagate into generated content, posing significant ethical challenges, particularly in socially sensitive contexts. We introduce FairCoT, a novel framework that enhances fairness in diffusion models through Chain-of-Thought (CoT) reasoning within multimodal generative large language models (LLMs). FairCoT employs iterative CoT refinement and attire-based attribute prediction to systematically mitigate biases, ensuring diverse and equitable representation in generated images. By integrating iterative reasoning processes, FairCoT addresses the limitations of zero-shot CoT in sensitive scenarios, balancing creativity with ethical responsibility. Experimental evaluations across multiple models, including DALL-E and various Stable Diffusion variants, demonstrate that FairCoT significantly improves fairness and diversity metrics without compromising image quality or relevance. Our approach advances ethical AI practices in generative modeling, promoting socially responsible content generation and setting new standards for fairness in AI-generated imagery.", "title_embedding_index": 9567, "title_abs_embedding_index": 9592}, {"title": "Stutter makes large language models smarter", "link_suffix": "/forum?id=UvYrFbKj8j", "link": "https://openreview.net/forum?id=UvYrFbKj8j", "pdf_link": "https://openreview.net/pdf?id=UvYrFbKj8j", "keywords": "Self-improvement for LLM", "abstract": "Large language models (LLMs) have achieved remarkable success in generating coherent and contextually relevant text. However, their large parameters and high memory requirements limit their efficiency and adoption in industry and academia. Recent studies have shown that dynamically adjusting inference operations can improve model performance without significantly increasing size. In this paper, we introduce the stutter mechanism, a novel method that enhances transformer models by selectively applying additional layers to more challenging tokens. This approach mimics a human speaker\u2019s stutter, allocating more computational effort where needed, thus improving\nlanguage capabilities without generating excessive tokens. Our experiments with various Pythia models demonstrate that the stutter mechanism consistently enhances performance across benchmark datasets. Specifically, the Pythia-410M model, enhanced by our method, outperforms the larger Pythia-1B model on WinoGrande and WSC. Additionally, our method is data-efficient, requiring only less than 1% of the pretraining data for the additional training. These results highlight the stutter mechanism\u2019s potential to enhance LLMs\u2019 efficiency and performance in real-world applications.", "title_embedding_index": 9568, "title_abs_embedding_index": 9593}, {"title": "6D Object Pose Tracking in Internet Videos for Robotic Manipulation", "link_suffix": "/forum?id=1CIUkpoata", "link": "https://openreview.net/forum?id=1CIUkpoata", "pdf_link": "https://openreview.net/pdf?id=1CIUkpoata", "keywords": "6DoF pose estimation, robotic manipulation from video", "abstract": "We seek to extract a temporally consistent 6D pose trajectory of a manipulated  object from an Internet instructional video. This is a challenging set-up for current 6D pose estimation methods due to uncontrolled capturing conditions, fine-grained dynamic object motions, and the fact that the exact mesh of the manipulated object is not known. To address these challenges, we present the following contributions. First, we develop a new method that estimates the 6D pose of any object in the input image without prior knowledge of the object itself. The method proceeds by (i) retrieving a CAD model similar to the depicted object from a large-scale model database, (ii) 6D aligning the retrieved CAD model with the input image, and (iii) grounding the absolute scale of the object with respect to the scene. Second, we extract smooth 6D object trajectories from Internet videos by carefully tracking the detected objects across video frames. The extracted object trajectories are then retargeted via trajectory optimization into the configuration space of a robotic manipulator. Third, we thoroughly evaluate and ablate our 6D pose estimation method on YCB-V and HOPE-Video datasets and demonstrate significant improvements over existing state-of-the-art RGB 6D pose estimation methods. Finally,  we show that the 6D object motion estimated from Internet videos can be transferred to a 7-axis robotic manipulator both in a virtual simulator as well as in the real world. Additionally, we successfully apply our method to egocentric videos taken from the EPIC-KITCHENS dataset, demonstrating potential for Embodied AI applications.", "title_embedding_index": 9569, "title_abs_embedding_index": 9594}, {"title": "Alice in Wonderland: Simple Tasks Reveal Severe Generalization and Basic Reasoning Deficits in State-Of-the-Art Large Language Models", "link_suffix": "/forum?id=EJgxMsiAO9", "link": "https://openreview.net/forum?id=EJgxMsiAO9", "pdf_link": "https://openreview.net/pdf?id=EJgxMsiAO9", "keywords": "large language models, foundation models, generalization, reasoning, function testing, evaluation, benchmarks, robustness, function breakdown", "abstract": "Large Language Models (LLMs) are often described as being instances of foundation models - that is, models that possess strong generalization and therefore transfer robustly across various tasks and conditions in few-show or zero-shot manner, while exhibiting scaling laws that predict generalization improvement when increasing the pre-training scale. These claims of strong generalization and advanced reasoning function enabling it rely on measurements by various standardized benchmarks where state-of-the-art (SOTA) models score high. We demonstrate here a dramatic breakdown of generalization and basic reasoning of all SOTA models which claim strong function, including advanced models like GPT-4 or Claude 3 Opus trained at the largest scales, using a simple, short, conventional common sense problem formulated in concise natural language, easily solvable by humans (AIW problem). The breakdown is dramatic as it manifests in strong performance fluctuations on the simple problem across its mild variations that should not affect problem solving at all, while also often expressing strong overconfidence in the wrong solutions, backed up by plausible sounding explanation-like confabulations. Various standard interventions in an attempt to get the right solution, like chain-of-thought prompting, or urging the models to reconsider the wrong solutions again by multi step re-evaluation, fail. We take these observations to the scientific and technological community to stimulate re-assessment of the claimed capabilities of current generation of LLMs. Such re-assessment also requires common action to create standardized benchmarks that would allow proper detection of such deficits in generalization and reasoning that obviously remain undiscovered by current state-of-the-art evaluation procedures, where SOTA LLMs obtain high scores. Code for reproducing experiments in the paper and raw experiments data can be found athttps://anonymous.4open.science/r/AITW_anonymous-69A6/", "title_embedding_index": 9570, "title_abs_embedding_index": 9595}, {"title": "Towards Foundation Models for Mixed Integer Linear Programming", "link_suffix": "/forum?id=6yENDA7J4G", "link": "https://openreview.net/forum?id=6yENDA7J4G", "pdf_link": "https://openreview.net/pdf?id=6yENDA7J4G", "keywords": "Mixed Integer Linear Programming, Large Language Models, Foundation Models, Contrastive Learning, Graph Neural Networks", "abstract": "Mixed Integer Linear Programming (MILP) is essential for modeling complex decision-making problems but faces challenges in computational tractability and requires expert formulation. Current deep learning approaches for MILP focus on specific problem classes and do not generalize to unseen classes. To address this shortcoming, we take a foundation model training approach, where we train a single deep learning model on a diverse set of MILP problems to generalize across problem classes. As existing datasets for MILP lack diversity and volume, we introduce MILP-Evolve, a novel LLM-based evolutionary framework that is capable of generating a large set of diverse MILP classes with an unlimited amount of instances. We study our methodology on three key learning tasks that capture diverse aspects of MILP: (1) integrality gap prediction, (2) learning to branch, and (3) a new task of aligning MILP instances with natural language descriptions. Our empirical results show that models trained on the data generated by MILP-Evolve achieve significant improvements on unseen problems, including MIPLIB benchmarks. Our work highlights the potential of moving towards a foundation model approach for MILP that can generalize to a broad range of MILP applications. We are committed to fully open-sourcing our work to advance further research.", "title_embedding_index": 9571, "title_abs_embedding_index": 9596}, {"title": "A Generic Framework for Conformal Fairness", "link_suffix": "/forum?id=xiQNfYl33p", "link": "https://openreview.net/forum?id=xiQNfYl33p", "pdf_link": "https://openreview.net/pdf?id=xiQNfYl33p", "keywords": "Fairness, Conformal Prediction, Graph Neural Networks", "abstract": "Conformal Prediction (CP) is a popular method for uncertainty quantification with machine learning models. While the method provides probabilistic guarantees regarding the coverage of the true label, these guarantees are agnostic to the presence of sensitive attributes within the dataset. In this work, we formalize \\textit{Conformal Fairness}, a notion of fairness using conformal predictors, and provide a theoretically well-founded algorithm and associated framework to control for the gaps in coverage between different sensitive groups. Our framework leverages the exchangeability assumption (implicit to CP) rather than the typical IID assumption, allowing us to apply the notion of Conformal Fairness to data types and tasks that are not IID, such as graph data. Experiments were conducted on graph and tabular datasets to demonstrate that the algorithm can control fairness-related gaps in addition to coverage aligned with theoretical expectations.", "title_embedding_index": 9572, "title_abs_embedding_index": 9597}, {"title": "MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases", "link_suffix": "/forum?id=EEbRrNsiiD", "link": "https://openreview.net/forum?id=EEbRrNsiiD", "pdf_link": "https://openreview.net/pdf?id=EEbRrNsiiD", "keywords": "Large Language Model, Mobile, Benchmarking", "abstract": "The deployment of Large Language Models (LLMs) and Large Multimodal Models (LMMs) on mobile devices has gained significant attention due to the benefits of enhanced privacy, stability, and personalization. However, the hardware constraints of mobile devices necessitate the use of models with fewer parameters and model compression techniques like quantization. Currently, there is limited understanding of quantization's impact on various task performances, including LLM tasks, LMM tasks, and, critically, trust and safety. There is a lack of adequate tools for systematically testing these models on mobile devices. To address these gaps, we introduce MobileAIBench, a comprehensive benchmarking framework for evaluating mobile-optimized LLMs and LMMs. MobileAIBench assesses models across different sizes, quantization levels, and tasks, measuring latency and resource consumption on real devices. Our two-part open-source framework includes a library for running evaluations on desktops and a mobile app for on-device latency and hardware utilization measurements. Our thorough analysis aims to accelerate mobile AI research and deployment by providing insights into the performance and feasibility of deploying LLMs and LMMs on mobile platforms.", "title_embedding_index": 9573, "title_abs_embedding_index": 9598}, {"title": "Concept Bottleneck Large Language Models", "link_suffix": "/forum?id=RC5FPYVQaH", "link": "https://openreview.net/forum?id=RC5FPYVQaH", "pdf_link": "https://openreview.net/pdf?id=RC5FPYVQaH", "keywords": "LLMs, Interpretability, Concept Bottleneck Model", "abstract": "We introduce the Concept Bottleneck Large Language Model (CB-LLM), a pioneering approach to creating inherently interpretable Large Language Models (LLMs). Unlike traditional black-box LLMs that rely on post-hoc interpretation methods with limited neuron function insights, CB-LLM sets a new standard with its built-in interpretability, scalability, and ability to provide clear, accurate explanations. We investigate two essential tasks in the NLP domain: text classification and text generation. In text classification, CB-LLM narrows the performance gap with traditional black-box models and provides clear interpretability. In text generation, we show how interpretable neurons in CB-LLM can be used for concept detection and steering text generation. Our CB-LLMs enable greater interaction between humans and LLMs across a variety of tasks --- a feature notably absent in existing LLMs.", "title_embedding_index": 9574, "title_abs_embedding_index": 9599}]