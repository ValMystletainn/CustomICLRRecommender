[{"title": "Swift4D: Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene", "link_suffix": "/forum?id=c1RhJVTPwT", "link": "https://openreview.net/forum?id=c1RhJVTPwT", "pdf_link": "https://openreview.net/pdf?id=c1RhJVTPwT", "keywords": "3D Gaussian Splatting, Dynamic scene reconstruction.", "abstract": "Novel view synthesis has long been a practical but challenging task, although the introduction of numerous methods to solve this problem, even combining advanced representations like 3D Gaussian Splatting, they still struggle to recover high-quality results and often consume too much storage memory and training time. \nIn this paper we propose Swift4D, a divide-and-conquer 3D Gaussian Splatting method that can handle static and dynamic primitives separately, achieving a good trade-off between rendering quality and efficiency, motivated by the fact that most of the scene is the static primitive and does not require additional dynamic properties. Concretely, we focus on modeling dynamic transformations only for the dynamic primitives which benefits both efficiency and quality. We first employ a learnable decomposition strategy to separate the primitives, which relies on an additional parameter to classify primitives as static or dynamic. For the dynamic primitives, we employ a compact multi-resolution 4D Hash mapper to transform these primitives from canonical space into deformation space at each timestamp, and then mix the static and dynamic primitives to produce the final output. This divide-and-conquer method facilitates efficient training and reduces storage redundancy. Our method not only achieves state-of-the-art rendering quality while being 20\u00d7 faster in training than previous SOTA methods with a minimum storage requirement of only 30MB on real-world datasets.", "title_embedding_index": 16650, "title_abs_embedding_index": 16675}, {"title": "Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives", "link_suffix": "/forum?id=ePJrZLIqpV", "link": "https://openreview.net/forum?id=ePJrZLIqpV", "pdf_link": "https://openreview.net/pdf?id=ePJrZLIqpV", "keywords": "audio-visual learning; adversarial attack", "abstract": "While audio-visual learning equips models with a richer understanding of the real world by leveraging multiple sensory modalities, this integration also introduces new vulnerabilities to adversarial attacks.\n In this paper, we present a comprehensive study of the adversarial robustness of audio-visual models, considering both temporal and modality-specific vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal invariance attack that exploits the inherent temporal redundancy across consecutive time segments and 2) a modality misalignment attack that introduces incongruence between the audio and visual modalities. These attacks are designed to thoroughly assess the robustness of audio-visual models against diverse threats. Furthermore, to defend against such attacks, we introduce a novel audio-visual adversarial training framework. This framework addresses key challenges in vanilla adversarial training by incorporating efficient adversarial perturbation crafting tailored to multi-modal data and an adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds dataset demonstrate that our proposed temporal and modality-based attacks in degrading model performance can achieve state-of-the-art performance, while our adversarial training defense largely improves the adversarial robustness as well as the adversarial training efficiency.", "title_embedding_index": 16651, "title_abs_embedding_index": 16676}, {"title": "FairSkin: Fair Diffusion for Skin Disease Image Geneartion", "link_suffix": "/forum?id=qW5f8TAZ4J", "link": "https://openreview.net/forum?id=qW5f8TAZ4J", "pdf_link": "https://openreview.net/pdf?id=qW5f8TAZ4J", "keywords": "fairness, diffusion model, medical image generation", "abstract": "Image generation is a prevailing technique for clinical data augmentation for advancing diagnostic accuracy and reducing healthcare disparities. Diffusion Model (DM) has become a leading method in generating synthetic medical images, but it suffers from a critical twofold bias: (1)The quality of images generated for Caucasian individuals is significantly higher, as measured by the Fr\u00e9chet Inception Distance (FID). (2)The ability of the downstream-task learner to learn critical features from disease images varies across different skin tones. These biases pose significant risks, particularly in skin disease detection, where underrepresentation of certain skin tones can lead to misdiagnosis or neglect of specific conditions. To address these challenges, we propose FairSkin, a novel DM framework that mitigates these biases through a three-level resampling mechanism, ensuring fairer representation across racial and disease categories. Our approach significantly improves the diversity and quality of generated images, contributing to more equitable skin disease detection in clinical settings.", "title_embedding_index": 16652, "title_abs_embedding_index": 16677}, {"title": "Exploring the Camera bias of Person Re-identification", "link_suffix": "/forum?id=SgymXhOEA5", "link": "https://openreview.net/forum?id=SgymXhOEA5", "pdf_link": "https://openreview.net/pdf?id=SgymXhOEA5", "keywords": "Person re-identification, Camera bias, Debiasing, Unsupervised learning", "abstract": "We empirically investigate the camera bias of person re-identification (ReID) models. Previously, camera-aware methods have been proposed to address this issue, but they are largely confined to training domains of the models. We measure the camera bias of ReID models on unseen domains and reveal that camera bias becomes more pronounced under data distribution shifts. As a debiasing method for unseen domain data, we revisit feature normalization on embedding vectors. While the normalization has been used as a straightforward solution, its underlying causes and broader applicability remain unexplored. We analyze why this simple method is effective at reducing bias and show that it can be applied to detailed bias factors such as low-level image properties and body angle. Furthermore, we validate its generalizability across various models and benchmarks, highlighting its potential as a simple yet effective test-time postprocessing method for ReID. In addition, we explore the inherent risk of camera bias in unsupervised learning of ReID models. The unsupervised models remain highly biased towards camera labels even for seen domain data, indicating substantial room for improvement. Based on observations of the negative impact of camera-biased pseudo labels on training, we suggest simple training strategies to mitigate the bias. By applying these strategies to existing unsupervised learning algorithms, we show that significant performance improvements can be achieved with minor modifications.", "title_embedding_index": 16653, "title_abs_embedding_index": 16678}, {"title": "Large Convolutional Model Tuning via Filter Subspace", "link_suffix": "/forum?id=E5YmIBvOqV", "link": "https://openreview.net/forum?id=E5YmIBvOqV", "pdf_link": "https://openreview.net/pdf?id=E5YmIBvOqV", "keywords": "Efficient Fine-tuning, Filter Decomposition, Filter Subspace", "abstract": "Efficient fine-tuning methods are critical to address the high computational and parameter complexity while adapting large pre-trained models to downstream tasks.\nOur study is inspired by prior research that represents each convolution filter as a linear combination of a small set of filter subspace elements, referred to as filter atoms. In this paper, we propose to fine-tune pre-trained models by adjusting only filter atoms, which are responsible for spatial-only convolution, while preserving spatially-invariant channel combination knowledge in atom coefficients.\nIn this way, we bring a new filter subspace view for model tuning. \nFurthermore, each filter atom can be recursively decomposed as a combination of another set of atoms, which naturally expands the number of tunable parameters in the filter subspace.\nBy only adapting filter atoms constructed by a small number of parameters, while maintaining the rest of model parameters constant, the proposed approach is highly parameter-efficient. It effectively preserves the capabilities of pre-trained models and prevents overfitting to downstream tasks. \nExtensive experiments show that such a simple scheme surpasses previous tuning baselines for both discriminate and generative tasks.", "title_embedding_index": 16654, "title_abs_embedding_index": 16679}, {"title": "A Large-scale Dataset with Behavior, Attributes, and Content of Mobile Short-video Platform", "link_suffix": "/forum?id=T4VK4U4aKb", "link": "https://openreview.net/forum?id=T4VK4U4aKb", "pdf_link": "https://openreview.net/pdf?id=T4VK4U4aKb", "keywords": "Large-scale dataset, Behavior, Attributes, Video content, Mobile short-video platform", "abstract": "Short-video platforms show an increasing impact on people\u2019s daily life nowadays, with billions of active users spending plenty of time each day. The interactions between users and online platforms give rise to many scientific problems across computational social science and artificial intelligence. However, despite the rapid development of short-video platforms, currently there are serious shortcomings in existing relevant datasets on three aspects: inadequate user-video feedback, limited user attributes and lack of video content. To address these problems, we provide a large-scale dataset with rich user behavior, attributes and video content from a real mobile short-video platform. This dataset covers 10,000 voluntary users and 153,561 videos, and we conduct three-fold technical validations of the dataset. First, we verify the richness of the behavior data including interaction frequency and feedback distribution. Second, we validate the wide coverage of user-side and video-side attribute data. Third, we confirm the representing ability of the content features. We believe the dataset could support the broad research community, including user modeling, social science, human behavior understanding, etc. Our dataset is available at this anonymous link:http://101.6.70.16:8080/.", "title_embedding_index": 16655, "title_abs_embedding_index": 16680}, {"title": "DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers", "link_suffix": "/forum?id=Z3xg3hxdky", "link": "https://openreview.net/forum?id=Z3xg3hxdky", "pdf_link": "https://openreview.net/pdf?id=Z3xg3hxdky", "keywords": "Sequence Parallel, Multi-Dimensional Transformers, Distributed Computing, High Performance Computing", "abstract": "Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.", "title_embedding_index": 16656, "title_abs_embedding_index": 16681}, {"title": "PAS: Plug-and-Play Prompt Augmentation System", "link_suffix": "/forum?id=VTG68CNUCf", "link": "https://openreview.net/forum?id=VTG68CNUCf", "pdf_link": "https://openreview.net/pdf?id=VTG68CNUCf", "keywords": "Auto Prompt Engineering, Plug-and-Play System, Data Curation", "abstract": "In recent years, the rise of Large Language Models (LLMs) has spurred a growing demand for plug-and-play AI systems. Among the various AI techniques, prompt engineering stands out as particularly significant. However, users often face challenges in writing prompts due to the steep learning curve and significant time investment, and existing automatic prompt engineering (APE) models can be difficult to use. To address this issue, we propose PAS, an LLM-based plug-and-play APE system.\nPAS utilizes LLMs trained on high-quality, automatically generated prompt complementary datasets, resulting in exceptional performance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA) results compared to previous APE models, with an average improvement of 6.09 points. Moreover, PAS is highly efficient, achieving SoTA performance with only 9000 data points. Additionally, PAS can autonomously generate prompt augmentation data without requiring additional human labor. Its flexibility also allows it to be compatible with all existing LLMs and applicable to a wide range of tasks.\nPAS excels in human evaluations, underscoring its suitability as a plug-in for users. This combination of high performance, efficiency, and flexibility makes PAS a valuable system for enhancing the usability and effectiveness of LLMs through automatic prompt engineering.", "title_embedding_index": 16657, "title_abs_embedding_index": 16682}, {"title": "TVNet: A Novel Time Series Analysis Method Based on Dynamic Convolution and 3D-Variation", "link_suffix": "/forum?id=MZDdTzN6Cy", "link": "https://openreview.net/forum?id=MZDdTzN6Cy", "pdf_link": "https://openreview.net/pdf?id=MZDdTzN6Cy", "keywords": "Time series Analysis, Dynamic convolution, Deep Learning", "abstract": "With the recent development and advancement of Transformer and MLP architectures, significant strides have been made in time series analysis. Conversely, the performance of Convolutional Neural Networks (CNNs) in time series analysis has fallen short of expectations, diminishing their potential for future applications. Our research aims to enhance the representational capacity of Convolutional Neural Networks (CNNs) in time series analysis by introducing novel perspectives and design innovations. To be specific, We introduce a novel time series reshaping technique that considers the inter-patch, intra-patch, and cross-variable dimensions. Consequently, we propose TVNet, a dynamic convolutional network leveraging a 3D perspective to employ time series analysis. TVNet retains the computational efficiency of CNNs and achieves state-of-the-art results in five key time series analysis tasks, offering a superior balance of efficiency and performance over the state-of-the-art Transformer-based and MLP-based models. Additionally, our findings suggest that TVNet exhibits enhanced transferability and robustness. Therefore, it provides a new perspective for applying CNN in advanced time series analysis tasks.", "title_embedding_index": 16658, "title_abs_embedding_index": 16683}, {"title": "Taming Gradient Oversmoothing and Expansion in Graph Neural Networks", "link_suffix": "/forum?id=EFhzmn3RJG", "link": "https://openreview.net/forum?id=EFhzmn3RJG", "pdf_link": "https://openreview.net/pdf?id=EFhzmn3RJG", "keywords": "graph neural network, deep neural network, oversmoothing, optimization", "abstract": "Oversmoothing has been claimed as a primary bottleneck for multi-layered graph neural networks (GNNs). Multiple analyses have examined how and why oversmoothing occurs. However, none of the prior work addressed how optimization is performed under the oversmoothing regime. In this work, we show the presence of $\\textit{gradient oversmoothing}$ preventing optimization during training. We further analyze that GNNs with residual connections, a well-known solution to help gradient flow in deep architecture, introduce $\\textit{gradient expansion}$, a phenomenon of the gradient explosion in diverse directions. Therefore, adding residual connections cannot be a solution for making a GNN deep. Our analysis reveals that constraining the Lipschitz bound of each layer can neutralize the gradient expansion. To this end, we provide a simple yet effective normalization method to prevent the gradient expansion. An empirical study shows that the residual GNNs with hundreds of layers can be efficiently trained with the proposed normalization without compromising performance. Additional studies show that the empirical observations corroborate our theoretical analysis.", "title_embedding_index": 16659, "title_abs_embedding_index": 16684}, {"title": "Decoupling the Class Label and the Target Concept in Machine Unlearning", "link_suffix": "/forum?id=OHOmpkGiYK", "link": "https://openreview.net/forum?id=OHOmpkGiYK", "pdf_link": "https://openreview.net/pdf?id=OHOmpkGiYK", "keywords": "Machine Unlearning, Label Domain Mismatch", "abstract": "Machine unlearning as an emerging research topic for data regulations, aims to adjust a trained model to approximate a retrained one that excludes a portion of training data. Previous studies showed that class-wise unlearning is effective in forgetting the knowledge of target data, either through gradient ascent on the forgetting data or fine-tuning with the remaining data. However, while these methods are useful, they are insufficient as the class label and the target concept are often considered to coincide. In this work, we expand the scope by considering the label domain mismatch and investigate three problems beyond the conventionalall matchedforgetting, e.g.,target mismatch,model mismatch, anddata mismatchforgetting. We systematically analyze the new challenges in restrictively forgetting the target concept and also reveal crucial forgetting dynamics in the representation level to realize these tasks. Based on that, we propose a general framework, namely,TARget-aware Forgetting(TARF). It enables the additional tasks to actively forget the target concept while maintaining the rest part, by simultaneously conducting annealed gradient ascent on the forgetting data and selected gradient descent on the hard-to-affect remaining data. Empirically, various experiments under our newly introduced settings are conducted to demonstrate the effectiveness of our TARF.", "title_embedding_index": 16660, "title_abs_embedding_index": 16685}, {"title": "MRFNet: Multi-Receptive Field  Network for Multivariate Time-Series Prediction", "link_suffix": "/forum?id=KEqerdxOBG", "link": "https://openreview.net/forum?id=KEqerdxOBG", "pdf_link": "https://openreview.net/pdf?id=KEqerdxOBG", "keywords": "time series forecasting", "abstract": "Time series forecasting is a critical topic in machine learning. Although existing deep learning methods have demonstrated outstanding performance and currently dominate this field, the latest state-of-the-art (SOTA) models are increasingly encountering the same limitations: the blockneck of performance. We believe this convergence is due to these models being based on the same mathematical foundations. To address this issue, we draw inspiration from the universal approximation theorem (UAT) and show that most commonly used deep learning models for time series forecasting are specific implementations of UAT. Based on UAT theory and the characteristics of time series data, we propose a new forecasting model called the Multi-Receptive Field Network (MRFNet). This architecture integrates linear, sparse matrix, convolutional, and Fourier transform modules, resulting in an interpretable model with multiple receptive fields that can capture both global and local information. The MRFNet model has been tested extensively on several popular time series forecasting datasets and has achieved superior results.", "title_embedding_index": 16661, "title_abs_embedding_index": 16686}, {"title": "Offline Multi-agent Reinforcement Learning with Sequential Score Decomposition", "link_suffix": "/forum?id=mc97L2QVIa", "link": "https://openreview.net/forum?id=mc97L2QVIa", "pdf_link": "https://openreview.net/pdf?id=mc97L2QVIa", "keywords": "Multi-agent Reinforcement Learning, Offline RL, Diffusion Models", "abstract": "Offline multi-agent reinforcement learning (MARL) faces significant challenges due to distribution shift issues, exacerbated by the high dimensionality of joint actions and complex joint behavior policy distributions. \nWhile existing methods often focus on independent learning or offline value decomposition with conservative value estimation, they may still lead to out-of-distribution (OOD) joint actions and reduced performance. \nThis is primarily due to the lack of exploration opportunity and implicit policy dependencies in offline settings. \nTo address these challenges, we propose an offline policy decomposition method incorporating joint policy regularization constraints. \nOur approach utilizes a diffusion generative model to capture the joint behavior policy, followed by a decomposition of the extracted score function. \nThis decomposition is then used to regularize individual policies in a decentralized manner. \nExperimental results demonstrate that our method achieves SOTA on continuous control tasks in standard offline MARL benchmarks.", "title_embedding_index": 16662, "title_abs_embedding_index": 16687}, {"title": "Articulate-Anything:  Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model", "link_suffix": "/forum?id=s3FTX4Ay55", "link": "https://openreview.net/forum?id=s3FTX4Ay55", "pdf_link": "https://openreview.net/pdf?id=s3FTX4Ay55", "keywords": "Computer vision, Vision-Language Models, Robotics", "abstract": "Interactive 3D simulated objects are crucial in AR/VR, animations, and robotics, driving immersive experiences and advanced automation.\nHowever, creating these articulated objects requires extensive human effort and expertise, limiting their broader applications. To overcome this challenge, we present Articulate-Anything, a system that automates the articulation of diverse, complex objects from many input modalities, including text, images, and videos.\nArticulate-Anything leverages vision-language models (VLMs) to generate code that can be compiled into an interactable digital twin for use in standard 3D simulators. Our system exploits existing 3D asset datasets via a mesh retrieval mechanism, along with an actor-critic system that iteratively proposes, evaluates, and refines solutions for articulating the objects, self-correcting errors to achieve a robust out- come. Qualitative evaluations demonstrate Articulate-Anything's capability to articulate complex and even ambiguous object affordances by leveraging rich grounded inputs. In extensive quantitative experiments on the standard PartNet-Mobility dataset, Articulate-Anything substantially outperforms prior work, increasing the success rate from 8.7-11.6% to 75% and setting a new bar for state-of-art performance.  We further showcase the utility of our generated assets by using them to train robotic policies for fine-grained manipulation tasks that go beyond basic pick and place.", "title_embedding_index": 16663, "title_abs_embedding_index": 16688}, {"title": "Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging", "link_suffix": "/forum?id=9I6UOIfbwf", "link": "https://openreview.net/forum?id=9I6UOIfbwf", "pdf_link": "https://openreview.net/pdf?id=9I6UOIfbwf", "keywords": "Face Editing, Face Re-Aging, Video Editing", "abstract": "Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory performance in age transformation. To tackle the issues, we propose (1) a novel synthetic video dataset that features subjects across a diverse range of age groups; (2) a baseline architecture designed to validate the effectiveness of our proposed dataset, and (3) the development of novel metrics tailored explicitly for evaluating the temporal consistency of video re-aging techniques. Our comprehensive experiments on public datasets, including VFHQ and CelebV-HQ, show that our method outperforms existing approaches in age transformation accuracy and temporal consistency. Notably, in user studies, our method was preferred for temporal consistency by 48.1% of participants for the older direction and by 39.3% for the younger direction.", "title_embedding_index": 16664, "title_abs_embedding_index": 16689}, {"title": "Investigating Pattern Neurons in Urban Time Series Forecasting", "link_suffix": "/forum?id=a9vey6B54y", "link": "https://openreview.net/forum?id=a9vey6B54y", "pdf_link": "https://openreview.net/pdf?id=a9vey6B54y", "keywords": "urban time series forecasting, neuron detection", "abstract": "Urban time series forecasting is crucial for smart city development and is key to sustainable urban management. Although urban time series models (UTSMs) are effective in general forecasting, they often overlook low-frequency events, such as emergencies and holidays, leading to degraded performance in practical applications. In this paper, we first investigate how UTSMs handle these infrequent patterns from a neural perspective. Based on our findings, we propose $\\textbf{P}$attern $\\textbf{N}$euron guided $\\textbf{Train}$ing ($\\texttt{PN-Train}$), a novel training method that features (i) a $\\textit{perturbation-based detector}$ to identify neurons responsible for low-frequency patterns in UTSMs, and (ii) a $\\textit{fine-tuning mechanism}$ that enhances these neurons without compromising representation learning on high-frequency patterns. Empirical results demonstrate that $\\texttt{PN-Train}$ considerably improves forecasting accuracy for low-frequency events while maintaining high performance for high-frequency events.", "title_embedding_index": 16665, "title_abs_embedding_index": 16690}, {"title": "Preference-Enhanced Instruction Tuning for Machine Translation", "link_suffix": "/forum?id=BfNylgbDuy", "link": "https://openreview.net/forum?id=BfNylgbDuy", "pdf_link": "https://openreview.net/pdf?id=BfNylgbDuy", "keywords": "machine translation, preference alignment, large language model", "abstract": "Although Large Language Models (LLMs) like GPT-4 perform excellently in machine translation, their high costs and scalability make them unavailable in many scenarios. Recently, there has been increased effort to build smaller LLMs that can achieve comparable performance. However, while typical instruction tuning methods tend to directly mimic reference translations, leading to less meaningful results, recent preference optimization methods have shown improvements. Despite this, they still fail to effectively utilize crucial preference information during inference.  In this paper, we introduce Preference-Enhanced Instruction Tuning (PEIT), a novel method that explicitly incorporates preferences into both the instruction fine-tuning and the inference phase. Our extensive experiments show that PEIT not only improves translation quality but also significantly outperforms state-of-the-art preference optimization methods and instruction tuning baselines on multiple language benchmarks.", "title_embedding_index": 16666, "title_abs_embedding_index": 16691}, {"title": "Towards Better Multi-head Attention via Channel-wise Sample Permutation", "link_suffix": "/forum?id=TXeqjGYxu4", "link": "https://openreview.net/forum?id=TXeqjGYxu4", "pdf_link": "https://openreview.net/pdf?id=TXeqjGYxu4", "keywords": "Efficient Transformers, sparse doubly stochastic attention, permutation, optimal transport", "abstract": "Transformer plays a central role in many fundamental deep learning models, e.g., the ViT in computer vision and the BERT and GPT in natural language processing, whose effectiveness is mainly attributed to its multi-head attention (MHA) mechanism. \nIn this study, we propose a simple and novel channel-wise sample permutation (CSP) operator, achieving a new structured MHA with fewer parameters and lower complexity. \nGiven an input matrix, CSP circularly shifts the samples of different channels with various steps and then sorts grouped samples of each channel. \nThis operator is equivalent to implicitly implementing cross-channel attention maps as permutation matrices, which achieves linear complexity and suppresses the risk of rank collapse when representing data. \nWe replace the MHA of some representative models with CSP and test the CSP-based models in several discriminative tasks, including image classification and long sequence analysis. \nExperiments show that the CSP-based models achieve comparable or better performance with fewer parameters and lower computational costs than the classic Transformer and its state-of-the-art variants. \nThe code is available athttps://anonymous.4open.science/r/CSP-BA52.", "title_embedding_index": 16667, "title_abs_embedding_index": 16692}, {"title": "AdaptVis: Spatial Understanding in Vision-Language Models Requires Adaptive Attention", "link_suffix": "/forum?id=wFAyp2CUnq", "link": "https://openreview.net/forum?id=wFAyp2CUnq", "pdf_link": "https://openreview.net/pdf?id=wFAyp2CUnq", "keywords": "Vision Language Models, Uncertainty, Mechanistic interpretability, Constrain Decoding, Spatial Understanding", "abstract": "Vision Large Language Models (VLLMs) often struggle with adequately attending to image information, leading to significant hallucinations across various domains, especially on spatial reasoning tasks. In this study, we analyze the attention behavior of VLLMs in spatial reasoning Question-Answering (QA) tasks from a mechanism interpretability view. By visualizing the crucial areas of an image that receive the highest attention scores in the intermediate layers, we identify an interesting pattern: failures often correspond to attention being misallocated to irrelevant objects within the image. Moreover, the attention patterns exhibit large differences between familiar and unfamiliar spatial relationships. Motivated by this observation, we further explore the feasibility of adaptively adjusting the attention scores during the inference process based on the confidence score. Our experiments on spatial reasoning benchmarks including WhatsUp and VSR demonstrate that our decoding methods yield promising results, e.g., achieving up to a 50-point improvement on the WhatsUp benchmark with negligible additional computation cost.", "title_embedding_index": 16668, "title_abs_embedding_index": 16693}, {"title": "Learning to Teach: Improving Mean Teacher in Semi-supervised Medical Image Segmentation with Dynamic Decay  Modulation", "link_suffix": "/forum?id=92FZfA99dP", "link": "https://openreview.net/forum?id=92FZfA99dP", "pdf_link": "https://openreview.net/pdf?id=92FZfA99dP", "keywords": "Meta learning, Medical image segmentation, semi-supervised learning", "abstract": "Medical image segmentation is essential in medical diagnostics but is hindered by the scarcity of labeled three-dimensional imaging data, which requires costly expert annotations. Semi-supervised learning (SSL) addresses this limitation by utilizing large amounts of unlabeled data alongside limited labeled samples. The Mean Teacher model, a prominent SSL method, enhances performance by employing an Exponential Moving Average (EMA) of the student model to form a teacher model, where the EMA decay coefficient is critical. However, using a fixed coefficient fails to adapt to the evolving training dynamics, potentially restricting the model's effectiveness. In this paper,\nwe propose Meta MeanTeacher, a novel framework that integrates meta-learning to dynamically adjust the EMA decay coefficient during training. We approach proposed Dynamic Decay Modulation (DDM) module in our Meta MeanTeacher framework, which captures the representational capacities of both student and teacher models. DDM heuristically learns the optimal EMA decay coefficient by taking the losses of the student and teacher networks as inputs and updating it through pseudo-gradient descent on a meta-objective. This dynamic adjustment allows the teacher model to more effectively guide the student as training progresses.\nExperiments on two datasets with different modalities, i.e., CT and MRI, show that Meta MeanTeacher consistently outperforms traditional Mean Teacher methods with fixed EMA coefficients. Furthermore, integrating Meta MeanTeacher into state-of-the-art frameworks like UA-MT, AD-MT, and PMT leads to significant performance enhancements, achieving new state-of-the-art results in semi-supervised medical image segmentation.", "title_embedding_index": 16669, "title_abs_embedding_index": 16694}, {"title": "Tight Clusters Make Specialized Experts", "link_suffix": "/forum?id=Pu3c0209cx", "link": "https://openreview.net/forum?id=Pu3c0209cx", "pdf_link": "https://openreview.net/pdf?id=Pu3c0209cx", "keywords": "Mixture of Experts, robustness, clustering", "abstract": "Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising approach to decoupling model capacity from computational cost. At the core of the MoE model is the router, which learns the underlying clustering structure of the input distribution in order to send input tokens to appropriate experts. However, latent clusters may be unidentifiable in high dimension, which causes slow convergence, susceptibility to data contamination, and overall degraded representations as the router is unable to perform appropriate token-expert matching. We examine the router through the lens of clustering optimization and derive optimal feature weights that maximally identify the latent clusters. We use these weights to compute the token-expert routing assignments in an adaptively transformed space that promotes well-separated clusters, which helps identify the best-matched expert for each token. In particular, for each expert cluster, we compute a set of weights that scales features according to whether that expert clusters tightly along that feature. We term this novel router the Adaptive Clustering (AC) router. Our AC router enables the MoE model to obtain three connected benefits: 1) faster convergence, 2) better robustness to data corruption, and 3) overall performance improvement, as experts are specialized in semantically distinct regions of the input space. We empirically demonstrate the advantages of our AC router over baseline routing methods when applied on a variety of MoE backbones for large-scale language modeling and object recognition tasks in both clean and corrupted settings.", "title_embedding_index": 16670, "title_abs_embedding_index": 16695}, {"title": "CURVALID: A Geometrically-guided Adversarial Prompt Detection", "link_suffix": "/forum?id=v1qNr99R5n", "link": "https://openreview.net/forum?id=v1qNr99R5n", "pdf_link": "https://openreview.net/pdf?id=v1qNr99R5n", "keywords": "Large language models, Adversarial attacks, Local Intrinsic Dimension, Curvature", "abstract": "Adversarial prompts that can jailbreak large language models (LLMs) and lead to undesirable behaviours pose a significant challenge to the safe deployment of LLMs. Existing defenses, such as input perturbation and adversarial training, depend on activating LLMs' defense mechanisms or fine-tuning LLMs individually, resulting in inconsistent performance across different prompts and LLMs. To address this, we propose CurvaLID, an algorithm that classifies benign and adversarial prompts by leveraging two complementary geometric measures: Local Intrinsic Dimensionality (LID) and curvature. LID provides an analysis of geometric differences at the prompt level, while curvature captures the degree of curvature in the manifolds and the semantic shifts at the word level. Together, these tools capture both prompt-level and word-level geometric properties, enhancing adversarial prompt detection. We demonstrate the limitations of using token-level LID, as applied in previous work, for capturing the geometric properties of text prompts. To address this, we propose PromptLID to calculate LID in prompt-level representations to explore the adversarial local subspace for detection. Additionally, we propose TextCurv to further analyze the local geometric structure of prompt manifolds by calculating the curvature in text prompts. CurvaLID achieves over 0.99 detection accuracy, effectively reducing the attack success rate of advanced adversarial prompts to zero or nearly zero. Importantly, CurvaLID provides a unified detection framework across different adversarial prompts and LLMs, as it achieves consistent performance regardless of the specific LLM targeted.", "title_embedding_index": 16671, "title_abs_embedding_index": 16696}, {"title": "InfiniteAudio: Infinite-Length Audio Generation with Consistent Acoustic Attributes", "link_suffix": "/forum?id=Hp6f6VKAeP", "link": "https://openreview.net/forum?id=Hp6f6VKAeP", "pdf_link": "https://openreview.net/pdf?id=Hp6f6VKAeP", "keywords": "Text-to-audio generation, diffusion models, lengthy generation, consistent generation", "abstract": "This work aims to generate long-duration audio while preserving acoustic coherence, utilizing existing text-conditional audio generation models through diffusion-based approaches. Current diffusion models, however, encounter significant challenges in generating long audio sequences due to memory constraints, as output size scales with input length. While one possible solution is to concatenate short clips, this often leads to inconsistencies due to a lack of shared temporal information across segments.\nTo address these challenges, we propose InfiniteAudio, a novel inference technique designed to generate long audio with consistent acoustic attributes. Our method is based on three key components. First, we implement a curved denoising approach with a fixed-size input, enabling theoretically infinite audio generation while maintaining a constant memory footprint. Second, we introduce conditional guidance alternation, a mechanism that enhances intelligibility in long speech generation. Finally, initial self-attention features are shared across future frames to maintain temporal coherence.\nThe effectiveness of InfiniteAudio is demonstrated through comprehensive comparisons with existing text-to-audio generation baselines. Generated audio samples are available on our anonymous project page\\footnote{https://anonymousforcf.github.io/InfiniteAudio/}.", "title_embedding_index": 16672, "title_abs_embedding_index": 16697}, {"title": "MISR: Measuring Instrumental Self-Reasoning in Frontier Models", "link_suffix": "/forum?id=MOEBghZGVq", "link": "https://openreview.net/forum?id=MOEBghZGVq", "pdf_link": "https://openreview.net/pdf?id=MOEBghZGVq", "keywords": "Self-Reasoning, Agents, AI Safety, Evaluations, Alignment", "abstract": "We propose a suite of tasks to evaluate the instrumental self-reasoning ability of large language model (LLM) agents. Instrumental self-reasoning ability could improve adaptability and enable self-modification, but it could also pose significant risks, such as enabling deceptive alignment. Prior work has only evaluated self-reasoning in non-agentic settings or in limited domains. In this paper, we propose evaluations for instrumental self-reasoning ability in agentic tasks in a wide range of scenarios, including self-modification, knowledge seeking, and opaque self-reasoning. We evaluate agents built using state-of-the-art LLMs, including commercial and open source systems. We find that instrumental self-reasoning ability emerges only in the most capable frontier models and that it is highly context-dependent. No model passes the the most difficult versions of our evaluations, hence our evaluation can be used to measure increases in instrumental self-reasoning ability in future models.", "title_embedding_index": 16673, "title_abs_embedding_index": 16698}, {"title": "How language models extrapolate outside the training data: A Case study in Textualized Gridworld", "link_suffix": "/forum?id=CfdPELywGN", "link": "https://openreview.net/forum?id=CfdPELywGN", "pdf_link": "https://openreview.net/pdf?id=CfdPELywGN", "keywords": "Cognitive map, NeuroAI, language model, language agent, planning", "abstract": "Language models\u2019 ability to extrapolate learned behaviors to novel, more complex environments beyond their training scope is highly unknown. This study introduces a path planning task in a textualized Gridworld to probe language models\u2019 extrapolation capabilities. We show that conventional approaches, including next-token prediction and Chain of Thought (CoT) fine-tuning, fail to generalize in larger, unseen environments. Inspired by human cognition and dual-process theory, we propose language models should construct cognitive maps before interaction. Our research demonstrates that autoregressive generation of cognitive maps and planning sequences enhances planning capabilities in extrapolated environments. Unlike CoT, we find that cognitive maps cannot be obtained through simple prompting, necessitating additional training schemes for integration. Our findings in Gridworld offer insights into training language models with improved reasoning and adaptability, potentially advancing more human-like cognition and opening avenues for enhancing model generalization across diverse, complex tasks.", "title_embedding_index": 16674, "title_abs_embedding_index": 16699}]