[{"title": "Understanding Skill Adaptation in Transformers Using Sparse Autoencoders: Chess as a Model System", "link_suffix": "/forum?id=Wxl0JMgDoU", "link": "https://openreview.net/forum?id=Wxl0JMgDoU", "pdf_link": "https://openreview.net/pdf?id=Wxl0JMgDoU", "keywords": "Skill Adaptation, Chess, Sparse Autoencoders, Mechanistic Interpretability", "abstract": "Understanding how skill shapes decision-making in complex environments is a challenging problem in AI interpretability. We investigate this question by applying Sparse Autoencoders (SAEs) to the internal representations of Maia-2, a human-like chess model that simulates human play across varying skill levels. Maia-2 incorporates a skill-aware transformer that integrates position features with categorical skill inputs, capturing nuanced relationships between player expertise and move selection. By training SAEs on these modulated representations, we identify latent features that reveal how the model's threat response policy adapts to different levels of play. We then use these features to intervene on the internal activations of Maia-2, eliciting both higher skill and lower skill play in specific contexts. We also apply mediated intervention with targeted SAE features to effectively enhance and sabotage the model's understanding and decision-making on context-specific chess tasks. Our findings suggest that SAE features can help shed light on how skill-specific information is encoded within a model to produce human-like behavior, and that these insights can be applied to steer the model's performance on specific sub-tasks. Our work is available at \\url{https://anonymous.4open.science/r/chess-sae-3C06/}", "title_embedding_index": 1000, "title_abs_embedding_index": 1025}, {"title": "VisionTS: Visual Masked Autoencoders Are Free-Lunch Zero-Shot Time Series Forecasters", "link_suffix": "/forum?id=IEs29RYxfK", "link": "https://openreview.net/forum?id=IEs29RYxfK", "pdf_link": "https://openreview.net/pdf?id=IEs29RYxfK", "keywords": "time series forecasting, foundation models, computer vision", "abstract": "Foundation models have emerged as a promising approach in time series forecasting (TSF). Existing approaches either repurpose large language models (LLMs) or build large-scale time series datasets to develop TSF foundation models for universal forecasting. However, these methods face challenges due to the severe cross-domain gap or in-domain heterogeneity. This paper explores a new road to building a TSF foundation model from rich and high-quality natural images. Our key insight is that a visual masked autoencoder, pre-trained on the ImageNet dataset, can naturally be a numeric series forecaster. By reformulating TSF as an image reconstruction task, we bridge the gap between image pre-training and TSF downstream tasks. Surprisingly, without further adaptation in the time-series domain, the proposed VisionTS could achieve superior zero-shot forecasting performance compared to existing TSF foundation models. With fine-tuning for one epoch, VisionTS could further improve the forecasting and achieve state-of-the-art performance in most cases.  Extensive experiments reveal intrinsic similarities between images and real-world time series, suggesting visual models may offer a \"free lunch'' for TSF and highlight the potential for future cross-modality research. Our code is available in the Supplementary Material.", "title_embedding_index": 1001, "title_abs_embedding_index": 1026}, {"title": "Retrieval-Augmented Editing Generation: Impact of Knowledge Editing and Fine-Tuning on RAG", "link_suffix": "/forum?id=R2OzZWOkjz", "link": "https://openreview.net/forum?id=R2OzZWOkjz", "pdf_link": "https://openreview.net/pdf?id=R2OzZWOkjz", "keywords": "Retrieval-Augmented Generation, Knowledge Editing, Parameter-Efficient Fine-Tuning", "abstract": "The knowledge embedded in Large Language Models (LLMs) is static, tied to the time when the training data was collected. \nWhile Retrieval-Augmented Generation (RAG) methods are widely used to introduce new knowledge, they simply rely on retrieved information for reasoning without integrating it into the model\u2019s parameters. This limits the model's ability for long-term knowledge retention and autonomous learning.\nTo overcome this, in this work, we propose the \\textbf{R}etrieval-\\textbf{A}ugmented \\textbf{E}diting \\textbf{G}eneration (RAEG) framework for open-domain question answering (ODQA) tasks. \nRAEG enhances model generation performance by first editing the retrieved paragraphs to inject necessary knowledge, followed by an augmented generation phase. This dual mechanism\u2014combining knowledge injection and retrieval augmentation\u2014provides complementary advantages in the reasoning process. When the injected knowledge alone is insufficient for accurate generation, the model can rely on the retrieved information to compensate, and conversely, when retrieval yields suboptimal results, the injected knowledge ensures continuity and accuracy in the response. This interplay between internalized and externally sourced knowledge reinforces the model's ability to produce correct answers, thereby enhancing overall task performance.\nWe explore the impact of two key methods for knowledge injection: Knowledge Editing (KE) and Parameter-Efficient Fine-Tuning (PEFT), and analyze how modifying the model's parameters influences its reasoning abilities and generation outcomes. To further improve RAEG's performance, we introduce a re-ranking mechanism to optimize the integration of external knowledge and apply parameter pruning to mitigate the potential drawbacks of parameter modifications during KE.\nEvaluations on two authoritative ODQA benchmarks show that RAEG is able to further replace RAG as a competitive method.\nOur data and code will be available at \\url{https://github.com/XXX/XXX}.", "title_embedding_index": 1002, "title_abs_embedding_index": 1027}, {"title": "Inverse Prompt Engineering for Task-Specific LLM Safety", "link_suffix": "/forum?id=3MDmM0rMPQ", "link": "https://openreview.net/forum?id=3MDmM0rMPQ", "pdf_link": "https://openreview.net/pdf?id=3MDmM0rMPQ", "keywords": "guardrails, safety, robustness, alignment", "abstract": "Most real-world deployments of large language models (LLMs) operate within well-scoped tasks, yet current safety measures are general-purpose and fail to leverage this information. As a result, even in narrowly-scoped tasks, LLM applications remain vulnerable to adversarial jailbreaks. In these settings, we argue that task-specific safety guardrails solve a more tractable problem than general-purpose methods. We introduce Inverse Prompt Engineering (IPE) as an initial approach to building automatic, task-specific safety guardrails around LLMs. Our key insight is that robust safety guardrails can be derived from prompt engineering data that is already on hand. IPE operationalizes the principle of least privilege from computer security, restricting LLM functionality to only what is necessary for the task. We evaluate our approach in two settings. First, in an example chatbot application, where IPE outperforms existing methods against both human-written and automated adversarial attacks. Second, on TensorTrust, a crowdsourced dataset of prompt-based attacks and defenses. Here, IPE improves average defense robustness by 93%, using real-world prompt engineering data.", "title_embedding_index": 1003, "title_abs_embedding_index": 1028}, {"title": "LISA: UNLEASHING 2D DIFFUSION FOR 3D GENERATION VIA LIGHTWEIGHT IMAGE SPLATS ADAPTATION", "link_suffix": "/forum?id=PLgHiJOjcH", "link": "https://openreview.net/forum?id=PLgHiJOjcH", "pdf_link": "https://openreview.net/pdf?id=PLgHiJOjcH", "keywords": "3D asset creation, lightweight adapter, Gaussian Splats", "abstract": "Despite its potential, 3D generation lags behind 2D generation in quality and utility, primarily due to the vast gap in the scale and diversity of training data\u2014high-quality 2D data is abundant, while high-quality 3D assets remain limited by orders of magnitude. Existing methods use 2D generative priors for 3D asset creation via distillation or generate-and-reconstruct schemes, both of which suffer from quality loss during optimization. In this paper, we propose a novel scheme to exploit 2D diffusion prior for 3d generation by integrating a lightweight adapter into the decoder of a frozen 2D diffusion model, allowing it to generate RGB images, Gaussian splats, and physics-based rendering material maps simultaneously. Once trained, the proposed Lightweight Image Splats Adaptation (LISA) directly produces relightable Gaussian splats in feed-forward manner, which can be converted into high-quality, relightable 3D meshes through an inverse rendering framework. Quantitative and qualitative results demonstrate that our method outperforms state-of-the-art approaches with a significantly lower computational budget for both training and sampling. More results can be found athttps://LISA-3dgen.github.io.", "title_embedding_index": 1004, "title_abs_embedding_index": 1029}, {"title": "Explanation using Simulation", "link_suffix": "/forum?id=iL9A4e8RdS", "link": "https://openreview.net/forum?id=iL9A4e8RdS", "pdf_link": "https://openreview.net/pdf?id=iL9A4e8RdS", "keywords": "Explainable AI, Reinforcement learning, Dynamical systems", "abstract": "In safety-critical domains, such as industrial systems, the lack of explainability in predictive `black-box' machine learning models can hinder trust and adoption. Standard explainability techniques, while powerful, often require deep expertise in data analytics and machine learning and fail to align with the sequential, dynamic nature of data in these environments. In this paper, we propose a novel explainability framework that leverages reinforcement learning (RL) to support model predictions with visual explanations based on dynamical system simulation. By training RL agents to simulate events that require prediction, we use these agents' critics to make classifications. Next, we employ the actors of the RL agents to simulate the potential future trajectories underlying these classifications, providing visual explanations that are more intuitive and align with the expertise of industrial domain experts. We demonstrate the applicability of this method through a case study involving monitoring a small industrial system for cyberattacks, showing how our framework generates actionable predictions that are supported with visual explanations. This approach aims to bridge the gap between advanced machine learning models and their real-world deployment in safety-critical environments.", "title_embedding_index": 1005, "title_abs_embedding_index": 1030}, {"title": "PHICO: Personalised Human-AI Cooperative Classification Using Augmented Noisy Labels and Model Prediction", "link_suffix": "/forum?id=gtCXzVeQxz", "link": "https://openreview.net/forum?id=gtCXzVeQxz", "pdf_link": "https://openreview.net/pdf?id=gtCXzVeQxz", "keywords": "Human-Ai Cooperation", "abstract": "The nuanced differences in human behavior and the complex dynamics of human-AI interactions pose significant challenges in optimizing human-AI cooperation. Existing approaches tend to oversimplify the problem and rely on a single global behavior model, which overlooks individual variability, leading to sub-optimal solutions. To bridge this gap, we introduce PHICO, a novel framework for human-AI cooperative classification that initially identifies a set of representative annotator profiles characterized by unique noisy label patterns. These patterns are then augmented to train personalised AI cooperative models, each tailored to an annotator profile. When these models are paired with human inputs that exhibit similar noise patterns from a corresponding profile, they consistently achieve a joint classification accuracy that exceeds those achieved by either AI or humans alone. We theoretically prove the convergence of PHICO, ensuring the reliability of the framework. To evaluate PHICO, we introduce novel measures for assessing human-AI cooperative classification and empirically demonstrate its generalisability and performance across diverse datasets including CIFAR-10N, CIFAR-10H, Fashion-MNIST-H, AgNews, and Chaoyang histopathology. PHICO is both a model-agnostic and effective solution for improving human-AI cooperation.", "title_embedding_index": 1006, "title_abs_embedding_index": 1031}, {"title": "Learning to Imitate with Less: Efficient Individual Behavior Modeling in Chess", "link_suffix": "/forum?id=bwhLqFjsxd", "link": "https://openreview.net/forum?id=bwhLqFjsxd", "pdf_link": "https://openreview.net/pdf?id=bwhLqFjsxd", "keywords": "Human Behavior Modeling, Chess, Data-Efficient Learning, Action Prediction, Meta Learning", "abstract": "As humans seek to collaborate with, learn from, and better understand artificial intelligence systems, developing AI agents that can accurately emulate individual decision-making becomes increasingly important. Chess, with its long-standing role as a benchmark for AI research and its precise measurement of skill through chess ratings, provides an ideal environment for studying human-AI alignment. However, existing approaches to modeling human behavior require large amounts of data from each individual, making them impractical for new or sparsely represented users. In this work, we introduce Maia4All, a model designed to learn and adapt to individual decision-making styles efficiently, even with limited data. Maia4All achieves this by leveraging a two-stage fine-tuning method to bridge population and individual-level models and uses a meta-network to initialize and refine these embeddings with minimal data. Our experimental results show that Maia4All can accurately predict individual moves and profile behavioral patterns with high fidelity, establishing a new standard for personalized human-like AI behavior modeling in chess. Our work provides an example of how population AI systems can flexibly adapt to individual users using a prototype model as a bridge, which could lead to better and more accessible human-AI collaboration in other fields like education, healthcare, and strategic decision-making.", "title_embedding_index": 1007, "title_abs_embedding_index": 1032}, {"title": "BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic Inheritance in Large Language Models", "link_suffix": "/forum?id=d465apqCqc", "link": "https://openreview.net/forum?id=d465apqCqc", "pdf_link": "https://openreview.net/pdf?id=d465apqCqc", "keywords": "supervised fine-tuning, parameter efficient fine-tuning, bias reduction", "abstract": "Large language models (LLMs) have demonstrated remarkable proficiency across various natural language processing (NLP) tasks. However, adapting LLMs to downstream applications requires computationally intensive and memory-demanding fine-tuning procedures. To alleviate these burdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a promising approach to tailor LLMs with minimal computational overhead. While PEFT methods offer substantial advantages, they do not fully address the pervasive issue of bias propagation from pre-training data. This work introduces Bias-Alleviating Low-Rank Adaptation (BA-LoRA), a novel PEFT method designed to counteract bias inheritance. BA-LoRA incorporates three distinct regularization terms: (1) a consistency regularizer, (2) a diversity regularizer, and (3) a singular value decomposition regularizer. These regularizers aim to enhance the models' consistency, diversity, and generalization capabilities during fine-tuning. We conduct extensive experiments on natural language understanding (NLU) and natural language generation (NLG) tasks using prominent LLMs such as LLaMA, Mistral, and Gemma. The results demonstrate that BA-LoRA outperforms LoRA and its state-of-the-art variants. Moreover, our method effectively mitigates the adverse effects of pre-training bias, leading to more reliable and robust model outputs.", "title_embedding_index": 1008, "title_abs_embedding_index": 1033}, {"title": "Evaluating Representational Similarity Measures from the Lens of Functional Correspondence", "link_suffix": "/forum?id=WyZT4ZmMzf", "link": "https://openreview.net/forum?id=WyZT4ZmMzf", "pdf_link": "https://openreview.net/pdf?id=WyZT4ZmMzf", "keywords": "Representational Similarity, Vision, Deep Neural Networks, Behavior", "abstract": "Neuroscience and artificial intelligence (AI) both face the challenge of interpreting high-dimensional neural data, where the comparative analysis of such data is crucial for revealing shared mechanisms and differences between these complex systems. Despite the widespread use of representational comparisons and the abundance classes of comparison methods, a critical question remains: which metrics are most suitable for these comparisons? While some studies evaluate metrics based on their ability to differentiate models of different origins or constructions (e.g., various architectures), another approach is to assess how well they distinguish models that exhibit distinct behaviors. To investigate this, we examine the degree of alignment between various representational similarity measures and behavioral outcomes, employing group statistics and a comprehensive suite of behavioral metrics for comparison. In our evaluation of eight commonly used representational similarity metrics in the visual domain\u2014spanning alignment-based, CCA-based, inner product kernel-based, and nearest-neighbor methods\u2014we found that metrics like linear CKA and Procrustes, which emphasize the overall geometric structure or shape of representations, excelled in differentiating trained from untrained models and aligning with behavioral measures, whereas metrics such as linear predictivity, commonly used in neuroscience, demonstrated only moderate alignment with behavior. These insights are crucial for selecting metrics that emphasize behaviorally meaningful comparisons in NeuroAI research.", "title_embedding_index": 1009, "title_abs_embedding_index": 1034}, {"title": "Efficient Policy Evaluation with Safety Constraint for Reinforcement Learning", "link_suffix": "/forum?id=Dem5LyVk8R", "link": "https://openreview.net/forum?id=Dem5LyVk8R", "pdf_link": "https://openreview.net/pdf?id=Dem5LyVk8R", "keywords": "Reinforcement Learning", "abstract": "In reinforcement learning, classic on-policy evaluation methods often suffer from high variance and require massive online data to attain the desired accuracy. Previous studies attempt to reduce evaluation variance by searching for or designing proper behavior policies to collect data. However, these approaches ignore the safety of such behavior policies---the designed behavior policies have no safety guarantee and may lead to severe damage during online executions. In this paper, to address the challenge of reducing variance while ensuring safety simultaneously, we propose an optimal variance-minimizing behavior policy under safety constraints. Theoretically, while ensuring safety constraints, our evaluation method is unbiased and has lower variance than on-policy evaluation. Empirically, our method is the only existing method to achieve both substantial variance reduction and safety constraint satisfaction. Furthermore, we show our method is even superior to previous methods in both variance reduction and execution safety.", "title_embedding_index": 1010, "title_abs_embedding_index": 1035}, {"title": "1-Bit FQT: Pushing the Limit of Fully Quantized Training to 1-bit", "link_suffix": "/forum?id=oWy06SBgt4", "link": "https://openreview.net/forum?id=oWy06SBgt4", "pdf_link": "https://openreview.net/pdf?id=oWy06SBgt4", "keywords": "efficient machine learning, quantization methods, efficient training algorithms, fully quantized training", "abstract": "Fully quantized training (FQT) accelerates the training of deep neural networks by quantizing the activations, weights, and gradients into lower precision. To explore the ultimate limit of FQT (the lowest achievable precision), we make a first attempt to 1-bit FQT. We provide a theoretical analysis of FQT based on Adam and SGD, revealing that the gradient variance influences the convergence of FQT. Building on these theoretical results, we introduce an Average 1-bit Quantization (AQ) strategy. The strategy leverages the heterogeneity of gradients to mitigate gradient variance by pruning less informative gradients and enhancing the numerical precision of remaining gradients. Additionally, we propose Sample Channel joint Quantization (SCQ), which utilizes different quantization strategies in the computation of weight gradients and activation gradients to ensure that the method is friendly to low-bitwidth hardware. Finally, we present a framework to deploy our algorithm. For fine-tuning VGGNet-16 and ResNet-18 on multiple datasets, our algorithm achieves an average accuracy improvement of approximately 6%, compared to per-sample quantization. Moreover, our training speedup can reach a maximum of 5.13\u00d7 compared to full precision training.", "title_embedding_index": 1011, "title_abs_embedding_index": 1036}, {"title": "Agent S: An Open Agentic Framework that Uses Computers Like a Human", "link_suffix": "/forum?id=lIVRgt4nLv", "link": "https://openreview.net/forum?id=lIVRgt4nLv", "pdf_link": "https://openreview.net/pdf?id=lIVRgt4nLv", "keywords": "Large Vision and Language Model, Agents, Retrieval Augmented Generation, GUI, Large Language Models, Agent Computer Interface", "abstract": "We present Agent S, an open agentic framework that enables autonomous interaction with computers through Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S addresses three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. \nIn addition, it employs an Agent-Computer Interface (ACI) to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models (MLLMs). Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark. Code will be made publicly available.", "title_embedding_index": 1012, "title_abs_embedding_index": 1037}, {"title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs", "link_suffix": "/forum?id=kO0DgO07hW", "link": "https://openreview.net/forum?id=kO0DgO07hW", "pdf_link": "https://openreview.net/pdf?id=kO0DgO07hW", "keywords": "Large Language Model, Safe Large Language Model, Earth Mover Distance, Supervised Fine-tuning", "abstract": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is a significant issue in their applications. While various efforts aim to address this safety concern, previous approaches often demand substantial human data collection or rely on the less dependable option of using another LLM to generate corrective data. In this paper, we aim to take this problem and overcome limitations of requiring significant high-quality human data. Our method requires only a small set of unsafe responses to toxic prompts, easily obtained from the unsafe LLM itself. By employing a semantic cost combined with a negative Earth Mover Distance (EMD) loss, we guide the LLM away from generating unsafe responses. Additionally, we propose a novel lower bound for EMD loss, enabling more efficient optimization. Our results demonstrate superior performance and data efficiency compared to baselines, and we further examine the nuanced effects of over-alignment and potential degradation of language capabilities when using contrastive data.", "title_embedding_index": 1013, "title_abs_embedding_index": 1038}, {"title": "Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence", "link_suffix": "/forum?id=o1Et3MogPw", "link": "https://openreview.net/forum?id=o1Et3MogPw", "pdf_link": "https://openreview.net/pdf?id=o1Et3MogPw", "keywords": "llm agent, multi-agent", "abstract": "The rapid advancement of large language models (LLMs) has paved the way for the development of highly capable autonomous agents. However, existing multi-agent frameworks often struggle with integrating diverse capable third-party agents due to reliance on agents defined within their own ecosystems. They also face challenges in simulating distributed environments, as most frameworks are limited to single-device setups. Furthermore, these frameworks often rely on hard-coded communication pipelines, limiting their adaptability to dynamic task requirements. Inspired by the concept of the Internet, we propose the Internet of Agents (IoA), a novel framework that addresses these limitations by providing a flexible and scalable platform for LLM-based multi-agent collaboration. IoA introduces an agent integration protocol, an instant-messaging-like architecture design, and dynamic mechanisms for agent teaming and conversation flow control. Through extensive experiments on general assistant tasks, embodied AI tasks, and retrieval-augmented generation benchmarks, we demonstrate that IoA consistently outperforms state-of-the-art baselines, showcasing its ability to facilitate effective collaboration among heterogeneous agents. IoA represents a step towards linking diverse agents in an Internet-like environment, where agents can seamlessly collaborate to achieve greater intelligence and capabilities. We will release our code to facilitate further research.", "title_embedding_index": 1014, "title_abs_embedding_index": 1039}, {"title": "Adversarial Generative Flow Network for Solving Vehicle Routing Problems", "link_suffix": "/forum?id=tBom4xOW1H", "link": "https://openreview.net/forum?id=tBom4xOW1H", "pdf_link": "https://openreview.net/pdf?id=tBom4xOW1H", "keywords": "Generative Flow Network, Adversarial Training, Vehicle Routing Problem", "abstract": "Recent research into solving vehicle routing problems (VRPs) has gained significant traction, particularly through the application of deep (reinforcement) learning for end-to-end solution construction. However, many current construction-based neural solvers predominantly utilize Transformer architectures, which can face scalability challenges and struggle to produce diverse solutions. To address these limitations, we introduce a novel framework beyond Transformer-based approaches, i.e., Adversarial Generative Flow Networks (AGFN). This framework integrates the generative flow network (GFlowNet)\u2014a probabilistic model inherently adept at generating diverse solutions (routes)\u2014with a complementary model for discriminating (or evaluating) the solutions. These models are trained alternately in an adversarial manner to improve the overall solution quality, followed by a proposed hybrid decoding method to construct the solution. We apply the AGFN framework to solve the capacitated vehicle routing problem (CVRP) and travelling salesman problem (TSP), and our experimental results demonstrate that AGFN surpasses the popular construction-based neural solvers, showcasing strong generalization capabilities on synthetic and real-world benchmark instances.", "title_embedding_index": 1015, "title_abs_embedding_index": 1040}, {"title": "DASH: Data-Efficient Learned Cost Models for Sparse Matrix Computations on Emerging Hardware Platforms", "link_suffix": "/forum?id=YeSwPnI4be", "link": "https://openreview.net/forum?id=YeSwPnI4be", "pdf_link": "https://openreview.net/pdf?id=YeSwPnI4be", "keywords": "learned cost models, sparse accelerators, transfer learning, ml for systems", "abstract": "Sparse matrix computations are becoming increasingly significant in deep learning and graph analytics, driving the development of specialized hardware systems known as accelerators to meet the growing need for optimized performance. Optimizing these computations, however, presents significant challenges due to their sensitivity to variations in input sparsity patterns and code optimizations. While ML-based cost models and search techniques have shown promise in optimizing sparse matrix computations in general-purpose hardware like CPUs, these cost models require large datasets for effective training. Collecting such extensive datasets is particularly impractical for emerging hardware platforms that only have access to expensive simulators in the early design stages. To overcome this, we propose DASH, which trains learned cost models using low-cost data samples from widely accessible general-purpose hardware (such as CPUs), followed by few-shot fine-tuning to efficiently adapt to emerging hardware platforms. DASH introduces a novel approach that leverages the homogeneity of input features across different hardware platforms while effectively mitigating heterogeneity. This enables DASH to achieve comparable accuracy using only 5% of the data samples required by a cost model trained exclusively using data samples from an accelerator. We evaluate DASH on two critical sparse operations\u2014SpMM and SDDMM\u2014on an emerging sparse accelerator using 715 distinct sparsity patterns. Our experimental results show that DASH outperforms existing techniques that use transfer learning by 28.44%, achieving average speedups of 1.47x (up to 5.46x) for SpMM and 1.39x (up to 4.22x) for SDDMM.", "title_embedding_index": 1016, "title_abs_embedding_index": 1041}, {"title": "Staple: Towards Reliable Problem Solving with Large Language Models via Plan Optimization and Tree Search", "link_suffix": "/forum?id=P8FS9byr1c", "link": "https://openreview.net/forum?id=P8FS9byr1c", "pdf_link": "https://openreview.net/pdf?id=P8FS9byr1c", "keywords": "Large language models; Prompt engineering; Complex reasoning;", "abstract": "Large language models (LLMs) exhibit the ability to perform step-by-step reasoning when tackling complex problems across various tasks. To improve the reliability of multi-step reasoning and mitigate potential hallucinations, sophisticated prompting techniques have been developed to provide instructions on $what$ $to$ $do$ at each step, offering reasoning guidance before addressing specific questions. However, this additional prompting can increase time and token consumption without guaranteeing effectiveness. In response, this paper proposes $Staple$, a novel plan retrieval augmented reasoning framework that utilizes offline plan optimization. This approach involves constructing a plan database of general-purpose reasoning instructions. Subsequently, online plan searching facilitates the direct retrieval of optimal and effective step-by-step plans from the database when addressing new questions, serving as guidance for LLMs to derive correct answers. The offline stage uses LLMs to self-generate and optimize plans, storing them as tree structures via Monte Carlo Tree Search (MCTS) to form the plan database. Extensive experiments on mathematical and multi-task problems show that $Staple$ achieves competitive problem-solving rates while minimizing token usage and interactions. Importantly, the plan trees in the database are human-interpretable, revealing the prioritization of various plan combinations for a given task. In addition, the plan database can be reused, updated, and expanded by users for a wider range of applications.", "title_embedding_index": 1017, "title_abs_embedding_index": 1042}, {"title": "Improving the Efficiency of Test-Time Search in LLMs with Backtracking", "link_suffix": "/forum?id=hJ2BCYGvFg", "link": "https://openreview.net/forum?id=hJ2BCYGvFg", "pdf_link": "https://openreview.net/pdf?id=hJ2BCYGvFg", "keywords": "LLMs, Reasoning, Test Time Inference, Backtracking, In-Context Verifiers", "abstract": "Solving reasoning problems is an iterative multi-step computation, where a reasoning agent progresses through a sequence of steps, with each step logically building upon the previous one to reach a desired conclusion. If the desired solution is not attained, the agent must backtrack and try reasoning chains that are quite different from previous attempts. Though prior work such as test-time search against an outcome verifier can improve performance, most search is done in parallel via Best-of-N reranking, and independently for each attempt at a problem, thus wasting a significant amount of computation in sampling multiple full solutions even beyond the point that is needed. Can we reduce the total amount of computation by sharing information and computation across multiple attempts to a given problem? In this paper, we build a novel approach combining process verifiers that predict likelihoods of success \\emph{per step} with preemptive backtracking to maximize performance per generated token. To do this, the PRM can be used to identify where a problematic step in a solution trace is by using the sensitivity of the predictions of the learned verifier and allowing the model to do focused resampling of the problematic portion of a solution. This approach can significantly reduce the amount of computation by leveraging partial computation from previous revisions. To further enhance the computational efficiency of inference, we introduce in-context process supervision, where the verifier is conditioned on the history of revisions that are attempted, reducing uncertainty in the verification decisions and improving the verifier's confidence with each round of backtracking. This framework for iterative backtracking, leveraging in-context process supervision, enables an effective tradeoff between inference and model performance.", "title_embedding_index": 1018, "title_abs_embedding_index": 1043}, {"title": "CoMMIT: Coordinated Instruction Tuning for Multimodal Large Language Models", "link_suffix": "/forum?id=6ADnEk90R2", "link": "https://openreview.net/forum?id=6ADnEk90R2", "pdf_link": "https://openreview.net/pdf?id=6ADnEk90R2", "keywords": "multimodal large language model, instruction tuning", "abstract": "Instruction tuning in multimodal large language models (MLLMs) generally involves smooth integration of a backbone LLM and a feature encoder that has non-text input modalities. The major challenge is how to efficiently find the synergy through cooperative learning, so that LLMs can adapt their reasoning abilities in downstream tasks while feature encoders can adjust to provide more relevant modality-specific information. In this paper, we analyze the MLLM instruction tuning from both theoretical and empirical perspectives, where we find unbalanced learning between the two modules, i.e., the feature encoder and the LLM, can cause problems of oscillation learning and insufficient training with diminishing learning gradients. Inspired by our findings, we propose a Multimodal Balance Coefficient that enable quantitative measurement on the learning balance. Based on this, we further design a dynamic learning scheduler that better coordinates the learning between the LLM and feature encoder, alleviating the oscillation and insufficient training. In addition, we introduce an auxiliary regularization on the gradient to promote updating with larger step sizes, which potentially prevents enables a more accurate estimation of the learning balance coefficient and further improves the training sufficiency. Our techniques are agnostic to the architecture of LLM and feature encoder, so can be generically integrated with various MLLM. Experiment results on multiple downstream tasks and modalities in vision and audio, demonstrate the proposed method\u2019s better efficiency and effectiveness in MLLM instruction tuning.", "title_embedding_index": 1019, "title_abs_embedding_index": 1044}, {"title": "Preference-Driven Spatial-Temporal Counting Process Models", "link_suffix": "/forum?id=0a7TRHhhcS", "link": "https://openreview.net/forum?id=0a7TRHhhcS", "pdf_link": "https://openreview.net/pdf?id=0a7TRHhhcS", "keywords": "choice model, spatial-temporal counting process model", "abstract": "Traditional spatial-temporal models often overlook the complex decision-making processes and social factors that shape spatial-temporal event data generated by humans. This paper introduces a novel framework that integrates choice theory with social intelligence to model and analyze counting processes, such as crime occurrences or bike-sharing activity, where the observed discrete events result from individual decisions influenced by social dynamics. \nOur approach aims to uncover latent human preference patterns, represented by utility functions, to capture the diverse decision-making factors within a population that result in the observed event counts. These latent factors help explain how choices\u2014such as where and when to commit a crime\u2014are shaped by personal preferences, environmental conditions, and social influences. By modeling the aggregate outcomes of these individual choices, we can better understand and predict patterns in counting processes. The proposed model adopts a preference-driven approach to counting data, providing interpretable insights at a detailed level. It also enables in-depth analysis of how external interventions, like law enforcement actions or policy changes, influence individual decisions and how these effects spread through the system. Empirical evaluation of crime and bike-sharing datasets demonstrates our model's ability to offer clear insights and achieve high predictive accuracy.", "title_embedding_index": 1020, "title_abs_embedding_index": 1045}, {"title": "Drift Type and Magnitude Detection in Image Classification Neural Networks", "link_suffix": "/forum?id=W6fIyuK8Lk", "link": "https://openreview.net/forum?id=W6fIyuK8Lk", "pdf_link": "https://openreview.net/pdf?id=W6fIyuK8Lk", "keywords": "Artificial Intelligence, Machine Learning, Image Processing, Classification Neural Networks, Drift Data Detection, Image Noise Level Estimation", "abstract": "A change in the input data stream of a machine-learning model is referred to as a data drift and may impact the model\u2019s accuracy. This paper proposes a framework to detect data drifts, identify the type of drift, and estimate the drift magnitude that occur in the input data stream of image classification neural networks due to various effects. It applies to any type of drift that occurs in images due to various factors such as noise, weather, etc. A novel statistical method is proposed for drift magnitude estimation. The method relies on the change in the prediction probability distributions of the predicted classes in the classification network caused by the data drift. The drift magnitude is estimated by applying a set of thresholds to the prediction probabilities. The drift type is identified using a classification neural network. Experimental results obtained using various datasets, drift types, and neural network architectures show that the proposed framework can accurately detect data drifts, accurately identify the drift type, and estimate the drift magnitude with a very low quantization error.", "title_embedding_index": 1021, "title_abs_embedding_index": 1046}, {"title": "Brain-Like Replay Naturally Emerges in Reinforcement Learning Agents", "link_suffix": "/forum?id=hKcDOfDxgn", "link": "https://openreview.net/forum?id=hKcDOfDxgn", "pdf_link": "https://openreview.net/pdf?id=hKcDOfDxgn", "keywords": "reinforcement learning, replay, neuroscience", "abstract": "Replay is a powerful strategy to promote learning in artificial intelligence and the brain. However, the conditions to generate it and its functional advantages have not been fully recognized. In this study, we develop a modular reinforcement learning model that could generate replay. We prove that replay generated in this way helps complete the task. We also analyze the information contained in the representation and provide a mechanism for how replay makes a difference. Our design avoids complex assumptions and enables replay to emerge naturally within a task-optimized paradigm. Our model also reproduces key phenomena observed in biological agents. This research explores the structural biases in modular ANN to generate replay and its potential utility in developing efficient RL.", "title_embedding_index": 1022, "title_abs_embedding_index": 1047}, {"title": "Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Attacks", "link_suffix": "/forum?id=K7xpl3LZQp", "link": "https://openreview.net/forum?id=K7xpl3LZQp", "pdf_link": "https://openreview.net/pdf?id=K7xpl3LZQp", "keywords": "Copyright Tracking, Large Vision-Language Models, Adversarial Attacks, Fine-tuning", "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable image understanding and dialogue capabilities, allowing them to handle a variety of visual question answering tasks. However, their widespread availability raises concerns about unauthorized usage and copyright infringement, where users or individuals can develop their own LVLMs by fine-tuning published models. In this paper, we propose a novel method called Parameter Learning Attack (PLA) for tracking the copyright of LVLMs without modifying the original model. Specifically, we construct adversarial images through targeted attacks against the original model, enabling it to generate specific outputs. To ensure these attacks remain effective on potential fine-tuned models to trigger copyright tracking, we allow the original model to learn the trigger images by updating parameters in the opposite direction during the adversarial attack process. Notably, the proposed method can be applied after the release of the original model, thus not affecting the model\u2019s performance and behavior. To simulate real-world applications, we fine-tune the original model using various strategies across diverse datasets, creating a range of models for copyright verification. Extensive experiments demonstrate that our method can more effectively identify the original copyright of fine-tuned models compared to baseline methods. Therefore, this work provides a powerful tool for tracking copyrights and detecting unlicensed usage of LVLMs.", "title_embedding_index": 1023, "title_abs_embedding_index": 1048}, {"title": "FB-Bench: A Fine-Grained Multi-Task Benchmark for Evaluating LLMs' Responsiveness to Human Feedback", "link_suffix": "/forum?id=P2BgxNCFs9", "link": "https://openreview.net/forum?id=P2BgxNCFs9", "pdf_link": "https://openreview.net/pdf?id=P2BgxNCFs9", "keywords": "benchmark;human feedback;LLMs", "abstract": "Human feedback is crucial in the interactions between humans and Large Language Models (LLMs). However, existing research primarily focuses on benchmarking LLMs in single-turn dialogues. Even in benchmarks designed for multi-turn dialogues, the user inputs are often independent, neglecting the nuanced and complex nature of human feedback within real-world usage scenarios. To fill this research gap, we introduce FB-Bench, a fine-grained, multi-task benchmark designed to evaluate LLMs' responsiveness to human feedback in real-world usage scenarios. Drawing from the two main interaction scenarios, FB-Bench comprises 734 meticulously curated samples, encompassing eight task types, five deficiency types of response, and nine feedback types. We extensively evaluate a broad array of popular LLMs, revealing significant variations in their performance across different interaction scenarios. Further analysis indicates that task, human feedback, and deficiencies of previous responses can also significantly impact LLMs' responsiveness. Our findings underscore both the strengths and limitations of current models, providing valuable insights and directions for future research. Both the toolkits and the dataset of FB-Bench will be released soon.", "title_embedding_index": 1024, "title_abs_embedding_index": 1049}]