[
    {
        "title": "Beyond Transformations: Augmenting Anything for Image Super-Resolution via Diffusion Model",
        "link_suffix": "/forum?id=JmGEZXkCH3",
        "link": "https://openreview.net/forum?id=JmGEZXkCH3",
        "pdf_link": "https://openreview.net/pdf?id=JmGEZXkCH3",
        "keywords": "data augmentation, data degradation, super-resolution",
        "abstract": "Image super-resolution (SR), aiming to restore accurate high-resolution images from low-resolution ones, plays a pivotal role in image processing. However, the performance of SR models is often hindered by conventional data augmentation and data degradation techniques. Conventional data augmentation methods for SR are typically limited to geometric transformations, lacking semantic richness. Traditional data degradation methods simulate degradation through a series of blurring, noise addition, compression, and resizing processes, lacking the complexity essential for robust model training. In this paper, based on pre-trained large-scale text-to-image diffusion models, we propose a novel data augmentation method and an innovative data degradation method in SR modeling. Our data augmentation method utilizes Stable Diffusion to modify image content at the semantic level for controlled data augmentation, enriching training datasets with nuanced variations while preserving the quality of the original images. Moreover, after fine-tuning Stable Diffusion with domain-matched data we further enhance the augmentation efficacy. Besides, by carefully designing control signals, our data degradation method utilizes diffusion to emulate degradation, simulating various unknown input corruptions to improve the performance of SR models across unfamiliar image degradation patterns. Our data augmentation method improves PSNR by 0.8 dB on the FFHQ dataset and by 0.28 dB on the Manga109 dataset for the SR tasks. Meanwhile, our data degradation technique has proven effective in significantly reducing artifacts in real-world SR imagery, distinctly exceeding the performance of traditional ones."
    },
    {
        "title": "Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks",
        "link_suffix": "/forum?id=uKZdlihDDn",
        "link": "https://openreview.net/forum?id=uKZdlihDDn",
        "pdf_link": "https://openreview.net/pdf?id=uKZdlihDDn",
        "keywords": "Graph Neural Networks, Diffusion Models, Physics Simulations",
        "abstract": "Physical systems with complex unsteady dynamics, such as fluid flows, are often poorly represented by a single mean solution. For many practical applications, it is crucial to access the full distribution of possible states, from which relevant statistics (e.g., RMS and two-point correlations) can be derived. Here, we propose a graph-based latent diffusion model that enables direct sampling of states from their equilibrium distribution, given a mesh discretization of the system and its physical parameters. This allows for the efficient computation of flow statistics without running long and expensive numerical simulations. The graph-based structure enables operations on unstructured meshes, which is critical for representing complex geometries with spatially localized high gradients, while latent-space diffusion modeling with a multi-scale GNN allows for efficient learning and inference of entire distributions of solutions. A key finding of our work is that the proposed networks can accurately learn full distributions even when trained on incomplete data from relatively short simulations. We apply this method to a range of fluid dynamics tasks, such as predicting pressure distributions on 3D wing models in turbulent flow, demonstrating both accuracy and computational efficiency in challenging scenarios. The ability to directly sample accurate solutions, and capturing their diversity from short ground-truth simulations, is highly promising for complex scientific modeling tasks."
    },
    {
        "title": "GLOMA: Global Video Text Spotting with Morphological Association",
        "link_suffix": "/forum?id=tMKibc9Uxi",
        "link": "https://openreview.net/forum?id=tMKibc9Uxi",
        "pdf_link": "https://openreview.net/pdf?id=tMKibc9Uxi",
        "keywords": "Text Spotting, Text Detection",
        "abstract": "Video Text Spotting (VTS) is a fundamental visual task that aims to predict the trajectories and content of texts in a video. Previous works usually conduct local associations and apply IoU-based distance and complex post-processing procedures to boost performance, ignoring the abundant temporal information and the morphological characteristics in VTS. In this paper, we propose \\model{} to model the tracking problem as global associations and utilize the Gaussian Wasserstein distance to guide the morphological correlation between frames. Our main contributions can be summarized as three folds. 1). We propose a Transformer-based global tracking method \\model{} for VTS and associate multiple frames simultaneously. 2). We introduce a Wasserstein distance-based method to conduct positional associations between frames. 3). We conduct extensive experiments on public datasets. On the ICDAR2015 video dataset, \\model{} achieves \\textbf{56.0} MOTA with \\textbf{4.6} absolute improvement compared with the previous SOTA method and outperforms the previous Transformer-based method by a significant \\textbf{8.3} MOTA."
    },
    {
        "title": "Proteome-wide prediction of mode of inheritance and molecular mechanism underlying genetic diseases using structural interactomics",
        "link_suffix": "/forum?id=0JOhLEf2bX",
        "link": "https://openreview.net/forum?id=0JOhLEf2bX",
        "pdf_link": "https://openreview.net/pdf?id=0JOhLEf2bX",
        "keywords": "Mode of inheritance, Functional effect, Genetic diseases mechanism, Graph neural networks, Graph-of-graphs, Structural interactomics",
        "abstract": "Genetic diseases can be classified according to their modes of inheritance and their underlying molecular mechanisms. Autosomal dominant disorders often result from DNA variants that cause loss-of-function, gain-of-function, or dominant-negative effects, while autosomal recessive diseases are primarily linked to loss-of-function variants. In this study, we introduce a graph-of-graphs approach that leverages protein-protein interaction networks and high-resolution protein structures to predict the mode of inheritance of diseases caused by variants in autosomal genes, and to classify dominant-associated proteins based on their functional effect. Our approach integrates graph neural networks, structural interactomics and topological network features to provide proteome-wide predictions, thus offering a scalable method for understanding genetic disease mechanisms."
    },
    {
        "title": "The Blessing of Smooth Initialization for Video Diffusion Models",
        "link_suffix": "/forum?id=QIp1YUqgCB",
        "link": "https://openreview.net/forum?id=QIp1YUqgCB",
        "pdf_link": "https://openreview.net/pdf?id=QIp1YUqgCB",
        "keywords": "Smoothing, Video Diffusion Models, Noise Initialization",
        "abstract": "Extending the success of text-to-image (T2I) synthesis to text-to-video (T2V) synthesis is a promising direction for visual generative AI. Popular training-free sampling algorithms currently generate high-fidelity images within the Stable Diffusion family. However, when applied to video diffusion models (VDMs), these techniques result in limited diversity and quality due to the low-quality data in a video datasets. We focus on inference to mitigate this issue, and then we propose a training-free paradigm that optimizes the initial Gaussian noise by introducing a targeted semantic prior bias into the sampling process from a smoothing perspective. The paradigm significantly improves both the fidelity and semantic faithfulness of the synthesized videos. Guided by theoretical analysis using random smoothing and differential equations, our resulting method SmoothInit can be understood as approximately incorporating third-order derivatives into gradient descent, which contributes to be better convergence in learning semantic information. A more efficient version, Fast-SmoothInit, is proposed to achieve better experimental results by leveraging a momentum mechanism. Both SmoothInit and Fast-SmoothInit demonstrate promising empirical results across various benchmarks, including UCF-101/MSR-VTT-related FVD, Chronomagic-bench, and T2V-Compbench, setting a new standard for noise initialization in VDMs."
    },
    {
        "title": "SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training",
        "link_suffix": "/forum?id=ETFfXGM3e4",
        "link": "https://openreview.net/forum?id=ETFfXGM3e4",
        "pdf_link": "https://openreview.net/pdf?id=ETFfXGM3e4",
        "keywords": "image generation, watermarking, latent diffusion model",
        "abstract": "The proliferation of AI-generated images necessitates effective watermarking to protect intellectual property and identify fake content. While existing training-based watermarking methods show promise, they often struggle with generalization across diverse image styles and tend to produce noticeable artifacts. To this end, we introduce a provably generalizable image watermarking method for Latent Diffusion Models with Self-Augmented Training (SAT-LDM), which aligns the training and testing phases by a free generation distribution to bolster the watermarking module\u2019s generalization capabilities. We theoretically consolidate our method by proving that the free generation distribution contributes to its tight generalization bound without the need to collect new data. Extensive experimental results demonstrate that SAT-LDM achieves robust watermarking while significantly improving the quality of watermarked images across diverse styles. Furthermore, we conduct experimental analyses to demonstrate the strong generalization abilities of SAT-LDM. We hope our method offers a practical and convenient solution for securing high-fidelity AI-generated content."
    },
    {
        "title": "TimeRAG: It's Time for Retrieval-Augmented Generation in Time-Series Forecasting",
        "link_suffix": "/forum?id=GvzL4LuycW",
        "link": "https://openreview.net/forum?id=GvzL4LuycW",
        "pdf_link": "https://openreview.net/pdf?id=GvzL4LuycW",
        "keywords": "time series, large language model, retrieval augmented generation",
        "abstract": "Time-series data are essential for forecasting tasks across various domains. While Large Language Models (LLMs) have excelled in many areas, they encounter significant challenges in time-series forecasting, particularly in extracting relevant information from extensive temporal datasets. Unlike textual data, time-series data lack explicit retrieval ground truths, complicating the retrieval process. To tackle these issues, we present TimeRAG, a novel retrieval-augmented approach tailored for time-series forecasting. Our method uniquely applies to continuous and complex temporal sequences, and it is trained using LLM feedback, effectively addressing the absence of ground truth and aligning the priorities of the retriever and the LLM. Experimental results demonstrate the effectiveness of TimeRAG, highlighting its ability to significantly enhance forecasting performance and showcasing the potential of LLMs in time-series prediction tasks."
    },
    {
        "title": "Semantic Object Navigation with Segmenting Decision Transformer",
        "link_suffix": "/forum?id=CuD9J1QxqC",
        "link": "https://openreview.net/forum?id=CuD9J1QxqC",
        "pdf_link": "https://openreview.net/pdf?id=CuD9J1QxqC",
        "keywords": "Reinforcement Learning, Semantic Segmentation, Navigation, Robotics",
        "abstract": "Understanding scene semantics plays an important role in solving the object navigation task, where an embodied intelligent agent has to find an object in the scene given its semantic category. This task can be divided into two stages: exploring the scene and reaching the found target. In this work, we consider the latter stage of reaching a given semantic goal. This stage is particularly sensitive to errors in the semantic understanding of the scene. To address this challenge, we propose a multimodal and multitasking method called SegDT, which is based on the joint training of a segmentation model and a decision transformer model. Our method aggregates information from multiple multimodal frames to predict the next action and the current segmentation mask of the target object. To optimize our model, we first performed a pre-training phase using a set of collected trajectories. In the second phase, online policy fine-tuning, we addressed the problems of long-term credit assignment and poor sampling efficiency of transformer models. Using the PPO algorithm, we simultaneously trained an RNN-based policy using ground-truth segmentation and transferred its knowledge to the proposed transformer-based model, which trains the segmentation in itself through an additional segmentation loss. We conducted extensive experiments in the Habitat Sim environment and demonstrated the advantage of the proposed method over the basic navigation approach as well as current state-of-the-art methods that do not consider the auxiliary task of improving the quality of the segmentation of the current frame during training."
    },
    {
        "title": "Robust Representation Consistency Model via Contrastive Denoising",
        "link_suffix": "/forum?id=armbJRJdrH",
        "link": "https://openreview.net/forum?id=armbJRJdrH",
        "pdf_link": "https://openreview.net/pdf?id=armbJRJdrH",
        "keywords": "Certified Robustness, Diffusion-based randomized smoothing",
        "abstract": "Robustness is essential for deep neural networks, especially in security-sensitive applications. To this end, randomized smoothing provides theoretical guarantees for certifying robustness against adversarial perturbations. Recently, diffusion models have been successfully employed for randomized smoothing to purify noise perturbed samples before making predictions with a standard classifier. While these methods excel at small perturbation radii, they struggle with larger perturbations and incur a significant computational overhead during inference in comparison with standard methods. To address this, we reformulate the generative modeling task along the diffusion trajectories in pixel space as a discriminative task in the latent space. Specifically, we use instance discrimination to achieve consistent representations along the trajectories by aligning temporally adjacent points. After fine-tuning based on the learned representations, our model enables implicit denoising-then-classification via a single prediction, substantially reducing computation cost during inference. We conduct extensive experiments on various datasets and achieve state-of-the-art performance given minimal inference computation budget.  For example, our method outperforms the certified accuracy of diffusion-based methods on ImageNet across all perturbation radii by up to 5.8%, significantly improving the performance at larger ones, while reducing inference costs by 46$\\times$."
    },
    {
        "title": "CtD: Composition through Decomposition in Emergent Communication",
        "link_suffix": "/forum?id=KlalQu2423",
        "link": "https://openreview.net/forum?id=KlalQu2423",
        "pdf_link": "https://openreview.net/pdf?id=KlalQu2423",
        "keywords": "Emergent communication, Compositionality, Codebook learning",
        "abstract": "Compositionality is a cognitive mechanism that allows humans to systematically combine known concepts in novel ways.\nThis study demonstrates how artificial neural agents acquire and utilize compositional generalization to describe previously unseen images.\nOur method, termed ``Composition through Decomposition'', involves two sequential training steps.\nIn the 'Decompose' step, the agents learn to decompose an image into basic concepts using a codebook acquired during interaction in a multi-target coordination game.\nSubsequently, in the `Compose' step, the agents employ this codebook to describe novel images by composing basic concepts into complex phrases.\nRemarkably, we observe cases where generalization in the `Compose' step is achieved zero-shot, without the need for additional training."
    },
    {
        "title": "Learning from Demonstration with Implicit Nonlinear Dynamics Models",
        "link_suffix": "/forum?id=tpqMR73GzS",
        "link": "https://openreview.net/forum?id=tpqMR73GzS",
        "pdf_link": "https://openreview.net/pdf?id=tpqMR73GzS",
        "keywords": "robotics; learning from demonstration; recurrent neural networks; reservoir computing;",
        "abstract": "Learning from Demonstration (LfD) is a useful paradigm for training policies that solve tasks involving complex motions, such as those encountered in robotic manipulation. In practice, the successful application of LfD requires overcoming error accumulation during policy execution, i.e. the problem of drift due to errors compounding over time and the consequent out-of-distribution behaviours. Existing works seek to address this problem through scaling data collection, correcting policy errors with a human-in-the-loop, temporally ensembling policy predictions or through learning a dynamical system model with convergence guarantees. In this work, we propose and validate an alternative approach to overcoming this issue. Inspired by reservoir computing, we develop a recurrent neural network layer that includes a fixed nonlinear dynamical system with tunable dynamical properties for modelling temporal dynamics. We validate the efficacy of our neural network layer on the task of reproducing human handwriting motions using the LASA Human Handwriting Dataset. Through empirical experiments we demonstrate that incorporating our layer into existing neural network architectures addresses the issue of compounding errors in LfD. Furthermore, we perform a comparative evaluation against existing approaches including a temporal ensemble of policy predictions and an Echo State Network (ESN) implementation. We find that our approach yields greater policy precision and robustness on the handwriting task while also generalising to multiple dynamics regimes and maintaining competitive latency scores."
    },
    {
        "title": "Dynamic Weighting: Exploiting the Potential of a Single Weight Across Different Modes",
        "link_suffix": "/forum?id=CyonEdshEn",
        "link": "https://openreview.net/forum?id=CyonEdshEn",
        "pdf_link": "https://openreview.net/pdf?id=CyonEdshEn",
        "keywords": "Weights Augmentation, Shadow Weights, Plain Weights, Accuracy-Oriented Mode, Desire-Oriented Mode",
        "abstract": "Weights play an essential role in determining the performance of deep networks. This paper introduces a new concept termed ``Weight Augmentation Strateg'' (WAS), which emphasizes the exploration of weight spaces rather than traditional network structure design. The core of WAS is the utilization of randomly transformed weight coefficients, referred to as Shadow Weights (SW), for deep networks to calculate the loss function and update the parameters. Differently, stochastic gradient descent is applied to Plain Weights (PW), which is referred to as the original weight of the network before the random transformation. During training, numerous SW collectively form a high-dimensional space, while PW is directly learned from the distribution of SW. To maximize the benefits of WAS, we introduce two operational modes, \\textit{i.e.},  the Accuracy-Priented Mode (AOM) and the Desire-Oriented Mode (DOM). To be concrete, AOM relies on PW, which ensures that the network remains highly robust and accurate. Meanwhile, DOM utilizes SW, which is determined by the specific objective of our proposed WAS, such as reduced computational complexity or lower sensitivity to particular data. These dual modes can be switched at any time as needed, thereby providing flexibility and adaptability to different tasks. By extending the concept of augmentation from data to weights, our WAS offers an easy-to-understand and implement technique that can significantly enhance almost all networks. Our experimental results demonstrate that convolutional neural networks, including VGG-16, ResNet-18, ResNet-34, GoogleNet, MobileNetV2, and EfficientNet-Lite, benefit substantially with little to no additional costs. On the CIFAR-100 and CIFAR-10 datasets, model accuracy increases by an average of 7.32% and 9.28%, respectively, with the highest improvements reaching 13.42% and 18.93%. In addition, DOM can reduce floating point operations (FLOPs) by up to 36.33%."
    },
    {
        "title": "OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents",
        "link_suffix": "/forum?id=9YNyiCJE3k",
        "link": "https://openreview.net/forum?id=9YNyiCJE3k",
        "pdf_link": "https://openreview.net/pdf?id=9YNyiCJE3k",
        "keywords": "Keywords: Large Language Model, OSDA, Zeolite, Molecular Design",
        "abstract": "Zeolites are crystalline porous materials that have been widely utilized in petrochemical industries as well as sustainable chemistry areas. Synthesis of zeolites often requires small molecules termed Organic Structure Directing Agents (OSDAs), which are critical in forming the porous structure. Molecule generation models can aid the design of OSDAs, but they are limited by single functionality and lack of interactivity. Meanwhile, large language models (LLMs) such as GPT-4, as general-purpose artificial intelligence systems, excel in instruction comprehension, logical reasoning, and interactive communication. However, LLMs lack in-depth chemistry knowledge and first-principle computation capabilities, resulting in uncontrollable outcomes even after fine-tuning. In this paper, we propose OSDA Agent, an interactive OSDA design framework that leverages LLMs as the brain, coupled with computational chemistry tools. The OSDA Agent consists of three main components: the Actor, responsible for generating potential OSDA structures; the Evaluator, which assesses and scores the generated OSDAs using computational chemistry tools; and the Self-reflector, which produces reflective summaries based on the Evaluator's feedback to refine the Actor's subsequent outputs. Experiments on representative zeolite frameworks show the generation-evaluation-reflection-refinement workflow can perform de novo design of OSDAs with superior generation quality than the pure LLM model, generating candidates consistent with experimentally validated OSDAs and optimizing known OSDAs. \nThe code and model will be publicly available."
    },
    {
        "title": "LASER: Script Execution by Autonomous Agents for On-demand Traffic Simulation",
        "link_suffix": "/forum?id=YNa0Mzx4P9",
        "link": "https://openreview.net/forum?id=YNa0Mzx4P9",
        "pdf_link": "https://openreview.net/pdf?id=YNa0Mzx4P9",
        "keywords": "Traffic Simulation; Autonomous Agents; LLM for Autonomous Driving",
        "abstract": "Autonomous Driving Systems (ADS) require diverse and safety-critical traffic scenarios for effective training and testing, but the existing data generation methods struggle to provide flexibility and scalability. We propose LASER, a novel framework that leverage large language models (LLMs) to conduct traffic simulations based on natural language inputs. The framework operates in two stages: it first generates scripts from user-provided descriptions and then executes them using autonomous agents in real time. Validated in the CARLA simulator, LASER successfully generates complex, on-demand driving scenarios, significantly improving ADS training and testing data generation."
    },
    {
        "title": "Image registration is a geometric deep learning task",
        "link_suffix": "/forum?id=d6Kk7moQH3",
        "link": "https://openreview.net/forum?id=d6Kk7moQH3",
        "pdf_link": "https://openreview.net/pdf?id=d6Kk7moQH3",
        "keywords": "Image registration, Geometric deep learning",
        "abstract": "Data-driven deformable image registration methods predominantly rely on operations that process grid-like inputs.\nHowever, applying deformable transformations to an image results in a warped space that deviates from a rigid grid structure.\nConsequently, data-driven approaches with sequential deformations have to apply grid resampling operations between each deformation step. \nWhile artifacts caused by resampling are negligible in high-resolution images, the resampling of sparse, high-dimensional feature grids introduces errors that affect the deformation modeling process.\nTaking inspiration from Lagrangian reference frames of deformation fields, our work introduces a novel paradigm for data-driven deformable image registration that utilizes geometric deep-learning principles to model deformations without grid requirements.\nSpecifically, we model image features as a set of nodes that freely move in Euclidean space, update their coordinates under graph operations, and dynamically readjust their local neighborhoods.\nWe employ this formulation to construct a multi-resolution deformable registration model, where deformation layers iteratively refine the overall transformation at each resolution without intermediate resampling operations on the feature grids.\nWe investigate our method's ability to fully deformably capture large deformations across a number of medical imaging registration tasks. \nIn particular, we apply our approach (GeoReg) to the registration of inter-subject brain MR images and inhale-exhale lung CT images, showing superior performance to the current state-of-the-art methods."
    },
    {
        "title": "Geometry of Lightning Self-Attention: Identifiability and Dimension",
        "link_suffix": "/forum?id=XtY3xYQWcW",
        "link": "https://openreview.net/forum?id=XtY3xYQWcW",
        "pdf_link": "https://openreview.net/pdf?id=XtY3xYQWcW",
        "keywords": "Lightning Self-Attention, Neuromanifolds, Algebraic Geometry",
        "abstract": "We consider function spaces defined by self-attention networks without normalization, and theoretically analyze their geometry. Since these networks are polynomial, we rely on tools from algebraic geometry. In particular, we study the identifiability of deep attention by providing a description of the generic fibers of the parametrization for an arbitrary number of layers and, as a consequence, compute the dimension of the function space. Additionally, for a single-layer model, we characterize the singular and boundary points. Finally, we formulate a conjectural extension of our results to normalized self-attention networks, prove it for a single layer, and numerically verify it in the deep case."
    },
    {
        "title": "Mask-Guided Video Generation: Enhancing Motion Control and Quality with Limited Data",
        "link_suffix": "/forum?id=9GNTtaIZh6",
        "link": "https://openreview.net/forum?id=9GNTtaIZh6",
        "pdf_link": "https://openreview.net/pdf?id=9GNTtaIZh6",
        "keywords": "Diffusion models, video generation",
        "abstract": "Recent advancements in diffusion models have brought new vitality into visual content creation. However, current text-to-video generation models still face challenges such as high training costs, substantial data requirements, and difficulties in maintaining consistency between given text and motion of the foreground object. To address these challenges, we propose mask-guided video generation, which requires only a small amount of data and is trained on a single GPU. Furthermore, to mitigate the impact of background interference on controllable text-to-video generation, we utilize mask  sequences obtained through drawing or extraction, along with the first-frame content, to guide video generation. Specifically, our model introduces foreground masks into existing architectures to learn region-specific attention, precisely matching text features and the motion of the foreground object.  Subsequently, video generation is guided by the mask sequences to prevent the sudden disappearance of foreground objects. Our model also incorporates a first-frame sharing strategy during inference, leading to better stability in the video generation. Additionally, our approach allows for incrementally  generation of longer video sequences. By employing this method, our model achieves efficient resource utilization and ensures controllability and consistency in video generation using mask sequences. Extensive qualitative and quantitative experiments demonstrate that this approach excels in various video generation tasks, such as video editing and generating artistic videos, outperforming previous methods in terms of consistency and quality."
    },
    {
        "title": "DC-PINNs: Physics-Informed Neural Networks for Solving Derivative-Constrained PDEs",
        "link_suffix": "/forum?id=U2ZtvonVQz",
        "link": "https://openreview.net/forum?id=U2ZtvonVQz",
        "pdf_link": "https://openreview.net/pdf?id=U2ZtvonVQz",
        "keywords": "PINNs, Physics-Informed Neural Networks, Multi-objective Learning, Partial Differential Equations, Derivative-Constrained, Machine Learning",
        "abstract": "Physics-Informed Neural Networks (PINNs) have emerged as a promising approach for solving partial differential equations (PDEs) using deep learning. However, standard PINNs do not address the problem of constrained PDEs, where the solution must satisfy additional equality or inequality constraints beyond the governing equations. In this paper, we introduce Derivative-Constrained PINNs (DC-PINNs), a novel framework that seamlessly incorporates constraint information into the PINNs training process. DC-PINNs employ a constraint-aware loss function that penalizes constraint violations while simultaneously minimizing the PDE residual. Key components include self-adaptive loss balancing techniques that automatically tune the relative weighting of each term, enhancing training stability, and the use of automatic differentiation to efficiently compute derivatives. This study demonstrates the effectiveness of DC-PINNs on several benchmark problems, from basic to complex, such as quantitative finance and applied physics, including heat diffusion, volatility surface calibration, and incompressible flow dynamics. The results showcase improvements in generating solutions that satisfy the constraints compared to baseline PINNs methods. The DC-PINNs framework opens up new possibilities for solving constrained PDEs in multi-objective optimization problems."
    },
    {
        "title": "Safe Reinforcement Learning in Black-Box Environments via Adaptive Shielding",
        "link_suffix": "/forum?id=82VzAtBZGk",
        "link": "https://openreview.net/forum?id=82VzAtBZGk",
        "pdf_link": "https://openreview.net/pdf?id=82VzAtBZGk",
        "keywords": "Reinforcement Learning, Safe Reinforcement Learning",
        "abstract": "Empowering safe exploration of reinforcement learning (RL) agents during training is a critical impediment towards deploying RL agents in many real-world scenarios. Training RL agents in unknown, black-box environments poses an even greater safety risk when prior knowledge of the domain/task is unavailable. We introduce ADVICE (Adaptive Shielding with a Contrastive Autoencoder), a novel post-shielding technique that distinguishes safe and unsafe features of state-action pairs during training, thus protecting the RL agent from executing actions that yield potentially hazardous outcomes. Our comprehensive experimental evaluation against state-of-the-art safe RL exploration techniques demonstrates how ADVICE can significantly reduce safety violations during training while maintaining a competitive outcome reward."
    },
    {
        "title": "ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability",
        "link_suffix": "/forum?id=ztzZDzgfrh",
        "link": "https://openreview.net/forum?id=ztzZDzgfrh",
        "pdf_link": "https://openreview.net/pdf?id=ztzZDzgfrh",
        "keywords": "Retrieval-Augmented Generation Hallucination, Hallucination Detection, Mechanistic Interpretability",
        "abstract": "Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) balance external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when theKnowledge FFNsin LLMs overemphasize parametric knowledge in the residual stream, whileCopying Headsfail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we proposeReDeEP, a novel method that detects hallucinations by decoupling LLM\u2019s utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads."
    },
    {
        "title": "DOME: Taming Diffusion Model into High-Fidelity Controllable Occupancy World Model",
        "link_suffix": "/forum?id=nlpCeFsSYJ",
        "link": "https://openreview.net/forum?id=nlpCeFsSYJ",
        "pdf_link": "https://openreview.net/pdf?id=nlpCeFsSYJ",
        "keywords": "World Model, Diffusion Model, Occupancy, Generative Model",
        "abstract": "We propose DOME, a diffusion-based world model that predicts future occupancy frames based on past occupancy observations. The ability of this world model to capture the evolution of the environment is crucial for planning in autonomous driving. Compared to 2D video-based world models, the occupancy world model utilizes a native 3D representation, which features easily obtainable annotations and is modality-agnostic. This flexibility has the potential to facilitate the development of more advanced world models. Existing occupancy world models either suffer from detail loss due to discrete tokenization or rely on simplistic diffusion architectures, leading to inefficiencies and difficulties in predicting future occupancy with controllability. Our DOME exhibits two key features: (1) High-Fidelity and Long-Duration Generation. We adopt a spatial-temporal diffusion transformer to predict future occupancy frames based on historical context. This architecture efficiently captures spatial-temporal information, enabling high-fidelity details and the ability to generate predictions over long durations. (2)Fine-grained Controllability. We address the challenge of controllability in predictions by introducing a trajectory resampling method, which significantly enhances the model\u2019s ability to generate controlled predictions. Extensive experiments on the widely used nuScenes dataset demonstrate that our method surpasses existing baselines in both qualitative and quantitative evaluations, establishing a new state-of-the-art performance on nuScenes. Specifically, our approach surpasses the baseline by 10.5% in mIoU and 21.2% in IoU for occupancy reconstruction, and by 36.0% in mIoU and 24.6% in IoU for 4D occupancy forecasting."
    },
    {
        "title": "PharmacoMatch: Efficient 3D Pharmacophore Screening via Neural Subgraph Matching",
        "link_suffix": "/forum?id=27Qk18IZum",
        "link": "https://openreview.net/forum?id=27Qk18IZum",
        "pdf_link": "https://openreview.net/pdf?id=27Qk18IZum",
        "keywords": "Contrastive Representation Learning, Neural Subgraph Matching, Virtual Screening, Pharmacophore Modeling",
        "abstract": "The increasing size of screening libraries poses a significant challenge for the development of virtual screening methods for drug discovery, necessitating a re-evaluation of traditional approaches in the era of big data.\nAlthough 3D pharmacophore screening remains a prevalent technique, its application to very large datasets is limited by the computational cost associated with matching query pharmacophores to database ligands.\nIn this study, we introduce PharmacoMatch, a novel contrastive learning approach based on neural subgraph matching. Our method reinterprets pharmacophore screening as an approximate subgraph matching problem and enables efficient querying of conformational databases by encoding query-target relationships in the embedding space.\nWe conduct comprehensive evaluations of the learned representations and benchmark our method on virtual screening datasets in a zero-shot setting. Our findings demonstrate significantly shorter runtimes for pharmacophore matching, offering a promising speed-up for screening very large datasets."
    },
    {
        "title": "OscillationInversion: Understand the structure of Large Flow Model through the Lens of Inversion Method",
        "link_suffix": "/forum?id=5zMKxmc1eh",
        "link": "https://openreview.net/forum?id=5zMKxmc1eh",
        "pdf_link": "https://openreview.net/pdf?id=5zMKxmc1eh",
        "keywords": "diffusion models; image generation",
        "abstract": "We investigate oscillation phenomena observed in inversion methods applied to large text-to-image diffusion models, particularly the ``Flux'' model. Using a fixed-point-inspired iteration method to invert real-world images, we find that the solution does not converge but instead oscillates between distinct clusters. Our results, validated both on real diffusion models and toy experiments, show that these oscillated clusters exhibit significant semantic coherence. \nWe propose that this phenomenon arises from oscillatory solutions in dynamic systems, linking it to the structure of rectified flow models. The oscillated clusters serve as local latent distributions that allow for effective semantic-based image optimization.We provide theoretical insights, linking these oscillations to fixed-point dynamics and proving conditions for stable cluster formation and differentiation in flow models."
    },
    {
        "title": "Improving Multi-modal Large Language Model through Boosting Vision Capabilities",
        "link_suffix": "/forum?id=0yTf37PXcH",
        "link": "https://openreview.net/forum?id=0yTf37PXcH",
        "pdf_link": "https://openreview.net/pdf?id=0yTf37PXcH",
        "keywords": "Multi-modal Large Language Model, Boosting Vision Capabilities, Multi-modal Lora, Ladder Adapter",
        "abstract": "We focus on improving the visual understanding capability for boosting the vision-language models. We propose \\textbf{Arcana}, a multiModal language model, which introduces two crucial techniques. First, we present Multimodal LoRA (MM-LoRA), a module designed to enhance the decoder. Unlike traditional language-driven decoders, MM-LoRA consists of two parallel LoRAs -- one for vision and one for language -- each with its own parameters. This disentangled parameters design allows for more specialized learning in each modality and better integration of multimodal information. Second, we introduce the Query Ladder adapter (QLadder) to improve the visual encoder. QLadder employs a learnable ``\\textit{ladder}'' structure to deeply aggregates the intermediate representations from the frozen pretrained visual encoder (e.g., CLIP image encoder). This enables the model to learn new and informative visual features, as well as remaining the powerful capabilities of the pretrained visual encoder. These techniques collectively enhance Arcana's visual perception power, enabling it to leverage improved visual information for more accurate and contextually relevant outputs across various multimodal scenarios. Extensive experiments and ablation studies demonstrate the effectiveness and generalization capability of our Arcana."
    },
    {
        "title": "CLIP-to-Seg Distillation for Inductive Zero-shot Semantic Segmentation",
        "link_suffix": "/forum?id=u7oY4kPKyN",
        "link": "https://openreview.net/forum?id=u7oY4kPKyN",
        "pdf_link": "https://openreview.net/pdf?id=u7oY4kPKyN",
        "keywords": "zero-shot learning, zero-shot segmentation, semantic segmentation, knowledge distillation",
        "abstract": "CLIP has greatly advanced zero-shot segmentation by leveraging its strong visual-language association and generalization capability. However, directly adapting CLIP for segmentation often yields suboptimal results due to inconsistencies between image and pixel-level prediction objectives. Additionally, merely combining segmentation and CLIP models often leads to disjoint optimization, introducing significant computational overhead and additional parameters. To address these issues, we propose a novel CLIP-to-Seg Distillation approach, incorporating global and local distillation to flexibly transfer CLIP\u2019s powerful zero-shot generalization capability to existing closed-set segmentation models. Global distillation leverages CLIP\u2019s CLS token to condense segmentation features and distills high-level concepts to the segmentation model via image-level prototypes. Local distillation adapts CLIP\u2019s local semantic transferability to dense prediction tasks using object-level features, aided by pseudo-mask generation for latent unseen class mining. To further generalize the CLIP-distilled segmentation model, we generate latent embeddings for the mined latent classes by coordinating their semantic embeddings and dense features. Our method equips existing closed-set segmentation models with strong generalization capabilities for open concepts through effective and flexible CLIP-to-Seg distillation. Without relying on the CLIP model or adding extra computational overhead/parameters during inference, our method can be seamlessly integrated into existing segmentation models and achieves state-of-the-art performance on multiple zero-shot segmentation benchmarks."
    }
]