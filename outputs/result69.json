[{"title": "FedEx-LoRA: Exact Aggregation for Federated and Efficient Fine-Tuning of Foundation Models", "link_suffix": "/forum?id=Yg998afEbH", "link": "https://openreview.net/forum?id=Yg998afEbH", "pdf_link": "https://openreview.net/pdf?id=Yg998afEbH", "keywords": "Parameter-efficient fine-tuning, low-rank adaptation, federated learning, fine-tuning", "abstract": "Low-Rank Adaptation (LoRA) is a popular technique for efficient fine-tuning of foundation models. However, applying LoRA in federated learning environments, where data is distributed across multiple clients, presents unique challenges. Existing methods rely on traditional federated averaging of LoRA adapters, resulting in inexact updates. To address this, we propose Federated Exact LoRA, or FedEx-LoRA, which adds a residual error term to the pretrained frozen weight matrix. Our approach achieves exact updates with minimal computational and communication overhead, preserving LoRA\u2019s efficiency. We evaluate the method on various Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks, showing consistent performance gains over state-of-the-art methods across multiple settings. Through extensive analysis, we quantify that the deviations in updates from the ideal solution are significant, highlighting the need for exact aggregation. Our method's simplicity, efficiency, and broad applicability position it as a promising solution for accurate and effective federated fine-tuning of foundation models.", "title_embedding_index": 3400, "title_abs_embedding_index": 3425}, {"title": "Soft Checksums to Flag Untrustworthy Machine Learning Surrogate Predictions and Application to Atomic Physics Simulations", "link_suffix": "/forum?id=aAI92OHA4t", "link": "https://openreview.net/forum?id=aAI92OHA4t", "pdf_link": "https://openreview.net/pdf?id=aAI92OHA4t", "keywords": "deep neural networks, surrogate models, out-of-distribution detection, non-local thermodynamic equilibrium", "abstract": "Trained neural networks (NN) are attractive as surrogate models to replace costly calculations in physical simulations, but are often unknowingly applied to states not adequately represented in the training dataset. We present the novel technique of soft checksums for scientific machine learning, a general-purpose method to differentiate between trustworthy predictions with small errors on in-distribution (ID) data points, and untrustworthy predictions with large errors on out-of-distribution (OOD) data points. By adding a check node to the existing output layer, we train the model to learn the chosen checksum function encoded within the NN predictions and show that violations of this function correlate with high prediction errors. As the checksum function depends only on the NN predictions, we can calculate the checksum error for any prediction with a single forward pass, incurring negligible time and memory costs. Additionally, we find that incorporating the checksum function into the loss function and exposing the NN to OOD data points during the training process improves separation between ID and OOD predictions. By applying soft checksums to a physically complex and high-dimensional non-local thermodynamic equilibrium atomic physics dataset, we show that a well-chosen threshold checksum error can effectively separate ID and OOD predictions.", "title_embedding_index": 3401, "title_abs_embedding_index": 3426}, {"title": "MetaInv: Overcoming Iterative and Direct Method Limitations for Inverse Learning", "link_suffix": "/forum?id=5cPEkoHHyG", "link": "https://openreview.net/forum?id=5cPEkoHHyG", "pdf_link": "https://openreview.net/pdf?id=5cPEkoHHyG", "keywords": "Invertible neural networks, switchable Architectures, analytical inverse", "abstract": "Invertible neural networks (INNs) have gained significant traction in tasks requiring reliable bidirectional inferences, such as data encryption, scientific computing, and real-time control. However, iterative methods like i-ResNet face notable limitations, including instability on non-contractive mappings and failure in scenarios requiring strict one-to-one mappings. In contrast, analytical approaches like DipDNN guarantee invertibility but at the expense of performance, particularly in tasks demanding rich feature extraction (e.g., convolutional operations in complex image processing). This work presents a detailed analysis of the limitations in current invertible architectures, examining the trade-offs between iterative and analytical approaches. We identify key failure modes, particularly when handling information redundancy or strict bijections, and propose a meta-inverse framework that dynamically combines the advantages of both i-ResNet and DipDNN. Our framework adapts in real-time based on task-specific signals, ensuring both flexibility and guaranteed invertibility. Extensive experiments across diverse domains demonstrate that our hybrid approach outperforms existing methods in forward accuracy, inverse consistency, and computational efficiency. Our results highlight the utility of this meta-inverse strategy for critical applications where precision, stability, and adaptability are crucial.", "title_embedding_index": 3402, "title_abs_embedding_index": 3427}, {"title": "DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search", "link_suffix": "/forum?id=tn2mjzjSyR", "link": "https://openreview.net/forum?id=tn2mjzjSyR", "pdf_link": "https://openreview.net/pdf?id=tn2mjzjSyR", "keywords": "large language model, reasoning", "abstract": "Enhancing the capability of large language models (LLMs) in reasoning has gained significant attention in recent years. Previous studies have demonstrated the effectiveness of various prompting strategies in aiding LLMs in reasoning (called \"reasoning actions\"), such as step-by-step thinking, reflecting before answering, solving with programs, and their combinations. However, these approaches often applied static, predefined reasoning actions uniformly to all questions, without considering the specific characteristics of each question or the capability of the task-solving LLM. In this paper, we propose DOTS, an approach enabling LLMs to reason Dynamically via Optimal reasoning Trajectories Search, tailored to the specific characteristics of each question and the inherent capability of the task-solving LLM. \nOur approach involves three key steps: i) defining atomic reasoning action modules that can be composed into various reasoning action trajectories; ii) searching for the optimal action trajectory for each training question through iterative exploration and evaluation for the specific task-solving LLM; and iii) using the collected optimal trajectories to train an LLM to plan for the reasoning trajectories of unseen questions. In particular, we propose two learning paradigms, i.e., fine-tuning an external LLM as a planner to guide the task-solving LLM, or directly fine-tuning the task-solving LLM with an internalized capability for reasoning actions planning. Our experiments across eight reasoning tasks show that our method consistently outperforms static reasoning techniques and the vanilla instruction tuning approach. Further analysis reveals that our method enables LLMs to adjust their computation based on problem complexity, allocating deeper thinking and reasoning to harder problems.", "title_embedding_index": 3403, "title_abs_embedding_index": 3428}, {"title": "Learning Diagrams: A Graphical Language for Compositional Training Regimes", "link_suffix": "/forum?id=dqyuCsBvn9", "link": "https://openreview.net/forum?id=dqyuCsBvn9", "pdf_link": "https://openreview.net/pdf?id=dqyuCsBvn9", "keywords": "ML Libraries, Training, Foundation Models, Multi-Task Learning", "abstract": "Motivated by deep learning regimes with multiple interacting yet distinct model components, we introduce learning diagrams, graphical depictions of training setups that capture parameterized learning as data rather than code. A learning diagram compiles to a unique loss function on which component models are trained. The result of training on this loss is a collection of models whose predictions ``agree\" with one another. We show that a number of popular learning setups such as few-shot multi-task learning, knowledge distillation, and multi-modal learning can be depicted as learning diagrams. We further implement learning diagrams in a library that allows users to build diagrams of PyTorch and Flux.jl models. By implementing some classic machine learning use cases, we demonstrate how learning diagrams allow practitioners to build complicated models as compositions of smaller components, identify relationships between workflows, and manipulate models during or after training. Leveraging a category theoretic framework, we introduce a rigorous semantics for learning diagrams that puts such operations on a firm mathematical foundation.", "title_embedding_index": 3404, "title_abs_embedding_index": 3429}, {"title": "Differential learning kinetics govern the transition from memorization to generalization during in-context learning", "link_suffix": "/forum?id=INyi7qUdjZ", "link": "https://openreview.net/forum?id=INyi7qUdjZ", "pdf_link": "https://openreview.net/pdf?id=INyi7qUdjZ", "keywords": "in-context learning, mechanistic interpretability, small transformers, memorization", "abstract": "Transformers exhibit in-context learning (ICL): the ability to use novel information presented in the context without additional weight updates. Recent work shows that ICL emerges when models are trained on a sufficiently diverse set of tasks and the transition from memorization to generalization is sharp with increasing task diversity. One interpretation is that a network's limited capacity to memorize favors generalization. Here, we examine the mechanistic underpinnings of this transition using a small transformer applied to a synthetic ICL task. Using theory and experiment, we show that the sub-circuits that memorize and generalize can be viewed as largely independent. The relativeratesat which these sub-circuits learn explains the transition from memorization to generalization, rather than capacity constraints. We uncover a memorization scaling law, which determines the task diversity threshold at which the network generalizes. The theory quantitatively explains a variety of other ICL-related phenomena, including the long-tailed distribution of when ICL is acquired, the bimodal behavior of solutions close to the task diversity threshold, the influence of contextual and data distributional statistics on ICL, and the transient nature of ICL.", "title_embedding_index": 3405, "title_abs_embedding_index": 3430}, {"title": "Joint Graph Rewiring and Feature Denoising via Spectral Resonance", "link_suffix": "/forum?id=zBbZ2vdLzH", "link": "https://openreview.net/forum?id=zBbZ2vdLzH", "pdf_link": "https://openreview.net/pdf?id=zBbZ2vdLzH", "keywords": "GNNs, Rewiring, Denoising, Spectral Resonance, cSBM", "abstract": "In graph learning the graph and the node features both contain noisy information about the node labels. In this paper we propose joint denoising and rewiring (JDR)\u2014an algorithm to jointly rewire the graph and denoise the features, which improves the performance of downstream node classification graph neural nets (GNNs). JDR improves the alignment between the leading eigenspaces of graph and feature matrices. To approximately solve the associated non-convex optimization problem we propose a heuristic that efficiently handles real-world graph datasets with multiple classes and different levels of homophily or heterophily. We theoretically justify JDR in a stylized setting and verify the effectiveness of our approach through extensive experiments on synthetic and real-world graph datasets. The results show that JDR consistently outperforms existing rewiring methods on node classification using GNNs as downstream models.", "title_embedding_index": 3406, "title_abs_embedding_index": 3431}, {"title": "The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions", "link_suffix": "/forum?id=vf5M8YaGPY", "link": "https://openreview.net/forum?id=vf5M8YaGPY", "pdf_link": "https://openreview.net/pdf?id=vf5M8YaGPY", "keywords": "Jailbreaks, Prompt Injections, Adversarial Robustness", "abstract": "Today's LLMs are susceptible to prompt injections, jailbreaks, and other attacks that allow adversaries to overwrite a model's original instructions with their own malicious prompts. In this work, we argue that one of the primary vulnerabilities underlying these attacks is that LLMs often consider system prompts (e.g., text from an application developer) to be the same priority as text from untrusted users and third parties. To address this, we propose an instruction hierarchy that explicitly defines how models should behave when instructions of different priorities conflict. We then propose a data generation method to demonstrate this hierarchical instruction following behavior, which teaches LLMs to selectively ignore lower-privileged instructions. We apply this method to GPT-3.5, showing that it drastically increases robustness---even for attack types not seen during training---while imposing minimal degradations on standard capabilities.", "title_embedding_index": 3407, "title_abs_embedding_index": 3432}, {"title": "Strategic Filtering for Content Moderation: Free Speech or Free of Distortion?", "link_suffix": "/forum?id=lqid8idmeG", "link": "https://openreview.net/forum?id=lqid8idmeG", "pdf_link": "https://openreview.net/pdf?id=lqid8idmeG", "keywords": "Content moderation, Strategic behavior, statistical learning, NP-hardness", "abstract": "User-generated content (UGC) on social media platforms is vulnerable to incitements and manipulations, necessitating effective regulations. To address these challenges, those platforms often deploy automated content moderators tasked with evaluating the harmfulness of UGC and filtering out content that violates established guidelines. However, such moderation inevitably gives rise to strategic responses from users, who strive to express themselves within the confines of guidelines. Such phenomenons call for a careful balance between: 1. ensuring freedom of speech --- by minimizing the restriction of expression; and 2. reducing social distortion --- measured by the total amount of content manipulation. We tackle the problem of optimizing this balance through the lens of mechanism design, aiming at optimizing the trade-off between minimizing social distortion and maximizing free speech. Although  determining the optimal trade-off is NP-hard, we propose practical methods to approximate the optimal solution. Additionally, we provide generalization guarantees that determine the amount of finite offline data required to effectively approximate the optimal moderator.", "title_embedding_index": 3408, "title_abs_embedding_index": 3433}, {"title": "SatDiffMoE: A Mixture of Estimation Method for Satellite Image Super-resolution with Latent Diffusion Models", "link_suffix": "/forum?id=BDf1IBIuFx", "link": "https://openreview.net/forum?id=BDf1IBIuFx", "pdf_link": "https://openreview.net/pdf?id=BDf1IBIuFx", "keywords": "Diffusion models, satellite imaging", "abstract": "During the acquisition of satellite images, there is generally a trade-off between spatial resolution and temporal resolution (acquisition frequency) due to the onboard sensors of satellite imaging systems. High-resolution satellite images are very important for land crop monitoring, urban planning, wildfire management and a variety of applications. It is a significant yet challenging task to achieve high spatial-temporal resolution in satellite imaging. With the advent of diffusion models, we can now learn strong generative priors to generate realistic satellite images with high resolution, which can be utilized to promote the super-resolution task as well. In this work, we propose a novel diffusion-based fusion algorithm called SatDiffMoE that can take an arbitrary number of sequential low-resolution satellite images at the same location as inputs, and fuse them into one high-resolution reconstructed image with more fine details, by leveraging and fusing the complementary information from different time points. Our algorithm is highly flexible and allows training and inference on arbitrary number of low-resolution images. Experimental results show that our proposed SatDiffMoE method not only achieves superior performance for the satellite image super-resolution tasks on a variety of datasets, but also gets an improved computational efficiency with reduced model parameters, compared with previous methods.", "title_embedding_index": 3409, "title_abs_embedding_index": 3434}, {"title": "Review and Rebuttal: Zero-shot In-context Adversarial Learning for Improving Research Ideation", "link_suffix": "/forum?id=AAjCYWXC5I", "link": "https://openreview.net/forum?id=AAjCYWXC5I", "pdf_link": "https://openreview.net/pdf?id=AAjCYWXC5I", "keywords": "scientific hypothesis generation, large language models, in-context learning, adversarial learning", "abstract": "Recent studies highlight that the advancements in Large Language Models (LLMs) have opened up exciting possibilities for scientific discovery, where LLMs can assist researchers in generating novel hypotheses and ideas. In this work, we draw inspiration from Generative Adversarial Networks (GANs) and make the first effort to formalize the concept of zero-shot in-context adversarial learning and implement it through multi-LLM-agent interactions to improve the research ideation process. Our approach takes the best of two worlds: (1) by making in-context learning adversarial, the utilization of an LLM\u2019s vast parametric knowledge can be optimized; and (2) by keeping adversarial learning in context, we eliminate the need for bi-level optimization through additional model training. To evaluate the quality of the open-ended generation produced by LLMs, we develop a relative quality ranking metric, designed to serve as a proxy for human evaluation when human assessments are impractical or costly. Our findings demonstrate that zero-shot in-context adversarial learning significantly enhances idea generation across two dimensions. Specifically, with GPT-4o, the novelty of generated ideas improved by 21%, and feasibility of the ideas saw an impressive increase of 322%. These results underscore the transformative potential of zero-shot in-context adversarial learning in driving innovation and creativity within the research process.", "title_embedding_index": 3410, "title_abs_embedding_index": 3435}, {"title": "Towards Efficient Confidence Estimation for Large Language Model Reasoning", "link_suffix": "/forum?id=60rQpnbgmE", "link": "https://openreview.net/forum?id=60rQpnbgmE", "pdf_link": "https://openreview.net/pdf?id=60rQpnbgmE", "keywords": "Large language models, Mathemtical reasoning, Confidence Estimation", "abstract": "Recent advances have demonstrated the powerful reasoning capabilities of large language models (LLMs), and accurately measuring the confidence of reasoning paths is crucial for improving the performance and trustworthy of AI systems. Benefiting from consistency function for reasoning, the self-consistency method often provides an effective confidence estimation. However, it suffers from the variance issue, which extremely constrains the performance when the sampling is insufficient. Existing methods such as the temperature sampling cannot well resolve this problem as it not only necessitates a calibration set but also tends to sacrifice the reasoning capability of LLMs. In this paper, we propose a data-free, and highly sampling efficient method to control the variance. The merit of our approach lies in a reasonable integration of the LLM's probability estimation and the self-consistency confidence. Our theoretical analysis confirms the efficacy of our method by achieving a lower estimation error and a higher error reduction rate. Furthermore, an in-depth analysis of the error decomposition reveals an improved technique, which can significantly improve error reduction rate with only a small scale of bias induced. Experimental results across seven benchmark datasets demonstrate that our proposed approaches achieve superior confidence estimation, boosting the accuracy on both mathematical reasoning tasks and code generation tasks. Our code is provided in the supplementary material.", "title_embedding_index": 3411, "title_abs_embedding_index": 3436}, {"title": "\u201dDOES YOUR MOBILE SUIT YOUR SKIN?\u201d: ADDRESSING SKIN TONE DISPARITIES IN PRESENTATION ATTACK DETECTION FOR ENHANCED INCLUSIVITY OF SMARTPHONE SECURITY", "link_suffix": "/forum?id=dEGYODD6iU", "link": "https://openreview.net/forum?id=dEGYODD6iU", "pdf_link": "https://openreview.net/pdf?id=dEGYODD6iU", "keywords": "Skin Tone, Fairness, Mobile Biometrics, Finger Photo, Presentation Attack Detection, Presentation Attack, Color Spaces", "abstract": "Mobile devices are at a heightened risk for cybercrime due to the sensitive personal and financial data they handle. Biometric authentication provides a robust,convenient, and secure way to protect smartphones by using unique user characteristics like fingerprints, facial features, or voice patterns for access. Existing mobile biometric technology often relies on RGB cameras to capture biometric\nsamples, such as face images or finger photos, making them vulnerable to spoofing (e.g., 3D masks, display or printout attack). The security of these systems is effectively addressed by integrating a Presentation Attack Detection (PAD) module. Existing PAD solutions do not account for diverse physical characteristics such as skin tone. As a result, marginalized groups face higher misidentification\nrates or false rejections, reducing access to services and increasing security risks.This paper introduces a novel deep learning framework called ColorCubeNet that is designed to process ColorCube, a multi dimensional data representation by combining information from RGB, HSV and YCbCr color spaces. This data cube leverages the joint capabilities of RGB, HSV, and YCbCr color spaces to depict color more sophisticatedly. By incorporating features from multiple complementary color channels, this approach can effectively handle a variety of skin tones. We utilized three EfficientNet-B0 models, each trained on ImageNet using RGB, HSV, and YCbCr color spaces, and then fine-tune them on the ColorCube representation to fully exploit the combined information from all three color spaces. Additionally, a channel-attention mechanism is integrated into the architecture, enabling the extraction of key features from different input channels and exploit their combined performance. Results show that the proposed approach outperforms traditional RGB methods by reducing skin tone disparities by 50%.", "title_embedding_index": 3412, "title_abs_embedding_index": 3437}, {"title": "Context-Scaling versus Task-Scaling in In-Context Learning", "link_suffix": "/forum?id=E8TPUAimyJ", "link": "https://openreview.net/forum?id=E8TPUAimyJ", "pdf_link": "https://openreview.net/pdf?id=E8TPUAimyJ", "keywords": "in-context learning, kernel smoothers, Hilbert estimate", "abstract": "Transformers exhibit In-Context Learning (ICL), a phenomenon in which these models solve new tasks by using examples in the prompt without additional training.  In our work, we analyze two key components of ICL: (1) context-scaling, where model performance improves as the number of in-context examples increases and (2) task-scaling, where model performance improves as the number of pre-training tasks increases.  While transformers are capable of both context-scaling and task-scaling, we empirically show that standard Multi-Layer Perceptrons (MLPs) with vectorized input are only capable of task-scaling.  To understand how transformers are capable of context-scaling, we first propose a significantly simplified transformer that performs ICL comparably to the original GPT-2 model in statistical learning tasks (e.g., linear regression, teacher-student settings).  By analyzing a single layer of our proposed model, we identify classes of feature maps that enable context scaling. Theoretically, these feature maps can implement the Hilbert estimate, a model that is provably consistent for context-scaling.  We then show that using the output of the Hilbert estimate along with vectorized input empirically enables both context-scaling and task-scaling with MLPs.  Overall, our findings provide insights into the fundamental mechanisms of how transformers are able to learn in context.", "title_embedding_index": 3413, "title_abs_embedding_index": 3438}, {"title": "Practical alignment requires more than learning from human feedback", "link_suffix": "/forum?id=LTpab44sdG", "link": "https://openreview.net/forum?id=LTpab44sdG", "pdf_link": "https://openreview.net/pdf?id=LTpab44sdG", "keywords": "reinforcement learning, alignment, human feedback, rlhf, AI safety", "abstract": "Ensuring the alignment of artificial intelligence (AI) systems with human objectives is a critical challenge in the development of safe and effective AI technologies. Reinforcement learning from human feedback (RLHF) has been a predominant method to tackle this challenge. However, this framework operates under the unrealistic assumptions that human preferences are accurate reflections of their desires and that they remain constant over time. This paper identifies and challenges these assumptions by illustrating how they can lead to undesirable consequences, particularly when human beliefs about the environment are incorrect or mutate over time. To address these challenges, we introduce a novel framework termed practical alignment. This framework redefines the alignment objective to accommodate the variability and irrationality of human beliefs, emphasizing the need for AI systems not only to learn from but also to teach humans about the world. We discuss the theoretical underpinnings of practical alignment and introduce MindGrid, a toolkit designed to simulate and evaluate alignment scenarios. Our experimental results using large language models in teaching scenarios underscore the importance of teaching skills as a requisite capability to achieve alignment.", "title_embedding_index": 3414, "title_abs_embedding_index": 3439}, {"title": "Performance Control in Early Exiting to Deploy Large Models at the Same Cost of Smaller Ones", "link_suffix": "/forum?id=LO76nlbvNt", "link": "https://openreview.net/forum?id=LO76nlbvNt", "pdf_link": "https://openreview.net/pdf?id=LO76nlbvNt", "keywords": "Efficient Inference, Early Exiting, Performance Control, Calibration, Classification", "abstract": "Early Exiting (EE) is a promising technique for speeding up inference at the cost of limited performance loss. It adaptively allocates compute budget to data points based on their difficulty by exiting at earlier layers when predictions are confident. In this study, we first present a novel perspective on the EE approach, demonstrating that larger models, when deployed with EE, can achieve higher performance than smaller models while maintaining similar computational costs. As existing EE approaches rely on confidence estimation at each exit point, we further study the impact of overconfidence on the controllability of the compute/performance trade-off. We introduce Performance Control Early Exiting (PCEE), a method that enables accuracy thresholding by basing decisions not on a datapoint's condfidence but on the average accuracy of samples with similar confidence levels from a held-out validation set. In our experiments with MSDNets and Vision Transformer architectures on CIFAR-10, CIFAR-100, and ImageNet, we show that PCEE offers a simple yet computationally efficient approach that provides better control over performance than standard confidence-based approaches, and allows us to scale up model sizes to yield performance gain while reducing the computational cost.", "title_embedding_index": 3415, "title_abs_embedding_index": 3440}, {"title": "Latent Feature Mining for Predictive Model Enhancement with Large Language Models", "link_suffix": "/forum?id=OnBCQgi2LY", "link": "https://openreview.net/forum?id=OnBCQgi2LY", "pdf_link": "https://openreview.net/pdf?id=OnBCQgi2LY", "keywords": "data mining, large language models, feature extraction, criminal justice, AI for social good, AI for healthcare", "abstract": "Predictive modeling often faces challenges due to limited data availability and quality, especially in domains where collected features are weakly correlated with outcomes and where additional data collection is constrained by ethical or practical difficulties. Traditional machine learning (ML) models struggle to incorporate unobserved yet critical factors. In this work, we introduce a novel approach to formulate latent feature mining as text-to-text propositional logical reasoning. We propose FLAME (Faithful Latent FeAture Mining for Predictive Model Enhancement), a framework that leverages large language models (LLMs) to augment observed features with latent features, enhancing the predictive power of ML models in downstream tasks. Our novel approach transforms the latent feature extraction task to a text-to-text propositional reasoning task. Our framework is generalizable across various domains with minimal domain-specific customization, ensuring easy transfer to other areas facing similar challenges in data availability. We validate our framework with two case studies: (1) the criminal justice system, a domain characterized by limited and ethically challenging data collection. (2) the healthcare domain, where patient privacy concerns and the complexity of medical data often limit comprehensive feature collection. Our results show that inferred latent features align well with ground truth labels and significantly enhance the downstream classifier.", "title_embedding_index": 3416, "title_abs_embedding_index": 3441}, {"title": "Multi-session, multi-task neural decoding from distinct cell-types and brain regions", "link_suffix": "/forum?id=IuU0wcO0mo", "link": "https://openreview.net/forum?id=IuU0wcO0mo", "pdf_link": "https://openreview.net/pdf?id=IuU0wcO0mo", "keywords": "neural population, multi-task, transformer, tokenization, two-photon calcium imaging, visual stimuli, cell types", "abstract": "Recent work has shown that scale is important for improved brain decoding, with more data leading to greater decoding accuracy. However, large-scale decoding across many different datasets is challenging because neural circuits are heterogeneous---each brain region contains a unique mix of cellular sub-types, and the responses to different stimuli are diverse across regions and sub-types. It is unknown whether it is possible to pre-train and transfer brain decoding models between distinct tasks, cellular sub-types, and brain regions. To address these questions, we developed a multi-task transformer architecture and trained it on the entirety of the Allen Institute's Brain Observatory dataset. This dataset contains responses from over 100,000 neurons in 6 areas of the brains of mice, observed with two-photon calcium imaging, recorded while the mice observed different types of visual stimuli. Our results demonstrate that transfer is indeed possible -combining data from different sources is beneficial for a number of downstream decoding tasks. As well, we can transfer the model between regions and sub-types, demonstrating that there is in fact common information in diverse circuits that can be extracted by an appropriately designed model. Interestingly, we found that the model's latent representations showed clear distinctions between different brain regions and cellular sub-types, even though it was never given any information about these distinctions. Altogether, our work demonstrates that training a large-scale neural decoding model on diverse data is possible, and this provides a means of studying the differences and similarities between heterogeneous neural circuits.", "title_embedding_index": 3417, "title_abs_embedding_index": 3442}, {"title": "Toward Robust Real-World Audio Deepfake Detection: Closing the Explainability Gap", "link_suffix": "/forum?id=uy9oR0nYCW", "link": "https://openreview.net/forum?id=uy9oR0nYCW", "pdf_link": "https://openreview.net/pdf?id=uy9oR0nYCW", "keywords": "self-supervised learning, explainability, deepfake audio, generalizability", "abstract": "The rapid proliferation of AI-manipulated or generated audio deepfakes poses serious challenges to media integrity and election security. Current AI-driven detection solutions lack explainability and underperform in real-world settings. In this paper, we introduce novel explainability methods for state-of-the-art transformer-based audio deepfake detectors and open-source a novel benchmark for real-world generalizability. By narrowing the explainability gap between transformer-based audio deepfake detectors and traditional methods, our results not only build trust with human experts, but also pave the way for unlocking the potential of citizen intelligence to overcome the scalability issue in audio deepfake detection.", "title_embedding_index": 3418, "title_abs_embedding_index": 3443}, {"title": "Reality Only Happens Once: Single-path Generalization Bounds for Transformers", "link_suffix": "/forum?id=qBSzdiKVcK", "link": "https://openreview.net/forum?id=qBSzdiKVcK", "pdf_link": "https://openreview.net/pdf?id=qBSzdiKVcK", "keywords": "learning theory, transformers, generalization bounds, llms", "abstract": "One of the inherent challenges in deploying transformers on time series is that \\emph{reality only happens once}; namely, one typically only has access to a single trajectory of the data-generating process comprised of non-i.i.d.\\ observations.  We derive non-asymptotic statistical guarantees in this setting through bounds on the \\textit{generalization} of a transformer network at a future-time $t$, given that it has been trained using $N\\le t$ observations from a single perturbed trajectory of a {bounded and exponentially ergodic} Markov process.  We obtain a generalization bound which effectively converges at the rate of $\\mathcal{O}(1/\\sqrt{N})$.  Our bound depends explicitly on the activation function ($\\operatorname{Swish}$, $\\operatorname{GeLU}$, or $\\tanh$ are considered), the number of self-attention heads, depth, width, and norm-bounds defining the transformer architecture.  Our bound consists of three components: (I) The first quantifies the gap between the stationary distribution of the data-generating Markov process and its distribution at time $t$, this term converges exponentially to $0$.  (II) The next term encodes the complexity of the transformer model and, given enough time, eventually converges to $0$ at the rate $\\mathcal{O}(\\log(N)^r/\\sqrt{N})$ for any $r>0$. (III) The third term guarantees that the bound holds with probability at least $1$-$\\delta$, and converges at a rate of $\\mathcal{O}(\\sqrt{\\log(1/\\delta)}/\\sqrt{N})$.Example of (non i.i.d.) data-generating processes which we can treat are the projection of several SDEs onto a compact convex set $C$, and bounded Markov processes satisfying a log-Sobolev inequality.", "title_embedding_index": 3419, "title_abs_embedding_index": 3444}, {"title": "An Information Theory of Compute-Optimal Size Scaling, Emergence, and Plateaus in Language Models", "link_suffix": "/forum?id=PtgfcMcQd5", "link": "https://openreview.net/forum?id=PtgfcMcQd5", "pdf_link": "https://openreview.net/pdf?id=PtgfcMcQd5", "keywords": "Language models, scaling law, emergence, plateauing, Low-Density Parity Check codes, sequential concept learning, composition of skills.", "abstract": "Recent empirical studies show three phenomena with increasing size of language models: compute-optimal size scaling, emergent capabilities, and performance plateauing. We present a simple unified mathematical framework to explain all of these language model scaling phenomena, building on recent skill-text bipartite graph frameworks for semantic learning. Modeling the learning of concepts from texts as an iterative process yields an analogy to iterative decoding of low-density parity check (LDPC) codes in information theory. Thence, drawing on finite-size scaling characterizations of LDPC decoding, we derive the compute-optimal size scaling (Chinchilla rule) for language models. Further, using tools from random network theory, we provide a simple explanation for both emergence of complex skills and plateauing of performance as the size of language models scale. We see multiple plateaus.", "title_embedding_index": 3420, "title_abs_embedding_index": 3445}, {"title": "LNUCB-TA: Linear-nonlinear Hybrid Bandit Learning with Temporal Attention", "link_suffix": "/forum?id=Bwhd7GUyHH", "link": "https://openreview.net/forum?id=Bwhd7GUyHH", "pdf_link": "https://openreview.net/pdf?id=Bwhd7GUyHH", "keywords": "Contextual Multi-Armed Bandit, Exploration-Exploitation Trade-off, Adaptive k-Nearest Neighbors (k-NN), Attention-Based Exploration Rate, Sub-linear Regret", "abstract": "Existing contextual multi-armed bandit (MAB) algorithms struggle to simultaneously capture long-term trends as well as local patterns across all arms, leading to suboptimal performance in complex environments with rapidly changing reward structures. Additionally, they typically employ static exploration rates, which do not adapt to dynamic conditions. To address these issues, we present LNUCB-TA, a hybrid bandit model that introduces a novel nonlinear component (adaptive $k$-Nearest Neighbors ($k$-NN)) designed to reduce time complexity, and an innovative global-and-local attention-based exploration mechanism. Our method incorporates a unique synthesis of linear and nonlinear estimation techniques, where the nonlinear component dynamically adjusts $k$ based on reward variance, thereby effectively capturing spatiotemporal patterns in the data. This is critical for reducing the likelihood of selecting suboptimal arms and accurately estimating rewards while reducing computational time. Also, our proposed attention-based mechanism prioritizes arms based on their historical performance and frequency of selection, thereby balancing exploration and exploitation in real-time without the need for fine-tuning exploration parameters. Incorporating both global attention (based on overall performance across all arms) and local attention (focusing on individual arm performance), the algorithm efficiently adapts to temporal and spatial complexities in the available context. Empirical evaluation demonstrates that LNUCB-TA significantly outperforms state-of-the-art contextual MAB algorithms, including purely linear, nonlinear, and vanilla combination of linear and nonlinear bandits based on cumulative and mean rewards, convergence performance, and demonstrates consistency of results across different exploration rates. Theoretical analysis further proves the robustness of LNUCB-TA with a sub-linear regret bound.", "title_embedding_index": 3421, "title_abs_embedding_index": 3446}, {"title": "Learning Continually by Spectral Regularization", "link_suffix": "/forum?id=Hcb2cgPbMg", "link": "https://openreview.net/forum?id=Hcb2cgPbMg", "pdf_link": "https://openreview.net/pdf?id=Hcb2cgPbMg", "keywords": "plasticity, neural networks, spectral regularization, continual learning", "abstract": "Loss of plasticity is a phenomenon where neural networks can become more difficult to train over the course of learning. Continual learning algorithms seek to mitigate this effect by sustaining good performance while maintaining network trainability. We develop a new technique for improving continual learning inspired by the observation that the singular values of the neural network parameters at initialization are an important factor for trainability during early phases of learning. From this perspective, we derive a new spectral regularizer for continual learning that better sustains these beneficial initialization properties throughout training. In particular, the regularizer keeps the maximum singular value of each layer close to one. Spectral regularization directly ensures that gradient diversity is maintained throughout training, which promotes continual trainability, while minimally interfering with performance in a single task. We present an experimental analysis that shows how the proposed spectral regularizer can sustain trainability and performance across a range of model architectures in continual supervised and reinforcement learning settings. Spectral regularization is less sensitive to hyperparameters while demonstrating better training in individual tasks, sustaining trainability as new tasks arrive, and achieving better generalization performance..", "title_embedding_index": 3422, "title_abs_embedding_index": 3447}, {"title": "SMI-TED: A large-scale foundation model for materials and chemistry", "link_suffix": "/forum?id=Yq8At31hLi", "link": "https://openreview.net/forum?id=Yq8At31hLi", "pdf_link": "https://openreview.net/pdf?id=Yq8At31hLi", "keywords": "SMILES, foundation model, molecular property prediction, classification, molecular reconstruction, synthesis yield prediction", "abstract": "We present SMI-TED (SMILE Transformer Encoder Decoder), a large-scale foundation model for materials and chemistry, trained on a massive dataset of 91 million SMILES samples (4 billion molecular tokens) from PubChem using self-supervised learning. Our encoder-decoder architecture enables a wide range of complex tasks, including the prediction of quantum chemical properties and reaction yields. We offer two model variants, with 289M and $8 \\times 289M$ parameters, respectively, to accommodate different use cases. Our model achieves state-of-the-art results across multiple benchmark datasets, demonstrating its versatility and effectiveness. Notably, our model's latent space exhibits compositionality and separability, essential properties for higher-level reasoning tasks and few-shot learning capabilities. To facilitate further research and applications, we make our model weights and source code publicly available on HuggingFace and GitHub, respectively.", "title_embedding_index": 3423, "title_abs_embedding_index": 3448}, {"title": "Representing speech through autoregressive prediction of cochlear tokens", "link_suffix": "/forum?id=TQdg1X6eqm", "link": "https://openreview.net/forum?id=TQdg1X6eqm", "pdf_link": "https://openreview.net/pdf?id=TQdg1X6eqm", "keywords": "audio, speech, biology-inspired model, autoregressive prediction, cochlea", "abstract": "We introduce a biologically-inspired model for encoding speech through an autoregressive prediction objective applied to input representations modeled after the human cochlea.\nOur modeling framework is inspired by the human auditory processing hierarchy. The first stage of our framework transforms the raw audio waveform to a time-frequency representation inspired by the human cochlea, with an intermediary step that effectively discretizes the audio representations (cochlear tokens). The second stage of our model learns a simple, yet powerful, autoregressive sequence model over the discretized audio input.\nWe demonstrate that our model learns meaningful representations of phonemes and word identities, and state-of-the-art representations of lexical semantic similarity. In addition, our model shows competitive performance on several downstream audio tasks from the SUPERB benchmark. In addition to our model\u2019s strong representational capabilities, we demonstrate our model's ability to generate continuations of audio at various temporal scales, which can be visualized in a cochleagram time-frequency space to provide insights into the model's predictions.\nOur model provides a novel framework for speech representation learning, aiming to advance the development of more human-like models that flexibly and efficiently handles a range of speech-based tasks.", "title_embedding_index": 3424, "title_abs_embedding_index": 3449}]