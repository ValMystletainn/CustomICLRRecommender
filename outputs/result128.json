[{"title": "KnowHalu: Multi-Form Knowledge Enhanced Hallucination Detection", "link_suffix": "/forum?id=Kap9vaGKwF", "link": "https://openreview.net/forum?id=Kap9vaGKwF", "pdf_link": "https://openreview.net/pdf?id=Kap9vaGKwF", "keywords": "Hallucination Detection; Large Language Model; Factual Checking; Multi-Form Knowledge", "abstract": "As large language models (LLMs) become increasingly integral to a wide array of applications, ensuring the factual accuracy of their outputs and mitigating hallucinations is paramount. Current approaches, which primarily rely on self-consistency checks or post-hoc fact-checking, often fall short by disregarding the nuanced structure of queries and the diverse forms of contextual knowledge required for accurate response generation.\nTo address these shortcomings, we introduce KnowHalu (pronounced \u201cNo Halu\u201d), the first multi-form knowledge-based hallucination detection framework. We also introduce a new category of hallucinations, off-target hallucinations, which occur when responses are factually accurate but irrelevant or nonspecific to the query (e.g., answering \"What\u2019s the primary language in Barcelona?\" with \"European language\").\nIn particular, KnowHalu employs a rigorous two-phase process to detect hallucinations. In the first phase, it isolates off-target hallucinations by analyzing the semantic alignment between the response and the query. In the second phase, it conducts a novel multi-form knowledge-based fact-checking through a comprehensive pipeline of reasoning and query decomposition, knowledge retrieval, knowledge form optimization, judgment generation, and judgment aggregation.\nExtensive evaluations demonstrate that KnowHalu significantly surpasses state-of-the-art (SOTA) baselines across diverse tasks, achieving over 15% improvement in question answering (QA) and 6% in summarization tasks when applied to the same underlying LLM. These results underscore the effectiveness and versatility of KnowHalu, setting a new benchmark for hallucination detection and paving the way for safer and more reliable LLM applications.", "title_embedding_index": 6350, "title_abs_embedding_index": 6375}, {"title": "Mr.Steve: Instruction-Following Agents in Minecraft with What-Where-When Memory", "link_suffix": "/forum?id=CjXaMI2kUH", "link": "https://openreview.net/forum?id=CjXaMI2kUH", "pdf_link": "https://openreview.net/pdf?id=CjXaMI2kUH", "keywords": "Generalist Agents, Minecraft, Place Event Memory", "abstract": "Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce Mr.Steve (Memory Recall STEVE-1), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, STEVE-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods, and we are releasing our code to support further research.", "title_embedding_index": 6351, "title_abs_embedding_index": 6376}, {"title": "Spatial-temporal Graph Attention Network for Forex Forecasting with Hierarchical Transformer", "link_suffix": "/forum?id=5x9kfRXhBd", "link": "https://openreview.net/forum?id=5x9kfRXhBd", "pdf_link": "https://openreview.net/pdf?id=5x9kfRXhBd", "keywords": "Graph Attention Networks, Transformer, Forex Forecasting", "abstract": "The foreign exchange market, with its daily trading volume reaching nearly trillions of dollars, presents significant opportunities for the application of advanced predictive analytics. Traditional exchange rate forecasting methods often overlook the interdependencies between currencies and struggle with long-range data dependencies, leading to challenges in capturing the true market dynamics. To overcome these limitations, this paper introduces a novel Spatial-Temporal Graph Attention Network with Hierarchical Transformer (STGAT). Our model innovatively combines spatial graph convolutions with a dual-view temporal transformer-based mechanism, utilizing a Temporal Linearity Graph Attention Network (TLGAT) to account for currency relations in a time-sensitive manner. By integrating a linear attention mechanism for enhanced efficiency and capturing both local and global sequential data embeddings, STGAT provides a framework based on a hierarchical transformer for predicting exchange rates. We validate our approach on exchange rates of seventeen currencies over 2,092 trading days, demonstrating superior performance compared to state-of-the-art models.", "title_embedding_index": 6352, "title_abs_embedding_index": 6377}, {"title": "Learning and Steering Game Dynamics Towards Desirable Outcomes", "link_suffix": "/forum?id=40BTVvYQWZ", "link": "https://openreview.net/forum?id=40BTVvYQWZ", "pdf_link": "https://openreview.net/pdf?id=40BTVvYQWZ", "keywords": "game dynamics, system identification, model predictive control, sum of squares optimization, steering", "abstract": "Game dynamics, which describe how agents' strategies evolve over time based on past interactions, can exhibit a variety of undesirable behaviours, including convergence to suboptimal equilibria, cycling, and chaos. While central planners can employ incentives to mitigate such behaviors and steer game dynamics towards desirable outcomes, the effectiveness of such interventions critically relies on accurately predicting agents' responses to these incentives---a task made particularly challenging when the underlying dynamics are unknown and observations are limited. To address this challenge, this work introduces the Side Information Assisted Regression with Model Predictive Control (SIAR-MPC) framework. We extend the recently introduced SIAR method to incorporate the effect of control, enabling it to utilize side-information constraints inherent to game theoretic applications to model agent responses to incentives from scarce data. MPC then leverages this model to implement adaptive incentive adjustments. Our experiments demonstrate the efficiency of SIAR-MPC in guiding systems towards socially optimal equilibria, stabilizing chaotic and cycling behaviors. Comparative analyses in data-scarce settings show SIAR-MPC's superior performance compared to pairing MPC with state-of-the-art alternatives like Sparse Identification of Nonlinear Dynamics (SINDy) and Physics Informed Neural Networks (PINNs).", "title_embedding_index": 6353, "title_abs_embedding_index": 6378}, {"title": "Intervention-based Causal Discrimination Discovery and Removal", "link_suffix": "/forum?id=1XzTxtezgj", "link": "https://openreview.net/forum?id=1XzTxtezgj", "pdf_link": "https://openreview.net/pdf?id=1XzTxtezgj", "keywords": "Fairness, Causal inference, Intervention-based metric", "abstract": "Causal inference is a recent and widely adopted paradigm to deal with algorithmic discrimination. Building on Pearl's structure causal model, several causality-based fairness notions have been developed, which estimates the unfair causal effects from the sensitive attribute to the outcomes by incorporating the intervention or counterfactual operators. Among them, interventional fairness (i.e., $K$-Fair) stands out as the most fundamental and broadly applicable concept that is computable from observantional data. However, existing interventional fairness notions fail to accurately evaluate causal fairness, due to their following inherent limitations: (i) the causal effects evaluated by interventional fairness cannot be uniquely computed; (ii) the violation of interventional fairness being zero is not a sufficient condition for a causally fair model. To address these issues, we firstly propose a novel causality-based fairness notion called post-Intervention Cumulative Ratio Disparity (ICRD) to assess causal fairness of the decision models. Subsequently, we present a fairness framework (ICCFL) based on the proposed ICRD metric. ICCFL firstly generates interventional samples, and then computes the differentiable approximation of the ICRD to train a causally fair model. Both theoretical and empirical results demonstrate that the proposed ICRD effectively assesses causal fairness, and ICCFL can better balance accuracy and fairness.", "title_embedding_index": 6354, "title_abs_embedding_index": 6379}, {"title": "Semantic or Covariate? A Study on the Intractable Case of Out-of-Distribution Detection", "link_suffix": "/forum?id=uWUovmBRUq", "link": "https://openreview.net/forum?id=uWUovmBRUq", "pdf_link": "https://openreview.net/pdf?id=uWUovmBRUq", "keywords": "Out-of-Distribution Detection, Definition, Theoretical Analysis", "abstract": "The primary goal of out-of-distribution (OOD) detection tasks is to identify inputs with semantic shifts, i.e., if samples from novel classes are absent in the in-distribution (ID) dataset used for training, we should reject these OOD samples rather than misclassifying them into existing ID classes. However, we find the current definition of \"semantic shift\" is ambiguous, which renders certain OOD testing protocols intractable for the post-hoc OOD detection methods based on a classifier trained on the ID dataset. In this paper, we offer a more precise definition of the Semantic Space and the Covariate Space for the ID distribution, allowing us to theoretically analyze which types of OOD distributions make the detection task intractable. To avoid the flaw in the existing OOD settings, we further define the \"Tractable OOD\" setting which ensures the distinguishability of OOD and ID distributions for the post-hoc OOD detection methods. Finally, we conduct several experiments to demonstrate the necessity of our definitions and validate the correctness of our theorems.", "title_embedding_index": 6355, "title_abs_embedding_index": 6380}, {"title": "CLF: Curve Line Fitting Neural Network Based On Bezier Curve", "link_suffix": "/forum?id=dxMffCAd4w", "link": "https://openreview.net/forum?id=dxMffCAd4w", "pdf_link": "https://openreview.net/pdf?id=dxMffCAd4w", "keywords": "CLF, MLP, Interpretable Neural Networks", "abstract": "The Multilayer Perceptron (MLP) serves as a fundamental architecture in deep learning, leveraging the universal function approximation theorem through linear regression combined with activation functions. Despite its widespread use, the inclusion of activation functions contributes to the inherent nature of MLPs as ``black boxes,\" limiting their interpretability. In this paper, we propose a novel Curve Line Fitting (CLF) network, which introduces Bezier curve fitting to directly address nonlinear distributions. By replacing traditional linear regression with Bezier curve regression, the CLF network offers a more efficient means of fitting target distributions. Additionally, the removal of activation functions makes the CLF model fully interpretable, enabling clear insights into the relationships between input dimensions and target distributions, as well as the interdependencies across different dimensions. (Sample code for the CLF model will be made available on GitHub.)", "title_embedding_index": 6356, "title_abs_embedding_index": 6381}, {"title": "NIMBA : Towards Robust and Principled Processing of Point Clouds With SSMs", "link_suffix": "/forum?id=aHSoKImhc2", "link": "https://openreview.net/forum?id=aHSoKImhc2", "pdf_link": "https://openreview.net/pdf?id=aHSoKImhc2", "keywords": "point clouds, SSMs, state-space models, Mamba, 3D, transformers, pointcloud", "abstract": "Transformers have become dominant in large-scale deep learning tasks across various domains, including text, 2D and 3D vision. However, the quadratic complexity of their attention mechanism limits their efficiency as the sequence length increases, particularly in high-resolution 3D data such as point clouds. Recently, state space models (SSMs) like Mamba have emerged as promising alternatives, offering linear complexity, scalability, and high performance in long-sequence tasks. The key challenge in the application of SSMs in this domain lies in reconciling the non-sequential structure of point clouds with the inherently directional (or bi-directional) order-dependent processing of recurrent models like Mamba. To achieve this, previous research proposed reorganizing point clouds along multiple directions or predetermined paths in 3D space, concatenating the results to produce a single 1D sequence capturing different views. In our work we introduce a method to convert point clouds into 1D sequences that maintains 3D spatial structure with no need for data replication, allowing Mamba\u2019s sequential processing to be applied effectively in an almost permutation-invariant manner. In contrast to other works, we found that our method does not require positional embeddings, and allows for shorter sequence lengths while still achieving state-of-the-art results in ModelNet40 and ScanObjectNN datasets and surpassing Transformer-based models in both accuracy and efficiency.", "title_embedding_index": 6357, "title_abs_embedding_index": 6382}, {"title": "OPTIMIZED SINGLE EEG CHANNEL SELECTION FOR DETECTING MAJOR DEPRESSIVE DISORDER", "link_suffix": "/forum?id=p30YulvDbj", "link": "https://openreview.net/forum?id=p30YulvDbj", "pdf_link": "https://openreview.net/pdf?id=p30YulvDbj", "keywords": "Major depressive disorder, deep learning, electroencephalogram, EEG, single-channel", "abstract": "Major depressive disorder (MDD) or depression is a chronic mental illness that significantly impacts individuals' well-being and is often diagnosed at advanced stages, increasing the risk of suicide. Current diagnostic practices, which rely heavily on subjective assessments and patient self-reports, are often hindered by challenges such as under-reporting and the failure to detect early, subtle symptoms. Early detection of MDD is crucial and requires monitoring vital signs in everyday living conditions. Electroencephalogram (EEG) is a valuable tool for monitoring brain activity, offering critical insights into MDD and its underlying neurological mechanisms. While traditional EEG systems typically involve multiple channels for recording, making them impractical for home-based monitoring, wearable sensors can effectively capture single-channel EEG data. However, generating meaningful features from this data poses challenges due to the need for specialized domain knowledge and significant computational power, which can hinder real-time processing. To address these issues, our study focuses on developing a deep learning model for the binary classification of MDD using single-channel EEG data. We focused on specific channels from various brain regions, including central (C3), frontal (Fp1), occipital (O1), temporal (T4), and parietal (P3). Our study found that the channels Fp1, C3, and O1 achieved an impressive accuracy of 88% when analyzed using a Convolutional Neural Network (CNN) with leave-one-subject-out cross-validation. Our study highlights the potential of utilizing single-channel EEG data for reliable MDD diagnosis, providing a less intrusive and more convenient wearable solution for mental health assessment.", "title_embedding_index": 6358, "title_abs_embedding_index": 6383}, {"title": "Unbounded Activations for Constrained Monotonic Neural Networks", "link_suffix": "/forum?id=N1DKrLIKhT", "link": "https://openreview.net/forum?id=N1DKrLIKhT", "pdf_link": "https://openreview.net/pdf?id=N1DKrLIKhT", "keywords": "Monotonic, Neural Networks", "abstract": "Monotonic multi-layer perceptrons (MLPs) are crucial in applications requiring interpretable and trustworthy machine learning models, particularly in domains where decisions must adhere to specific input-output relationships. Traditional approaches that build monotonic MLPs with universal approximation guarantees often rely on constrained weights and bounded activation functions, which suffer from optimization issues. \nIn this work, we prove that non-negative constrained weights MLPs with activations that saturate on alternating sides are universal approximators for the class of monotonic functions. Thanks to this new result, we show that non-positive constrained weights MLPs with convex monotone activations, contrary to their non-negative constrained counterpart, are universal approximators. \nDespite such guarantees, we also show that such classes of MLPs are hard to optimize. Therefore, we propose a novel parametrization that eliminates the need for weight constraints, allowing the network to dynamically adjust activations based on weight signs, thus enhancing optimization stability and performance. \nExperiments demonstrate that our approach maintains theoretical guarantees and significantly outperforms existing monotonic architectures in approximation accuracy.", "title_embedding_index": 6359, "title_abs_embedding_index": 6384}, {"title": "Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression", "link_suffix": "/forum?id=ReItdfwMcg", "link": "https://openreview.net/forum?id=ReItdfwMcg", "pdf_link": "https://openreview.net/pdf?id=ReItdfwMcg", "keywords": "instrumental variable regression, DFIV, deep neural networks, minimax optimality", "abstract": "We provide a convergence analysis of deep feature instrumental variable (DFIV) regression, a nonparametric approach to IV regression using data-adaptive features learned by deep neural networks in two stages. We prove that the DFIV algorithm achieves the minimax optimal learning rate when the target structural function lies in a Besov space. This is shown under standard nonparametric IV assumptions, and an additional assumption on the regularity of the conditional distribution of the covariate given the instrument. We further demonstrate that DFIV, as a data-adaptive algorithm, is superior to fixed-feature (kernel or sieve) IV methods in two ways. First, when the target function possesses low spatial homogeneity (i.e., it has both smooth and spiky/discontinuous regions), DFIV still achieves the optimal rate, while fixed-feature methods are shown to be strictly suboptimal. Second, comparing with kernel-based two-stage regression estimators, DFIV is provably more data efficient in the Stage 1 samples.", "title_embedding_index": 6360, "title_abs_embedding_index": 6385}, {"title": "Neural Solver Selection for Combinatorial Optimization", "link_suffix": "/forum?id=CFLEIeX7iK", "link": "https://openreview.net/forum?id=CFLEIeX7iK", "pdf_link": "https://openreview.net/pdf?id=CFLEIeX7iK", "keywords": "learn to optimize", "abstract": "Machine learning has increasingly been employed to solve NP-hard combinatorial optimization problems, resulting in the emergence of neural solvers that demonstrate remarkable performance, even with minimal domain-specific knowledge. To date, the community has created numerous open-source neural solvers with distinct motivations and inductive biases. While considerable efforts are devoted to designing powerful single solvers, our findings reveal that existing solvers typically demonstrate complementary performance across different problem instances. This suggests that significant improvements could be achieved through effective coordination of neural solvers at the instance level.\nIn this work, we propose the first general framework to coordinate the neural solvers, which involves feature extraction, selection model, and selection strategy, aiming to allocate each instance to the most suitable solvers. To instantiate, we collect several typical neural solvers with state-of-the-art performance as alternatives, and explore various methods for each component of the framework. We evaluated our framework on two extensively studied combinatorial optimization problems, Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP). Experimental results show that the proposed framework can effectively distribute instances and the resulting composite solver can achieve significantly better performance (e.g., reduce the optimality gap by 0.88% on TSPLIB and 0.71% on CVRPLIB) than the best individual neural solver with little extra time cost.", "title_embedding_index": 6361, "title_abs_embedding_index": 6386}, {"title": "MCTBench: Multimodal Cognition towards Text-Rich Visual Scenes Benchmark", "link_suffix": "/forum?id=BVACdtrPsh", "link": "https://openreview.net/forum?id=BVACdtrPsh", "pdf_link": "https://openreview.net/pdf?id=BVACdtrPsh", "keywords": "Multimodal Benchmark, MLLM, OCR, Cognition, perception", "abstract": "The comprehension of text-rich visual scenes has become a focal point for evaluating Multi-modal Large Language Models (MLLMs) due to their widespread applications. Current benchmarks tailored to the scenario emphasize perceptual capabilities, while overlooking the assessment of cognitive abilities. To address this limitation, we introduce a $\\textbf{M}$ultimodal benchmark towards $\\textbf{T}$ext-rich visual scenes, to evaluate the $\\textbf{C}$ognitive capabilities of MLLMs through visual reasoning and content-creation tasks ($\\textbf{MCTBench}$). To mitigate potential evaluation bias from the varying distributions of datasets, MCTBench incorporates several perception tasks (e.g., scene text recognition) to ensure a consistent comparison of both the cognitive and perceptual capabilities of MLLMs. To improve the efficiency and fairness of content-creation evaluation, we conduct an automatic evaluation pipeline. Evaluations of various MLLMs on MCTBench reveal that, despite their impressive perceptual capabilities, their cognition abilities require enhancement. We hope MCTBench will offer the community an efficient resource to explore and enhance cognitive capabilities towards text-rich visual scenes.", "title_embedding_index": 6362, "title_abs_embedding_index": 6387}, {"title": "A Novel Listwise Alignment Approach for Language Models with Explicit Rewards", "link_suffix": "/forum?id=Ek50sQQI1w", "link": "https://openreview.net/forum?id=Ek50sQQI1w", "pdf_link": "https://openreview.net/pdf?id=Ek50sQQI1w", "keywords": "large language models, preference alignment, listwise optimization objective", "abstract": "Existing alignment techniques, including Direct Preference Optimization (DPO), are primarily designed for pairwise preference data where rewards are inferred rather than explicitly provided. In this paper, we propose a comprehensive framework for aligning large language models (LLMs) by introducing a new optimization objective that facilitates the processing of reward datasets, which consist of a list of responses explicitly marked with scalar preference scores. Our contribution includes the development of a novel algorithm, termed Soft Preference Optimization (LPO), which allows for the direct derivation of an LLM policy from both reward and preference datasets. At the heart of LPO is a unique listwise preference optimization objective formulated using an exponential-logarithmic function and an adaptive loss coefficient, which effectively integrates listwise preference signals into the LLM. We assess the efficacy of our approach under both reward and preference scenarios using different sizes of Mistral models. Experimental results indicate that our method outperforms several preference-based benchmarks, particularly when reward datasets are utilized. Additionally, our method demonstrates a significant advantage over DPO in intricate reasoning tasks, such as mathematical problem-solving and coding.", "title_embedding_index": 6363, "title_abs_embedding_index": 6388}, {"title": "LightRAG: Simple and Fast Retrieval-Augmented Generation", "link_suffix": "/forum?id=bbVH40jy7f", "link": "https://openreview.net/forum?id=bbVH40jy7f", "pdf_link": "https://openreview.net/pdf?id=bbVH40jy7f", "keywords": "Retrieval-Augmented Generation, Large Language Model, Graph", "abstract": "Retrieval-Augmented Generation (RAG) systems enhance large language models (LLMs) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing RAG systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose LightRAG, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective in dynamic environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. For the reproducibility of our results, we have made our LightRAG available anonymously at:https://anonymous.4open.science/r/LightRAG-2BEE.", "title_embedding_index": 6364, "title_abs_embedding_index": 6389}, {"title": "GeoSplating: Towards Geometry Guided Gaussian Splatling for Physically-based Inverse Rendering", "link_suffix": "/forum?id=l5VA9wHJ8u", "link": "https://openreview.net/forum?id=l5VA9wHJ8u", "pdf_link": "https://openreview.net/pdf?id=l5VA9wHJ8u", "keywords": "Geometry, Gaussian Splatling, Physically-based Inverse Rendering", "abstract": "We consider the problem of physically-based inverse rendering using 3D Gaussian Splatting (3DGS) representations Kerbl et al. (2023b). While recent 3DGS methods have achieved remarkable results in novel view synthesis (NVS), accurately capturing high-fidelity geometry, physically interpretable materials, and lighting remains challenging, as it requires precise geometry modeling to provide accurate surface normals, along with physically-based rendering (PBR) techniques to ensure correct material and lighting disentanglement. Previous 3DGS methods resort to approximating surface normal, which often struggle with noisy local geometry, leading to inaccurate normal estimation and suboptimal material-lighting decomposition. In this paper, we introduce GeoSplatting, a novel hybrid representation that augments 3DGS with explicit geometric guidance and differentiable PBR equations. Specifically, we bridge isosurface and 3DGS together, where we first extract the isosurface mesh from a scalar field, then convert it into 3DGS points and formulate PBR equations for them in a fully differentiable manner. In GeoSplatting, 3DGS is grounded on the mesh geometry, enabling precise surface normal modeling, which facilitates the use of PBR frameworks for material decomposition. This approach further maintains the efficiency and quality of NVS from 3DGS while ensuring accurate geometry from the isosurface. Comprehen sive evaluations across diverse datasets demonstrate the superior of GeoSplatting, consistently outperforming existing methods both quantitatively and qualitatively.", "title_embedding_index": 6365, "title_abs_embedding_index": 6390}, {"title": "Online importance sampling for stochastic gradient optimization", "link_suffix": "/forum?id=3AQAUMObuc", "link": "https://openreview.net/forum?id=3AQAUMObuc", "pdf_link": "https://openreview.net/pdf?id=3AQAUMObuc", "keywords": "SGD, Importance sampling", "abstract": "Machine learning optimization commonly relies on stochastic gradient descent, where the accuracy of gradient estimation is crucial for model performance. Rather than relying on uniform sampling, importance sampling can improve accuracy by focusing on data points that have more significant impact on learning. However, existing methods for importance sampling face challenges with computational efficiency and integration into practical machine learning workflows.\nIn this work, we introduce a novel adaptive metric based on the loss derivative wrt the network output that can be used for both importance sampling and data pruning. Our metric not only enhances gradient accuracy by prioritizing influential data points but also enables effective pruning by identifying and removing data that contributes minimally to training. We propose an efficient adaptive algorithm that leverages this metric with minimal computational overhead. Our evaluations on classification and regression tasks demonstrate improved convergence and reduced training data requirements, validating the efficacy of our approach.", "title_embedding_index": 6366, "title_abs_embedding_index": 6391}, {"title": "GAD-VLP: Geometric Adversarial Detection for Vision-Language Pre-Trained Models", "link_suffix": "/forum?id=4HL2aiDV97", "link": "https://openreview.net/forum?id=4HL2aiDV97", "pdf_link": "https://openreview.net/pdf?id=4HL2aiDV97", "keywords": "Adversarial detection, Geometric Distance, Multimodal models", "abstract": "Vision-language pre-trained models (VLPs) have been deployed in numerous real-world applications; however, these models are vulnerable to adversarial attacks. Existing adversarial detection methods have shown their efficacy in single-modality settings (either vision or language), while their performance on VLPs, as multimodal models, remains uncertain. In this work, we propose a novel aspect of adversarial detection called GAD-VLP, which detects adversarial examples by exploiting vision and joint vision-language embeddings within VLP architectures. We leverage the geometry of the embedding space and demonstrate the unique characteristics of adversarial regions within these models. We explore the embedding space of the vision modality or the combined vision-language modalities, depending on the type of VLP, to identify adversarial examples. Some of the geometric methods do not require explicit knowledge of the adversary's targets in downstream tasks (e.g., zero-shot classification or image-text retrieval), offering a model-agnostic detection framework applicable across VLPs. Despite its simplicity, we demonstrate that these methods deliver a nearly perfect detection rate on state-of-the-art adversarial attacks against VLPs, including both separate and combined attacks on the vision and joint modalities.", "title_embedding_index": 6367, "title_abs_embedding_index": 6392}, {"title": "Mitigating Object Hallucination in Large Vision Language Model with Human-Free Reinforcement Learning", "link_suffix": "/forum?id=bO31lfEdos", "link": "https://openreview.net/forum?id=bO31lfEdos", "pdf_link": "https://openreview.net/pdf?id=bO31lfEdos", "keywords": "Reinforcement Learning, Vision Large Language Model, Large Language Model, Hallucination", "abstract": "Large Vision-Language Models (LVLMs) have excelled in joint visual and language understanding, particularly in generating detailed image captions. However, they still struggle with object hallucination, where non-existent objects are described, especially in long captions. While fine-tuning through supervised learning with enhanced datasets or reinforcement learning from human feedback can alleviate this issue, these methods demand considerable human effort, limiting scalability. This paper addresses this challenge by introducing a human-free framework to mitigate object hallucination in LVLMs for image captioning, utilizing reinforcement learning driven exclusively by automatic natural language processing metrics. We demonstrate that the following framework can effectively mitigate hallucination: (1) caption generation is formulated as a Markov Decision Process (MDP); (2) minimizing hallucination while maintaining caption quality is guided by a reward function, combining a proposed \\textit{F1Score} with a penalty on Kullback\u2013Leibler divergence from the pre-trained model; (3) fine-tuning the LVLM within the MDP framework can be performed directly by Proximal Policy Optimization (PPO) with careful attention to architectural details. Extensive experiments demonstrate a significant reduction in hallucination by up to 41% while preserving the caption quality compared to the baseline model, InstructBLIP, on the COCO dataset. This improvement is reflected in consistent gains in object coverage and accuracy across various models and datasets. Notably, our method achieves comparable or superior performance to alternative approaches, all without requiring any human involvement.", "title_embedding_index": 6368, "title_abs_embedding_index": 6393}, {"title": "Energy-based Model Training Objective Robust to Inaccurate SGLD Samples", "link_suffix": "/forum?id=9uELGn17Db", "link": "https://openreview.net/forum?id=9uELGn17Db", "pdf_link": "https://openreview.net/pdf?id=9uELGn17Db", "keywords": "EBM, Energy-based Model, Stochastic Gradient Langevin Dynamics, SGLD, Self-Normalizd Importance Sampling, SNIS, Joint Energy-based Model, JEM", "abstract": "We propose a novel technique for training Energy-based Models (EBMs), which are neural network-based models capable of modeling complex probability distributions. The standard approach to EBM training relies on samples generated from the modeled distribution using Stochastic Gradient Langevin Dynamics (SGLD). However, this training method is known to be unstable, as SGLD may fail to provide reliable samples. Compared to other popular generative models, EBMs have the advantage of directly evaluating unnormalized log-likelihoods for input observations x. Unfortunately, trained EBMs typically fail to robustly estimate the likelihoods for distant input observations, as the training procedure only considers the gradients of the log-likelihood with respect to x and not the actual log-likelihood values. In this paper, we propose a generalization of the standard training objective that addresses both issues. The proposed objective explicitly incorporates estimated unscaled log-likelihoods, allowing the EBM to estimate the likelihoods more reliably. Notably, EBMs do not need to (and as we point out, cannot) correctly estimate log-likelihoods to be effective for sampling using the non-convergent SGLD procedure. The proposed objective is controlled by a single hyper-parameter, which balances the trade-off between the quality of the estimated log-likelihoods and the generated samples. A specific setting of this parameter recovers the standard EBM training objective. Moreover, the proposed objective enhances robustness to unreliable SGLD samples by de-weighting contribution from samples that appear inconsistent with the modeled distribution, i.e., samples with very low estimated likelihoods compared to other generated samples or real training data. We demonstrate the improvement in log-likelihood modeling on toy datasets and enhanced stability in a real data scenario, where this stability leads to better performance.", "title_embedding_index": 6369, "title_abs_embedding_index": 6394}, {"title": "Pullback Flow Matching on Data Manifolds", "link_suffix": "/forum?id=mBXLtNKpeQ", "link": "https://openreview.net/forum?id=mBXLtNKpeQ", "pdf_link": "https://openreview.net/pdf?id=mBXLtNKpeQ", "keywords": "Pullback Geometry, Manifold Learning, Generative Modeling, Neural ODEs, Data manifolds", "abstract": "We propose Pullback Flow Matching (PFM), a novel framework for generative modeling on data manifolds. Unlike existing methods that assume or learn restrictive closed-form manifold mappings for training Riemannian Flow Matching (RFM) models, PFM leverages pullback geometry and isometric learning to preserve the underlying manifold\u2019s geometry while enabling efficient generation and precise interpolation in latent space. This approach not only facilitates closed-form mappings on the data manifold but also allows for designable latent spaces, using assumed metrics on both data and latent manifolds. By enhancing isometric learning through Neural ODEs and proposing a scalable training objective, we achieve a latent space more suitable for interpolation, leading to improved manifold learning and generative performance. We demonstrate PFM\u2019s effectiveness through applications in synthetic data, protein dynamics and protein sequence data, generating novel proteins with specific properties. This method shows strong potential for drug discovery and materials science, where generating novel samples with specific properties is of great interest.", "title_embedding_index": 6370, "title_abs_embedding_index": 6395}, {"title": "Enhancing Linear Bound Tightness in Neural Network Verification via Sampling-Based Underestimation", "link_suffix": "/forum?id=qQS2VuHb74", "link": "https://openreview.net/forum?id=qQS2VuHb74", "pdf_link": "https://openreview.net/pdf?id=qQS2VuHb74", "keywords": "Formal Verification of Neural Networks; Probabilistic approaches; Linear Relaxation-Based Perturbation Analysis", "abstract": "We present $\\texttt{PT-LiRPA}$ (Probabilistically Tightened LiRPA), a novel approach that enhances existing linear relaxation-based perturbation analysis (LiRPA) methods for neural network verification. $\\texttt{PT-LiRPA}$ combines LiRPA approaches with a sampling-based underestimation technique to compute probabilistically optimal intermediate bounds, resulting in tighter linear lower and upper bounds. Notably, we show that this approach preserves the soundness of verification results while significantly tightening the bounds for generic non-linear functions. Additionally, we introduce a new metric, $\\Delta^*$, to quantify the tightness for LiRPA bounds and to bound the magnitude of the possible error in the sample-based overestimation, thus complementing the probabilistic bound of statistical results we use. Our empirical evaluation, conducted on several state-of-the-art benchmarks, including those from the International Verification of Neural Networks Competition, demonstrates that $\\texttt{PT-LiRPA}$ achieves higher or comparable verified accuracy with lower verification times. The significantly tighter bounds and better efficiency allow us to verify instances where state-of-the-art methods could not provide a specific answer.", "title_embedding_index": 6371, "title_abs_embedding_index": 6396}, {"title": "An Information-Theoretic Approach to Diversity Evaluation of Prompt-based Generative Models", "link_suffix": "/forum?id=IJiTI0fB0e", "link": "https://openreview.net/forum?id=IJiTI0fB0e", "pdf_link": "https://openreview.net/pdf?id=IJiTI0fB0e", "keywords": "Information measures, diversity evaluation, conditional generative models", "abstract": "Text-guided sample generation schemes are commonly evaluated based on the quality of generated data and their alignment with the input text prompt. On the other hand, several applications of prompt-based generative models require adequate diversity in the generated data to ensure the models' capability of generating samples possessing a variety of features. However, the existing diversity metrics are designed for unconditional generative models, and thus cannot distinguish the diversity of created data because of the variety of text prompts and the diversity contributed by the generation model. In this work, our goal is to quantify the prompt-caused and model-caused diversity of samples produced by a prompt-based generative model. We propose an information-theoretic approach to the model's internal diversity quantification, where we decompose the kernel-based entropy $H(X)$ of generated data $X$, to the sum of conditional entropy $H(X|T)$ given text variable $T$ and the mutual information $I(X;T)$ between the text and data variables. We utilize the conditional entropy $H(X|T)$ to define theConditional-Vendiscore for the quantification of the internal diversity of the model and prove theoretical results interpreting the application of these scores to mixture distributions. We perform several numerical experiments to show the correlation between the Conditional-Vendi score and standard text-based generative models. We also demonstrate the application of the proposed framework to detect inequities in text-based sample generation schemes.", "title_embedding_index": 6372, "title_abs_embedding_index": 6397}, {"title": "Optimizing Attention with Mirror Descent: Generalized Max-Margin Token Selection", "link_suffix": "/forum?id=9M5georQ9T", "link": "https://openreview.net/forum?id=9M5georQ9T", "pdf_link": "https://openreview.net/pdf?id=9M5georQ9T", "keywords": "Attention Mechanism, Mirror Descent, Implicit Regularization, Transformers", "abstract": "Attention mechanisms have revolutionized numerous domains of artificial intelligence, including natural language processing and computer vision, by enabling models to selectively focus on relevant parts of the input data. Building on recent results characterizing the optimization dynamics of gradient descent (GD) and the structural properties of its preferred solutions in attention-based models, this paper explores the convergence properties and implicit bias of a family of mirror descent (MD) algorithms designed for softmax attention mechanisms, with the potential function chosen as the $p$-th power of the $\\ell_p$-norm. Specifically, we show the directional convergence of these algorithms to a generalized hard-margin SVM with an $\\ell_p$-norm objective when applied to a classification problem using a one-layer softmax attention model. Our theoretical results demonstrate that these algorithms not only converge directionally to the generalized max-margin solutions but also do so at a rate comparable to that of traditional GD in simpler models, despite the highly nonlinear and nonconvex nature of the present problem. Additionally, we delve into the joint optimization dynamics of the key-query matrix and the decoder, establishing conditions under which this complex joint optimization converges to their respective hard-margin SVM solutions.", "title_embedding_index": 6373, "title_abs_embedding_index": 6398}, {"title": "Addressing Label Shift in Distributed Learning via Entropy Regularization\u200b", "link_suffix": "/forum?id=kuYxecnlv2", "link": "https://openreview.net/forum?id=kuYxecnlv2", "pdf_link": "https://openreview.net/pdf?id=kuYxecnlv2", "keywords": "label shifts; test-to-train label density ratio estimation; entropy-based regularization; distributed learning", "abstract": "We address the challenge of minimizingtrue riskin multi-node distributed learning. These systems are frequently exposed to both inter-node and intra-nodelabel shifts, which present a critical obstacle to effectively optimizing model performance while ensuring that data remains confined to each node.\nTo tackle this, we propose the Versatile Robust Label Shift (VRLS) method, which enhances the maximum likelihood estimation of the test-to-train label density ratio. VRLS incorporates Shannon entropy-based regularization and adjusts the density ratio during training  to better handle label shifts at the test time.\nIn multi-node learning environments, VRLS further extends its capabilities by learning and adapting density ratios across nodes, effectively mitigating label shifts and improving overall model performance. Experiments conducted on MNIST, Fashion MNIST, and CIFAR-10 demonstrate the effectiveness of VRLS, outperforming baselines by up to 20% in imbalanced settings. These results highlight the significant improvements VRLS offers in addressing label shifts. Our theoretical analysis further supports this by establishing high-probability bounds on estimation errors.", "title_embedding_index": 6374, "title_abs_embedding_index": 6399}]