[{"title": "Bayesian Tree-Dependent Factorization", "link_suffix": "/forum?id=gwNQuVXEEV", "link": "https://openreview.net/forum?id=gwNQuVXEEV", "pdf_link": "https://openreview.net/pdf?id=gwNQuVXEEV", "keywords": "factorization, Bayesian models, multi-view, hierarchical, gene expression, clinical", "abstract": "We propose Bayesian Tree-Dependent Factorization (BTF), a novel probabilistic representation learning model that uncovers hierarchical, continuous latent factors in complex datasets. BTF constructs a tree-based model that discovers interpretable factorizations of the data wherein each factor has a conditional  relationship to its parent, allowing it to capture both global and local effects. This approach is particularly well-suited for biological data, where traditional methods like PCA fail to capture higher-order dependencies and hierarchical structure. A significant contribution of this work is the multi-view extension of BTF, which allows for the joint analysis of multiple data modalities. By learning shared loadings across views while maintaining distinct factors for each modality, multi-view BTF improves performance and enables deeper insights into the relationships between different data types. We demonstrate the performance of BTF in simulations as well as in a real-world application to gene expression and clinical data in breast cancer patients, revealing biologically and clinically meaningful patient trends, and showing that BTF is a valuable representation learning tool for analysis and hypothesis generation.", "title_embedding_index": 2300, "title_abs_embedding_index": 2325}, {"title": "Let\u2019s Think Var-by-Var: Large Language Models Enable Ad Hoc Probabilistic Reasoning", "link_suffix": "/forum?id=kJFQ0Pf4jw", "link": "https://openreview.net/forum?id=kJFQ0Pf4jw", "pdf_link": "https://openreview.net/pdf?id=kJFQ0Pf4jw", "keywords": "Large Language Models; Ad hoc Probabilisitic Models; Commonsense Reasoning", "abstract": "A hallmark of intelligence is the ability to flesh out underspecified situations using \u201ccommon sense.\u201d We propose to extract that common sense from large language models (LLMs), in a form that can feed into probabilistic inference. We focus our investigation on guesstimation questions such as \u201cHow much are Airbnb listings in Newark, NJ?\u201d Formulating a sensible answer without access to data requires drawing on, and integrating, bits of common knowledge about how Price and Location may relate to other variables, such as Property Type. Our framework answers such a question by synthesizing an ad hoc probabilistic model. First we prompt an LLM to propose a set of random variables relevant to the question, followed by moment constraints on their joint distribution. We then optimize the joint distribution p within a log-linear family to maximize the overall constraint satisfaction. Our experiments show that LLMs can successfully be prompted to propose reasonable variables, and while the proposed numerical constraints can be noisy, jointly optimizing for their satisfaction reconciles them. When evaluated on probabilistic questions derived from three real-world tabular datasets, we find that our framework performs comparably to a direct prompting baseline in terms of total variation distance from the dataset distribution, and is similarly robust to noise.", "title_embedding_index": 2301, "title_abs_embedding_index": 2326}, {"title": "CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking", "link_suffix": "/forum?id=iyJOUELYir", "link": "https://openreview.net/forum?id=iyJOUELYir", "pdf_link": "https://openreview.net/pdf?id=iyJOUELYir", "keywords": "code representation learning, code re-ranking, contrastive learning", "abstract": "Effective code retrieval plays a crucial role in advancing code generation, bug fixing, and software maintenance, particularly as software systems increase in complexity. While current code embedding models have demonstrated promise in retrieving code snippets for small-scale, well-defined tasks, they often underperform in more demanding real-world applications such as bug localization within GitHub repositories. We hypothesize that a key issue is their reliance on noisy and inconsistent datasets for training, which impedes their ability to generalize to more complex retrieval scenarios. To address these limitations, we introduce CoRNStack, a large-scale, high-quality contrastive training dataset for code that spans multiple programming languages. This dataset is curated using consistency filtering to eliminate noisy positives and is further enriched with mined hard negatives, thereby facilitating more effective learning. We demonstrate that contrastive training of embedding models using CoRNStack leads to state-of-the-art performance across a variety of code retrieval tasks. Furthermore, the dataset can be leveraged for training code reranking models, a largely underexplored area compared to text reranking. Our finetuned code reranking model significantly improves the ranking quality over the retrieved results. Finally, by employing our code retriever and reranker together, we demonstrate significant improvements in function localization for GitHub issues, an important\ncomponent of real-world software development.", "title_embedding_index": 2302, "title_abs_embedding_index": 2327}, {"title": "Flex: End-to-End Text-Instructed Visual Navigation with Foundation Models", "link_suffix": "/forum?id=chwxOOwzFR", "link": "https://openreview.net/forum?id=chwxOOwzFR", "pdf_link": "https://openreview.net/pdf?id=chwxOOwzFR", "keywords": "Robot learning, Foundation Models, Visual Navigation", "abstract": "End-to-end learning directly maps sensory inputs to actions, creating highly integrated and efficient policies for complex robotics tasks. However, such models are tricky to efficiently train and often struggle to generalize beyond their training scenarios, limiting adaptability to new environments, tasks, and concepts. In this work, we investigate the minimal data requirements and architectural adaptations necessary to achieve robust closed-loop performance with vision-based control policies under unseen text instructions and visual distribution shifts.\nTo this end, we design datasets with various levels of data representation richness, refine feature extraction protocols by leveraging multi-modal foundation model encoders, and assess the suitability of different policy network heads. Our findings are synthesized in Flex (Fly-lexically), a framework that uses pre-trained Vision Language Models (VLMs) as frozen patch-wise feature extractors, generating spatially aware embeddings that integrate semantic and visual information. These rich features form the basis for training highly robust downstream policies capable of generalizing across platforms, environments, and text-specified tasks.\nWe demonstrate the effectiveness of this approach on quadrotor fly-to-target tasks, where agents trained via behavior cloning on a small simulated dataset successfully generalize to real-world scenes, handling diverse novel goals and command formulations.", "title_embedding_index": 2303, "title_abs_embedding_index": 2328}, {"title": "Spatial Reasoning with MLLMs: A New Path to Graph-Structured Optimization", "link_suffix": "/forum?id=s5T9A9tXTX", "link": "https://openreview.net/forum?id=s5T9A9tXTX", "pdf_link": "https://openreview.net/pdf?id=s5T9A9tXTX", "keywords": "Multimodal large language model, graph representation, graph-structured problem, combinatorial optimization", "abstract": "Graph-structured problems pose significant challenges due to their complex structures and large scales, often making traditional computational approaches suboptimal or costly. However, when these problems are visually represented, humans can often solve them more intuitively, leveraging our inherent spatial reasoning capabilities. In this work, we introduce an original and novel approach by feeding graphs as images into multimodal large language models (MLLMs), aiming for a loss-free representation that preserves the graph's structural integrity and enables machines to mimic this human-like thinking. Our pioneering exploration of MLLMs addresses various graph-structured challenges, from combinatorial tasks like influence maximization to sequential decision-making processes such as network dismantling, along with tackling six basic graph-related problems. Our experiments reveal that MLLMs possess remarkable spatial intelligence and a unique aptitude for these problems, marking a significant step forward in enabling machines to understand and analyze graph-structured data with human-like depth and intuition. These findings also suggest that combining MLLMs with straightforward optimization techniques could offer a new, effective paradigm for managing large-scale graph problems without complex derivations, computationally demanding training and fine-tuning.", "title_embedding_index": 2304, "title_abs_embedding_index": 2329}, {"title": "BingoGuard: LLM Content Moderation Tools with Risk Levels", "link_suffix": "/forum?id=HPSAkIHRbb", "link": "https://openreview.net/forum?id=HPSAkIHRbb", "pdf_link": "https://openreview.net/pdf?id=HPSAkIHRbb", "keywords": "LLM, safety guardrail, content moderator", "abstract": "Malicious content generated by large language models (LLMs) can pose varying degrees of harm. \nAlthough existing LLM-based moderators can detect harmful content, they struggle to assess risk levels and may miss lower-risk outputs. \nAccurate risk assessment allows platforms with different safety thresholds to tailor content filtering and rejection. In this paper, we introduce per-topic severity rubrics for 11 harmful topics and build BingoGuard, an LLM-based moderation system designed to predict both binary safety labels and severity levels. \nTo address the lack of annotations on levels of severity, we propose a scalable generate-then-filter framework that first generates responses across different severity levels and then filters out low-quality responses. Using this framework, we create BingoGuardTrain, a training dataset with 54,897 examples covering a variety of topics, response severity, styles, and BingoGuardTest, a test set with 988 examples explicitly labeled based on our severity rubrics that enables fine-grained analysis on model behaviors on different severity levels. Our BingoGuard-8B, trained on BingoGuardTrain, achieves the state-of-the-art performance on several moderation benchmarks, including WildGuardTest and HarmBench, as well as BingoGuardTest, outperforming best public models, WildGuard, by 4.3%. Our analysis demonstrates that incorporating severity levels into training significantly enhances detection performance and enables the model to effectively gauge the severity of harmful responses. Warning: this paper includes red-teaming examples that may be harmful in nature.", "title_embedding_index": 2305, "title_abs_embedding_index": 2330}, {"title": "Learn hybrid prototypes for multivariate time series anomaly detection", "link_suffix": "/forum?id=8TBGdH3t6a", "link": "https://openreview.net/forum?id=8TBGdH3t6a", "pdf_link": "https://openreview.net/pdf?id=8TBGdH3t6a", "keywords": "prototypes;time series;anomaly detection", "abstract": "In multivariate time series anomaly detection (MTSAD), reconstruction-based models reconstruct testing series with learned knowledge of only normal series and identify anomalies with higher reconstruction errors. In practice, over-generalization often occurs with unexpectedly well reconstruction of anomalies. Although memory banks are employed by reconstruction-based models to fight against over-generalization, these models are only efficient to detect point anomalies since they learn normal prototypes from time points, leaving contextual anomalies and periodical anomalies to be discovered. To settle this problem, this paper propose a hybrid prototypes learning model for MTSAD based on reconstruction, named as H-PAD. First, normal prototypes are learned from different sizes of patches for time series to discover short-term anomalies. These prototypes in different sizes are integrated together to reconstruct query series so that any anomalies would be smoothed off and high reconstruction errors are produced. Furthermore, period prototypes are learned to discover periodical anomalies. One period prototype is memorized for one variable of query series. Finally, extensive experiments on five benchmark datasets show the effectiveness of H-PAD with state-of-the-art performance.", "title_embedding_index": 2306, "title_abs_embedding_index": 2331}, {"title": "Pre-Memorization Train Accuracy Reliably Predicts Generalization in LLM Reasoning", "link_suffix": "/forum?id=OegBJMucyM", "link": "https://openreview.net/forum?id=OegBJMucyM", "pdf_link": "https://openreview.net/pdf?id=OegBJMucyM", "keywords": "LLMs, Generalization, Memorization, Reasoning", "abstract": "When large language models (LLMs) are finetuned on reasoning tasks, they can either reduce their training loss by developing problem-solving abilities, or by simply memorizing target traces in the training data. Our work aims to better understand how this learning process shapes a model's ability to generalize. We observe that, while LLMs often perfectly memorize most target solution traces by the end of training, their predictions at intermediate checkpoints can provide valuable insights into their behavior at test time. Concretely, we introduce the concept of pre-memorization train accuracy: the accuracy of model samples for training queries prior to exactly reproducing reasoning traces in the training data. We find that the average pre-memorization train accuracy of the model is strongly predictive of its test performance, with coefficients of determination around or exceeding 0.9 across various models (Llama3-8B, Gemma2-9B), datasets (GSM8k, MATH), and training setups. Beyond this aggregate statistic, we find that the pre-memorization train accuracy of individual examples can predict the model\u2019s sensitivity to input perturbations for those examples, allowing us to identify examples for which the model fails to learn robust solutions. A natural application of this insight is in data curation. We find that prioritizing the collection of examples with low pre-memorization accuracy leads to 1.5-2x data efficiency compared to i.i.d. data scaling, and outperforms other standard data curation techniques.", "title_embedding_index": 2307, "title_abs_embedding_index": 2332}, {"title": "Teaching LLMs to Decode Activations Into Natural Language", "link_suffix": "/forum?id=cselR6Jne3", "link": "https://openreview.net/forum?id=cselR6Jne3", "pdf_link": "https://openreview.net/pdf?id=cselR6Jne3", "keywords": "llm safety, llm interpretability, ml safety, activation steering", "abstract": "Interpretability methods seek to understand language model representations, yet the outputs of most such methods---circuits, vectors, scalars---are uninterpretable, requiring further effort to interpret. In contrast, we propose to study LatentQA, the task of answering open-ended questions about model activations in natural language. Towards solving LatentQA, we propose Latent Interpretation Tuning (LIT), which finetunes a decoder LLM on a dataset of activations and associated question-answer pairs, similar to how visual instruction tuning trains on question-answer pairs associated with images. We use the decoder for diverse reading applications, such as extracting relational knowledge from representations or uncovering system prompts governing model behavior. Our decoder also specifies a differentiable loss that we use to control models, such as debiasing models on stereotyped sentences and controlling the sentiment of generations. Finally, we extend LatentQA to reveal harmful model capabilities, such as generating recipes for bioweapons and code for hacking, from safety-tuned models, even when given benign prompts.", "title_embedding_index": 2308, "title_abs_embedding_index": 2333}, {"title": "Improved Stochastic Controlled Averaging for Distributed and Federated Learning", "link_suffix": "/forum?id=OfsGBHdFgk", "link": "https://openreview.net/forum?id=OfsGBHdFgk", "pdf_link": "https://openreview.net/pdf?id=OfsGBHdFgk", "keywords": "Federated Learning", "abstract": "Distributed and federated learning (D/FL) is a powerful machine learning (ML) paradigm in which clients collaborate to train a model under the coordination of a central server. Depending on the nature of clients, data in each client might have the same distribution (called the homogeneous setting) or different distributions (the heterogeneous setting). The state-of-the-art D/FL algorithm SCAFFOLD addresses the critical issue of data heterogeneity through the use of control variables. However, while theoretical analysis suggests that the convergence rate of SCAFFOLD is independent of data heterogeneity, the practical performance of SCAFFOLD is often inconsistent in homogeneous and heterogeneous settings. Motivated by the disagreement between theory and practice of SCAFFOLD, in this work, we propose a novel D/FL algorithm to bridge this experimental performance gap while preserving similar theoretical guarantees as SCAFFOLD. The proposed algorithm accommodates arbitrary data heterogeneity, partial participation, local updates, and supports unbiased communication compression. Theoretically, we prove that our algorithm is unaffected by data heterogeneity and achieves state-of-the-art convergence rate as SCAFFOLD. Furthermore, numerical experiments indicate that our algorithm achieves consistent (similar) test accuracy in both homogeneous and heterogeneous settings while often converges faster than existing baselines.", "title_embedding_index": 2309, "title_abs_embedding_index": 2334}, {"title": "Sparse Mamba: Reinforcing Controllability In Structural State Space Models", "link_suffix": "/forum?id=vOfDGYGVyj", "link": "https://openreview.net/forum?id=vOfDGYGVyj", "pdf_link": "https://openreview.net/pdf?id=vOfDGYGVyj", "keywords": "Mamba, state space models, natural language processing", "abstract": "In this work, we introduce the concept of controllability and observability to the Mamba SSM's architecture in our Sparse-Mamba (S-Mamba) for natural language processing (NLP) applications. The structured state space model (SSM) development in recent studies, such as Mamba and Mamba2, outperformed and solved the computational inefficiency of transformers and large language models at small to medium scale. The Mamba SSMs architecture drops the need for attention layers or multilayer perception blocks in transformers. However, current Mamba models lack reinforcement of controllability in state-space equations for computing the $A$, $B$, $C$, and $D$ matrices at each time step, leading to increased complexity and computational costs. In this paper, we demonstrate a reduction of parameters in comparison to the first published Mamba and Mamba2. We showcase an improvement in perplexity by 5% and a decrease in training time by 3% after reinforcing controllability and observability on the original Mamba architecture in our proposed S-Mamba. The controllable $n \\times n$ state  matrix $A$ is sparse and it has only $n$ free parameters. Our novel approach will ensure a controllable system which will be the gate key for Mamba3.", "title_embedding_index": 2310, "title_abs_embedding_index": 2335}, {"title": "Online learning meets Adam: The Road of Interpretable Adaptive Optimizer Design", "link_suffix": "/forum?id=Fj6Yv5rPRe", "link": "https://openreview.net/forum?id=Fj6Yv5rPRe", "pdf_link": "https://openreview.net/pdf?id=Fj6Yv5rPRe", "keywords": "online learning, non-smooth non-convex optimization, online-to-nonconvex conversion framework, optimizer, Adam", "abstract": "This paper explores the theoretical foundations of Adam, a widely used adaptive optimizer. Building on recent developments in non-convex optimization and online learning, particularly the discounted-to-nonconvex conversion framework, we present two aspects of results: First, we introduce clip-free FTRL, a novel variant of the classical Follow-the-Regularized-Leader (FTRL) algorithm. Unlike scale-free FTRL and the recently proposed $\\beta$-FTRL, our clip-free variant eliminates the need for clipping operations, aligning more closely with Adam's practical implementation. This modification provides deeper theoretical insights into Adam's empirical success and aligns the theoretical framework with practical implementations. By incorporating a refined analysis, our second result establishes a theoretical guarantee for the Last Iterate Convergence (LIC) under the proposed discounts-to-nonconvex conversion algorithm in LIC, which differs from the previous guarantee that has convergence evenly distributed in all iterations. Additionally, we extend this result to provide the last iterate convergence guarantee for the popular $\\beta$-FTRL algorithm under the same framework. However, the derived last iterate convergence of $\\beta$-FTRL reveals a persistent fixed error, potentially suggesting either limitations in popular online learning methods or the need for additional assumptions about the objective function.", "title_embedding_index": 2311, "title_abs_embedding_index": 2336}, {"title": "Understanding Reasoning with Looped Models", "link_suffix": "/forum?id=din0lGfZFd", "link": "https://openreview.net/forum?id=din0lGfZFd", "pdf_link": "https://openreview.net/pdf?id=din0lGfZFd", "keywords": "looped models, reasoning, language model, iterative algorithm, inductive bias", "abstract": "Large language models have shown promising abilities in reasoning problems and scaling laws suggest that parameter count is a key driver. Recent works (Chen & Zou, 2024; Ye et al., 2024) argue that for reasoning, depth plays a very important role in addition to parameter count. In this work, we make a more fine-grained claim \u2014 many reasoning problems require large depth but not necessarily many parameters, in the sense that they can be solved via looped models. This unlocks a novel application of looped models for reasoning. We empirically study various synthetic reasoning problems like addition, variable assignment and math problems. For each of these, we find that $k$-layer transformer model looped $L$ times nearly matches the quality of a $kL$-layer non-looped model and is much better than a k-layer model. Thus, using a small model and providing depth via looping can suffice for such reasoning problems. We then show theoretical results proving that many such reasoning problems can be solved via iterative algorithms, and thus, can be solved with looped models. Motivated by these findings, we train autoregressive models on general language modeling datasets with looping and compare a $k$-layer model looped $L$ times to a $kL$-layer model. While the looped model is understandably worse on perplexity and memorization tasks, it surprisingly does very well on tasks that require reasoning, like open book QA, math word problems and reasoning primitives. Despite having significantly fewer parameters, it can even match or outperform the non-looped $kL$-layer model on some of these tasks. These results suggest a novel inductive bias of looped models towards enhanced reasoning. We provide further evidence for this inductive bias by visualizing perplexity vs downstream isoplots, and design a looping-inspired regularization that solidifies this hypothesis.", "title_embedding_index": 2312, "title_abs_embedding_index": 2337}, {"title": "Secure FLOATING - Scalable Federated Learning Framework for Real-time Trust in Mobility Data using Secure Multi-Party Computation and Blockchain", "link_suffix": "/forum?id=7XrVS0K8yr", "link": "https://openreview.net/forum?id=7XrVS0K8yr", "pdf_link": "https://openreview.net/pdf?id=7XrVS0K8yr", "keywords": "federated learning, smpc, privacy, connected and autonomous vehicles", "abstract": "The safety of Connected and Autonomous Vehicles (CAVs), Micro-mobility devices (e-scooter, e-bikes) and  smartphone users rely on trusting the trajectory data they generate for navigation around each other. There is a need for real-time verification of mobility data from these devices without compromising privacy as malicious data used for navigation could be deadly, specially for vulnerable road users. In this paper, we propose Secure-FLOATING, a scalable framework leveraging federated learning and blockchain for nearby nodes to coordinate and learn to trust mobility data from nearby devices and store this information via consensus on a tamper-proof distributed ledger. We employ lightweight Secure Multi-party computation (SMPC) with reduced messages exchanges to preserve privacy of the users and ensure data validation in real-time. Secure-FLOATING is evaluated using realistic trajectories for up to 8,000 nodes (vehicles, micro-mobility devices and pedestrians) in New York City, and it shows to achieve lower delays and overhead, thereby accurately validating each others' mobility data in a scalable manner, with up to 75% successful endorsement for as high as 50% attacker penetration.", "title_embedding_index": 2313, "title_abs_embedding_index": 2338}, {"title": "RED \u2013 ROBUST ENVIRONMENTAL DESIGN", "link_suffix": "/forum?id=H3lK5FV16C", "link": "https://openreview.net/forum?id=H3lK5FV16C", "pdf_link": "https://openreview.net/pdf?id=H3lK5FV16C", "keywords": "adversarial machine learning; security", "abstract": "The classification of road signs by autonomous systems, especially those reliant on\nvisual inputs, is highly susceptible to adversarial attacks. Traditional approaches to\nmitigating such vulnerabilities have focused on enhancing the robustness of classi-\nfication models. In contrast, this paper adopts a fundamentally different strategy\naimed at increasing robustness through the redesign of road signs themselves. We\npropose an attacker-agnostic learning scheme to automatically design road signs\nthat are robust to a wide array of patch-based attacks. Empirical tests conducted in\nboth digital and physical environments demonstrate that our approach significantly\nreduces vulnerability to patch attacks, outperforming existing techniques.", "title_embedding_index": 2314, "title_abs_embedding_index": 2339}, {"title": "AIM: Adversarial Information Masking for Evaluating EEG-DL Interpretations", "link_suffix": "/forum?id=B5i88Tj1nk", "link": "https://openreview.net/forum?id=B5i88Tj1nk", "pdf_link": "https://openreview.net/pdf?id=B5i88Tj1nk", "keywords": "Explainable AI, Post-hoc explanation, EEG, Feature attribution method, Saliency map, in-distribution imputation", "abstract": "The analysis of post-hoc explanations for deep learning models has revealed significant benefits in enhancing transparency and advancing explainable artificial intelligence (XAI). Despite this progress, a critical gap remains in the reliable evaluation of the faithfulness of explanations, particularly for data that encompasses multi-domain information. To address the challenges of distribution shifts and information leakage present in existing methods, we propose a novel framework involving multi-domain adversarial information masking (AIM) based on Multi-Domain Adversarial Robustness (mdAR), enabling in-distribution imputation and facilitating a reliable assessment of faithfulness for feature attribution methods. Extensive experiments validate our approach across various model architectures and multi-channel electroencephalogram (EEG) datasets. The code and sample data for this work are available athttps://anonymous.4open.science/r/EEG-explanation-faithfulness-5C05.", "title_embedding_index": 2315, "title_abs_embedding_index": 2340}, {"title": "Conservative Contextual Bandits: Beyond Linear Representations", "link_suffix": "/forum?id=SThJXvucjQ", "link": "https://openreview.net/forum?id=SThJXvucjQ", "pdf_link": "https://openreview.net/pdf?id=SThJXvucjQ", "keywords": "Contextual Bandits, Safety, Neural Bandits, Constrained Bandits", "abstract": "Conservative Contextual Bandits (CCBs) address safety in sequential decision making by requiring that an agent's policy, along with minimizing regret, also satisfies a safety constraint: the performance is not worse than a baseline policy (e.g., the policy that the company has in production) by more than $(1+\\alpha)$ factor. \nPrior work developed UCB-style\nalgorithms for this problem in the multi-armed (Wu et al., 2016)  and contextual\nlinear (Kazerouni et al., 2017) settings.\nHowever, in practice the cost of the arms\nis often a non-linear function, and therefore existing UCB algorithms are ineffective in such settings. \nIn this paper, we consider CCBs beyond the linear case and develop two algorithms $\\mathtt{C\\text{-}SquareCB}$ and $\\mathtt{C\\text{-}FastCB}$, using Inverse Gap Weighting (IGW) based exploration and an online regression oracle. \nWe show that the safety constraint is satisfied in high probability and that the regret for $\\mathtt{C\\text{-}SquareCB}$ is sub-linear in horizon $T$, while the the regret for $\\mathtt{C\\text{-}FastCB}$ is first-order and is sub-linear in $L^*$, the cumulative loss of the optimal policy. \nSubsequently, we use a neural network for function approximation and online gradient descent as the regression oracle to provide $\\tilde{\\mathcal{O}}\\big(\\sqrt{KT} + K/\\alpha\\big) $ and $\\tilde{\\mathcal{O}}\\big(\\sqrt{KL^*} + K (1 + 1/\\alpha)\\big)$ regret bounds respectively. \nFinally, we demonstrate the efficacy of our algorithms on real world data, and show that they significantly outperform the existing baseline while maintaining the performance guarantee.", "title_embedding_index": 2316, "title_abs_embedding_index": 2341}, {"title": "Quantifying Generalization Complexity for Large Language Models", "link_suffix": "/forum?id=jpSLXoRKnH", "link": "https://openreview.net/forum?id=jpSLXoRKnH", "pdf_link": "https://openreview.net/pdf?id=jpSLXoRKnH", "keywords": "large language model, generalization, evaluation", "abstract": "While large language models (LLMs) have shown exceptional capabilities in understanding complex queries and performing sophisticated tasks, their generalization abilities are often deeply entangled with  memorization, necessitating more precise evaluation. To address this challenge, we introduce Scylla, a dynamic evaluation framework that quantitatively measures the generalization abilities of LLMs. Scylla disentangles generalization from memorization via assessing model performance on both in-distribution (ID) and out-of-distribution (OOD) data through 20 tasks across 5 levels of complexity. Through extensive experiments, we uncover a non-monotonic relationship between task complexity and the performance gap between ID and OOD data, which we term the generalization valley. Specifically, this phenomenon reveals a critical threshold---referred to as critical complexity---where reliance on non-generalizable behavior peaks, indicating the upper bound of LLMs' generalization capabilities. As model size increases, the critical complexity shifts toward higher levels of task complexity, suggesting that larger models can handle more complex reasoning tasks before over-relying on memorization. Leveraging Scylla and the concept of critical complexity, we benchmark 28LLMs including both open-sourced models such as LLaMA and Qwen families, and close-sourced models like Claude and GPT, providing a more robust evaluation and establishing a clearer understanding of LLMs' generalization capabilities.", "title_embedding_index": 2317, "title_abs_embedding_index": 2342}, {"title": "Project MPG: towards a generalized performance quotient for LLM intelligence", "link_suffix": "/forum?id=MGceYYNvXp", "link": "https://openreview.net/forum?id=MGceYYNvXp", "pdf_link": "https://openreview.net/pdf?id=MGceYYNvXp", "keywords": "evaluation, LLM, LMSys, benchmarks", "abstract": "There exists an extremely wide array of LLM benchmarking tasks, whereas oftentimes a single number is the most actionable for decision making, especially by non-experts. No such aggregation schema exists that is not Elo based, which could be costly or time consuming. Here we propose a method to aggregate performance across a general space of benchmarks, nicknamed Project \u201cMPG\u201d, here dubbed Model Performance and Goodness, in addition referencing a metric widely understood to be an important yet inaccurate and crude measure of car performance. Here, we create two numbers: an ``Goodness'' number (answer accuracy), and a \u201cFastness\u201d number (cost or QPS). We compare models against each other and present a ranking according to our general metric as well as subdomains. We find significant agreement between the raw pearson correlation of our scores and thosee of LMSys, even improving on the correlation of the MMLU leaderboard to LMSys.", "title_embedding_index": 2318, "title_abs_embedding_index": 2343}, {"title": "Simultaneous Online System Identification and Control using Composite Adaptive Lyapunov-Based Deep Neural Networks", "link_suffix": "/forum?id=5AB33izFxP", "link": "https://openreview.net/forum?id=5AB33izFxP", "pdf_link": "https://openreview.net/pdf?id=5AB33izFxP", "keywords": "Adaptive control, Online Learning, Control Theory, Robotics", "abstract": "Although deep neural network (DNN)-based controllers are popularly used to control uncertain nonlinear dynamic systems, most results use DNNs that are pretrained offline and the corresponding controller is implemented post-training. Recent advancements in adaptive control have developed controllers with Lyapunov-based update laws (i.e., control and update laws derived from a Lyapunov-based stability analysis) for updating the DNN weights online to ensure the system states track a desired trajectory. However, the update laws are based on the tracking error, and offer guarantees on only the tracking error convergence, without providing any guarantees on system identification. This paper provides the first result on composite adaptive Lyapunov-based update law, involving a combination of the tracking error and a prediction error of the system dynamics, to adjust the weights of all layers of a DNN-based controller. As a result, the DNN can rapidly adapt to simultaneously achieve the goals of tracking a desired trajectory and identifying the system dynamics online. Since evaluating the DNN's prediction error typically requires state-derivative information, a dynamic state-derivative estimator is developed and interlaced with the weight update law. A combined Lyapunov-based stability analysis is provided, which guarantees that the tracking error, state-derivative estimation error, and DNN weight estimation errors are uniformly ultimately bounded. Additionally, a persistence of excitation (PE) condition is developed for the DNN. When the PE condition is satisfied, significantly tighter bounds are obtained on the tracking and weight estimation errors, thus achieving system identification and enhanced trajectory tracking performance. As an outcome of the system identification, the DNN model can be propagated forward to predict and compensate for the uncertainty in dynamics under intermittent loss of state feedback. Comparative simulation results are provided on a two-link manipulator system and an unmanned underwater vehicle system with intermittent loss of state feedback, where the developed method yields significant performance improvement compared to baseline methods.", "title_embedding_index": 2319, "title_abs_embedding_index": 2344}, {"title": "Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot Molecular Perturbation Prediction", "link_suffix": "/forum?id=tKn6gpvlUX", "link": "https://openreview.net/forum?id=tKn6gpvlUX", "pdf_link": "https://openreview.net/pdf?id=tKn6gpvlUX", "keywords": "Foundation models, fine-tuning, parameter efficient fine-tuning, adapters, Molecular Perturbation, drug discovery, scgpt", "abstract": "Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, makes the task challenging. In this study, we leverage single-cell foundation models (FMs) pretrained on tens of millions of single cells, encompassing multiple cell types, states, and disease annotations, to address molecular perturbation prediction. We introduce a drug-conditional adapter that allows efficient fine-tuning by training less than 1% of the original foundation model, thus enabling molecular conditioning while preserving the rich biological representation learned during pretraining. The proposed strategy allows not only the prediction of cellular responses to novel drugs, but also the zero-shot generalization to unseen cell types.  We establish a robust evaluation framework to assess model performance across different generalization tasks, demonstrating state-of-the-art results across all settings, with significant improvements in the few-shot and zero-shot generalization to new cell types compared to existing baselines.", "title_embedding_index": 2320, "title_abs_embedding_index": 2345}, {"title": "Holistically Evaluating the Environmental Impact of Creating Language Models", "link_suffix": "/forum?id=04qx93Viwj", "link": "https://openreview.net/forum?id=04qx93Viwj", "pdf_link": "https://openreview.net/pdf?id=04qx93Viwj", "keywords": "machine learning, artificial intelligence, language model, large language models, environmental impact, carbon emissions, water usage", "abstract": "As the performance of artificial intelligence systems has dramatically increased, so too has the environmental impact of creating these systems. While many model developers release estimates of the power consumption and carbon emissions from the final training runs for their latest models, there is comparatively little transparency into the impact of model development, hardware manufacturing, and total water usage throughout. In this work, we estimate the real-world environmental impact of developing a series of language models, ranging from 20 million to 7 billion active parameters, trained on up to 5 trillion tokens each. When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released $\\textbf{270 metric tons}$ of carbon emissions, equivalent to powering about 53 homes in the United States for one year, and consumed $\\textbf{1.137 million liters of water}$, equivalent to about 10 years of water usage by a person in the United States, even though our data center is extremely water-efficient. We measure and report the environmental impact of our model development; to the best of our knowledge we are the first to do so for LLMs, and we find that model development, the impact of which is generally not disclosed by most model developers, amounted to $\\sim$$\\textbf{80}$% of that of training. By looking at detailed time series data for power consumption, we also find that power usage throughout training is not consistent, fluctuating between $\\sim$15% and $\\sim$85% of our hardware's maximum power draw, with negative implications for grid-scale planning as demand continues to grow. We close with a discussion on the continued difficulty of estimating the environmental impact of AI systems, and key takeaways for model developers and the public at large.", "title_embedding_index": 2321, "title_abs_embedding_index": 2346}, {"title": "SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget", "link_suffix": "/forum?id=9HK2rHNAhd", "link": "https://openreview.net/forum?id=9HK2rHNAhd", "pdf_link": "https://openreview.net/pdf?id=9HK2rHNAhd", "keywords": "KV-cache, LLM inference optimization", "abstract": "Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. However, most of these methods treat all layers equally, allocating the same KV budget to each layer. This approach is suboptimal, as some layers may be less sensitive to input tokens yet still receive the same budget as others. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions. Based on our observations regarding layer-wise importance in inference, we propose SQUEEZEATTENTION to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative token sparsification algorithms to compress the KV-cache for each layer with its very own budget. Specifically, we first measure each layer\u2019s importance by calculating the cosine similarity of the input prompt differences before and after the self-attention layers. Based on this similarity, we then categorize the layers into two groups and adjust their KV budgets accordingly. By optimizing the KV-cache from both sequence\u2019s and layer\u2019s dimensions, SQUEEZEATTENTION achieves around 30% to 70% of the memory reductions and up to 2.2 \u00d7 of throughput improvements in a wide range of LLMs and benchmarks.", "title_embedding_index": 2322, "title_abs_embedding_index": 2347}, {"title": "Interpolating Autoregressive and Discrete Denoising Diffusion Language Models", "link_suffix": "/forum?id=tyEyYT267x", "link": "https://openreview.net/forum?id=tyEyYT267x", "pdf_link": "https://openreview.net/pdf?id=tyEyYT267x", "keywords": "Diffusion Models, Text Diffusion, Generative Models", "abstract": "Diffusion language models offer unique benefits over autoregressive (AR) models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of semi-autoregressive (SAR) diffusion models that interpolate between discrete denoising diffusion and autoregressive models. \nWe propose a recipe for building effective SAR models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. SAR models overcome key limitations of diffusion language models, setting a new state-of-the-art performance on language modeling benchmarks and enabling generation of arbitrary-length sequences.", "title_embedding_index": 2323, "title_abs_embedding_index": 2348}, {"title": "Non-Parametric State-Space Models Over Datapoints and Sequence Alignments", "link_suffix": "/forum?id=HcY3fbVDqa", "link": "https://openreview.net/forum?id=HcY3fbVDqa", "pdf_link": "https://openreview.net/pdf?id=HcY3fbVDqa", "keywords": "Non-Parametric Models, State-Space Models, Genotype Imputation, Comp Bio", "abstract": "Non-parametric models are flexible and can leverage a context set to express rich mappings from inputs to outputs. However, these methods often scale super-linearly in context size, e.g., attention-based\nmethods scale quadratically in the number of data points, which in turn limits model expressivity.  In this work, we leverage advances in state-space modeling and introduce Non-Parametric State\n Space Models (NPSSM). We find that NPSSMs attain similar performance to existing non-parametric attention-based models while scaling linearly in the number of datapoints. We apply NPSSMs to the task of genotype imputation, where the linear scaling enables larger context sets resulting in competitive performance relative to other methods and widely used industry-standard tools. We also demonstrate the effectiveness of\nNPSSMs in the context of meta-learning where the ability to efficiently scale to larger training sets provides more favorable compute-to-accuracy tradeoffs.", "title_embedding_index": 2324, "title_abs_embedding_index": 2349}]