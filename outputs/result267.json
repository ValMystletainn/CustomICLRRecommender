[{"title": "Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers", "link_suffix": "/forum?id=0UvlnHgaii", "link": "https://openreview.net/forum?id=0UvlnHgaii", "pdf_link": "https://openreview.net/pdf?id=0UvlnHgaii", "keywords": "Inverse Reinforcement Learning, Generative Diffusion Model", "abstract": "An important prerequisite for safe control is aligning the policy with the underlying constraints in the environment. In many real-world applications, due to the difficulty of manually specifying these constraints, existing works have proposed recovering constraints from expert demonstrations by solving the Inverse Constraint Learning (ICL) problem. However, ICL is inherently ill-posed, as multiple constraints can equivalently explain the experts' preferences, making the optimal solutions not uniquely identifiable. In this work, instead of focusing solely on a single constraint, we propose the novel approach of Exploratory ICL (ExICL). The goal of ExICL is to recover a diverse set of feasible constraints, thereby providing practitioners the flexibility to select the most appropriate constraint based on the needs of practical deployment. To achieve this goal, we design a generative diffusion verifier, which guides the trajectory generation process using the probabilistic representation of an optimal constrained policy. By comparing these decisions with those made by expert agents, we can efficiently verify a candidate constraint. Driven by the verification feedback, ExICL implements an exploratory constraint update mechanism that strategically facilitates the diversity within the collection of feasible constraints. Our empirical results demonstrate that ExICL can seamlessly and reliably generalize across different tasks and environments.", "title_embedding_index": 13300, "title_abs_embedding_index": 13325}, {"title": "Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models", "link_suffix": "/forum?id=DD11okKg13", "link": "https://openreview.net/forum?id=DD11okKg13", "pdf_link": "https://openreview.net/pdf?id=DD11okKg13", "keywords": "Object-centric Learning, Foundation Models", "abstract": "Object-centric (OC) representations, which represent the state of a visual scene by modeling it as a composition of objects, have the potential to be used in various downstream tasks to achieve systematic compositional generalization and facilitate reasoning. However, these claims have not been thoroughly analyzed yet.\nRecently, foundation models have demonstrated unparalleled capabilities across diverse domains from language to computer vision, marking them as a potential cornerstone of future research for a multitude of computational tasks.\nIn this paper, we conduct an extensive empirical study on representation learning for downstream Visual Question Answering (VQA), which requires an accurate compositional understanding of the scene. \nWe thoroughly investigate the benefits and trade-offs of OC models and alternative approaches including large pre-trained foundation models on both synthetic and real-world data, and demonstrate a viable way to achieve the best of both worlds. \nThe extensiveness of our study, encompassing over 600 downstream VQA models and 15 different types of upstream representations, also provides several additional insights that we believe will be of interest to the community at large.", "title_embedding_index": 13301, "title_abs_embedding_index": 13326}, {"title": "Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders", "link_suffix": "/forum?id=ghH6YYDs15", "link": "https://openreview.net/forum?id=ghH6YYDs15", "pdf_link": "https://openreview.net/pdf?id=ghH6YYDs15", "keywords": "interpretability, sparse coding, neuroscience, language models, superposition", "abstract": "A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. In this paper, we investigate sparse inference and learning in SAEs through the lens of sparse coding. Specifically, we show that SAEs perform amortised sparse inference with a computationally restricted encoder and, using compressed sensing theory, we prove that this mapping is inherently insufficient for accurate sparse inference, even in solvable cases. Building on this theory, we empirically explore conditions where more sophisticated sparse inference methods outperform traditional SAE encoders. Our key contribution is the decoupling of the encoding and decoding processes, which allows for a comparison of various sparse encoding strategies. We evaluate these strategies on two dimensions: alignment with true underlying sparse features and correct inference of sparse codes, while also accounting for computational costs during training and inference. Our results reveal that substantial performance gains can be achieved with minimal increases in compute cost. We demonstrate that this generalises to SAEs applied to large language models (LLMs), where advanced encoders achieve similar interpretability. This work opens new avenues for understanding neural network representations and offers important implications for improving the tools we use to analyse the activations of large language models.", "title_embedding_index": 13302, "title_abs_embedding_index": 13327}, {"title": "Large Language Models Suffer From Their Own Output: An Analysis of the Self-Consuming Training Loop", "link_suffix": "/forum?id=SaOxhcDCM3", "link": "https://openreview.net/forum?id=SaOxhcDCM3", "pdf_link": "https://openreview.net/pdf?id=SaOxhcDCM3", "keywords": "self-consuming training loop, large language models, model collapse, generative models", "abstract": "Large Language Models (LLM) are already widely used to generate content for a variety of online platforms. As we are not able to safely distinguish LLM-generated content from human-produced content, LLM-generated content is used to train the next generation of LLMs, giving rise to a self-consuming training loop. From the image generation domain we know that such a self-consuming training loop reduces both quality and diversity of images finally ending in a model collapse. However, it is unclear whether this alarming effect can also be observed for LLMs. Therefore, we present the first study investigating the self-consuming training loop for LLMs. Further, we propose a novel method based on logic expressions that allows us to unambiguously verify the correctness of LLM-generated content, which is difficult for natural language text. We find that the self-consuming training loop produces correct outputs, however, the output declines in its diversity depending on the proportion of the used generated data. Fresh data can slow down this decline, but not stop it. Given these concerning results, we encourage researchers to study methods to negate this process.", "title_embedding_index": 13303, "title_abs_embedding_index": 13328}, {"title": "A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints", "link_suffix": "/forum?id=Gws8Q4wSrJ", "link": "https://openreview.net/forum?id=Gws8Q4wSrJ", "pdf_link": "https://openreview.net/pdf?id=Gws8Q4wSrJ", "keywords": "Orthogonality Constraints, Nonconvex Optimization, Nonsmooth Composite Optimization, Block Coordinate Descent, Convergence Analysis", "abstract": "Nonsmooth composite optimization with orthogonality constraints is crucial in statistical learning and data science, but it presents challenges due to its nonsmooth objective and computationally expensive, non-convex constraints. In this paper, we propose a new approach called \\textbf{OBCD}, which leverages Block Coordinate Descent (BCD) to address these challenges. \\textbf{OBCD} is a feasible method with a small computational footprint. In each iteration, it updates $k$ rows of the solution matrix, where $k \\geq 2$, while globally solving a small nonsmooth optimization problem under orthogonality constraints. We prove that \\textbf{OBCD} converges to block-$k$ stationary points, which offer stronger optimality than standard critical points. Notably, \\textbf{OBCD} is the first greedy descent method with monotonicity for this problem class. Under the Kurdyka-Lojasiewicz (KL) inequality, we establish strong limit-point convergence. We also extend \\textbf{OBCD} with breakpoint searching methods for subproblem solving and greedy strategies for working set selection. Comprehensive experiments demonstrate the superior performance of our approach across various tasks.", "title_embedding_index": 13304, "title_abs_embedding_index": 13329}, {"title": "Fair4Free: Generating High-fidelity Fair Synthetic Samples using Data-Free Distillation", "link_suffix": "/forum?id=iRgzG5DKgA", "link": "https://openreview.net/forum?id=iRgzG5DKgA", "pdf_link": "https://openreview.net/pdf?id=iRgzG5DKgA", "keywords": "data fairness, fair generative models, knowledge distillation, latent space distillation, synthetic data, biased data", "abstract": "This work presents Fair4Free, a novel generative model to generate synthetic fair data using data-free distillation in the latent space. Fair4Free can work on the situation when the data is private or inaccessible.  In our approach, we first train a teacher model to create fair representation and then distil the knowledge to a student model (using a smaller architecture). The process of distilling the student model is data-free, i.e. the student model does not have access to the training dataset while distilling. After the distillation, we use the distilled model to generate fair synthetic samples. Our extensive experiments show that our synthetic samples outperform state-of-the-art models in all three criteria (fairness, utility and synthetic quality) with a performance increase of 5% for fairness, 8% for utility and 12% in synthetic quality for both tabular and image datasets.", "title_embedding_index": 13305, "title_abs_embedding_index": 13330}, {"title": "ADMM for Nonconvex Optimization under Minimal Continuity Assumption", "link_suffix": "/forum?id=GKAQ92ua3A", "link": "https://openreview.net/forum?id=GKAQ92ua3A", "pdf_link": "https://openreview.net/pdf?id=GKAQ92ua3A", "keywords": "Nonconvex Optimization, Proximal Linearized ADMM, Nonsmooth Optimization, Convergence Analysis", "abstract": "This paper introduces a novel approach to solving multi-block nonconvex composite optimization problems through a proximal linearized Alternating Direction Method of Multipliers (ADMM). This method incorporates an Increasing Penalization and Decreasing Smoothing (IPDS) strategy. Distinguishing itself from existing ADMM-style algorithms, our approach (denoted IPDS-ADMM) imposes a less stringent condition, specifically requiring continuity in just one block of the objective function. IPDS-ADMM requires that the penalty increases and the smoothing parameter decreases, both at a controlled pace. When the associated linear operator is bijective, IPDS-ADMM uses an over-relaxation stepsize for faster convergence; however, when the linear operator is surjective, IPDS-ADMM uses an under-relaxation stepsize for global convergence. We devise a novel potential function to facilitate our convergence analysis and prove an oracle complexity $O(\\epsilon^{-3})$ to achieve an $\\epsilon$-approximate critical point. To the best of our knowledge, this is the first complexity result for using ADMM to solve this class of nonsmooth nonconvex problems. Finally, some experiments on the sparse PCA problem are conducted to demonstrate the effectiveness of our approach.", "title_embedding_index": 13306, "title_abs_embedding_index": 13331}, {"title": "MissScore: High-Order Score Estimation in the Presence of Missing Data", "link_suffix": "/forum?id=SvCFkNgjTE", "link": "https://openreview.net/forum?id=SvCFkNgjTE", "pdf_link": "https://openreview.net/pdf?id=SvCFkNgjTE", "keywords": "denoising score matching; missing data; causal discovery;", "abstract": "The first order derivative (score) of data density, typically estimated via denoising score matching, has emerged as an effective tool for modeling data distribution and generating synthetic data. Extending this concept to higher-order scores could uncover more detailed local information of the data distribution, enabling new applications. However, learning these high-order scores usually requires complete data, which is often unavailable in real-world scenarios such as healthcare and finance due to privacy and cost constraints. In this work, we introduce MissScore, a novel score-based framework for learning high-order scores from observations with missing data. We derive objective functions for estimating high-order scores under different missing data mechanisms and propose a new algorithm to handle missing data effectively. Our empirical results demonstrate that MissScore efficiently and accurately approximates high-order scores with missing data, while enhancing sampling speed and data quality, as validated through several downstream tasks, including data generation and causal discovery.", "title_embedding_index": 13307, "title_abs_embedding_index": 13332}, {"title": "ADMM for Nonsmooth Composite Optimization under Orthogonality Constraints", "link_suffix": "/forum?id=K1G8UKcEBO", "link": "https://openreview.net/forum?id=K1G8UKcEBO", "pdf_link": "https://openreview.net/pdf?id=K1G8UKcEBO", "keywords": "Orthogonality Constraints, Nonconvex Optimization, Nonsmooth Composite Optimization, ADMM, Convergence Analysis", "abstract": "We consider a class of structured, nonconvex, nonsmooth optimization problems under orthogonality constraints, where the objectives combine a smooth function, a nonsmooth concave function, and a nonsmooth weakly convex function. This class of problems finds diverse applications in statistical learning and data science. Existing methods for addressing these problems often fail to exploit the specific structure of orthogonality constraints, struggle with nonsmooth functions, or result in suboptimal oracle complexity. We propose {\\sf OADMM}, an Alternating Direction Method of Multipliers (ADMM) designed to solve this class of problems using efficient proximal linearized strategies. Two specific variants of {\\sf OADMM} are explored: one based on Euclidean Projection ({\\sf OADMM-EP}) and the other on Riemannian Retraction ({\\sf OADMM-RR}). Under mild assumptions, we prove that {\\sf OADMM} converges to a critical point of the problem with an ergodic convergence rate of $\\mathcal{O}(1/\\epsilon^{3})$. Additionally, we establish a super-exponential convergence rate or polynomial convergence rate for {\\sf OADMM}, depending on the specific setting, under the Kurdyka-Lojasiewicz (KL) inequality. To the best of our knowledge, this is the first non-ergodic convergence result for this class of nonconvex nonsmooth optimization problems. Numerical experiments demonstrate that the proposed algorithm achieves state-of-the-art performance.", "title_embedding_index": 13308, "title_abs_embedding_index": 13333}, {"title": "DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models", "link_suffix": "/forum?id=XBHoaHlGQM", "link": "https://openreview.net/forum?id=XBHoaHlGQM", "pdf_link": "https://openreview.net/pdf?id=XBHoaHlGQM", "keywords": "Weight Similarity, Large Language Models, Distribution of Cosine Similarity, Cluster Analysis", "abstract": "We introduce a novel index, the Distribution of Cosine Similarity (DOCS), for quantitatively assessing the similarity between weight matrices in Large Language Models (LLMs), aiming to facilitate the analysis of their complex architectures. Leveraging DOCS, our analysis uncovers intriguing patterns in the latest open-source LLMs: adjacent layers frequently exhibit high weight similarity and tend to form clusters, suggesting depth-wise functional specialization. Additionally, we prove that DOCS is theoretically effective in quantifying similarity for orthogonal matrices, a crucial aspect given the prevalence of orthogonal initializations in LLMs. This research contributes to a deeper understanding of LLM architecture and behavior, offering tools with potential implications for developing more efficient and interpretable models.", "title_embedding_index": 13309, "title_abs_embedding_index": 13334}, {"title": "SR2: BOOSTING 3D LARGE LANGUAGE MODEL WITH SPATIAL RELATION REASONING", "link_suffix": "/forum?id=2seVGyWZOX", "link": "https://openreview.net/forum?id=2seVGyWZOX", "pdf_link": "https://openreview.net/pdf?id=2seVGyWZOX", "keywords": "3D Large Language Model, Spatial Relation Reasoning, 3D Segmentation", "abstract": "Recent research in point cloud perception has achieved considerable progress in enhancing scene understanding by means of vision-language alignment through large language models (LLMs). However, existing methods may still encounter challenges in handling complex instructions that require accurate spatial reasoning, even if the 3D point cloud data has provided detailed spatial cues such as size, position, and orientation for identifying the targets.\nTo tackle this issue, this study introduces a new 3D multi-modal LLM framework, Spatial Relation Reasoning (SR$^2$). This framework is designed to strengthen relational reasoning capabilities in 3D environments. SR$^2$ mimics human reasoning behavior by first broadly identifying all relevant elements and then carefully examining them to determine the target.\nIn addition, as current datasets may not comprehensively evaluate the complex spatial reasoning capabilities of various models, we propose a new benchmark named 3D ReasonSeg that consists of 25,000 and 4,152 high-quality samples for training and evaluation respectively.\nBoth quantitative and qualitative experiments demonstrate that SR$^2$ and 3D ReasonSeg effectively endow 3D point cloud perception with stronger spatial reasoning capabilities, and we hope that the proposed SR$^2$ and 3D ReasonSeg can serve as a new baseline and benchmark for future work. The code and model will be made publicly available.", "title_embedding_index": 13310, "title_abs_embedding_index": 13335}, {"title": "REPOFILTER: Adaptive Retrieval Context Trimming for Repository-Level Code Completion", "link_suffix": "/forum?id=oOSeOEXrFA", "link": "https://openreview.net/forum?id=oOSeOEXrFA", "pdf_link": "https://openreview.net/pdf?id=oOSeOEXrFA", "keywords": "Code completion, RAG, LLMs", "abstract": "Retrieval-Augmented Generation (RAG) has recently emerged as a promising approach for repository-level code completion by integrating cross-file knowledge with in-file preceding code to provide comprehensive contexts for generation. To better understand the contribution of the retrieved cross-file contexts, we introduce a likelihood-based metric to evaluate the impact of each retrieved code chunk on the completion. Our analysis reveals that, despite retrieving numerous chunks, only a small subset positively contributes to the target completion, while some chunks even degrade performance. To address this issue, we leverage this metric to construct a repository-level dataset where each retrieved chunk is labeled as positive, neutral, or negative based on its relevance to the target completion. We then propose an adaptive retrieval context trimming framework, REPOFILTER, trained on this dataset to mitigate the harmful effects of negative retrieved contexts in RAG-based code completion. Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks demonstrates that REPOFILTER consistently improves completion accuracy compared to approaches without filtering operations across various tasks. Additionally, REPOFILTER significantly reduces the length of the input prompt, enhancing computational efficiency while exhibiting strong generalizability across different models. These results underscore the potential of REPOFILTER to enhance the accuracy, efficiency, and attributability of RAG-based repository-level code completion.", "title_embedding_index": 13311, "title_abs_embedding_index": 13336}, {"title": "Can LLMs Understand Time Series Anomalies?", "link_suffix": "/forum?id=LGafQ1g2D2", "link": "https://openreview.net/forum?id=LGafQ1g2D2", "pdf_link": "https://openreview.net/pdf?id=LGafQ1g2D2", "keywords": "Large Language Models (LLMs), Time Series Analysis, Anomaly Detection, Multimodal Learning", "abstract": "Large Language Models (LLMs) have gained popularity in time series forecasting, but their potential for anomaly detection remains largely unexplored. Our study investigates whether LLMs can understand and detect anomalies in time series data, focusing on zero-shot and few-shot scenarios. Inspired by conjectures about LLMs' behavior from time series forecasting research, we formulate key hypotheses about LLMs' capabilities in time series anomaly detection. We design and conduct principled experiments to test each of these hypotheses.Our investigation reveals several surprising findings about LLMs for time series:LLMs understand time series better asimagesrather than as textLLMs did not demonstrate enhanced performance when prompted to engage inexplicit reasoningabout time series analysisContrary to common beliefs, LLM's understanding of time seriesdo notstem from their repetition biases or arithmetic abilitiesLLMs' behaviors and performance in time series analysisvary significantlyacross different model architecturesThis study provides the first comprehensive analysis of contemporary LLM capabilities in time series anomaly detection. Our results suggest that while LLMs can understand time series anomalies, many common conjectures based on their reasoning capabilities do not hold. These insights pave the way for more effective LLM-based approaches in time series analysis, bridging the gap between forecasting and anomaly detection applications.", "title_embedding_index": 13312, "title_abs_embedding_index": 13337}, {"title": "Node-Time Conditional Prompt Learning in Dynamic Graphs", "link_suffix": "/forum?id=kVlfYvIqaK", "link": "https://openreview.net/forum?id=kVlfYvIqaK", "pdf_link": "https://openreview.net/pdf?id=kVlfYvIqaK", "keywords": "Graph mining, prompt learning, pre-training, dynamic graph", "abstract": "Dynamic graphs capture evolving interactions between entities, such as in social networks, online learning platforms, and crowdsourcing projects. For dynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a mainstream technique. However, they are generally pre-trained on the link prediction task, leaving a significant gap from the objectives of downstream tasks such as node classification. To bridge the gap, prompt-based learning has gained traction on graphs, but most existing efforts  focus on static graphs, neglecting the evolution of dynamic graphs. In this paper, we propose DyGPrompt, a novel pre-training and prompt learning framework for dynamic graph modeling. First, we designdual promptsto address the gap in both task objectives and temporal variations across pre-training and downstream tasks. Second, we recognize that node and time features mutually characterize each other, \nand proposedual condition-netsto model the evolving node-time patterns in downstream tasks. Finally, we thoroughly evaluate and analyze DyGPrompt through extensive experiments on four public datasets.", "title_embedding_index": 13313, "title_abs_embedding_index": 13338}, {"title": "Towards Accurate and Efficient Sub-8-Bit Integer Training", "link_suffix": "/forum?id=wJ3GeGLFmc", "link": "https://openreview.net/forum?id=wJ3GeGLFmc", "pdf_link": "https://openreview.net/pdf?id=wJ3GeGLFmc", "keywords": "Low-precision training; Model compression", "abstract": "Neural network training is a memory- and compute-intensive task. Quantization, which enables low-bitwidth formats in training, can significantly mitigate the workload. To reduce quantization error, recent methods have developed new data formats and additional pre-processing operations on quantizers. However, it remains quite challenging to achieve high accuracy and efficiency simultaneously. In this paper, we explore sub-8-bit integer training from its essence of gradient descent optimization. Our integer training framework includes two components: ShiftQuant to realize accurate gradient estimation, and L1 normalization to smoothen the loss landscape. \n  ShiftQuant attains performance that approaches the theoretical upper bound of group quantization. Furthermore, it liberates group quantization from inefficient memory rearrangement. The L1 normalization facilitates the implementation of fully quantized normalization layers with impressive convergence accuracy. \n  Our method frees sub-8-bit integer training from pre-processing and supports general devices. \n  This framework achieves negligible accuracy loss across various neural networks and tasks ($0.92%$ on 4-bit ResNets, $0.61%$ on 6-bit Transformers, $0.61%$ on 6-bit GNNs). \n  The prototypical implementation of ShiftQuant achieves more than $1.85\\times/15.3%$ performance improvement on CPU/GPU compared to its FP16 counterparts, and $33.9%$ resource consumption reduction on FPGA than the FP16 counterparts. The proposed fully-quantized L1 normalization layers achieve more than $35.54%$ improvement in throughout on CPU compared to traditional L2 normalization layers. Moreover, theoretical analysis verifies the advancement of our method.", "title_embedding_index": 13314, "title_abs_embedding_index": 13339}, {"title": "Explainable Molecular Property Prediction: Aligning Chemical Concepts with Predictions via Language Models", "link_suffix": "/forum?id=c9TSRcdqBf", "link": "https://openreview.net/forum?id=c9TSRcdqBf", "pdf_link": "https://openreview.net/pdf?id=c9TSRcdqBf", "keywords": "molecular property prediction, explainability", "abstract": "Providing explainable molecular property predictions is critical for many scientific domains, such as drug discovery and material science. Though transformer-based language models have shown great potential in accurate molecular property prediction, they neither provide chemically meaningful explanations nor faithfully reveal the molecular structure-property relationships. In this work, we develop a framework for explainable molecular property prediction based on language models, dubbed as Lamole, which can provide chemical concepts-aligned explanations. We take a string-based molecular representation --- Group SELFIES --- as input tokens to pretrain and fine-tune our Lamole, as it provides chemically meaningful semantics. By disentangling the information flows of Lamole,  we propose combining self-attention weights and gradients for better quantification of each chemically meaningful substructure's impact on the model's output. To make the explanations more faithfully respect the structure-property relationship, we then carefully craft a marginal loss to explicitly optimize the explanations to be able to align with the chemists' annotations. We bridge the manifold hypothesis with the elaborated marginal loss to prove that the loss can align the explanations with the tangent space of the data manifold, leading to concept-aligned explanations. Experimental results over six mutagenicity datasets and one hepatotoxicity dataset demonstrate Lamole can achieve comparable classification accuracy and boost the explanation accuracy by up to 14.3%, being the state-of-the-art in explainable molecular property prediction.", "title_embedding_index": 13315, "title_abs_embedding_index": 13340}, {"title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification and KV Cache Compression", "link_suffix": "/forum?id=bx0IbCcBvO", "link": "https://openreview.net/forum?id=bx0IbCcBvO", "pdf_link": "https://openreview.net/pdf?id=bx0IbCcBvO", "keywords": "Efficient Models, KV Cache Compression, Vision-language Models", "abstract": "The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance.  Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6$\\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.", "title_embedding_index": 13316, "title_abs_embedding_index": 13341}, {"title": "OLAPH: Improving Factuality in Biomedical Long-form Question Answering", "link_suffix": "/forum?id=o9ewXD1JuB", "link": "https://openreview.net/forum?id=o9ewXD1JuB", "pdf_link": "https://openreview.net/pdf?id=o9ewXD1JuB", "keywords": "medical question answering, automatic evaluation, factuality, hallucination", "abstract": "In the medical domain, numerous scenarios necessitate the long-form generation ability of large language models (LLMs). Specifically, when addressing patients' questions, it is essential that the model's response conveys factual claims, highlighting the need for an automated method to evaluate those claims. Thus, we introduce MedLFQA, a benchmark dataset reconstructed using long-form question-answering datasets related to the biomedical domain. We use MedLFQA to facilitate a cost-effective automatic evaluations of factuality. We also propose OLAPH, a simple and novel framework that utilizes cost-effective and multifaceted automatic evaluation to construct a synthetic preference set and answers questions in our preferred manner. Our framework leads us to train LLMs step-by-step to reduce hallucinations and include crucial medical claims. We highlight that, even on evaluation metrics not used during training, LLMs trained with our OLAPH framework demonstrate significant performance improvement in factuality. Our findings reveal that a 7B LLM trained with our OLAPH framework can provide long answers comparable to the medical experts' answers in terms of factuality. We believe that our work could shed light on gauging the long-text generation ability of LLMs in the medical domain. Our code and datasets are available.", "title_embedding_index": 13317, "title_abs_embedding_index": 13342}, {"title": "Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems", "link_suffix": "/forum?id=0FxnSZJPmh", "link": "https://openreview.net/forum?id=0FxnSZJPmh", "pdf_link": "https://openreview.net/pdf?id=0FxnSZJPmh", "keywords": "Inverse Problems, Stability, Operator Learning, Physics-Informed Machine Learning", "abstract": "Inverse problems involving partial differential equations (PDEs) can be seen as discovering a mapping from measurement data to unknown quantities, often framed within an operator learning approach. However, existing methods typically rely on large amounts of labeled training data, which is impractical for most real-world applications. Moreover, these supervised models may fail to capture the underlying physical principles accurately. To address these limitations, we propose a novel architecture called Physics-Informed Deep Inverse Operator Networks (PI-DIONs), which can learn the solution operator of PDE-based inverse problems without any labeled training data. We extend the stability estimates established in the inverse problem literature to the operator learning framework, thereby providing a robust theoretical foundation for our method. These estimates guarantee that the proposed model, trained on a finite sample and grid, generalizes effectively across the entire domain and function space. Extensive experiments are conducted to demonstrate that PI-DIONs can effectively and accurately learn the solution operators of the inverse problems without the need for labeled data.", "title_embedding_index": 13318, "title_abs_embedding_index": 13343}, {"title": "Physics-informed Dynamics Representation Learning for Parametric PDEs", "link_suffix": "/forum?id=nlHEfTRo0b", "link": "https://openreview.net/forum?id=nlHEfTRo0b", "pdf_link": "https://openreview.net/pdf?id=nlHEfTRo0b", "keywords": "physics-informed neural networks", "abstract": "While physics-informed neural networks have achieved remarkable progress in modeling dynamical systems governed by partial differential equations (PDEs), their ability to generalize across different scenarios remains restricted. \nTo address this limitation, we present PIDO, a novel physics-informed neural PDE solver that demonstrates robust generalization across various aspects of PDE configurations, including initial conditions, PDE coefficients, and training time horizons.\nPIDO leverages the shared intrinsic structure inherent to dynamical systems with varying properties by \nprojecting the PDE solutions into a latent space via auto-decoding and subsequently learning the dynamics of these latent embeddings conditioned on the PDE coefficients.\nHowever, the inherent optimization challenges associated with physics-informed loss present substantial obstacles to integrating such latent dynamics models. \nTo tackle this issue, we adopt a novel perspective by diagnosing these challenges within the latent space. This approach enables us to enhance both temporal extrapolation ability and training stability of PIDO via simple yet effective regularization techniques, ultimately leading to superior generalization performance compared to its data-driven counterpart.\nThe effectiveness of PIDO is validated on diverse benchmarks, including 1D combined equations and 2D Navier-Stokes equations. Moreover, we investigate the transferability of its learned representations to downstream tasks like long-term integration and inverse problems.", "title_embedding_index": 13319, "title_abs_embedding_index": 13344}, {"title": "AMAP: Automatic Multi-head Attention Pruning by similarity-based pruning indicator", "link_suffix": "/forum?id=8GMUa79ZKc", "link": "https://openreview.net/forum?id=8GMUa79ZKc", "pdf_link": "https://openreview.net/pdf?id=8GMUa79ZKc", "keywords": "Automatic Pruning, Vision Transformer, Multi-Head Pruning, Channel Similarity, Score Adjustment, Reweight Module", "abstract": "Despite the strong performance of Transformers, quadratic computation complexity of self-attention presents challenges in applying them to vision tasks. Linear attention reduces this complexity from quadratic to linear, offering a strong computation-performance trade-off. To further optimize this, automatic pruning is an effective method to find a structure that maximizes performance within a target resource through training without any heuristic approaches. However, directly applying it to multi-head attention is not straightforward due to channel mismatch. In this paper, we propose an automatic pruning method to deal with this problem. Different from existing methods that rely solely on training without any prior knowledge, we integrate channel similarity-based weights into the pruning indicator to preserve the more informative channels within each head. Then, we adjust the pruning indicator to enforce that channels are removed evenly across all heads, thereby avoiding any channel mismatch. We incorporate a reweight module to mitigate information loss due to channel removal and introduce an effective pruning indicator initialization for linear attention, based on the attention differences between the original structure and each channel. By applying our pruning method to the FLattenTransformer on ImageNet-1K, which incorporates original and linear attention mechanisms, we achieve a 30% reduction of FLOPs in a near lossless manner. It also has 1.96% of accuracy gain over the DeiT-B model while reducing FLOPs by 37%, and 1.05% accuracy increase over the Swin-B model with a 10% reduction in FLOPs as well. The proposed method outperforms previous state-of-the-art efficient models and the recent pruning methods.", "title_embedding_index": 13320, "title_abs_embedding_index": 13345}, {"title": "X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing", "link_suffix": "/forum?id=b42wmsdwmB", "link": "https://openreview.net/forum?id=b42wmsdwmB", "pdf_link": "https://openreview.net/pdf?id=b42wmsdwmB", "keywords": "human sensing, multimodal learning", "abstract": "Human sensing, which employs various sensors and advanced deep learning technologies to accurately capture and interpret human body information, has significantly impacted fields like public security and robotics. However, current human sensing primarily depends on modalities such as cameras and LiDAR, each of which has its own strengths and limitations. Furthermore, existing multi-modal fusion solutions are typically designed for fixed modality combinations, requiring extensive retraining when modalities are added or removed for diverse scenarios. In this paper, we propose a modality-invariant foundation model for all modalities, X-Fi, to address this issue. X-Fi enables the independent or combinatory use of sensor modalities without additional training by utilizing a transformer structure to accommodate variable input sizes and incorporating a novel ``X-fusion\" mechanism to preserve modality-specific features during multimodal integration. This approach not only enhances adaptability but also facilitates the learning of complementary features across modalities. Extensive experiments conducted on the MM-Fi and XRF55 datasets, employing six distinct modalities, demonstrate that X-Fi achieves state-of-the-art performance in human pose estimation (HPE) and human activity recognition (HAR) tasks. The findings indicate that our proposed model can efficiently support a wide range of human sensing applications, ultimately contributing to the evolution of scalable, multimodal sensing technologies.", "title_embedding_index": 13321, "title_abs_embedding_index": 13346}, {"title": "LARM: Large Auto-Regressive Model for Long-Horizon Embodied Intelligence", "link_suffix": "/forum?id=anAZ42rYFK", "link": "https://openreview.net/forum?id=anAZ42rYFK", "pdf_link": "https://openreview.net/pdf?id=anAZ42rYFK", "keywords": "Embodied Intelligence, Minecraft, Large Language Model, Large Auto-regressive Model", "abstract": "Due to the need of interacting with the world, embodied agents are required to possess comprehensive task-relevant knowledge, long-horizon planning capability, and a swift response speed. Large language models (LLMs), owing to their rich general knowledge, recently achieve promising results in open-world embodied tasks, like the world exploration in Minecraft. However, the outputs of LLMs are descriptive sentences or code, which are slow to generate and not end-to-end, as a translator is required to translate the LLM outputs into actions to perform. To address these limitations, we introduce the large auto-regressive model (LARM). LARM leverages environment observations as input and predicts subsequent actions in an auto-regressive manner. Compared with LLM based methods, LARM directly predicts the next skill for execution according to the current observation. In addition, considering that the commonly adopted training paradigms do not reflect the mutual influence and dependency between actions and observations, we develop a novel data format named auto-regressive node transmission structure and assemble a corresponding dataset to train LARM. Combining these techniques, LARM successfully harvests enchanted equipment in Minecraft, which demands significantly more complex decision-making chains than the highest achievements of prior best methods. Besides, the speed of LARM is 6.8x faster than LLMs with similar parameter volume.", "title_embedding_index": 13322, "title_abs_embedding_index": 13347}, {"title": "Pre-Training Robo-Centric World Models For Efficient Visual Control", "link_suffix": "/forum?id=DJw1JBTmuk", "link": "https://openreview.net/forum?id=DJw1JBTmuk", "pdf_link": "https://openreview.net/pdf?id=DJw1JBTmuk", "keywords": "Pretraining World Models, Model-based Reinforcement Learning, Visual Robot Control", "abstract": "Humans can accurately anticipate their movements to behave as expected in various manipulation tasks. We are inspired to propose that integrating prior knowledge of robot dynamics into world models can effectively improve the sample efficiency of model-based reinforcement learning (MBRL) in visual robot control tasks. In this paper, we introduce the Robo-Centric World Model (RCWM), which explicitly decouples the robot dynamics from the environment and enables pre-training to learn generalized and robust robot dynamics as prior knowledge to accelerate learning new tasks. Specifically, we construct respective dynamics models for the robot and the environment and learn their interactions through cross-attention mechanism. With the mask-guided reconfiguration mechanism, we only need a few prior robot segmentation masks to guide the RCWM to disentangle the robot and environment features and learn their respective dynamics. Our approach enables independent inference of robot dynamics from the environment, allowing accurate prediction of robot movement across various unseen tasks without being distracted by environmental variations. Our results in Meta-world demonstrate that RCWM is able to efficiently learn robot dynamics, improving sample efficiency for downstream tasks and enhancing policy robustness against environmental disturbances compared to the vanilla world model in DreamerV3. Code and visualizations are available on the project website:https://robo-centric-wm.github.io.", "title_embedding_index": 13323, "title_abs_embedding_index": 13348}, {"title": "Entropy-based Activation Function Optimization: A Method on Searching Better Activation Functions", "link_suffix": "/forum?id=7TZYM6Hm9p", "link": "https://openreview.net/forum?id=7TZYM6Hm9p", "pdf_link": "https://openreview.net/pdf?id=7TZYM6Hm9p", "keywords": "Deep Learning, Activation Functions, Information Entropy", "abstract": "The success of artificial neural networks (ANNs) hinges greatly on the judicious selection of an activation function, introducing non-linearity into network and enabling them to model sophisticated relationships in data. However, the search of activation functions has largely relied on empirical knowledge in the past, lacking theoretical guidance, which has hindered the identification of more effective activation functions. In this work, we offer a proper solution to such issue. Firstly, we theoretically demonstrate the existence of the worst activation function with boundary conditions (WAFBC) from the perspective of information entropy. Furthermore, inspired by the Taylor expansion form of information entropy functional, we propose the Entropy-based Activation Function Optimization (EAFO) methodology. EAFO methodology presents a novel perspective for designing static activation functions in deep neural networks and the potential of dynamically optimizing activation during iterative training. Utilizing EAFO methodology, we derive a novel activation function from ReLU, known as Correction Regularized ReLU (CRReLU). Experiments conducted with vision transformer and its variants on CIFAR-10, CIFAR-100 and ImageNet-1K datasets demonstrate the superiority of CRReLU over  existing corrections of ReLU. Extensive empirical studies on task of large language model (LLM) fine-tuning, CRReLU exhibits superior performance compared to GELU, suggesting its broader potential for practical applications.", "title_embedding_index": 13324, "title_abs_embedding_index": 13349}]