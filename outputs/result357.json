[{"title": "RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards", "link_suffix": "/forum?id=Pnktu2PBXD", "link": "https://openreview.net/forum?id=Pnktu2PBXD", "pdf_link": "https://openreview.net/pdf?id=Pnktu2PBXD", "keywords": "Retrieval-Augmented Generation", "abstract": "Retrieval-Augmented Generation (RAG) has proven its effectiveness in mitigating hallucinations in Large Language Models (LLMs) by retrieving knowledge from external resources. To adapt LLMs for RAG pipelines, current approaches use instruction tuning to optimize LLMs, improving their ability to utilize retrieved knowledge. This supervised fine-tuning (SFT) approach focuses on equipping LLMs to handle diverse RAG tasks using different instructions. However, it trains RAG modules to overfit training signals and overlooks the varying data preferences among agents within the RAG system. In this paper, we propose a Differentiable Data Rewards (DDR) method, which end-to-end trains RAG systems by aligning data preferences between different RAG modules. DDR works by collecting the rewards to optimize each agent with a rollout method. This method prompts agents to sample some potential responses as perturbations, evaluates the impact of these perturbations on the whole RAG system, and subsequently optimizes the agent to produce outputs that improve the performance of the RAG system. Our experiments on various knowledge-intensive tasks demonstrate that DDR significantly outperforms the SFT method, particularly for LLMs with smaller-scale parameters that depend more on the retrieved knowledge. Additionally, DDR exhibits a stronger capability to align the data preference between RAG modules. The DDR method makes generation module more effective in extracting key information from documents and mitigating conflicts between parametric memory and external knowledge. All codes will be released via GitHub.", "title_embedding_index": 17800, "title_abs_embedding_index": 17825}, {"title": "Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research", "link_suffix": "/forum?id=4gaySj8kvX", "link": "https://openreview.net/forum?id=4gaySj8kvX", "pdf_link": "https://openreview.net/pdf?id=4gaySj8kvX", "keywords": "Deep Reinforcement Learning, GPU-accelerated Physics Simulators, Contrastive Learning, Unsupervised Reinforcement Learning", "abstract": "Self-supervision has the potential to transform reinforcement learning (RL), paralleling the breakthroughs it has enabled in other areas of machine learning. While self-supervised learning in other domains aims to find patterns in a fixed dataset, self-supervised goal-conditioned reinforcement learning (GCRL) agents discovernewbehaviors by learning from the goals achieved during unstructured interaction with the environment. However, these methods have failed to see similar success, both due to a lack of data from slow environment simulations as well as a lack of stable algorithms. We take a step toward addressing both of these issues by releasing a high-performance codebase and benchmark (JaxGCRL) for self-supervised GCRL, enabling researchers to train agents for millions of environment steps in minutes on a single GPU. By utilizing GPU-accelerated replay buffers, environments, and a stable contrastive RL algorithm, we reduce training time by up to $22\\times$. Additionally, we assess key design choices in contrastive RL, identifying those that most effectively stabilize and enhance training performance. With this approach, we provide a foundation for future research in self-supervised GCRL, enabling researchers to quickly iterate on new ideas and evaluate them in diverse and challenging environments. Code:https://anonymous.4open.science/r/JaxGCRL-2316/README.md", "title_embedding_index": 17801, "title_abs_embedding_index": 17826}, {"title": "Self-controller: Controlling LLMs with Multi-round Step-by-step Self-awareness", "link_suffix": "/forum?id=GjSstLcxAs", "link": "https://openreview.net/forum?id=GjSstLcxAs", "pdf_link": "https://openreview.net/pdf?id=GjSstLcxAs", "keywords": "large language models, self-awareness, agent, controllability", "abstract": "The applications of large language models (LLMs) have been widely spread across all domains. \nHowever, the basic abilities such as the controllability of LLMs are still limited.\nTo address this, we propose \"$\\textbf{Self-controller}$\", a novel agentic framework bringing self-awareness into LLMs\u2019 reasoning logic.\nThe core idea of this work is to maintain states based on the LLM's response, letting the LLM become self-aware of current status and think step by step in a multi-round chain-of-thought paradigm. \nOur experiment on the state of textual length has shown the controllability and effectiveness of the Self-controller. We further implement a binary search algorithm to accelerate the generation process based on the linearity and monotonicity of the textual length state. Another advantage of the Self-controller comes with DeepSeek's Context Caching technology, which significantly saves computational token consumption when a cluster of conversations shares the same prefix of context. Theoretically, we prove that in this scenario the extra time complexity is  $O(c \\log n)$. Results of the back-of-the-envelope estimation suggest that the token consumption of our method is no more than twice as much as that of the trivial single-round generation. Furthermore, our ablation study on word constraints demonstrates the Self-controller's consistent controllability across all foundation models.", "title_embedding_index": 17802, "title_abs_embedding_index": 17827}, {"title": "ViDROP: Video Dense Representation through Spatio-Temporal Sparsity", "link_suffix": "/forum?id=LRizkALc84", "link": "https://openreview.net/forum?id=LRizkALc84", "pdf_link": "https://openreview.net/pdf?id=LRizkALc84", "keywords": "Self-Supervised Learning, Video Representation Learning, Vision Transformers", "abstract": "Self-supervised learning (SSL) has revolutionized image processing, but extending its success to video understanding presents unique challenges due to increased data complexity and computational demands. We introduce ViDROP (Video Dense Representation thrOugh spatio-temporal sParsity), a novel SSL architecture for video understanding that combines token dropping and masking strategies. \nOur approach eliminates the need for a decoder and enables per-patch loss computation, overcoming limitations of previous video SSL methods. Moreover, we propose a simple yet effective video compression technique using k-means clustering in pixel space, significantly accelerating data loading and facilitating rapid experimentation. ViDROP demonstrates remarkable scalability across model sizes, from ViT-Small to ViT-Huge, when starting from pretrained models (VideoMAE or V-JEPA), achieving significant performance gains. Pushing the boundaries even further, we leverage network expansion techniques to successfully train ViT-Huge from scratch using modest computational resources, achieving comparable accuracy to VideoMAE 25$\\times$ faster in training time. This marks a significant breakthrough in large-scale video SSL, enabling the training of state-of-the-art models with limited resources.\nExtensive experiments show that ViDROP achieves state-of-the-art performance on various video understanding benchmarks, including Kinetics400, SSv2, UCF101, and HMDB51, as well as in temporal action detection (THUMOS14). These results highlight the effectiveness of our fine-grained token-level learning strategy in a domain traditionally dominated by fine-tuned SSL models, while enabling the training of large-scale models with limited computational resources.", "title_embedding_index": 17803, "title_abs_embedding_index": 17828}, {"title": "MTMC: Generalized Category Discovery via Maximum Token Manifold Capacity", "link_suffix": "/forum?id=vkOaerjEcz", "link": "https://openreview.net/forum?id=vkOaerjEcz", "pdf_link": "https://openreview.net/pdf?id=vkOaerjEcz", "keywords": "generalized category discovery, deep cluster, manifold capacity", "abstract": "Identifying previously unseen data is crucial for enhancing the robustness of deep learning models in the open world. Generalized category discovery (GCD) is a representative problem that requires clustering unlabeled data that includes known and novel categories. Current GCD methods mostly focus on minimizing intra-cluster variations, often at the cost of manifold capacity, thus limiting the richness of within-class representations. In this paper, we introduce a novel GCD approach that emphasizes maximizing the token manifold capacity (MTMC) within class tokens, thereby preserving the diversity and complexity of the data's intrinsic structure. Specifically, MTMC's efficacy is fundamentally rooted in its ability to leverage the nuclear norm of the singular values as a quantitative measure of the manifold capacity. MTMC enforces a richer and more informative representation within the manifolds of different patches constituting the same sample. MTMC ensures that, for each cluster, the representations of different patches of the same sample are compact and lie in a low-dimensional space, thereby enhancing discriminability. By doing so, the model could capture each class's nuanced semantic details and prevent the loss of critical information during the clustering process. MTMC promotes a comprehensive, non-collapsed representation that improves inter-class separability without adding excessive complexity.", "title_embedding_index": 17804, "title_abs_embedding_index": 17829}, {"title": "Selective Aggregation for Low-Rank Adaptation in Federated Learning", "link_suffix": "/forum?id=iX3uESGdsO", "link": "https://openreview.net/forum?id=iX3uESGdsO", "pdf_link": "https://openreview.net/pdf?id=iX3uESGdsO", "keywords": "federated learning, low-rank adaptation", "abstract": "We investigate LoRA in federated learning through the lens of the asymmetry analysis of the learned $A$ and $B$ matrices. In doing so, we uncover that $A$ matrices are responsible for learning general knowledge, while $B$ matrices focus on capturing client-specific knowledge. Based on this finding, we introduce Federated Share-A Low-Rank Adaptation (FedSA-LoRA), which employs two low-rank trainable matrices $A$ and $B$ to model the weight update, but only $A$ matrices are shared with the server for aggregation. Moreover, we delve into the relationship between the learned $A$ and $B$ matrices in other LoRA variants, such as rsLoRA and VeRA, revealing a consistent pattern. Consequently, we extend our FedSA-LoRA method to these LoRA variants, resulting in FedSA-rsLoRA and FedSA-VeRA. In this way, we establish a general paradigm for integrating LoRA with FL, offering guidance for future work on subsequent LoRA variants combined with FL. Extensive experimental results on natural language understanding and generation tasks demonstrate the effectiveness of the proposed method. Our code is available athttps://anonymous.4open.science/r/FedSA-LoRA-3498/.", "title_embedding_index": 17805, "title_abs_embedding_index": 17830}, {"title": "O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions", "link_suffix": "/forum?id=4EjdYiNRzE", "link": "https://openreview.net/forum?id=4EjdYiNRzE", "pdf_link": "https://openreview.net/pdf?id=4EjdYiNRzE", "keywords": "score-based generative model, diffusion model, denoising diffusion probabilistic model, sampling", "abstract": "Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical guarantees are often constrained by stringent assumptions or suboptimal convergence rates. In this paper, we establish a fast convergence theory for a popular SDE-based sampler under minimal assumptions. Our analysis shows that, provided $\\ell_{2}$-accurate estimates of the score functions, the total variation distance between the target and generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionality and $T$ is the number of steps. This result holds for any target distribution with finite first-order moment. To our knowledge, this improves upon existing convergence theory for both the SDE-based sampler and another ODE-based sampler, while imposing minimal assumptions on the target data distribution and score estimates. This is achieved through a novel set of analytical tools that provides a fine-grained characterization of how the error propagates at each step of the reverse process.", "title_embedding_index": 17806, "title_abs_embedding_index": 17831}, {"title": "GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation", "link_suffix": "/forum?id=fIMf9zQo9d", "link": "https://openreview.net/forum?id=fIMf9zQo9d", "pdf_link": "https://openreview.net/pdf?id=fIMf9zQo9d", "keywords": "3D generation; Text to 3D; Image to 3D; AIGC", "abstract": "Text-to-3D generation by distilling pretrained large-scale text-to-image diffusion models has shown great promise but still suffers from inconsistent 3D geometric structures (Janus problems) and severe artifacts. The aforementioned problems mainly stem from 2D diffusion models lacking 3D awareness during the lifting. In this work, we present GeoDream, a novel method that incorporates explicit generalized 3D priors with 2D diffusion priors to enhance the capability of obtaining unambiguous 3D consistent geometric structures without sacrificing diversity or fidelity. Specifically, we first utilize a multi-view diffusion model to generate posed images and then construct cost volume from the predicted image, which serves as native 3D geometric priors, ensuring spatial consistency in 3D space. Subsequently, we further propose to harness 3D geometric priors to unlock the great potential of 3D awareness in 2D diffusion priors via a disentangled design. Notably, disentangling 2D and 3D priors allows us to refine 3D geometric priors further. We justify that the refined 3D geometric priors aid in the 3D-aware capability of 2D diffusion priors, which in turn provides superior guidance for the refinement of 3D geometric priors. Our numerical and visual comparisons demonstrate that GeoDream generates more 3D consistent textured meshes with high-resolution realistic renderings (i.e., 1024 * 1024) and adheres more closely to semantic coherence.", "title_embedding_index": 17807, "title_abs_embedding_index": 17832}, {"title": "Bag-level Self-supervised instance based distance in Multiple Instance Learning", "link_suffix": "/forum?id=8vUcEqFGE1", "link": "https://openreview.net/forum?id=8vUcEqFGE1", "pdf_link": "https://openreview.net/pdf?id=8vUcEqFGE1", "keywords": "Multiple Instance Learning, Self supervised, Bag, Instance, Energy distance, Embedding", "abstract": "Multiple Instance Learning (MIL) methods are typically supervised. However,  a bag-to-bag metric is needed in many applications, including clustering, statistical tests, and dimension reduction.\nSuch a metric should differentiate between bags, regardless of the sparsity or overlap between the instances of the bags.  We propose SUMIT (Self sUpervised MIL dIsTance) as an instance-embedding-based distance that maximizes the distinction between bags. SUMIT is optimized using five criteria: self-similarity within a bag, quality of instance reconstruction, robustness to sampling depth, conservation of triangle inequality, and separation of instances to clusters. We show using current standard MIL datasets and a novel wiki-based set of wiki topics that the within bag-similarity loss is the most important for a bag-to-bag metric that best separates bags of similar classes.  SUMIT  bridges the gap between instance-level and bag-level approaches, by keeping the embedding of all instances but ensuring their proximity within a bag.", "title_embedding_index": 17808, "title_abs_embedding_index": 17833}, {"title": "VIA: Unified Spatiotemporal Video Adaptation for Global and Local Video Editing", "link_suffix": "/forum?id=mhFToLPjM5", "link": "https://openreview.net/forum?id=mhFToLPjM5", "pdf_link": "https://openreview.net/pdf?id=mhFToLPjM5", "keywords": "Video Editing, Generative AI", "abstract": "Video editing is a cornerstone of digital media, from entertainment and education to professional communication. However, previous methods often overlook the necessity of comprehensively understanding both global and local contexts, leading to inaccurate and inconsistent edits in the spatiotemporal dimension, especially for long videos.\nIn this paper, we introduce VIA, a unified spatiotemporal Video Adaptation framework for global and local video editing, pushing the limits of consistently editing minute-long videos. First, to ensure local consistency within individual frames, we designed test-time editing adaptation to adapt a pre-trained image editing model for improving consistency between potential editing directions and the text instruction, and adapts masked latent variables for precise local control. Furthermore, to maintain global consistency over the video sequence, we introduce spatiotemporal adaptation that recursively gather consistent attention variables in key frames and strategically applies them across the whole sequence to realize the editing effects. Extensive experiments demonstrate that, compared to baseline methods, our VIA approach produces edits that are more faithful to the source videos, more coherent in the spatiotemporal context, and more precise in local control. More importantly, we show that VIA can achieve consistent long video editing in minutes, unlocking the potential for advanced video editing tasks over long video sequences.", "title_embedding_index": 17809, "title_abs_embedding_index": 17834}, {"title": "AgentMonitor: A Plug-and-Play Framework for Predictive and Secure Multi-Agent Systems", "link_suffix": "/forum?id=gKM8wwsTOg", "link": "https://openreview.net/forum?id=gKM8wwsTOg", "pdf_link": "https://openreview.net/pdf?id=gKM8wwsTOg", "keywords": "LLM-Agents, Multi-Agents, Large Language Models", "abstract": "Rapid advancement of large language models (LLMs) has catalyzed the emergence of LLM-based agents. Recent research has shifted from single-agent systems to multi-agent frameworks, demonstrating that collaboration can outperform the capabilities of individual LLMs. However, effectively pre-configuring a Multi-Agent System (MAS) for a specific task remains a challenging problem, with performance outcomes only observable after execution. Inspired by the well-established scaling laws in LLM development that model downstream task performance or validation loss as functions of various factors during training, we seek to investigate the predictability of MAS performance. Specifically, we explore whether it is possible to predict the downstream task performance of a configured MAS. In addition, MAS face a growing challenge in ensuring reliable and trustworthy responses. The introduction of malicious agents can lead to the generation and spread of harmful content, which poses significant security risks. To address the above issues, we introduce AgentMonitor, a framework that integrates with existing MAS at the agent level. AgentMonitor captures inputs and outputs at each step; this enables (1) transforming them into relevant statistics supporting the training of a regression model to predict task performance and (2) the application of on-the-fly corrections to mitigate negative impacts on final outcomes. Extensive experiments demonstrate that training a simple XGBoost model achieves a high Spearman rank correlation of 0.89 in an in-domain setting. In more challenging scenarios, where the statistics of a specific task or architecture is absent from the training set, our method maintains a moderate average correlation of 0.58. Furthermore, by employing AgentMonitor in a maliciously configured MAS, the system ultimately generates 6.2% less harmful content and 1.8% more helpful content on average, reducing safety risks and improving reliability.", "title_embedding_index": 17810, "title_abs_embedding_index": 17835}, {"title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models", "link_suffix": "/forum?id=4dAhjhm2Mm", "link": "https://openreview.net/forum?id=4dAhjhm2Mm", "pdf_link": "https://openreview.net/pdf?id=4dAhjhm2Mm", "keywords": "score-based density formula, score-based generative model, evidence lower bound, denoising diffusion probabilistic model", "abstract": "Score-based generative models (SGMs) have revolutionized the field of generative modeling, achieving unprecedented success in generating realistic and diverse content. Despite empirical advances, the theoretical basis for why optimizing the evidence lower bound (ELBO) on the log-likelihood is effective for training diffusion generative models, such as DDPMs, remains largely unexplored. In this paper, we address this question by establishing a density formula for a continuous-time diffusion process, which can be viewed as the continuous-time limit of the forward process in an SGM. This formula reveals the connection between the target density and the score function associated with each step of the forward process. Building on this, we demonstrate that the minimizer of the optimization objective for training DDPMs nearly coincides with that of the true objective, providing a theoretical foundation for optimizing DDPMs using the ELBO. Furthermore, we offer new insights into the role of score-matching regularization in training GANs, the use of ELBO in diffusion classifiers, and the recently proposed diffusion loss.", "title_embedding_index": 17811, "title_abs_embedding_index": 17836}, {"title": "PSHuman: Photorealistic Single-view Human Reconstruction using Cross-Scale Diffusion", "link_suffix": "/forum?id=8EaDOGMPUL", "link": "https://openreview.net/forum?id=8EaDOGMPUL", "pdf_link": "https://openreview.net/pdf?id=8EaDOGMPUL", "keywords": "Human recontruction, Cross-scale diffusion, Generative model", "abstract": "Detailed and photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, full-body reconstruction from a monocular RGB image remains challenging due to the ill-posed nature of the problem and sophisticated clothing topology with self-occlusions. In this paper, we proposePSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model. It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling detailed and identity-preserved novel-view generation without any geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models like SMPL-X, which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multi-view normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experimental results and quantitative evaluations on CAPE and THuman2.1 datasets demonstrate PSHumans superiority in geometry details, texture fidelity, and generalization capability.", "title_embedding_index": 17812, "title_abs_embedding_index": 17837}, {"title": "DFED: Data-Free Ensemble Distillation with Multi-Source GANs for Heterogeneous Federated Learning", "link_suffix": "/forum?id=p3NVJg6ywM", "link": "https://openreview.net/forum?id=p3NVJg6ywM", "pdf_link": "https://openreview.net/pdf?id=p3NVJg6ywM", "keywords": "Federated learning, Data heterogeneity, Data-Free Knowledge Distillation", "abstract": "Federated Learning (FL) is a decentralized machine learning paradigm that enables clients to collaboratively train models while preserving data privacy. However, surmounting the obstacles introduced by data heterogeneity in heterogeneous federated learning remains a profound challenge, as it drives each client towards distinct convergence trajectories, impeding the global model's convergence. To transcend these challenges, we propose DFED, a novel data-free ensemble knowledge distillation method designed to counteract the effects of data heterogeneity. DFED leverages multi-source Generative Adversarial Networks (GANs) to generate synthetic data that aligns with local distributions, ensuring privacy while promoting diverse feature representations across clients. Additionally, DFED aggregates client models into an ensemble based on their specialized knowledge, and applies ensemble distillation to refine the global model, mitigating the issues caused by disparities in data distributions. Across a variety of image classification benchmarks, DFED demonstrates superior performance compared to several state-of-the-art (SOTA) methods. The source code will be made publicly accessible once the paper has been accepted for publication.", "title_embedding_index": 17813, "title_abs_embedding_index": 17838}, {"title": "D2G: Debiased Learning with Distribution Guidance for Generalized Category Discovery", "link_suffix": "/forum?id=9B8o9AxSyb", "link": "https://openreview.net/forum?id=9B8o9AxSyb", "pdf_link": "https://openreview.net/pdf?id=9B8o9AxSyb", "keywords": "Generalized Category Discovery, Semi-supervised Learning, Out-of-distribution Detection", "abstract": "In this paper, we tackle the problem of Generalized Category Discovery (GCD). Given a dataset containing both labelled and unlabelled images, the objective is to cluster all images in the unlabelled subset, irrespective of whether they are from known or unknown classes. \nIn GCD, an inherent label bias exists between known and unknown classes due to the lack of ground-truth labels for the latter. State-of-the-art GCD methods employ parametric classifiers trained with self-distillation using soft labels, leaving the bias issue unattended. Besides, they treat all unlabelled samples uniformly, neglecting variations in certainty levels and resulting in suboptimal learning. Moreover, the explicit identification of semantic distribution shifts between known and unknown classes, a vital aspect for effective GCD, has been neglected. To overcome these obstacles, we introduce the \\textbf{D}ebiased Learning with \\textbf{D}istribution \\textbf{G}uidance (\\textbf{D2G}) framework. Initially, D2G co-trains an auxiliary debiased classifier in the same feature space as the GCD classifier, progressively enhancing the GCD features. Moreover, we introduce a semantic distribution detector in a separate feature space to implicitly boost the learning efficacy of GCD. Additionally, we employ a curriculum learning strategy based on semantic distribution certainty to steer the debiased learning at an optimized pace. Thorough evaluations on GCD benchmarks demonstrate the consistent state-of-the-art performance of our D2G framework, highlighting its superiority.", "title_embedding_index": 17814, "title_abs_embedding_index": 17839}, {"title": "Video-to-Audio generation with Hidden Alignment", "link_suffix": "/forum?id=YQjdNC0NkW", "link": "https://openreview.net/forum?id=YQjdNC0NkW", "pdf_link": "https://openreview.net/pdf?id=YQjdNC0NkW", "keywords": "audio generation, video-to-audio, diffusion", "abstract": "Generating semantically and temporally aligned audio content in accordance with video input has become a focal point for researchers, particularly following the remarkable breakthrough in text-to-video generation. In this work, we aim to offer insights into the video-to-audio generation paradigm, focusing on three crucial aspects: vision encoders, auxiliary embeddings, and data augmentation techniques.\nBeginning with a foundational model built on a simple yet surprisingly effective intuition, we explore various vision encoders and auxiliary embeddings through ablation studies. Employing a comprehensive evaluation pipeline that emphasizes generation quality and video-audio synchronization alignment, we demonstrate that our model exhibits state-of-the-art video-to-audio generation capabilities. Furthermore, we provide critical insights into the impact of different data augmentation methods on enhancing the generation framework\u2019s overall capacity. We showcase possibilities to advance the challenge of generating synchronized audio from semantic and temporal perspectives. We hope these insights will serve as a stepping stone toward developing more realistic and accurate audio-visual generation models.", "title_embedding_index": 17815, "title_abs_embedding_index": 17840}, {"title": "GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training", "link_suffix": "/forum?id=6RiBl5sCDF", "link": "https://openreview.net/forum?id=6RiBl5sCDF", "pdf_link": "https://openreview.net/pdf?id=6RiBl5sCDF", "keywords": "Geometry Problem Solving, Complicated Task Reasoning", "abstract": "Despite their proficiency in general tasks, Multi-modal Large Language Models (MLLMs) struggle with automatic Geometry Problem Solving (GPS), which demands understanding diagrams, interpreting symbols, and performing complex reasoning. This limitation arises from their pre-training on natural images and texts, along with the lack of automated verification in the problem-solving process. Besides, current geometric specialists are limited by their task-specific designs, making them less effective for broader geometric problems. To this end, we present GeoX, a multi-modal large model focusing on geometric understanding and reasoning tasks. Given the significant differences between geometric diagram-symbol and natural image-text, we introduce unimodal pre-training to develop a diagram encoder and symbol decoder, enhancing the understanding of geometric images and corpora. Furthermore, we introduce geometry-language alignment, an effective pre-training paradigm that bridges the modality gap between unimodal geometric experts. We propose a Generator-And-Sampler Transformer (GS-Former) to generate discriminative queries and eliminate uninformative representations from unevenly distributed geometric signals. Finally, GeoX benefits from visual instruction tuning, empowering it to take geometric images and questions as input and generate verifiable solutions. Experiments show that GeoX outperforms both generalists and geometric specialists on publicly recognized benchmarks, such as GeoQA, UniGeo, Geometry3K, and PGPS9k. Our data and code will be released soon to accelerate future research on automatic GPS.", "title_embedding_index": 17816, "title_abs_embedding_index": 17841}, {"title": "Oblivious Unlearning by Learning: Machine Unlearning Without Exposing Erased Data", "link_suffix": "/forum?id=wAemQcyWqq", "link": "https://openreview.net/forum?id=wAemQcyWqq", "pdf_link": "https://openreview.net/pdf?id=wAemQcyWqq", "keywords": "Machine unlearning, Privacy Preserving", "abstract": "Machine unlearning enables users to remove the influence of their data from trained models, thus protecting their privacy. However, it is paradoxical that most unlearning methods require users first to upload their to-be-removed data to machine learning servers and notify the servers of their unlearning intentions to prepare appropriate unlearning methods. Both unlearned data and unlearning intentions are sensitive user information. Exposing this information to the server for unlearning operations conflicts with the privacy protection goal. In this paper, we investigate the challenge of implementing unlearning without exposing erased data and unlearning intentions to the server. We propose an Oblivious Unlearning by Learning (OUbL) approach to address this privacy-preserving machine unlearning problem. In OUbL, the users construct a new dataset with synthesized unlearning noise, ensuring that once the server continually updates the model using the original learning algorithm based on this dataset, it can implement unlearning. The server does not need to perform any tailored unlearning operation and remains unaware that the constructed samples are for unlearning. As a result, the process is oblivious to the server regarding unlearning intentions. Additionally, by transforming the original erased data into unlearning noise and distributing this noise across numerous auxiliary samples, our approach protects the privacy of the unlearned data while effectively implementing unlearning. The effectiveness of the proposed OUbL method is evaluated through extensive experiments on three representative datasets across various model architectures and four mainstream unlearning benchmarks. The results demonstrate the significant superiority of OUbL over the state-of-the-art privacy-preserving unlearning benchmarks in terms of both privacy protection and unlearning effectiveness.", "title_embedding_index": 17817, "title_abs_embedding_index": 17842}, {"title": "RelChaNet: Neural Network Feature Selection using Relative Change Scores", "link_suffix": "/forum?id=3M3jtMDjUb", "link": "https://openreview.net/forum?id=3M3jtMDjUb", "pdf_link": "https://openreview.net/pdf?id=3M3jtMDjUb", "keywords": "Feature Selection, Neural Networks, Pruning", "abstract": "There is an ongoing effort to develop feature selection algorithms to improve interpretability, reduce computational resources, and minimize overfitting in predictive models. Neural networks stand out as architectures on which to build feature selection methods, and recently, neuron pruning and regrowth have emerged from the sparse neural network literature as promising new tools. We introduce RelChaNet, a novel and lightweight feature selection algorithm that uses neuron pruning and regrowth in the input layer of a dense neural network. For neuron pruning, a gradient sum metric measures the relative change induced in a network after a feature enters, while neurons are randomly regrown. We also propose an extension that adapts the size of the input layer at runtime. Extensive experiments on nine different datasets show that our approach generally outperforms the current state-of-the-art methods, and in particular improves the average accuracy by 2% on the MNIST dataset. Our code is available in the supplementary material.", "title_embedding_index": 17818, "title_abs_embedding_index": 17843}, {"title": "Towards Infinite-Long Prefix in Transformer", "link_suffix": "/forum?id=gzmInLJSoW", "link": "https://openreview.net/forum?id=gzmInLJSoW", "pdf_link": "https://openreview.net/pdf?id=gzmInLJSoW", "keywords": "Large Language Model, Prefix Learning, Neural Tangent Kernel", "abstract": "Prompting and context-based fine-tuning methods, which we call Prefix Learning, have been proposed to enhance the performance of language models on various downstream tasks. They are empirically efficient and effective, matching the performance of full parameter fine-tuning, but the theoretical understandings are limited. In this paper, we aim to address this limitation by studying their ability from the perspective of prefix length. \nIn particular, we provide a convergence guarantee for training an ultra-long prefix in a stylized setting using the Neural Tangent Kernel (NTK) framework. Based on this strong theoretical guarantee, we design and implement an algorithm that only needs to introduce and fine-tune a few extra trainable parameters instead of an infinite-long prefix in each layer of a transformer, and can approximate the prefix attention to a guaranteed polynomial-small error.\nPreliminary experimental results on vision, natural language, and math data show that our method achieves superior or competitive performance compared to existing methods like full parameters fine-tuning, P-Tuning V2, and LoRA. This demonstrates our method is promising for parameter-efficient fine-tuning.", "title_embedding_index": 17819, "title_abs_embedding_index": 17844}, {"title": "ELU-GCN:  Effectively Label-Utilizing Graph Convolutional Network", "link_suffix": "/forum?id=tolvZ5BS50", "link": "https://openreview.net/forum?id=tolvZ5BS50", "pdf_link": "https://openreview.net/pdf?id=tolvZ5BS50", "keywords": "graph neural network, semi-supervised learning, node classification", "abstract": "The message-passing mechanism of graph convolutional networks (i.e., GCNs) enables label information to be propagated to a broader range of neighbors, thereby increasing the utilization of labels. However, the label information is not always effectively utilized in the traditional GCN framework. To address this issue, we propose a new two-step framework called ELU-GCN. In the first stage,  ELU-GCN conducts graph learning to learn a new graph structure (i.e., ELU-graph), which enables GCNs to effectively utilize label information. In the second stage, we design a new graph contrastive learning on the GCN framework for representation learning by exploring the consistency and mutually exclusive information between the learned ELU graph and the original graph.  Moreover, we theoretically demonstrate that the proposed method can ensure the generalization ability of GCNs. Extensive experiments validate the superiority of the proposed method.", "title_embedding_index": 17820, "title_abs_embedding_index": 17845}, {"title": "3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds", "link_suffix": "/forum?id=GThTiuXgDC", "link": "https://openreview.net/forum?id=GThTiuXgDC", "pdf_link": "https://openreview.net/pdf?id=GThTiuXgDC", "keywords": "3D.+Large Language Model.+Robot.+Affordance.+Perception", "abstract": "3D Affordance detection is a challenging problem with broad applications on various robotic tasks. \nExisting methods typically formulate the detection paradigm as a label-based semantic segmentation task.\nThis paradigm relies on predefined labels and lacks the ability to comprehend complex natural language, resulting in limited generalization in open-world scene.\nTo address these limitations, we reformulate the traditional affordance detection paradigm into \\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. \nThis task is designed to output a affordance mask region given a query reasoning text, which avoids fixed categories of input labels.\nWe accordingly propose the \\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning affordance detection in 3D open-scene.\nSpecifically, 3D-ADLLM introduces large language models (LLMs) to 3D affordance perception with a custom-designed decoder for generating affordance masks, thus achieving open-world reasoning affordance detection.\nIn addition, given the scarcity of 3D affordance datasets for training large models, we seek to extract knowledge from general segmentation data and transfer it to affordance detection.\nThus, we propose a multi-stage training strategy that begins with a novel pre-training task, i.e., \\textit{Referring Object Part Segmentation}~(ROPS).\nThis stage is designed to equip the model with general recognition and segmentation capabilities at the object-part level.\nThen followed by fine-tuning with the IRAS task, 3D-ADLLM obtains the reasoning ability for affordance detection. \nIn summary, 3D-ADLLM leverages the rich world knowledge and human-object interaction reasoning ability of LLMs, achieving approximately an 8% improvement in mIoU on open-vocabulary affordance detection tasks.", "title_embedding_index": 17821, "title_abs_embedding_index": 17846}, {"title": "Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought", "link_suffix": "/forum?id=r3DF5sOo5B", "link": "https://openreview.net/forum?id=r3DF5sOo5B", "pdf_link": "https://openreview.net/pdf?id=r3DF5sOo5B", "keywords": "Chain of Thought, Transformer optimization, Training dynamics", "abstract": "Chain of Thought (CoT) prompting has been shown to significantly improve the performance of large language models (LLMs), particularly in arithmetic and reasoning tasks, by instructing the model to produce intermediate reasoning steps. Despite the remarkable empirical success of CoT and its theoretical advantages in enhancing expressivity, the mechanisms underlying CoT training remain largely unexplored. In this paper, we study the training dynamics of transformers over a CoT objective on a in-context weight prediction task for linear regression. We prove that while a one-layer linear transformer without CoT can only implement a single step of gradient descent (GD) and fails to recover the ground-truth weight vector, a transformer with CoT prompting can learn to perform multi-step GD autoregressively, achieving near-exact recovery. Furthermore, we show that the trained transformer effectively generalizes on the unseen data. Empirically, we demonstrate that CoT prompting yields substantial performance improvements.", "title_embedding_index": 17822, "title_abs_embedding_index": 17847}, {"title": "Detecting and Evaluating Medical Hallucinations in Large Vision Language Models", "link_suffix": "/forum?id=WgpAFnjvPr", "link": "https://openreview.net/forum?id=WgpAFnjvPr", "pdf_link": "https://openreview.net/pdf?id=WgpAFnjvPr", "keywords": "Hallucination Benchamrk, Hallucination Evaluation Method, Medical Large Vision Language Model", "abstract": "Large Vision Language Models (LVLMs) are increasingly integral to healthcare applications, including medical visual question answering and imaging report generation. While these models inherit the robust capabilities of foundational Large Language Models (LLMs), they also inherit susceptibility to hallucinations\u2014a significant concern in high-stakes medical contexts where the margin for error is minimal. However, currently, there are no dedicated methods or benchmarks for hallucination detection and evaluation in the medical field. To bridge this gap, we introduce Med-HallMark, the first benchmark specifically designed for hallucination detection and evaluation within the medical multimodal domain. This benchmark provides multi-tasking hallucination support, multifaceted hallucination data, and hierarchical hallucination categorization. Furthermore, we propose the MediHall Score, a new medical evaluative metric designed to assess LVLMs' hallucinations through a hierarchical scoring system that considers the severity and type of hallucination, thereby enabling a granular assessment of potential clinical impacts. We also present MedihallDetector, a novel Medical LVLM engineered for precise hallucination detection, which employs multitask training for hallucination detection. Through extensive experimental evaluations, we establish baselines for popular LVLMs using our benchmark. The findings indicate that MediHall Score provides a more nuanced understanding of hallucination impacts compared to traditional metrics and demonstrate the enhanced performance of MedihallDetector. We hope this work can significantly improve the reliability of LVLMs in medical applications. All resources of this work have been released.", "title_embedding_index": 17823, "title_abs_embedding_index": 17848}, {"title": "AI as Humanity\u2019s Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text", "link_suffix": "/forum?id=ilOEOIqolQ", "link": "https://openreview.net/forum?id=ilOEOIqolQ", "pdf_link": "https://openreview.net/pdf?id=ilOEOIqolQ", "keywords": "Machine Creativity, Large Language Model, Science of LLM, Machine Text Detection", "abstract": "Creativity has long been considered one of the most difficult aspect of human intelligence for AI to mimic. However, the rise of Large Language Models (LLMs), like ChatGPT, has raised questions about whether AI can match or even surpass\nhuman creativity. We present CREATIVITY INDEX as the first step to quantify the linguistic creativity of a text by reconstructing it from existing text snippets on the web. CREATIVITY INDEX is motivated by the hypothesis that the seemingly remarkable creativity of LLMs may be attributable in large part to the creativity of human-written texts on the web. To compute CREATIVITY INDEX efficiently, we introduce DJ SEARCH, a novel dynamic programming algorithm that can search verbatim and near-verbatim matches of text snippets from a given document against the web. Experiments reveal that the CREATIVITY INDEX of professional human authors is on average 66.2% higher than that of LLMs, and that alignment reduces the CREATIVITY INDEX of LLMs by an average of 30.1%. In addition, we find that distinguished authors like Hemingway exhibit measurably higher CREATIVITY INDEX compared to other human writers. Finally, we demonstrate that CREATIVITY INDEX can be used as a surprisingly effective criterion for zero-shot machine text detection, surpassing the strongest existing zero-shot system, DetectGPT, by a significant margin of 30.2%, and even outperforming the strongest supervised system, GhostBuster, in five out of six domains.", "title_embedding_index": 17824, "title_abs_embedding_index": 17849}]