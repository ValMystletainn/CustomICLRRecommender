[{"title": "Is Free Self-Alignment Possible?", "link_suffix": "/forum?id=8OcM1pTfHm", "link": "https://openreview.net/forum?id=8OcM1pTfHm", "pdf_link": "https://openreview.net/pdf?id=8OcM1pTfHm", "keywords": "self-alignment, representation engineering", "abstract": "Aligning pretrained language models (LMs) is a complex and resource-intensive process, often requiring access to large amounts of ground-truth preference data and substantial compute. Are these costs necessary? That is, is it possible to align using only inherent model knowledge and without additional training? We tackle this challenge with AlignEZ, a novel approach that uses (1) self-generated preference data and (2) representation editing to provide nearly cost-free alignment. During inference, AlignEZ modifies LM representations to reduce undesirable and boost desirable components using subspaces identified via self-generated preference pairs. Our experiments reveal that this nearly cost-free procedure significantly narrows the gap between base pretrained and tuned models by an average of 29.1%, observed across five datasets and two model architectures. Additionally, we explore the potential of using AlignEZ as a means of expediting more expensive alignment procedures.  Our experiments show that AlignEZ improves DPO models tuned only using a small subset of ground-truth preference data.", "title_embedding_index": 18250, "title_abs_embedding_index": 18275}, {"title": "Two-Player Zero-Sum Differential Games with One-Sided Information and Continuous Actions", "link_suffix": "/forum?id=SEjdainnpB", "link": "https://openreview.net/forum?id=SEjdainnpB", "pdf_link": "https://openreview.net/pdf?id=SEjdainnpB", "keywords": "Incomplete Information Game, Differential Game", "abstract": "Unlike Poker where the action space $\\mathcal{A}$ is discrete, differential games in the physical world often have continuous action spaces not amenable to discrete abstraction, rendering no-regret algorithms with $\\mathcal{O}(|\\mathcal{A}|)$ complexity not scalable. To address this challenge within the scope of two-player zero-sum (2p0s) games with one-sided information, we show that (1) a computational complexity independent of $|\\mathcal{A}|$ can be achieved by exploiting the \"Cav u\" property of behavioral strategies in incomplete-information games and the Isaacs' condition that commonly holds for control systems, and that (2) the computation of the two equilibrium strategies can be decoupled under the Isaacs' condition. We provide computational complexity of the resultant algorithm for approximating continuous-action mixed strategies (CAMS). Empirically, we demonstrate correctness of CAMS using a homing game where the Nash equilibrium exists analytically, and scalability through the same game with higher-dimensional actions. Codes available inanonymous repo.", "title_embedding_index": 18251, "title_abs_embedding_index": 18276}, {"title": "Diffusion On Syntax Trees For Program Synthesis", "link_suffix": "/forum?id=wN3KaUXA5X", "link": "https://openreview.net/forum?id=wN3KaUXA5X", "pdf_link": "https://openreview.net/pdf?id=wN3KaUXA5X", "keywords": "neurosymbolic, search, programming languages, inverse graphics", "abstract": "Large language models generate code one token at a time. Their autoregressive generation process lacks the feedback of observing the program's output. Training LLMs to suggest edits directly can be challenging due to the scarcity of rich edit data. To address these problems, we propose neural diffusion models that operate on syntax trees of any context-free grammar. Similar to image diffusion models, our method also inverts \"noise\" applied to syntax trees. Rather than generating code sequentially, we iteratively edit it while preserving syntactic validity, which makes it easy to combine this neural model with search. We apply our approach to inverse graphics tasks, where our model learns to convert images into programs that produce those images. Combined with search, our model is able to write graphics programs, see the execution result, and debug them to meet the required specifications. We additionally show how our system can write graphics programs for hand-drawn sketches. Video results can be found athttps://td-anon.github.io.", "title_embedding_index": 18252, "title_abs_embedding_index": 18277}, {"title": "Understanding Alignment in Multimodal LLMs: A Comprehensive Study", "link_suffix": "/forum?id=49qqV4NTdy", "link": "https://openreview.net/forum?id=49qqV4NTdy", "pdf_link": "https://openreview.net/pdf?id=49qqV4NTdy", "keywords": "foundation models, multimodal llm, alignment, image understanding", "abstract": "Preference alignment has become a crucial component in enhancing the performance of Large Language Models (LLMs), yet its impact in Multimodal Large Language Models (MLLMs) remains comparatively underexplored. Similar to language models, MLLMs for image understanding tasks encounter challenges like hallucination. In MLLMs, hallucination can occur not only by stating incorrect facts but also by producing responses that are inconsistent with the image content. A primary objective of alignment for MLLMs is to encourage these models to align responses more closely with image information. Recently, multiple works have introduced preference datasets for MLLMs and examined different alignment methods, including Direct Preference Optimization (DPO) and Proximal Policy Optimization (PPO). However, due to variations in datasets, base model types, and alignment methods, it remains unclear which specific elements contribute most significantly to the reported improvements in these works. In this paper, we independently analyze each aspect of preference alignment in MLLMs. We start by categorizing the alignment algorithms into two groups, offline (such as DPO), and online (such as online-DPO), and show that combining offline and online methods can improve the performance of the model in certain scenarios. \nWe review a variety of published multimodal preference datasets and discuss how the details of their construction impact model performance. Based on these insights, we introduce a novel way of creating multimodal preference data called Bias-Driven Hallucination Sampling (BDHS) that needs neither additional annotation nor external models, and show that it can achieve competitive performance to previously published alignment work for multimodal models across a range of benchmarks.", "title_embedding_index": 18253, "title_abs_embedding_index": 18278}, {"title": "Glider: Global and Local Instruction-Driven Expert Router", "link_suffix": "/forum?id=0gVatTOgEv", "link": "https://openreview.net/forum?id=0gVatTOgEv", "pdf_link": "https://openreview.net/pdf?id=0gVatTOgEv", "keywords": "Parameter Efficient Fine-Tuning, LoRA, Cross-Task Generalization", "abstract": "The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. This has enabled the creation of powerful and adaptive routing-based \u201cModel MoErging\" methods with the goal of using expert modules to create an aggregate system with improved performance or generalization. However, existing MoErging methods often prioritize generalization to unseen tasks at the expense of performance on held-in tasks. This limitation adversely impacts practical applicability, as real-world deployments require robust performance across both known and novel tasks. We observe that current token-level routing mechanisms neglect the global semantic context of the input task. This token-wise independence hinders effective expert selection, particularly for held-in tasks, as routing decisions fail to incorporate the holistic semantic properties of the task. To address this, we propose a novel method, Global and Local Instruction Driven Expert Router (GLIDER) that integrates a multi-scale routing mechanism, encompassing a semantic global router and a learned local router. As recent LLMs demonstrate advanced reasoning capabilities for semantic-related contexts, the global router leverages this ability to enhance expert selection. By utilizing the input query and an LLM, the router generates semantic task instructions that guide the retrieval of the most relevant experts across all layers. This global guidance is complemented by a local router that facilitates token-level routing decisions within each module, enabling finer control and enhanced performance on unseen and challenging tasks. Our experiments using T5-based expert models for T0 and FLAN tasks demonstrate that GLIDER achieves substantially improved held-in performance while maintaining strong generalization on held-out tasks. Additionally, we perform ablations experiments to dive deeper into the components of GLIDER and plot routing distributions to show that GLIDER can effectively retrieve correct expert for held-in tasks while also demonstrating compositional capabilities for held-out tasks. Our experiments highlight the importance of our multi-scale routing that leverages LLM-driven semantic reasoning for MoErging methods. Our code is attached as supplementary material.", "title_embedding_index": 18254, "title_abs_embedding_index": 18279}, {"title": "Domain Indexing Collaborative Filtering for Recommender System", "link_suffix": "/forum?id=PnZ2lbQaao", "link": "https://openreview.net/forum?id=PnZ2lbQaao", "pdf_link": "https://openreview.net/pdf?id=PnZ2lbQaao", "keywords": "Recommendation System, Domain Adaptation, Bayesian Deep Learning", "abstract": "In cross-domain recommendation systems, addressing cold-start items remains a significant challenge. Previous methods typically focus on maximizing performance using cross-domain knowledge, often treating the knowledge transfer process as a black box. However, the recent development of domain indexing introduces a new approach to better address such challenges. We have developed an adversarial Bayesian framework, Domain Indexing Collaborative Filtering (DICF), that infers domain indices during cross-domain recommendation. This framework not only significantly improves the recommendation performance but also provides interpretability for cross-domain knowledge transfer. This is verified by our empirical results on both synthetic and real-world datasets.", "title_embedding_index": 18255, "title_abs_embedding_index": 18280}, {"title": "Beyond Model Collapse: Scaling Up with Synthesized Data Requires Verification", "link_suffix": "/forum?id=MQXrTMonT1", "link": "https://openreview.net/forum?id=MQXrTMonT1", "pdf_link": "https://openreview.net/pdf?id=MQXrTMonT1", "keywords": "Learning with Synthetic Data, Data Curation, Avoiding Model Collapse", "abstract": "Large Language Models (LLM) are increasingly trained on data generated by other LLM, either because generated text and images become part of the pre-training corpus, or because synthetized data is used as a replacement for expensive human-annotation. This raises concerns aboutmodel collapse, a drop in model performance when their training sets include generated data. Considering that it is easier for both humans and machines to tell between good and bad examples than to generate high-quality samples, we investigate the use of verification on synthesized data to prevent model collapse. We provide a theoretical characterization using Gaussian mixtures, linear classifiers, and linear verifiers to  derive conditions with measurable proxies to assess whether the verifier can effectively select synthesized data that leads to optimal performance. We experiment with two practical tasks -- computing matrix eigenvalues with transformers and news summarization with LLMs -- which both exhibit model collapse when trained on generated data, and show that verifiers, even imperfect ones, can indeed be harnessed to prevent model collapse and that our proposed proxy measure strongly correlates with performance.", "title_embedding_index": 18256, "title_abs_embedding_index": 18281}, {"title": "HESSO: Towards Automatic Efficient and User Friendly Any Neural Network Training and Pruning", "link_suffix": "/forum?id=LXlTdn9hY9", "link": "https://openreview.net/forum?id=LXlTdn9hY9", "pdf_link": "https://openreview.net/pdf?id=LXlTdn9hY9", "keywords": "Structured Pruning, Training, AutoML", "abstract": "Structured pruning is one of the most popular approaches to effectively compress the heavy deep neural networks (DNNs) into compact sub-networks while retaining the original network performance. The existing methods suffer from multi-stage procedures along with significant engineering efforts and human expertise. The Only-Train-Once series (OTOv1-v3) has been recently proposed to resolve the many pain points by streamlining the workflow. However, the built-in sparse optimizers in the OTO series, i.e., the Half-Space Projected Gradient (HSPG) family, have limitations that require hyper-parameter tuning and the implicit controls of the sparsity exploration, consequently requires intervening by human expertise. To further address such limitations, we propose a novel Hybrid Efficient Structured Sparse Optimizer (HESSO). HESSO could automatically and efficiently train a DNN within a single run to produce a high-performing sub-network. Meanwhile, it is almost tuning-free and enjoys user-friendly integration for generic training applications. To address another common issue of irreversible pruning performance collapse observed in some DNNs, we further propose a novel Corrective Redundant Identification Cycle (CRIC) to plug into HESSO for reliably identifying indispensable structures. We numerically demonstrate the efficacy of HESSO and its enhanced version HESSO-CRIC on a variety of applications ranging from computer vision to natural language processing, including large language model. The numerical results showcase that HESSO can achieve competitive performance to varying state-of-the-art benchmarks and support most DNN architectures. Meanwhile, CRIC can effectively prevent the irreversible performance collapse and further enhance the performance of HESSO on certain applications.", "title_embedding_index": 18257, "title_abs_embedding_index": 18282}, {"title": "Hidden Logos in Web-Scale Data Disrupt Large Vision Language Models", "link_suffix": "/forum?id=awuw503LzY", "link": "https://openreview.net/forum?id=awuw503LzY", "pdf_link": "https://openreview.net/pdf?id=awuw503LzY", "keywords": "Spurious Correlations, Bias, Logos", "abstract": "Vision-Language Models are trained on very large, minimally curated image datasets that contain many spurious correlations  between categories and visual patterns. This causes VLMs to learn shortcuts, e.g., between smiling and gender. Although logos are ubiquitous in VLM training data and are  a potential source of such shortcuts, there is very limited study of this issue. Prior work pointed out that logos may indeed cause such problems, but the analysis was limited to a single text-based logo. In this paper, we undertake a broad study of logos in VLM training data and their potential to insert \"hidden\" spurious correlations into VLMs. We construct a new logo dataset, CC12M-LogoBank,  propose an algorithm that uncovers spurious logos affecting a given VLM prediction task, and test it on several representative tasks: person attribute classification, object classification, and harmful content detection.  Our key finding is that some logos indeed lead to spurious incorrect predictions, for example, adding the Adidas logo to a photo of a person causes a model classify the person as \"greedy\". Furthermore, we argue that the uncovered logos can be seen as effective attacks against foundational models; for example, an attacker could place a spurious logo on harmful content, causing the model to misclassify it as harmless. This threat is alarming considering the simplicity of logo attacks, increasing the attack surface of VLM models. As a defense, we explore two effective yet simple mitigation strategies that seamlessly integrate with zero-shot inference of foundation models.", "title_embedding_index": 18258, "title_abs_embedding_index": 18283}, {"title": "Simple synthetic data reduces sycophancy in large language models", "link_suffix": "/forum?id=WDheQxWAo4", "link": "https://openreview.net/forum?id=WDheQxWAo4", "pdf_link": "https://openreview.net/pdf?id=WDheQxWAo4", "keywords": "sycophancy, natural language processing, large language models", "abstract": "Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.First, on a set of three sycophancy tasks where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for large language models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP tasks and encourages models to be robust to user opinions on these tasks. Adding these data in a lightweight finetuning step can significantly reduce sycophantic behavior on held-out prompts. Code for generating synthetic data for intervention can be found athttps://anonymous.4open.science/r/sycophancy-intervention-F0D1/.", "title_embedding_index": 18259, "title_abs_embedding_index": 18284}, {"title": "Data-Centric AI Governance: Addressing the Limitations of Model-Focused Policies", "link_suffix": "/forum?id=iuqprf3GuR", "link": "https://openreview.net/forum?id=iuqprf3GuR", "pdf_link": "https://openreview.net/pdf?id=iuqprf3GuR", "keywords": "ai policy, data-centric ai", "abstract": "Current regulations on powerful AI capabilities are narrowly focused on \"foundation\" or \"frontier\" models. However, these terms are vague and inconsistently defined, leading to an unstable foundation for governance efforts. Critically, policy debates often fail to consider the data used with these models, despite the clear link between data and model performance. Even (relatively) \"small\" models that fall outside the typical definitions of foundation and frontier models can achieve equivalent outcomes when exposed to sufficiently specific datasets. In this work, we illustrate the importance of considering dataset size and content as essential factors in assessing the risks posed by models both today and in the future. More broadly, we emphasize the risk posed by over-regulating reactively and provide a path towards careful, quantitative evaluation of capabilities that can lead to a simplified regulatory environment.", "title_embedding_index": 18260, "title_abs_embedding_index": 18285}, {"title": "OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation", "link_suffix": "/forum?id=9HZtP6I5lv", "link": "https://openreview.net/forum?id=9HZtP6I5lv", "pdf_link": "https://openreview.net/pdf?id=9HZtP6I5lv", "keywords": "Physics-based Modeling, 3D Dynamics, 3D Gaussian Splatting, Video Score Distillation", "abstract": "Recently, significant advancements have been made in the reconstruction and generation of 3D assets, including static cases and those with physical interactions. To recover the physical properties of 3D assets, existing methods typically assume that all materials belong to a specific predefined category ($\\textit{e.g.}$, elasticity). However, such assumptions ignore the complex composition of multiple heterogeneous objects in real scenarios and tend to render less physically plausible animation given a wider range of objects. We propose $OmniPhysGS$ for synthesizing a physics-based 3D dynamic scene composed of more general objects. A key design of $OmniPhysGS$ is treating each 3D asset as a collection of constitutive 3D Gaussians. For each Gaussian, \nits physical material is represented by an ensemble of 12 physical domain-expert sub-models (rubber, metal, honey, water, etc.), which greatly enhances the flexibility of the proposed model. In the implementation, we define a scene by user-specified prompts and supervise the estimation of material weighting factors via a pretrained video diffusion model. Comprehensive experiments demonstrate that $OmniPhysGS$ achieves more general and realistic physical dynamics across a broader spectrum of materials, including elastic, viscoelastic, plastic, and fluid substances, as well as interactions between different materials. Our method surpasses existing methods by approximately 3% to 16% in metrics of visual quality and text alignment.  The code and data will be made publicly available.", "title_embedding_index": 18261, "title_abs_embedding_index": 18286}, {"title": "Self-supervised Privacy-preservation via Latent Anonymization for Generalizable Video Understanding", "link_suffix": "/forum?id=7M6OGwZ0XV", "link": "https://openreview.net/forum?id=7M6OGwZ0XV", "pdf_link": "https://openreview.net/pdf?id=7M6OGwZ0XV", "keywords": "privacy preservation, video understanding", "abstract": "The rapid advancements in large video models have unlocked new horizons in video understanding, enhancing applications in various domains such as surveillance, healthcare, and entertainment. However, these models often compromise individual privacy by inadvertently revealing sensitive private information such as skin color and gender. Existing privacy preservation methods are often limited in their scope and tailored to specific downstream tasks. Since current methods directly apply an anonymization function to the input pixel space, they demand extensive computational resources due to the retraining of the utility video model. To address these challenges, we propose a novel approach that shifts privacy-preserving anonymization from the input pixel space to the latent feature space, significantly reducing computational costs and enabling deployment in large foundational video models. Our method employs a self-supervised privacy budget in the latent space by minimizing the mutual information between static clip features. This approach notably allows, for the first time, supervision from downstream tasks such as anomaly detection and temporal action detection through collaborative co-training. Furthermore, we introduce a latent consistency loss to maintain the utility video model's multitask generalization capabilities and prevent single task overfitting. Our extensive evaluations demonstrate a significant ($\\approx$\\textbf{29%}) reduction in privacy leakage while maintaining near peak (within \\textbf{1%}) utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). Moreover, we propose new protocols for assessing gender bias in action recognition models, demonstrating that our method effectively mitigates such biases and promotes equitable video understanding.", "title_embedding_index": 18262, "title_abs_embedding_index": 18287}, {"title": "Dual Caption Preference Optimization for Diffusion Models", "link_suffix": "/forum?id=w0lhe9prqH", "link": "https://openreview.net/forum?id=w0lhe9prqH", "pdf_link": "https://openreview.net/pdf?id=w0lhe9prqH", "keywords": "Preference Optimization, Diffusion Models, Alignment", "abstract": "Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a conflict distribution. Additionally, we identified a performance issue in previous optimization methods, where using the same prompt for preferred and less preferred images, known as the irrelevant prompt issue, restricts model performance. To address these challenges, we propose Dual Caption Preference Optimization (DCPO), a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the Pick-Double Caption dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, SFT-Chosen, Diffusion-DPO and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.", "title_embedding_index": 18263, "title_abs_embedding_index": 18288}, {"title": "Text To Stealthy Adversarial Face Masks", "link_suffix": "/forum?id=12iSWNLDzj", "link": "https://openreview.net/forum?id=12iSWNLDzj", "pdf_link": "https://openreview.net/pdf?id=12iSWNLDzj", "keywords": "facial recognition, adversarial accessories, diffusion models, adversarial benchmarks", "abstract": "Recent studies have demonstrated that modern facial recognition systems, which are based on deep neural networks, are vulnerable to adversarial attacks, including the use of accessories, makeup patterns, or precision lighting. However, developing attacks that are both robust (resilient to changes in viewing angles and environmental conditions) and stealthy (do not attract suspicion by, for example, incorporating obvious facial features) remains a significant challenge. In this context, we introduce a novel diffusion-based method (DAFR) capable of generating robust and stealthy face masks for dodging recognition systems (where the system fails to identify the attacker). Specifically our approach is capable of producing high-fidelity printable textures using the guidance of textual prompts to determine the style. This method can also be adapted for impersonation purposes, where the system misidentifies the attacker as a specific other individual. Finally, we address a gap in the existing literature by presenting a comprehensive benchmark (FAAB) for evaluating adversarial accessories in three dimensions, assessing their robustness and stealthiness.", "title_embedding_index": 18264, "title_abs_embedding_index": 18289}, {"title": "Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective", "link_suffix": "/forum?id=nWdQX5hOL9", "link": "https://openreview.net/forum?id=nWdQX5hOL9", "pdf_link": "https://openreview.net/pdf?id=nWdQX5hOL9", "keywords": "Graph Neural Networks, Spectral Graph Theory, Graph Signal Filtering", "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph-based learning tasks. While their performance is often attributed to the powerful neighborhood aggregation mechanism, recent studies suggest that other components such as non-linear layers may also significantly affecting how GNNs process the input graph data in the spectral domain. Such evidence challenges the prevalent opinion that neighborhood aggregation mechanisms dominate the behavioral characteristics of GNNs in the spectral domain. To demystify such a conflict, this paper introduces a comprehensive benchmark to measure and evaluate GNNs' capability in capturing and leveraging the information encoded in different frequency components of the input graph data. Specifically, we first conduct an exploratory study demonstrating that GNNs can flexibly yield outputs with diverse frequency components even when certain frequencies are absent or filtered out from the input graph data. We then formulate a novel research problem of measuring and benchmarking the performance of GNNs from a spectral perspective. To take an initial step towards a comprehensive benchmark, we design an evaluation protocol supported by comprehensive theoretical analysis. Finally, we introduce a comprehensive benchmark on real-world datasets, revealing insights that challenge prevalent opinions from a spectral perspective. We believe that our findings will open new avenues for future advancements in this area. Our implementations can be found at:https://anonymous.4open.science/r/Spectral-Benchmark-9B56/.", "title_embedding_index": 18265, "title_abs_embedding_index": 18290}, {"title": "Deep Kernel Posterior Learning under Infinite Variance Prior Weights", "link_suffix": "/forum?id=usFdPd4Ghs", "link": "https://openreview.net/forum?id=usFdPd4Ghs", "pdf_link": "https://openreview.net/pdf?id=usFdPd4Ghs", "keywords": "Kernel methods, Deep Gaussian processes, Infinite variance priors, Deep Bayesian neural networks", "abstract": "Neal (1996) proved that infinitely wide shallow Bayesian neural networks (BNN) converge to Gaussian processes (GP), when the network weights have bounded prior variance. Cho & Saul (2009) provided a useful recursive formula for deep kernel processes for relating the covariance kernel of each layer to the layer immediately below. Moreover, they worked out the form of the layer-wise covariance kernel in an explicit manner for several common activation functions, including the ReLU. Subsequent works have made the connection between these two works, and provided useful results on the covariance kernel of a deep GP arising as wide limits of various deep Bayesian network architectures. However, recent works, including Aitchison et al. (2021), have highlighted that the covariance kernels obtained in this manner are deterministic and hence, precludes any possibility of representation learning, which amounts to learning a non-degenerate posterior of a random kernel given the data. To address this, they propose adding artificial noise to the kernel to retain stochasticity, and develop deep kernel Wishart and inverse Wishart processes. Nonetheless, this artificial noise injection could be critiqued in that it would not naturally emerge in a classic BNN architecture under an infinite-width limit. To address this, we show that a Bayesian deep neural network, where each layer width approaches infinity, and all network weights are elliptically distributed with infinite variance, converges to a process with $\\alpha$-stable marginals in each layer that has a conditionally Gaussian representation. These conditional random covariance kernels could be recursively linked in the manner of Cho & Saul (2009), even though marginally the process exhibits stable behavior, and hence covariances are not even necessarily defined. We also provide useful generalizations of the recent results of Lor\u00eda & Bhadra (2024) on shallow networks to multi-layer networks, and remedy the prohibitive computational burden of their approach. The computational and statistical benefits over competing approaches stand out in simulations and in demonstrations on benchmark data sets.", "title_embedding_index": 18266, "title_abs_embedding_index": 18291}, {"title": "Prioritized Generative Replay", "link_suffix": "/forum?id=5IkDAfabuo", "link": "https://openreview.net/forum?id=5IkDAfabuo", "pdf_link": "https://openreview.net/pdf?id=5IkDAfabuo", "keywords": "online learning, model-based reinforcement learning, generative modeling, synthetic data, continual learning", "abstract": "Sample-efficient online reinforcement learning often uses replay buffers to store experience for reuse when updating the value function. \nHowever, uniform replay is inefficient, since certain classes of transitions can be more relevant to learning. While prioritization of more useful samples is helpful, this strategy can also lead to overfitting, as useful samples are likely to be more rare. \nIn this work, we instead propose a prioritized, parametric version of an agent's memory, using generative models to capture online experience. This paradigm enables (1) densification of past experience, with new generations that benefit from the generative model's generalization capacity and (2) guidance via a family of ``relevance functions'' that push these generations towards more useful parts of an agent's acquired history. We show this recipe can be instantiated using conditional diffusion models and simple relevance functions such as curiosity- or value-based metrics. \nOur approach consistently improves performance and sample efficiency in both state- and pixel-based domains. We expose the mechanisms underlying these gains, showing how guidance promotes diversity in our generated transitions and reduces overfitting. We also showcase how our approach can train policies with even higher update-to-data ratios than before, opening up avenues to better scale online RL agents.", "title_embedding_index": 18267, "title_abs_embedding_index": 18292}, {"title": "DP-SGD for non-decomposable objective functions", "link_suffix": "/forum?id=F52tAK5Gbg", "link": "https://openreview.net/forum?id=F52tAK5Gbg", "pdf_link": "https://openreview.net/pdf?id=F52tAK5Gbg", "keywords": "Differential privacy, private learning, contrastive learning.", "abstract": "Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as the contrastive loss, that favor minimizing the distance between similar inputs and maximizing the distance between distinct inputs. As privacy concerns mount, training these models using differential privacy has become more important. However, due to how inputs are generated for these losses, one of their undesirable properties is that their $L_2$ sensitivity grows with the batch size. This property is particularly disadvantageous for differentially private training methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD variant for similarity based loss functions --- in particular, the commonly-used contrastive loss --- that manipulates gradients of the objective function in a novel way to obtain a sensitivity of the summed gradient that is $O(1)$ for batch size $n$.  We test our DP-SGD variant on some CIFAR-10 pre-training and CIFAR-100 finetuning tasks and show that, in both tasks, our method's performance comes close to that of a non-private model and generally outperforms DP-SGD applied directly to the contrastive loss.", "title_embedding_index": 18268, "title_abs_embedding_index": 18293}, {"title": "Diffusion priors for Bayesian 3D reconstruction from incomplete measurements", "link_suffix": "/forum?id=JZgqoOu4Ml", "link": "https://openreview.net/forum?id=JZgqoOu4Ml", "pdf_link": "https://openreview.net/pdf?id=JZgqoOu4Ml", "keywords": "Diffusion model guidance, 3D reconstruction, cryo-electron microscopy, inverse problems, diffusion posterior sampling", "abstract": "Many inverse problems are ill-posed and need to be complemented by prior information that restricts the class of admissible models. Bayesian approaches encode this information as prior distributions that impose generic properties on the model such as sparsity, non-negativity or smoothness. However, in case of complex structured models such as images, graphs or three-dimensional (3D) objects, generic prior distributions tend to favor models that differ largely from those observed in the real world. Here we explore the use of diffusion models as priors that are combined with experimental data within a Bayesian framework. We use 3D point clouds to represent 3D objects such as household items or biomolecular complexes formed from proteins and nucleic acids. We train diffusion models that generate coarse-grained 3D structures at a medium resolution and integrate these with incomplete and noisy experimental data. To demonstrate the power of our approach, we focus on the reconstruction of biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which is an important inverse problem in structural biology. We find that posterior sampling with diffusion model priors allows for 3D reconstruction from very sparse, low-resolution and partial observations.", "title_embedding_index": 18269, "title_abs_embedding_index": 18294}, {"title": "Differentially private learners for heterogeneous treatment effects", "link_suffix": "/forum?id=1z3SOCwst9", "link": "https://openreview.net/forum?id=1z3SOCwst9", "pdf_link": "https://openreview.net/pdf?id=1z3SOCwst9", "keywords": "Causality, differential privacy, treatment effect estimation", "abstract": "Patient data is widely used to estimate heterogeneous treatment effects and understand the effectiveness and safety of drugs. Yet, patient data includes highly\nsensitive information that must be kept private. In this work, we aim to estimate\nthe conditional average treatment effect (CATE) from observational data under\ndifferential privacy. Specifically, we present DP-CATE, a novel framework for\nCATE estimation that isdoubly robustand ensuresdifferential privacyof the estimates. For this, we build upon non-trivial tools from semi-parametric and robust statistics to exploit the connection between privacy and model robustness.\nOur framework is highly general and applies to any two-stage CATE meta-learner\nwith a Neyman-orthogonal loss function. It can be used with all machine learning models employed for nuisance estimation. We further provide an extension\nof DP-CATE where we employ RKHS regression to release the complete doubly\nrobust CATE function while ensuring differential privacy. We demonstrate the effectiveness of DP-CATE across various experiments using synthetic and real-world\ndatasets. To the best of our knowledge, we are the first to provide a framework for\nCATE estimation that is doubly robust and differentially private.", "title_embedding_index": 18270, "title_abs_embedding_index": 18295}, {"title": "Sparsely multimodal data fusion", "link_suffix": "/forum?id=iSLDihAfYi", "link": "https://openreview.net/forum?id=iSLDihAfYi", "pdf_link": "https://openreview.net/pdf?id=iSLDihAfYi", "keywords": "multimodal, representation learning, contrastive, transformer, masked attention, sentiment, sparsity, embedding space", "abstract": "In order to create unified representations of multimodal data, masked multimodal transformer architectures use attention masking to constrain attention to occur between unimodal and fused intermediate representations. In this study, it is demonstrated that they can create an aligned embedding space when samples include only a fraction of data modalities. This is done by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are tested, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.", "title_embedding_index": 18271, "title_abs_embedding_index": 18296}, {"title": "Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark", "link_suffix": "/forum?id=vMIVqlEWRw", "link": "https://openreview.net/forum?id=vMIVqlEWRw", "pdf_link": "https://openreview.net/pdf?id=vMIVqlEWRw", "keywords": "Vision-Language Models, Benchmarks, Scalling Suites", "abstract": "The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks.  We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP -  a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.", "title_embedding_index": 18272, "title_abs_embedding_index": 18297}, {"title": "LVLM-CL: Make Large Vision-Language Models Work Better under Continual Learning Settings", "link_suffix": "/forum?id=JIlIYIHMuv", "link": "https://openreview.net/forum?id=JIlIYIHMuv", "pdf_link": "https://openreview.net/pdf?id=JIlIYIHMuv", "keywords": "Continual Learning; Large vision-language model", "abstract": "The development of Large Vision-Language Models\n(LVLMs) is striving to catch up with the success of Large\nLanguage Models (LLMs), yet it faces more challenges to\nbe resolved. When finetuning LVLMs with user-specific\ndata in the practical use, the pretrained weights would face\nthe problems of forgetting and performance degradation.\nSo it is important to improve LVLM\u2019s performance under\nthe continual learning settings. Some existing CL methods\nhave explored continual learning on VLM. However, the\ncontinual learning settings they have proposed couldn\u2019t\nbe adopted to LVLMs smoothly because the training and\nfinetuning process of LVLMs need amount of data while\nprevious VLM continual learning settings built on limited\ndata and different model architectures. In this work, we first\ndevise a task-specific continual learning setting especially\nfor LVLMs by classifying the instruction tuning data\nfor the second finetune process of LVLMs into several\ndifferent tasks. Mimicking the process of finetuning with\nuser-specific task data, we found that the performance of\nLVLMs would decline without any modules designed for\ncontinual learning settings. So we present LVLM-CL, a\nnovel approach capable of continual learning settings for\nlarge vision-language models when finetuning with different\nkinds of tasks. Specifically, our LVLM-CL consists of a\ntext feature based prompt that are different between tasks\nto keep the special feature of different tasks. To meet the\nsetting of continual learning, we also design a memory bank\nwhich storage previous trained tasks which helps LVLMs\napply knowledge to unfamiliar combinations. Extensive case\nstudies and quantitative evaluations show LVLM-CL has\nstrong capability in understanding the pivotal features of\ndifferent tasks and emerges impressive memory capabilities\nunder the continual learning settings. This work fosters the\nadvancements of LVLMs by enabling them to support better\ncontinual finetuning toward practical use in the real world.", "title_embedding_index": 18273, "title_abs_embedding_index": 18298}, {"title": "Latent Abstractions in Generative Diffusion Models", "link_suffix": "/forum?id=dUCMO9lwSv", "link": "https://openreview.net/forum?id=dUCMO9lwSv", "pdf_link": "https://openreview.net/pdf?id=dUCMO9lwSv", "keywords": "diffusion models, world modelling, information theory, nonlinear filtering", "abstract": "In this work we study how diffusion-based generative models produce high-dimensional data, such as an image, by implicitly relying on a manifestation of a low-dimensional set of latent abstractions, that guide the generative process.\nWe present a novel theoretical framework that extends Nonlinear Filtering (NLF), and that offers a unique perspective on SDE-based generative models. The development of our theory relies on NLF, including a novel formulation of the joint (state and measurement) dynamics, and an information-theoretic measure of the influence of the system state on the measurement process. According to our theory, diffusion models can be cast as a system of SDE, describing a non-linear filter in which the evolution of unobservable latent abstractions steers the dynamics of an observable measurement process (corresponding to the generative pathways). In addition, we present an empirical study to validate our theory and previous empirical results on the emergence of latent abstractions at different stages of the generative process.", "title_embedding_index": 18274, "title_abs_embedding_index": 18299}]