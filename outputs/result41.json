[{"title": "An Exploration of Speech Conditioned Large Language Models (SLMs)", "link_suffix": "/forum?id=cLws58ZojF", "link": "https://openreview.net/forum?id=cLws58ZojF", "pdf_link": "https://openreview.net/pdf?id=cLws58ZojF", "keywords": "Speech Conditioned Large Language Models", "abstract": "Efforts to enable Large Language Models (LLMs) to understand human speech have spurred the development of an increasing number of Speech-Conditioned Large Language Models (SLMs). While these models have demonstrated success on various speech-related tasks, such as automatic speech recognition (ASR), the design space of SLMs has not been thoroughly explored. In this work, we revisit key design choices for SLMs, aiming to gain insights into how these choices impact the performance of SLMs and how we could optimize them for better results. Surprisingly, our experiments reveal that current SLMs struggle to follow speech instructions or respond to speech inputs, even for simple queries like \u201dwho has been to the moon?\u201d. Our experimental findings indicate that speech instruction following data is crucial for improving these capabilities. Leveraging this insight, we propose to use synthetic speech instruction following data to enhance speech instruction following capability. Combining the findings from our other experiments, we provide an effective recipe for developing SLMs. Our model, called SiM, not only achieves strong ASR performance, but also significantly outperforms existing SLMs in speech instruction following.", "title_embedding_index": 2000, "title_abs_embedding_index": 2025}, {"title": "Large Language Models Are Active Critics in NLG Evaluation", "link_suffix": "/forum?id=IcovaKGyMp", "link": "https://openreview.net/forum?id=IcovaKGyMp", "pdf_link": "https://openreview.net/pdf?id=IcovaKGyMp", "keywords": "Large Langaugel Models\uff0c Natural Language Generation\uff0c Evaluation\uff0cPrompt Optimization\uff0c Explanable Evaluation", "abstract": "The conventional paradigm of using large language models (LLMs) for evaluating natural language generation (NLG) systems typically relies on two key inputs: (1) a clear definition of the NLG task to be evaluated and (2) a list of pre-defined evaluation criteria. This process treats LLMs as ''passive critics,'' strictly following human-defined criteria for evaluation. However, as new NLG tasks emerge, the criteria for assessing text quality can vary greatly. Consequently, these rigid evaluation methods struggle to adapt to diverse NLG tasks without extensive prompt engineering customized for each specific task. To address this limitation, we introduce Active-Critic, a novel LLM-based NLG evaluation protocol that enables LLMs to function as ''active critics.'' Specifically, our protocol comprises two key stages. In the first stage, the LLM is instructed to infer the target NLG task and establish relevant evaluation criteria from the data. Building on this self-inferred information, the second stage dynamically optimizes the prompt to guide the LLM toward more human-aligned scoring decisions, while also generating detailed explanations to justify its evaluations. Experiments across four NLG evaluation tasks show that our approach achieves stronger alignment with human judgments than state-of-the-art evaluation methods. Our comprehensive analysis further highlights the effectiveness and explainability of Active-Critic with only a small amount of labeled data. We will share our code and data on GitHub.", "title_embedding_index": 2001, "title_abs_embedding_index": 2026}, {"title": "Online Sequential Learning from Physiological Data with Weighted Prototypes: Tackling Cross-Subject Variability", "link_suffix": "/forum?id=jUCtGezFwH", "link": "https://openreview.net/forum?id=jUCtGezFwH", "pdf_link": "https://openreview.net/pdf?id=jUCtGezFwH", "keywords": "Online Continual Learning, Physiological Signals, Cross-Subject Variability", "abstract": "Online Continual Learning (OCL) enables machine learning models to adapt to sequential data streams in real-time, especially when only a small amount of data is available. However, applying OCL to physiological data such as electroencephalography (EEG) and electrocardiography (ECG) is often complicated by inter-subject variability, which can lead to catastrophic forgetting and performance degradation. Existing OCL methods are currently unable to effectively address this challenge, leading to difficulties in retaining previously learned knowledge while adapting to new data. This paper presents Online Prototypes Weighted Aggregation (OPWA), a novel method specifically designed to address the problem of catastrophic forgetting in the presence of inter-subject variability through the use of prototypical networks. OPWA facilitates the retention of knowledge from past subjects while adapting to new data streams.\nThe OPWA method uses an innovative prototype aggregation mechanism that fuses intra-class prototypes into generalized representations by accounting for both within-class and inter-class variation between subjects. Extensive experiments show that OPWA consistently outperforms existing OCL methods in terms of fast adaptation and mitigation of catastrophic forgetting on different physiological datasets with different modalities, and provides a robust solution for learning on sequential data streams.", "title_embedding_index": 2002, "title_abs_embedding_index": 2027}, {"title": "In-context Time Series Predictor", "link_suffix": "/forum?id=dCcY2pyNIO", "link": "https://openreview.net/forum?id=dCcY2pyNIO", "pdf_link": "https://openreview.net/pdf?id=dCcY2pyNIO", "keywords": "Time Series Forecasting, In-context Learning, Transformer", "abstract": "Recent Transformer-based large language models (LLMs) demonstrate in-context learning ability to perform various functions based solely on the provided context, without updating model parameters. To fully utilize the in-context capabilities in time series forecasting (TSF) problems, unlike previous Transformer-based or LLM-based time series forecasting methods, we reformulate \"time series forecasting tasks\" as input tokens by constructing a series of (lookback, future) pairs within the tokens. This method aligns more closely with the inherent in-context mechanisms and is more parameter-efficient without the need of using pre-trained LLM parameters. Furthermore, it addresses issues such as overfitting in existing Transformer-based TSF models, consistently achieving better performance across full-data, few-shot, and zero-shot settings compared to previous architectures.", "title_embedding_index": 2003, "title_abs_embedding_index": 2028}, {"title": "SpaLLM: Unified Compressive Adaptation of Large Language Models with Sketching", "link_suffix": "/forum?id=tTOo7r4G9j", "link": "https://openreview.net/forum?id=tTOo7r4G9j", "pdf_link": "https://openreview.net/pdf?id=tTOo7r4G9j", "keywords": "large language model, sketching", "abstract": "Compressive adaptation approaches, such as QLoRA, are widely popular alternatives for reducing memory requirements during fine-tuning of large language models (LLMs) while producing models capable of handling various downstream tasks. The key idea is to employ a \u201ctwo-tower\u201d architecture: compressing pretrained LLM parameters into compact representations and fine-tuning the additive full-precision adapter, which typically has few tunable parameters in low-rank format. However, the strict algebraic assumptions, such as low-rank assumption, and the complexity of composing two-tower architectures are some of the known shortcomings, resulting in a poor accuracy-efficiency trade-off. In response to these known limitations, we propose SpaLLM (Sketched Parameter Adaptation of LLMs), a novel compressive adaptation approach for LLMs. This method is also the first to illustrate parameter-sharing compression methods for LLM fine-tuning, which, unlike QLoRA, are free from strict low-rank algebraic assumptions on adapters. Furthermore, our proposal unifies model compression and adaptation into a single, streamlined process, eliminating the need for two-tower architectures. SpaLLM sketches pre-trained LLM weights into lookup tables and directly fine-tunes the values in these tables. This approach simplifies LLMs\u2019 compressive adaptation workflow, potentially improves multi-user serving efficiency, and delivers significantly better accuracy for both natural language understanding and generation tasks. Moreover, by avoiding the \u201ctwo-tower\u201d architecture, our framework only requires one compressed matrix multiplication per layer during inference, demonstrating superior inference efficiency compared to previous methods.", "title_embedding_index": 2004, "title_abs_embedding_index": 2029}, {"title": "More Experts Than Galaxies: Conditionally-Overlapping Experts with Biologically-Inspired Fixed Routing", "link_suffix": "/forum?id=1qq1QJKM5q", "link": "https://openreview.net/forum?id=1qq1QJKM5q", "pdf_link": "https://openreview.net/pdf?id=1qq1QJKM5q", "keywords": "Deep learning, Mixture of Experts, Modularity, Sparsity, Conditional Computation", "abstract": "The evolution of biological neural systems has led to both modularity and sparse coding, which enables efficiency in energy usage, and robustness across the diversity of tasks in the lifespan. In contrast, standard neural networks rely on dense, non-specialized architectures, where all model parameters are simultaneously updated to learn multiple tasks, leading to representation interference. Current sparse neural network approaches aim to alleviate this issue, but are often hindered by limitations such as 1) trainable gating functions that cause representation collapse; 2) non-overlapping experts that result in redundant computation and slow learning; and 3) reliance on explicit input or task IDs that impose significant constraints on flexibility and scalability. In this paper we propose Conditionally Overlapping Mixture of ExperTs (COMET), a general deep learning method that addresses these challenges by inducing a modular, sparse architecture with an exponential number of overlapping experts. COMET replaces the trainable gating function used in Sparse Mixture of Experts with a fixed, biologically inspired random projection applied to individual input representations. This design causes the degree of expert overlap to depend on input similarity, so that similar inputs tend to share more parameters. This facilitates positive knowledge transfer, resulting in faster learning and improved generalization. We demonstrate the effectiveness of COMET on a range of tasks, including image classification, language modeling, and regression, using several popular deep learning architectures", "title_embedding_index": 2005, "title_abs_embedding_index": 2030}, {"title": "Can Watermarks be Used to Detect LLM IP Infringement For Free?", "link_suffix": "/forum?id=KRMSH1GxUK", "link": "https://openreview.net/forum?id=KRMSH1GxUK", "pdf_link": "https://openreview.net/pdf?id=KRMSH1GxUK", "keywords": "large language models, watermark, model copyright, model infringement detection", "abstract": "The powerful capabilities of LLMs stem from their rich training data and high-quality labeled datasets, making the training of strong LLMs a resource-intensive process, which elevates the importance of IP protection for such LLMs. Compared to gathering high-quality labeled data, directly sampling outputs from these fully trained LLMs as training data presents a more cost-effective approach. This practice\u2014where a suspect model is fine-tuned using high-quality data derived from these LLMs, thereby gaining capabilities similar to the target model\u2014can be seen as a form of IP infringement against the original LLM. In recent years, LLM watermarks have been proposed and used to detect whether a text is AI-generated. Intuitively, if data sampled from a watermarked LLM is used for training, the resulting model would also be influenced by this watermark. This raises the question: can we directly use such watermarks to detect IP infringement of LLMs? In this paper, we explore the potential of LLM watermarks for detecting model infringement. We find that there are two issues with direct detection: (1) The queries used to sample output from the suspect LLM have a significant impact on detectability. (2) The watermark that is easily learned by LLMs exhibits instability regarding the watermark's hash key during detection. To address these issues, we propose LIDet, a detection method that leverages available anchor LLMs to select suitable queries for sampling from the suspect LLM. Additionally, it adapts the detection threshold to mitigate detection failures caused by different hash keys. To demonstrate the effectiveness of this approach, we construct a challenging model set containing multiple suspect LLMs on which direct detection methods struggle to yield effective results. Our method achieves over 90% accuracy in distinguishing between infringing and clean models, demonstrating the feasibility of using LLM watermarks to detect LLM IP infringement.", "title_embedding_index": 2006, "title_abs_embedding_index": 2031}, {"title": "CALM: Critic Automation with Language Models", "link_suffix": "/forum?id=TY9mstpD02", "link": "https://openreview.net/forum?id=TY9mstpD02", "pdf_link": "https://openreview.net/pdf?id=TY9mstpD02", "keywords": "automatic scientific discovery, model criticism", "abstract": "Understanding the world through models is a fundamental goal of scientific research.\nWhile large language model (LLM) based approaches show promise in automating scientific discovery, they often overlook the importance of criticizing scientific models.\nCriticizing models deepens scientific understanding and drives the development of more accurate models.\nMoreover, criticism can improve the reliability of LLM-based scientist systems by acting as a safeguard against hallucinations.\nAutomating model criticism is difficult because it traditionally requires a human expert to define how to compare a model with data and evaluate if the discrepancies are significant--both rely heavily on understanding the modeling assumptions and domain.\nAlthough LLM-based critic approaches are appealing, they introduce new challenges: LLMs might hallucinate the critiques themselves. \nMotivated by this, we introduce CALM (Critic Automation with Language Models). CALM uses LLMs to generate summary statistics that highlight discrepancies between model predictions and data, and applies hypothesis tests to evaluate their significance.\nWe can view CALM as a verifier that validates models and critiques by embedding them in a hypothesis testing framework. \nIn experiments, we evaluate CALM across key quantitative and qualitative dimensions.\nIn settings where we synthesize discrepancies between models and datasets, CALM reliably generates correct critiques without hallucinating incorrect ones.\nWe show that both human and LLM judges consistently prefer CALM\u2019s critiques over alternative approaches in terms of transparency and actionability.\nFinally, we show that CALM's critiques enable an LLM scientist to improve upon human-designed models on real-world datasets.", "title_embedding_index": 2007, "title_abs_embedding_index": 2032}, {"title": "Sharper Analysis of Data Echoing and New Communication-Efficient Algorithm for Data Parallelism", "link_suffix": "/forum?id=8ZA7lrzw7O", "link": "https://openreview.net/forum?id=8ZA7lrzw7O", "pdf_link": "https://openreview.net/pdf?id=8ZA7lrzw7O", "keywords": "data echoing, data loading bottleneck", "abstract": "Over the past decade, breakthroughs in both general-purpose and specialized hardware have propelled the success of large-scale machine learning. However, the advancements in general-purpose hardware are not keeping pace with those in specialized hardware. Consequently, operations conducted on the general-purpose hardware have become the primary performance bottleneck. Notably, data loading significantly lags behind the gradient computation during training. To address this issue, the technique of data echoing has been introduced, whereby the current batch of samples is reused for gradient computation to minimize idle time while waiting for new data. However, this approach can lead to overfitting on the current batch, and it remains unclear whether convergence benefits from this practice. In this paper, we provide a sharper analysis on a stochastic variant of data echoing and show that it obtains linear speedup proportional to the number of reuse times. Additionally, we investigate the impact of the communication bottleneck in data parallelism of data echoing, and propose a new communication-efficient data echoing algorithm via reducing the frequency of model averaging. We then show that it is possible to perform data echoing without additional communication cost with data parallelism. Finally, we perform empirical experiments to verify our analysis on the data echoing and the proposed efficient algorithm for data parallelism.", "title_embedding_index": 2008, "title_abs_embedding_index": 2033}, {"title": "Autoregressive Moving-average Attention Mechanism for Time Series Forecasting", "link_suffix": "/forum?id=Z9N3J7j50k", "link": "https://openreview.net/forum?id=Z9N3J7j50k", "pdf_link": "https://openreview.net/pdf?id=Z9N3J7j50k", "keywords": "Attention, Transformer, Autoregressive Moving-average, Time Series Forecasting", "abstract": "We propose an Autoregressive (AR) Moving-average (MA) attention structure that can adapt to various linear attention mechanisms, enhancing their ability to capture long-range and local temporal patterns in time series. In this paper, we first demonstrate that, for the time series forecasting (TSF) task, the previously overlooked decoder-only autoregressive Transformer model can achieve results comparable to the best baselines when appropriate tokenization and training methods are applied. Moreover, inspired by the ARMA model from statistics and recent advances in linear attention, we introduce the full ARMA structure into existing autoregressive attention mechanisms. By using an indirect MA weight generation method, we incorporate the MA term while maintaining the time complexity and parameter size of the underlying efficient attention models. We further explore how indirect parameter generation can produce implicit MA weights that align with the modeling requirements for local temporal impacts. Experimental results show that incorporating the ARMA structure consistently improves the performance of various AR attentions on TSF tasks, achieving state-of-the-art results. The code implementation is available at the following link:https://anonymous.4open.science/r/ARMA-attention-3437.", "title_embedding_index": 2009, "title_abs_embedding_index": 2034}, {"title": "Learning-Augmented Frequent Directions", "link_suffix": "/forum?id=WcZLG8XxhD", "link": "https://openreview.net/forum?id=WcZLG8XxhD", "pdf_link": "https://openreview.net/pdf?id=WcZLG8XxhD", "keywords": "learning-augmented algorithms, algorithms with predictions, data streams, streaming algorithms, frequency estimation, heavy hitters, frequent directions, low-rank approximation", "abstract": "An influential paper of Hsu et al. (ICLR'19) introduced the study of learning-augmented streaming algorithms in the context of frequency estimation. A fundamental problem in the streaming literature, the goal of frequency estimation is to approximate the number of occurrences of items appearing in a long stream of data using only a small amount of memory. Hsu et al. develop a natural framework to combine the worst-case guarantees of popular solutions such as CountMin and CountSketch with learned predictions of high frequency elements. They demonstrate that learning the underlying structure of data can be used to yield better streaming algorithms, both in theory and practice.We simplify and generalize past work on learning-augmented frequency estimation. Our first contribution is a learning-augmented variant of the Misra-Gries algorithm which improves upon the error of learned CountMin and learned CountSketch and achieves the state-of-the-art performance of randomized algorithms (Aamand et al., NeurIPS'23) with a simpler, deterministic algorithm. Our second contribution is to adapt learning-augmentation to a high-dimensional generalization of frequency estimation corresponding to finding important directions (top singular vectors) of a matrix given its rows one-by-one in a stream. We analyze a learning-augmented variant of the Frequent Directions algorithm, extending the theoretical and empirical understanding of learned predictions to matrix streaming.", "title_embedding_index": 2010, "title_abs_embedding_index": 2035}, {"title": "Discovering Influential Neuron Path in Vision Transformers", "link_suffix": "/forum?id=WQQyJbr5Lh", "link": "https://openreview.net/forum?id=WQQyJbr5Lh", "pdf_link": "https://openreview.net/pdf?id=WQQyJbr5Lh", "keywords": "Explainability, Vision Transformer, Neuron", "abstract": "Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there\u2019s been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, show- casing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning.", "title_embedding_index": 2011, "title_abs_embedding_index": 2036}, {"title": "SecCodePLT: A Unified Platform for Evaluating the Security of Code GenAI", "link_suffix": "/forum?id=0R3ha8oNPU", "link": "https://openreview.net/forum?id=0R3ha8oNPU", "pdf_link": "https://openreview.net/pdf?id=0R3ha8oNPU", "keywords": "Code Generation, Cybersecurity, Safety, Large Language Models", "abstract": "Existing works have established multiple benchmarks to highlight the security risks associated with Code GenAI.\nThese risks are primarily reflected in two areas: a model\u2019s potential to generate insecure code (insecure coding) and its utility in cyberattacks (cyberattack helpfulness).\nWhile these benchmarks have made significant strides, there remain opportunities for further improvement.\nFor instance, many current benchmarks tend to focus more on a model\u2019s ability to provide attack suggestions rather than its capacity to generate executable attacks.\nAdditionally, most benchmarks rely heavily on static evaluation metrics (e.g., LLM judgment), which may not be as precise as dynamic metrics such as passing test cases. \nFurthermore, some large-scale benchmarks, while efficiently generated through automated methods, could benefit from more expert verification to ensure data quality and relevance to security scenarios. \nConversely, expert-verified benchmarks, while offering high-quality data, often operate at a smaller scale.\nTo address these gaps, we develop SecCodePLT, a unified and comprehensive evaluation platform for code GenAIs' risks.\nFor insecure code, we introduce a new methodology for data creation that combines experts with automatic generation. \nOur methodology ensures the data quality while enabling large-scale generation. \nWe also associate samples with test cases to conduct code-related dynamic evaluation.\nFor cyberattack helpfulness, we set up a real environment and construct samples to prompt a model to generate actual attacks, along with dynamic metrics in our environment.\nWe conduct extensive experiments and show that SecCodePLT outperforms the state-of-the-art (SOTA) benchmark CyberSecEval in security relevance.\nFurthermore, it better identifies the security risks of SOTA models in insecure coding and cyberattack helpfulness. \nFinally, we apply SecCodePLT to the SOTA code agent, Cursor, and, for the first time, identify non-trivial security risks in this advanced coding agent.", "title_embedding_index": 2012, "title_abs_embedding_index": 2037}, {"title": "GROOT: Graph Edge Re-growth and Partitioning for the Verification of Large Designs in Logic Synthesis", "link_suffix": "/forum?id=wq4AeBWQJ4", "link": "https://openreview.net/forum?id=wq4AeBWQJ4", "pdf_link": "https://openreview.net/pdf?id=wq4AeBWQJ4", "keywords": "Graph Neural Networks for EDA, Logic Synthesis, Formal Verification", "abstract": "Traditional verification methods in chip design are highly time-consuming and computationally demanding, especially for large-scale circuits. Graph neural networks (GNNs) have gained popularity as a potential solution to improve verification efficiency. However, there lacks a joint framework that considers all chip design domain knowledge, graph theory, and GPU kernel designs. To address\nthis challenge, we introduce GROOT, an algorithm and system co-design framework that contains chip design domain knowledge, graph theory, and redesigned GPU kernels, to improve verification efficiency. More specifically, we redesign node features utilizing the circuit node types and the polarity of the connections between the input edges to nodes in And-Inverter Graphs (AIGs). We utilize a graph partitioning algorithm based on the observation that approximately only 10% of boundary edges (nodes) between clusters, to divide the large graphs into smaller sub-graphs for fast GPU processing. We carefully profile the EDA graph workloads and observe the uniqueness of their polarized distribution of high-degree (HD) nodes and low-degree (LD) nodes. We redesign two GPU kernels (HD-kernel and LD-kernel), to fit the EDA graph learning workload on a single GPU. We evaluate the performance of GROOT on large circuit designs, e.g., Carry Save Adder (CSA) multipliers, the 7nm technology-mapped CSA multipliers, and Booth Multipliers. We compare the results with state-of-the-art GNN-based GAMORA and the traditional ABC framework. Results show that GROOT achieves a significant reduction in memory footprint (59.38 %), with high accuracy (99.96%) for a very large CSA multiplier, i.e. 1,024 bits with a batch size of 16, which consists of 134,103,040 nodes and 268,140,544 edges. We also compare GROOT with state-of-the-art GPU-based GPU Kernel designs such as cuSPARSE, MergePath-SpMM, and GNNAdvisor. We achieve up to 1.104\u00d7, 5.796\u00d7, and 1.469\u00d7 improvement in runtime, respectively.", "title_embedding_index": 2013, "title_abs_embedding_index": 2038}, {"title": "Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration", "link_suffix": "/forum?id=RWJX5F5I9g", "link": "https://openreview.net/forum?id=RWJX5F5I9g", "pdf_link": "https://openreview.net/pdf?id=RWJX5F5I9g", "keywords": "explore-exploit, stochastic Hopfield network, Thompson sampling, decision under uncertainty, brain-inspired algorithm, reinforcement learning", "abstract": "How to balance between exploration and exploitation in an uncertain environment is a central challenge in reinforcement learning. In contrast, humans and animals have demonstrated superior exploration efficiency in novel conditions. To understand how the brain\u2019s neural network controls exploration under uncertainty, we analyzed the dynamical systems model of a biological neural network that controls explore-exploit decisions during foraging. Mathematically, this type of network (which is named the Brain Bandit Net, or BBN) is a special type of stochastic continuous Hopfield networks. We show through theory and simulation that BBN can perform posterior sampling of action values with a tunable bias towards or against uncertain options. We then demonstrate that, in multi-armed bandit (MAB) tasks, BBN can generate probabilistic choice behavior with an uncertainty bias in a way that resembles human and animal choice patterns. In addition to its high efficiency in MAB tasks, BBN can also be embedded with reinforcement learning algorithms to accelerate learning in MDP tasks. Altogether, our findings reveal the theoretical basis for efficient exploration in biological neural networks and proposes a general, brain-inspired algorithmic architecture for efficient exploration in RL.", "title_embedding_index": 2014, "title_abs_embedding_index": 2039}, {"title": "Robust Learning in Bayesian Parallel Branching Graph Neural Networks: The Narrow Width Limit", "link_suffix": "/forum?id=CkUHtnyhpY", "link": "https://openreview.net/forum?id=CkUHtnyhpY", "pdf_link": "https://openreview.net/pdf?id=CkUHtnyhpY", "keywords": "Bayesian Networks, Gaussian Process, Kernel Renormalization, Graph Neural Networks, Residual Network, Theory of Generalization", "abstract": "The infinite width limit of random neural networks is known to result in Neural Networks as Gaussian Process (NNGP), characterized by task-independent kernels. It is widely accepted that larger network widths contribute to improved generalization. However, this work challenges this notion by investigating the narrow width limit of the Bayesian Parallel Branching Graph Neural Network (BPB-GNN), an architecture that resembles residual GCN. We demonstrate that when the width of a BPB-GNN is significantly smaller compared to the number of training examples, each branch exhibits more robust learning due to a symmetry breaking of branches in kernel renormalization. Surprisingly, the performance of a BPB-GNN in the narrow width limit is generally superior or comparable to that achieved in the wide width limit in bias-limited scenarios. Furthermore, the readout norms of each branch in the narrow width limit are mostly independent of the architectural hyperparameters but generally reflective of the nature of the data. Our results characterize a newly defined narrow-width regime for parallel branching networks in general.", "title_embedding_index": 2015, "title_abs_embedding_index": 2040}, {"title": "World-simulation as pre-training for scalable perception", "link_suffix": "/forum?id=dgb4rfPzaw", "link": "https://openreview.net/forum?id=dgb4rfPzaw", "pdf_link": "https://openreview.net/pdf?id=dgb4rfPzaw", "keywords": "autonomous driving; computer vision; autoregressive transformer; self-supervised learning", "abstract": "Image-based autoregressive next-token prediction offers a promising avenue for developing world video simulators for autonomous driving. However, applications of these autoregressive models for common perception tasks such as geometric and semantic understanding remains under-explored, largely due to the difficulty of applying discrete token modeling to perception tasks. In this paper, we introduce PerceptionLM, an  end-to-end framework that leverages autoregressive world simulators to effectively improve Perception tasks. It consists of a token-based pretraining stage and a novel fine-tuning stage that adapts discrete tokens to continuous embeddings for perception tasks. During pretraining, we leverage the world knowledge from Segment Anything and Depth Anything through autoregressive next-token prediction to imbue the model with world knowledge from multiple vision modalities.  During fine-tuning, we propose a novel decoder adaptor to fuse discrete tokens with continuous embeddings from image encoders, which overcomes the limitations of discrete tokens. With PerceptionLM, we observe impressive scaling properties, where quality is consistently improved when providing more training compute or longer temporal context. On multiple public benchmarks including nuScenes, nuImages, Waymo Open Dataset, and Waymo Open Motion Dataset, PerceptionLM demonstrates significant performance improvements for common perception tasks such as depth estimation and semantic segmentation, highlighting its potential for scaling vision-only foundation models for autonomous driving.", "title_embedding_index": 2016, "title_abs_embedding_index": 2041}, {"title": "Do Think Tags Really Help LLMs Plan? A Critical Evaluation of ReAct-Style Prompting", "link_suffix": "/forum?id=85Ik12q2hP", "link": "https://openreview.net/forum?id=85Ik12q2hP", "pdf_link": "https://openreview.net/pdf?id=85Ik12q2hP", "keywords": "Large Language Models, ReAct, Sequential Decision-Making", "abstract": "The reasoning abilities of Large Language Models (LLMs) remain a topic of debate, which are critically tested in sequential decision-making problems. ReAct, a recently popular method has gained popularity for claiming to enhance LLM reasoning abilities while directly prompting them by $``\\textit{interleaving reasoning trace with action execution}\"$ in text-based planning domains such as AlfWorld and WebShop. However, given the different components of ReAct-style prompting, it remains unclear what the source of improvement in LLM performance is. In this paper, we critically examine the claims of ReAct-style prompting for sequential decision-making problems. By introducing systematic variations to the input prompt, we perform a sensitivity analysis along the original claims of ReAct. Contrary to these claims and common use-cases that utilize ReAct-style prompting, we find that the performance is minimally influenced by the interleaved reasoning trace or by the content of these generated reasoning traces. Instead, the performance of LLMs is primarily driven by the unreasonably high degree of similarity between input example tasks and queries, implicitly forcing the prompt designer to provide instance-specific examples which significantly increases the cognitive burden on the human. Our empirical results, on the same suite of domains as ReAct, show that the perceived reasoning abilities of LLMs stem from the exemplar-query similarity and approximate retrieval rather than any inherent reasoning abilities.", "title_embedding_index": 2017, "title_abs_embedding_index": 2042}, {"title": "Directional Gradient Projection for Robust Fine-tuning of Foundation Models", "link_suffix": "/forum?id=goBaGHLAdP", "link": "https://openreview.net/forum?id=goBaGHLAdP", "pdf_link": "https://openreview.net/pdf?id=goBaGHLAdP", "keywords": "Fine-tuning, transfer learning, foundation models, robustness, visual question answering", "abstract": "Robust fine-tuning aims to adapt large foundation models to downstream tasks while preserving their robustness to distribution shifts. Existing methods primarily focus on constraining and projecting current model towards the pre-trained initialization based on the magnitudes between fine-tuned and pre-trained weights, which often require extensive hyper-parameter tuning and can sometimes result in underfitting. In this work, we propose $\\textbf{Di}$rectional $\\textbf{Gra}$dient $\\textbf{P}$rojection (DiGraP), a novel layer-wise trainable method that incorporates directional information from gradients to bridge regularization and multi-objective optimization. Besides demonstrating our method on image classification, as another contribution we generalize this area to the multi-modal evaluation settings for robust fine-tuning. Specifically, we first bridge the uni-modal and multi-modal gap by performing analysis on Image Classification reformulated Visual Question Answering (VQA) benchmarks and further categorize ten out-of-distribution (OOD) VQA datasets by distribution shift types and degree (i.e. near versus far OOD). Experimental results show that DiGraP consistently outperforms existing baselines across Image Classfication and VQA tasks with discriminative and generative backbones, improving both in-distribution (ID) generalization and OOD robustness.", "title_embedding_index": 2018, "title_abs_embedding_index": 2043}, {"title": "Reward Learning from Multiple Feedback Types", "link_suffix": "/forum?id=9Ieq8jQNAl", "link": "https://openreview.net/forum?id=9Ieq8jQNAl", "pdf_link": "https://openreview.net/pdf?id=9Ieq8jQNAl", "keywords": "Reinforcement Learning, RLHF, Machine Learning, Multi-Type Feedback", "abstract": "Learning rewards from preference feedback has become an important tool in the alignment of agentic models. Preference-based feedback, often implemented as a binary comparison between multiple completions, is an established method to acquire large-scale human feedback. However, human feedback in other contexts is often much more diverse. Such diverse feedback can better support the goals of a human annotator, and the simultaneous use of multiple sources might be mutually informative for the learning process or carry type-dependent biases for the reward learning process.\nDespite these potential benefits, learning from different feedback types has yet to be explored extensively.\nIn this paper, we bridge this gap by enabling experimentation and evaluating multi-type feedback in a wide set of environments. We present a process to generate high-quality simulated feedback of six different types. Then, we implement reward models and downstream RL training for all six feedback types.\nBased on the simulated feedback, we investigate the use of types of feedback across five RL environments and compare them to pure preference-based baselines. We show empirically that diverse types of feedback can be utilized simultaneously and lead to improved reward modeling performance. This work is the first strong indicator of the potential of true multi-type feedback for RLHF.", "title_embedding_index": 2019, "title_abs_embedding_index": 2044}, {"title": "Lowering Data Diversity can Accelerate Training: Case Studies in Synthetic Tasks", "link_suffix": "/forum?id=xlxDTVAbNM", "link": "https://openreview.net/forum?id=xlxDTVAbNM", "pdf_link": "https://openreview.net/pdf?id=xlxDTVAbNM", "keywords": "synthetic tasks, data diversity, curriculum learning, data filtering, learning plateaus, batch gradients", "abstract": "We identify a loss plateau at the start of training in the three synthetic settings of in-context linear regression, sparse parity, and fact memorization. While careful tweaks to the optimization algorithm can mitigate these plateaus, we find that a simpler orthogonal approach oflowering the data diversity, and in doing so, biasing the training distributionawayfrom the test distribution, counter-intuitively also speeds up training. This connection between data diversity and training speed holds for three different diversity-reducinginterventions across our varied synthetic settings. Our findings offer a new perspective on data filtering and curriculum learning for training machine learning models.", "title_embedding_index": 2020, "title_abs_embedding_index": 2045}, {"title": "Hint Marginalization for Improved Reasoning in Large Language Models", "link_suffix": "/forum?id=DzKdjWe59v", "link": "https://openreview.net/forum?id=DzKdjWe59v", "pdf_link": "https://openreview.net/pdf?id=DzKdjWe59v", "keywords": "reasoning, large language models", "abstract": "Large Language Models (LLMs) have exhibited an impressive capability to perform reasoning tasks, especially if they are encouraged to generate a sequence of intermediate steps. Reasoning performance can be improved by suitably combining multiple LLM responses, generated either in parallel in a single query, or via sequential interactions with LLMs throughout the reasoning process. Existing strategies for combination, such as self-consistency and progressive-hint-prompting, make inefficient usage of the LLM responses. We present Hint Marginalization, a novel and principled algorithmic framework to enhance the reasoning capabilities of LLMs. Our approach can be viewed as an iterative sampling strategy for forming a Monte Carlo approximation of an underlying distribution of answers, with the goal of identifying the mode the most likely answer. Empirical evaluation on several benchmark datasets for arithmetic reasoning demonstrates the superiority of the proposed approach.", "title_embedding_index": 2021, "title_abs_embedding_index": 2046}, {"title": "MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning", "link_suffix": "/forum?id=fBhgu6PsA7", "link": "https://openreview.net/forum?id=fBhgu6PsA7", "pdf_link": "https://openreview.net/pdf?id=fBhgu6PsA7", "keywords": "LLM fine-tuning, catastrophic forgetting", "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks. Typically, an LLM is first pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during fine-tuning, LLMs may forget some knowledge acquired in the pre-training stage, leading to a decline in general capabilities. To address this challenge, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). \nAs an extension of greedy block coordinate descent (BCD) methods, MoFO iteratively selects and updates the model parameters with the largest momentum magnitudes.\nMoFO achieves similar fine-tuning performance to the default fine-tuning algorithm while effectively mitigating knowledge forgetting.\nFurthermore, MoFO does not require access to pre-training data, making it highly suitable for scenarios where the pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. \nWe validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting.", "title_embedding_index": 2022, "title_abs_embedding_index": 2047}, {"title": "Orthogonalized Estimation of Difference of Q-functions", "link_suffix": "/forum?id=hQOLtZ40hZ", "link": "https://openreview.net/forum?id=hQOLtZ40hZ", "pdf_link": "https://openreview.net/pdf?id=hQOLtZ40hZ", "keywords": "offline reinforcement learning, causal inference, orthogonal estimation, heterogeneous treatment effects", "abstract": "Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q_\\pi$-functions, $Q_\\pi(s,1)$\u2212$Q_\\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.", "title_embedding_index": 2023, "title_abs_embedding_index": 2048}, {"title": "A Simulation-Free Deep Learning Approach to Stochastic Optimal Control", "link_suffix": "/forum?id=oX4FcNA4UC", "link": "https://openreview.net/forum?id=oX4FcNA4UC", "pdf_link": "https://openreview.net/pdf?id=oX4FcNA4UC", "keywords": "Stochastic Optimal Control, Simulation-Free Methods", "abstract": "We propose a simulation-free algorithm for the solution of generic problems in stochastic optimal control (SOC). Unlike existing methods, our approach does not require the solution of an adjoint problem, but rather leverages Girsanov theorem to directly calculate the gradient of the SOC objective on-policy. This allows us to speed up the optimization of control policies parameterized by neural networks since it completely avoids the expensive back-propagation step through stochastic differential equations (SDEs) used in the Neural SDE framework. In particular, it enables us to solve SOC problems in high dimension and on long time horizons. We demonstrate the efficiency of our approach in various domains of applications, including standard stochastic optimal control problems, sampling from unnormalized distributions via construction of a Schr\"odinger-F\"ollmer process, and fine-tuning of pre-trained diffusion models. In all cases our method is shown to outperform the existing methods in both the computing time and memory efficiency.", "title_embedding_index": 2024, "title_abs_embedding_index": 2049}]