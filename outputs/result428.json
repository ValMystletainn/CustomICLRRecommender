[
    {
        "title": "Deep Networks Learn Features From Local Discontinuities in the Label Function",
        "link_suffix": "/forum?id=52UtL8uA35",
        "link": "https://openreview.net/forum?id=52UtL8uA35",
        "pdf_link": "https://openreview.net/pdf?id=52UtL8uA35",
        "keywords": "Deep Learning, Feature learning, Interpretable, Local Discontinuities, Deep learning theory, Deep neural architectures, Supervised learning",
        "abstract": "Deep neural networks outperform kernel machines on several datasets due to feature learning that happens during gradient descent training. In this paper, we analyze the mechanism through which feature learning happens and use a notion of features that corresponds to discontinuities in the true label function. We hypothesize that the core feature learning mechanism is label function discontinuities attracting model function discontinuities during training. To test this hypothesis, we perform experiments on classification data where the true label function is given by an oblique decision tree. This setup allows easy enumeration of label function discontinuities, while still remaining intractable for static kernel/linear methods. We then design/construct a novel deep architecture called a Deep Linearly Gated Network (DLGN), whose discontinuities in the input space can be easily enumerated.  In this setup, we provide supporting evidence demonstrating the movement of model function discontinuities towards the label function discontinuities during training. The easy enumerability of discontinuities in the DLGN also enables greater mechanistic interpretability. We demonstrate this by extracting the parameters of a high-accuracy decision tree from the parameters of a DLGN. We also show that the DLGN is competitive with ReLU networks and other tree-learning algorithms on several real-world tabular datasets."
    },
    {
        "title": "Data Pruning by Information Maximization",
        "link_suffix": "/forum?id=93XT0lKOct",
        "link": "https://openreview.net/forum?id=93XT0lKOct",
        "pdf_link": "https://openreview.net/pdf?id=93XT0lKOct",
        "keywords": "Data Pruning, Deep Learning",
        "abstract": "In this paper, we present InfoMax, a novel data pruning method, also known as coreset selection, designed to maximize the information content of selected samples while minimizing redundancy. By doing so, InfoMax enhances the overall informativeness of the coreset. The information of individual samples is measured by importance scores, which capture their influence or difficulty in model learning. To quantify redundancy, we use pairwise sample similarities, based on the premise that similar samples contribute similarly to the learning process.\nWe formalize the coreset selection problem as a discrete quadratic programming (DQP) task, with the objective of maximizing the total information content, represented as the sum of individual sample contributions minus the redundancies introduced by similar samples within the coreset.\nTo ensure practical scalability, we introduce an efficient gradient-based solver, complemented by sparsification techniques applied to the similarity matrix and dataset partitioning strategies. \nThis enables InfoMax to seamlessly scale to datasets with millions of samples. \nExtensive experiments demonstrate the superior performance of InfoMax in various data pruning tasks, including image classification, vision-language pre-training, and instruction tuning for large language models."
    },
    {
        "title": "Diff-In: Data Influence Estimation with Differential Approximation",
        "link_suffix": "/forum?id=Jds4tiTo2a",
        "link": "https://openreview.net/forum?id=Jds4tiTo2a",
        "pdf_link": "https://openreview.net/pdf?id=Jds4tiTo2a",
        "keywords": "Deep Learning, Influence Function",
        "abstract": "In this paper, we introduce a new formulation to approximate a sample's influence by accumulating the differences in influence between consecutive learning steps, which we term Diff-In. Specifically, we formulate the sample-wise influence as the cumulative sum of its changes/differences across successive training iterations. \nBy employing second-order approximations, we approximate these difference terms with high accuracy while eliminating the need for model convexity required by existing methods.\nDespite being a second-order method, Diff-In maintains computational complexity comparable to that of first-order methods and remains scalable. This efficiency is achieved by computing the product of the Hessian and gradient, which can be efficiently approximated using finite differences of first-order gradients. \nWe assess the approximation accuracy of Diff-In both theoretically and empirically. Our theoretical analysis demonstrates that Diff-In achieves significantly lower approximation error compared to existing influence estimators. Extensive experiments further confirm its superior performance across multiple benchmark datasets in three data-centric tasks: data cleaning, data deletion, and coreset selection. \nNotably, our experiments on data pruning for large-scale vision-language pre-training show that Diff-In can scale to millions of data points and outperforms strong baselines."
    },
    {
        "title": "Mufu:  Multilingual Fused Learning for Low-Resource Translation with LLM",
        "link_suffix": "/forum?id=0eMsrRMmCw",
        "link": "https://openreview.net/forum?id=0eMsrRMmCw",
        "pdf_link": "https://openreview.net/pdf?id=0eMsrRMmCw",
        "keywords": "translation, low-resource, large language model",
        "abstract": "Multilingual large language models (LLMs) are great translators, but this is largely limited to high-resource languages. For many LLMs, translating in and out of low-resource languages remains a challenging task. To maximize data efficiency in this low-resource setting, we introduce Mufu, which includes a selection of automatically generated multilingual candidates and an instruction to correct inaccurate translations in the prompt. Mufu prompts turn a translation task into a postediting one, and seek to harness the LLM\u2019s reasoning capability with auxiliary translation candidates, from which the model is required to assess the input quality, align the semantics cross-lingually, copy from relevant inputs and override instances that are incorrect. Our experiments on En-XX translations over the Flores-200 dataset show LLMs finetuned against Mufu-style prompts are robust to poor quality auxiliary translation candidates, achieving performance superior to NLLB 1.3B distilled model in 64% of low- and very-low-resource language pairs. We then distill these models to reduce inference cost, while maintaining on average 3.1 chrF improvement over finetune-only baseline in low-resource translations."
    },
    {
        "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
        "link_suffix": "/forum?id=j3BWS9kDYm",
        "link": "https://openreview.net/forum?id=j3BWS9kDYm",
        "pdf_link": "https://openreview.net/pdf?id=j3BWS9kDYm",
        "keywords": "Egocentric Motion Understanding, Egocentric Motion Tracking, Language Model",
        "abstract": "As wearable devices become more prevalent, understanding the user's motion is crucial for improving contextual AI systems. We introduce EgoLM, a versatile framework designed for egocentric motion understanding using multi-modal data. EgoLM integrates the rich contextual information from egocentric videos and motion sensors afforded by wearable devices. It also combines dense supervision signals from motion and language, leveraging the vast knowledge encoded in pre-trained large language models (LLMs). EgoLM models the joint distribution of egocentric motions and natural language using LLMs, conditioned on observations from egocentric videos and motion sensors. It unifies a range of motion understanding tasks, including motion narration from video or motion data, as well as motion generation from text or sparse sensor data. Unique to wearable devices, it also enables a novel task to generate text descriptions from sparse sensors. Through extensive experiments, we validate the effectiveness of EgoLM in addressing the challenges of under-constrained egocentric motion learning, and demonstrate its capability as a generalist model through a variety of applications."
    },
    {
        "title": "Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond",
        "link_suffix": "/forum?id=huo8MqVH6t",
        "link": "https://openreview.net/forum?id=huo8MqVH6t",
        "pdf_link": "https://openreview.net/pdf?id=huo8MqVH6t",
        "keywords": "LLM Unlearning",
        "abstract": "Large language models (LLMs) should undergo rigorous audits to identify potential risks, such as copyright and privacy infringements. Once these risks emerge, timely updates are crucial to remove undesirable responses, ensuring legal and safe model usage. It has spurred recent research into LLM unlearning, focusing on erasing targeted undesirable knowledge without compromising the integrity of other, non-targeted responses. Existing studies have introduced various unlearning objectives to pursue LLM unlearning without necessitating complete retraining. However, each of these objectives has unique properties, and no unified framework is currently available to comprehend them thoroughly. To fill the gap, we propose the metric of the G-effect, quantifying the impacts of unlearning objectives on model performance from a gradient lens. A significant advantage of our metric is its broad ability to detail the unlearning impacts from various aspects across instances, updating steps, and LLM layers. Accordingly, the G-effect offers new insights into identifying drawbacks of existing unlearning objectives, further motivating us to explore a series of candidate solutions for their mitigation and improvements. Finally, we outline promising directions that merit further studies, aiming at contributing to the community to advance this critical field."
    },
    {
        "title": "HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction",
        "link_suffix": "/forum?id=SBzIbJojs8",
        "link": "https://openreview.net/forum?id=SBzIbJojs8",
        "pdf_link": "https://openreview.net/pdf?id=SBzIbJojs8",
        "keywords": "3D reconstruction, Gaussian Splatting, Generalizable Multi-View Reconstruction",
        "abstract": "Reconstructing 3D scenes from multiple viewpoints is a fundamental task in stereo vision. Recently, advances in generalizable 3D Gaussian Splatting have enabled high-quality novel view synthesis for unseen scenes from sparse input views by feed-forward predicting per-pixel Gaussian parameters without extra optimization. However, existing methods typically generate single-scale 3D Gaussians, which lack representation of both large-scale structure and texture details, resulting in mislocation and artefacts. In this paper, we propose a novel framework, HiSplat, which introduces a hierarchical manner in generalizable 3D Gaussian Splatting to construct hierarchical 3D Gaussians via a coarse-to-fine strategy. Specifically, HiSplat generates large coarse-grained Gaussians to capture large-scale structures, followed by fine-grained Gaussians to enhance delicate texture details. To promote inter-scale interactions, we propose an Error Aware Module for Gaussian compensation and a Modulating Fusion Module for Gaussian repair. Our method achieves joint optimization of hierarchical representations, allowing for novel view synthesis using only two-view reference images. Comprehensive experiments on various datasets demonstrate that HiSplat significantly enhances reconstruction quality and cross-dataset generalization compared to prior single-scale methods. The corresponding ablation study and analysis of different-scale 3D Gaussians reveal the mechanism behind the effectiveness. Codes will be released upon acceptance."
    },
    {
        "title": "Objects matter: object-centric world models improve reinforcement learning in visually complex environments",
        "link_suffix": "/forum?id=Q2hkp8WIDS",
        "link": "https://openreview.net/forum?id=Q2hkp8WIDS",
        "pdf_link": "https://openreview.net/pdf?id=Q2hkp8WIDS",
        "keywords": "reinforcement learning, model-based RL, object-centric RL, video object segmentation, Atari, Hollow Knight",
        "abstract": "Deep reinforcement learning has achieved remarkable success in learning control policies from pixels across a wide range of tasks, yet its application remains hindered by low sample efficiency, requiring significantly more environment interactions than humans to reach comparable performance.\nModel-based reinforcement learning (MBRL) offers a solution by leveraging learnt world models to generate simulated experience, thereby improving sample efficiency.\nHowever, in visually complex environments, small or dynamic elements can be critical for decision-making.\nYet, traditional MBRL methods in pixel-based environments typically rely on auto-encoding with an $L_2$ loss, which is dominated by large areas and often fails to capture decision-relevant details.\nTo address these limitations, we propose anobject-centric MBRL pipeline, which integrates recent advances in computer vision to allow agents to focus on key decision-related elements.\nOur approach consists of four main steps: (1) annotating key objects related to rewards and goals with segmentation masks, (2) extracting object features using a pre-trained, frozen foundation vision model, (3) incorporating these object features with the raw observations to predict environmental dynamics, and (4) training the policy using imagined trajectories generated by this object-centric world model.\nBuilding on the efficient MBRL algorithm STORM, we call this pipelineOC-STORM.\nWe demonstrate OC-STORM's practical value in overcoming the limitations of conventional MBRL approaches on both Atari games and the visually complex game Hollow Knight.\nCode and videos are available in the supplementary materials."
    },
    {
        "title": "Modeling All-Atom Glycan Structures via Hierarchical Message Passing and Multi-Scale Pre-training",
        "link_suffix": "/forum?id=HcRpl6hOAl",
        "link": "https://openreview.net/forum?id=HcRpl6hOAl",
        "pdf_link": "https://openreview.net/pdf?id=HcRpl6hOAl",
        "keywords": "Glycan Machine Learning, Heterogeneous Graph Modeling, Self-Supervised Pre-training",
        "abstract": "Understanding the various properties of glycans with machine learning has shown some preliminary promise. However, previous methods mainly focused on modeling the backbone structure of glycans as graphs of monosaccharides (i.e., sugar units), while they neglected the atomic structures underlying each monosaccharide, which are actually important indicators of glycan properties. In this work, we fill this blank by introducing the GlycanAA model for All-Atom-wise Glycan modeling. GlycanAA models a glycan as a heterogeneous graph with monosaccharide nodes representing its global backbone structure and atom nodes representing its local atomic-level structures. Based on such a graph, GlycanAA performs hierarchical message passing to capture from local atomic-level interactions to global monosaccharide-level interactions hierarchically. To further enhance the model capability, we pre-train GlycanAA on a high-quality unlabeled glycan dataset in a self-supervised way, deriving the PreGlycanAA model. Specifically, we design a multi-scale mask prediction algorithm to endow the model with knowledge about different levels of dependencies in a glycan. Extensive benchmark results show the superiority of GlycanAA over existing glycan encoders and verify the further improvements achieved by PreGlycanAA."
    },
    {
        "title": "DEL-Ranking: Ranking-Correction Denoising Framework for Elucidating Molecular Affinities in DNA-Encoded Libraries",
        "link_suffix": "/forum?id=QfyZ28FpVY",
        "link": "https://openreview.net/forum?id=QfyZ28FpVY",
        "pdf_link": "https://openreview.net/pdf?id=QfyZ28FpVY",
        "keywords": "DEL Denoising, Deep Learning, Bioinformatics",
        "abstract": "DNA-encoded library (DEL) screening has revolutionized the detection of protein-ligand interactions through read counts, enabling the rapid exploration of vast chemical spaces. However, noise in read counts, stemming from nonspecific interactions, can mislead this exploration process. Neural networks trained on DEL libraries have been employed to discern and capture nuanced, task-specific patterns within chemical landscapes. However, existing methods overlook two critical aspects: (1) the inherent ranking nature of read counts and (2) the potential of true activity labels to correct systematic biases. We present DEL-Ranking, a novel distribution-correction denoising framework that addresses these challenges. Our approach introduces two key innovations: (1) a novel ranking loss that rectifies relative magnitude relationships between read counts enabling the learning of causal features determining activity levels, and (2) an iterative algorithm employing self-training and consistency loss to establish model coherence between activity label and read count predictions. Furthermore, we contribute three new DEL screening datasets, the first to comprehensively include multi-dimensional molecular representations, protein-ligand enrichment values, and their activity labels. These datasets mitigate data scarcity and incomplete data issues in AI-driven DEL screening research, providing novel benchmark datasets for this field. Rigorous evaluation on diverse DEL datasets demonstrates DEL-Ranking's superior performance across multiple correlation metrics, with significant improvements in binding affinity prediction accuracy. Our model exhibits zero-shot generalization ability across different protein targets and successfully identifies potential motifs determining compound binding affinity. Our work not only advances the field of DEL screening analysis but also provides valuable resources for future research in this area."
    },
    {
        "title": "Exploring contextual modeling with linear complexity for point cloud segmentation",
        "link_suffix": "/forum?id=E1ML0nEReb",
        "link": "https://openreview.net/forum?id=E1ML0nEReb",
        "pdf_link": "https://openreview.net/pdf?id=E1ML0nEReb",
        "keywords": "point cloud segmentation, efficient, contextual modeling",
        "abstract": "Point cloud segmentation is an important topic in 3D understanding. Traditionally, this task has been tackled using either the CNN or Transformer. Recently, a newcomer, Mamba, has emerged as a promising alternative, offering efficient long-range contextual modeling capabilities without the quadratic complexity associated with the Transformer\u2019s attention mechanisms. However, despite Mamba\u2019s potential, early efforts have all failed to achieve better performance than the best CNN-based and Transformer-based methods. Since each of these diverse architectures possesses unique strengths and weaknesses, it's difficult to determine the most suitable approach for this task. In this work, we address this challenge by identifying the key components of an efficient and effective point cloud segmentation architecture. Specifically, we show that: 1) Spatial locality and robust contextual understanding are critical for strong performance, and 2) Mamba features linear computational complexity, offering superior data and inference efficiency compared to Transformers, while still capable of delivering strong contextual understanding. Additionally, we further enhance the standard Mamba specifically for point cloud segmentation by identifying its two key shortcomings. First, the enforced causality in the original Mamba is unsuitable for processing point clouds that have no such dependencies. Second, its unidirectional scanning strategy imposes a directional bias, hampering its ability to capture the full context of unordered point clouds in a single pass. To address these issues, we carefully remove the causal convolutions and introduce a novel Strided Bidirectional SSM to enhance the model\u2019s capability to capture spatial relationships. Our efforts culminate in the development of a novel architecture named MEEPO, which effectively integrates the strengths of CNN and Mamba. MEEPO surpasses the previous state-of-the-art method, PTv3, by up to +0.8 mIoU on key benchmark datasets, including ScanNet, ScanNet200, S3DIS, and nuScenes, while being 42.1% faster and 5.53$\\times$ more memory efficient. Our code and data will be released."
    },
    {
        "title": "Transforming Ocean Analysis: Learning 4D ocean field from in-situ observations via uncertainty-aware implicit representations",
        "link_suffix": "/forum?id=H9plefjzuR",
        "link": "https://openreview.net/forum?id=H9plefjzuR",
        "pdf_link": "https://openreview.net/pdf?id=H9plefjzuR",
        "keywords": "implicit neural representation, meta learning, climate, ocean gridded dataset",
        "abstract": "A complete and accurate representation of Earth's time-evolving ocean field is crucial for understanding global warming as well as climate dynamics. However, the sparsity of current in-situ ocean measurements presents a significant challenge in estimating values in largely unobserved regions. Traditional methods, such as objective interpolation (OI), struggle with accuracy due to their reliance on discrete grids and fixed spatial correlation structures. In this paper, we propose a novel approach to reconstruct 4D ocean fields only from raw observations using implicit neural representations (INRs). Our method improves field representations by leveraging neural networks to capture continuous, complex, and nonlinear patterns inherent in ocean data. To address uncertainties in ocean measurements and the limited availability of daily observations, we incorporate uncertainty estimates and a meta-learning strategy into existing INRs. These innovations enable our approach to provide daily, resolution-free ocean temperature reconstructions, a significant improvement over  monthly averaged discrete fields. Experiments demonstrate the accuracy and adaptability of our method compared with approaches, establishing our method as a transformative solution for future ocean analysis and climate monitoring."
    },
    {
        "title": "A prescriptive theory for brain-like inference",
        "link_suffix": "/forum?id=oRfHv642qD",
        "link": "https://openreview.net/forum?id=oRfHv642qD",
        "pdf_link": "https://openreview.net/pdf?id=oRfHv642qD",
        "keywords": "iterative inference, elbo, variational inference, ood generalization, variational autoencoder, sampling",
        "abstract": "The Evidence Lower Bound (ELBO) is a widely used objective for training deep generative models, such as Variational Autoencoders (VAEs). In the neuroscience literature, an identical objective is known as the Free Energy Principle (FEP), hinting at a potential unified framework for brain function and machine learning. Despite its utility in interpreting generative models, including diffusion models, ELBO maximization is often seen as too broad to offer prescriptive guidance for specific architectures in neuroscience or machine learning. In this work, we show that maximizing ELBO under Poisson assumptions for general sequences leads to a spiking neural network that performs Bayesian posterior inference through its membrane potential dynamics. The resulting model, the iterative Poisson VAE (iP-VAE), has a closer connection to biological neurons than previous brain-inspired predictive coding models based on Gaussian assumptions. Compared to amortized and iterative VAEs, iP-VAE learns sparser representations and exhibits superior generalization to out-of-distribution samples. These findings suggest that optimizing ELBO, combined with Poisson assumptions, provides a solid foundation for developing prescriptive theories in NeuroAI."
    },
    {
        "title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training",
        "link_suffix": "/forum?id=8q3WIvJhkl",
        "link": "https://openreview.net/forum?id=8q3WIvJhkl",
        "pdf_link": "https://openreview.net/pdf?id=8q3WIvJhkl",
        "keywords": "Diffusion Model; Efficient Training",
        "abstract": "Training diffusion models is always a computation-intensive task. In this paper, we introduce a novel speed-up method for diffusion model training, called, which is based on a closer look at time steps. Our key findings are: i) Time steps can be empirically divided into acceleration, deceleration, and convergence areas based on the process increment. ii) These time steps are imbalanced, with many concentrated in the convergence area. iii) The concentrated steps provide limited benefits for diffusion training. To address this, we design an asymmetric sampling strategy that reduces the frequency of steps from the convergence area while increasing the sampling probability for steps from other areas. Additionally, we propose a weighting strategy to emphasize the importance of time steps with rapid-change process increments. As a plug-and-play and architecture-agnostic approach, SpeeD consistently achieves 3-times acceleration across various diffusion architectures, datasets, and tasks. Notably, due to its simple design, our approach significantly reduces the cost of diffusion model training with minimal overhead. Our research enables more researchers to train diffusion models at a lower cost."
    },
    {
        "title": "Cross-Domain Reinforcement Learning via Preference Consistency",
        "link_suffix": "/forum?id=aXPOA3urmA",
        "link": "https://openreview.net/forum?id=aXPOA3urmA",
        "pdf_link": "https://openreview.net/pdf?id=aXPOA3urmA",
        "keywords": "Reinforcement learning, Cross-domain transfer, Transfer learning, Preference-based RL",
        "abstract": "Cross-domain reinforcement learning (CDRL) aims to utilize the knowledge acquired from a source domain to efficiently learn tasks in a target domain. Unsupervised CDRL assumes no access to any signal (e.g., rewards) from the target domain, and most methods utilize state-action correspondence or cycle consistency. In this work, we identify the critical correspondence identifiability issue (CII) that arises in existing unsupervised CDRL methods. To address this identifiability issue, we propose leveraging pairwise trajectory preferences in the target domain as weak supervision. Specifically, we introduce the principle of cross-domain preference consistency (CDPC)\u2013a policy is more transferable across the domains if the source and target domains have similar preferences over trajectories\u2013to provide additional guidance for establishing proper correspondence between the source and target domains. To substantiate the principle of CDPC, we present an algorithm that integrates a state decoder learned through preference consistency loss during training with a cross-domain MPC method for action selection during inference. Through extensive experiments in both MuJoCo and Robosuite, we demonstrate that CDPC enables effective and data-efficient knowledge transfer across domains, outperforming state-of-the-art CDRL benchmark methods."
    },
    {
        "title": "A Probabilistic Generative Method for Safe Physical System Control Problems",
        "link_suffix": "/forum?id=WwQdcQROmb",
        "link": "https://openreview.net/forum?id=WwQdcQROmb",
        "pdf_link": "https://openreview.net/pdf?id=WwQdcQROmb",
        "keywords": "safe PDE control, physical systems, generative models, conformal prediction, fine-tuning",
        "abstract": "Controlling complex physical systems is a crucial task in science and engineering, often requiring the balance of control objectives and safety constraints. Recently, diffusion models have demonstrated a strong ability to model high-dimensional state spaces, giving them an advantage over recent deep learning and reinforcement learning-based methods in complex control tasks. However, they do not inherently address safety concerns. In contrast, while safe reinforcement learning methods consider safety, they typically fail to provide guarantees for satisfying safety constraints.\nTo address these limitations, we propose Safe Conformal Physical system control (SafeConPhy), which optimizes the diffusion model with a provable safety bound iteratively to satisfy the safety constraint.\nWe pre-train a diffusion model on the training set. Given the calibration set and the specific control targets, we derive a provable safety bound using conformal prediction. After iteratively enhancing the safety of the diffusion model with the progressively updated bound, the model's output can be certified as safe with a user-defined probability.\nWe evaluate our algorithm on two control tasks: 1D Burgers' equation and 2D incompressible fluid. Our results show that our algorithm satisfies safety constraints, and outperforms prior control methods and safe offline RL algorithms."
    },
    {
        "title": "Continuous Space-Time Video Super-Resolution via Event Camera",
        "link_suffix": "/forum?id=a8uJXdi7Df",
        "link": "https://openreview.net/forum?id=a8uJXdi7Df",
        "pdf_link": "https://openreview.net/pdf?id=a8uJXdi7Df",
        "keywords": "Event Camera, Video Super-resolution, Video Frame Interpolation, Continuous Space-time Video Super-resolution",
        "abstract": "Continuous space-time video super-resolution (C-STVSR) aims to simultaneously enhance video resolution and frame rate at an arbitrary scale. Recently, implicit neural representation (INR) has been applied to video restoration, representing videos as implicit fields that can be decoded at an arbitrary scale. However, the highly ill-posed nature of C-STVSR limits the effectiveness of current INR-based methods: they assume linear motion between frames and use interpolation or feature warping to generate features at arbitrary spatiotemporal positions with \\ubtxt{two} consecutive frames. This restrains C-STVSR from capturing rapid and \\ubtxt{nonlinear motion} and \\ubtxt{long-term dependencies} (\\textit{involving more than two frames}) in complex dynamic scenes. In this paper, we propose a novel C-STVSR framework, called \\textbf{HR-INR}, which captures both \\textbf{h}olistic dependencies and \\textbf{r}egional motions based on INR. It is assisted by an event camera -- a novel sensor renowned for its high temporal resolution and low latency. To fully utilize the rich temporal information from events, we design a feature extraction consisting of (1) a regional event feature extractor -- taking events as inputs via the proposed event temporal pyramid representation to capture the regional nonlinear motion and (2) a holistic event-frame feature extractor for long-term dependence and continuity motion. We then propose a novel INR-based decoder with spatiotemporal embeddings to capture long-term dependencies with a larger temporal perception field. We validate the effectiveness and generalization of our method on four datasets (both simulated and real data), showing the superiority of our method."
    },
    {
        "title": "OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?",
        "link_suffix": "/forum?id=hL5jone2Oh",
        "link": "https://openreview.net/forum?id=hL5jone2Oh",
        "pdf_link": "https://openreview.net/pdf?id=hL5jone2Oh",
        "keywords": "oracle bone inscriptions, ancient character deciphering, large multi-modal models, benchmark",
        "abstract": "We introduce OBI-Bench, a holistic benchmark crafted to systematically evaluate large multi-modal models (LMMs) on whole-process oracle bone inscriptions (OBI) processing tasks demanding expert-level domain knowledge and deliberate cognition. OBI-Bench includes 5,523 meticulously collected diverse-sourced images, covering five key domain problems: recognition, rejoining, classification, retrieval, and deciphering. These images span centuries of archaeological findings and years of research by front-line scholars, comprising multi-stage font appearances from excavation to synthesis, such as original oracle bone, inked rubbings, oracle bone fragments, cropped single character, and handprinted character. Unlike existing benchmarks, OBI-Bench focuses on advanced visual perception and reasoning with OBI-specific knowledge, challenging LMMs to perform tasks akin to those faced by experts. The evaluation of 6 proprietary LMMs as well as 17 open-source LMMs highlights the substantial challenges and demands posed by OBI-Bench. Even the latest versions of GPT-4o, Gemini 1.5 Pro, and Qwen-VL-Max are still far from public-level humans in some fine-grained perception tasks. However, they perform at a level comparable to untrained humans in deciphering task, indicating remarkable capabilities in offering new interpretative perspectives and generating creative guesses. We hope OBI-Bench can facilitate the community to develop domain-specific multi-modal foundation models towards ancient language research and delve deeper to discover and enhance these untapped potentials of LMMs."
    },
    {
        "title": "Generalizable Human Gaussians from Single-View Image",
        "link_suffix": "/forum?id=dQ2xiSIYzp",
        "link": "https://openreview.net/forum?id=dQ2xiSIYzp",
        "pdf_link": "https://openreview.net/pdf?id=dQ2xiSIYzp",
        "keywords": "Human Gaussians, 3D Human Reconstruction",
        "abstract": "In this work, we tackle the task of learning 3D human Gaussians from a single image, focusing on recovering detailed appearance and geometry including unobserved regions. We introduce a single-view generalizable Human Gaussian Model (HGM), which employs a novel generate-then-refine pipeline with the guidance from human body prior and diffusion prior. Our approach uses a ControlNet to refine rendered back-view images from coarse predicted human Gaussians, then uses the refined image along with the input image to reconstruct refined human Gaussians. To mitigate the potential generation of unrealistic human poses and shapes, we incorporate human priors from the SMPL-X model as a dual branch, propagating image features from the SMPL-X volume to the image Gaussians using sparse convolution and attention mechanisms. Given that the initial SMPL-X estimation might be inaccurate, we gradually refine it with our HGM model. We validate our approach on several publicly available datasets. Our method surpasses previous methods in both novel view synthesis and surface reconstruction. Our approach also exhibits strong generalization for cross-dataset evaluation and in-the-wild images."
    },
    {
        "title": "Wavelet Diffusion Neural Operator",
        "link_suffix": "/forum?id=FQhDIGuaJ4",
        "link": "https://openreview.net/forum?id=FQhDIGuaJ4",
        "pdf_link": "https://openreview.net/pdf?id=FQhDIGuaJ4",
        "keywords": "PDE, physics, simulation, control, diffusion model, wavelet, abrupt changes, multi-resolution",
        "abstract": "Simulating and controlling physical systems described by partial differential equations (PDEs) are crucial tasks across science and engineering. Recently, diffusion generative models have emerged as a competitive class of methods for these tasks due to their ability to capture long-term dependencies and model high-dimensional states. However, diffusion models typically struggle with handling system states with abrupt changes and generalizing to higher resolutions.\nIn this work, we propose Wavelet Diffusion Neural Operator (WDNO), a novel PDE simulation and control framework that enhances the handling of these complexities.\nWDNO comprises two key innovations. Firstly, WDNO performs diffusion-based generative modeling in the wavelet domain for the entire trajectory to handle abrupt changes and long-term dependencies effectively. Secondly, to address the issue of poor generalization across different resolutions, which is one of the fundamental tasks in modeling physical systems, we introduce multi-resolution training. We validate WDNO on three challenging physical systems with abrupt changes including 1D Burgers' equation, 1D compressible Navier-Stokes equation and 2D incompressible fluid, which demonstrates superior performance on both simulation and control tasks over state-of-the-art methods, with significant improvements in long-term and detail prediction accuracy. Remarkably, in the challenging context of the 2D high-dimensional and indirect control task aimed at reducing smoke leakage, WDNO reduces the leakage by 33.2% compared to the second-best baseline."
    },
    {
        "title": "MMSearch: Benchmarking the Potential of Large Models as Multi-modal Search Engines",
        "link_suffix": "/forum?id=J2Jyp1SZ0n",
        "link": "https://openreview.net/forum?id=J2Jyp1SZ0n",
        "pdf_link": "https://openreview.net/pdf?id=J2Jyp1SZ0n",
        "keywords": "Large Multimodal Model, AI Search Engine, Benchmark",
        "abstract": "The advent of Large Language Models (LLMs) has paved the way for AI search engines, e.g., SearchGPT, showcasing a new paradigm in human-internet interaction. However, most current AI search engines are limited to text-only settings, neglecting the multimodal user queries and the text-image interleaved nature of website information. Recently, Large Multimodal Models (LMMs) have made impressive strides. Yet, whether they can function as AI search engines remains under-explored, leaving the potential of LMMs in multimodal search an open question. To this end, we first design a delicate pipeline, MMSearch-Engine, to empower any LMMs with multimodal search capabilities. On top of this, we introduce MMSearch, a comprehensive evaluation benchmark to assess the multimodal search performance of LMMs. The curated dataset contains 300 manually collected instances spanning 14 subfields, which involves no overlap with the current LMMs' training data, ensuring the correct answer can only be obtained within searching. By using MMSearch-Engine, the LMMs are evaluated by performing three individual tasks (requery, rerank, and summarization), and one challenging end-to-end task with a complete searching process. We conduct extensive experiments on closed-source and open-source LMMs. Among all tested models, GPT-4o with MMSearch-Engine achieves the best results, which surpasses the commercial product, Perplexity Pro, in the end-to-end task, demonstrating the effectiveness of our proposed pipeline. We further present error analysis to unveil current LMMs still struggle to fully grasp the multimodal search tasks, and conduct ablation study to indicate the potential of scaling test-time computation for AI search engine. We hope MMSearch may provide unique insights to guide the future development of multimodal AI search engine."
    },
    {
        "title": "Coreset Selection via Reducible Loss in Continual Learning",
        "link_suffix": "/forum?id=mAztx8QO3B",
        "link": "https://openreview.net/forum?id=mAztx8QO3B",
        "pdf_link": "https://openreview.net/pdf?id=mAztx8QO3B",
        "keywords": "Continual learning, Coreset selection",
        "abstract": "A natural solution for rehearsal-based continual learning is to select a coreset as memory. A coreset serves as an informative summary of a large dataset, enabling a model trained solely on the coreset to achieve performance comparable to training on the full dataset. Previous bi-level coreset selection methods adjust sample weights or probabilities to minimize the outer loss, which is computed over the entire dataset. For non-representative samples like ambiguous or noisy samples, since these samples are not well learned even training model on the full dataset, loss of these samples in the outer loss are not worthy to be reduced. However, their high loss values may cause them to be selected in an attempt to minimize the outer loss, which may lead to suboptimal performance for models trained on the coreset. To address this issue, we first investigate how the performance of a trained model changes when a sample is added to the training dataset and approximate this performance gain using reducible loss. We then select samples with the highest performance gain in the coreset so that performance of model trained on coreset could be maximized. We show that samples with high performance gain are informative and representative. Furthermore, reducible loss requires only forward computation, making it significantly more efficient than previous methods. To better apply coreset selection in continual learning, we extend our method to address key challenges such as task interference, streaming data, and knowledge distillation. Experiments on data summarization and continual learning demonstrate the effectiveness and efficiency of our approach."
    },
    {
        "title": "SegLLM: Multi-round Reasoning Segmentation with Large Language Model",
        "link_suffix": "/forum?id=Pm1NXHgzyf",
        "link": "https://openreview.net/forum?id=Pm1NXHgzyf",
        "pdf_link": "https://openreview.net/pdf?id=Pm1NXHgzyf",
        "keywords": "LLMs, Reasoning Segmentation, Muiti-round Conversations",
        "abstract": "We present SegLLM, a novel multi-round interactive reasoning segmentation model that enhances LLM-based segmentation by exploiting conversational memory of both visual and textual outputs. By leveraging a mask-aware multimodal LLM, SegLLM re-integrates previous segmentation results into its input stream, enabling it to reason about complex user intentions and segment objects in relation to previously identified entities, including positional, interactional, and hierarchical relationships, across multiple interactions. This capability allows SegLLM to respond to visual and text queries in a chat-like manner. Evaluated on the newly curated MRSeg benchmark, SegLLM outperforms existing methods in multi-round interactive reasoning segmentation by over 20%. In addition, SegLLM obtains a 5.5% improvement in cIoU for standard single-round referring segmentation and a 4.5% increase inAcc@0.5for referring expression comprehension."
    },
    {
        "title": "ZERO-1-to-G: Taming Pretrained 2D Diffusion Models for Direct 3D Generation",
        "link_suffix": "/forum?id=nmc9ujrZ5R",
        "link": "https://openreview.net/forum?id=nmc9ujrZ5R",
        "pdf_link": "https://openreview.net/pdf?id=nmc9ujrZ5R",
        "keywords": "native 3D diffusion, 3D generative model, gaussian splats",
        "abstract": "Recent advances in 2D image generation have achieved remarkable quality, largely driven by the capacity of diffusion models and the availability of large-scale datasets. However, direct 3D generation is still constrained by the scarcity and lower fidelity of 3D datasets. In this paper, we introduce Zero-1-to-G, a novel approach that addresses this problem by enabling direct 3D generation on Gaussian splats through 2D diffusion models. Our key insight is that Gaussian splats, a 3D representation, can be decomposed into multi-view images encoding different attributes. This reframes the challenging task of direct 3D generation within a 2D diffusion framework, allowing us to leverage the rich priors of pretrained 2D diffusion models. To incorporate 3D awareness, we introduce cross-view and cross-attribute attention layers, which capture complex correlations and enforce 3D consistency across generated splats. This makes Zero-1-to-G the first direct 3D generative model to effectively utilize 2D pretrained diffusion priors, enabling efficient training and improved generalization to unseen objects. Extensive experiments on both synthetic and in-the-wild datasets demonstrate superior performance in 3D object generation, offering a new approach to high-quality 3D generation."
    },
    {
        "title": "Adaptive Exponential Decay Rates for Adam",
        "link_suffix": "/forum?id=5nldnvvHfw",
        "link": "https://openreview.net/forum?id=5nldnvvHfw",
        "pdf_link": "https://openreview.net/pdf?id=5nldnvvHfw",
        "keywords": "Optimization method, deep neural networks, Adam and its variants",
        "abstract": "Adam and its variants, including AdaBound, AdamW, and AdaBelief, have gained widespread popularity for enhancing the learning speed and generalization performance of deep neural networks. This optimization technique adjusts weight vectors by utilizing predetermined exponential decay rates (i.e.,$\\beta_1$ = 0.9, $\\beta_2$ = 0.999) based on the first moment estimate and the second raw moment estimate of the gradient. However, the default exponential decay rates might not be optimal, and the process of tuning them through trial and error with experience proves to be time-consuming. In this paper, we introduce AdamE, a novel variant of Adam designed to automatically leverage dynamic exponential decay rates on the first moment estimate and the second raw moment estimate of the gradient. Additionally, we provide theoretical proof of the convergence of AdamE in both convex and non-convex cases. To validate our claims, we perform experiments across various neural network architectures and tasks. Comparative analyses with adaptive methods utilizing default exponential decay rates reveal that AdamE consistently achieves rapid convergence and high accuracy in language modeling, node classification, and graph clustering tasks."
    }
]