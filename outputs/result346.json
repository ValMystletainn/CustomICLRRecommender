[
    {
        "title": "Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning",
        "link_suffix": "/forum?id=2uPZ4aX1VV",
        "link": "https://openreview.net/forum?id=2uPZ4aX1VV",
        "pdf_link": "https://openreview.net/pdf?id=2uPZ4aX1VV",
        "keywords": "Goal Conditioned Reinforcement Learning, Factor Interactions, Factored State, Hindsight Experience Replay, Counterfactual",
        "abstract": "Hindsight relabeling is a powerful tool for overcoming sparsity in goal-conditioned reinforcement learning (GCRL). While effective in some domains like navigation and locomotion, hindsight relabeling can struggle in object-centric domains. For example, suppose that the goal space consists of a robotic arm pushing a particular target block to a goal location. In this case, hindsight relabeling will give high rewards to any trajectory that does not interact with the block. However, these behaviors are only useful when the object is already at the goal\u2014an extremely rare case in practice. A dataset dominated by these kinds of trajectories will make learning more difficult. On the other hand, much of the meaningful behavior is filtered through interactions such as pushing the block with the gripper. To address this issue, we introduce Hindsight Relabeling using Interactions (HInt), which combines interactions with hindsight relabeling to improve the sample efficiency of downstream RL. However, interactions do not have a general consensus statistical definition, and especially one useful for downstream GCRL. Therefore, we propose a definition of interactions based on the concept of null counterfactual: a cause object is interacting with a target object if in a world where the cause object did not exist, the target object would have different transition dynamics. We leverage this definition to infer interactions in Null Counterfactual Interaction Inference (NCII), which uses a \u201cnulling\u201d operation with a learned model to simulate absences and infer interactions. We demonstrate that NCII is able to achieve significantly improved interaction inference accuracy on both simple linear dynamics domains and dynamic robotic domains in Robosuite, Robot Air Hockey, and Franka Kitchen. Furthermore, we demonstrate that HInt improves sample efficiency by up to 4\u00d7 in these domains as goal-conditioned tasks."
    },
    {
        "title": "PAD: Personalized Alignment at Decoding-time",
        "link_suffix": "/forum?id=e7AUJpP8bV",
        "link": "https://openreview.net/forum?id=e7AUJpP8bV",
        "pdf_link": "https://openreview.net/pdf?id=e7AUJpP8bV",
        "keywords": "Alignment, personalization",
        "abstract": "Aligning with personalized preferences, which vary significantly across cultural, educational, and political differences, poses a significant challenge due to the computational costs and data demands of traditional alignment methods. In response, this paper presents Personalized Alignment at Decoding-time (PAD), a novel framework designed to align LLM outputs with diverse personalized preferences during the inference phase, eliminating the need for additional training. By introducing a unique personalized reward modeling strategy, this framework decouples the text generation process from personalized preferences, facilitating the generation of generalizable token-level personalized rewards. The PAD algorithm leverages these rewards to guide the decoding process, dynamically tailoring the base model\u2019s predictions to personalized preferences. Extensive experimental results demonstrate that PAD not only outperforms existing training-based alignment methods in terms of aligning with diverse preferences but also shows significant generalizability to preferences unseen during training and scalability across different base models. This work advances the capability of LLMs to meet user needs in real-time applications, presenting a substantial step forward in personalized LLM alignment."
    },
    {
        "title": "Next state prediction gives rise to entangled, yet compositional representations of objects",
        "link_suffix": "/forum?id=7QGyDi9VsO",
        "link": "https://openreview.net/forum?id=7QGyDi9VsO",
        "pdf_link": "https://openreview.net/pdf?id=7QGyDi9VsO",
        "keywords": "Compositionality, object-centric representations, unsupervised learning, latent dynamics",
        "abstract": "Compositional representations are thought to enable humans to generalize across combinatorially vast state spaces. Models with learnable object slots, which encode information about objects in separate latent codes, have shown promise for this type of generalization but rely on strong architectural priors. Models with distributed representations, on the other hand, use overlapping, potentially entangled neural codes, and their ability to support compositional generalization remains underexplored. In this paper we examine whether distributed models can develop linearly separable representations of objects, like slotted models, through unsupervised training on videos of object interactions. We show that, surprisingly, models with distributed representations often match or outperform models with object slots in downstream prediction tasks. Furthermore, we find that linearly separable object representations can emerge without object-centric priors, with auxiliary objectives like next-state prediction playing a key role. Finally, we observe that distributed models' object representations are never fully disentangled, even if they are linearly separable: Multiple objects can be encoded through partially overlapping neural populations while still being highly separable with a linear classifier. We hypothesize that maintaining partially shared codes enables distributed models to better compress object dynamics, potentially enhancing generalization."
    },
    {
        "title": "De-biasing Diffusion: Data-Free FP8 Quantization of Text-to-Image Models with Billions of Parameters",
        "link_suffix": "/forum?id=nExUJBF5tR",
        "link": "https://openreview.net/forum?id=nExUJBF5tR",
        "pdf_link": "https://openreview.net/pdf?id=nExUJBF5tR",
        "keywords": "Diffusion, Quantization, Floating-Point, Data-Free, Text-To-Image",
        "abstract": "Diffusion neural networks have become the go-to solution for tasks involving automatic image generation, but the generation process is expensive in terms of memory, energy, and computational cost. Several works have aimed to reduce the cost via quantization to 8 bits or less. Despite that, half-precision (FP16) is still the default mode for large text-to-image generation models --- where reducing numerical precision becomes more challenging. In this work, we show that the reduction of quantization bias can be more important than the reduction in quantization (mean square) error. We propose a data-free method for model quantization, with the goal of producing images that are indistinguishable from those generated by large, full-precision diffusion models. We show that simple methods like stochastic rounding can decrease the quantization bias and improve image generation quality with little to no cost. To close the remaining gap between full-precision and quantized models, we suggest a feasible method for partial stochastic-rounding of weights. When using the MS-COCO dataset as the baseline, we show our quantization methods achieve as good FID scores as the full-precision model. Moreover, our methods decrease the quantization-induced distortion of the images generated by the full-precision model, with the distortion decreasing with the number of diffusion steps."
    },
    {
        "title": "BiCompFL: Stochastic Federated Learning with Bi-Directional Compression",
        "link_suffix": "/forum?id=ogIFNo2bQw",
        "link": "https://openreview.net/forum?id=ogIFNo2bQw",
        "pdf_link": "https://openreview.net/pdf?id=ogIFNo2bQw",
        "keywords": "Communication-efficiency, Importance sampling, Stochastic federated learning",
        "abstract": "Communication is a prominent bottleneck in federated learning (FL). State-of-the-art accuracy performance under limited uplink communications from the clients to the federator is achieved by stochastic FL approaches. It has been recently shown that leveraging side information in the form of a prior distribution at the federator can drastically reduce the uplink communication cost in stochastic FL. Here, the latest global model distribution serves as a natural prior since it can be shared with the clients under ideal downlink communication from the federator to the clients. Nevertheless, downlink communication is often limited in practical settings, and bi-directional compression must be considered to reduce the overall communication cost. The extension of existing stochastic FL solutions to bi-directional compression is non-trivial due to the lack of a globally shared common prior distribution at each iteration. In this paper, we propose BiCompFL, which employs importance sampling to send samples from the updated local models in the uplink, and the aggregated global model in the downlink by carefully choosing common prior distributions as side-information. We theoretically study the communication cost by a new analysis of importance sampling that refines known results, and exposes the interplay between uplink and downlink communication costs. We also show through numerical experiments that BiCompFL enables multi-fold savings in communication cost compared to the state-of-the-art."
    },
    {
        "title": "MarDini: Masked Autoregressive Diffusion for Video Generation at Scale",
        "link_suffix": "/forum?id=YJwnlplKQ7",
        "link": "https://openreview.net/forum?id=YJwnlplKQ7",
        "pdf_link": "https://openreview.net/pdf?id=YJwnlplKQ7",
        "keywords": "Video Generation, Diffusion Model, Masked Auto-regressive Model",
        "abstract": "We introduce MarDini, a new family of video diffusion models that integrate the advantages of masked auto-regression (MAR) into a unified diffusion model (DM) framework. Here, MAR handles temporal planning, while DM focuses on spatial generation in an asymmetric network design: i) a MAR-based planning model containing most of the parameters generates planning signals for each masked frame using low-resolution input; ii) a lightweight generation model uses these signals to produce high-resolution frames via diffusion de-noising. MarDini\u2019s MAR enables video generation conditioned on any number of masked frames at any frame positions: a single model can handle video interpolation (e.g., masking middle frames), image-to-video generation (e.g., masking from the second frame onward), and video expansion (e.g., masking half the frames). The efficient design allocates most of the computational resources to the low-resolution planning model, making computationally expensive but important spatio-temporal attention feasible at scale. MarDini sets a new state-of-the-art for video interpolation; meanwhile, within few inference steps, it efficiently generates videos on par with those of much more expensive advanced image-to-video models."
    },
    {
        "title": "Foldable SuperNets: Scalable Merging of Transformers with Different Initializations and Tasks",
        "link_suffix": "/forum?id=LJGY2GVcit",
        "link": "https://openreview.net/forum?id=LJGY2GVcit",
        "pdf_link": "https://openreview.net/pdf?id=LJGY2GVcit",
        "keywords": "Model merging, Knowledge Distillation, Deep Learning",
        "abstract": "Many recent methods aim to merge neural networks (NNs) with identical architectures trained on different tasks to obtain a single multi-task model. Most existing works tackle the simpler setup of merging NNs initialized from a common pre-trained network, where simple heuristics like weight averaging work well. This work targets a more challenging goal: merging large transformers trained on different tasks from distinct initializations. First, we demonstrate that traditional merging methods fail catastrophically in this setup. To overcome this challenge, we propose Foldable SuperNet Merge (FS-Merge), a method that optimizes a SuperNet to fuse the original models using a feature reconstruction loss. FS-Merge is simple, data-efficient, and capable of merging models of varying widths. We test FS-Merge against existing methods, including knowledge distillation, on MLPs and transformers across various settings, sizes, tasks, and modalities. FS-Merge consistently outperforms them, achieving SOTA results, particularly in limited data scenarios."
    },
    {
        "title": "LLM Pruning and Distillation in Practice",
        "link_suffix": "/forum?id=mMmzHS28ht",
        "link": "https://openreview.net/forum?id=mMmzHS28ht",
        "pdf_link": "https://openreview.net/pdf?id=mMmzHS28ht",
        "keywords": "llm, compression, pruning, distillation",
        "abstract": "Structured pruning with knowledge distillation is a potent combination for obtaining small language models (SLMs) with significantly fewer training tokens and compute resources compared to training from scratch. In this work, we investigate how this strategy can be effectively applied in instances where access to the the original pretraining dataset is restricted. We introduce a newteacher correctionphase before distillation which lets the teacher model adjust to our specific data distribution using a lightweight fine-tuning phase. We apply this strategy to compress the Mistral NeMo 12B and Llama 3.1 8B models to 8B and 4B parameters, respectively, using pruning and distillation. We explore two distinct pruning strategies: (1) depth pruning and (2) joint hidden/attention/MLP (width) pruning, and evaluate the results on common benchmarks from the LM Evaluation Harness. The models are then aligned with NeMo Aligner and further tested for instruction following, role-play, math, coding and function calling capabilities. This approach produces the state-of-the-art Mistral-NeMo-Compressed-8B (\\MNMinitron for brevity) model from Mistral NeMo 12B, and a compelling 4B model from Llama 3.1 8B."
    },
    {
        "title": "Embedding-Converter: A Unified Framework for Cross-Model Embedding Transformation",
        "link_suffix": "/forum?id=ga9PAnFsAt",
        "link": "https://openreview.net/forum?id=ga9PAnFsAt",
        "pdf_link": "https://openreview.net/pdf?id=ga9PAnFsAt",
        "keywords": "Embeddings, Embedding Converter, Embedding transformation",
        "abstract": "Embeddings, numerical representations of data like text and images, are fundamental to machine learning. However, the continuous emergence of new embedding models poses a challenge: migrating to these potentially superior models often requires computationally expensive re-embedding of entire datasets even without guarantees of improvement. This paper introduces Embedding-Converter, a unified framework and a novel paradigm for efficiently converting embeddings between different models, eliminating the need for costly re-embedding. In real-world scenarios, the proposed method yields O(100) times faster and cheaper computation of embeddings with new models. Our experiments demonstrate that Embedding-Converter not only facilitates seamless transitions to new models but can even surpass the source model's performance, approaching that of the target model. This enables efficient evaluation of new embedding models and promotes wider adoption by reducing the overhead associated with model switching. Moreover, Embedding-Converter addresses latency constraints by enabling the use of smaller models for online tasks while leveraging larger models for offline processing. By encouraging users to release converters alongside new embedding models, Embedding-Converter fosters a more dynamic and user-friendly paradigm for embedding model development and deployment."
    },
    {
        "title": "Temporal Prompting Matters: Rethinking Referring Video Object Segmentation",
        "link_suffix": "/forum?id=GcYcUE3GvY",
        "link": "https://openreview.net/forum?id=GcYcUE3GvY",
        "pdf_link": "https://openreview.net/pdf?id=GcYcUE3GvY",
        "keywords": "Referring Video Object Segmentation, Foundation Models, Prompting",
        "abstract": "Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework."
    },
    {
        "title": "Learning Arbitrary Logical Formula as a Sparse Neural Network Module",
        "link_suffix": "/forum?id=x3cFAoorct",
        "link": "https://openreview.net/forum?id=x3cFAoorct",
        "pdf_link": "https://openreview.net/pdf?id=x3cFAoorct",
        "keywords": "Neuro-Symbolic AI; System 2 intelligence; Deep Symbolic Learning (DSL); Equation Learner (EQL); differentiable Neural Logic Networks (dNL)",
        "abstract": "NeSy (Neuro-Symbolic) predictors are hybrid models composed of symbolic predictive models chained after neural networks. Most existing NeSy predictors require either given symbolic knowledge or iterative training. DSL (Deep Symbolic Learning) is the first NeSy predictor that supports fully end-to-end training from scratch, but it learns a look-up table rather than arbitrary programs or formulas. We propose the Logical Formula Learner framework, a general framework of network modules that explicitly equate a logical formula after convergence. We then propose 3 novel designs within the LFL framework with different levels of combinatorial search freedom: LFL-Type1 learns arbitrary logical formula, LFL-Type2 learns a look-up table, and LFL-Type3 has combinatorial search freedom between them. LFL-Type1 and LFL-Type2 show improvements over previous designs, and all three types can be wrapped into NeSy predictors. To our knowledge, LFL-Type1-based NeSy predictor is the first NeSy predictor that supports fully end-to-end training from scratch and explicitly learns arbitrary logical formulas."
    },
    {
        "title": "Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes",
        "link_suffix": "/forum?id=bgk4O69SoL",
        "link": "https://openreview.net/forum?id=bgk4O69SoL",
        "pdf_link": "https://openreview.net/pdf?id=bgk4O69SoL",
        "keywords": "large language model, alignment, human preference, variational inference",
        "abstract": "Large language models (LLMs) have achieved remarkable success, yet aligning their generations with human preferences remains a critical challenge. Existing approaches to preference modeling often rely on an explicit or implicit reward function, overlooking the intricate and multifaceted nature of human preferences that may encompass conflicting factors across diverse tasks and populations. To address this limitation, we introduce Latent Preference Coding (LPC), a novel framework that models the implicit factors as well as their combinations behind holistic preferences using discrete latent codes. LPC seamlessly integrates with various offline alignment algorithms, automatically inferring the underlying factors and their importance from data without relying on pre-defined reward functions and hand-crafted combination weights. Extensive experiments on multiple benchmarks demonstrate that LPC consistently improves upon three alignment algorithms (DPO, SimPO, and IPO) using three base models (Mistral-7B, Llama3-8B, and Llama3-Instruct-8B). Furthermore, deeper analysis reveals that the learned latent codes effectively capture the differences in the distribution of human preferences and significantly enhance the robustness of alignment algorithms against noise in data. By providing a unified representation for the multifarious preference factors, LPC paves the way towards developing more robust and versatile alignment techniques for responsible deployment of powerful LLMs."
    },
    {
        "title": "FedSR: Frequency-Aware Enhancement for Diffusion-based Image Super-Resolution",
        "link_suffix": "/forum?id=VYfYISQncf",
        "link": "https://openreview.net/forum?id=VYfYISQncf",
        "pdf_link": "https://openreview.net/pdf?id=VYfYISQncf",
        "keywords": "Image Super-resolution, Frequency-Domain, Diffusion Models",
        "abstract": "Image super-resolution (ISR) is a classic and challenging problem in low-level vision because the data collection process often introduces complex and unknown degradation patterns. Leveraging powerful generative priors, diffusion-based algorithms have recently established new state-of-the-art ISR performance. Despite the promise, current diffusion-based ISR methods mostly focus on the spatial domain, revealing a lack of understanding of the frequency domain. To bridge this gap, we first experimentally verify that the key to solving the ISR problem lies in addressing the degradation of image amplitude information and high-frequency details. Based on this insight, we propose a novel frequency-aware enhancement framework ($\\textbf{FedSR}$) for diffusion-based ISR methods, which consists of two critical components. Firstly, we design the Amplitude Enhancement Module (AEM), which selectively enhances crucial amplitude channels through weighted optimization. Secondly, we introduce the High-Frequency Enhancement Module (HEM) that adaptively masks the skip features to perform high-pass filtering. Through extensive evaluations on both synthetic datasets and real-world image collections, our method demonstrates outstanding performance in reproducing realistic image details without additional tuning. For instance, FedSR improves StableSR across three datasets by an average of $\\textbf{+10.53}\\%$ on MUSIQ metric."
    },
    {
        "title": "From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities",
        "link_suffix": "/forum?id=3TnLGGHhNx",
        "link": "https://openreview.net/forum?id=3TnLGGHhNx",
        "pdf_link": "https://openreview.net/pdf?id=3TnLGGHhNx",
        "keywords": "Multimodal Large Language Models, Image Tokenizer, Token Merge",
        "abstract": "Multimodal Large Language Models have made significant strides in integrating visual and textual information, yet they often struggle with effectively aligning these modalities. We introduce a novel image tokenizer that bridges this gap by applying the principle of Byte-Pair Encoding (BPE) to visual data. Unlike conventional approaches that rely on separate visual encoders, our method directly incorporates structural prior information into image tokens, mirroring the successful tokenization strategies used in text-only Large Language Models. This innovative approach enables Transformer models to more effectively learn and reason across modalities. Through theoretical analysis and extensive experiments, we demonstrate that our BPE Image Tokenizer significantly enhances MLLMs' multimodal understanding capabilities, even with limited training data. Our method not only improves performance across various benchmarks but also shows promising scalability, potentially paving the way for more efficient and capable multimodal foundation models."
    },
    {
        "title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera",
        "link_suffix": "/forum?id=0QePvFoqY6",
        "link": "https://openreview.net/forum?id=0QePvFoqY6",
        "pdf_link": "https://openreview.net/pdf?id=0QePvFoqY6",
        "keywords": "3D Gaussian, Event Camera",
        "abstract": "Implicit neural representation and explicit 3D Gaussian Splatting (3D-GS) for novel\nview synthesis have achieved remarkable progress with frame-based camera (e.g.\nRGB and RGB-D cameras) recently. Compared to frame-based camera, a novel\ntype of bio-inspired visual sensor, i.e. event camera, has demonstrated advantages\nin high temporal resolution, high dynamic range, low power consumption and\nlow latency. Due to its unique asynchronous and irregular data capturing process,\nlimited work has been proposed to apply neural representation or 3D Gaussian\nsplatting for an event camera. In this work, we present IncEventGS, an incremental\n3D Gaussian Splatting reconstruction algorithm with a single event camera. To\nrecover the 3D scene representation incrementally, we exploit the tracking and\nmapping paradigm of conventional SLAM pipelines for IncEventGS. Given the\nincoming event stream, the tracker firstly estimates an initial camera motion based\non prior reconstructed 3D-GS scene representation. The mapper then jointly refines\nboth the 3D scene representation and camera motion based on the previously\nestimated motion trajectory from the tracker. The experimental results demonstrate\nthat IncEventGS delivers superior performance compared to prior NeRF-based\nmethods and other related baselines, even we do not have the ground-truth camera poses.\nFurthermore, our method can also deliver better performance compared to state-of-\nthe-art event visual odometry methods in terms of camera motion estimation."
    },
    {
        "title": "Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport",
        "link_suffix": "/forum?id=u1cQYxRI1H",
        "link": "https://openreview.net/forum?id=u1cQYxRI1H",
        "pdf_link": "https://openreview.net/pdf?id=u1cQYxRI1H",
        "keywords": "diffusion model, illumination editing, image editing",
        "abstract": "Diffusion-based image generators are becoming unique methods for illumination harmonization and editing. The current bottleneck in scaling up the training of diffusion-based illumination editing models is mainly in the difficulty of preserving the underlying image details and maintaining intrinsic properties, such as albedos, unchanged. Without appropriate constraints, directly training the latest large image models with complex, varied, or in-the-wild data is likely to produce a structure-guided random image generator, rather than achieving the intended goal of precise illumination manipulation. We propose Imposing Consistent Light (IC-Light) transport during training, rooted in the physical principle that the linear blending of an object's appearances under different illumination conditions is consistent with its appearance under mixed illumination. This consistency allows for stable and scalable illumination learning, uniform handling of various data sources, and facilitates a physically grounded model behavior that modifies only the illumination of images while keeping other intrinsic properties unchanged. Based on this method, we can scale up the training of diffusion-based illumination editing models to large data quantities (> 10 million), across all available data types (real light stages, rendered samples, in-the-wild synthetic augmentations, etc), and using strong backbones (SDXL, Flux, etc). We also demonstrate that this approach reduces uncertainties and mitigates artifacts such as mismatched materials or altered albedos."
    },
    {
        "title": "Diffusion Models as Cartoonists! The Curious Case of High Density Regions",
        "link_suffix": "/forum?id=RiS2cxpENN",
        "link": "https://openreview.net/forum?id=RiS2cxpENN",
        "pdf_link": "https://openreview.net/pdf?id=RiS2cxpENN",
        "keywords": "diffusion models, likelihood estimation",
        "abstract": "We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-probability sampler that consistently generates images of higher likelihood than usual samplers. Our empirical findings reveal the existence of significantly higher likelihood samples that typical samplers do not produce, often manifesting as cartoon-like drawings or blurry images depending on the noise level. Curiously, these patterns emerge in datasets devoid of such examples. We also present a novel approach to track sample likelihoods in diffusion SDEs, which remarkably incurs no additional computational cost."
    },
    {
        "title": "A Consistent Pattern for Identifying Decisive Code Snippets for LLM-Based Code Inference",
        "link_suffix": "/forum?id=QgA0auXUU0",
        "link": "https://openreview.net/forum?id=QgA0auXUU0",
        "pdf_link": "https://openreview.net/pdf?id=QgA0auXUU0",
        "keywords": "Programming Languages; interpretability; Knowledge editing",
        "abstract": "Which parts of pre-target input are most influential for next-token prediction in the context of programming languages? In this paper, we present evidence that code snippets at specific locations in pre-target inputs play a decisive role in large language model (LLM) inference, and these snippets exhibit a consistent pattern. Firstly, we introduce a novel causal tracing method to identify tokens,  so-called high-information tokens, that significantly contribute to next-token prediction. Building on this, we propose a multi-phase causal tracing process to analyze the importance distribution of high-information tokens, revealing a consistent pattern, named the  Important Position Rule (IPR). To further validate this hypothesis, we assess the role of IPR across various LLMs, languages, and tasks. Our extensive evaluations for code translation, code correction and code completion tasks (Java, Python, C++) on models CodeLlama-7b/13b/34b-Instruct and GPT-3.5/4-turbo, confirm this hypothesis. Furthermore, we observe that IPR exhibits structural and semantic properties similar to the $\\langle \\text{subject},  \\text{relation}, \\text{object} \\rangle$ paradigm in natural language. Leveraging this insight, we successfully combine IPR with the knowledge editing method ROME in order to repair translation errors, achieving a correction rate of  62.73% to 75.31%. To our knowledge, this is the first application of knowledge editing  in the context of programming languages."
    },
    {
        "title": "Diffusion Attacker: Diffusion-Driven Prompt Manipulation for LLM Jailbreak",
        "link_suffix": "/forum?id=u08UxVNdIo",
        "link": "https://openreview.net/forum?id=u08UxVNdIo",
        "pdf_link": "https://openreview.net/pdf?id=u08UxVNdIo",
        "keywords": "LLM safety; LLM jailbreak; Diffusion Language Model; Gumbel Softmax;",
        "abstract": "Large Language Models can generate harmful content when prompted with carefully crafted inputs, a vulnerability known as LLM jailbreaking. As LLMs become more powerful, studying jailbreaking becomes a critical aspect of enhancing security and human value alignment. Currently, jailbreak is usually implemented by adding suffixes or using prompt templates, which suffers from low attack diversity. Inspired by diffusion models, this paper introduces the DiffusionAttacker, an end-to-end generative method for jailbreak rewriting. Our approach employs a seq2seq text diffusion model as a generator, conditioning on the original prompt and guiding the denoising process with a novel attack loss. This method preserves the semantic content of the original prompt while producing harmful content. Additionally, we leverage the Gumbel-Softmax technique to make the sampling process from the output distribution of the diffusion model differentiable, thereby eliminating the need for an iterative token search. Through extensive experiments on the Advbench and Harmbench, we show that DiffusionAttacker outperforms previous methods in various evaluation indicators including attack success rate (ASR), fluency, and diversity."
    },
    {
        "title": "Step-wise Triple-Consistent Diffusion Sampling for Inverse Problems",
        "link_suffix": "/forum?id=XGFfFKqy3Y",
        "link": "https://openreview.net/forum?id=XGFfFKqy3Y",
        "pdf_link": "https://openreview.net/pdf?id=XGFfFKqy3Y",
        "keywords": "Diffusion Model; Inverse Problems; Image Restoration",
        "abstract": "Diffusion models (DMs) are a class of generative models that allow sampling from a distribution learned over a training set. When applied to solving inverse imaging problems (IPs), the reverse sampling steps of DMs are typically modified to approximately sample from a measurement-conditioned distribution in the image space. However, these modifications may be unsuitable for certain settings (such as in the presence of measurement noise) and non-linear tasks, as they often struggle to correct errors from earlier sampling steps and generally require a large number of optimization and/or sampling steps. To address these challenges, we state three conditions for achieving measurement-consistent diffusion trajectories. Building on these conditions, we propose a new optimization-based sampling method that not only enforces the standard data manifold measurement consistency and forward diffusion consistency, as seen in previous studies, but also incorporates backward diffusion consistency that maintains a diffusion trajectory by optimizing over the input of the pre-trained model at every sampling step. By enforcing these conditions, either implicitly or explicitly, our sampler requires significantly fewer reverse steps. Therefore, we refer to our accelerated method asStep-wiseTriple-Consistent Sampling (SITCOM). Compared to existing state-of-the-art baseline methods, under different levels of measurement noise, our extensive experiments across five linear and three non-linear image restoration tasks demonstrate that SITCOM achieves competitive or superior results in terms of standard image similarity metrics while requiring a significantly reduced run-time across all considered tasks."
    },
    {
        "title": "End-to-End Rule Induction from Raw Sequence Inputs",
        "link_suffix": "/forum?id=zDjHOsSQxd",
        "link": "https://openreview.net/forum?id=zDjHOsSQxd",
        "pdf_link": "https://openreview.net/pdf?id=zDjHOsSQxd",
        "keywords": "Neuro-Symbolic Methods, Interpretability, Inductive Logic Programming",
        "abstract": "Rule learning-based models are widely used in highly interpretable scenarios for their transparent structures. Inductive logic programming (ILP) is a form of machine learning that induces rules from facts and keeps the interpretability. Differentiable ILP models enhance their learning ability in a robust and scalable manner with the advantages of neural networks. However, most differentiable ILP methods learn from symbolic datasets. Learning from raw data needs an ILP model to tackle the symbol grounding problem: The inability to map continuous inputs to symbolic variables without explicit supervision. In this work, we incorporate a self-supervised differentiable clustering model and a novel differentiable ILP model to learn from raw data in an end-to-end way without leaking the labels. The learned rules describe the raw data with its features. We demonstrate that our method learns generalized rules from time series and images intuitively and precisely."
    },
    {
        "title": "On the Modeling Capabilities of Large Language Models for Sequential Decision Making",
        "link_suffix": "/forum?id=vodsIF3o7N",
        "link": "https://openreview.net/forum?id=vodsIF3o7N",
        "pdf_link": "https://openreview.net/pdf?id=vodsIF3o7N",
        "keywords": "reinforcement learning, large language models, ai agents, preference based learning, reward design",
        "abstract": "Large pretrained models are showing increasingly better performance in reasoning and planning tasks across different modalities, opening the possibility to leverage them for complex sequential decision making problems. In this paper, we investigate the capabilities of Large Language Models (LLMs) for reinforcement learning (RL) across a diversity of interactive domains. We evaluate their ability to produce decision-making policies, either directly, by generating actions, or indirectly, by first generating reward models to train an agent with RL. Our results show that, even without task-specific fine-tuning, LLMs excel at reward modeling. In particular, crafting rewards through artificial intelligence (AI) feedback yields the most generally applicable approach and can enhance performance by improving credit assignment and exploration. Finally, in environments with unfamiliar dynamics, we explore how fine-tuning LLMs with synthetic data can significantly improve their reward modeling capabilities while mitigating catastrophic forgetting, further broadening their utility in sequential decision-making tasks."
    },
    {
        "title": "Enabling Scalable Evaluation of Bias Patterns in Medical LLMs",
        "link_suffix": "/forum?id=IXGHSVBBCF",
        "link": "https://openreview.net/forum?id=IXGHSVBBCF",
        "pdf_link": "https://openreview.net/pdf?id=IXGHSVBBCF",
        "keywords": "Medical LLMs, Fairness Evaluation",
        "abstract": "Large language models (LLMs) have shown impressive potential in helping with numerous medical challenges. Deploying LLMs in high-stakes applications such as medicine, however, brings in many concerns. One major area of concern relates to biased behaviors of LLMs in medical applications, leading to unfair treatment of individuals. To pave the way for the responsible and impactful deployment of Med LLMs, rigorous evaluation is a key prerequisite. Due to the huge complexity and variability of different medical scenarios, existing work in this domain has primarily relied on using manually crafted datasets for bias evaluation. In this study, we present a new method to scale up such bias evaluations by automatically generating test cases based on rigorous medical evidence. We specifically target the challenges of domain-specificity of bias characterization, hallucinating while generating the test cases, and various dependencies between the health outcomes and sensitive attributes. To that end, we offer new methods to address these challenges integrated with our generative pipeline. Specifically, we use medical knowledge graphs and medical ontologies; and customize general LLM evaluation frameworks in our method. Through a series of extensive experiments, we show that the test cases generated by our proposed method are reliable and can effectively reveal bias patterns in LLMs. Additionally,  we publish a large bias evaluation dataset, which provides a comprehensive platform for testing and improving the fairness of clinical LLMs. A live demo of our application for vignette generation is available athttps://vignette.streamlit.app. Our code is also available athttps://anonymous.4open.science/r/vignette_llm-2853."
    },
    {
        "title": "Improving classifier decision boundaries and interpretability using nearest neighbors",
        "link_suffix": "/forum?id=RomiC05ApM",
        "link": "https://openreview.net/forum?id=RomiC05ApM",
        "pdf_link": "https://openreview.net/pdf?id=RomiC05ApM",
        "keywords": "decision boundary, computer vision, CNN, kNN",
        "abstract": "Neural networks are not learning optimal decision boundaries. We show that decision boundaries are situated in areas of low training data density. They are impacted by few training samples which can easily lead to overfitting. We provide a simple algorithm performing a weighted average of the prediction of a sample and its nearest neighbors' (computed in latent space) leading to minor favorable outcomes for a variety of important measures for neural networks. In our evaluation, we employ various self-trained and (state-of-the-art) pre-trained convolutional neural networks to show that our approach improves (i) resistance to label noise, (ii) robustness against adversarial attacks, (iii) classification accuracy, and yields novel means for (iv) interpretability. Our interpretability analysis is of independent interest to the XAI community, as it is applicable to any network. While improvements are not necessarily large in all four areas, our approach is conceptually simple, i.e., improvements come without any modification to network architecture, training procedure or dataset. Furthermore, our approach is in stark contrast to prior works that often require trade-offs among the four objectives combined with architectural adaptations or provide valuable, but non-actionable insights. Finally, we provide a theoretical analysis."
    },
    {
        "title": "Exploring Image-Text Discrepancy for Universal Fake Image Detection",
        "link_suffix": "/forum?id=SfTy1ac4OX",
        "link": "https://openreview.net/forum?id=SfTy1ac4OX",
        "pdf_link": "https://openreview.net/pdf?id=SfTy1ac4OX",
        "keywords": "vision-language model, image-text discrepancy, fake image detection",
        "abstract": "With the rapid development of generative models, detecting generated images to prevent their malicious use has become a critical issue recently. Existing methods frame this challenge as a binary image classification task. However, such methods focus only on visual space, yielding trained detectors susceptible to overfitting specific image patterns and incapable of generalizing to unseen models. In this paper, we address this issue from a multi-modal perspective and find that fake images exhibit more distinct discrepancies with corresponding captions compared to real images. Upon this observation, we propose to leverage the \\textbf{I}mage-\\textbf{T}ext \\textbf{D}iscrepanc\\textbf{y}~(\\textbf{TIDY}) in joint visual-language space for \\textit{universal fake image detection}. Specifically, we first measure the distance of the images and corresponding captions in the latent spaces of CLIP, and then tune an MLP head to perform the usual detection task. Since there usually exists local artifacts in fake images, we further propose a global-to-local discrepancy scheme that first explores the discrepancy on the whole image and then each semantic object described in the caption, which can explore more fine-grained local semantic clues. Extensive experiments demonstrate the superiority of our method against other state-of-the-art competitors with impressive generalization and robustness on various recent generative models."
    }
]