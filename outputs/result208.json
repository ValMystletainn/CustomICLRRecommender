[
    {
        "title": "Exploring Representations and Interventions in Time Series Foundation Models",
        "link_suffix": "/forum?id=IRL9wUiwab",
        "link": "https://openreview.net/forum?id=IRL9wUiwab",
        "pdf_link": "https://openreview.net/pdf?id=IRL9wUiwab",
        "keywords": "Time Series Foundation Models, Model Steering, Interpretability, Pruning",
        "abstract": "Time series foundation models promise to be powerful tools for a wide range of applications. However, their internal representations and learned concepts are still not well understood. In this study, we investigate the structure and redundancy of representations across various TSFMs, examining the self-similarity of model layers within and across different model sizes. This analysis reveals block-like redundancy in the representations, which can be utilized for informed pruning to improve inference speed and efficiency. Additionally, we explore the concepts learned by these models\u2014such as periodicity and trends\u2014and how these can be manipulated through latent space steering to influence model behavior. Our experiments show that steering interventions can introduce new features, like adding periodicity or trends to signals that initially lacked them. These findings underscore the value of representational analysis for optimizing models and demonstrate how conceptual steering offers new possibilities for more controlled time series modeling."
    },
    {
        "title": "LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters",
        "link_suffix": "/forum?id=l80AgHoRaN",
        "link": "https://openreview.net/forum?id=l80AgHoRaN",
        "pdf_link": "https://openreview.net/pdf?id=l80AgHoRaN",
        "keywords": "LoRA, large language models, parameter-efficiency, NLP, PEFT",
        "abstract": "The rapid expansion of large language models (LLMs) has underscored the need for parameter-efficient fine-tuning methods, with LoRA (Low-Rank Adaptation) emerging as a popular solution. Although LoRA reduces the number of trainable parameters, serving multiple (task or user-specific) LoRA modules on top of a base model still creates significant storage challenges. To address this, using theoretical derivation, we introduce LoRA-XS (Low-Rank Adaptation with eXtremely Small number of parameters), a novel low-rank adaptation method that considerably reduces the trainable parameters while showing superior or competitive performance. LoRA-XS achieves this by inserting a small, trainable $r \\times r$ weight matrix between frozen low-rank matrices, which are constructed by Singular Value Decomposition (SVD) of the original weight matrix. This lightweight matrix enables fine-tuning with drastically reduced storage requirements, making it feasible to deploy millions of personalized models while minimizing memory overhead. For instance, LoRA-XS achieves a remarkable reduction of trainable parameters by over 100x in 7B models compared to LoRA. Our evaluations across various benchmarks (including GLUE, GSM8K, MATH, and eight commonsense reasoning datasets) demonstrate that LoRA-XS performs competitively or better than LoRA and other recent methods like VeRA while being significantly more parameter efficient. We also provide an extensive ablation study on the importance of singular vectors in transformer weights, shedding light on the underlying mechanisms driving LoRA-XS\u2019s enhanced efficiency. These findings suggest that LoRA-XS is not only a storage-efficient alternative, but also a powerful tool for scaling and personalizing LLMs at unprecedented scales."
    },
    {
        "title": "Learning in complex action spaces without policy gradients",
        "link_suffix": "/forum?id=MEF8SyXuXG",
        "link": "https://openreview.net/forum?id=MEF8SyXuXG",
        "pdf_link": "https://openreview.net/pdf?id=MEF8SyXuXG",
        "keywords": "action-value learning, policy gradient methods, complex action spaces",
        "abstract": "Conventional wisdom suggests that policy gradient methods are better suited to complex action spaces than action-value methods. However, foundational studies have shown equivalences between these paradigms in small and finite action spaces (O'Donoghue et al., 2017; Schulman et al., 2017a). This raises the question of why their computational applicability and performance diverge as the complexity of the action space increases. We hypothesize that the apparent superiority of policy gradients in such settings stems not from intrinsic qualities of the paradigm, but from universal principles that can also be applied to action-value methods to serve similar functionality. We identify three such principles and provide a framework for incorporating them into action-value methods. To support our hypothesis, we instantiate this framework in what we term QMLE, for Q-learning with maximum likelihood estimation. Our results show that QMLE can be applied to complex action spaces with a controllable computational cost that is comparable to that of policy gradient methods, all without using policy gradients. Furthermore, QMLE demonstrates strong performance on the DeepMind Control Suite, even when compared to the state-of-the-art methods such as DMPO and D4PG."
    },
    {
        "title": "On Speeding Up Language Model Evaluation",
        "link_suffix": "/forum?id=3cvwO5DBZn",
        "link": "https://openreview.net/forum?id=3cvwO5DBZn",
        "pdf_link": "https://openreview.net/pdf?id=3cvwO5DBZn",
        "keywords": "large language models, evaluation, matrix factorization",
        "abstract": "Developing prompt-based methods with Large Language Models (LLMs) requires making numerous decisions, which give rise to a combinatorial search problem over hyper-parameters. This exhaustive evaluation can be time-consuming and costly. In this paper, we propose an \\textit{adaptive} approach to explore this space. We are exploiting the fact that often only few samples are needed to identify clearly superior or inferior settings, and that many evaluation tests are highly correlated. We lean on multi-armed bandits to sequentially identify the next (method, validation sample)-pair to evaluate and utilize low-rank matrix factorization to fill in missing evaluations. We carefully assess the efficacy of our approach on several competitive benchmark problems and show that it can identify the top-performing method using only 5-15% of the typical resources---resulting in 85-95% LLM cost savings."
    },
    {
        "title": "Iterative Feature Space Optimization through Incremental Adaptive Evaluation",
        "link_suffix": "/forum?id=xtTut5lisc",
        "link": "https://openreview.net/forum?id=xtTut5lisc",
        "pdf_link": "https://openreview.net/pdf?id=xtTut5lisc",
        "keywords": "Automated Feature Optimization, Incremental Learning, Feature Space Evaluator",
        "abstract": "Iterative feature space optimization involves systematically evaluating and adjusting the feature space to improve downstream task performance. However, existing works suffer from three key limitations: 1) overlooking differences among data samples leads to evaluation bias; 2) tailoring feature spaces to specific machine learning models results in overfitting and poor generalization; 3) requiring\nthe evaluator to be retrained from scratch during each optimization iteration significantly reduces the overall efficiency of the optimization process. To bridge these gaps, we propose a gEneralized Adaptive feature Space Evaluator (EASE) to efficiently produce optimal and generalized feature spaces. This framework consists of two key components: Feature-Sample Subspace Generator and Contextual Attention Evaluator. The first component aims to decouple the information distribution within the feature space to mitigate evaluation bias. To achieve this, we first identify features most relevant to prediction tasks and samples most challenging for evaluation based on feedback from the subsequent evaluator. These identified feature and samples are then used to construct feature subspaces for next optimization iteration. This decoupling strategy makes the evaluator consistently target the most challenging aspects of the feature space. The second component intends to incrementally capture evolving patterns of the feature space for efficient evaluation. We propose a weighted-sharing multi-head attention mechanism to encode key characteristics of the feature space into an embedding vector for evaluation. Moreover, the evaluator is updated incrementally, retaining prior evaluation knowledge while incorporating new insights, as consecutive feature spaces during the optimization process share partial information. Extensive experiments on twelve real-world datasets demonstrate the effectiveness of the proposed framework. Our code and data are publicly available (https://anonymous.4open.science/r/EASE-1C51)."
    },
    {
        "title": "TURNIP: A \u201cNondeterministic\u201d GPU Runtime with CPU RAM Offload",
        "link_suffix": "/forum?id=vyzPMQ5weJ",
        "link": "https://openreview.net/forum?id=vyzPMQ5weJ",
        "pdf_link": "https://openreview.net/pdf?id=vyzPMQ5weJ",
        "keywords": "CPU Offload, Memory Management, Nondeterministic Execution, Machine Learning System",
        "abstract": "An obvious way to alleviate memory difficulties in GPU-based AI computing is via CPU offload, where data are moved between GPU and CPU RAM, so inexpensive CPU RAM is used to increase the amount of storage available. While CPU offload is an obvious idea, it can greatly slow down a computation, due to the relatively slow transfer rate between CPU RAM and GPU RAM. Thus, any system for CPU offload needs to ensure that when such a transfer needs to happen, no computation is blocked waiting for the transfer to finish. One of the key challenges when using CPU offload is that memory transfers introduce nondeterminacy into the system: it is not possible to know before runtime when the transfers will finish, and hence what is the best order of operations to run to ensure there is no blocking. In this paper, we describe TURNIP, which is a system for running AI computations using CPU offload. The key innovation in TURNIP is the compilation of the AI computation into a dependency graph that gives the TURNIP runtime freedom to run operations such as GPU kernel calls in many different orders; at runtime, TURNIP chooses the best order in response to real-time events."
    },
    {
        "title": "FIMP: Foundation Model-Informed Message Passing for Graph Neural Networks",
        "link_suffix": "/forum?id=esf4Lduba2",
        "link": "https://openreview.net/forum?id=esf4Lduba2",
        "pdf_link": "https://openreview.net/pdf?id=esf4Lduba2",
        "keywords": "Graph Neural Networks, Message-Passing, Foundation Models",
        "abstract": "Foundation models have achieved remarkable success across many domains, relying on pretraining over vast amounts of data. Graph-structured data often lacks the same scale as unstructured data, making the development of graph foundation models challenging. In this work, we propose Foundation-Informed Message Passing (FIMP), a Graph Neural Network (GNN) message-passing framework that repurposes existing pretrained non-textual foundation models for graph-based tasks. We show that the self-attention layers of foundation models can effectively be leveraged on graphs to perform cross-node attention-based message-passing. Our model is evaluated across diverse domains on image networks, single-cell RNA sequencing, and fMRI brain activity recordings in finetuned and zero-shot settings. FIMP outperforms strong baselines, demonstrating that it can effectively leverage state-of-the-art foundation models in graph tasks."
    },
    {
        "title": "Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning",
        "link_suffix": "/forum?id=nibeaHUEJx",
        "link": "https://openreview.net/forum?id=nibeaHUEJx",
        "pdf_link": "https://openreview.net/pdf?id=nibeaHUEJx",
        "keywords": "Time series analysis, invariance in neural networks",
        "abstract": "Deep learning models often lack shift invariance, making them sensitive to input shifts that cause changes in output. While recent techniques seek to address this for images, our findings show that these approaches fail to provide shift-invariance in time series, where the data generation mechanism is more challenging due to the interaction of low and high frequencies. Worse, they also decrease performance across several tasks. In this paper, we propose a differentiable bijective function that maps samples from their high-dimensional data manifold to another manifold of the same dimension, without any dimensional reduction. Our approach guarantees that samples---when subjected to random shifts---are mapped to a unique point in the data manifold while preserving all task-relevant information without loss. We theoretically and empirically demonstrate that the proposed transformation guarantees shift-invariance in deep learning models without imposing any limits to the shift. Our experiments on five-time series tasks with state-of-the-art methods show that our proposed approach consistently improves the performance while enabling models to achieve complete shift-invariance without modifying or imposing restrictions on the model's topology. Source code: Double-blind."
    },
    {
        "title": "The Superposition of Diffusion Models",
        "link_suffix": "/forum?id=2o58Mbqkd2",
        "link": "https://openreview.net/forum?id=2o58Mbqkd2",
        "pdf_link": "https://openreview.net/pdf?id=2o58Mbqkd2",
        "keywords": "generative modelling, protein generation, image generation, diffusion models",
        "abstract": "The undeniable success of deep generative models for learning complex and high-dimensional data distributions has led to the proliferation of large-scale diffusion models across the entire machine-learning application spectrum. This Cambrian explosion of easily accessible pre-trained models, including fine-tuned open-source models on user-specific data, suggests a demand for methods that combine multiple different pre-trained models without incurring the significant computational burden of re-training a larger combined model. In this paper, we cast the problem of combining multiple pre-trained diffusion models at the generation stage under a novel proposed framework termed superposition. Theoretically, we derive superposition from rigorous first principles stemming from the celebrated continuity equation and design two novel algorithms tailor-made for combining diffusion models in SuperDiff. We demonstrate that SuperDiff is scalable to large pre-trained diffusion models as superposition is performedsolely through composition during inference, and also enjoys painless implementation as it combines different pre-trained vector fields through an automated re-weighting scheme. Notably, we show that SuperDiffis efficient during inference time, and mimics traditional composition operators such as the logical $\\texttt{OR}$ and the logical $\\texttt{AND}$. We empirically demonstrate the utility of using SuperDiff for generating more diverse images on CIFAR-10, more faithful prompt conditioned image editing using Stable Diffusion, and improved unconditionalde novostructure design of proteins."
    },
    {
        "title": "Prompt Recovery for Image Generation Models: A Comparative Study of Discrete Optimizers",
        "link_suffix": "/forum?id=LS1VuhkReU",
        "link": "https://openreview.net/forum?id=LS1VuhkReU",
        "pdf_link": "https://openreview.net/pdf?id=LS1VuhkReU",
        "keywords": "Discrete Optimization, Prompt Inversion, Benchmarking",
        "abstract": "Recovering natural language prompts for image generation models, solely based on the generated images is a difficult discrete optimization problem. In this work, we present the first head-to-head comparison of recent discrete optimization techniques for the problem of prompt inversion. Following prior work on prompt inversion, we use CLIP's (Radford et al., 2021) text-image alignment as an inexpensive proxy for the distribution of prompt-image pairs, and compare several discrete optimizers against BLIP2's image captioner (Li et al., 2024) and PRISM (He et al., 2024) in order to evaluate the quality of discretely optimized prompts across various metrics related to the quality of inverted prompts and the images that they generate. We find that while the discrete optimizers effectively minimize their objectives, CLIP similarity between the inverted prompts and the ground truth image acts as a poor proxy for the distribution of prompt-image pairs -- responses from well-trained captioners often lead to generated images that more closely resemble those produced by the original prompts. This finding highlights the need for further investigation into inexpensive methods of modeling the relationship between the prompts for generative models and their output space."
    },
    {
        "title": "Human-Aligned Chess With a Bit of Search",
        "link_suffix": "/forum?id=bc2H72hGxB",
        "link": "https://openreview.net/forum?id=bc2H72hGxB",
        "pdf_link": "https://openreview.net/pdf?id=bc2H72hGxB",
        "keywords": "chess, alignment, adaptive MCTS, inference-time scaling",
        "abstract": "Chess has long been a testbed for AI's quest to match human intelligence, and in recent years, chess AI systems have surpassed the strongest humans at the game.\nHowever, these systems arenot human-aligned; they are unable to match the skill levels of all human partners or model human-like behaviors beyond piece movement.\nIn this paper, we introduce Allie, a chess-playing AI designed to bridge the gap between artificial and human intelligence in this classic game.\nAllie is trained on log sequences of real chess games to model the behaviors of human chess players across the skill spectrum, including non-move behaviors such as pondering times and resignations\nIn offline evaluations, we find that Allie exhibits humanlike behavior: it outperforms the existing state-of-the-art in human chess move prediction and ``ponders'' at critical positions.\nThe model learns to reliably assign reward at each game state, which can be used at inference as a reward function in a noveltime-adaptiveMonte-Carlo tree search (MCTS) procedure, where the amount of search depends on how long humans would think in the same positions.\nAdaptive search enables remarkableskill calibration; in a large-scale online evaluation against players with ratings from 1000 to 2600 Elo, our adaptive search method leads to a skill gap of only 49 Elo on average, substantially outperforming search-free and standard MCTS baselines.\nAgainst grandmaster-level (2500 Elo) opponents, Allie with adaptive search exhibits the strength of a fellow grandmaster, all while learningexclusively from humans."
    },
    {
        "title": "MVLight: Relightable Text-to-3D Generation via Light-conditioned Multi-View Diffusion",
        "link_suffix": "/forum?id=PoL2joPZQ4",
        "link": "https://openreview.net/forum?id=PoL2joPZQ4",
        "pdf_link": "https://openreview.net/pdf?id=PoL2joPZQ4",
        "keywords": "Text-to-3D generation, Multi-view diffusion, PBR material",
        "abstract": "Recent advancements in text-to-3D generation, building on the success of high-performance text-to-image generative models, have made it possible to create imaginative and richly textured 3D objects from textual descriptions. However, a key challenge remains in effectively decoupling light-independent and lighting-dependent components to  enhance the quality of generated 3D models and their relighting performance. In this paper, we present MVLight, a novel light-conditioned multi-view diffusion model that explicitly integrates lighting conditions directly into the generation process. This enables the model to synthesize high-quality images that faithfully reflect the specified lighting environment across multiple camera views. By leveraging this capability to Score Distillation Sampling (SDS), we can effectively synthesize 3D models with improved geometric precision and relighting capabilities. We validate the effectiveness of MVLight through extensive experiments and a user study."
    },
    {
        "title": "Scaling Laws for Predicting Downstream Performance in LLMs",
        "link_suffix": "/forum?id=BDisxnHzRL",
        "link": "https://openreview.net/forum?id=BDisxnHzRL",
        "pdf_link": "https://openreview.net/pdf?id=BDisxnHzRL",
        "keywords": "Scaling Laws, Downstream Performance Prediction, Large Language Models",
        "abstract": "Precise estimation of downstream performance in large language models (LLMs) prior to training is essential for guiding their development process. Scaling laws analysis utilizes the statistics of a series of significantly smaller sampling language models (LMs) to predict the performance of the target LLM. For downstream performance prediction, the critical challenge lies in the emergent abilities in LLMs that occur beyond task-specific computational thresholds. In this work, we focus on the pre-training loss as a more computation-efficient metric for performance estimation. Our two-stage approach consists of first estimating a function that maps computational resources (e.g.,FLOPs) to the pre-trainingLoss using a series of sampling models, followed by mapping the pre-training loss to downstream taskPerformance after the critical \"emergent phase\". In preliminary experiments, thisFLPsolution accurately predicts the performance of LLMs with 7B and 13B parameters using a series of sampling LMs up to 3B, achieving error margins of 5% and 10%, respectively, and significantly outperforming the FLOPs-to-Performance approach. This motivatesFLP-M, a fundamental approach for performance prediction that addresses the practical need to integrate datasets from multiple sources during pre-training, specifically blending general corpora with code data to accurately represent the common necessity. FLP-M extends the power law analytical function to predict domain-specific pre-training loss based on FLOPs across data sources, and employs a two-layer neural network to model the non-linear relationship between multiple domain-specific loss and downstream performance. By utilizing a 3B LLM trained on a specific ratio and a series of smaller sampling LMs, FLP-M can effectively forecast the performance of 3B and 7B LLMs across various data mixtures for most benchmarks within 10% error margins."
    },
    {
        "title": "Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces",
        "link_suffix": "/forum?id=bmbRCRiNDu",
        "link": "https://openreview.net/forum?id=bmbRCRiNDu",
        "pdf_link": "https://openreview.net/pdf?id=bmbRCRiNDu",
        "keywords": "planning, reasoning, Sequential Decision Making",
        "abstract": "In human cognition theory, human thinking is governed by two systems: the fast and intuitive System 1 and the slower but more deliberative System 2. Recent studies have shown that incorporating System 2 process into Transformers including large language models (LLMs), significantly enhances their reasoning capabilities.   Nevertheless, models that purely resemble System 2 thinking require substantially higher computational costs and are much slower to respond. To address this challenge, we present \\dualformer, a single Transformer model that seamlessly integrates both the fast and slow reasoning modes. \\dualformer is obtained by training on data with randomized reasoning traces, where different parts of the traces are dropped during training. The dropping strategies are specifically tailored according to the trace structure, analogous to analyzing our thinking process and creating shortcuts with patterns. At inference time, our model can be configured to output only the solutions (\\emph{fast mode}) or both the reasoning chain and the final solution (\\emph{slow mode}), or automatically decide which mode to engage (\\emph{auto mode}).  In all cases, \\dualformer outperforms the corresponding baseline models in both performance and computational efficiency: \\textbf{(1)} in slow mode, \\dualformer optimally solves unseen $30 \\times 30$ maze navigation tasks $97.6%$ of the time, surpassing the \\searchformer (trained on data with complete reasoning traces) baseline performance of  93.3%, while only using $45.5%$ fewer reasoning steps; \\textbf{(2)} in fast mode, \\dualformer completes those tasks with an $80%$ optimal rate,  significantly outperforming the Solution-Only model (trained on solution-only data),  which has an optimal rate of only 30%;  \\textbf{(3)} when operating in auto mode, \\dualformer achieves an optimal rate of 96.6% while utilizing $59.9%$ fewer reasoning steps compared to \\searchformer. For math problems, our techniques have also achieved improved performance with LLM fine-tuning, showing its generalization beyond task-specific models."
    },
    {
        "title": "ARVideo: Autoregressive Pretraining for Self-Supervised Video Representation Learning",
        "link_suffix": "/forum?id=hWlCc7Iksi",
        "link": "https://openreview.net/forum?id=hWlCc7Iksi",
        "pdf_link": "https://openreview.net/pdf?id=hWlCc7Iksi",
        "keywords": "Autoregressive Pretraining; Self-Supervised Video Representation Learning",
        "abstract": "This paper presents a new self-supervised video representation learning framework \\textbf{ARVideo}, which \\textit{autoregressively} predict the next video token in a tailored sequence order. Two key designs are included. First, we organize autoregressive video tokens into clusters that span both \\textit{spatially} and \\textit{temporally}, thereby enabling a richer aggregation of contextual information compared to the standard spatial-only or temporal-only clusters. Second, we adopt a randomized spatiotemporal prediction order to facilitate learning from multi-dimensional data, addressing the limitations of a handcrafted spatial-first or temporal-first sequence order. Extensive experiments establish ARVideo as an effective paradigm for self-supervised video representation learning. For example, when trained with the ViT-B backbone, ARVideo competitively attains 81.2% on Kinetics-400 and 70.9% on Something-Something V2, which are on par with the strong benchmark set by VideoMAE. Importantly, ARVideo also demonstrates higher training efficiency, \\ie, it trains 14% faster and requires 58% less GPU memory compared to VideoMAE."
    },
    {
        "title": "Autoregressive Pretraining with Mamba in Vision",
        "link_suffix": "/forum?id=PQpvhUrA1C",
        "link": "https://openreview.net/forum?id=PQpvhUrA1C",
        "pdf_link": "https://openreview.net/pdf?id=PQpvhUrA1C",
        "keywords": "Auto regressive Pretraining",
        "abstract": "The vision community has started to build with the recently developed state space model, Mamba, as the new backbone for a range of tasks. This paper shows that Mamba's visual capability can be significantly enhanced through autoregressive pretraining, a direction not previously explored. Efficiency-wise, the autoregressive nature can well capitalize on the Mamba's unidirectional recurrent structure, enabling faster overall training speed and reduced GPU memory usage compared to other training strategies. Performance-wise, autoregressive pretraining equips the Mamba architecture with markedly higher accuracy over its supervised-trained counterparts and, more importantly, successfully unlocks its scaling potential to large and even huge model sizes. For example, with autoregressive pretraining, a base-size Mamba outperforms its supervised counterpart by 2.0% on ImageNet classification; our best model, a huge-size Mamba, attains 85.0% top-1 ImageNet accuracy, significantly outperforming all existing Mamba variants in vision."
    },
    {
        "title": "Imit-Diff: Semantics Guided Diffusion Transformer with Dual Resolution Fusion for Imitation Learning",
        "link_suffix": "/forum?id=xtp6QPnwLu",
        "link": "https://openreview.net/forum?id=xtp6QPnwLu",
        "pdf_link": "https://openreview.net/pdf?id=xtp6QPnwLu",
        "keywords": "Imitation learning, Diffusion Policy, Dual Resolution, Semantics Injection",
        "abstract": "Diffusion-based methods have become one of the most important paradigms in the field of imitation learning. However, even in state-of-the-art diffusion-based policies, there has been insufficient focus on semantics and fine-grained feature extraction, resulting in weaker generalization and a reliance on controlled environments. To address this issue, we propose Imit-Diff, which consists of three key components: 1) Dual Resolution Fusion for extracting fine-grained features with a manageable number of tokens by integrating high-resolution features into low-resolution visual embedding through an attention mechanism; 2) Semantics Injection to explicitly incorporate semantic information by using prior masks obtained from open vocabulary models, achieving a world-level understanding of imitation learning tasks; and 3) Consistency Policy on Diffusion Transformer to reduce the inference time of diffusion models by training a student model to implement few-step denoising on the Probability Flow ODE trajectory. Experimental results show that our method significantly outperforms state-of-the-art methods, especially in cluttered scenes, and is highly robust to task interruptions. The code will be publicly available."
    },
    {
        "title": "Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling",
        "link_suffix": "/forum?id=p6ncr0eTKE",
        "link": "https://openreview.net/forum?id=p6ncr0eTKE",
        "pdf_link": "https://openreview.net/pdf?id=p6ncr0eTKE",
        "keywords": "task-adaptive pretraining, language models, importance sampling, domain adaptation",
        "abstract": "Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead. We adjust the training distribution of the generalist data with guidance from the limited domain-specific data. We explore several approaches, with clustered importance sampling standing out. This method clusters the generalist dataset and samples from these clusters based on their frequencies in the smaller specialist dataset. It is scalable, suitable for pretraining and continued pretraining, it works well in multi-task settings. Our findings demonstrate improvements across different domains in terms of language modeling perplexity and accuracy on multiple-choice question tasks. We also present ablation studies that examine the impact of dataset sizes, clustering configurations, and model sizes."
    },
    {
        "title": "Sparse Autoencoders Do Not Find Canonical Units of Analysis",
        "link_suffix": "/forum?id=9ca9eHNrdH",
        "link": "https://openreview.net/forum?id=9ca9eHNrdH",
        "pdf_link": "https://openreview.net/pdf?id=9ca9eHNrdH",
        "keywords": "sparse autoencoders, mechanistic interpretability",
        "abstract": "A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features, and it has been postulated that they can be used to find a \\textit{canonical} set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: novel latents, which improve performance when added to the smaller SAE, indicating they capture novel information, and \\emph{reconstruction latents}, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs - SAEs trained on the decoder matrix of another SAE - we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic.  The resulting decompositions are often interpretable; e.g. a latent representing \"Einstein'\" decomposes into \"scientist\", \"Germany\", and \"famous person\". To train meta-SAEs we introduce BatchTopK SAEs, an improved variant of the popular TopK SAE method, that only enforces a fixed average sparsity. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs:https://metasaes.streamlit.app/"
    },
    {
        "title": "Unpaired Single-Cell Dataset Alignment with Wavelet Optimal Transport",
        "link_suffix": "/forum?id=BYWVwmbqwK",
        "link": "https://openreview.net/forum?id=BYWVwmbqwK",
        "pdf_link": "https://openreview.net/pdf?id=BYWVwmbqwK",
        "keywords": "single cell, optimal transport, unpaired dataset alignment, spectral graph wavelets, gromov wasserstein",
        "abstract": "Aligning single-cell samples across different datasets and modalities is an important task with the rise of high-throughput single-cell technologies. Currently, collecting multi-modality datasets with paired samples is difficult, expensive, and impossible in some cases, motivating methods to align unpaired samples from distinct uni-modality datasets. While dataset alignment problems have been addressed in various domains, single-cell data introduce additional complexity including high levels of noise, dropout, and non-isometry between data spaces. In response to these unique challenges, we propose Wavelet Optimal Transport (WOT), a multi-resolution optimal transport method that aligns samples by minimizing the spectral graph wavelet discrepancies across datasets. Filters are incorporated into the optimization process to eliminate non-essential scales and wavelets, enhancing the quality of correspondences. We demonstrate the capacity of WOT in highly noisy and non-isometric conditions, outperforming previous state-of-the-art methods by significant margins, especially on real single-cell datasets."
    },
    {
        "title": "An Efficient Plugin Method for Metric Optimization of Black-Box Models",
        "link_suffix": "/forum?id=yEnJvc7ogD",
        "link": "https://openreview.net/forum?id=yEnJvc7ogD",
        "pdf_link": "https://openreview.net/pdf?id=yEnJvc7ogD",
        "keywords": "optimization, black box systems, domain adaptation, distribution shift, classification",
        "abstract": "Many machine learning algorithms and classifiers are available only via API queries as a ``black-box'' --- that is, the downstream user has no ability to change, re-train, or fine-tune the model on a particular target distribution.\nIndeed, a downstream user may not have any knowledge of the training distribution or performance metric used to construct and optimize the black-box model.\nWe propose a simple and efficient plugin method which takes as input arbitrary multiclass predictions and post-processes them in order to adapt them to a new target distribution, while simultaneously optimizing for a particular metric of the confusion matrix.\nImportantly, the plugin method is \\textit{post-hoc}, does not rely on feature information, and only requires a small number of probabilistic predictions along with their corresponding true label.\nWe empirically demonstrate that plugin has performance competitive with related methods on a variety of tabular and language tasks."
    },
    {
        "title": "Exploring the Causal Mechanisms: Towards Robust and Explainable Algorithm Selection",
        "link_suffix": "/forum?id=7Fh57rIpXT",
        "link": "https://openreview.net/forum?id=7Fh57rIpXT",
        "pdf_link": "https://openreview.net/pdf?id=7Fh57rIpXT",
        "keywords": "Algorithm Selection, Automated Machine Learning, Robustness, Explainability",
        "abstract": "Algorithm selection aims to identify the optimal performing algorithm before execution. Existing techniques typically focus on the observed correlations between algorithm performance and meta-features. However, little research has explored the underlying mechanisms of algorithm selection, specifically what characteristics an algorithm must possess to effectively tackle problems with certain feature values. This gap not only limits the explainability but also makes existing models vulnerable to data bias and distribution shift. This paper introduces causality to describe this mechanism, proposing a novel modeling paradigm that aligns more closely with the fundamental logic of algorithm selection. By leveraging causal relationships to characterize the algorithm feature distribution conditioned on problem features, our approach enhances robustness against marginal distribution changes and allows for finer-grained predictions through the reconstruction of optimal algorithm features, with the final decision relying on differences between reconstructed and rejected algorithm features. Furthermore, we demonstrate that, the learned causal graph and the proposed counterfactual calculations offer our approach with both model-level and instance-level explainability. Extensive experiments on the ASlib benchmark validate the advantages of the proposed model in terms of robustness and explainability. The code will make publicly available after the review process."
    },
    {
        "title": "Attention Speaks Volumes: Localizing and Mitigating Bias in Language Models",
        "link_suffix": "/forum?id=Cz8KnDYj1L",
        "link": "https://openreview.net/forum?id=Cz8KnDYj1L",
        "pdf_link": "https://openreview.net/pdf?id=Cz8KnDYj1L",
        "keywords": "Bias, Attention, LLMs",
        "abstract": "We explore the internal mechanisms of how bias emerges in large language models (LLMs) when provided with ambiguous comparative prompts: inputs that compare or enforce choosing between two or more entities without providing clear context for preference. Most approaches for bias mitigation focus on either post-hoc analysis or data augmentation. However, these are transient solutions, without addressing the root cause: the model itself. Numerous prior works show the influence of the attention module towards steering generations. We believe that analyzing attention is also crucial for understanding bias, as it provides insight into how the LLM distributes its focus across different entities and how this contributes to biased decisions. To this end, we first introduce a metric to quantify the LLM's preference for one entity over another. We then propose $ATLAS$ (Attention-based Targeted Layer Analysis and Scaling), a technique to localize bias to specific layers of the LLM by analyzing attention scores and then reduce bias by scaling attention in these biased layers. To evaluate our method, we conduct experiments across 3 datasets (BBQ, Crows-Pairs, and WinoGender) using $GPT$-$2$ $XL$ (1.5B), $GPT$-$J$ (6B), $LLaMA$-$2$ (7B) and $LLaMA$-$3$ (8B). Our experiments demonstrate that bias is concentrated in the later layers, typically around the last third. We also show how $ATLAS$ effectively mitigates bias through targeted interventions without compromising downstream performance and an average increase of only 0.82% in perplexity when the intervention is applied. We see an average improvement of 0.28 points in the bias score across all the datasets."
    },
    {
        "title": "Hessian-Aware Training for Enhancing Model Resilience for In-Memory Computing",
        "link_suffix": "/forum?id=hxpbOfBywA",
        "link": "https://openreview.net/forum?id=hxpbOfBywA",
        "pdf_link": "https://openreview.net/pdf?id=hxpbOfBywA",
        "keywords": "DNN Resilience, Parameter corruptions, Hessian-aware training",
        "abstract": "Deep neural networks are not resilient to bitwise errors in their parameters: even a single-bit error in their memory representation can lead to significant performance degradation. This susceptibility poses great challenges in deploying models on emerging computing platforms, such as in-memory computing devices, where frequent bitwise errors occur. Most prior work addresses this issue with hardware or system-level approaches, such as additional hardware components for checking a model\u2019s integrity at runtime. However, these methods have not been widely deployed since they necessitate substantial infrastructure-wide modifications. In this paper, we study a new approach to address this challenge: we present a novel training method aimed at enhancing a model\u2019s inherent resilience to parameter errors. We define a model-sensitivity metric to measure this resilience and propose a training algorithm with an objective of minimizing the sensitivity. Models trained with our method demonstrate increased resilience to bitwise errors in parameters, particularly with a 50% reduction in the number of bits in the model parameter space whose flipping leads to a 90\u2013100% accuracy drop. Our method also aids in extreme model compression, such as lower bit-width quantization or pruning \u223c70% of parameters, with reduced performance loss. Moreover, our method is compatible with existing strategies to mitigate this susceptibility."
    },
    {
        "title": "Emergent Symbol-Like Number Variables in Artificial Neural Networks",
        "link_suffix": "/forum?id=zxbQLztmwb",
        "link": "https://openreview.net/forum?id=zxbQLztmwb",
        "pdf_link": "https://openreview.net/pdf?id=zxbQLztmwb",
        "keywords": "mechanistic interpretability, numeric cognition, causal interventions, DAS",
        "abstract": "Symbolic programs, defined by discrete variables with explicit rules and relations, often have the benefit of interpretability, ease of communication, and generalization. This is contrasted against neural systems, consisting of distributed representations with rules and relations defined by learned parameters, which often have opaque inner mechanisms. There is an interest in finding unity between these two types of systems for cognitive and computer scientists alike.  There is no guarantee, however, that these two types of systems are reconcilable. To what degree do neural networks induce abstract, mutable, slot-like variables in order to achieve next-token prediction (NTP) goals? Can neural functions be thought of analogously to a computer program? In this work, we train neural systems using NTP on numeric cognitive tasks and then seek to understand them at the level of symbolic programs. We use a combination of causal interventions and visualization methods in pursuit of this goal. We find that models of sufficient dimensionality do  indeed develop strong analogs of symbolic algorithms purely from the NTP objective. We then ask how variations on the tasks and model architectures  affect the models' learned solutions to find that numeric symbols are not formed for every variant of the task, and transformers solve the problem in a different fashion than their recurrent counterparts. Lastly, we show that in all cases, some degree of gradience exists in the neural symbols, highlighting the difficulty of finding simple, interpretable symbolic stories of how neural networks perform their tasks.  Taken together, our results are consistent with the view that neural networks can approximate interpretable symbolic programs of number cognition, but the particular program they approximate and the extent to which they approximate it can vary widely, depending on the network architecture, training data, extent of training, and network size."
    }
]