[{"title": "Unlocking the Potential of Model Calibration in Federated Learning", "link_suffix": "/forum?id=Osr0KZJeTX", "link": "https://openreview.net/forum?id=Osr0KZJeTX", "pdf_link": "https://openreview.net/pdf?id=Osr0KZJeTX", "keywords": "Federated Learning, Model Calibration", "abstract": "Over the past several years, various federated learning (FL) methodologies have been developed to improve model accuracy, a primary performance metric in machine learning. However, to utilize FL in practical decision-making scenarios, beyond considering accuracy, the trained model must also have a reliable confidence in each of its predictions, an aspect that has been largely overlooked in existing FL research. Motivated by this gap, we propose Non-Uniform Calibration for Federated Learning (NUCFL), a generic framework that integrates FL with the concept of model calibration. The inherent data heterogeneity in FL environments makes model calibration particularly difficult, as it must ensure reliability across diverse data distributions and client conditions. Our NUCFL addresses this challenge by dynamically adjusting the model calibration objectives based on statistical relationships between each client's local model and the global model in FL.  In particular, NUCFL assesses the similarity between local and global model relationships, and controls the penalty term for the calibration loss during client-side local training.  By doing so, NUCFL effectively aligns calibration needs for the global model in  heterogeneous FL settings while not sacrificing accuracy. Extensive experiments show that NUCFL offers flexibility and effectiveness across various FL algorithms, enhancing accuracy  as well as model calibration.", "title_embedding_index": 11300, "title_abs_embedding_index": 11325}, {"title": "Contrastive Meta Learning for Dynamical Systems", "link_suffix": "/forum?id=S8nFZ98pmU", "link": "https://openreview.net/forum?id=S8nFZ98pmU", "pdf_link": "https://openreview.net/pdf?id=S8nFZ98pmU", "keywords": "dynamical system, meta learning, contrastive learning", "abstract": "Recent advancements in deep learning have significantly impacted the study of dynamical systems. Traditional approaches predominantly rely on supervised learning paradigms, limiting their scope to large scale problems and adaptability to new systems. This paper introduces a novel meta learning framework tailored for dynamical system forecasting, hinging on the concept of mapping the observed trajectories to a system-specific embedding space which encapsulates the inter-system characteristics and enriches the feature set for downstream prediction tasks. Central to our framework is the use of contrastive learning for trajectory data coupled with a series of neural network architecture designs to extract the features as augmented embedding for modeling system behavior. We present the application of zero-shot meta-learning to dynamical systems, demonstrating a substantial enhancement in performance metrics compared to existing baseline models. A notable byproduct of our methodology is the improved interpretability of the embeddings, which now carries explicit physical significance. Our results not only set a new benchmark in the field but also pave the way for enhanced interpretability and deeper understanding of complex dynamical systems, potentially opens new directions for how we approach system analysis and prediction.", "title_embedding_index": 11301, "title_abs_embedding_index": 11326}, {"title": "Meta ControlNet: Enhancing Task Adaptation via Meta Learning", "link_suffix": "/forum?id=SjRLAzrVRo", "link": "https://openreview.net/forum?id=SjRLAzrVRo", "pdf_link": "https://openreview.net/pdf?id=SjRLAzrVRo", "keywords": "Diffusion Models, Meta Learning", "abstract": "Diffusion-based image synthesis has attracted extensive attention recently. In particular, \nControlNet that uses image-based prompts exhibits powerful capability in image tasks such as canny edge detection and generates images well aligned with these prompts. However, vanilla ControlNet generally requires extensive training of around 5000 steps to achieve a desirable control for a single task. Recent context-learning approaches have improved its adaptability, but mainly for edge-based tasks, and rely on paired examples. Thus, two important open issues are yet to be addressed to reach the full potential of ControlNet: (i) zero-shot control for certain tasks and (ii) faster adaptation for non-edge-based tasks. In this paper, we introduce a novel Meta ControlNet method, \nwhich adopts the task-agnostic meta learning technique and features a new layer freezing design. Meta ControlNet significantly reduces learning steps to attain control ability from 5000 to 1000. Further, Meta ControlNet exhibits direct zero-shot adaptability in edge-based tasks without any finetuning, and achieves control within only 100 finetuning steps in more complex non-edge tasks such as Human Pose.", "title_embedding_index": 11302, "title_abs_embedding_index": 11327}, {"title": "AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements", "link_suffix": "/forum?id=ybfmpJiKXX", "link": "https://openreview.net/forum?id=ybfmpJiKXX", "pdf_link": "https://openreview.net/pdf?id=ybfmpJiKXX", "keywords": "natural language processing, modern slavery, corporate statements, benchmark, text extraction, large language models", "abstract": "Despite over a decade of legislative efforts to address modern slavery in the supply chains of large corporations, the effectiveness of government oversight remains hampered by the challenge of scrutinizing thousands of statements annually. While Large Language Models (LLMs) can be considered a well established solution for the automatic analysis and summarization of documents, recognizing concrete modern slavery countermeasures taken by companies and differentiating those from vague claims remains a challenging task. To help evaluate and fine-tune LLMs for the assessment of corporate statements, we introduce a dataset composed of 5,731 modern slavery statements taken from the Australian Modern Slavery Register and annotated at the sentence level. This paper details the construction steps for the dataset that include the careful design of annotation specifications, the selection and preprocessing of statements, and the creation of high-quality annotation subsets for effective model evaluations. To demonstrate our dataset's utility, we propose a machine learning methodology for the detection of sentences relevant to mandatory reporting requirements set by the Australian Modern Slavery Act. We then follow this methodology to benchmark modern language models under zero-shot and supervised learning settings.", "title_embedding_index": 11303, "title_abs_embedding_index": 11328}, {"title": "REAL-TIME LAYOUT ADAPTATION USING GENERATIVE AI", "link_suffix": "/forum?id=zqA19DirIT", "link": "https://openreview.net/forum?id=zqA19DirIT", "pdf_link": "https://openreview.net/pdf?id=zqA19DirIT", "keywords": "GenAI, SupervisedLearning, React, Web-Design, ChatGPT", "abstract": "In modern web design, ensuring adaptability and user engagement through dynamic layouts is increasingly important. With the growing demand for personalized user experiences, traditional static web layouts are insufficient for meeting user preferences. This paper introduces an innovative approach that leverages generative AI to dynamically adapt web layouts in real-time. With the help of data that is collected under the banner of user interactions through technologies such as JavaScript and Node.js, we are able to save those interactions, which not only include the click patterns but also the timestamps, user\u2019s name, day and date, and number of clicks.These clicks correspond to interactions of users with different React components. This data is being stored as a CSV file, as it is easier to read when it comes to parsing it to an AI model. Once every designated cycle, the data is fed to a Python script which does an API call to the $Chat GPT 4o$ model, which then analyzes the data and rewrites the CSS to create a new web layout based on the user\u2019s interactions.This successfully gives a web interface that adapts its layout in real-time, which is somewhat similar to many recommendation systems of popular applications like Netflix and Amazon Prime. Its significance extends across multiple fields, as this approach can enhance user engagement by dynamically displaying components based on user interaction patterns. Additionally, it offers potential revenue growth for companies, allowing them to charge higher rates for ads strategically placed in high-engagement areas of the layout, based on inferred user data.For example, let the number of clicks be represented as $N_c$ and the user interaction patterns as $P_u$. The revenue potential $R$ can be expressed as:\n$$\nR = k \\cdot N_c \\cdot P_u,\n$$\nwhere $k$ is a constant representing the ad placement value.", "title_embedding_index": 11304, "title_abs_embedding_index": 11329}, {"title": "Multi-Scale Fusion for Object Representation", "link_suffix": "/forum?id=nobDw4d1k7", "link": "https://openreview.net/forum?id=nobDw4d1k7", "pdf_link": "https://openreview.net/pdf?id=nobDw4d1k7", "keywords": "Object-Centric Learning (OCL), Variational Autoencoder (VAE), Multi-Scale, Unsupervised Object Segmentation", "abstract": "Representing images or videos as object-level feature vectors, rather than pixel-level feature maps, facilitates advanced visual tasks. Object-Centric Learning (OCL) primarily achieves this by reconstructing the input under the guidance of Variational Autoencoder (VAE) intermediate representation to drive so-called $\\textit{slots}$ to aggregate as much object information as possible. However, existing VAE guidance does not explicitly address that objects can vary in pixel sizes while models typically excel at specific pattern scales. We propose $\\textit{Multi-Scale Fusion}$ (MSF) to enhance VAE guidance for OCL training. To ensure objects of all sizes fall within VAE's comfort zone, we adopt the $\\textit{image pyramid}$, which produces intermediate representations at multiple scales; To foster scale-invariance/variance in object super-pixels, we devise $\\textit{inter}$/$\\textit{intra-scale fusion}$, which augments low-quality object super-pixels of one scale with corresponding high-quality super-pixels from another scale. On standard OCL benchmarks, our technique improves mainstream methods, including state-of-the-art diffusion-based ones. The source code is available in the supplemental material.", "title_embedding_index": 11305, "title_abs_embedding_index": 11330}, {"title": "ReFOCUS: Recurrent False Object Correction Using guidance Strategies in Object Detection", "link_suffix": "/forum?id=pjKdWj5NSR", "link": "https://openreview.net/forum?id=pjKdWj5NSR", "pdf_link": "https://openreview.net/pdf?id=pjKdWj5NSR", "keywords": "Object Detection, False Positive, Computer Vision, Recurrent Errors, Correction", "abstract": "This work addresses the issue of recurrent false positive classification in object detection. We consider two experimental setups imitating real-world scenarios that lead to such errors: i) erroneous annotations, ii) non-objects that resemble actual objects. We show that resulting models can be corrected efficiently using a two-step protocol that leverages false positive annotations. For the first step, we present and compare two correction approaches that guide false positives toward true negatives, in either the latent or the logit space. The second step then consists in standard continuous fine-tuning on correct annotations. The latent guidance framework relies on a decoder that maps the bounding box of a given false positive to its target true negative embedding. The decoder is trained as part of an autoencoder, where appropriate true negative samples are generated by a learnable Gaussian mixture model in the latent space. By leveraging the properties of the Wasserstein distance, the mixture model is optimized through standard backpropagation. In both experimental setups, the two correction methods significantly outperform standard continuous fine-tuning on correct annotations and demonstrate competitive performance when compared to models retrained from scratch on correct annotations. In particular, in the second experimental setup, the latent guidance framework consistently outperforms these models, effectively enhancing detection performance at the cost of supplementary false positive annotations. Additionally, the proposed techniques prove effective in a few-shot learning context.", "title_embedding_index": 11306, "title_abs_embedding_index": 11331}, {"title": "Personalized Prompt Tuning for Unsupervised Federated Learning", "link_suffix": "/forum?id=VirIZC5v7E", "link": "https://openreview.net/forum?id=VirIZC5v7E", "pdf_link": "https://openreview.net/pdf?id=VirIZC5v7E", "keywords": "federated learning, CLIP, unsupervised learning", "abstract": "Federated learning facilitates collaborative model training across multiple distributed clients without requiring data sharing.\nHowever, conventional federated methods struggle with classification tasks in an unsupervised paradigm due to the absence of category knowledge.\nRecently, CLIP, a prominent visual language model, has demonstrated impressive results, particularly its remarkable zero-shot classification ability, which alleviates the dependence on labeled data.\nIn this paper, we first explore a new realistic problem, unsupervised federated learning using CLIP, where clients with unlabeled heterogeneous data collaborate to enhance global performance.\nTo address this problem, we propose FedPP, a method that incorporates a cooperative pseudo-label selection strategy and a partial prompt aggregation protocol.\nOur selection strategy ensures that all classes are trained in a balanced manner through global pseudo-label allocation.\nConcurrently, the aggregation protocol divides parameters into aggregated and retained components to optimize global performance while supporting local personalization.\nExtensive experiments across six datasets with various types of heterogeneity demonstrate the effectiveness of FedPP.\nOur code is available in the supplementary materials.", "title_embedding_index": 11307, "title_abs_embedding_index": 11332}, {"title": "Factor Graph-based Interpretable Neural Networks", "link_suffix": "/forum?id=10DtLPsdro", "link": "https://openreview.net/forum?id=10DtLPsdro", "pdf_link": "https://openreview.net/pdf?id=10DtLPsdro", "keywords": "interpretable neural network, factor graph, perturbation, explanation rectification, graph learning", "abstract": "Comprehensible neural network explanations are foundations for a better understanding of decisions, especially when the input data are infused with malicious perturbations. Existing solutions generally mitigate the impact of perturbations through adversarial training, yet they fail to generate comprehensible explanations under unknown perturbations. To address this challenge, we propose AGAIN, a fActor GrAph-based Interpretable neural Network, which is capable of generating comprehensible explanations under unknown perturbations. Instead of retraining like previous solutions, the proposed AGAIN directly integrates logical rules by which logical errors in explanations are identified and rectified during inference. Specifically, we construct the factor graph to express logical rules between explanations and categories. By treating logical rules as exogenous knowledge, AGAIN can identify incomprehensible explanations that violate real-world logic. Furthermore, we propose an interactive intervention switch strategy rectifying explanations based on the logical guidance from the factor graph without learning perturbations, which overcomes the inherent limitation of adversarial training-based methods in defending only against known perturbations. Additionally, we theoretically demonstrate the effectiveness of employing factor graph by proving that the comprehensibility of explanations is strongly correlated with factor graph. Extensive experiments are conducted on three datasets and experimental results illustrate the superior performance of AGAIN compared to state-of-the-art baselines.", "title_embedding_index": 11308, "title_abs_embedding_index": 11333}, {"title": "Task-oriented Sequential Grounding in 3D Scenes", "link_suffix": "/forum?id=LvgOm9Rmih", "link": "https://openreview.net/forum?id=LvgOm9Rmih", "pdf_link": "https://openreview.net/pdf?id=LvgOm9Rmih", "keywords": "3D visual grounding, 3D scene understanding, vision and language, grounded task planning", "abstract": "Grounding natural language in physical 3D environments is essential for the advancement of embodied artificial intelligence. Current datasets and models for 3D visual grounding predominantly focus on identifying and localizing objects from static, object-centric descriptions. These approaches do not adequately address the dynamic and sequential nature of task-oriented grounding necessary for practical applications. In this work, we propose a new task: Task-oriented Sequential Grounding in 3D scenes, wherein an agent must follow detailed step-by-step instructions to complete daily activities by locating a sequence of target objects in indoor scenes. To facilitate this task, we introduce SG3D, a large-scale dataset containing 22,346 tasks with 112,236 steps across 4,895 real-world 3D scenes. The dataset is constructed using a combination of RGB-D scans from various 3D scene datasets and an automated task generation pipeline, followed by human verification for quality assurance. We adapted three state-of-the-art 3D visual grounding models to the sequential grounding task and evaluated their performance on SG3D. Our results reveal that while these models perform well on traditional benchmarks, they face significant challenges with task-oriented sequential grounding, underscoring the need for further research in this area.", "title_embedding_index": 11309, "title_abs_embedding_index": 11334}, {"title": "LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models", "link_suffix": "/forum?id=gZnBI7WS1K", "link": "https://openreview.net/forum?id=gZnBI7WS1K", "pdf_link": "https://openreview.net/pdf?id=gZnBI7WS1K", "keywords": "Efficient AI, Large Multimodal Model", "abstract": "Large Multimodal Models (LMMs) have shown significant visual reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically take in a fixed and large amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which further increases the number of visual tokens significantly. However, due to the inherent design of the Transformer architecture, the computational costs of these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism that identifies significant spatial redundancy among visual tokens. In response, we propose PruMerge, a novel adaptive visual token reduction strategy that significantly reduces the number of visual tokens without compromising the performance of LMMs. Specifically, to metric the importance of each token, we exploit the sparsity observed in the visual encoder, characterized by the sparse distribution of attention scores between the class token and visual tokens. This sparsity enables us to dynamically select the most crucial visual tokens to retain. Subsequently, we cluster the selected (unpruned) tokens based on their key similarity and merge them with the unpruned tokens, effectively supplementing and enhancing their informational content.\nEmpirically, when applied to LLaVA-1.5 and Video-LLaVA, our approach can reduce the number of visual tokens by 4 times, and achieve comparable or better performance across diverse visual question-answering and reasoning tasks.", "title_embedding_index": 11310, "title_abs_embedding_index": 11335}, {"title": "A Graph Enhanced Symbolic Discovery Framework For Efficient Circuit Synthesis", "link_suffix": "/forum?id=EG9nDN3eGB", "link": "https://openreview.net/forum?id=EG9nDN3eGB", "pdf_link": "https://openreview.net/pdf?id=EG9nDN3eGB", "keywords": "Chip Design, Circuit Synthesis, Symbolic Regression, Knowledge Distillation", "abstract": "The efficiency of Circuit Synthesis (CS) has become one of the key bottlenecks in chip design. To prompt efficient CS, previous studies propose using a key scoring function to predict and prune a large number of ineffective nodes of the CS heuristics. \nHowever, the existing scoring functions struggle to balance inference efficiency, interpretability, and\ngeneralization performance, which severely hinders their application to modern CS tools. To address this challenge, we propose a novel data-driven circuit symbolic learning framework, namely CMO, to learn lightweight, interpretable, and generalizable scoring functions.\nThe major challenge of developing CMO is to discover symbolic functions that can well generalize to unseen circuits, i.e., the circuit symbolic generalization problem. Thus, the major technical contribution of CMO is the novel $\\textit{Graph Enhanced Symbolic Discovery} $ framework, which distills dark knowledge from a well-designed graph neural network (GNN) to enhance the generalization capability of the learned symbolic functions. To the best of our knowledge, CMO is $\\textit{the first}$ graph-enhanced approach for discovering lightweight and interpretable symbolic functions that can well generalize to unseen circuits in CS. Experiments on three challenging circuit benchmarks show that the $\\textit{interpretable}$ symbolic functions learned by CMO outperform previous state-of-the-art (SOTA) GPU-based and human-designed approaches in terms of $\\textit{inference efficiency}$ and $\\textit{generalization capability}$. Moreover, we integrate CMO with the Mfs2 heuristic---one of the most time-consuming CS heuristics. \nThe empirical results demonstrate that CMO significantly improves its efficiency while keeping comparable optimization performance when executed on a CPU-based machine, achieving up to 2.5$\\times$ faster runtime.", "title_embedding_index": 11311, "title_abs_embedding_index": 11336}, {"title": "Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation", "link_suffix": "/forum?id=oFBu7qaZpS", "link": "https://openreview.net/forum?id=oFBu7qaZpS", "pdf_link": "https://openreview.net/pdf?id=oFBu7qaZpS", "keywords": "Retrieval Augmented Generation, Knowledge Driven Reasoning", "abstract": "Retrieval-augmented generation (RAG) has improved large language models (LLMs) by using knowledge retrieval to overcome knowledge deficiencies. However, current RAG methods often fall short of ensuring the depth and completeness of retrieved information, which is necessary for complex reasoning tasks. In this work, we introduce Think-on-Graph 2.0 (ToG-2), a hybrid RAG framework that iteratively retrieves information from both unstructured and structured knowledge sources in a tight-coupling manner. Specifically, ToG-2 leverages knowledge graphs (KGs) to link documents via entities, facilitating deep and knowledge-guided context retrieval. Simultaneously, it utilizes documents as entity contexts to achieve precise and efficient graph retrieval. \nToG-2 alternates between graph retrieval and context retrieval to search for in-depth clues relevant to the question, enabling LLMs to generate answers.\nWe conduct a series of well-designed experiments to highlight the following advantages of ToG-2: 1) ToG-2 tightly couples the processes of context retrieval and graph retrieval, deepening context retrieval via the KG while enabling reliable graph retrieval based on contexts; 2) it achieves deep and faithful reasoning in LLMs through an iterative knowledge retrieval process of collaboration between contexts and the KG;  and 3) ToG-2 is training-free and plug-and-play compatible with various LLMs. Extensive experiments demonstrate that ToG-2 achieves overall state-of-the-art (SOTA) performance on 6 out of 7 knowledge-intensive datasets with GPT-3.5, and can elevate the performance of smaller models (e.g., LLAMA-2-13B) to the level of GPT-3.5\u2019s direct reasoning. The source code is available onhttps://anonymous.4open.science/r/ToG2.", "title_embedding_index": 11312, "title_abs_embedding_index": 11337}, {"title": "Can Large Language Models Reason? A Characterization via 3-SAT", "link_suffix": "/forum?id=FP77VtEuaT", "link": "https://openreview.net/forum?id=FP77VtEuaT", "pdf_link": "https://openreview.net/pdf?id=FP77VtEuaT", "keywords": "Large Language Models, Logic, Reasoning, Satisfiability, Phase Transitions", "abstract": "Large Language Models (LLMs) have been touted as AI models possessing advanced reasoning abilities. However, recent works have shown that LLMs often bypass true reasoning using shortcuts, sparking skepticism. To study the reasoning capabilities in a principled fashion, we adopt a computational theory perspective and propose an experimental protocol centered on 3-SAT -- the prototypical NP-complete problem lying at the core of logical reasoning and constraint satisfaction tasks. Specifically, we examine the phase transitions in random 3-SAT and characterize the reasoning abilities of LLMs by varying the inherent hardness of the problem instances. Our experimental evidence shows that LLMs are incapable of performing true reasoning, as required for solving 3-SAT problems. Moreover, we observe significant performance variation based on the inherent hardness of the problems -- performing poorly on harder instances and vice versa. Importantly, we show that integrating external reasoners can considerably enhance LLM performance. By following a principled experimental protocol, our study draws concrete conclusions and moves beyond the anecdotal evidence often found in LLM reasoning research.", "title_embedding_index": 11313, "title_abs_embedding_index": 11338}, {"title": "Generation and Comprehension Hand-in-Hand: Vision-guided Expression Diffusion for Boosting Referring Expression Generation and Comprehension", "link_suffix": "/forum?id=1qbZekXGrp", "link": "https://openreview.net/forum?id=1qbZekXGrp", "pdf_link": "https://openreview.net/pdf?id=1qbZekXGrp", "keywords": "Referring expression generation, referring expression comprehension, vision-guided expression diffusion, vision-text condition", "abstract": "Referring expression generation (REG) and comprehension (REC) are vital and complementary in joint visual and textual reasoning.  Existing REC datasets typically contain insufficient image-expression pairs for training, hindering the generalization of REC models to unseen referring expressions. Moreover, REG methods frequently struggle to bridge the visual and textual domains due to the limited capacity, leading to low-quality and restricted diversity in expression generation. To address these issues, we propose a novel VIsion-guided Expression Diffusion Model (VIE-DM) for the REG task, where diverse synonymous expressions adhering to both image and text contexts of the target object are generated to augment REC datasets. VIE-DM consists of a vision-text condition (VTC) module and a transformer decoder. Our VTC and token selection design effectively addresses the feature discrepancy problem prevalent in existing REG methods. This enables us to generate high-quality, diverse synonymous expressions that can serve as augmented data for REC model learning. Extensive experiments on five datasets demonstrate the high quality and large diversity of our generated expressions. Furthermore, the augmented image-expression pairs consistently enhance the performance of existing REC models, achieving state-of-the-art results.", "title_embedding_index": 11314, "title_abs_embedding_index": 11339}, {"title": "LaDEEP: A Deep Learning-based Surrogate Model for Large Deformations of Elastic-Plastic Solids", "link_suffix": "/forum?id=KMYr8qwhbS", "link": "https://openreview.net/forum?id=KMYr8qwhbS", "pdf_link": "https://openreview.net/pdf?id=KMYr8qwhbS", "keywords": "Continuum Mechanics, Large Deformation, Elastic-Plastic Solid, Transformer, Deep Learning-based Surrogate Model", "abstract": "The scientific computing for large deformations of elastic-plastic solids is critical for numerous real-world applications. Classical numerical solvers rely primarily on local discrete linear approximations, which are constrained by an inherent trade-off between accuracy and efficiency. Recently, Deep Learning models have achieved impressive progress in solving PDEs. While previous models have explored various architectures and constructed coefficient-solution mappings, they are designed for general instances without considering specific problem properties and hard to accurately handle with complex elastic-plastic solids involving contact, loading and unloading. In this work, we take stretch bending, a popular metal fabrication technique, as our case study and introduce LaDEEP, a deep learning-based surrogate model for \\textbf{La}rge \\textbf{De}formations of \\textbf{E}lastic-\\textbf{P}lastic Solids. We encode the partitioned regions of the involved solids into a token sequence to maintain their essential order property. To characterize the physical process of the solid deformation, a two-stage Transformer-based module is designed to predict the deformation with the sequence of tokens as input. Empirically, LaDEEP achieves five magnitudes faster speed than finite element methods with a comparable accuracy, and gains 20.47% relative improvement on average compared to other deep learning baselines. We have also deployed our model into a real-world industrial production system, and it has shown remarkable performance in both accuracy and efficiency.", "title_embedding_index": 11315, "title_abs_embedding_index": 11340}, {"title": "EEG-Language Pretraining for Highly Label-Efficient Pathology Detection", "link_suffix": "/forum?id=OfCtAsnxvT", "link": "https://openreview.net/forum?id=OfCtAsnxvT", "pdf_link": "https://openreview.net/pdf?id=OfCtAsnxvT", "keywords": "deep learning, multimodal, neuroscience, eeg, medical", "abstract": "Multimodal language modeling constitutes a recent breakthrough which leverages advances in large language models to pretrain capable multimodal models. The integration of natural language during pretraining has been shown to significantly improve learned representations, particularly in computer vision. However, the efficacy of multimodal language modeling in the realm of functional brain data, specifically for advancing pathology detection, remains unexplored. This study pioneers EEG-language models (ELMs) trained on clinical reports and 15000 EEGs. We propose to combine multimodal alignment in this novel domain with timeseries cropping and text segmentation. This also enables an extension based on multiple instance learning to alleviate misalignment between irrelevant EEG or text segments. Our results indicate that models learn richer representations from being exposed to a variety of report segments, including the patient's clinical history, description of the EEG, and the physician's interpretation. Compared to models exposed to narrower clinical text information, we find such models to retrieve EEGs based on clinical reports (and vice versa) with substantially higher accuracy. Particularly in regimes with few annotations, we observe that ELMs can significantly improve pathology detection compared to EEG-only models, as demonstrated by both zero-shot classification and linear probes. The integration of multiple instance learning further improves performance across tasks. In sum, these results highlight the potential of integrating brain activity data with clinical text, suggesting that ELMs represent significant progress for clinical applications.", "title_embedding_index": 11316, "title_abs_embedding_index": 11341}, {"title": "Optimality of Matrix Mechanism on\u2113pp-metric", "link_suffix": "/forum?id=fbqOEOqurU", "link": "https://openreview.net/forum?id=fbqOEOqurU", "pdf_link": "https://openreview.net/pdf?id=fbqOEOqurU", "keywords": "differential privacy", "abstract": "In this paper, we introduce the $\\ell_p^p$-error metric (for $p \\geq 2$) when answering linear queries under the constraint of differential privacy. We characterize such an error under $(\\epsilon,\\delta)$-differential privacy in the natural add/remove model. Before this paper, tight characterization in the hardness of privately answering linear queries was known under $\\ell_2^2$-error metric (Edmonds et al. 2020) and $\\ell_p^2$-error metric for unbiased mechanisms in the substitution model (Nikolov et al. 2024). As a direct consequence of our results, we give tight bounds on answering prefix sum and parity queries under differential privacy for all constant $p$ in terms of the $\\ell_p^p$ error, generalizing the bounds in Hhenzinger et al. for $p=2$.", "title_embedding_index": 11317, "title_abs_embedding_index": 11342}, {"title": "MMA: Benchmarking Multi-Modal Large Language Model in Ambiguity Contexts", "link_suffix": "/forum?id=ywKlmMor0f", "link": "https://openreview.net/forum?id=ywKlmMor0f", "pdf_link": "https://openreview.net/pdf?id=ywKlmMor0f", "keywords": "Multi-Modal Large Language Model, Ambiguity, Benchmark", "abstract": "Multi-Modal Large Language Models (MLLMs) recently demonstrated strong capabilities in both instruction comprehension and responding, positioning them as promising tools for human-computer interaction.  However, the inherent ambiguity of language poses a challenge, potentially leading models astray in task implementation due to differing interpretations of the same text within varying contexts. In multi-modal settings, visual information serves as a natural aid in disambiguating such scenarios. In this paper, we introduce the first benchmark specifically designed to evaluate the performance of \\textbf{M}LL\\textbf{M}s in \\textbf{A}mbiguous contexts (MMA). This benchmark employs a multiple-choice visual question-answering format and includes 261 textual contexts and \nquestions with ambiguous meaning. Each question is linked to a pair of images that suggest divergent scenarios, thus leading to different answers given the same question. These questions are stratified into three categories of ambiguity: lexical, syntactic, and semantic, to facilitate a detailed examination of MLLM performance across varying levels of ambiguity. By evaluating 24 proprietary and open-sourced MLLMs, we find that: (1) MLLMs often overlook scenario-specific information provided by images to clarify the ambiguity of texts. When presented with two different contextual images and asked the same question, \n    MLLMs achieved an accuracy rate of only 53.22% in answering both correctly, \n    compared to human performance at 88.97%.(2) Among the three types of ambiguity, models perform best under lexical ambiguity and worst under syntactic ambiguity. (3) Open-sourced models generally perform significantly lower than proprietary MLLMs, with an average performance gap of 12.59%, Claude 3.5 Sonnet, emerges as the top model, achieving 74.32% accuracy. These findings firstly underscore the current limitations of MLLMs in integrating visual information to clarify textual ambiguities and highlight critical areas for future improvements. The codes and benchmark data are \\href{https://github.com/AnonymousSubmitter-gpu/MMA_Anony}{available}.", "title_embedding_index": 11318, "title_abs_embedding_index": 11343}, {"title": "Memory-Pruning Algorithm for Bayesian Optimization with Strict Computational Cost Guarantees", "link_suffix": "/forum?id=diKykN0Yaa", "link": "https://openreview.net/forum?id=diKykN0Yaa", "pdf_link": "https://openreview.net/pdf?id=diKykN0Yaa", "keywords": "Bayesian Optimization, Gaussian Processes, Embedded Systems, Random Eviction, Computational Constraints", "abstract": "Bayesian Optimization (BO) is a powerful tool for optimizing noisy and expensive-to-evaluate black-box functions, widely used in fields such as machine learning and various branches of engineering. However, BO faces significant challenges when applied to large datasets or when it requires numerous optimization iterations. The computational and memory demands of updating Gaussian Process (GP) models can result in unmanageable computation times. To address these limitations, we propose a new Bayesian Optimization algorithm with memory pruning (MP-BO), which restricts the maximum training data size by acquiring new queries while concurrently removing data points from the training set. This approach guarantees a maximum algorithmic complexity of $\\bigO(m^3)$, where $m \\ll n$ is a fixed value and $n$ represent the size of the full training set. The pruning strategy ensures reduced and constant memory usage and computation time, without significantly degrading performance. We evaluate MP-BO on synthetic benchmarks and a real neurostimulation dataset, demonstrating its robustness and efficiency in scenarios where traditional BO would fail under strict computational constraints. Our results suggest that MP-BO is a promising solution for applications that require efficient optimization with limited computing resources.", "title_embedding_index": 11319, "title_abs_embedding_index": 11344}, {"title": "Combining Text-based and Drag-based Editing for Precise and Flexible Image Editing", "link_suffix": "/forum?id=2HjRezQ1nj", "link": "https://openreview.net/forum?id=2HjRezQ1nj", "pdf_link": "https://openreview.net/pdf?id=2HjRezQ1nj", "keywords": "Computer Vision, Generative Model, Diffusion Model, Image Editing.", "abstract": "Precise and flexible image editing remains a fundamental challenge in computer vision. Based on the modified areas, most editing methods can be divided into two main types: global editing and local editing. In this paper, we choose the two most common editing approaches (\\ie text-based editing and drag-based editing) and analyze their drawbacks. Specifically, text-based methods often fail to describe the desired modifications precisely, while drag-based methods suffer from ambiguity. To address these issues, we proposed \\textbf{CLIPDrag}, a novel image editing method that is the first to combine text and drag signals for precise and ambiguity-free manipulations on diffusion models. To fully leverage these two signals, we treat text signals as global guidance and drag points as local information. Then we introduce a novel global-local motion supervision method to integrate text signals into existing drag-based methods by adapting a pre-trained language-vision model like CLIP. Furthermore, we also address the problem of slow convergence in CLIPDrag by presenting a fast point-tracking method that enforces drag points moving toward correct directions. Extensive experiments demonstrate that CLIPDrag outperforms existing single drag-based methods or text-based methods.", "title_embedding_index": 11320, "title_abs_embedding_index": 11345}, {"title": "G-Transformer for Conditional Average Potential Outcome Estimation over Time", "link_suffix": "/forum?id=XUJcsLvpaQ", "link": "https://openreview.net/forum?id=XUJcsLvpaQ", "pdf_link": "https://openreview.net/pdf?id=XUJcsLvpaQ", "keywords": "causal inference, potential outcomes, treatment effects, G-computation, time-varying confounding, medicine", "abstract": "Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. Yet, existing neural methods for this task either (1) do not perform proper adjustments for time-varying confounders, or (2) suffer from large estimation variance. In order to address both limitations, we introduce the G-transformer (GT). Our GT is a novel, neural end-to-end model which adjusts for time-varying confounders, and provides low-variance estimation of conditional average potential outcomes (CAPOs) over time. Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our GT across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.", "title_embedding_index": 11321, "title_abs_embedding_index": 11346}, {"title": "Feature Discrimination Analysis for Binary and Ternary Quantization", "link_suffix": "/forum?id=34syfledje", "link": "https://openreview.net/forum?id=34syfledje", "pdf_link": "https://openreview.net/pdf?id=34syfledje", "keywords": "binary quantization, ternary quantization, feature quantization, discriminant analysis, sparse representation", "abstract": "Quantization serves as a fundamental operation in machine learning, widely used for algorithm-hardware deployment and the simplification of data representation. Given that classification stands as a pivotal role in machine learning, it is crucial to investigate the impact of quantization on  classification. Generally, the investigation revolves around quantization errors, under the assumption   that  higher quantization errors typically lead to poorer classification performance. However, this assumption lacks a solid  theoretical foundation, and often contradicts empirical findings. For example, some  extremely low bit-width quantization methods, such as the $(0,1)$-binary quantization and $(0, \\pm1)$-ternary quantization, sometimes can achieve comparable or even superior classification accuracy than the original non-quantized data, although suffering from high quantization errors. To provide a more reliable estimate of classification performance, rather than focusing on quantization errors,  we  propose to directly examine the feature discrimination of quantized data. It is proved that the aforementioned binary and ternary quantization can surprisingly enhance, rather than diminish,  the feature discrimination of original data. This remarkable performance is validated through classification experiments conducted on various types of data, including the image, speech and text.", "title_embedding_index": 11322, "title_abs_embedding_index": 11347}, {"title": "Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling", "link_suffix": "/forum?id=Ch9rHRomYe", "link": "https://openreview.net/forum?id=Ch9rHRomYe", "pdf_link": "https://openreview.net/pdf?id=Ch9rHRomYe", "keywords": "Exploration, Directional Statistics, Hyperspherical Embeddings, Reinforcement Learning, Scalability, von Mises-Fisher Distribution, Recommender Systems", "abstract": "This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable method for exploring large action sets in reinforcement learning problems where hyperspherical embedding vectors represent actions. vMF-exp involves initially sampling a state embedding representation using a von Mises-Fisher distribution, then exploring this representation's nearest neighbors, which scales to virtually unlimited numbers of candidate actions.\nWe show that, under theoretical assumptions, vMF-exp asymptotically maintains the same probability of exploring each action as Boltzmann Exploration (B-exp), a popular alternative that, nonetheless, suffers from scalability issues as it requires computing softmax values for each action.\nConsequently, vMF-exp serves as a scalable alternative to B-exp for exploring large action sets with hyperspherical embeddings. \nIn the final part of this paper, we further validate the empirical relevance of vMF-exp by discussing its successful deployment at scale on a music streaming service. On this service, vMF-exp has been employed for months to recommend playlists inspired by initial songs to millions of users, from millions of possible actions for each playlist.", "title_embedding_index": 11323, "title_abs_embedding_index": 11348}, {"title": "Interactive Adjustment for Human Trajectory Prediction with Individual Feedback", "link_suffix": "/forum?id=DCpukR83sw", "link": "https://openreview.net/forum?id=DCpukR83sw", "pdf_link": "https://openreview.net/pdf?id=DCpukR83sw", "keywords": "Human Trajectory Prediction", "abstract": "Human trajectory prediction is fundamental for autonomous driving and service robot. The research community has studied various important aspects of this task and made remarkable progress recently. However, there is an essential perspective which is not well exploited in previous research all along, namely individual feedback. Individual feedback exists in the sequential nature of trajectory prediction, where earlier predictions of a target can be verified over time by his ground-truth trajectories to obtain feedback which provides valuable experience for subsequent predictions on the same agent. In this paper, we show such feedback can reveal the strengths and weaknesses of the model's predictions on a specific target and heuristically guide to deliver better predictions on him. We present an interactive adjustment network to effectively model and leverage the feedback. This network first exploits the feedback from previous predictions to dynamically generate an adjuster which then interactively makes appropriate adjustments to current predictions for more accurate ones. We raise a novel displacement expectation loss to train this interactive architecture. Through experiments on representative prediction methods and widely-used benchmarks, we demonstrate the great value of individual feedback and the superior effectiveness of proposed interactive adjustment network. Our code will be made publicly available.", "title_embedding_index": 11324, "title_abs_embedding_index": 11349}]