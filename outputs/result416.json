[{"title": "UniDrive: Towards Universal Driving Perception Across Camera Configurations", "link_suffix": "/forum?id=jVDPq9EdzT", "link": "https://openreview.net/forum?id=jVDPq9EdzT", "pdf_link": "https://openreview.net/pdf?id=jVDPq9EdzT", "keywords": "Autonomous Driving, 3D Detection, Sensor Configuration", "abstract": "Vision-centric autonomous driving has demonstrated excellent performance with economical sensors. As the fundamental step, 3D perception aims to infer 3D information from 2D images based on 3D-2D projection. This makes driving perception models susceptible to sensor configuration (e.g., camera intrinsics and extrinsics) variations. However, generalizing across camera configurations is important for deploying autonomous driving models on different car models. In this paper, we present UniDrive, a novel framework for vision-centric autonomous driving to achieve universal perception across camera configurations. We deploy a set of unified virtual cameras and propose a ground-aware projection method to effectively transform the original images into these unified virtual views. We further propose a virtual configuration optimization method by minimizing the expected projection error between original cameras and virtual cameras. The proposed virtual camera projection can be applied to existing 3D perception methods as a plug-and-play module to mitigate the challenges posed by camera parameter variability, resulting in more adaptable and reliable driving perception models. To evaluate the effectiveness of our framework, we collect a dataset on Carla by driving the same routes while only modifying the camera configurations. Experimental results demonstrate that our method trained on one specific camera configuration can generalize to varying configurations with minor performance degradation. The code and benchmark toolkit will be publicly available.", "title_embedding_index": 20750, "title_abs_embedding_index": 20775}, {"title": "Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations", "link_suffix": "/forum?id=arbj7RJ5oh", "link": "https://openreview.net/forum?id=arbj7RJ5oh", "pdf_link": "https://openreview.net/pdf?id=arbj7RJ5oh", "keywords": "Object centric learning, grounding, embodied reasoning, visual tokenizers", "abstract": "Several accounts of human cognition posit that our intelligence is rooted in our ability to form abstract composable \nconcepts, ground them in our environment, and reason over these grounded entities. This trifecta of human \nthought has remained elusive in modern intelligent machines. In this work, we investigate whether slot representations \nextracted from visual scenes serve as appropriate compositional abstractions for grounding and reasoning. We present the \nNeural Slot Interpreter (NSI), which learns to ground object semantics in slots. At the core of NSI is an XML-like \nschema that uses simple syntax rules to organize the object semantics of a scene into object-centric schema primitives. \nThen, the NSI metric learns to ground primitives into slots through a structured objective that reasons over the intermodal \nalignment. We show that the grounded slots surpass unsupervised slots in real-world object discovery and scale with scene \ncomplexity. Experiments with a bi-modal object-property and scene retrieval task demonstrate the grounding efficacy and \ninterpretability of correspondences learned by NSI. Finally, we investigate the reasoning abilities of the grounded slots. \nVision Transformers trained on grounding-aware NSI tokenizers using as few as ten tokens outperform patch-based tokens on \nchallenging few-shot classification tasks.", "title_embedding_index": 20751, "title_abs_embedding_index": 20776}, {"title": "MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility", "link_suffix": "/forum?id=kFsWpSxkFz", "link": "https://openreview.net/forum?id=kFsWpSxkFz", "pdf_link": "https://openreview.net/pdf?id=kFsWpSxkFz", "keywords": "Micromobility, Simulation Environment, Embodied AI", "abstract": "Public urban spaces like streetscapes and plazas serve residents and accommodate social life in all its vibrant variations. Recent advances in Robotics and Embodied AI make public urban spaces no longer exclusive to humans. Food delivery bots and electric wheelchairs have started sharing sidewalks with pedestrians, while robot dogs and humanoids have recently emerged in the street.Micromobilityenabled by AI for short-distance travel in public urban spaces plays a crucial component in the future transportation system. Ensuring the generalizability and safety of AI models maneuvering mobile machines is essential. In this work, we presentMetaUrban, acompositionalsimulation platform for the AI-driven urban micromobility research. MetaUrban can construct aninfinitenumber of interactive urban scenes from compositional elements, covering a vast array of ground plans, object placements, pedestrians, vulnerable road users, and other mobile agents\u2019 appearances and dynamics. We design point navigation and social navigation tasks as the pilot study using MetaUrban for urban micromobility research and establish various baselines of Reinforcement Learning and Imitation Learning. We conduct extensive evaluation across mobile machines, demonstrating that heterogeneous mechanical structures significantly influence the learning and execution of AI policies. We perform a thorough ablation study, showing that the compositional nature of the simulated environments can substantially improve the generalizability and safety of the trained mobile agents. MetaUrban will be made publicly available to provide research opportunities and foster safe and trustworthy embodied AI and micromobility in cities. The code and dataset will be publicly available.", "title_embedding_index": 20752, "title_abs_embedding_index": 20777}, {"title": "ImageFolder: Autoregressive Image Generation with Folded Tokens", "link_suffix": "/forum?id=QE1LFzXQPL", "link": "https://openreview.net/forum?id=QE1LFzXQPL", "pdf_link": "https://openreview.net/pdf?id=QE1LFzXQPL", "keywords": "Semantic tokenizer, Autoregressive generation, Product quantization", "abstract": "Image tokenizers are crucial for visual generative models, \\eg, diffusion models (DMs) and autoregressive (AR) models, as they construct the latent representation for modeling. Increasing token length is a common approach to improve image reconstruction quality. However, tokenizers with longer token lengths are not guaranteed to achieve better generation quality. There exists a trade-off between reconstruction and generation quality regarding token length. In this paper, we investigate the impact of token length on both image reconstruction and generation and provide a flexible solution to the tradeoff. We propose \\textbf{ImageFolder}, a semantic tokenizer that provides spatially aligned image tokens that can be folded during autoregressive modeling to improve both efficiency and quality. To enhance the representative capability without increasing token length, we leverage dual-branch product quantization to capture different contexts of images. Specifically, semantic regularization is introduced in one branch to encourage compacted semantic information while another branch is designed to capture pixel-level details. Extensive experiments demonstrate the superior quality of image generation and shorter token length with ImageFolder tokenizer.", "title_embedding_index": 20753, "title_abs_embedding_index": 20778}, {"title": "Multi-modal Controlled Coherent Motion Synthesis", "link_suffix": "/forum?id=i5Gxilzk0u", "link": "https://openreview.net/forum?id=i5Gxilzk0u", "pdf_link": "https://openreview.net/pdf?id=i5Gxilzk0u", "keywords": "Human Motion Generation; Multi-Modal; Generative Models", "abstract": "We walk and talk at the same time all the time. It is just natural for us. This paper tackles the challenge of replicating such natural behaviors in 3D avatar motion generation driven by concurrent multi-modal inputs, e.g., a text description ``a man is walking\" alongside a speech audio. Existing methods, constrained by the scarcity of aligned multi-modal data, typically combine motions from individual modalities sequentially or through weighted averaging. These strategies often result in mismatched or unrealistic movements. To overcome these limitations, we propose MOCO, a novel diffusion-based framework capable of processing multiple simultaneous inputs\u2014including speech audio, text descriptions, and trajectory data\u2014to generate coherent and lifelike motions without requiring additional datasets. Our key innovation lies in decoupling the motion generation process. During each denoising step, the diffusion model independently generates motions for each modality from the input noise and assembles the body parts according to predefined spatial rules. The resulting combined motion is then diffused and serves as the input noise for the subsequent denoising step. This iterative approach enables each modality to refine its contribution within the context of the overall motion, progressively harmonizing movements across modalities. Consequently, the generated motions become increasingly natural and fluid with each iteration, achieving coherent and synchronized behaviors. We evaluate our approach using a purpose-built multi-modal benchmark. Experimental results demonstrate that MOCO significantly outperforms existing baselines, advancing the field of multi-modal motion generation for 3D avatars. The code will be released.", "title_embedding_index": 20754, "title_abs_embedding_index": 20779}, {"title": "Comparisons Are All You Need for Optimizing Smooth Functions", "link_suffix": "/forum?id=PFRWGeUhJx", "link": "https://openreview.net/forum?id=PFRWGeUhJx", "pdf_link": "https://openreview.net/pdf?id=PFRWGeUhJx", "keywords": "comparison-based optimization, sub-zeroth order method, gradient estimation, convex optimization, escape from saddle points", "abstract": "When optimizing machine learning models, there are various scenarios where gradient computations are challenging or even infeasible. Furthermore, in reinforcement learning (RL), preference-based RL that only compares between options has wide applications, including reinforcement learning with human feedback in large language models. In this paper, we systematically study optimization of a smooth function $f\\colon\\mathbb{R}^n\\to\\mathbb{R}$ only assuming an oracle that compares function values at two points and tells which is larger. When $f$ is convex, we give two algorithms using $\\tilde{O}(n/\\epsilon)$ and $\\tilde{O}(n^{2})$ comparison queries to find an $\\epsilon$-optimal solution, respectively. When $f$ is nonconvex, our algorithm uses $\\tilde{O}(n/\\epsilon^2)$ comparison queries to find an $\\epsilon$-approximate stationary point. All these results match the best-known zeroth-order algorithms with function evaluation queries in $n$ dependence, thus suggesting that \\emph{comparisons are all you need for optimizing smooth functions using derivative-free methods}. In addition, we also give an algorithm for escaping saddle points and reaching an $\\epsilon$-second order stationary point of a nonconvex $f$, using $\\tilde{O}(n^{1.5}/\\epsilon^{2.5})$ comparison queries.", "title_embedding_index": 20755, "title_abs_embedding_index": 20780}, {"title": "InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly", "link_suffix": "/forum?id=ky7vVlBQBY", "link": "https://openreview.net/forum?id=ky7vVlBQBY", "pdf_link": "https://openreview.net/pdf?id=ky7vVlBQBY", "keywords": "additive models, GAM, SHAP, Shapley", "abstract": "In recent years, the Shapley value and SHAP explanations have emerged as one\nof the most dominant paradigms for providing post-hoc explanations of blackbox models. Despite their well-founded theoretical properties, many recent works\nhave focused on the limitations in both their computational efficiency and their\nrepresentation power. The underlying connection with additive models, however,\nis left critically under-emphasized in the current literature. In this work, we find\nthat a variational perspective linking GAM models and SHAP explanations is able\nto provide deep insights into nearly all recent developments. In light of this connection, we borrow in the other direction to develop a new method to train interpretable GAM models which are automatically purified to compute the Shapley\nvalue in a single forward pass. Finally, we provide theoretical results showing the\nlimited representation power of GAM models is the same Achilles\u2019 heel existing\nin SHAP and discuss the implications for SHAP\u2019s modern usage in CV and NLP.", "title_embedding_index": 20756, "title_abs_embedding_index": 20781}, {"title": "Prompt Optimization with Logged Bandit Data", "link_suffix": "/forum?id=1upXwlEW8y", "link": "https://openreview.net/forum?id=1upXwlEW8y", "pdf_link": "https://openreview.net/pdf?id=1upXwlEW8y", "keywords": "off-policy evaluation, prompt tuning, large language models, contextual bandits", "abstract": "We study how to use naturally available user feedback, such as clicks, to optimize large language model (LLM) pipelines for generating personalized sentences using prompts. Naive approaches, which estimate the policy gradient in the prompt space, suffer either from variance caused by the large action space of prompts or bias caused by inaccurate reward predictions. To circumvent these challenges, we proposeDirect Sentence Off-policy gradient(DSO), which estimates the policy gradient by leveraging similarity among generated sentences, substantially reducing variance while suppressing the bias. Empirical results on our newly established suite of benchmarks, calledOfflinePrompts, demonstrate the effectiveness of the proposed approach in generating personalized descriptions for movie recommendations, particularly when the number of candidate prompts is large.", "title_embedding_index": 20757, "title_abs_embedding_index": 20782}, {"title": "Watermark Anything With Localized Messages", "link_suffix": "/forum?id=IkZVDzdC8M", "link": "https://openreview.net/forum?id=IkZVDzdC8M", "pdf_link": "https://openreview.net/pdf?id=IkZVDzdC8M", "keywords": "Image Watermarking; Segmentation", "abstract": "Image watermarking methods are not tailored to handle small watermarked areas.\nThis restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited.\nWe introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). \nThe WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked.\nThe models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks.\nExperiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. \nMoreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions -- no larger than 10% of the image surface -- even for small $256\\times 256$ images.", "title_embedding_index": 20758, "title_abs_embedding_index": 20783}, {"title": "Embedding-based statistical inference on generative models", "link_suffix": "/forum?id=QmmTEgIbzA", "link": "https://openreview.net/forum?id=QmmTEgIbzA", "pdf_link": "https://openreview.net/pdf?id=QmmTEgIbzA", "keywords": "model inference, embeddings", "abstract": "The recent cohort of publicly available generative models can produce human expert level content across a variety of topics and domains. Given a model in this cohort as a base model, methods such as parameter efficient fine-tuning, in-context learning, and constrained decoding have further increased generative capabilities and improved both computational and data efficiency. Entire collections of derivative models have emerged as a byproduct of these methods and each of these models has a set of associated covariates such as a score on a benchmark, an indicator for if the model has (or had) access to sensitive information, etc. that may or may not be available to the user. For some model-level covariates, it is possible to use \u201csimilar\u201d models to predict an unknown covariate. In this paper we extend recent results related to embedding-based representations of generative models \u2013 the data kernel perspective space \u2013 to classical statistical inference settings. We demonstrate that using the perspective space as the basis of a notion of \u201csimilar\u201d is effective for multiple model-level inference tasks.", "title_embedding_index": 20759, "title_abs_embedding_index": 20784}, {"title": "M^3PC: Test-time Model Predictive Control using Pretrained Masked Trajectory Model", "link_suffix": "/forum?id=inOwd7hZC1", "link": "https://openreview.net/forum?id=inOwd7hZC1", "pdf_link": "https://openreview.net/pdf?id=inOwd7hZC1", "keywords": "Offline-to-Online Reinforcement Learning, Model-based Reinforcement Learning, Masked Autoencoding, Robot Learning", "abstract": "Recent work in Offline Reinforcement Learning (RL) has shown that  a unified transformer trained under a masked auto-encoding objective can effectively capture the relationships between different modalities (e.g., states, actions, rewards) within given trajectory datasets. However, this information has not been fully exploited during the inference phase, where the agent needs to generate an optimal policy instead of just reconstructing masked components from unmasked. Given that a pretrained trajectory model can act as both a Policy Model and a World Model with appropriate mask patterns, we propose using Model Predictive Control (MPC) at test time to leverage the model's own predictive capacity to guide its action selection. Empirical results on D4RL and RoboMimic show that our inference-phase MPC significantly improves the decision-making performance of a pretrained trajectory model without any additional parameter training. Furthermore, our framework can be adapted to Offline to Online (O2O) RL and Goal Reaching RL, resulting in more substantial performance gains when an additional online interaction budget is given, and better generalization capabilities when different task targets are specified. Our code and models will be released.", "title_embedding_index": 20760, "title_abs_embedding_index": 20785}, {"title": "PF3plat: Pose-Free Feed-Forward 3D Gaussian Splatting", "link_suffix": "/forum?id=3QinqLlMCj", "link": "https://openreview.net/forum?id=3QinqLlMCj", "pdf_link": "https://openreview.net/pdf?id=3QinqLlMCj", "keywords": "Generalized Pose-Free Novel View Synthesis, 3D Reconstruction", "abstract": "We consider the problem of novel view synthesis from unposed images in a single feed-forward. Our framework capitalizes on fast speed, scalability, and high-quality 3D reconstruction and view synthesis capabilities of 3DGS, where we further extend it to offer a practical solution that relaxes common assumptions such as dense image views, accurate camera poses, and substantial image overlaps. We achieve this through identifying and addressing unique challenges arising from the use of pixel-aligned 3DGS: misaligned 3D Gaussians across different views induce noisy or sparse gradients that destabilize training and hinder convergence, especially when above assumptions are not met. To mitigate this, we employ pre-trained monocular depth estimation and visual correspondence models to achieve coarse alignments of 3D Gaussians. We then introduce lightweight, learnable modules to refine depth and pose estimates from the coarse alignments, improving the quality of 3D reconstruction and novel view synthesis. Furthermore, the refined estimates are leveraged to estimate geometry confidence scores, which assess the reliability of 3D Gaussian centers and condition the prediction of Gaussian parameters accordingly. Extensive evaluations on large-scale real-world datasets demonstrate that PF3plat sets a new state-of-the-art across all benchmarks, supported by comprehensive ablation studies validating our design choices. We will make the code and weights publicly available.", "title_embedding_index": 20761, "title_abs_embedding_index": 20786}, {"title": "Informing Reinforcement Learning Agents by Grounding Language to Markov Decision Processes", "link_suffix": "/forum?id=1EEst6oDU7", "link": "https://openreview.net/forum?id=1EEst6oDU7", "pdf_link": "https://openreview.net/pdf?id=1EEst6oDU7", "keywords": "Language Grounding, RLang, RL, Formal Language, LLM", "abstract": "While significant efforts have been made to leverage natural language to accelerate reinforcement learning, utilizing diverse forms of language efficiently remains unsolved. Existing methods focus on mapping natural language to individual elements of MDPs such as reward functions or policies, but such approaches limit the scope of language they consider to make such mappings possible. We present an approach for leveraging general language advice by translating sentences to a grounded formal language for expressing information abouteveryelement of an MDP and its solution including policies, plans, reward functions, and transition functions. We also introduce a new model-based reinforcement learning algorithm, RLang-Dyna-Q, capable of leveraging all such advice, and demonstrate in two sets of experiments that grounding language to every element of an MDP leads to significant performance gains.", "title_embedding_index": 20762, "title_abs_embedding_index": 20787}, {"title": "Personalized Large Vision-Language Model", "link_suffix": "/forum?id=RVfom47pEu", "link": "https://openreview.net/forum?id=RVfom47pEu", "pdf_link": "https://openreview.net/pdf?id=RVfom47pEu", "keywords": "Personalization model, Large Vision-Language Model", "abstract": "The personalization model has gained significant attention in the field of image generation yet remains underexplored for large vision-language models (LVLMs). Beyond generic ones, with personalization, LVLMs handle interactive dialogues using clearly referential concepts (e.g., \u201cMike and Susan are talking.\u201d) instead of the generic form (e.g., \u201ca boy and a girl are talking.\u201d), making the conversation more customizable and referentially friendly. In addition, PLVM is equipped with the ability of continuously adding new concepts during a dialogue without incurring additional costs, which significantly enhances the practicality. Basically, PLVM proposes Aligner, a pre-trained visual encoder to align referential concepts with the queried images. During the dialogues, it extracts features of reference images with these corresponding concepts and recognize them in the queried image, enabling personalization. We note that the computational cost and parameter count of the Aligner are negligible within the entire framework. With comprehensive qualitative and quantitative analyses, we reveal the effectiveness and superiority of PLVM.", "title_embedding_index": 20763, "title_abs_embedding_index": 20788}, {"title": "Distilling the Knowledge in Data Pruning", "link_suffix": "/forum?id=9ccZzuix2D", "link": "https://openreview.net/forum?id=9ccZzuix2D", "pdf_link": "https://openreview.net/pdf?id=9ccZzuix2D", "keywords": "Data pruning, Knowledge distillation", "abstract": "With the increasing size of datasets used for training neural networks, data pruning has gained traction in recent years.\nHowever, most current data pruning algorithms are limited in their ability to preserve accuracy compared to models trained on the full data, especially in high pruning regimes. \nIn this paper we explore the application of data pruning while incorporating knowledge distillation (KD) when training on a pruned subset. \nThat is, rather than relying solely on ground-truth labels, we also use the soft predictions from a teacher network pre-trained on the complete data.\nBy integrating KD into training, we demonstrate significant improvement across datasets, pruning methods, and on all pruning fractions. \nWe first establish a theoretical motivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using KD, simple random pruning is comparable or superior to sophisticated pruning methods across all pruning regimes.\nOn ImageNet for example, we achieve superior accuracy despite training on a random subset of only 50% of the data. \nAdditionally, we demonstrate a crucial connection between the pruning factor and the optimal knowledge distillation weight. This helps mitigate the impact of samples with noisy labels and low-quality images retained by typical pruning algorithms.\nFinally, we make an intriguing observation: when using lower pruning fractions, larger teachers lead to accuracy degradation, while surprisingly, employing teachers with a smaller capacity than the student's may improve results.\nOur code will be made available.", "title_embedding_index": 20764, "title_abs_embedding_index": 20789}, {"title": "Causally Motivated Diffusion Sampling Frameworks for Harnessing Contextual Bias", "link_suffix": "/forum?id=xLPakPOKDX", "link": "https://openreview.net/forum?id=xLPakPOKDX", "pdf_link": "https://openreview.net/pdf?id=xLPakPOKDX", "keywords": "Causal Inference, Diffusion Models, Contextual bias, Spurious Correlations, Object Cooccurrence, StableDiffusion", "abstract": "Diffusion models have shown remarkable performance in text-guided image generation when trained on large-scale datasets, usually collected from the Internet. These large-scale datasets have contextual biases (e.g., co-occurrence of objects) which will naturally cascade into the diffusion model. For example, given a text prompt of ``a photo of the living room'', diffusion models frequently generate a couch, a rug, and a lamp together while rarely generating objects that do not commonly occur in a living room. Intuitively, contextual bias can be helpful because it naturally draws the scene even without detailed information (i.e., visual autofill). On the other hand, contextual bias can limit the diversity of generated images (e.g., diverse object combinations) to focus on common image compositions. To have the best of both worlds, we argue that contextual bias needs to be strengthened or weakened depending on the situation. Previous causally-motivated studies have tried to deal with such issues by analyzing confounders (i.e., contextual bias) and augmenting training data or designing their models to directly learn the interventional distribution. However, due to the large-scale nature of these models, obtaining and analyzing the data or training the huge model from scratch is beyond reach in practice. To tackle this problem, we propose two novel frameworks for strengthening or weakening the contextual bias of pretrained diffusion models without training any parameters or accessing training data. Briefly, we first propose causal graphs to explicitly model contextual bias in the generation process. We then sample the hidden confounder due to contextual bias by sampling from a chain of pretrained large-scale models. Finally, we use samples from the confounder to strengthen or weaken the contextual bias based on methods from causal inference. Experiment results show that our proposed methods are effective in generating more realistic and diverse images than the regular sampling method.", "title_embedding_index": 20765, "title_abs_embedding_index": 20790}, {"title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos", "link_suffix": "/forum?id=a1P5kh2oo8", "link": "https://openreview.net/forum?id=a1P5kh2oo8", "pdf_link": "https://openreview.net/pdf?id=a1P5kh2oo8", "keywords": "temporal reasoning; counterfactual reasoning; short video comprehension", "abstract": "There has been growing sentiment recently that modern large multimodal models (LMMs) have addressed most of the key challenges related to short video comprehension. As a result, both academia and industry are gradually shifting their attention towards the more complex challenges posed by understanding long-form videos. \nHowever, is this really the case?  Our studies indicate that LMMs still lack many fundamental reasoning capabilities even when dealing with short videos.  We introduce Vinoground, a temporal counterfactual LMM evaluation benchmark encompassing 1000 short and natural video-caption pairs. We demonstrate that existing LMMs severely struggle to distinguish temporal differences between different actions and object transformations.  For example, the best model GPT-4o only obtains $\\sim$50% on our text and video scores, showing a large gap compared to the human baseline of $\\sim$90%. All open-source multimodal models and CLIP-based models perform much worse, producing mostly random chance performance. Through this work, we shed light onto the fact that temporal reasoning in short videos is a problem yet to be fully solved. We will make our benchmark publicly available.", "title_embedding_index": 20766, "title_abs_embedding_index": 20791}, {"title": "TemporalBench: Towards Fine-grained Temporal Understanding for  Multimodal Video  Models", "link_suffix": "/forum?id=Wto5U7q6I2", "link": "https://openreview.net/forum?id=Wto5U7q6I2", "pdf_link": "https://openreview.net/pdf?id=Wto5U7q6I2", "keywords": "video, benchmark, multimodel", "abstract": "Understanding fine-grained temporal dynamics is crucial for video understanding. Yet, popular video benchmarks, such as MSRVTT and TGIF, often fail to effectively evaluate AI models' temporal reasoning abilities due to the lack of fine-grained temporal annotations. \nAs a result, text-based models, leveraging strong language priors, often perform comparably to video models, and image-trained models have been reported to outperform their video-trained counterparts on MSRVTT and TGIF. This paper introduces a new TemporalBench benchmark for fine-grained temporal event understanding in videos. TemporalBench, sourced from a diverse video datasets, consists of $\\sim$10K pairs of video description questions, derived from $\\sim$2K high-quality human-annotated video captions.  Uniquely, our benchmark provides fine-grained temporal annotations to evaluate models' temporal reasoning abilities. Our results show that state-of-the-art models like GPT-4o achieve only 38.0% multiple binary QA accuracy on TemporalBench, demonstrating a significant human-AI gap in temporal understanding. We hope that TemporalBench is instrumental to fostering research on improving models' temporal reasoning capabilities.", "title_embedding_index": 20767, "title_abs_embedding_index": 20792}, {"title": "Offline-to-Online Reinforcement Learning with Prioritized Experience Selection", "link_suffix": "/forum?id=jQyKywGtpW", "link": "https://openreview.net/forum?id=jQyKywGtpW", "pdf_link": "https://openreview.net/pdf?id=jQyKywGtpW", "keywords": "Reinforcement Learning; Offline-to-Online Reinforcement Learning; Prioritized Experience Selection", "abstract": "Offline-to-online reinforcement learning (O2O RL) offers a promising paradigm that first pre-trains an offline policy and fine-tunes it with further online interactions. Nevertheless, the distribution shift between the offline and online phase often hinders the fine-tuning performance, sometimes even incurring performance collapse. Existing methods mitigate this by enhancing training robustness with Q-ensemble, training a density ratio estimator to balance offline and online data, etc. But they often rely on components like ensemble and have higher training costs. In this paper, we address this issue by establishing a concrete performance bound for the optimal policies between two consecutive online steps. Motivated by the theoretical insight, we propose a simple yet effective fine-tuning method, \\textbf{P}rioritized \\textbf{E}xperience \\textbf{S}election (PES). During the online stage, PES maintains a dynamically updated priority queue containing a portion of high-return trajectories, and only selects online samples that are close to the samples in the queue for fine-tuning. In this way, the distribution shift issue can be mitigated and the fine-tuning performance can be boosted. PES is computationally efficient and compatible with numerous approaches. Experimental results on a variety of D4RL datasets show that PES can benefit different offline and O2O RL algorithms and enhance Q-value estimate. Our code is available and will be open-source.", "title_embedding_index": 20768, "title_abs_embedding_index": 20793}, {"title": "TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction", "link_suffix": "/forum?id=pTeOOKnjGM", "link": "https://openreview.net/forum?id=pTeOOKnjGM", "pdf_link": "https://openreview.net/pdf?id=pTeOOKnjGM", "keywords": "Expression reconstruction, Hybrid parameters", "abstract": "3D facial reconstruction from a single in-the-wild image is a crucial task in human-centered computer vision tasks. While existing methods can recover accurate facial shapes, there remains significant space for improvement in fine-grained expression capture.  Current approaches struggle with irregular mouth shapes, exaggerated expressions, and asymmetrical facial movements. We present TEASER (Token EnhAnced Spatial modeling for Expressions Reconstruction), which addresses these challenges and enhances 3D facial geometry performance\u2060\u2060. TEASER tackles two main limitations of existing methods: insufficient photometric loss for self-reconstruction and inaccurate localization of subtle expressions. We introduce a multi-scale tokenizer to extract facial appearance information. Combined with a neural renderer, these tokens provide precise geometric guidance for expression reconstruction. Furthermore, TEASER incorporates a pose-dependent landmark loss to further improve geometric performance\u2060. Our approach not only significantly enhances expression reconstruction quality but also offers interpretable tokens suitable for various downstream applications, such as photorealistic facial video driving, expression transfer, and identity swapping. Quantitative and qualitative experimental results across multiple datasets demonstrate that TEASER achieves state-of-the-art performance in precise expression reconstruction.", "title_embedding_index": 20769, "title_abs_embedding_index": 20794}, {"title": "SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars", "link_suffix": "/forum?id=1x1gGg49jr", "link": "https://openreview.net/forum?id=1x1gGg49jr", "pdf_link": "https://openreview.net/pdf?id=1x1gGg49jr", "keywords": "dynamic head avatars, rigging, inverse-graphics", "abstract": "Recent advancements in head avatar rendering using Gaussian primitives have achieved significantly high-fidelity results. Although precise head geometry is crucial for applications like mesh reconstruction and relighting, current methods struggle to capture intricate geometric details and render unseen poses due to their reliance on similarity transformations, which cannot handle stretch and shear transforms essential for detailed deformations of geometry. To address this, we propose SurFhead, a novel method that reconstructs riggable head geometry from RGB videos using 2D Gaussian surfels, which offer well-defined geometric properties, such as precise depth from fixed ray intersections and normals derived from their surface orientation, making them advantageous over 3D counterparts. SurFhead ensures high-fidelity rendering of both normals and images, even in extreme poses, by leveraging classical mesh-based deformation transfer and affine transformation interpolation. SurFhead introduces precise geometric deformation and blends surfels through polar decomposition of transformations, including those affecting normals. Our key contribution lies in bridging classical graphics techniques, such as mesh-based deformation, with modern Gaussian primitives, achieving state-of-the-art geometry reconstruction and rendering quality. Unlike previous avatar rendering approaches, SurFhead enables efficient reconstruction driven by Gaussian primitives while preserving high-fidelity geometry.", "title_embedding_index": 20770, "title_abs_embedding_index": 20795}, {"title": "PuppetMaster: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics", "link_suffix": "/forum?id=HCUksccuFx", "link": "https://openreview.net/forum?id=HCUksccuFx", "pdf_link": "https://openreview.net/pdf?id=HCUksccuFx", "keywords": "Video generation, motion, diffusion models", "abstract": "We present PuppetMaster, a video generator that understands part-level object dynamics. Given an image of an object and a number of drags defining the desired trajectory of selected points of the object, PuppetMaster synthesizes a video where the object moves according to the specified drags in a physically plausible manner. PuppetMaster is obtained by fine-tuning an off-the-shelf video diffusion model, extended with a new component that encodes the input drags. PuppetMaster also introduces all-to-first attention, a replacement for the common spatial attention module, which removes artifacts that arise from fine-tuning a video generator out-of-domain and significantly improves the quality of the synthesized videos. PuppetMaster is learned from Objaverse-Animation-HQ, a new dataset of curated part-level motion clips obtained by rendering synthetic 3D animations. We propose strategies to automatically filter out sub-optimal animations and augment the synthetic renderings with meaningful drags. By using this data, PuppetMaster learns to generate part-level motions, unlike other motion-conditioned video generators that mostly move the object as a whole. PuppetMaster generalizes well to real images, outperforming existing methods in real-world benchmarks in a zero-shot manner. We refer the reader to the supplementary material for video visualizations.", "title_embedding_index": 20771, "title_abs_embedding_index": 20796}, {"title": "AgentQuest: Benchmarking LLM and VLM Agents on Long-Horizon Interactive Tasks", "link_suffix": "/forum?id=fp6t3F669F", "link": "https://openreview.net/forum?id=fp6t3F669F", "pdf_link": "https://openreview.net/pdf?id=fp6t3F669F", "keywords": "LLM, VLM, Agents, Benchmark, RL", "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities, however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies\u2014areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce AgentQuest, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). \nWe devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release AgentQuest as an open and user-friendly benchmark to facilitate future research and development in the agentic community.", "title_embedding_index": 20772, "title_abs_embedding_index": 20797}, {"title": "High-quality and controllable time series generation with diffusion in transformers", "link_suffix": "/forum?id=etUJR2xBYa", "link": "https://openreview.net/forum?id=etUJR2xBYa", "pdf_link": "https://openreview.net/pdf?id=etUJR2xBYa", "keywords": "Diffusion in Transformers, time series generation", "abstract": "Current research on time series generation frequently depends on oversimplified data and lenient evaluation methods, making it challenging to apply these models effectively in real-world scenarios. Diffusion in Transformers (DiT) has demonstrated that the traditional inductive biases in neural networks are unnecessary. This paper shows that the advantages of DiT can be extended to time series generation.  We add the attention mask and dilated causal convolution to introduce the temporal characteristic. Additionally, we introduce a novel smooth guidance policy for style control during generation, leveraging a property of the diffusion process. Furthermore, our proposed model can generate longer sequences with training in short sequences. Experimental results reveal that our variant of DiT achieves state-of-the-art performance across various data types.", "title_embedding_index": 20773, "title_abs_embedding_index": 20798}, {"title": "TesseraQ: Ultra Low-Bit LLM Post-Training Quantization with Block Reconstruction", "link_suffix": "/forum?id=GTe9PDhm8v", "link": "https://openreview.net/forum?id=GTe9PDhm8v", "pdf_link": "https://openreview.net/pdf?id=GTe9PDhm8v", "keywords": "LLM, Quantization, Efficient Inference", "abstract": "Large language models (LLMs) have revolutionized natural language processing, albeit at the cost of immense memory and computation requirements. Post-training quantization (PTQ) is becoming the \\emph{de facto} method to reduce the memory footprint and improve the inference throughput of LLMs.\nIn this work, we aim to push the upper limit of LLM PTQ by optimizing the weight rounding parameters with the block reconstruction technique, a predominant method in previous vision models.\nWe propose TesseraQ, a new state-of-the-art PTQ technique, to quantize the weights of LLMs to ultra-low bits.\nTo effectively optimize the rounding in LLMs and stabilize the reconstruction process, we introduce progressive adaptive rounding. This approach iteratively transits the soft rounding variables to hard variables during the reconstruction process. Additionally, we optimize the dequantization scale parameters to fully leverage the block reconstruction technique.\nWe demonstrate that TesseraQ can be seamlessly integrated with existing scaling or clipping-based PTQ algorithms such as AWQ and OmniQuant, significantly enhancing their performance and establishing a new state-of-the-art.\nFor instance, when compared to AWQ, TesseraQ improves the wikitext2 perplexity from 14.65 to 6.82 and average downstream accuracy from 50.52 to 59.27 with 2-bit weight-only quantization of LLaMA-2-7B. \nAcross a range of quantization schemes, including W2A16, W3A16, W3A3, and W4A4, TesseraQ consistently exhibits superior performance.", "title_embedding_index": 20774, "title_abs_embedding_index": 20799}]