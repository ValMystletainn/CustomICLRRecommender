[{"title": "MeFBO: A Moreau Envelope Based First-Order Stochastic Gradient Method for Nonconvex Federated Bilevel Optimization", "link_suffix": "/forum?id=GZYCCQacwH", "link": "https://openreview.net/forum?id=GZYCCQacwH", "pdf_link": "https://openreview.net/pdf?id=GZYCCQacwH", "keywords": "Federated Bilevel Optimization, Nonconvex, Hessian-free, Convergence Analysis, Linear Speedup", "abstract": "Federated Bilevel Optimization (FBO) enables training machine learning models with nested structures across distributed devices while preserving data privacy. However, current FBO methods often impose restrictive assumptions, particularly the requirement of strong convexity in the lower-level objective. To overcome this limitation, we propose a first-order stochastic gradient method for general FBO problems, leveraging a Moreau envelope-based min-max optimization reformulation to handle potentially non-convex lower-level objectives. Unlike implicit gradient methods, our approach eliminates the need for second-order derivative information. We also establish rigorous theoretical guarantees for convergence rate and communication complexity, demonstrating linear speedup as the number of devices increases. Numerical experiments validate the effectiveness and efficiency of our method, showing comparable or superior performances in challenging scenarios, including federated loss function tuning on imbalanced datasets and federated hyper-representation.", "title_embedding_index": 8250, "title_abs_embedding_index": 8275}, {"title": "Hi-TPH: A Large-Scale Hierarchical Dataset for TCR-pHLA Binding Prediction", "link_suffix": "/forum?id=AKnLoj80Fd", "link": "https://openreview.net/forum?id=AKnLoj80Fd", "pdf_link": "https://openreview.net/pdf?id=AKnLoj80Fd", "keywords": "T cell receptor, Peptide recognition, Protein language model", "abstract": "The interaction between the T cell receptor (TCR) and peptide-human leukocyte antigen complex (pHLA) is a fundamental process underlying T cell-mediated immunity. Computational methods have been developed to predict TCR-pHLA binding, but most existing models were trained on relatively small datasets and focused solely on the Complementarity Determining Region 3 (CDR3) of the TCR $\\beta$ chain. A key barrier to developing advanced prediction models is the limited availability of comprehensive data containing understudied prediction components. In this light, we developed the Hi-TPH dataset with more protein sequences and gene annotations. The dataset is stratified into five hierarchical subsets at four different levels, ranging from Hi-TPH level I with only the peptide sequence and TCR CDR3 $\\beta$ to Hi-TPH level II, III, and IV that incorporate increasing levels of HLA sequences, full TCR $\\alpha$ and $\\beta$ chains, and gene annotations. Hi-TPH at any level represents the largest dataset with corresponding prediction components to date, for instance, the Hi-TPH level IV dataset is at least 5.99 times the size of existing ones regarding the number of TCR-pHLA pairs. We further report benchmark results on the Hi-TPH dataset, establishing valuable baselines for the TCR-pHLA binding prediction task. This comprehensive dataset and associated benchmarks provide a valuable resource for developing advanced TCR-pHLA binding prediction models and exploring research directions such as understanding the contribution of different components and enhancing model generalization to unseen peptides, with potential applications in developing targeted therapies, including personalized vaccines and immunotherapies.", "title_embedding_index": 8251, "title_abs_embedding_index": 8276}, {"title": "Frequency-Conditioned Diffusion Models for Time Series Generation", "link_suffix": "/forum?id=aIJTNrF2Sg", "link": "https://openreview.net/forum?id=aIJTNrF2Sg", "pdf_link": "https://openreview.net/pdf?id=aIJTNrF2Sg", "keywords": "Diffusion models, Time series generation, Power spectrum, Fourier transformation", "abstract": "Time series data, commonly used in fields like climate studies, finance, and healthcare, usually faces challenges such as missing data and privacy concerns. Recently, diffusion models have emerged as effective tools for generating high-quality data, but applying them to time series is still difficult, especially for capturing long-range dependencies and complex information. In this paper, we introduce a new diffusion model that uses frequency domain information to improve time series data generation. In particular, we apply Fourier analysis to adaptively separate low-frequency global trends from high-frequency details, which helps the model better understand important patterns during the denoising process. Finally, our approach uses a specialized frequency encoder to integrate this information, enhancing the model's ability to capture both global and local features. Through exhaustive experiments on various public datasets, our model shows an impressive performance in generating time series data for diverse tasks like forecasting and imputation, outperforming existing methods in accuracy and flexibility.", "title_embedding_index": 8252, "title_abs_embedding_index": 8277}, {"title": "On the Cycle Consistency of Image-Text Mappings", "link_suffix": "/forum?id=1Qn1pMLYas", "link": "https://openreview.net/forum?id=1Qn1pMLYas", "pdf_link": "https://openreview.net/pdf?id=1Qn1pMLYas", "keywords": "cycle consistency, multimodal learning, vision-language modeling, text-to-image generation, synthetic data", "abstract": "The increasing exchange of image and text in large multimodal models leads us to ask: to what degree are mappings from text to image, and back, cycle-consistent? First, we find that current image-to-text models paired with text-to-image models do achieve a degree of perceptual cycle consistency, even when these models are not trained to have this effect. However, these mappings are far from perfect, motivating us to analyze in what ways they fail. First, we observe a strong correlation between cycle consistency and downstream performance in both image captioning and text-to-image generation. Next, we investigate how divergent are text-to-image mappings as a function of the number of objects described by the text, and how it affects achieving cycle consistency. Surprisingly, we find that more descriptive text leads to a a broader distribution of generated images, but also results in overall better reconstructions. Finally, we show possible challenges of training cycle consistent models due to the sensitivity of text-to-image models.", "title_embedding_index": 8253, "title_abs_embedding_index": 8278}, {"title": "Robust RLHF with Noisy Rewards", "link_suffix": "/forum?id=Cfbr56K4gp", "link": "https://openreview.net/forum?id=Cfbr56K4gp", "pdf_link": "https://openreview.net/pdf?id=Cfbr56K4gp", "keywords": "Alignment, LLM, RLHF", "abstract": "Reinforcement learning from human feedback (RLHF) is the mainstream paradigm to align large language models (LLMs) with human preferences. Yet existing RLHF heavily relies on accurate and informative reward models, which are vulnerable and sensitive to noise from various sources, e.g. human labeling errors, making the pipeline fragile. In this work, we formulate the problem of performing robust RLHF with noisy reward models. Our goal is to design robust RLHF algorithms that explicitly acknowledge the potential noise in a reward model. Our first contribution is an analysis that revealed a certain transformation of the preference function improves its robustness to noise in the reward function. This observation leads to a new reward function design that involves two steps: (1) an offline sampling step to obtain responses to prompts that serve as baseline calculation and (2) a contrastive reward calculated using the baseline responses in Proximal Policy Optimization (PPO). We show that our suggested rewards enable the LLM to penalize reward uncertainty, improve robustness, encourage improvement over baselines, calibrate according to task difficulty, and reduce variance in PPO. We also empirically demonstrate contrastive reward can improve RLHF substantially, evaluated by both GPTs and humans, and it consistently outperforms strong baselines.", "title_embedding_index": 8254, "title_abs_embedding_index": 8279}, {"title": "Establishing Knowledge Preference in Language Models", "link_suffix": "/forum?id=n8IzL0Vy4G", "link": "https://openreview.net/forum?id=n8IzL0Vy4G", "pdf_link": "https://openreview.net/pdf?id=n8IzL0Vy4G", "keywords": "Knowledge Preference, Knowledge Conflicts, Retrieval-Augmented Generation, Question Answering, Large Language Model", "abstract": "Language models are known to encode a great amount of factual knowledge through pretraining. However, such knowledge might be insufficient to cater to user requests, requiring the model to integrate external knowledge sources and adhere to user-provided specifications.  When answering questions about ongoing events, the model should use recent news articles to update its response; when asked to provide recommendations, the model should prioritize user specifications over retrieved product reviews; when some facts are edited in the model, the updated facts should override all prior knowledge learned by the model even if they are conflicting. In all of the cases above, the model faces a decision between its own parametric knowledge, (retrieved) contextual knowledge, and user instruction knowledge. In this paper, we (1) unify such settings into the problem of $\\textit{knowledge preference}$ and define a three-level preference hierarchy over these knowledge sources; (2) compile a collection of existing datasets IfQA, MQuAKE, and MRQA covering a combination of settings (with/without user specifications, with/without context documents) to systematically evaluate how well models obey the intended knowledge preference; and (3) propose a dataset synthesis method that composes diverse question-answer pairs with user assumptions and related context to directly fine-tune LMs for instilling the hierarchy of knowledge.\nWe demonstrate that a 7B model, fine-tuned on only a few thousand examples automatically generated by our proposed method, effectively achieves superior performance (more than 18% improvement across all evaluation benchmarks) in adhering to the desired knowledge preference hierarchy.", "title_embedding_index": 8255, "title_abs_embedding_index": 8280}, {"title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data", "link_suffix": "/forum?id=kDakBhOaBV", "link": "https://openreview.net/forum?id=kDakBhOaBV", "pdf_link": "https://openreview.net/pdf?id=kDakBhOaBV", "keywords": "data centric machine learning, large language models, nlp, natural language processing, diversity, data metrics", "abstract": "Current trends in pre-training Large Language Models (LLMs) primarily focus on the scaling of model and dataset size.\nWhile the \\textit{quality} of pre-training data is considered an important factor for training powerful LLMs, it remains a nebulous concept that has not been rigorously characterized.\nTo this end, we propose a formalization of one key aspect of data quality -- measuring the \\textit{variability} of natural language data -- specifically via a measure we call the diversity coefficient. \nOur empirical analysis shows that the proposed diversity coefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. \nThen, we measure the diversity coefficient of publicly available pre-training datasets and demonstrate that their formal diversity is high compared to theoretical lower and upper bounds.\nFinally, we conduct a comprehensive set of controlled \\textit{interventional} experiments with GPT-2 and LLaMAv2 that demonstrate the diversity coefficient of pre-training data characterizes useful aspects of downstream model evaluation performance---totaling 44 models of various sizes (51M  to 7B parameters).\nWe conclude that our formal notion of diversity is an important aspect of data quality that captures variability and causally leads to improved evaluation performance.", "title_embedding_index": 8256, "title_abs_embedding_index": 8281}, {"title": "Dueling in the Dark: An Efficient and OptimalO(T)Mirror Descent Approach for Competing against Adversarial Preferences", "link_suffix": "/forum?id=z1Jq1PLQWs", "link": "https://openreview.net/forum?id=z1Jq1PLQWs", "pdf_link": "https://openreview.net/pdf?id=z1Jq1PLQWs", "keywords": "Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF), gradient descent-based algorithm, theoretical foundations, active no-regret learning, preference feedback, trajectory preferences, multi-way feedback, human-AI alignment, practical impact.", "abstract": "Recent developments in Large Language Models (LLMs) have sparked significant attention in Reinforcement Learning from Human Feedback (RLHF), which uses reinforcement learning techniques to optimize a model's performance through human-provided feedback. A simple, widely used, and cost-effective method for gathering human feedback is through relative queries based on human preferences, often modeled using sigmoid utility models. Despite the popularity of sigmoid model-based RLHF algorithms, their theoretical foundations remain underdeveloped as existing algorithms often lack performance guarantees or are limited to small-scale problems due to computationally intractable steps. We address the challenge of developing no-regret learning algorithms for training optimal policy RLHF, and develop the first efficient gradient descent-based algorithm with near-optimal regret guarantees. More technically, we consider the adversarial online convex optimization problem with preference feedback and propose a mirror descent method to obtain a regret of $O(\\sqrt{T})$ over $T$ rounds. The main challenge we are required to solve lies in finding a suitable `gradient-approximation' of the underlying utility functions solely from a binary preference feedback. Following this we extend our results to policy optimization in the RLHF framework with trajectory preferences and design no-regret RL policies using a variant of mirror descent. We also extend our methods beyond pairwise preferences --- to multi-way (batched pairwise) feedback and ranking feedback --- and analyze the trade-off between learning rate with increasing subset size. Our contribution lays the groundwork for a practical gradient descent-based algorithm in RLHF with human preferences. Supported by robust theoretical guarantees, our approach holds promise in the current landscape of developing efficient algorithms for LLMs and addressing human-AI alignment challenges. Empirical evaluations validate our theoretical findings.", "title_embedding_index": 8257, "title_abs_embedding_index": 8282}, {"title": "Advancing Differential Privacy through Synthetic Dataset Alignment", "link_suffix": "/forum?id=TbOcySs6g8", "link": "https://openreview.net/forum?id=TbOcySs6g8", "pdf_link": "https://openreview.net/pdf?id=TbOcySs6g8", "keywords": "synthtic data, differential privacy, foundation models", "abstract": "Privacy in training data is crucial to protect sensitive personal information, prevent data misuse, and ensure compliance with legal regulations, all while maintaining trust and safeguarding individuals' rights in the development of ML models. \nUnfortunately, state-of-the-art methods that train ML models on image datasets with differential privacy constraints typically result in reduced accuracy due to noise. Alternatively, using synthetic data avoids the direct use of private data, preserving privacy, but suffers from domain discrepancies when compared to test data. This paper proposes a new methodology that combines both approaches by generating differentially private synthetic data closely aligned with the target domain, thereby improving the utility-privacy trade-off.Our approach begins with creating a synthetic base dataset using a class-conditional generative model. To address the domain gap between the synthetic dataset and the private dataset, we introduce the \\textbf{Privacy-Aware Synthetic Dataset Alignment \\text{(PASDA)}}, which leverages the feature statistics of the private dataset to guide the domain alignment process. PASDA produces a synthetic dataset that guarantees privacy while remaining highly functional for downstream training tasks.\nBuilding on this, we achieve state-of-the-art performance, surpassing the most competitive baseline by over 13% on CIFAR-10.\nFurthermore, our $(1,10^{-5})$-DP synthetic data achieves model performance on par with or surpassing models trained on the original STL-10, ImageNette and CelebA dataset. With zero-shot generation, our method does not require resource-intensive retraining, offering a synthetic data generation solution that introduces \\textbf{privacy} to a machine learning pipeline with both high \\textbf{efficiency} and \\textbf{efficacy}.", "title_embedding_index": 8258, "title_abs_embedding_index": 8283}, {"title": "Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment", "link_suffix": "/forum?id=8egnwady4b", "link": "https://openreview.net/forum?id=8egnwady4b", "pdf_link": "https://openreview.net/pdf?id=8egnwady4b", "keywords": "Skill Learning, Hierarchical Reinforcement Learning", "abstract": "Reinforcement learning (RL) has made significant progress in various domains, but scaling it to long-horizon tasks with complex decision-making remains challenging. Skill learning attempts to address this by abstracting actions into higher-level behaviors. However, current approaches often fail to recognize semantically similar behaviors as the same skill and use fixed skill lengths, limiting flexibility and generalization. To address this, we propose Dynamic Contrastive Skill Learning (DCSL), a novel framework that redefines skill representation and learning. DCSL introduces three key ideas: state-transition based skill definition, skill similarity function learning, and dynamic skill length adjustment. By focusing on state transitions and leveraging contrastive learning, DCSL effectively captures the semantic context of behaviors and adapts skill lengths to match the appropriate temporal extent of behaviors. Our approach enables more flexible and adaptive skill extraction, particularly in complex or noisy datasets, and demonstrates competitive performance compared to existing methods in task completion and efficiency.", "title_embedding_index": 8259, "title_abs_embedding_index": 8284}, {"title": "Comprehensive Artistic Style Representation for Quantitative Evaluation", "link_suffix": "/forum?id=g8TF3gd01u", "link": "https://openreview.net/forum?id=g8TF3gd01u", "pdf_link": "https://openreview.net/pdf?id=g8TF3gd01u", "keywords": "Visual Representation, Representation Decoupling, Artistic Style, Vision-Language Models", "abstract": "Artistic style, a unique medium for artists to express creativity through elements like form, color, and composition, poses a challenge for computer vision due to its intricate patterns and nuanced aesthetics. Contemporary models, often reliant on specific datasets, face limitations in their generalizability and precision in identifying individual artists' styles. From an information theory perspective, we examine the limitations of fine-tuning and investigate techniques to disentangle content from style information. We note differences in artistic style representation between unimodal and multimodal models. As a result, we propose a plug-and-play approach designed to efficiently separate content information within Vision-Language Models (VLMs), preserving stylistic details. Furthermore, we present the WeART dataset, a large-scale art dataset with high-quality annotations, to evaluate the artistic style representation capabilities of models. Experimental results show that our method improves the performance of VLMs in style retrieval tasks across several datasets. We will publicly release the proposed dataset and code.", "title_embedding_index": 8260, "title_abs_embedding_index": 8285}, {"title": "MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection", "link_suffix": "/forum?id=BQwsRy1h3U", "link": "https://openreview.net/forum?id=BQwsRy1h3U", "pdf_link": "https://openreview.net/pdf?id=BQwsRy1h3U", "keywords": "Inference Optimization, KV Cache Compression, Low-rank Projection", "abstract": "KV cache has become ade factotechnique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. \nAs the size of the model and data grows, the KV cache can, yet, quickly become a bottleneck within the system in both storage and memory transfer.\nTo address this, prior studies usually focus on the first three axes of the cache tensors for compression.This paper supplements them, focusing on the feature dimension axis, \nby utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. \nWe begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (PCA). \nWe identify the drawback of PCA projection that model performance degrades rapidly under relatively low compression rates (less than 60%).\nThis phenomenon is elucidated by insights derived from the principles of attention mechanisms.\nTo bridge the gap, we propose to directly tune the orthogonal projection matrix on the continual pre-training or supervised fine-tuning datasets with an elaborate Matryoshka learning strategy.\nThanks to such a strategy, we can adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. \nCompared to Multi-head Latent Attention (MLA), our method can easily embrace pre-trained LLMs and hold a smooth tradeoff between performance and compression rate. \nWe witness the high data efficiency of our training procedure and find that our method can sustain over 90% performance with an average KV cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular LLMs like LLaMA2 and Mistral.", "title_embedding_index": 8261, "title_abs_embedding_index": 8286}, {"title": "In-context KV-Cache Eviction for LLMs via Attention-Gate", "link_suffix": "/forum?id=tvQNysCP7C", "link": "https://openreview.net/forum?id=tvQNysCP7C", "pdf_link": "https://openreview.net/pdf?id=tvQNysCP7C", "keywords": "KV-Cache eviction, Attention-Gate, LLM inference acceleration", "abstract": "The KV-Cache technique has become the standard for the inference of large language models (LLMs).\nIt caches states of self-attention to avoid recomputation. \nYet, it is widely criticized that KV-Cache can become a bottleneck of the LLM inference system, especially when confronted with ultra-large models and long-context queries. \nA natural remedy is to discard the KV-Cache for less important tokens, with StreamingLLM as an example, but the used static eviction strategies cannot flexibly adapt to varying contexts. \nRemedies like H2O leverage accumulative attention scores to perform dynamic eviction but suffer from the attention bias issue in capturing contextual information. \nThis paper bridges this gap by devising a parameterized KV-Cache eviction mechanism, dubbed asAttention-Gate, which accepts the whole context as input and yields eviction flags for each token to realizein-contexteviction. \nThe subsequent self-attention module proceeds according to the flags and only the KV states for the remaining tokens need to be cached. \nThe Attention-Gates can vary among different heads and layers and be trivially plugged into pre-trained LLMs, tuned by cost-effective continual pre-training or supervised fine-tuning objectives to acquire what to discard. \nThe computational and memory overhead introduced by Attention-Gates is minimal.\nOur method is validated across multiple tasks, demonstrating both efficiency and adaptability.\nAfter a highly efficient continual pre-training, it achieves higher average accuracy and evicts more tokens compared to traditional training-free methods. \nIn supervised fine-tuning, it not only evicts many tokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE, where it improves accuracy by 13.9% while evicting 62.8% of tokens, showing that effective eviction of redundant tokens can even enhance performance.", "title_embedding_index": 8262, "title_abs_embedding_index": 8287}, {"title": "From Appearance to Motion: Aligning Visual Representations for Robotic Manipulation", "link_suffix": "/forum?id=wl1Kup6oES", "link": "https://openreview.net/forum?id=wl1Kup6oES", "pdf_link": "https://openreview.net/pdf?id=wl1Kup6oES", "keywords": "pretrained, frozen, motion, features, policy, behavioral-cloning", "abstract": "Pre-trained vision models used in robotics often misalign with manipulation tasks due to the loss used to train these vision models being focused on appearance rather than motion. In order to enhance motion encoding within vision models, we introduce a simple novel contrastive training framework that operates over predictions of motion. After training over EPIC Kitchens, model evaluations on behavioral cloning show a improvement in success rate over state-of-the-art methods across a benchmark of $3$ environments and $21$ object manipulation tasks.", "title_embedding_index": 8263, "title_abs_embedding_index": 8288}, {"title": "SymMaP: Improving Computational Efficiency in Linear Solvers through Symbolic Preconditioning", "link_suffix": "/forum?id=4WvCoXU2dF", "link": "https://openreview.net/forum?id=4WvCoXU2dF", "pdf_link": "https://openreview.net/pdf?id=4WvCoXU2dF", "keywords": "Matrix Preconditioning, Symbolic Learning, Linear System Solver", "abstract": "Matrix preconditioning is a crucial modern technique for accelerating the solving of linear systems. \nIts effectiveness heavily depends on the choice of preconditioning parameters. \nTraditional methods often depend on domain expertise to define a set of fixed constants for specific scenarios. \nHowever, the characteristics of each problem instance also affect the selection of optimal parameters, while fixed constants do not account for specific instance characteristics and may lead to performance loss.\nIn this paper, we proposeSymbolicMatrixPreconditioning (SymMaP), a novel framework based on Recurrent Neural Networks (RNNs) for automatically generating symbolic expressions to compute efficient preconditioning parameters. \nOur method begins with a grid search to identify optimal parameters according to task-specific performance metrics. \nSymMaP then performs a risk-seeking search over the high-dimensional discrete space of symbolic expressions, using the best-found expression as the evaluation criterion. \nThe resulting symbolic expressions are seamlessly integrated into modern linear system solvers to improve computational efficiency.\nExperimental results demonstrate that SymMaP consistently outperforms traditional algorithms across various benchmarks. The learned symbolic expressions can be easily embedded into existing specialized solvers with negligible computational overhead. Furthermore, the high interpretability of these concise mathematical expressions facilitates deeper understanding and further optimization of matrix preconditioning strategies.", "title_embedding_index": 8264, "title_abs_embedding_index": 8289}, {"title": "Potential Outcome Imputation for CATE Estimation", "link_suffix": "/forum?id=RwEVUegARu", "link": "https://openreview.net/forum?id=RwEVUegARu", "pdf_link": "https://openreview.net/pdf?id=RwEVUegARu", "keywords": "causal inference, treatment effects, data augmentation", "abstract": "One of the most significant challenges in Conditional Average Treatment Effect (CATE) estimation is the statistical discrepancy between distinct treatment groups. To address this, we propose a model-agnostic data augmentation method for CATE estimation. We first derive regret bounds for general data augmentation methods, indicating that reduced group discrepancy and low imputation error enhance CATE estimation. Inspired by this, we introduce a contrastive learning approach that reliably imputes missing potential outcomes for a selected subset of individuals based on a similarity measure. These reliable imputations augment the original dataset, reducing the discrepancy between treatment groups while inducing minimal imputation error. The augmented dataset can then be used to train standard CATE estimation models. We provide theoretical guarantees and extensive numerical studies, demonstrating our approach's effectiveness in improving the accuracy and robustness of various CATE estimation models.", "title_embedding_index": 8265, "title_abs_embedding_index": 8290}, {"title": "Representative Guidance: Diffusion Model Sampling with Consistency", "link_suffix": "/forum?id=gWgaypDBs8", "link": "https://openreview.net/forum?id=gWgaypDBs8", "pdf_link": "https://openreview.net/pdf?id=gWgaypDBs8", "keywords": "generative models, diffusion model", "abstract": "The diffusion sampling process faces a persistent challenge stemming from its incoherence, attributable to varying noise directions across different time steps. \nOur Representative Guidance (RepG) offers a new perspective to handle this issue by reformulating the sampling process with a coherent direction towards a representative target.\nIn this formulation, while the classic classifier guidance improves feature discernment by steering the model away from ambiguous features, it fails to provide a favorable representative target, since the class label is overly compact and leads to sacrificed diversity and the adversarial generation problem.\nIn contrast, we leverage self-supervised representations as the coherent target and treat sampling as a downstream task, which refines image details and corrects errors rather than settling for simpler samples.\nOur representative guidance achieves superior performance and also illustrates the potential of pre-trained self-supervised models in image sampling. Our findings demonstrate that RepG not only substantially enhances vanilla diffusion sampling but also surpasses state-of-the-art benchmarks when combined with the classifier-free guidance. Our code will be released.", "title_embedding_index": 8266, "title_abs_embedding_index": 8291}, {"title": "Gone With the Bits: Revealing Racial Bias in Low-Rate Neural Compression for Facial Images", "link_suffix": "/forum?id=Dolm7rrrQd", "link": "https://openreview.net/forum?id=Dolm7rrrQd", "pdf_link": "https://openreview.net/pdf?id=Dolm7rrrQd", "keywords": "Fairness, Bias, Neural Compression, Phenotype Classification", "abstract": "Neural compression methods are gaining popularity due to their impressive rate-distortion performance and their ability to compress data to extremely small bitrates, below 0.1 bits per pixel (bpp). As deep learning architectures, these models are prone to bias during the training process, potentially leading to unfair outcomes for individuals in different groups. In this paper, we present a general, structured, scalable framework for evaluating bias in neural image compression models. Using this framework, we investigate racial bias in neural compression algorithms by analyzing 7 popular models and their variants. Through this investigation we first demonstrate that traditional distortion metrics are ineffective in capturing bias in neural compression models. Next, we highlight that racial bias is present in all neural compression models and can be captured by examining facial phenotype degradation in image reconstructions. Additionally, we reveal a task-dependent correlation between bias and model architecture. We then examine the relationship between bias and realism in the image reconstructions and demonstrate a trade-off across models. Finally, we show that utilizing a racially balanced training set can reduce bias but is not a sufficient bias mitigation strategy.", "title_embedding_index": 8267, "title_abs_embedding_index": 8292}, {"title": "Do Stochastic, Feel Noiseless: Stable Stochastic Optimization via a Double Momentum Mechanism", "link_suffix": "/forum?id=zCZnEXF3bN", "link": "https://openreview.net/forum?id=zCZnEXF3bN", "pdf_link": "https://openreview.net/pdf?id=zCZnEXF3bN", "keywords": "online convex optimization, stochastic convex optimization", "abstract": "Optimization methods are crucial to the success of machine learning, with Stochastic Gradient Descent (SGD) serving as a foundational algorithm for training models. However, SGD is often sensitive to the choice of the learning rate, which necessitates extensive hyperparameter tuning. In this work, we introduce a new variant of SGD that brings enhanced stability in two key aspects. First, our method allows the use of the same fixed learning rate to attain optimal convergence rates regardless of the noise magnitude, eliminating the need to adjust learning rates between noiseless and noisy settings. Second, our approach achieves these optimal rates over a wide range of learning rates, significantly reducing sensitivity compared to standard SGD, which requires precise learning rate selection.\nOur key innovation is a novel gradient estimator based on a double-momentum mechanism that combines two recent momentum-based techniques. Utilizing this estimator, we design both standard and accelerated algorithms that are robust to the choice of learning rate. Specifically, our methods attain optimal convergence rates in both noiseless and noisy stochastic convex optimization scenarios without the need for learning rate decay or fine-tuning. We also prove that our approach maintains optimal performance across a wide spectrum of learning rates, underscoring its stability and practicality.  Empirical studies further validate the robustness and enhanced stability of our approach.", "title_embedding_index": 8268, "title_abs_embedding_index": 8293}, {"title": "Evolved LLM Schemas for Mid Vision Feedback", "link_suffix": "/forum?id=WDO5hfLZvN", "link": "https://openreview.net/forum?id=WDO5hfLZvN", "pdf_link": "https://openreview.net/pdf?id=WDO5hfLZvN", "keywords": "cifar, objects, classification, computer-vision, llm, context, feedback, imagenet", "abstract": "In this work, we present ELF (Evolving LLM-Based Schemas for Mid-Vision Feedback), a framework that integrates schema evolution with Mid Vision Feedback (MVF) for visual learning. We leverage Large Language Models (LLMs) to automatically generate schemas: executable semantic programs operating over sets of context categories (e.g., \"animate\" or \"inanimate\"). We integrate schemas into visual processing via MVF, a method that utilizes top-down feedback connections to inform mid-level visual processing with high-level contextual knowledge. To optimize these schemas, we use EvoPrompt, an evolutionary algorithm that refines schemas through iterative search, resulting in improvements in accuracy and contextual consistency. We demonstrate the effectiveness of ELF across multiple datasets and multiple architectures.", "title_embedding_index": 8269, "title_abs_embedding_index": 8294}, {"title": "Reward-free World Models for Online Imitation Learning", "link_suffix": "/forum?id=GsGmdxcFNL", "link": "https://openreview.net/forum?id=GsGmdxcFNL", "pdf_link": "https://openreview.net/pdf?id=GsGmdxcFNL", "keywords": "World Models, Imitation Learning, Inverse Reinforcement Learning", "abstract": "Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.", "title_embedding_index": 8270, "title_abs_embedding_index": 8295}, {"title": "Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback", "link_suffix": "/forum?id=RLzeoy4FzP", "link": "https://openreview.net/forum?id=RLzeoy4FzP", "pdf_link": "https://openreview.net/pdf?id=RLzeoy4FzP", "keywords": "Preference Learning, RLHF, AI Feedback, Language Models", "abstract": "Learning from human feedback has enabled the alignment of language models (LMs) with human preferences. However, directly collecting human preferences can be expensive, time-consuming, and can have high variance. An appealing alternative is to distill preferences from LMs as a source of synthetic annotations as they are more consistent, cheaper, and scale better than human annotation; however, they are also prone to biases and errors. In this work, we introduce a routing framework that combines inputs from humans and LMs to achieve better annotation quality, while reducing the total cost of human annotation. The crux of our approach is to identify preference instances that will benefit from human annotations. We formulate this as an optimization problem: given a preference dataset and an evaluation metric, we train a performance prediction model to predict a reward model's performance on an arbitrary combination of human and LM annotations and employ a routing strategy that selects a combination that maximizes predicted performance. We train the performance prediction model on MultiPref, a new preference dataset with 10K instances paired with human and LM labels. We show that the selected hybrid mixture of LM and direct human preferences using our routing framework achieves better reward model performance compared to using either one exclusively. We simulate selective human preference collection on three other datasets and show that our method generalizes well to all three. We analyze features from the routing model to identify characteristics of instances that can benefit from human feedback, e.g., prompts with a moderate safety concern or moderate intent complexity. We release the dataset, annotation platform, and source code used in this study to foster more efficient and accurate preference collection in the future.", "title_embedding_index": 8271, "title_abs_embedding_index": 8296}, {"title": "Scaling and evaluating sparse autoencoders", "link_suffix": "/forum?id=tcsZt9ZNKD", "link": "https://openreview.net/forum?id=tcsZt9ZNKD", "pdf_link": "https://openreview.net/pdf?id=tcsZt9ZNKD", "keywords": "interpretability, sparse autoencoders, superposition, scaling laws", "abstract": "Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.", "title_embedding_index": 8272, "title_abs_embedding_index": 8297}, {"title": "Towards characterizing the value of edge embeddings in graph neural networks", "link_suffix": "/forum?id=XrtFVM1f6w", "link": "https://openreview.net/forum?id=XrtFVM1f6w", "pdf_link": "https://openreview.net/pdf?id=XrtFVM1f6w", "keywords": "graph neural networks, theory, representational power, communication complexity, memory tradeoffs, edge embeddings", "abstract": "Graph neural networks (GNNs) are the dominant approach to solving machine learning problems defined over graphs. Despite much theoretical and empirical work in recent years, our understanding of finer-grained aspects of architectural design for GNNs remains impoverished. In this paper, we consider the benefits of architectures that maintain and update edge embeddings. On the theoretical front, under a suitable computational abstraction for a layer in the model, as well as memory constraints on the embeddings, we show that there are natural tasks on graphical models for which architectures leveraging edge embeddings can be much shallower. Our techniques are inspired by results on time-space tradeoffs in theoretical computer science. Empirically, we show architectures that maintain edge embeddings almost always improve on their node-based counterparts---frequently significantly so in topologies that have \"hub\" nodes.", "title_embedding_index": 8273, "title_abs_embedding_index": 8298}, {"title": "Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension ability", "link_suffix": "/forum?id=gsShHPxkUW", "link": "https://openreview.net/forum?id=gsShHPxkUW", "pdf_link": "https://openreview.net/pdf?id=gsShHPxkUW", "keywords": "Large Language Models, Causality, Interpretability", "abstract": "Large language models (LLMs) have shown remarkable capability in natural language tasks, yet debate persists on whether they truly comprehend deep structure (i.e., core semantics) or merely rely on surface structure (e.g., presentation format). Prior studies observe that LLMs' performance declines when intervening on surface structure, arguing their success relies on surface structure recognition. However, surface structure sensitivity does not prevent deep structure comprehension. Rigorously evaluating LLMs' capability requires analyzing both, yet deep structure is often overlooked. To this end, we assess LLMs' comprehension ability using causal mediation analysis, aiming to fully discover the capability of using both deep and surface structures. Specifically, we formulate the comprehension of deep structure as direct causal effect (DCE) and that of surface structure as indirect causal effect (ICE), respectively. To address the non-estimability of original DCE and ICE --- stemming from the infeasibility of isolating mutual influences of deep and surface structures, we develop the corresponding quantifiable surrogates, including approximated DCE (ADCE) and approximated ICE (AICE). We further apply the ADCE to evaluate a series of mainstream LLMs (and the one with random weights), showing that most of them exhibit deep structure comprehension ability, which grows along with the prediction accuracy. Comparing ADCE and AICE demonstrates closed-source LLMs (e.g., GPT) rely more on deep structure, while open-source LLMs (e.g., Llama) are more surface-sensitive, which decreases with model scale. Theoretically, ADCE is a bidirectional evaluation, which measures both the sufficiency and necessity of deep structure changes in causing output variations, thus offering a more comprehensive assessment than accuracy, a common evaluation in LLMs. Our work provides new insights into LLMs' deep structure comprehension and offers novel methods for LLMs evaluation. The code for our project is available athttps://anonymous.4open.science.", "title_embedding_index": 8274, "title_abs_embedding_index": 8299}]