[{"title": "Probabilistic Hash Embeddings for Temporal Tabular Data Streams", "link_suffix": "/forum?id=WesOWkjJmg", "link": "https://openreview.net/forum?id=WesOWkjJmg", "pdf_link": "https://openreview.net/pdf?id=WesOWkjJmg", "keywords": "hash embedding, Bayesian online learning, tabular data, dynamic vocabulary", "abstract": "We study temporal tabular data-streams (TTD) where each observation has both categorical and numerical values, and where the universe of distinct categorical items is not known upfront and can even grow unboundedly over time. Such data is common in many large-scale systems, such as user activity in computer system logs and scientific experiment records. Feature hashing is commonly used as a pre- processing step to map the categorical items into a known universe, before doing representation learning (Coleman et al., 2024; Desai et al., 2022). However, these methods have been developed and evaluated for the offline or batch settings. In this paper, we consider the pre-processing step of hashing before representation learning in the online setting for TTD. We show that deterministic embeddings suffer from forgetting in online learning with TTD, leading to performance deterioration. To mitigate the issue, we propose a probabilistic hash embedding (PHE) model that treats hash embeddings as stochastic and applies Bayesian online learning to learn incrementally with data. Based on the structure of PHE, we derive a scalable inference algorithm to learn model parameters and infer/update the posteriors of hash embeddings and other latent variables. Our algorithm (i) can handle evolving vocabulary of categorical items, (ii) is adaptive to new items without forgetting old items, (iii) is implementable with a bounded set of parameters that does not grow with the number of distinct observed items on the stream, and (iv) is efficiently implementable both in the offline and the online streaming setting. Experiments in classification, sequence modeling, and recommendation systems with TTD demonstrate the superior performance of PHE compared to baselines.", "title_embedding_index": 20250, "title_abs_embedding_index": 20275}, {"title": "SeRA: Self-Reviewing and Alignment of LLMs using Implicit Reward Margins", "link_suffix": "/forum?id=uIGnuyDSB9", "link": "https://openreview.net/forum?id=uIGnuyDSB9", "pdf_link": "https://openreview.net/pdf?id=uIGnuyDSB9", "keywords": "Preference Alignment, Large Language Models, Implicit Reward Margin, Sample Selection, Preference Bootstrapping", "abstract": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives to Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the preferences used by DAAs are usually collected before alignment training begins and remain unchanged (off-policy). This design leads to two problems where the policy model (1) picks up on spurious correlations in the dataset (as opposed to only learning alignment to human preferences), and (2) overfits to feedback on off-policy trajectories that have less likelihood of being generated by the updated policy model. To address these issues, we introduce Self-Reviewing and Alignment (SeRA), a cost-efficient and effective method that can be readily combined with existing DAAs. SeRA comprises of two components: (1) sample selection using implicit reward margin to alleviate over-optimization on such undesired features, and (2) preference bootstrapping using implicit rewards to augment preference data with updated policy models in a cost-efficient manner. Extensive experiments, including on instruction-following tasks, demonstrate the effectiveness and generality of SeRA in training LLMs with diverse offline preference datasets and and DAAs.", "title_embedding_index": 20251, "title_abs_embedding_index": 20276}, {"title": "Atlas Gaussians Diffusion for 3D Generation", "link_suffix": "/forum?id=H2Gxil855b", "link": "https://openreview.net/forum?id=H2Gxil855b", "pdf_link": "https://openreview.net/pdf?id=H2Gxil855b", "keywords": "3D generation, diffusion, 3D Gaussian Splatting", "abstract": "Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation.", "title_embedding_index": 20252, "title_abs_embedding_index": 20277}, {"title": "Simplifying, Stabilizing and Scaling Continuous-time Consistency Models", "link_suffix": "/forum?id=LyJi5ugyJx", "link": "https://openreview.net/forum?id=LyJi5ugyJx", "pdf_link": "https://openreview.net/pdf?id=LyJi5ugyJx", "keywords": "continuous-time consistency models, diffusion models, fast sampling", "abstract": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512\u00d7512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64\u00d764, and 1.88 on ImageNet 512\u00d7512, narrowing the gap in FID scores with the best existing diffusion models to within 10%.", "title_embedding_index": 20253, "title_abs_embedding_index": 20278}, {"title": "Bidirectional Decoding: Improving Action Chunking via Closed-Loop Resampling", "link_suffix": "/forum?id=qZmn2hkuzw", "link": "https://openreview.net/forum?id=qZmn2hkuzw", "pdf_link": "https://openreview.net/pdf?id=qZmn2hkuzw", "keywords": "Robot Learning, Action Chunking, Test-Time Decoding", "abstract": "Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. Yet, its reported effects on the learned policy are inconsistent: some studies find it crucial for achieving strong results, while others observe decreased performance. In this paper, we first dissect how action chunking impacts the divergence between a learner and a demonstrator. We find that action chunking allows the learner to better capture the temporal dependencies in demonstrations (e.g., latent strategies) but at the cost of reduced reactivity in stochastic environments (e.g., action noise, object motions). To address this tradeoff, we propose Bidirectional Decoding (BID), a test-time inference algorithm that bridges action chunking with closed-loop operations. BID samples multiple predictions at each time step and searches for the optimal one based on two criteria: (i) backward coherence, which favors samples aligned with previous decisions, (ii) forward contrast, which favors samples close to outputs of a stronger policy and distant from those of a weaker policy. By coupling decisions within and across action chunks, BID promotes strong temporal consistency over multiple steps while maintaining high reactivity to unexpected state changes. Experimental results show that BID boosts the performance of two state-of-the-art robot policies across seven simulation benchmarks and two real-world tasks.", "title_embedding_index": 20254, "title_abs_embedding_index": 20279}, {"title": "Segment as You Wish: Free-Form Language-Based Segmentation for Medical Images", "link_suffix": "/forum?id=NtMf8DejbV", "link": "https://openreview.net/forum?id=NtMf8DejbV", "pdf_link": "https://openreview.net/pdf?id=NtMf8DejbV", "keywords": "Medical Image Segmentation, Foundation Models, Equivariance", "abstract": "Medical imaging is crucial for diagnosing a patient\u2019s health condition, and accurate segmentation of these images is essential for isolating regions of interest to ensure precise diagnosis and treatment planning. Existing methods primarily rely on bounding boxes or point-based prompts, while few have explored text-related prompts, despite clinicians often describing their observations and instructions in natural language. To address this gap, we first propose a RAG-based free-form text prompt generator, that leverages the domain corpus to generate diverse and realistic descriptions. Then, we introduce FLanS, a novel medical image segmentation model that handles various free-form text prompts, including professional anatomy-informed queries, anatomy-agnostic position-driven queries, or anatomy-agnostic size-driven queries. Additionally, our model also incorporates a symmetry-aware canonicalization module to ensure consistent, accurate segmentations across varying scan orientations and reduce confusion between the anatomical position of an organ and its appearance in the scan. FLanS is trained on a large-scale dataset of over 100k medical images from 7 public datasets. Comprehensive experiments demonstrate the model\u2019s superior language understanding and segmentation precision, along with a deep comprehension of the relationship between them, outperforming SOTA baselines on both in-domain and out-of-domain datasets.", "title_embedding_index": 20255, "title_abs_embedding_index": 20280}, {"title": "Rigid Body Dynamics Simulation Based on GNNs with Constraints", "link_suffix": "/forum?id=s77FHD4wra", "link": "https://openreview.net/forum?id=s77FHD4wra", "pdf_link": "https://openreview.net/pdf?id=s77FHD4wra", "keywords": "Graph Neural Network; Dynamics Simulation", "abstract": "In recent years, the utilization of Graph Neural Network (GNN)-based methods for simulating complex physical systems has opened new avenues for the fields of computational science and engineering. Despite their success, current GNN-based methods for rigid body dynamic simulation are constrained to relatively simple scenarios, hindering their practical use in industrial settings where complex mechanical structures and interconnected components prevail. These methods face challenges in handling intricate force relationships within rigid bodies, primarily due to the difficulty in obtaining force-related data for objects in industrial environments. To address this, we propose a novel constraint-guided method that incorporates force analysis into GNN-based simulations. The model incorporates computations related to both contact and non-contact forces into the prediction process. Additionally, it imposes physical constraints on the prediction process based on Kane's equations. We have rigorously demonstrated the model's rationality and effectiveness with thorough theoretical demonstration and empirical analysis. \\textit{Codes and anonymous links to the datasets are available in the supplementary materials.", "title_embedding_index": 20256, "title_abs_embedding_index": 20281}, {"title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "link_suffix": "/forum?id=0tMcsHsHgQ", "link": "https://openreview.net/forum?id=0tMcsHsHgQ", "pdf_link": "https://openreview.net/pdf?id=0tMcsHsHgQ", "keywords": "Nasty teacher, Knowledge distillation, Intellectual property protection", "abstract": "A deep neural network (DNN) is said to be undistillable if used as a black-box input-output teacher, it can not be distilled by knowledge distillation (KD) to train a student model so that the distilled student (called knockoff student) outperforms the student trained alone with label smoothing (LS student) in terms of prediction accuracy. To protect intellectual property of DNNs, it is desirable to build undistillable DNNs. To this end, it is first observed that an undistillable DNN may have the trait that each cluster of its output probability distributions in response to all sample instances with the same label should be highly concentrated to the extent that each cluster corresponding to each label should ideally collapse into one probability distribution. Based on this observation and by measuring the concentration of each cluster in terms of conditional mutual information (CMI), a new training method called CMI minimized (CMIM) method is proposed, which trains a DNN by jointly minimizing the conventional cross entropy (CE) loss and the CMI values of all temperature scaled clusters across the entire temperature spectrum. The resulting CMIM model is shown, by extensive experiments, to be undistillable by all tested KD methods existing in the literature. That is, the knockoff students distilled by these KD methods from the CMIM model underperform the respective LS students. In addition, the CMIM model is also shown to performs better than the model trained with the CE loss alone in terms of their own prediction accuracy.", "title_embedding_index": 20257, "title_abs_embedding_index": 20282}, {"title": "6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering", "link_suffix": "/forum?id=sUvBTEYXGt", "link": "https://openreview.net/forum?id=sUvBTEYXGt", "pdf_link": "https://openreview.net/pdf?id=sUvBTEYXGt", "keywords": "3D Gaussian splatting, 6D Gaussian splatting, volumetric rendering", "abstract": "Novel view synthesis has advanced significantly with the development of neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS). However, achieving high quality without compromising real-time rendering remains challenging, particularly for physically-based ray tracing with view-dependent effects. Recently, N-dimensional Gaussians (N-DG) introduced a 6D spatial-angular representation to better incorporate view-dependent effects, but the Gaussian representation and control scheme are sub-optimal. In this paper, we revisit 6D Gaussians and introduce 6D Gaussian Splatting (6DGS), which enhances color and opacity representations and leverages the additional directional information in the 6D space for optimized Gaussian control. Our approach is fully compatible with the 3DGS framework and significantly improves real-time radiance field rendering by better modeling view-dependent effects and fine details. Experiments demonstrate that 6DGS significantly outperforms 3DGS and N-DG, achieving up to a 15.73 dB improvement in PSNR with a reduction of 66.5% Gaussian points compared to 3DGS.", "title_embedding_index": 20258, "title_abs_embedding_index": 20283}, {"title": "LLM Cascade with Multi-Objective Optimal Consideration", "link_suffix": "/forum?id=GI5cgzVjK6", "link": "https://openreview.net/forum?id=GI5cgzVjK6", "pdf_link": "https://openreview.net/pdf?id=GI5cgzVjK6", "keywords": "LLM Cascade; Multi-Objective; On-device Intelligence", "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in understanding and generating natural language. However, their high deployment costs often pose a barrier to practical applications, especially. Cascading local and server models offers a promising solution to this challenge. While existing studies on LLM cascades have primarily focused on the performance-cost trade-off, real-world scenarios often involve more complex requirements. This paper introduces a novel LLM Cascade strategy with Multi-Objective Optimization, enabling LLM cascades to consider additional objectives (e.g., privacy) and better align with the specific demands of real-world applications while maintaining their original cascading abilities. Extensive experiments on three benchmarks validate the effectiveness and superiority of our approach.", "title_embedding_index": 20259, "title_abs_embedding_index": 20284}, {"title": "Regularized Diffusion Modeling for CAD Representation Generation", "link_suffix": "/forum?id=sCGIbhv4Yv", "link": "https://openreview.net/forum?id=sCGIbhv4Yv", "pdf_link": "https://openreview.net/pdf?id=sCGIbhv4Yv", "keywords": "CAD, Diffusion Regularization, 3D Generation", "abstract": "Computer-Aided Design (CAD) has significant practical value in various industrial applications. However, achieving high-quality and diverse shape generation, as well as flexible conditional control, remains a challenge in the field of CAD model generation. To address these issues, we propose CADiffusion, a diffusion-based generative model with a hierarchical latent representation tailored to the complexities of CAD design processes. To enhance the performance and reliability of the model in generating accurate CAD models, we have developed a specialized decoder with regularization strategies that navigate through the noise space of the diffusion model, smoothing the results. This approach not only improves the diversity and quality of the generated CAD models but also enhances their practical applicability, marking a significant advancement in the integration of generative models and automated CAD systems.", "title_embedding_index": 20260, "title_abs_embedding_index": 20285}, {"title": "Towards Universal Certified Robustness with Multi-Norm Training", "link_suffix": "/forum?id=fXb7MgySp8", "link": "https://openreview.net/forum?id=fXb7MgySp8", "pdf_link": "https://openreview.net/pdf?id=fXb7MgySp8", "keywords": "Certified Training, Certified Robustness", "abstract": "Existing certified training methods can only train models to be robust against a certain perturbation type (e.g. $l_\\infty$ or $l_2$). However, an $l_\\infty$ certifiably robust model may not be certifiably robust against $l_2$ perturbation (and vice versa) and also has low robustness against other perturbations (e.g. geometric transformation). To this end, we propose the first multi-norm certified training framework \\textbf{CURE}, consisting of a new $l_2$ deterministic certified training defense and several multi-norm certified training methods, to attain better \\emph{union robustness} when training from scratch or fine-tuning a pre-trained certified model. Further, we devise bound alignment and connect natural training with certified training for better union robustness. Compared with SOTA certified training, \\textbf{CURE} improves union robustness up to $22.8%$ on MNIST, $23.9%$ on CIFAR-10, and $8.0%$ on TinyImagenet. Further, it leads to better generalization on a diverse set of challenging unseen geometric perturbations, up to $6.8%$ on CIFAR-10. Overall, our contributions pave a path towards \\textit{universal certified robustness}.", "title_embedding_index": 20261, "title_abs_embedding_index": 20286}, {"title": "PopAlign: Population-Level Alignment for Fair Text-to-Image Generation", "link_suffix": "/forum?id=KLIN1QdcX4", "link": "https://openreview.net/forum?id=KLIN1QdcX4", "pdf_link": "https://openreview.net/pdf?id=KLIN1QdcX4", "keywords": "text-to-image", "abstract": "Text-to-image (T2I) models achieve high-fidelity generation through extensive training on large datasets. However, these models may unintentionally pick up undesirable biases of their training data, such as over-representation of particular identities in gender or ethnicity neutral prompts. Existing alignment methods such as Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) fail to address this problem effectively because they operate on pairwise preferences consisting of individual \\textit{samples}, while the aforementioned biases can only be measured at a \\textit{population} level. For example, a single sample for the prompt ``doctor\" could be male or female, but a model generating predominantly male doctors even with repeated sampling reflects a gender bias. To address this limitation, we introduce PopAlign, a novel approach for population-level preference optimization, while standard optimization would prefer entire sets of samples over others. We further derive a stochastic lower bound that directly optimizes for individual samples from preferred populations over others for scalable training.Using human evaluation and standard image quality and bias metrics, we show that PopAlign significantly mitigates the bias of pretrained T2I models while largely preserving the generation quality.", "title_embedding_index": 20262, "title_abs_embedding_index": 20287}, {"title": "Jigsaw++: Imagining Complete Shape Priors for Object Reassembly", "link_suffix": "/forum?id=qHVUdP1EEU", "link": "https://openreview.net/forum?id=qHVUdP1EEU", "pdf_link": "https://openreview.net/pdf?id=qHVUdP1EEU", "keywords": "Fracture Reassembly, Object Reassembly, Generative Model", "abstract": "The automatic assembly problem has attracted increasing interest due to its complex challenges that involve 3D representation. This paper introduces Jigsaw++, a novel generative method designed to tackle the multifaceted challenges of reconstruction for the reassembly problem. Existing approach focusing primarily on piecewise information for both part and fracture assembly, often overlooking the integration of complete object prior. Jigsaw++ distinguishes itself by learning a category-agnostic shape prior of complete objects. It employs the proposed \"retargeting\" strategy that effectively leverages the output of any existing assembly method to generate complete shape reconstructions. This capability allows it to function orthogonally to the current methods. Through extensive evaluations on Breaking Bad dataset and PartNet, Jigsaw++ has demonstrated its effectiveness, reducing reconstruction errors and enhancing the precision of shape reconstruction, which sets a new direction for future reassembly model developments.", "title_embedding_index": 20263, "title_abs_embedding_index": 20288}, {"title": "Structure Learning for Unfaithful Distributions: The Minimal Dependence Faithfulness", "link_suffix": "/forum?id=or8wkKoBP4", "link": "https://openreview.net/forum?id=or8wkKoBP4", "pdf_link": "https://openreview.net/pdf?id=or8wkKoBP4", "keywords": "Bayesian networks, Causality, Faithfulness, Structure learning", "abstract": "Causality detection is to identify the ``true'' directed acyclic graph (DAG) of a causal model from the joint probability distribution of the observed variables.\nAlgorithms such as PC and its modified versions perform this task under the restrictive faithfulness assumption, that is the DAG encodes all conditional independencies imposed by the distribution. \nHowever, all existing algorithms fail to detect the simple structure where a variable is the XOR of several Bernoulli variables, violating faithfulness. We generalize this type of unfaithfulness that appears in other, non-XOR, examples and define the \\emph{minimal dependence} of a given variable $X$ as the set of variables, such that $X$ is independent of each variable in the set but depends on at least one of them, the \\emph{dependent member} if conditioned on the remainder of the set.\nMinimal dependencies of size at least two violate faithfulness. Consequently, we relax faithfulness to \\emph{minimal dependence faithfulness}, restricting the neighbors of a node to its dependent members, and impose \\emph{minimal orientation faithfulness} that generalizes the orientation rules under faithfulness.\nWe then determine the structure of the dependent members of a node $X$ in the true DAG and show that they are connected to $X$ either directly or indirectly by a collider. \nFinally, we provide a sound and complete modification of the PC algorithm to detect this kind of unfaithfulness and output all possible candidates for the true DAG.", "title_embedding_index": 20264, "title_abs_embedding_index": 20289}, {"title": "Semantically Consistent Video Inpainting with Conditional Diffusion Models", "link_suffix": "/forum?id=h7fZvaU93L", "link": "https://openreview.net/forum?id=h7fZvaU93L", "pdf_link": "https://openreview.net/pdf?id=h7fZvaU93L", "keywords": "diffusion models, video inpainting, conditional generative modeling", "abstract": "Current state-of-the-art methods for video inpainting typically rely on optical flow or attention-based approaches to inpaint masked regions by propagating visual information across frames. While such approaches have led to significant progress on standard benchmarks, they struggle with tasks that require the synthesis of novel content that is not present in other frames. In this paper, we reframe video inpainting as a conditional generative modeling problem and present a framework for solving such problems with conditional video diffusion models. We introduce inpainting-specific sampling schemes which capture crucial long-range dependencies in the context, and devise a novel method for conditioning on the known pixels in incomplete frames. We highlight the advantages of using a generative approach for this task, showing that our method is capable of generating diverse, high-quality inpaintings and synthesizing new content that is spatially, temporally, and semantically consistent with the provided context.", "title_embedding_index": 20265, "title_abs_embedding_index": 20290}, {"title": "Contrastive Learners Are Semantic Learners", "link_suffix": "/forum?id=6EadiKkfgR", "link": "https://openreview.net/forum?id=6EadiKkfgR", "pdf_link": "https://openreview.net/pdf?id=6EadiKkfgR", "keywords": "contrastive learning, self-supervised learning, embedding", "abstract": "In this work, we explore the definition of semantic equivalence to establish a connection between contrastive tasks and their downstream counterparts. Specifically, we investigate when a contrastive dataset can learn representations that encode semantic relations for a specific downstream task. In our analysis, we recover a surprising hypothesis resembling the distributional one\u2014dubbed distributional alignment hypothesis. Under this assumption, we demonstrate that a simple contrastive learning procedure can generate representations that encode semantic relations for the downstream task. Furthermore, we support the theory with a series of experiments designed to test the presented intuitions.", "title_embedding_index": 20266, "title_abs_embedding_index": 20291}, {"title": "Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs", "link_suffix": "/forum?id=5GuhYMgaap", "link": "https://openreview.net/forum?id=5GuhYMgaap", "pdf_link": "https://openreview.net/pdf?id=5GuhYMgaap", "keywords": "Reasoning, LLM, Inductive, Deductive", "abstract": "Reasoning encompasses two typical types: deductive reasoning and inductive reasoning. Despite extensive research into the reasoning capabilities of Large Language Models (LLMs), most studies have failed to rigorously differentiate between inductive and deductive reasoning, leading to a blending of the two. This raises an essential question: In LLM reasoning, which poses a greater challenge - deductive or inductive reasoning? While the deductive reasoning capabilities of LLMs, (i.e. their capacity to follow instructions in reasoning tasks), have received considerable attention, their abilities in true inductive reasoning remain largely unexplored due to the inseparability of the two types of reasoning in most of the tasks. To delve into the true inductive reasoning capabilities of LLMs, we propose a novel framework, SolverLearner. This framework enables LLMs to learn the underlying function (i.e., $y = f_w(x)$), that maps input data points $(x)$ to their corresponding output values $(y)$, using only in-context examples. By focusing on inductive reasoning and separating it from LLM-based deductive reasoning, we can isolate and investigate inductive reasoning of LLMs in its pure form via SolverLearner. Our observations reveal that LLMs demonstrate remarkable inductive reasoning capabilities through SolverLearner, achieving near-perfect performance with ACC of 1 in most cases. Surprisingly, despite their strong inductive reasoning abilities, LLMs tend to relatively lack deductive reasoning capabilities, particularly in tasks involving ``counterfactual'' reasoning.", "title_embedding_index": 20267, "title_abs_embedding_index": 20292}, {"title": "Credit-based self organizing maps: training deep topographic networks with minimal performance degradation", "link_suffix": "/forum?id=wMgr7wBuUo", "link": "https://openreview.net/forum?id=wMgr7wBuUo", "pdf_link": "https://openreview.net/pdf?id=wMgr7wBuUo", "keywords": "Computer vision, Neuroscience, Convolutional Networks, topographical organization, self-organizing maps, functional organization", "abstract": "In the primate neocortex, neurons with similar function are often found to be spatially close. Kohonen's self-organizing map (SOM) has been one of the most influential approaches for simulating brain-like topographical organization in artificial neural network models. However, integrating these maps into deep neural networks with multitude of layers has been challenging, with self-organized deep neural networks suffering from substantially diminished capacity to perform visual recognition. We identified a key factor leading to the performance degradation in self-organized topographical neural network models: the discord between predominantly bottom-up learning updates in the self-organizing maps, and those derived from top-down, credit-based learning approaches. To address this, we propose an alternative self organization algorithm, tailored to align with the top-down learning processes in deep neural networks. This model not only emulates critical aspects of cortical topography but also significantly narrows the performance gap between non-topographical and topographical models. This advancement underscores the substantial importance of top-down assigned credits in shaping topographical organization. Our findings are a step in reconciling topographical modeling with the functional efficacy of neural network models, paving the way for more intricate and accurate simulations of brain-like neural architectures.", "title_embedding_index": 20268, "title_abs_embedding_index": 20293}, {"title": "CamCo: Camera-Controllable 3D-Consistent Image-to-Video Generation", "link_suffix": "/forum?id=BkJrXT3e5T", "link": "https://openreview.net/forum?id=BkJrXT3e5T", "pdf_link": "https://openreview.net/pdf?id=BkJrXT3e5T", "keywords": "Video Generation, 3D Generation", "abstract": "Recently video diffusion models have emerged as expressive generative tools for high-quality video content creation readily available to general users. However, these models often do not offer precise control over camera poses for video generation, limiting the expression of cinematic language and user control. To address this issue, we introduceCamCo, which allows fine-grained Camera pose Control for image-to-video generation. We equip a pre-trained image-to-video generator with accurately parameterized camera pose input using Pl\u00fccker coordinates. To enhance 3D consistency in the videos produced, we integrate an epipolar attention module in each attention block that enforces epipolar constraints to the feature maps. Additionally, we fine-tune CamCo on real-world videos with camera poses estimated through structure-from-motion algorithms to better synthesize object motion. Our experiments show that CamCo significantly improves 3D consistency and camera control capabilities compared to previous models while effectively generating plausible object motion.", "title_embedding_index": 20269, "title_abs_embedding_index": 20294}, {"title": "Training the Untrainable: Introducing Inductive Bias via Representational Alignment", "link_suffix": "/forum?id=C33p2CNOQ8", "link": "https://openreview.net/forum?id=C33p2CNOQ8", "pdf_link": "https://openreview.net/pdf?id=C33p2CNOQ8", "keywords": "Representational alignment, neural network optimization", "abstract": "We demonstrate that architectures which traditionally are considered to be ill-suited for a task can be trained using inductive biases from another architecture.  Networks are considered untrainable when they overfit, underfit, or converge to poor results even when tuning their hyperparameters. For example, plain fully connected networks overfit on object recognition while deep convolutional networks without residual connections underfit. The traditional answer is to change the architecture to impose some inductive bias, although what that bias is, is unknown. We introduce guidance, where a guide network guides a target network using a neural distance function. The target is optimized to perform well and to match its internal representations, layer-by-layer, to those of the guide; the guide is unchanged. If the guide is trained, this transfers over part of the architectural prior and knowledge of the guide to the target. If the guide is untrained, this transfers over only part of the architectural prior of the guide. In this manner, we can investigate what kinds of priors different architectures place on a fully connected network. We demonstrate that this method overcomes the immediate overfitting of fully connected networks on vision tasks, makes plain CNNs competitive to ResNets, closes much of the gap between plain vanilla RNNs and Transformers, and can even help Transformers learn tasks which RNNs can perform more easily. We also discover evidence that better initializations of fully connected networks likely exist to avoid overfitting. Our method provides a mathematical tool to investigate priors and architectures, and in the long term, may demystify the dark art of architecture creation, even perhaps turning architectures into a continuous optimizable parameter of the network.", "title_embedding_index": 20270, "title_abs_embedding_index": 20295}, {"title": "M-VAR: Decoupled Scale-wise Autoregressive Modeling for High-Quality Image Generation", "link_suffix": "/forum?id=BYoN2c0o6M", "link": "https://openreview.net/forum?id=BYoN2c0o6M", "pdf_link": "https://openreview.net/pdf?id=BYoN2c0o6M", "keywords": "Scale-wise Autoregressive Model", "abstract": "There exists recent work in computer vision, named VAR, that proposes a new autoregressive paradigm for image generation. Diverging from the vanilla next-token prediction, VAR structurally reformulates the image generation into a coarse to fine next-scale prediction. \nIn this paper, we show that this scale-wise autoregressive framework can be effectively decoupled into \\textit{intra-scale modeling}, which captures local spatial dependencies within each scale, and \\textit{inter-scale modeling}, which models cross-scale relationships progressively from coarse-to-fine scales.\nThis decoupling structure allows to rebuild VAR in a more computationally efficient manner. Specifically, for intra-scale modeling --- crucial for generating high-fidelity images --- we retain the original bidirectional self-attention design to ensure comprehensive modeling; for inter-scale modeling, which semantically connects different scales but is computationally intensive, we apply linear-complexity mechanisms like Mamba to substantially reduce computational overhead. \nWe term this new framework M-VAR. Extensive experiments demonstrate that our method outperforms existing models in both image quality and generation speed. For example, our 1.5B model, with fewer parameters and faster inference speed, outperforms the largest VAR-d32-2B. Moreover, our largest model M-VAR-d32 impressively registers 1.78 FID on ImageNet 256$\\times$256 and outperforms the prior-art autoregressive models LlamaGen/VAR by 0.4/0.19 and popular diffusion models LDM/DiT by 1.82/0.49, respectively.", "title_embedding_index": 20271, "title_abs_embedding_index": 20296}, {"title": "An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels", "link_suffix": "/forum?id=tjNf0L8QjR", "link": "https://openreview.net/forum?id=tjNf0L8QjR", "pdf_link": "https://openreview.net/pdf?id=tjNf0L8QjR", "keywords": "locality, convolutional networks, transformers", "abstract": "This work does not introduce a new method. Instead, we present an interesting finding that questions the necessity of the inductive bias of locality in modern computer vision architectures. Concretely, we find that vanilla Transformers can operate by directly treating each individual pixel as a token and achieve highly performant results. This is substantially different from the popular design in Vision Transformer, which maintains the inductive bias from ConvNets towards local neighborhoods (e.g., by treating each 16x16 patch as a token). We showcase the effectiveness of pixels-as-tokens across three well-studied computer vision tasks: supervised learning for classification and regression, self-supervised learning via masked autoencoding, and image generation with diffusion models. Although it's computationally less practical to directly operate on individual pixels, we believe the community must be made aware of this surprising piece of knowledge when devising the next generation of neural network architectures for computer vision.", "title_embedding_index": 20272, "title_abs_embedding_index": 20297}, {"title": "Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models", "link_suffix": "/forum?id=ZGkfoufDaU", "link": "https://openreview.net/forum?id=ZGkfoufDaU", "pdf_link": "https://openreview.net/pdf?id=ZGkfoufDaU", "keywords": "pre-training data detection, large language model", "abstract": "The problem of pre-training data detection for large language models (LLMs) has received growing attention due to its implications in critical issues like copyright violation and test data contamination. Despite improved performance, existing methods (including the state-of-the-art, Min-K%) are mostly developed upon simple heuristics and lack solid, reasonable foundations. In this work, we propose a novel and theoretically motivated methodology for pre-training data detection, named Min-K%++. Specifically, we present a key insight that training samples tend to be local maxima of the modeled distribution along each input dimension through maximum likelihood training, which in turn allow us to insightfully translate the problem into identification of local maxima. Then, we design our method accordingly that works under the discrete distribution modeled by LLMs, whose core idea is to determine whether the input forms a mode or has relatively high probability under the conditional categorical distribution. Empirically, the proposed method achieves new SOTA performance across multiple settings (evaluated with 5 families of 10 models and 2 benchmarks). On the WikiMIA benchmark, Min-K%++ outperforms the runner-up by 6.2% to 10.5% in detection AUROC averaged over five models. On the more challenging MIMIR benchmark, it consistently improves upon reference-free methods while performing on par with reference-based method that requires an extra reference model.", "title_embedding_index": 20273, "title_abs_embedding_index": 20298}, {"title": "Boosting Adversarial Robustness with CLAT: Criticality Leveraged Adversarial Training", "link_suffix": "/forum?id=ZxcMfJzFaZ", "link": "https://openreview.net/forum?id=ZxcMfJzFaZ", "pdf_link": "https://openreview.net/pdf?id=ZxcMfJzFaZ", "keywords": "Adversarial robustness, criticality, Computer vision, Adversarial attacks, defense", "abstract": "Adversarial training (AT) is a common technique for enhancing neural network robustness. Typically, AT updates all trainable parameters, but such comprehensive adjustments can lead to overfitting and increased generalization errors on clean data. Research suggests that fine-tuning specific parameters may be more effective; however, methods for identifying these essential parameters and establishing effective optimization objectives remain unclear and inadequately addressed. We present CLAT, an innovative adversarial fine-tuning algorithm that mitigates adversarial overfitting by integrating \"criticality\" into the training process. Instead of tuning the entire model, CLAT identifies and fine-tunes fewer parameters in robustness-critical layers\u2014those predominantly learning non-robust features\u2014while keeping the rest of the model fixed. Additionally, CLAT employs a dynamic layer selection process that adapts to changes in layer criticality during training. Empirical results demonstrate that CLAT can be seamlessly integrated with existing adversarial training methods, enhancing clean accuracy and adversarial robustness by over 2% compared to baseline approaches.", "title_embedding_index": 20274, "title_abs_embedding_index": 20299}]