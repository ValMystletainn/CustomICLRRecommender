[{"title": "HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models", "link_suffix": "/forum?id=VejUqXsDYa", "link": "https://openreview.net/forum?id=VejUqXsDYa", "pdf_link": "https://openreview.net/pdf?id=VejUqXsDYa", "keywords": "Large Vision Language Model, High Resolution Image Input, Adapter", "abstract": "High-resolution image inputs allow Large Vision-Language Models (LVLMs) to capture finer visual details, improving comprehension. However, the increased training and computational costs associated with such inputs pose significant challenges. A common approach to mitigate these costs involves slicing the input into uniform patches using sliding windows, each aligned with the vision encoder\u2019s input size. While efficient, this method fragments the input, disrupting the continuity of contextual, which negatively impacts cross-patch perception\ntasks. To address these limitations, we proposeHiRes-LLaVA, a novel framework designed to efficiently process high-resolution inputs of any size without altering the original contextual and geometric information. HiRes-LLaVA introduces two key components: (i) a SliceRestore adapter (SRA) that reconstructs sliced patches into their original form, enabling efficient extraction of both global and local\nfeatures through down-up-sampling and convolutional layers, and (ii) a Self-Mining Sampler (SMS) that compresses vision tokens based on internal relationships, preserving original context and positional information while reducing training overhead. To assess the ability of handling context fragmentation, we construct a new benchmark, EntityGrid-QA, consisting of edge-related tasks. Extensive experiments demonstrate the superiority of HiRes-LLaVA on both existing public benchmarks and EntityGrid-QA. For example, with SRA, our method achieves a performance improvement of \u223c 9% over state-of-the-art LVLMs in addressing fragmentation issues. Additionally, our SMS outperforms other visual token downsamplers, while offering comparable efficiency.", "title_embedding_index": 16700, "title_abs_embedding_index": 16725}, {"title": "Trajectory-LLM: A Language-based Data Generator for Trajectory Prediction in Autonomous Driving", "link_suffix": "/forum?id=UapxTvxB3N", "link": "https://openreview.net/forum?id=UapxTvxB3N", "pdf_link": "https://openreview.net/pdf?id=UapxTvxB3N", "keywords": "Trajectory Prediction, Large Language Model, Autonomous Driving", "abstract": "Vehicle trajectory prediction is a crucial aspect of autonomous driving, which requires extensive trajectory data to train prediction models to understand the complex, varied, and unpredictable patterns of vehicular interactions. However, acquiring real-world data is expensive, so we advocate using Large Language Models (LLMs) to generate abundant and realistic trajectories of interacting vehicles efficiently. These models rely on textual descriptions of vehicle-to-vehicle interactions on a map to produce the trajectories. We introduce Trajectory-LLM (Traj-LLM), a new approach that takes brief descriptions of vehicular interactions as input and generates corresponding trajectories. Unlike language-based approaches that translate text directly to trajectories, Traj-LLM uses reasonable driving behaviors to align the vehicle trajectories with the text. This results in an \"interaction-behavior-trajectory\" translation process. We have also created a new dataset, Language-to-Trajectory (L2T), which includes 240K textual descriptions of vehicle interactions and behaviors, each paired with corresponding map topologies and vehicle trajectory segments. By leveraging the L2T dataset, Traj-LLM can adapt interactive trajectories to diverse map topologies. Furthermore, Traj-LLM generates additional data that enhances downstream prediction models, leading to consistent performance improvements across public benchmarks.", "title_embedding_index": 16701, "title_abs_embedding_index": 16726}, {"title": "Robust Prompt Learning For Vision-Language Models With Noisy Labels", "link_suffix": "/forum?id=MpuMza23aL", "link": "https://openreview.net/forum?id=MpuMza23aL", "pdf_link": "https://openreview.net/pdf?id=MpuMza23aL", "keywords": "Vision Language Models, Prompt Learning, Noisy Labels", "abstract": "Recent advancements in vision-language models (VLMs), designed for simultaneous comprehension of vision and language, have demonstrated significant success in achieving zero-shot classification capabilities. However, despite their impressive performance, it is widely acknowledged that fine-tuning is essential to adapt these models to new target tasks. This adaptation process requires the collection of target datasets, which may introduce incorrect labels and greatly compromise the model performance after fine-tuning. In this paper, our objective is to enhance classification fine-tuning performance by leveraging the zero-shot classification capability under a noisy labeled training dataset. We first conduct a detailed exploration of the behavior of the pre-trained VLMs under various classification text prompts, including human-crafted and LLM-crafted visual characteristics. This investigation reveals that VLMs have tilted knowledge towards some classes, and each prompt exhibits varying expertise for each class. Based on these observations, we introduce a robust training method called PoND, which employs a complementary approach across different types of prompts, leveraging the expertise of each class. We systematically compare the efficacy of the proposed algorithm with existing denoising techniques designed for VLMs and substantiate that our proposed algorithm outperforms prior approaches across 11 real-world datasets.", "title_embedding_index": 16702, "title_abs_embedding_index": 16727}, {"title": "LASER: A Neuro-Symbolic Framework for Learning Spatio-Temporal Scene Graphs with Weak Supervision", "link_suffix": "/forum?id=HEXtydywnE", "link": "https://openreview.net/forum?id=HEXtydywnE", "pdf_link": "https://openreview.net/pdf?id=HEXtydywnE", "keywords": "neuro-symbolic, video understanding, spatial-temporal scene graph, weak supervision", "abstract": "Supervised approaches for learning spatio-temporal scene graphs (STSG) from video are greatly hindered due to their reliance on STSG-annotated videos, which are labor-intensive to construct at scale. Is it feasible to instead use readily available video captions as weak supervision? To address this question, we propose LASER, a neuro-symbolic framework to enable training STSG generators using only video captions. LASER employs large language models to first extract logical specifications with rich spatio-temporal semantic information from video captions. LASER then trains the underlying STSG generator to align the predicted STSG with the specification. The alignment algorithm overcomes the challenges of weak supervision by leveraging a differentiable symbolic reasoner and using a combination of contrastive, temporal, and semantics losses. The overall approach efficiently trains low-level perception models to extract a fine-grained STSG that conforms to the video caption. In doing so, it enables a novel methodology for learning STSGs without tedious annotations. We evaluate our method on three video datasets: OpenPVSG, 20BN, and MUGEN. Our approach demonstrates substantial improvements over fully-supervised baselines, achieving a unary predicate prediction accuracy of 27.78% (+12.65%) and a binary recall@5 of 0.42 (+0.22) on OpenPVSG.  Additionally, LASER exceeds baselines by 7% on 20BN and 5.2% on MUGEN in terms of overall predicate prediction accuracy.", "title_embedding_index": 16703, "title_abs_embedding_index": 16728}, {"title": "SimUSER: When Language Models Pretend to Be Believable Users in Recommender Systems", "link_suffix": "/forum?id=rv9c1BqY0L", "link": "https://openreview.net/forum?id=rv9c1BqY0L", "pdf_link": "https://openreview.net/pdf?id=rv9c1BqY0L", "keywords": "Recommender systems, Recommender systems evaluation, agent-based language models", "abstract": "Recommender systems play a central role in numerous real-life applications, yet evaluating their performance remains a significant challenge due to the gap between offline metrics and online behaviors. We introduce SimUSER, an agent framework that serves as believable and cost-effective human proxies for the evaluation of recommender systems. Leveraging the inductive bias of foundation models, SimUSER emulates synthetic users by first identifying self-consistent personas from historical data, enriching user profiles with unique backgrounds and personalities. Then, central to this evaluation are users equipped with persona, memory, perception, and brain modules, engaging in interactions with the recommender system. Specifically, the memory module consists of an episodic memory to log interactions and preferences, and a knowledge-graph memory that captures relationships between users and items. The perception module enables visual-driven reasoning, while the brain module translates retrieved information into actionable plans. We demonstrate through ablation studies that the components of our agent architecture contribute to the believability of user behavior. Across a set of recommendation domains, SimUSER exhibits closer alignment with genuine humans than prior state-of-the-art, both at micro and macro levels. Additionally, we conduct insightful experiments to explore the effects of thumbnails on click rates, the exposure effect, and the impact of reviews on user engagement. The source code is released athttps://github.com/SimUSER-paper/SimUSER.", "title_embedding_index": 16704, "title_abs_embedding_index": 16729}, {"title": "Adaptive Priors from Learning Trajectories for Function-Space Bayesian Neural Networks", "link_suffix": "/forum?id=Pdh1yMqwev", "link": "https://openreview.net/forum?id=Pdh1yMqwev", "pdf_link": "https://openreview.net/pdf?id=Pdh1yMqwev", "keywords": "Function-space Bayesian neural network, Function-space variational inference, Gaussian process, Stochastic weight averaging gaussian (SWAG))", "abstract": "Tractable Function-space Variational Inference (T-FVI) provides a way to estimate the function-space Kullback-Leibler (KL) divergence between a random prior function and its posterior. This allows the optimization of the function-space KL divergence via Stochastic Gradient Descent (SGD) and thus simplifies the training of function-space Bayesian Neural Networks (BNNs). However, function-space BNNs on high-dimensional datasets typically require deep neural networks (DNN) with numerous parameters, and thus defining suitable function-space priors remains challenging. For instance, the Gaussian Process (GP) prior suffers from scalability issues, and DNNs do not provide a clear way to set appropriate weight parameters to achieve meaningful function-space priors. To address this issue, we propose an explicit form of function-space priors that can be easily integrated into widely-used DNN architectures, while adaptively incorporating different levels of uncertainty based on the function's inputs. To achieve this, we consider DNNs as Bayesian last-layer models to\nobtain the explicit mean and variance functions of our prior. The parameters of these explicit functions are determined using the weight statistics over the learning trajectory. Our empirical experiments show improved uncertainty estimation in image classification, transfer learning, and UCI regression tasks.", "title_embedding_index": 16705, "title_abs_embedding_index": 16730}, {"title": "SubgoalXL: Subgoal-based Expert Learning for Theorem Proving", "link_suffix": "/forum?id=mb2rHLcKN5", "link": "https://openreview.net/forum?id=mb2rHLcKN5", "pdf_link": "https://openreview.net/pdf?id=mb2rHLcKN5", "keywords": "theorem proving, subgoal-based proofs, expert learning", "abstract": "Formal theorem proving, a field at the intersection of mathematics and computer science, has seen renewed interest with advancements in large language models (LLMs). This paper introduces SubgoalXL, a novel approach that synergizes subgoal-based proofs with expert learning to enhance LLMs' capabilities in formal theorem proving within the Isabelle environment. SubgoalXL addresses two critical challenges: the scarcity of specialized mathematics and theorem-proving data, and the need for improved multi-step reasoning abilities in LLMs. By optimizing data efficiency and employing subgoal-level supervision, SubgoalXL extracts richer information from limited human-generated proofs. The framework integrates subgoal-oriented proof strategies with an expert learning system, iteratively refining formal statement, proof, and subgoal generators. Leveraging the Isabelle environment's advantages in subgoal-based proofs, SubgoalXL achieves a new state-of-the-art performance of 56.1% in Isabelle on the standard miniF2F dataset, marking an absolute improvement of 4.9%. Notably, SubgoalXL successfully solves 41 AMC12, 9 AIME, and 3 IMO problems from miniF2F. These results underscore the effectiveness of maximizing limited data utility and employing targeted guidance for complex reasoning in formal theorem proving, contributing to the ongoing advancement of AI reasoning capabilities.", "title_embedding_index": 16706, "title_abs_embedding_index": 16731}, {"title": "COPU: Recognizing Time Series' Heterogeneity In Stacked Neural Network", "link_suffix": "/forum?id=eF1i7YTVen", "link": "https://openreview.net/forum?id=eF1i7YTVen", "pdf_link": "https://openreview.net/pdf?id=eF1i7YTVen", "keywords": "Neural Network, Time Series, Regression, Nonlinear Modeling, Natural Gradient", "abstract": "Neural networks (NNs) have been widely studied in complex fields due to their remarkable capacity for nonlinear modeling. \nHowever, in the realm of time series analysis, researches indicate that merely stacking NNs does not yield promising nonlinear modeling outputs and hinders model performance. Conventional NN architectures overemphasize homogeneous feature extraction, impeding the learning of diverse features and diminishing their nonlinear modeling capability. To address this gap, we propose the $\\textbf{C}$ross-correlation Enhanced Approximated $\\textbf{O}$rthogonal $\\textbf{P}$rojection $\\textbf{U}$nit (COPU) to quantify and augment the NN's nonlinear modeling capacity. COPU efficiently computes the local cross-correlation characteristics between features, amplifying heterogeneous components while compressing homogeneous ones. By reducing redundant information, COPU facilitates the learning of unique and independent features, thereby enhancing nonlinear modeling capability. Extensive experiments demonstrate that our method achieves superior performance across two real-world regression applications.", "title_embedding_index": 16707, "title_abs_embedding_index": 16732}, {"title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe", "link_suffix": "/forum?id=Lnuy691O8Q", "link": "https://openreview.net/forum?id=Lnuy691O8Q", "pdf_link": "https://openreview.net/pdf?id=Lnuy691O8Q", "keywords": "Supervised Fine-Tuning, Large Language Model", "abstract": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications.", "title_embedding_index": 16708, "title_abs_embedding_index": 16733}, {"title": "Gumbel-Softmax Discretization Constraint,  Differentiable IDS Channel,  and an IDS-Correcting Code for DNA Storage", "link_suffix": "/forum?id=XoZMP9GX9o", "link": "https://openreview.net/forum?id=XoZMP9GX9o", "pdf_link": "https://openreview.net/pdf?id=XoZMP9GX9o", "keywords": "Gumbel-Softmax, IDS Channel, IDS-correcting Code, DNA Storage", "abstract": "Insertion, deletion, and substitution (IDS) error-correcting codes have garnered increased attention with recent advancements in DNA storage technology. However, a universal method for designing IDS-correcting codes across varying channel settings remains underexplored. We present an autoencoder-based method, THEA-code, aimed at efficiently generating IDS-correcting codes for complex IDS channels. In the work, a Gumbel-Softmax discretization constraint is proposed to discretize the features of the autoencoder, and a simulated differentiable IDS channel is developed as a differentiable alternative for IDS operations. These innovations facilitate the successful convergence of the autoencoder, resulting in channel-customized IDS-correcting codes with commendable performance across complex IDS channels.", "title_embedding_index": 16709, "title_abs_embedding_index": 16734}, {"title": "Achieving Optimal Complexity in Decentralized Learning over Row-Stochastic Networks", "link_suffix": "/forum?id=wSozvhEYq7", "link": "https://openreview.net/forum?id=wSozvhEYq7", "pdf_link": "https://openreview.net/pdf?id=wSozvhEYq7", "keywords": "decentralized stochastic optimization, directed graph, row-stochastic matrix, gradient tracking", "abstract": "A key challenge in decentralized optimization is determining the optimal convergence rate and designing algorithms that can achieve it. While this issue has been thoroughly addressed for doubly-stochastic and column-stochastic mixing matrices, the row-stochastic setting remains largely unexplored. This study establishes the first convergence lower bound for decentralized learning over row-stochastic networks. However, developing algorithms to achieve this lower bound is highly challenging due to several factors: (i) the widely used Row-Only gossip protocol,  Pull-Diag, suffers from significant instability in achieving average consensus; (ii)  Pull-Diag-based algorithms are sensitive to data heterogeneity; and (iii) there has been no analysis in nonconvex and stochastic settings to date. This work addresses these deficiencies by proposing and analyzing a new gossip protocol called Pull-Sum, along with its gradient tracking extension, Pull-Sum-GT. The Pull-Sum protocol mitigates the instability issues of Pull-Diag, while Pull-Sum-GT achieves the first linear speedup convergence rate without relying on data heterogeneity assumptions. Additionally, we introduce a multi-step strategy that enables Pull-Sum-GT to match the established lower bound up to logarithmic factors, demonstrating its near-optimal performance and the tightness of our established lower bound. Experiments validate our theoretical results.", "title_embedding_index": 16710, "title_abs_embedding_index": 16735}, {"title": "Real-Time Video Generation with Pyramid Attention Broadcast", "link_suffix": "/forum?id=hDBrQ4DApF", "link": "https://openreview.net/forum?id=hDBrQ4DApF", "pdf_link": "https://openreview.net/pdf?id=hDBrQ4DApF", "keywords": "Diffusion Acceleration, DiT, Video Generation, Efficient, Real-Time, Parallelism, Sequence Parallelism", "abstract": "We present Pyramid Attention Broadcast (PAB), a real-time, high quality and training-free approach for DiT-based video generation. Our method is founded on the observation that attention difference in the diffusion process exhibits a U-shaped pattern, indicating significant redundancy. We mitigate this by broadcasting attention outputs to subsequent steps in a pyramid style. It applies different broadcast strategies to each attention based on their variance for best efficiency. We further introduce broadcast sequence parallel for more efficient distributed inference. PAB demonstrates superior results across three models compared to baselines, achieving real-time generation for up to 720p videos. We anticipate that our simple yet effective method will serve as a robust baseline and facilitate future research and application for video generation.", "title_embedding_index": 16711, "title_abs_embedding_index": 16736}, {"title": "MD-LSM: An Efficient Tool for Real-time Monitoring Linear Separability of Hidden-layer Outputs of Deep Networks", "link_suffix": "/forum?id=hoEanaoP4i", "link": "https://openreview.net/forum?id=hoEanaoP4i", "pdf_link": "https://openreview.net/pdf?id=hoEanaoP4i", "keywords": "linear separability, deep network, hidden layer, Minkowski difference", "abstract": "Many studies have shown that evaluating the linear separability of hidden-layer outputs plays a key role in understanding the working mechanism of deep networks. However, it is still challenging to develop the linear separability measure (LSM) that satisfies all of the following requirements: 1) it should be an absolute measure; 2) it should be insensitive to the outliers; and 3) its computational cost should be low for real-time monitoring the behavior of each hidden layer. In this paper, we propose the Minkowski difference-based linear separability measures (MD-LSMs) that just meet the first two requirements. Moreover, we also introduce an approximate calculation method to significantly decrease their computation costs with only a slight precision sacrifice. As an application example, we conduct the experiments on the real-time monitoring for the hidden-layer behaviors of several popular deep networks, and show that the outputs of the hidden layers adjacent to the output layer have higher linear separability degrees. We also observe that the change of linear separability degree of hidden layers (especially the ones are adjacent to the output layers) are in sync with the change of the training accuracy of the entire network. It implies that the linear separability of some important hidden layers can be treated as a performance criterion to characterize the network's training behavior. The relevant theoretical discussion also validates this finding.", "title_embedding_index": 16712, "title_abs_embedding_index": 16737}, {"title": "Improving Editability in Compositional Image Diffusion with Layer-wise Memory", "link_suffix": "/forum?id=U91wktaOXS", "link": "https://openreview.net/forum?id=U91wktaOXS", "pdf_link": "https://openreview.net/pdf?id=U91wktaOXS", "keywords": "Diffusion Model, Image Generation, Image Editing, Interactive Generation", "abstract": "Enhancing the editability of the Text-to-Image model has been a challenge. Hence, various Layout-to-Image and Editing methods have attempted to provide spatial control of generated objects. However, most approaches in this category constrain the layout to be in flat canvas arrangements or do not support edits with complex entanglement between objects. We introduce an improved approach to support comprehensive layouts with denoted instance orders. We first introduce layer-wise memory to store the latent of previous denoising steps and prompt embeddings for each object being generated in the specified layer. By utilizing the memorized previous step's latents, Background Consistency Guidance helps to maintain the latent of the previous editing step. Moreover, we suggest Multi-Query Disentanglement in the cross-attention mechanism to effectively separate new objects from existing prompt embeddings and help the new object naturally adapt to the background.  Finally, we present a benchmark dataset to overcome the limitations of the prior layout-to-image benchmarks that do not evaluate semantic alignment or address the interactive editing scenario. Through extensive testing against various baselines, we demonstrate that our method generates complex compositions through enhanced editing capability.", "title_embedding_index": 16713, "title_abs_embedding_index": 16738}, {"title": "Mitigating Suboptimality of Deterministic Policy Gradients in Complex Q-functions", "link_suffix": "/forum?id=aeY0CAOnca", "link": "https://openreview.net/forum?id=aeY0CAOnca", "pdf_link": "https://openreview.net/pdf?id=aeY0CAOnca", "keywords": "Deterministic Policy Gradients, Complex Q-functions, Actor-critic RL, TD3.", "abstract": "In reinforcement learning, off-policy actor-critic approaches like DDPG and TD3 are based on the deterministic policy gradient. Herein, the Q-function is trained from off-policy environment data and the actor (policy) is trained to maximize the Q-function via gradient ascent. We observe that in complex tasks like dexterous manipulation and restricted locomotion, the Q-value is a complex function of action, having several local optima or discontinuities. This poses a challenge for gradient ascent to traverse and makes the actor prone to get stuck at local optima. To address this, we introduce a new actor architecture that combines two simple insights: (i) use multiple actors and evaluate the Q-value maximizing action, and (ii) learn surrogates to the Q-function that are simpler to optimize with gradient-based methods.\nWe evaluate tasks such as restricted locomotion, dexterous manipulation, and large discrete-action space recommender systems and show that our actor finds more optimal actions and outperforms alternate actor architectures.", "title_embedding_index": 16714, "title_abs_embedding_index": 16739}, {"title": "QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing", "link_suffix": "/forum?id=aUZEeb2yvK", "link": "https://openreview.net/forum?id=aUZEeb2yvK", "pdf_link": "https://openreview.net/pdf?id=aUZEeb2yvK", "keywords": "Multi-task Reinforcement Learning, Behavior Sharing", "abstract": "Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeling data between tasks.  In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors.  We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm.  Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments.   Videos are available athttps://sites.google.com/view/qmp-mtrl.", "title_embedding_index": 16715, "title_abs_embedding_index": 16740}, {"title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model", "link_suffix": "/forum?id=SI2hI0frk6", "link": "https://openreview.net/forum?id=SI2hI0frk6", "pdf_link": "https://openreview.net/pdf?id=SI2hI0frk6", "keywords": "multimodal foundation model, multimodal generation and understanding, diffusion, next token prediction", "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.\nTransfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.\nWe pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.\nOur experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.\nBy introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.\nWe further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds.", "title_embedding_index": 16716, "title_abs_embedding_index": 16741}, {"title": "How Social is It? A Benchmark for LLMs' Capabilities in Multi-user Multi-turn Social Agent Tasks", "link_suffix": "/forum?id=RuwAMoFxzG", "link": "https://openreview.net/forum?id=RuwAMoFxzG", "pdf_link": "https://openreview.net/pdf?id=RuwAMoFxzG", "keywords": "LLM, Multi-user, Social agent, Evaluation", "abstract": "Expanding the application of large language models (LLMs) to societal life, instead of primary function only as auxiliary assistants to communicate with only one person at a time, necessitates LLMs' capabilities to independently play roles in multi-user, multi-turn social agent tasks within complex social settings. However, currently the capability has not been systematically measured with available benchmarks. To address this gap, we first introduce an agent task leveling framework grounded in sociological principles. Concurrently, we propose a novel benchmark, How Social Is It (we call it HSII below), designed to assess LLM's social capabilities in comprehensive social agents tasks and benchmark representative models. HSII comprises four stages: format parsing, target selection, target switching conversation, and stable conversation, which collectively evaluate the communication and task completion capabilities of LLMs within realistic social interaction scenarios dataset, HSII-Dataset. The dataset is derived step by step from news dataset. We perform an ablation study by doing clustering to the dataset. Additionally, we investigate the impact of chain of thought (COT) method on enhancing LLMs' social performance. Since COT cost more computation, we further introduce a new statistical metric, COT-complexity, to quantify the efficiency of certain LLMs with COTs for specific social tasks and strike a better trade-off between measurement of correctness and efficiency. Various results of our experiments demonstrate that our benchmark is well-suited for evaluating social skills in LLMs.", "title_embedding_index": 16717, "title_abs_embedding_index": 16742}, {"title": "DM3D: Parameter-Efficient and Lossless 3D Object Detection via Distortion Minimization", "link_suffix": "/forum?id=tvjcGkBf0g", "link": "https://openreview.net/forum?id=tvjcGkBf0g", "pdf_link": "https://openreview.net/pdf?id=tvjcGkBf0g", "keywords": "3d Object Detection, Model Pruning, Model Compression", "abstract": "Recent advancements in 3D deep learning have garnered significant attention, given their superior performance in fields like AR/VR, autonomous driving, and robotics. \nHowever, as the models and point cloud data continues to scale up, managing computational and memory demands becomes a critical challenge, particularly for real-world applications with strict latency and energy requirements.\nPrevious methods have primarily focused on reducing computational costs and memory usage by addressing spatial redundancy, \\textit{i.e.}, filtering out irrelevant points or voxels. In contrast, this work presents a novel post-training weight pruning technique tailored specifically for 3D object detection.\nOur approach stands out in two key ways: (1) it operates independently from existing point cloud sparsification methods, targeting redundant parameters in pre-trained models that minimally affect both spatial accuracy and detection confidence (collectively referred to as \"detection distortion\"), and (2) it provides a flexible, plug-and-play framework compatible with other sparsity schemes including spatial sparsity and with any 3D detection model.\nOur method reduces detection distortion by employing a second-order Taylor approximation to identify layer-wise sparsity, allowing for a substantial reduction in model complexity without sacrificing detection accuracy. \nTo efficiently manage the necessary second-order information, we devised a lightweight algorithm to gather Hessian information, followed by dynamic programming to optimize layer-wise sparsity allocation.\nExtensive experiments on the KITTI, nuScenes, and ONCE datasets validate the effectiveness of our approach, where we not only preserve detection performance but also notice enhancement while significantly reducing computational overhead. \nNoticeably, we achieve FLOPs reductions for Centerpoint model of as much as $\\mathbf{3.89}\\times$ and $\\mathbf{3.01}\\times$ on ONCE and nuScenes datasets respectively, without noticeable loss in mean Average Precision (mAP), and at most $\\mathbf{1.65}\\times$ reduction \\textbf{losslessly} for PVRCNN model on the ONCE dataset, thus pushing the boundaries of state-of-the-art performance.", "title_embedding_index": 16718, "title_abs_embedding_index": 16743}, {"title": "Primphormer: Leveraging Primal Representation for Graph Transformers", "link_suffix": "/forum?id=8yZ3hh4gg9", "link": "https://openreview.net/forum?id=8yZ3hh4gg9", "pdf_link": "https://openreview.net/pdf?id=8yZ3hh4gg9", "keywords": "Graph Transformers, self-attention, primal-dual representation, kernel methods", "abstract": "Graph Transformers (GTs) have emerged as a promising approach for graph representation learning. Despite their successes, the quadratic complexity of GTs limits scalability on large graphs due to their pair-wise computations. To fundamentally reduce the computational burden of GTs, we introduce Primphormer, a primal-dual framework that interprets the self-attention mechanism on graphs as a dual representation and then models the corresponding primal representation with linear complexity. Theoretical evaluations demonstrate that Primphormer serves as a universal approximator for functions on both sequences and graphs, showcasing its strong expressive power. Extensive experiments on various graph benchmarks demonstrate that Primphormer achieves competitive empirical results while maintaining a more user-friendly memory and computational costs.", "title_embedding_index": 16719, "title_abs_embedding_index": 16744}, {"title": "Preference Optimization for Reasoning with Pseudo Feedback", "link_suffix": "/forum?id=jkUp3lybXf", "link": "https://openreview.net/forum?id=jkUp3lybXf", "pdf_link": "https://openreview.net/pdf?id=jkUp3lybXf", "keywords": "Large Language Model, Code Generation, Natural Language Reasoning, Reinforcement Learning", "abstract": "Preference optimization techniques, such as Direct Preference Optimization (DPO), are frequently employed to enhance the reasoning capabilities of large language models (LLMs) in domains like mathematical reasoning and coding, typically following supervised fine-tuning. These methods rely on high-quality labels for reasoning tasks to generate preference pairs; however, the availability of reasoning datasets with human-verified labels is limited.\nIn this study, we introduce a novel approach to generate pseudo feedback for reasoning tasks by framing the labeling of solutions to reason problems as an evaluation against associated \\emph{test cases}. \nWe explore two forms of pseudo feedback based on test cases: one generated by frontier LLMs and the other by extending self-consistency to multi-test-case.\nWe conduct experiments on both mathematical reasoning and coding tasks using pseudo feedback for preference optimization, and observe improvements across both tasks. Specifically, using Mathstral-7B as our base model, we improve MATH results from 58.3 to 68.6, surpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and College Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3, respectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.3 on LiveCodeBench (from 21.1), surpassing Claude-3-Haiku.", "title_embedding_index": 16720, "title_abs_embedding_index": 16745}, {"title": "Regularization by Texts for Latent Diffusion Inverse Solvers", "link_suffix": "/forum?id=TtUh0TOlGX", "link": "https://openreview.net/forum?id=TtUh0TOlGX", "pdf_link": "https://openreview.net/pdf?id=TtUh0TOlGX", "keywords": "Inverse problem, Text regularization, Diffusion model", "abstract": "The recent development of diffusion models has led to significant progress in solving inverse problems by leveraging these models as powerful generative priors. However, challenges persist due to the ill-posed nature of such problems, often arising from ambiguities in measurements or intrinsic system symmetries. To address this, we introduce a novel latent diffusion inverse solver, regularization by text (TReg), inspired by the human ability to resolve visual ambiguities through perceptual biases. TReg integrates textual descriptions of preconceptions about the solution during reverse diffusion sampling, dynamically reinforcing these descriptions through null-text optimization, which we refer to as adaptive negation. Our comprehensive experimental results demonstrate that TReg effectively mitigates ambiguity in inverse problems, improving both accuracy and efficiency.", "title_embedding_index": 16721, "title_abs_embedding_index": 16746}, {"title": "MLP-KAN: Unifying Deep Representation and Function Learning", "link_suffix": "/forum?id=F9JZiGradI", "link": "https://openreview.net/forum?id=F9JZiGradI", "pdf_link": "https://openreview.net/pdf?id=F9JZiGradI", "keywords": "representational learning, functional learning, unified model", "abstract": "Recent advancements in both representation learning and function learning have demonstrated substantial promise across diverse domains of artificial intelligence. However, the effective integration of these paradigms poses a significant challenge, particularly in cases where users must manually decide whether to apply a representation learning or function learning model based on dataset characteristics. To address this issue, we introduce MLP-KAN, a unified method designed to eliminate the need for manual model selection. By integrating Multi-Layer Perceptrons (MLPs) for representation learning and Kolmogorov-Arnold Networks (KANs) for function learning within a Mixture-of-Experts (MoE) architecture, MLP-KAN dynamically adapts to the specific characteristics of the task at hand, ensuring optimal performance. Embedded within a transformer-based framework, our work achieves remarkable results on four widely-used datasets across diverse domains. Extensive experimental evaluation demonstrates its superior versatility, delivering competitive performance across both deep representation and function learning tasks. These findings highlight the potential of MLP-KAN to simplify the model selection process, offering a comprehensive, adaptable solution across various domains.", "title_embedding_index": 16722, "title_abs_embedding_index": 16747}, {"title": "MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts", "link_suffix": "/forum?id=t7P5BUKcYv", "link": "https://openreview.net/forum?id=t7P5BUKcYv", "pdf_link": "https://openreview.net/pdf?id=t7P5BUKcYv", "keywords": "Mixture of Experts, Large Language Models, Efficient Foundation Models", "abstract": "In this work, we aim to simultaneously enhance the effectiveness and efficiency of Mixture-of-Experts (MoE) methods. To achieve this, we propose MoE++, a general and heterogeneous MoE framework that integrates both Feed-Forward Network (FFN) and zero-computation experts. Specifically, we introduce three types of zero-computation experts: the zero expert, copy expert, and constant expert, which correspond to discard, skip, and replace operations, respectively. This design offers three key advantages: (i)Low Computing Overhead: Unlike the uniform mixing mechanism for all tokens within vanilla MoE, MoE++ allows each token to engage with a dynamic number of FFNs, be adjusted by constant vectors, or even skip the MoE layer entirely. (ii)High Performance: By enabling simple tokens to utilize fewer FFN experts, MoE++ allows more experts to focus on challenging tokens, thereby unlocking greater performance potential than vanilla MoE. (iii)Deployment Friendly: Given that zero-computation experts have negligible parameters, we can deploy all zero-computation experts on each GPU, eliminating the significant communication overhead and expert load imbalance associated with FFN experts distributed across different GPUs. Moreover, we leverage gating residuals, enabling each token to consider the pathway taken in the previous layer when selecting the appropriate experts. Extensive experimental results demonstrate that MoE++ achieves better performance while delivering 1.1$\\sim$2.1$\\times$ expert forward throughput compared to a vanilla MoE model of the same size, which lays a solid foundation for developing advanced and efficient MoE-related models.", "title_embedding_index": 16723, "title_abs_embedding_index": 16748}, {"title": "WATER-GS: Watermark-embedded 3D Gaussian Splatting via Plug-and-play Decoder", "link_suffix": "/forum?id=H48OMCCiI7", "link": "https://openreview.net/forum?id=H48OMCCiI7", "pdf_link": "https://openreview.net/pdf?id=H48OMCCiI7", "keywords": "3D gaussian splatting, digital watermarking", "abstract": "3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene representation, providing rapid rendering speeds and high fidelity. As 3DGS gains prominence, safeguarding its intellectual property becomes increasingly crucial since 3DGS could be used to imitate unauthorized scene creations and raise copyright issues. Existing watermarking methods for implicit NeRFs cannot be directly applied to 3DGS due to its explicit representation and real-time rendering process, leaving watermarking for 3DGS largely unexplored. In response, we propose WATER-GS, a novel method designed to protect 3DGS copyrights through a plug-and-play strategy. First, we introduce a pre-trained watermark decoder, treating raw 3DGS generative modules as potential watermark encoders to ensure imperceptibility. Additionally, we implement novel 3D distortion layers to enhance the robustness of the embedded watermark against common real-world distortions of point cloud data. Comprehensive experiments and ablation studies demonstrate that WATER-GS effectively embeds imperceptible and robust watermarks into 3DGS without compromising rendering efficiency and quality. Our experiments indicate that the 3D distortion layers can yield up to a 20% improvement in accuracy rate. Notably, our method is adaptable to different 3DGS variants, including 3DGS compression frameworks and 2D Gaussian splatting.", "title_embedding_index": 16724, "title_abs_embedding_index": 16749}]