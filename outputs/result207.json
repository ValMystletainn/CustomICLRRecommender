[
    {
        "title": "Investigating Mixture Policies in Entropy-Regularized Actor-Critic",
        "link_suffix": "/forum?id=LIz0cBmHn5",
        "link": "https://openreview.net/forum?id=LIz0cBmHn5",
        "pdf_link": "https://openreview.net/pdf?id=LIz0cBmHn5",
        "keywords": "policy parameterization, entropy regularization, actor-critic, policy optimization, exploration, continuous control, reinforcement learning",
        "abstract": "We study mixture policies in entropy-regularized reinforcement learning. Mixture policies offer greater flexibility than base policies like Gaussians, which we show theoretically provides improved solution quality and robustness to the entropy scale. Despite these potential benefits, they are rarely used for algorithms like Soft Actor-Critic, potentially due to the fact that Gaussians are easily reparameterized to get lower variance gradient updates, but mixtures are not. We fill this gap, introducing reparameterization gradient estimators for the mixture policy. Through extensive experiments on environments from classic control, MuJoCo, the DeepMind Control Suite and a suite of randomly generated bandits, our results show that mixture policies explore more efficiently in tasks with unshaped rewards (across entropy scales), while performing comparably to base policies in tasks with shaped rewards, and are more robust to multimodal critic surfaces."
    },
    {
        "title": "ToddlerDiffusion: Interactive Structured Image Generation with Cascaded Schr\u00f6dinger Bridge",
        "link_suffix": "/forum?id=Jszf4et48m",
        "link": "https://openreview.net/forum?id=Jszf4et48m",
        "pdf_link": "https://openreview.net/pdf?id=Jszf4et48m",
        "keywords": "Generative AI, Diffusion models, Cascaded models, Interpretability, Image Editing",
        "abstract": "Diffusion models break down the challenging task of generating data from high-dimensional distributions into a series of easier denoising steps. Inspired by this paradigm, we propose a novel approach that extends the diffusion framework into modality space, decomposing the complex task of RGB image generation into simpler, interpretable stages. Our method, termed {\\papernameAbbrev}, cascades modality-specific models, each responsible for generating an intermediate representation, such as contours, palettes, and detailed textures, ultimately culminating in a high-quality RGB image.\nInstead of relying on the naive LDM concatenation conditioning mechanism to connect the different stages together, we employ Schr\"odinger Bridge to determine the optimal transport between different modalities.\nAlthough employing a cascaded pipeline introduces more stages, which could lead to a more complex architecture, each stage is meticulously formulated for efficiency and accuracy, surpassing Stable-Diffusion (LDM) performance.\nModality composition not only enhances overall performance but enables emerging proprieties such as consistent editing, interaction capabilities, high-level interpretability, and faster convergence and sampling rate. \nExtensive experiments on diverse datasets, including LSUN-Churches, ImageNet, CelebHQ, and LAION-Art, demonstrate the efficacy of our approach, consistently outperforming state-of-the-art methods.\nFor instance, {\\papernameAbbrev} achieves notable efficiency, matching LDM performance on LSUN-Churches while operating 2$\\times$ faster with a 3$\\times$ smaller architecture.\nThe project website is available at:\n\\href{https://toddlerdiffusion.github.io/website/}{$https://toddlerdiffusion.github.io/website/$}"
    },
    {
        "title": "ATLAS: Automatic Local Symmetry Discovery",
        "link_suffix": "/forum?id=VXKt1lwysO",
        "link": "https://openreview.net/forum?id=VXKt1lwysO",
        "pdf_link": "https://openreview.net/pdf?id=VXKt1lwysO",
        "keywords": "local symmetry discovery, symmetry discovery, equivariance, gauge equivariant neural network, Lie theory",
        "abstract": "Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they are unable to capture the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (ATLAS), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate ATLAS is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks."
    },
    {
        "title": "Identification of Mean-Field Dynamics using Transformers",
        "link_suffix": "/forum?id=XazJbPgLcV",
        "link": "https://openreview.net/forum?id=XazJbPgLcV",
        "pdf_link": "https://openreview.net/pdf?id=XazJbPgLcV",
        "keywords": "Mean field dynamics, Transformers, Universal approximation",
        "abstract": "This paper investigates the use of transformer architectures to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including gas dynamics, opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. We demonstrate that transformers, which inherently possess permutation equivariance, are well-suited for approximating these dynamics. Specifically, we prove that if a finite-dimensional transformer can effectively approximate the finite-dimensional vector field governing the particle system, then the expected output of this transformer provides a good approximation for the infinite-dimensional mean-field vector field. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer. We validate our theoretical findings through numerical simulations on the Cucker-Smale model for flocking, and the mean-field system for training two-layer neural networks."
    },
    {
        "title": "Learning to Filter Outlier Edges in Global SfM",
        "link_suffix": "/forum?id=HTjJpwY5AU",
        "link": "https://openreview.net/forum?id=HTjJpwY5AU",
        "pdf_link": "https://openreview.net/pdf?id=HTjJpwY5AU",
        "keywords": "3D reconstruction, Structure-from-Motion, Translation Averaging",
        "abstract": "This paper introduces a novel approach to improve camera position estimation in global Structure-from-Motion (SfM) frameworks by filtering inaccurate pose graph edges, representing relative translation estimates, before applying translation averaging. In SfM, pose graph vertices represent cameras and edges relative poses (rotation and translation) between cameras. We formulate the edge filtering problem as a vertex filtering in the dual graph -- a line graph where the vertices stem from edges in the original graph, and the edges from cameras. Exploiting such a representation, we frame the problem as a binary classification over nodes in the dual graph. To learn such a classification and find outlier edges, we employ a Transformer architecture-based technique. To address the challenge of memory overflow often caused by converting to a line graph, we introduce a clustering-based graph processing approach, enabling the application of our method to arbitrarily large pose graphs. The proposed method outperforms existing relative translation filtering techniques in terms of final camera position accuracy and can be seamlessly integrated with any other filter. The code will be made public."
    },
    {
        "title": "Is Complex Query Answering Really Complex?",
        "link_suffix": "/forum?id=2FMdrDp3zI",
        "link": "https://openreview.net/forum?id=2FMdrDp3zI",
        "pdf_link": "https://openreview.net/pdf?id=2FMdrDp3zI",
        "keywords": "complex query answering, knowledge graph, multi-hop reasoning",
        "abstract": "Complex query answering (CQA) on knowledge graphs (KGs) is gaining momentum as a challenging reasoning task. In this paper, we show that the current benchmarks for CQA are not really complex, and the way they are built distorts our perception of progress in this field. For example, we find that in these benchmarks most queries (up to 98% for some query types) can be reduced to simpler problems, e.g., link prediction, where only one link needs to be predicted. The performance of state-of-the-art CQA models drops significantly when such models are evaluated on queries that cannot be reduced to easier types. Thus, we propose a set of more challenging benchmarks, composed of queries that require models to reason over multiple hops and better reflect the construction of real-world KGs. In a systematic empirical investigation, the new benchmarks show that current methods leave much to be desired from current CQA methods."
    },
    {
        "title": "Amortized SHAP values via sparse Fourier function approximation",
        "link_suffix": "/forum?id=jh7JQkgWGe",
        "link": "https://openreview.net/forum?id=jh7JQkgWGe",
        "pdf_link": "https://openreview.net/pdf?id=jh7JQkgWGe",
        "keywords": "interpretability, explainability, shap values",
        "abstract": "SHAP values -- a.k.a.~SHapley Additive exPlanations -- are a popular local feature-attribution method widely used in interpretable and explainable AI. We tackle the problem of efficiently computing these values. We cover both the model-agnostic (black-box) setting, where one only has query access to the model and also the case of (ensembles of) trees where one has access to the structure of the tree.  For both the black-box and the tree setting we propose a two-stage approach for estimating SHAP values.Our algorithm's first step harnesses recent results showing that many real-world predictors have a spectral bias that allows us to either exactly represent (in the case of ensembles of decision trees), or efficiently approximate them (in the case of neural networks) using a compact Fourier representation. \nFor the case of trees, given access to the tree structure, one can extract the Fourier representation using a simple recursive algorithm.\nFor the black-box setting, given query access to the black-box function, we utilize a sparse Fourier approximation algorithm to efficiently extract its compact Fourier approximation.In the second step of the algorithm, we use the Fourier representation to exactly compute SHAP values. The second step is computationally very cheap because firstly, the representation is compact and secondly, we prove that there exists a closed-form expression for SHAP values for the Fourier basis functions. Furthermore, the expression we derive effectively ``linearizes'' the computation into a simple summation and is amenable to parallelization on multiple cores or a GPU. Since the function approximation (first step) is only done once, it allows us to produce Shapley values in an amortized way. We show speedups compared to relevant baseline methods equal levels of accuracy for both the tree and black-box settings. Moreover, this approach introduces a reliable and fine-grained continuous trade-off between computation and accuracy through the sparsity of the Fourier approximation, a feature previously unavailable in all black-box methods."
    },
    {
        "title": "HDFlow: Enhancing LLM Complex Problem-Solving with Hybrid Thinking and Dynamic Workflows",
        "link_suffix": "/forum?id=wGqf7YMF8R",
        "link": "https://openreview.net/forum?id=wGqf7YMF8R",
        "pdf_link": "https://openreview.net/pdf?id=wGqf7YMF8R",
        "keywords": "Large Language Models (LLMs), Complex Reasoning, Hybrid Thinking, Symbolic Reasoning",
        "abstract": "Despite recent advancements in large language models (LLMs), their performance on complex reasoning problems requiring multi-step thinking and combining various skills is still limited. To address this, we propose a novel framework HDFlow for complex reasoning with LLMs that combines fast and slow thinking modes in an adaptive manner. Our approach consists of two key components: 1) a new approach for slow, deliberate reasoning called Dynamic Workflow, which automatically decomposes complex problems into more manageable sub-tasks and dynamically designs a workflow to assemble specialized LLM or symbolic reasoning tools to solve sub-tasks; 2) Hybrid Thinking, a general framework that dynamically combines fast and slow thinking based on problem complexity. \nFinally, we propose an easy-to-scale method for automatically synthesizing a large-scale dataset of 27K challenging reasoning problems for complex reasoning and a hybrid thinking tuning method that trains smaller LLMs on this dataset to internalize the fast/slow hybrid reasoning strategies.\nExperiments on four reasoning benchmark datasets demonstrate that our slow thinking with dynamic workflows significantly outperforms Chain-of-Thought, and hybrid thinking achieves the highest accuracy while providing an effective balance between computational efficiency and performance. Fine-tuning using our hybrid thinking approach also significantly boosts the complex reasoning capabilities of open-source language models. The results showcase the promise of slow thinking, dynamic workflows, and hybrid thinking in expanding the frontier of complex problem-solving with LLMs."
    },
    {
        "title": "MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs",
        "link_suffix": "/forum?id=7EhS3YBxjY",
        "link": "https://openreview.net/forum?id=7EhS3YBxjY",
        "pdf_link": "https://openreview.net/pdf?id=7EhS3YBxjY",
        "keywords": "Multimodal LLM; Instruction Following; Benchmark",
        "abstract": "Effective evaluation of Multimodal Large Language Models (MLLMs) is essential for understanding their capabilities and limitations. In this paper, we introduce MIA-Bench, a benchmark designed to assess MLLMs\u2019 ability to strictly adhere to complex instructions. Our benchmark comprises a diverse set of 400 image-prompt pairs, each crafted to challenge the models\u2019 compliance with layered instructions in generating accurate and contextually appropriate responses. Evaluation results from a wide array of state-of-the-art MLLMs reveal significant variations in performance, highlighting areas for improvement in instruction fidelity. Additionally, we create extra training data and explore supervised fine-tuning and direct preference optimization to enhance the models\u2019 ability to strictly follow instructions without compromising performance on other tasks. We hope this benchmark not only serves as a tool for measuring MLLM adherence to instructions, but also guides future developments in MLLM training methods."
    },
    {
        "title": "SGD with memory: fundamental properties and stochastic acceleration",
        "link_suffix": "/forum?id=Qzd4BloAjQ",
        "link": "https://openreview.net/forum?id=Qzd4BloAjQ",
        "pdf_link": "https://openreview.net/pdf?id=Qzd4BloAjQ",
        "keywords": "Stochastic Gradient Descent, SGD, spectral power laws, acceleration, effective learning rate, quadratic problems, stability, Heavy Ball, momentum",
        "abstract": "An important open problem is the theoretically feasible acceleration of mini-batch SGD-type algorithms on quadratic problems with power-law spectrum. In the non-stochastic setting, the optimal exponent $\\xi$ in the loss convergence $L_t\\sim C_Lt^{-\\xi}$ is double that in plain GD and is achievable using Heavy Ball (HB) with a suitable schedule; this no longer works in the presence of mini-batch noise. We address this challenge by considering first-order methods with an arbitrary fixed number $M$ of auxiliary velocity vectors (memory-$M$ algorithms). We first prove an equivalence between two forms of such algorithms and describe them in terms of suitable characteristic polynomials. Then we develop a general expansion of the loss in terms ofsignal and noise propagators. Using it, we show that losses of stationary stable memory-$M$ algorithms always retain the exponent $\\xi$ of plain GD, but can have different constants $C_L$ depending on theireffective learning ratethat generalizes that of HB. We prove that in memory-1 algorithms we can make $C_L$ arbitrarily small while maintaining stability. As a consequence, we propose a memory-1 algorithm with a time-dependent schedule that we show heuristically and experimentally to improve the exponent $\\xi$ of plain SGD."
    },
    {
        "title": "Policy Gradient with Tree Expansion",
        "link_suffix": "/forum?id=mTgMLy2iPt",
        "link": "https://openreview.net/forum?id=mTgMLy2iPt",
        "pdf_link": "https://openreview.net/pdf?id=mTgMLy2iPt",
        "keywords": "Reinforcement Learning, Policy Gradient, Tree Expansion, Softmax",
        "abstract": "Policy gradient methods are notorious for having a large variance and high sample complexity. To mitigate this, we introduce SoftTreeMax---a generalization of softmax that employs planning. In SoftTreeMax, we extend the traditional logits with the multi-step discounted cumulative reward, topped with the logits of future states. We analyze SoftTreeMax and explain how tree expansion helps to reduce its gradient variance. We prove that the variance depends on the chosen tree-expansion policy. Specifically, we show that the closer the induced transitions are to being state-independent, the stronger the variance decay. With approximate forward models, we prove that the resulting gradient bias diminishes with the approximation error while retaining the same variance reduction. Ours is the first result to bound the gradient bias for an approximate model. In a practical implementation of SoftTreeMax we utilize a parallel GPU-based simulator for fast and efficient tree expansion. Using this implementation in Atari, we show that SoftTreeMax reduces the gradient variance by three orders of magnitude. This leads to better sample complexity and improved performance compared to distributed PPO."
    },
    {
        "title": "On Orchestrating Personalized LLMs",
        "link_suffix": "/forum?id=nKVYQOgD0q",
        "link": "https://openreview.net/forum?id=nKVYQOgD0q",
        "pdf_link": "https://openreview.net/pdf?id=nKVYQOgD0q",
        "keywords": "Personalized LLM, Reinforcement Learning from Personalized Feedback",
        "abstract": "This paper presents a novel approach to aligning large language models (LLMs) with individual human preferences, sometimes referred to as Reinforcement Learning fromPersonalizedHuman Feedback (RLPHF). Given stated preferences along multiple dimensions, such as helpfulness, conciseness, or humor, the goal is to create an LLM -- without completely re-training -- that best adheres to this specification. Starting from specialized expert LLMs, each trained for one such particular preference dimension, we propose a black-box method that merges their outputs on a per-token level. We train a lightweight Preference Control Model (PCM) that dynamically translates the preference description and current context into next-token prediction weights. By combining the expert models' outputs at the token level, our approach dynamically generates text that optimizes the given preference. Empirical tests show that our method matches or surpasses existing preference merging techniques, providing a scalable, efficient alternative to fine-tuning LLMs for individual personalization."
    },
    {
        "title": "Distance-Based Tree-Sliced Wasserstein Distance",
        "link_suffix": "/forum?id=OiQttMHwce",
        "link": "https://openreview.net/forum?id=OiQttMHwce",
        "pdf_link": "https://openreview.net/pdf?id=OiQttMHwce",
        "keywords": "tree-sliced wasserstein distance, optimal transport, equivariance",
        "abstract": "To overcome computational challenges of Optimal Transport (OT), several variants of Sliced Wasserstein (SW) has been developed in the literature. These approaches exploit the closed-form expression of the univariate OT by projecting measures onto one-dimensional lines. However, projecting measures onto low-dimensional spaces can lead to a loss of topological information. Tree-Sliced Wasserstein distance on Systems of Lines (TSW-SL) has emerged as a promising alternative that replaces these lines with a more intricate structure called tree systems. The tree structures enhance the ability to capture topological information of the metric while preserving computational efficiency. However, at the core of TSW-SL, the splitting maps, which serve as the mechanism for pushing forward measures onto tree systems, focus solely on the position of the measure supports while disregarding the projecting domains. Moreover, the specific splitting map used in TSW-SL leads to a metric that is not invariant under Euclidean transformations, a typically expected property for OT on Euclidean space. In this work, we propose a novel class of splitting maps that generalizes the existing one studied in TSW-SL enabling the use of all positional information from input measures, resulting in a novel Distance-based Tree-Sliced Wasserstein (Db-TSW) distance. In addition, we introduce a simple tree sampling process better suited for Db-TSW, leading to an efficient GPU-friendly implementation for tree systems, similar to the original SW. We also provide a comprehensive theoretical analysis of proposed class of splitting maps to verify the injectivity of the corresponding Radon Transform, and demonstrate that Db-TSW is an Euclidean invariant metric. We empirically show that Db-TSW significantly improves accuracy compared to recent SW variants while maintaining low computational cost via a wide range of experiments on gradient flows, image style transfer, and generative models."
    },
    {
        "title": "Transfer Learning Under High-Dimensional Graph Convolutional Regression Model for Node Classification",
        "link_suffix": "/forum?id=wRbSdbGyfj",
        "link": "https://openreview.net/forum?id=wRbSdbGyfj",
        "pdf_link": "https://openreview.net/pdf?id=wRbSdbGyfj",
        "keywords": "Transfer learning, Node Classification, Graph Convolution, High-Dimensional",
        "abstract": "Node classification is a fundamental task, but obtaining node classification labels can be challenging and expensive in many real-world scenarios.  Transfer learning has emerged as a promising solution to address this challenge by leveraging knowledge from source domains to enhance learning in a target domain.  Existing transfer learning methods for node classification primarily focus on integrating Graph Convolutional Networks (GCNs) with various transfer learning techniques. While these approaches have shown promising results, they often suffer from a lack of theoretical guarantees, restrictive conditions, and high sensitivity to hyperparameter choices. To overcome these limitations, we employ a Graph Convolutional Multinomial Logistic Lasso Regression (GCR) model which simplifies GCN, and develop a transfer learning method called Trans-GCR based on the GCR model. We provide theoretical guarantees of the estimate obtained under the GCR model in high-dimensional settings. Moreover, Trans-GCR demonstrates superior empirical performance, has a low computational cost, and requires fewer hyperparameters than existing methods."
    },
    {
        "title": "Continuous Speech Synthesis using per-token Latent Diffusion",
        "link_suffix": "/forum?id=wkmCbrrDQN",
        "link": "https://openreview.net/forum?id=wkmCbrrDQN",
        "pdf_link": "https://openreview.net/pdf?id=wkmCbrrDQN",
        "keywords": "Speech Synthesis, Continuous Sequence Modeling, Latent Diffusion",
        "abstract": "The success of autoregressive transformer models with discrete tokens has inspired quantization-based approaches for continuous modalities, though these often limit reconstruction quality.\nWe therefore introduce SALAD, a per-token latent diffusion model for zero-shot text-to-speech, that operates on continuous representations.\nSALAD builds upon the recently proposed expressive diffusion head for image generation, and extends it to generate variable-length outputs. \nOur approach utilizes semantic tokens for providing contextual information and determining the stopping condition.\nWe suggest three continuous variants for our method, extending popular discrete speech synthesis techniques. \nAdditionally, we implement discrete baselines for each variant and conduct a comparative analysis of discrete versus continuous speech modeling techniques.\nOur results demonstrate that both continuous and discrete approaches are highly competent, and that SALAD achieves a superior intelligibility score while obtaining speech quality and speaker similarity on par with the ground-truth audio."
    },
    {
        "title": "Large Language Model Alignment via Inverse Reinforcement Learning from Demonstrations",
        "link_suffix": "/forum?id=0lMhptUGxP",
        "link": "https://openreview.net/forum?id=0lMhptUGxP",
        "pdf_link": "https://openreview.net/pdf?id=0lMhptUGxP",
        "keywords": "Large Language Model Alignment, Alignment from Demonstration",
        "abstract": "Aligning Large Language Models (LLMs) is crucial for enhancing their safety and utility. However, existing methods, primarily based on preference datasets, face challenges such as noisy labels, high annotation costs, and privacy concerns. \nIn this work, we introduceAlignment from Demonstrations(AfD), a novel approach leveraging high-quality demonstration data to overcome these challenges. We formalize AfD within a sequential decision-making framework, highlighting its unique challenge of missing reward signals. Drawing insights from forward and inverse reinforcement learning, we introduce divergence minimization objectives for AfD.\nAnalytically, we elucidate the mass-covering and mode-seeking behaviors of various approaches, explaining when and why certain methods are superior.\nPractically, we propose a computationally efficient algorithm that extrapolates over a tailored reward model for AfD. We validate our key insights through experiments on the Harmless and Helpful tasks, demonstrating their strong empirical performance while maintaining simplicity."
    },
    {
        "title": "Convergence Analysis of the Wasserstein Proximal Algorithm beyond Convexity",
        "link_suffix": "/forum?id=WPz5e5V85k",
        "link": "https://openreview.net/forum?id=WPz5e5V85k",
        "pdf_link": "https://openreview.net/pdf?id=WPz5e5V85k",
        "keywords": "optimization in Wasserstein space, proximal algorithm, gradient flow, sampling, mean-field",
        "abstract": "The proximal algorithm is a powerful tool to minimize nonlinear and nonsmooth functionals in a general metric space. Motivated by the recent progress in studying the training dynamics of the noisy gradient descent algorithm on two-layer neural networks in the mean-field regime, we provide in this paper a simple and self-contained analysis for the convergence of the general-purpose Wasserstein proximal algorithm without assuming geodesic convexity on the objective functional. Under a natural Wasserstein analog of the Euclidean Polyak-{\\L}ojasiewicz inequality, we show that the proximal algorithm achieves an unbiased and linear convergence rate. Our convergence rate improves upon existing rates of the proximal algorithm for solving Wasserstein gradient flows under strong geodesic convexity. We also extend our analysis to the inexact proximal algorithm for geodesically semiconvex objectives. In our numerical experiments, proximal training demonstrates a faster convergence rate than the noisy gradient descent algorithm on mean-field neural networks."
    },
    {
        "title": "How Many Van Goghs Does It Take to Van Gogh? Finding the Imitation Threshold",
        "link_suffix": "/forum?id=zb1UI74kxA",
        "link": "https://openreview.net/forum?id=zb1UI74kxA",
        "pdf_link": "https://openreview.net/pdf?id=zb1UI74kxA",
        "keywords": "Data Interpretability, Privacy, Text-to-Image Models",
        "abstract": "Text-to-image models are trained using large datasets collected by scraping image-text pairs from the internet. These datasets often include private, copyrighted, and licensed material. Training models on such datasets enables them to generate images with such content, which might violate copyright laws and individual privacy. This phenomenon is termed imitation -- generation of images with content that has recognizable similarity to its training images. In this work we study the relationship between a concept's frequency in the training dataset and the ability of a model to imitate it. We seek to determine the point at which a model was trained on enough instances to imitate a concept -- the imitation threshold. We posit this question as a new problem: Finding the Imitation Threshold (FIT) and propose an efficient approach that estimates the imitation threshold without incurring the colossal cost of training multiple models from scratch. We experiment with two domains -- human faces and art styles -- for which we create four datasets, and evaluate three text-to-image models which were trained on two pretraining datasets. Our results reveal that the imitation threshold of these models is in the range of 200-600 images, depending on the domain and the model. The imitation threshold can provide an empirical basis for copyright violation claims and acts as a guiding principle for text-to-image model developers that aim to comply with copyright and privacy laws. We will release the code and data upon publication."
    },
    {
        "title": "Divide and Conform: Unleashing Spatial Filter Atoms for Unsupervised Target Transferability",
        "link_suffix": "/forum?id=57yOS3nIVm",
        "link": "https://openreview.net/forum?id=57yOS3nIVm",
        "pdf_link": "https://openreview.net/pdf?id=57yOS3nIVm",
        "keywords": "Filter Decomposition, Domain Transferability, Efficiency",
        "abstract": "The straightforward fine-tuning of the pre-trained model for the target task, bears the risk of under-utilizing the foundational knowledge accrued by the pre-trained model, resulting in the sub-optimal utilization of transferable knowledge, consequently impeding peak performance on the target task. To address this, we introduce $\\textit{Divide and Conform}$, aimed at augmenting the transferability of pre-trained convolutional neural networks (ConvNets), $\\textit{in the absence of base data}$. This strategy exploits the mathematical equivalence of the convolution operation, conceptualizing it as a two-step process involving spatial-only convolution and channel combination. To achieve this, we decompose ($\\textit{Divide}$) the filters of pre-trained ConvNets into spatial filter atoms (responsible for spatial-only convolution) and their corresponding atom-coefficients (responsible for channel combination). Our observations reveal that solely fine-tuning ($\\textit{Conform}$-ing) the spatial filter atoms, comprising of only a few hundred parameters, renders the transferability of the model efficient, without compromising on the predictive performance. Simultaneously, the static atom-coefficients serve to retain the base (foundational) knowledge from the pre-trained model. We rigorously assess this dual-faceted approach within the demanding and practical framework of cross-domain few-shot learning, showcasing the approach's substantial capability of transferring the knowledge in a parameter-efficient manner."
    },
    {
        "title": "Dynamic Similarity Graph Construction with Kernel Density Estimation",
        "link_suffix": "/forum?id=tra8ktyk0E",
        "link": "https://openreview.net/forum?id=tra8ktyk0E",
        "pdf_link": "https://openreview.net/pdf?id=tra8ktyk0E",
        "keywords": "kernel density estimation, similarity graphs, spectral clustering",
        "abstract": "In the kernel density estimation (KDE) problem, we are given a set  $X$ of data points in $\\mathbb{R}^d$, a kernel function $k: \\mathbb{R}^d \\times \\mathbb{R}^d \\rightarrow \\mathbb{R}$, and a query point $\\mathbf{q} \\in \\mathbb{R}^d$, and the objective is to quickly output an estimate of $\\sum_{\\mathbf{x} \\in X} k(\\mathbf{q}, \\mathbf{x})$.\nIn this paper, we consider $\\textsf{KDE}$ in the dynamic setting, and introduce a data structure that efficiently maintains the estimates for a set of query points as data points are added to $X$ over time.\nBased on this, we design a dynamic data structure that maintains a sparse approximation of the fully connected similarity graph on \n$X$, and develop a fast dynamic spectral clustering algorithm.\nWe further evaluate the effectiveness of our algorithms on both synthetic and real-world datasets."
    },
    {
        "title": "Advancing Cross-Lingual Capabilities for Humanoid Robots: Leveraging Chinese NLP through Pictophonetic Advantages",
        "link_suffix": "/forum?id=gwZ90hFSL2",
        "link": "https://openreview.net/forum?id=gwZ90hFSL2",
        "pdf_link": "https://openreview.net/pdf?id=gwZ90hFSL2",
        "keywords": "Chinese NLP, Cross-Lingual, humanoid robots, multimodal intelligence, SIFT",
        "abstract": "Humanoid robots, as a critical trajectory in the development of artificial intelligence, are poised to play a key role in the era of cross-lingual and multimodal intelligence. This paper explores the unique capabilities of humanoid robots in multilingual processing by harnessing the pictophonetic advantages inherent in the Chinese language. Unlike phonetic languages such as English, Chinese characters encapsulate ideographic, phonetic, and semantic components within a single symbol, providing a rich, multidimensional data source. By analyzing the successful localization of the periodic table in Chinese, this study illustrates how the unique naming conventions used by Chinese chemists bridge scientific and linguistic understanding. It advocates adopting the systematic approach seen in Chinese chemical nomenclature to further advance research in Chinese natural language processing (CNLP). To this end, the Six-Writings Pictophonetic Coding (SWPC) technology is introduced, which constructs efficient character and word matrices to enable humanoid robots to process Chinese language inputs effectively. The integration of SWPC with techniques such as Scale-Invariant Feature Transform (SIFT) and machine learning facilitates multimodal recognition of characters and words, allowing robots to prioritize Chinese information and seamlessly process it in conjunction with other languages. This approach has the potential to significantly enhance natural language understanding and generation in complex Chinese contexts. By drawing insights from Chinese chemical nomenclature, the paper lays a foundation for intelligent cross-lingual interactions, providing a new direction for CNLP research and paving the way for humanoid robots to achieve deeper integration into future intelligent societies."
    },
    {
        "title": "On Differentially Private String Distances",
        "link_suffix": "/forum?id=nbwDsdfJJd",
        "link": "https://openreview.net/forum?id=nbwDsdfJJd",
        "pdf_link": "https://openreview.net/pdf?id=nbwDsdfJJd",
        "keywords": "edit distance, data structure, differential privacy",
        "abstract": "Given a database of bit strings $A_1,\\ldots,A_m\\in \\{0,1\\}^n$, a fundamental data structure task is to estimate the distances between a given query $B\\in \\{0,1\\}^n$ with all the strings in the database. In addition, one might further want to ensure the integrity of the database by releasing these distance statistics in a secure manner. In this work, we propose differentially private (DP) data structures for this type of tasks, with a focus on Hamming and edit distance. On top of the strong privacy guarantees, our data structures are also time- and space-efficient. In particular, our data structure is $\\epsilon$-DP against any sequence of queries of arbitrary length, and for any query $B$ such that the maximum distance to any string in the database is at most $k$, we output $m$ distance estimates. Moreover,For Hamming distance, our data structure answers any query in $\\widetilde O(mk+n)$ time and each estimate deviates from the true distance by at most $\\widetilde O(k/e^{\\epsilon/\\log k})$;For edit distance, our data structure answers any query in $\\widetilde O(mk^2+n)$ time and each estimate deviates from the true distance by at most $\\widetilde O(k/e^{\\epsilon/(\\log k \\log n)})$.For moderate $k$, both data structures support sublinear query operations. We obtain these results via a novel adaptation of the randomized response technique as a bit flipping procedure, applied to the sketched strings."
    },
    {
        "title": "Scaling Channel-Invariant Self-Supervised Learning",
        "link_suffix": "/forum?id=aefNwingnS",
        "link": "https://openreview.net/forum?id=aefNwingnS",
        "pdf_link": "https://openreview.net/pdf?id=aefNwingnS",
        "keywords": "Self Supervised Learning, Vision Transformer, microscopy, channel adaptive",
        "abstract": "Recent advances in self-supervised pretraining of foundation models for natural images have made them a popular choice for various visual systems and applications. Self-supervised strategies have also shown promise in non-RGB scientific imaging domains such as in biology, medical and satellite imagery, but their broader application is hampered by heterogeneity in channel composition and semantics between relevant datasets: two datasets may contain different numbers of channels, and these may reveal distinct aspects of objects of interest. This work explores this challenge in the context of fluorescent microscopy images. We revisit existing deep-learning approaches in that field and build upon the DINOv2 architecture and self-supervised pretraining objective to propose a general purpose feature extractor for biological microscopy images that handles images with varying numbers of channels and reliably generalizes to unseen channel combinations at evaluation time."
    },
    {
        "title": "Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling",
        "link_suffix": "/forum?id=P6IVIoGRRg",
        "link": "https://openreview.net/forum?id=P6IVIoGRRg",
        "pdf_link": "https://openreview.net/pdf?id=P6IVIoGRRg",
        "keywords": "MCMC, Annealed Langevin Monte Carlo, Non-log-concave sampling, Non-asymptotic analysis",
        "abstract": "We consider the outstanding problem of sampling from an unnormalized density that may be non-log-concave and multimodal. To enhance the performance of simple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type have been widely used. However, quantitative theoretical guarantees of these techniques are under-explored. This study takes a first step toward providing a non-asymptotic analysis of annealed MCMC. Specifically, we establish, for the first time, an oracle complexity of $\\widetilde{O}\\left(\\frac{d\\beta^2{\\cal A}^2}{\\varepsilon^6}\\right)$ for the simple annealed Langevin Monte Carlo algorithm to achieve $\\varepsilon^2$ accuracy in Kullback-Leibler divergence to the target distribution $\\pi\\propto{\\rm e}^{-V}$ on $\\mathbb{R}^d$ with $\\beta$-smooth potential $V$. Here, ${\\cal A}$ represents the action of a curve of probability measures interpolating the target distribution $\\pi$ and a readily sampleable distribution."
    },
    {
        "title": "Real Time Macro-Block Rate Control for Task-Aware Video Compression Using Reinforcement Learning",
        "link_suffix": "/forum?id=aQ7qYnY2nF",
        "link": "https://openreview.net/forum?id=aQ7qYnY2nF",
        "pdf_link": "https://openreview.net/pdf?id=aQ7qYnY2nF",
        "keywords": "Video compression, Rate control, Reinforcement Learning, Downstream task",
        "abstract": "Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a  downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression."
    }
]