[{"title": "Statistical Advantages of Perturbing Cosine Router in Mixture of Experts", "link_suffix": "/forum?id=faDMOmnsjx", "link": "https://openreview.net/forum?id=faDMOmnsjx", "pdf_link": "https://openreview.net/pdf?id=faDMOmnsjx", "keywords": "mixture of experts, cosine router, perturbation", "abstract": "The cosine router in Mixture of Experts (MoE) has recently emerged as an attractive alternative to the conventional linear router. Indeed, the cosine router demonstrates favorable performance in image and language tasks and exhibits better ability to mitigate the representation collapse issue, which often leads to parameter redundancy and limited representation potentials. Despite its empirical success, a comprehensive analysis of the cosine router in MoE has been lacking. Considering the least square estimation of the cosine routing MoE, we demonstrate that due to the intrinsic interaction of the model parameters in the cosine router via some partial differential equations, regardless of the structures of the experts, the estimation rates of experts and model parameters can be as slow as $\\mathcal{O}(1/\\log^{\\tau}(n))$ where $\\tau > 0$ is some constant and $n$ is the sample size. Surprisingly, these pessimistic non-polynomial convergence rates can be circumvented by the widely used technique in practice to stabilize the cosine router --- simply adding noises to the $L^2$ norms in the cosine router, which we refer to asperturbed cosine router. Under the strongly identifiable settings of the expert functions, we prove that the estimation rates for both the experts and model parameters under the perturbed cosine routing MoE are significantly improved to polynomial rates. Finally, we conduct extensive simulation studies in both synthetic and real data settings to empirically validate our theoretical results.", "title_embedding_index": 21700, "title_abs_embedding_index": 21725}, {"title": "Warfare: Breaking the Watermark Protection of AI-Generated Content", "link_suffix": "/forum?id=oAGSLx4VEs", "link": "https://openreview.net/forum?id=oAGSLx4VEs", "pdf_link": "https://openreview.net/pdf?id=oAGSLx4VEs", "keywords": "AIGC, Content Watermark, Watermark Remove, Watermark Forge", "abstract": "AI-Generated Content (AIGC) is gaining great popularity, with many emerging commercial services and applications. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images and fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).A promising solution to achieve this goal is watermarking, which adds unique and imperceptible watermarks on the content for service verification and attribution. Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely bypassing the regulation of the service provider. (2) Watermark forging: the adversary can create illegal content with forged watermarks from another user, causing the service provider to make wrong attributions. We propose Warfare, a unified methodology to achieve both attacks in a holistic way. The key idea is to leverage a pre-trained diffusion model for content processing and a generative adversarial network for watermark removal or forging. We evaluate Warfare on different datasets and embedding setups. The results prove that it can achieve high success rates while maintaining the quality of the generated content. Compared to the inference process of existing diffusion model-based attacks, Warfare is 5,050~11,000x faster.", "title_embedding_index": 21701, "title_abs_embedding_index": 21726}, {"title": "Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning", "link_suffix": "/forum?id=fzbmTawTUB", "link": "https://openreview.net/forum?id=fzbmTawTUB", "pdf_link": "https://openreview.net/pdf?id=fzbmTawTUB", "keywords": "3D reconstruction; 3D Inpainting;  Conceptional Diffusion", "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) faces the challenge of effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ the uncertainties to learn a semantic concept of the scene without the masked object and use a diffusion model to fill masked objects in the input images based on the learned concept. Finally, we build a novel 3DGI framework VISTA by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, which includes fast-moving fish as inpainting targets.", "title_embedding_index": 21702, "title_abs_embedding_index": 21727}, {"title": "Time-variant Duo-image Inpainting via Interactive Distribution Transition Estimation", "link_suffix": "/forum?id=VSaJr03e2R", "link": "https://openreview.net/forum?id=VSaJr03e2R", "pdf_link": "https://openreview.net/pdf?id=VSaJr03e2R", "keywords": "Image Inpainting, Diffusion Sampling", "abstract": "In this work, we focus on a novel and practical task, i.e., Time-vAriant duo-iMage inPainting (TAMP). The aim of TAMP is to inpaint two damaged images by leveraging their complementary information, where the two images are captured at the same scene with a significant time gap between them, i.e., time-variant duo-image. Different from existing reference-guided image inpainting, TAMP considered the potential pixel damage and content mismatch of reference images when they are collected from the Internet for real-world applications. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to address this task due to inappropriate image complementation. To address the issue, we propose a novel Interactive Transition Distribution Estimation (ITDE) module that interactively complements the duo-image with semantic consistency and provides refined inputs for the consequent image inpainting process. The designed ITDE is inpainting pipeline independent making it a plug-and-play image complement module. Thus, we further propose the Interactive Transition Distribution-driven Diffusion (ITDiff) model, which integrated ITDE with a SOTA diffusion model, as our final solution for TAMP. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on our TAMP-Street and conventional DPED50k datasets which show our methods consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.", "title_embedding_index": 21703, "title_abs_embedding_index": 21728}, {"title": "Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios", "link_suffix": "/forum?id=PVHoELf5UN", "link": "https://openreview.net/forum?id=PVHoELf5UN", "pdf_link": "https://openreview.net/pdf?id=PVHoELf5UN", "keywords": "unsupervised learning, low-light image enhancement, image denoising", "abstract": "Real-world low-light images often suffer from complex degradations such as local overexposure, low brightness, noise, and uneven illumination. Supervised methods tend to overfit to specific scenarios, while unsupervised methods, though better at generalization, struggle to model these degradations due to the lack of reference images. To address this issue, we propose an interpretable, zero-reference joint denoising and low-light enhancement framework tailored for real-world scenarios. Our method derives a training strategy based on paired sub-images with varying illumination and noise levels, grounded in physical imaging principles and retinex theory. Additionally, we leverage the Discrete Cosine Transform (DCT) to perform frequency domain decomposition in the sRGB space, and introduce an implicit-guided hybrid representation strategy that effectively separates intricate compounded degradations. In the backbone network design, we develop retinal decomposition network guided by implicit degradation representation mechanisms. Extensive experiments demonstrate the superiority of our method. The code will be released soon.", "title_embedding_index": 21704, "title_abs_embedding_index": 21729}, {"title": "Extend Model Merging from Fine-Tuned to Pre-Trained Large Language Models via Weight Disentanglement", "link_suffix": "/forum?id=2pvMZKGYDR", "link": "https://openreview.net/forum?id=2pvMZKGYDR", "pdf_link": "https://openreview.net/pdf?id=2pvMZKGYDR", "keywords": "Model Merging, Large Language Models", "abstract": "Merging Large Language Models (LLMs) aims to amalgamate multiple homologous LLMs into one with all the capabilities. Ideally, any LLMs sharing the same backbone should be mergeable, irrespective of whether they are Fine-Tuned (FT) with minor parameter changes or Pre-Trained (PT) with substantial parameter shifts. However, existing methods often manually assign the model importance, rendering them feasible only for LLMs with similar parameter alterations, such as multiple FT LLMs. The diverse parameter changed ranges between FT and PT LLMs pose challenges for current solutions in empirically determining the optimal combination. In this paper, we make a pioneering effort to broaden the applicability of merging techniques from FT to PT LLMs. We initially examine the efficacy of current methods in merging FT and PT LLMs, discovering that they struggle to deal with PT LLMs. Subsequently, we introduce an approach based on WeIght DisENtanglement (WIDEN) to effectively extend the merging scope, which first disentangles model weights into magnitude and direction components, and then performs adaptive fusion by considering their respective contributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with instruction-following skills) with Sailor (a PT LLM with multilingual abilities) across 7B and 14B model scales. Results reveal that: (1) existing solutions usually fail when merging Sailor, either losing both abilities or only retaining instruction-following skills; (2) WIDEN successfully injects the multilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in Southeast Asian languages, achieving enhancements in the fundamental capabilities. In light of previous research, we also merge multiple 13B FT LLMs and observe that WIDEN achieves a balanced amalgamation of instruction following, mathematical reasoning, and code generation skills.", "title_embedding_index": 21705, "title_abs_embedding_index": 21730}, {"title": "Learning Clustering-based Prototypes for Compositional Zero-Shot Learning", "link_suffix": "/forum?id=eE2PXlNydB", "link": "https://openreview.net/forum?id=eE2PXlNydB", "pdf_link": "https://openreview.net/pdf?id=eE2PXlNydB", "keywords": "Compositional Zero-Shot Learning, Prototype Learning, Representation Disentanglement", "abstract": "Learning primitive (i.e., attribute and object) concepts from seen compositions is the primary challenge of Compositional Zero-Shot Learning (CZSL). Existing CZSL solutions typically rely on oversimplified data assumptions, e.g., modeling each primitive with a single centroid primitive presentation, ignoring the natural diversities of the attribute (resp. object) when coupled with different objects (resp. attribute). In this work, we develop ClusPro, a robust clustering-based prototype mining framework for CZSL that defines the conceptual boundaries of primitives through a set of diversified prototypes. Specifically, ClusPro conducts within-primitive clustering on the embedding space for automatically discovering and dynamically updating prototypes. To learn high-quality embeddings for discriminative prototype construction, ClusPro repaints a well-structured and independent primitive embedding space, ensuring intra-primitive separation and inter-primitive decorrelation through prototype-based contrastive learning and decorrelation learning. Moreover, ClusPro effectively performs prototype clustering in a non-parametric fashion without the introduction of additional learnable parameters or computational budget during testing. Experiments on three benchmarks demonstrate ClusPro outperforms various top-leading CZSL solutions under both closed-world and open-world settings. Code will be released.", "title_embedding_index": 21706, "title_abs_embedding_index": 21731}, {"title": "Adversarial Attack Robust dataset pruning", "link_suffix": "/forum?id=mORwTTZfWq", "link": "https://openreview.net/forum?id=mORwTTZfWq", "pdf_link": "https://openreview.net/pdf?id=mORwTTZfWq", "keywords": "dataset condensation", "abstract": "Dataset pruning, while effective for reducing training data size, often leads to models vulnerable to adversarial attacks. This paper introduces a novel approach to create adversarially robust coresets. We first theoretically analyze how existing pruning methods result in non-smooth loss surfaces, increasing susceptibility to attacks. To address this, we propose two key innovations: (1) a Frequency-Selective Excitation Network (FSE-Net) that dynamically selects important frequency components, smoothing the loss surface while reducing storage requirements, and (2) a \"Jointentropy\" score for selecting stable and informative samples. Our method significantly outperforms state-of-the-art pruning algorithms across various adversarial attacks and pruning ratios. On CIFAR-10, our approach achieves up to 58.19% accuracy under AutoAttack with an 80% pruning ratio, compared to 42.98% for previous methods. Moreover, our frequency pruning technique improves robustness even on full datasets, demonstrating its potential for enhancing model security while reducing computational costs.", "title_embedding_index": 21707, "title_abs_embedding_index": 21732}, {"title": "SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection", "link_suffix": "/forum?id=hLwcNSFhC2", "link": "https://openreview.net/forum?id=hLwcNSFhC2", "pdf_link": "https://openreview.net/pdf?id=hLwcNSFhC2", "keywords": "Deep Learning, Bioinformatics, Phosphorylation prediction", "abstract": "Phosphorylation site prediction based on kinase-substrate interaction plays a vital role in understanding cellular signaling pathways and disease mechanisms. Computational methods for this task can be categorized into kinase-family-focused and individual kinase-targeted approaches. Individual kinase-targeted methods have gained prominence for their ability to explore a broader protein space and provide more precise target information for kinase inhibitors. However, most existing individual kinase-based approaches focus solely on sequence inputs, neglecting crucial structural information. To address this limitation, we introduce SAGEPhos (Structure-aware kinAse-substrate bio-coupled and bio-auGmented nEtwork for Phosphorylation site prediction), a novel framework that modifies the semantic space of main protein inputs using auxiliary inputs at two distinct modality levels. At the inter-modality level, SAGEPhos introduces a Bio-Coupled Modal Fusion method, distilling essential kinase sequence information to refine task-oriented local substrate feature space, creating a shared semantic space that captures crucial kinase-substrate interaction patterns. Within the substrate's intra-modality domain, it focuses on Bio-Augmented Fusion, emphasizing 2D local sequence information while selectively incorporating 3D spatial information from predicted structures to complement the sequence space. Moreover, to address the lack of structural information in current datasets, we contribute a new, refined phosphorylation site prediction dataset, which incorporates crucial structural elements and will serve as a new benchmark for the field. Experimental results demonstrate that SAGEPhos significantly outperforms baseline methods, notably achieving almost 10% and 12% improvements in prediction accuracy and AUC-ROC, respectively. We further demonstrate our algorithm's robustness and generalization through stable results across varied data partitions and significant improvements in zero-shot scenarios. These results underscore the effectiveness of constructing a larger and more precise protein space in advancing the state-of-the-art in phosphorylation site prediction.", "title_embedding_index": 21708, "title_abs_embedding_index": 21733}, {"title": "Depth Any Video with Scalable Synthetic Data", "link_suffix": "/forum?id=gWqFbnKsqR", "link": "https://openreview.net/forum?id=gWqFbnKsqR", "pdf_link": "https://openreview.net/pdf?id=gWqFbnKsqR", "keywords": "Video Depth Estimation, Synthetic Game Data", "abstract": "Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results. In this paper, we introduce Depth Any Video, a model that tackles the challenge through two key innovations. First, we develop a scalable synthetic data pipeline, capturing real-time video depth data from diverse game environments, yielding 40,000 video clips of 5-second duration, each with precise depth annotations. Second, we leverage the powerful priors of generative video diffusion models to handle real-world videos effectively, integrating advanced techniques such as rotary position encoding and flow matching to further enhance flexibility and efficiency. Unlike previous models, which are limited to fixed-length video sequences, our approach introduces a novel mixed-duration training strategy that handles videos of varying lengths and performs robustly across different frame rates\u2014even on single frames. At inference, we propose a depth interpolation method that enables our model to infer high-resolution video depth across sequences of up to 150 frames. Our model outperforms all previous generative depth models in terms of spatial accuracy and temporal consistency. The data and code will be open-sourced upon publication.", "title_embedding_index": 21709, "title_abs_embedding_index": 21734}, {"title": "TimeWalker: Personalized Neural Space for Life-long Head Avatar", "link_suffix": "/forum?id=gNWTngNX31", "link": "https://openreview.net/forum?id=gNWTngNX31", "pdf_link": "https://openreview.net/pdf?id=gNWTngNX31", "keywords": "Life-long Personalized Representation;Neural Parametric Morphable Model;", "abstract": "We present TimeWalker, a novel framework that models realistic, full-scale 3D head avatars of a person on lifelong scale. Unlike current human head avatar pipelines that capture a person's identity only at the momentary level (i.e., instant photography, or short videos), TimeWalker constructs a person's comprehensive identity from unstructured data collection over his/her various life stages, offering a paradigm to achieve full reconstruction and animation of that person at different moments of life.  At the heart of TimeWalker's success is a novel neural parametric model that learns personalized representation with the disentanglement of shape, expression, and appearance across ages. Central to our methodology are the concepts of two aspects: (1) We track back to the principle of modeling a person's identity in an additive combination of his/her average head representation in the canonical space, and moment-specific head attribute representations driven from a set of neural head basis. To learn the set of head basis that could represent the comprehensive head variations of the target person in a compact manner, we propose a Dynamic Neural Basis-Blending Module (Dynamo). It dynamically adjusts the number and blend weights of neural head bases, according to both shared and specific traits of the target person over ages. (2) We introduce Dynamic 2D Gaussian Splatting (DNA-2DGS), an extension of Gaussian splatting representation, to model head motion deformations like facial expressions without losing the realism of rendering and reconstruction of full head. DNA-2DGS includes a set of controllable 2D oriented planar Gaussian disks that utilize the priors from a parametric morphable face model, and move/rotate with the change of expression.  Through extensive experimental evaluations, we show TimeWalker's ability to reconstruct and animate avatars across decoupled dimensions with realistic rendering effects, demonstrating a way to achieve personalized ``time traveling'' in a breeze.", "title_embedding_index": 21710, "title_abs_embedding_index": 21735}, {"title": "Beyond Predefined Depots: A Dual-Mode Generative DRL Framework for Proactive Depot Generation in Location-Routing Problem", "link_suffix": "/forum?id=DPYPpC0cBC", "link": "https://openreview.net/forum?id=DPYPpC0cBC", "pdf_link": "https://openreview.net/pdf?id=DPYPpC0cBC", "keywords": "Generative DRL, Depot Generation, Routing planning", "abstract": "The Location-Routing Problem (LRP), which combines the challenges of facility (depot) locating and vehicle route planning, is critically constrained by the reliance on predefined depot candidates, limiting the solution space and potentially leading to suboptimal outcomes. Previous research on LRP without predefined depots is scant and predominantly relies on heuristic algorithms that iteratively attempt depot placements across a planar area. Such approaches lack the ability to proactively generate depot locations that meet specific geographic requirements, revealing a notable gap in current research landscape. To bridge this gap, we propose a data-driven generative DRL framework, designed to proactively generate depots for LRP without predefined depot candidates, solely based on customer requests data which include geographic and demand information. It can operate in two distinct modes: direct generation of exact depot locations, and the creation of a multivariate Gaussian distribution for flexible depots sampling. By extracting depots' geographic pattern from customer requests data, our approach can dynamically respond to logistical needs, identifying high-quality depot locations that further reduce total routing costs compared to traditional methods. Extensive experiments demonstrate that, for a same group of customer requests, compared with those depots identified through random attempts, our framework can proactively generate depots that lead to superior solution routes with lower routing cost. The implications of our framework potentially extend into real-world applications, particularly in emergency medical rescue and disaster relief logistics, where rapid establishment and adjustment of depot locations are paramount, showcasing its potential in addressing LRP for dynamic and unpredictable environments.", "title_embedding_index": 21711, "title_abs_embedding_index": 21736}, {"title": "SPA: 3D Spatial-Awareness Enables Effective Embodied Representation", "link_suffix": "/forum?id=6TLdqAZgzn", "link": "https://openreview.net/forum?id=6TLdqAZgzn", "pdf_link": "https://openreview.net/pdf?id=6TLdqAZgzn", "keywords": "embodied AI, representation learning, 3D spatial awareness, multi-view image, robot manipulation, neural rendering", "abstract": "In this paper, we introduce SPA, a novel representation learning framework that emphasizes the importance of 3D spatial awareness in embodied AI. Our approach leverages differentiable neural rendering on multi-view images to endow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding. We present the most comprehensive evaluation of embodied representation learning to date, covering 268 tasks across 8 simulators with diverse policies in both single-task and language-conditioned multi-task scenarios. The results are compelling: SPA consistently outperforms more than 10 state-of-the-art representation methods, including those specifically designed for embodied AI, vision-centric tasks, and multi-modal applications, while using less training data. Furthermore, we conduct a series of real-world experiments to confirm its effectiveness in practical scenarios. These results highlight the critical role of 3D spatial awareness for embodied representation learning. Our strongest model takes more than 6000 GPU hours to train and we are committed to open-sourcing all code and model weights to foster future research in embodied representation learning.", "title_embedding_index": 21712, "title_abs_embedding_index": 21737}, {"title": "Incremental Causal Effect for Time to Treatment Initialization", "link_suffix": "/forum?id=0mtz0pet1z", "link": "https://openreview.net/forum?id=0mtz0pet1z", "pdf_link": "https://openreview.net/pdf?id=0mtz0pet1z", "keywords": "Causal Inference, Positivity, Incremental intervention, Incremental Causal Effect, Inverse probability weighting", "abstract": "We consider time to treatment initialization. This can commonly occur in preventive medicine, such as disease screening and vaccination; it can also occur with non-fatal health conditions such as HIV infection without the onset of AIDS; or in tech industry where items wait to be reviewed manually for spam or abusive contents, etc. While traditional causal inference focused on `when to treat' and its effects, including their possible dependence on subject characteristics, we consider the incremental causal effect when the intensity of time to treatment initialization is intervened upon. We provide identification of the incremental causal effect without the commonly required positivity assumption, as well as an estimation framework using inverse probability weighting. We illustrate our approach via simulation, and apply it to a rheumatoid arthritis study to evaluate the incremental effect of time to start methotrexate on joint pain.", "title_embedding_index": 21713, "title_abs_embedding_index": 21738}, {"title": "Towards Robust Concept Erasure in Diffusion Models: Unlearning Identity, Nudity and Artistic Styles", "link_suffix": "/forum?id=Ox2A1WoKLm", "link": "https://openreview.net/forum?id=Ox2A1WoKLm", "pdf_link": "https://openreview.net/pdf?id=Ox2A1WoKLm", "keywords": "Diffusion Model, machine unlearning, erasing concepts", "abstract": "Diffusion models have achieved remarkable success in generative tasks across various domains. However, the increasing demand for content moderation and the removal of specific concepts from these models has introduced the challenge of \\textit{unlearning}. In this work, we present a suite of robust methodologies that significantly enhance the unlearning process by employing advanced loss functions within knowledge distillation frameworks. Specifically, we utilize the Cramer-Wold distance and Jensen-Shannon (JS) divergence to facilitate more efficient and versatile concept removal. Although current non-learning techniques are effective in certain scenarios, they are typically limited to specific categories such as identity, nudity, or artistic style. In contrast, our proposed methods demonstrate robust versatility, seamlessly adapting to and performing effectively across a wide range of concept erasure categories. Our approach outperforms existing techniques, achieving consistent results across different unlearning categories and showcasing its broad applicability. Through extensive experiments, we show that our method not only surpasses previous benchmarks but also addresses key limitations of current unlearning techniques, paving the way for more responsible use of text-to-image diffusion models.", "title_embedding_index": 21714, "title_abs_embedding_index": 21739}, {"title": "ConDa: Fast Federated Unlearning with Contribution Dampening", "link_suffix": "/forum?id=PD3I0iOYOd", "link": "https://openreview.net/forum?id=PD3I0iOYOd", "pdf_link": "https://openreview.net/pdf?id=PD3I0iOYOd", "keywords": "federated unlearning, machine unlearning, fast unlearning, non-IID federated unlearning", "abstract": "Federated learning (FL) has enabled collaborative model training across decentralized data sources or clients. While adding new participants to a shared model does not pose great technical hurdles, the removal of a participant and their related information contained in the shared model remains a challenge. To address this problem, federated unlearning has emerged as a critical research direction, seeking to remove information from globally trained models without harming the model performance on the remaining data. Most modern federated unlearning methods use costly approaches such as the use of remaining clients data to retrain the global model or methods that would require heavy computation on client or server side. We introduce Contribution Dampening (\\textsc{ConDa}), a framework that performs efficient unlearning by tracking down the parameters which affect the global model for each client and performs synaptic dampening on the parameters of the global model that have privacy infringing contributions from the forgetting client. Our technique does not require clients data or any kind of retraining and it does not put any computational overhead on either the client or server side. We perform experiments on multiple datasets and demonstrate that \\textsc{ConDa} is effective to forget a client\u2019s data. In experiments conducted on the MNIST, CIFAR10, and CIFAR100 datasets, \\textsc{ConDa} proves to be the fastest federated unlearning method, outperforming the nearest state-of-the-art approach by at least 100\u00d7. Our emphasis is on the non-IID Federated Learning setting, which presents the greatest challenge for unlearning. Additionally, we validate \\textsc{ConDa}'s robustness through backdoor and membership inference attacks. We envision this work as a crucial component for FL in adhering to legal and ethical requirements.", "title_embedding_index": 21715, "title_abs_embedding_index": 21740}, {"title": "GeoMath: A Benchmark for Multimodal Mathematical Reasoning in Remote Sensing", "link_suffix": "/forum?id=i3aFjkfnXO", "link": "https://openreview.net/forum?id=i3aFjkfnXO", "pdf_link": "https://openreview.net/pdf?id=i3aFjkfnXO", "keywords": "Benchmark, Vision-language Models, Multimodal Mathematical Reasoning, Remote Sensing", "abstract": "Vision-language models (VLMs) have demonstrated impressive performance in various Earth observation tasks, particularly in zero-shot capabilities. However, their mathematical reasoning skills in remote sensing (RS) remain unexplored due to the lack of relevant data. To close this gap, we introduce \\dataset, a multimodal mathematical reasoning benchmark meticulously designed for the RS domain. It comprises 3773 high-quality vehicle-related questions from aerial perspectives, spanning 6 mathematical subjects and 20 topics. All data used in this benchmark were collected by our drones from various altitudes and perspectives. Despite the limited geographical coverage, full access to all parameters of the RS images and detailed vehicle information ensures that the constructed mathematical problems are rigorous and diverse. With GeoMath, we have conducted a comprehensive and quantitative evaluation of 14 prominent VLMs. Solving these math problems requires high-resolution visual perception and domain-specific mathematical knowledge, which poses a challenge even for state-of-the-art VLMs. We further explore the impact of image resolution and the zero-shot prompting strategy on the scores, analyzing the reasons behind GPT-4o's reasoning errors. By comparing the gap between InternVL2 and GPT-4o, we find that the latter exhibits some level of cross-view knowledge transfer capability.", "title_embedding_index": 21716, "title_abs_embedding_index": 21741}, {"title": "Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning", "link_suffix": "/forum?id=DSyHRkpI7v", "link": "https://openreview.net/forum?id=DSyHRkpI7v", "pdf_link": "https://openreview.net/pdf?id=DSyHRkpI7v", "keywords": "Reinforcement Learning, Human-in-the-loop, Preference learning, Learning from scalar feedback", "abstract": "To create useful reinforcement learning (RL) agents, step zero is to design a suitable reward function that captures the nuances of the task. However, reward engineering can be a difficult and time-consuming process.Instead, human-in-the-loop (HitL) RL methods hold the promise of learning reward functions from human feedback. Despite recent successes, many of the HitL RL methods still require numerous human interactions to learn successful reward functions.\nTo improve the feedback efficiency of HitL RL methods (i.e., require less human interaction), this paper introduces Sub-optimal Data Pre-training, SDP, an approach that leverages reward-free, sub-optimal data to improve scalar- and preference-based HitL RL algorithms. In SDP, we start by pseudo-labeling all low-quality data with the minimum environment reward. Through this process, we obtain reward labels \nto pre-train our reward model \\emph{without} requiring human labeling or preferences. \nThis pre-training phase provides the reward model a head start in learning, enabling it to recognize that low-quality transitions should be assigned low rewards.\nExtensive experiments with both simulated and human teachers reveal that SDP can at least meet, but often significantly improve, state-of-the-art HitL RL performance across a variety of simulated robotic tasks.", "title_embedding_index": 21717, "title_abs_embedding_index": 21742}, {"title": "Training-Free Dataset Pruning for Instance Segmentation", "link_suffix": "/forum?id=rvxWEbTtRY", "link": "https://openreview.net/forum?id=rvxWEbTtRY", "pdf_link": "https://openreview.net/pdf?id=rvxWEbTtRY", "keywords": "dataset pruning, instance segmentation", "abstract": "Existing dataset pruning techniques primarily focus on classification tasks, limiting their applicability to more complex and practical tasks like instance segmentation. Instance segmentation presents three key challenges: pixel-level annotations, instance area variations, and class imbalances, which significantly complicate dataset pruning efforts. Directly adapting existing classification-based pruning methods proves ineffective due to their reliance on time-consuming model training process. To address this, we propose a novelTraining-FreeDatasetPruning (TFDP) method for instance segmentation. Specifically, we leverage shape and class information from image annotations to design a Shape Complexity Score (SCS),  refining it into a Scale-Invariant (SI-SCS) and Class-Balanced (CB-SCS) versions to address instance area variations and class imbalances, all without requiring model training. We achieve state-of-the-art results on VOC 2012, Cityscapes, and MS COCO datasets, generalizing well across CNN and Transformer architectures. Remarkably, our approach accelerates the pruning process by an average of1349$\\times$on COCO compared to the adapted baselines.", "title_embedding_index": 21718, "title_abs_embedding_index": 21743}, {"title": "SuperMark: Robust and Training-free Image Watermarking via Diffusion-based Super-Resolution", "link_suffix": "/forum?id=T0ebbDO60R", "link": "https://openreview.net/forum?id=T0ebbDO60R", "pdf_link": "https://openreview.net/pdf?id=T0ebbDO60R", "keywords": "image watermarking, diffusion models", "abstract": "In today's digital landscape, the intermingling of AI-generated and authentic content has heightened the importance of copyright protection and content authentication. Watermarking has emerged as a crucial technology to address these challenges, offering a general approach to safeguard both generated and real content. To be effective, watermarking methods must withstand various distortions and attacks. While current deep watermarking techniques typically employ an encoder\u2013noise layer\u2013decoder architecture and incorporate various distortions to enhance robustness, they often struggle to balance robustness and fidelity, and remain vulnerable to adaptive attacks, despite extensive training. To overcome these limitations, we propose SuperMark, a novel robust and training-free watermarking framework. Our approach draws inspiration from the parallels between watermark embedding/extraction in watermarking models and the denoising/noising processes in diffusion models. Specifically, SuperMark embeds the watermark into initial Gaussian noise using existing techniques and then applies pretrained Super-Resolution (SR) models to denoise the watermarked noise, producing the final watermarked image. For extraction, the process is reversed: the watermarked image is converted back to the initial watermarked noise via DDIM Inversion, from which the embedded watermark is then extracted. This flexible framework supports various noise injection methods and diffusion-based SR models, allowing for enhanced performance customization. The inherent robustness of the DDIM Inversion process against various perturbations enables SuperMark to demonstrate strong resilience to many distortions while maintaining high fidelity. Extensive experiments demonstrate SuperMark's effectiveness, achieving fidelity comparable to existing methods while significantly surpassing most in terms of robustness. Under normal distortions, SuperMark achieves an average watermark extraction bit accuracy of 99.46%, and 89.29% under adaptive attacks. Furthermore, SuperMark exhibits strong transferability across different datasets, SR models, watermark embedding methods, and resolutions.", "title_embedding_index": 21719, "title_abs_embedding_index": 21744}, {"title": "Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias", "link_suffix": "/forum?id=SKW10XJlAI", "link": "https://openreview.net/forum?id=SKW10XJlAI", "pdf_link": "https://openreview.net/pdf?id=SKW10XJlAI", "keywords": "Diffusion model, Deep learning theory, generative model, Hallucination", "abstract": "Score-based diffusion models have achieved incredible performance in generating realistic images, audio, and video data. While these models produce high-quality samples with impressive details, they often introduce unrealistic artifacts, such as distorted fingers or hallucinated texts with no meaning. This paper focuses on textual hallucinations, where diffusion models correctly generate individual symbols but assemble them in a nonsensical manner. Through experimental probing, we consistently observe that such phenomenon is attributed it to the network's local generation bias. Denoising networks tend to produce outputs that rely heavily on highly correlated local regions, particularly when different dimensions of the data distribution are nearly pairwise independent. This behavior leads to a generation process that decomposes the global distribution into separate, independent distributions for each symbol, ultimately failing to capture the global structure, including underlying grammar. Intriguingly, this bias persists across various denoising network architectures including MLP and transformers which have the structure to model global dependency. These findings also provide insights into understanding other types of hallucinations, extending beyond text, as a result of implicit biases in the denoising models. Additionally, we theoretically analyze the training dynamics for a specific case involving a two-layer MLP learning parity points on a hypercube, offering an explanation of its underlying mechanism.", "title_embedding_index": 21720, "title_abs_embedding_index": 21745}, {"title": "VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking", "link_suffix": "/forum?id=uzz3qAYy0D", "link": "https://openreview.net/forum?id=uzz3qAYy0D", "pdf_link": "https://openreview.net/pdf?id=uzz3qAYy0D", "keywords": "video, watermarking, tamper localization", "abstract": "Artificial Intelligence Generated Content (AIGC) has advanced significantly, particularly with the development of video generation models such as text-to-video (T2V) models and image-to-video (I2V) models. However, like other AIGC types, video generation requires robust content control. A common approach is to embed watermarks, but most research has focused on images, with limited attention given to videos. Traditional methods, which embed watermarks frame-by-frame in a post-processing manner, often degrade video quality. In this paper, we propose VideoShield, a novel watermarking framework specifically designed for popular diffusion-based video generation models. Unlike post-processing methods, VideoShield embeds watermarks directly during video generation, eliminating the need for additional training. To ensure video integrity, we introduce a tamper localization feature that can detect changes both temporally (across frames) and spatially (within individual frames). Our method maps watermark bits to template bits, which are then used to generate watermarked noise during the denoising process. Using DDIM Inversion, we can reverse the video to its original watermarked noise, enabling straightforward watermark extraction. Additionally, template bits allow precise detection for potential spatial and temporal modification. Extensive experiments across various video models (both T2V and I2V models) demonstrate that our method effectively extracts watermarks and detects tamper without compromising video quality. Furthermore, we show that this approach is applicable to image generation models, enabling tamper detection in generated images as well.", "title_embedding_index": 21721, "title_abs_embedding_index": 21746}, {"title": "Video Representation Learning Without Natural Videos", "link_suffix": "/forum?id=xz3dmxfFva", "link": "https://openreview.net/forum?id=xz3dmxfFva", "pdf_link": "https://openreview.net/pdf?id=xz3dmxfFva", "keywords": "video representation learning, learning from synthetic data", "abstract": "In this paper, we show that useful video representations can be learned from synthetic videos and natural images, without incorporating natural videos in the training. We propose a progression of video datasets synthesized by simple generative processes, that model a growing set of natural video properties (e.g. motion, acceleration, and shape transformations). The downstream performance of video models pre-trained on these generated datasets gradually increases with the dataset progression. A VideoMAE model pre-trained on our synthetic videos closes 97.2% of the performance gap on UCF101 action classification between training from scratch and self-supervised pre-training from natural videos, and outperforms the pre-trained model on HMDB51. Introducing crops of static images to the pre-training stage results in similar performance to UCF101 pre-training and outperforms the UCF101 pre-trained model on 11 out of 14 out-of-distribution datasets of UCF101-P. Analyzing the low-level properties of the datasets, we identify correlations between frame diversity, frame similarity to natural data, and downstream performance. Our approach provides a more controllable and transparent alternative to video data curation processes for pre-training.", "title_embedding_index": 21722, "title_abs_embedding_index": 21747}, {"title": "PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks", "link_suffix": "/forum?id=T5QLRRHyL1", "link": "https://openreview.net/forum?id=T5QLRRHyL1", "pdf_link": "https://openreview.net/pdf?id=T5QLRRHyL1", "keywords": "Human-Robot Collaboration, Planning, Embodied AI", "abstract": "We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation-in-the-loop for the grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects. We analyze state-of-the-art LLMs on PARTNR tasks, across the axes of planning, perception and skill execution. The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors. When LLMs are paired with 'real' humans, they require 1.5x as many steps as two humans collaborating and 1.1x more steps than a single human, underscoring the potential for improvement in these models. We further show that fine-tuning smaller LLMs with planning data can achieve performance on par with models 9 times larger, while being 8.6x faster at inference. Overall, PARTNR highlights significant challenges facing collaborative embodied agents and aims to drive research in this direction.", "title_embedding_index": 21723, "title_abs_embedding_index": 21748}, {"title": "Farther Than Mirror: Explore Pattern-Compensated Depth of Mirror with Temporal Changes for Video Mirror Detection", "link_suffix": "/forum?id=HlvruwLQth", "link": "https://openreview.net/forum?id=HlvruwLQth", "pdf_link": "https://openreview.net/pdf?id=HlvruwLQth", "keywords": "Video mirror detection, depth estimation, affinity, visual pattern compensation", "abstract": "Current video mirror detection models demonstrate satisfactory performance by analyzing different attributes of mirrors and incorporating temporal information. However, these models still struggle to detect mirrors in complex and dynamic scenarios. \nA simple yet critical visual cue is that objects reflected in a mirror appear to be farther away than the mirror itself. Motivated by this observation, we propose to explicitly analyze the Depth of Mirror (DOM) within a video to effectively localize mirrors - DOM refers to distinct perceived distances that make mirror regions appear farther away from their surroundings.  Specifically, we devise a novel framework called FTM-Net, which contains two main contributions: a Pattern-Compensated DOM estimation strategy and a Dual-Granularity Affinity module. The Pattern-Compensated DOM estimation strategy uses multiple visual mirror patterns to refine the DOM, enhancing the accuracy of mirror localization in a single image. Furthermore, the Dual-Granularity Affinity module can effectively detect mirrors in video sequences by tracking and integrating DOM changes across frames. Experimental results on a benchmark dataset show that our model significantly outperforms 18 state-of-the-art methods in the video mirror detection task.", "title_embedding_index": 21724, "title_abs_embedding_index": 21749}]