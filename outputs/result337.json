[{"title": "Bidirectional Generative Retrieval with Multi-Modal LLMs for Text-Video Retrieval", "link_suffix": "/forum?id=Opq0InLQn1", "link": "https://openreview.net/forum?id=Opq0InLQn1", "pdf_link": "https://openreview.net/pdf?id=Opq0InLQn1", "keywords": "Video Retrieval, Multi-modal Large Language Model", "abstract": "In recent years, multi-modal large language models (MLLMs) have shown outstanding advancement in various multi-modal understanding tasks by leveraging the powerful knowledge of large language models (LLMs). Extending MLLMs to text-video retrieval enables handling more complex queries with multiple modalities beyond simple uni-modal queries for traditional search engines. It also provides a new opportunity to incorporate search into a unified conversational system, but MLLM-based text-video retrieval has been less explored in the literature. To this end, we investigate MLLMs' capabilities in text-video retrieval as a generation task, namely, generative retrieval, in two directions. An intuitive direction is $\\textit{content generation}$ that directly generates the content given a query. Another direction is $\\textit{query generation}$, which generates the query given the content. Interestingly, we observe that in both text-to-video and video-to-text retrieval tasks, query-generation less suffers from the bias and significantly outperforms content-generation. In this paper, we propose a novel framework, Bidirectional Text-Video Generative Retrieval (BGR), that handles both text-to-video and video-to-text retrieval tasks by measuring the relevance using two generation directions. Our framework trains MLLMs by simultaneously optimizing two objectives, $\\textit{i.e.}$, video-grounded text generation (VTG) and text-grounded video feature generation (TVG). At inference, our framework ensembles predictions by both generation directions. We also introduce a Prior Normalization, a simple plug-and-play module, to further alleviate the $\\textit{prior bias}$ induced by the likelihood of uni-modal content data that often overwhelms the relevance between query and content. Our extensive experiments on multi-modal benchmarks demonstrate that BGR and Prior Normalization are effective in alleviating the prior bias, especially the text prior bias from LLMs' pretrained knowledge in MLLMs, achieving state-of-the-art performance.", "title_embedding_index": 16800, "title_abs_embedding_index": 16825}, {"title": "Controllable Continual Test-Time Adaptation", "link_suffix": "/forum?id=TSZh4610VG", "link": "https://openreview.net/forum?id=TSZh4610VG", "pdf_link": "https://openreview.net/pdf?id=TSZh4610VG", "keywords": "Domain shift, Continual Test-Time Adaptation, Regularization", "abstract": "Continual Test-Time Adaptation (CTTA) is an emerging and challenging task where a model trained in a source domain must adapt to continuously changing conditions during testing, without access to the original source data. CTTA is prone to error accumulation due to uncontrollable domain shifts, leading to blurred decision boundaries between categories. Existing CTTA methods primarily focus on suppressing domain shifts, which proves inadequate during the unsupervised test phase.\nIn contrast, we introduce a novel approach that guides rather than suppresses these shifts.\nSpecifically, we propose $\\textbf{C}$ontrollable $\\textbf{Co}$ntinual $\\textbf{T}$est-$\\textbf{T}$ime $\\textbf{A}$daptation (C-CoTTA), which explicitly prevents any single category from encroaching on others, thereby mitigating the mutual influence between categories caused by uncontrollable shifts. \nMoreover, our method reduces the sensitivity of model to domain transformations, thereby minimizing the magnitude of category shifts. \nExtensive quantitative experiments demonstrate the effectiveness of our method, while qualitative analyses, such as t-SNE plots, confirm the theoretical validity of our approach.", "title_embedding_index": 16801, "title_abs_embedding_index": 16826}, {"title": "TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models", "link_suffix": "/forum?id=cqsw28DuMW", "link": "https://openreview.net/forum?id=cqsw28DuMW", "pdf_link": "https://openreview.net/pdf?id=cqsw28DuMW", "keywords": "Lanauge Models, Knowledge Distillation", "abstract": "Causal language models have demonstrated remarkable capabilities, but their size poses significant challenges for deployment in resource-constrained environments. Knowledge distillation, a widely-used technique for transferring knowledge from a large teacher model to a small student model, presents a promising approach for model compression.\nA significant remaining issue lies in the major differences between teacher and student models, namely the substantial capacity gap, mode averaging, and mode collapse, which pose barriers during distillation.\nTo address these issues, we introduce $\\textit{Temporally Adaptive Interpolated Distillation (TAID)}$, a novel knowledge distillation approach that dynamically interpolates student and teacher distributions through an adaptive intermediate distribution, gradually shifting from the student's initial distribution towards the teacher's distribution. We provide a theoretical analysis demonstrating TAID's ability to prevent mode collapse and empirically show its effectiveness in addressing the capacity gap while balancing mode averaging and mode collapse.\nOur comprehensive experiments demonstrate TAID's superior performance across various model sizes and architectures in both instruction tuning and pre-training scenarios. Furthermore, we showcase TAID's practical impact by developing two state-of-the-art compact foundation models: $\\texttt{TAID-LLM-1.5B}$ for language tasks and $\\texttt{TAID-VLM-2B}$ for vision-language tasks.\nThese results demonstrate TAID's effectiveness in creating high-performing and efficient models, advancing the development of more accessible AI technologies.", "title_embedding_index": 16802, "title_abs_embedding_index": 16827}, {"title": "Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search", "link_suffix": "/forum?id=I4YAIwrsXa", "link": "https://openreview.net/forum?id=I4YAIwrsXa", "pdf_link": "https://openreview.net/pdf?id=I4YAIwrsXa", "keywords": "Neural Theorem Proving, Formal Math, Large Language Model, Reinforcement Learning, Monte-Carlo Tree Search", "abstract": "Lean is an advanced proof assistant designed to facilitate formal theorem proving by providing a variety of interactive feedback. In this paper, we explore methodologies to leverage proof assistant feedback to augment the capabilities of large language models in constructing formal proofs. First, we deploy online reinforcement learning using Lean verification outcomes as the reward signal to improve the proof completion policy. This straightforward approach shows great promise in enhancing the model's alignment with the formal verification system. In addition, we propose RMaxTS, a variant of Monte-Carlo tree search that employs an intrinsic-reward-driven exploration strategy to generate diverse proof paths. The tree structure is organized to represent the transitions of intermediate tactic states, extracted from the compilation messages given by Lean's tactic mode. The intrinsic reward is constructed to incentivize the discovery of novel tactic states, which helps to to mitigate the sparse-reward problem inherent in proof search. These techniques lead to a more efficient planning scheme for formal proof generation, achieving new state-of-the-art results on both miniF2F and ProofNet benchmarks.", "title_embedding_index": 16803, "title_abs_embedding_index": 16828}, {"title": "UD-Mamba: A pixel-level uncertainty-driven mamba model for medical image segmentation", "link_suffix": "/forum?id=OuUKXhV2Uz", "link": "https://openreview.net/forum?id=OuUKXhV2Uz", "pdf_link": "https://openreview.net/pdf?id=OuUKXhV2Uz", "keywords": "Medical Image Segmentation, State Space Models, Mamba, Uncertainty", "abstract": "Recent advancements have highlighted the Mamba framework, a state-space models (SSMs) known for its efficiency in capturing long-range dependencies with linear computational complexity. While Mamba has shown competitive performance in medical image segmentation, it encounters difficulties in modeling local features due to the sporadic nature of traditional location-based scanning methods and the complex, ambiguous boundaries often present in medical images. To overcome these challenges, we propose Uncertainty-Driven Mamba (UD-Mamba), which redefines the pixel-order scanning process by incorporating channel uncertainty into the scanning mechanism. UD-Mamba introduces two key scanning techniques: sequential scanning, which prioritizes regions with high uncertainty by scanning in a row-by-row fashion, and skip scanning, which processes columns vertically, moving from high-to-low or low-to-high uncertainty at fixed intervals. Sequential scanning efficiently clusters high-uncertainty regions, such as boundaries and foreground objects, to improve segmentation precision, while skip scanning enhances the interaction between background and foreground regions, allowing for timely integration of background information to support more accurate foreground inference. Recognizing the advantages of scanning from certain to uncertain areas, we introduce four learnable parameters to balance the importance of features extracted from different scanning methods. Additionally, a cosine consistency loss is employed to mitigate the drawbacks of transitioning between uncertain and certain regions during the scanning process. Our method demonstrates robust segmentation performance, validated across three distinct medical imaging datasets involving pathology, dermatological lesions, and cardiac tasks.", "title_embedding_index": 16804, "title_abs_embedding_index": 16829}, {"title": "Efficient Adversarial Detection and Purification with Diffusion Models", "link_suffix": "/forum?id=AHqXvTK4KG", "link": "https://openreview.net/forum?id=AHqXvTK4KG", "pdf_link": "https://openreview.net/pdf?id=AHqXvTK4KG", "keywords": "Adversarial Purification, Adversarial Detection, Diffusion Models, Unrestricted Adversarial Attack", "abstract": "Adversarial training and adversarial purification are two effective and practical defense methods to enhance a model's robustness against adversarial attacks. However, adversarial training necessitates additional training, while adversarial purification suffers from low time efficiency. More critically, current defenses are designed under the perturbation-based adversarial threat model, which is ineffective against the recently proposed unrestricted adversarial attacks.\nIn this paper, we propose an effective and efficient adversarial defense method that counters both perturbation-based and unrestricted adversarial attacks. Our defense is inspired by the observation that adversarial attacks are typically located near the decision boundary and are sensitive to pixel changes. To address this, we introduce adversarial anti-aliasing to mitigate adversarial modifications. Additionally, we propose adversarial super-resolution, which leverages prior knowledge from clean datasets to benignly recover images. These approaches do not require additional training and are computationally efficient.\nExtensive experiments against both perturbation-based and unrestricted adversarial attacks demonstrate that our defense method outperforms state-of-the-art adversarial purification methods.", "title_embedding_index": 16805, "title_abs_embedding_index": 16830}, {"title": "Erasing Conceptual Knowledge from Language Models", "link_suffix": "/forum?id=AdiNf568ne", "link": "https://openreview.net/forum?id=AdiNf568ne", "pdf_link": "https://openreview.net/pdf?id=AdiNf568ne", "keywords": "Safety, Knowledge, Concept Erasing, Model Editing, Safety, LLM", "abstract": "Concept erasure in language models has traditionally lacked a comprehensive evaluation framework, leading to incomplete assessments of effectiveness of erasure methods. We propose an evaluation paradigm centered on three critical criteria: innocence (complete knowledge removal), seamlessness (maintaining conditional fluent generation), and specificity (preserving unrelated task performance). Our evaluation metrics naturally motivate the development of Erasure of Language Memory (ELM), a new method designed to address all three dimensions. ELM employs targeted low-rank updates to alter output distributions for erased concepts while preserving overall model capabilities including fluency when prompted for an erased concept. We demonstrate ELM's efficacy on biosecurity, cybersecurity, and literary domain erasure tasks. Comparative analysis shows that ELM achieves superior performance across our proposed metrics, including near-random scores on erased topic assessments, generation fluency, maintained accuracy on unrelated benchmarks, and robustness under adversarial attacks.", "title_embedding_index": 16806, "title_abs_embedding_index": 16831}, {"title": "A Hybrid Loss Framework for Decomposition-based Time Series Forecasting Methods: Balancing Global and Component Errors", "link_suffix": "/forum?id=Y89o3LAEHX", "link": "https://openreview.net/forum?id=Y89o3LAEHX", "pdf_link": "https://openreview.net/pdf?id=Y89o3LAEHX", "keywords": "time series forecasting, series decomposition, hybrid loss framework", "abstract": "Accurate time series forecasting, predicting future values based on past data, is crucial for diverse industries. Many current time series methods decompose time series into multiple sub-series, applying different model architectures and training with an end-to-end overall loss for forecasting. However, this raises a question: does this overall loss prioritize the importance of critical sub-series within the decomposition for the better performance? To investigate this, we conduct a study on the impact of overall loss on existing time series methods with sequence decomposition. Our findings reveal that overall loss may introduce bias in model learning, hindering the learning of the prioritization of more significant sub-series and limiting the forecasting performance. To address this, we propose a hybrid loss framework combining the global and component errors. This framework introduces component losses for each sub-series alongside the original overall loss. It employs a dual min-max algorithm to dynamically adjust weights between the overall loss and component losses, and within component losses. This enables the model to achieve better performance of current time series methods by focusing on more critical sub-series while still maintaining a low overall loss. We integrate our loss framework into several time series methods and evaluate the performance on multiple datasets. Results show an average improvement of 0.5-2% over existing methods without any modifications to the model architectures.", "title_embedding_index": 16807, "title_abs_embedding_index": 16832}, {"title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning", "link_suffix": "/forum?id=O34CXUAZ0E", "link": "https://openreview.net/forum?id=O34CXUAZ0E", "pdf_link": "https://openreview.net/pdf?id=O34CXUAZ0E", "keywords": "Federated Learning, Backdoor Attack, Backdoor Defense", "abstract": "Federated Learning (FL) is a distributed paradigm aimed at protecting participant data privacy by exchanging model parameters to achieve high-quality model training. However, this distributed nature also makes FL highly vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art (SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether the backdoor models have been accepted by the defender and adaptively optimizes backdoor models, rendering existing defenses ineffective. In this paper, we first reveal that the failure of existing defenses lies in the employment of empirical statistical measures that are loosely coupled with backdoor attacks. Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that leverages backdoor energy (BE) to indicate the malicious extent of each neuron. To amplify malignity, we further extract the most prominent BE values from each model to form a concentrated backdoor energy (CBE). Finally, a novel Wasserstein distance-based clustering method is introduced to effectively identify backdoor models. Extensive experiments demonstrate that MARS can defend against SOTA backdoor attacks and significantly outperforms existing defenses.", "title_embedding_index": 16808, "title_abs_embedding_index": 16833}, {"title": "VIRT: Vision Instructed Transformer for Robotic Manipulation", "link_suffix": "/forum?id=6o9Vy1m0Jv", "link": "https://openreview.net/forum?id=6o9Vy1m0Jv", "pdf_link": "https://openreview.net/pdf?id=6o9Vy1m0Jv", "keywords": "Robotic Manipulation, Demonstration Learning, Robotic Pre-training, Vision Instruction", "abstract": "Robotic manipulation, owing to its multi-modal nature, often faces significant training ambiguity, necessitating explicit instructions to clearly delineate the manipulation details in tasks. In this work, we highlight that vision instruction is naturally more comprehensible to recent robotic policies than the commonly adopted text instruction, as these policies are born with some vision understanding ability like human infants. Building on this premise and drawing inspiration from cognitive science, we introduce the robotic imagery paradigm, which realizes large-scale robotic data pre-training without text annotations. Additionally, we propose the robotic gaze strategy that emulates the human eye gaze mechanism, thereby guiding subsequent actions and focusing the attention of the policy on the manipulated object. Leveraging these innovations, we develop VIRT, a fully Transformer-based policy. We design comprehensive tasks using both a physical robot and simulated environments to assess the efficacy of VIRT. The results indicate that VIRT can complete very competitive tasks like ``opening the lid of a tightly sealed bottle'', and the proposed techniques boost the success rates of the baseline policy on diverse challenging tasks from nearly 0% to more than 65%.", "title_embedding_index": 16809, "title_abs_embedding_index": 16834}, {"title": "CM^2: Cross-Modal Contextual Modeling for Audio-Visual Speech Enhancement", "link_suffix": "/forum?id=EO2hZTtK3M", "link": "https://openreview.net/forum?id=EO2hZTtK3M", "pdf_link": "https://openreview.net/pdf?id=EO2hZTtK3M", "keywords": "Speech Enhancement, Audio-Visual, Contextual Modeling", "abstract": "Audio-Visual Speech Enhancement (AVSE) aims to improve speech quality in noisy environments by utilizing synchronized audio and visual cues.\nIn real-world scenarios, noise is often non-stationary, interfering with speech signals at varying intensities over time.\nDespite these fluctuations, humans can discern and understand masked spoken words as if they were clear.\nThis capability stems from the auditory system's ability to perceptually reconstruct interrupted speech using visual cues and semantic context in noisy environments, a process known as phonemic restoration.\nInspired by this phenomenon, we propose Cross-Modal Contextual Modeling (CM$^2$), integrating contextual information across different modalities and levels to enhance speech quality. \nSpecifically, we target two types of contextual information: semantic-level context and signal-level context.\nSemantic-level context enables the model to infer missing or corrupted content by leveraging semantic consistency across segments.\nSignal-level context further explores coherence within the signals developed from the semantic consistency.\nAdditionally, we particularly highlight the role of visual appearance in modeling the frequency-domain characteristics of speech, aiming to further refine and enrich the expression of these contexts.\nGuided by this understanding, we introduce a Semantic Context Module (SeCM) at the very beginning of our framework to capture the initial semantic contextual information from both audio and visual modalities.\nNext, we propose a Signal Context Module (SiCM) to obtain signal-level contextual information from both  raw noisy audio signal and the previously acquired audio-visual semantic-level context.\nBuilding on this rich contextual information, we finally introduce a Cross-Context Fusion Module (CCFM) to facilitate fine-grained context fusion across different modalities and types of contexts for further speech enhancement process.\nComprehensive evaluations across various datasets demonstrate that our method significantly outperforms current state-of-the-art approaches, particularly in low signal-to-noise ratio (SNR) environments.", "title_embedding_index": 16810, "title_abs_embedding_index": 16835}, {"title": "Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs", "link_suffix": "/forum?id=d2H1oTNITn", "link": "https://openreview.net/forum?id=d2H1oTNITn", "pdf_link": "https://openreview.net/pdf?id=d2H1oTNITn", "keywords": "Hallucination Mitigation, Large Language Model, Fine-grained Alignment", "abstract": "Large language models (LLMs) exhibit hallucinations (i.e., unfaithful or nonsensical information) when serving as AI assistants in various domains. Since hallucinations always come with truthful content in the LLM responses, previous factuality alignment methods that conduct response-level preference learning inevitably introduced noises during training. Therefore, this paper proposes a fine-grained factuality alignment method based on Direct Preference Optimization (DPO), called Mask-DPO.  Incorporating sentence-level factuality as mask signals, Mask-DPO only learns from factually correct sentences in the preferred samples and prevents the penalty on factual contents in the not preferred samples, which resolves the ambiguity in the preference learning. Extensive experimental results demonstrate that Mask-DPO can significantly improve the factuality of LLMs responses to questions from both in-domain and out-of-domain datasets, although these questions and their corresponding topics are unseen during training. Only trained on the ANAH train set, the score of Llama3.1-8B-Instruct on the ANAH test set is improved from 49.19% to 77.53%, even surpassing the score of Llama3.1-70B-Instruct (53.44%), while its FactScore on the out-of-domain Biography dataset is also improved from 30.29% to 39.39%. We further study the generalization property of Mask-DPO using different training sample scaling strategies and find that scaling the number of topics in the dataset is more effective than the number of questions. We provide a hypothesis of what factual alignment is doing with LLMs, on the implication of this phenomenon, and conduct proof-of-concept experiments to verify it. We hope the method and the findings pave the way for future research on scaling factuality alignment.", "title_embedding_index": 16811, "title_abs_embedding_index": 16836}, {"title": "Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic", "link_suffix": "/forum?id=dj0TktJcVI", "link": "https://openreview.net/forum?id=dj0TktJcVI", "pdf_link": "https://openreview.net/pdf?id=dj0TktJcVI", "keywords": "task arithmetic, weight disentanglement, merging models, multi-task learning", "abstract": "For the past several years,task arithmetichas gained increasing attention. This approach edits pre-trained models directly in weight space by combining the fine-tuned weights of various tasks into aunified model. Its efficiency and cost-effectiveness stem from its training-free combination, contrasting with traditional methods that require model training on large datasets for multiple tasks. However, applying such a unified model to individual tasks can lead to interference from other tasks (lack ofweight disentanglement). To address this issue, Neural Tangent Kernel (NTK) linearization has been employed to leverage a ''kernel behavior'', facilitating weight disentanglement and mitigating adverse effects from unrelated tasks. Despite its benefits, NTK linearization presents drawbacks, including doubled training costs, as well as reduced performance of individual models. To tackle this problem, we propose a simple yet effective and efficient method that is to finetune the attention modules only in the Transformer. Our study reveals that the attention modules exhibit kernel behavior, and fine-tuning the attention modules only significantly improves weight disentanglement. To further understand how our method improves the weight disentanglement of task arithmetic, we present a comprehensive study of task arithmetic by differentiating the role of the representation module and task-specific module. In particular, we find that the representation module plays an important role in improving weight disentanglement whereas the task-specific modules such as the classification heads can degenerate the weight disentanglement performance.", "title_embedding_index": 16812, "title_abs_embedding_index": 16837}, {"title": "Rethinking Self-Distillation: Label Averaging and Enhanced Soft Label Refinement with Partial Labels", "link_suffix": "/forum?id=EJfLvrzh2Q", "link": "https://openreview.net/forum?id=EJfLvrzh2Q", "pdf_link": "https://openreview.net/pdf?id=EJfLvrzh2Q", "keywords": "self-distillation, partial label learning, label noise correction, training with soft labels, multi-class classification", "abstract": "We investigate the mechanisms of self-distillation in multi-class classification, particularly in the context of linear probing with fixed feature extractors where traditional feature learning explanations do not apply. Our theoretical analysis reveals that multi-round self-distillation effectively performs label averaging among instances with high feature correlations, governed by the eigenvectors of the Gram matrix derived from input features. This process leads to clustered predictions and improved generalization, mitigating the impact of label noise by reducing the model's reliance on potentially corrupted labels. We establish conditions under which multi-round self-distillation achieves 100% population accuracy despite label noise. Furthermore, we introduce a novel, efficient single-round self-distillation method using refined partial labels from the teacher's top two softmax outputs, referred to as the PLL student model. This approach replicates the benefits of multi-round distillation in a single round, achieving comparable or superior performance--especially in high-noise scenarios--while significantly reducing computational cost.", "title_embedding_index": 16813, "title_abs_embedding_index": 16838}, {"title": "Natural Language Inference Improves Compositionality in Vision-Language Models", "link_suffix": "/forum?id=G3aXjVAJjU", "link": "https://openreview.net/forum?id=G3aXjVAJjU", "pdf_link": "https://openreview.net/pdf?id=G3aXjVAJjU", "keywords": "text to image evaluation, image to text generation, natural language inference, sentence decomposition, large language models, visual question answering, question generation, benchmark", "abstract": "Compositional reasoning in Vision-Language Models (VLMs) remains challenging as these models often struggle to relate objects, attributes, and spatial relationships. Recent methods aim to address these limitations by relying on the semantics of the textual description, using Large Language Models (LLMs) to break them down into subsets of questions and answers. However, these methods primarily operate on the surface level, failing to incorporate deeper lexical understanding while introducing incorrect assumptions generated by the LLM. In response to these issues, we present Caption Expansion with Contradictions and Entailments (CECE), a principled approach that leverages Natural Language Inference (NLI) to generate entailments and contradictions from a given premise. CECE produces lexically diverse sentences while maintaining their core meaning. Through extensive experiments, we show that CECE enhances interpretability and reduces overreliance on biased or superficial features. By balancing CECE along the original premise, we achieve significant improvements over previous methods without requiring additional fine-tuning, producing state-of-the-art results on benchmarks that score agreement with human judgments for image-text alignment, and achieving an increase in performance on Winoground of $+19.2%$ (group score) and $+12.9%$ on EqBen (group score) over the best prior work (finetuned with targeted data).", "title_embedding_index": 16814, "title_abs_embedding_index": 16839}, {"title": "KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks", "link_suffix": "/forum?id=atXCzVSXTJ", "link": "https://openreview.net/forum?id=atXCzVSXTJ", "pdf_link": "https://openreview.net/pdf?id=atXCzVSXTJ", "keywords": "Kolmogorov-Arnold Networks, Attentive Graph Neural Networks", "abstract": "Graph neural networks (GNNs) with attention mechanisms, often referred to as attentive GNNs, have emerged as a prominent paradigm in advanced GNN models in recent years. However, our understanding of the critical process of scoring neighbor nodes remains limited, leading to the underperformance of many existing attentive GNNs. In this paper, we unify the scoring functions of current attentive GNNs and propose Kolmogorov-Arnold Attention (KAA), which integrates the Kolmogorov-Arnold Network (KAN) architecture into the scoring process. KAA enhances the performance of scoring functions across the board and can be applied to nearly all existing attentive GNNs. To compare the expressive power of KAA with other scoring functions, we introduce Maximum Ranking Distance (MRD) to quantitatively estimate their upper bounds in ranking errors for node importance. Our analysis reveals that, under limited parameters and constraints on width and depth, both linear transformation-based and MLP-based scoring functions exhibit finite expressive power. In contrast, our proposed KAA, even with a single-layer KAN parameterized by zero-order B-spline functions, demonstrates nearly infinite expressive power. Extensive experiments on both node-level and graph-level tasks using various backbone models show that KAA-enhanced scoring functions consistently outperform their original counterparts, achieving performance improvements of over 20% in some cases.", "title_embedding_index": 16815, "title_abs_embedding_index": 16840}, {"title": "Isometric Regularization for Manifolds of Functional Data", "link_suffix": "/forum?id=xBuURiCChw", "link": "https://openreview.net/forum?id=xBuURiCChw", "pdf_link": "https://openreview.net/pdf?id=xBuURiCChw", "keywords": "Isometric regularization, Geometric reularization, Implicit Neural Representation, Manifold Learning, Neural SDF, Neural BRDF, Neural Operator", "abstract": "While conventional data are represented as discrete vectors, Implicit Neural Representations (INRs) utilize neural networks to represent data points as continuous functions. By incorporating a shared network that maps latent vectors to individual functions, one can model the distribution of functional data, which has proven effective in many applications, such as learning 3D shapes, surface reflectance, and operators.\nHowever, the infinite-dimensional nature of these representations makes them prone to overfitting, necessitating sufficient regularization. Na\u00efve regularization methods -- those commonly used with discrete vector representations -- may enforce smoothness to increase robustness but result in a loss of data fidelity due to improper handling of function coordinates. \nTo overcome these challenges, we start by interpreting the mapping from latent variables to INRs as a parametrization of a Riemannian manifold. We then recognize that preserving geometric quantities -- such as distances and angles -- between the latent space and the data manifold is crucial. As a result, we obtain a manifold with minimal intrinsic curvature, leading to robust representations while maintaining high-quality data fitting. Our experiments on various data modalities demonstrate that our method effectively discovers a well-structured latent space, leading to robust data representations even for challenging datasets, such as those that are small or noisy.", "title_embedding_index": 16816, "title_abs_embedding_index": 16841}, {"title": "Stochastic Diffusion: A Diffusion Based Model for Stochastic Time Series Forecasting", "link_suffix": "/forum?id=gVbPYihQag", "link": "https://openreview.net/forum?id=gVbPYihQag", "pdf_link": "https://openreview.net/pdf?id=gVbPYihQag", "keywords": "diffusion probabilistic model, stochastic time series forecasting, data-driven prior", "abstract": "Recent successes in diffusion probabilistic models have demonstrated their strength in modelling and generating different types of data, paving the way for their application in generative time series forecasting. However, most existing diffusion based approaches rely on sequential models and unimodal latent variables to capture global dependencies and model entire observable data, resulting in difficulties when it comes to highly stochastic time series data. In this paper, we propose a novelStochasticDiffusion (StochDiff) model that integrates the diffusion process into time series modelling stage and utilizes the representational power of the stochastic latent spaces to capture the variability of the stochastic time series data. Specifically, the model applies diffusion module at each time step within the sequential framework and learns a step-wise, data-driven prior for generative diffusion process. These features enable the model to effectively capture complex temporal dynamics and the multi-modal nature of the highly stochastic time series data. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed model for probabilistic time series forecasting, particularly in scenarios with high stochasticity. Additionally, with a real-world surgical use case, we highlight the model's potential in medical application.", "title_embedding_index": 16817, "title_abs_embedding_index": 16842}, {"title": "Long-LRM: Long-sequence Large Reconstruction Model for Wide-coverage Gaussian Splats", "link_suffix": "/forum?id=meOELl7HRf", "link": "https://openreview.net/forum?id=meOELl7HRf", "pdf_link": "https://openreview.net/pdf?id=meOELl7HRf", "keywords": "3D Reconstruction", "abstract": "We propose Long-LRM, a generalizable 3D Gaussian reconstruction model that is capable of reconstructing a large scene from \na long sequence of input images. Specifically, our model can process 32 source images at 960$\\times$540 resolution within only 1.3 seconds on a single A100 80G GPU. Our architecture features a mixture of the recent Mamba2 blocks and the classical transformer blocks which allowed many more tokens to be processed than prior work, enhanced by efficient token merging and Gaussian pruning steps that balance between quality and efficiency. Unlike previous generalizable 3D GS models that are limited to taking 1$\\sim$4 input images and can only reconstruct a small portion of a large scene, Long-LRM reconstructs the entire scene in a single feed-forward step. On large-scale scene datasets such as DL3DV-140 and Tanks and Temples, our method achieves performance comparable to optimization-based approaches while being two orders of magnitude more efficient.", "title_embedding_index": 16818, "title_abs_embedding_index": 16843}, {"title": "Point Cloud Dataset Distillation", "link_suffix": "/forum?id=QeYnKwFhsU", "link": "https://openreview.net/forum?id=QeYnKwFhsU", "pdf_link": "https://openreview.net/pdf?id=QeYnKwFhsU", "keywords": "Dataset Distillation, Point Cloud, Geometric Deep Learning", "abstract": "This study introduces dataset distillation (DD) tailored for 3D data, particularly point clouds. DD aims to substitute large-scale real datasets with a small set of synthetic samples while preserving model performance. Existing methods mainly focus on structured data such as images. However, adapting DD for unstructured point clouds poses challenges due to their diverse orientations and resolutions in 3D space. To address these challenges, we theoretically demonstrate the importance of matching rotation-invariant features between real and synthetic data for 3D distillation. We further propose a plug-and-play point cloud rotator to align the point cloud to a canonical orientation, facilitating the learning of rotation-invariant features by all point cloud models. Furthermore, instead of optimizing fixed-size synthetic data directly, we devise a point-wise generator to produce point clouds at various resolutions based on the sampled noise amount. Compared to conventional DD methods, the proposed approach, termed DD3D, enables efficient training on low-resolution point clouds while generating high-resolution data for evaluation, thereby significantly reducing memory requirements and enhancing model scalability. Extensive experiments validate the effectiveness of DD3D in shape classification and part segmentation tasks across diverse scenarios, such as cross-architecture and cross-resolution settings.", "title_embedding_index": 16819, "title_abs_embedding_index": 16844}, {"title": "Entropy-Based Aggregation for Fair and Effective Federated Learning", "link_suffix": "/forum?id=yqST7JwsCt", "link": "https://openreview.net/forum?id=yqST7JwsCt", "pdf_link": "https://openreview.net/pdf?id=yqST7JwsCt", "keywords": "Fairness, Heterogeneous Federated Learning", "abstract": "Federated Learning (FL) enables collaborative model training across distributed devices while preserving data privacy. Nonetheless, the heterogeneity of edge devices often leads to inconsistent performance of the globally trained models, resulting in unfair outcomes among users. Existing federated fairness algorithms strive to enhance fairness but often fall short in maintaining the overall performance of the global model, typically measured by the average accuracy across all clients. To address this issue, we propose a novel algorithm that leverages entropy-based aggregation combined with model and gradient alignments to simultaneously optimize fairness and global model performance. Our method employs a bi-level optimization framework, where we derive an analytic solution to the aggregation probability in the inner loop, making the optimization process computationally efficient. Additionally, we introduce an innovative alignment update and an adaptive strategy in the outer loop to further balance global model's performance and fairness. Theoretical analysis indicates that our approach guarantees convergence even in non-convex FL settings and demonstrates significant fairness improvements in generalized regression and strongly convex models. Empirically, our approach surpasses state-of-the-art federated fairness algorithms, ensuring consistent performance among clients while improving the overall performance of the global model.", "title_embedding_index": 16820, "title_abs_embedding_index": 16845}, {"title": "\u03bb-SecAgg: Partial Vector Freezing for Lightweight Secure Aggregation in Federated Learning", "link_suffix": "/forum?id=E1Tr7wTlIt", "link": "https://openreview.net/forum?id=E1Tr7wTlIt", "pdf_link": "https://openreview.net/pdf?id=E1Tr7wTlIt", "keywords": "Secure aggregation, Federated learning", "abstract": "Secure aggregation of user update vectors (e.g. gradients) has become a critical issue in the field of federated learning. Many Secure Aggregation Protocols (SAPs) face exorbitant computation costs, severely constraining their applicability. Given the observation that a considerable portion of SAP's computation burden stems from processing each entry in the private vectors, we propose Partial Vector Freezing (PVF), a portable module for compressing computation costs without introducing additional communication overhead. $\\lambda$-SecAgg, which integrates SAP with PVF, \"freezes\" a substantial portion of the private vector through specific transformations, requiring only $\\frac{1}{\\lambda}$ of the original vector to participate in SAP. Eventually, users can \"thaw\" the public sum of the \"frozen entries\" by the result of SAP. To enhance privacy, we devise Disrupting Variables Extension for PVF. We demonstrate that PVF can seamlessly integrate with various SAPs and it poses no threat to user privacy in the semi-honest and active adversary settings. We include $7$ baselines, encompassing $5$ distinct types of masking schemes, and explore the acceleration effects of PVF on these SAPs. Empirical investigations indicate that when $\\lambda=100$, PVF yields up to $99.5\\times$ speedup and up to $32.3\\times$ communication reduction.", "title_embedding_index": 16821, "title_abs_embedding_index": 16846}, {"title": "Personalize to generalize: Towards a universal medical multi-modality generalization through personalization", "link_suffix": "/forum?id=nQoRKLeOP0", "link": "https://openreview.net/forum?id=nQoRKLeOP0", "pdf_link": "https://openreview.net/pdf?id=nQoRKLeOP0", "keywords": "Medical modalities, multi-modality generalization, personalized medicine", "abstract": "Personalized medicine is a groundbreaking healthcare framework for the $21^{st}$ century, tailoring medical treatments to individuals based on unique clinical characteristics, including diverse medical imaging modalities. These modalities differ significantly due to distinct underlying imaging principles, creating substantial challenges for generalization in multi-modal medical image tasks. Previous methods addressing multi-modal generalization rarely consider personalization, primarily focusing on common anatomical information. This paper aims to connect multi-modal generalization with the concept of personalized medicine. Specifically, we propose a novel approach to derive a tractable form of the underlying personalized invariant representation $\\mathbb{X}_h$ using individual-level constraints and a learnable biological prior. We demonstrate that learning a personalized $\\mathbb{X}_h$ is both feasible and beneficial, as this representation proves highly generalizable and transferable across various multi-modal medical tasks. Our method is rigorously validated on medical imaging modalities emphasizing both physical structure and functional information, encompassing a range of tasks that require generalization. Extensive experimental results consistently show that our approach significantly improves performance across diverse scenarios, confirming its effectiveness.", "title_embedding_index": 16822, "title_abs_embedding_index": 16847}, {"title": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework", "link_suffix": "/forum?id=S7F7IMGX4O", "link": "https://openreview.net/forum?id=S7F7IMGX4O", "pdf_link": "https://openreview.net/pdf?id=S7F7IMGX4O", "keywords": "Video Generation, Multi-agent, Adaption Training", "abstract": "Text-to-video generation has made significant strides, but replicating the capabilities of advanced systems like OpenAI\u2019s Sora remains challenging due to their closed-source nature. Existing open-source methods struggle to achieve comparable performance, often hindered by ineffective agent collaboration and inadequate training data quality. In this paper, we introduce Mora, a novel multi-agent framework that leverages existing open-source modules to replicate Sora\u2019s functionalities. We address these fundamental limitations by proposing three key techniques: (1) multi-agent fine-tuning with a self-modulation factor to enhance inter-agent coordination, (2) a data-free training strategy that uses large models to synthesize training data, and (3) a human-in-the-loop mechanism combined with multimodal large language models for data filtering to ensure high-quality training datasets. Our comprehensive experiments on six video generation tasks demonstrate that Mora achieves performance comparable to Sora on VBench \\cite{huang2024vbench}, outperforming existing open-source methods across various tasks. Specifically, in the text-to-video generation task, Mora achieved a Video Quality score of 0.800, surpassing Sora\u2019s 0.797 and outperforming all other baseline models across six key metrics. Additionally, in the image-to-video generation task, Mora achieved a perfect Dynamic Degree score of 1.00, demonstrating exceptional capability in enhancing motion realism and achieving higher Imaging Quality than Sora. These results highlight the potential of collaborative multi-agent systems and human-in-the-loop mechanisms in advancing text-to-video generation.", "title_embedding_index": 16823, "title_abs_embedding_index": 16848}, {"title": "Dynamic Switching Teacher: How to Generalize Temporal Action Detection Models", "link_suffix": "/forum?id=o8SPZJaJyj", "link": "https://openreview.net/forum?id=o8SPZJaJyj", "pdf_link": "https://openreview.net/pdf?id=o8SPZJaJyj", "keywords": "Video Understanding, Temporal Action Detection, Domain Adaptation", "abstract": "Temporal Action Detection (TAD) is a crucial task in video understanding, focusing on the precise identification of the onset and termination of specific actions within video sequences. Despite advancements on certain datasets, existing methods often struggle to maintain their efficacy when applied to datasets from disparate domain. In this study, we introduce, for the first time, the application of source-free domain adaptation (SFDA) techniques to the field of TAD, aiming to enhance the generalization capability of TAD models on unlabeled target datasets without access to source data. Most popular SFDA methods predominantly follow the Mean-Teacher (MT) framework and often falter due to the significant domain shift. The generation of pseudo labels by a pre-trained teacher model on the source domain can lead to a cascade of errors when these labels guide the training of a student model, potentially causing a harmful TAD feedback loop. To address this issue, we propose a novel dynamic switching teacher strategy that integrates both dynamic and static teacher models. The dynamic teacher model updates its parameters by learning knowledge from the student model. Concurrently, the static teacher model engages in periodic weight exchange with the student model, ensuring baseline performance and maintaining the quality of pseudo labels. This approach significantly mitigates the label noise. We establish the first benchmark for SFDA in TAD tasks and conduct extensive experiments across various datasets. Our method demonstrates state-of-the-art performance, substantiating the suitability of our method for TAD.", "title_embedding_index": 16824, "title_abs_embedding_index": 16849}]