[{"title": "ACUS: Audio Captioning with Unbiased Sliced Wasserstein Kernel", "link_suffix": "/forum?id=3sf7SpOYIe", "link": "https://openreview.net/forum?id=3sf7SpOYIe", "pdf_link": "https://openreview.net/pdf?id=3sf7SpOYIe", "keywords": "audio captioning, exposure bias, multimodal learning", "abstract": "Teacher-forcing training for audio captioning usually leads to exposure bias due to training and inference mismatch. Prior works propose the contrastive method to deal with caption degeneration. However, the contrastive method ignores the temporal information when measuring similarity across acoustic and linguistic modalities, leading to inferior performance. In this work, we develop the temporal-similarity score by introducing the unbiased sliced Wasserstein RBF (USW-RBF) kernel equipped with rotary positional embedding to account for temporal information across modalities. In contrast to the conventional sliced Wasserstein RBF kernel, we can form an unbiased estimation of USW-RBF kernel via Monte Carlo estimation. Therefore, it is well-suited to stochastic gradient optimization algorithms, and its approximation error decreases at a parametric rate of $\\mathcal{O}(L^{-1/2})$ with $L$ Monte Carlo samples. Additionally, we introduce an audio captioning framework based on the unbiased sliced Wasserstein kernel, incorporating stochastic decoding methods to mitigate caption degeneration during the generation process. We conduct extensive quantitative and qualitative experiments on two datasets, AudioCaps and Clotho, to illustrate the capability of generating high-quality audio captions. Experimental results show that our framework is able to increase caption length, lexical diversity, and text-to-audio self-retrieval accuracy. We also carry out an experiment on two popular encoder-decoder audio captioning backbones to illustrate that our framework can be compatible with a diversity of encoder-decoder architectures.", "title_embedding_index": 17000, "title_abs_embedding_index": 17025}, {"title": "Retrospective Learning from Interactions", "link_suffix": "/forum?id=BSBZCa6N3E", "link": "https://openreview.net/forum?id=BSBZCa6N3E", "pdf_link": "https://openreview.net/pdf?id=BSBZCa6N3E", "keywords": "continual learning, natural language processing, interactive learning, reinforcement learning", "abstract": "Multi-turn language interactions naturally include implicit feedback signals. For example, if a listener responds in an unexpected way to an instruction, the instructor may rephrase it, express frustration, or pivot to an alternative task. These signals are task-independent and occupy a relatively constrained subspace of language, allowing a language model to identify them even if it fails on the actual task. This holds the promise of continually learning and improving from interactions without additional annotations. We introduceReSpect, a method to learn from signals in past interactions via retrospection. We deployReSpectin a new multimodal interaction scenario, where humans instruct an LLM to solve an abstract reasoning task with a combinatorial solution space. Through thousands of interactions with humans, we show howReSpectgradually improves task completion rate from 31% to 82%, all without any external annotation.", "title_embedding_index": 17001, "title_abs_embedding_index": 17026}, {"title": "Subtask-Aware Visual Reward Learning from Segmented Demonstrations", "link_suffix": "/forum?id=mqKVe6F3Up", "link": "https://openreview.net/forum?id=mqKVe6F3Up", "pdf_link": "https://openreview.net/pdf?id=mqKVe6F3Up", "keywords": "Reinforcement Learning, Reward Learning, Robotic Manipulation", "abstract": "Reinforcement Learning (RL) agents have demonstrated their potential across various robotic tasks. However, they still heavily rely on human-engineered reward functions, requiring extensive trial-and-error and access to target behavior information, often unavailable in real-world settings. This paper introduces REDS: REward learning from Demonstration with Segmentations, a novel reward learning framework that leverages action-free videos with minimal supervision. Specifically, REDS employs video demonstrations segmented into subtasks from diverse sources and treats these segments as ground-truth rewards. We train a dense reward function conditioned on video segments and their corresponding subtasks to ensure alignment with ground-truth reward signals by minimizing the Equivalent-Policy Invariant Comparison distance. Additionally, we employ contrastive learning objectives to align video representations with subtasks, ensuring precise subtask inference during online interactions. Our experiments show that REDS significantly outperforms baseline methods on complex robotic manipulation tasks in Meta-World and more challenging real-world tasks, such as furniture assembly in FurnitureBench, with minimal human intervention. Moreover, REDS facilitates generalization to unseen tasks and robot embodiments, highlighting its potential for scalable deployment in diverse environments.", "title_embedding_index": 17002, "title_abs_embedding_index": 17027}, {"title": "Generalizing Stochastic Smoothing for Differentiation and Gradient Estimation", "link_suffix": "/forum?id=GBWqZNoeIk", "link": "https://openreview.net/forum?id=GBWqZNoeIk", "pdf_link": "https://openreview.net/pdf?id=GBWqZNoeIk", "keywords": "differentiable, sorting, rendering, ranking, variance reduction, continuous, cryo-et, differentiable simulator, shortest-path, stochastic relaxation", "abstract": "We deal with the problem of gradient estimation for stochastic differentiable relaxations of algorithms, operators, simulators, and other non-differentiable functions. Stochastic smoothing conventionally perturbs the input of a non-differentiable function with a differentiable density distribution with full support, smoothing it and enabling gradient estimation. Our theory starts at first principles to derive stochastic smoothing with reduced assumptions, without requiring a differentiable density nor full support, and presenting a general framework for relaxation and gradient estimation of non-differentiable black-box functions $f:\\mathbb{R}^n\\to\\mathbb{R}^m$. We develop variance reduction for gradient estimation from 3 orthogonal perspectives. Empirically, we benchmark 6 distributions and up to 24 variance reduction strategies for differentiable sorting and ranking, differentiable shortest-paths on graphs, differentiable rendering for pose estimation, as well as differentiable cryo-electron tomography simulations.", "title_embedding_index": 17003, "title_abs_embedding_index": 17028}, {"title": "Decomposing The Dark Matter of Sparse Autoencoders", "link_suffix": "/forum?id=5IZfo98rqr", "link": "https://openreview.net/forum?id=5IZfo98rqr", "pdf_link": "https://openreview.net/pdf?id=5IZfo98rqr", "keywords": "Sparse Autoencoders, Dictionary Learning, Language Model Features, Scaling Laws, Mechanistic Interpretability", "abstract": "Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in ``dark matter''\u2014unexplained variance in activations. In this work, we predict and verify that much of SAE dark matter can be linearly predicted from the activation vector. We exploit this fact to deconstruct dark matter into three top-level components: 1) unlearned linear features, 2) unlearned dense features, and 3) nonlinear errors introduced by the SAE. Through a scaling laws analysis, we estimate that nonlinear SAE errors stay constant as SAEs scale and serve as a lower bound of SAE performance on both an average and per-token level. We next empirically analyze the nonlinear SAE error term and show that it is not entirely a sparse sum of unlearned linear features, but that it is still responsible for some of the downstream reduction in cross entropy loss when SAE activations are inserted back into the model. Finally, we examine two methods to reduce nonlinear error: inference time gradient pursuit, which leads to a very slight decrease in nonlinear error, and linear transformations from earlier layer SAE dictionaries, which leads to a larger reduction.", "title_embedding_index": 17004, "title_abs_embedding_index": 17029}, {"title": "Scaling up Image Segmentation across Data and Tasks", "link_suffix": "/forum?id=Qa6VvpE2Py", "link": "https://openreview.net/forum?id=Qa6VvpE2Py", "pdf_link": "https://openreview.net/pdf?id=Qa6VvpE2Py", "keywords": "image segmentation, scalable, open-vocabulary", "abstract": "Traditional segmentation models, while effective in isolated tasks, often fail to generalize to more complex and open-ended segmentation problems, such as free-form, open-vocabulary, and in-the-wild scenarios. To bridge this gap, we propose to scale up image segmentation across diverse datasets and tasks such that the knowledge across different tasks and datasets can be integrated while improving the generalization ability. QueryMeldNet, a novel segmentation framework, is introduced and designed to scale seamlessly across both data size and task diversity. It is built upon a dynamic object query mechanism called query meld, which fuses different types of queries using cross-attention. This hybrid approach enables the model to balance between instance- and stuff-level segmentation, providing enhanced scalability for handling diverse object types. We further enhance scalability by leveraging synthetic data-generating segmentation masks and captions for pixel-level and open-vocabulary tasks-drastically reducing the need for costly human annotations. By training on multiple datasets and tasks at scale, QueryMeldNet continuously improves performance as the volume and diversity of data and tasks increase. It exhibits strong generalization capabilities, boosting performance in open-set segmentation tasks SeginW by 7 points. These advancements mark a key step toward universal, scalable segmentation models capable of addressing the demands of real-world applications.", "title_embedding_index": 17005, "title_abs_embedding_index": 17030}, {"title": "Language Models for Textual Data Valuation", "link_suffix": "/forum?id=OdoS6cH8MP", "link": "https://openreview.net/forum?id=OdoS6cH8MP", "pdf_link": "https://openreview.net/pdf?id=OdoS6cH8MP", "keywords": "Language Model, Data Valuation, Similarity, Diversity", "abstract": "In the rapidly evolving field of machine learning (ML), the quality of training data significantly impacts model performance, especially with the rise of foundation models capable of generating data. Measuring data quality may be linked to two statistical metrics: similarity and diversity, relative to a baseline dataset. We introduce DetEmbedMetrics, a novel deterministic embedding-based metric that enables textual data quality assessment by integrating a language model (LM) with deterministic similarity and diversity measurement functions. The core methodology constrains LM-generated embeddings to align with deterministic mathematical measurement functions, endowing the embeddings with desirable statistical properties. This approach enables the valuation of data quality by providing consistent and reliable similarity and diversity measurements, in contrast to methods directly employing neural networks for measuring data quality. Specifically, our approach involves fine-tuning an LM by inputting textual data samples with varying levels of similarity and diversity. The model learns to generate embeddings that, when applied to deterministic similarity and diversity functions, effectively capture the relationship between data sample pairs. This method allows the model to provide associated probabilities for different levels of similarity and diversity, offering clearer interpretation and decision-making compared to continuous scores. Extensive experiments on synthetic datasets demonstrate the effectiveness of DetEmbedMetrics in identifying similarity and diversity within various datasets. Notably, DetEmbedMetrics exhibits generalizability by performing robustly across different deterministic similarity and diversity functions, not relying on specific measurement techniques. This flexibility enhances its applicability as a robust framework for various measurement functions. By providing high-quality embeddings that facilitate the valuation of similarity and diversity between datasets, this research contributes to the growing field of data-centric ML, emphasizing the importance of data quality in the ML pipeline.", "title_embedding_index": 17006, "title_abs_embedding_index": 17031}, {"title": "Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models", "link_suffix": "/forum?id=rbHOLX8OWh", "link": "https://openreview.net/forum?id=rbHOLX8OWh", "pdf_link": "https://openreview.net/pdf?id=rbHOLX8OWh", "keywords": "explanation faithfulness, feature attribution, knowledge tracing/discovering/inducing", "abstract": "We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones. This makes it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics like Singular Value Canonical Correlation Analysis to analyze these SAE features across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.", "title_embedding_index": 17007, "title_abs_embedding_index": 17032}, {"title": "Beyond Levels and Continuity: A New Statistical Method for DNNs Robustness Evaluation", "link_suffix": "/forum?id=qKfzDc8Qiv", "link": "https://openreview.net/forum?id=qKfzDc8Qiv", "pdf_link": "https://openreview.net/pdf?id=qKfzDc8Qiv", "keywords": "Robustness; Point Process; Adversarial examples", "abstract": "Evaluating the robustness of deep neural networks (DNNs) is crucial in safety-critical areas, driving research into methods that accurately measure and enhance their resilience against adversarial attacks, specifically from a statistical perspective due to scalability issues faced by deterministic methods. Existing approaches based on independent sampling usually fail to directly capture such instances due to their rarity. Hence in this work, we treat the existence of adversarial examples as a rare event, and propose an innovative statistical framework for assessing the adversarial robustness of DNNs, called REPP.  Our approach redefines the problem of calculating the occurrence of adversarial examples as the exponential of the mixture of a Poisson random variable and some potential geometric random variables. We adopt the point process to develop a Minimum Variance Unbiased Estimator (MVUE) to accurately estimate the likelihood of encountering adversarial examples, with an upper bound of the true probability with high confidence. Unlike existing rare-event methods based on Multi-level Splitting, REPP does not require the inherent level concept or the continuity condition of the cumulative distribution function (CDF) within DNNs. This adaptation allows for practical application across both computer vision and natural language processing tasks. Experimental results demonstrate that our method is more flexible and effective, offering a more reliable robustness evaluation than existing statistical approaches.", "title_embedding_index": 17008, "title_abs_embedding_index": 17033}, {"title": "Universal Semantic Disentangled Privacy-preserving Speech Representation Learning", "link_suffix": "/forum?id=Id2JMVSQHZ", "link": "https://openreview.net/forum?id=Id2JMVSQHZ", "pdf_link": "https://openreview.net/pdf?id=Id2JMVSQHZ", "keywords": "speech, tokenization, disentanglement, large-language models, LLM, privacy, secure", "abstract": "The use of audio recordings of human speech to train LLMs poses privacy concerns due to these models' potential to generate outputs that closely resemble artifacts in the training data. In this study, we propose a speaker privacy-preserving representation learning method through the Universal Speech Codec (USC), a computationally efficient encoder-decoder model that disentangles speech into: (i) privacy-preserving semantically rich representations, capturing content and speech paralinguistics, and (ii) residual acoustic and speaker representations that enables high-fidelity reconstruction. Extensive evaluations presented show that USC's semantic representation preserves content, prosody, and sentiment, while removing potentially identifiable speaker attributes. Combining both representations, USC achieves state-of-the-art speech reconstruction. Additionally, we introduce an evaluation methodology for measuring privacy-preserving properties, aligning with perceptual tests. We compare USC against other codecs in the literature and demonstrate its effectiveness on privacy-preserving representation learning, illustrating the trade-offs of speaker anonymization, paralinguistics retention and content preservation in the learned semantic representations.", "title_embedding_index": 17009, "title_abs_embedding_index": 17034}, {"title": "Not All Language Model Features Are Linear", "link_suffix": "/forum?id=d63a4AM4hb", "link": "https://openreview.net/forum?id=d63a4AM4hb", "pdf_link": "https://openreview.net/pdf?id=d63a4AM4hb", "keywords": "Mechanistic Interpretability, Learned Representations, Dictionary Learning, Sparse Autoencoders, Features, Circuits", "abstract": "Recent work has proposed that language models perform computation by manipulating one-dimensional representations of concepts (\"features\") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. $\\textit{circular}$ features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Next, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B. Finally, we find further circular representations by breaking down the hidden states for these tasks into interpretable components, and we examine the continuity of the days of the week feature in Mistral 7B.", "title_embedding_index": 17010, "title_abs_embedding_index": 17035}, {"title": "SELECTFORMER: PRIVATE AND PRACTICAL DATA SE- LECTION FOR TRANSFORMERS", "link_suffix": "/forum?id=2cF3f9t31y", "link": "https://openreview.net/forum?id=2cF3f9t31y", "pdf_link": "https://openreview.net/pdf?id=2cF3f9t31y", "keywords": "Secure Multiparty Computation, Machine Learning, Efficiency, Transformer model", "abstract": "Critical to a free data market is $ \\textit{private data selection}$, i.e. the model owner selects and then appraises training data from the data owner before both parties commit to a transaction. To keep the data and model private, this process shall evaluate the target model to be trained over Multi-Party Computation (MPC). While prior work suggests that evaluating Transformer-based models over MPC is prohibitively expensive, this paper makes it practical for the purpose of data selection. Our contributions are three: (1) a new pipeline for private data selection over MPC; (2) emulating high-dimensional nonlinear operators with low-dimension MLPs, which are trained on a small sample of the data of interest; (3) scheduling MPC in a parallel, multiphase fashion. We evaluate our method on diverse Transformer models and NLP/CV benchmarks. Compared to directly evaluating the target model over MPC, our method reduces the delay from thousands of hours to tens of hours, while only seeing around 0.20% accuracy degradation from training with the selected data.", "title_embedding_index": 17011, "title_abs_embedding_index": 17036}, {"title": "Enhancing Language Model Agents using Diversity of Thoughts", "link_suffix": "/forum?id=ZsP3YbYeE9", "link": "https://openreview.net/forum?id=ZsP3YbYeE9", "pdf_link": "https://openreview.net/pdf?id=ZsP3YbYeE9", "keywords": "Large Language Models, Reasoning, Programming", "abstract": "A popular approach to building agents using Language Models (LMs) involves iteratively prompting the LM, reflecting on its outputs, and updating the input prompts until the desired task is achieved. However, our analysis reveals two key shortcomings in the existing methods: $(i)$ limited exploration of the decision space due to repetitive reflections, which result in redundant inputs, and $(ii)$ an inability to leverage insights from previously solved tasks. To address these issues, we introduce DoT (Diversity of Thoughts), a novel framework that a) explicitly reduces redundant reflections to enhance decision-space exploration, and b) incorporates a task-agnostic memory component to enable knowledge retrieval from previously solved tasks\u2014unlike current approaches that operate in isolation for each task. Through extensive experiments on a suite of programming benchmarks (HumanEval, MBPP, and LeetCodeHardGym) using a variety of LMs, DoT demonstrates up to a $\\textbf{10}$% improvement in Pass@1 while maintaining cost-effectiveness. Furthermore, DoT is modular by design. For instance, when the diverse reflection module of DoT is integrated with existing methods like Tree of Thoughts (ToT), we observe a significant $\\textbf{13}$% improvement on Game of 24 (one of the main benchmarks of ToT), highlighting the broad applicability and impact of our contributions across various reasoning tasks.", "title_embedding_index": 17012, "title_abs_embedding_index": 17037}, {"title": "Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy", "link_suffix": "/forum?id=sjWG7B8dvt", "link": "https://openreview.net/forum?id=sjWG7B8dvt", "pdf_link": "https://openreview.net/pdf?id=sjWG7B8dvt", "keywords": "Instruction Hierarchy, Segment Embedding, LLM safety and robustness", "abstract": "Large Language Models (LLMs) are susceptible to security and safety threats, such as prompt injection, prompt extraction, and harmful requests.\nOne major cause of these vulnerabilities is the lack of an instruction hierarchy.\nModern LLM architectures treat all inputs equally, failing to distinguish between and prioritize various types of instructions, such as system messages, user prompts, and data. \nAs a result, lower-priority user prompts may override more critical system instructions, including safety protocols. \nExisting approaches to achieving instruction hierarchy, such as delimiters and instruction-based training, do not address this issue at the architectural level.\nWe introduce the $\\textbf{I}$nstructional $\\textbf{S}$egment $\\textbf{E}$mbedding (ISE) technique, inspired by BERT, to modern large language models, which embeds instruction priority information directly into the model. \nThis approach enables models to explicitly differentiate and prioritize various instruction types, significantly improving safety against malicious prompts that attempt to override priority rules. \nOur experiments on the Structured Query and Instruction Hierarchy benchmarks demonstrate an average robust accuracy increase of up to 15.75% and 18.68%, respectively. \nFurthermore, we observe an improvement in instruction-following capability of up to 4.1% evaluated on AlpacaEval. \nOverall, our approach offers a promising direction for enhancing the safety and effectiveness of LLM architectures.", "title_embedding_index": 17013, "title_abs_embedding_index": 17038}, {"title": "Multimodality Helps Few-Shot 3D Point Cloud Semantic Segmentation", "link_suffix": "/forum?id=jXvwJ51vcK", "link": "https://openreview.net/forum?id=jXvwJ51vcK", "pdf_link": "https://openreview.net/pdf?id=jXvwJ51vcK", "keywords": "few-shot 3D point cloud semantic segmentation, multimodality", "abstract": "Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models to segment novel categories with minimal annotated support samples. While existing FS-PCS methods have shown promise, they primarily focus on unimodal point cloud inputs, overlooking the potential benefits of leveraging multimodal information. In this paper, we address this gap by introducing a cost-free multimodal FS-PCS setup, utilizing textual labels and the potentially available 2D image modality. Under this easy-to-achieve setup, we present the MultiModal Few-Shot SegNet (MM-FSS), a model effectively harnessing complementary information from multiple modalities. MM-FSS employs a shared backbone with two heads to extract intermodal and unimodal visual features, and a pretrained text encoder to generate text embeddings. To fully exploit the multimodal information, we propose a Multimodal Correlation Fusion (MCF) module to generate multimodal correlations, and a Multimodal Semantic Fusion (MSF) module to refine the correlations using text-aware semantic guidance. Additionally, we propose a simple yet effective Test-time Adaptive Cross-modal Calibration (TACC) technique to mitigate training bias, further improving generalization. Experimental results on S3DIS and ScanNet datasets demonstrate significant performance improvements achieved by our method. The efficacy of our approach indicates the benefits of leveraging commonly-ignored free modalities for FS-PCS, providing valuable insights for future research. The code will be released.", "title_embedding_index": 17014, "title_abs_embedding_index": 17039}, {"title": "EIA: ENVIRONMENTAL INJECTION ATTACK ON GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE", "link_suffix": "/forum?id=xMOLUzo2Lk", "link": "https://openreview.net/forum?id=xMOLUzo2Lk", "pdf_link": "https://openreview.net/pdf?id=xMOLUzo2Lk", "keywords": "Web Agent, Attack", "abstract": "Recently, generalist web agents have demonstrated remarkable potential in autonomously completing a wide range of tasks on real websites, significantly boosting human productivity. However, web tasks, such as booking flights, usually involve users' personally identifiable information (PII), which may be exposed to potential privacy risks if web agents accidentally interact with compromised websites\u2014a scenario that remains largely unexplored in the literature. In this work, we narrow this gap by conducting the first study on the privacy risks of generalist web agents in adversarial environments. First, we present a realistic threat model for attacks on the website, where we consider two adversarial targets: stealing users' specific PII or the entire user request. Then, we propose a novel attack method, termed Environmental Injection Attack (EIA). EIA injects malicious content designed to adapt well to environments where the agents operate and our work instantiates EIA specifically for privacy scenarios in web environments. We collect 177 action steps that involve diverse PII categories on realistic websites from the Mind2Web dataset, and conduct experiments using one of the most capable generalist web agent frameworks to date. The results demonstrate that EIA achieves up to 70% attack success rate (ASR) in stealing users' specific PII and 16% ASR in stealing a full user request at an action step. Additionally, by accessing the stealthiness and experimenting with a defensive system prompt, we indicate that EIA is hard to detect and mitigate. Notably, attacks that are not well adapted for a webpage can be detected through careful human inspection, leading to our discussion about the trade-off between security and autonomy. However, extra attackers' efforts can make EIA seamlessly adapted, rendering such human supervision ineffective. Thus, we further discuss the implications on defenses at the pre- and post-deployment stages of the websites without relying on human supervision and call for more advanced defense strategies.", "title_embedding_index": 17015, "title_abs_embedding_index": 17040}, {"title": "Enhancing Deep Partial Label Learning via Casting it to a Satisfiability Problem", "link_suffix": "/forum?id=Z25xTjf3Mv", "link": "https://openreview.net/forum?id=Z25xTjf3Mv", "pdf_link": "https://openreview.net/pdf?id=Z25xTjf3Mv", "keywords": "weak supervised learning, partial label learning", "abstract": "Partial label learning (PLL) is a challenging real-world problem in the field of weakly supervised learning, in which each data instance contains a set of candidate labels with multiple ambiguous labels and one gold label. Although recent progress in PLL using deep representation learning has led to significant advances, the methods continue to experience significant performance drops on data with high label ambiguity and fine-grained categories. By casting PLL into a satisfiability problem and incorporating a loss based on this reduction,  we show that the accuracy of those techniques can be further improved. We establish several key theoretical properties of the proposed SATisfiability-based (SAT) loss and its learning error bound. Our extensive empirical comparison reveals that the proposed loss improves over existing PLL techniques by up to 25.12% on multi-class benchmarks and 12.50% on fine-grained categorized benchmarks.", "title_embedding_index": 17016, "title_abs_embedding_index": 17041}, {"title": "SE(3)-Hyena Operator for Scalable Equivariant Learning", "link_suffix": "/forum?id=evzUQDjXKV", "link": "https://openreview.net/forum?id=evzUQDjXKV", "pdf_link": "https://openreview.net/pdf?id=evzUQDjXKV", "keywords": "architecrture, equivariance, global context, long convolution, scalability, mechanistic interpretability", "abstract": "Modeling global geometric context while maintaining equivariance is crucial for accurate predictions in many fields such as biology, chemistry, or when modeling physical systems. Yet, this is challenging due to the computational demands of processing high-dimensional data at scale. Existing approaches such as equivariant self-attention or distance-based message passing, suffer from quadratic complexity with respect to sequence length, while localized methods sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, in this work, we introduce the SE(3)-Hyena operator, the first equivariant long-convolutional model. SE(3)-Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on dynamical system modeling and all-atom large molecule property prediction, SE(3)-Hyena matches or outperforms equivariant self-attention while requiring significantly less memory and compute for long geometric sequences. Additionally, we propose equivariant associative recall as a new mechanistic interpretability task for studying contextual learning capabilities of equivariant models. Notably, our model processes the geometric context of 20k tokens x3.5 faster than the equivariant transformer and allows x175 longer context within the same memory budget.", "title_embedding_index": 17017, "title_abs_embedding_index": 17042}, {"title": "Joint Modeling of Spatial and Temporal Multiscales in Molecular Dynamics", "link_suffix": "/forum?id=IFXvpRpci0", "link": "https://openreview.net/forum?id=IFXvpRpci0", "pdf_link": "https://openreview.net/pdf?id=IFXvpRpci0", "keywords": "Multi-scale molecular dynamics; Graph Fourier transformation; Neural ODEs", "abstract": "Molecular dynamics simulations are crucial for understanding complex physical, chemical, and biological processes at the atomic level. However, accurately capturing interactions across multiple spatial and temporal scales remains a significant challenge. We present a novel framework that jointly models spatial and temporal multiscale interactions in molecular dynamics. Our approach leverages Graph Fourier Transforms to decompose molecular structures into different spatial scales and employs Neural Ordinary Differential Equations to model the temporal dynamics at each scale. This unified framework explicitly links spatial structures with temporal evolution, enabling more accurate and comprehensive simulations of molecular systems. We evaluate our model on the MD17 dataset, demonstrating consistent performance improvements over state-of-the-art baselines across multiple molecules, particularly under challenging conditions such as irregular timestep sampling and long-term prediction horizons. Ablation studies confirm the significant contributions of both spatial and temporal multiscale modeling components. Our method advances the simulation of complex molecular systems, potentially accelerating research in computational chemistry, drug discovery, and materials science.", "title_embedding_index": 17018, "title_abs_embedding_index": 17043}, {"title": "VipAct: Visual-Perception Enhancement via Specialized VLM Agent Collaboration and Tool-use", "link_suffix": "/forum?id=zVagbJLgkP", "link": "https://openreview.net/forum?id=zVagbJLgkP", "pdf_link": "https://openreview.net/pdf?id=zVagbJLgkP", "keywords": "Visual Language Model, Visual Perception, Language Agent", "abstract": "While vision-language models (VLMs) have demonstrated remarkable performance across various tasks combining textual and visual information, they continue to struggle with fine-grained visual perception tasks that require detailed pixel-level analysis. Effectively eliciting comprehensive reasoning from VLMs on such intricate visual elements remains an open challenge. In this paper, we present VipAct, an agent framework that enhances VLMs by integrating multi-agent collaboration and vision expert models, enabling more precise visual understanding and comprehensive reasoning. VipAct consists of an orchestrator agent, which manages task requirement analysis, planning, and coordination, along with specialized agents that handle specific tasks such as image captioning and vision expert models that provide high-precision perceptual information. This multi-agent approach allows VLMs to better perform fine-grained visual perception tasks by synergizing planning, reasoning, and tool use. We evaluate VipAct on benchmarks featuring a diverse set of visual perception tasks, with experimental results demonstrating significant performance improvements over state-of-the-art baselines across all tasks. Furthermore, comprehensive ablation studies reveal the critical role of multi-agent collaboration in eliciting more detailed System-2 reasoning and highlight the importance of image input for task planning. Additionally, our error analysis identifies patterns of VLMs' inherent limitations in visual perception, providing insights into potential future improvements. VipAct offers a flexible and extensible framework, paving the way for more advanced visual perception systems across various real-world applications.", "title_embedding_index": 17019, "title_abs_embedding_index": 17044}, {"title": "Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale", "link_suffix": "/forum?id=t9JUTS9ADL", "link": "https://openreview.net/forum?id=t9JUTS9ADL", "pdf_link": "https://openreview.net/pdf?id=t9JUTS9ADL", "keywords": "multi-modal agent", "abstract": "Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.", "title_embedding_index": 17020, "title_abs_embedding_index": 17045}, {"title": "DON\u2019T STOP ME NOW: EMBEDDING BASED SCHEDULING FOR LLMS", "link_suffix": "/forum?id=7JhGdZvW4T", "link": "https://openreview.net/forum?id=7JhGdZvW4T", "pdf_link": "https://openreview.net/pdf?id=7JhGdZvW4T", "keywords": "LLM serving, scheduling, algorithms with predictions", "abstract": "Efficient scheduling is crucial for interactive Large Language Model (LLM) applications, where low request completion time directly impacts user engagement. Size-based scheduling algorithms like Shortest Remaining Process Time (SRPT) aim to reduce average request completion time by leveraging known or estimated request sizes and allowing preemption by incoming jobs with shorter service times. However, two main challenges arise when applying size-based scheduling to LLM systems. First, accurately predicting output lengths from prompts is challenging and often resource-intensive, making it impractical for many systems. As a result, the state-of-the-art LLM systems default to first-come, first-served scheduling, which can lead to head-of-line blocking and reduced system efficiency. Second, preemption introduces extra memory overhead to LLM systems as they must maintain intermediate states for unfinished (preempted) requests.\nIn this paper, we propose TRAIL, a method to obtain output predictions from the target LLM itself. After generating each output token, we recycle the embedding of its internal structure as input for a lightweight classifier that predicts the remaining length for each running request. Using these predictions, we propose a prediction-based SRPT variant with limited preemption designed to account for memory overhead in LLM systems. This variant allows preemption early in request execution when memory consumption is low but restricts preemption as requests approach completion to optimize resource utilization. On the theoretical side, we derive a closed-form formula for this SRPT variant in an M/G/1 queue model, which demonstrates its potential value. In our system, we implement this preemption policy alongside our embedding-based prediction method. Our refined predictions from layer embeddings achieve 2.66x lower mean absolute error compared to BERT predictions from sequence prompts. TRAIL achieves 1.66x to 2.01x lower mean latency on the Alpaca dataset and 1.76x to 24.07x lower mean time to the first token compared to the state-of-the-art serving system.", "title_embedding_index": 17021, "title_abs_embedding_index": 17046}, {"title": "REVIP: Rethinking Visual Prompting for Multimodal Large Language Models with External Knowledge", "link_suffix": "/forum?id=h5UdvNFHee", "link": "https://openreview.net/forum?id=h5UdvNFHee", "pdf_link": "https://openreview.net/pdf?id=h5UdvNFHee", "keywords": "Multimodal Large Language Models; Large Language Models", "abstract": "In recent years, multimodal large language models (MLLMs) have made significant strides by training on vast high-quality image-text datasets, enabling them to generally understand images well. However, the inherent difficulty in explicitly conveying fine-grained or spatially dense information (e.g., object masks) in the text format poses a challenge for MLLMs, limiting their ability to answer questions requiring an understanding of detailed or localized visual elements. Drawing inspiration from the Retrieval-Augmented Generation (RAG) concept, this paper proposes a new visual prompt approach to integrate fine-grained external knowledge, obtained from specialized vision models (e.g., instance segmentation/OCR models), into MLLMs. This is a promising yet underexplored direction for enhancing MLLMs' performance. Our approach diverges from concurrent works, which transform external knowledge into additional text prompts, necessitating the model to indirectly learn the correspondence between visual content and text coordinates. Instead, we propose embedding fine-grained object knowledge directly into a spatial embedding map as a visual prompt. This design can be easily incorporated into various MLLMs, such as LLaVA and Mipha, considerably improving their visual understanding performance. Through rigorous experiments, we demonstrate that our method can enhance MLLM performance across 11 benchmarks, improving their fine-grained context-aware capabilities.", "title_embedding_index": 17022, "title_abs_embedding_index": 17047}, {"title": "Data-scarce distillation for large-scale vision language models", "link_suffix": "/forum?id=I5S1a1NKxo", "link": "https://openreview.net/forum?id=I5S1a1NKxo", "pdf_link": "https://openreview.net/pdf?id=I5S1a1NKxo", "keywords": "distillation, few-shot, compression, CLIP, foundation models", "abstract": "Vision-language models (VLMs) have emerged as extremely strong zero-shot and few-shot image classifiers, performing on par with task-specific models. However, they can be unnecessarily heavy-weight for task-specific downstream applications. While existing lines of work have successfully compressed VLMs and other foundation models to varying degrees, most focus on preserving the generality of these models, rather than leveraging their power for a particular task.\nIn this work, we focus on the setting in which we have a limited amount of data on a downstream image classification task and a limited inference budget.\nTo satisfy these constraints, we focus on distilling the strong few-shot performance of CLIP on image classification tasks into a more efficient model. \nWe introduce the SIDCLIP (Synthesize-Initialize-Distill CLIP) method and highlight its three components that are critical to obtaining strong performance: 1) augmenting the classifier with \\textit{synthetic data} generated by leveraging CLIP itself; 2) \\textit{initializing} the modeling process using a smaller CLIP model pretrained on the target architecture; and 3) incorporating \\textit{knowledge distillation} to maximally mimic the performance of the larger model.Our set of proposed strategies produces a compact model that performs within 16% and 10% of CLIP's linear probe performance on 1 and 8 shot datasets respectively, while using a model with less than 2% of the parameters of CLIP's image encoder. \nWe hope our work can be useful as a practical guide for leveraging the power of foundation models in downstream data-scarce and budget constrained settings.", "title_embedding_index": 17023, "title_abs_embedding_index": 17048}, {"title": "cryoSPHERE: Single-Particle HEterogeneous REconstruction from cryo EM", "link_suffix": "/forum?id=n8O0trhost", "link": "https://openreview.net/forum?id=n8O0trhost", "pdf_link": "https://openreview.net/pdf?id=n8O0trhost", "keywords": "cryoEM; protein structure; Deep Learning; Machine Learning; generative modelling", "abstract": "The three-dimensional structure of proteins plays a crucial role in determining their function. Protein structure prediction methods, like AlphaFold, offer rapid access to a protein\u2019s structure. However, large protein complexes cannot be reliably predicted, and proteins are dynamic, making it important to resolve their full conformational distribution. Single-particle cryo-electron microscopy (cryo-EM) is a powerful tool for determining the structures of large protein complexes. Importantly, the numerous images of a given protein contain underutilized information about conformational heterogeneity. These images are very noisy projections of the protein, and traditional methods for cryo-EM reconstruction are limited to recovering only one or a few consensus conformations.In this paper, we introduce cryoSPHERE, which is a deep learning method that uses a nominal protein structure (e.g., from AlphaFold) as input, learns how to divide it into segments, and moves these segments as approximately rigid bodies to fit the different conformations present in the cryo-EM dataset. This approach provides enough constraints to enable meaningful reconstructions of single protein structural ensembles. We demonstrate this with two synthetic datasets featuring varying levels of noise, as well as one real dataset. We show that cryoSPHERE is very resilient to the high levels of noise typically encountered in experiments, where we see consistent improvements over the current state-of-the-art for heterogeneous reconstruction.", "title_embedding_index": 17024, "title_abs_embedding_index": 17049}]