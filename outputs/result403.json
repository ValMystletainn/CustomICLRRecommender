[
    {
        "title": "A Framework of SO(3)-equivariant Non-linear Representation Learning and its Application to Electronic-Structure Hamiltonian Prediction",
        "link_suffix": "/forum?id=ZP8ZSJyP1U",
        "link": "https://openreview.net/forum?id=ZP8ZSJyP1U",
        "pdf_link": "https://openreview.net/pdf?id=ZP8ZSJyP1U",
        "keywords": "SO(3)-equivariant representation learning; Non-linear expressiveness; Electronic-structure Hamiltonian prediction",
        "abstract": "We propose both a theoretical and a methodological framework to address a critical challenge in applying deep learning to physical systems: the reconciliation of non-linear expressiveness with SO(3)-equivariance in predictions of SO(3)-equivariant quantities. Inspired by covariant theory in physics, we present a solution by  exploring the mathematical relationships between SO(3)-invariant and SO(3)-equivariant quantities and their representations. We first construct theoretical SO(3)-invariant quantities derived from the SO(3)-equivariant regression targets, and use these invariant quantities as supervisory labels to guide the learning of high-quality SO(3)-invariant features. Given that SO(3)-invariance is preserved under non-linear operations, the encoding process for invariant features can extensively utilize non-linear mappings, thereby fully capturing the non-linear patterns inherent in physical systems. Building on this, we propose a gradient-based mechanism to induce SO(3)-equivariant encodings of various degrees from the learned SO(3)-invariant features. This mechanism can incorporate non-linear expressive capabilities into SO(3)-equivariant representations, while theoretically preserving their equivariant properties as we prove, establishing a strong foundation for regressing complex SO(3)-equivariant targets. We apply our theory and method to the electronic-structure Hamiltonian prediction tasks, experimental results on eight benchmark databases covering multiple types of systems and challenging scenarios show substantial improvements on the state-of-the-art prediction accuracy of deep learning paradigm. Our method boosts Hamiltonian prediction accuracy by up to 40% and enhances downstream physical quantities, such as occupied orbital energy, by a maximum of 76%. Our method also significantly promotes the acceleration ratios for the convergence of traditional Density Functional Theory (DFT) methods."
    },
    {
        "title": "Kolmogorov-Arnold Transformer",
        "link_suffix": "/forum?id=BCeock53nt",
        "link": "https://openreview.net/forum?id=BCeock53nt",
        "pdf_link": "https://openreview.net/pdf?id=BCeock53nt",
        "keywords": "Kolmogorov-Arnold Network; Transformer",
        "abstract": "Transformers stand as the cornerstone of mordern deep learning. Traditionally, these models rely on multi-layer\nperceptron (MLP) layers to mix the information between channels. In this paper, we introduce the Kolmogorov\u2013Arnold\nTransformer (KAT), a novel architecture that replaces MLP layers with Kolmogorov-Arnold Network (KAN) layers to\nenhance the expressiveness and performance of the model. Integrating KANs into transformers, however, is no easy\nfeat, especially when scaled up. Specifically, we identify three key challenges: (C1) Base function. The standard B-spline\nfunction used in KANs is not optimized for parallel computing on modern hardware, resulting in slower inference speeds.\n(C2) Parameter and Computation Inefficiency. KAN requires a unique function for each input-output pair, making the\ncomputation extremely large. (C3) Weight initialization. The initialization of weights in KANs is particularly challenging\ndue to their learnable activation functions, which are critical for achieving convergence in deep neural networks. To\novercome the aforementioned challenges, we propose three key solutions: (S1) Rational basis. We replace B-spline functions\nwith rational functions to improve compatibility with modern GPUs. By implementing this in CUDA, we achieve faster\ncomputations. (S2) Group KAN. We share the activation weights through a group of neurons, to reduce the computational\nload without sacrificing performance. (S3) Variance-preserving initialization. We carefully initialize the activation weights\nto make sure that the activation variance is maintained across layers. With these designs, KAT scales effectively and readily\noutperforms traditional MLP-based transformers. We demonstrate the advantages of KAT across various tasks, including\nimage recognition, object detection, and semantic segmentation. It consistently enhances performance over the standard\ntransformer architectures of different model sizes."
    },
    {
        "title": "Multi-view Object-Centric Learning with Identifiable Representations",
        "link_suffix": "/forum?id=rqBc4WnvUP",
        "link": "https://openreview.net/forum?id=rqBc4WnvUP",
        "pdf_link": "https://openreview.net/pdf?id=rqBc4WnvUP",
        "keywords": "multi-view object-centric learning, identifiability, probabilistic-slot-attention",
        "abstract": "Modular object-centric representations are key to unlocking human-like reasoning capabilities. However, addressing challenges such as object occlusions to obtain meaningful object-level representations presents both theoretical and practical difficulties. We introduce a novel multi-view probabilistic approach that aggregates view-specific slots to captureinvariant contentinformation while simultaneously learning disentangled globalviewpoint-levelinformation. Our model resolves spatial ambiguities and provides theoretical guarantees for learning identifiable representations, setting it apart from prior work focusing on single-view settings and lacking theoretical foundations.  Along with our identifiability analysis, we provide extensive empirical validation with promising results on both benchmark and proposed large-scale datasets carefully designed to evaluate multi-view methods."
    },
    {
        "title": "MiniDrive: More Efficient Vision-Language Models with Multi-Level 2D Features as Text Tokens for Autonomous Driving",
        "link_suffix": "/forum?id=zdKgyC2vnQ",
        "link": "https://openreview.net/forum?id=zdKgyC2vnQ",
        "pdf_link": "https://openreview.net/pdf?id=zdKgyC2vnQ",
        "keywords": "Vision-language models, Autonomous driving",
        "abstract": "Vision-language models (VLMs) serve as general-purpose end-to-end models in autonomous driving, performing subtasks such as prediction, planning, and perception through question-and-answer interactions. However, most existing methods rely on computationally expensive visual encoders and large language models (LLMs), making them difficult to deploy in real-world scenarios and real-time applications. Meanwhile, most existing VLMs lack the ability to process multiple images, making it difficult to adapt to multi-camera perception in autonomous driving. To address these issues, we propose a novel framework called MiniDrive, which incorporates our proposed Feature Engineering Mixture of Experts (FE-MoE) module and Dynamic Instruction Adapter (DI-Adapter). The FE-MoE effectively maps 2D features into visual token embeddings before being input into the language model. The DI-Adapter enables the visual token embeddings to dynamically change with the instruction text embeddings, resolving the issue of static visual token embeddings for the same image in previous approaches. The DI-Adapter enables the FE-MoE to further extract and process 2D visual features based on user instructions, focus on attention regions, and reduce redundancy. Compared to previous works, MiniDrive achieves state-of-the-art performance in terms of parameter size, floating point operations, and response efficiency, with the smallest version containing only 83M parameters."
    },
    {
        "title": "Multi-player Multi-armed Bandits with Delayed Feedback",
        "link_suffix": "/forum?id=4nU3BLG1ni",
        "link": "https://openreview.net/forum?id=4nU3BLG1ni",
        "pdf_link": "https://openreview.net/pdf?id=4nU3BLG1ni",
        "keywords": "multi-player multi-armed bandits, delayed feedback",
        "abstract": "Multi-player multi-armed bandits have been researched for a long time due to their application in cognitive radio networks. In this setting, multiple players select arms at each time and instantly receive the feedback. Most research on this problem focuses on the content of the immediate feedback, whether it includes both the reward and collision information or the reward alone. However, delay is common in cognitive networks when users perform spectrum sensing. In this paper, we design an algorithm DDSE (Decentralized Delayed Successive Elimination) in multi-player multi-armed bandits with stochastic delay feedback and establish a regret bound. Compared with existing algorithms that fail to address this problem, our algorithm enables players to adapt to delayed feedback and avoid collision. We also derive a lower bound in centralized setting to prove the algorithm achieves near-optimal. Extensive experiments validate the effectiveness of our algorithm."
    },
    {
        "title": "Fragment-Augmented Diffusion for Molecular Conformation Generation",
        "link_suffix": "/forum?id=r0QqfaCkF8",
        "link": "https://openreview.net/forum?id=r0QqfaCkF8",
        "pdf_link": "https://openreview.net/pdf?id=r0QqfaCkF8",
        "keywords": "Molecular Conformation Generation, Data Augmentation",
        "abstract": "Molecular conformer generation is a fundamental challenge in computational chemistry, particularly for large and complex molecules. \nIn this work, we propose a novel approach called Fragment-Augmented Diffusion (FADiff), which integrates molecular fragmentations into diffusion models as a data augmentation strategy to enhance molecular conformation generation. By decomposing molecules into smaller, manageable fragments for the purpose of data augmentation, FADiff enhances the diffusion generation process, effectively capturing local structural variations while preserving the integrity of the entire molecule. Extensive experiments across multiple datasets demonstrate that FADiff consistently outperforms state-of-the-art methods, particularly in data-scarce scenarios, where the fragment-based augmentation approach significantly enhances model performance. We also provide a comprehensive analysis of different fragmentation rules and their impact on model performance, and theoretically validate FADiff's effectiveness in improving generalization. Overall, FADiff advances molecular conformation generation by enhancing the exploration of conformational space, offering a powerful tool for computational chemistry. The code is available athttps://anonymous.4open.science/r/fragaug-5960/."
    },
    {
        "title": "Time-Dependent VAE for Building Latent Representations from Visual Neural Activity with Complex Dynamics",
        "link_suffix": "/forum?id=N83O2FcqzN",
        "link": "https://openreview.net/forum?id=N83O2FcqzN",
        "pdf_link": "https://openreview.net/pdf?id=N83O2FcqzN",
        "keywords": "Latent Variable Models, Visual Cortex, Sequential Variational Auto-Encoder, Neural Embeddings",
        "abstract": "Seeking high-quality representations with latent variable models (LVMs) to reveal the intrinsic correlation between neural activity and behavior or sensory stimuli has attracted much interest. Most work has focused on analyzing motor neural activity that controls clear behavioral traces and has modeled neural temporal relationships in a way that does not conform to natural reality. For studies of visual brain regions, naturalistic visual stimuli are high-dimensional and time-dependent, making neural activity exhibit intricate dynamics. To cope with such conditions, we propose Time-Dependent Split VAE (TiDeSPL-VAE), a sequential LVM that decomposes visual neural activity into two latent representations while considering time dependence. We specify content latent representations corresponding to the component of neural activity driven by the current visual stimulus, and style latent representations corresponding to the neural dynamics influenced by the organism's internal state. To progressively generate the two latent representations over time, we introduce state factors to construct conditional distributions with time dependence and apply self-supervised contrastive learning to shape them. By this means, TiDeSPL-VAE can effectively analyze complex visual neural activity and model temporal relationships in a natural way. We compare our model with alternative approaches on synthetic data and neural data from the mouse visual cortex. The results show that our model not only yields the best decoding performance on naturalistic scenes/movies but also extracts explicit neural dynamics, demonstrating that it builds latent representations more relevant to visual stimuli."
    },
    {
        "title": "Optimal Algorithm for Max-Min Fair Bandit",
        "link_suffix": "/forum?id=C9YyVygCpG",
        "link": "https://openreview.net/forum?id=C9YyVygCpG",
        "pdf_link": "https://openreview.net/pdf?id=C9YyVygCpG",
        "keywords": "multi-player multi-armed bandits, max-min fariness",
        "abstract": "We consider a multi-player multi-armed bandit problem (MP-MAB) where $N$ players compete for $K$ arms in $T$ rounds. The reward distribution is heterogeneous where each player has a different expected reward for the same arm. When multiple players select the same arm, they collide and obtain zero reward. In this paper, we aim to find the max-min fairness matching that maximizes the reward of the player who receives the lowest reward. This paper improves the existing regret upper bound result of $O(\\log T\\log \\log T)$ to achieve max-min fairness. More specifically, our decentralized fair elimination algorithm (DFE) deals with heterogeneity and collision carefully and attains a regret upper bounded of $O((N^2+K)\\log T / \\Delta)$, where $\\Delta$ is the minimum reward gap between max-min value and sub-optimal arms. We assume $N\\leq K$ to guarantee all players can select their arms without collisions. In addition, we also provide an $\\Omega(\\max{N^2, K} \\log T / \\Delta)$ regret lower bound for this problem. This lower bound indicates that our algorithm is optimal with respect to key parameters, which significantly improves the performance of algorithms in previous work. Numerical experiments again verify the efficiency and improvement of our algorithms."
    },
    {
        "title": "BrainUICL: An Unsupervised Individual Continual Learning Framework for EEG Applications",
        "link_suffix": "/forum?id=6jjAYmppGQ",
        "link": "https://openreview.net/forum?id=6jjAYmppGQ",
        "pdf_link": "https://openreview.net/pdf?id=6jjAYmppGQ",
        "keywords": "Continual Learning; EEG Applications",
        "abstract": "Electroencephalography (EEG) is a non-invasive brain-computer interface technology used for recording brain electrical activity. It plays an important role in human life and has been widely uesd in real life, including sleep staging, emotion recognition, and motor imagery. However, existing EEG-related models cannot be well applied in practice, especially in clinical settings, where new patients with individual discrepancies appear every day. Such EEG-based model trained on fixed datasets cannot generalize well to the continual flow of numerous unseen subjects in real-world scenarios. This limitation can be addressed through continual learning (CL), wherein the CL model can continuously learn and advance over time. Inspired by CL, we introduce a novel Unsupervised Individual Continual Learning paradigm for handling this issue in practice. We propose the BrainUICL framework, which enables the EEG-based model to continuously adapt to the incoming new subjects. Simultaneously, BrainUICL helps the model absorb new knowledge during each adaptation, thereby advancing its generalization ability for all unseen subjects. The effectiveness of the proposed BrainUICL has been evaluated on three different mainstream EEG tasks. The BrainUICL can effectively balance both the plasticity and stability during CL, achieving better plasticity on new individuals and better stability across all the unseen individuals, which holds significance in a practical  setting."
    },
    {
        "title": "Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution",
        "link_suffix": "/forum?id=ODiY6pbHZQ",
        "link": "https://openreview.net/forum?id=ODiY6pbHZQ",
        "pdf_link": "https://openreview.net/pdf?id=ODiY6pbHZQ",
        "keywords": "Multi-modal Large Language Model, Multi-Modal Understanding, Arbitary Resolution",
        "abstract": "Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to fixed-resolution images or patches for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These designs enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously."
    },
    {
        "title": "Is Cognition consistent with Perception? Assessing and Mitigating Multimodal Knowledge Conflicts in Document Understanding",
        "link_suffix": "/forum?id=tBZK9BI2GZ",
        "link": "https://openreview.net/forum?id=tBZK9BI2GZ",
        "pdf_link": "https://openreview.net/pdf?id=tBZK9BI2GZ",
        "keywords": "knowledge conflicts, MLLM, document understanding",
        "abstract": "Multimodal large language models (MLLMs) have shown impressive capabilities in document understanding, a rapidly growing research area with significant industrial demand in recent years. As a multimodal task, document understanding requires models to possess both perceptual and cognitive abilities. However, current MLLMs often face conflicts between perception and cognition. Taking a document VQA task (cognition) as an example, an MLLM might generate answers that do not match the corresponding visual content identified by its OCR (perception). This conflict suggests that the MLLM might struggle to establish an intrinsic connection between the information it \n\"sees\" and what it \"understands.\" Such conflicts challenge the intuitive notion that cognition is consistent with perception, hindering the performance and explainability of MLLMs. In this paper, we define the conflicts between cognition and perception as Cognition and Perception (C&P) knowledge conflicts, a form of multimodal knowledge conflicts, and systematically assess them with a focus on document understanding. Our analysis reveals that even GPT-4o, a leading MLLM, achieves only 68.6% C&P consistency. To mitigate the C&P knowledge conflicts, we propose a novel method called Multimodal Knowledge Consistency Fine-tuning. This method first ensures task-specific consistency and then connects the cognitive and perceptual knowledge. Our method significantly reduces C&P knowledge conflicts across all tested MLLMs and enhances their performance in both cognitive and perceptual tasks in most scenarios."
    },
    {
        "title": "Is In-Context Learning Sufficient for Instruction Following in LLMs?",
        "link_suffix": "/forum?id=STEEDDv3zI",
        "link": "https://openreview.net/forum?id=STEEDDv3zI",
        "pdf_link": "https://openreview.net/pdf?id=STEEDDv3zI",
        "keywords": "Large Language Models, In-context Learning, Alignment",
        "abstract": "In-context learning (ICL) allows LLMs to learn from examples without changing their weights: this is a particularly promising capability for long-context LLMs that can potentially learn from many examples. Recently, Lin et al. (2024) proposed URIAL, a method using only three in-context examples to align base LLMs, achieving non-trivial instruction following performance. In this work, we show that, while effective, ICL alignment with URIAL still underperforms compared to instruction fine-tuning on established benchmarks such as MT-Bench and AlpacaEval 2.0 (LC), especially with more capable base LLMs. We then uncover the most relevant elements for successful in-context alignment, finding the crucial role of the decoding parameters. Based on these insights, we show that the approach of URIAL can indeed be improved by adding more, potentially carefully selected, high-quality demonstrations in context, getting closer to the performance of instruct models. Finally, we provide the first, to our knowledge, systematic comparison of ICL and instruction fine-tuning (IFT) for instruction following in the low data regime. Overall, our work advances the understanding of ICL as an alignment technique and its relationship to IFT."
    },
    {
        "title": "Exploring the Collaborative Advantage of Low-level Information on Generalizable AI-generateted Image Detection",
        "link_suffix": "/forum?id=dyzdDSzoKi",
        "link": "https://openreview.net/forum?id=dyzdDSzoKi",
        "pdf_link": "https://openreview.net/pdf?id=dyzdDSzoKi",
        "keywords": "AI-generateted Image Detection, Low-level Information",
        "abstract": "This paper investigates the generalization issue in AI-Generated image detection, aiming to generalize from training on one AI-Generated image dataset to detecting unseen AI-Generated images. Many methods consider extracting low-level information from RGB images to aid the generalization of AI-Generated image detection. However, these methods often consider a single type of low-level information and this may lead to suboptimal generalization. In our analysis, different low-level information often exhibit generalization capabilities for different forgery types. Additionally, simple fusion strategies are insufficient to leverage the detection advantages of each low-level and high-level information for various forgery types. Therefore, we propose the Adaptive Low-level Experts Injection (ALEI) framework.\nOur approach introduces Lora Experts to enable the transformer-based backbone to learn knowledge from different low-level information. We incorporate a Cross-Low-level Attention layer to fuse these features at intermediate layers. To prevent the backbone from losing modeling capabilities for different low-level features, we develop a Low-level Information Adapter that interacts with the features extracted by the backbone. Finally, we propose Dynamic Feature Selection to maximize the generalization detection capability by dynamically selecting the most suitable features for detecting the current image.\nExtensive experiments demonstrate that our method, finetuned on only four categories of ProGAN data, performs excellently and achieves state-of-the-art results on multiple datasets containing unseen GAN and Diffusion methods."
    },
    {
        "title": "SP-LoRA: Sparsity-Preserved Low-Rank Adaptation for Sparse Large Language Model",
        "link_suffix": "/forum?id=rXNGpyxsLQ",
        "link": "https://openreview.net/forum?id=rXNGpyxsLQ",
        "pdf_link": "https://openreview.net/pdf?id=rXNGpyxsLQ",
        "keywords": "sparsity, parameter efficient fine-tuning, low rank adaptation, large language model",
        "abstract": "Large Language Models (LLMs) have shown remarkable performance in various natural language processing tasks but suffer from substantial hardware resource requirements and inference latency issues due to their vast parameter counts. To mitigate these challenges, several post-training pruning techniques such as SparseGPT, Wanda, and RIA have been developed to reduce parameter sizes. However, these methods often result in performance gaps, particularly for smaller models, and lack efficient fine-tuning strategies that preserve sparsity. This paper introduces SP-LoRA, a novel approach that combines the benefits of low-rank adaptation (LoRA) with the efficiency of sparse models. SP-LoRA addresses the issue of density reversion when merging LoRA adapters with sparse matrices through the introduction of a mask matrix $\\mathcal{M}$, ensuring sparsity is maintained. Furthermore, since maintaining sparsity tends to result in a large memory overhead, we propose gradient checkpointing and memory reuse techniques to optimize GPU memory usage during fine-tuning, achieving comparable efficiency to standard LoRA. Through extensive evaluations on pruned LLMs using methods like Wanda and SparseGPT, followed by fine-tuning with SP-LoRA, we demonstrate its effectiveness in both zero-shot scenarios and domain-specific tasks. Our key contributions include a parameter-efficient fine-tuning method for sparse LLMs, an optimized algorithm for reduced GPU memory overhead, and comprehensive empirical validation across diverse models."
    },
    {
        "title": "Are Synthetic Time-series Data Really not as Good as Real Data?",
        "link_suffix": "/forum?id=dIaykjbiiL",
        "link": "https://openreview.net/forum?id=dIaykjbiiL",
        "pdf_link": "https://openreview.net/pdf?id=dIaykjbiiL",
        "keywords": "Time-Series Data, Data Synthesis, Non-Deep-Learning Data Synthesis, InfoBoost, Prediction, Imputation, Feature Decomposition",
        "abstract": "To alleviate the commonly encountered inadequate time-series data problem in DL (DL), we develop a non-DL generic data synthesis method. When current methods require real data or data statistics to train generators or synthesize data, our method InfoBoost enables zero-shot training of models without the need for real data or data statistics. Additionally, as an application of our synthetic data, we train an unconditional feature (rhythm, noise, trend) decomposer based on our synthetic data, which is applicable to real time-series data.  Through experiments, our non-DL synthetic data enables models to achieve superior performance on unsupervised tasks and self-supervised prediction & imputation compared models using real data. Visualized case studies further demonstrate the effectiveness of our novel unconditional feature decomposer trained with our synthetic data."
    },
    {
        "title": "GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling",
        "link_suffix": "/forum?id=Fur0DtynPX",
        "link": "https://openreview.net/forum?id=Fur0DtynPX",
        "pdf_link": "https://openreview.net/pdf?id=Fur0DtynPX",
        "keywords": "Partial Differential Equations, Neural Fields",
        "abstract": "Significant advancements have been achieved in PDE modeling using neural fields. Despite their effectiveness, existing methods rely on global modulation, limiting their ability to reconstruct local details. While spatial modulation with vanilla grid-based representations offers a promising alternative, it struggles with inadequate global information modeling and over-fitting to the training spatial domain. To address these challenges, we propose GridMix, a novel approach that models spatial modulation as a mixture of grid-based representations. GridMix effectively explores global structures while preserving locality for fine-grained modulation. Furthermore, we introduce spatial domain augmentation to enhance the robustness of the modulated neural fields against spatial domain variations. \nWith all these innovations,\nour comprehensive approach culminates in MARBLE, a framework that significantly advancing the capabilities of neural fields in PDE modeling. The effectiveness of MARBLE is extensively\nvalidated on diverse benchmarks encompassing dynamics modeling and geometric prediction."
    },
    {
        "title": "BlackDAN: A Black-Box Multi-Objective Approach to Effective and Contextual Jailbreaking of Language Models",
        "link_suffix": "/forum?id=kT6oc5CpEi",
        "link": "https://openreview.net/forum?id=kT6oc5CpEi",
        "pdf_link": "https://openreview.net/pdf?id=kT6oc5CpEi",
        "keywords": "LLM Safety, Multi-Objective Optimization, Genetic Algorithm, Black-box Jailbreaking",
        "abstract": "While large language models (LLMs) exhibit remarkable capabilities across various tasks, they encounter potential security risks such as jailbreak attacks, which exploit vulnerabilities to bypass security measures and generate harmful outputs. Existing jailbreak strategies mainly focus on maximizing attack success rate (ASR), frequently neglecting other critical factors, including the relevance of the jailbreak response to the query and the level of stealthiness. This narrow focus on single objectives can result in ineffective attacks that either lack contextual relevance or are easily recognizable. In this work, we introduce BlackDAN, an innovative black-box attack framework with multi-objective optimization, aiming to generate high-quality prompts that effectively facilitate jailbreaking while maintaining contextual relevance and minimizing detectability. BlackDAN leverages Multiobjective Evolutionary Algorithms (MOEAs), specifically the NSGA-II algorithm, to optimize jailbreaks across multiple objectives including ASR, stealthiness, and semantic relevance. By integrating mechanisms like mutation, crossover, and Pareto-dominance, BlackDAN provides a transparent and interpretable process for generating jailbreaks. Furthermore, the framework allows customization based on user preferences, enabling the selection of prompts that balance harmfulness, relevance, and other factors. Experimental results demonstrate that BlackDAN outperforms traditional single-objective methods, yielding higher success rates and improved robustness across various LLMs and multimodal LLMs, while ensuring jailbreak responses are both relevant and less detectable."
    },
    {
        "title": "ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction",
        "link_suffix": "/forum?id=4HRRcqE9SU",
        "link": "https://openreview.net/forum?id=4HRRcqE9SU",
        "pdf_link": "https://openreview.net/pdf?id=4HRRcqE9SU",
        "keywords": "Normal Deflection Fields, High-Fidelity Indoor Reconstruction",
        "abstract": "Neural implicit reconstruction via volume rendering has demonstrated its effectiveness in recovering dense 3D surfaces. However, it is non-trivial to simultaneously recover meticulous geometry and preserve smoothness across regions with differing characteristics. To address this issue, previous methods typically employ geometric priors, which are often constrained by the performance of the prior models. In this paper, we propose ND-SDF, which learns a Normal Deflection field to represent the angular deviation between the scene normal and the prior normal. Unlike previous methods that uniformly apply geometric priors on all samples, introducing significant bias in accuracy, our proposed normal deflection field dynamically learns and adapts the utilization of samples based on their specific characteristics, thereby improving both the accuracy and effectiveness of the model. Our method not only obtains smooth weakly textured regions such as walls and floors but also preserves the geometric details of complex structures. In addition, we introduce a novel ray sampling strategy based on the deflection angle to facilitate the unbiased rendering process, which significantly improves the quality and accuracy of intricate surfaces, especially on thin structures. Consistent improvements on various challenging datasets demonstrate the superiority of our method."
    },
    {
        "title": "Visual Context Window Extension: A New Perspective for Long Video Understanding",
        "link_suffix": "/forum?id=X4Rcxi9588",
        "link": "https://openreview.net/forum?id=X4Rcxi9588",
        "pdf_link": "https://openreview.net/pdf?id=X4Rcxi9588",
        "keywords": "Large Multimodal Models, Long Video Understanding, Visual Context Window",
        "abstract": "Large Multimodal Models (LMMs) have demonstrated impressive performance in short video understanding tasks but face great challenges when applied to long video understanding. In contrast, Large Language Models (LLMs) exhibit outstanding capabilities in modeling long texts. Existing work attempts to address this issue by introducing long video-text pairs during training. However, these approaches require substantial computational and data resources. In this paper, we tackle the challenge of long video understanding from the perspective of context windows, aiming to apply LMMs to long video tasks without retraining on long video datasets. We first conduct an in-depth analysis of why pretrained LMMs struggle to understand lengthy video content, identifying that discrepancies between visual and language modalities lead to different context windows for visual and language tokens, making it difficult to directly extend the visual tokens to match the language context window. Based on this, we propose to adapt LMMs for long video understanding tasks by extending the visual context window, eliminating the need for retraining on large-scale long video datasets. To further mitigate the significant memory consumption caused by long sequences, we introduce a progressive pooling inference strategy that selectively adjusts the spatial resolution of frame embeddings, reducing the number of visual tokens while retaining important spatial information. Across multiple long video understanding benchmarks, our method consistently improves the performance as the number of video frames increases. On the MLVU benchmark, our method outperforms GPT-4o, even though our model size is only 7B. Additionally, in the 256-frame setting, our method reduces memory usage by approximately 45% compared to the baseline, without introducing any performance loss."
    },
    {
        "title": "AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models",
        "link_suffix": "/forum?id=NRY0QAvGNT",
        "link": "https://openreview.net/forum?id=NRY0QAvGNT",
        "pdf_link": "https://openreview.net/pdf?id=NRY0QAvGNT",
        "keywords": "Image Address Localization; Large Vision Language Model; Cross-view Alignment; Supervised Fine-tuning",
        "abstract": "Large visual language models (LVLMs) have demonstrated impressive performance in coarse-grained geo-localization at the country or city level, but they struggle with fine-grained street-level localization within urban areas. In this paper, we explore integrating city-wide address localization capabilities into LVLMs, facilitating flexible address-related question answering using street-view images. A key challenge is that the street-view visual question-and-answer (VQA) data provides only microscopic visual cues, leading to subpar performance in fine-tuned models. To tackle this issue, we incorporate perspective-invariant satellite images as macro cues and propose cross-view alignment tuning including a satellite-view and street-view image grafting mechanism, along with an automatic alignment label generation mechanism. This helps build connections between street-view images through cross-view matching, thus enhancing LVLM's global understanding of street distribution. We name our proposed model AddressVLM consisting of two-stage training protocols: cross-view alignment tuning and address localization tuning. Furthermore, we have constructed two street-view VQA datasets based on image address localization datasets from Pittsburgh and San Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM outperforms counterpart LVLMs by over 9% and 12% in average address localization accuracy on the Pitts-VQA and SF-Base-VQA datasets, respectively."
    },
    {
        "title": "Adaptive Constraint Integration for Simultaneously Optimizing Crystal Structures with Multiple Targeted Properties",
        "link_suffix": "/forum?id=NVKwjCIAAX",
        "link": "https://openreview.net/forum?id=NVKwjCIAAX",
        "pdf_link": "https://openreview.net/pdf?id=NVKwjCIAAX",
        "keywords": "deep learning, materials discovery, materials design",
        "abstract": "In materials science, finding crystal structures that have targeted properties is crucial. While recent methodologies such as Bayesian optimization and deep generative models have made some advances on this issue, these methods often face difficulties in adaptively incorporating various constraints, such as electrical neutrality and targeted properties optimization, while keeping the desired specific crystal structure. To address these challenges, we have developed the Simultaneous Multi-property Optimization using Adaptive Crystal Synthesizer (SMOACS), which utilizes state-of-the-art property prediction models and their gradients to directly optimize input crystal structures for targeted properties simultaneously. SMOACS enables the integration of adaptive constraints into the optimization process without necessitating model retraining. Thanks to this feature, SMOACS has succeeded in simultaneously optimizing targeted properties while maintaining perovskite structures, even with models trained on diverse crystal types. We have demonstrated the band gap optimization while meeting a challenging constraint, that is, maintaining electrical neutrality in large atomic configurations up to 135 atom sites, where the verification of the electrical neutrality is challenging. The properties of the most promising materials have been confirmed by density functional theory calculations."
    },
    {
        "title": "OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer",
        "link_suffix": "/forum?id=GDS5eN65QY",
        "link": "https://openreview.net/forum?id=GDS5eN65QY",
        "pdf_link": "https://openreview.net/pdf?id=GDS5eN65QY",
        "keywords": "Multiple-Object Tracking, Transformer, End-to-End, Open-Vocabulary",
        "abstract": "Open-vocabulary multiple object tracking aims to generalize trackers to unseen categories during training, enabling their application across a variety of real-world scenarios. However, the existing open-vocabulary tracker, which relies on off-the-shelf open-vocabulary detector, perceives categories and locations independently in each frame, causing instability and making it vulnerable to similar appearances and irregular motion in diverse scenes. In this paper, we propose OVTR (End-to-End Open-Vocabulary Multiple Object Tracking with TRansformer), the first end-to-end open-vocabulary tracker that models motion, appearance, and category simultaneously. To achieve stable classification and continuous tracking, we designed the CIP (Category Information Propagation) strategy, which establishes multiple high-level category information priors for subsequent frames. Additionally, we introduce a dual-branch structure for generalization capability and deep multimodal interaction, and incorporate protective strategies in the decoder to enhance performance. Notably, our method does not require proposals that contain novel categories, yet still achieves strong results on the open-vocabulary MOT benchmark. Moreover, experiment transferring the model to other dataset demonstrates its effective adaptability."
    },
    {
        "title": "Meta-Black-Box-Optimization through Offline Q-function Learning with Mamba Architecture",
        "link_suffix": "/forum?id=cSgEW7EZ9h",
        "link": "https://openreview.net/forum?id=cSgEW7EZ9h",
        "pdf_link": "https://openreview.net/pdf?id=cSgEW7EZ9h",
        "keywords": "Black-Box Optimization, Dynamic Algorithm Configuration, Learning to Optimize, Offline Reinforcement Learning, Mamba",
        "abstract": "Recent progress in Meta-Black-Box-Optimization (MetaBBO) has demonstrated that meta-training a neural network based meta-level control policy over an optimization task distribution could significantly enhance the optimization performance of the low-level black-box optimizers. However, achieving such performance enhancement requires effective policy optimization/search method to locate optimal control policy within a massive joint-action space. The online learning fashion of existing works further makes the efficiency of MetaBBO problematic. To address these technical challenges, we propose an offline learning framework in this paper, termed Q-Mamba. Concretely, our method uses a Mamba neural network architecture to meta-learn decomposed Q-functions for each configurable component in the low-level optimizer. By decomposing the Q-function of the configuration decisions of all components in an optimizer, we can apply effective sequence modelling to avoid searching the control policy in the massive joint-action space. Furthermore, by leveraging the long-sequence modelling advantage of Mamba and moderate offline trajectory samples, Q-Mamba can be efficiently trained through a synergy of offline Temporal-Difference update and Conservative Q-Learning regularization to achieve competitive performance against the online learning paradigms. Through extensive benchmarking, we observe that Q-Mamba achieves competitive or even superior optimization performance to prior online/offline learning baselines, while significantly improving the training efficiency of existing online learning baselines. Additional ablation studies show that each of the proposed key designs contributes to this good performance."
    },
    {
        "title": "Evaluating the Quality of Hallucination Benchmarks for Large Vision-Language Models",
        "link_suffix": "/forum?id=kjVgyR3RFr",
        "link": "https://openreview.net/forum?id=kjVgyR3RFr",
        "pdf_link": "https://openreview.net/pdf?id=kjVgyR3RFr",
        "keywords": "Large Vision-Language Models, Hallucination, Benchmark",
        "abstract": "Despite the rapid progress and outstanding performance of Large Vision-Language Models (LVLMs) in recent years, LVLMs have been plagued by the issue of hallucination, i.e., LVLMs tend to generate responses that are inconsistent with the corresponding visual inputs. To evaluate the degree of hallucination in LVLMs, previous works have proposed a series of benchmarks featuring different types of tasks and evaluation metrics. However, we find that the quality of the existing hallucination benchmarks varies, with some suffering from problems, e.g., inconsistent evaluation results under repeated tests, and misalignment with human evaluation. To this end, we propose a Hallucination benchmark Quality Measurement framework (HQM), which leverages various indicators to assess the reliability and validity of existing hallucination benchmarks separately. Specifically, for reliability we explore test-retest reliability and parallel-forms reliability, while for validity we examine criterion validity and coverage of hallucination types. Furthermore, we construct a High-Quality Hallucination Benchmark (HQH) for LVLMs, which demonstrates superior reliability and validity under our HQM framework. We conduct an extensive evaluation of over 10 representative LVLMs, including GPT-4o and Gemini-1.5-Pro, to provide an in-depth analysis of the hallucination issues in existing models. Our benchmark is publicly available athttps://github.com/HQHBench/HQHBench."
    },
    {
        "title": "EvalAlign: Supervised Fine-Tuning Multimodal LLMs with Human-Aligned Data for Evaluating Text-to-Image Models",
        "link_suffix": "/forum?id=xreOs2yjqf",
        "link": "https://openreview.net/forum?id=xreOs2yjqf",
        "pdf_link": "https://openreview.net/pdf?id=xreOs2yjqf",
        "keywords": "Text-to-Image Generative Models, Evaluation Metrics, Multimodal Large Language Models (MLLMs), Text-Image Consistency, Image Generation Fidelity, Supervised Fine-Tuning (SFT), Human Evaluative Judgments",
        "abstract": "The recent advancements in text-to-image generative models have been remarkable. Yet, the field suffers from a lack of evaluation metrics that accurately reflect the performance of these models, particularly lacking fine-grained metrics that can guide the optimization of the models. In this paper, we propose EvalAlign, a metric characterized by its accuracy, stability, and fine granularity. Our approach leverages the capabilities of Multimodal Large Language Models (MLLMs) pre-trained on extensive data. We develop evaluation protocols that focus on two key dimensions: image faithfulness and text-image alignment. Each protocol comprises a set of detailed, fine-grained instructions linked to specific scoring options, enabling precise manual scoring of the generated images. We supervised fine-tune (SFT) the MLLM to align with human evaluative judgments, resulting in a robust evaluation model. Our evaluation across 24 text-to-image generation models demonstrate that EvalAlign not only provides superior metric stability but also aligns more closely with human preferences than existing metrics, confirming its effectiveness and utility in model assessment. We will make the code, data, and pre-trained models publicly available."
    }
]