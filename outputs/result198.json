[
    {
        "title": "Scalable Bayesian Learning with posteriors",
        "link_suffix": "/forum?id=fifXzmzeGy",
        "link": "https://openreview.net/forum?id=fifXzmzeGy",
        "pdf_link": "https://openreview.net/pdf?id=fifXzmzeGy",
        "keywords": "Bayesian deep learning, PyTorch, Variational Inference, MCMC",
        "abstract": "Although theoretically compelling, Bayesian learning with modern machine learning models is computationally challenging since it requires approximating a high dimensional posterior distribution. In this work, we (i) introduce posteriors, an easily extensible PyTorch library hosting general-purpose implementations making Bayesian learning accessible and scalable to large data and parameter regimes; (ii) present a tempered framing of stochastic gradient Markov chain Monte Carlo, as implemented in posteriors, that transitions seamlessly into optimization and unveils a minor modification to deep ensembles to ensure they are asymptotically unbiased for the Bayesian posterior, and (iii) demonstrate and compare the utility of Bayesian approximations through experiments including an investigation into the cold posterior effect and applications with large language models."
    },
    {
        "title": "On the Convergence of No-Regret Dynamics in Information Retrieval Games with Proportional Ranking Functions",
        "link_suffix": "/forum?id=jJXZvPe5z0",
        "link": "https://openreview.net/forum?id=jJXZvPe5z0",
        "pdf_link": "https://openreview.net/pdf?id=jJXZvPe5z0",
        "keywords": "Game theory, no-regret dynamics, recommendation systems, information retrieval",
        "abstract": "Publishers who publish their content on the web act strategically, in a behavior that can be modeled within the online learning framework. \nRegret, a central concept in machine learning, serves as a canonical measure for assessing the performance of learning agents within this framework.\nWe prove that any proportional content ranking function with a concave activation function induces games in which no-regret learning dynamics converge. \nMoreover, for proportional ranking functions, we prove the equivalence of the concavity of the activation function, the social concavity of the induced games and the concavity of the induced games.\nWe also study the empirical trade-offs between publishers' and users' welfare, under different choices of the activation function, using a state-of-the-art no-regret dynamics algorithm. Furthermore, we demonstrate how the choice of the ranking function and changes in the ecosystem structure affect these welfare measures, as well as the dynamics' convergence rate."
    },
    {
        "title": "G2Sphere: Learning High-Frequnecy Spherical Signals From Geometric Data",
        "link_suffix": "/forum?id=Cf0K6jgzZt",
        "link": "https://openreview.net/forum?id=Cf0K6jgzZt",
        "pdf_link": "https://openreview.net/pdf?id=Cf0K6jgzZt",
        "keywords": "Equivariance, Geometric, Fourier, Spherical Signals, SO(3), Radar",
        "abstract": "Many modeling tasks from disparate domains can be framed the same way, computing spherical signals from a geometric input, for example, computing the radar response or aerodynamics drag of different objects, or navigating through an environment. This paper introduces G2Sphere, a general method for mapping object geometries to spherical signals. G2Sphere operates entirely in Fourier space, encoding geometric structure into latent Fourier features using equivariant neural networks and then outputting the Fourier coefficients of the output signal. Combining these coefficients with spherical harmonics enables the simultaneous prediction of all values of the continuous spherical signal at any resolution. We perform experiments on various challenging domains including radar response modeling, aerodynamics drag prediction, and policy learning for manipulation and navigation. We find that G2Sphere significantly outperforms baselines in terms of accuracy and inference time. We also demonstrate that equivariance and Fourier features lead to improved sample efficiency and generalization."
    },
    {
        "title": "Generative Location Modeling for Spatially Aware Object Insertion",
        "link_suffix": "/forum?id=eQjJeO7pTF",
        "link": "https://openreview.net/forum?id=eQjJeO7pTF",
        "pdf_link": "https://openreview.net/pdf?id=eQjJeO7pTF",
        "keywords": "Object insertion, image editing, location modeling",
        "abstract": "Generative models have become a powerful tool for image editing tasks, including object insertion. However, these methods often lack spatial awareness, generating objects with unrealistic locations and scales, or unintentionally altering the scene background. A key challenge lies in maintaining visual coherence, which requires both a geometrically suitable object location and a high-quality image edit. In this paper, we focus on the former, creating alocation modeldedicated to identifying realistic object locations. Specifically, we train an autoregressive model that generates bounding box coordinates, conditioned on the background image and the desired object class.\nThis formulation allows to effectively handle sparse placement annotations and to incorporate implausible locations into a preference dataset by performing direct preference optimization. Our extensive experiments demonstrate that our generative location model, when paired with an inpainting method, substantially outperforms state-of-the-art instruction-tuned models and location modeling baselines in object insertion tasks, delivering accurate and visually coherent results."
    },
    {
        "title": "MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos",
        "link_suffix": "/forum?id=tRNKe2Vgqt",
        "link": "https://openreview.net/forum?id=tRNKe2Vgqt",
        "pdf_link": "https://openreview.net/pdf?id=tRNKe2Vgqt",
        "keywords": "Video Understanding, Benchmark",
        "abstract": "Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of \"world models\"---interpreting and reasoning about complex real-world dynamics. To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities. To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding. MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc. MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. The evaluation includes 4 proprietary and 11 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4o performs the best with only 62.5% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos."
    },
    {
        "title": "PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization",
        "link_suffix": "/forum?id=TKuYWeFE6S",
        "link": "https://openreview.net/forum?id=TKuYWeFE6S",
        "pdf_link": "https://openreview.net/pdf?id=TKuYWeFE6S",
        "keywords": "neural combinatorial optimization, learning to optimize, reinforcement learning, routing problems",
        "abstract": "Reinforcement learning-based methods for constructing solutions to combinatorial optimization problems are rapidly approaching the performance of human-designed algorithms. To further narrow the gap, learning-based approaches must efficiently explore the solution space during the search process. Recent approaches artificially increase exploration by enforcing diverse solution generation through handcrafted rules, however, these rules can impair solution quality and are difficult to design for more complex problems.  In this paper, we introduce PolyNet, an approach for improving exploration of the solution space by learning complementary solution strategies. In contrast to other works, PolyNet uses only a single-decoder and a training schema that does not enforce diverse solution generation through handcrafted rules. We evaluate PolyNet on four combinatorial optimization problems and observe that the implicit diversity mechanism allows PolyNet to find better solutions than approaches that explicitly enforce diverse solution generation."
    },
    {
        "title": "CAN DEEPFAKE SPEECH BE RELIABLY DETECTED?",
        "link_suffix": "/forum?id=St7k6NJKn1",
        "link": "https://openreview.net/forum?id=St7k6NJKn1",
        "pdf_link": "https://openreview.net/pdf?id=St7k6NJKn1",
        "keywords": "Deepfake audio detection, TTS, Adversarial Example",
        "abstract": "Recent advances in text-to-speech (TTS) systems, particularly those with voice cloning capabilities, have made voice impersonation readily accessible, raising ethical and legal concerns due to potential misuse for malicious activities like misinformation campaigns and fraud.  While synthetic speech detectors (SSDs) exist to combat this, they are vulnerable to ``test domain shift\", exhibiting decreased performance when audio is altered through transcoding, playback, or background noise.  This vulnerability is further exacerbated by deliberate manipulation of synthetic speech aimed at deceiving detectors. This work presents the first systematic study of such active malicious attacks against state-of-the-art open-source SSDs. White-box attacks, black-box attacks, and their transferability are studied from both attack effectiveness and stealthiness, using both hardcoded metrics and human ratings. The results highlight the urgent need for more robust detection methods in the face of evolving adversarial threats."
    },
    {
        "title": "iMotion-LLM: Motion Prediction Instruction Tuning",
        "link_suffix": "/forum?id=VlWWzN7RtJ",
        "link": "https://openreview.net/forum?id=VlWWzN7RtJ",
        "pdf_link": "https://openreview.net/pdf?id=VlWWzN7RtJ",
        "keywords": "Trajectory Prediction, Conditional Trajectory Prediction",
        "abstract": "We introduce iMotion-LLM, a Multimodal Large Language Model (LLM) integrated with trajectory prediction, designed to guide interactive multi-agent scenarios. Unlike conventional multimodal trajectory prediction approaches, iMotion-LLM generates diverse and feasible future trajectories conditioned on textual instructions as a guidance signal. By augmenting real-world driving scenarios in the Waymo Open Motion Dataset (WOMD) with textual motion instructions, we propose InstructWaymo data augmentation. Leveraging this data augmentation, iMotion-LLM integrates a pretrained LLM, fine-tuned with LoRA, to map scene features into the LLM input space. Key results demonstrate that making the trajectory prediction model conditional improves its instruction-following capabilities. Specifically, the integration of the LLM enables a 11.07x ratio of actual-scenario feasible to infeasible recall instruction following, compared to 5.92x when using the Conditional GameFormer alone. These findings highlight the ability of iMotion-LLM to generate trajectories that not only align with feasible instructions but also reject infeasible ones, enhancing overall safety. Despite its improvements in instruction following, iMotion-LLM inherits the strong trajectory prediction performance of the baseline model, making it versatile across different driving modes. This combination of skills positions iMotion-LLM as a powerful augmentation technique for trajectory prediction models, empowering autonomous navigation systems to better interpret and predict the dynamics of multi-agent environments. This work lays the groundwork for future advancements in instruction-based motion prediction."
    },
    {
        "title": "Permutation Invariant Learning with High-Dimensional Particle Filters",
        "link_suffix": "/forum?id=6XodKiDS3B",
        "link": "https://openreview.net/forum?id=6XodKiDS3B",
        "pdf_link": "https://openreview.net/pdf?id=6XodKiDS3B",
        "keywords": "permutation-invariant learning, continual learning, loss of plasticity, catastrophic forgetting, particle filter, high-dimensional",
        "abstract": "Sequential learning in deep models often suffers from challenges such as catastrophic forgetting and loss of plasticity, largely due to the permutation dependence of gradient-based algorithms, where the order of training data impacts the learning outcome. In this work, we introduce a novel permutation-invariant learning framework based on high-dimensional particle filters. We theoretically demonstrate that particle filters are invariant to the sequential ordering of training minibatches or tasks, offering a principled solution to mitigate catastrophic forgetting and loss-of-plasticity. We develop an efficient particle filter for optimizing high-dimensional models, combining the strengths of Bayesian methods with gradient-based optimization. Through extensive experiments on continual supervised and reinforcement learning benchmarks, including SplitMNIST, SplitCIFAR100, and ProcGen, we empirically show that our method consistently improves performance, while reducing variance compared to standard baselines."
    },
    {
        "title": "Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling",
        "link_suffix": "/forum?id=VAeThhoJR2",
        "link": "https://openreview.net/forum?id=VAeThhoJR2",
        "pdf_link": "https://openreview.net/pdf?id=VAeThhoJR2",
        "keywords": "Virtual Try-On, Human Image Generation, Diffusion Models",
        "abstract": "Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures.  Our work addresses these issues through a dual approach. First, we introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. Second, we propose an Error-Aware Refinement-based Schr\"odinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the Schr\"odinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases."
    },
    {
        "title": "RLSF: Reinforcement Learning via Symbolic Feedback",
        "link_suffix": "/forum?id=vf8iou7FNF",
        "link": "https://openreview.net/forum?id=vf8iou7FNF",
        "pdf_link": "https://openreview.net/pdf?id=vf8iou7FNF",
        "keywords": "Symbolic Feedback, Reinforcement Learning, Large Language Models, Program Synthesis",
        "abstract": "Reinforcement Learning with Human Feedback (RLHF) is considered a standard approach to fine-tuning Large Language Models (LLMs). However, such methods often face limitations such as unsound black-box reward models, difficulties in collecting human preference data, and the reliance on sparse scalar rewards. These methods often fall short when applied to tasks that require complex domain-specific understanding.To address these challenges, we propose a new fine-tuning paradigm we refer to as Reinforcement Learning via Symbolic Feedback (RLSF), which aims to improve domain-specific understanding of LLMs more effectively than traditional reward signals. In the RLSF setting, the LLM being fine-tuned is considered an RL agent, while the environment is allowed access to reasoning or domain knowledge tools (e.g., solvers, provers, algebra systems, or knowledge bases). Crucially, in RLSF, these reasoning tools can provide feedback to the LLMs via poly-sized certificates (e.g., proofs), that characterize errors in the LLM-generated object with respect to some correctness specification. As a bonus, our RLSF approach does not require the reasoning systems we use to be differentiable. The ability of RLSF-based fine-tuning to leverage certificate-generating symbolic tools enables sound fine-grained (token-level) reward signals to LLMs, and thus addresses the limitations of traditional reward models mentioned above.Via extensive evaluations, we show that our RLSF-based fine-tuning of LLMs outperforms traditional approaches on five different applications (that have some associated logical or domain constraints), namely, program synthesis from natural language pseudo-code to programming language (+31.43% in functional correctness for Google's CodeGemma-2b compared to supervised fine-tuning, +17.01% in functional correctness compared to GPT-3.5 -- 100$\\boldsymbol\\times$ larger), three chemistry tasks (+5.5% exact match for molecule generation, +19.4% exact match for forward synthesis, +33.7% exact match for retrosynthesis, using Meta's Galactica-1.3b, compared to GPT-4 -- 1000$\\boldsymbol\\times$ larger), and solving the Game of 24 (+25% success rate using Meta's Llama2-7b compared to traditional methods, and +7% success rate compared to GPT-3.5 -- 25$\\boldsymbol\\times$ larger). A takeaway is that fine-tuning via RLSF enables relatively smaller LLMs to significantly outperform closed-source models that are orders of magnitude larger (e.g., GPT-4)."
    },
    {
        "title": "MetaOOD: Automatic Selection of OOD Detection Models",
        "link_suffix": "/forum?id=9qpdDiDQ2H",
        "link": "https://openreview.net/forum?id=9qpdDiDQ2H",
        "pdf_link": "https://openreview.net/pdf?id=9qpdDiDQ2H",
        "keywords": "Out-of-distribution Detection, Meta-learning, Language Modeling, AutoML",
        "abstract": "How can we automatically select an out-of-distribution (OOD) detection model for various underlying tasks? This is crucial for maintaining the reliability of open-world applications by identifying data distribution shifts, particularly in critical domains such as online transactions, autonomous driving, and real-time patient diagnosis. Despite the availability of numerous OOD detection methods, the challenge of selecting an optimal model for diverse tasks remains largely underexplored, especially in scenarios lacking ground truth labels. In this work, we introduce MetaOOD, the first zero-shot, unsupervised framework that utilizes meta-learning to automatically select an OOD detection model. As a meta-learning approach, MetaOOD leverages historical performance data of existing methods across various benchmark OOD datasets, enabling the effective selection of a suitable model for new datasets without the need for labeled data at the test time. To quantify task similarities more accurately, we introduce language model-based embeddings that capture the distinctive OOD characteristics of both datasets and detection models. Through extensive experimentation with 24 unique test dataset pairs to choose from among 11 OOD detection models, we demonstrate that MetaOOD significantly outperforms existing methods and only brings marginal time overhead. Our results, validated by Wilcoxon statistical tests, show that MetaOOD surpasses a diverse group of 11 baselines, including established OOD detectors and advanced unsupervised selection methods."
    },
    {
        "title": "MGDA Converges under Generalized Smoothness, Provably",
        "link_suffix": "/forum?id=wgDB1QuxIA",
        "link": "https://openreview.net/forum?id=wgDB1QuxIA",
        "pdf_link": "https://openreview.net/pdf?id=wgDB1QuxIA",
        "keywords": "Multi-Objective Optimization, Generalized Smoothness, Convergence Analysis, Sample Complexity",
        "abstract": "Multi-objective optimization (MOO) is receiving more attention in various fields such as multi-task learning. Recent works provide some effective algorithms with theoretical analysis but they are limited by the standard $L$-smooth or bounded-gradient assumptions, which typically do not hold for neural networks, such as Long short-term memory (LSTM) models and Transformers. In this paper, we study a more general and realistic class of generalized $\\ell$-smooth loss functions, where $\\ell$ is a general non-decreasing function of gradient norm. We revisit and analyze the fundamental multiple gradient descent algorithm (MGDA) and its stochastic version with double sampling for solving the generalized $\\ell$-smooth MOO problems,  which approximate the conflict-avoidant (CA) direction that maximizes the minimum improvement among objectives. We provide a comprehensive convergence analysis of these algorithms and show that they converge to an $\\epsilon$-accurate Pareto stationary point with a guaranteed $\\epsilon$-level average CA distance (i.e., the gap between the updating direction and the CA direction) over all iterations,  where totally $\\mathcal{O}(\\epsilon^{-2})$ and $\\mathcal{O}(\\epsilon^{-4})$ samples are needed for deterministic and stochastic settings, respectively.  We prove that they can also guarantee a tighter $\\epsilon$-level CA distance in each iteration using more samples. Moreover, we analyze an efficient variant of MGDA named MGDA-FA using only $\\mathcal{O}(1)$ time and space, while achieving the same performance guarantee as MGDA."
    },
    {
        "title": "Thermodynamic Natural Gradient Descent",
        "link_suffix": "/forum?id=icNel2Thrt",
        "link": "https://openreview.net/forum?id=icNel2Thrt",
        "pdf_link": "https://openreview.net/pdf?id=icNel2Thrt",
        "keywords": "optimization, gradient descent, natural gradient descent, thermodynamic computing, analog computing",
        "abstract": "Second-order training methods have better convergence properties than gradient descent but are rarely used in practice for large-scale training due to their computational overhead. This can be viewed as a hardware limitation (imposed by digital computers). Here we show that natural gradient descent (NGD), a second-order method, can have a similar computational complexity per iteration to a first-order method, when employing appropriate hardware. We present a new hybrid digital-analog algorithm for training neural networks that is equivalent to NGD in a certain parameter regime but avoids prohibitively costly linear system solves. Our algorithm exploits the thermodynamic properties of an analog system at equilibrium, and hence requires an analog thermodynamic computer. The training occurs in a hybrid digital-analog loop, where the gradient and Fisher information matrix (or any other positive semi-definite curvature matrix) are calculated at given time intervals while the analog dynamics take place. We numerically demonstrate the superiority of this approach over state-of-the-art digital first- and second-order training methods on classification tasks and language model fine-tuning tasks."
    },
    {
        "title": "Audio Prototypical Network for Controllable Music Recommendation",
        "link_suffix": "/forum?id=pKDmt7pc6h",
        "link": "https://openreview.net/forum?id=pKDmt7pc6h",
        "pdf_link": "https://openreview.net/pdf?id=pKDmt7pc6h",
        "keywords": "Scrutable Recommender System, Controllability, Explainable Machine Learning",
        "abstract": "Traditional recommendation systems represent user preferences in dense representations obtained through black-box encoder models. While these models often provide strong recommendation performance, they lack interpretability for users, leaving users unable to understand or control the system\u2019s modeling of their preferences. This limitation is especially challenging in music recommendation, where user preferences are highly personal and often evolve based on nuanced qualities like mood, genre, tempo, or instrumentation. \n    In this paper, we propose an audio prototypical network for controllable music recommendation. This network expresses user preferences in terms of prototypes representative of semantically meaningful features pertaining to musical qualities. We show that the model obtains competitive recommendation performance compared to popular baseline models while also providing interpretable and controllable user profiles."
    },
    {
        "title": "Faster, More Efficient RLHF through Off-Policy Asynchronous Learning",
        "link_suffix": "/forum?id=FhTAG591Ve",
        "link": "https://openreview.net/forum?id=FhTAG591Ve",
        "pdf_link": "https://openreview.net/pdf?id=FhTAG591Ve",
        "keywords": "reinforcement learning from human feedback, efficient llm finetuning, off-policy RL",
        "abstract": "The dominant paradigm for RLHF isonlineandon-policyRL: synchronously generating from the large language model (LLM) policy, labelling with a reward model, and learning using feedback on the LLM's own outputs. While performant, it is computationally inefficient. Inspired by classical deep RL literature, we propose separating generation and learning in RLHF. This enables asynchronous generation of new samples while simultaneously training  on old samples, leading to faster training and more compute-optimal scaling. However, asynchronous training relies on an underexplored regime, online butoff-policyRLHF: learning on samples from previous iterations of our model. To understand the challenges in this regime, we investigate a fundamental question: how much off-policyness can we tolerate for asynchronous training to speed up learning but maintain performance? Among several RLHF algorithms we tested, we find that online DPO is most robust to off-policy data, and robustness increases with the scale of the policy model. We show even further compute optimizations but demonstrate that they come at a performance cost, giving rise to a trade-off. Finally, we verify our design choices by training LLaMA 3.1 8B with RLHF on instruction following tasks 40% faster than a synchronous run while matching final performance measured with GPT-4o."
    },
    {
        "title": "Label Privacy in Split Learning for Large Models with Parameter-Efficient Training",
        "link_suffix": "/forum?id=DRKkO2Tejc",
        "link": "https://openreview.net/forum?id=DRKkO2Tejc",
        "pdf_link": "https://openreview.net/pdf?id=DRKkO2Tejc",
        "keywords": "Split Learning, Vertical Federated Learning, Federated Learning, Parameter Efficient Fine-tuning, Privacy, Large Language Models",
        "abstract": "As deep learning models become larger and more expensive, many practitioners turn to fine-tuning APIs. \nThese web services allow fine-tuning a model between two parties: the client that provides the data, and the server that hosts the model.\nWhile convenient, these APIs raise a new concern: the data of the client is at risk of privacy breach during the training procedure.\nThis challenge presents an important practical case of vertical federated learning, where the two parties perform parameter-efficient fine-tuning (PEFT) of a large model.\nIn this study, we systematically search for a way to fine-tune models over an APIwhile keeping the labels private.\nWe analyze the privacy of LoRA, a popular approach for parameter-efficient fine-tuning when training over an API.\nUsing this analysis, we propose P$^3$EFT, a multi-party  split learning algorithm that takes advantage of existing PEFT properties to maintain privacy at a lower performance overhead.\nTo validate our algorithm, we fine-tune DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range of NLP tasks. We find that P$^3$EFT is competitive with existing privacy-preserving methods in multi-party and  two-party setups while having higher accuracy."
    },
    {
        "title": "RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph",
        "link_suffix": "/forum?id=dw9VUsSHGB",
        "link": "https://openreview.net/forum?id=dw9VUsSHGB",
        "pdf_link": "https://openreview.net/pdf?id=dw9VUsSHGB",
        "keywords": "AI Software Engineering, Large Language Models, Code Intelligence, Graph Structure",
        "abstract": "Large Language Models (LLMs) excel in code generation yet struggle with modern AI software engineering tasks. Unlike traditional function-level or file-level coding tasks, AI software engineering requires not only basic coding proficiency but also advanced skills in managing and interacting with code repositories. However, existing methods often overlook the need for repository-level code understanding, which is crucial for accurately grasping the broader context and developing effective solutions. On this basis, we present RepoGraph, a plug-in module that manages a repository-level structure for modern AI software engineering solutions. RepoGraph offers the desired guidance and serves as a repository-wide navigation for AI software engineers. We evaluate RepoGraph on the SWE-bench by plugging it into four different methods of two lines of approaches, where RepoGraph substantially boosts the performance of all systems, leading toa new state-of-the-artamong open-source frameworks. Our analyses also demonstrate the extensibility and flexibility of RepoGraph by testing on another repo-level coding benchmark, CrossCodeEval. Our code is available athttps://anonymous.4open.science/r/Repo_Graph."
    },
    {
        "title": "ET-Plan-Bench: Embodied Task-level Planning Benchmark Towards Spatial-Temporal Cognition with Foundation Models",
        "link_suffix": "/forum?id=UiLtbLsiPU",
        "link": "https://openreview.net/forum?id=UiLtbLsiPU",
        "pdf_link": "https://openreview.net/pdf?id=UiLtbLsiPU",
        "keywords": "Embodied task planning benchmark",
        "abstract": "Recent advancements in Large Language Models (LLMs) have spurred numerous attempts to apply these technologies to embodied tasks, particularly focusing on high-level task planning and task decomposition. To further explore this area, we introduce a new embodied task planning benchmark, ET-Plan-Bench, which specifically targets embodied task planning using LLMs. It features a controllable and diverse set of embodied tasks varying in different levels of difficulties and complexities, and is designed to evaluate two critical dimensions of LLMs' application in embodied task understanding: spatial (relation constraint, occlusion for target objects) and temporal & causal understanding of the sequence of actions in the environment. By using multi-source simulators as the backend simulator, it can provide immediate environment feedback to LLMs, which enables LLMs to interact dynamically with the environment and re-plan as necessary. We  evaluated the state-of-the-art open source and closed source foundation models, including GPT-4, LLAMA and Mistral on our proposed benchmark. While they perform adequately well on simple navigation tasks, their performance can significantly deteriorate when faced with tasks that require a deeper understanding of spatial, temporal, and causal relationships. Thus, our benchmark distinguishes itself as a large-scale, quantifiable, highly automated, and fine-grained diagnostic framework that presents a significant challenge to the latest foundation models. We hope it can spark and drive further research in embodied task planning using foundation models."
    },
    {
        "title": "On the Price of Differential Privacy for Hierarchical Clustering",
        "link_suffix": "/forum?id=yLhJYvkKA0",
        "link": "https://openreview.net/forum?id=yLhJYvkKA0",
        "pdf_link": "https://openreview.net/pdf?id=yLhJYvkKA0",
        "keywords": "Hierarchical clustering, differential privacy, sparsest cut",
        "abstract": "Hierarchical clustering is a fundamental unsupervised machine learning task with the aim of organizing data into a hierarchy of clusters. Many applications of hierarchical clustering involve sensitive user information, therefore motivating recent studies on differentially private hierarchical clustering under the rigorous framework of Dasgupta's objective. However, it has been shown that any privacy-preserving algorithm under edge-level differential privacy necessarily suffers a large error. To capture practical applications of this problem, we focus on the weight privacy model, where each edge of the input graph is at least unit weight. We present a novel algorithm in the weight privacy model that shows significantly better approximation than known impossibility results in the edge-level DP setting. In particular, our algorithm achieves $O(\\log^{1.5}n/\\varepsilon)$ multiplicative error for $\\varepsilon$-DP and runs in polynomial time, where $n$ is the size of the input graph, and the cost is never worse than the optimal additive error in existing work. We complement our algorithm by showing if the unit-weight constraint does not apply, the lower bound for weight-level DP hierarchical clustering is essentially the same as the edge-level DP, i.e. $\\Omega(n^2/\\varepsilon)$ additive error. As a result, we also obtain a new lower bound of $\\tilde{\\Omega}(1/\\varepsilon)$ additive error for balanced sparsest cuts in the weight-level DP model, which may be of independent interest. Finally, we evaluate our algorithm on synthetic and real-world datasets. Our experimental results show that our algorithm performs well in terms of extra cost and has good scalability to large graphs."
    },
    {
        "title": "Generalization Aware Minimization",
        "link_suffix": "/forum?id=XvA1Mn9OFy",
        "link": "https://openreview.net/forum?id=XvA1Mn9OFy",
        "pdf_link": "https://openreview.net/pdf?id=XvA1Mn9OFy",
        "keywords": "generalization, sharpness aware minimization, loss landscape, optimization",
        "abstract": "Sharpness-Aware Minimization (SAM) algorithms have effectively improved neural network generalization by steering model parameters away from sharp regions of the training loss landscape, which tend to generalize poorly. However, the underlying mechanisms of SAM are not fully understood, and recent studies question whether its bias toward flatter regions is why it improves generalization. In this work, we introduce Generalization-Aware Minimization (GAM), a generalized version of SAM that employs multiple perturbation steps instead of SAM's single-step perturbations. This allows GAM to directly guide model parameters toward areas of the landscape that generalize better. We show that the expected true (test) loss landscape is a rescaled version of the observed training loss landscape and demonstrate how GAM's multiple perturbative updates can be designed to optimize this expected true loss. Finally, we present a practical online algorithm that adapts GAM's perturbative steps during training to improve generalization, and we empirically validate its superior performance over SAM on benchmark datasets. We believe GAM sheds light on the generalization improvements of sharpness-based algorithms and can inspire the development of optimizers with even better generalization."
    },
    {
        "title": "Goal Achievement Guided Exploration: Mitigating Premature Convergence in Reinforcement Learning",
        "link_suffix": "/forum?id=90UhF7e8jo",
        "link": "https://openreview.net/forum?id=90UhF7e8jo",
        "pdf_link": "https://openreview.net/pdf?id=90UhF7e8jo",
        "keywords": "reinforcement learning, exploration, deep reinforcement learning",
        "abstract": "Premature convergence to suboptimal policies remains a significant challenge in reinforcement learning (RL), particularly in tasks with sparse rewards or non-convex reward landscapes. Existing work usually utilizes reward shaping, such as curiosity-based internal rewards, to encourage exploring promising spaces. However, this may inadvertently introduce new local optima and impair the optimization for the actual target reward. To address this issue, we propose Goal Achievement Guided Exploration (GAGE), a novel approach that incorporates an agent's goal achievement as a dynamic criterion for balancing exploration and exploitation. GAGE adaptively adjusts the exploitation level based on the agent's current performance relative to an estimated optimal performance, thereby mitigating premature convergence. Extensive evaluations demonstrate that GAGE substantially improves learning outcomes across various challenging tasks by adapting convergence based on task success. Applicable to both continuous and discrete tasks, GAGE seamlessly integrates into existing RL frameworks, highlighting its potential as a versatile tool for enhancing exploration strategies in RL."
    },
    {
        "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-Time Alignment",
        "link_suffix": "/forum?id=J0qTpmbSbh",
        "link": "https://openreview.net/forum?id=J0qTpmbSbh",
        "pdf_link": "https://openreview.net/pdf?id=J0qTpmbSbh",
        "keywords": "large language model alignment; controlled decoding;",
        "abstract": "Large Language Models (LLMs) exhibit impressive capabilities but require careful alignment with human preferences. Traditional training-time methods finetune LLMs using human preference datasets but incur significant training costs and require repeated training to handle diverse user preferences. Test-time alignment methods address this by using reward models (RMs) to guide frozen LLMs without retraining. However, existing test-time approaches rely on trajectory-level RMs which are designed to evaluate complete responses, making them unsuitable for autoregressive text generation that requires computing next-token rewards from partial responses. To address this, we introduce GenARM, a test-time alignment approach that leverages the Autoregressive Reward Model\u2014a novel reward parametrization designed to predict next-token rewards for efficient and effective autoregressive generation. Theoretically, we demonstrate that this parametrization can provably guide frozen LLMs toward any distribution achievable by traditional RMs within the KL-regularized reinforcement learning framework. Experimental results show that GenARM significantly outperforms prior test-time alignment baselines and matches the performance of training-time methods. Additionally, GenARM enables efficient weak-to-strong guidance, aligning larger LLMs with smaller RMs without the high costs of training larger models. Furthermore, GenARM supports multi-objective alignment, allowing real-time trade-offs between preference dimensions and catering to diverse user preferences without retraining."
    },
    {
        "title": "A Framework for Finding Local Saddle Points in Two-Player Zero-Sum Black-Box Games",
        "link_suffix": "/forum?id=H2zRScPSR7",
        "link": "https://openreview.net/forum?id=H2zRScPSR7",
        "pdf_link": "https://openreview.net/pdf?id=H2zRScPSR7",
        "keywords": "black box optimization, zero sum games, saddle points, Bayesian optimization",
        "abstract": "Saddle point optimization is a critical problem employed in numerous real-world applications, including portfolio optimization, generative adversarial networks, and robotics. It has been extensively studied in cases where the objective function is known and differentiable. Existing work in black-box settings with unknown objectives that can only be sampled either assumes convexity-concavity in the objective to simplify the problem or operates with noisy gradient estimators. In contrast, we introduce a framework inspired by Bayesian optimization which utilizes Gaussian processes to model the unknown (potentially nonconvex-nonconcave) objective and requires only zeroth-order samples. Our approach frames the saddle point optimization problem as a two-level process which can flexibly integrate existing and novel approaches to this problem. The upper level of our framework produces a model of the objective function by sampling in promising locations, and the lower level of our framework uses the existing model to frame and solve a general-sum game to identify locations to sample. This lower level procedure can be designed in complementary ways, and we demonstrate the flexibility of our approach by introducing variants which appropriately trade off between factors like runtime, the cost of function evaluations, and the number of available initial samples. We experimentally demonstrate these algorithms on synthetic and realistic datasets, showcasing their ability to efficiently locate local saddle points in these contexts."
    },
    {
        "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
        "link_suffix": "/forum?id=LC2KxRwC3n",
        "link": "https://openreview.net/forum?id=LC2KxRwC3n",
        "pdf_link": "https://openreview.net/pdf?id=LC2KxRwC3n",
        "keywords": "Sparse Autoencoders, SAEs, LLMs, interpretability",
        "abstract": "Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call \"feature absorption\" where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution."
    }
]