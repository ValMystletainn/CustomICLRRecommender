[{"title": "CursorCore: Assist Programming through Aligning Anything", "link_suffix": "/forum?id=QxbJYBZVbE", "link": "https://openreview.net/forum?id=QxbJYBZVbE", "pdf_link": "https://openreview.net/pdf?id=QxbJYBZVbE", "keywords": "AI-Assisted Programming, Large Language Models for Code, Code Benchmarks", "abstract": "Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants.", "title_embedding_index": 16200, "title_abs_embedding_index": 16225}, {"title": "Strong Model Collapse", "link_suffix": "/forum?id=et5l9qPUhm", "link": "https://openreview.net/forum?id=et5l9qPUhm", "pdf_link": "https://openreview.net/pdf?id=et5l9qPUhm", "keywords": "Model Collapse, Regression, High dimensional asymptotics, Synthetic Data, Scaling Laws", "abstract": "Within the scaling laws paradigm, which underpins the training of large neural networks like ChatGPT and Llama, we consider a supervised regression setting and establish a strong form of the model collapse phenomenon, a critical performance degradation due to synthetic data in the training corpus. Our results show that even the smallest fraction of synthetic data (e.g., as little as 1 per 1000) can still lead to model collapse: larger and larger training sets do not enhance performance.  We further investigate whether increasing model size, an approach aligned with current trends in training large language models, exacerbates or mitigates model collapse. In a simplified regime where neural networks are approximated via random projections of tunable size, we both theoretically and empirically show that larger models can amplify model collapse. Interestingly, our theory also indicates that, beyond the interpolation threshold (which can be extremely high for very large datasets), larger models may mitigate the collapse, although they do not entirely prevent it. Our theoretical findings are empirically verified through experiments on language models and neural networks for images.", "title_embedding_index": 16201, "title_abs_embedding_index": 16226}, {"title": "Compositional Generative Multiphysics and Multi-component Simulation", "link_suffix": "/forum?id=ElDpb1BWE3", "link": "https://openreview.net/forum?id=ElDpb1BWE3", "pdf_link": "https://openreview.net/pdf?id=ElDpb1BWE3", "keywords": "multiphysics, multi-component, PDE simulation, physical simulation, generative", "abstract": "Multiphysics simulation, which models the interactions between multiple physical fields, and multi-component simulation of complex structures are critical in fields like nuclear and aerospace engineering. Previous studies often rely on numerical solvers or machine learning-based surrogate models to solve or accelerate these simulations. However, multiphysics simulations typically require integrating multiple specialized solvers\u2014each responsible for evolving a specific physical field\u2014into a coupled program, which introduces significant development challenges.  Furthermore, no universal algorithm exists for multi-component simulations, which adds to the complexity.\nHere we propose compositional Multiphysics and Multi-component Simulation with Diffusion models (MulSimDiff) to overcome these challenges. During diffusion-based training, MulSimDiff learns energy functions modeling the conditional probability of one physical field/component conditioned on other fields/components. In inference, MulSimDiff generates coupled multiphysics solutions and multi-component structures by sampling from the joint probability distribution, achieved by composing the learned energy functions in a structured way. We test our method in three tasks. In the reaction-diffusion and nuclear thermal coupling problems, MulSimDiff successfully predicts the coupling solution using decoupled data, while the surrogate model fails in the more complex second problem. For the thermal and mechanical analysis of the prismatic fuel element, MulSimDiff trained for single component prediction accurately predicts a larger structure with 64 components, reducing the relative error by 40.3% compared to the surrogate model.", "title_embedding_index": 16202, "title_abs_embedding_index": 16227}, {"title": "Differentially Private Federatedk-Means with Server-Side Data", "link_suffix": "/forum?id=WhIuLQWCWS", "link": "https://openreview.net/forum?id=WhIuLQWCWS", "pdf_link": "https://openreview.net/pdf?id=WhIuLQWCWS", "keywords": "Clustering, Differential Privacy, Federated Learning", "abstract": "Clustering has long been a cornerstone of data analysis. It is particularly suited to identifying coherent subgroups or substructures in unlabeled data, as are generated continuously in large amounts these days. However, in many cases traditional clustering methods are not applicable, because data are increasingly being produced and stored in a distributed way, e.g. on edge devices, and privacy concerns prevent it from being transferred to a central server. To address this challenge, we present FedDP-KMeans, a new algorithm for k-means clustering that is fully-federated as well as differentially private. Our approach leverages (potentially small and out-of-distribution) server-side data to overcome the primary challenge of differentially private clustering methods: the need for a good initialization. Combining our initialization with a simple federated DP-Lloyds algorithm we obtain an algorithm that achieves excellent results on synthetic and real-world benchmark tasks. We also provide a theoretical analysis of our method that provides bounds on the convergence speed and cluster identification success.", "title_embedding_index": 16203, "title_abs_embedding_index": 16228}, {"title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom", "link_suffix": "/forum?id=AkUer8ooMi", "link": "https://openreview.net/forum?id=AkUer8ooMi", "pdf_link": "https://openreview.net/pdf?id=AkUer8ooMi", "keywords": "Proactive reasoning, Problem-oriented, LLM-assisted multi-modal reasoning, Large vision-language model, Visual reasoning, Multi-step reasoning framework", "abstract": "Large vision-language models (LVLMs) have witnessed significant progress on visual understanding tasks. \nHowever, they often prioritize language knowledge over image information on visual reasoning tasks, incurring performance degradation.\nTo tackle this issue,  we first identify the drawbacks of existing solutions (i.e., insufficient and irrelevant visual descriptions, and limited multi-modal capacities).\nWe then decompose visual reasoning process into two stages: visual perception (i.e., eyesight) and textual reasoning (i.e., wisdom), and introduce a novel visual reasoning framework named ProReason. \nThis framework features multi-run proactive perception and decoupled vision-reasoning capabilities.\nBriefly, given a multi-modal question, ProReason iterates \nproactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual descriptions.\nNotably, the disassociation of capabilities allows seamless integration of existing large language models (LLMs) to compensate for the reasoning deficits of LVLMs.\nOur extensive experiments demonstrate that ProReason outperforms both existing multi-step reasoning frameworks and passive peer methods on a wide range of benchmarks\nfor both open-source and closed-source models.\nIn addition, with the assistance of LLMs,\nProReason achieves a performance improvement of up to 15%\non MMMU benchmark. \nOur insights into existing solutions and the decoupled perspective for feasible integration of LLMs illuminate future research on visual reasoning techniques, especially LLM-assisted ones.", "title_embedding_index": 16204, "title_abs_embedding_index": 16229}, {"title": "Implicit Search via Discrete Diffusion: A Study on Chess", "link_suffix": "/forum?id=A9y3LFX4ds", "link": "https://openreview.net/forum?id=A9y3LFX4ds", "pdf_link": "https://openreview.net/pdf?id=A9y3LFX4ds", "keywords": "discrete diffusion model, search, planning, chess, MCTS", "abstract": "In the post-AlphaGo era, there has been a resurgence of interest in search techniques like Monte Carlo Tree Search (MCTS) within the realm of Large Language Models (LLMs). This renewed attention is driven by the recognition that current next-token prediction models often lack the ability for long-term planning. Is it possible to instill search-like abilities within the models to enhance their planning abilities without relying on explicit search? We propose DiffuSearch, a model that does implicit search by looking into the future world via discrete diffusion modeling. We instantiate DiffuSearch on a classical board game, Chess, where explicit search is known to be essential. Through extensive controlled experiments, we show DiffuSearch outperforms both the searchless and explicit search-enhanced policies. Specifically, DiffuSearch outperforms the one-step policy by 19.2% and the MCTS-enhanced policy by 14% on action accuracy. Furthermore, DiffuSearch demonstrates a notable 30% enhancement in puzzle-solving abilities compared to explicit search, along with a significant 540 Elo increase in game-playing strength assessment.", "title_embedding_index": 16205, "title_abs_embedding_index": 16230}, {"title": "Leveraging Additional Information in POMDPs with Guided Policy Optimization", "link_suffix": "/forum?id=VRRuYBaq9u", "link": "https://openreview.net/forum?id=VRRuYBaq9u", "pdf_link": "https://openreview.net/pdf?id=VRRuYBaq9u", "keywords": "Reinforcement Learning, Imitation Learning, POMDP, policy gradient", "abstract": "Reinforcement Learning (RL) in partially observable environments poses significant challenges due to the complexity of learning under uncertainty. \nWhile additional information, such as that available in simulations, can enhance training, effectively leveraging it remains an open problem. \nTo address this, we introduce Guided Policy Optimization (GPO), a framework that co-trains a guider and a learner. \nThe guider takes advantage of supplementary information while ensuring alignment with the learner's policy, which is primarily trained via Imitation Learning (IL). \nWe theoretically demonstrate that this learning scheme achieves optimality comparable to direct RL, thereby overcoming key limitations inherent in IL approaches. \nOur approach includes two practical variants, GPO-penalty and GPO-clip, and empirical evaluations show strong performance across various tasks, including continuous control with partial observability and noise, and memory-based challenges, significantly outperforming existing methods.", "title_embedding_index": 16206, "title_abs_embedding_index": 16231}, {"title": "Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning", "link_suffix": "/forum?id=NRYgUzSPZz", "link": "https://openreview.net/forum?id=NRYgUzSPZz", "pdf_link": "https://openreview.net/pdf?id=NRYgUzSPZz", "keywords": "diffusion model, autoregressive model, complex reasoning", "abstract": "Autoregressive language models, despite their impressive capabilities, struggle with complex reasoning and long-term planning tasks. We introduce discrete diffusion models as a novel solution to these challenges. Through the lens of subgoal imbalance, we demonstrate how diffusion models effectively learn difficult subgoals that elude autoregressive approaches. We propose Multi-granularity Diffusion Modeling (MDM), which prioritizes subgoals based on difficulty during learning. On complex tasks like Countdown, Sudoku, and Boolean Satisfiability Problems, MDM significantly outperforms autoregressive models without using search techniques. For instance, MDM achieves 91.5% and 100% accuracy on Countdown and Sudoku, respectively, compared to 45.8% and 20.7% for autoregressive models. Our work highlights the potential of diffusion-based approaches in advancing AI capabilities for sophisticated language understanding and problem-solving tasks.", "title_embedding_index": 16207, "title_abs_embedding_index": 16232}, {"title": "RGB-Event ISP: The Dataset and Benchmark", "link_suffix": "/forum?id=BqtoARyz7Y", "link": "https://openreview.net/forum?id=BqtoARyz7Y", "pdf_link": "https://openreview.net/pdf?id=BqtoARyz7Y", "keywords": "event camera, image signal processor, color correction, denoising", "abstract": "Event-guided imaging has received significant attention due to its potential to revolutionize instant imaging systems. However, the prior methods primarily focus on enhancing RGB images in a post-processing manner, neglecting the challenges of image signal processor (ISP) dealing with event sensor and the benefits events provide for reforming the ISP process. To achieve this, we conduct the first research on event-guided ISP. First, we present a new event-RAW paired dataset, collected with a novel but still confidential sensor that records pixel-level aligned events and RAW images. This dataset includes 3373 RAW images with $2248\\times 3264$ resolution and their corresponding events, spanning 24 scenes with 3 exposure modes and 3 lenses. Second, we propose a convential ISP pipeline to generate good RGB frames as reference. This convential ISP pipleline performs basic ISP operations, e.g., demosaicing, white balancing, denoising and color space transforming, with a ColorChecker as reference. Third, we classify the existing learnable ISP methods into 3 classes, and select multiple methods to train and evaluate on our new dataset. Lastly, since there is no prior work for reference, we propose a simple event-guided ISP method and test it on our dataset. We further put forward key technical challenges and future directions in RGB-Event ISP. In summary, to the best of our knowledge, this is the very first research focusing on event-guided ISP, and we hope it will inspire the community.", "title_embedding_index": 16208, "title_abs_embedding_index": 16233}, {"title": "Diff-Prompt: Diffusion-driven Prompt Generator with Mask Supervision", "link_suffix": "/forum?id=LfghnrSJNg", "link": "https://openreview.net/forum?id=LfghnrSJNg", "pdf_link": "https://openreview.net/pdf?id=LfghnrSJNg", "keywords": "prompt learning, diffusion model, multimodal learning", "abstract": "Prompt learning has demonstrated promising results in fine-tuning pre-trained multimodal models. However, the performance improvement is limited when applied to more complex and fine-grained tasks. The reason is that most existing methods directly optimize the parameters involved in the prompt generation process through loss backpropagation, which constrains the richness and specificity of the prompt representations. In this paper, we propose Diff-Prompt (Diffusion-driven Prompt Generator), aiming to use the diffusion model to generate rich, fine-grained prompt information for complex downstream tasks. Specifically, our approach consists of three stages. In the first stage, we train a Mask-VAE to compress the masks into latent space. In the second stage, we leverage an improved DiT (Diffusion Transformer) to train a prompt generator in the latent space, using the masks for supervision. In the third stage, we align the denoising process of the prompt generator with the pre-trained model in the semantic space, and use the generated prompts to fine-tune the model. We conduct experiments on a complex pixel-level downstream task, referring expression comprehension, and compare our method with various parameter-efficient fine-tuning approaches. Diff-Prompt achieves a maximum improvement of 8.87 in R@1 and 14.05 in R@5 compared to the foundation model and also outperforms other state-of-the-art methods across multiple metrics. The experimental results validate the effectiveness of our approach and highlight the potential of using generative models for prompt generation. Code is available athttps://anonymous.4open.science/r/Diff-Prompt-FF2D.", "title_embedding_index": 16209, "title_abs_embedding_index": 16234}, {"title": "Keep Your Friends Close, and Your Enemies Farther: Distance-aware Voxel-wise Contrastive Learning for Semi-supervised Multi-organ Segmentation", "link_suffix": "/forum?id=eBgIzHaegm", "link": "https://openreview.net/forum?id=eBgIzHaegm", "pdf_link": "https://openreview.net/pdf?id=eBgIzHaegm", "keywords": "Semi-supervised Learning; Contrastive Learning\uff1bMulti-organ Segmentation\uff1bMedical Image Segmentation\uff1b", "abstract": "Voxel-wise contrastive learning (VCL) is a prominent approach in semi-supervised medical image segmentation. Based on the initially generated pseudo-labels, VCL pulls voxels with the same pseudo-labels toward their prototypes while pushes those with different labels apart, thereby learns effective representations for the segmentation task. However, in multi-organ segmentation (MoS), the complex anatomical structures of certain organs often lead to many unreliable pseudo-labels. Directly applying VCL can introduce confirmation bias, resulting in poor segmentation performance. A common practice is to first transform these unreliable pseudo-labels into more reliable complementary ones, which represent classes that voxels are least likely to belong to, and then push voxels away from the prototypes of their complementary labels.  However, we find that in this approach, if voxels with unreliable pseudo-labels are originally close in feature space, they can end up far apart after being pushed away from their complementary prototypes. This disruption of the semantic relationships among voxels can be detrimental to the MoS task. In this paper, we propose DVCL, a novel distance-aware VCL method for semi-supervised MoS. DVCL is based on the observation that voxels close to each other in the feature space ('neighbors') likely belong to the same semantic category, while distant ones ('outsiders') likely belong to different categories. In DVCL, we first identify neighbors and outsiders for all voxels with unreliable pseudo-labels, and then pull their neighbors into the same clusters while pushing outsiders away. In this way, neighbors of unreliable voxels remain their neighbors and outsiders remain outsiders. This approach helps maintain useful semantic relationships among unreliable voxels while still enjoying the advantages of VCL. We conduct extensive experiments on four datasets to validate the effectiveness.  Extensive experiments on four datasets demonstrate the superior performance of DVCL compared to state-of-the-art methods.", "title_embedding_index": 16210, "title_abs_embedding_index": 16235}, {"title": "Provable Privacy Attacks on Trained Shallow Neural Networks", "link_suffix": "/forum?id=GlPVnuL66V", "link": "https://openreview.net/forum?id=GlPVnuL66V", "pdf_link": "https://openreview.net/pdf?id=GlPVnuL66V", "keywords": "Deep Learning Theory, Privacy, Neural Networks, Membership Inference Attack", "abstract": "We study what provable privacy attacks can be shown on trained, 2-layer ReLU neural networks. We explore two types of attacks; data reconstruction attacks, and membership inference attacks. We prove that theoretical results on the implicit bias of 2-layer neural networks can be used to provably reconstruct a set of which at least a constant fraction are training points in a univariate setting, and can also be used to identify with high probability whether a given point was used in the training set in a high dimensional setting. To the best of our knowledge, our work is the first to show provable vulnerabilities in this setting.", "title_embedding_index": 16211, "title_abs_embedding_index": 16236}, {"title": "Safeguarding System Prompts: A Surrogate-Based Defense Against Injection Attacks", "link_suffix": "/forum?id=5eqkTIQD9v", "link": "https://openreview.net/forum?id=5eqkTIQD9v", "pdf_link": "https://openreview.net/pdf?id=5eqkTIQD9v", "keywords": "LLM safety;", "abstract": "System prompts, essential for guiding model outputs, play a pivotal role as large language models proliferate across diverse applications. Despite their importance, these prompts are highly vulnerable to injection attacks. Intuitively, adding defensive prompts and implementing output filtering could offer strong protection, but these defenses rely on direct access to the system prompt\u2014a luxury increasingly unavailable in today\u2019s evolving prompt market and third-party defense scenarios, where prompts must remain concealed and confidential. To address this pressing limitation, we introduce SurF (Surrogate-based Filtering), a novel approach that compensates for the lack of system prompt access by utilizing a surrogate prompt pool. Namely, we leverage the prompt pool as the surrogate of the system prompt. Once a potential leak from this pool is identified, the input is classified as harmful, and the system resists generating a response. Experiments on various models, including both offline and online LLM services, demonstrate SurF\u2019s effectiveness in reducing attack success rates. Furthermore, we evaluate the trade-off between defense robustness and response consistency on natural inputs using a response-following metric. Our findings indicate that while stronger defenses reduce attack success, they may also degrade the quality of legitimate responses.", "title_embedding_index": 16212, "title_abs_embedding_index": 16237}, {"title": "DRoP: Distributionally Robust Pruning", "link_suffix": "/forum?id=fxv0FfmDAg", "link": "https://openreview.net/forum?id=fxv0FfmDAg", "pdf_link": "https://openreview.net/pdf?id=fxv0FfmDAg", "keywords": "Data Pruning, Classification Bias, Robustness", "abstract": "In the era of exceptionally data-hungry models, careful selection of the training data is essential to mitigate the extensive costs of deep learning. Data pruning offers a solution by removing redundant or uninformative samples from the dataset, which yields faster convergence and improved neural scaling laws. However, little is known about its impact on classification bias of the trained models. We conduct the first systematic study of this effect and reveal that existing data pruning algorithms can produce highly biased classifiers. We present theoretical analysis of the classification risk in a mixture of Gaussians to argue that choosing appropriate class pruning ratios, coupled with random pruning within classes  has potential to improve worst-class performance. We thus propose DRoP, a distributionally robust approach to pruning and empirically demonstrate its performance on standard computer vision benchmarks. In sharp contrast to existing algorithms, our proposed method continues improving distributional robustness at a tolerable drop of average performance as we prune more from the datasets.", "title_embedding_index": 16213, "title_abs_embedding_index": 16238}, {"title": "Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension", "link_suffix": "/forum?id=L5dUM6prKw", "link": "https://openreview.net/forum?id=L5dUM6prKw", "pdf_link": "https://openreview.net/pdf?id=L5dUM6prKw", "keywords": "Natural Perturbations, Robustness Evaluation, Machine Reading Comprehension", "abstract": "As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.", "title_embedding_index": 16214, "title_abs_embedding_index": 16239}, {"title": "Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models", "link_suffix": "/forum?id=2tIyA5cri8", "link": "https://openreview.net/forum?id=2tIyA5cri8", "pdf_link": "https://openreview.net/pdf?id=2tIyA5cri8", "keywords": "reinforcement learning, in-context learning, representation learning, sparse autoencoders (SAEs), large language models (LLMs)", "abstract": "In-context learning, the ability to adapt based on a few examples in the input prompt, is a ubiquitous feature of large language models (LLMs). However, as LLMs' in-context learning abilities continue to improve, understanding this phenomenon mechanistically becomes increasingly important. In particular, it is not well-understood how LLMs learn to solve specific classes of problems, such as reinforcement learning (RL) problems, in-context. Through three different tasks, we first show that Llama $3$ $70$B can solve simple RL problems in-context. We then analyze the residual stream of Llama using Sparse Autoencoders (SAEs) and find representations that closely match temporal difference (TD) errors. Notably, these representations emerge despite the model only being trained to predict the next token. We verify that these representations are indeed causally involved in the computation of TD errors and $Q$-values by performing carefully designed interventions on them. Taken together, our work establishes a methodology for studying and manipulating in-context learning with SAEs, paving the way for a more mechanistic understanding.", "title_embedding_index": 16215, "title_abs_embedding_index": 16240}, {"title": "FairDen: Fair Density-Based Clustering", "link_suffix": "/forum?id=aPHHhnZktB", "link": "https://openreview.net/forum?id=aPHHhnZktB", "pdf_link": "https://openreview.net/pdf?id=aPHHhnZktB", "keywords": "Fairness, Density-based Clustering, Unsupervised Learning", "abstract": "Fairness in data mining tasks like clustering has recently become an increasingly important aspect. \nHowever, few clustering algorithms exist that focus on fair groupings of data with sensitive attributes. \nIncluding fairness in the clustering objective is especially hard for density-based clustering, as it does not directly optimize a closed form objective like centroid-based or spectral methods.This paper introduces FairDen, the first fair, density-based clustering algorithm.\nWe capture the dataset's density-connectivity structure in a similarity matrix that we manipulate to encourage a balanced clustering. \nIn contrast to state-of-the-art, FairDen inherently handles categorical attributes, noise, and data with several sensitive attributes or groups.\nWe show that FairDen finds meaningful and fair clusters in extensive experiments.", "title_embedding_index": 16216, "title_abs_embedding_index": 16241}, {"title": "Trajectory attention for fine-grained video motion control", "link_suffix": "/forum?id=2z1HT5lw5M", "link": "https://openreview.net/forum?id=2z1HT5lw5M", "pdf_link": "https://openreview.net/pdf?id=2z1HT5lw5M", "keywords": "Trajectory attention, video generation, motion control", "abstract": "Recent advancements in video generation have been greatly driven by video diffusion models, with camera motion control emerging as a crucial challenge in creating view-customized visual content. This paper introduces trajectory attention, a novel approach that performs attention along available pixel trajectories for fine-grained camera motion control. Unlike existing methods that often yield imprecise outputs or neglect temporal correlations, our approach possesses a stronger inductive bias that seamlessly injects trajectory information into the video generation process. Importantly, our approach models trajectory attention as an auxiliary branch alongside traditional temporal attention. This design enables the original temporal attention and the trajectory attention to work in synergy, ensuring both\nprecise motion control and new content generation capability, which is critical when the trajectory is only partially available. Experiments on camera motion control for images and videos demonstrate significant improvements in precision and long-range consistency while maintaining high-quality generation. Furthermore, we show that our approach can be extended to other video motion control tasks, such as first-frame-guided video editing, where it excels in maintaining content consistency over large spatial and temporal ranges.", "title_embedding_index": 16217, "title_abs_embedding_index": 16242}, {"title": "GraphSTAGE: Channel-Preserving Graph Neural Networks for Time Series Forecasting", "link_suffix": "/forum?id=5dKiZeF3MD", "link": "https://openreview.net/forum?id=5dKiZeF3MD", "pdf_link": "https://openreview.net/pdf?id=5dKiZeF3MD", "keywords": "Time Series Forecasting, Graph Neural Networks, Channel-Preserving", "abstract": "Recent advancements in multivariate time series forecasting (MTSF) have increasingly focused on the core challenge of learning dependencies within sequences, specifically intra-series (temporal), inter-series (spatial), and cross-series dependencies. While extracting multiple types of dependencies can theoretically enhance the richness of learned correlations, it also increases computational complexity and may introduce additional noise. The trade-off between the variety of dependencies extracted and the potential interference has not yet been fully explored. To address this challenge, we propose GraphSTAGE, a purely graph neural network (GNN)-based model that decouples the learning of intra-series and inter-series dependencies. GraphSTAGE features a minimal architecture with a specially designed embedding and patching layer, along with the STAGE (Spatial-Temporal Aggregation Graph Encoder) blocks. Unlike channel-mixing approaches, GraphSTAGE is a channel-preserving method that maintains the shape of the input data throughout training, thereby avoiding the interference and noise typically caused by channel blending. Extensive experiments conducted on 13 real-world datasets demonstrate that our model achieves performance comparable to or surpassing state-of-the-art methods. Moreover, comparative experiments between our channel-preserving framework and channel-mixing designs show that excessive dependency extraction and channel blending can introduce noise and interference. As a purely GNN-based model, GraphSTAGE generates learnable graphs in both temporal and spatial dimensions, enabling the visualization of data periodicity and node correlations to enhance model interpretability.", "title_embedding_index": 16218, "title_abs_embedding_index": 16243}, {"title": "Revolutionizing EMCCD Denoising through a Novel Physics-Based Learning Framework for Noise Modeling", "link_suffix": "/forum?id=vmulbBDCan", "link": "https://openreview.net/forum?id=vmulbBDCan", "pdf_link": "https://openreview.net/pdf?id=vmulbBDCan", "keywords": "EMCCD, physics-based noise modeling, deep high-sensitivity imaging, fluorescence microscopy image denoising", "abstract": "Electron-multiplying charge-coupled device (EMCCD) has been instrumental in sensitive observations under low-light situations including astronomy, material science, and biology. \nDespite its ingenious designs to enhance target signals overcoming read-out circuit noises, produced images are not completely noise free, which could still cast a cloud on desired experiment outcomes, especially in fluorescence microscopy.\nExisting studies on EMCCD's noise model have been focusing on statistical characteristics in theory, yet unable to incorporate latest advancements in the field of computational photography, where physics-based noise models are utilized to guide deep learning processes, creating adaptive denoising algorithms for ordinary image sensors.\nStill, those models are not directly applicable to EMCCD.\nIn this paper, we intend to pioneer EMCCD denoising by introducing a systematic study on physics-based noise model calibration procedures for an EMCCD camera, accurately estimating statistical features of observable noise components in experiments, which are then utilized to generate substantial amount of authentic training samples for one of the most recent neural networks.\nA first real-world test image dataset for EMCCD is captured, containing both images of ordinary daily scenes and those of microscopic contents.\nBenchmarking upon the testset and authentic microscopic images, we demonstrate distinct advantages of our model against previous methods for EMCCD and physics-based noise modeling, forging a promising new path for EMCCD denoising.", "title_embedding_index": 16219, "title_abs_embedding_index": 16244}, {"title": "GROD: Enhancing Generalization of Transformer with Out-of-Distribution Detection", "link_suffix": "/forum?id=zUrdd5NRLH", "link": "https://openreview.net/forum?id=zUrdd5NRLH", "pdf_link": "https://openreview.net/pdf?id=zUrdd5NRLH", "keywords": "OOD detection, learning theory, transformer models", "abstract": "Transformer networks excel in natural language processing (NLP) and computer vision (CV) tasks. However, they face challenges in generalizing to Out-of-Distribution (OOD) datasets, that is, data whose distribution differs from that seen during training. The OOD detection aims to distinguish data that deviates from the expected distribution, while maintaining optimal performance on in-distribution (ID) data. This paper introduces a novel approach based on OOD detection, termed the Generate Rounded OOD Data (GROD) algorithm, which significantly bolsters the generalization performance of transformer networks across various tasks. GROD is motivated by our new OOD detection Probably Approximately Correct (PAC) Theory for transformer. The transformer has learnability in terms of OOD detection that is, when the data is sufficient the outlier can be well represented. By penalizing the misclassification of OOD data within the loss function and generating synthetic outliers, GROD guarantees learnability and refines the decision boundaries between inlier and outlier. This strategy demonstrates robust adaptability and general applicability across different data types. Evaluated across diverse OOD detection tasks in NLP and CV, GROD achieves SOTA regardless of data format. The code is available athttps://anonymous.4open.science/r/GROD-OOD-Detection-with-transformers-B70F.", "title_embedding_index": 16220, "title_abs_embedding_index": 16245}, {"title": "Co3Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion", "link_suffix": "/forum?id=VaowElpVzd", "link": "https://openreview.net/forum?id=VaowElpVzd", "pdf_link": "https://openreview.net/pdf?id=VaowElpVzd", "keywords": "3D co-speech gesture generation, human motion modeling", "abstract": "Generating gestures from human speech has gained tremendous progress in animating virtual avatars. While the existing methods enable synthesizing gestures cooperated by people self-talking, they overlook the practicality of concurrent gesture modeling with two-person interactive conversations. Moreover, the lack of high-quality datasets with concurrent co-speech gestures also limits handling this issue. To fulfill this goal, we first construct a large-scale concurrent co-speech gesture dataset that contains more than 7M frames for diverse two-person interactive posture sequences, dubbed $\\textbf{GES-Inter}$. Moreover, we propose Co$^{\\mathbf{3}}$Gesture, a novel framework that enables concurrent coherent co-speech gesture synthesis including two-person interactive movements. Our framework is built upon two cooperative generation branches conditioned on decomposed speaker audio. Specifically, to enhance the coordination of human postures w.r.t corresponding speaker audios while interacting with the conversational partner, we present a Temporal-Interaction Module ($\\textbf{TIM}$). TIM can effectively model the temporal association representation between two speakers' gesture sequences as interaction guidance and fuse it into the concurrent gesture generation. Then, we devise a mutual attention mechanism to further boost learning dependencies of interacted concurrent motions, thereby enabling us to generate vivid and coherent gestures. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on our newly collected GES-Inter dataset.", "title_embedding_index": 16221, "title_abs_embedding_index": 16246}, {"title": "Targeted synthetic data generation for tabular data via hardness characterization", "link_suffix": "/forum?id=tH12wjcuXx", "link": "https://openreview.net/forum?id=tH12wjcuXx", "pdf_link": "https://openreview.net/pdf?id=tH12wjcuXx", "keywords": "data Shapley, hardness characterization, synthetic data generation", "abstract": "Synthetic data generation has been proven successful in improving model performance and robustness in the context of scarce or low-quality data. Using the data valuation framework to statistically identify beneficial and detrimental observations, we introduce a novel augmentation pipeline that generates only high-value training points based on hardness characterization. We first demonstrate via benchmarks on real data that Shapley-based data valuation methods perform comparably with learning-based methods in hardness characterisation tasks, while offering significant theoretical and computational advantages. Then, we show that synthetic data generators trained on the hardest points outperform non-targeted data augmentation on simulated data and on a large scale credit default prediction task. In particular, our approach improves the quality of out-of-sample predictions and it is  computationally more efficient compared to non-targeted methods.", "title_embedding_index": 16222, "title_abs_embedding_index": 16247}, {"title": "GeoCon: Compositional Generalization Through Geometric Constraints on Representation Structure", "link_suffix": "/forum?id=dggRphAcCj", "link": "https://openreview.net/forum?id=dggRphAcCj", "pdf_link": "https://openreview.net/pdf?id=dggRphAcCj", "keywords": "Represention Learning, Compositional Generalization.", "abstract": "Compositional generalization, referring to the capacity to generalize novel combinations of fundamental and essential concepts, is thought to be the mechanism underlying a human\u2019s remarkable ability of rapid generalization to new knowledge and tasks. Recent research on brain neural activation space has found that the geometric structure of neural representations is highly related to human compositional generalization capability. In this paper, we extend the above observations from neuroscience to deep neural networks to validate the potential relationship between the geometric structure of representations and compositional generalization capability. In particular, we first construct a new compositional generalization benchmark, which aims to discriminate multiple concepts simultaneously through a powerful representation. Meanwhile, for the aforementioned geometric constraint, the parallelism score is formally defined for deep neural networks. Subsequently, we decompose the deep neural network into two parts: the featurizer and the classifier, to investigate the relationship between compositional generalization capability and parallelism score separately. Our proposed method, Geometric Constraint (GeoCon), involves distance variance minimization regularization on the classifier and parallelism score maximization on the featurizer. Experiments on synthetic and real-world datasets demonstrate significant improvement of our approach, verifying the effectiveness of our neuroscience-inspired GeoCon approach towards human-like superior generalization ability.", "title_embedding_index": 16223, "title_abs_embedding_index": 16248}, {"title": "SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation", "link_suffix": "/forum?id=UiEjzBRYeI", "link": "https://openreview.net/forum?id=UiEjzBRYeI", "pdf_link": "https://openreview.net/pdf?id=UiEjzBRYeI", "keywords": "Versatile Segmentation, Composable Prompts, Open-vocabulary", "abstract": "The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges. This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes them for versatile segmentation. Specifically, given a set of classes (in texts) and a set of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a text label, and the Type-II prompt judges whether two SAM patches with the same text label also belong to the same instance. To decrease the complexity in dealing with a large number of semantic classes and patches, we establish a unified framework that calculates the affinity between (semantic and instance) queries and SAM patches, and then merges patches with high affinity to the query. Experiments show that SAM-CP achieves semantic, instance, and panoptic segmentation in both open and closed domains. In particular, it achieves state-of-the-art performance in open-vocabulary segmentation. Our research offers a novel and generalized methodology for equipping vision foundation models like SAM with multi-grained semantic perception abilities. Codes will be publicly available.", "title_embedding_index": 16224, "title_abs_embedding_index": 16249}]