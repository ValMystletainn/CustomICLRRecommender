[{"title": "Optimizing Latent Goal by Learning from Trajectory Preference", "link_suffix": "/forum?id=VJgCp60WtL", "link": "https://openreview.net/forum?id=VJgCp60WtL", "pdf_link": "https://openreview.net/pdf?id=VJgCp60WtL", "keywords": "open-world agent, continual learning, preference learning, policy post-training, sequantial control", "abstract": "A glowing body of work has emerged focusing on instruction-following policies for open-world agents, aiming to better align the agent's behavior with human intentions. However, the performance of these policies is highly susceptible to the initial prompt, which leads to extra efforts in selecting the best instructions. We propose a framework named \\emph{\\textbf{P}reference \\textbf{G}oal \\textbf{T}uning} (PGT). PGT allows policies to interact with the environment to collect several trajectories, which will be categorized into positive and negative examples based on preference. A preference optimization algorithm is used to fine-tune the initial goal latent representation using the collected trajectories while keeping the policy backbone frozen. The experiment result shows that with minimal data and training, PGT achieves an average relative improvement of $72.0%$ and $81.6%$ over 17 tasks in 2 different foundation policies respectively, and outperforms the best human-selected instructions. Moreover, PGT surpasses full fine-tuning in the out-of-distribution (OOD) task-execution environments by $13.4%$, indicating that our approach retains strong generalization capabilities. Since our approach stores a single latent representation for each task independently, it can be viewed as an efficient method for Continual Learning, without the risk of catastrophic forgetting or task interference. In short, PGT enhances the performance of agents across nearly all tasks in the Minecraft Skillforge benchmark and demonstrates robustness to the execution environment.", "title_embedding_index": 4950, "title_abs_embedding_index": 4975}, {"title": "RILe: Reinforced Imitation Learning", "link_suffix": "/forum?id=sVBnGcbkkM", "link": "https://openreview.net/forum?id=sVBnGcbkkM", "pdf_link": "https://openreview.net/pdf?id=sVBnGcbkkM", "keywords": "Imitation Learning, Inverse Reinforcement Learning, Reinforcement Learning", "abstract": "Reinforcement Learning has achieved significant success in generating complex behavior but often requires extensive reward function engineering. Adversarial variants of Imitation Learning and Inverse Reinforcement Learning offer an alternative by learning policies from expert demonstrations via a discriminator. However, these methods struggle in complex tasks where randomly sampling expert-like behaviors is challenging. This limitation stems from their reliance on policy-agnostic discriminators, which provide insufficient guidance for agent improvement, especially as task complexity increases and expert behavior becomes more distinct. We introduce RILe (Reinforced Imitation Learning environment), a novel trainer-student system that learns a dynamic reward function based on the student's performance and alignment with expert demonstrations. In RILe, the student learns an action policy while the trainer, using reinforcement learning, continuously updates itself via the discriminator's feedback to optimize the alignment between the student and the expert. The trainer optimizes for long-term cumulative rewards from the discriminator, enabling it to provide nuanced feedback that accounts for the complexity of the task and the student's current capabilities. This approach allows for greater exploration of agent actions by providing graduated feedback rather than binary expert/non-expert classifications. By reducing dependence on policy-agnostic discriminators, RILe enables better performance in complex settings where traditional methods falter, outperforming existing methods by 2x in complex simulated robot-locomotion tasks.", "title_embedding_index": 4951, "title_abs_embedding_index": 4976}, {"title": "Canonic Signed Spike Coding for Efficient Spiking Neural Networks", "link_suffix": "/forum?id=mtmqwhQiaG", "link": "https://openreview.net/forum?id=mtmqwhQiaG", "pdf_link": "https://openreview.net/pdf?id=mtmqwhQiaG", "keywords": "Spiking neural network, Spike coding scheme, Stepwise weighted spike", "abstract": "Spiking Neural Networks (SNNs) seek to mimic the spiking behavior of biological neurons and are expected to play a key role in the advancement of neural computing and artificial intelligence. The conversion of Artificial Neural Networks (ANNs) to SNNs is the most widely used training method, which ensures that the resulting SNNs perform comparably to ANNs on large-scale datasets. The efficiency of these conversion-based SNNs is often determined by the neural coding schemes. Current schemes typically use spike count or timing for encoding, which is linearly related to ANN activations and increases the required number of time steps. To address this limitation, we propose a novel Canonic Signed Spike (CSS) coding scheme. This method incorporates non-linearity into the encoding process by weighting spikes at each step of neural computation, thereby increasing the information encoded in spikes. We identify the temporal coupling phenomenon arising from weighted spikes and introduce negative spikes along with a Ternary Self-Amplifying (TSA) neuron model to mitigate the issue. A one-step silent period is implemented during neural computation, achieving high accuracy with low latency. We apply the proposed methods to directly convert full-precision ANNs and evaluate performance on CIFAR-10 and ImageNet datasets. Our experimental results demonstrate that the CSS coding scheme effectively compresses time steps for coding and reduces inference latency with minimal conversion loss.", "title_embedding_index": 4952, "title_abs_embedding_index": 4977}, {"title": "Learning Geometric Reasoning Networks For Robot Task And Motion Planning", "link_suffix": "/forum?id=ajxAJ8GUX4", "link": "https://openreview.net/forum?id=ajxAJ8GUX4", "pdf_link": "https://openreview.net/pdf?id=ajxAJ8GUX4", "keywords": "Graph Neural Networks, Supervised Learning for Robotics, Task and Motion Planning, Robot Manipulation Planning", "abstract": "Task and Motion Planning (TAMP) is a computationally challenging robotics problem due to the tight coupling of discrete symbolic planning and continuous geometric planning of robot motions. In particular, planning manipulation tasks in complex 3D environments leads to a large number of costly geometric planner queries to verify the feasibility of considered actions and plan their motions. To address this issue, we propose Geometric Reasoning Networks (GRN), a graph neural network (GNN)-based model for action and grasp feasibility prediction, designed to significantly reduce the dependency on the geometric planner. Moreover, we introduce two key interpretability mechanisms: inverse kinematics (IK) feasibility prediction and grasp obstruction (GO) estimation. These modules not only improve feasibility predictions accuracy, but also explain why certain actions or grasps are infeasible, thus allowing a more efficient search for a feasible solution. Through extensive experimental results, we show that our model outperforms state-of-the-art methods, while maintaining generalizability to more complex environments, diverse object shapes, multi-robot settings, and real-world robots.", "title_embedding_index": 4953, "title_abs_embedding_index": 4978}, {"title": "Diffusion Modulation via Environment Mechanism Modeling for Planning", "link_suffix": "/forum?id=x1SfON9HvT", "link": "https://openreview.net/forum?id=x1SfON9HvT", "pdf_link": "https://openreview.net/pdf?id=x1SfON9HvT", "keywords": "Reinforcement Learning, Offline Reinforcement Learning, Planning, Diffusion Model", "abstract": "Diffusion models have shown promising capabilities in trajectory generation for planning in offline reinforcement learning (RL). However, conventional diffusion-based planning methods often fail to account for the fact that generating trajectories in RL requires unique consistency between transitions to ensure coherence in real environments. This oversight can result in considerable discrepancies between the generated trajectories and the underlying mechanisms of a real environment. To address this problem, we propose a novel diffusion-based planning method, termed as Diffusion Modulation via Environment Mechanism Modeling (DMEMM). DMEMM modulates diffusion model training by incorporating key RL environment mechanisms, particularly transition dynamics and reward functions. Experimental results demonstrate that DMEMM achieves state-of-the-art performance for planning with offline reinforcement learning.", "title_embedding_index": 4954, "title_abs_embedding_index": 4979}, {"title": "An X-Ray Is Worth 15 Features: Sparse Autoencoders for Interpretable Radiology Report Generation", "link_suffix": "/forum?id=ZLAQ6Pjf9y", "link": "https://openreview.net/forum?id=ZLAQ6Pjf9y", "pdf_link": "https://openreview.net/pdf?id=ZLAQ6Pjf9y", "keywords": "Radiology, Mechanistic Interpretability, Medical Imaging, Sparse Autoencoders", "abstract": "Radiological services are experiencing unprecedented demand, leading to increased interest in automating radiology report generation. Existing Vision-Language Models (VLMs) suffer from hallucinations, lack interpretability, and require expensive fine-tuning. We introduce SAE-Rad, which uses sparse autoencoders (SAEs) to decompose latent representations from a pre-trained vision transformer into human-interpretable features.\nOur hybrid architecture combines state-of-the-art SAE advancements, achieving accurate latent reconstructions while maintaining sparsity.\nUsing an off-the-shelf language model, we distil ground-truth reports into radiological descriptions for each SAE feature, which we then compile into a full report for each image, eliminating the need for fine-tuning large models for this task.\nTo the best of our knowledge, SAE-Rad represents the first instance of using mechanistic interpretability techniques explicitly for a downstream multi-modal reasoning task. On the MIMIC-CXR dataset, SAE-Rad achieves competitive radiology-specific metrics compared to state-of-the-art models while using significantly fewer computational resources for training. Qualitative analysis reveals that SAE-Rad learns meaningful visual concepts and generates reports aligning closely with expert interpretations. Our results suggest that SAEs can enhance multimodal reasoning in healthcare, providing a more interpretable alternative to existing VLMs.", "title_embedding_index": 4955, "title_abs_embedding_index": 4980}, {"title": "Continual Learning via Continual Weighted Sparsity and Meta-Plasticity Scheduling", "link_suffix": "/forum?id=DaUsIJe2Az", "link": "https://openreview.net/forum?id=DaUsIJe2Az", "pdf_link": "https://openreview.net/pdf?id=DaUsIJe2Az", "keywords": "Continual learning", "abstract": "Continual Learning (CL) is fundamentally challenged by the stability-plasticity dilemma: the trade-off between acquiring new information and maintaining past knowledge. To address the stability, many methods keep a replay buffer containing a small set of samples from prior tasks and employ parameter isolation strategies that allocate separate parameter subspaces for each task, reducing interference between tasks. To get more refined, task-specific groups, we adapt a dynamic sparse training technique and introduce a continual weight score function to guide the iterative pruning process over multiple rounds of training. We refer to this method as the continual weighted sparsity scheduler. Furthermore, with more incremental tasks introduced, the network inevitably becomes saturated, leading to a loss of plasticity, where the model's adaptability decreases due to dormant or saturated neurons. To mitigate this, we draw inspiration from biological meta-plasticity mechanisms, and develop a meta-plasticity scheduler to dynamically adjust these task-specific groups' learning rates based on the sensitive score function we designed, ensuring a balance between retaining old knowledge and acquiring new skills. The results of comparison on popular datasets demonstrate that our approach consistently outperforms existing state-of-the-art methods, confirming its effectiveness in managing the stability-plasticity trade-off.", "title_embedding_index": 4956, "title_abs_embedding_index": 4981}, {"title": "Graph GOSPA Similarity Function for Gaussian Process Regression on Graphs", "link_suffix": "/forum?id=9DK6GI0YN2", "link": "https://openreview.net/forum?id=9DK6GI0YN2", "pdf_link": "https://openreview.net/pdf?id=9DK6GI0YN2", "keywords": "Gaussian Process, Graph Matching, Molecular Graph", "abstract": "In this paper, we propose a similarity function between graphs based on a mathematically principled metric for graphs of different sizes: the graph generalised optimal subpattern assignment (GOSPA) metric. The similarity function is based on an optimal assignment between nodes and has an interpretable meaning in terms of similarity for node attribute error, number of unassigned nodes, and number of edge mismatches. The proposed similarity function is computable in polynomial time. We also propose its use in Gaussian processes (GPs) for graphs to predict molecular properties. Experimental results show the benefits of the proposed GP model compared to other GP baselines.", "title_embedding_index": 4957, "title_abs_embedding_index": 4982}, {"title": "ToMA: Token Merging with Attention For Diffusion Models", "link_suffix": "/forum?id=xhtqgW5b93", "link": "https://openreview.net/forum?id=xhtqgW5b93", "pdf_link": "https://openreview.net/pdf?id=xhtqgW5b93", "keywords": "Diffusion, Token Merge, Attention", "abstract": "Diffusion models have emerged as leading models for image generation. \nPlug-and-play token merging techniques have recently been introduced to mitigate the heavy computation cost of transformer blocks in diffusion models. \nHowever, existing methods overlook two key factors: 1. they fail to incorporate modern efficient implementation of attention, so that, the overhead backfires the achieved algorithmic efficiency 2. the selection of token to merge ignores the relation among tokens, limiting the image quality. \nIn this paper, we propose Token Merging with Attention(ToMA) with three major improvements. Firstly, we utilize submodular-based token selection method to identify diverse tokens as merge destination, representative of the entire token set. Secondly, we propose attention merge, utilizing the efficient attention implementation, to perform the merge with negligible overhead. Also we abstract the (un-)merging as (inverse-)linear transformations which also allows shareable transformation across layers/iterations. Finally, we utilize the image locality to further accelerate the computation by performing all the operations on tokens in local tiles.", "title_embedding_index": 4958, "title_abs_embedding_index": 4983}, {"title": "Elucidating the Design Space of Language Models for Image Generation", "link_suffix": "/forum?id=zkMRmW3gcT", "link": "https://openreview.net/forum?id=zkMRmW3gcT", "pdf_link": "https://openreview.net/pdf?id=zkMRmW3gcT", "keywords": "Image generation, Large language model, Generative model", "abstract": "The success of autoregressive (AR) language models in text generation has inspired the computer vision community to adopt Large Language Models (LLMs) for image generation. However, considering the essential differences between text and image modalities, the design space of language models for image generation remains underexplored. We observe that image tokens exhibit greater randomness compared to text tokens, which presents challenges when training with token prediction. Nevertheless, AR models demonstrate their potential by effectively learning patterns even from a seemingly suboptimal optimization problem. Our analysis also reveals that while all models successfully grasp the importance of local information in image generation, smaller models struggle to capture the global context. In contrast, larger models showcase improved capabilities in this area, helping to explain the performance gains achieved when scaling up model size. We further elucidate the design space of language models for vision generation, including tokenizer choice, model choice, model scalability, vocabulary design, and sampling strategy, through extensive comparative experiments. Our work is the first to analyze the optimization behavior of language models in vision generation, and we believe it can inspire more effective designs when applying LMs to other domains. Finally, our elucidated language model for image generation, termed ELM, achieves state-of-the-art performance on the ImageNet 256\u00d7256 benchmark.", "title_embedding_index": 4959, "title_abs_embedding_index": 4984}, {"title": "Prototypical evoluation for few-shot learning in vision-language model adaptation", "link_suffix": "/forum?id=ZaudLwn0Hm", "link": "https://openreview.net/forum?id=ZaudLwn0Hm", "pdf_link": "https://openreview.net/pdf?id=ZaudLwn0Hm", "keywords": "CLIP, few-shot classification", "abstract": "Vision-Language Models (e.g., CLIP), with their immense capacity and extensive exposure to vast data during pre-training, have demonstrated a strong ability to capture real-world concepts. When fast adapted to downstream tasks with only a few labeled samples, parameter-efficient methods, such as prompt-based and adapter-based approaches, which adjust only a small portion of the parameters, have proven effective in reducing the escalating costs in large vision-language models. However, conventional efficient fine-tuning techniques, using task-specific objectives like cross-entropy loss, often lead to overfitting the downstream data distributions. This overfitting diminishes the model\u2019s ability to retain its original generalization capacity, especially on out-of-distribution (OOD) samples. Unlike the pretraining stage, where rich textual descriptions are available, fine-tuning is typically constrained to using only class names. This creates suboptimal text-image alignment in the shared feature space, as it may exacerbate image feature variance within the same class. To address this issue, we propose Prototypical Evolutionary  Adaptation (PEA), leveraging off-the-shelf image centroids as prototypes to regulate image feature variance, mitigating the excessive feature variance within the same class caused by selective bias. Additionally, we introduce learnable shift vectors to capture the dynamics of class prototypes, ensuring that they remain compact and informative. Experiments across diverse datasets and model architectures in few-shot learning demonstrate that our approach consistently outperforms existing methods while maintaining robust generalization under varying distribution shifts.", "title_embedding_index": 4960, "title_abs_embedding_index": 4985}, {"title": "LLM-ABBA: Fine-Tuning Large Language Models For Time Series Using Symbolic Approximation", "link_suffix": "/forum?id=ZT33ACedmn", "link": "https://openreview.net/forum?id=ZT33ACedmn", "pdf_link": "https://openreview.net/pdf?id=ZT33ACedmn", "keywords": "large language models, time series, symbolic approximation", "abstract": "The success of large language models (LLMs) for the time series domain has been demonstrated through various benchmarks. Utilizing symbolic time series representation, one can efficiently bridge the gap between LLMs and time series. However, the remaining challenge is to exploit the semantic information hidden in time series by using symbols or existing tokens of LLMs, while aligning the embedding space of LLMs according to the domain hidden information of time series. The symbolic time series approximation method called ABBA shows outstanding efficacy in preserving salient time series features by modeling time series patterns in terms of amplitude and period while using existing tokens of LLMs.In this paper, we introduce a method, called LLM-ABBA, that integrates ABBA into large language models for various time series downstream tasks. By symbolizing time series, LLM-ABBA compares favorably to the recent state-of-the-art (SOTA) in UCR and three medical time series classification tasks. Meanwhile, a fixed-polygonal chain trick in ABBA is introduced to avoid large vibrations during prediction tasks by significantly mitigating the effects of cumulative error arisen from misused symbols during the transition from symbols to numerical values. In time series regression tasks, LLM-ABBA achieves the new SOTA on Time Series Extrinsic Regression (TSER) benchmarks. LLM-ABBA also shows competitive prediction capability compared to recent time series prediction SOTA results. We believe this framework can also seamlessly extend to other time series domains.", "title_embedding_index": 4961, "title_abs_embedding_index": 4986}, {"title": "JuxtAlign:  A Foundational Analysis on Alignment of Certified Reinforcement Learning", "link_suffix": "/forum?id=yrf5RmaHfG", "link": "https://openreview.net/forum?id=yrf5RmaHfG", "pdf_link": "https://openreview.net/pdf?id=yrf5RmaHfG", "keywords": "alignment, juxtaposition, reinforcement learning", "abstract": "Sequential decision making in highly complex MDPs with high-dimensional observations and state dynamics became possible with the progress achieved in deep reinforcement learning research. At the same time, deep neural policies have been observed to be highly unstable with respect to the minor sensitivities in their state space induced by non-robust directions. To alleviate these volatilities a line of work suggested techniques to cope with this problem via explicitly regularizing the temporal difference loss for the worst-case sensitivity. \nIn this paper we provide theoretical foundations on the failure instances of the approaches proposed to overcome instabilities of the deep neural policy manifolds. Our comprehensive analysis reveals that certified reinforcement learning learns misaligned values. Our empirical analysis in the Arcade Learning Environment further demonstrates that the state-of-the-art certified policies learn inconsistent and overestimated value functions compared to standard training techniques. In connection to this analysis, we highlight the intrinsic gap between how natural intelligence understands and interacts with an environment in contrast to policies learnt via certified training. This intrinsic gap between natural intelligence and the restrictions induced by certified training on the capabilities of artificial intelligence further demonstrates the need to rethink the approach in establishing reliable and aligned deep reinforcement learning policies.", "title_embedding_index": 4962, "title_abs_embedding_index": 4987}, {"title": "Differentially Private Steering for Large Language Model Alignment", "link_suffix": "/forum?id=lLkgj7FEtZ", "link": "https://openreview.net/forum?id=lLkgj7FEtZ", "pdf_link": "https://openreview.net/pdf?id=lLkgj7FEtZ", "keywords": "differential privacy, large language models, alignment, activation engineering", "abstract": "Aligning Large Language Models (LLMs) with human values and away from undesirable behaviors (such as hallucination) has become increasingly important. Recently, steering LLMs towards a desired behavior via activation editing has emerged as an effective method to mitigate harmful  generations at inference-time. Activation editing modifies LLM representations by preserving information from positive demonstrations (e.g., truthful) and minimising information from negative demonstrations (e.g., hallucinations). When these demonstrations come from a private dataset, the aligned LLM may leak private information contained in those private samples. In this work, we present the first study of aligning LLM behavior with private datasets. Our work proposes the \\textit{\\underline{P}rivate \\underline{S}teering for LLM \\underline{A}lignment (PSA)} algorithm to edit LLM activations with differential privacy (DP) guarantees. We conduct extensive experiments on seven different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and model families (LlaMa and Qwen). Our results show that PSA achieves DP guarantees for LLM alignment with minimal loss in performance, including alignment metrics, open-ended text generation quality, and general-purpose reasoning. We also develop the first Membership Inference Attack (MIA) for evaluating and auditing the empirical privacy for the problem of LLM steering via activation editing. Our attack is tailored for activation editing and relies solely on the generated texts without their associated probabilities. Our experiments support the theoretical guarantees by showing improved guarantees for our \\textit{PSA} algorithm compared to several existing non-private techniques.", "title_embedding_index": 4963, "title_abs_embedding_index": 4988}, {"title": "Mixture of Parrots: Experts improve memorization more than reasoning", "link_suffix": "/forum?id=9XETcRsufZ", "link": "https://openreview.net/forum?id=9XETcRsufZ", "pdf_link": "https://openreview.net/pdf?id=9XETcRsufZ", "keywords": "Mixture of Experts, memorization, reasoning", "abstract": "The Mixture-of-Experts (MoE) architecture enables a significant increase in the total number of model parameters with minimal computational overhead. \nHowever, it is not clear what performance tradeoffs, if any, exist between MoEs and standard dense transformers.\nIn this paper, \nwe show that as we increase the number of experts (while fixing the number of active parameters), the memorization performance consistently increases while the reasoning capabilities saturate.We begin by analyzing the theoretical limitations of MoEs at reasoning. We prove that there exist graph  problems that cannot be solved by any number of experts of a certain width; however, the same task can be easily solved by a dense model with a slightly larger width. \nOn the other hand, we find that on memory-intensive tasks, MoEs can effectively leverage a small number of active parameters with a large number of experts to memorize the data. \nWe empirically validate these findings on synthetic graph problems and memory-intensive closed book retrieval tasks. \nLastly, we  pre-train a series of MoEs and dense transformers and evaluate them on commonly used benchmarks in math and natural language. \nWe find that increasing the number of experts helps solve knowledge-intensive tasks, but fails to yield the same benefits for reasoning tasks.", "title_embedding_index": 4964, "title_abs_embedding_index": 4989}, {"title": "A Trajectory Probability Network for City-Scale Road Volume Prediction", "link_suffix": "/forum?id=j4PXHRmA88", "link": "https://openreview.net/forum?id=j4PXHRmA88", "pdf_link": "https://openreview.net/pdf?id=j4PXHRmA88", "keywords": "Data Mining, Traffic Volume Prediction, Learning on Graph", "abstract": "City-scale road volume prediction is a fundamental task in traffic management. However, the observation data are often incomplete and biased, posting a challenge for accurate prediction. Existing methods address this issue through interpolation techniques or manual priors, but they typically provide only a deterministic restoration, overlooking the influence of other potential scenarios. To overcome these limitations, we propose a novel neural network-based probabilistic model, the Trajectory Probability Network (TraPNet), which predicts traffic volume through the aggregation of the joint distribution of potential trajectories. TraPNet makes full use of current observations, historical data, and road network information to offer a comprehensive inference of road volumes. Unlike autoregressive methods, TraPNet makes predictions in a single step, substantially reducing computational time while maintaining high predictive accuracy. Experiments on real-world road networks demonstrate that TraPNet outperforms state-of-the-art methods, and can keep the advantage with only 20% observation ratio. The code will be made publicly available.", "title_embedding_index": 4965, "title_abs_embedding_index": 4990}, {"title": "Evolution guided generative flow networks", "link_suffix": "/forum?id=6Vl9Uvxocp", "link": "https://openreview.net/forum?id=6Vl9Uvxocp", "pdf_link": "https://openreview.net/pdf?id=6Vl9Uvxocp", "keywords": "GFlowNets, Evolutionary Algorithms, Optimization", "abstract": "Generative Flow Networks (GFlowNets) are a family of probabilistic generative models recently invented that learn to sample compositional objects proportional to their rewards. One big challenge of GFlowNets is training them effectively when dealing with long time horizons and sparse rewards. To address this, we propose Evolution guided generative flow networks (EGFN), a simple but powerful augmentation to the GFlowNets training using Evolutionary algorithms (EA). Our method can work on top of any GFlowNets training objective, by training a set of agent parameters using EA, storing the resulting trajectories in the prioritized replay buffer, and training the GFlowNets agent using the stored trajectories. We present a thorough investigation over a wide range of toy and real-world benchmark tasks showing the effectiveness of our method in handling long trajectories and sparse rewards.", "title_embedding_index": 4966, "title_abs_embedding_index": 4991}, {"title": "Regularized Distribution Matching Distillation for One-step Unpaired Image-to-Image Translation", "link_suffix": "/forum?id=jK5r1HBfym", "link": "https://openreview.net/forum?id=jK5r1HBfym", "pdf_link": "https://openreview.net/pdf?id=jK5r1HBfym", "keywords": "diffusion distillation, distribution matching distillation, optimal transport, image-to-image translation", "abstract": "Diffusion-based generative models achieve SOTA results in mode coverage and generation quality but suffer from inefficient sampling. Recently introduced diffusion distillation techniques approach this issue by transforming the original multi-step model into a one-step generator with approximately the same output distribution. Among these methods, Distribution Matching Distillation (DMD) offers a suitable framework for training general-form one-step generators, applicable beyond unconditional generation. In this paper, we propose a modification of DMD, called Regularized Distribution Matching Distillation (RDMD), which applies to the unpaired image-to-image (I2I) translation problem. To achieve this, we regularize the generator objective from DMD with the transport cost between its input and output. We validate the method's applicability in theory by establishing its connection with optimal transport. Moreover, we demonstrate its empirical performance in application to several translation tasks, including 2D examples and I2I between different image datasets, where it performs on par or better than multi-step diffusion baselines.", "title_embedding_index": 4967, "title_abs_embedding_index": 4992}, {"title": "Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching", "link_suffix": "/forum?id=EpnZEzYDUT", "link": "https://openreview.net/forum?id=EpnZEzYDUT", "pdf_link": "https://openreview.net/pdf?id=EpnZEzYDUT", "keywords": "Multi-agent Reinforcement Learning, Offline MARL, Diffusion based Reinforcement Learning, Trajectory Stitching", "abstract": "Learning from offline data without interacting with the environment is a promising way to fully leverage the intelligent decision-making capabilities of multi-agent reinforcement learning (MARL). Previous approaches have primarily focused on developing learning techniques, such as conservative methods tailored to MARL using limited offline data. However, these methods often overlook the temporal relationships across different timesteps and spatial relationships between teammates, resulting in low learning efficiency in imbalanced data scenarios. To comprehensively explore the data structure of MARL and enhance learning efficiency, we propose Multi-Agent offline coordination via Diffusion-based Trajectory Stitching (MADiTS), a novel diffusion-based data augmentation pipeline that systematically generates trajectories by stitching high-quality coordination segments together. MADiTS first generates trajectory segments using a trained diffusion model, followed by applying a bidirectional dynamics constraint to ensure that the trajectories align with environmental dynamics. Additionally, we develop an offline credit assignment technique to identify and optimize the behavior of underperforming agents in the generated segments. This iterative procedure continues until we obtain a satisfactory augmented episode trajectory. Empirical results on MPE and SMAC imbalanced datasets demonstrate that MADiTS significantly improves MARL performance.", "title_embedding_index": 4968, "title_abs_embedding_index": 4993}, {"title": "Can I Understand What I Create? Self-Knowledge Evaluation of Large Language Models", "link_suffix": "/forum?id=Vi1PJjvEdh", "link": "https://openreview.net/forum?id=Vi1PJjvEdh", "pdf_link": "https://openreview.net/pdf?id=Vi1PJjvEdh", "keywords": "Evaluation, Self-knowledge", "abstract": "Large language models (LLMs) have achieved remarkable progress in linguistic tasks, necessitating robust evaluation frameworks to understand their capabilities and limitations. Inspired by Feynman's principle of understanding through creation, we introduce a self-knowledge evaluation framework that is easy to implement, evaluating models on their ability to comprehend and respond to self-generated questions. Our findings, based on testing multiple models across diverse tasks, reveal significant gaps in the model's self-knowledge ability. Further analysis indicates these gaps may be due to misalignment with human attention mechanisms. Additionally, fine-tuning on self-generated math task may enhance the model's math performance, highlighting the potential of the framework for efficient and insightful model evaluation and may also contribute to the improvement of LLMs.", "title_embedding_index": 4969, "title_abs_embedding_index": 4994}, {"title": "Concept-driven Off Policy Evaluation", "link_suffix": "/forum?id=qmodrqswtF", "link": "https://openreview.net/forum?id=qmodrqswtF", "pdf_link": "https://openreview.net/pdf?id=qmodrqswtF", "keywords": "Off Policy Evaluation, Reinforcement Learning, Interpretability, Concept Bottleneck Models", "abstract": "Evaluating a set of decisions based on batch data as in off-policy evaluation is challenging as high variance and limited sample sizes can severely hinder reliable evaluation. Identifying and addressing the sources of this variance is essential for improving OPE performance. Recent work on Concept Bottleneck Models (CBMs) shows how a set of human-explainable concepts can be used for predictions, enabling clearer understanding and inspection of these models. Our work proposes incorporating concepts into OPE to identify and reduce variance through targeted interventions. For example, concepts such as shared disease characteristics could help predict better treatments, despite differing vital signs among two patients. We introduce a family of concept-based OPE estimators, and provide theoretical guarantees that when given a set of known concepts, these estimators are unbiased and reduce variance compared to traditional methods. However, in many real-world applications, these concepts are often unknown and need to be estimated. We develop an end-to-end algorithm for learning parameterized concepts that are interpretable, concise, diverse, and optimized for variance reduction in OPE. Through extensive experiments on synthetic and real-world datasets, we demonstrate that both known and learned concept-based estimators significantly improve OPE performance. Crucially, we show that unlike other methods for OPE, concept-based estimators can easily be interpreted and offer opportunities for targeted interventions on specific concepts of interest to further improve the quality of these estimators.", "title_embedding_index": 4970, "title_abs_embedding_index": 4995}, {"title": "MEMREASONER: A MEMORY-AUGMENTED LANGUAGE MODEL ARCHITECTURE FOR MULTI-HOP REASONING", "link_suffix": "/forum?id=d4gu2XgccF", "link": "https://openreview.net/forum?id=d4gu2XgccF", "pdf_link": "https://openreview.net/pdf?id=d4gu2XgccF", "keywords": "reasoning, multi hop, memory, large language models, generalization", "abstract": "Recent benchmarks suggest that there remains significant room to improve large language models\u2019 ability to robustly reason across facts distributed in extremely long documents. In this work, we propose MemReasoner, a new memory-augmented LLM architecture that is trained to perform temporal reasoning, along with multiple computational steps,  over the context stored in the memory. Experiments show that MemReasoner trained on the core reasoning facts generalizes better, when compared to off-the-shelf large language models and existing recurrent models, on a test distribution  where  the required facts are scattered across long natural text up to 128k tokens. Further, MemReasoner demonstrates robust reasoning performance relative to the baselines, when the answer distribution in test samples  differs from that in the training set.", "title_embedding_index": 4971, "title_abs_embedding_index": 4996}, {"title": "Deep Learning for Protein-Ligand Docking: Are We There Yet?", "link_suffix": "/forum?id=ZuU4mZILBB", "link": "https://openreview.net/forum?id=ZuU4mZILBB", "pdf_link": "https://openreview.net/pdf?id=ZuU4mZILBB", "keywords": "Protein-Ligand Docking, Generative Modeling, Benchmarking, Multi-Ligands", "abstract": "The effects of ligand binding on protein structures and their in vivo functions carry numerous implications for modern biomedical research and biotechnology development efforts such as drug discovery. Although several deep learning (DL) methods and benchmarks designed for protein-ligand docking have recently been introduced, to date no prior works have systematically studied the behavior of docking methods within the broadly applicable context of (1) using predicted (apo) protein structures for docking (e.g., for applicability to unknown structures); (2) docking multiple ligands concurrently to a given target protein (e.g., for enzyme design); and (3) having no prior knowledge of binding pockets (e.g., for unknown pocket generalization). To enable a deeper understanding of docking methods' real-world utility, we introduce PoseBench, the first comprehensive benchmark for broadly applicable protein-ligand docking. PoseBench enables researchers to rigorously and systematically evaluate DL docking methods for apo-to-holo protein-ligand docking and protein-ligand structure generation using both single and multi-ligand benchmark datasets, the latter of which we introduce for the first time to the DL community. Empirically, using PoseBench, we find that (1) DL methods consistently outperform conventional docking algorithms; (2) most recent DL docking methods fail to generalize to multi-ligand protein targets; and (3) training DL methods with physics-informed loss functions on diverse clusters of protein-ligand complexes is a promising direction for future work. Code, data, tutorials, and benchmark results are available athttps://anonymous.4open.science/r/PoseBench-2CD8.", "title_embedding_index": 4972, "title_abs_embedding_index": 4997}, {"title": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models", "link_suffix": "/forum?id=0BujOfTqab", "link": "https://openreview.net/forum?id=0BujOfTqab", "pdf_link": "https://openreview.net/pdf?id=0BujOfTqab", "keywords": "jailbreak, adversarial attack, audio-language model", "abstract": "Recent advancements in large audio-language models (LALMs) have enabled speech-based user interactions, significantly enhancing user experience and accelerating the deployment of LALMs in real-world applications. However, ensuring the safety of LALMs is crucial to prevent risky outputs that may raise societal concerns or violate AI regulations. Despite the importance of this issue, research on jailbreaking LALMs remains limited due to their recent emergence and the additional technical challenges they present compared to attacks on DNN-based audio models. Specifically, the audio encoders in LALMs, which involve discretization operations, often lead to gradient shattering, hindering the effectiveness of attacks relying on gradient-based optimizations. The behavioral variability of LALMs further complicates the identification of effective (adversarial) optimization targets. Moreover, enforcing stealthiness constraints on adversarial audio waveforms introduces a reduced, non-convex feasible solution space, further intensifying the challenges of the optimization process. To overcome these challenges, we develop AdvWave, the first jailbreak framework against LALMs. We propose a dual-phase optimization method that addresses gradient shattering, enabling effective end-to-end gradient-based optimization. Additionally, we develop an adaptive adversarial target search algorithm that dynamically adjusts the adversarial optimization target based on the response patterns of LALMs for specific queries. To ensure that adversarial audio remains perceptually natural to human listeners, we design a classifier-guided optimization approach that generates adversarial noise resembling common urban sounds. Extensive evaluations on multiple advanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving a 40% higher average jailbreak attack success rate. Both audio stealthiness metrics and human evaluations confirm that adversarial audio generated by AdvWave is indistinguishable from natural sounds. We believe AdvWave will inspire future research aiming to enhance the safety alignment of LALMs, supporting their responsible deployment in real-world scenarios.", "title_embedding_index": 4973, "title_abs_embedding_index": 4998}, {"title": "Union-over-Intersections: Object Detection beyond Winner-Takes-All", "link_suffix": "/forum?id=HqLHY4TzGj", "link": "https://openreview.net/forum?id=HqLHY4TzGj", "pdf_link": "https://openreview.net/pdf?id=HqLHY4TzGj", "keywords": "localization based feature representation, intersection over union, object detection.", "abstract": "This paper revisits the problem of predicting box locations in object detection architectures. Typically, each box proposal or box query aims to directly maximize the intersection-over-union (IoU) score with the ground truth, followed by a winner-takes-all non-maximum suppression (NMS) where only the highest scoring box in each region is retained. We observe that both steps are sub-optimal: the first involves regressing proposals to the entire ground truth, which is a difficult task even with large receptive fields, and the second neglects valuable information from boxes other than the top candidate. Instead of regressing proposals to the whole ground truth, we propose a simpler approach\u2014regress only to the area of intersection between the proposal and the ground truth. This avoids the need for proposals to extrapolate beyond their visual scope, improving localization accuracy. Rather than adopting a winner-takes-all strategy, we take the union over the regressed intersections of all boxes in a region to generate the final box outputs. Our plug-and-play method integrates seamlessly into any detection architecture with minimal modifications, significantly improving object localization and instance segmentation. We demonstrate its broad applicability and versatility across various detection and segmentation tasks.", "title_embedding_index": 4974, "title_abs_embedding_index": 4999}]