[
    {
        "title": "Altared Environments: The Role of Normative Infrastructure in AI Alignment",
        "link_suffix": "/forum?id=Gd6QrBLHBN",
        "link": "https://openreview.net/forum?id=Gd6QrBLHBN",
        "pdf_link": "https://openreview.net/pdf?id=Gd6QrBLHBN",
        "keywords": "Instituions, Norms, Cooperative AI, Multi-agent Systems, Reinforcement Learning, Alignment",
        "abstract": "Cooperation is central to human life, distinguishing humans as uniquely cooperative among mammals. As AI agents gain autonomy in shared environments, it becomes essential that their integration into human societies is driven by their ability to cooperate with humans and other agents. Humans often achieve stable cooperation by developing normative competence—the ability to recognize, reason about, and coordinate around shared norms. How can we design and train AI agents to be normatively competent, thereby cultivating cooperative intelligence in them? Most AI research frames this as an alignment challenge, focusing on embedding norms and values into AI agents. While this approach is promising, it often overlooks how humans solve cooperation challenges—through normative institutions that establish acceptable behavior within groups. Inspired by this, we propose a novel solution to the AI alignment challenge by introducing the concept of an altar—a dynamic environmental feature functioning as an institution that encodes sanctionable actions and evolves its content over time. Formalized within the context of Markov decision processes, this framework enables agents to learn to recognize institutions and adapt their enforcement behavior in response to dynamic institutional changes. We hypothesize that this learning process will lead to the emergence of compliance behavior in trained agents, resulting in stable cooperative outcomes as authorized by the institution. In mixed-motive environments, which present cooperation challenges such as equilibrium selection, free-riding, and the tragedy of the commons, we demonstrate that agents trained with the altar quickly learn and sustain cooperative behavior by adapting to various environmental and institutional configurations. This further allows them to adjust rapidly to new environments where institutions have different meanings. Additionally, we explore the effectiveness of institutions in scenarios with larger group sizes, mirroring realistic social structures. Our results suggest that integrating normative infrastructure into AI training systems is a crucial step for future research on designing agents capable of seamlessly integrating into human societies."
    },
    {
        "title": "Human-like Episodic Memory for Infinite Context LLMs",
        "link_suffix": "/forum?id=BI2int5SAC",
        "link": "https://openreview.net/forum?id=BI2int5SAC",
        "pdf_link": "https://openreview.net/pdf?id=BI2int5SAC",
        "keywords": "large language models, long context, retrieval, episodic memory, event cognition, training-free",
        "abstract": "Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs with no fine-tuning, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient and human-like access to relevant information. Experiments on the LongBench and InfiniteBench benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the state-of-the-art retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart, RAG, in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses full-context models in most tasks, while successfully performing retrieval across 5 million tokens -- a scale computationally infeasible for such models. Finally, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting a bridge between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms."
    },
    {
        "title": "Code diffusion models are continuous human noise operators",
        "link_suffix": "/forum?id=aOAgMiOXU2",
        "link": "https://openreview.net/forum?id=aOAgMiOXU2",
        "pdf_link": "https://openreview.net/pdf?id=aOAgMiOXU2",
        "keywords": "Program Repair, Text Diffusion, Code Diffusion, Language Models",
        "abstract": "Diffusion for code generates code by iteratively removing noise from the latent representation of a code snippet.\nDuring later steps of the diffusion process, when the code snippet has almost converged, these edits resemble last-mile repairs applied to broken or incomplete code. We evaluate the extent to which these errors are similar to those that humans are faced with and the capability of these models to perform last-mile repair. Our insight has two applications with significant impact for code repair. First, we can leverage the diffusion model for last-mile repair by adding noise to a broken code snippet and resuming the diffusion process. Second, we can leverage the diffusion model to generate an arbitrary amount of training data for other last-mile repair approaches (that are computationally more efficient) by sampling an intermediate program (input) and the final program (output) from the diffusion process. We perform experiments to evaluate both applications, as well as analyze trends in the evolution of representation through the diffusion pipeline providing insights on the reasoning observed."
    },
    {
        "title": "Evaluating Oversight Robustness with Incentivized Reward Hacking",
        "link_suffix": "/forum?id=licAR8FPTW",
        "link": "https://openreview.net/forum?id=licAR8FPTW",
        "pdf_link": "https://openreview.net/pdf?id=licAR8FPTW",
        "keywords": "scalable oversight, robustness, reward hacking",
        "abstract": "Scalable oversight aims to train systems to perform tasks that are hard for humans to specify, demonstrate and validate. \nAs ground truth is not available for such tasks, evaluating scalable oversight techniques is challenging: existing methods measure the success of an oversight method based on whether it allows an artificially weak overseer to successfully supervise an AI to perform a task.\nIn this work, we additionally measure the robustness of scalable oversight techniques by testing their vulnerability to reward hacking by an adversarial supervisee.\nIn experiments on a synthetic domain, we show that adding an explicit reward hacking incentive to the model being trained leads it to exploit flaws in a weak overseer, and that scalable oversight methods designed to mitigate these flaws can make the optimization more robust to reward hacking.\nWe hope these experiments lay a foundation for future work to validate scalable oversight methods' ability to mitigate reward hacking in realistic settings."
    },
    {
        "title": "Beyond accuracy: understanding the performance of LLMs on exams designed for humans",
        "link_suffix": "/forum?id=vgvnfUho7X",
        "link": "https://openreview.net/forum?id=vgvnfUho7X",
        "pdf_link": "https://openreview.net/pdf?id=vgvnfUho7X",
        "keywords": "large language models, model evaluation, psychometrics",
        "abstract": "Many recent studies of LLM performance have focused on the ability of LLMs to achieve outcomes comparable to humans on academic and professional exams. However, it is not clear whether such studies shed light on the extent to which models show reasoning ability, and there is controversy about the significance and implications of such results. We seek to look more deeply into the question of how and whether the performance of LLMs on exams designed for humans reflects true aptitude inherent in LLMs. We do so by making use of the tools of psychometrics which are designed to perform meaningful measurement in test taking. We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil. With respect to the evaluation of LLM abilities, we show that the tools of Item Response Theory (IRT) provide a more informative evaluation of model performance than the usual accuracy metrics employed in previous studies. Digging deeper, we show that the modeling framework of IRT, by explicitly modeling the difficulty levels of questions, allows us to quantitatively distinguish between LLMs that answer questions in “human-like” patterns versus LLMs that do not. We also show how to quantitatively identify cases in which exam results are not reliable measurements of an LLM's ability. Using the tools of IRT we can also identify specific questions that appear to be either much easier, or much harder, for machines than for humans, and we give some reasons for those differences. Overall, our study shows that the conventional focus on accuracy as the primary performance metric for LLM studies does not allow us to deeply understand the true capabilities of LLMs and compare them to that of humans. Thus, we claim that psychometric modeling should play a larger role in the evaluation of LLM capabilities on exams designed for humans."
    },
    {
        "title": "EVOSCHEMA: TOWARDS TEXT-TO-SQL ROBUSTNESS AGAINST SCHEMA EVOLUTION",
        "link_suffix": "/forum?id=NfUHBaZdLw",
        "link": "https://openreview.net/forum?id=NfUHBaZdLw",
        "pdf_link": "https://openreview.net/pdf?id=NfUHBaZdLw",
        "keywords": "text-to-SQL, schema evolution, robustness",
        "abstract": "Neural text-to-SQL models, which translate natural language questions (NLQs) into SQL queries given a database schema, have achieved remarkable performance. However, database schemas frequently evolve to meet new requirements. Such schema evolution often leads to performance degradation for models trained on static schemas. Existing work either mainly focuses on simply paraphrasing some syntactic or semantic mappings among NLQ, DB and SQL or lacks a comprehensive and controllable way to investigate the model robustness issue\nunder the schema evolution. In this work, we approach this crucial problem by introducing a novel framework, EvoSchema, to systematically simulate diverse schema changes that occur in real-world scenarios. EvoSchema builds on our newly defined schema evolution taxonomy, which encompasses a comprehensive set of eight perturbation types, covering both column-level and table-level modifications. We utilize this framework to build an evaluation benchmark to assess the models’ robustness against different schema evolution types. Meanwhile, we propose a new training paradigm, which augments existing training data with diverse schema designs and forces the model to distinguish the schema difference for the same questions to avoid learning spurious patterns. Our experiments demonstrate that the existing models are more easily affected by table-level perturbations than column-level perturbations. In addition, the models trained under our paradigm exhibit significantly improved robustness, achieving up to 33 points improvement on the evaluation benchmark compared to models trained on unperturbed data. This work represents a significant step towards building more resilient text-to-SQL systems capable of handling the dynamic nature of database schemas."
    },
    {
        "title": "CAN TRANSFORMERS IN-CONTEXT LEARN BEHAVIOR OF A LINEAR DYNAMICAL SYSTEM?",
        "link_suffix": "/forum?id=XZhpS5Imzx",
        "link": "https://openreview.net/forum?id=XZhpS5Imzx",
        "pdf_link": "https://openreview.net/pdf?id=XZhpS5Imzx",
        "keywords": "In-context Learning, Transformers, Kalman Filter",
        "abstract": "We investigate whether transformers can learn to track a random process when given observations of a related process and parameters of the dynamical system that relates them as context. More specifically, we consider a finite-dimensional state-space model described by the state transition matrix $F$, measurement matrices $h_1, \\dots, h_N$, and the process and measurement noise covariance matrices $Q$ and $R$, respectively; these parameters, randomly sampled, are provided to the transformer along with the observations $y_1,\\dots,y_N$ generated by the corresponding linear dynamical system. We argue that in such settings transformers learn to approximate the celebrated Kalman filter, and empirically verify this both for the task of estimating hidden states $\\hat{x_{N|1,2,3,...,N}}$ as well as for one-step prediction of the $(N+1)^{st}$ observation, $\\hat{y}_{N+1|1,2,3,...,N}$. A further study of the transformer's robustness reveals that its performance is retained even if the model's parameters are partially withheld. In particular, we demonstrate that the transformer remains accurate at the considered task even in the absence of state transition and noise covariance matrices, effectively emulating operations of the Dual-Kalman filter."
    },
    {
        "title": "Robustness of Truss Decomposition and Implications for GNN-based Edge Classification",
        "link_suffix": "/forum?id=2QkWSUMQh5",
        "link": "https://openreview.net/forum?id=2QkWSUMQh5",
        "pdf_link": "https://openreview.net/pdf?id=2QkWSUMQh5",
        "keywords": "Graph mining, dense subgraph discovery, truss decomposition, robustness, edge classification",
        "abstract": "Truss decomposition is an effective and practical algorithm for dense subgraph discovery. However, it is sensitive to the changes in the graph: dropping a few edges or a bit of noise can drastically impact the truss numbers of the edges. It is of practical importance to understand and characterize the robustness of truss decomposition. In this work, we study and utilize the robustness of truss decomposition in an edge-driven way. We propose to construct a dependency graph among edges to denote the impact of an edge's removal on the neighboring edges. By using the dependency graph, we introduce three measures to capture the diverse and unique properties of the edges. We provide theoretical findings and design an efficient algorithm to compute the dependency graph faster than the naive baseline. We also show that our new edge-based truss robustness measures capture intrinsic graph structures and have the potential to unearth peculiar differences that can help with various downstream tasks, such as edge classification. We integrate our measures into the state-of-the-art GNN for edge classification and demonstrate improved performance on multi-class datasets. The overhead of computing our edge-based measures is insignificant when compared to the training time. We believe that utilizing edge-based truss and robustness measures can further be helpful in edge-driven downstream tasks."
    },
    {
        "title": "ODE Parameter Identification: An Integral Matching Approach",
        "link_suffix": "/forum?id=X3IcgZEUEi",
        "link": "https://openreview.net/forum?id=X3IcgZEUEi",
        "pdf_link": "https://openreview.net/pdf?id=X3IcgZEUEi",
        "keywords": "ODEs, System Identification, Collocation",
        "abstract": "We present a novel method to identify parameter of nonlinear Ordinary Differential Equations (ODEs) using time series data. Our approach fits parameters by matching a collocation-based estimate of the integral of the learned derivative to an interpolation of the trajectory, thus avoiding the computational cost of ODE solvers in adjoint methods and the sensitivity to noise of derivative estimates in gradient matching methods. By employing batching strategies based on time subintervals and state components, our method achieves linear complexity in relation to system dimensions and dataset sizes. The method is highly parallel enabling fast gradient evaluations and a faster convergence than adjoint methods. For fully observed systems, we demonstrate the method on canonical dynamical systems, where the method achieves speed-ups of three orders of magnitude over adjoint methods and an increased robustness against observational noise. We provide an extension to partially observed systems and demonstrate the method on the Lorenz63 attractor."
    },
    {
        "title": "Global Convergence of Policy Gradient in Average Reward MDPs",
        "link_suffix": "/forum?id=2PRpcmJecX",
        "link": "https://openreview.net/forum?id=2PRpcmJecX",
        "pdf_link": "https://openreview.net/pdf?id=2PRpcmJecX",
        "keywords": "Policy Gradient, Reinforcement Learning, Average Reward MDPs",
        "abstract": "We present the first comprehensive finite-time global convergence analysis of policy gradient for infinite horizon average reward Markov decision processes (MDPs). Specifically, we focus on ergodic tabular MDPs with finite state and action spaces. Our analysis shows that the policy gradient iterates converge to the optimal policy at a sublinear rate of $O({\\frac{1}{T}}),$ which translates to $O({\\log(T)})$ regret, where $T$ represents the number of iterations. Performance bounds for discounted reward MDPs cannot be easily extended to average reward MDPs as the bounds grow proportional to the fifth power of the effective horizon. Recent work on such extensions make a smoothness assumption that has not been verified. Thus, our primary contribution is in providing the first complete proof that the policy gradient algorithm converges globally for average-reward MDPs, without such an assumption. We also obtain the corresponding finite-time performance guarantees. In contrast to the existing discounted reward performance bounds, our performance bounds have an explicit dependence on constants that capture the complexity of the underlying MDP. Motivated by this observation, we reexamine and improve the existing performance bounds for discounted reward MDPs. We also present simulations which empirically validate the result."
    },
    {
        "title": "Reward Adaptation Via Q-Manipulation",
        "link_suffix": "/forum?id=ZK4VSRzBNC",
        "link": "https://openreview.net/forum?id=ZK4VSRzBNC",
        "pdf_link": "https://openreview.net/pdf?id=ZK4VSRzBNC",
        "keywords": "Transfer RL, Reusable RL, Action Pruning, Reward Adaptation",
        "abstract": "In this paper, we propose a new solution to reward adaptation (RA), the problem where the learning agent adapts to a target reward function based on one or multiple existing behaviors learned a priori under the same domain dynamics but different reward functions.  RA has many applications, such as adapting an autonomous driving agent that can already operate either fast or safe to operating both fast and safe. Learning the target behavior from scratch is possible but often inefficient given the available source behaviors. Our work represents a new approach to RA via the manipulation of Q-functions.  Assuming that the target reward function is a known function of the source reward functions, our approach to RA  computes bounds of the Q function. We introduce an iterative process to tighten the bounds, similar to value iteration. This enables action pruning in the target domain before learning even starts. We refer to such a method as Q-Manipulation (Q-M). We formally prove that our pruning strategy does not affect the optimality of the returned policy while empirically show that it improves the sample complexity. Comparison with baselines is performed in a variety of synthetic and simulation domains to demonstrate its effectiveness and generalizability."
    },
    {
        "title": "Better Call Graphs: A New Dataset of Function Call Graphs for Malware Classification",
        "link_suffix": "/forum?id=nwjgeFGbAF",
        "link": "https://openreview.net/forum?id=nwjgeFGbAF",
        "pdf_link": "https://openreview.net/pdf?id=nwjgeFGbAF",
        "keywords": "Malware classification, FCG",
        "abstract": "Malware classification by using function call graphs (FCG) is an important task in cybersecurity.\nOne big challenge in this direction is the lack of representative, large, and unique FCG datasets.\nExisting datasets typically contain obsolete Android application packages (APKs), largely consist of small graphs, and include many duplicate FCGs due to repackaging.\nThis results in misleading graph classification performance.\nIn this paper, we propose a new comprehensive dataset, Better Call Graphs (BCG), that contains large and unique FCGs from recent APKs, along with graph-level APK features, with benign and malware samples from different types and families.\nWe establish the necessity of BCG through the evaluation of several baseline approaches on existing datasets.BCG is available athttps://iclr.me."
    },
    {
        "title": "What Matters in Learning from Large-Scale Datasets for Robot Manipulation",
        "link_suffix": "/forum?id=LqhorpRLIm",
        "link": "https://openreview.net/forum?id=LqhorpRLIm",
        "pdf_link": "https://openreview.net/pdf?id=LqhorpRLIm",
        "keywords": "imitation learning, robotics, dataset composition",
        "abstract": "Imitation learning from large multi-task demonstration datasets has emerged as a promising path for building generally-capable robots. As a result, 1000s of hours have been spent on building such large-scale datasets around the globe. Despite the continuous growth of such efforts, we still lack a systematic understanding of what data should be collected to improve the utility of a robotics dataset and facilitate downstream policy learning. In this work, we conduct a large-scale dataset composition study to answer this question. We develop a data generation framework to procedurally emulate common sources of diversity in existing datasets (such as sensor placements and object types and arrangements), and use it to generate large-scale robot datasets with controlled compositions, enabling a suite of dataset composition studies that would be prohibitively expensive in the real world. We focus on two practical settings: (1) what types of diversity should be emphasized when future researchers collect large-scale datasets for robotics, and (2) how should current practitioners retrieve relevant demonstrations from existing datasets to maximize downstream policy performance on tasks of interest. Our study yields several critical insights -- for example, we find that camera poses and spatial arrangements are crucial dimensions for both diversity in collection and alignment in retrieval. In real-world robot learning settings, we find that not only do our insights from simulation carry over, but our retrieval strategies on existing datasets such as DROID allow us to consistently outperform existing training strategies by up to 70%."
    },
    {
        "title": "Leveraging Side Information with Deep Learning for Linear Inverse Problems: Applications to MR Image Reconstruction",
        "link_suffix": "/forum?id=kqZCOliBhV",
        "link": "https://openreview.net/forum?id=kqZCOliBhV",
        "pdf_link": "https://openreview.net/pdf?id=kqZCOliBhV",
        "keywords": "MR image reconstruction, side information, linear inverse problems, Trust-Guided Variational Network",
        "abstract": "Reducing the time it takes to acquire a Magnetic Resonance Imaging (MRI) scan is an important problem in healthcare, as it can improve patient care and reduce costs. One way to achieve this is by acquiring only a fraction of the frequency space data and reconstructing diagnostic-quality images from it. This problem can be formulated as a linear inverse problem (LIP), where the forward operator, which maps the structure of the imaged object to the acquired frequency space data, can become rank-deficient or exhibit many small singular values. This leads to ambiguities in the reconstruction process, where multiple images (most of them non-diagnostic) can map to the same set of acquired data. To resolve these ambiguities, it is essential to leverage domain knowledge and, whenever possible, exploit additional context (a.k.a., elevant side information) when solving the LIP. We present a novel, end-to-end trainable deep learning-based method, called Trust-Guided Variational Network (TGVN), that reliably incorporates side information into LIPs to eliminate undesirable solutions from the ambiguous space of the forward operator, while remaining faithful to the acquired data. We demonstrate its effectiveness through applications in multi-coil, multi-contrast MR image reconstruction, where incomplete or low-quality measurements from one contrast are used as side information to reconstruct a high-quality image of another contrast from heavily under-sampled data. Its generalizability is validated by reconstructing images from different contrasts across different anatomies and field strengths. Compared to a set of baselines that also use side-information, our method reconstructs high-quality images in the presence of heretofore challenging levels of under-sampling, thereby speeding up the acquisition drastically while providing protection against hallucinations. Our approach is also versatile enough to incorporate many different types of side information into any LIP."
    },
    {
        "title": "Learning Discrete Latent Models from Discrete Observations",
        "link_suffix": "/forum?id=Rkpdfia4Sz",
        "link": "https://openreview.net/forum?id=Rkpdfia4Sz",
        "pdf_link": "https://openreview.net/pdf?id=Rkpdfia4Sz",
        "keywords": "Latent Variable Identification, Nonlinear Independent Component Analysis (ICA)",
        "abstract": "A central challenge in machine learning is discovering meaningful representations of high-dimensional data, commonly referred to as representation learning. However, many existing methods lack a theoretical foundation, leading to unreliable representations and limited inferential capabilities. In approaches where certain uniqueness of representation is guaranteed, such as nonlinear ICA, variables are typically assumed to be continuous. While recent work has extended identifiability to binarized observed variables, no principled method has been developed for scenarios involving discrete latent variables. In this paper, we show how multi-domain information can be leveraged to achieve identifiability when both latent and observed variables are discrete. We propose general identification conditions that do not depend on specific data distributional assumptions or parametric model forms. The effectiveness of our approach is validated through experiments on both simulated and real-world datasets."
    },
    {
        "title": "Diffusion Models are Few-shot Learners for Dense Vision Tasks",
        "link_suffix": "/forum?id=az5WtGe48n",
        "link": "https://openreview.net/forum?id=az5WtGe48n",
        "pdf_link": "https://openreview.net/pdf?id=az5WtGe48n",
        "keywords": "Few-shot Learning; Dense Prediction; Generative Model",
        "abstract": "The ability to adapt to new, unseen tasks with only a handful of training examples is a key factor behind the unprecedented success of language models. However, in computer vision, few-shot adaption has largely focused on adapting to new semantic categories or answering new visual questions. Adapting a model to dense vision tasks – depth estimation, surface normal estimation, semantic segmentation – has only been possible with large amounts of training data and with custom decoder heads, since the output spaces for each task varies widely. For instance, depth estimation outputs continuous values while semantic segmentation generates discrete categorical assignments. In this paper, we found that the diffusion prior can effectively adapt to various dense tasks, and based on this, we introduce an adaptation mechanism that exploits a pretrained diffusion model for 12 different dense vision tasks using only a few training examples. Moreover, adapting to different tasks requires only modifying the input, without changing the internal parameters of the model. Our key insight is to reframe all dense prediction tasks into a codebook-conditioned classification problem, even for continuous outputs.\nSpecifically, we learn two set of parameters: (1) concept embeddings that condition the diffusion model to encode task-specific representations in their attention masks; and (2) codebook embeddings that recombine discrete outputs to continuous ones. With this novel design, we achieve state-of-the-art results across 12 datasets for few shot learning."
    },
    {
        "title": "Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning",
        "link_suffix": "/forum?id=XwUrzurG94",
        "link": "https://openreview.net/forum?id=XwUrzurG94",
        "pdf_link": "https://openreview.net/pdf?id=XwUrzurG94",
        "keywords": "Robot Learning, Reinforcement Learning, Fine-Tuning",
        "abstract": "Robot learning requires a considerable amount of data to realize the promise of generalization. However, it can be challenging to actually collect the magnitude of high-quality data necessary for generalization entirely in the real world. Simulation can serve as a source of plentiful data, wherein techniques such as reinforcement learning can obtain broad coverage over states and actions.  However, high-fidelity physics simulators are fundamentally misspecified approximations to reality, making direct zero-shot transfer challenging, especially in tasks where precise and forceful manipulation is necessary. This makes real-world fine-tuning of policies pretrained in simulation an attractive approach to robot learning. However, exploring the real-world dynamics with standard RL fine-tuning techniques is to inefficient for many real-world applications. This paper introduces Simulation-Guided Fine-Tuning, a general framework which leverages the structure of the simulator to guide exploration, substantially accelerating adaptation to the real-world. We demonstrate our approach across several manipulation tasks in the real world, learning successful policies for problems that are challenging to learn using purely real-world data. We further provide theoretical backing for the paradigm."
    },
    {
        "title": "TrajGPT: Irregular Time-Series Representation Learning for Health Trajectory Analysis",
        "link_suffix": "/forum?id=OYT7yZfBFw",
        "link": "https://openreview.net/forum?id=OYT7yZfBFw",
        "pdf_link": "https://openreview.net/pdf?id=OYT7yZfBFw",
        "keywords": "GPT, Representation learning, Linear attention, ODE, Irregularly-sample time series, zero-shot learning, trajectory analysis",
        "abstract": "In many domains, such as healthcare, time-series data is often irregularly sampled with varying intervals between observations. This poses challenges for classical time-series models that require equally spaced data. To address this, we propose a novel time-series Transformer calledTrajectory Generative Pre-trained Transformer (TrajGPT). TrajGPT employs a novel Selective Recurrent Attention (SRA) mechanism, which utilizes a data-dependent decay to adaptively filter out irrelevant past information based on contexts. By interpreting TrajGPT as discretized ordinary differential equations (ODEs), it effectively captures the underlying continuous dynamics and enables time-specific inference for forecasting arbitrary target timesteps. Experimental results demonstrate that TrajGPT excels in trajectory forecasting, drug usage prediction, and phenotype classification without requiring task-specific fine-tuning. By evolving the learned continuous dynamics,  TrajGPT can interpolate and extrapolate disease risk trajectories from partially-observed time series. The visualization of predicted health trajectories shows that TrajGPT forecasts unseen diseases based on the history of clinically relevant phenotypes (i.e., contexts)."
    },
    {
        "title": "Fed-REACT: Federated Representation Learning for Heterogeneous Time Series Data",
        "link_suffix": "/forum?id=c6hGb8IsRN",
        "link": "https://openreview.net/forum?id=c6hGb8IsRN",
        "pdf_link": "https://openreview.net/pdf?id=c6hGb8IsRN",
        "keywords": "Federated learning, representation learning, time series data",
        "abstract": "Motivated by high resource costs and privacy concerns that characterize centralized machine learning, federated learning (FL) emerged as an efficient alternative that allows the participating clients to collaboratively train global model while keeping their data local.\nIn practice, distributions of clients' data vary over time and from one client to another, creating heterogeneous conditions that deteriorate performance of conventional FL algorithms. In this work, we study an FL framework where clients train on heterogeneous time series data and introduce to these settings Fed-REACT, a novel federated learning method leveraging representation learning and evolutionary clustering. The algorithm consists of two stages: (1) in the first stage, the clients learn a model that extracts meaningful features from local time series data; (2) in the second stage, the server adaptively groups clients into clusters and coordinated cluster-wise learning of task (i.e., post-representation) models for local downstream tasks, e.g., classification or regression. We demonstrated high accuracy and robustness of the proposed algorithm in experiments on real-world time series datasets, and provided theoretical analysis of its performance."
    },
    {
        "title": "Learning to Discover Regulatory Elements for Gene Expression Prediction",
        "link_suffix": "/forum?id=Mfnh1Sqdwf",
        "link": "https://openreview.net/forum?id=Mfnh1Sqdwf",
        "pdf_link": "https://openreview.net/pdf?id=Mfnh1Sqdwf",
        "keywords": "Gene Expression, Deep Learning, Sequence Modeling",
        "abstract": "We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3."
    },
    {
        "title": "Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents",
        "link_suffix": "/forum?id=FEpAUnS7f7",
        "link": "https://openreview.net/forum?id=FEpAUnS7f7",
        "pdf_link": "https://openreview.net/pdf?id=FEpAUnS7f7",
        "keywords": "LLM, Agent, Usable Privacy Policies, Benchmarking, HCI",
        "abstract": "This paper presents a novel application of large language models (LLMs) to enhance user comprehension of privacy policies through an interactive dialogue agent. We demonstrate that LLMs significantly outperform traditional models in tasks like Data Practice Identification, Choice Identification, Policy Summarization, and Privacy Question Answering, setting new benchmarks in privacy policy analysis. Building on these findings, we introduce an innovative LLM-based agent that functions as an expert system for processing website privacy policies, guiding users through complex legal language without requiring them to pose specific questions. A user study with 100 participants showed that users assisted by the agent had higher comprehension levels (mean score of 2.6 out of 3 vs. 1.8 in the control group), reduced cognitive load (task difficulty ratings of 3.2 out of 10 vs. 7.8), increased confidence in managing privacy, and completed tasks in less time (5.5 minutes vs. 15.8 minutes). This work highlights the potential of LLM-based agents to transform user interaction with privacy policies, leading to more informed consent and empowering users in the digital services landscape."
    },
    {
        "title": "OCEBO: Object-Centric Pretraining by Target Encoder Bootstrapping",
        "link_suffix": "/forum?id=7d2JwGbxhA",
        "link": "https://openreview.net/forum?id=7d2JwGbxhA",
        "pdf_link": "https://openreview.net/pdf?id=7d2JwGbxhA",
        "keywords": "Object-centric learning, bootstrapping, self-supervised pretraining",
        "abstract": "Object-centric representation learning has recently been successfully applied to real-world datasets. This success can be attributed to pretrained non-object-centric foundation models, whose features serve as reconstruction targets for slot attention. However, targets must remain frozen throughout the training, which sets an upper bound on the performance object-centric models can attain. Attempts to update the target encoder by bootstrapping result in large performance drops, which can be attributed to its lack of object-centric inductive biases, causing the object-centric model's encoder to drift away from representations useful as reconstruction targets.\nTo address these limitations, we propose \\textbf{O}bject-\\textbf{Ce}ntric Pretraining by Target Encoder \\textbf{Bo}otstrapping, a self-distillation setup for training object-centric models from scratch, on real-world data, for the first time ever. In OCEBO, the target encoder is updated as an exponential moving average of the object-centric model, thus explicitly being enriched with object-centric inductive biases introduced by slot attention while removing the upper bound on performance present in other models. We mitigate the slot collapse caused by random initialization of the target encoder by introducing a novel cross-view patch filtering approach that limits the supervision to sufficiently informative patches. When pretrained on 241k images from COCO, OCEBO achieves unsupervised object discovery performance comparable to that of object-centric models with frozen non-object-centric target encoders pretrained on hundreds of millions of images."
    },
    {
        "title": "Modality-Specialized Synergizers for Interleaved Vision-Language Generalists",
        "link_suffix": "/forum?id=7UgQjFEadn",
        "link": "https://openreview.net/forum?id=7UgQjFEadn",
        "pdf_link": "https://openreview.net/pdf?id=7UgQjFEadn",
        "keywords": "vision-language generation, interleaved vision-language instruction tuning",
        "abstract": "Recent advancements in Vision-Language Models (VLMs) have led to the emergence of Vision-Language Generalists (VLGs) capable of understanding and generating both text and images. However, seamlessly generating an arbitrary sequence of text and images remains a challenging task for the current VLGs. One primary limitation lies in applying a unified architecture and the same set of parameters to simultaneously model discrete text tokens and continuous image features. Recent works attempt to tackle this fundamental problem by introducing modality-aware expert models. However, they employ identical architectures to process both text and images, disregarding the intrinsic inductive biases in these two modalities. In this work, we introduce Modality-Specialized Synergizers (MoSS), a novel design that efficiently optimizes existing unified architectures of VLGs with modality-specialized adaptation layers, i.e., a Convolutional LoRA for modeling the local priors of image patches and a Linear LoRA for processing sequential text. This design enables more effective modeling of modality-specific features while maintaining the strong cross-modal integration gained from pretraining. In addition, to improve the instruction-following capability on interleaved text-and-image generation, we introduce LeafInstruct, the first open-sourced interleaved instruction tuning dataset comprising 184,982 high-quality instances on more than 10 diverse domains. Extensive experiments show that VLGs integrated with MoSS achieve state-of-the-art performance, significantly surpassing baseline VLGs in complex interleaved generation tasks. Furthermore, our method exhibits strong generalizability on different VLGs."
    },
    {
        "title": "Coarse Correspondences Boost 3D Spacetime Understanding in Multimodal Language Model",
        "link_suffix": "/forum?id=8ibaVk4mU8",
        "link": "https://openreview.net/forum?id=8ibaVk4mU8",
        "pdf_link": "https://openreview.net/pdf?id=8ibaVk4mU8",
        "keywords": "Multimodal Language Model; 3D Understanding; Temporal Understanding",
        "abstract": "Multimodal language models (MLLMs) are increasingly being applied in real-\nworld environments, necessitating their ability to interpret 3D spaces and compre-\nhend temporal dynamics. Current methods often rely on specialized architectural\ndesigns or task-specific fine-tuning to achieve this. We introduce COARSE CORRE-\nSPONDENCES, a simple lightweight method which enhances MLLMs’ understand-\ning of 3D and temporal concepts using only 2D images, without modifying the\narchitecture or task-specific fine-tuning. Our method uses a lightweight tracking\nmodel to identify primary object correspondences between frames in a video or\nacross different image viewpoints, and then conveys this information to MLLMs\nthrough visual prompting. We demonstrate that this simple training-free approach\nbrings substantial gains to GPT4-V/O consistently on four benchmarks that require\n3D and temporal understanding, including +20.5% improvement on ScanQA,\n+9.7% on OpenEQA’s episodic memory subset, +6.0% on the long-form video\nbenchmark EgoSchema, and +11% on the R2R navigation benchmark. Addition-\nally, we show that COARSE CORRESPONDENCES can also enhance open-source\nMLLMs’ understanding of 3D space (by +6.9% on ScanQA) when applied in both\ntraining and inference and that the improvement can generalize to unseen datasets\nsuch as SQA3D (+3.1%). Taken together, we show that COARSE CORRESPON-\nDENCES effectively and efficiently boosts models’ performance on downstream\ntasks requiring 3D and/or temporal understanding."
    },
    {
        "title": "Zero Shot Generalization of Vision-Based RL Without Data Augmentation",
        "link_suffix": "/forum?id=OpNMWVDdKS",
        "link": "https://openreview.net/forum?id=OpNMWVDdKS",
        "pdf_link": "https://openreview.net/pdf?id=OpNMWVDdKS",
        "keywords": "reinforcement learning, representation learning, disentangled representation learning, associative memory, robotics",
        "abstract": "Generalizing vision-based reinforcement learning (RL) agents to novel environments remains a difficult and open challenge. Current trends are to collect large-scale datasets or use data augmentation techniques to prevent overfitting and improve downstream generalization. However, the computational and data collection costs increase exponentially with the number of task variations and can destabilize the already difficult task of training RL agents. In this work, we take inspiration from recent advances in computational neuroscience and propose a model, Associative Latent DisentAnglement (ALDA), that builds on standard off-policy RL towards zero-shot generalization. Specifically, we revisit the role of latent disentanglement in RL and show how combining it with a model of associative memory achieves zero-shot generalization on difficult task variationswithoutrelying on data augmentation. Finally, we formally show that data augmentation techniques are a form of weak disentanglement and discuss the implications of this insight."
    }
]