[{"title": "ENHANCING DIVERSITY AND ACCURACY IN PERSONALIZED TAG RECOMMENDATIONS: A HYBRID SEMANTIC AND CONTEXTUAL ANALYSIS APPROACH", "link_suffix": "/forum?id=N4QQNU9HK3", "link": "https://openreview.net/forum?id=N4QQNU9HK3", "pdf_link": "https://openreview.net/pdf?id=N4QQNU9HK3", "keywords": "Personalized content, Tag recommendation, Content-based, Collaborative filtering, Context-aware, Semantic clustering", "abstract": "This paper introduces HYCOMB, a cascading Hybrid model that innovatively integrates\nCollaborative Filtering (CF), Content-Based Filtering (CB), and Context-\nAware (CA) methods to address the challenge of data sparsity in tag recommendation\nsystems. Unlike traditional models that rely heavily on user-item interactions,\nHYCOMB enhances recommendation diversity and interpretability by utilizing\nsemantic clustering in CF to extract and analyze user sentiment from tags, adding\na layer of nuanced understanding often missing in conventional systems. The CB\ncomponent advances this by applying sophisticated NLP techniques to refine these\nrecommendations based on item attributes, while the CA component incorporates\nmovie synopses for deeper contextual understanding. Developed and tested using\nthe MovieLens 20M dataset, our model demonstrates significant outperformance\nover baseline methods in terms of precision and recall, achieving scores of 0.813\nand 0.364 respectively. Further, a newly introduced Overall Total Similarity metric\nthat underscores its ability to deliver relevant and diverse recommendations.\nHYCOMB\u2019s strategic amalgamation of CF, CB, and CA not only mitigates the\neffects of sparse data but also improves the precision and diversity of tag recommendations,\nreflecting a more accurate alignment with user preferences.", "title_embedding_index": 19100, "title_abs_embedding_index": 19125}, {"title": "Masked VAE: Distributionally-Informed Self-Supervised Vision Learning", "link_suffix": "/forum?id=tt0SCefKQL", "link": "https://openreview.net/forum?id=tt0SCefKQL", "pdf_link": "https://openreview.net/pdf?id=tt0SCefKQL", "keywords": "Self-Supervised Learning, Transformers, Vision, Masked Autoencoders", "abstract": "Masked pre-training with transformers is a popular self-supervised representation learning paradigm, initially showing success in NLP before moving to CV.\nHowever, an aspect of masked pre-training that is missing in CV is the ability to capture the distribution of possible outputs. In NLP pre-training methods, the distribution is expressed as a softmax output layer. In CV, the SoTA masked autoencoder (MAE) simply ignores the possibility of a distribution, only giving a point estimate of the masked pixels' RGB values. This formulation is fundamentally limited, as it models an under-constrained problem as well-posed. \nThis poses limitations when deployed for completion tasks: it can give only one possible completion, when in reality, a scene could be completed in many different ways, e.g., a partial kitchen could have spoons, cups, pizzas, etc. under the mask. This inability to complete multiple modes indicates the weakness of the underlying representation in capturing contextual relationships.\nTowards creating a distributionally-aware formulation with contextually-aware representations, we propose the Masked VAE, a transformer-based self-supervised learning method that combines ideas from the MAE and the variational autoencoder (VAE). Like the VAE, we model the \"masked\" latent space tokens as samples from a multivariate Gaussian distribution, while keeping the MAE's deterministic latent codes for the visible tokens. \nEvaluations show that our method can create contextually plausible masked completions in a distributionally-aware manner, while matching the state-of-the-art in representation performance in downstream classification tasks.", "title_embedding_index": 19101, "title_abs_embedding_index": 19126}, {"title": "Prompt-Agnostic Erasure for Diffusion Models Using Task Vectors", "link_suffix": "/forum?id=2wDXNF0Gv4", "link": "https://openreview.net/forum?id=2wDXNF0Gv4", "pdf_link": "https://openreview.net/pdf?id=2wDXNF0Gv4", "keywords": "Concept Erasure", "abstract": "With the rapid growth of text-to-image models, a variety of techniques have been suggested to prevent undesirable image generations. Yet, these methods often only protect against specific user prompts and have been shown to allow undesirable generations with other inputs. Here we focus on \\textit{unconditionally} erasing a concept from a text-to-image model rather than conditioning the erasure on the user's prompt. We first show that compared to input-dependent erasure methods, concept erasure that uses Task Vectors (TV) is more robust to unexpected user inputs, not seen during training. However, TV-based erasure can also affect the core performance of the edited model, particularly when the required edit strength is unknown. To this end, we propose a method called \\textit{Diverse Inversion}, which we use to estimate the required strength of the TV edit. Diverse Inversion finds within the model input space a large set of word embeddings, each of which induces the generation of the target concept. We find that encouraging diversity in the set makes our estimation more robust to unexpected prompts. Finally, we show that Diverse Inversion enables us to apply a TV edit only to a subset of the model weights, enhancing the erasure capabilities while better maintaining model utility.", "title_embedding_index": 19102, "title_abs_embedding_index": 19127}, {"title": "From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks", "link_suffix": "/forum?id=ZXaocmXc6d", "link": "https://openreview.net/forum?id=ZXaocmXc6d", "pdf_link": "https://openreview.net/pdf?id=ZXaocmXc6d", "keywords": "Deep learning, Learning theory, Learning Regime, Rich, Lazy", "abstract": "Biological and artificial neural networks develop internal representations that enable them to perform complex tasks. In artificial networks, the effectiveness of these models relies on their ability to build task specific representation, a process influenced by interactions among datasets, architectures, initialization strategies, and optimization algorithms. Prior studies highlight that different initializations can place networks in either a lazy regime, where representations remain static, or a rich/feature learning regime, where representations evolve dynamically. Here, we examine how initialization influences learning dynamics in deep linear neural networks, deriving exact solutions for lambda-balanced initializations-defined by the relative scale of weights across layers. These solutions capture the evolution of representations and the Neural Tangent Kernel across the spectrum from the rich to the lazy regimes. Our findings deepen the theoretical understanding of the impact of weight initialization on learning regimes, with implications for continual learning, reversal learning, and transfer learning, relevant to both neuroscience and practical applications.", "title_embedding_index": 19103, "title_abs_embedding_index": 19128}, {"title": "Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models", "link_suffix": "/forum?id=I4e82CIDxv", "link": "https://openreview.net/forum?id=I4e82CIDxv", "pdf_link": "https://openreview.net/pdf?id=I4e82CIDxv", "keywords": "Interpretability, mechanistic interpretability, circuits, spurious correlations, generalization, dictionary learning", "abstract": "We introduce methods for discovering and applyingsparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms in neural networks. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.", "title_embedding_index": 19104, "title_abs_embedding_index": 19129}, {"title": "Stochastic Matching Bandits under Preference Feedback", "link_suffix": "/forum?id=iKLSISIPH7", "link": "https://openreview.net/forum?id=iKLSISIPH7", "pdf_link": "https://openreview.net/pdf?id=iKLSISIPH7", "keywords": "Matching bandits, Preference Feedback", "abstract": "In this study, we propose a new bandit framework of stochastic matching employing the Multinomial Logit (MNL) choice model with feature information. In this framework, agents on one side are assigned to arms on the other side, and each arm stochastically accepts an agent among the assigned pool of agents based on its unknown preference, allowing a possible outside option of not accepting any. \nThe objective is to minimize regret by maximizing the probability of successful matching. \nFor this framework, we first propose an elimination-based algorithm that achieves a regret bound of $\\tilde{O}\\big(K\\sqrt{rKT} \\big)$ over time horizon $T$, where $K$ is the number of arms and $r$ is the rank of feature space.  Furthermore, we propose an approach to resolve the computation issue regarding combinatorial optimization in the algorithm.\nLastly, we evaluate the performances of our algorithm through experiments comparing with the existing showing the superior performances of our algorithm.", "title_embedding_index": 19105, "title_abs_embedding_index": 19130}, {"title": "Dynamic Multi-product Selection and Pricing under Preference Feedback", "link_suffix": "/forum?id=DOXnqYLCcd", "link": "https://openreview.net/forum?id=DOXnqYLCcd", "pdf_link": "https://openreview.net/pdf?id=DOXnqYLCcd", "keywords": "Dynamic pricing, Preference Feedback, Bandits", "abstract": "In this study, we investigate the problem of dynamic multi-product selection and pricing by introducing a novel framework based on acensored multinomial logit(C-MNL) choice model. In this model, sellers present a set of products with prices, and buyers filter out products priced above their valuation, purchasing at most one product from the remaining options based on their preferences. The goal is to maximize seller revenue by dynamically adjusting product offerings and prices, while learning both product valuations and buyer preferences through purchase feedback. To achieve this, we propose a Lower Confidence Bound (LCB) pricing strategy. By combining this pricing strategy with either an Upper Confidence Bound (UCB) or Thompson Sampling (TS) product selection approach, our algorithms achieve regret bounds of $\\tilde{O}(d^{\\frac{3}{2}}\\sqrt{T})$ and $\\tilde{O}(d^{2}\\sqrt{T})$, respectively. Finally, we validate the performance of our methods through simulations, demonstrating their effectiveness.", "title_embedding_index": 19106, "title_abs_embedding_index": 19131}, {"title": "Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference", "link_suffix": "/forum?id=cf7NTWv1iW", "link": "https://openreview.net/forum?id=cf7NTWv1iW", "pdf_link": "https://openreview.net/pdf?id=cf7NTWv1iW", "keywords": "LLM inference acceleration, prompt tuning, hardware-aware design", "abstract": "The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in their hardware performance. \nWhile recent research has investigated various speculative decoding techniques for multi-token generation, these efforts have primarily focused on improving processing speed such as throughput.\nCrucially, they often neglect other metrics essential for real-life deployments, such as memory consumption and training cost.\nTo overcome these limitations, we propose a novel parallel prompt decoding that requires only $0.0002$% trainable parameters, enabling efficient training on a single A100-40GB GPU in just 16 hours. \nInspired by the human natural language generation process,  PPD approximates outputs generated at future timesteps in parallel by using multiple prompt tokens. This approach partially recovers the missing conditional dependency information necessary for multi-token generation, resulting in up to a 28% higher acceptance rate for long-range predictions.\nFurthermore, we present a hardware-aware two-stage tree pruning algorithm that adaptively optimizes this decoding scheme to fully leverage the computational capacities on different GPUs.\nThrough extensive experiments across LLMs ranging from MobileLlama to Vicuna-13B on a wide range of benchmarks, our approach demonstrates up to 2.49$\\times$ speedup and maintains a minimal runtime memory overhead of just $0.0004%$.\nMore importantly, our parallel prompt decoding can serve as an orthogonal optimization for synergistic integration with existing speculative decoding,\nshowing up to $1.22\\times$ further speed improvement. Our code will be open-sourced upon acceptance of the paper.", "title_embedding_index": 19107, "title_abs_embedding_index": 19132}, {"title": "WATERMARKING GRAPH NEURAL NETWORKS VIA EXPLANATIONS FOR OWNERSHIP PROTECTION", "link_suffix": "/forum?id=EgP6IEyfYJ", "link": "https://openreview.net/forum?id=EgP6IEyfYJ", "pdf_link": "https://openreview.net/pdf?id=EgP6IEyfYJ", "keywords": "Watermarking, GNNs, Ownership, Verification, Ownership Verification, Explanation, Graph", "abstract": "Graph Neural Networks (GNNs) are the mainstream method to learn pervasive graph data and are widely deployed in industry, making their intellectual property valuable. However, protecting GNNs from unauthorized use remains a challenge. Watermarking, which embeds ownership information into a model, is a potential solution. However, existing watermarking methods have two key limitations: First, almost all of them focus on non-graph data, with watermarking GNNs for complex graph data largely unexplored. Second, the de facto backdoor-based watermarking methods pollute training data and induce ownership ambiguity through intentional misclassification. We develop a novel method that watermarks explanations of GNN predictions. Our explanation-based watermarking inherits the strengths of backdoor-based methods (e.g., robust to watermark removal attacks), but avoids data pollution and eliminates intentional misclassification. In particular, our method learns to embed the watermark in GNN explanations such that this unique watermark is statistically distinct from other potential solutions, and ownership claims must show statistical significance to be verified. We theoretically prove that, even with full knowledge of our method, locating the watermark is an NP-hard problem. Empirically, our method manifests robustness to removal attacks like fine-tuning and pruning. By addressing these challenges, our approach marks a significant advancement in protecting GNN intellectual property.", "title_embedding_index": 19108, "title_abs_embedding_index": 19133}, {"title": "Hierarchical Corpus Encoder: Fusing Generative Retrieval and Dense Indices", "link_suffix": "/forum?id=Zx10nVb3Bs", "link": "https://openreview.net/forum?id=Zx10nVb3Bs", "pdf_link": "https://openreview.net/pdf?id=Zx10nVb3Bs", "keywords": "information retrieval, generative retrieval, dense retrieval", "abstract": "Generative retrieval employs sequence models for conditional generation of document IDs based on a query (DSI (Tay et al. (2022); NCI (Wang et al., 2022); inter alia). While this has led to improved performance in zero-shot retrieval, it is a challenge to support documents not seen during training.  We identify the performance of generative retrieval lies in contrastive training between sibling nodes in a document hierarchy. This motivates our proposal, thehierarchical corpus encoder(HCE), which can be supported by traditional dense encoders. Our experiments show that HCE achieves superior results than generative retrieval models under both unsupervised zero-shot and supervised settings, while also allowing the easy addition and removal of documents to the index.", "title_embedding_index": 19109, "title_abs_embedding_index": 19134}, {"title": "BrailleVision: Text Instruction Tuning of LLMs to Improve Visual Skills", "link_suffix": "/forum?id=31ssWC2gL8", "link": "https://openreview.net/forum?id=31ssWC2gL8", "pdf_link": "https://openreview.net/pdf?id=31ssWC2gL8", "keywords": "LLMs, Vision-Language Models", "abstract": "Large Language Models (LLMs) have shown exceptional proficiency in natural language processing tasks. More recently, their potential is being explored in vision-centric applications. Current multimodal large language models (MLLMs) incorporate general-purpose LLMs through multimodal instruction tuning. These LLMs, however, lack prior vision centric text based training, potentially limiting their effectiveness. In this work, we propose a novel approach to enhance vision-related capabilities of general-purpose LLMs through instruction fine-tuning with vision-centric text data. Specifically, we curate a diverse dataset, BrailleVision-360K, to teach skills such as visual perception, abstraction, and spatio-temporal reasoning without the use of visual data, analogous to how Braille codes are used by the visually impaired. The dataset is constructed in an automated manner by utilizing LLMs, bootstrapping from existing datasets, and employing VLMs to improve quality. Next, to fine-tune an LLM with this dataset, we introduce Fine-SFT, a novel fine-tuning approach that improves upon standard supervised fine-tuning and preference optimization techniques. Our vision-specialized LLM shows significant performance gains in tasks such as visual classification and open vocabulary detection. Furthermore, when used as the `backbone' for an MLLM, our model outperforms existing LLMs on standard visual QA benchmarks while reducing hallucinations, highlighting the importance of vision-centric pretraining of LLMs in multimodal tasks.", "title_embedding_index": 19110, "title_abs_embedding_index": 19135}, {"title": "Release the Powers of Prompt Tuning: Cross-Modality Prompt Transfer", "link_suffix": "/forum?id=SYnIf4LxAG", "link": "https://openreview.net/forum?id=SYnIf4LxAG", "pdf_link": "https://openreview.net/pdf?id=SYnIf4LxAG", "keywords": "Cross-Modality, Prompt Transfer", "abstract": "Prompt Tuning adapts frozen models to new tasks by prepending a few learnable embeddings to the input.\nHowever, it struggles with tasks that suffer from data scarcity.\nTo address this, we explore Cross-Modality Prompt Transfer, leveraging prompts pretrained on a data-rich modality to improve performance on data-scarce tasks in another modality.\nAs a pioneering study, we first verify the feasibility of cross-modality prompt transfer by directly applying frozen source prompts (trained on the source modality) to the target modality task.\nTo empirically study cross-modality prompt transferability, we train a linear layer to adapt source prompts to the target modality, thereby boosting performance and providing ground-truth transfer results.\nRegarding estimating prompt transferability, existing methods show ineffectiveness in cross-modality scenarios where the gap between source and target tasks is larger.\nWe address this by decomposing the gap into the modality gap and the task gap, which we measure separately to estimate the prompt transferability more accurately.\nAdditionally, we propose Attention Transfer to further reduce the gaps by injecting target knowledge into the prompt and reorganizing a top-transferable source prompt using an attention block.\nWe conduct extensive experiments involving prompt transfer from 13 source language tasks to 19 target vision tasks under three settings.\nOur findings demonstrate that:\n(i) cross-modality prompt transfer is feasible, supported by in-depth analysis;\n(ii) measuring both the modality and task gaps is crucial for accurate prompt transferability estimation, a factor overlooked by previous studies;\n(iii) cross-modality prompt transfer can significantly release the powers of prompt tuning on data-scarce tasks, as evidenced by comparisons with a newly released prompt-based benchmark.", "title_embedding_index": 19111, "title_abs_embedding_index": 19136}, {"title": "Quantifying Variance in Evaluation Benchmarks", "link_suffix": "/forum?id=E2RyjrBMVZ", "link": "https://openreview.net/forum?id=E2RyjrBMVZ", "pdf_link": "https://openreview.net/pdf?id=E2RyjrBMVZ", "keywords": "Evaluations, Language Models, LLMs", "abstract": "Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models -- both openly available and pretrained from scratch -- we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale (\u223c7B) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.", "title_embedding_index": 19112, "title_abs_embedding_index": 19137}, {"title": "Sample-Efficient Co-Optimization of Agent Morphology and Policy with Self-Imitation Learning", "link_suffix": "/forum?id=Iz230vHUy0", "link": "https://openreview.net/forum?id=Iz230vHUy0", "pdf_link": "https://openreview.net/pdf?id=Iz230vHUy0", "keywords": "Deep Reinforcement Learning, Imitation Learning, Multi-Embodiment, Design Optimization", "abstract": "The task of co-optimizing the body and behaviour of agents has been a longstanding problem in the fields of evolutionary robotics and embodied AI. Previous work has largely focused on the development of learning methods exploiting massive parallelization of agent evaluations with large population sizes, a paradigm which is applicable to simulated agents but cannot be transferred to the real world\ndue to the assoicated costs with the production of embodiments and robots. Furthermore, recent data-efficient approaches utilizing \n reinforcement learning can suffer from distributional shifts in transition dynamics as well as in state and action spaces when experiencing new body morphologies. \nIn this work, we propose a new co-adaptation method combining reinforcement learning and State-Aligned SelfImitation Learning to co-optimize embodiment and behavioural policies withing a handful of design iterations. We show that the integration of a self-imitation signal\nimproves the data-efficiency of the co-adaptation process as well as the behavioural recovery when adapting morphological parameters.", "title_embedding_index": 19113, "title_abs_embedding_index": 19138}, {"title": "Flow-based Maximum Entropy Domain Randomization for Multi-step Assembly", "link_suffix": "/forum?id=sHmfmQjfCy", "link": "https://openreview.net/forum?id=sHmfmQjfCy", "pdf_link": "https://openreview.net/pdf?id=sHmfmQjfCy", "keywords": "Reinforcement Learning, Domain Randomization, Uncertainty, Assembly, Planning", "abstract": "Domain randomization in reinforcement learning is an established technique for increasing the robustness of control policies learned in simulation. By randomizing properties of the environment during training, the learned policy can be conformant to uncertainty along the randomized dimensions. While the environment distribution is typically specified by hand, in this paper we investigate the problem of automatically discovering this sampling distribution via entropy-regularized reward maximization of a neural sampling distribution in the form of a normalizing flow. We show that this architecture is more flexible and results in better robustness than existing approaches to learning simple parameterized sampling distributions. We demonstrate that these policies can be used to learn robust policies for contact-rich assembly tasks. Additionally, we explore how these sampling distributions can be used for out-of-distribution detection in the context of an uncertainty-aware multi-step manipulation planner.", "title_embedding_index": 19114, "title_abs_embedding_index": 19139}, {"title": "Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness", "link_suffix": "/forum?id=9HsfTgflT7", "link": "https://openreview.net/forum?id=9HsfTgflT7", "pdf_link": "https://openreview.net/pdf?id=9HsfTgflT7", "keywords": "spiking neural networks, direct training, event-driven friendliness", "abstract": "Spiking Neural Networks (SNNs), models inspired by neural mechanisms in the brain, allow for energy-efficient implementation on neuromorphic hardware. However, SNNs trained with current direct training approaches are constrained to a specific time step. This \"temporal inflexibility\" 1) hinders SNNs' deployment on time-step-free fully event-driven chips and 2) prevents energy-performance balance based on dynamic inference time steps. In this study, we first explore the feasibility of training SNNs that generalize across different time steps. We then introduce Mixed Time-step Training (MTT), a novel method that enables the development of Temporally Flexible SNNs (TFSNNs), allowing SNNs to adapt to diverse temporal structures. During each iteration of MTT, random time steps are assigned to different SNN stages, with spikes transmitted between stages via communication modules. After training, the weights are deployed and evaluated on both time-step-based and fully event-driven platforms. Experimental results show that TFSNN demonstrates remarkable temporal flexibility, excellent event-driven friendliness for deployment (nearly lossless on N-MNIST and 10.1% higher than standard methods on CIFAR10-DVS), robust generalization capability surpassing existing fixed-time-step training methods, and near SOTA performance. To the best of our knowledge, this is the first work to report the results of large-scale SNN deployment on fully event-driven scenarios.", "title_embedding_index": 19115, "title_abs_embedding_index": 19140}, {"title": "Improving out-of-distribution generalization by mimicking the human visual diet.", "link_suffix": "/forum?id=glUf3YGcJQ", "link": "https://openreview.net/forum?id=glUf3YGcJQ", "pdf_link": "https://openreview.net/pdf?id=glUf3YGcJQ", "keywords": "out-of-distribution generalization, human vision, generalization, scene context", "abstract": "Human visual experience is markedly different from the large-scale computer vision datasets consisting of internet images. Babies densely sample a few $3D$ scenes with diverse variations such as object viewpoints or illuminations, while datasets like ImageNet contain one single snapshot from millions of 3D scenes. We investigated how these differences in input data composition (i.e., visual diet) impact the Out-Of-Distribution (OOD) generalization capabilities of a visual system. Training models on a dataset mimicking attributes of the human-like visual diet improved generalization to OOD lighting, material, and viewpoint changes by up to $18\\%$. This observation held despite the fact that the models were trained on $1,000$-fold less training data. Furthermore, when trained on purely synthetic data and tested on natural images, incorporating these visual diet attributes in the training dataset improved OOD generalization by $17\\%$. These experiments are enabled by our newly proposed benchmark---the Human Visual Diet (HVD) dataset, and a new model (Human Diet Network) designed to leverage the attributes of a human-like visual diet. These findings highlight a critical problem in modern day Artificial Intelligence---building better datasets requires thinking beyond dataset size and rather focus on improving data composition. All data and source code will be made available upon publication.", "title_embedding_index": 19116, "title_abs_embedding_index": 19141}, {"title": "Angle Graph Transformer: Capturing Higher-Order Structures for Accurate Molecular Geometry Learning", "link_suffix": "/forum?id=WPDsT4eIKH", "link": "https://openreview.net/forum?id=WPDsT4eIKH", "pdf_link": "https://openreview.net/pdf?id=WPDsT4eIKH", "keywords": "Molecular Representation Learning, Graph Transformer, Molecular Geometry Pretraining", "abstract": "Existing Graph Transformer models primarily focus on leveraging atomic and chemical bond properties along with basic geometric structures to learn representations of fundamental elements in molecular graphs, such as nodes and edges. However, higher-order structures like bond angles and torsion angles, which significantly influence key molecular properties, have not received sufficient attention. This oversight leads to inadequate geometric conformation accuracy and difficulties in precise local chirality determination, thereby limiting model performance in molecular property prediction tasks.\nTo address this issue, we propose the $A$ngle $G$raph $T$ransformer ($AGT$). AGT directly models directed bond angles and torsion angles, introducing higher-order structural representations to molecular graph learning for the first time. This approach enables AGT to determine local chirality within molecular representations and directly predict torsion angles. We introduce a novel directed cycle angle loss, allowing AGT to predict bond angles and torsion angles from low-precision molecular conformations. These properties, along with interatomic distances, are then applied to downstream molecular property prediction tasks using a pre-trained AGT with hierarchical virtual nodes.\nOur model achieves new state-of-the-art (SOTA) results on the PCQM4Mv2 and OC20 IS2RE datasets. Through transfer learning, AGT also demonstrates competitive performance on molecular property prediction benchmarks including QM9, LIT-PCBA, MOLPCBA, and MOLHIV. Further ablation studies reveal that the conformations generated by our method most closely approximate DFT-generated conformations among existing methods, owing to the constraints imposed by bond angles and torsion angles.", "title_embedding_index": 19117, "title_abs_embedding_index": 19142}, {"title": "GOFA: A Generative One-For-All Model for Joint Graph Language Modeling", "link_suffix": "/forum?id=mIjblC9hfm", "link": "https://openreview.net/forum?id=mIjblC9hfm", "pdf_link": "https://openreview.net/pdf?id=mIjblC9hfm", "keywords": "GNN;Graph foundation model;LLM;", "abstract": "Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better---yet, no existing work can achieve both simultaneously. In this paper, we first identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, structural understanding, and information retrieval tasks to obtain the above GFM properties. The pre-trained model is further instruction fine-tuned to obtain the task-solving ability. Our GOFA model is evaluated on various downstream tasks unseen during the pre-training and fine-tuning phases, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios.", "title_embedding_index": 19118, "title_abs_embedding_index": 19143}, {"title": "Supervised Chain of Thought", "link_suffix": "/forum?id=pXIbcRPxWR", "link": "https://openreview.net/forum?id=pXIbcRPxWR", "pdf_link": "https://openreview.net/pdf?id=pXIbcRPxWR", "keywords": "Chain of Thought, LLMs, Prompting, CoT, Model Analysis", "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing and hold significant promise for advancements in Artificial Intelligence. However, the backbone architecture of most mainstream LLMs, the Transformer, has inherent limitations regarding computational depth, making them theoretically incapable of solving many reasoning tasks that require increasing depth. Chain of Thought (CoT) techniques, however, have been shown to mitigate these architectural limitations, as demonstrated by several theoretical works, offering a viable approach to solving complex reasoning tasks that were previously out of reach.\nDespite its successes, CoT and its variants (such as Tree of Thought, Graph of Thought, etc.) follow a one-prompt-for-all-tasks approach. Specifically, they rely on a single prompt structure (e.g., \"think step by step\") for a wide range of tasks, from counting to sorting, and from solving mathematical problems to tackling algorithmic challenges. This creates significant challenges for the model to generate the correct steps template for different tasks, as it requires searching in large prompt template space.\nIn this work, we build on previous theoretical analyses of CoT to demonstrate how the \"one-prompt-for-all-tasks\" template can negatively impact the computability of LLMs. We divide the solution space into prompt space and answer space, showing that the CoT process requires task-specific supervision to accurately navigate the prompt space and achieve optimal performance. Through experiments with the latest  LLMs, we reveal a significant gap in reasoning ability when supervision is applied versus when it is not.\nOur aim is to provide insights into the mechanisms behind CoT and to inspire the effective design of CoT variants. Additionally, we highlight the key limitations of traditional ``unsupervised'' prompting approaches, suggesting the need for more nuanced, task-specific \"supervised\" CoT for effective reasoning with LLMs.", "title_embedding_index": 19119, "title_abs_embedding_index": 19144}, {"title": "FALCON: A Feedback-Driven Adaptive Long/Short-Term Memory Reinforced Coding Optimization", "link_suffix": "/forum?id=N18Z2MkMEa", "link": "https://openreview.net/forum?id=N18Z2MkMEa", "pdf_link": "https://openreview.net/pdf?id=N18Z2MkMEa", "keywords": "Automated agent generation framework   Self-evolving agents    LLM Manager   Task-specific Role Builder", "abstract": "Recently, large language models (LLMs) have achieved significant progress in automated code generation. Despite their strong instruction-following capabilities, these models frequently struggled to align with user intent in the coding scenario. In particular, they were hampered by datasets that lacked diversity and failed to address specialized tasks or edge cases. Furthermore, challenges in supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) led to failures in generating precise, human-intent-aligned code. To tackle these challenges and improve the code generation performance for automated programming systems, we propose Feedback-driven Adaptive Long/short-term memory reinforced Coding OptimizatioN (i.e., FALCON). FALCON is structured into two hierarchical levels, from the global level, long-term memory improves code quality by retaining and applying learned knowledge, while from the local level, short-term memory allows for the incorporation of immediate feedback from compilers and AI systems. Additionally, we introduce meta-reinforcement learning with feedback rewards to solve the global-local bi-level optimization problem and enhance the model\u2019s adaptability across diverse code generation tasks. Extensive experiments are conducted and it is found that our technique achieves state-of-the-art performance, leading other reinforcement learning methods by more than 4.5 percentage points on the MBPP benchmark and 6.1 percentage points on the Humaneval benchmark. The open-sourced code is publicly available athttps://anonymous.4open.science/r/FALCON-BFE0/README.md.", "title_embedding_index": 19120, "title_abs_embedding_index": 19145}, {"title": "Continuous Surface Normal Integration", "link_suffix": "/forum?id=P5rRGMk40p", "link": "https://openreview.net/forum?id=P5rRGMk40p", "pdf_link": "https://openreview.net/pdf?id=P5rRGMk40p", "keywords": "normal integration, shape modelling, shape recovery", "abstract": "We address a novel task for monocular explicit surface reconstruction that extends traditional surface normal integration over measurements on a regular grid to direct continuous surface depth estimation. Our solution accepts coordinates as queries and predicts both the normal and depth of an arbitrary query point by its relative locations and orientations to the points distributed in its vicinity. In general, all points are regarded by our model as random samples drawn from an underlying continuous gradient field of a surface which we parameterize using a field of polynomials to establish its topology. We establish a mapping from coordinates to a sequence of learnable polynomial coefficients to model a continuous surface and train a neural network to approximate it. We  decompose a continuous surface representation into two components: (1) a set of grid points of unknown orientations whose locations are picked by a quadtree and (2) a set of sample points whose orientations are directly observable. Our training workflow estimates the normal of grid points and the locations of depth discontinuities iteratively. During each iteration, we generate a normal map of grid points for it to be processed by a standard bilateral normal integrator to identify the locations of depth discontinuities, which we use to refine the estimation for grid-based normal map in the subsequent iteration. As a result, the learned model generates both normal and depth for arbitrary coordinates accurately in a continuous field. We provide both theoretical formulation for our design and extensive empirical evidence to demonstrate that our proposed method not only delivers a performance as effective as its grid-based counterpart approaches but also flexibly and accurately addresses the continuous cases that existing methods are unable to handle.", "title_embedding_index": 19121, "title_abs_embedding_index": 19146}, {"title": "Analyzing and Optimizing Perturbation of DP-SGD Geometrically", "link_suffix": "/forum?id=gG7P1SL0QS", "link": "https://openreview.net/forum?id=gG7P1SL0QS", "pdf_link": "https://openreview.net/pdf?id=gG7P1SL0QS", "keywords": "Differential Privacy, Stochastic Gradient Descent, Deep Learning, Efficiency Analysis and Enhancement", "abstract": "Differential privacy (DP) has become a prevalent privacy model in\na wide range of machine learning tasks, especially after the debut\nof DP-SGD. However, DP-SGD, which directly perturbs gradients\nin the training iterations, fails to mitigate the negative impacts of\nnoise on gradient direction. As a result, DP-SGD is often inefficient.\nAlthough various solutions (e.g., clipping to reduce the sensitivity\nof gradients and amplifying privacy bounds to save privacy budgets)\nare proposed to trade privacy for model efficiency, the root cause of\nits inefficiency is yet unveiled.\nIn this work, we first generalize DP-SGD and theoretically derive\nthe impact of DP noise on the training process. Our analysis reveals\nthat, in terms of a perturbed gradient, only the noise on a direction\nhas eminent impact on the model efficiency while that on magnitude\ncan be mitigated by optimization techniques, i.e., fine-tuning gradient\nclipping and learning rate. Besides, we confirm that traditional\nDP introduces biased noise on the direction when adding unbiased\nnoise to the gradient itself. Overall, the perturbation of DP-SGD is\nactually sub-optimal from a geometric perspective. Motivated by\nthis, we design a geometric perturbation strategy GeoDP within the\nDP framework, which perturbs the direction and the magnitude of a\ngradient, respectively. By directly reducing the noise on the direction,\nGeoDP mitigates the negative impact of DP noise on model\nefficiency with the same DP guarantee. Extensive experiments on\ntwo public datasets (i.e., MNIST and CIFAR-10), one synthetic\ndataset and three prevalent models (i.e., Logistic Regression, CNN\nand ResNet) confirm the effectiveness and generality of our strategy.", "title_embedding_index": 19122, "title_abs_embedding_index": 19147}, {"title": "Auditing Data Controller Compliance with Data Withdrawal", "link_suffix": "/forum?id=85X9awoVtv", "link": "https://openreview.net/forum?id=85X9awoVtv", "pdf_link": "https://openreview.net/pdf?id=85X9awoVtv", "keywords": "transparency, auditing, data privacy, right-to-object, verification", "abstract": "We study auditing total data withdrawal, the case in which a user requests the exclusion of their data from both the training and test data for some machine learning task. This approach is motivated by the need for comprehensive compliance with data privacy regulations and legal frameworks around the world. We conceptualize the task of auditing total data withdrawal as an optimization problem. Compliance verification is conducted under mild assumptions using a dedicated verification algorithm. We then evaluate this formulation over various datasets, architectures, and verification hyperparameters. Our verification algorithm serves as a tool for regulators to ensure auditable compliance and provides enhanced privacy guarantees for users.", "title_embedding_index": 19123, "title_abs_embedding_index": 19148}, {"title": "Riemannian Manifold Learning for Stackelberg Games with Neural Flow Representations", "link_suffix": "/forum?id=eAFNJk63KE", "link": "https://openreview.net/forum?id=eAFNJk63KE", "pdf_link": "https://openreview.net/pdf?id=eAFNJk63KE", "keywords": "Neural Normalizing Flows, Stackelberg Games, Riemannian Manifolds", "abstract": "We present a novel framework for online learning in Stackelberg general-sum games, where two agents, the leader and follower, engage in sequential turn-based interactions. At the core of this approach is a learned diffeomorphism that maps the joint action space to a smooth Riemannian manifold, referred to as the $\\textit{Stackelberg manifold}$. This mapping, facilitated by neural normalizing flows, ensures the formation of tractable isoplanar subspaces, enabling efficient techniques for online learning. By assuming linearity between the agents' reward functions on the $\\textit{Stackelberg manifold}$, our construct allows the application of standard bandit algorithms. We then provide a rigorous theoretical basis for regret minimization on convex manifolds and establish finite-time bounds on simple regret for learning Stackelberg equilibria. This integration of manifold learning into game theory uncovers a previously unrecognized potential for neural normalizing flows as an effective tool for multi-agent learning. We present empirical results demonstrating the effectiveness of our approach compared to standard baselines, with applications spanning domains such as cybersecurity and economic supply chain optimization.", "title_embedding_index": 19124, "title_abs_embedding_index": 19149}]