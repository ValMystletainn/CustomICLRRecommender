[
    {
        "title": "Gray-Box Fine-Tuning for Single Backbone Domain Experts",
        "link_suffix": "/forum?id=j7oTk2nEoN",
        "link": "https://openreview.net/forum?id=j7oTk2nEoN",
        "pdf_link": "https://openreview.net/pdf?id=j7oTk2nEoN",
        "keywords": "Multi-modal Learning, Vision-Language, Foundation Models",
        "abstract": "The emergence of foundational models has greatly improved performance across various downstream tasks, with fine-tuning often yielding even better results. However, existing fine-tuning approaches typically require access to model weights and layers, leading to challenges such as managing multiple model copies or inference pipelines, inefficiencies in edge device optimization, and concerns over proprietary rights, privacy, and exposure to unsafe model variants. In this paper, we address these challenges by exploring \"Gray-box\" fine-tuning approaches, where the model's architecture and weights remain hidden, allowing only gradient propagation. We introduce a novel yet simple and effective framework that adapts to new tasks using two lightweight learnable modules at the model's input and output. Additionally, we present a less restrictive variant that offers more entry points into the model, balancing performance with model exposure. We evaluate our approaches across several backbones on benchmarks for text-image alignment, text-video alignment, and sketch-image alignment. Our results demonstrate that, despite having limited access to the model, our Gray-box approaches achieve competitive performance with full-access fine-tuning methods."
    },
    {
        "title": "MolReFlect: Towards Fine-grained In-Context Alignment between Molecules and Texts",
        "link_suffix": "/forum?id=uo6UsVkkEQ",
        "link": "https://openreview.net/forum?id=uo6UsVkkEQ",
        "pdf_link": "https://openreview.net/pdf?id=uo6UsVkkEQ",
        "keywords": "Large Language Models, In-Context Tuning, Reflection Tuning, Molecule Discovery, Molecule-Text Alignment",
        "abstract": "Molecule discovery is a pivotal research field, impacting everything from the medicines we take to the materials we use. Recently, Large Language Models (LLMs) have been widely adopted in molecule understanding and generation, yet the alignments between molecules and their corresponding captions remain a significant challenge. Previous endeavours often treat the molecule as a general SMILES string or molecular graph, neglecting the fine-grained alignments between the molecular sub-structures and the descriptive textual phrases, which are crucial for accurate and explainable predictions. In this case, we introduce MolReFlect, a novel teacher-student framework designed to contextually perform the molecule-caption alignments in a fine-grained way. Our approach initially leverages a larger teacher LLM to label the detailed alignments by directly extracting critical phrases from molecule captions or SMILES strings and implying them to corresponding sub-structures or characteristics. To refine these alignments, we propose In-Context Selective Reflection, which retrieves previous extraction results as context examples for teacher LLM to reflect and lets a smaller student LLM select from in-context reflection and previous extraction results. Finally, we enhance the learning process of the student LLM through Chain-of-Thought In-Context Molecule Tuning, integrating the fine-grained alignments and the reasoning processes within the Chain-of-Thought format. Our experimental results demonstrate that MolReFlect enables LLMs like Mistral-7B to significantly outperform the previous baselines, achieving SOTA performance on the ChEBI-20 dataset. This advancement not only enhances the generative capabilities of LLMs in the molecule-caption translation task, but also contributes to a more explainable framework."
    },
    {
        "title": "AniMer: Animal Pose and Shape Estimation Using Transformer",
        "link_suffix": "/forum?id=mhJvgHRErR",
        "link": "https://openreview.net/forum?id=mhJvgHRErR",
        "pdf_link": "https://openreview.net/pdf?id=mhJvgHRErR",
        "keywords": "Animal Pose and Shape Estimation; Transformer; Synthetic Dataset",
        "abstract": "Quantitative analysis of animal behavior and biomechanics requires accurate animal pose and shape estimation across species, and is important for animal welfare and biological research. However, the small network capacity of previous methods and limited multi-species dataset leave this problem underexplored. To this end, this paper presents AniMer to estimate animal pose and shape using Transformer, enhancing the reconstruction accuracy of diverse quadrupedal species. AniMer aims to unify the understanding of various quadrupedal forms within a single framework, overcoming the limitations of traditional methods that focus on narrow specific species. A key feature of AniMer is its integration of a high-capacity Transformer-based backbone, which significantly boosts performance. To effectively train AniMer, we aggregate most available open-source quadrupedal datasets, either with 3D or 2D labels, and introduce CtrlAni3D, a novel large-scale synthetic dataset created through a diffusion-based image generation model, consisting of 9.7k pixel-aligned SMAL mesh-labeled images. This combination of a robust backbone and an expansive dataset enables AniMer to outperform existing methods on the multi-species Animal3D dataset and singlespecies dog benchmarks. Experiments on the unseen AnimalKingdom dataset further demonstrate the effectiveness of CtrlAni3D in enhancing generalization capabilities. Our study, through the development of AniMer and CtrlAni3D, underscores the significance of a large-capacity backbone and AI-driven synthetic data generation in advancing animal pose estimation research. Code and data will be released upon publication."
    },
    {
        "title": "I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength",
        "link_suffix": "/forum?id=AcAD4VEgCX",
        "link": "https://openreview.net/forum?id=AcAD4VEgCX",
        "pdf_link": "https://openreview.net/pdf?id=AcAD4VEgCX",
        "keywords": "Video Generation, Camera Control",
        "abstract": "Video generation technologies are developing rapidly and have broad potential applications. Among these technologies, camera control is crucial for generating professional-quality videos that accurately meet user expectations. However, existing camera control methods still suffer from several limitations, including control precision and the neglect of the control for subject motion dynamics. In this work, we propose I2VControl-Camera, a novel camera control method that significantly enhances controllability while providing adjustability over the strength of subject motion. To improve control precision, we employ point trajectory in the camera coordinate system instead of only extrinsic matrix information as our control signal. To accurately control and adjust the strength of subject motion, we explicitly model the higher-order components of the video trajectory expansion, not merely the linear terms, and design an operator that effectively represents the motion strength. We use an adapter architecture that is independent of the base model structure. Experiments on static and dynamic scenes show that our framework outperformances previous methods both quantitatively and qualitatively. Please see the video results in our anonymous github repository:https://github.com/iclr2025sub1844/iclr2025sub1844."
    },
    {
        "title": "BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models",
        "link_suffix": "/forum?id=YaeZwhXJ4k",
        "link": "https://openreview.net/forum?id=YaeZwhXJ4k",
        "pdf_link": "https://openreview.net/pdf?id=YaeZwhXJ4k",
        "keywords": "Model Quantization, Model Compression, Generative Model, Diffusion Model",
        "abstract": "With the advancement of diffusion models (DMs) and the substantially increased computational requirements, quantization emerges as a practical solution to obtain compact and efficient low-bit DMs. However, the highly discrete representation leads to severe accuracy degradation, hindering the quantization of diffusion models to ultra-low bit-widths. This paper proposes a novel weight binarization approach for DMs, namely BinaryDM, pushing binarized DMs to be accurate and efficient by improving the representation and optimization. From the representation perspective, we present an Evolvable-Basis Binarizer (EBB) to enable a smooth evolution of DMs from full-precision to accurately binarized. EBB enhances information representation in the initial stage through the flexible combination of multiple binary bases and applies regularization to evolve into efficient single-basis binarization. The evolution only occurs in the head and tail of the DM architecture to retain the stability of training. From the optimization perspective, a Low-rank Representation Mimicking (LRM) is applied to assist the optimization of binarized DMs. The LRM mimics the representations of full-precision DMs in low-rank space, alleviating the direction ambiguity of the optimization process caused by fine-grained alignment. Comprehensive experiments demonstrate that BinaryDM achieves significant accuracy and efficiency gains compared to SOTA quantization methods of DMs under ultra-low bit-widths. With 1-bit weight and 4-bit activation (W1A4), BinaryDM achieves as low as 7.74 FID and saves the performance from collapse (baseline FID 10.87). As the first binarization method for diffusion models, W1A4 BinaryDM achieves impressive 15.2x OPs and 29.2x model size savings, showcasing its substantial potential for edge deployment."
    },
    {
        "title": "MirrorCheck: Efficient Adversarial Defense for Vision-Language Models",
        "link_suffix": "/forum?id=p4jCBTDvdu",
        "link": "https://openreview.net/forum?id=p4jCBTDvdu",
        "pdf_link": "https://openreview.net/pdf?id=p4jCBTDvdu",
        "keywords": "Adversarial Attacks, Adversarial Defenses, Vision-Language Models, StableDiffusion",
        "abstract": "Vision-Language Models (VLMs) are becoming increasingly vulnerable to adversarial attacks as various novel attack strategies are being proposed against these models. While existing defenses excel in unimodal contexts, they currently fall short in safeguarding VLMs against adversarial threats. To mitigate this vulnerability, we propose a novel, yet elegantly simple approach for detecting adversarial samples in VLMs. Our method leverages Text-to-Image (T2I) models to generate images based on captions produced by target VLMs. Subsequently, we calculate the similarities of the embeddings of both input and generated images in the feature space to identify adversarial samples. Empirical evaluations conducted on different datasets validate the efficacy of our approach, outperforming baseline methods adapted from image classification domains. Furthermore, we extend our methodology to classification tasks, showcasing its adaptability and model-agnostic nature. Theoretical analyses and empirical findings also show the resilience of our approach against adaptive attacks, positioning it as an excellent defense mechanism for real-world deployment against adversarial threats."
    },
    {
        "title": "LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs",
        "link_suffix": "/forum?id=kQ5s9Yh0WI",
        "link": "https://openreview.net/forum?id=kQ5s9Yh0WI",
        "pdf_link": "https://openreview.net/pdf?id=kQ5s9Yh0WI",
        "keywords": "long context, large language model, long-form generation",
        "abstract": "Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability."
    },
    {
        "title": "Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping",
        "link_suffix": "/forum?id=X1OfiRYCLn",
        "link": "https://openreview.net/forum?id=X1OfiRYCLn",
        "pdf_link": "https://openreview.net/pdf?id=X1OfiRYCLn",
        "keywords": "Dynamic Evaluation, Vision-Language Bootstrapping, data contamination, Flexible Complexity, Large Vision-Language Model",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across multimodal tasks such as visual perception and reasoning, leading to good performance on various multimodal evaluation benchmarks. However, these benchmarks keep a static nature and overlap with the pre-training data, resulting in fixed complexity constraints and data contamination issues. This raises the concern regarding the validity of the evaluation. To address these two challenges, we introduce a dynamic multimodal evaluation protocol called Vision-Language Bootstrapping (VLB). VLB provides a robust and comprehensive assessment for LVLMs with reduced data contamination and flexible complexity. To this end, VLB dynamically generates new visual question-answering samples through a multimodal bootstrapping module that modifies both images and language, while ensuring that newly generated samples remain consistent with the original ones by a judge module. By composing various bootstrapping strategies, VLB offers dynamic variants of existing benchmarks with diverse complexities, enabling the evaluation to co-evolve with the ever-evolving capabilities of LVLMs. Extensive experimental results across multiple benchmarks, including SEEDBench, MMBench, and MME, show that VLB significantly reduces data contamination and exposes performance limitations of LVLMs."
    },
    {
        "title": "OccVAR: Scalable 4D Occupancy Prediction via Next-Scale Prediction",
        "link_suffix": "/forum?id=X2HnTFsFm8",
        "link": "https://openreview.net/forum?id=X2HnTFsFm8",
        "pdf_link": "https://openreview.net/pdf?id=X2HnTFsFm8",
        "keywords": "Autonomous driving, World model, 3D generation",
        "abstract": "In this paper, we propose OCCVAR, a generative occupancy world model that simulates the movement of the ego vehicle and the evolution of the surrounding environment.\nDifferent from visual generation, the occupancy world model should capture the fine-grained 3D geometry and dynamic evolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential to predict vehicle movement and future occupancy scenes simultaneously from historical observations, but they typically suffer from the inefficiency and temporal degradation in long-time generation. To holistically address the efficiency and quality issues, we propose a spatial-temporal transformer via temporal next-scale prediction, aiming at predicting the 4D occupancy scenes from coarse to fine scales. To model the dynamic evolution of the scene, we incorporate the ego movement before the tokenized occupancy sequence, enabling the prediction of ego movement and controllable scene generation.\nTo model the fine-grained 3D geometry, OCCVAR utilizes a muitli-scale scene tokenizer to capture the hierarchical information of the 3D scene. \nExperiments show that OCCVAR is capable of high-quality occupancy reconstruction, long-time generation and fast inference speed compared to prior works."
    },
    {
        "title": "ROOT DEFENCE STRATEGIES: ENSURING SAFETY OF LLM AT THE DECODER LEVEL",
        "link_suffix": "/forum?id=PL6e9HkVxk",
        "link": "https://openreview.net/forum?id=PL6e9HkVxk",
        "pdf_link": "https://openreview.net/pdf?id=PL6e9HkVxk",
        "keywords": "Large Language Models; LLMs safety; LLMs defense; Speculative decoding",
        "abstract": "Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious instruction prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful responses from the prefill-level lacks utilization of the model's decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful responses based on a single evaluation can significantly impair the model's helpfulness. This paper examines the LLMs' capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost secure decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model's ability to discern hazardous information, maintaining its helpfulness compared to existing methods."
    },
    {
        "title": "DrivingWorld: Constructing World Model for Autonomous Driving via Video GPT",
        "link_suffix": "/forum?id=xJtWqVBZya",
        "link": "https://openreview.net/forum?id=xJtWqVBZya",
        "pdf_link": "https://openreview.net/pdf?id=xJtWqVBZya",
        "keywords": "world model, video generation",
        "abstract": "Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. By leveraging the next-token prediction strategy, GPT-style models can forecast future events from past data. Some research aims to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting the ego state. However, the prior works tend to produce unsatisfactory results, since the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent capability to model the spatial and temporal dynamics necessary for video generation. In this paper, we present DrivingWorld, a video-based world model for autonomous driving via a new GPT structure with spatial-temporal design. The key idea is to disentangle temporal and spatial information in the generation. Specifically, we first propose next-frame-prediction strategy to model temporal coherence between consecutive frames and then apply next-token-prediction strategy to capture spatial information within a frame. With the hybrid design, our work is capable of producing high-fidelity and consistent video clips with long-time duration. Experiments show that compared to the prior works, our method presents better quality of visual effects and more accurate controllable future video generation."
    },
    {
        "title": "Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration",
        "link_suffix": "/forum?id=OANUpvmnuf",
        "link": "https://openreview.net/forum?id=OANUpvmnuf",
        "pdf_link": "https://openreview.net/pdf?id=OANUpvmnuf",
        "keywords": "Reinforcement Learning; Large Language Model; Efficient Exploration",
        "abstract": "With expansive state-action spaces, efficient multi-agent exploration remains a longstanding challenge in reinforcement learning.\nAlthough pursuing novelty, diversity, or uncertainty attracts increasing attention, redundant efforts brought by exploration without proper guidance choices poses a practical issue for the community.\nThis paper introduces a systematic approach, termed LEMAE, choosing to channel informative task-relevant guidance from a knowledgeable Large Language Model (LLM) for Efficient Multi-Agent Exploration. \nSpecifically, we ground linguistic knowledge from LLM into symbolic key states, that are critical for task fulfillment, in a discriminative manner at low LLM inference costs. \nTo unleash the power of key states, \nwe design Subspace-based Hindsight Intrinsic Reward (SHIR) to guide agents toward key states by increasing reward density.  Additionally, we build the Key State Memory Tree (KSMT) to track transitions between key states in a specific task for organized exploration. Benefiting from diminishing redundant explorations, LEMAE outperforms existing SOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large margin, achieving a 10x acceleration in certain scenarios.\nOur code is available athttps://anonymous.4open.science/r/LEMAE."
    },
    {
        "title": "UI-Pro: A Hidden Recipe for Building Vision-Language Models for GUI Grounding",
        "link_suffix": "/forum?id=5wmAfwDBoi",
        "link": "https://openreview.net/forum?id=5wmAfwDBoi",
        "pdf_link": "https://openreview.net/pdf?id=5wmAfwDBoi",
        "keywords": "Vision-language models; GUI understanding; Visual Grounding",
        "abstract": "Building autonomous UI agents that automate user interactions with interfaces has long been a vision in the field of artificial intelligence. Central to these agents is the capability for UI element grounding, which involves accurately locating UI elements (e.g., buttons and links) based on referring expression, such as user intents and functionality descriptions. Developing these agents with robust grounding capabilities using vision-language models (VLMs) offers a promising path forward. However, a practical framework for creating VLMs with strong element grounding capabilities remains under-explored. To address this gap, we conduct systematic experiments within the design space of VLMs to uncover an effective recipe for building VLMs with strong UI element grounding ability. Firstly, we find that fine-tuning with general visual grounding tasks as a warming-up step mitigates the challenges of fine-tuning with downstream UI element grounding data. Next, we explore different fine-tuning sequences of UI grounding training data from various sources and find that a simple-to-complex fine-tuning curriculum can maximize data utility. Moreover, we find that scaling up the size of either the warming-up data or the UI grounding data in downstream fine-tuning significantly enhances UI element grounding accuracy. Lastly, we explore various image feature compression techniques and find that using a convolution-based compressor to compress UI sub-image features significantly enhances the grounding capabilities on high-resolution UI images. Integrating these insights, we successfully develop UI-Pro, an expert VLM that achieves state-of-the-art UI grounding accuracy with fewer parameters across multiple benchmarks. We hope this work serves as a valuable roadmap for researchers in the UI-VLM domain and inspires future research."
    },
    {
        "title": "Saturn: Sample-efficient Generative Molecular Design using Memory Manipulation",
        "link_suffix": "/forum?id=Hbpzrh7JbN",
        "link": "https://openreview.net/forum?id=Hbpzrh7JbN",
        "pdf_link": "https://openreview.net/pdf?id=Hbpzrh7JbN",
        "keywords": "generative design, drug discovery, sample efficiency, language models, reinforcement learning, scaling",
        "abstract": "Generative molecular design for drug discovery has very recently achieved a wave\nof experimental validation, with language-based backbones being the most common\narchitectures employed. The most important factor for downstream success is\nwhether anin silicooracle is well correlated with the desired end-point. To this end,\ncurrent methods use cheaper proxy oracles with higher throughput before evaluating\nthe most promising subset with high-fidelity oracles. The ability to directly optimize\nhigh-fidelity oracles would greatly enhance generative design and be expected to\nimprove hit rates. However, current models are not efficient enough to consider such\na prospect, exemplifying the sample efficiency problem. In this work, we introduceSaturn, which leverages the Augmented Memory algorithm and demonstrates the\nfirst application of the Mamba architecture for generative molecular design. We\nelucidatehowexperience replay with data augmentation improves sample efficiency\nandhowMamba synergistically exploits this mechanism. Saturn outperforms 22\nmodels on multi-parameter optimization tasks relevant to drug discovery and may\npossess sufficient sample efficiency to consider the prospect of directly optimizing\nhigh-fidelity oracles."
    },
    {
        "title": "GROOT-2: Weakly Supervised Multimodal Instruction Following Agents",
        "link_suffix": "/forum?id=S9GyQUXzee",
        "link": "https://openreview.net/forum?id=S9GyQUXzee",
        "pdf_link": "https://openreview.net/pdf?id=S9GyQUXzee",
        "keywords": "Reinforcement Learning, Open-world Agent, Weakly Supervised Learning, Goal-Conditioned Policy",
        "abstract": "Developing agents that can follow multimodal instructions remains a fundamental challenge in robotics and AI. Although large-scale pre-training on unlabeled datasets has enabled agents to learn diverse behaviors, these agents often struggle with following instructions. While augmenting the dataset with instruction labels can mitigate this issue, acquiring such high-quality annotations at scale is impractical. \nTo address this issue, we frame the problem as a semi-supervised learning task and introduce \\agent, a multimodal instructable agent trained using a novel approach that combines weak supervision with latent variable models. Our method consists of two key components: constrained self-imitating, which utilizes large amounts of unlabeled demonstrations to enable the policy to learn diverse behaviors, and human intention alignment, which uses a smaller set of labeled demonstrations to ensure the latent space reflects human intentions. \\agent\u2019s effectiveness is validated across four diverse environments, ranging from video games to robotic manipulation, demonstrating its robust multimodal instruction-following capabilities."
    },
    {
        "title": "From Pixels to Tokens: Revisiting Object Hallucinations in Large Vision-Language Models",
        "link_suffix": "/forum?id=ZPTHI3X9y8",
        "link": "https://openreview.net/forum?id=ZPTHI3X9y8",
        "pdf_link": "https://openreview.net/pdf?id=ZPTHI3X9y8",
        "keywords": "large vision language model, hallucination, virtual token",
        "abstract": "Hallucinations in Large Vision-Language Models (LVLMs) are a significant challenge, i.e., generating objects that are not presented in the visual input, which impairs their reliability. Recent studies often attribute hallucinations to a lack of understanding of visual input, yet ignore a more fundamental issue: the model's inability to effectively extract or decouple visual features. In this paper, we revisit the hallucinations in LVLMs from an architectural perspective, investigating whether the primary cause lies in the visual encoder (feature extraction) or the modal alignment module (feature decoupling). Motivated by our findings on the preliminary investigation, we propose a novel tuning strategy, PATCH, to mitigate hallucinations in LVLMs. This plug-and-play method can be integrated into various LVLMs, utilizing adaptive virtual tokens to extract object features from bounding boxes, thereby addressing hallucinations caused by insufficient decoupling of visual features. PATCH achieves state-of-the-art performance on multiple multi-modal hallucination datasets. We hope this approach provides researchers with deeper insights into the underlying causes of hallucinations in LVLMs, fostering further advancements and innovation in this field."
    },
    {
        "title": "Knowledge Lift Alignment Fine Tuning",
        "link_suffix": "/forum?id=uxYbEAEWm4",
        "link": "https://openreview.net/forum?id=uxYbEAEWm4",
        "pdf_link": "https://openreview.net/pdf?id=uxYbEAEWm4",
        "keywords": "PEFT, PLM, LLM, VLM, Multi-modal, Image captioning",
        "abstract": "We present a visual tuning framework, \\textbf{K}nowledge \\textbf{L}ift \\textbf{A}lignment \\textbf{F}ine \\textbf{T}uning (KLAFT), \nwhich enhances the expressive image captioning capabilities of Pre-trained Language Models (PLMs), including LLMs and VLMs.\nAs this task involves generating more detailed and comprehensive captions than basic image descriptions,\nthe core idea behind KLAFT is that fine-grained alignment could exploit the capabilities of PLMs and a given target domain dataset.\nThis idea motivates and challenges us to explore the framework that deeply understands both given images and text for this alignment and tuning PLMs towards expressive image captioning.\nThis direction modifies the attention mechanism (Modified Attention Mechanism, MAM) and develops both a Topic Control Mechanism (TCM) and their training objectives.\nThe innovation of KLAFT lies in its approach to addressing the disparities in knowledge - visual versus textual via MAM\nand source versus target domain via TCM.\nAs these hidden spaces are conceptualized as distinct sub-networks within the PLM, each possessing specific knowledge,\nKLAFT's unique contribution is in aligning and adjusting the weights of these sub-networks in a fine-grained manner,\nand fine-tuning this PLM.\nOur empirical studies demonstrate that KLAFT significantly improves expressive captioning tasks by aligning and amplifying target knowledge, with the potential for Parameter-Efficient Fine-Tuning (PEFT) at low computational cost."
    },
    {
        "title": "SurfDesign: Effective Protein Design on Molecular Surfaces",
        "link_suffix": "/forum?id=JCFJFBm5rE",
        "link": "https://openreview.net/forum?id=JCFJFBm5rE",
        "pdf_link": "https://openreview.net/pdf?id=JCFJFBm5rE",
        "keywords": "Molecular Surfaces, Protein Design, Geometric Deep Learning",
        "abstract": "Structure-based inverse folding has been extensively explored in recent years. In contrast, surface-conditioned protein generation is still an under-explored area. Molecular surfaces characterized by a compact and smooth composition of atoms at their boundary hold a more direct relevance to biomolecular interactions and function.\nIn this work, we introduce a novel framework named SurfDesign with several key improvements. Firstly, considering the theoretical fact that the molecular surface is a continuous manifold with infinite resolution, we propose surface-based equivariant message passing (SEMP) to incorporate the normal vector and curvatures and get aware of the manifold's Euclidean locality. Besides, a hybrid parameter-efficient fine-tuning (PEFT) technique is employed to combine the knowledge of protein language models (PLMs) with the surface geometric encoder. We extensively evaluate SurfDesign on the CATH, TS50, TS500, and PDB datasets, achieving an average recovery of more than 70%.  Our work opens another road to designing functional proteins, underscoring the importance of including surface attributes in conventional inverse folding."
    },
    {
        "title": "SAN-Diff: Structure-aware noise for super-resolution diffusion model",
        "link_suffix": "/forum?id=5cYTAcZAgt",
        "link": "https://openreview.net/forum?id=5cYTAcZAgt",
        "pdf_link": "https://openreview.net/pdf?id=5cYTAcZAgt",
        "keywords": "Diffusion Model, Image Super-Resolution",
        "abstract": "Recent advances in diffusion models, like Stable Diffusion, have been shown to significantly improve performance in image super-resolution (SR) tasks. However, existing diffusion techniques often sample noise from just one distribution, which limits their effectiveness when dealing with complex scenes or intricate textures in different semantic areas. With the advent of the segment anything model (SAM), it has become possible to create highly detailed region masks that can improve the recovery of fine details in diffusion SR models. Despite this, incorporating SAM directly into SR models significantly increases computational demands. In this paper, we propose the SAN-Diff model, which can utilize the fine-grained structure information from SAM in the process of sampling noise to improve the image quality without additional computational cost during inference. In the process of training, we encode structural position information into the segmentation mask from SAM. Then the encoded mask is integrated into the forward diffusion process by modulating it to the sampled noise. This adjustment allows us to independently adapt the noise mean within each corresponding segmentation area. The diffusion model is trained to estimate this modulated noise. Crucially, our proposed framework does NOT change the reverse diffusion process and does NOT require SAM at inference. Experimental results demonstrate the effectiveness of our proposed method, which exhibits the fewest artifacts compared to other generated models, and surpassing existing diffusion-based methods by 0.74 dB at\nthe maximum in terms of PSNR on DIV2K dataset."
    },
    {
        "title": "CTV-FAS: Compensate Texts with Visuals for Generalizable Face Anti-spoofing",
        "link_suffix": "/forum?id=UEE13WQlNU",
        "link": "https://openreview.net/forum?id=UEE13WQlNU",
        "pdf_link": "https://openreview.net/pdf?id=UEE13WQlNU",
        "keywords": "Face anti-spoofing Vision-language model Domain generalization",
        "abstract": "Generalizable Face Anti-Spoofing (FAS) approaches have recently gained significant attention for their robustness in unseen scenarios. \nRecent methods incorporate vision-language models into FAS, capitalizing on their remarkable pre-trained performance to enhance generalization. \nThese methods predominantly rely on text prompts to learn the concept of attacks in FAS.\nHowever, certain attacks, such as high-resolution replay attacks, cannot be described linguistically. \nRelying solely on text prompts cannot accurately tackle such attacks, resulting in performance degradation.\nTo tackle these limitations, we introduce a novel framework named CTV-FAS, designed to exploit visual anchors to compensate for the shortcomings of semantic prompts.\nSpecifically, we employ a Self-Supervised Consistency Module (SSCM) to boost the generalization of visual anchors, which utilizes consistency regularization to facilitate visual feature learning.\nSubsequently, a Visual Anchors Updating Module (VAUM) is proposed to incorporate the visual anchors through an adaptive updating scheme, guiding the feature learning process from a visual standpoint.\nFurthermore, we propose an Adaptive Modality Integration Module (AMIM), designed to merge visual and textual information during inference seamlessly. This integration optimizes the synergy between modalities, significantly boosting the efficacy of Face Anti-Spoofing (FAS) tasks.\nOur extensive experimental evaluations and in-depth analysis affirm that our method outperforms current state-of-the-art counterparts with a notable margin of superiority."
    },
    {
        "title": "Medium-Difficulty Samples Constitute Smoothed Decision Boundary for Knowledge Distillation on Pruned Datasets",
        "link_suffix": "/forum?id=Rz4UkJziFe",
        "link": "https://openreview.net/forum?id=Rz4UkJziFe",
        "pdf_link": "https://openreview.net/pdf?id=Rz4UkJziFe",
        "keywords": "Knowledge distillation, dataset pruning, image recognition",
        "abstract": "This paper tackles a new problem of dataset pruning for Knowledge Distillation (KD), from a fresh perspective of Decision Boundary (DB) preservation and drifts. Existing dataset pruning methods generally assume that the post-pruning DB formed by the selected samples can be well-captured by future networks that use those samples for training. Therefore, they tend to preserve hard samples since hard samples are closer to the DB and better characterize the nuances in the distribution of the entire dataset. However, in KD, the limited learning capacity from the student network leads to imperfect preservation of the teacher's feature distribution, resulting in the drift of DB in the student space. Specifically, hard samples worsen such drifts as they are difficult for the student to learn, creating a situation where the student's DB can drift deeper into other classes and make incorrect classifications. Motivated by these findings, our method selects medium-difficulty samples for KD-based dataset pruning. We show that these samples constitute a smoothed version of the teacher's DB and are easier for the student to learn, obtaining a general feature distribution preservation for a class of samples and reasonable DB between different classes for the student. In addition, to reduce the distributional shift due to dataset pruning, we leverage the class-wise distributional information of the teacher's outputs to reshape the logits of the preserved samples. Experiments show that the proposed static pruning method can even perform better than the state-of-the-art dynamic pruning method which needs access to the entire dataset. In addition, our method halves the training times of KD and improves the student's accuracy by 0.4% on ImageNet with a 50% keep ratio. When the ratio further increases to 70%, our method achieves higher accuracy over the vanilla KD while reducing the training times by 30%."
    },
    {
        "title": "SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models",
        "link_suffix": "/forum?id=tjlTczcnPz",
        "link": "https://openreview.net/forum?id=tjlTczcnPz",
        "pdf_link": "https://openreview.net/pdf?id=tjlTczcnPz",
        "keywords": "Large Language Model, Low-bit Quantization, Inference, Machine Learning",
        "abstract": "Large language models (LLMs) have achieved remarkable progress, but their extensive number of parameters results in high memory usage, significant loading latency, and substantial computational demands. To address these challenges, post-training quantization (PTQ) has emerged as an effective technique for compressing model weights. In the context of PTQ for LLMs, existing uniform quantization methods, though efficient in terms of memory and computational requirements, often struggle to maintain performance. In this paper, we propose SliM-LLM, a Salience-Driven Mixed-Precision Quantization scheme that achieves group-wise bit-width allocation with mixed precisions for efficient LLMs with high accuracy. Building on our observation that salient/important weights often follow a structured distribution, we incorporate two core components to preserve post-quantization performance in LLMs while maintaining efficiency: 1) Salience-Determined Bit Allocation adaptively assigns bit widths to groups within each layer based on their group-level salience, aiming to minimize the reconstruction error of activations; and 2) Salience-Weighted Quantizer Calibration optimizes quantizer parameters by incorporating element-level salience, ensuring that the most critical weights are preserved, further preserving important weights information. With its structured group partitioning, SliM-LLM offers a hardware-friendly quantization approach, maintaining computational and memory efficiency comparable to highly optimized uniform quantization methods. Extensive experiments demonstrate that SliM-LLM significantly improves the accuracy of various LLMs when quantized to ultra-low bit widths. For instance, a 2-bit quantized LLaMA-7B model achieves nearly 6x memory reduction compared to its floating-point counterpart, alongside a 48% reduction in perplexity compared to the leading gradient-free PTQ method, all while maintaining GPU inference speed. Furthermore, SliM-LLM+, which incorporates gradient-based quantizers, reduces perplexity by an additional 35.1%."
    },
    {
        "title": "Identify Dominators: The Key To Improve Large-Scale Maximum Inner Product Search",
        "link_suffix": "/forum?id=wtNxByjLW3",
        "link": "https://openreview.net/forum?id=wtNxByjLW3",
        "pdf_link": "https://openreview.net/pdf?id=wtNxByjLW3",
        "keywords": "high-dimensional vector, information retrieval, vector based retrieval, graph methods, nearest neighbor, maximum inner product search, similarity search",
        "abstract": "Maximum Inner Product Search (MIPS) is essential for machine learning and information retrieval, particularly in applications that operate on high-dimensional data, such as recommender systems and retrieval-augmented generation (RAG), using inner product or cosine similarity. While numerous techniques have been developed for efficient MIPS, their performance often suffers due to a limited understanding of the geometric properties of Inner Product (IP) space.  Many approaches reduce MIPS to Nearest Neighbor Search (NNS) through nonlinear transformations, which rely on strong assumptions and can hinder performance. To address these limitations, we propose a novel approach that directly leverages the geometry of IP space. We focus on a class of special vectors called dominators and introduce the Monotonic Relative Dominator Graph MRDG, an IP-space-native, sparse, and strongly-connected graph designed for efficient MIPS, offering theoretical solid foundations. To ensure scalability, we further introduce the Approximate Relative Dominator Graph (ARDG), which retains MRDG\u2019s benefits while significantly reducing indexing complexity. Extensive experiments on 8 public datasets demonstrate that ARDG achieves a 30% average speedup in search at high precision and reduces index size by 2x compared to state-of-the-art graph-based methods."
    },
    {
        "title": "MC-MoE: Mixture Compressor for Mixture-of-Experts LLMs Gains More",
        "link_suffix": "/forum?id=hheFYjOsWO",
        "link": "https://openreview.net/forum?id=hheFYjOsWO",
        "pdf_link": "https://openreview.net/pdf?id=hheFYjOsWO",
        "keywords": "Mixture-of-Expert, LLM, Quantization, Pruning",
        "abstract": "Mixture-of-Experts large language models (MoE-LLMs) marks a significant step forward of language models, however, they encounter two critical challenges in practice: 1) expert parameters lead to considerable memory consumption and loading latency; and 2) the current activated experts are redundant, as many tokens may only require a single expert. Motivated by these issues, we investigate the MoE-LLMs and make two key observations: a) different experts exhibit varying behaviors on activation reconstruction error, routing scores, and activated frequencies, highlighting their differing importance, and b) not all tokens are equally important-- only a small subset is critical. Building on these insights, we propose MC-MoE, a training-free Mixture-Compressor for MoE-LLMs, which leverages the significance of both experts and tokens to achieve an extreme compression. First, to mitigate storage and loading overheads, we introduce Pre-Loading Mixed-Precision Quantization (PMQ), which formulates the adaptive bit-width allocation as a Linear Programming (LP) problem, where the objective function balances multi-factors reflecting the importance of each expert. Additionally, we develop Online Dynamic Pruning (ODP),  which identifies important tokens to retain and dynamically select activated experts for other tokens during inference to optimize efficiency while maintaining performance. Our MC-MoE integrates static quantization and dynamic pruning to collaboratively achieve extreme compression for MoE-LLMs with less accuracy loss, ensuring an optimal trade-off between performance and efficiency Extensive experiments confirm the effectiveness of our approach. For instance, at 2.54 bits, MC-MoE compresses 76.6% of the model, with only a 3.8% average accuracy loss. During dynamic inference, we further reduce activated parameters by 15%, with a performance drop of less than 0.6%. Remarkably, MC-MoE even surpasses floating-point 13b dense LLMs with significantly smaller parameter sizes, suggesting that mixture compression in MoE-LLMs has the potential to outperform both comparable and larger dense LLMs."
    },
    {
        "title": "Learning K-U-Net in Constant Complexity with Application to Time Series Forecasting",
        "link_suffix": "/forum?id=pNdPJACSLB",
        "link": "https://openreview.net/forum?id=pNdPJACSLB",
        "pdf_link": "https://openreview.net/pdf?id=pNdPJACSLB",
        "keywords": "machine learning, time series, complexity reduction",
        "abstract": "Training deep models for time series forecasting is a critical task with an inherent challenge of time complexity. While current methods generally ensure linear time complexity, our observations on temporal redundancy show that high-level features are learned 99.5% slower than low-level features. To address this issue, we introduce a new exponentially weighted stochastic gradient descent algorithm designed to achieve constant time complexity in deep learning models. We prove that the theoretical complexity of this learning method is constant. Evaluation of this method on Kernel U-Net (K-U-Net) on synthetic datasets shows a significant reduction in complexity while improving the accuracy of the test set."
    }
]