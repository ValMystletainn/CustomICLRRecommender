[
    {
        "title": "Denoising with a Joint-Embedding Predictive Architecture",
        "link_suffix": "/forum?id=d4njmzM7jf",
        "link": "https://openreview.net/forum?id=d4njmzM7jf",
        "pdf_link": "https://openreview.net/pdf?id=d4njmzM7jf",
        "keywords": "AIGC, JEPA, Diffusion Model, Flow Matching, Image Synthetic",
        "abstract": "Joint-embedding predictive architectures (JEPAs) have shown substantial promise in self-supervised representation learning, yet their application in generative modeling remains underexplored. Conversely, diffusion models have demonstrated significant efficacy in modeling arbitrary probability distributions. In this paper, we introduce Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), pioneering the integration of JEPA within generative modeling. By recognizing JEPA as a form of masked image modeling, we reinterpret it as a generalized next-token prediction strategy, facilitating data generation in an auto-regressive manner. Furthermore, we incorporate diffusion loss to model the per-token probability distribution, enabling data generation in a continuous space. We also adapt flow matching loss as an alternative to diffusion loss, thereby enhancing the flexibility of D-JEPA. Empirically, with increased GFLOPs, D-JEPA consistently achieves lower FID scores with fewer training epochs, indicating its good scalability. Our base, large, and huge models outperform all previous generative models across all scales on class-conditional ImageNet benchmarks. Beyond image generation, D-JEPA is well-suited for other continuous data modeling, including video and audio."
    },
    {
        "title": "Hyper-Connections",
        "link_suffix": "/forum?id=9FqARW7dwB",
        "link": "https://openreview.net/forum?id=9FqARW7dwB",
        "pdf_link": "https://openreview.net/pdf?id=9FqARW7dwB",
        "keywords": "Network Architecture, Residual Connections, LLMs, Pre-training",
        "abstract": "We present hyper-connections, a simple yet effective method that can serve as an alternative to residual connections. This approach specifically addresses common drawbacks observed in residual connection variants, such as the seesaw effect between gradient vanishing and representation collapse. Theoretically, hyper-connections allow the network to adjust the strength of connections between features at different depths and dynamically rearrange layers. We conduct experiments focusing on the pre-training of large language models, including dense and sparse models, where hyper-connections show significant performance improvements over residual connections. Additional experiments conducted on vision tasks also demonstrate similar improvements. We anticipate that this method will be broadly applicable and beneficial across a wide range of AI problems."
    },
    {
        "title": "Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation",
        "link_suffix": "/forum?id=tTPHgb0EtV",
        "link": "https://openreview.net/forum?id=tTPHgb0EtV",
        "pdf_link": "https://openreview.net/pdf?id=tTPHgb0EtV",
        "keywords": "Harmful fine-tuning, LLM, safety alignment",
        "abstract": "Harmful fine-tuning attack \\citep{qi2023fine} poses serious safety concerns for Large language models' fine-tuning-as-a-service. While existing defenses have been proposed to mitigate the issue, their performances are still far away from satisfactory, and the root cause of the problem has not been fully recovered. To this end, we in this paper show that \\textit{harmful perturbation} over the model weights could be a probable cause of alignment-broken. In order to attenuate the negative impact of harmful perturbation, we propose an alignment-stage solution, dubbed Booster. Technically, along with the original alignment loss,  we append a loss regularizer in the alignment stage's optimization. The regularizer ensures that the model's harmful loss reduction after the simulated harmful perturbation is attenuated, thereby mitigating the subsequent fine-tuning risk.     Empirical results show that Booster can effectively reduce the harmful score of the fine-tuned models while maintaining the performance of downstream tasks. Our code is available at \\url{https://anonymous.4open.science/r/Booster-EF18}."
    },
    {
        "title": "Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark",
        "link_suffix": "/forum?id=sY3anJ8C68",
        "link": "https://openreview.net/forum?id=sY3anJ8C68",
        "pdf_link": "https://openreview.net/pdf?id=sY3anJ8C68",
        "keywords": "Video object counting, masked autoencoder, multimodal self-representation learning",
        "abstract": "The dynamic imbalance of the fore-background is a major challenge in video object counting, which is usually caused by the sparsity of foreground objects. This often leads to severe under- and over-prediction problems and has been less studied in existing works.\nTo tackle this issue in video object counting, we propose a density-embedded Efficient Masked Autoencoder Counting (E-MAC) framework in this paper. To effectively capture the dynamic variations across frames, we utilize an optical flow-based temporal collaborative fusion that aligns features to derive multi-frame density residuals. The counting accuracy of the current frame is boosted by harnessing the information from adjacent frames. More importantly, to empower the representation ability of dynamic foreground objects for intra-frame, we first take the density map as an auxiliary modality to perform Density-Embedded Masked mOdeling (DEMO) for multimodal self-representation learning to regress density map. However, as DEMO contributes effective cross-modal regression guidance, it also brings in redundant background information and hard to focus on foreground regions. To handle this dilemma, we further propose an efficient spatial adaptive masking derived from density maps to boost efficiency. In addition, considering most existing datasets are limited to human-centric scenarios, we first propose a large video bird counting dataset $\\textit{DroneBird}$, in natural scenarios for migratory bird protection. Extensive experiments on three crowd datasets and our $\\textit{DroneBird}$ validate our superiority against the counterparts."
    },
    {
        "title": "TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting",
        "link_suffix": "/forum?id=rDe9yQQYKt",
        "link": "https://openreview.net/forum?id=rDe9yQQYKt",
        "pdf_link": "https://openreview.net/pdf?id=rDe9yQQYKt",
        "keywords": "spiking neural network, time series forecasting, Application",
        "abstract": "Spiking Neural Networks (SNNs) offer a promising, biologically inspired approach for processing spatiotemporal data, particularly for time series forecasting.\nHowever, conventional neuron models like the Leaky Integrate-and-Fire (LIF) struggle to capture long-term dependencies and effectively process multi-scale temporal dynamics.\nTo overcome these limitations, we introduce the Temporal Segment Leaky Integrate-and-Fire (TS-LIF) model, featuring a novel dual-compartment architecture.\nThe dendritic and somatic compartments specialize in capturing distinct frequency components, providing functional heterogeneity that enhances the neuron's ability to process both low- and high-frequency information.\nFurthermore, the newly introduced direct somatic current injection reduces information loss during intra-neuronal transmission, while dendritic spike generation improves multi-scale information extraction.\nWe provide a theoretical stability analysis of the TS-LIF model and explain how each compartment contributes to distinct frequency response characteristics.\nExperimental results show that TS-LIF outperforms traditional SNNs in time series forecasting, demonstrating better accuracy and robustness, even with missing data.\nTS-LIF advances the application of SNNs in time-series forecasting, providing a biologically inspired approach that captures complex temporal dynamics and offers potential for practical implementation in diverse forecasting scenarios."
    },
    {
        "title": "ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference-Time",
        "link_suffix": "/forum?id=QoDDNkx4fP",
        "link": "https://openreview.net/forum?id=QoDDNkx4fP",
        "pdf_link": "https://openreview.net/pdf?id=QoDDNkx4fP",
        "keywords": "VLMs, safety alignment, inference-time",
        "abstract": "Vision Language Models (VLMs) have become essential backbones for multi-modal intelligence, yet significant safety challenges limit their real-world application. While textual inputs can often be effectively safeguarded, adversarial visual inputs can often easily bypass VLM defense mechanisms. Existing defense methods are either resource-intensive, requiring substantial data and compute, or fail to simultaneously ensure safety and usefulness in responses. To address these limitations, we propose a novel two-phase inference-time alignment framework,EvaluatingThenAligning (ETA): i) Evaluating input visual contents and output responses to establish a robust safety awareness in multimodal settings, and ii) Aligning unsafe behaviors at both shallow and deep levels by conditioning the VLMs' generative distribution with an interference prefix and performing sentence-level best-of-$N$ to search the most harmless and helpful generation paths. Extensive experiments show that ETA outperforms baseline methods in terms of harmlessness, helpfulness, and efficiency, reducing the unsafe rate by 87.5% in cross-modality attacks and achieving 96.6% win-ties in GPT-4 helpfulness evaluation."
    },
    {
        "title": "Retrieval Augmented Zero-Shot Enzyme Generation for Specified Substrate",
        "link_suffix": "/forum?id=T7lQGq73Lm",
        "link": "https://openreview.net/forum?id=T7lQGq73Lm",
        "pdf_link": "https://openreview.net/pdf?id=T7lQGq73Lm",
        "keywords": "Protein Design, Diffusion model",
        "abstract": "The ability to generate novel enzymes that catalyze specific target molecules is a critical advancement in biomaterial synthesis and chemical production. However, a significant challenge arises when no recorded enzymes exist for the target molecule, making it a zero-shot generation problem. This absence of known enzymes complicates the training of generative models tailored to the target substrate. To address this, we propose a retrieval-augmented generation method that leverages existing enzyme-substrate data to overcome the lack of direct examples. Since there is no recorded catalytic performance between the enzymes and the new target molecule, the challenge shifts to identifying enzymes that helpful for generation. Our approach tackles this by retrieving enzymes whose substrates exhibit structural similarities to the target molecule, thereby exploiting functional similarities reflected in the enzymes' catalytic capability. This leads to the next challenge: how to utilize the retrieved enzymes to generate a novel enzyme capable of catalyzing the target molecule, given that none of the retrieved enzymes directly catalyze it. To solve this, we employ a conditioned discrete diffusion model that takes the aligned retrieved enzymes to generate a new enzyme. We train the generator with guidance from an enzyme-substrate relationship classifier to make it output the optimal protein sequence distribution for different target molecule. We evaluate our model on enzyme design tasks involving a diverse set of real-world substrates, and our results including catalytic rate predictions, foldability assessments, and docking position analyses, demonstrate that our model outperforms existing protein generation methods for substrate-specified enzyme generation. Additionally, we formally define the zero-shot substrate-specified enzyme generation task and contribute a comprehensive dataset with evaluation methods."
    },
    {
        "title": "KEEP: Towards a Knowledge-Enhanced Explainable Prompting Framework for Vision-Language Models",
        "link_suffix": "/forum?id=cTG25RXtJA",
        "link": "https://openreview.net/forum?id=cTG25RXtJA",
        "pdf_link": "https://openreview.net/pdf?id=cTG25RXtJA",
        "keywords": "Prompt, Domain Knowledge, VLM, XAI",
        "abstract": "Large-scale vision-language models (VLMs) embedded with expansive representations and visual concepts have showcased significant potential in the computer vision community. Efficiently adapting VLMs such as CLIP, to downstream tasks has garnered growing attention, with prompt learning emerging as a representative approach. However, most existing prompt-based adaptation methods, which rely solely on coarse-grained textual prompts, suffer from limited performance and interpretability when handling tasks that require domain-specific knowledge. This results in a failure to satisfy the stringent trustworthiness requirements of Explainable Artificial Intelligence (XAI) in high-risk scenarios like healthcare. To address this issue, we propose a Knowledge-Enhanced Explainable Prompting (KEEP) framework that leverages fine-grained domain-specific knowledge to enhance the adaptation process across various domains, facilitating bridging the gap between the general domain and other specific domains. We present to our best knowledge the first work to incorporate retrieval augmented generation and domain-specific foundation models to provide more reliable image-wise knowledge for prompt learning in various domains, alleviating the lack of fine-grained annotations, while offering both visual and textual explanations. Extensive experiments and explainability analyses conducted on eight datasets of different domains, demonstrate that our method simultaneously achieves superior performance and interpretability, shedding light on the effectiveness of the collaboration between foundation models and XAI. The code will be made publically available."
    },
    {
        "title": "Temporal Distribution-aware Quantization for Diffusion Models",
        "link_suffix": "/forum?id=lF5U9jTdyq",
        "link": "https://openreview.net/forum?id=lF5U9jTdyq",
        "pdf_link": "https://openreview.net/pdf?id=lF5U9jTdyq",
        "keywords": "diffusion models, quantization",
        "abstract": "Diffusion models for image generation have achieved notable success in various applications. However, these models often require tremendous storage overhead and inference time cost, severely  hampering their deployment on resource-constrained devices. Post-training quantization (PTQ) has recently emerged as a promising way to reduce the model size and the inference latency, by converting the float-point values into lower bit-precision. Nevertheless, most existing PTQ approaches neglect the accumulating quantization errors arising from the substantial distribution variations across distinct layers and blocks at different timesteps, thus suffering a significant accuracy degradation. To address these issues, we propose a novel temporal distribution-aware quantization (DAQ) method for diffusion models. DAQ firstly develops a distribution-aware finetuning (DAF) framework to dynamically suppress the accumulating quantization errors in the calibration process. Subsequently, DAQ employs a full-precision noise estimation network to optimize the quantized noise estimation network at each sampling timestep, further aligning the quantizers with varying input distributions. We evaluate the proposed method on the widely used public benchmarks for image generation tasks. The experimental results clearly demonstrate that DAQ reaches the state-of-the-art performance compared to existing works. We also display that DAQ can be applied as a plug-and-play module to existing PTQ models, remarkably boosting the overall performance."
    },
    {
        "title": "Fast and Slow Generating: An Empirical Study on Large and Small Language Models Collaborative Decoding",
        "link_suffix": "/forum?id=4a9doRh3Jv",
        "link": "https://openreview.net/forum?id=4a9doRh3Jv",
        "pdf_link": "https://openreview.net/pdf?id=4a9doRh3Jv",
        "keywords": "Large Language Models, Collaborative Decoding",
        "abstract": "Large Language Models (LLMs) exhibit impressive capabilities across various applications but encounter substantial challenges such as high inference latency, considerable training costs, and the generation of hallucinations. Collaborative decoding between large and small language models (SLMs) presents a promising strategy to mitigate these issues through methods including speculative decoding, contrastive decoding, and emulator or proxy fine-tuning. However, the specifics of such collaborations, particularly from a unified perspective, remain largely unexplored. Inspired by dual-process cognitive theory, we propose a unified framework in this paper, termed Fast and Slow Generating (FS-GEN). Within this framework, LLMs (sometimes along with SLMs) are categorized as System 2 (slow and deliberate), while independent SLMs are designated as System 1 (fast and intuitive). We provide a comprehensive analysis of these collaborative methodologies, elucidating their common properties and shedding light on the differential knowledge capabilities of System 2 versus System 1 through the FS-GEN framework. Our findings indicate that only a small proportion of collaborative interactions (approximately less than 20% in most instances) are necessary across various methods. These interactions between System 1 and System 2 conform to a scaling law related to the parameter ratios, enabling predictable collaboration. Furthermore, we explore the specific conditions under which collaboration proves most effective, particularly from an uncertainty perspective, offering novel insights that may guide future optimization efforts. Our research underscores that the fundamental distinction between System 1 and System 2 lies in the uncertainty of next token predictions, where interventions by System 2 are crucial to support System 1."
    },
    {
        "title": "Ranking-aware adapter for text-driven image ordering with CLIP",
        "link_suffix": "/forum?id=KbCh7zbw2K",
        "link": "https://openreview.net/forum?id=KbCh7zbw2K",
        "pdf_link": "https://openreview.net/pdf?id=KbCh7zbw2K",
        "keywords": "Vision Language Models, CLIP, Learning-to-Rank",
        "abstract": "Recent advances in vision-language models (VLMs) have made significant progress in downstream tasks that require quantitative concepts such as facial age estimation and image quality assessment, enabling VLMs to explore applications like image ranking and retrieval. However, existing studies typically focus on the reasoning based on a single image and heavily depend on text prompting, limiting their ability to learn comprehensive understanding from multiple images. To address this, we propose an effective yet efficient approach that reframes the CLIP model into a learning-to-rank task and introduces a lightweight adapter to augment CLIP for text-guided image ranking. Specifically, our approach incorporates learnable prompts to adapt to new instructions for ranking purposes and an auxiliary branch with ranking-aware attention, leveraging text-conditioned visual differences for additional supervision in image ranking. Our ranking-aware adapter consistently outperforms fine-tuned CLIPs on various tasks and achieves competitive results compared to state-of-the-art models designed for specific tasks like facial age estimation and image quality assessment. Overall, our approach primarily focuses on ranking images with a single instruction, which provides a natural and generalized way of learning from visual differences across images, bypassing the need for extensive text prompts tailored to individual tasks."
    },
    {
        "title": "Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion",
        "link_suffix": "/forum?id=xaYlO03tIk",
        "link": "https://openreview.net/forum?id=xaYlO03tIk",
        "pdf_link": "https://openreview.net/pdf?id=xaYlO03tIk",
        "keywords": "Robotics, Imitation Learning, Visual Imitation Learning, Robustness, Diffusion Model, Diffusion Inversion",
        "abstract": "Visual imitation learning methods demonstrate strong performance, yet they lack generalization when faced with visual input perturbations like variations in lighting and textures. This limitation hampers their practical application in real-world settings. To address this, we proposeStem-OBthat leverages the inversion process of pretrained image diffusion models to suppress low-level visual differences while maintaining high-level scene structures. This image inversion process is akin to transforming the observation into a shared representation, from which other observations also stem.Stem-OBoffers a simple yet effective plug-and-play solution that stands in contrast to data augmentation approaches. It demonstrates robustness to various unspecified appearance changes without the need for additional training. We provide theoretical insights and empirical results that validate the efficacy of our approach in simulated and real settings.Stem-OBshows an exceptionally significant improvement in real-world robotic tasks, where challenging light and appearance changes are present, with an average increase of22.2%in success rates compared to the best baseline. Please refer tothis linkfor more videos and details."
    },
    {
        "title": "Test-time Correction with Human Feedback: An Online 3D Detection System via Visual Prompting",
        "link_suffix": "/forum?id=4VNfufHtoS",
        "link": "https://openreview.net/forum?id=4VNfufHtoS",
        "pdf_link": "https://openreview.net/pdf?id=4VNfufHtoS",
        "keywords": "Autonomous Driving, 3D Object Detection, Test-time Error Correction",
        "abstract": "This paper introduces Test-time Correction (TTC) system, a novel online 3D detection system designated for online correction of test-time errors via human feedback, to guarantee the safety of deployed autonomous driving systems. Unlike well studied offline 3D detectors frozen at inference, TTC explores the capability of instant online error rectification. By leveraging user feedback with interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC could immediately update the corresponding detection results for future streaming inputs, even though the model is deployed with fixed parameters. This enables autonomous driving systems to adapt to new scenarios flexibly and decrease deployment risks reliably without additional expensive training. To achieve such TTC system, we equip existing 3D detectors with OA module, an online adapter with prompt-driven design for online correction. At the core of OA module are visual prompts, images of missed object-of-interest for guiding the corresponding detection and subsequent tracking. Those visual prompts, belonging to missed objects through online inference, are maintained by the visual prompt buffer for continuous error correction in subsequent frames. By doing so, TTC consistently detects online missed objects and immediately lowers down driving risks. It achieves reliable, versatile, and adaptive driving autonomy. Extensive experiments demonstrate significant gain on instant error rectification over pre-trained 3D detectors, even in challenging scenarios with limited labels, zero-shot detection, and adverse conditions. We hope this work would inspire the community to investigate online rectification systems for autonomous driving post-deployment. Code would be publicly shared."
    },
    {
        "title": "Towards Synergistic, Generalized, and Efficient Dual-System for Robotic Manipulation",
        "link_suffix": "/forum?id=3flhuT2QGB",
        "link": "https://openreview.net/forum?id=3flhuT2QGB",
        "pdf_link": "https://openreview.net/pdf?id=3flhuT2QGB",
        "keywords": "Robotic Manipulation, Vision-Language-Action Models",
        "abstract": "The increasing demand for versatile robotic systems to operate in diverse and dynamic environments has emphasized the importance of a generalist policy, which leverages a large cross-embodiment data corpus to facilitate broad adaptability and high-level reasoning. However, the generalist would struggle with inefficient inference and cost-expensive training. The specialist policy, instead, is curated for specific domain data and excels at task-level precision with efficiency. Yet, it lacks the generalization capacity for a wide range of applications. Inspired by these observations, we introduce RoboDual, a synergistic dual-system that supplements the merits of both generalist and specialist policy. A diffusion transformer-based specialist is devised for multi-step action rollouts, exquisitely conditioned on the high-level task understanding and discretized action output of a vision-language-action (VLA) based generalist. Compared to OpenVLA, RoboDual achieves 26.7% improvement in real-world setting and 12% gain on CALVIN by introducing a specialist policy with merely 20M trainable parameters. It maintains strong performance with  5% of demonstration data only, and enables a 3.8$\\times$ higher control frequency in real-world deployment. Code would be made publicly available."
    },
    {
        "title": "AutoGenDA: Automated Generative Data Augmentation for Imbalanced Classifications",
        "link_suffix": "/forum?id=nLlBLzPpeG",
        "link": "https://openreview.net/forum?id=nLlBLzPpeG",
        "pdf_link": "https://openreview.net/pdf?id=nLlBLzPpeG",
        "keywords": "data augmentation, machine learning",
        "abstract": "Data augmentation is an approach to increasing the training dataset size for deep learning using synthetic data. Recent advancements in image generative models have unleashed the potential of synthesizing high-quality images in data augmentation. However, real-life datasets commonly follow an imbalanced class distribution, where some classes have fewer samples than others. Image generation models may, therefore, struggle to synthesize diverse images for less common classes that lack richness and diversity. To address this, we introduce an automated generative data augmentation method, AutoGenDA, to extract and transfer label-invariant changes across data classes through image captions and text-guided generative models. We also propose an automated search strategy to optimize the data augmentation process for each data class, leading to better generalization. Our experiments demonstrate the effectiveness of AutoGenDA in various object classification datasets. We improve the standard data augmentation baselines by up to 4.9% on Pascal VOC, Caltech101, MS-COCO, and LVIS under multiple imbalanced classification settings."
    },
    {
        "title": "BroadWay: Boost Your Text-to-Video Generation Model in a Training-free Way",
        "link_suffix": "/forum?id=uzKG83YJ3t",
        "link": "https://openreview.net/forum?id=uzKG83YJ3t",
        "pdf_link": "https://openreview.net/pdf?id=uzKG83YJ3t",
        "keywords": "generative models, text-to-video generation, video quality enhancement",
        "abstract": "The text-to-video (T2V) generation models, offering convenient visual creation, have recently garnered increasing attention. Despite their substantial potential, the generated videos may present artifacts, including structural implausibility, temporal inconsistency, and a lack of motion, often resulting in near-static video. In this work, we have identified a correlation between the disparity of temporal attention maps across different blocks and the occurrence of temporal inconsistencies. Additionally, we have observed that the energy contained within the temporal attention maps is directly related to the magnitude of motion amplitude in the generated videos. Based on these observations, we present BroadWay, a training-free method to improve the quality of text-to-video generation without introducing additional parameters, augmenting memory or sampling time. Specifically, BroadWay is composed of two principal components: 1) Temporal Self-Guidance improves the structural plausibility and temporal consistency of generated videos by reducing the disparity between the temporal attention maps across various decoder blocks. 2) Fourier-based Motion Enhancement enhances the magnitude and richness of motion by amplifying the energy of the map.  Extensive experiments demonstrate that BroadWay significantly improves the quality of text-to-video generation with negligible additional cost."
    },
    {
        "title": "Deciphering and Enhancing Commonsense Reasoning in LLMs from the Perspective of Intrinsic Factual Knowledge Retrieval",
        "link_suffix": "/forum?id=MbtA7no8Ys",
        "link": "https://openreview.net/forum?id=MbtA7no8Ys",
        "pdf_link": "https://openreview.net/pdf?id=MbtA7no8Ys",
        "keywords": "Model interpretability, Chain-of-Thought, Large Language Models",
        "abstract": "Commonsense reasoning in large language models (LLMs) bridges the gap to physical world, thus allowing them to think and behave more like humans. Previous research has shown that LLMs acquire the underlying factual knowledge from extensive training corpora and store it within their parameters. However, how LLMs apply this knowledge during the inference phase remains unclear. This lack of transparency makes it difficult to determine whether shortcomings in LLMs are due to a lack of factual knowledge or insufficient reasoning capabilities.\nIn this work, we aim to decipher the commonsense reasoning process into human-understandable steps. By interpreting the hidden states in different transformer layers and token positions, we uncover a specific mechanism by which LLMs execute reasoning.\nOur extensive experiments indicate: 1) both attention head and multi-layer perceptron (MLP) contribute to the generation of factual knowledge from different perspective. 2) The process of commonsense reasoning in LLMs involves a clear sequence of knowledge augmentation, knowledge retrieval and answer generation, akin to retrieval-augmented generation.\nBuilding on these findings, we have discovered that LLMs often contain relevant facutal knowledge but fail to retrieve the correct knowledge at top. To address this issure, we selectively fine-tuned the key heads and MLPs, resulting in notably improvements in reasoning performance in both in-domain and out-of-domain settings."
    },
    {
        "title": "Evaluating Diversity of LLM-generated Datasets: A Classification Perspective",
        "link_suffix": "/forum?id=mnB4hDTIDr",
        "link": "https://openreview.net/forum?id=mnB4hDTIDr",
        "pdf_link": "https://openreview.net/pdf?id=mnB4hDTIDr",
        "keywords": "Diversity evaluation, LLM-generated dataset, Large language models",
        "abstract": "LLM-generated datasets have been recently leveraged as training data to mitigate data scarcity in specific domains. However, these LLM-generated datasets exhibit limitations on training models due to a lack of diversity, which underscores the need for effective diversity evaluation. Despite the growing demand, the diversity evaluation of LLM-generated datasets remains under-explored. To this end, we propose a diversity evaluation method for LLM-generated datasets from a classification perspective, namely, DCScore. Specifically, DCScore treats the diversity evaluation as a sample classification task, considering mutual relationships among samples. We further provide theoretical verification of the diversity-related axioms satisfied by DCScore, demonstrating it as a principled diversity evaluation method. Additionally, we show that existing methods can be incorporated into our proposed method in a unified manner. Meanwhile, DCScore enjoys much lower computational costs compared to existing methods. Finally, we conduct experiments on LLM-generated datasets to validate the effectiveness of DCScore. The experimental results indicate that DCScore correlates better with diversity-related generation hyperparameters and human judgments, thereby verifying its effectiveness."
    },
    {
        "title": "BiC-Occ: Bi-directional Circulated 3D Occupancy Prediction for Autonomous Driving",
        "link_suffix": "/forum?id=fMvcffpsDo",
        "link": "https://openreview.net/forum?id=fMvcffpsDo",
        "pdf_link": "https://openreview.net/pdf?id=fMvcffpsDo",
        "keywords": "3D occupancy prediciton, Bi-directional, View transformation, Autonomous driving",
        "abstract": "Vision-based 3D occupancy prediction is the cornerstone in autonomous driving systems to provide comprehensive scene perception for subsequent decisions, which requires assessing voxelized 3D scenes with multi-view 2D images.  Existing methods mainly adopt unidirectional pipelines projecting image features to BEV representations for following supervision, whose performances are limited by the sparsity and ambiguity of voxel labels.  To address this issue, we propose a Bi-directional Circulated 3D Occupancy Prediction (BiC-Occ) framework for more accurate voxel predictions and supervisions. Specifically, we design a Bi-directional View Transformer module that approximates invertible transition matrices of the view transformation process, promoting the self-consistency between 2D image features and 3D BEV representations. Furthermore, we propose a Circulated Interpolation Predictor module that exploits local geometric structures to align multi-scale BEV representations, correcting local ambiguity with consistent occupancy predictions across different resolutions. With the synergy of these two modules, the self-consistency within different perception views and occupancy resolutions compensates for the sparsity and ambiguity of voxel labels, leading to more accurate 3D occupancy predictions. Extensive experiments and analyses demonstrate the effectiveness of our BiC-Occ framework."
    },
    {
        "title": "PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid Visual Redundancy Reduction",
        "link_suffix": "/forum?id=5ncdKonxd4",
        "link": "https://openreview.net/forum?id=5ncdKonxd4",
        "pdf_link": "https://openreview.net/pdf?id=5ncdKonxd4",
        "keywords": "Large Vision Language Model, Efficient Training, Efficient Inference",
        "abstract": "In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom ``A picture is worth a thousand words\" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of image tokens either before or within the early layers of LVLMs. However, these strategies inevitably result in the loss of crucial image information, ultimately diminishing model performance. To address this challenge, we conduct an empirical study revealing that all visual tokens are necessary for LVLMs in the shallow layers, and token redundancy progressively increases in the deeper layers of the model.\nTo this end, we propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost their efficiency in both training and inference with neglectable performance loss. Specifically, we partition the LVLM into several stages and drop part of the image tokens at the end of each stage with a pre-defined ratio, creating pyramid-like visual tokens across model layers. The dropping is based on a lightweight similarity calculation with a negligible time overhead. Extensive experiments demonstrate that PyramidDrop can achieve a 40% training time and 55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance. Besides, the PyramidDrop could also serve as a plug-and-play strategy for inference acceleration without training, with better performance and lower inference cost than counterparts. We hope that the insights and approach introduced by PyramidDrop will inspire future research to further investigate the role of image tokens in LVLMs and explore additional methods to enhance their efficiency."
    },
    {
        "title": "ReMatching Dynamic Reconstruction Flow",
        "link_suffix": "/forum?id=bwhI6bCGY1",
        "link": "https://openreview.net/forum?id=bwhI6bCGY1",
        "pdf_link": "https://openreview.net/pdf?id=bwhI6bCGY1",
        "keywords": "Dynamic Reconstruction, Flow Modeling",
        "abstract": "Reconstructing dynamic scenes from image inputs is a fundamental computer\nvision task with many downstream applications. Despite recent advancements, existing\napproaches still struggle to achieve high-quality reconstructions from unseen\nviewpoints and timestamps. This work introduces the ReMatching framework,\ndesigned to improve generalization quality by incorporating deformation priors into\ndynamic reconstruction models. Our approach advocates for velocity-field-based\npriors, for which we suggest a matching procedure that can seamlessly supplement\nexisting dynamic reconstruction pipelines. The framework is highly adaptable\nand can be applied to various dynamic representations. Moreover, it supports\nintegrating multiple types of model priors and enables combining simpler ones to\ncreate more complex classes. Our evaluations on popular benchmarks involving\nboth synthetic and real-world dynamic scenes demonstrate a clear improvement in\nreconstruction accuracy of current state-of-the-art models."
    },
    {
        "title": "KnowTrace: Explicit Knowledge Tracing for Structured Retrieval-Augmented Generation",
        "link_suffix": "/forum?id=F6rZaxOC6m",
        "link": "https://openreview.net/forum?id=F6rZaxOC6m",
        "pdf_link": "https://openreview.net/pdf?id=F6rZaxOC6m",
        "keywords": "Knowledge Graph, Retrieval-Augmented Generation, Multi-Hop Question Answering, Multi-Step Reasoning",
        "abstract": "Recent advances in retrieval-augmented generation (RAG) furnish large language models (LLMs) with iterative retrievals of relevant information to strengthen their capabilities in addressing complex multi-hop questions. However, these methods typically accumulate the retrieved natural language text into LLM prompts, imposing an increasing burden on the LLM to grasp the underlying knowledge structure for high-quality multi-step reasoning. Despite a few attempts to reduce this burden by restructuring all retrieved passages or even entire external corpora, these efforts are afflicted with significant restructuring overhead and potential knowledge loss. To tackle this challenge, we introduce a new structured paradigm (KnowTrace) from the perspective of explicit knowledge tracing, which treats LLM as an agent to progressively acquire desired knowledge triplets during iterative retrievals and ultimately trace out a specific knowledge graph conditioned on the input question. This paradigm clearly unveils the logical relationships behind the unstructured text and thus can directly facilitate LLM\u2019s inference. Notably, it also naturally inspires a reflective mechanism of knowledge backtracing to identify supportive evidence and filter out useless retrievals in the correct trajectories, thus offering an effective way to stimulate LLM\u2019s self-taught finetuning. Extensive experiments demonstrate the superiority of our paradigm over three standard multi-hop question answering benchmarks. Our code is available athttps://github.com/xxrep/SRAG."
    },
    {
        "title": "Balancing Differential Discriminative Knowledge For Clothing-Irrelevant Lifelong Person Re-identification",
        "link_suffix": "/forum?id=5lUdTogEL3",
        "link": "https://openreview.net/forum?id=5lUdTogEL3",
        "pdf_link": "https://openreview.net/pdf?id=5lUdTogEL3",
        "keywords": "Person re-identification, Cloth-changing, Lifelong learning, Prototype learning",
        "abstract": "Lifelong person re-identification (L-ReID) focuses on learning sequentially collected datasets from different domains to match the same person. Advanced L-ReID methods typically balance the domain gap between different datasets via domain knowledge modeling, such as knowledge rectification or distribution prototyping. However, existing methods dismiss balancing discriminative knowledge within different datasets, resulting in conflicts when sequentially accumulating differential discriminative information in different datasets, e.g., sequentially learning cloth-changing/cloth-consistent knowledge simultaneously, which brings critical catastrophic forgetting problems of old discriminative knowledge. In this paper, we focus on a new but practical task called Cloth-Irrelevant Lifelong Per-\nsue, we proposed an Adaptive Discriminative Knowledge Consolidation (ADKC) framework to balance the discriminative information of different domains on L-ReID. Specifically, we propose a Selective Knowledge Forgetting (SKF) module to correct potential overfitting to specific discrimination (e.g., clothing information) based on new knowledge. In addition, we design a Selective Knowledge Retention (SKR) module to adaptively compensate for the potential lack of discriminative information based on old knowledge and accelerate differential discrimination into a unified framework. To validate our method, two CIL-ReID benchmarks are first established, while extensive experiments on the above two benchmark datasets demonstrate that our method leads to existing advanced methods in the CIL-ReID task."
    },
    {
        "title": "Adaptive Proximal Gradient Optimizer: Addressing Gradient Inexactness in Predict+Optimize Framework",
        "link_suffix": "/forum?id=cya3eEczAx",
        "link": "https://openreview.net/forum?id=cya3eEczAx",
        "pdf_link": "https://openreview.net/pdf?id=cya3eEczAx",
        "keywords": "Predict+optimize, Inexact gradient, Proximal gradient descent, Optimizer",
        "abstract": "Gradient descent is a fundamental approach for end-to-end learning in neural networks, relying on exact gradients derived from differentiable loss functions. However, in the Predict+Optimize (P+O) framework, where predictions feed into optimization problems for end-to-end decision-making, the loss function often becomes non-differentiable. \nAttempts to update models using surrogate gradients introduce a significant yet underexplored challenge: inexact gradients, which can lead to training non-convergence and instability.\nTo address this problem, we propose the Adaptive Proximal Gradient Optimizer (AProx), the first optimizer specifically designed to handle inexact gradients within the P+O framework. By constructing composite functions and employing proximal gradient methods, AProx leverages information from the smooth components of the loss function to perform effective gradient updates. This approach achieves convergence speeds comparable to those of gradient algorithms applied to smooth problems. Additionally, we integrate multiple optimizer design strategies\u2014including adaptive learning rates, momentum, and parameter averaging\u2014to enhance performance beyond direct gradient descent methods.\nWe validate AProx on several classical combination optimization benchmarks within the P+O framework. Experimental results demonstrate that AProx not only accelerates convergence but also improves overall performance compared to methods that ignore the inexactness of gradients."
    },
    {
        "title": "Can't See the Wood for the Trees: Can Visual Adversarial Patches Fool Hard-Label Large Vision-Language Models?",
        "link_suffix": "/forum?id=XFeiq8FMEF",
        "link": "https://openreview.net/forum?id=XFeiq8FMEF",
        "pdf_link": "https://openreview.net/pdf?id=XFeiq8FMEF",
        "keywords": "large vision-language model, evaluation",
        "abstract": "Large vision-language models (LVLMs) have demonstrated impressive capabilities in handling multi-modal downstream tasks, gaining increasing popularity. However, recent studies show that LVLMs are susceptible to both intentional and inadvertent attacks. Existing attackers ideally optimize adversarial perturbations with backpropagated gradients from LVLMs, thus limiting their scalability in practical scenarios as real-world LVLM applications will not provide any LVLM's gradient or details. Motivated by this research gap and counter-practical phenomenon, we propose the first and novel hard-label attack method for LVLMs, named HardPatch, to generate visual adversarial patches by solely querying the model. Our method provides deeper insights into how to investigate the vulnerability of LVLMs in local visual regions and generate corresponding adversarial substitution under the practical yet challenging hard-label setting. Specifically, we first split each image into uniform patches and mask each of them to individually assess their sensitivity to the LVLM model. Then, according to the descending order of sensitive scores, we iteratively select the most vulnerable patch to initialize noise and estimate gradients with further additive random noises for optimization. In this manner, multiple patches are perturbed until the altered image satisfies the adversarial condition. Extensive LVLM models and datasets are evaluated to demonstrate the adversarial nature of the proposed HardPatch. Our empirical observations suggest that with appropriate patch substitution and optimization, HardPatch can craft effective adversarial images to attack hard-label LVLMs."
    }
]