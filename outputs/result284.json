[
    {
        "title": "Multi-Task Dense Predictions via Unleashing the Power of Diffusion",
        "link_suffix": "/forum?id=TzdTRC85SQ",
        "link": "https://openreview.net/forum?id=TzdTRC85SQ",
        "pdf_link": "https://openreview.net/pdf?id=TzdTRC85SQ",
        "keywords": "Diffusion model, Multi-task learning, Dense prediction, Joint denoising, Cross-task encoding",
        "abstract": "Diffusion models have exhibited extraordinary performance in dense prediction tasks. However, there are few works exploring the diffusion pipeline for multi-task dense predictions. In this paper, we unlock the potential of diffusion models in solving multi-task dense predictions and propose a novel diffusion-based method, called TaskDiffusion, which leverages the conditional diffusion process in the decoder. Instead of denoising the noisy labels for different tasks separately, we propose a novel joint denoising diffusion process to capture the task relations during denoising. To be specific, our method first encodes the task-specific labels into a task-integration feature space to unify the encoding strategy. This allows us to get rid of the cumbersome task-specific encoding process. In addition, we also propose a cross-task diffusion decoder conditioned on task-specific multi-level features, which can model the interactions among different tasks and levels explicitly while preserving efficiency. Experiments show that our TaskDiffusion outperforms previous state-of-the-art methods for all dense prediction tasks on the widely-used PASCAL-Context and NYUD-v2 datasets. Our code will be made publicly available."
    },
    {
        "title": "P-BERT: Hardware-Aware Optimization of BERT Using Evolutionary Techniques",
        "link_suffix": "/forum?id=xaXvHdH9Y4",
        "link": "https://openreview.net/forum?id=xaXvHdH9Y4",
        "pdf_link": "https://openreview.net/pdf?id=xaXvHdH9Y4",
        "keywords": "Model Compression, Large Language Models, Computation Complexity, BERT, Hardware-Aware",
        "abstract": "Transformer-based models have emerged as the go-to standards in Natural Language Processing (NLP), revolutionizing the landscape of NLP applications. As complex models continue to proliferate, the need for more efficient computational processing becomes increasingly imperative. This has led to the rise of model compression techniques, implemented to target computational inefficiencies. Expounding on this, we propose Pyramid-BERT (P-BERT), the integration of three established model compression techniques to further reduce the computational inefficiency of the standard BERT models, and subsequently optimize BERT under the hardware characteristics. Specifically, the techniques employed are pruning, quantization, and knowledge distillation. The first two aforementioned correlated techniques work simultaneously to remove redundant specifications while leveraging knowledge transfer from baseline models. These techniques enable a substantial reduction in computational cost, making P-BERT highly suitable for portable, low-power devices such as cellphones, wearable devices, and smartwatches, and thus enabling hardware-friendly processing on various computing engines. Additionally, we will be proposing a new metric, the inverted computational complexity to quantify the complexity and efficacy of the model. This metric aims to more accurately capture the hardware-specific performance characteristics. Our experimental results show that P-BERT achieves a remarkable reduction of at least 60% in the inverted computational complexity ratio while ensuring comparable accuracy and scores across many downstream tasks compared with the baseline BERT models."
    },
    {
        "title": "Demystifying the Token Dynamics of Deep Selective State Space Models",
        "link_suffix": "/forum?id=qtTIP5Gjc5",
        "link": "https://openreview.net/forum?id=qtTIP5Gjc5",
        "pdf_link": "https://openreview.net/pdf?id=qtTIP5Gjc5",
        "keywords": "Selective state-space model, continuous-time limit, dynamical system, asymptotic behavior, token reordering",
        "abstract": "Selective state space models (SSM), such as Mamba, have gained prominence for their effectiveness in modeling sequential data. Despite their outstanding empirical performance, a comprehensive theoretical understanding of deep selective SSM remains elusive, hindering their further development and adoption for applications that need high fidelity. In this paper, we investigate the dynamical properties of tokens in a pre-trained Mamba model. In particular, we derive the dynamical system governing the continuous-time limit of the Mamba model and characterize the asymptotic behavior of its solutions. In the one-dimensional case, we prove that only one of the following two scenarios happens: either all tokens converge to zero, or all tokens diverge to infinity.  We provide criteria based on model parameters to determine when each scenario occurs. For the convergent scenario, we empirically verify that this scenario negatively impacts the model's performance.  For the divergent scenario, we prove that different tokens will diverge to infinity at different rates, thereby contributing unequally to the updates during model training.  Based on these investigations, we propose two refinements for the model: excluding the convergent scenario and reordering tokens based on their importance scores, both aimed at improving practical performance.  Our experimental results validate these refinements, offering insights into enhancing Mamba's effectiveness in real-world applications."
    },
    {
        "title": "Insufficient Task Description can Impair In-context Learning: A Study from Information Perspective",
        "link_suffix": "/forum?id=8Ezv4kDDee",
        "link": "https://openreview.net/forum?id=8Ezv4kDDee",
        "pdf_link": "https://openreview.net/pdf?id=8Ezv4kDDee",
        "keywords": "in-context learning",
        "abstract": "Transformers have demonstrated remarkable performance in a wide range of applications, making in-context learning an essential technique. In-context learning primarily relies on two types of information: in-context examples and task description. While previous research has extensively investigated the influence of in-context examples on learning behavior, the role of task description has not been adequately explored, despite their practical significance. In this paper, we present a study examining the impact of task description on the in-context learning performance of transformers. We devise a synthetic experiment setting, making the information of task description controllable. Through a series of well-designed experiments, we systematically vary task description information and assess the resulting effects on model performance across multiple tasks. Our findings reveal the double-side roles of task description: insufficient task description will lead the model to ignore in-context examples, resulting a poor in-context performance; once the information in task description surpasses a certain threshold, the impact of task description transfers from negative to positive, and a performance emergence can be observed. We further conduct the tasks on GPT-4 and observe a similar double-side impact. In conclusion, this study contributes to a deeper understanding of the in-context learning from a task description perspective."
    },
    {
        "title": "Model Collapse Analysis and Improvement for Rectified Flow Models",
        "link_suffix": "/forum?id=Yan3Ll5oCp",
        "link": "https://openreview.net/forum?id=Yan3Ll5oCp",
        "pdf_link": "https://openreview.net/pdf?id=Yan3Ll5oCp",
        "keywords": "Model Collapse, Generative Models, Iterative Training, Diffusion, Rectified Flow",
        "abstract": "Generative models aim to produce data indistinguishable from real distributions, but training on self-generated outputs can lead to model collapse, degrading performance. Focusing on Rectified Flow\u2014a simulation-free model prone to this issue due to its iterative use of self-generated data\u2014we provide a theoretical analysis of model collapse starting from Denoising Autoencoders. To prevent collapse, we propose methods that incorporate real data into the training process, even without direct noise-image pairs. Our approaches, Reverse Collapse-Avoiding (RCA) Reflow and Online Collapse-Avoiding Reflow (OCAR), effectively prevent collapse while maintaining sampling efficiency. Experiments on standard image datasets demonstrate improved generation quality and reduced sampling steps, confirming the effectiveness of our methods."
    },
    {
        "title": "Contrastive Learning Via Equivariant Representation",
        "link_suffix": "/forum?id=E4NShSRRDP",
        "link": "https://openreview.net/forum?id=E4NShSRRDP",
        "pdf_link": "https://openreview.net/pdf?id=E4NShSRRDP",
        "keywords": "Contrastive Learning, Self-Supervised Learning, Equivariant Contrastive Learning, Invariant Contrastive Learning",
        "abstract": "Invariant Contrastive Learning (ICL) methods have achieved impressive performance across various domains. However, the absence of latent space representation for distortion (augmentation)-related information in the latent space makes ICL sub-optimal regarding training efficiency and robustness in downstream tasks. Recent studies suggest that introducing equivariance into Contrastive Learning (CL) can improve overall performance. In this paper, we revisit the roles of augmentation strategies and equivariance in improving CL's efficacy. We propose CLeVER (Contrastive Learning Via Equivariant Representation), a novel equivariant contrastive learning framework compatible with augmentation strategies of arbitrary complexity for various mainstream CL backbone models. Experimental results demonstrate that CLeVER effectively extracts and incorporates equivariant information from practical natural images, thereby improving the training efficiency and robustness of baseline models in downstream tasks and achieving state-of-the-art (SOTA) performance. Moreover, we find that leveraging equivariant information extracted by CLeVER simultaneously enhances rotational invariance and sensitivity across experimental tasks, and helps stabilize the framework when handling complex augmentations, particularly for models with small-scale backbones."
    },
    {
        "title": "ScalePerson: Towards Good Practices in Evaluating Physical Adversarial Attacks on Person Detection",
        "link_suffix": "/forum?id=3iGponpukH",
        "link": "https://openreview.net/forum?id=3iGponpukH",
        "pdf_link": "https://openreview.net/pdf?id=3iGponpukH",
        "keywords": "Physical Adversarial Attack, Person Detection, Dataset",
        "abstract": "Person detection is widely used in safety-critical tasks but is known to be vulnerable to physical adversarial attacks. Numerous pioneering attack methods have been proposed, each claiming superior performance and exposing potential security risks. However, assessing actual progress in this field is challenging due to two common limitations in existing evaluations. First, inconsistent experimental setups and ambiguous evaluation metrics hinder fair comparisons. Second, the absence of a dedicated dataset for this task has led to evaluations on datasets originally designed for object detection, which, while informative, are inadequate. To address these limitations, we present a comprehensive benchmark and introduce ScalePerson, the first dataset specifically designed for evaluating physical adversarial attacks in person detection. This dataset incorporates critical factors for this task, such as person scale, orientation, number of individuals, and capture devices. Our benchmark includes standardized evaluation metrics and a modular codebase to enhance reproducibility and transparency. Leveraging this benchmark, we conduct an extensive evaluation of 11 state-of-the-art attacks against 7 mainstream detectors across 3 datasets, totaling 231 experiments. We present detailed analyses from multiple perspectives, examining the impact of various factors on the efficacy of physical adversarial attacks in person detection. The source code and dataset will be made publicly available upon acceptance of this paper."
    },
    {
        "title": "Understanding Self-supervised Learning as an Approximation of Supervised Learning",
        "link_suffix": "/forum?id=54jmXCHrTY",
        "link": "https://openreview.net/forum?id=54jmXCHrTY",
        "pdf_link": "https://openreview.net/pdf?id=54jmXCHrTY",
        "keywords": "representation learning, self-supervised learning, contrastive learning, theoretical framework",
        "abstract": "Self-supervised representation learning has mainly advanced in an empirical rather than theoretical manner. Many successful algorithms combine multiple techniques that are supported by experiments. This approach makes it difficult for the community to understand self-supervised learning fundamentally. To help settle this situation, we take a principled approach. We theoretically formulate a self-supervised learning problem as an approximation of a supervised learning problem. From the formulated problem, we derive a loss that is closely related to existing contrastive losses, thereby providing a foundation for these losses. The concepts of prototype representation bias and balanced contrastive loss are naturally introduced in the derivation, which provide insights to help understand self-supervised learning. We discuss how components of our framework align with practices of self-supervised learning algorithms, focusing on SimCLR. We also investigate the impact of balancing the attracting force between positive pairs and the repelling force between negative pairs. The proofs of our theorems are provided in the appendix, and the code to reproduce experimental results is provided in the supplementary material."
    },
    {
        "title": "Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset",
        "link_suffix": "/forum?id=zqtql1YmlS",
        "link": "https://openreview.net/forum?id=zqtql1YmlS",
        "pdf_link": "https://openreview.net/pdf?id=zqtql1YmlS",
        "keywords": "Offline Reinforcement Learning; Data Selection; Grad Match",
        "abstract": "Research in offline reinforcement learning (RL) marks a paradigm shift in RL. However, a critical yet under-investigated aspect of offline RL is determining the subset of the offline dataset, which is used to improve algorithm performance while accelerating algorithm training. Moreover, the size of reduced datasets can uncover the requisite offline data volume essential for addressing analogous challenges. Based on the above considerations, we propose identifying Reduced Datasets for Offline RL (ReDOR) by formulating it as a gradient approximation optimization problem.  We prove that the common actor-critic framework in reinforcement learning can be transformed into a submodular objective. This insight enables us to construct a subset by adopting the orthogonal matching pursuit (OMP). Specifically, we have made several critical modifications to OMP to enable successful adaptation with Offline RL algorithms. The experimental results indicate that the data subsets constructed by the ReDOR can significantly improve algorithm performance with low computational complexity."
    },
    {
        "title": "GroupCoOp: Group-robust Fine-tuning via Group Prompt Learning",
        "link_suffix": "/forum?id=bj9P8nt5hp",
        "link": "https://openreview.net/forum?id=bj9P8nt5hp",
        "pdf_link": "https://openreview.net/pdf?id=bj9P8nt5hp",
        "keywords": "spurious correlation, debiasing, group robustness, prompt learning, vision-language models",
        "abstract": "Parameter-efficient fine-tuning (PEFT) of vision-language models (VLMs) excels in various vision tasks thanks to the rich knowledge and generalization ability of VLMs. However, recent studies revealed that such fine-tuned VLMs are vulnerable to spurious correlations stemming from the subgroup imbalance in the fine-tuning datasets. To resolve this issue, we propose Group Context Optimization (GroupCoOp), a simple and effective debiased fine-tuning algorithm that enhances the group robustness of fine-tuned VLMs without group labels. Its key idea is to employ group-specific text prompts as group representatives serving as multiple classifiers for their target class. The rich semantic knowledge of the text encoder of VLM enables the discovery of effective group prompts even for groups with a small number of training samples. Leveraging the group prompts for each class addresses the issues caused by the group-imbalanced training set, such as the neglect of minority groups and the scattered distribution of each class in the embedding space. Moreover, we propose a simple yet fairly effective pseudo group labeling algorithm, which allows GroupCoOp to fine-tune VLMs without manual group labels. GroupCoOp achieved the best results on five benchmarks across five CLIP architectures and even outperformed prior methods that train the entire network, despite training only 0.016% of the network's parameters. GroupCoOp demonstrates robust performance even with extremely limited training samples, where the minority group sample is limited to a single instance."
    },
    {
        "title": "Off-policy Evaluation with Deeply-abstracted States",
        "link_suffix": "/forum?id=bprVvbmL6T",
        "link": "https://openreview.net/forum?id=bprVvbmL6T",
        "pdf_link": "https://openreview.net/pdf?id=bprVvbmL6T",
        "keywords": "Markov decision process, Off-policy evaluation, State abstraction, Reinforcement learning theory",
        "abstract": "Off-policy evaluation (OPE) is crucial  for assessing a target policy's impact offline before its deployment. However, achieving accurate OPE in  large state spaces remains challenging. This paper studies state abstractions -- originally designed for policy learning -- in the context of OPE. Our contributions are three-fold: (i) We define a set of irrelevance conditions central to learning state abstractions for OPE, and derive a backward-model-irrelevance condition for achieving irrelevance in  (marginalized) importance sampling ratios by constructing a time-reversed Markov decision process (MDP) based on the standard MDP. (ii) We propose a novel iterative procedure that sequentially projects the original state space into a smaller space, resulting in a deeply-abstracted state, which substantially simplify the sample complexity of OPE arising from high cardinality. (iii) We prove the Fisher consistencies of various OPE estimators when applied to our proposed abstract state spaces."
    },
    {
        "title": "Latent Adversarial Training Improves Robustness to Persistent Harmful Behaviors in LLMs",
        "link_suffix": "/forum?id=wI5uHZLeCZ",
        "link": "https://openreview.net/forum?id=wI5uHZLeCZ",
        "pdf_link": "https://openreview.net/pdf?id=wI5uHZLeCZ",
        "keywords": "adversarial attacks, adversarial training, jailbreaks, trojans, backdoors, unlearning, robustenss",
        "abstract": "Large language models (LLMs) can often be made to behave in undesirable ways that they are explicitly fine-tuned not to. For example, the LLM red-teaming literature has produced a wide variety of 'jailbreaking' techniques to elicit harmful text from models that were fine-tuned to be harmless. Recent work on red-teaming, model editing, and interpretability suggests that this challenge stems from how (adversarial) fine-tuning largely serves to suppress rather than remove undesirable capabilities from LLMs. Prior work has introduced latent adversarial training (LAT) as a way to improve robustness to broad classes of failures. These prior works have considered untargeted latent space attacks where the adversary perturbs latent activations to maximize loss on examples of desirable behavior. Untargeted LAT can provide a generic type of robustness but does not leverage information about specific failure modes. Here, we experiment with targeted LAT where the adversary seeks to minimize loss on a specific competing task. We find that it can augment a wide variety of state-of-the-art methods. First, we use targeted LAT to improve robustness to jailbreaks, outperforming a strong R2D2 baseline with orders of magnitude less compute. Second, we use it to more effectively remove backdoors with no knowledge of the trigger. Finally, we use it to more effectively unlearn knowledge for specific undesirable tasks in a way that is also more robust to re-learning. Overall, our results suggest that targeted LAT can be an effective tool for defending against harmful behaviors from LLMs."
    },
    {
        "title": "Incremental Aggregated Asynchronous SGD for Arbitrarily Heterogeneous Data",
        "link_suffix": "/forum?id=m3x4kDbYAK",
        "link": "https://openreview.net/forum?id=m3x4kDbYAK",
        "pdf_link": "https://openreview.net/pdf?id=m3x4kDbYAK",
        "keywords": "distributed optimization, asynchronous SGD, data heterogeneity",
        "abstract": "We consider the distributed learning problem with data dispersed across multiple workers under the orchestration of a central server. Asynchronous Stochastic Gradient Descent (SGD) has been widely explored in such a setting to reduce the synchronization overhead associated with parallelization. However, prior works have shown that the performance of asynchronous SGD algorithms depends on a bounded dissimilarity condition among the workers' local data, a condition that can drastically affect their efficiency when the workers' data are highly heterogeneous. To overcome this limitation, we introduce the Incremental Aggregated Asynchronous SGD (IA$^2$SGD) algorithm. With a server-side buffer, IA$^2$SGD makes full use of stale stochastic gradients from all workers to neutralize the adverse effects of data heterogeneity. In an asynchronous implementation setting, the algorithm entails two distinct time lags in the model parameters and data samples utilized in the server's iterations. Furthermore, by adopting an incremental aggregation strategy, IA$^2$SGD maintains a per-iteration computational cost that is on par with traditional asynchronous SGD algorithms. Our analysis demonstrates that IA$^2$SGD achieves a consistent convergence rate for smooth nonconvex problems for arbitrarily heterogeneous data. Numerical experiments indicate that IA$^2$SGD compares favorably with existing asynchronous and synchronous SGD-based algorithms."
    },
    {
        "title": "Distinct and Shared Concept Discovery for Fine-grained Concept Inversion",
        "link_suffix": "/forum?id=eHEYwrN4lw",
        "link": "https://openreview.net/forum?id=eHEYwrN4lw",
        "pdf_link": "https://openreview.net/pdf?id=eHEYwrN4lw",
        "keywords": "Vision language models, generative model, concept discovery",
        "abstract": "A real-world object is expressed by composing distinctive characteristics that distinguish it from others and some common properties shared with different objects. Recent advances in generative modeling focus on identifying the shared concepts within images of individual identities. However, it remains unclear how to identify shared concepts beyond multiple identities while preserving the unique concepts inherent to each. In this work, we address this new problem of simultaneously discovering similarities and differences between two sets of images and propose a two-stage framework coined DISCOD (DIstinct and Shared COncept Discovery). In the first stage of DISCOD, we introduce information-regularized textual inversion, focusing on separating representative concepts distinctive from others while capturing the shared concepts among different objects. In the next stage, we further optimize them to align composited concepts of those with the corresponding objects, respectively. We demonstrate the effectiveness of DISCOD by showing that DISCOD discovers the concepts better than baselines, as measured by CLIPScore and success rate. The human study also validates the reasonable discovery capability of DISCOD. Furthermore, we show the practical applicability of our approach by applying to various applications: image editing, few-shot personalization of diffusion models, and group bias mitigation in recognition."
    },
    {
        "title": "Rethinking the Roles of Time and Frequency Domains Before Tackling Time Series UDA",
        "link_suffix": "/forum?id=tvLnYGAaY1",
        "link": "https://openreview.net/forum?id=tvLnYGAaY1",
        "pdf_link": "https://openreview.net/pdf?id=tvLnYGAaY1",
        "keywords": "unsupervised domain adaptation, time-series domain adaptation, TSUDA",
        "abstract": "In time-series unsupervised domain adaptation (UDA), the adaptation of both temporal and frequency domain features has been relatively underexplored. To address this gap, we conduct a comprehensive series of experiments to revisit the roles of these domains in UDA. Our findings reveal that the temporal domain contains more diverse features, offering higher discriminability, while the frequency domain is more domain-invariant, providing better transferability. Combining the strengths of both domains, we propose TidalFlow, a UDA framework that synergistically integrates temporal and frequency domain features. TidalFlow enhances feature extraction and captures subtle, class-specific features without relying on traditional alignment strategies. By utilizing simple hyperparameter adjustments and using frequency embeddings from the source domain as reference points for domain adaptation, TidalFlow achieves nearly a 10% improvement across five benchmark datasets in time-series UDA. This research highlights the unique strengths of both domains and marks a paradigm shift in UDA methods, showcasing TidalFlow\u2019s robust performance in real-world applications."
    },
    {
        "title": "JudgeRail: Harnessing Open-Source LLMs for Fast Harmful Text Detection with Judicial Prompting and Logit Rectification",
        "link_suffix": "/forum?id=CEvGuwMum0",
        "link": "https://openreview.net/forum?id=CEvGuwMum0",
        "pdf_link": "https://openreview.net/pdf?id=CEvGuwMum0",
        "keywords": "Large Language Model, Harmful Text Detection, Toxic Speech Detection, Content Moderation",
        "abstract": "Large language models (LLMs) simultaneously facilitate the generation and detection of harmful text. Leading LLM developers, such as OpenAI, Meta, and Google, are driving a paradigm shift in the detection of harmful text, moving from conventional detectors to fine-tuned LLMs. However, these newly released models, which require substantial computational and data resources, have not yet been thoroughly investigated for their effectiveness in this new paradigm. In this work, we propose JudgeRail, a novel and generic framework that guides open-source LLMs to adhere to judicial principles during text moderation. Additionally, we introduce a new logit rectification method that accurately interprets an LLM's classification intent, rigorously controls its output format, and significantly accelerates detection. By integrating several top-performing open-source LLMs into JudgeRail without any fine-tuning and evaluating them against OpenAI Moderation API, LlamaGuard3, ShieldGemma, and other conventional moderation solutions across various datasets, including those specifically designed for jailbreaking LLMs, we demonstrate that JudgeRail can adapt these LLMs to be competitive with fine-tuned moderation models and significantly outperform conventional solutions. Moreover, we evaluate all models for detection latency, a critical yet rarely examined practical aspect, and show that LLMs with JudgeRail require only 46% to 55% of the time needed by LlamaGuard3 and ShieldGemma. The generic nature and competitive performance of JudgeRail highlight its potential for promoting the practicality of LLM-based harmful text detectors."
    },
    {
        "title": "Turning Challenges into Opportunities: How Distribution Shifts Enhance Identifiability in Causal Representation Learning",
        "link_suffix": "/forum?id=q07DDpu8Xb",
        "link": "https://openreview.net/forum?id=q07DDpu8Xb",
        "pdf_link": "https://openreview.net/pdf?id=q07DDpu8Xb",
        "keywords": "Causal Representation Learning, Latent Causal Models, Identifiability, Distribution Shifts",
        "abstract": "Causal representation learning seeks to uncover latent causal variables and their relationships from observed, unstructured data, a task complicated by identifiability challenges. While distribution shifts, viewed as natural interventions on latent causal variables, often present difficulties in traditional machine learning tasks, they also create valuable opportunities for identifiability by introducing variability in latent variables. In this paper, we study a non-parametric condition characterizing the types of distribution shifts that contribute to identifiability within the context of latent additive noise models. We also present partial identifiability results when only a portion of distribution shifts meets the condition. Furthermore, we extend our findings to latent post-nonlinear causal models. Building on our theoretical results, we propose a practical algorithm facilitating the acquisition of reliable latent causal representations. Our algorithm, guided by our underlying theory, has demonstrated outstanding performance across a diverse range of synthetic and real-world datasets. The empirical observations closely align with the theoretical findings, affirming the robustness and effectiveness of our proposed approach."
    },
    {
        "title": "Training-free Detection of AI-generated Images via High-frequency Influence",
        "link_suffix": "/forum?id=lwn5fbqf74",
        "link": "https://openreview.net/forum?id=lwn5fbqf74",
        "pdf_link": "https://openreview.net/pdf?id=lwn5fbqf74",
        "keywords": "Zero-shot detection, AI-generated image detection, Training-free, Deepfake detection, safety, LDM-generated image",
        "abstract": "Dramatic advances in the quality of the latent diffusion models (LDMs) also led to the malicious use of AI-generated images. While current AI-generated image detection methods assume the availability of real/AI-generated images for training, this is practically limited given the vast expressibility of LDMs. This motivates the training-free detection setup where no related data are available in advance. In this paper, we propose that the level of aliasing detected in reconstructed images produced by the autoencoder of LDMs can serve as a criterion for distinguishing between real and AI-generated images. Specifically, we propose a novel detection score function, termed high-frequency influence (HFI), which quantifies the impact of the spatial filtering-based high-frequency components of the input image on the perceptual reconstruction distance. HFI is training-free, efficient, and consistently outperforms other training-free methods in detecting challenging images generated by various generative models. We also show that HFI can successfully detect the images generated from the specified LDM. HFI outperforms the best baseline method while achieving magnitudes of speedup."
    },
    {
        "title": "From Commands to Prompts: LLM-based Semantic File System",
        "link_suffix": "/forum?id=2G021ZqUEZ",
        "link": "https://openreview.net/forum?id=2G021ZqUEZ",
        "pdf_link": "https://openreview.net/pdf?id=2G021ZqUEZ",
        "keywords": "Large Language Model, Semantic File System",
        "abstract": "Large language models (LLMs) have demonstrated significant potential in the development of intelligent LLM-based agents. However, when users use these agent applications perform file operations, their interaction with the file system still remains the traditional paradigm: reliant on manual navigation through precise commands. This paradigm poses a bottleneck to the usability of these systems as users are required to navigate complex folder hierarchies and remember cryptic file names. To address this limitation, we propose an LLM-based Semantic File System ( LSFS) for prompt-driven file management. Unlike conventional approaches, LSFS incorporates LLMs to enable users or agents to interact with files through natural language prompts, facilitating semantic file management. At the macro-level, we develop a comprehensive API set to achieve semantic file management functionalities, such as semantic file retrieval, file update summarization, and semantic file rollback). At the micro-level, we store files by constructing semantic indexes for them, design and implement syscalls of different semantic operations, e.g., CRUD (create, read, update, delete), group by, join. Our experiments show that LSFS can achieve at least 15% retrieval accuracy improvement with 2.1\u00d7 higher retrieval speed in the semantic file retrieval task compared with the traditional file system. In the traditional keyword-based file retrieval task (i.e., retrieving by string-matching), LSFS also performs stably well, i.e., over 89% F1-score with improved usability, especially when the keyword conditions become more complex. Additionally, LSFS supports more advanced file management operations, i.e., semantic file rollback and file sharing and achieves 100% success rates in these tasks, further suggesting the capability of LSFS. The code is available athttps://anonymous.4open.science/r/LSFS-8CCF/."
    },
    {
        "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads",
        "link_suffix": "/forum?id=CkCFoN3j4s",
        "link": "https://openreview.net/forum?id=CkCFoN3j4s",
        "pdf_link": "https://openreview.net/pdf?id=CkCFoN3j4s",
        "keywords": "Long-context Inference, Memory Efficient Inference, Large Language Models",
        "abstract": "Large language models (LLMs) have shown remarkable advances in supporting long-context comprehension and processing tasks. However, scaling the generation inference of LLMs to such long contexts incurs significant additional computation load, and demands a substantial GPU memory footprint to maintain the key-value (KV) cache of transformer-based LLMs. Existing KV cache compression methods, such as quantization, face memory bottlenecks as context length increases, while static-sized caches, such as selective eviction, suffer from inefficient policies. These limitations restrict deployment on consumer-grade devices like a single Nvidia 4090 GPU. To overcome this, we propose Locret, an efficient framework for long-context LLM inference that introduces retaining heads to evaluate the causal importance of KV cache units, allowing for more accurate eviction within a fixed cache size. Locret is fine-tuned on top of the frozen backbone LLM using a minimal amount of data from standard long-context SFT datasets. During inference, we evict low-importance cache units along with a chunked prefill pattern, significantly reducing peak GPU memory usage. We conduct an extensive empirical study to evaluate Locret, where the experimental results show that Locret outperforms the recent popular and competitive approaches, including InfLLM, Quantization, SirLLM, and MInference, in terms of memory efficiency and the quality of generated contents --- Locret achieves over a $20\\times$ and $8\\times$ KV cache compression ratio compared to the full KV cache for Phi-3-mini-128K and Llama-3.1-8B-instruct. Additionally, Locret can be combined with other efficient inference methods, such as quantization and token merging. To the best of our knowledge, Locret is the first framework capable of deploying Llama-3.1-8B or similar models on a single Nvidia 4090 GPU, enabling 128K long-context inference without compromising generation quality, and requiring little additional system optimizations."
    },
    {
        "title": "Language Models can Self-Lengthen to Generate Long Texts",
        "link_suffix": "/forum?id=gGElk5T8sD",
        "link": "https://openreview.net/forum?id=gGElk5T8sD",
        "pdf_link": "https://openreview.net/pdf?id=gGElk5T8sD",
        "keywords": "LLM, Synthetic Data, Long Output",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to process long contexts, yet a notable gap remains in generating long, aligned outputs. \nThis limitation stems from a training gap where pre-training lacks effective instructions for long-text generation, and post-training data primarily consists of short query-response pairs. \nCurrent approaches, such as instruction backtranslation and behavior imitation, face challenges including data quality, copyright issues, and constraints on proprietary model usage.\nIn this paper, we introduce an innovative iterative training framework called Self-Lengthen that leverages only the intrinsic knowledge and skills of LLMs without the need for auxiliary data or proprietary models. \nThe framework consists of two roles: the Generator and the Extender. The Generator produces the initial response, which is then split and expanded by the Extender. This process results in a new, longer response, which is used to train both the Generator and the Extender iteratively. Through this process, the models are progressively trained to handle increasingly longer responses. \nExperiments on benchmarks and human evaluations show that Self-Lengthen outperforms existing methods in long-text generation, when applied to top open-source LLMs such as Qwen2 and LLaMA3."
    },
    {
        "title": "Unispeaker: A unified speech generation model for multimodality-driven voice control",
        "link_suffix": "/forum?id=NVASzf27bL",
        "link": "https://openreview.net/forum?id=NVASzf27bL",
        "pdf_link": "https://openreview.net/pdf?id=NVASzf27bL",
        "keywords": "Speech Generation, Voice Control, Multi-Modality Alignment",
        "abstract": "Recent advancements in zero-shot speech generation have brought synthetic speech increasingly close to the realism of target speakers' recordings, yet multimodal voice creation remains an evolving field. In various scenarios, individuals often seek to control and create voice characteristics through different voice description modalities. To address the limitations in both the versatility and performance of voice control found in previous methods, this paper introduces UniSpeaker, a unified, multimodal-driven speech generation model that integrates face images, text descriptions, attribute descriptions, and reference speech for comprehensive voice control and creation. Specifically, we propose a unified voice aggregator based on KV-Former, applying soft contrastive loss to map diverse voice description modalities into a shared voice space, ensuring that the generated voice aligns more closely with the input descriptions. In addition, multimodal voice control is incorporated within a large-scale speech generation framework, employing self-distillation to enhance voice disentanglement. We introduce the MVC benchmark to evaluate multimodality-driven voice control, focusing on voice suitability, voice diversity, and speech quality. We assess UniSpeaker across five tasks using the MVC benchmark, and the experimental results demonstrate that UniSpeaker outperforms previous modality-specific models. Speech samples are available athttps://UniSpeaker.github.io."
    },
    {
        "title": "Intriguing Properties of Large Language and Vision Models",
        "link_suffix": "/forum?id=bb2Cm6Xn6d",
        "link": "https://openreview.net/forum?id=bb2Cm6Xn6d",
        "pdf_link": "https://openreview.net/pdf?id=bb2Cm6Xn6d",
        "keywords": "LLVM;clustering;projector;investigation",
        "abstract": "Recently, large language and vision models (LLVMs) have received significant attention and development efforts due to their remarkable generalization performance across a wide range of tasks requiring perception and cognitive abilities. A key factor behind their success is their simple architecture, which consists of a vision encoder, a projector, and a large language model (LLM). Despite their achievements in advanced reasoning tasks, their performance on fundamental perception-related tasks (\\eg MMVP) remains surprisingly low. This discrepancy raises the question of how LLVMs truly perceive images and exploit the advantages of the vision encoder. To address this, we systematically investigate this question regarding several aspects: \\textit{permutation invariance}, \\textit{robustness}, \\textit{synthetic data}, \\textit{alignment preserving} and \\textit{importance}, by evaluating the most common LLVM's families (\\ie LLaVA) across 13 evaluation benchmarks. Our extensive experiments reveal several intriguing properties of current LLVMs: (1) they internally process the image in a global manner, even whenthe order of visual patch sequences is randomly permuted; (2) they are sometimes able to solve math problems without fully perceiving detailed numerical information; (3) the cross-modal alignment is overfitted to complex reasoning tasks, thereby, causing them to lose some of the original perceptual capabilities of their vision encoder; (4) the representation space in the lower layers ($<25%$) plays a crucial role in determining performance and enhancing visual understanding. Lastly, based on the above observations, we suggest potential future directions for building better LLVMs and constructing more challenging evaluation benchmarks."
    },
    {
        "title": "Task Arithmetic in Trust Region: A Training-Free Model Merging Approach to Navigate Knowledge Conflicts",
        "link_suffix": "/forum?id=q3ztjJRQuJ",
        "link": "https://openreview.net/forum?id=q3ztjJRQuJ",
        "pdf_link": "https://openreview.net/pdf?id=q3ztjJRQuJ",
        "keywords": "Multi-task Learning, Model Merging, negative transfer",
        "abstract": "Multi-task model merging offers an efficient solution for integrating knowledge from multiple fine-tuned models, mitigating the significant computational and storage demands associated with multi-task training. As a key technique in this field, Task Arithmetic (TA) defines task vectors by subtracting the pre-trained model ($\\theta_{\\text{pre}}$) from the fine-tuned task models in parameter space, then adjusting the weight between these task vectors and $\\theta_{\\text{pre}}$ to balance task-generalized and task-specific knowledge. Despite the promising performance of TA, conflicts can arise among the task vectors, particularly when different tasks require distinct model adaptations. In this paper, we formally define this issue as knowledge conflicts, characterized by the performance degradation of one task after merging with a model fine-tuned for another task. Through in-depth analysis, we show that these conflicts stem primarily from the components of task vectors that align with the gradient of task-specific losses at $\\theta_{\\text{pre}}$. To address this, we propose Task Arithmetic in Trust Region (TATR), which defines the trust region as dimensions in the model parameter space that cause only small changes (corresponding to the task vector components with gradient orthogonal direction) in the task-specific losses. Restricting parameter merging within this trust region, TATR can effectively alleviate knowledge conflicts. Moreover, TATR serves as both an independent approach and a plug-and-play module compatible with a wide range of TA-based methods. Extensive empirical evaluations on eight distinct datasets robustly demonstrate that TATR improves the multi-task performance of several TA-based model merging methods by an observable margin."
    },
    {
        "title": "Fewer Questions, Better Answers: Efficient Offline Preference-based Reinforcement Learning via In-Dataset Exploration",
        "link_suffix": "/forum?id=MFwYXa796v",
        "link": "https://openreview.net/forum?id=MFwYXa796v",
        "pdf_link": "https://openreview.net/pdf?id=MFwYXa796v",
        "keywords": "preference-based RL, offline RL, reinforcement learning",
        "abstract": "Preference-based reinforcement learning (PbRL) can help avoid sophisticated reward designs and align better with human intentions, showing great promise in various real-world applications. However, obtaining human feedback for preferences can be expensive and time-consuming, which forms a strong barrier for PbRL.  In this work, we address the problem of low query efficiency in offline PbRL, pinpointing two primary reasons: inefficient exploration and overoptimization of learned reward functions. In response to these challenges, we propose a novel algorithm, Offline PbRL via In-Dataset Exploration (OPRIDE), designed to enhance the query efficiency of offline PbRL. OPRIDE consists of two key features: a principled exploration strategy that maximizes the informativeness of the queries and a discount scheduling mechanism aimed at mitigating overoptimization of the learned reward functions. Through empirical evaluations, we demonstrate that OPRIDE significantly outperforms prior methods, achieving strong performance with notably fewer queries. Moreover, we provide theoretical guarantees of the algorithm's efficiency. Experimental results across various locomotion, manipulation, and navigation tasks underscore the efficacy and versatility of our approach."
    }
]