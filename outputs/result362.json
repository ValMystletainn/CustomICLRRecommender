[
    {
        "title": "PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition",
        "link_suffix": "/forum?id=ZsU52Zkzjr",
        "link": "https://openreview.net/forum?id=ZsU52Zkzjr",
        "pdf_link": "https://openreview.net/pdf?id=ZsU52Zkzjr",
        "keywords": "point cloud video, 3d vision",
        "abstract": "Point cloud video perception has become an essential task for the realm of 3D vision. Current 4D representation learning techniques typically engage in iterative processing coupled with dense query operations. Although effective in capturing temporal features, this approach leads to substantial computational redundancy. In this work, we propose a framework, named as PvNeXt, for effective yet efficient point cloud video recognition, via personalized one-shot query operation. Specially, PvNeXt consists of two modules, a Motion Imitator and a Single-Step Motion Encoder.  The former module, the Motion Imitator, is designed to capture the temporal dynamics inherent in sequences of point clouds, thus generating the virtual motion corresponding to each frame. The Single-Step Motion Encoder, is tasked with performing a one-step query operation from points to their corresponding virtual frames with motion. Through the integration of these two modules, PvNeXt enables personalized one-shot queries for each frame, effectively eliminating the need for frame-specific looping and intensive query processes.  Extensive experiments on multiple benchmarks demonstrate the effectiveness of our method, with significantly cheaper training costs. Notably, PvNeXt gains a $1.95$ accuracy increase with $23$x inference speedup on the widely used point cloud video recognition benchmark MSR-Action3D compared to PST-Transformer, while the latter takes over $60$x larger parameters. Codes will be available soon."
    },
    {
        "title": "Targeted Low-rank Refinement: Enhancing Sparse Neural Networks with Precision",
        "link_suffix": "/forum?id=s6Q7aVZWIn",
        "link": "https://openreview.net/forum?id=s6Q7aVZWIn",
        "pdf_link": "https://openreview.net/pdf?id=s6Q7aVZWIn",
        "keywords": "Model Compression, Low-Rank Refinement, Model Pruning",
        "abstract": "Pruning is a widely used technique for compressing large neural networks that eliminate weights that have minimal impact on the model's performance. Current pruning methods, exemplified by magnitude pruning, assign an importance score to each weight based on its magnitude and remove weights with scores below a certain threshold. Nonetheless, these methods often create a gap between the original dense and the pruned sparse model, potentially impairing performance. Especially when the sparsity ratio is high, the gap becomes more pronounced. To mitigate this issue, we introduce to bridge the gap left by pruning by utilizing a low-rank approximation of the difference between the dense and sparse matrices. Our method specifically entails the iterative refinement of the sparse weight matrix, augmented by a low-rank adjustment. This technique captures and retains the essential information often lost during pruning, thereby improving the performance of the pruned model. Furthermore, we offer a comprehensive theoretical analysis of our approach, emphasizing its convergence properties and establishing a solid basis for its efficacy. Experimental results on LLaMa models validate its effectiveness on large language models across various pruning techniques and sparsity levels. Our method shows significant improvements: at 50% sparsity, it reduces perplexity by 53.9% compared to conventional magnitude pruning on LLaMa-7B.Furthermore, to achieve a specific performance target, our approach enables an 8.6% reduction in model parameters while maintaining a sparsity ratio of about 50%."
    },
    {
        "title": "Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?",
        "link_suffix": "/forum?id=28qOQwjuma",
        "link": "https://openreview.net/forum?id=28qOQwjuma",
        "pdf_link": "https://openreview.net/pdf?id=28qOQwjuma",
        "keywords": "LLMs, Hypergraph, Benchmark",
        "abstract": "Existing benchmarks like NLGraph and GraphQA evaluate LLMs on graphs by focusing mainly on pairwise relationships, overlooking the high-order correlations found in real-world data. Hypergraphs, which can model complex beyond-pairwise relationships, offer a more robust framework but are still underexplored in the context of LLMs. To address this gap, we introduce LLM4Hypergraph, the first comprehensive benchmark comprising 21,500 problems across eight low-order, five high-order, and two isomorphism tasks, utilizing both synthetic and real-world hypergraphs from citation networks and protein structures. We evaluate six prominent LLMs, including GPT-4o, demonstrating our benchmark\u2019s effectiveness in identifying model strengths and weaknesses. Our specialized prompt- ing framework incorporates seven hypergraph languages and introduces two novel techniques, Hyper-BAG and Hyper-COT, which enhance high-order reasoning and achieve an average 4% (up to 9%) performance improvement on structure classification tasks. This work establishes a foundational testbed for integrating hypergraph computational capabilities into LLMs, advancing their comprehension."
    },
    {
        "title": "3DGS-Det: Empower 3D Gaussian Splatting with Boundary Guidance and Box-Focused Sampling for 3D Object Detection",
        "link_suffix": "/forum?id=9SmukfhJoF",
        "link": "https://openreview.net/forum?id=9SmukfhJoF",
        "pdf_link": "https://openreview.net/pdf?id=9SmukfhJoF",
        "keywords": "3D Gaussian Splatting, 3D Object Detection, Neural Radiance Fields",
        "abstract": "Neural Radiance Fields (NeRF) is a widely adopted class of methods for novel view synthesis. Some works have introduced it into the 3D Object Detection (3DOD) task, paving the way for promising exploration of 3D object detection based on view synthesis representation. However, NeRF has inherent limitations: (1) limited representational capacity for 3DOD as an implicit representation, and (2) slow rendering speed. Recently, 3D Gaussian Splatting (3DGS) emerged as an explicit 3D representation with faster rendering, overcoming these limitations. This paper is the first to introduce 3DGS into 3DOD and identifies two primary challenges: (a) 3DGS mainly focuses on 2D pixel-level parsing instead of 3D geometry, leading to unclear 3D spatial distribution and indistinct differentiation between objects and background, which hinders 3DOD; (b) 2D images often contain many background pixels, resulting in densely reconstructed 3DGS with noisy points representing the background, impacting detection. To address (a), we consider that 3DGS reconstruction originates from 2D images and design an elegant and efficient solution by incorporating2D Boundary Guidanceto enhance the spatial distribution of 3DGS. Specifically, we perform boundary detection on posed images, overlay the boundaries on the images, and then train 3DGS. Interestingly, as shown in figure 1, this precise strategy significantly improves the spatial distribution of Gaussians and brings clearer differentiation between objects and background. For (b), we propose aBox-Focused Samplingstrategy using 2D boxes to establish object probability spaces, allowing probabilistic sampling of Gaussians to retain more object points and reduce background noise. Benefiting from 2D Boundary Guidance and Box-Focused Sampling, our final method,3DGS-DET, achieves significant improvements (5.6 pointson mAP0.25,3.7 pointson mAP0.5) over the baseline version without the proposed two strategies, with introducingzeroadditional learnable parameters. Furthermore, 3DGS-DET significantly outperforms the state-of-the-art NeRF-based method, NeRF-Det, on both ScanNet and ARKITScenes. We commit to releasing all codes and data within one month of paper acceptance."
    },
    {
        "title": "How Much is a  Noisy Image Worth? Data Scaling Laws for Ambient Diffusion.",
        "link_suffix": "/forum?id=qZwtPEw2qN",
        "link": "https://openreview.net/forum?id=qZwtPEw2qN",
        "pdf_link": "https://openreview.net/pdf?id=qZwtPEw2qN",
        "keywords": "ambient diffusion, noisy data, dataset design, gaussian mixtures",
        "abstract": "The quality of generative models depends on the quality of the data they are trained on. Creating large-scale, high-quality datasets is often expensive and sometimes impossible, e.g.in certain scientific applications where there is no access to clean data due to physical or instrumentation constraints. Ambient Diffusion and related frameworks train diffusion models with solely corrupted data (which are usually cheaper to acquire) but ambient models significantly underperform models trained on clean data. We study this phenomenon at scale by training more than $80$ models on data with different corruption levels across three datasets ranging from $30,000$ to $\\approx 1.3$M samples. We show that it is impossible, at these sample sizes, to match the performance of models trained on clean data when only training on noisy data. Yet, a combination of a small set of clean data (e.g.$10%$ of the total dataset) and a large set of highly noisy data suffices to reach the performance of models trained solely on similar-size datasets of clean data, and in particular to achieve near state-of-the-art performance. We provide theoretical evidence for our findings by developing novel sample complexity bounds for learning from Gaussian Mixtures with heterogeneous variances. Our theoretical model suggests that, for large enough datasets, the effective marginal utility of a noisy sample is exponentially worse that of a clean sample. Providing a small set of clean samples can significantly reduce the sample size requirements for noisy data, as we also observe in our experiments."
    },
    {
        "title": "Posterior sampling via Langevin dynamics based on generative priors",
        "link_suffix": "/forum?id=F6SaYwJ3eV",
        "link": "https://openreview.net/forum?id=F6SaYwJ3eV",
        "pdf_link": "https://openreview.net/pdf?id=F6SaYwJ3eV",
        "keywords": "Posterior Sampling, Inverse Problems, Consistency Models",
        "abstract": "Posterior sampling in high-dimensional spaces using generative models holds significant promise for various applications, including but not limited to inverse problems and guided generation tasks. Despite many recent developments, generating diverse posterior samples remains a challenge, as existing methods require restarting the entire generative process for each new sample, making the procedure computationally expensive. In this work, we propose efficient posterior sampling by simulating Langevin dynamics in the noise space of a pre-trained generative model. By exploiting the mapping between the noise and data spaces which can be provided by distilled flows or consistency models, our method enables seamless exploration of the posterior without the need to re-run the full sampling chain, drastically reducing computational overhead. Theoretically, we prove a guarantee for the proposed noise-space Langevin dynamics to approximate the posterior, assuming that the generative model sufficiently approximates the prior distribution. Our framework is experimentally validated on image restoration tasks involving noisy linear and nonlinear forward operators applied to LSUN-Bedroom (256 x 256) and ImageNet (64 x 64) datasets. The results demonstrate that our approach generates high-fidelity samples with enhanced semantic diversity even under limited number of function evaluations, offering superior efficiency and performance compared to existing diffusion-based posterior sampling techniques."
    },
    {
        "title": "Quantile-Optimal Policy Learning under Unmeasured Confounding",
        "link_suffix": "/forum?id=ngxxEksoJi",
        "link": "https://openreview.net/forum?id=ngxxEksoJi",
        "pdf_link": "https://openreview.net/pdf?id=ngxxEksoJi",
        "keywords": "Quantile Treatment Effect, Causal Inference, Offline Contextual Bandit",
        "abstract": "We study quantile-optimal policy learning where the goal is to find a policy whose reward distribution has the largest $\\alpha$-th quantile for some $\\alpha \\in (0, 1)$. We focus on the offline setting whose generating process involves unobserved confounders. Such a problem suffers from three main challenges: (i) nonlinearity of the quantile objective as a functional of the reward distribution,  (ii) unobserved confounding issue, and  (iii) insufficient coverage of the offline dataset. To address these challenges, we propose a suite of causal-assisted policy learning methods that provably enjoy strong theoretical guarantees under mild conditions. In particular, to address (i) and (ii), using causal inference tools such as instrumental variables and negative controls, we propose to estimate the quantile objectives by solving nonlinear functional integral equations. Then we adopt a minimax estimation approach with nonparametric models to solve these integral equations, and propose to construct conservative policy estimates that address (iii). The final policy is the one that maximizes these pessimistic estimates. In addition, we propose a novel regularized policy learning method that is more amenable to computation. Finally, we prove that the policies learned by these methods are $\\tilde{O}(n^{-1/2})$ quantile-optimal under a mild coverage assumption on the offline dataset. To our best knowledge, we propose the first sample-efficient policy learning algorithms for estimating the quantile-optimal policy when there exists unmeasured confounding."
    },
    {
        "title": "InstructBrush: Learning Attention-based Visual Instruction for Image Editing",
        "link_suffix": "/forum?id=dSjCFKiEdK",
        "link": "https://openreview.net/forum?id=dSjCFKiEdK",
        "pdf_link": "https://openreview.net/pdf?id=dSjCFKiEdK",
        "keywords": "Image Editing.+Visual In-Context Learning.+Diffusion Models",
        "abstract": "Diffusion-based image editing methods have garnered significant attention in image editing. However, despite encompassing a wide range of editing priors, these methods are helpless when handling editing tasks that are challenging for users to accurately describe. We propose InstructBrush, an inversion method for instruction-based image editing methods to bridge this gap. It extracts editing effects from example image pairs as editing instructions to guide the editing of new images. Two key techniques are introduced into InstructBrush, Attention-based Instruction Optimization and Transformation-oriented Instruction Initialization, to address the limitations of the previous method in terms of inversion effects and instruction generalization. To explore the ability of visual prompt editing methods to guide image editing in open scenarios, we establish a Transformation-Oriented Paired Benchmark (TOP-Bench). Quantitatively and qualitatively, our approach achieves superior performance in editing and is more semantically consistent with the target editing effects. The code and benchmark will be released upon acceptance."
    },
    {
        "title": "High-dimension Prototype is a Better Incremental Object Detection Learner",
        "link_suffix": "/forum?id=6T8czSBWce",
        "link": "https://openreview.net/forum?id=6T8czSBWce",
        "pdf_link": "https://openreview.net/pdf?id=6T8czSBWce",
        "keywords": "Object Detection; Incremental Learning; Prototype Learning",
        "abstract": "Incremental object detection (IOD), surpassing simple classification, requires the simultaneous overcoming of catastrophic forgetting in both recognition and localization tasks, primarily due to the significantly higher feature space complexity. Integrating Knowledge Distillation (KD) would mitigate the occurrence of catastrophic forgetting. However, the challenge of knowledge shift caused by invisible previous task data hampers existing KD-based methods, leading to limited improvements in IOD performance. This paper aims to alleviate knowledge shift by enhancing the accuracy and granularity in describing complex high-dimensional feature spaces. To this end, we put forth a novel higher-dimension-prototype learning approach for KD-based IOD, enabling a more flexible, accurate, and fine-grained representation of feature distributions without the need to retain any previous task data. Existing prototype learning methods calculate feature centroids or statistical Gaussian distributions as prototypes, disregarding actual irregular distribution information or leading to inter-class feature overlap, which is not directly applicable to the more difficult task of IOD with complex feature space. To address the above issue, we propose a Gaussian Mixture Distribution-based Prototype (GMDP), which explicitly models the distribution relationships of different classes by directly measuring the likelihood of embedding from new and old models into class distribution prototypes in a higher dimension manner. Specifically, GMDP  dynamically adapts the component weights and corresponding means/variances of class distribution prototypes to represent both intra-class and inter-class variability more accurately. Progressing into a new task, GMDP constrains the distance between the distribution of new and previous task classes, minimizing overlap with existing classes and thus striking a balance between stability and adaptability. GMDP can be readily integrated into existing IOD methods to enhance performance further. Extensive experiments on the PASCAL VOC and MS-COCO show that our method consistently exceeds four baselines by a large margin and significantly outperforms other state-of-the-art results under various incremental settings. Source codes are included in supplementary materials."
    },
    {
        "title": "HG-Adapter: Improving Pre-Trained Heterogeneous Graph Neural Networks with Dual Adapters",
        "link_suffix": "/forum?id=AEglX9CHFN",
        "link": "https://openreview.net/forum?id=AEglX9CHFN",
        "pdf_link": "https://openreview.net/pdf?id=AEglX9CHFN",
        "keywords": "Heterogeneous graph, Pre-trained models, Adapter-tuning",
        "abstract": "The \"pre-train, prompt-tuning'' paradigm has demonstrated impressive  performance for tuning pre-trained heterogeneous graph neural networks (HGNNs) by mitigating the gap between pre-trained models and downstream tasks. However, most prompt-tuning-based works may face at least two limitations: (i) the model may be insufficient to fit the graph structures well as they are generally ignored in the prompt-tuning stage, increasing the training error to decrease the generalization ability; and (ii) the model may suffer from the limited labeled data during the prompt-tuning stage, leading to a large generalization gap between the training error and the test error to further affect the model generalization. To alleviate the above limitations, we first derive the generalization error bound for existing prompt-tuning-based methods, and then propose a unified framework that combines two new adapters with potential labeled data extension to improve the generalization of pre-trained HGNN models. Specifically, we design dual structure-aware adapters to adaptively fit task-related homogeneous and heterogeneous structural information. We further design a label-propagated contrastive loss and two self-supervised losses to optimize dual adapters and incorporate unlabeled nodes as potential labeled data. Theoretical analysis indicates that the proposed method achieves a lower generalization error bound than existing methods, thus obtaining superior generalization ability. Comprehensive experiments demonstrate the effectiveness and generalization of the proposed method on different downstream tasks."
    },
    {
        "title": "Self-supervised Transfer Learning via Adversarial Contrastive Training",
        "link_suffix": "/forum?id=JGTYlyVogb",
        "link": "https://openreview.net/forum?id=JGTYlyVogb",
        "pdf_link": "https://openreview.net/pdf?id=JGTYlyVogb",
        "keywords": "Unsupervised transfer learning, adversarial contrastive training, deep neural network, end to end error",
        "abstract": "Learning a data representation with strong transferability from an unlabeled scenario is both crucial and challenging. In this paper, we propose a novel unbiased self-supervised transfer learning approach via Adversarial Contrastive Training (ACT). Additionally, we establish an end-to-end theoretical understanding for self-supervised contrastive pretraining and its implications for downstream classification tasks in a misspecified, over-parameterized setting. Our theoretical findings highlight the provable advantages of adversarial contrastive training in the source domain towards improving the accuracy of downstream tasks in the target domain. Furthermore, we illustrate that downstream tasks necessitate only a minimal sample size when working with a well-trained representation, offering valuable insights on few-shot learning. Moreover, extensive experiments across various datasets demonstrate a significant enhancement in classification accuracy when compared to existing state-of-the-art self-supervised learning methods."
    },
    {
        "title": "EVA-Gaussian: 3D Gaussian-based Real-time Human Novel View Synthesis under Diverse Camera Settings",
        "link_suffix": "/forum?id=7vV8KZ7VEl",
        "link": "https://openreview.net/forum?id=7vV8KZ7VEl",
        "pdf_link": "https://openreview.net/pdf?id=7vV8KZ7VEl",
        "keywords": "Fast Human Reconstruction; Generalizable 3D Gaussian Splatting",
        "abstract": "The feed-forward based 3D Gaussian Splatting method has demonstrated exceptional capability in real-time human novel view synthesis. However, existing approaches are restricted to dense viewpoint settings, which limits their flexibility in free-viewpoint rendering across a wide range of camera view angle discrepancies. To address this limitation, we propose a real-time pipeline named EVA-Gaussian for 3D human novel view synthesis across diverse camera settings. Specifically, we first introduce an Efficient cross-View Attention (EVA) module to accurately estimate the position of each 3D Gaussian from the source images. Then, we integrate the source images with the estimated Gaussian position map to predict the attributes and feature embeddings of the 3D Gaussians. Moreover, we employ a recurrent feature refiner to correct artifacts caused by geometric errors in position estimation and enhance visual fidelity. To further improve synthesis quality, we incorporate a powerful anchor loss function for both 3D Gaussian attributes and human face landmarks. Experimental results on the THuman2.0 and THumansit datasets showcase the superiority of our EVA-Gaussian approach in rendering quality across diverse camera settings. Project page:https://anonymousiclr2025.github.io/iclr2025/EVA-Gaussian."
    },
    {
        "title": "Benchmarking and Analyzing Monocular Geometry Estimation Models",
        "link_suffix": "/forum?id=jGGylopiO8",
        "link": "https://openreview.net/forum?id=jGGylopiO8",
        "pdf_link": "https://openreview.net/pdf?id=jGGylopiO8",
        "keywords": "generative and discrimative pretrain, monocular geometry estimation, benchmarks",
        "abstract": "Recent advances in discriminative and generative pretraining have yielded geometry estimation foundation models with strong generalization capabilities. While most discriminative monocular geometry estimation methods rely on large-scale finetuning data to achieve zero-shot generalization, several generative-based paradigms show the potential of achieving impressive generalization performance on unseen scenes by leveraging pre-trained diffusion models and fine-tuning on even a small-scale of synthetic training data. Frustratingly, these models are trained with different recipes on different datasets, making it hard to find out the critical factors that determine the evaluation performance. To resolve the above issue, (1) we build fair and strong baselines in a unified codebase for evaluating and analyzing the state-of-the-art (SOTA) geometry estimation models from pre-training style, finetuning data, and model architecture perspectives; (2) we thoroughly evaluate geometry models on challenging benchmarks with diverse scenes and high-quality annotations. Under the fair training and evaluation configuration, our results reveal that stochastic diffusion-based protocol is not optimal for fine-tuning generative-based geometry estimation methods. One-step finetuning and inference protocol is sufficient for generative-based depth and surface normal estimation. Besides, we find that both discriminative and generative pretraining can generalize well under small-scale fine-tuning high-quality data in scale-invariant depth estimation task. DINOv2-pretrained discriminative models achieve slightly higher performance than generative counterparts with the same small amount of synthetic data. Furthermore, we have observed that metric depth estimation requires significantly more finetuning data than scale-invariant depth estimation for learning the depth scale distribution. We hope this work will inspire future geometry estimation research in building more high-quality fine-tuning datasets and designing more powerful geometry estimation models."
    },
    {
        "title": "HyperPLR: Hypergraph Generation through Projection, Learning, and Reconstruction",
        "link_suffix": "/forum?id=TYnne6Pa35",
        "link": "https://openreview.net/forum?id=TYnne6Pa35",
        "pdf_link": "https://openreview.net/pdf?id=TYnne6Pa35",
        "keywords": "hypergraph, graph generation, clique cover problem",
        "abstract": "Hypergraphs are essential in modeling higher-order complex networks, excelling in representing group interactions within real-world contexts. This is particularly evident in collaboration networks, where they facilitate the capture of groupwise polyadic patterns, extending beyond traditional pairwise dyadic interactions. The use of hypergraph generators, or generative models, is a crucial method for promoting and validating our understanding of these structures. If such generators accurately replicate observed hypergraph patterns, it reinforces the validity of our interpretations. In this context, we introduce a novel hypergraph generative paradigm, \\textbf{HyperPLR}, encompassing three phases: Projection, Learning, and Reconstruction. Initially, the hypergraph is projected onto a weighted graph. Subsequently, the model learns this graph's structure within a latent space, while simultaneously computing a distribution between the hyperedge and the projected graph. Finally, leveraging the learned model and distribution, HyperPLR generates new weighted graphs and samples cliques from them. These cliques are then used to reconstruct new hypergraphs by solving a specific clique cover problem.\nWe have evaluated HyperPLR on existing real-world hypergraph datasets, which consistently demonstrate superior performance and validate the effectiveness of our approach."
    },
    {
        "title": "FARV: Leveraging Facial and Acoustic Representation in Vocoder For Video-to-Speech Synthesis",
        "link_suffix": "/forum?id=5fRlsiNDZR",
        "link": "https://openreview.net/forum?id=5fRlsiNDZR",
        "pdf_link": "https://openreview.net/pdf?id=5fRlsiNDZR",
        "keywords": "Video-to-speech (V2S), vocoder, speech synthesis",
        "abstract": "In this paper, we introduce FARV, a vocoder specifically designed for Video-to-Speech (V2S) synthesis, which integrates both facial embeddings and acoustic units to generate speech waveforms. By sharing the acoustic unit vocabulary in our two-stage V2S pipeline, FARV effectively bridges the domain gap between the visual frontend and the vocoder without requiring finetuning. Furthermore, by embedding visual speaker images into the acoustic unit representations, FARV enhances its ability to preserve speaker identity. Experimental results demonstrate that FARV achieves leading scores in intelligibility and strikes a favorable balance between speaker characterisitcs preservation and acoustic quality, making it well-suited for practical V2S applications."
    },
    {
        "title": "MAGNET: Augmenting Generative Decoders with Representation Learning and Infilling Capabilities",
        "link_suffix": "/forum?id=XC0nEtnevb",
        "link": "https://openreview.net/forum?id=XC0nEtnevb",
        "pdf_link": "https://openreview.net/pdf?id=XC0nEtnevb",
        "keywords": "decoder-only LLMs, representation learning, text infilling, generation, unified model",
        "abstract": "While originally designed for unidirectional generative modeling, decoder-only large language models (LLMs) are increasingly being adapted for bidirectional modeling. However, these unidirectional and bidirectional models are typically trained independently with distinct objectives (generation or representation learning) thereby missing the potential opportunity for one objective to enhance the other. In this work, we introduce MAGNET, an adaptation of decoder-only LLMs that enhances their capabilities in generating robust representations and infilling missing text spans, while retaining their original text generation capabilities. MAGNET employs three self-supervised training objectives and introduces an attention mechanism that combines bidirectional and causal attention, enabling unified training across all objectives. We show that LLMs adapted using MAGNET can outperform state-of-the-art text encoders on token-level and sentence-level representation learning tasks. We also demonstrate that MAGNET enhances the base LLM's ability to generate contextually appropriate text infillings by enabling it to take future context into consideration. Lastly, we show that, unlike other bidirectional language models for representation learning, the LLMs adapted using MAGNET can still perform open-ended text generation."
    },
    {
        "title": "Probabilistic Token Alignment for Large Language Model Fusion",
        "link_suffix": "/forum?id=ksBhCsSUaE",
        "link": "https://openreview.net/forum?id=ksBhCsSUaE",
        "pdf_link": "https://openreview.net/pdf?id=ksBhCsSUaE",
        "keywords": "Large Language Models, Model Fusion",
        "abstract": "With the drive of scaling laws, the size of large language models (LLMs) continues to grow, causing a burden when constructing stronger baselines from scratch. In response, we propose a more robust baseline by employing knowledge fusion under the model fusion paradigm. However, a key challenge in existing knowledge fusion approaches is their dependence on manually predefined alignment strategies, which may not generalize well across diverse contexts, leading to performance degradation in several zero-shot evaluation tasks. To address this challenge, we draw inspiration from distribution learning and propose theprobabilistic token alignmentmethod as a general and soft mapping solution for alignment, resulting inPTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem:optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibitsinterpretability from a distributional perspective, offering insights into the essence of the token alignment task. Our approach is validated across diverse benchmarks and tasks using three prominent LLMs with distinct architectures\u2014Llama-2, MPT, and OpenLLaMA. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities."
    },
    {
        "title": "Unveiling AI's Blind Spots: An Oracle for In-Domain, Out-of-Domain, and Adversarial Errors",
        "link_suffix": "/forum?id=MZ324wU7Hj",
        "link": "https://openreview.net/forum?id=MZ324wU7Hj",
        "pdf_link": "https://openreview.net/pdf?id=MZ324wU7Hj",
        "keywords": "Error Prediction, AI reliability, In-domain, Out-of-Domain, Adversarial Attack",
        "abstract": "AI models make mistakes when recognizing images\u2014whether in-domain, out-of-domain, or adversarial. Predicting these errors is critical for improving system reliability, reducing costly mistakes, and enabling proactive corrections in real-world applications such as healthcare, finance, and autonomous systems. However, understanding what mistakes AI models make, why they occur, and how to predict them remains an open challenge. Here, we conduct comprehensive empirical evaluations using a \"mentor\" model \u2014a deep neural network designed to predict another model\u2019s errors. Our findings show that the mentor model excels at learning from a mentee's mistakes on adversarial images with small perturbations and generalizes effectively to predict in-domain and out-of-domain errors of the mentee. Additionally, transformer-based mentor models excel at predicting errors across various mentee architectures. Subsequently, we draw insights from these observations and develop an \"oracle\" mentor model, dubbed SuperMentor, that achieves 78% accuracy in predicting errors across different error types. Our error prediction framework paves the way for future research on anticipating and correcting AI model behaviours, ultimately increasing trust in AI systems. All code, models, and data will be made publicly available."
    },
    {
        "title": "Continuity-Preserving Autoencoders for Learning Continuous Latent Dynamical Models from Images",
        "link_suffix": "/forum?id=MxALfOAnXv",
        "link": "https://openreview.net/forum?id=MxALfOAnXv",
        "pdf_link": "https://openreview.net/pdf?id=MxALfOAnXv",
        "keywords": "Latent dynamical system, Autoencoders, Learning dynamics, Continuity-preserving",
        "abstract": "Continuous dynamical systems are cornerstones of many scientific and engineering disciplines.\nWhile machine learning offers powerful tools to model these systems from trajectory data, challenges arise when these trajectories are captured as images, resulting in pixel-level observations that are discrete in nature.\nConsequently, a naive application of a convolutional autoencoder can result in latent coordinates that are discontinuous in time.\nTo resolve this, we propose continuity-preserving autoencoders (CpAEs) to learn continuous latent states and their corresponding continuous latent dynamical models from discrete image frames. \nWe present a mathematical formulation for learning dynamics from image frames, which illustrates issues with previous approaches and motivates our methodology based on promoting the continuity of convolution filters, thereby preserving the continuity of the latent states.\nThis approach enables CpAEs to produce latent states that evolve continuously with the underlying dynamics, leading to more accurate latent dynamical models.\nExtensive experiments across various scenarios demonstrate the effectiveness of CpAEs."
    },
    {
        "title": "Teaching with Uncertainty: Unleashing the Potential of Knowledge Distillation in Object Detection",
        "link_suffix": "/forum?id=NrDUhtIWsY",
        "link": "https://openreview.net/forum?id=NrDUhtIWsY",
        "pdf_link": "https://openreview.net/pdf?id=NrDUhtIWsY",
        "keywords": "Object Detection, Knowledge Distillation, Uncertainty Estimation",
        "abstract": "Knowledge distillation (KD) has become a fundamental technique for model compression in object detection tasks. The data noise and training randomness may cause the knowledge of the teacher model to be unreliable, referred to as knowledge uncertainty. Existing methods only transfer this knowledge and could limit the student's ability to capture and understand the potential ``dark knowledge''. In this work, we introduce a new strategy that explicitly incorporates knowledge uncertainty, named Uncertainty-Driven Knowledge Extraction and Transfer (UET). Given that the knowledge distribution is unknown and high-dimensional in practice, we introduce a simple yet effective sampling method with Monte Carlo dropout (MC dropout)  to estimate the teacher\u2019s knowledge uncertainty. Leveraging information theory, we integrate knowledge uncertainty into the conventional KD process, allowing the student model to benefit from knowledge diversity. UET is a plug-and-play method that integrates seamlessly with existing distillation techniques. We validate our approach through comprehensive experiments across various distillation strategies, detectors, and backbones. Specifically, UET achieves state-of-the-art results, with a ResNet50-based GFL detector obtaining 44.1% mAP on the COCO dataset\u2014surpassing baseline performance by 3.9%."
    },
    {
        "title": "SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects",
        "link_suffix": "/forum?id=OdMqKszKSd",
        "link": "https://openreview.net/forum?id=OdMqKszKSd",
        "pdf_link": "https://openreview.net/pdf?id=OdMqKszKSd",
        "keywords": "3D articulated objects creation; generative model",
        "abstract": "We address the challenge of creating 3D assets for household articulated objects from a single image.\nPrior work on articulated object creation either requires multi-view multi-state input, or only allows coarse control over the generation process.\nThese limitations hinder the scalability and practicality for articulated object modeling.\nIn this work, we propose a method to generate articulated objects from a single image.\nObserving the object in a resting state from an arbitrary view, our method generates an articulated object that is visually consistent with the input image.\nTo capture the ambiguity in part shape and motion posed by a single view of the object, we design a diffusion model that learns the plausible variations of objects in terms of geometry and kinematics.\nTo tackle the complexity of generating structured data with attributes in multiple domains, we design a pipeline that produces articulated objects from high-level structure to geometric details in a coarse-to-fine manner, where we use a part connectivity graph and part abstraction as proxies.\nOur experiments show that our method outperforms the state-of-the-art in articulated object creation by a large margin in terms of the generated object realism, resemblance to the input image, and reconstruction quality."
    },
    {
        "title": "Dynamic Mixture-of-Experts for Incremental Graph Learning",
        "link_suffix": "/forum?id=EZExZ5d8ES",
        "link": "https://openreview.net/forum?id=EZExZ5d8ES",
        "pdf_link": "https://openreview.net/pdf?id=EZExZ5d8ES",
        "keywords": "Graph neural networks; Incremental Learning",
        "abstract": "Graph incremental learning is a learning paradigm that aims to adapt models trained on previous data to continuously incremented data or tasks over time without the need for retraining on the full dataset. However, regular graph machine learning methods suffer from catastrophic forgetting when applied to incremental learning settings, where previously learned knowledge is overridden by new knowledge. Previous approaches have tried to address this by treating the previously trained model as an inseparable unit and using regularization, experience replay, and parameter isolation to maintain old behaviors while learning new knowledge.\nThese approaches, however, do not account for the fact that not all previously acquired knowledge is equally beneficial for learning new tasks, and maintaining all previous knowledge and the latest knowledge in a single model is ineffective. Some prior patterns can be transferred to help learn new data, while others may deviate from the new data distribution and be detrimental. To address this, we propose a dynamic mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a DyMoE GNN layer adds new expert networks specialized in modeling the incoming data blocks. We design a customized regularization loss that utilizes data sequence information so existing experts can maintain their ability to solve old tasks while helping the new expert learn the new data effectively. As the number of data blocks grows over time, the computational cost of the full mixture-of-experts (MoE) model increases. To address this, we introduce a sparse MoE approach, where only the top-$k$ most relevant experts make predictions, significantly reducing the computation time. Our model achieved 5.47% relative accuracy increase compared to the best baselines on class incremental learning with minimal computation increase, showing the model's exceptional power."
    },
    {
        "title": "ActiveAD: Planning-Oriented Active Learning for End-to-End Autonomous Driving",
        "link_suffix": "/forum?id=7Zppme1swQ",
        "link": "https://openreview.net/forum?id=7Zppme1swQ",
        "pdf_link": "https://openreview.net/pdf?id=7Zppme1swQ",
        "keywords": "Active Learning, Autonomous Driving",
        "abstract": "End-to-end differentiable learning has emerged as a prominent paradigm in autonomous driving (AD). A significant bottleneck in this approach is its substantial demand for high-quality labeled data, such as 3D bounding boxes and semantic segmentation, which are especially expensive to annotate manually. This challenge is exacerbated by the long tailed distribution in AD datasets, where a substantial portion of the collected data might be trivial (e.g. simply driving straight on a straight road) and only a minority of instances are critical to safety.  In this paper, we propose ActiveAD, a planning-oriented active learning strategy designed to enhance sampling and labeling efficiency in end-to-end autonomous driving. ActiveAD progressively annotates parts of collected raw data based on our newly developed metrics. We design innovative diversity metrics to enhance initial sample selection, addressing the cold-start problem. Furthermore, we develop uncertainty metrics to select valuable samples for the ultimate purpose of route planning during subsequent batch selection. Empirical results demonstrate that our approach significantly surpasses traditional active learning methods. Remarkably, our method achieves comparable results to state-of-the-art end-to-end AD methods - by using only 30% data in both open-loop nuScenes and closed-loop CARLA evaluation."
    },
    {
        "title": "Scaling Multimodal Theory-of-Mind with Weak-to-Strong Bayesian Reasoning",
        "link_suffix": "/forum?id=HHKboqbkec",
        "link": "https://openreview.net/forum?id=HHKboqbkec",
        "pdf_link": "https://openreview.net/pdf?id=HHKboqbkec",
        "keywords": "Neuro-symbolic concept, logic and forming reasoning, theory-of-mind",
        "abstract": "Theory of Mind (ToM) enables individuals to understand and predict thoughts, emotions, and intentions of the others. To replicate this cognitive ability in machines, especially under complex multimodal environments, recent advances combine Bayesian-based state inference with deep learning models to estimate mental states, where the Bayesian model handles state transitions and a language model (LM) estimates the likelihood of intermediate states. However, while post-training an LM to specialise in ToM tasks improves performance, the computational cost increases as the LM scales, limiting the model size to 7 billion parameters. Despite this post-training process, smaller LMs still struggle with the physical and mental modelling demands of ToM due to their limited world knowledge and reasoning capacity. To address this, we propose a scalable solution that leverages the strengths of larger LMs (up to 70 and 405 billion parameters, respectively), including their vast world knowledge and atomic-level reasoning capabilities, without increasing post-training resource requirements. Our method transfers ToM-specific behaviours from a post-trained small LM to guide the latent reasoning of a larger LM during test time. This weak-to-strong control mechanism enables the larger LM to improve Bayesian likelihood estimation at each inference step, harnessing its reasoning power in ToM scenarios while reducing the need for additional training resources. Extensive experiments demonstrate the significant effectiveness of our scaled approach. It is better at inferring human mental states in complex and interactive environments, outperforming the state-of-the-art solution by $\\sim4.6$% across multiple tasks on the multimodal ToM benchmark and unseen scenarios."
    },
    {
        "title": "Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport",
        "link_suffix": "/forum?id=gQlxd3Mtru",
        "link": "https://openreview.net/forum?id=gQlxd3Mtru",
        "pdf_link": "https://openreview.net/pdf?id=gQlxd3Mtru",
        "keywords": "optimal transport, Schr\u00f6dinger bridge, trajectory inference, single-cell",
        "abstract": "Reconstructing dynamics using samples from sparsely time-resolved snapshots is an important problem in both natural sciences and machine learning. Here, we introduce a new deep learning approach for solving regularized unbalanced optimal transport (RUOT) and inferring continuous unbalanced stochastic dynamics from observed snapshots. Based on the RUOT form, our method models these dynamics without requiring prior knowledge of growth and death processes or additional information, allowing them to be learnt directly from data.  Theoretically, we  explore the connections between the RUOT and Schr\u00f6dinger bridge problem and discuss the key challenges and potential solutions. The effectiveness of our method is demonstrated with a synthetic gene regulatory network, high-dimensional Gaussian Mixture Model, and single-cell RNA-seq data from blood development. Compared with other methods, our approach accurately identifies growth and transition patterns, eliminates false transitions, and constructs the Waddington developmental landscape."
    }
]