[{"title": "On Re-Encoding Short-Term Memory of Large Language Models in Conversations", "link_suffix": "/forum?id=sRrHy0wetR", "link": "https://openreview.net/forum?id=sRrHy0wetR", "pdf_link": "https://openreview.net/pdf?id=sRrHy0wetR", "keywords": "LLM, misinformation correction, zero-shot self-correction", "abstract": "Large language models (LLMs), such as GPT-4, are adept at generating coherent and fluent responses within conversational contexts. \nHowever, there has been a paucity of comprehensive research exploring LLMs to dynamically update their knowledge in response to corrections of misinformation provided by users during dialogue sessions. \nFrom the cognitive psychology perspective, such an adaptive process is akin to memory re-encoding (MRE), which entails the modification of previously stored information in human memory, typically for rectifying inaccuracies.In this paper, we present a novel framework termed Knowledge Editing In Conversation (KEIC), along with an accompanying dataset, devised to assess the efficacy of LLMs in emulating the MRE process in an in-context setting.\nThrough in-depth investigations, we observe that the contemporary LLMs exhibit a modicum of proficiency in this task.\nTo enhance their in-context MRE abilities, we propose a structured strategy to handle the information update for LLMs in a multi-turn conversation.\nWe demonstrate that our approach is effective and suggest insights for research communities in this emerging and essential issue.", "title_embedding_index": 22550, "title_abs_embedding_index": 22575}, {"title": "Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction", "link_suffix": "/forum?id=Ombm8S40zN", "link": "https://openreview.net/forum?id=Ombm8S40zN", "pdf_link": "https://openreview.net/pdf?id=Ombm8S40zN", "keywords": "Discrete diffusion models, language modeling, probabilistic inference", "abstract": "Generative modeling of discrete data underlies important applications spanning text-based agents like ChatGPT to the design of the very building blocks of life in protein sequences. However, application domains need to exert control over the generated data by steering the generative process\u2014typically via RLHF\u2014to satisfy a specified property, reward, or affinity metric. In this paper, we study the problem of steering Masked Diffusion Models (MDMs), a recent class of discrete diffusion models that offer a compelling alternative to traditional autoregressive models. We introduce Discrete Denoising Posterior Prediction (DDPP), a novel framework that casts the task of steering pretrained MDMs as a problem of probabilistic inference by learning to sample from a target Bayesian posterior. Our DDPP framework leads to a family of three novel objectives that are all simulation-free, and thus scalable while applying to general non-differentiable reward functions. Empirically, we instantiate DDPP by steering MDMs to perform class-conditional pixel-level image modeling, RLHF-based alignment of MDMs using text based rewards, and finetuning protein language models to generate more diverse secondary structures and shorter proteins. We substantiate our designs via wet-lab validation, where we observe transient expression of reward-optimized protein sequences.", "title_embedding_index": 22551, "title_abs_embedding_index": 22576}, {"title": "Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation", "link_suffix": "/forum?id=TD3SGJfBC7", "link": "https://openreview.net/forum?id=TD3SGJfBC7", "pdf_link": "https://openreview.net/pdf?id=TD3SGJfBC7", "keywords": "Distribution shifts, Visual prompt, Foundation model, Few-Shot Test-Time Domain Adaptation", "abstract": "Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time to a specific domain using only a few unlabeled examples, addressing domain shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities by generating domain-specific prompts to guide its generalized, frozen features. However, since downstream datasets are not explicitly seen by CLIP, solely depending on the feature space knowledge is constrained by CLIP's prior knowledge. Notably, when using a less robust backbone like ViT-B/16, performance significantly drops on challenging real-world benchmarks. Departing from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP, this work introduces learning directly on the input space to complement the dataset-specific knowledge for frozen CLIP. Specifically, an independent side branch is attached in parallel with CLIP and enforced to learn exclusive knowledge via revert attention.  To better capture the dataset-specific label semantics for downstream adaptation, we propose to enhance the inter-dispersion among text features via greedy text ensemble and refinement. The text and visual features are then progressively fused in a domain-aware manner by a generated domain prompt to adapt toward a specific domain. Extensive experiments show our method's superiority on 5 large-scale benchmarks (WILDS and DomainNet), notably improving over smaller networks like ViT-B/16 with gains of \\textbf{+5.1} in F1 for iWildCam and \\textbf{+3.1%} in WC Acc for FMoW.", "title_embedding_index": 22552, "title_abs_embedding_index": 22577}, {"title": "Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution", "link_suffix": "/forum?id=zl0HLZOJC9", "link": "https://openreview.net/forum?id=zl0HLZOJC9", "pdf_link": "https://openreview.net/pdf?id=zl0HLZOJC9", "keywords": "learning to defer, expectation - maximisation", "abstract": "Recent progress in machine learning research is gradually shifting its focus towardshuman - AI cooperationdue to the advantages of exploiting the reliability of human experts and the efficiency of AI models. One of the promising approaches in human - AI cooperation islearning to defer(L2D), where the system analyses the input data and decides to make its own decision or defer to human experts. Although L2D has demonstrated state-of-the-art performance, in its standard setting, L2D entails a severe limitation: all human experts must annotate the whole training dataset of interest, resulting in a slow and expensive annotation process which can subsequently influence the size and diversity of the training set. Moreover, the current L2D does not have a principled way to control workload distribution among human experts and the AI classifier that is important to optimise resource allocation. We, therefore, propose a new probabilistic modelling approach inspired from mixture-of-experts, where the Expectation - Maximisation algorithm is leveraged to address the issue of missing expert's annotations. Furthermore, we introduce a constraint, which can be solved efficiently during the E-step, to control the workload distribution among human experts and the AI classifier. Empirical evaluation on synthetic and real-world datasets show that our proposed probabilistic approach performs competitively, or even surpasses previously proposed methods assessed on the same benchmarks.", "title_embedding_index": 22553, "title_abs_embedding_index": 22578}, {"title": "Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis", "link_suffix": "/forum?id=A4aG3XeIO7", "link": "https://openreview.net/forum?id=A4aG3XeIO7", "pdf_link": "https://openreview.net/pdf?id=A4aG3XeIO7", "keywords": "Bilevel Optimization, Tuning-Free, Adaptive Optimization", "abstract": "Bilevel optimization has recently attracted considerable attention due to its abundant applications in machine learning problems. However, existing methods rely on prior knowledge of problem parameters to determine stepsizes, resulting in significant effort in tuning stepsizes when these parameters are unknown. In this paper, we propose two novel tuning-free algorithms, D-TFBO and S-TFBO. D-TFBO employs a double-loop structure with stepsizes adaptively adjusted by the \"inverse of cumulative gradient norms\" strategy. S-TFBO features a simpler fully single-loop structure that updates three variables simultaneously with a theory-motivated joint design of adaptive stepsizes for all variables. We provide a comprehensive convergence analysis for both algorithms and show that D-TFBO and S-TFBO respectively require $\\mathcal{O}(\\frac{1}{\\epsilon})$ and $\\mathcal{O}(\\frac{1}{\\epsilon}\\log^4(\\frac{1}{\\epsilon}))$ iterations to find an $\\epsilon$-accurate stationary point, (nearly) matching their well-tuned counterparts using the information of problem parameters. Experiments on various problems show that our methods achieve performance comparable to existing well-tuned approaches, while being more robust to the selection of initial stepsizes. \nTo the best of our knowledge, our methods are the first to completely eliminate the need for stepsize tuning, while achieving theoretical guarantees.", "title_embedding_index": 22554, "title_abs_embedding_index": 22579}, {"title": "Scalable Ensemble Diversification for OOD Generalization and Detection", "link_suffix": "/forum?id=YvtiTdXjfn", "link": "https://openreview.net/forum?id=YvtiTdXjfn", "pdf_link": "https://openreview.net/pdf?id=YvtiTdXjfn", "keywords": "diverse, ensemble, scalable, robustness, uncertainty, OOD detection, OOD generalization", "abstract": "Training a diverse ensemble of models has several practical applications such as providing candidates for model selection with better out-of-distribution (OOD) generalization, and enabling the detection of OOD samples via Bayesian principles.\nAn existing approach to diverse ensemble training encourages the models to disagree on provided OOD samples. However, the approach is computationally expensive and it requires well-separated ID and OOD examples, such that it has only been demonstrated in small-scale settings.Method.This work presents a Hardness-based Diversification Regularizer (HDR)\napplicable to large-scale settings (e.g. ImageNet)\nthat does not require OOD samples.\nInstead, HDR identifies hard training samples on the fly\nand encourages the ensemble members to disagree on these.\nTo improve scaling, we show how to avoid the expensive computations in existing methods of exhaustive pairwise disagreements across models.Results.We evaluate the benefits of diversification with experiments on ImageNet.\nFirst, for OOD generalization, we observe large benefits from the diversification in multiple settings including output-space (classical) ensembles and weight-space ensembles (model soups).\nSecond, for OOD detection, we turn the diversity of ensemble hypotheses into a novel uncertainty score estimator that surpasses a large number of OOD detection baselines.", "title_embedding_index": 22555, "title_abs_embedding_index": 22580}, {"title": "From discrete-time policies to continuous-time diffusion samplers: Asymptotic equivalences and faster training", "link_suffix": "/forum?id=1hT2fsHbK9", "link": "https://openreview.net/forum?id=1hT2fsHbK9", "pdf_link": "https://openreview.net/pdf?id=1hT2fsHbK9", "keywords": "diffusion, variational inference, SDEs, PDEs, sampling, stochastic processes, GFlowNets", "abstract": "We study the problem of training neural stochastic differential equations, or diffusion models, to sample from a Boltzmann distribution without access to target samples. Existing methods for training such models enforce time-reversal of the generative and noising processes, using either differentiable simulation or off-policy reinforcement learning (RL). We prove equivalences between families of objectives in the limit of infinitesimal discretization steps, linking entropic RL methods (GFlowNets) with continuous-time objects (partial differential equations and path space measures). We further show that an appropriate choice of coarse time discretization during training allows greatly improved sample efficiency and the use of time-local objectives, achieving competitive performance on standard sampling benchmarks with reduced computational cost.", "title_embedding_index": 22556, "title_abs_embedding_index": 22581}, {"title": "CHG Shapley: Efficient Data Valuation and Selection towards Trustworthy Machine Learning", "link_suffix": "/forum?id=uVMZgtw2pf", "link": "https://openreview.net/forum?id=uVMZgtw2pf", "pdf_link": "https://openreview.net/pdf?id=uVMZgtw2pf", "keywords": "Data Valuation, Shapley Value, Data selection", "abstract": "Understanding the decision-making process of machine learning models is crucial for ensuring trustworthy machine learning. Data Shapley, a landmark study on data valuation, advances this understanding by assessing the contribution of each datum to model performance. However, the resource-intensive and time-consuming nature of multiple model retraining poses challenges for applying Data Shapley to large datasets. To address this, we propose the CHG (compound of Hardness and Gradient) utility function, which approximates the utility of each data subset on model performance in every training epoch. By deriving the closed-form Shapley value for each data point using the CHG utility function, we reduce the computational complexity to that of a single model retraining, achieving a quadratic improvement over existing marginal contribution-based methods. We further leverage CHG Shapley for real-time data selection, conducting experiments across three settings: standard datasets, label noise datasets, and class imbalance datasets. These experiments demonstrate its effectiveness in identifying high-value and noisy data. By enabling efficient data valuation, CHG Shapley promotes trustworthy model training through a novel data-centric perspective.", "title_embedding_index": 22557, "title_abs_embedding_index": 22582}, {"title": "Second-Order Forward-Mode Automatic Differentiation for Optimization", "link_suffix": "/forum?id=XW4Xnx0xlH", "link": "https://openreview.net/forum?id=XW4Xnx0xlH", "pdf_link": "https://openreview.net/pdf?id=XW4Xnx0xlH", "keywords": "Optimization, Automatic Differentiation", "abstract": "Forward gradient methods offer a promising alternative to backpropagation. Optimization that only requires forward passes could simplify hardware implementation, improve parallelism, lower memory cost, and allow for more biologically plausible learning models. This has motivated recent forward-mode automated differentiation (AD) methods. This paper presents a novel second-order forward-mode AD method for optimization that generalizes a second-order line search to a $K$-dimensional hyperplane. Unlike recent work that relies on directional derivatives (or Jacobian\u2013Vector Products, JVPs), we use hyper-dual numbers to jointly evaluate both directional derivatives and their second-order quadratic terms. As a result, we introduce forward-mode weight perturbation with Hessian information for K-dimensional hyper-plane search (FoMoH-$K$D). We derive the convergence properties of FoMoH-$K$D and show how it generalizes to Newton\u2019s method for $K = D$. We demonstrate this generalization empirically, and compare the performance of FoMoH-$K$D to forward gradient descent (FGD) on three case studies: Rosenbrock function used widely for evaluating optimization methods, logistic regression with 7,850 parameters, and learning a CNN classifier with 431,080 parameters. Our experiments show that FoMoH-$K$D not only achieves better performance and accuracy, but also converges faster, thus, empirically verifying our theoretical results.", "title_embedding_index": 22558, "title_abs_embedding_index": 22583}, {"title": "ParetoFlow: Guided Flows in Multi-Objective Optimization", "link_suffix": "/forum?id=mLyyB4le5u", "link": "https://openreview.net/forum?id=mLyyB4le5u", "pdf_link": "https://openreview.net/pdf?id=mLyyB4le5u", "keywords": "Multi-objective optimization; flow matching; classifier guidance.", "abstract": "In offline multi-objective optimization (MOO), we leverage an offline dataset of designs and their associated labels to simultaneously minimize multiple objectives. This setting more closely mirrors complex real-world problems compared to single-objective optimization. Recent works mainly employ evolutionary algorithms and Bayesian optimization, with limited attention given to the generative modeling capabilities inherent in such data. In this study, we explore generative modeling in offline MOO through flow matching, noted for its effectiveness and efficiency. We introduce a \\textit{ParetoFlow} method, specifically designed to guide flow sampling to approximate the Pareto front. Traditional predictor~(classifier) guidance is inadequate for this purpose because it models only a single objective. In response, we propose a \\textit{multi-objective predictor guidance} module that \nassigns each sample a weight vector, representing a weighted distribution across multiple objective predictions. A local filtering scheme is introduced to address non-convex Pareto fronts. These weights uniformly cover the entire objective space, effectively directing sample generation towards the Pareto front. Since distributions with similar weights tend to generate similar samples, we introduce a \\textit{neighboring evolution} module to foster knowledge sharing among neighboring distributions. This module generates offspring from these distributions, and selects the most promising one for the next iteration. Our method achieves state-of-the-art performance across various tasks. Our code is available.", "title_embedding_index": 22559, "title_abs_embedding_index": 22584}, {"title": "ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning", "link_suffix": "/forum?id=YZEzVR5awV", "link": "https://openreview.net/forum?id=YZEzVR5awV", "pdf_link": "https://openreview.net/pdf?id=YZEzVR5awV", "keywords": "model compression, quantization, efficient finetuning, llm", "abstract": "As large language models (LLMs) continue to scale, model compression becomes increasingly important for enabling edge deployment and ensuring accessibility to users with limited resources. Weight-only quantization is a key technique for model compression, allowing for a substantial reduction in model size while preserving performance. However, as bit-width decreases, the performance of quantized LLMs tends to degrade significantly. Additionally, due to the non-differentiable operation in quantization, standard finetuning on quantized LLMs is unsupported, and alternative finetuning approaches often fail to match the effectiveness of full finetuning. In this paper, we introduce ClusComp, a novel and simple model compression paradigm. ClusComp first clusters the weight matrices to generate codebooks, and then tunes these codebooks block-by-block to reconstruct intermediate activations. Despite its simplicity, ClusComp (1) consistently achieves better performance in 2-4 bit precision; (2) pushes the compression limit to the 1-bit level, and outperforms existing ultra-low-bit methods with limited finetuning steps; (3) facilitates seamless and efficient finetuning, surpasses existing quantization-based or memory-efficient finetuning methods, and even rivals full finetuning of the FP16 model. Notably, these procedures can be executed on a single NVIDIA A6000-48GB GPU for LLMs with as many as 70B parameters.", "title_embedding_index": 22560, "title_abs_embedding_index": 22585}, {"title": "Covariances for Free: Exploiting Mean Distributions for Federated Learning with Pre-trained Models", "link_suffix": "/forum?id=7NtAIghBsE", "link": "https://openreview.net/forum?id=7NtAIghBsE", "pdf_link": "https://openreview.net/pdf?id=7NtAIghBsE", "keywords": "Federated Learning, Transfer Learning", "abstract": "Using pre-trained models has been found to reduce the effect of data heterogeneity and speed up federated learning algorithms. Recent works have investigated the use of first-order statistics and second-order statistics to aggregate local client data distributions at the server and achieve very high performance without any training. In this work we propose a training-free method based on an unbiased estimator of class covariance matrices. Our method, which only uses first-order statistics in the form of class means communicated by clients to the server, incurs only a fraction of the communication costs required by methods based on communicating second-order statistics. We show how these estimated class covariances can be used to initialize a linear classifier, thus exploiting the covariances without actually sharing them. When compared to state-of-the-art methods which also share only class means, our approach improves performance in the range of 4-26% with exactly the same communication cost. Moreover, our method achieves performance competitive or superior to sharing second-order statistics with dramatically less communication overhead. Finally, using our method to initialize classifiers and then performing federated fine-tuning yields better and faster convergence.", "title_embedding_index": 22561, "title_abs_embedding_index": 22586}, {"title": "DET: Learn to Solve the Tunnel Traveling Salesmen Problem using Double-Encoder Transformer", "link_suffix": "/forum?id=2YzeOOjvOi", "link": "https://openreview.net/forum?id=2YzeOOjvOi", "pdf_link": "https://openreview.net/pdf?id=2YzeOOjvOi", "keywords": "Combinatorial Optimization; Transformer; Deep Reinforcement Learning; Tunnel TSP", "abstract": "We delve into a challenging variant of the Traveling Salesman Problem (TSP), namely tunnel TSP, which incorporates a new important constraint requiring the traversal of a prescribed set of tunnels. While traditional deep reinforcement learning (DRL) based neural TSP algorithms excel in optimizing routes without tunnel restrictions, they often struggle to achieve optimal performance in tunnel TSP due to the neglect of the crucial role of tunnel attributes during solution generation. To address this challenge, we propose a simple but effective and flexible technique, called Double-Encoder Transformer (DET), which can be seamlessly integrated into various existing autoregressive neural TSP solvers. DET processes node and tunnel location information separately and encodes them in two distinct feature spaces. Following an efficient fusion strategy, DET then integrates the encoded information from nodes and tunnels, harnessing their intricate interactions. Experimental validation demonstrates that integrating DET into existing autoregressive neural solvers significantly improves performance, enabling us to reduce the average optimality gap for tunnel TSP from 12.58% (of the previous Single-Encoder model) to 7.35%.", "title_embedding_index": 22562, "title_abs_embedding_index": 22587}, {"title": "Quantized Spike-driven Transformer", "link_suffix": "/forum?id=5J9B7Sb8rO", "link": "https://openreview.net/forum?id=5J9B7Sb8rO", "pdf_link": "https://openreview.net/pdf?id=5J9B7Sb8rO", "keywords": "Spiking Neural Network+Spike-driven+Quantized Spiking Transformer+ Neuromorphic Computing", "abstract": "Spiking neural networks (SNNs) are emerging as a promising energy-efficient alternative to traditional artificial neural networks (ANNs) due to their spike-driven paradigm.\nHowever, recent research in the SNN domain has mainly focused on enhancing accuracy by designing large-scale Transformer structures, which typically rely on substantial computational resources, limiting their deployment on resource-constrained devices.\nTo overcome this challenge, we propose a quantized spike-driven Transformer baseline (QSD-Transformer), which achieves reduced resource demands by utilizing a low bit-width parameter. \nRegrettably, the QSD-Transformer often suffers from severe performance degradation.\nIn this paper, we first conduct empirical analysis and find that the bimodal distribution of quantized spike-driven self-attention (Q-SDSA) leads to spike information distortion (SID) during quantization, causing significant performance degradation. To mitigate this issue, we take inspiration from mutual information entropy and propose a bi-level optimization strategy to rectify the information distribution in Q-SDSA.\nSpecifically, at the lower level, we introduce an information-enhanced LIF to rectify the information distribution in Q-SDSA.\nAt the upper level, we propose a fine-grained distillation scheme for the QSD-Transformer to align the distribution in Q-SDSA with that in the counterpart ANN.\nBy integrating the bi-level optimization strategy, the QSD-Transformer can attain enhanced energy efficiency without sacrificing its high-performance advantage.\nWe validate the QSD-Transformer on various visual tasks, and experimental results indicate that our method achieves state-of-the-art results in the SNN domain.\nFor instance, when compared to the prior SNN benchmark on ImageNet, the QSD-Transformer achieves 80.3% top-1 accuracy, accompanied by significant reductions of 6.0$\\times$ and 8.1$\\times$ in power consumption and model size, respectively.", "title_embedding_index": 22563, "title_abs_embedding_index": 22588}, {"title": "S4D: Streaming 4D Real-World Reconstruction with Gaussians and 3D Control Points", "link_suffix": "/forum?id=k3Z8CnHdfg", "link": "https://openreview.net/forum?id=k3Z8CnHdfg", "pdf_link": "https://openreview.net/pdf?id=k3Z8CnHdfg", "keywords": "3D Gaussian, Dynamic Reconstruction, Novel View Synthesis", "abstract": "Dynamic scene reconstruction using Gaussians has recently attracted increased interest. Mainstream approaches typically employ a global deformation field to warp a 3D scene in canonical space. However, the inherent low-frequency nature of implicit neural fields often leads to ineffective representations of complex motions. Moreover, their structural rigidity can hinder adaptation to scenes with varying resolutions and durations.\nTo address these challenges, we introduce a novel approach for streaming 4D real-world reconstruction utilizing discrete 3D control points. This method physically models local rays and establishes a motion-decoupling coordinate system. By effectively merging traditional graphics with learnable pipelines, it provides a robust and efficient local 6-degrees-of-freedom (6-DoF) motion representation. Additionally, we have developed a generalized framework that integrates our control points with Gaussians.\nStarting from an initial 3D reconstruction, our workflow decomposes the streaming 4D reconstruction into four independent submodules: 3D segmentation, 3D control point generation, object-wise motion manipulation, and residual compensation. Experimental results demonstrate that our method outperforms existing state-of-the-art 4D Gaussian splatting techniques on both the Neu3DV and CMU-Panoptic datasets. \nNotably, the optimization of our 3D control points is achievable in 100 iterations and within just 2 seconds per frame on a single NVIDIA 4070 GPU.", "title_embedding_index": 22564, "title_abs_embedding_index": 22589}, {"title": "Mat\u00e9rn Kernels for Tunable Implicit Surface Reconstruction", "link_suffix": "/forum?id=Ox4AJ2Vurb", "link": "https://openreview.net/forum?id=Ox4AJ2Vurb", "pdf_link": "https://openreview.net/pdf?id=Ox4AJ2Vurb", "keywords": "surface reconstruction, kernel methods, neural tangent kernel", "abstract": "We propose to use the family of Mat\u00e9rn kernels for tunable implicit surface reconstruction, building upon the recent success of kernel methods for 3D reconstruction of oriented point clouds.\nAs we show, both, from a theoretical and practical perspective, Mat\u00e9rn kernels have some appealing properties which make them particularly well suited for surface reconstruction---outperforming state-of-the-art methods based on the arc-cosine kernel while being significantly easier to implement, faster to compute, and scaleable.\nBeing stationary, we demonstrate that the Mat\u00e9rn kernels' spectrum can be tuned in the same fashion as Fourier feature mappings help coordinate-based MLPs to overcome spectral bias. \nMoreover, we theoretically analyze Mat\u00e9rn kernel's connection to SIREN networks as well as its relation to previously employed arc-cosine kernels. \nFinally, based on recently introduced Neural Kernel Fields, we present data-dependent Mat\u00e9rn kernels and conclude that especially the Laplace kernel (being part of the Mat\u00e9rn family) is extremely competitive, performing almost on par with state-of-the-art methods in the noise-free case while having a more than five times shorter training time.", "title_embedding_index": 22565, "title_abs_embedding_index": 22590}, {"title": "Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs", "link_suffix": "/forum?id=9nUBh4V6SA", "link": "https://openreview.net/forum?id=9nUBh4V6SA", "pdf_link": "https://openreview.net/pdf?id=9nUBh4V6SA", "keywords": "Self-driving laboratories, protocol design, automated design, domain-specific language", "abstract": "Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand of rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, we propose a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. We further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.", "title_embedding_index": 22566, "title_abs_embedding_index": 22591}, {"title": "CrossQuant: A Post-Training Quantization Method with Smaller Quantization Kernel for Precise Large Lanugage Model Compression", "link_suffix": "/forum?id=igiQUYs53F", "link": "https://openreview.net/forum?id=igiQUYs53F", "pdf_link": "https://openreview.net/pdf?id=igiQUYs53F", "keywords": "Post-Training Quantization, Weight-Activation Quantization, Quantization Kernel", "abstract": "Post-Training Quantization (PTQ) is an effective technique for compressing Large Language Models (LLMs). While many studies focus on quantizing both weights and activations, it is still a challenge to maintain the accuracy of LLM after activating quantization. To investigate the primary cause, we extend the concept of kernel from linear algebra to quantization functions to define a new term, \"quantization kernel\", which refers to the set of elements in activations that are quantized to zero. Through quantitative analysis of the quantization kernel, we find that these elements are crucial for maintaining the accuracy of quantized LLMs. With the decrease of quantization kernel, the precision of quantized LLMs increases. If the quantization kernel proportion is kept below 19% for OPT models and below 1% for LLaMA models, the precision loss from quantizing activations to INT8 becomes negligible. Motivated by the goal of developing a quantization method with small quantization kernel, we propose CrossQuant\u2014a simple yet effective method for quantizing activations. CrossQuant cross-quantizes elements using row and column-wise absolute maximum vectors, achieving a quantization kernel of approximately 16% for OPT models and less than 0.1% for LLaMA models. Experimental results on LLMs (LLaMA, OPT) ranging from 6.7B to 70B parameters demonstrate that CrossQuant improves or maintains perplexity and accuracy in language modeling, zero-shot, and few-shot tasks.", "title_embedding_index": 22567, "title_abs_embedding_index": 22592}, {"title": "Leveraging MLLM Embeddings and Attribute Smoothing for Compositional Zero-Shot Learning", "link_suffix": "/forum?id=8gSrJOL2oc", "link": "https://openreview.net/forum?id=8gSrJOL2oc", "pdf_link": "https://openreview.net/pdf?id=8gSrJOL2oc", "keywords": "Compositional zero-shot learning, visual disentanglement", "abstract": "Compositional zero-shot learning (CZSL) aims to recognize novel compositions of attributes and objects learned from seen compositions. Previous works disentangle attribute and object by extracting shared and exclusive parts between image pairs sharing the same attribute (object), as well as aligning them with pretrained word embeddings to improve unseen attribute-object recognition. Despite the significant achievements of existing efforts, they are hampered by three limitations: (1) the efficacy of disentanglement is compromised due to the influence of the background and the intricate entanglement of attribute with object in the same parts. (2) existing word embeddings fail to capture complex multimodal semantic information. (3) overconfidence exhibited by existing models in seen compositions hinders their generalization to novel compositions. Being aware of these, we propose a novel framework named Multimodal Large Language Model (MLLM) embeddings and attribute smoothing guided disentanglement (TRIDENT) for CZSL. First, we leverage feature adaptive aggregation (FAA) modules to mitigate the impact of background, and utilize learnable condition masks to capture multi-granularity features for subsequent disentanglement. Then, the last hidden states of MLLM are employed as word embeddings for their superior representation capabilities. Moreover, we propose attribute smoothing through leveraging auxiliary attributes generated by Large Language Model (LLM) for each seen composition, addressing the issue of overconfidence by encouraging the model to learn more attributes in one given composition instead of just fitting a fixed attribute-object combination. Extensive experiments demonstrate that TRIDENT achieves state-of-the-art performance on three challenging datasets: MIT-States, C-GQA, and VAW-CZSL, respectively.", "title_embedding_index": 22568, "title_abs_embedding_index": 22593}, {"title": "Generative Lines Matching Models", "link_suffix": "/forum?id=a5EFuQuuPb", "link": "https://openreview.net/forum?id=a5EFuQuuPb", "pdf_link": "https://openreview.net/pdf?id=a5EFuQuuPb", "keywords": "Generative Flow, Denoising Score Matching, Denoising Diffusion Probability Models", "abstract": "In this paper we identify the source of a singularity in the training loss of key denoising models, that causes the denoiser's predictions to collapse towards the mean of the source or target distributions. This degeneracy creates false basins of attraction, distorting the denoising trajectories and ultimately increasing the number of steps required to sample these models.We circumvent this artifact by leveraging the deterministic ODE-based samplers, offered by certain denoising diffusion and score-matching models, which establish a well-defined change-of-variables between the source and target distributions. Given this correspondence, we propose a new probability flow model, the Lines Matching Model (LMM), which matches globally straight lines interpolating the two distributions. We demonstrate that the flow fields produced by the LMM exhibit notable temporal consistency, resulting in trajectories with excellent straightness scores.Beyond its sampling efficiency, the LMM formulation allows us to enhance the fidelity of the generated samples by integrating domain-specific reconstruction and adversarial losses, and by optimizing its training for the sampling procedure used. Overall, the LMM achieves state-of-the-art FID scores with minimal NFEs on established benchmark datasets: 1.57/1.39 (NFE=1/2) on CIFAR-10, 1.47/1.17 on ImageNet 64x64, and 2.68/1.54 on AFHQ 64x64.Finally, we provide a theoretical analysis showing that the use of optimal transport to relate the two distributions suffers from a curse of dimensionality, where the pairing set size (mini-batch) must scale exponentially with the signal dimension.", "title_embedding_index": 22569, "title_abs_embedding_index": 22594}, {"title": "Networked Communication for Decentralised Agents in Mean-Field Games", "link_suffix": "/forum?id=o4byGNa98y", "link": "https://openreview.net/forum?id=o4byGNa98y", "pdf_link": "https://openreview.net/pdf?id=o4byGNa98y", "keywords": "mean-field games, reinforcement learning, communication network, multi-agent systems", "abstract": "We introduce networked communication to the mean-field game framework, in particular to oracle-free settings where $N$ decentralised agents learn along a single, non-episodic run of the empirical system. We prove that our architecture has sample guarantees bounded between those of the centralised- and independent-learning cases. We provide the order of the difference in these bounds in terms of network structure and number of communication rounds, and also contribute a policy-update stability guarantee. We discuss how the sample guarantees of the three theoretical algorithms do not actually result in practical convergence. We therefore show that in practical settings where the theoretical parameters are not observed (leading to poor estimation of the Q-function), our communication scheme significantly accelerates convergence over the independent case (and sometimes even the centralised case), without relying on the assumption of a centralised learner. We contribute further practical enhancements to all three theoretical algorithms, allowing us to present their first empirical demonstrations. Our experiments confirm that we can remove several of the theoretical assumptions of the algorithms, and display the empirical convergence benefits brought by our new networked communication. We additionally show that the networked approach has significant advantages, over both the centralised and independent alternatives, in terms of robustness to unexpected learning failures and to changes in population size.", "title_embedding_index": 22570, "title_abs_embedding_index": 22595}, {"title": "MMG-VL: A Vision-Language Driven Approach for Multi-Person Motion Generation", "link_suffix": "/forum?id=B5AN6IRyXc", "link": "https://openreview.net/forum?id=B5AN6IRyXc", "pdf_link": "https://openreview.net/pdf?id=B5AN6IRyXc", "keywords": "Human Motion Generation; VLM; 3D Generative Models", "abstract": "Generating realistic 3D human motion is crucial in the frontier applications of embodied intelligence, such as human-computer interaction and virtual reality. However, existing methods that rely solely on text or initial human pose inputs struggle to capture the rich semantic understanding and interaction with the environment, and most focus on single-person motion generation, neglecting the needs of multi-person scenarios. To address these challenges, we propose the VL2Motion generation paradigm, which combines natural language instruction and environmental visual inputs to generate realistic 3D human motion. The visual inputs not only provide precise analysis of spatial layouts and environmental details but also incorporate inherent 3D spatial and world knowledge constraints to ensure that the generated motions are natural and contextually appropriate in real-world scenarios. Building on this, we introduce MMG-VL, a novel Multi-person Motion Generation approach driven by Vision and Language for generating 3D human motion in multi-room home scenarios. This approach employs a two-stage pipeline: first, it uses Vision-Language Auxiliary Instruction (VILA) module to integrate multimodal input information and generate multi-human motion instructions that align with real-world constraints; second, it utilizes Scenario-Interaction Diffusion (SID) module to accurately generate multiple human motions. Our experiments demonstrate the superiority of the VL2Motion paradigm in environmental perception and interaction, as well as the effectiveness of MMG-VL in generating multi-human motions in multi-room home scenarios. Additionally, we have released a complementary HumanVL dataset, containing 584 multi-room household images and 35,622 human motion samples, aiming to further advance innovation and development in this domain.", "title_embedding_index": 22571, "title_abs_embedding_index": 22596}, {"title": "Mechanism and emergence of stacked attention heads in multi-layer transformers", "link_suffix": "/forum?id=rUC7tHecSQ", "link": "https://openreview.net/forum?id=rUC7tHecSQ", "pdf_link": "https://openreview.net/pdf?id=rUC7tHecSQ", "keywords": "mechanistic interpretability, large language models, transformers, emergent abilities, curriculum learning", "abstract": "In this paper, we introduce the retrieval problem, a simple reasoning task that can be solved only by transformers with a minimum number of layers. The task has an adjustable difficulty that can further increase the required number of layers to any arbitrary value. We demonstrate that large language models can solve the task under different prompting formulations without any fine-tuning. To understand how transformers solve the retrieval problem, we train several transformers on a minimal formulation. We find that successful learning occurs only under the presence of an implicit curriculum. We uncover the learned mechanisms by studying the attention maps in the trained transformers. We also study the training process, uncovering that attention heads always emerge in a specific sequence.", "title_embedding_index": 22572, "title_abs_embedding_index": 22597}, {"title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset", "link_suffix": "/forum?id=N0ts5QGEW9", "link": "https://openreview.net/forum?id=N0ts5QGEW9", "pdf_link": "https://openreview.net/pdf?id=N0ts5QGEW9", "keywords": "Medical Report Generation; X-ray Image Pre-training; State Space Model; Benchmark", "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence which can significantly reduce diagnostic burdens and patient wait times. Despite significant progress, we believe that the task has reached a bottleneck due to the limited benchmark datasets and the existing large models' insufficient capability enhancements in this specialized domain. Specifically, the recently released CheXpert Plus dataset lacks comparative evaluation algorithms and their results, providing only the dataset itself. This situation makes the training, evaluation, and comparison of subsequent algorithms challenging. Thus, we conduct a comprehensive benchmarking of existing mainstream X-ray report generation models and large language models (LLMs), on the CheXpert Plus dataset. We believe that the proposed benchmark can provide a solid comparative basis for subsequent algorithms and serve as a guide for researchers to quickly grasp the state-of-the-art models in this field. More importantly, we propose a big model for the X-ray image report generation using a multi-stage pre-training strategy, including self-supervised autoregressive generation and Xray-report contrastive learning, and supervised fine-tuning. Extensive experimental results indicate that the autoregressive pre-training based on Mamba effectively encodes X-ray images, and the image-text contrastive pre-training further aligns the feature spaces, achieving better experimental results. All the source codes will be released upon acceptance.", "title_embedding_index": 22573, "title_abs_embedding_index": 22598}, {"title": "Can a Bayesian oracle prevent harm from an agent?", "link_suffix": "/forum?id=2Oh2EOcFSO", "link": "https://openreview.net/forum?id=2Oh2EOcFSO", "pdf_link": "https://openreview.net/pdf?id=2Oh2EOcFSO", "keywords": "AI safety, probabilistic guarantees, guardrails, safe-by-design AI, Bayesian inference, posterior convergence", "abstract": "Is there a way to design powerful AI systems based on machine learning methods that would satisfy probabilistic safety guarantees? With the long-term goal of obtaining a probabilistic guarantee that would apply in every context, we consider estimating a context-dependent bound on the probability of violating a given safety specification.  Such a risk evaluation would need to be performed at run-time to provide a guardrail against dangerous actions of an AI. Noting that different plausible hypotheses about the world could produce very different outcomes, and because we do not know which one is right, we derive bounds on the safety violation probability predicted  under the true but unknown hypothesis. Such bounds could be used to reject potentially dangerous actions. Our main results involve searching for cautious but plausible hypotheses, obtained by a maximization that involves Bayesian posteriors over hypotheses. We consider two forms of this result, in the i.i.d. case and in the non-i.i.d. case, and conclude with open problems towards turning such theoretical results into practical AI guardrails.", "title_embedding_index": 22574, "title_abs_embedding_index": 22599}]