[
    {
        "title": "DeepNT: Path-Centric Graph Neural Networks for Network Tomography",
        "link_suffix": "/forum?id=pQOHbTpAwf",
        "link": "https://openreview.net/forum?id=pQOHbTpAwf",
        "pdf_link": "https://openreview.net/pdf?id=pQOHbTpAwf",
        "keywords": "Network Tomography, Graph Structure Learning",
        "abstract": "Network tomography is a crucial problem in network monitoring, where observable path measurements are used to infer unmeasured network properties, making it essential for tasks such as route selection, fault diagnosis, and traffic control. However, most existing methods assume that the network topology is fully known\u2014an assumption that rarely holds in practice. The incomplete topology introduces significant challenges in extracting path information for predicting path performance. Furthermore, these approaches are typically designed for a single path performance metric and lack the flexibility to handle multiple metrics that may be of interest in a single network. To address these limitations, we propose Deep Network Tomography (DeepNT), a new framework that simultaneously infers the adjacency matrix and predicts unmeasured paths by leveraging Graph Neural Networks (GNNs) as backbones to learn path-centric end-node pair representations under incomplete network topology. To ensure that the learned adjacency matrix aligns with the characteristics of real-world networks, we propose a novel learning objective that constrains the model in terms of connectivity, sparsity, and path performance bounds, enabling robust generalization across a variety of performance metrics. Extensive experiments on real-world and synthetic datasets demonstrate the superiority of DeepNT in predicting performance metrics and inferring graph topology compared to state-of-the-art methods."
    },
    {
        "title": "Learning Evolving Tools for Large Language Models",
        "link_suffix": "/forum?id=wtrDLMFU9v",
        "link": "https://openreview.net/forum?id=wtrDLMFU9v",
        "pdf_link": "https://openreview.net/pdf?id=wtrDLMFU9v",
        "keywords": "Tool Learning, Monte Calro Tree Search, Large Language Models",
        "abstract": "Tool learning enables large language models (LLMs) to interact with external tools and APIs, greatly expanding the application scope of LLMs. However, due to the dynamic nature of external environments, these tools and APIs may become outdated over time, preventing LLMs from correctly invoking tools. Existing research primarily focuses on static environments and overlooks this issue, limiting the adaptability of LLMs in real-world applications. In this paper, we propose ToolEVO, a novel framework designed to enhance the adaptive and reflective capabilities of LLMs against tool variability. By leveraging Monte Carlo Tree Search, ToolEVO facilitates active exploration and interaction of LLMs within dynamic environments, allowing for autonomous self-reflection and self-updating of tool usage based on environmental feedback. Additionally, we introduce ToolQA-D, a benchmark specifically designed to evaluate the impact of tool variability. Extensive experiments demonstrate the effectiveness and stability of our approach, highlighting the importance of adaptability to tool variability for effective tool learning."
    },
    {
        "title": "Towards Effective Evaluations and Comparison for LLM Unlearning Methods",
        "link_suffix": "/forum?id=wUtCieKuQU",
        "link": "https://openreview.net/forum?id=wUtCieKuQU",
        "pdf_link": "https://openreview.net/pdf?id=wUtCieKuQU",
        "keywords": "llm unlearning",
        "abstract": "The imperative to eliminate undesirable data memorization underscores the significance of machine unlearning for large language models (LLMs). Recent research has introduced a series of promising unlearning methods, notably boosting the practical significance of the field. Nevertheless, adopting a proper evaluation framework to reflect the true unlearning efficacy is also essential yet has not received adequate attention. This paper seeks to improve the evaluation of LLM unlearning by addressing two key challenges---a) the robustness of evaluation metrics and b) the trade-offs between competing goals. The first challenge stems from findings that current metrics are susceptible to various red teaming scenarios. It indicates that they may not reflect the true extent of knowledge retained by LLMs but rather tend to mirror superficial model behaviors, thus prone to attacks. We address this issue by devising and assessing a series of candidate metrics, selecting the most robust ones under various types of attacks. The second challenge arises from the conflicting goals of eliminating unwanted knowledge while retaining those of others. This trade-off between unlearning and retention often fails to conform the Pareto frontier, rendering it subtle to compare the efficacy between methods that excel only in either unlearning or retention.  We handle this issue by proposing a calibration method that can restore the original performance on non-targeted data after unlearning, thereby allowing us to focus exclusively on assessing the strength of unlearning. Our evaluation framework notably enhances the effectiveness when assessing and comparing various LLM unlearning methods, further allowing us to benchmark existing works, identify their proper hyper-parameters, and explore new tricks to enhance their practical efficacy."
    },
    {
        "title": "Q-Mamba: Towards more efficient Mamba models via Post-Training Quantization",
        "link_suffix": "/forum?id=AY1S52vr0a",
        "link": "https://openreview.net/forum?id=AY1S52vr0a",
        "pdf_link": "https://openreview.net/pdf?id=AY1S52vr0a",
        "keywords": "Mamba, Quantization",
        "abstract": "State Space Models (SSMs), such as Mamba, have recently demonstrated the potential to match or even surpass Transformers in language understanding tasks, making them a promising alternative for designing Large Language Models (LLMs). \nConcurrently, model quantization, especially Post-Training Quantization (PTQ), has been proven effective in reducing memory usage and inference latency in LLMs.\nIn this paper, we explore post-training quantization for Mamba (\\textbf{Q-Mamba}) by turning both linear projections and state caches into low-bit integers for efficient inference. \nAfter a theoretical analysis of the causes of outliers in states, we propose \\textbf{Decoupled Scale Quantization (DSQ)}, which mitigates outliers in both the state and channel dimensions by applying separate quantization scales.\nTo preserve the selective ability of quantized Mamba, we introduce \\textbf{Efficient Selectivity Reconstruction (ESR)}, a block-wise reconstruction method that involves a novel quantization simulation scheme, enabling fast parallel scan algorithms with the non-linear quantization function.\nWe demonstrate the effectiveness of Q-Mamba across various quantization settings, model sizes, and both generation and zero-shot tasks. \nIn particular, for Mamba2-2.7B with W8A8H4 quantization, Q-Mamba achieves a 50% reduction in memory consumption with only a 2.13% average accuracy degradation on zero-shot tasks."
    },
    {
        "title": "EmoAttack: Emotion-to-Image Diffusion Models for Emotional Backdoor Generation",
        "link_suffix": "/forum?id=cQCrBJHy0C",
        "link": "https://openreview.net/forum?id=cQCrBJHy0C",
        "pdf_link": "https://openreview.net/pdf?id=cQCrBJHy0C",
        "keywords": "Emotion;Backdoor Attack;Diffusion model;Personalization",
        "abstract": "Text-to-image diffusion models can generate realistic images based on textual inputs, enabling users to convey their opinions visually through language. Meanwhile, within language, emotion plays a crucial role in expressing personal opinions in our daily and the inclusion of maliciously negative content can lead users astray, exacerbating negative emotions. Recognizing the success of  diffusion models and the significance of emotion, we investigate a previously overlooked risk associated with text-to-image diffusion models, that is, utilizing emotion in the input texts to introduce negative content and provoke unfavorable emotions in users. Specifically, we identify a new backdoor attack, i.e., emotion-aware backdoor attack (EmoAttack), which introduces malicious negative content triggered by emotional texts during image generation. We formulate such an attack as a diffusion personalization problem to avoid extensive model retraining and propose the \\textit{EmoBooth}. Unlike existing personalization methods, our approach fine-tunes a pre-trained diffusion model by establishing a mapping between a cluster of emotional words and a given reference image containing malicious negative content. To validate the effectiveness of our method, we built a dataset and conducted extensive analysis and discussion about its effectiveness. Given consumers' widespread use of diffusion models, uncovering this threat is critical for society."
    },
    {
        "title": "SoloPose: One-Stage Kinematic 3D Human Pose Estimation with Mocap Data Augmentation",
        "link_suffix": "/forum?id=uBxN9JA29p",
        "link": "https://openreview.net/forum?id=uBxN9JA29p",
        "pdf_link": "https://openreview.net/pdf?id=uBxN9JA29p",
        "keywords": "monocular pose estimation, data augmentation",
        "abstract": "While recent two-stage many-to-one deep learning models have demonstrated great success in 3D human pose estimation, such models are inefficient in 3D key point detection and also tend to pass on first stage errors onto the second stage. In this paper, we introduce SoloPose, a novel one-stage, many-to-many spatio-temporal transformer model for kinematic 3D human pose estimation of video. SoloPose is further fortified by HeatPose, a 3D heatmap based on Gaussian Mixture Model distributions that factors target key points as well as kinematically adjacent key points. Finally, we address data diversity constraints with the 3D AugMotion Toolkit, a methodology to augment existing 3D human pose datasets, specifically by projecting four top public 3D human pose datasets (Human3.6M, MADS, AIST Dance++, MPI INF 3DHP) into a novel dataset (Human7.1M) with a universal coordinate system. Extensive experiments are conducted on both Human3.6M and the augmented Human7.1M dataset, and SoloPose demonstrates superior results relative to the state-of-the-art approaches."
    },
    {
        "title": "Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning",
        "link_suffix": "/forum?id=BfUugGfBE5",
        "link": "https://openreview.net/forum?id=BfUugGfBE5",
        "pdf_link": "https://openreview.net/pdf?id=BfUugGfBE5",
        "keywords": "reinforcement learning, in-context learning",
        "abstract": "Recent studies have demonstrated that Transformers can perform in-context reinforcement learning (RL) by imitating a source RL algorithm. This enables them to adapt to new tasks in a sample-efficient manner without parameter updates. However, since the Transformers are trained to mimic the source algorithm, they also reproduce its suboptimal behaviors. Model-based planning offers a promising solution to this limitation by allowing the agents to simulate potential outcomes before taking action, providing an additional mechanism to deviate from the source algorithm's behavior. Rather than learning a separate dynamics model, we propose Distillation for In-Context Planning (DICP), an in-context model-based RL framework where the Transformer simultaneously learns environment dynamics and improves policy in-context. With experiments across a diverse set of discrete and continuous environments such as Darkroom variants and Meta-World, we show that this method achieves state-of-the-art performance, requiring significantly fewer environmental interactions than the baselines including both in-context model-free counterparts and existing meta-RL methods."
    },
    {
        "title": "CamI2V: Camera-Controlled Image-to-Video Diffusion Model",
        "link_suffix": "/forum?id=dIZB7jeSUv",
        "link": "https://openreview.net/forum?id=dIZB7jeSUv",
        "pdf_link": "https://openreview.net/pdf?id=dIZB7jeSUv",
        "keywords": "Camera Control, Image-to-video, Video Diffusion Model",
        "abstract": "Recently, camera pose, as a user-friendly and physics-related condition, has been introduced into text-to-video diffusion model for camera control. However, existing methods simply inject camera conditions through a side input (like T2I-Adapter). These approaches neglect the inherent physical knowledge of camera pose, resulting in imprecise camera control, inconsistencies, and also poor interpretability.\nIn this paper, we emphasize the necessity of integrating explicit physical constraints into model design. We propose to apply epipolar attention for modeling all cross-frame relationships from a novel perspective of noised condition. This ensures that features are aggregated from corresponding epipolar lines in all noised frames, which overcomes the limitations of current attention mechanisms in tracking displaced features across frames, especially when features move significantly with the camera and become obscured by noise.\nAdditionally, we introduce register tokens to handle cases without intersections between frames, commonly caused by rapid camera movements, dynamic objects, or occlusions. To support image-to-video, we propose the multiple guidance scale to allow for precise control for image, text, and camera, respectively. Furthermore, we establish a more robust and reproducible evaluation pipeline to solve the inaccuracy and instability of existing camera control measurement caused by the limitations of structure-from-motion (SfM). Experimental results on RealEstate10K and out-of-domain datasets demonstrate that our approach achieves state-of-the-art performance. Checkpoints and training/evaluation codes are planned for release."
    },
    {
        "title": "Chain-of-Focus Prompting: Leveraging Sequential Visual Cues to Prompt Large Autoregressive Vision Models",
        "link_suffix": "/forum?id=noidywkBba",
        "link": "https://openreview.net/forum?id=noidywkBba",
        "pdf_link": "https://openreview.net/pdf?id=noidywkBba",
        "keywords": "Autoregressive Large Vision Models, Visual In-context Learning, Prompt learning",
        "abstract": "In-context learning (ICL) has revolutionized natural language processing by enabling models to adapt to diverse tasks with only a few illustrative examples. However, the exploration of ICL within the field of computer vision remains limited. Inspired by Chain-of-Thought (CoT) prompting in the language domain, we propose Chain-of-Focus (CoF) Prompting, which enhances vision models by enabling step-by-step visual comprehension. CoF Prompting addresses the challenges of absent logical structure in visual data by generating intermediate reasoning steps through visual saliency. Moreover, it provides a solution for creating tailored prompts from visual inputs by selecting contextually informative prompts based on query similarity and target richness. The significance of CoF prompting is demonstrated by the recent introduction of Large Autoregressive Vision Models (LAVMs), which predict downstream targets via in-context learning with pure visual inputs. By integrating intermediate reasoning steps into visual prompts and effectively selecting the informative ones, the LAVMs are capable of generating significantly better inferences. Extensive experiments on downstream visual understanding tasks validate the effectiveness of our proposed method for visual in-context learning."
    },
    {
        "title": "KV-Dict: Sparse KV Cache Compression with Universal Dictionaries",
        "link_suffix": "/forum?id=FkXYvV7nEB",
        "link": "https://openreview.net/forum?id=FkXYvV7nEB",
        "pdf_link": "https://openreview.net/pdf?id=FkXYvV7nEB",
        "keywords": "transformer, kv cache, compression, quantization",
        "abstract": "Transformer has become the de facto architecture for Large Language Models (LLMs), yet its substantial memory required for long contexts make it costly to deploy. Managing the memory usage of the key-value (KV) cache during inference has become a pressing challenge, as the cache grows with both model size and input length, consuming significant GPU memory.We introduce a novel post-training KV cache compression method using KV-Dict, a universal dictionary that can accurately decompose and reconstruct key-value states. Unlike traditional quantization methods, KV-dict leverages sparse dictionary learning, allowing for flexible memory usage with minimal performance loss through fine-grained controls of sparsity levels. Moreoever, we retain competitive performance in the low memory regimes that 2-bit compression struggles to offer.KV-Dict is remarkably universal, as it uses a small, input-agnostic dictionary that is shared across tasks and batches without scaling memory. This universality, combined with the ability to control sparsity for different memory requirements, offers a flexible and efficient solution to the KV cache bottleneck, maintaining strong performance on complex reasoning tasks, such as LongBench and GSM8k."
    },
    {
        "title": "Visual Perception in Text Strings",
        "link_suffix": "/forum?id=etToTig9Fp",
        "link": "https://openreview.net/forum?id=etToTig9Fp",
        "pdf_link": "https://openreview.net/pdf?id=etToTig9Fp",
        "keywords": "Large Language Model, Multi-modal Large Language Model, Visual Perception, ASCII Art",
        "abstract": "Understanding visual semantics embedded in consecutive characters is a crucial capability for both large language models (LLMs) and multi-modal large language models (MLLMs). This type of artifact possesses the unique characteristic that identical information can be readily formulated in both texts and images, making them a significant proxy for analyzing modern LLMs' and MLLMs' capabilities in modality-agnostic vision understanding. In this work, we select ASCII art as a representative artifact, where the lines and brightness used to depict each concept are rendered by characters, and we frame the problem as an ASCII art recognition task. We benchmark model performance on this task by constructing an evaluation dataset with an elaborate categorization tree and also collect a training set to elicit the models' visual perception ability. Through a comprehensive analysis of dozens of models, results reveal that although humans can achieve nearly 100% accuracy, the state-of-the-art LLMs and MLLMs lag far behind. Models are capable of recognizing concepts depicted in the ASCII arts given only text inputs indicated by over 60% accuracy for some concepts, but most of them achieves merely around 30% accuracy when averaged across all categories. When provided with images as inputs, GPT-4o gets 82.68%, outperforming the strongest open-source MLLM by 21.95%. Although models favor different kinds of ASCII art depending on the modality provided, none of the MLLMs successfully benefit when both modalities are supplied simultaneously. Moreover, supervised fine-tuning helps improve models' accuracy especially when provided with the image modality, but also highlights the need for better training techniques to enhance the information fusion among modalities. All resources are available athttps://anonymous.4open.science/r/VisionInText-08D3."
    },
    {
        "title": "DICR: Direct Intra-image Constrastive Regularization for Contrastive Learning",
        "link_suffix": "/forum?id=Jg9Ol9aVjx",
        "link": "https://openreview.net/forum?id=Jg9Ol9aVjx",
        "pdf_link": "https://openreview.net/pdf?id=Jg9Ol9aVjx",
        "keywords": "Contrastive Learning, Representation Learning, Self-Supervised Learning",
        "abstract": "Typical contrastive self-supervised learning methods apply inter-image contrast to post-projector embeddings, thereby indirectly encouraging the pre-projector representations' invariance to several augmentation operators.\nWhile effective, these methods do not account for the inherent difference between semantics-altering (such as cropping and cutout) and semantics-preserving augmentation operators (such as resizing, flipping and color distortion), and thereby lack an explicit mechanism to encourage distinguishable representations for semantically different contents within the same image.\nWe explain, both in reason and in practice, that these issues can harm the generalizability of the representations in downstream tasks.\nTo address these issues, we propose Direct Intra-image Contrastive Regularization (DICR), a plug-and-play regularization method that directly applies intra-image contrast to pre-projector representations.\nEmpirical results show that DICR can significantly enhance the generalizability of existing methods in downstream tasks, and validate the crucial role of semantic content distinguishability in the generalizable performance of contrastive learning."
    },
    {
        "title": "A Probabilistic Approach to Optimizing Hardware Control Parameters in System Property Estimation using Gumbel-Softmax",
        "link_suffix": "/forum?id=m9BiWVTJDx",
        "link": "https://openreview.net/forum?id=m9BiWVTJDx",
        "pdf_link": "https://openreview.net/pdf?id=m9BiWVTJDx",
        "keywords": "categorical reparameterization, Gumbel Softmax, Optimization, System properties, physics model",
        "abstract": "Optimizing hardware control parameters is crucial for a deeper understanding of the behavior and properties of physical systems. Basically, multiple hardware parameters are simultaneously controlled to generate a signal from physical system. Repetitive acquisitions with different control parameter combinations create distinct signal modulations and then system properties are deduced from prior knowledge of physics-based relationship between modulated signals and control parameters. The choice of control parameters, which determines the characteristics of signal modulation, directly impacts the inverse problem in system property estimation. Thus, the multidimensional control parameter optimization remains an open research topic in many fields for accurate analysis of system. Typically, optimal parameters are determined by iteratively updating sets of control parameters to maximize the estimation accuracy of the physical system properties. However, the conventional optimization process is restricted to explore only the vicinity of control parameters at the current iteration. Therefore, it could highly depend on initialization and current parameters, which might lead to inefficient search when noise is present in the system. In this work, to mitigate this limitation, we propose a novel Gumbel-Softmax-based optimization scheme that enables a probabilistic search across an expanding set of all candidates for each control parameter using categorical reparameterization. As a case study, the proposed method is employed to find optimal hardware control parameters for MRI system. We demonstrate that our Gumbel-Softmax-based optimization simultaneously explores the entire range of hardware control parameters from early iterations and outperforms the conventional optimization approach on accuracy of MR property estimation, especially under noisy environments."
    },
    {
        "title": "Rethinking Structure Learning For Graph Neural Networks",
        "link_suffix": "/forum?id=YERRy6v5uA",
        "link": "https://openreview.net/forum?id=YERRy6v5uA",
        "pdf_link": "https://openreview.net/pdf?id=YERRy6v5uA",
        "keywords": "Graph Neural Networks, Graph Structure Learning",
        "abstract": "Graph Structure Learning (GSL) has been widely employed to enhance the performance of Graph Neural Networks (GNNs). While GSL is believed to capture semantically similar nodes in graphs, there is a lack of theoretical analysis quantifying its necessity. Moreover, GSL typically requires longer training periods and extensive hyperparameter tuning. Recent studies further suggest that, under fair comparisons with the same hyperparameter settings, GSL does not consistently outperform baseline GNNs. This raises a critical question: \\textit{How effective is GSL for GNNs?} To address this question, this paper makes two key contributions. First, we propose a new framework for categorizing existing GSL methods. Second, we analyze the differences in mutual information (MI) between node representations derived from the original topology and those from the newly constructed topology. Surprisingly, our empirical observations and theoretical analysis reveal that, no matter which type of GSL methods are used, after passing the GSL basis \\ie{} the representation used for GSL, to the newly constructed graph, there is no MI gain compared to the original GSL basis. We fairly re-evaluate the effectiveness of GSL by adding GSL to GNN baselines and removing GSL in state-of-the-art models under the same GSL basis. The results show that GSL cannot consistently improve the performance of GNNs. These insights contribute to a deeper understanding of GSL and prompt a re-evaluation of the essential components in the design of GNNs moving forward."
    },
    {
        "title": "BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL",
        "link_suffix": "/forum?id=UnCKU8pZVe",
        "link": "https://openreview.net/forum?id=UnCKU8pZVe",
        "pdf_link": "https://openreview.net/pdf?id=UnCKU8pZVe",
        "keywords": "Multi-Objective Bayesian Optimization, Transformers, Hyperparameter Optimization, Reinforcement Learning, Acquisition Function",
        "abstract": "Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myopic nature. Despite this, the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the hypervolume identifiability issue, which results from the non-Markovian nature of MOBO problems. To tackle this, inspired by the non-Markovian RL literature and the success of Transformers in language modeling, we present a generalized deep Q-learning framework and propose BOFormer, which substantiates this framework for MOBO via sequence modeling. Through extensive evaluation, we demonstrate that BOFormer constantly achieves better performance than the benchmark rule-based and learning-based algorithms in various synthetic MOBO and real-world multi-objective hyperparameter optimization problems."
    },
    {
        "title": "Deep Signature: Characterization of Large-Scale Molecular Dynamics",
        "link_suffix": "/forum?id=xayT1nn8Mg",
        "link": "https://openreview.net/forum?id=xayT1nn8Mg",
        "pdf_link": "https://openreview.net/pdf?id=xayT1nn8Mg",
        "keywords": "Molecular dynamics; representation learning; graph neural network; path signature",
        "abstract": "Understanding protein dynamics are essential for deciphering protein functional mechanisms and developing molecular therapies. However, the complex high-dimensional dynamics and interatomic interactions of biological processes pose significant challenge for existing computational techniques. In this paper, we approach this problem for the first time by introducing Deep Signature, a novel computationally tractable framework that characterizes complex dynamics and interatomic interactions based on their evolving trajectories. Specifically, our approach incorporates soft spectral clustering that locally aggregates cooperative dynamics to reduce the size of the system, as well as signature transform that collects iterated integrals to provide a global characterization of the non-smooth interactive dynamics. Theoretical analysis demonstrates that Deep Signature exhibits several desirable properties, including invariance to translation, near invariance to rotation, equivariance to permutation of atomic coordinates, and invariance under time reparameterization. Furthermore, experimental results on three benchmarks of biological processes verify that our approach can achieve superior performance compared to baseline methods."
    },
    {
        "title": "RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation",
        "link_suffix": "/forum?id=yAzN4tz7oI",
        "link": "https://openreview.net/forum?id=yAzN4tz7oI",
        "pdf_link": "https://openreview.net/pdf?id=yAzN4tz7oI",
        "keywords": "robot learning, diffusion models, foundation models, bimanual manipulation",
        "abstract": "Bimanual manipulation is essential in robotics, yet developing foundation models is extremely challenging due to the inherent complexity of coordinating two robot arms (leading to multi-modal action distributions) and the scarcity of training data. In this paper, we present the Robotics Diffusion Transformer (RDT), a pioneering diffusion foundation model for bimanual manipulation. It is built on scalable Diffusion Transformers (DiTs), which can effectively represent multi-modality, with innovative designs to deal with the heterogeneity of multi-modal inputs and to capture the nonlinearity and high frequency of robotic data. To address data scarcity, we first introduce a Physically Interpretable Unified Action Space, which can unify the action representations of various robots while preserving the physical meanings of original actions, facilitating learning transferrable physical knowledge. With the above designs, we managed to pre-train RDT on the largest collection of multi-robot datasets to date and scaled it up to $1.2$B parameters, which is the largest diffusion-based foundation model for robotic manipulation. We further fine-tuned RDT on a self-created multi-task bimanual dataset with over $6$K+ episodes to refine its manipulation capabilities. Experiments on real robots demonstrate that RDT significantly outperforms existing methods. It exhibits zero-shot generalization to unseen objects and scenes, understands and follows language instructions, learns new skills with just 1$\\sim$5 demonstrations, and effectively handles complex, dexterous tasks. Code and a Demo video are provided in the supplementary materials."
    },
    {
        "title": "CellDJBench: Benchmark Datasets for Data-Driven Biological Fluid Simulation",
        "link_suffix": "/forum?id=KTHUTtEX5F",
        "link": "https://openreview.net/forum?id=KTHUTtEX5F",
        "pdf_link": "https://openreview.net/pdf?id=KTHUTtEX5F",
        "keywords": "Cell; biological dynamic simulation",
        "abstract": "Biological fluid simulation is a critical tool for comprehending the intricate and complex fluid dynamics that occur within biological systems. Recently, data-driven techniques have emerged as a promising avenue to enhance the accuracy and efficiency of biological fluid simulations. However, the community encounters two challenges. (1) Existing biological datasets only capture static snapshots, lacking the ability to capture dynamic biological processes. (2) These datasets are limited in scale due to the demanding experimental conditions. To address these challenges, this paper introduces four comprehensive large-scale datasets: Tension, Wets, CellDivision and Jellyfish, containing a wealth of biological dynamics and pushing the boundary of data-driven methods. These datasets have been meticulously designed to encompass a wide array of biological fluid dynamics scenarios. By incorporating physical modeling techniques such as phase-field method, these datasets provide a standardized evaluation framework for data-driven approaches. They empower researchers to objectively assess and compare different methodologies, fostering advancements in the field of biological fluid simulation. Furthermore, the availability of these benchmark datasets facilitates reproducibility and enhances the comparability of results across studies, promoting knowledge sharing and collaboration within the research community. Researchers can build upon existing models, leading to cumulative progress in the development of accurate and efficient data-driven models for simulating complex fluid dynamics within biological systems. We offer benchmark code and \\model\\ dataset link through the following link: \\href{https://anonymous.4open.science/r/BioJCell--9E53/README.md}{https://anonymous.4open.science/r/CellDJBench}."
    },
    {
        "title": "Diffusion Transformer Policy",
        "link_suffix": "/forum?id=PvvXDazPMs",
        "link": "https://openreview.net/forum?id=PvvXDazPMs",
        "pdf_link": "https://openreview.net/pdf?id=PvvXDazPMs",
        "keywords": "Visual-Language-Action, Diffusion Policy",
        "abstract": "Recent large visual-language action models pretrained on diverse robot datasets have demonstrated the potential for generalizing to new environments with a few in-domain data. However, those approaches usually predict discretized or continuous actions by a small action head, which limits the ability in handling diverse action spaces. \nIn contrast, we model the continuous action with a large multi-modal diffusion transformer, dubbed as Diffusion Transformer Policy, in which we directly denoise action chunks by a large transformer model rather than a small action head. \nBy leveraging the scaling capability of transformers, the proposed approach can effectively model continuous end-effector actions across large diverse robot datasets, and achieve better generalization performance. \nExtensive experiments demonstrate Diffusion Transformer Policy pre-trained on diverse robot data can generalize to different embodiments, including simulation environments like Maniskill2 and Calvin, as well as the real-world Franka arm. \nSpecifically, without bells and whistles, the proposed approach achieves state-of-the-art performance in the Calvin novel task setting, and the pre-training stage significantly facilitates the success sequence length on the Calvin by over 1.2. The code will be publicly available."
    },
    {
        "title": "IoT-LLM: Enhancing Real-World IoT Task Reasoning with Large Language Models",
        "link_suffix": "/forum?id=jJ7azzLMdE",
        "link": "https://openreview.net/forum?id=jJ7azzLMdE",
        "pdf_link": "https://openreview.net/pdf?id=jJ7azzLMdE",
        "keywords": "large language models, Internet of Things, IoT task reasoning",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across textual and visual domains but often generate outputs that violate physical laws, revealing a gap in their understanding of the physical world. Inspired by human cognition\u2014where perception is fundamental to reasoning\u2014we explore augmenting LLMs with enhanced perception abilities using Internet of Things (IoT) sensor data and pertinent knowledge for IoT task reasoning in the physical world. In this work, we systematically study LLMs' capability to address real-world IoT tasks by augmenting their perception and knowledge base, and then propose a unified framework, IoT-LLM, to enhance such capability. In IoT-LLM, we customize three steps for LLMs: preprocessing IoT data into formats amenable to LLMs, activating their commonsense knowledge through chain-of-thought prompting and specialized role definitions, and expanding their understanding via IoT-oriented retrieval-augmented generation based on in-context learning. To evaluate the performance, We design a new benchmark with five real-world IoT tasks with different data types and reasoning difficulties and provide the benchmarking results on six open-source and close-source LLMs. Experimental results demonstrate the limitations of existing LLMs with naive textual inputs that cannot perform these tasks effectively. We show that IoT-LLM significantly enhances the performance of IoT tasks reasoning of LLM, such as GPT-4, achieving an average improvement of 65% across various tasks against previous methods. The results also showcase LLMs' ability to comprehend IoT data and the physical law behind data by providing a reasoning process. Limitations of our work are claimed to inspire future research in this new era."
    },
    {
        "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
        "link_suffix": "/forum?id=02haSpO453",
        "link": "https://openreview.net/forum?id=02haSpO453",
        "pdf_link": "https://openreview.net/pdf?id=02haSpO453",
        "keywords": "Unified Visual Language Model, Autoregressive Model",
        "abstract": "VILA-U is a Unified foundation model that integrates Video, Image, Language understanding and generation. Traditional visual language models (VLMs) use separate modules for understanding and generating visual content, which can lead to misalignment and increased complexity. In contrast, VILA-U employs a single autoregressive next-token prediction framework for both tasks, eliminating the need for additional components like diffusion models. This approach not only simplifies the model but also achieves near state-of-the-art performance in visual language understanding and generation. The success of VILA-U is attributed to two main factors: the unified vision tower that aligns discrete visual tokens with textual inputs during pretraining, which enhances visual perception, and autoregressive image generation can achieve similar quality as diffusion models with high-quality dataset. This allows VILA-U to perform comparably to more complex models using a fully token-based autoregressive framework."
    },
    {
        "title": "VILA^2: VLM Augmented VLM with Self-Improvement",
        "link_suffix": "/forum?id=M2YCdfxNVx",
        "link": "https://openreview.net/forum?id=M2YCdfxNVx",
        "pdf_link": "https://openreview.net/pdf?id=M2YCdfxNVx",
        "keywords": "Multimodal Large Models",
        "abstract": "Visual language models (VLMs) have rapidly progressed, driven by the success of large language models (LLMs). While model architectures and training infrastructures advance rapidly, data curation remains under-explored. When data quantity and quality become a bottleneck, existing work either directly crawls more raw data from the Internet that does not have a guarantee of data quality or distills from black-box commercial models (e.g., GPT-4V / Gemini) causing the performance upper bounded by that model. In this work, we introduce a novel approach that includes a self-augment step and a specialist-augment step to iteratively improve data quality and model performance. In the self-augment step, a VLM recaptions its own pretraining data to enhance data quality, and then retrains from scratch using this refined dataset to improve model performance. This process can iterate for several rounds. Once self-augmentation saturates, we employ several specialist VLMs finetuned from the self-augmented VLM with domain-specific expertise, to further infuse specialist knowledge into the generalist VLM through task-oriented recaptioning and retraining. With the combined self-augmented and specialist-augmented training, we introduce VILA2 (VLM-augmented-VLM), a VLM family that consistently improves the accuracy on a wide range of tasks over prior art, including MMMU leaderboard, with a reusable pretraining dataset that is 300x more cost-efficient than human labeling."
    },
    {
        "title": "Feast Your Eyes:  Mixture-of-Resolution Adaptation for Multimodal Large Language Models",
        "link_suffix": "/forum?id=1EnpStvBU8",
        "link": "https://openreview.net/forum?id=1EnpStvBU8",
        "pdf_link": "https://openreview.net/pdf?id=1EnpStvBU8",
        "keywords": "high-resolution adaptation, multimodal large language models",
        "abstract": "In existing multimodal large language models (MLLMs), image resolution plays a significant role for  granular visual recognition.  However, directly increasing image resolution leads to expensive computational cost for MLLMs.  In this paper, we  reveal that a combination of low- and high-resolution visual features can efficiently mitigate this shortcoming.  Based on this principle, we propose a novel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation (MRA). In particular, MRA adopts two visual pathways for  images of different resolutions, where  high-resolution visual information is embedded into the low-resolution pathway via the novel mixture-of-resolution adapters (MR-Adapters). This design also   greatly  reduces the input sequence length of MLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the new model  LLaVA-HR. We conduct extensive  experiments on 17 vision-language (VL) tasks, which show that LLaVA-HR outperforms existing MLLMs on 15 VL tasks,e.g., +5.2% on TextVQA.  More importantly,    both training and inference  of LLaVA-HR remain efficient with MRA, e.g.,  20 training hours and faster inference speed than LLaVA-NeXT.  Source codes are anonymously released at:https://anonymous.4open.science/r/LLaVA-HR-4BB5."
    },
    {
        "title": "Contrastive learning of cell state dynamics in response to perturbations",
        "link_suffix": "/forum?id=7iCT2vmYAR",
        "link": "https://openreview.net/forum?id=7iCT2vmYAR",
        "pdf_link": "https://openreview.net/pdf?id=7iCT2vmYAR",
        "keywords": "contrastive learning, dynamics, cell biology",
        "abstract": "We introduce DynaCLR, a self-supervised framework for modeling cell dynamics through contrastive learning of representations of time-lapse images. Time-lapse imaging of cells and organelles is widely used to identify causal links between the perturbations and cellular responses. Human annotation of diverse cell states observed in large time-lapse datasets is time-consuming and prone to bias. DynaCLR utilizes cell tracking and a pretext task of time-aware contrastive learning to map the images of the cells at neighboring time points to neighboring embeddings. The cell states can be annotated efficiently in the learned embedding space for diverse downstream analysis, such as classification, clustering, or interpretation. We illustrate the features and applications of DynaCLR with the following experiments: analyzing the kinetics of viral infection in human cells, detecting transient changes in cell morphology due to cell division, and mapping the dynamics of organelles due to viral infection. Models trained with DynaCLR consistently achieve $>95%$ classification accuracy, enable the detection of transients in cell states, and reliably embed unseen experiments. DynaCLR provides a flexible framework for comparative analysis of cell state dynamics due to perturbations, such as infection, gene knockouts, and drugs."
    },
    {
        "title": "UniQA: Unified Vision-Language Pre-training for Image Quality and Aesthetic Assessment",
        "link_suffix": "/forum?id=8mE8KNHTjd",
        "link": "https://openreview.net/forum?id=8mE8KNHTjd",
        "pdf_link": "https://openreview.net/pdf?id=8mE8KNHTjd",
        "keywords": "Image assessment, Vision-language learning, Multimodal large language models",
        "abstract": "Image Quality Assessment (IQA) and Image Aesthetic Assessment (IAA) aim to simulate human subjective perception of image visual quality and aesthetic appeal. Despite distinct learning objectives, they have underlying interconnectedness due to consistent human assessment perception. Existing unified methods typically combine datasets of two tasks for regression training directly, which fail to learn mutually beneficial representations shared by both tasks explicitly. To confront this challenge, we propose \\textbf{Uni}fied vision-language pre-training of \\textbf{Q}uality and \\textbf{A}esthetics (\\textbf{UniQA}), to extract useful and common representations from two tasks, thereby benefiting them simultaneously. Unfortunately, the lack of text in the IQA datasets and the textual noise in the IAA datasets pose severe challenges for multimodal pre-training. To address this, we (1) utilize multimodal large language models (MLLMs) to generate high-quality text descriptions; (2) use the generated text for IAA  as metadata to purify noisy IAA data. To effectively adapt the pre-trained UniQA to downstream tasks, we further propose a lightweight adapter that utilizes versatile cues to fully exploit the extensive knowledge of the pre-trained model. Extensive experiments show that our approach achieves state-of-the-art performance on both IQA and IAA tasks, while also demonstrating exceptional few-label image assessment capabilities."
    }
]