[
    {
        "title": "Markov Persuasion Processes: Learning to Persuade From Scratch",
        "link_suffix": "/forum?id=DGjzxNRbKU",
        "link": "https://openreview.net/forum?id=DGjzxNRbKU",
        "pdf_link": "https://openreview.net/pdf?id=DGjzxNRbKU",
        "keywords": "Bayesian Persuasion, Online Learning, Markov Persuasion Process",
        "abstract": "In Bayesian persuasion, an informed sender strategically discloses information to a receiver so as to persuade them to undertake desirable actions. Recently, Markov persuasion processes (MPPs) have been introduced to capture sequential scenarios where a sender faces a stream of myopic receivers in a Markovian environment. The MPPs studied so far in the literature suffer from issues that prevent them from being fully operational in practice, e.g., they assume that the sender knows receivers' rewards. We fix such issues by addressing MPPs where the sender has no knowledge about the environment. We design a learning algorithm for the sender, working with partial feedback. We prove that its regret with respect to an optimal information-disclosure policy grows sublinearly in the number of episodes, as it is the case for the loss in persuasiveness cumulated while learning. Moreover, we provide a lower bound for our setting matching the guarantees of our algorithm."
    },
    {
        "title": "Magnituder Layers for Implicit Neural Representations in 3D",
        "link_suffix": "/forum?id=uswS6tUCN2",
        "link": "https://openreview.net/forum?id=uswS6tUCN2",
        "pdf_link": "https://openreview.net/pdf?id=uswS6tUCN2",
        "keywords": "NeRF, SDF, Implicit Representations",
        "abstract": "Improving the efficiency and performance of implicit neural representations in 3D, particularly Neural Radiance Fields (NeRF) and Signed Distance Fields (SDF) is crucial for enabling their use in real-time applications. These models, while capable of generating photo-realistic novel views and detailed 3D reconstructions, often suffer from high computational costs and slow inference times. To address this, we introduce a novel neural network layer called the ``magnituder\", designed to reduce the number of training parameters in these models without sacrificing their expressive power. By integrating magnituders into standard feed-forward layer stacks, we achieve improved inference speed and adaptability. Furthermore, our approach enables a zero-shot performance boost in trained implicit neural representation models through layer-wise knowledge transfer without backpropagation, leading to more efficient scene reconstruction in dynamic environments."
    },
    {
        "title": "AutoTune for Time Series Transformers using Low Rank Adaptation and Limited Discrepancy Search",
        "link_suffix": "/forum?id=qZz7PKt4bE",
        "link": "https://openreview.net/forum?id=qZz7PKt4bE",
        "pdf_link": "https://openreview.net/pdf?id=qZz7PKt4bE",
        "keywords": "Time Series Transformers, LoRA, Time Series Forecasting",
        "abstract": "Transformer models have achieved remarkable results in the field of Natural Language Processing (NLP) with the introduction of breakthrough large language models like GPT and LLaMA recently. Motivated by their ability to capture long-range dependencies, researchers have successfully adapted these models to the task of time series forecasting. However, despite their potential, effectiveness of applying these pre-trained time series transformer models in the target domain is limited due to the need for hyper-parameter optimisation to match the characteristics of the target domain. This paper presents a novel algorithm that uses parameter efficient fine-tuning such as Low Rank Adaptation (LoRA) coupled with Limited Discrepancy Search (LDS) to efficiently auto fine-tune pre-trained time series transformers for a given target domain. Our approach helps in making informed design choices involving LoRA tunable hyper-parameters with strong performance-cost trade-offs that are highly transferable across different target domains. Our experiments demonstrate that autotune efficiently identifies the optimal configuration of LoRA hyper-parameters, achieving an average MASE\nimprovement of 5.21% across all datasets and 4.76% for out-of-domain datasets compared to zero shot pre-trained models, with improvements as high as 20.59% for one of the out-of-domain datasets."
    },
    {
        "title": "Solving Multiplayer Partially Observable Stochastic Games by Divergence-Regularized Discounted Aggregation",
        "link_suffix": "/forum?id=KD5nJUgeW4",
        "link": "https://openreview.net/forum?id=KD5nJUgeW4",
        "pdf_link": "https://openreview.net/pdf?id=KD5nJUgeW4",
        "keywords": "partially observable stochastic game, Nash distribution, divergence regularization, hypomonotonicity, last-iterate convergence, Nash equilibrium",
        "abstract": "This paper presents Divergence-Regularized Discounted Aggregation (DRDA), a multi-round learning system for solving partially observable stochastic games (POSGs), which unify normal-form games (NFGs), extensive-form games (EFGs), and Markov games (MGs). In each single round, DRDA can be viewed as a discounted variant of Follow the Regularized Leader (FTRL) under a general value function for POSGs concerning imperfect information and an infinite horizon. While previous studies on this FTRL variant have demonstrated its last-iterate convergence towards quantal response equilibrium (QRE) in NFGs, this paper extends the theoretical results to POSGs by defining a generalized Nash distribution (GND), which extends the QRE concept of Nash distribution in NFGs through divergence regularization. The linear last-iterate convergence of single-round DRDA to its rest point is proved under a general assumption of hypomonotonicity. When the rest point is unique, it induces the unique GND, which has a bounded deviation with respect to Nash equilibrium (NE). Under multiple learning rounds, DRDA keeps replacing the base policy for divergence regularization with the policy at the rest point in the previous round. It is further proved that the limit point of multi-round DRDA must be an exact NE rather than a QRE under the unique rest point assumption. In experiments, the last iterates of multi-round DRDA converge to NE at a near-exponential rate in NFGs, outperforming existing baselines including moving-magnet magnetic mirror descent (MMD) in multiplayer EFGs. In an infinite-horizon MG, DRDA significantly outperforms the applicable algorithms based on best-response computations."
    },
    {
        "title": "Paramanu: A Family of Novel Efficient Generative Foundation Language Models for Indian Languages",
        "link_suffix": "/forum?id=lAkke7Yj1T",
        "link": "https://openreview.net/forum?id=lAkke7Yj1T",
        "pdf_link": "https://openreview.net/pdf?id=lAkke7Yj1T",
        "keywords": "generative language models, low-resource NLG, pretraining, multilingual, tokenization, instruction fine-tuning",
        "abstract": "We present PARAMANU (which means “atom” in multiple Indian languages), a\nfamily of novel language models for Indian languages. It is a collection of auto-\nregressive monolingual, bilingual, and multilingual Indian language models pre-\ntrained from scratch, currently covering 10 Indian languages (Assamese, Bangla,\nHindi, Konkani, Maithili, Marathi, Odia, Sanskrit, Tamil, Telugu) across 5 scripts\n(Bangla, Devanagari, Odia, Tamil, Telugu). The models are pretrained with a con-\ntext size of 1024 on a single GPU, and are of varying sizes ranging from 13.29 M\nto 367.5 M parameters. We proposed a RoPE embedding scaling method that en-\nables us to pretrain language models from scratch at larger sequence length context\nsize than the equivalent GPU memory. We have also developed an efﬁcient and\nadvanced novel tokenizer for Indian languages using a combination of BPE and\nUnigram that can also tokenize unseen languages written in the same script or the\nRoman script. We also proposed language speciﬁc tokenization for multilingual\nmodels and domain speciﬁc tokenization for monolingual language models. In\norder to avoid the “curse of multi-linguality” in our multilingual M PARAMANU\nmodel, we pretrained on comparable corpora by typological grouping using the\nsame script. We proposed and performed pretraining for more than 1 epoch of\ntraining for most of our language models. From our results, we observed the lan-\nguage transfer phenomenon from low resource to high resource within languages\nof the same script and typology. We performed human evaluation of our pretrained\nmodels for open end text generation on grammar, coherence, creativity, and factu-\nality metrics for several languages. Our Paramanu models outperformed standard\nlarge language models (LLMs) by a large margin in performance despite being\nsmaller in size by 64 to 20 times. We studied the impact of language speciﬁc tok-\nenization versus language agnostic tokenization for bilingual language modeling.\nWe also studied the impact of BPE versus Unigram tokenization for Devanagari\nscript languages. We further created instruction-tuning datasets and instruction-\ntuned our pretrained models on 23,000 instructions in respective languages. Com-\nparison with multilingual LLMs on various commonsense reasoning benchmarks\nfor natural language understanding, natural language inference, and machine read-\ning comprehension shows the advantage of our models. The performance of our\nParamanu models leads to the conclusion that high quality generative language\nmodels are possible without high amount of compute power and enormous num-\nber of parameters."
    },
    {
        "title": "Complex Numerical Computation  with Numerical Semantic Pre-training Framework",
        "link_suffix": "/forum?id=1epaSm9QRs",
        "link": "https://openreview.net/forum?id=1epaSm9QRs",
        "pdf_link": "https://openreview.net/pdf?id=1epaSm9QRs",
        "keywords": "Numerical Reasoning, Complex Query Answering, Knowledge Graph",
        "abstract": "Multi-hop complex reasoning over incomplete knowledge graphs has been extensively studied, but research on numerical knowledge graphs remains relatively limited. Recent approaches focus on separately encoding entities and numerical values, using neural networks to process query encodings for reasoning. However, in complex multi-hop reasoning tasks, numerical values are not merely symbols; they carry specific semantics and logical relationships that must be accurately represented. Directly encoding numerical values often leads to the loss of such semantic information. In this work, we propose a Complex Numerical Reasoning with Numerical Semantic Pre-Training Framework (CNR-NST). We designed a joint link predictor that incorporates the relationships between numerical values and entities into the learning process of numerical semantics. The proposed framework is the first to enable binary operations on numerical attributes in numerical knowledge graphs, allowing new numerical attributes to be inferred from existing knowledge. The CNR-NST framework can perform binary operations on numerical attributes in numerical knowledge graphs, enabling it to infer new numerical attributes from existing knowledge. Our approach effectively handles up to 102 types of complex numerical reasoning queries. On three public datasets, CNR-NST demonstrates state-of-the-art performance in complex numerical queries, achieving an average improvement of over 40% compared to existing methods. Notably, this work expands the range of query types for complex multi-hop numerical reasoning and introduces a new evaluation metric for numerical answers, which has been validated through comprehensive experiments."
    },
    {
        "title": "Enjoy Your Layer Normalization with the Computation Efficiency of RMSNorm",
        "link_suffix": "/forum?id=bVdcAZAW2h",
        "link": "https://openreview.net/forum?id=bVdcAZAW2h",
        "pdf_link": "https://openreview.net/pdf?id=bVdcAZAW2h",
        "keywords": "Layer normalization, RMSNorm, Deep Learning",
        "abstract": "Layer Normalization (LN) is a milestone technique in deep learning and has been widely used in various network architectures. It performs centering and scaling over the layer activations of a neural network for each example, stabilizing and accelerating the training of neural network. However, it introduces extra computation cost during inference and the computation problem has recently been addressed by its counterpart RMSNorm that only adopts scaling. This paper investigates how to exploit the theoretical advantages of LN but with the cost of RMSNorm. This paper formally defines the condition that the centering operation of LN can be removed and this condition can be obtained by imposing the column centering constraint on the adjacent linear module before the LN. We propose column centered weight transformation (CCWT) to ensure an LN without centering operation (i.e., RMSNorm) have the same output as the original one in a pre-trained model.\n Our method can be directly applied to various pre-trained large language models (LLMs) and large vision language models (VLMs) with LN, enabling an immediate reduction in computation cost meanwhile maintaining equivalent prediction during inference.\n We further propose a reparameterization method, called column based weight centering (CBWC), to ensure the linear module column centered during training. We show that RMSNorm combining CBWC can obtain an equivalent effects to the  LN counterpart during training, but with more efficient computation."
    },
    {
        "title": "Provably safe Reinforcement Learning using Bender's Decomposition Oracles",
        "link_suffix": "/forum?id=RAdBtquPiI",
        "link": "https://openreview.net/forum?id=RAdBtquPiI",
        "pdf_link": "https://openreview.net/pdf?id=RAdBtquPiI",
        "keywords": "Constrained Reinforcement Learning, Safe Reinforcement learning, Constrained Optimization",
        "abstract": "One of the core challenges when applying reinforcement learning to solve real world problems is the violation of numerous safety, feasibility or physical constraints during training and deployment.\nWe propose Bender's Oracle Optimization (BOO) that manages to achieve provable safety during both training and deployment, under the assumption that one has access to a representation of the feasible set, e.g., through a (possibly inaccurate) simulator or encoded rules. \nThis method is particularly useful for cases where a simple (deterministic) model of the problem is available, but said model is too inaccurate or incomplete to solve the problem directly.\nWe showcase our method by applying it to a challenging reward-maximizing stochastic job-shop scheduling problem, where we demonstrate a 17% improvement, and a nonlinear, nonconvex packing problem where we achieve close to globally optimal performance while improving the convergence speed by a factor of 800."
    },
    {
        "title": "Right on Time: Revising Time Series Models by Constraining their Explanations",
        "link_suffix": "/forum?id=O6W9SJRZRA",
        "link": "https://openreview.net/forum?id=O6W9SJRZRA",
        "pdf_link": "https://openreview.net/pdf?id=O6W9SJRZRA",
        "keywords": "XIL, time series, spurious corelation, Interaction, XAI, Dataset",
        "abstract": "The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to incorrect outputs. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To avoid “Clever-Hans” moments in time series, i.e., to mitigate confounders, we introduce the method Right on Time (RioT). RioT enables, for the first time interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets."
    },
    {
        "title": "PGLearn - An Open-Source Learning Toolkit for Optimal Power Flow",
        "link_suffix": "/forum?id=cecIf0CKnH",
        "link": "https://openreview.net/forum?id=cecIf0CKnH",
        "pdf_link": "https://openreview.net/pdf?id=cecIf0CKnH",
        "keywords": "optimal power flow, machine learning, dataset",
        "abstract": "Machine learning techniques for Optimal Power Flow (OPF) problems have recently garnered significant attention, reflecting a broader trend of leveraging machine learning to approximate and/or accelerate the resolution of complex optimization problems. These developments are necessitated by the increased volatility and scale in energy production for modern and future grids. However, progress in ML for OPF is hindered by the lack of standardized datasets and evaluation metrics, from generating and solving OPF instances, to training and benchmarking machine learning models. To address this challenge, this paper introduces PGLearn, a comprehensive suite of standardized datasets and evaluation tools for ML and OPF. PGLearn implements realistic data generation procedures that capture both global and local variability, ensuring that datasets are representative of real-world conditions. In addition, it supports multiple OPF formulations, including AC, DC, and second-order cone formulations. Standardized datasets are made publicly available to democratize access to this field, reduce the burden of data generation, and enable the fair comparison of various methodologies. PGLearn also includes a robust toolkit for training, evaluating, and benchmarking machine learning models for OPF, with the goal of standardizing performance evaluation across the field. By promoting open, standardized datasets and evaluation metrics, PGLearn aims at democratizing and accelerating research and innovation in machine learning applications for optimal power flow problems."
    },
    {
        "title": "EEGMamba: Bidirectional State Space Model with Mixture of Experts for EEG Multi-task Classification",
        "link_suffix": "/forum?id=13PclvlVBa",
        "link": "https://openreview.net/forum?id=13PclvlVBa",
        "pdf_link": "https://openreview.net/pdf?id=13PclvlVBa",
        "keywords": "EEG Classification, State Space Models, Mixture of Experts, Brain-Computer Interfaces",
        "abstract": "In recent years, with the development of deep learning, electroencephalogram (EEG) classification networks have achieved certain progress. Transformer-based models can perform well in capturing long-term dependencies in EEG signals. However, their quadratic computational complexity poses a substantial computational challenge. Moreover, most EEG classification models are only suitable for single tasks and struggle with generalization across different tasks, particularly when faced with variations in signal length and channel count. In this paper, we introduce EEGMamba, the first universal EEG classification network to truly implement multi-task learning for EEG applications. EEGMamba seamlessly integrates the Spatio-Temporal-Adaptive (ST-Adaptive) module, bidirectional Mamba, and Mixture of Experts (MoE) into a unified framework. The proposed ST-Adaptive module performs unified feature extraction on EEG signals of different lengths and channel counts through spatial-adaptive convolution and incorporates a class token to achieve temporal-adaptability. Moreover, we design a bidirectional Mamba particularly suitable for EEG signals for further feature extraction, balancing high accuracy, fast inference speed, and efficient memory-usage in processing long EEG signals. To enhance the processing of EEG data across multiple tasks, we introduce task-aware MoE with a universal expert, effectively capturing both differences and commonalities among EEG data from different tasks. We evaluate our model on eight publicly available EEG datasets, and the experimental results demonstrate its superior performance in four types of tasks: seizure detection, emotion recognition, sleep stage classification, and motor imagery. The code is set to be released soon."
    },
    {
        "title": "The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG",
        "link_suffix": "/forum?id=b57IG6N20B",
        "link": "https://openreview.net/forum?id=b57IG6N20B",
        "pdf_link": "https://openreview.net/pdf?id=b57IG6N20B",
        "keywords": "eeg, ieeg, intracranial eeg, electroencephalography, compression, transfer, seizure, seizure detection, motor imagery",
        "abstract": "All data modalities are not created equal, even when the signal they measure comes from the same source. In the case of the brain, two of the most important data modalities are the scalp electroencephalogram (EEG), and the intracranial electroencephalogram (iEEG). iEEG benefits from a higher signal-to-noise ratio (SNR), as it measures the electrical activity directly in the brain, while EEG is noisier and has lower spatial and temporal resolutions. Nonetheless, both EEG and iEEG are important sources of data for human neurology, from healthcare to brain–machine interfaces. They are used by human experts, supported by deep learning (DL) models, to accomplish a variety of tasks, such as seizure detection and motor imagery classification. Although the differences between EEG and iEEG are well understood by human experts, the performance of DL models across these two modalities remains under-explored. To help characterize the importance of clean data on the performance of DL models, we propose BrainCodec, a high-fidelity EEG and iEEG neural compressor. We find that training BrainCodec on iEEG and then transferring to EEG yields higher reconstruction quality than training on EEG directly. Our work indicates that data sources with higher SNR, such as iEEG, provide better performance across the board also in the medical time-series domain. This finding is consistent with reports coming from natural language processing, where clean data sources appear to have an outsized effect on the performance of the DL model overall. BrainCodec also achieves up to a 64x compression on iEEG and EEG without a notable decrease in quality. BrainCodec markedly surpasses current state-of-the-art compression models both in final compression ratio and in reconstruction fidelity. We also evaluate the fidelity of the compressed signals objectively on a seizure detection and a motor imagery task performed by standard DL models. Here, we find that BrainCodec achieves a reconstruction fidelity high enough to ensure no performance degradation on the downstream tasks. Finally, we collect the subjective assessment of an expert neurologist, that confirms the high reconstruction quality of BrainCodec in a realistic scenario."
    },
    {
        "title": "Grad-TopoCAM: EEG Brain Region Visual Interpretability via Gradient-Based Topographic Class Activation Map",
        "link_suffix": "/forum?id=FHQDCQFD8y",
        "link": "https://openreview.net/forum?id=FHQDCQFD8y",
        "pdf_link": "https://openreview.net/pdf?id=FHQDCQFD8y",
        "keywords": "Electroencephalogram, Class Activation Map, Deep Learning, Visualization, Interpretability",
        "abstract": "The visualization and interpretability of electroencephalogram (EEG) decoding significantly contribute to brain-computer interfaces (BCI) and cognitive neuroscience. Although some existing research has attempted to map EEG features to specific brain regions, these approaches fail to fully utilize raw signals and lack extensibility to other Deep Learning (DL) models. In this work, Grad-TopoCAM (Gradient-Based Topographic Class Activation Map) is proposed, which enhances interpretability in DL models for EEG decoding adaptively. Grad-TopoCAM calculates the gradient of feature maps for the target class at the target layer. The weights of the feature maps are obtained through global average pooling of the gradients. The class activation map is generated by performing a linear combination of weights and feature maps, which is subsequently mapped to different brain regions. Grad-TopoCAM is validated across eight DL models on four public datasets. Experimental results indicate that Grad-TopoCAM effectively identifies and visualizes brain regions that significantly influence decoding outcomes, while also facilitating channel selection for different decoding tasks. The code and data are open-source."
    },
    {
        "title": "FedPMVR: Addressing Data Heterogeneity in Federated Learning through Partial Momentum Variance Reduction",
        "link_suffix": "/forum?id=H3jGJzw0DN",
        "link": "https://openreview.net/forum?id=H3jGJzw0DN",
        "pdf_link": "https://openreview.net/pdf?id=H3jGJzw0DN",
        "keywords": "Federated Learning, Data Heterogeneity, Variance Reduction, Image Classification.",
        "abstract": "Federated learning (FL) emerges as a promising paradigm for training machine learning models on decentralized data sources while preserving privacy. However, the presence of not independent and identically distributed (non-IID) data among the clients introduces high variance in gradient updates, posing a significant challenge to the global model's performance in terms of accuracy and convergence. To mitigate the adverse effects of data heterogeneity, we propose a novel momentum-based partial variance reduction technique. Our approach adjusts the gradient updates for the final classification layers of the client's neural network by leveraging the gradient differences between local and global models. This adjustment aims to effectively capture and mitigate client drift, a key challenge arises from the presence of non-IID data distributions across clients. We systematically explains client drifts and conduct extensive experiments on three widely-used datasets, demonstrating that our method significantly enhances global model accuracy while reducing the communication rounds needed for convergence. Notably, our momentum-based partial variance reduction technique provides a robust mechanism, rendering more efficient and effective in scenarios with inherently non-IID and heterogeneous data distributions. By addressing the critical challenge of data heterogeneity in FL, our proposed approach paves the way for more reliable and accurate model training while preserving the privacy of decentralized data sources. The code is available at the following link {https://anonymous.4open.science/r/FedPMVR-33C1}."
    },
    {
        "title": "A Novel Security Threat Model for Automated AI Accelerator Generation Platforms",
        "link_suffix": "/forum?id=ckicHjoTgf",
        "link": "https://openreview.net/forum?id=ckicHjoTgf",
        "pdf_link": "https://openreview.net/pdf?id=ckicHjoTgf",
        "keywords": "AI accelerator generation platforms, Design Space Exploration (DSE), Hardware Trojan (HT), Security threat model",
        "abstract": "In recent years, the design of Artificial Intelligence (AI) accelerators has gradually shifted from focusing solely on standalone accelerator hardware to considering the entire system, giving rise to a new AI accelerator design paradigm that emphasizes full-stack integration. Systems designed based on this paradigm offer a user-friendly, end-to-end solution for deploying pre-trained models. While previous studies have identified vulnerabilities in individual hardware components or models, the security of this paradigm has not yet been thoroughly evaluated. This work, from an attacker's perspective, proposes a threat model based on this paradigm and reveals the potential security vulnerabilities of systems by embedding malicious code in the design flow, highlighting the necessity for protection to address this security gap. In exploration and generation, maliciously leverage the exploration unit to identify sensitive parameters in the model's intermediate layers and insert hardware Trojan (HT) into the accelerator. In execution, malicious information is concealed within the control instructions, triggering the HT. Experimental results demonstrate that the proposed method, which manipulates sensitive parameters in a few selected kernels across the middle convolutional layers, successfully misclassifies input images into specified categories with high misclassification rates across various models: 97.3% in YOLOv8 by modifying only three parameters per layer in three layers, 99.2% in ResNet-18 by altering four parameters per layer in three layers and 98.1% for VGG-16 by changing seven parameters per layer in four layers. Additionally, the area overhead introduced by the proposed HT occupies no more than 0.34% of the total design while maintaining near-original performance as in uncompromised designs, which clearly illustrates the concealment of the proposed security threat."
    },
    {
        "title": "AutoGeTS: Automated Generation of Text Synthetics for Improving Text Classification",
        "link_suffix": "/forum?id=JL18agpSc3",
        "link": "https://openreview.net/forum?id=JL18agpSc3",
        "pdf_link": "https://openreview.net/pdf?id=JL18agpSc3",
        "keywords": "Text Classification, Synthetic Data, Data Augmentation, Large Language Model, Text Analysis, Optimization",
        "abstract": "When developing text classification models for real world applications, one major challenge is the difficulty to collect sufficient data for all text classes. In this work, we address this challenge by utilizing large language models (LLMs) to generate synthetic data and using such data to improve the performance of the models without waiting for more real data to be collected and labelled. As an LLM generates different synthetic data in response to different input examples, we formulate an automated workflow, which searches for input examples that lead to more \"effective'' synthetic data for improving the model concerned. We study three search strategies with an extensive set of experiments, and use experiment results to inform an ensemble algorithm that selects a search strategy according to the characteristics of a class. Our further experiments demonstrate that this ensemble approach is more effective than each individual strategy in our automated workflow for improving classification models using LLMs."
    },
    {
        "title": "Preference Diffusion for Recommendation",
        "link_suffix": "/forum?id=6GATHdOi1x",
        "link": "https://openreview.net/forum?id=6GATHdOi1x",
        "pdf_link": "https://openreview.net/pdf?id=6GATHdOi1x",
        "keywords": "Sequential Recommendation，Diffusion Model",
        "abstract": "Recommender systems predict personalized item rankings based on user preference distributions derived from historical behavior data. Recently, diffusion models (DMs) have gained attention in recommendation for their ability to model complex distributions, yet current DM-based recommenders often rely on traditional objectives like mean squared error (MSE) or recommendation objectives, which are not optimized for personalized ranking tasks or fail to fully leverage DM's generative potential. To address this, we propose PreferDiff, a tailored optimization objective for DM-based recommenders. PreferDiff transforms BPR into a log-likelihood ranking objective and integrates multiple negative samples to better capture user preferences. Specifically, we employ variational inference to handle the intractability through minimizing the variational upper bound and replaces MSE with cosine error to improve alignment with recommendation tasks. Finally, we balance learning generation and preference to enhance the training stability of DMs. PreferDiff offers three key benefits: it is the first personalized ranking loss designed specifically for DM-based recommenders and it improves ranking and faster convergence by addressing hard negatives. We also prove that it is theoretically connected to Direct Preference Optimization which indicates that it has the potential to align user preferences in DM-based recommenders via generative modeling. Extensive experiments across three benchmarks validate its superior recommendation performance and commendable general sequential recommendation capabilities. Our codes are available at \\url{https://anonymous.4open.science/r/PreferDiff}."
    },
    {
        "title": "SQL-GEN: Bridging the Dialect Gap for Text-to-SQL Via Synthetic Data And Model Merging",
        "link_suffix": "/forum?id=RaSLSUCKz0",
        "link": "https://openreview.net/forum?id=RaSLSUCKz0",
        "pdf_link": "https://openreview.net/pdf?id=RaSLSUCKz0",
        "keywords": "Text-to-SQL, LLM, Synthetic Data Generation, Databases",
        "abstract": "Text-to-SQL systems, that convert natural language queries into SQL programs, have seen significant progress with recent breakthroughs.  However,  these have been primarily for the SQLite dialect and adapting Text-to-SQL systems to other SQL dialects like BigQuery and PostgreSQL remains a challenge due to the diversity  in  SQL  syntaxes  and  functions,  along  with  the  high  cost  of  collecting and curating SQL-specific training data.   To this end,  we introduce SQL-GEN,a framework for generating high-quality synthetic data for any dialect guided by dialect-specific tutorials.  We demonstrate the effectiveness of SQL-GEN in creating training data to significantly improve the downstream Text-to-SQL performance for other dialects – it improves the execution accuracy by up to 20% over previous  methods,  and  reduces  the  gap  with  large-scale  human-annotated  data on unseen real world multi-dialect benchmarks.  Moreover, combining our synthetic data with human-annotated data provides additional performance boosts up to 5.6%. Towards unifying the multi-dialect capability in a single system, we also introduce a novel Mixture of Experts (MoE) initialization method that integrates dialect-specific models by merging self-attention layers and initializing the gates with dialect-specific keywords, yielding one unified and versatile model adept for multiple SQL dialects,  further enhancing performance across different SQL dialects. By leveraging shared core features of multiple dialect-specific models, our MOE demonstrated superior performance compared with models trained on individual dialects alone"
    },
    {
        "title": "DyGNeX : Efficient Distributed Training of Dynamic Graph Neural Networks with Cross-Time-Window Scheduling",
        "link_suffix": "/forum?id=daVCPIBCtQ",
        "link": "https://openreview.net/forum?id=daVCPIBCtQ",
        "pdf_link": "https://openreview.net/pdf?id=daVCPIBCtQ",
        "keywords": "Dynamic Graph Neural Networks, Distributed training, load balance",
        "abstract": "Dynamic Graph Neural Networks (DGNNs) are advanced methods for processing evolving graph data, capturing both structural and temporal dependencies efficiently. However, existing distributed DGNN training methods face challenges in achieving load balance across GPUs and minimizing communication overhead, which limits their efficiency.  In this paper, we introduce DyGNeX, a distributed training system designed to address this issue. DyGNeX utilizes a cross-time-window snapshot group scheduling algorithm that balances computational loads across GPUs without introducing additional cross-GPU feature aggregation or hidden state communication. Based on the specific scenario, the scheduling algorithm is applied using greedy or Integer Linear Programming (ILP) methods, referred to as DyGNeX-G and DyGNeX-L, respectively. DyGNeX-L and DyGNeX-G achieve average reductions of 28% and 24% in per-epoch training time compared to state-of-the-art methods, maintaining load imbalance across GPUs at approximately 4% and 8%, while preserving model convergence across various DGNN models and datasets. In simulation experiments, as the number of GPUs increases, DyGNeX-G shows good scalability, efficiently handling clusters with up to 512 GPUs while maintaining 95% efficiency."
    },
    {
        "title": "Your Actions Talk: DUET - A Multimodal Dataset for Contextualizable Dyadic Activities",
        "link_suffix": "/forum?id=PWia19rgzV",
        "link": "https://openreview.net/forum?id=PWia19rgzV",
        "pdf_link": "https://openreview.net/pdf?id=PWia19rgzV",
        "keywords": "Dyadic activity datasets, dyadic human activity recognition, contextualization, kinesics",
        "abstract": "Human activity recognition (HAR) has advanced significantly with the availability of diverse datasets, yet the field remains limited by a scarcity of resources focusing on two-person, or ''dyadic,'' interactions. Existing datasets primarily cater to single-person activities, overlooking the intricate dynamics and contextual dependencies present in interactions between two individuals. Failing to extend HAR to dyadic settings limits opportunities to enhance areas like collaborative learning, healthcare, robotics, augmented reality, and psychological assessments by better understanding interpersonal dynamics. To address this gap, we introduce the Dyadic User Engagement dataseT (DUET), a comprehensive dataset designed to enhance the understanding and recognition of dyadic activities. DUET comprises 14,400 video samples across 12 interaction classes, capturing the highest sample-class ratio known to date. Each sample is recorded using RGB, depth, infrared, and 3D skeleton joints, ensuring a robust dataset for multimodal analysis. DUET features a detailed taxonomization of interactions based on five fundamental communication functions: emblems, illustrators, affect displays, regulators, and adaptors. This classification, rooted in psychology, supports human activity contextualization by extracting the embedded semantics of bodily movements. Data collection was conducted at three locations using a novel technique that captures interactions from multiple views with a single camera, thereby improving model resilience against background noise and view variations. We benchmark six state-of-the-art, open-source HAR algorithms on DUET, demonstrating the dataset's complexity and current models' limitations in recognizing dyadic interactions. Our results highlight the need for further research into multimodal and context-aware HAR methodologies. DUET is publicly available at \\url{https://huggingface.co/datasets/Anonymous-Uploader1/DUET}, providing a valuable resource for the research community to advance HAR in dyadic settings."
    },
    {
        "title": "Sparsity beyond TopK: A Novel Cosine Loss for Sparse Binary Representations",
        "link_suffix": "/forum?id=UbLvSPMvMA",
        "link": "https://openreview.net/forum?id=UbLvSPMvMA",
        "pdf_link": "https://openreview.net/pdf?id=UbLvSPMvMA",
        "keywords": "sparse, binary, interpretability, latent, embedding, vector, representations, cosine similarity, sigmoid",
        "abstract": "While binary vectorization and sparse representations have recently emerged as promising strategies for efficient vector storage and mechanistic interpretability, the integration of these two paradigms has till now remained largely unexplored.\nIn this paper, we introduce an exciting approach for sparse binary representations, leveraging a soft TopK Cosine Loss to facilitate the transition from dense to sparse latent spaces.\nUnlike traditional TopK methods which impose rigid sparsity constraints, our approach naturally yields a more flexible distribution of activations, effectively capturing the varying degrees of conceptual depth present in the data.\nFurthermore, our cosine loss formulation inherently mitigates the emergence of inactive features, thereby eliminating the need for complex re-activation strategies prevalent in other recent works. \nWe validate our method on a large dataset of biomedical concept embeddings, demonstrating enhanced interpretability and significant reductions in storage overhead.\nOur present findings highlight the clear potential of cosine-based binary sparsity alignment for developing interpretable and efficient concept representations, positioning our approach as a compelling solution for applications in decision-making systems and compact vector databases."
    },
    {
        "title": "VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment",
        "link_suffix": "/forum?id=5mJrGtXVwz",
        "link": "https://openreview.net/forum?id=5mJrGtXVwz",
        "pdf_link": "https://openreview.net/pdf?id=5mJrGtXVwz",
        "keywords": "LLM, Reasoning, Credit Assignment, RLHF, Post-Training",
        "abstract": "Large language models (LLMs) are increasingly applied to complex reasoning tasks that require executing several complex steps before receiving any reward. Properly assigning credit to these steps is essential for enhancing model performance. Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning (RL) algorithm used for LLM finetuning, employs value networks to tackle credit assignment. However, value networks face challenges in predicting the expected cumulative rewards accurately in complex reasoning tasks, often leading to high-variance updates and suboptimal performance. In this work, we systematically evaluate the efficacy of value networks and reveal their significant shortcomings in reasoning-heavy LLM tasks, showing that they barely outperform a random baseline when comparing alternative steps. To address this, we propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks. Our method consistently outperforms PPO and other RL-free baselines across MATH and GSM8K datasets with fewer gradient updates (up to 9x), less wall-clock time (up to 3.0x). These results emphasize the importance of accurate credit assignment in  RL finetuning of LLM and demonstrate VinePPO’s potential as a superior alternative."
    },
    {
        "title": "Dynamic multi-channel EEG graph modeling for time-evolving brain network",
        "link_suffix": "/forum?id=ZkHtfl77JG",
        "link": "https://openreview.net/forum?id=ZkHtfl77JG",
        "pdf_link": "https://openreview.net/pdf?id=ZkHtfl77JG",
        "keywords": "EEG, Neuro Science, Graph, Time Series",
        "abstract": "We describe a novel dynamic graph neural network (GNN) approach for seizure detection and prediction from multi-channel Electroencephalography (EEG) data thet addresses several limitations of existing methods. \nWhile deep learning models have achieved notable success in automating seizure detection, static graph-based methods fail to capture the evolving nature of brain networks, especially during seizure events. \nTo overcome this, we propose EvoBrain, which uses a time-then-graph strategy that first models the temporal dynamics of EEG signals and graphs, and then employs GNNs to learn evolving spatial EEG representations. \nOur contributions include \n(a) a theoretical analysis proving the expressivity advantage of time-then-graph over other approaches, \n(b) a simple and efficient model that significantly improves AUROC and F1 scores compared with state-of-the-art methods, and \n(c) the introduction of dynamic graph structures that better reflect transient changes in brain connectivity. \nWe evaluate our method on the challenging early seizure prediction task.\nThe results show improved performance, making EvoBrain a valuable tool for clinical applications.\nThe source code is available at:https://anonymous.4open.science/r/EvoBrain-FBC5"
    },
    {
        "title": "HessianGrad: Optimizing AI Systems with Hessian-Aware Textual Gradients",
        "link_suffix": "/forum?id=0hc7iQLhCt",
        "link": "https://openreview.net/forum?id=0hc7iQLhCt",
        "pdf_link": "https://openreview.net/pdf?id=0hc7iQLhCt",
        "keywords": "LLM, Prompt Optimization, Gradient Descent",
        "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced the ability of LLM-based systems to perform complex tasks through natural language processing and tool interaction. However, optimizing these LLM-based systems for specific tasks remains challenging, often requiring manual interventions like prompt engineering and hyperparameter tuning. Existing automatic optimization methods, such as textual feedback-based techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to using first-order derivatives in traditional numerical gradient descent. However, relying solely on first-order derivatives can be limited when the gradient is either very small or fluctuates irregularly, which may slow down or stall optimization. To address these limitations, better adaptation in regions with small or fluctuating gradients is necessary. Second-order gradient methods, which incorporate the Hessian matrix, offer a promising solution by enabling more precise adjustments. Inspired by this, in this paper, we introduce HessianGrad, a novel optimization method that leverages textual feedback and tracks the iterative evolution of LLM systems responses across iterations, leading to more dynamic and adaptive optimization. We evaluate the effectiveness of HessianGrad on three tasks: prompt optimization, solution optimization, and code optimization. Experimental results demonstrate that HessianGrad consistently improves performance across all three tasks, achieving a7.8%improvement in prompt optimization, a20.72%gain in solution refinement, and a29.17%increase in code optimization compared to baselines, highlighting its adaptability and effectiveness in optimizing LLM-based systems."
    },
    {
        "title": "In-Context Learning at Representation Level via Unlabeled Texts",
        "link_suffix": "/forum?id=Af7CsWMUNI",
        "link": "https://openreview.net/forum?id=Af7CsWMUNI",
        "pdf_link": "https://openreview.net/pdf?id=Af7CsWMUNI",
        "keywords": "In-context Learning, Language models, Zero-shot, Representation Learning",
        "abstract": "Large language models (LLMs) have exhibited impressive capability of In-Context\nLearning (ICL), where LLMs perform relatively complicated tasks beyond the\npre-training objective by conditioning on the given demonstrations. Nevertheless,\nICL introduces two gaps between pre-training and inference: label appearance\n(presence of inserted labels in the demonstrations) and weak semantic relevance\n(independently sampled demonstrations exhibit less semantic coherence compared\nto consecutive text segments in pretraining corpora). We propose a new inference\nmethod that only use unlabeled inputs from the test set and label space. In this\nmethod, we extract the representations of the demonstrations inputs independently\nand fuse them to reshape the representation of the test input for inference. Inter-\nestingly, without access to labels, our method outperforms traditional ICL with\nextra information of gold labels. Furthermore, our method allows small models\nto outperform the zero-shot performance of models that are twice their size (e.g.,\nGPT-Neo-2.7B surpasses Llama2-7B, and Llama2-7B outperforms Llama2-13B).\nOur code will be available at this."
    }
]