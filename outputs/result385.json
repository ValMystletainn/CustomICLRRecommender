[{"title": "DriveE2E: Benchmarking Closed-Loop End-to-End Autonomous Driving Based-on Real-World Traffic Scenarios", "link_suffix": "/forum?id=acPDTHPsOz", "link": "https://openreview.net/forum?id=acPDTHPsOz", "pdf_link": "https://openreview.net/pdf?id=acPDTHPsOz", "keywords": "Autonomous Driving, End-to-End, benchmark", "abstract": "End-to-end learning has demonstrated considerable promise in advancing autonomous driving by fully leveraging sensor data. Recently, many end-to-end models have been developed, with a substantial number evaluated using the nuScenes dataset in an open-loop manner. However, open-loop evaluations, which lack interaction with the environment, fail to fully capture the driving capabilities of these models. While closed-loop evaluations, such as those using the CARLA simulator, allow for interaction with the environment, they often rely on rule-based, manually configured traffic scenarios. This approach leads to evaluations that diverge significantly from real-world driving conditions, thus limiting their ability to reflect actual driving performance.\nTo address these limitations, we introduce a novel closed-loop evaluation framework that closely integrates real-world driving scenarios with the CARLA simulator, effectively bridging the gap between simulated environments and real-world driving conditions. Our approach involves the creation of digital twins for 15 real-world intersections and the incorporation of 800 real-world traffic scenarios selected from a comprehensive 100-hour video dataset captured with highly installed infrastructure sensors. These digital twins accurately replicate the physical and environmental characteristics of their real-world counterparts, while the traffic scenarios capture a diverse range of driving behaviors, locations, weather conditions, and times of day. Within this twinned environment, CARLA enables realistic simulations where autonomous agents can dynamically interact with their surroundings. Furthermore, we have established a comprehensive closed-loop benchmark that evaluates end-to-end autonomous driving models across these diverse scenarios. Notably, this is the first closed-loop end-to-end autonomous driving benchmark based on real-world traffic scenarios. Video demos are provided in the supplementary materials.", "title_embedding_index": 19200, "title_abs_embedding_index": 19225}, {"title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis", "link_suffix": "/forum?id=Kb9PnkWYNT", "link": "https://openreview.net/forum?id=Kb9PnkWYNT", "pdf_link": "https://openreview.net/pdf?id=Kb9PnkWYNT", "keywords": "Text-to-Video, Diffusion Models, Temporal Attention", "abstract": "Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. Meanwhile, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To tackle this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we adjust the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: Video Synopsis Prompting (VSP) and Temporal Attention Regularization (TAR), the latter being our core contribution. Based on a systematic analysis, we discover that the temporal units in pretrained T2V models are crucial to control the video dynamics. Upon this finding, we propose a novel regularization technique to refine the temporal attention, enabling training-free longer video synthesis in a single inference pass. For prompts involving visual progression, we leverage LLMs to generate video synopsis - description of key visual states - based on the original prompt to provide better guidance along the temporal axis. We experimentally showcase the superiority of our method in synthesizing longer, visually appealing videos over open-sourced T2V models.", "title_embedding_index": 19201, "title_abs_embedding_index": 19226}, {"title": "Towards Scalable Exact Machine Unlearning Using Parameter-Efficient Fine-Tuning", "link_suffix": "/forum?id=oe51Q5Uo37", "link": "https://openreview.net/forum?id=oe51Q5Uo37", "pdf_link": "https://openreview.net/pdf?id=oe51Q5Uo37", "keywords": "Machine Unlearning, Data Deletion, Exact Unlearning", "abstract": "Machine unlearning is the process of efficiently removing the influence of a training data instance from a trained machine learning model without retraining it from scratch. A popular subclass of unlearning approaches is exact machine unlearning, which focuses on techniques that explicitly guarantee the removal of the influence of a data instance from a model.  Exact unlearning approaches use a machine learning model in which individual components are trained on disjoint subsets of the data. During deletion, exact unlearning approaches only retrain the affected components rather than the entire model. While existing approaches reduce retraining costs, it can still be expensive for an organization to retrain a model component as it requires halting a system in production, which leads to service failure and adversely impacts customers.  To address these challenges, we introduce an exact unlearning framework -- Sequence-aware Sharded Sliced Training  (S3T), which is designed to enhance the deletion capabilities of an exact unlearning system while minimizing the impact on model's performance. At the core of S3T, we utilize a lightweight parameter-efficient fine-tuning approach that enables parameter isolation by sequentially training layers with disjoint data slices. This enables efficient unlearning by simply deactivating the layers affected by data deletion. Furthermore, to reduce the retraining cost and improve model performance, we train the model on multiple data sequences, which allows S3T to handle an increased number of deletion requests. Both theoretically and empirically, we demonstrate that S3T attains superior deletion capabilities and enhanced performance compared to baselines across a wide range of settings.", "title_embedding_index": 19202, "title_abs_embedding_index": 19227}, {"title": "Retrieval Head Mechanistically Explains Long-Context Factuality", "link_suffix": "/forum?id=EytBpUGB1Z", "link": "https://openreview.net/forum?id=EytBpUGB1Z", "pdf_link": "https://openreview.net/pdf?id=EytBpUGB1Z", "keywords": "Large language models, long context, interpretability, attention", "abstract": "Despite the recent progress in long-context language models, it remains elusive how transformer-based models exhibit the capability to retrieve relevant information from arbitrary locations within the long context. This paper aims to address this question. Our systematic investigation across a wide spectrum of models reveals that a special type of attention heads are largely responsible for retrieving information, which we dub retrieval heads. We identify intriguing properties of retrieval heads:(1) universal: all the explored models with long-context capability have a set of retrieval heads; (2) sparse: only a small portion (less than 5%) of the attention heads are retrieval. (3) intrinsic: retrieval heads already exist in models pretrained with short context. When extending the context length by continual pretraining, it is still the same set of heads that perform information retrieval. (4) dynamically activated: take Llama-2 7B for example, 12 retrieval heads always attend to the required information no matter how the context is changed. The rest of the retrieval heads are activated in different contexts. (5) causal: completely pruning retrieval heads leads to failure in retrieving relevant information and results in hallucination, while pruning random non-retrieval heads does not affect the model's retrieval ability. We further show that retrieval heads strongly influence chain-of-thought (CoT) reasoning, where the model needs to frequently refer back the question and previously-generated context. Conversely, tasks where the model directly generates the answer using its intrinsic knowledge are less impacted by masking out retrieval heads. These observations collectively explain which internal part of the model seeks information from the input tokens. We believe our insights will foster future research on reducing hallucination, improving reasoning, and compressing the KV cache.", "title_embedding_index": 19203, "title_abs_embedding_index": 19228}, {"title": "LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA", "link_suffix": "/forum?id=mMXdHyBcHh", "link": "https://openreview.net/forum?id=mMXdHyBcHh", "pdf_link": "https://openreview.net/pdf?id=mMXdHyBcHh", "keywords": "long context, large language model, attributed LLM", "abstract": "Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o. We also discover that SFT with citation information can further improve the correctness of model responses compared to standard long-context SFT.", "title_embedding_index": 19204, "title_abs_embedding_index": 19229}, {"title": "Truth-Guided Negative Sampling in Self-supervised Graph Representation Learning", "link_suffix": "/forum?id=juZNRYRMPn", "link": "https://openreview.net/forum?id=juZNRYRMPn", "pdf_link": "https://openreview.net/pdf?id=juZNRYRMPn", "keywords": "graph representation learning, negative sampling, recommendation system", "abstract": "Negative sampling is an important yet challenging component in self-supervised graph representation learning, particularly for recommendation systems where user-item interactions are modeled as bipartite graphs. Existing methods often rely on heuristics or human-specified principles to design negative sampling distributions. This potentially overlooks the usage of an underlying ``true'' negative distribution, which we might be able to access as an oracle despite not knowing its exact form. \nIn this work, we shift the focus from manually designing negative sampling distributions to a method that approximates and leverages the underlying true distribution.  We expand this idea in the analysis of two scenarios: (1) when the observed graph is an unbiased sample from the true distribution, and (2) when the observed graph is biased with partially observable positive edges. The analysis result is the derivation of a sampling strategy as the numerical approximation of a well-established learning objective. Our theoretical findings are also empirically validated, and our new sampling methods achieve state-of-the-art performance on real-world datasets.", "title_embedding_index": 19205, "title_abs_embedding_index": 19230}, {"title": "Autoencoder-Based Hybrid Replay for Class-Incremental Learning", "link_suffix": "/forum?id=0eu837jdBD", "link": "https://openreview.net/forum?id=0eu837jdBD", "pdf_link": "https://openreview.net/pdf?id=0eu837jdBD", "keywords": "Catastrophic Forgetting, Class-Incremental Learning, Continual Learning, Task Confusion.", "abstract": "In class-incremental learning (CIL), effective incremental learning strategies are essential to mitigate task confusion and catastrophic forgetting, especially as the number of tasks $t$ increases. Current exemplar replay strategies impose $\\mathcal{O}(t)$ memory/compute complexities. We propose an autoencoder-based hybrid replay (AHR) strategy that leverages our new hybrid autoencoder (HAE) to function as a compressor to alleviate the requirement for large memory, achieving $\\mathcal{O}(0.1 t)$ at the worst case with the computing complexity of $\\mathcal{O}(t)$ while accomplishing state-of-the-art performance. The decoder later recovers the exemplar data stored in the latent space, rather than in raw format. Additionally, HAE is designed for both discriminative and generative modeling, enabling classification and replay capabilities, respectively. HAE adopts the charged particle system energy minimization equations and repulsive force algorithm for the incremental embedding and distribution of new class centroids in its latent space. Our results demonstrate that AHR consistently outperforms recent baselines across multiple benchmarks while operating with the same memory/compute budgets.", "title_embedding_index": 19206, "title_abs_embedding_index": 19231}, {"title": "Replicate and Quantize: A Plug-and-Play Strategy for Load Balancing in Sparse Mixture-of-Experts LLMs", "link_suffix": "/forum?id=0wfmHoKQX6", "link": "https://openreview.net/forum?id=0wfmHoKQX6", "pdf_link": "https://openreview.net/pdf?id=0wfmHoKQX6", "keywords": "mixture-of-experts;load balance", "abstract": "While the rapid increase in the number of model parameters poses significant benefits to the development of large language models (LLMs), computational costs are also raised. In order to tackle this difficulty, the sparse mixture-of-experts(SMoE) model was introduced to tackle LLM scaling by activating a subset of experts per input. Therefore, how to leverage the knowledge of multiple experts will be an important topic. Normally, in the most extreme scenario, employing a balanced expert allocation system will result in a time-saving of $n$ times compared to utilizing only a single expert. Thus, in this paper we (1) systematically analyzed the performance and functionality of each expert. (2) Introduced a metric to fill the blank of evaluating load balance for the sparse mixture-of-experts(SMoE) model, based on the observation. (3) Proposed a dynamic plug-and-play strategy that is both trainingless and near-lossless, effectively resolving the load balancing problem, in contrast to previous works that focused on training strategies.", "title_embedding_index": 19207, "title_abs_embedding_index": 19232}, {"title": "Understanding the Stability-based Generalization of Personalized Federated Learning", "link_suffix": "/forum?id=znhZbonEoe", "link": "https://openreview.net/forum?id=znhZbonEoe", "pdf_link": "https://openreview.net/pdf?id=znhZbonEoe", "keywords": "stability analysis+generalization gap+excess risk+personalized federated learning", "abstract": "Despite great achievements in algorithm design for Personalized Federated Learning (PFL), research on the theoretical analysis of generalization is still in its early stages. Some recent theoretical results have investigated the generalization performance of personalized models under the problem setting and hypothesis in the convex condition, which do not consider the real iteration performance during the non-convex training. To further understand the testing performance from the theoretical perspective, we propose the first algorithm-matter generalization analysis with uniform stability for the typical PFL method Partial Model Personalization on smooth and non-convex objectives. In an attempt to distinguish the shared and personalized errors, we decouple the shared aggregation and the local fine-tuning progress and illustrate the interaction mechanism between the shared and personalized variables. The algorithm-matter generalization bounds analyze the impact of the trivial hyperparameters like learning steps and stepsizes as well as the communication modes in both Centralized and Decentralized PFL (C-PFL and D-PFL), which also concludes that C-PFL generalizes better than D-PFL. Combined with the convergence errors, we then obtain the excess risk analysis and establish the better early stopping point for the optimal population risk of PFL. Promising experiments on CIFAR dataset also corroborate our theoretical results.", "title_embedding_index": 19208, "title_abs_embedding_index": 19233}, {"title": "H-QLoRA: Enhancing Quantized LLMs with Hierarchical Residual Learning", "link_suffix": "/forum?id=B4S1GAMBLG", "link": "https://openreview.net/forum?id=B4S1GAMBLG", "pdf_link": "https://openreview.net/pdf?id=B4S1GAMBLG", "keywords": "parameter efficient fine-tuning (PEFL), quantized LLMs, LoRA, hierarchical learning", "abstract": "Fine-tuning large language models (LLMs) in resource-constrained environments poses significant challenges due to their size and computational demands. While current methods often rely on aggressive weight quantization to alleviate memory and computational costs, this can lead to a noticeable loss of accuracy. This paper introduces H-QLoRA, a novel approach that leverages hierarchical adaptors with low-rank weights to enhance performance. By fine-tuning models from the LLaMA and Gemma families, we demonstrate H-QLoRA's efficacy across multiple instruction datasets. H-QLoRA not only outperforms state-of-the-art results for certain model types by recovering high-frequency information lost during 4-bit weight quantization, but it also maintains efficiency in terms of inference costs and memory usage. While traditional methods may compromise accuracy in pursuit of efficiency, H-QLoRA mitigates this issue by implementing a hierarchical adaptor structure that captures more nuanced patterns within the data. This allows H-QLoRA to fine-tune models with the same number of trainable parameters as QLoRA, yet it proves to be more optimal for specific architectures. Overall, H-QLoRA aims to enhance fine-tuning outcomes for quantized models in low-resource environments.", "title_embedding_index": 19209, "title_abs_embedding_index": 19234}, {"title": "How Does Cross-Layer Correlation in Deep Neural Networks Influence Generalization and Adversarial Robustness?", "link_suffix": "/forum?id=MJ8ALv35sj", "link": "https://openreview.net/forum?id=MJ8ALv35sj", "pdf_link": "https://openreview.net/pdf?id=MJ8ALv35sj", "keywords": "Deep Neural Network, Generalization, Adversarial Robustness", "abstract": "\\textit{Generalization} and \\textit{adversarial robustness} are two critical concepts in machine learning. Understanding the key factors that affect the trade-off between these concepts is essential for guiding architectural design and developing training strategies, such as adversarial training, especially for deep neural networks. In this paper, we investigate the impact of cross-layer correlations in weight matrices on both generalization and adversarial robustness. We provide a theoretical analysis demonstrating that increasing cross-layer correlations leads to a monotonic increase in the generalization gap. Furthermore, we establish a connection between adversarial risk and natural risk. Leveraging this connection, we show that in linear models, higher cross-layer correlations also degrade adversarial robustness. Finally, we validate our theoretical findings through experiments conducted on MLPs.", "title_embedding_index": 19210, "title_abs_embedding_index": 19235}, {"title": "Disentangling the QiGan Encoded by a DNN towards the Go Game", "link_suffix": "/forum?id=FnIRtzK5wX", "link": "https://openreview.net/forum?id=FnIRtzK5wX", "pdf_link": "https://openreview.net/pdf?id=FnIRtzK5wX", "keywords": "Katago, Knowledge discovery", "abstract": "Given a deep neural network (DNN) that has surpassed human beings in a task, disentangling the explicit knowledge encoded by the DNN to obtain some new insights into the task is a new promising-yet-challenging regime in explainable AI. In this paper, we aim to disentangle the ''QiGan'' encoded by the AI model for the Go game, which has beat top human players. Specifically, we disentangle primitive shape patterns of stones memorized by the value network, and these shape patterns represent the ''QiGan'' used to conduct a fast situation assessment of the current board state. The universal-matching property of interactions ensure that human players can learn accurate and verifiable shape patterns, rather than specious intuitive analysis. In experiments, our method explains lots of novel shape patterns beyond traditional shape patterns in human knowledge.", "title_embedding_index": 19211, "title_abs_embedding_index": 19236}, {"title": "Think Beyond Size: Dynamic Prompting for More Effective Reasoning", "link_suffix": "/forum?id=c87QZPTVVm", "link": "https://openreview.net/forum?id=c87QZPTVVm", "pdf_link": "https://openreview.net/pdf?id=c87QZPTVVm", "keywords": "model efficiency, prompt optimization, task-specific prompting, small-scale LLMs, real-time adjustment, efficient problem-solving, adaptive reasoning, incremental prompting, step-wise refinement, model performance improvement, task adaptability, few-shot prompting, scalable models.", "abstract": "This paper presents Dynamic Prompting, a novel framework aimed at improving the reasoning capabilities of Large Language Models (LLMs). In contrast to conventional static prompting methods, Dynamic Prompting enables the adaptive modification of prompt sequences and step counts based on real-time task complexity and model performance. This dynamic adaptation facilitates more efficient problem-solving, particularly in smaller models, by reducing hallucinations and repetitive cycles. Our empirical evaluations demonstrate that Dynamic Prompting allows smaller LLMs to perform competitively with much larger models, thereby challenging the conventional emphasis on model size as the primary determinant of reasoning efficacy.", "title_embedding_index": 19212, "title_abs_embedding_index": 19237}, {"title": "PDDFormer: Pairwise Distance Distribution Graph Transformer for Crystal Material Property Prediction", "link_suffix": "/forum?id=ewjN1MAnJi", "link": "https://openreview.net/forum?id=ewjN1MAnJi", "pdf_link": "https://openreview.net/pdf?id=ewjN1MAnJi", "keywords": "Pairwise Distance Distribution, Graph Transformer, Periodic crystal", "abstract": "The crystal structure can be simplified as a periodic point set repeating across the entire three-dimensional space along an underlying lattice. Traditionally, methods for representing crystals rely on descriptors like lattice parameters, symmetry, and space groups to characterize the structure. However, in reality, atoms in material always vibrate above absolute zero, causing continuous fluctuations in their positions. This dynamic behavior disrupts the underlying periodicity of the lattice, making crystal graphs based on static lattice parameters and conventional descriptors discontinuous under even slight perturbations. To this end, chemists proposed the Pairwise Distance Distribution (PDD) method, which has been used to distinguish all periodic structures in the world's largest real materials collection, the Cambridge Structural Database. However, achieving the completeness of PDD requires defining a large number of neighboring atoms, resulting in high computational costs. Moreover, it does not account for atomic information, making it challenging to directly apply PDD to crystal material property prediction tasks. To address these challenges, we propose the atom-Weighted Pairwise Distance Distribution (WPDD) and Unit cell Pairwise Distance Distribution (UPDD) for the first time, incorporating them into the construction of multi-edge crystal graphs. Based on this, we further developed WPDDFormer and UPDDFormer, graph transformer architecture constructed using WPDD and UPDD crystal graphs. We demonstrate that this method maintains the continuity and completeness of crystal graphs even under slight perturbations in atomic positions. Moreover, by modeling PDD as global information and integrating it into matrix-based message passing, we significantly reduced computational costs. Comprehensive evaluation results show that WPDDFormer achieves state-of-the-art predictive accuracy across tasks on benchmark datasets such as the Materials Project and JARVIS-DFT.", "title_embedding_index": 19213, "title_abs_embedding_index": 19238}, {"title": "Highly efficient Speech Separation using relative Context", "link_suffix": "/forum?id=UPOUVsEafz", "link": "https://openreview.net/forum?id=UPOUVsEafz", "pdf_link": "https://openreview.net/pdf?id=UPOUVsEafz", "keywords": "speech separation, single-channel, source separation, sequence modelling, differencing, signal processing", "abstract": "Speech separation is a problem area where a mixture with overlapping speech signals is the input and estimations of the clean speech signals which make up the mixture is the output. In this paper we propose a novel sequence modelling method called relative context and use it for a speech separation architecture called RCSep. \nThe main advantages of relative context is that it does not require trainable parameters, is very lightweight and highly parallelized. The RCSep model which heavily uses relative context is an extremely efficient source separation model. It has less than 500k trainable parameters, lower memory usage and is significantly faster than all previous source separation methods while still maintaining high separation accuracy.\nFurthermore, we also used relative context instead of LSTMs in a current SOTA architecture which simultaneously improved separation accuracy and decreased computation time, memory usage and model size.", "title_embedding_index": 19214, "title_abs_embedding_index": 19239}, {"title": "Adaptive Rentention & Correction for Continual Learning", "link_suffix": "/forum?id=9bLdbp46Q1", "link": "https://openreview.net/forum?id=9bLdbp46Q1", "pdf_link": "https://openreview.net/pdf?id=9bLdbp46Q1", "keywords": "Continual Learning; Computer Vision; Transfer Learning", "abstract": "Continual learning, also known as lifelong learning or incremental learning, refers to the process by which a model learns from a stream of incoming data over time. A common problem in continual learning is the classification layer\u2019s bias towards the most recent task. Traditionally, methods have relied on incorporating data from past tasks during training to mitigate this issue. However, the recent shift in continual learning to memory-free environments has rendered these approaches infeasible. In this study, we propose a solution focused on the testing phase. We first introduce a simple Out-of-Task Detection method, OTD, designed to accurately identify samples from past tasks during testing. Leveraging OTD, we then propose: (1) an Adaptive Retention mechanism for dynamically tuning the classifier layer on past task data; (2) an Adaptive Correction mechanism for revising predictions when the model classifies data from previous tasks into classes from the current task. We name our approach Adaptive Retention & Correction (ARC). While designed for memory-free environments, ARC also proves effective in memorybased settings. Extensive experiments show that our proposed method can be plugged in to virtually any existing continual learning approach without requiring any modifications to its training procedure. Specifically, when integrated with state-of-the-art approaches, ARC achieves an average performance increase of 2.7% and 2.6% on the CIFAR-100 and Imagenet-R datasets, respectively", "title_embedding_index": 19215, "title_abs_embedding_index": 19240}, {"title": "Unsupervised Model Tree Heritage Recovery", "link_suffix": "/forum?id=QVj3kUvdvl", "link": "https://openreview.net/forum?id=QVj3kUvdvl", "pdf_link": "https://openreview.net/pdf?id=QVj3kUvdvl", "keywords": "Weight Space Learning, Deep Weight Space, Model Tree", "abstract": "The number of models shared online has recently skyrocketed, with over one million public models available on Hugging Face. Sharing models allows other users to build on existing models, using them as initialization for fine-tuning, improving accuracy and saving compute and energy. However, it also raises important intellectual property issues, as fine-tuning may violate the license terms of the original model or that of its training data. A Model Tree, i.e., a tree data structure rooted at a foundation model and having directed edges between a parent model and other models directly fine-tuned from it (children), would settle such disputes by making the model heritage explicit. Unfortunately, current models are not well documented, with most model metadata (e.g., \"model cards\") not providing accurate information about heritage. In this paper, we introduce the task of Unsupervised Model Tree Heritage Recovery (Unsupervised MoTHer Recovery) for collections of neural networks. For each pair of models, this task requires: i) determining if they are directly related, and ii) establishing the direction of the relationship. Our hypothesis is that model weights encode this information, the challenge is to decode the underlying tree structure given the weights. We discover several properties of model weights that allow us to perform this task. By using these properties, we formulate the MoTHer Recovery task as finding a directed minimal spanning tree. In extensive experiments we demonstrate that our method successfully reconstructs complex Model Trees.", "title_embedding_index": 19216, "title_abs_embedding_index": 19241}, {"title": "Deep Linear Probe Generators for Weight Space Learning", "link_suffix": "/forum?id=XoYdD3m0mv", "link": "https://openreview.net/forum?id=XoYdD3m0mv", "pdf_link": "https://openreview.net/pdf?id=XoYdD3m0mv", "keywords": "Deep Weight Space, Weight Classification", "abstract": "Weight space learning aims to extract information about a neural network, such as its training dataset or generalization error. Recent approaches learn directly from model weights, but this presents many challenges as weights are high-dimensional and include permutation symmetries between neurons. An alternative approach, Probing, represents a model by passing a set of learned inputs (probes) through the model, and training a predictor on top of the corresponding outputs. Although probing is typically not used as a stand alone approach, our preliminary experiment found that a vanilla probing baseline worked surprisingly well. However, we discover that current probe learning strategies are ineffective. We therefore propose Deep Linear Probe Generators (ProbeGen), a simple and effective modification to probing approaches. ProbeGen adds a shared generator module with a deep linear architecture, providing an inductive bias towards structured probes thus reducing overfitting.\nWhile simple, ProbeGen performs significantly better than the state-of-the-art and is very efficient, requiring between 30 to 1000 times fewer FLOPs than other top approaches.", "title_embedding_index": 19217, "title_abs_embedding_index": 19242}, {"title": "Questioning Simplicity Bias Assumptions", "link_suffix": "/forum?id=bU0JMHJ8zL", "link": "https://openreview.net/forum?id=bU0JMHJ8zL", "pdf_link": "https://openreview.net/pdf?id=bU0JMHJ8zL", "keywords": "Simplicity Bias, Shortcut Learning", "abstract": "The Simplicity Bias (SB) is the observation that the training of most commonly used neural network architectures with standard training techniques is biased toward learning simple functions. This phenomenon can be a benefit or drawback depending on the relative complexity of the desired function to be learnt. If the desired function is relatively simple it's a positive. However, if there are simpler features that are highly predictive; commonly named shortcuts or spurious features, that are not present in the test environment, the SB can result in poor generalisation performance. Most existing works on mitigating the SB make various assumptions, either about the features present in the train and test domains or by assuming access to information about the test domain at train time. In this paper we review recent work on the SB and take a critical look at these assumptions.", "title_embedding_index": 19218, "title_abs_embedding_index": 19243}, {"title": "FairFedMed: Achieving Equity in Medical Federated Learning via FairLoRA", "link_suffix": "/forum?id=hjdIQ91ssY", "link": "https://openreview.net/forum?id=hjdIQ91ssY", "pdf_link": "https://openreview.net/pdf?id=hjdIQ91ssY", "keywords": "Fairness, Equity, Federated Learning, FairLoRA", "abstract": "Fairness remains a critical concern in healthcare, where unequal access to services and treatment outcomes can adversely affect patient health. While Federated Learning (FL) presents a collaborative and privacy-preserving approach to model training, ensuring fairness is challenging due to heterogeneous data across institutions, and current research primarily addresses non-medical applications. To fill this gap, we introduce FairFedMed, the first FL dataset specifically designed to study group fairness (i.e., demographics) in the medical field. It consists of paired 2D SLO funfus images and 3D OCT B-Scans from 15,165 glaucoma patients, along with six different demographic attributes. Existing state-of-the-art FL models may work well for natural images but often struggle with medical images due to their unique characteristics. Moreover, these models do not sufficiently address performance disparities across diverse demographic groups. To overcome these limitations, we propose FairLoRA, a novel fairness-aware FL framework based on singular value decomposition(SVD)-based low-rank approximation. FairLoRA incorporates customized singular value matrices for each demographic group and shares singular vector matrices across all demographic groups, ensuring both model equity and computational efficiency. Experimental results on the FairFedMed dataset demonstrate that FairLoRA not only achieves state-of-the-art performance in medical image classification but also significantly improves fairness across diverse populations. Our code and dataset can be accessible via the Github anonymous link:https://github.com/Anonymouse4Science/FairFedMed-FairLoRA.git", "title_embedding_index": 19219, "title_abs_embedding_index": 19244}, {"title": "On the Role of Image Statistics and Gradient Learning in the Adversarial Vulnerability of Neural Networks", "link_suffix": "/forum?id=xIW2WtCuYE", "link": "https://openreview.net/forum?id=xIW2WtCuYE", "pdf_link": "https://openreview.net/pdf?id=xIW2WtCuYE", "keywords": "Adversarial Examples, Image Statistics, Gradient Learning", "abstract": "Perhaps the most surprising failure of classifiers learned by modern neural networks is that they  can be fooled by tiny, imperceptible,  perturbations to the input. \n  In this paper, we present theoretical and empirical results which\n  suggest that this failure is related to the use of randomly-initialized gradient-based learning together with the statistics of natural images. Our results are based on the previously reported 'PC-bias' of gradient-based learning: projections of the classifier in directions with large variance are learned much faster than directions with small variance. We prove that when the PC-bias is combined with the rapidly decreasing eigenspectrum of natural images, then gradient learning will provably learn a classifier that is highly vulnerable to small perturbations and we show experimentally that this behavior occurs when training deep, nonlinear neural networks. We use our analysis to suggest a simple post-processing of a learned classifier which can significantly improve its robust accuracy.", "title_embedding_index": 19220, "title_abs_embedding_index": 19245}, {"title": "Efficient Gradient Clipping Methods in DP-SGD for Convolution Models", "link_suffix": "/forum?id=bykD2108Qv", "link": "https://openreview.net/forum?id=bykD2108Qv", "pdf_link": "https://openreview.net/pdf?id=bykD2108Qv", "keywords": "Differential Privacy, SGD, Clipping, CNNs, FFT, DP-SGD, Computational Complexity", "abstract": "Differentially private stochastic gradient descent (DP-SGD) is a well-known method for training machine learning models with  a specified level of privacy. \nHowever, its basic implementation is generally bottlenecked by the computation of the gradient norm (gradient clipping) for each example in an input batch. \nWhile various techniques have been developed to mitigate this issue, \nthere are only a handful of methods pertaining to convolution models, e.g., vision models.\nIn this work, we present three methods for performing gradient clipping that improve upon previous state-of-art methods. Two of these methods use in-place operations to reduce memory overhead, while the third one leverages a relationship between Fourier transforms and convolution layers. \nTo demonstrate the numerical efficiency of our methods, we also present several benchmark experiments that compare against other algorithms.", "title_embedding_index": 19221, "title_abs_embedding_index": 19246}, {"title": "Do WGANs succeed because they minimize the Wasserstein Distance? Lessons from Discrete Generators", "link_suffix": "/forum?id=7YXaOvunqo", "link": "https://openreview.net/forum?id=7YXaOvunqo", "pdf_link": "https://openreview.net/pdf?id=7YXaOvunqo", "keywords": "GANs, Wasserstein Distance", "abstract": "Since WGANs were first introduced, there has been considerable debate whether their success in generating realistic images can be attributed to minimizing the Wasserstein distance between the distribution of  generated images and the training distribution. In this paper we present theoretical and experimental results that show that successful WGANs {\\em do} minimize the Wasserstein distance but the form of the distance that is minimized depends highly on the discriminator architecture and its inductive biases. Specifically, we show that when the discriminator is convolutional,  WGANs minimize the Wasserstein distance between {\\em patches} in the generated images and the training images,  not the Wasserstein distance between images.\nOur results are obtained by considering {\\em discrete} generators for which the Wasserstein distance between the generator distribution and the training distribution can be computed exactly and the minimum can be characterized analytically.   We present experimental results with discrete GANs that generate  realistic fake images (comparable in quality to their continuous counterparts) and present evidence that they are minimizing the Wasserstein distance between real and fake patches and not the distance between  real and fake images.", "title_embedding_index": 19222, "title_abs_embedding_index": 19247}, {"title": "Neural-Symbolic Message Passing with Dynamic Pruning", "link_suffix": "/forum?id=lA6du4Q0Zc", "link": "https://openreview.net/forum?id=lA6du4Q0Zc", "pdf_link": "https://openreview.net/pdf?id=lA6du4Q0Zc", "keywords": "knowledge graph, complex query answering, neural and symbolic, message passing", "abstract": "Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a fundamental yet challenging task. Given that KGs are usually incomplete, the CQA models not only need to execute logical operators, but aslo need to leverage observed knowledge to predict the missing one. Recently, a line of message-passing-based research has been proposed to re-use pre-trained neural link predictors to solve CQA. However, they perform unsatisfactorily on negative queries and fail to address the unnecessary noisy messages between variable nodes in the query graph. Moreover, like most neural CQA models, these message passing models offer little interpretability and require complex query data and resource-intensive training. In this paper, we propose a Neural-Symbolic Message Passing framework (NSMP), which integrates neural and symbolic reasoning. By re-using a simple pre-trained neural link predictor, NSMP generalizes to complex queries based on fuzzy logic theory, without requiring training on complex query datasets, while providing interpretable answers. Furthermore, we introduce an effective dynamic pruning strategy to filter out noisy messages between variable nodes during message passing. Empirically, our model demonstrates strong performance and offers efficient inference. Our code can be found athttps://anonymous.4open.science/r/NSMP.", "title_embedding_index": 19223, "title_abs_embedding_index": 19248}, {"title": "A Visual Case Study of the Training Dynamics in Neural Networks", "link_suffix": "/forum?id=s1f7jybVTo", "link": "https://openreview.net/forum?id=s1f7jybVTo", "pdf_link": "https://openreview.net/pdf?id=s1f7jybVTo", "keywords": "Training dynamics, Transformer, Mechanistic Interpretability, Visualization", "abstract": "This paper introduces a visual sandbox designed to explore the training dynamics of a small-scale transformer model, with the embedding dimension constrained to $d=2$\nThis restriction allows for a comprehensive two-dimensional visualization of each layer's dynamics. \nThrough this approach, we gain insights into training dynamics, circuit transferability, and the causes of loss spikes, including those induced by the high curvature of normalization layers. \nWe propose strategies to mitigate these spikes, demonstrating how good visualization facilitates the design of innovative ideas of practical interest.\nAdditionally, we believe our sandbox could assist theoreticians in assessing essential training dynamics mechanisms and integrating them into future theories.", "title_embedding_index": 19224, "title_abs_embedding_index": 19249}]