[
    {
        "title": "Beyond CVaR: Leveraging Static Spectral Risk Measures for Enhanced Decision-Making in Distributional Reinforcement Learning",
        "link_suffix": "/forum?id=3SMBSTG3qN",
        "link": "https://openreview.net/forum?id=3SMBSTG3qN",
        "pdf_link": "https://openreview.net/pdf?id=3SMBSTG3qN",
        "keywords": "Reinforcement Learning, Distributional Reinforcement Learning, Risk Aversion, Spectral Risk Measures, Time-Consistency",
        "abstract": "In domains such as finance, healthcare, and robotics, managing worst-case scenarios is critical, as failure to do so can lead to catastrophic outcomes. Distributional Reinforcement Learning (DRL) provides a natural framework to incorporate risk sensitivity into decision-making processes. However, existing approaches face two key limitations: (1) the use of fixed risk measures at each decision step often results in overly conservative policies, and (2) the interpretation and theoretical properties of the learned policies remain unclear. While optimizing a static risk measure addresses these issues, its use in the DRL framework has been limited to the simple static CVaR risk measure. In this paper, we present a novel DRL algorithm with convergence guarantees that optimizes for a broader class of static Spectral Risk Measures (SRM). Additionally, we provide a clear interpretation of the learned policy by leveraging the distribution of returns in DRL and the decomposition of static coherent risk measures. Extensive experiments demonstrate that our model learns policies aligned with the SRM objective, and outperforms existing risk-neutral and risk-sensitive DRL models in various settings."
    },
    {
        "title": "AutoCustomization: A Unified Framework for Effortless, Selective LLM Bias and Style Finetuning",
        "link_suffix": "/forum?id=M7CblLwJB8",
        "link": "https://openreview.net/forum?id=M7CblLwJB8",
        "pdf_link": "https://openreview.net/pdf?id=M7CblLwJB8",
        "keywords": "large language models, model customization",
        "abstract": "Large language models are transforming the landscape of applications, with their influence poised to expand. One important practical challenge is how to selectively customize models to align with specific expectations, such as tone, formality, or underlying biases. To solve this task, we develop AutoCustomization. The key to our approach is leveraging the vast knowledge encoded in modern language models to construct fine-tuning datasets focused on a specific customization axis in contrast to prior methods, which depend primarily on tediously constructed libraries of prompts. AutoCustomization demonstrates several desirable properties. It is universally applicable to any bias axis (e.g., political, stylistic). It is efficient with small automatically generated datasets and short fine-tuning. It allows for precise monitoring of the resulting bias change with our BiasShift evaluation metric proven to be alligned with human perception, generalizable to held-out aspects, and selective in preserving other model capabilities. We verify AutoCustomization through human evaluation and show that it outperforms existing prompting techniques while being simpler."
    },
    {
        "title": "Elliptic Loss Regularization",
        "link_suffix": "/forum?id=YwzxpZW3p7",
        "link": "https://openreview.net/forum?id=YwzxpZW3p7",
        "pdf_link": "https://openreview.net/pdf?id=YwzxpZW3p7",
        "keywords": "regularizer, loss landscape, diffusion, elliptic",
        "abstract": "Regularizing neural networks is important for anticipating model behavior in regions of the data space that are not well represented. In this work, we propose a regularization technique for enforcing a level of smoothness in the mapping between the input space and the loss. We specify the level of regularity by requiring that the loss of the network satisfies an elliptic operator over the data domain. To do this, we modify the usual empirical risk minimization objective such that we instead minimize a new objective that satisfies an elliptic operator over points within the domain. This allows us to use existing theory on elliptic operators to anticipate the behavior of the error for points outside the training set. We propose a tractable computational method that approximates the behavior of the elliptic operator while being computationally efficient. Finally, we analyze the properties of the proposed regularization to understand the performance on common problems of distribution shift and group imbalance. Numerical experiments empirically confirm the promise of the proposed regularization technique."
    },
    {
        "title": "Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization",
        "link_suffix": "/forum?id=Xj66fkrlTk",
        "link": "https://openreview.net/forum?id=Xj66fkrlTk",
        "pdf_link": "https://openreview.net/pdf?id=Xj66fkrlTk",
        "keywords": "generative flow networks, gflownets, reinforcement learning, sampling, generative models",
        "abstract": "Generative Flow Networks (GFlowNets) are a family of generative models that learn to sample objects with probabilities proportional to a given reward function. The key concept behind GFlowNets is the use of two stochastic policies: a forward policy, which incrementally constructs compositional objects, and a backward policy, which sequentially deconstructs them. Recent results show a close relationship between GFlowNet training and entropy-regularized reinforcement learning (RL) problems with a particular reward design. However, this connection applies only in the setting of a fixed backward policy, which might be a significant limitation. As a remedy to this problem, we introduce a simple backward policy optimization algorithm that involves direct maximization of the value function in an entropy-regularized Markov Decision Process (MDP) over intermediate rewards. We provide an extensive experimental evaluation of the proposed approach across various benchmarks in combination with both RL and GFlowNet algorithms and demonstrate its faster convergence and mode discovery in complex environments."
    },
    {
        "title": "Learning Skill-level Student Abilities with Item Response Theory",
        "link_suffix": "/forum?id=u4RVksX8co",
        "link": "https://openreview.net/forum?id=u4RVksX8co",
        "pdf_link": "https://openreview.net/pdf?id=u4RVksX8co",
        "keywords": "knowledge tracing, item response theory, student abilities",
        "abstract": "Knowledge tracing (KT) aims to estimate knowledge states of students over a given set of skills based on their historical learning activities. The learned knowledge states of students can be used to build skill-meters to understand the weak areas of students so that proper interventions can be taken to help students. Many deep learning models have been applied to KT with encouraging performance, but they either have relatively low accuracy or do not directly generate students' knowledge states at skill level for skill-meter building. Item Response Theory (IRT) models student knowledge states (ability) and question characteristics separately. A question arising naturally is whether we can use IRT to estimate students' knowledge states at skill level while achieving high prediction accuracy at the same time. We examined existing IRT based deep KT models and found that none of them achieves this objective. Most existing IRT-based models either learn overall student abilities or question-level student abilities. Overall student abilities are too summative, and it is hard to tell the weak areas of students from a single value. Question-level abilities are too fine-grained. When there are a large number of unique questions per skill, they can cause information overload for teachers. In this paper, we propose an IRT-based deep KT model called SKKT-IRT to learn skill-level student abilities which provide just the right amount of information for teachers to understand students' knowledge states. Our model consists of an LSTM layer to learn student historical states, a student ability network for learning skill-level student abilities, a question difficulty network for learning question difficulties and a question discrimination network for learning question discrimination. It also learns question-skill relationships as an auxiliary task so that the embedding of a skill can better capture the information of its questions. We further regularize the outputs of question difficulty network and question discrimination network for better performance. Our experimental results show that our model achieves the objective of learning skill-level student abilities with SOTA accuracy. It is also very efficient and produces consistent outputs to be easily used for downstream tasks like adaptive learning and personalized recommendations."
    },
    {
        "title": "Drift2Matrix: Kernel-Induced Self Representation for Concept Drift Adaptation in Co-evolving Time Series",
        "link_suffix": "/forum?id=prSJlvWrgE",
        "link": "https://openreview.net/forum?id=prSJlvWrgE",
        "pdf_link": "https://openreview.net/pdf?id=prSJlvWrgE",
        "keywords": "co-evolving time series, concept drift, kernel representation learning",
        "abstract": "In the realm of time series analysis, tackling the phenomenon of concept drift poses a significant challenge. Concept drift -- characterized by the evolving statistical properties of time series data, affects the reliability and accuracy of conventional analysis models. This is particularly evident in co-evolving scenarios where interactions among variables are crucial. This paper presents Drift2Matrix, a novel framework that leverages kernel-induced self-representation for adaptive responses to concept drift in time series. Drift2Matrix employs a kernel-based learning mechanism to generate a representation matrix, encapsulating the inherent dynamics of co-evolving time series. This matrix serves as a key tool for identification and adaptation to concept drift by observing its temporal variations. Furthermore, Drift2Matrix effectively identifies prevailing patterns and offers insights into emerging trends through pattern evolution analysis. Our empirical evaluation of Drift2Matrix across various datasets demonstrates its effectiveness in handling the complexities of concept drift. This approach introduces a novel perspective in the theoretical domain of co-evolving time series analysis, enhancing adaptability and accuracy in the face of dynamic data environments. Code is available at GitHub."
    },
    {
        "title": "RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning",
        "link_suffix": "/forum?id=zPPy79qKWe",
        "link": "https://openreview.net/forum?id=zPPy79qKWe",
        "pdf_link": "https://openreview.net/pdf?id=zPPy79qKWe",
        "keywords": "Large language models, automatic code generation, reinforcement learning, LLM agents",
        "abstract": "Large language models (LLMs) deployed as agents solve user-specified tasks over multiple steps while keeping the required manual engagement to a minimum. Crucially, such LLMs need to ground their generations in any feedback obtained to reliably achieve desired outcomes. We propose an end-to-end reinforcement learning method for teaching models to leverage execution feedback in the realm of code synthesis, where state-of-the-art LLMs struggle to improve code iteratively compared to independent sampling. We benchmark on competitive programming tasks, where we achieve new start-of-the art results with both small (8B parameters) and large (70B) models while reducing the amount of samples required by an order of magnitude. Our analysis of inference-time behavior demonstrates that our method produces LLMs that effectively leverage automatic feedback over multiple steps."
    },
    {
        "title": "Professor X: Manipulating EEG BCI with Invisible and Robust Backdoor Attack",
        "link_suffix": "/forum?id=5sdUTpDlbX",
        "link": "https://openreview.net/forum?id=5sdUTpDlbX",
        "pdf_link": "https://openreview.net/pdf?id=5sdUTpDlbX",
        "keywords": "safety, backdoor attack, EEG, brain-computer interface",
        "abstract": "While electroencephalogram (EEG) based brain-computer interface (BCI) has been widely used for medical diagnosis, health care, and device control, the safety of EEG BCI has long been neglected. In this paper, we proposeProfessor X, an invisible and robust \u201cmind-controller\u201d that can arbitrarily manipulate the outputs of EEG BCI through backdoor attack, to alert the EEG community of the potential hazard. However, existing EEG attacks mainly focus on single-target class attacks, and they either require engaging the training stage of the target BCI, or fail to maintain high stealthiness. Addressing these limitations, Professor X exploits a three-stage clean label poisoning attack:1)selecting one trigger for each class;2)learning optimal injecting EEG electrodes and frequencies strategy with reinforcement learning for each trigger;3)generating poisoned samples by injecting the corresponding trigger\u2019s frequencies into poisoned data for each class by linearly interpolating the spectral amplitude of both data according to previously learned strategies. Experiments on datasets of three common EEG tasks demonstrate the effectiveness and robustness of Professor X, which also easily bypasses existing backdoor defenses. Code will be released soon."
    },
    {
        "title": "Improving Reasoning Performance in Large Language Models via Representation Engineering",
        "link_suffix": "/forum?id=IssPhpUsKt",
        "link": "https://openreview.net/forum?id=IssPhpUsKt",
        "pdf_link": "https://openreview.net/pdf?id=IssPhpUsKt",
        "keywords": "Large Language Models, Reasoning, Intelligence, Representation Learning",
        "abstract": "Recent advancements in large language models (LLMs) have resulted in increasingly anthropomorphic language concerning the ability of LLMs to reason. \nWhether reasoning in LLMs should be understood to be inherently special is, however, widely debated. \nWe propose utilizing a representation engineering approach wherein model activations are read from the residual stream of an LLM when processing a reasoning task. The activations are used to derive a control vector that is applied to the model as an inference-time intervention, modulating the representational space of the model, to improve performance on the specified task.\nWe additionally contribute a framework for deriving control vectors and analyzing model representations. The framework allows us to induce improved reasoning performance and assess how control vectors influence the final logit distribution of a model via metrics such as KL divergence and entropy. We apply control vectors to Mistral-7B-Instruct and a range of Pythia models on a deductive and an inductive reasoning task respectively.\nWe show that an LLM can, to a certain degree, be controlled to improve its perceived reasoning ability by modulating activations. The intervention is dependent upon the ability to reliably extract the model's typical state when correctly solving a task.\nOur results suggest that there is no intrinsic difference between the process of reasoning and other information-processing tasks performed by LLMs. They furthermore demonstrate that we are capable of improving the reasoning performance of LLMs via a simple intervention on the residual stream with no additional training."
    },
    {
        "title": "Breaking the Detection-Generalization Paradox on Out-Of-Distribution Data",
        "link_suffix": "/forum?id=w0jk3L3IjV",
        "link": "https://openreview.net/forum?id=w0jk3L3IjV",
        "pdf_link": "https://openreview.net/pdf?id=w0jk3L3IjV",
        "keywords": "Trustworthy Machine Learning; Out of distribution data",
        "abstract": "This work studies the trade-off between out-of-distribution (OOD) detection and generalization. We identify the Detection-Generalization Paradox in OOD data, where optimizing one objective can degrade the other. We investigate this paradox by analyzing the behaviors of models trained under different paradigms, focusing on representation, logits, and loss across in-distribution, covariate-shift, and semantic-shift data. Based on our findings, we propose Distribution-Robust Sharpness-Aware Minimization (DR-SAM), an optimization framework that balances OOD detection and generalization. Extensive experiments demonstrate the method's effectiveness, offering a clear, empirically validated approach for improving detection and generalizationability in different benchmarks."
    },
    {
        "title": "Improving Uncertainty Estimation through Semantically Diverse Language Generation",
        "link_suffix": "/forum?id=HSi4VetQLj",
        "link": "https://openreview.net/forum?id=HSi4VetQLj",
        "pdf_link": "https://openreview.net/pdf?id=HSi4VetQLj",
        "keywords": "llm, nlg, uncertainty estimation, uncertainty measures, semantic uncertainty, aleatoric uncertainty, semantic entropy, mc estimation, importance sampling",
        "abstract": "Large language models (LLMs) can suffer from hallucinations when generating text. These hallucinations impede various applications in society and industry by making LLMs untrustworthy. Current LLMs generate text in an autoregressive fashion by predicting and appending text tokens. When an LLM is uncertain about the semantic meaning of the next tokens to generate, it is likely to start hallucinating. Thus, it has been suggested that predictive uncertainty is one of the main causes of hallucinations. We introduce Semantically Diverse Language\nGeneration (SDLG) to quantify predictive uncertainty in LLMs. SDLG steers the LLM to generate semantically diverse yet likely alternatives for an initially generated text. This approach provides a precise measure of aleatoric semantic uncertainty, detecting whether the initial text is likely to be hallucinated. Experiments on question-answering tasks demonstrate that SDLG consistently outperforms existing methods while being the most computationally efficient, setting a new standard for uncertainty estimation in LLMs."
    },
    {
        "title": "LANE: Label-Aware Noise Elimination for Fine-Grained Text Classification",
        "link_suffix": "/forum?id=bG61JDN4E8",
        "link": "https://openreview.net/forum?id=bG61JDN4E8",
        "pdf_link": "https://openreview.net/pdf?id=bG61JDN4E8",
        "keywords": "label noise, training dynamics, example reweighting",
        "abstract": "We propose Label-Aware Noise Elimination (LANE), a new approach that improves the robustness of deep learning models in fine-grained text classification when trained under increased label noise. LANE leverages the semantic relations between classes and monitors the training dynamics of the model on each training example to dynamically lower the importance of training examples that may have noisy labels. We test the effectiveness of LANE in fine-grained text classification and benchmark our approach on a wide variety of datasets with various number of classes and various amounts of label noise. LANE considerably outperforms strong baselines on all datasets, obtaining significant improvements ranging from an average improvement of 2.4% in F1 on manually annotated datasets to a considerable average improvement of 4.5% F1 on datasets with injected noisy labels. We carry out comprehensive analyses of LANE and identify the key components that lead to its success."
    },
    {
        "title": "Semi-autoregressive Decoding for Efficient LLM Inference",
        "link_suffix": "/forum?id=gfDbD1MRYk",
        "link": "https://openreview.net/forum?id=gfDbD1MRYk",
        "pdf_link": "https://openreview.net/pdf?id=gfDbD1MRYk",
        "keywords": "efficient inference, dependence modeling, non-autoregressive models, LLMs, speculative decoding",
        "abstract": "Inference in large language models (LLMs) is often slow due to their autoregressive nature. \nIn this work, we formulate a semi-autoregressive decoding paradigm for LLMs that delegates part of the expensive computation from the original large model to a smaller, more efficient autoregressive model. The core of our design lies in the separate modeling of token dependencies, where the large model handles long-term dependencies on distant tokens, while the smaller model addresses short-term dependencies on recent tokens. When employed as a draft model in speculative decoding, our method allows for substantial reuse of computation in the LLM without missing any token dependencies, thereby striking a good balance between draft quality and drafting speed. Experiments on text summarization, medical QA, code generation, and mathematical reasoning tasks demonstrates the efficacy of our method."
    },
    {
        "title": "Interaction Makes Better Segmentation: An Interaction-based Framework for Temporal Action Segmentation",
        "link_suffix": "/forum?id=sEARCNzhrP",
        "link": "https://openreview.net/forum?id=sEARCNzhrP",
        "pdf_link": "https://openreview.net/pdf?id=sEARCNzhrP",
        "keywords": "Video Understanding; Video Analysis;",
        "abstract": "Temporal action segmentation aims to classify the action category of each frame in untrimmed videos, primarily using RGB video and skeleton data. Most existing methods adopt a two-stage process: feature extraction and temporal modeling. However, we observe significant limitations in their spatio-temporal modeling: (i) Existing temporal modeling modules conduct frame-level and action-level interactions at a fixed temporal resolution, which over-smooths temporal features and leads to blurred action boundaries; (ii) Skeleton-based methods generally adopt temporal modeling modules originally designed for RGB video data, causing a misalignment between extracted features and temporal modeling modules. In this paper, we propose a novel Interaction-based framework for Action segmentation (InterAct) to address these issues. Firstly, we propose multi-scale frame-action interaction (MFAI) to facilitate frame-action interactions across varying temporal scales. This enhances the model's ability to capture complex temporal dynamics, producing more expressive temporal representations and alleviating the over-smoothing issue. Meanwhile, recognizing the complementary nature of different spatial modalities, we propose decoupled spatial modality interaction (DSMI). It decouples the modeling of spatial modalities and applies a deep fusion strategy to interactively integrate multi-scale spatial features. This results in more discriminative spatial features that are better aligned with the temporal modeling modules. Extensive experiments on six large-scale benchmarks demonstrate that InterAct significantly outperforms state-of-the-art methods on both RGB-based and skeleton-based datasets across diverse scenarios."
    },
    {
        "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
        "link_suffix": "/forum?id=CbfsKHiWEn",
        "link": "https://openreview.net/forum?id=CbfsKHiWEn",
        "pdf_link": "https://openreview.net/pdf?id=CbfsKHiWEn",
        "keywords": "Direct Preference Optimization, LLM's alignment, Distributionally Robust Optimization",
        "abstract": "This study addresses the challenge of noise in training datasets for Direct Preference Optimization (DPO), a method for aligning Large Language Models (LLMs) with human preferences. We categorize noise into pointwise noise, which includes low-quality data points, and pairwise noise, which encompasses erroneous data pair associations that affect preference rankings. Utilizing Distributionally Robust Optimization (DRO), we enhance DPO's resilience to these types of noise. Our theoretical insights reveal that DPO inherently embeds DRO principles, conferring robustness to pointwise noise, with the regularization coefficient $\\beta$ playing a critical role in its noise resistance. Extending this framework, we introduce Distributionally Robustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing against worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr. DPO allows for fine-tuned control over data pair reliability, providing a strategic balance between exploration and exploitation in noisy training environments. Empirical evaluations demonstrate that Dr. DPO substantially improves the quality of generated text and response accuracy in preference datasets, showcasing enhanced performance in both noisy and noise-free settings."
    },
    {
        "title": "Sparse Causal Model: A Novel Approach for Causal Discovery and Attributions on Sparse Dataset",
        "link_suffix": "/forum?id=fSxiromxAq",
        "link": "https://openreview.net/forum?id=fSxiromxAq",
        "pdf_link": "https://openreview.net/pdf?id=fSxiromxAq",
        "keywords": "Causality, Causal Inference",
        "abstract": "This paper introduces a novel approach to tackle the challenges of causal modeling and attribution in sparse and non-continuous data with limited feature knowledge. Traditional methods rely on static inputs and lack adaptability to dynamic changes in causal relationships, resulting in a limited understanding and goodness-of-fit. We introduce a unique causal discovery framework on real-world sparse datasets to address this challenge. We leverage a Directed Acyclic Graph (DAG) by discovering causal relationships between the variables by identifying confounder-treatment pairs that make the variable selection process robust and efficient. We propose a three-stage causal model that uses multiple distinct regressors such as likelihood-based, tree-based, and Generalized Additive Models (GAMs). Furthermore, we introduce a Model Score by including the sensitivity analysis involving random shuffling confounders and treatments to select the best optimal model. We implement a partial dependency approach to understand the attribution of variables, contributing by adding a 53% increase in the R2 score compared to traditional methods. This research underscores the limitations of conventional approaches in addressing real-world challenges to address practical scenarios effectively."
    },
    {
        "title": "Task-agnostic Pre-training and Task-guided Fine-tuning for Versatile Diffusion Planner",
        "link_suffix": "/forum?id=fcJKzwlwcs",
        "link": "https://openreview.net/forum?id=fcJKzwlwcs",
        "pdf_link": "https://openreview.net/pdf?id=fcJKzwlwcs",
        "keywords": "reinforcement learning, diffusion models, planning",
        "abstract": "Diffusion models have demonstrated their capabilities in modeling trajectories of multi-tasks. However, existing multi-task planners or policies typically rely on task-specific demonstrations via multi-task imitation, or require task-specific reward labels to facilitate policy optimization via Reinforcement Learning (RL). They heavily rely on the task-specific labeled data which can be difficult to acquire. To address these challenges, we aim to develop a versatile diffusion planner that can leverage large-scale inferior data that contains task-agnostic sub-optimal trajectories, with the ability to fast adapt to specific tasks. In this paper, we propose SODP, a two-stage framework that leverages Sub-Optimal data to learn a Diffusion Planner, which is generalizable for various downstream tasks. Specifically, in the pre-training stage, we train a foundation diffusion planner that extracts general planning capabilities by modeling the versatile distribution of multi-task trajectories, which can be sub-optimal and has wide data coverage. Then for downstream tasks, we adopt RL-based fine-tuning with task-specific rewards to fast refine the diffusion planner, which aims to generate action sequences with higher task-specific returns. Experimental results from multi-task domains including Meta-World and Adroit demonstrate that SODP outperforms state-of-the-art methods with only a small amount of data for reward-guided fine-tuning."
    },
    {
        "title": "On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent",
        "link_suffix": "/forum?id=97rOQDPmk2",
        "link": "https://openreview.net/forum?id=97rOQDPmk2",
        "pdf_link": "https://openreview.net/pdf?id=97rOQDPmk2",
        "keywords": "Sign Gradient Descent; Transformer; Training Dynamics; Theory",
        "abstract": "The Adam optimizer is widely used for transformer optimization in practice, which makes understanding the underlying optimization mechanisms an important problem.\nHowever, due to the Adam's complexity, theoretical analysis of how it optimizes transformers remains a challenging task. \nFortunately, Sign Gradient Descent (SignGD) serves as an effective surrogate for Adam.\nDespite its simplicity, theoretical understanding of how SignGD optimizes transformers still lags behind.\nIn this work, we study how SignGD optimizes a two-layer transformer -- consisting of a softmax attention layer with trainable query-key parameterization followed by a linear layer -- on \na linearly separable noisy dataset.\nWe identify four stages in the training dynamics, each exhibiting intriguing behaviors.\nBased on the training dynamics, we prove the fast convergence but poor generalization of the learned transformer on the noisy dataset.\nWe also show that Adam behaves similarly to SignGD in terms of both optimization and generalization in this setting.\nAdditionally, we find that the poor generalization of SignGD is not solely due to data noise,\nsuggesting that both SignGD and Adam requires high-quality data for real-world tasks.\nFinally, experiments on synthetic and real-world datasets empirically support our theoretical results."
    },
    {
        "title": "Lens: Rethinking Multilingual Enhancement for Large Language Models",
        "link_suffix": "/forum?id=8kGonpsiHb",
        "link": "https://openreview.net/forum?id=8kGonpsiHb",
        "pdf_link": "https://openreview.net/pdf?id=8kGonpsiHb",
        "keywords": "Multilingual Enhancement, Large Language Models",
        "abstract": "Despite the growing global demand for large language models (LLMs) that serve users from diverse linguistic backgrounds, most cutting-edge LLMs remain predominantly English-centric. This creates a performance gap across languages, restricting access to advanced AI services for non-English speakers. Current methods to enhance multilingual capabilities largely rely on data-driven post-training techniques, such as multilingual instruction tuning or continual pre-training. However, these approaches encounter significant challenges, including the scarcity of high-quality multilingual datasets and the limited enhancement of multilingual capabilities. They often suffer from off-target issues and catastrophic forgetting of central language abilities. To this end, we propose \\textsc{Lens}, a novel approach to enhance multilingual capabilities of LLMs by leveraging their internal language representation spaces. Specially, \\textsc{Lens} operates by manipulating the hidden representations within the language-agnostic and language-specific subspaces from top layers of LLMs. Using the central language as a pivot, the target language is drawn closer to it within the language-agnostic subspace, allowing it to inherit well-established semantic representations. Meanwhile, in the language-specific subspace, the representations of the target and central languages are pushed apart, enabling the target language to express itself distinctly. Extensive experiments on one English-centric and two multilingual LLMs demonstrate that \\textsc{Lens} effectively improves multilingual performance without sacrificing the model\u2019s original central language capabilities, achieving superior results with much fewer computational resources compared to existing post-training approaches."
    },
    {
        "title": "The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels",
        "link_suffix": "/forum?id=m60n31iYMw",
        "link": "https://openreview.net/forum?id=m60n31iYMw",
        "pdf_link": "https://openreview.net/pdf?id=m60n31iYMw",
        "keywords": "Structured State Space Models, Implicit Bias, Implicit Regularization, Clean Label Poisoning",
        "abstract": "Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs), regarded as an efficient alternative to transformers. Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e. having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, particularly in large language models, we believe significant efforts should be invested in further delineating their susceptibility to clean-label poisoning, and in developing methods for overcoming this susceptibility."
    },
    {
        "title": "Attention as a Hypernetwork",
        "link_suffix": "/forum?id=V4K9h1qNxE",
        "link": "https://openreview.net/forum?id=V4K9h1qNxE",
        "pdf_link": "https://openreview.net/pdf?id=V4K9h1qNxE",
        "keywords": "attention, hypernetwork, compositional generalization, abstract reasoning",
        "abstract": "Transformers can under some circumstances generalize to novel problem instances whose constituent parts might have been encountered during training but whose compositions have not.\nWhat mechanisms underlie this ability for compositional generalization?\nBy reformulating multi-head attention as a hypernetwork, we reveal that a composable, low-dimensional latent code specifies key-query specific operations.\nWe find empirically that this latent code is predictive of the subtasks the network performs on unseen task compositions revealing that latent codes acquired during training are reused to solve unseen problem instances.\nTo further examine the hypothesis that the intrinsic hypernetwork of multi-head attention supports compositional generalization, we ablate whether making the hypernetwork generated linear value network nonlinear strengthens compositionality.\nWe find that this modification improves compositional generalization on abstract reasoning tasks.\nIn particular, we introduce a symbolic version of the Raven Progressive Matrices human intelligence test which gives us precise control over the problem compositions encountered during training and evaluation.\nWe demonstrate on this task how scaling model size and data enables compositional generalization in transformers and gives rise to a functionally structured latent space."
    },
    {
        "title": "IN the known, OUT of the ordinary: Probing OOD detection methods with Synthetic datasets.",
        "link_suffix": "/forum?id=KK29oh8jZs",
        "link": "https://openreview.net/forum?id=KK29oh8jZs",
        "pdf_link": "https://openreview.net/pdf?id=KK29oh8jZs",
        "keywords": "OOD detection, Benchmarking",
        "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability of machine learning models, especially in visual tasks. Most existing benchmarks focus on isolating distribution shifts and creating varying levels of detection difficulty, often relying on manual curation or classifier-based scoring with human annotations. Additionally, large-scale benchmarks are typically derivatives of ImageNet-21k classes or combinations of ImageNet with other datasets. However, no existing work offers a setup where only one attribute such as color or class changes in a controlled manner, while other attributes of the object remain constant. This limits our ability to precisely study the impact of individual attributes on OOD detection performance. We aim to address this by proposing two novel synthetic datasets, SHAPES and CHARS, designed to explore OOD detection under controlled and fine-grained distribution shifts. SHAPES consist of 2D and 3D geometric shapes with variations in color, size, position, and rotation, while CHARS consists of alphanumeric characters with similar variations. Each dataset presents three scenarios: (1) known classes with unseen attributes, (2) unseen classes with known attributes, and (3) entirely novel classes and attributes. We train 10 architectures and assess 13 OOD detection methods across the three scenarios, concentrating on the impact of  attribute shifts on OOD scores, while also conducting additional analysis on how image corruption influences OOD scores. By systematically examining how specific attribute shifts affect OOD scores and the affects of noisy test samples, we aim to bring greater transparency to where these methods succeed or fail, helping to identify their limitations under various conditions."
    },
    {
        "title": "PFGuard: A Generative Framework with Privacy and Fairness Safeguards",
        "link_suffix": "/forum?id=8rbkePAapb",
        "link": "https://openreview.net/forum?id=8rbkePAapb",
        "pdf_link": "https://openreview.net/pdf?id=8rbkePAapb",
        "keywords": "Trustworthy AI, Responsible AI, ML Fairness, Differential Privacy, Generative Model",
        "abstract": "Generative models must ensure both privacy and fairness for Trustworthy AI. While these goals have been pursued separately, recent studies propose to combine existing privacy and fairness techniques to achieve both goals. However, naively combining these techniques can be insufficient due to privacy-fairness conflicts, where a sample in a minority group may be amplified for fairness, only to be suppressed for privacy. We demonstrate how these conflicts lead to adverse effects, such as privacy violations and unexpected fairness-utility tradeoffs. To mitigate these risks, we propose PFGuard, a generative framework with privacy and fairness safeguards, which simultaneously addresses privacy, fairness, and utility. By using an ensemble of multiple teacher models, PFGuard balances privacy-fairness conflicts between fair and private training stages and achieves high utility based on ensemble learning. Extensive experiments show that PFGuard successfully generates synthetic data on high-dimensional data while providing both fairness convergence and strict DP guarantees - the first of its kind to our knowledge."
    },
    {
        "title": "Rethinking Uncertainty Estimation in Natural Language Generation",
        "link_suffix": "/forum?id=xeP03R58RH",
        "link": "https://openreview.net/forum?id=xeP03R58RH",
        "pdf_link": "https://openreview.net/pdf?id=xeP03R58RH",
        "keywords": "llm, nlg, uncertainty estimation, uncertainty measures, proper scoring rules",
        "abstract": "Large language models (LLMs) are increasingly employed in real-world applications, driving a need to determine when their generated text can be trusted or should be questioned. To assess the trustworthiness of the generated text, reliable uncertainty estimation is essential. Current LLMs generate text through a stochastic process that can lead to different output sequences for the same prompt. Consequently, leading uncertainty measures require generating multiple output sequences to estimate the LLM\u2019s uncertainty. However, generating additional output sequences is computationally expensive, making these uncertainty estimates impractical at scale. In this work, we challenge the theoretical foundations of the leading measures and derive an alternative measure that eliminates the need for generating multiple output sequences. Our new measure is based solely on the negative log-likelihood of the most likely output sequence. This vastly simplifies uncertainty estimation while maintaining theoretical rigor. Empirical results demonstrate that our new measure achieves state-of-the-art performance across various models and tasks. Our work lays the foundation for reliable and efficient uncertainty estimation in LLMs, challenging the necessity of the more complicated methods currently leading the field."
    },
    {
        "title": "Evaluating Robustness of Reward Models for Mathematical Reasoning",
        "link_suffix": "/forum?id=0er6aOyXUD",
        "link": "https://openreview.net/forum?id=0er6aOyXUD",
        "pdf_link": "https://openreview.net/pdf?id=0er6aOyXUD",
        "keywords": "mathematical reasoning, RLHF, reward models, reward overoptimization, language models, benchmark",
        "abstract": "Reward models are key in reinforcement learning from human feedback (RLHF) systems, aligning the model behavior with human preferences.\nParticularly in the math domain, there have been plenty of studies using reward models to align policies for improving reasoning capabilities.\nRecently, as the importance of reward models has been emphasized, RewardBench is proposed to understand their behavior.\nHowever, we figure out that the math subset of RewardBench has different representations between chosen and rejected completions, and relies on a single comparison, which may lead to unreliable results as it only see an isolated case.\nTherefore, it fails to accurately present the robustness of reward models, leading to a misunderstanding of its performance and potentially resulting in reward hacking.\nIn this work, we introduce a new design for reliable evaluation of reward models, and to validate this, we construct RewardMATH, a benchmark that effectively represents the robustness of reward models in mathematical reasoning tasks.\nWe demonstrate that the scores on RewardMATH strongly correlate with the results of optimized policy and effectively estimate reward overoptimization, whereas the existing benchmark shows almost no correlation.\nThe results underscore the potential of our design to enhance the reliability of evaluation, and represent the robustness of reward model.\nWe make our code and data publicly available."
    }
]