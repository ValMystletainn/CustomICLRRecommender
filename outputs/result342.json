[{"title": "Contextualizing biological perturbation experiments through language", "link_suffix": "/forum?id=5WEpbilssv", "link": "https://openreview.net/forum?id=5WEpbilssv", "pdf_link": "https://openreview.net/pdf?id=5WEpbilssv", "keywords": "large language models, Perturb-seq, perturbation experiments, knowledge graphs, retrieval-augmented generation, chain of thought prompting", "abstract": "High-content genetic perturbation experiments provide insights into biomolecular pathways at unprecedented resolution, yet experimental and analysis costs pose barriers to their widespread adoption. In-silico modeling of unseen perturbations has the potential to alleviate this burden by leveraging prior knowledge to enable more efficient exploration of the perturbation space. However, current knowledge-graph approaches neglect the semantic richness of the relevant biology, beyond simple adjacency graphs. To enable holistic modeling, we hypothesize that natural language is an appropriate medium for interrogating experimental outcomes and representing biological relationships. We propose PerturbQA as a set of real-world tasks for benchmarking large language model (LLM) reasoning over structured, biological data. PerturbQA is comprised of three tasks: prediction of differential expression and change of direction for unseen perturbations, and gene set enrichment. As a proof of concept, we present SUMMER (SUMMarize, retrievE, and answeR), a simple LLM-based framework that matches or exceeds the current state-of-the-art on this benchmark. We evaluated graph and language-based models on differential expression and direction of change tasks, finding that SUMMER performed best overall. Notably, SUMMER's outputs, unlike models that solely rely on knowledge graphs, are easily interpretable by domain experts, aiding in understanding model limitations and contextualizing experimental outcomes. Additionally, SUMMER excels in gene set enrichment, surpassing over-representation analysis baselines in most cases and effectively summarizing clusters lacking a manual annotation.", "title_embedding_index": 17050, "title_abs_embedding_index": 17075}, {"title": "Rethinking Fair Representation Learning for Performance-Sensitive Tasks", "link_suffix": "/forum?id=pBZntPrdrI", "link": "https://openreview.net/forum?id=pBZntPrdrI", "pdf_link": "https://openreview.net/pdf?id=pBZntPrdrI", "keywords": "Fairness, Causality, Representation Learning, Distribution Shift", "abstract": "We investigate the prominent class of fair representation learning methods for bias mitigation. Using causal reasoning to define and formalise different sources of dataset bias, we reveal important implicit assumptions inherent to these methods. We prove fundamental limitations on fair representation learning when evaluation data is drawn from the same distribution as training data and run experiments across a range of medical modalities to examine the performance of fair representation learning under distribution shifts. Our results explain apparent contradictions in the existing literature and reveal how rarely considered causal and statistical aspects of the underlying data affect the validity of fair representation learning. We raise doubts about current evaluation practices and the applicability of fair representation learning methods in performance-sensitive settings. We argue that fine-grained analysis of dataset biases should play a key role in the field moving forward.", "title_embedding_index": 17051, "title_abs_embedding_index": 17076}, {"title": "Privacy Auditing of Large Language Models", "link_suffix": "/forum?id=60Vd7QOXlM", "link": "https://openreview.net/forum?id=60Vd7QOXlM", "pdf_link": "https://openreview.net/pdf?id=60Vd7QOXlM", "keywords": "llm memorization, canaries design, membership inference attacks, privacy auditing, differential privacy", "abstract": "Current techniques for privacy auditing of large language models (LLMs) have limited efficacy---they rely on basic approaches to generate canaries which leads to weak membership inference attacks that in turn give loose lower bounds on the empirical privacy leakage.\nWe develop canaries that are far more effective than those used in prior work under threat models that cover a range of realistic settings. \nWe demonstrate through extensive experiments on multiple families of fine-tuned LLMs that our approach sets a new standard for detection of privacy leakage. For measuring the memorization rate of non-privately trained LLMs, our designed canaries largely surpassing the prior SOTA. For example, on the Qwen2.5-0.5B model, our designed canaries achieves $26.0%$ TPR at $1%$ FPR, largely surpassing the prior SOTA of $1.3%$ TPR at $1%$ FPR. Our method can be used to provide a privacy audit of $\\varepsilon \\approx 1$ for a model trained with theoretical $\\varepsilon$ of 4. To the best of our knowledge, this is the first time that a privacy audit of LLM training has achieved nontrivial auditing success in the setting where the attacker cannot train shadow models, insert gradient canaries, or access the model at every iteration.", "title_embedding_index": 17052, "title_abs_embedding_index": 17077}, {"title": "Archon: An Architecture Search Framework for Inference-Time Techniques", "link_suffix": "/forum?id=5wuZyG1ACs", "link": "https://openreview.net/forum?id=5wuZyG1ACs", "pdf_link": "https://openreview.net/pdf?id=5wuZyG1ACs", "keywords": "inference-time techniques, test-time scaling, machine learning, natural language processing", "abstract": "Inference-time techniques are emerging as highly effective tools to enhance large language model (LLM) capabilities. However, best practices for developing systems that combine these techniques remain underdeveloped due to our limited understanding of the utility of individual inference-time techniques and the interactions between them. Additionally, efficiently and automatically searching the space of model choices, inference-time techniques, and their compositions is challenging due to the large design space. To address these challenges, we introduce Archon, a modular framework for selecting, combining, and stacking layers of inference-time techniques to construct optimized LLM systems for target benchmarks. Rather than relying on a single LLM called once, we leverage a diverse set of LLMs and inference-time techniques, creating LLM systems greater than the sum of their parts. Archon defines an extensible design space, encompassing techniques such as generation ensembling, repeated sampling, ranking, fusion, critiquing, verification, and unit testing. It transforms the problem of building LLM systems into a hyperparameter optimization objective. Given the available LLMs, inference-time techniques, and compute budget, Archon utilizes hyperparameter search techniques to discover optimized architectures for target benchmark(s). We evaluate Archon architectures across a range of instruction-following, reasoning, and coding benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval, MixEval Hard, MATH, and CodeContests. Archon architectures outperform frontier models, such as GPT-4o and Claude 3.5 Sonnet, on these benchmarks, achieving an average accuracy increase of 15.1 percentage points by using all available LLMs.", "title_embedding_index": 17053, "title_abs_embedding_index": 17078}, {"title": "Exploring Diffusion Models' Corruption Stage in Few-Shot Fine-tuning and Mitigating with Bayesian Neural Networks", "link_suffix": "/forum?id=PpP6ALezeK", "link": "https://openreview.net/forum?id=PpP6ALezeK", "pdf_link": "https://openreview.net/pdf?id=PpP6ALezeK", "keywords": "Few-shot Fine-tuning, Diffusion Models, Bayesian Neural Networks", "abstract": "Few-shot fine-tuning of Diffusion Models (DMs) is a key advancement, significantly reducing training costs and enabling personalized AI applications. However, we explore the training dynamics of DMs and observe an unanticipated phenomenon: during the training process, image fidelity initially improves, then unexpectedly deteriorates with the emergence of noisy patterns, only to recover later with severe overfitting. We term the stage with generated noisy patterns as corruption stage. To understand this corruption stage, we begin by heuristically modeling the one-shot fine-tuning scenario, and then extend this modeling to more general cases. Through this modeling, we identify the primary cause of this corruption stage: a narrowed learning distribution inherent in the nature of few-shot fine-tuning. To tackle this, we apply Bayesian Neural Networks (BNNs) on DMs with variational inference to implicitly broaden the learned distribution, and present that the learning target of the BNNs can be naturally regarded as an expectation of the diffusion loss and a further regularization with the pretrained DMs. This approach is highly compatible with current few-shot fine-tuning methods in DMs and does not introduce any extra inference costs. Experimental results demonstrate that our method significantly mitigates corruption, and improves the fidelity, quality and diversity of the generated images in both object-driven and subject-driven generation tasks. The code is available at an anonymous link.", "title_embedding_index": 17054, "title_abs_embedding_index": 17079}, {"title": "Robust Watermarking for Diffusion Models: A Unified Multi-Dimensional Recipe", "link_suffix": "/forum?id=O13fIFEB81", "link": "https://openreview.net/forum?id=O13fIFEB81", "pdf_link": "https://openreview.net/pdf?id=O13fIFEB81", "keywords": "Diffusion model, Watermark, Unified framework", "abstract": "Diffusion models are known for the supreme capability to generate realistic images. However, ethical concerns, such as copyright protection and generation of inappropriate content, pose significant challenges for the practical deployment of diffusion models. Recent work has proposed a flurry of watermarking techniques that inject visually noteless patterns into generated images, offering a promising solution to these issues. While effective, the essential elements for watermarking and the interconnections among various methods are still chaos. In this paper, we dissect the design principles of state-of-the-art watermarking techniques and introduce a unified framework.  We identify a set of dimensions that explain the manipulation enforced by watermarking methods, including the distribution of individual elements, the specification of watermark regions within each channel, and the choice of channels for watermark embedding. Moreover, under this framework we instantiate a new watermarking method to minimize impacts on the model performance from a distributional perspective. Through the empirical studies on regular text-to-image applications and the first systematic attempt on watermarking image-to-image diffusion models, we thoroughly verify the effectiveness of our proposed framework through comprehensive evaluations. On all the diffusion models, including Stable Diffusion, our approach induced from the proposed framework not only preserves image quality but also outperforms existing methods in robustness against a range of attacks.", "title_embedding_index": 17055, "title_abs_embedding_index": 17080}, {"title": "Dense Backpropagation Improves Routing for Sparsely-Gated Mixture-of-Experts", "link_suffix": "/forum?id=huy8g3iKy0", "link": "https://openreview.net/forum?id=huy8g3iKy0", "pdf_link": "https://openreview.net/pdf?id=huy8g3iKy0", "keywords": "Mixtureofexperts, MoE, routing, transformer, LLM", "abstract": "Mixture of Experts (MoE) pretraining is more scalable than dense Transformer pretraining, because MoEs learn to route inputs to a sparse set of their feedforward parameters. However, this means that MoEs only receive a sparse backward update, leading to problems such as router load imbalance where some experts receive more tokens than others. We present a lightweight approximation method that gives the MoE a dense gradient while only sparsely activating its parameters. A key insight into the design of our method is that at scale, many tokens not routed to a given expert may nonetheless lie in the span of tokens that were routed to that expert, allowing us to create an approximation for the expert output of that token from existing expert outputs. Our dense backpropagation outperforms standard TopK routing across multiple MoE configurations without increasing runtime.", "title_embedding_index": 17056, "title_abs_embedding_index": 17081}, {"title": "Revealing the Unseen: Guiding Personalized Diffusion Models to Expose Training Data", "link_suffix": "/forum?id=1ziPqVsDLc", "link": "https://openreview.net/forum?id=1ziPqVsDLc", "pdf_link": "https://openreview.net/pdf?id=1ziPqVsDLc", "keywords": "Diffusion Models, Data Extraction, Few-shot Fine-tuning, Copyright Protection, Trustworthy AI, Security", "abstract": "Diffusion Models (DMs) have evolved into advanced image generation tools, especially for few-shot fine-tuning where a pretrained DM is fine-tuned on a small set of images to capture specific styles or objects. Many people upload these personalized checkpoints online, fostering communities such as Civitai and HuggingFace. However, model owners may overlook the potential risks of data leakage by releasing their fine-tuned checkpoints. Moreover, concerns regarding copyright violations arise when unauthorized data is used during fine-tuning.  In this paper, we ask:\u201cCan training data be extracted from these fine-tuned DMs shared online?\u201dA successful extraction would present not only data leakage threats but also offer tangible evidence of copyright infringement. To answer this, we propose FineXtract, a framework for extracting fine-tuning data.  Our method approximates fine-tuning as a gradual shift in the model's learned distribution---from the original pretrained DM toward the fine-tuning data. By extrapolating the models before and after fine-tuning, we guide the generation toward high-probability regions within the fine-tuned data distribution. We then apply a clustering algorithm to extract the most probable images from those generated using this extrapolated guidance. Experiments on DMs fine-tuned with datasets such as WikiArt, DreamBooth, and real-world checkpoints posted online validate the effectiveness of our method, extracting approximately 20% of fine-tuning data in most cases, significantly surpassing baseline performance. The code is available at an anonymous link.", "title_embedding_index": 17057, "title_abs_embedding_index": 17082}, {"title": "Variance-Reducing Couplings for Random Features", "link_suffix": "/forum?id=oJLpXraSLb", "link": "https://openreview.net/forum?id=oJLpXraSLb", "pdf_link": "https://openreview.net/pdf?id=oJLpXraSLb", "keywords": "Monte Carlo, variance reduction, quasi Monte Carlo, transformers, performers, optimal transport, random Fourier features, graphs, Gaussian processes, kernels", "abstract": "Random features (RFs) are a popular technique to scale up kernel methods in machine learning, replacing exact kernel evaluations with stochastic Monte Carlo estimates. They underpin models as diverse as efficient transformers (by approximating attention) to sparse spectrum Gaussian processes (by approximating the covariance function). Efficiency can be further improved by speeding up the convergence of these estimates: a variance reduction problem. We tackle this through the unifying lens of optimal transport, finding couplings to improve RFs defined on both Euclidean and discrete input spaces. They enjoy theoretical guarantees and sometimes provide strong downstream gains, including for scalable approximate inference on graphs. We reach surprising conclusions about the benefits and limitations of variance reduction as a paradigm, showing that other properties of the coupling should be optimised for attention estimation in efficient transformers.", "title_embedding_index": 17058, "title_abs_embedding_index": 17083}, {"title": "Is Knowledge in Multilingual Language Models Cross-Lingually Consistent?", "link_suffix": "/forum?id=HMa8mIiBT8", "link": "https://openreview.net/forum?id=HMa8mIiBT8", "pdf_link": "https://openreview.net/pdf?id=HMa8mIiBT8", "keywords": "Multilingual Models, Fact-checking, Cross-lingual Knowledge Consistency, Self-consistency, Model Parity", "abstract": "Few works study the variation and cross-lingual consistency of factual knowledge embedded in multilingual models. However, cross-lingual consistency should be considered to assess cross-lingual transferability, maintain the factuality of the model\u2019s knowledge across languages, and preserve the parity of language model performance. We are thus interested in analyzing, evaluating, and interpreting cross-lingual consistency for factual knowledge. We apply interpretability approaches to analyze a model\u2019s behavior in cross-lingual contexts, discovering that multilingual models show different levels of consistency, subject to either language families or linguistic factors. Further, we identify a cross-lingual consistency bottleneck manifested in middle layers. To mitigate this problem, we try vocabulary expansion, additional cross-lingual objectives, and adding biases from monolingual inputs. We find that all these methods boost cross-lingual consistency to some extent, with cross-lingual supervision offering the best improvement.", "title_embedding_index": 17059, "title_abs_embedding_index": 17084}, {"title": "Beyond Forecasting: Compositional Time Series Reasoning for End-to-End Task Execution", "link_suffix": "/forum?id=NCUKdeqz4X", "link": "https://openreview.net/forum?id=NCUKdeqz4X", "pdf_link": "https://openreview.net/pdf?id=NCUKdeqz4X", "keywords": "Time Series, LLM, Reasoning", "abstract": "In recent decades, there have been substantial advances in time series models and benchmarks across various individual tasks, such as time series forecasting, classification, and anomaly detection.\nMeanwhile, compositional reasoning in time series prevalent in real-world applications (e.g., decision-making and compositional question answering) is in great demand. Unlike simple tasks that primarily focus on predictive accuracy, compositional reasoning emphasizes the synthesis of diverse information from both time series data and various domain knowledge, making it distinct and extremely more challenging. In this paper, we introduce Compositional Time Series Reasoning, a new task of handling intricate multistep reasoning tasks from time series data. Specifically, this new task focuses on various question instances requiring structural and compositional reasoning abilities on time series data, such as decision-making and compositional question answering.\nAs an initial attempt to tackle this novel task, we developed TS Reasoner, a program-aided approach that utilizes large language model (LLM) to decompose a complex task into steps of programs that leverage existing time series models and numerical subroutines. Unlike existing reasoning work which only calls off-the-shelf modules, TS Reasoner allows for the creation of custom modules and provides greater flexibility to incorporate domain knowledge as well as user-specified constraints. We demonstrate the effectiveness of our method through a comprehensive set of experiments. These promising results indicate potential opportunities in the new task of time series reasoning and highlight the need for further research.", "title_embedding_index": 17060, "title_abs_embedding_index": 17085}, {"title": "Interpolating Video-LLMs:  Toward Longer-sequence LMMs in a Training-free Manner", "link_suffix": "/forum?id=QrTvFCa4nX", "link": "https://openreview.net/forum?id=QrTvFCa4nX", "pdf_link": "https://openreview.net/pdf?id=QrTvFCa4nX", "keywords": "Video Understanding", "abstract": "Advancements in Large Language Models (LLMs) inspire various strategies for integrating video modalities. \nA key approach is Video-LLMs, which incorporate an optimizable interface linking sophisticated video encoders to LLMs. \nHowever, due to computation and data limitations, these Video-LLMs are typically pre-trained to process only short videos, limiting their broader application for understanding longer video content. Additionally, fine-tuning Video-LLMs to handle longer videos is cost-prohibitive.\nConsequently, it becomes essential to explore the interpolation of Video-LLMs under a completely training-free setting. In this paper, we first identify the primary challenges in interpolating Video-LLMs: (1) the video encoder and modality alignment projector are fixed, preventing the integration of additional frames into Video-LLMs, and (2) the LLM backbone is limited in its content length capabilities, which complicates the processing of an increased number of video tokens.\nTo address these challenges, we propose a specific INTerPolation method for Video-LLMs (INTP-Video-LLMs). We introduce an alternative video token rearrangement technique that circumvents limitations imposed by the fixed video encoder and alignment projector. Furthermore, we introduce a training-free LLM context window extension method to enable Video-LLMs to understand a correspondingly increased number of visual tokens.", "title_embedding_index": 17061, "title_abs_embedding_index": 17086}, {"title": "LJ-Bench: Ontology-based Benchmark for Crime", "link_suffix": "/forum?id=1ymGFnxfVB", "link": "https://openreview.net/forum?id=1ymGFnxfVB", "pdf_link": "https://openreview.net/pdf?id=1ymGFnxfVB", "keywords": "Ontology, Knowledge Graph, Crime, Language Models", "abstract": "Despite the remarkable capabilities of Large Language Models (LLMs), their potential to provide harmful information remains a significant concern due to the vast breadth of illegal queries they may encounter. In this work, we firstly introduce structured knowledge in the form of an ontology of crime-related concepts, grounded in legal frameworks. This ontology serves as the foundation for the creation of a comprehensive benchmark, called LJ-Bench, the first extensive dataset designed to rigorously evaluate the robustness of LLMs against a wide range of illegal activities. LJ-Bench includes 76 distinct types of crime, organized into a taxonomy. By systematically assessing the performance of diverse attacks on our benchmark, we gain valuable insights into the vulnerabilities of LLMs across various crime categories, indicating that LLMs exhibit heightened susceptibility to attacks targeting societal harm rather than those directly impacting individuals. Our benchmark aims to facilitate the development of more robust and trustworthy LLMs.", "title_embedding_index": 17062, "title_abs_embedding_index": 17087}, {"title": "HyperCLIP: Adapting Vision-Language models with Hypernetworks", "link_suffix": "/forum?id=a4nSE2kpoq", "link": "https://openreview.net/forum?id=a4nSE2kpoq", "pdf_link": "https://openreview.net/pdf?id=a4nSE2kpoq", "keywords": "vision-language models, edge computing, CLIP, hyper-networks", "abstract": "Self-supervised vision-language models trained with contrastive objectives form the basis of current state-of-the-art methods in AI vision tasks. The success of these models is a direct consequence of the huge web-scale datasets used to train them, but they require correspondingly large vision components to properly learn powerful and general representations from such a broad data domain. This poses a challenge for deploying large vision-language models, especially in resource-constrained environments. To address this, we propose an alternate vision-language architecture, called HyperCLIP, that uses a small image encoder along with a hypernetwork that dynamically adapts image encoder weights to each new set of text inputs.  All three components of the model (hypernetwork, image encoder, and text encoder) are pre-trained jointly end-to-end, and with a trained HyperCLIP model, we can generate new zero-shot deployment-friendly image classifiers for any task with a single forward pass through the text encoder and hypernetwork. HyperCLIP increases the zero-shot accuracy of SigLIP trained models with small image encoders by up to 3% on ImageNet and 5% on CIFAR-100 with minimal training throughput overhead.", "title_embedding_index": 17063, "title_abs_embedding_index": 17088}, {"title": "Linear Transformer Topological Masking with Graph Random Features", "link_suffix": "/forum?id=6MBqQLp17E", "link": "https://openreview.net/forum?id=6MBqQLp17E", "pdf_link": "https://openreview.net/pdf?id=6MBqQLp17E", "keywords": "transformer, linear, attention, graph, random walk, Monte Carlo, encoding, topological masking, point cloud, Performer", "abstract": "When training transformers on graph-structured data, incorporating information about the underlying topology is crucial for good performance. Topological masking, a type of relative position encoding, achieves this by upweighting or downweighting attention depending on the relationship between the query and keys in the graph. In this paper, we propose to parameterise topological masks as a learnable function of a weighted adjacency matrix -- a novel, flexible approach which incorporates a strong structural inductive bias. By approximating this mask with graph random features (for which we prove the first known concentration bounds), we show how this can be made fully compatible with linear attention, preserving $\\mathcal{O}(N)$ time and space complexity with respect to the number of input tokens. The fastest previous alternative was $\\mathcal{O}(N \\log N)$ and only suitable for specific graphs. Our efficient masking algorithms provide strong performance gains for image and point cloud data, including with $>30$k nodes.", "title_embedding_index": 17064, "title_abs_embedding_index": 17089}, {"title": "MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark", "link_suffix": "/forum?id=TeVAZXr3yv", "link": "https://openreview.net/forum?id=TeVAZXr3yv", "pdf_link": "https://openreview.net/pdf?id=TeVAZXr3yv", "keywords": "Benchmark, Audio Language Models, Complex Reasoning", "abstract": "The ability to comprehend audio\u2014which includes speech, non-speech sounds, and music\u2014is crucial for AI agents to interact effectively with the world. We present MMAU, a novel benchmark designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning. MMAU comprises 10k carefully curated audio clips paired with human-annotated natural language questions and answers spanning speech, environmental sounds, and music. It includes information extraction and reasoning questions, requiring models to demonstrate 27 distinct skills across unique and challenging tasks. Unlike existing benchmarks, MMAU emphasizes advanced perception and reasoning with domain-specific knowledge, challenging models to tackle tasks akin to those faced by experts. We assess 18 open-source and proprietary (Large) Audio-Language Models, demonstrating the significant challenges\nposed by MMAU. Notably, even the most advanced Gemini Pro v1.5 achieves only 52.97% accuracy, and the state-of-the-art open-source Qwen2-Audio achieves only 52.50%, highlighting considerable room for improvement. We believe MMAU will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.", "title_embedding_index": 17065, "title_abs_embedding_index": 17090}, {"title": "Context-Augmented Code Generation Using Programming Knowledge Graphs", "link_suffix": "/forum?id=EHfn5fbFHw", "link": "https://openreview.net/forum?id=EHfn5fbFHw", "pdf_link": "https://openreview.net/pdf?id=EHfn5fbFHw", "keywords": "Code Generation, RAG, Knowledge Graphs, Large Language Models", "abstract": "Large Language Models (LLMs) and Code-LLMs (CLLMs) have significantly improved code generation, but, they frequently face difficulties when dealing with challenging and complex problems. Retrieval-Augmented Generation (RAG) addresses this issue by retrieving and integrating external knowledge at the inference time. However, retrieval models often fail to find most relevant context, and generation models, with limited context capacity, can hallucinate when given irrelevant data. We present a novel framework that leverages a Programming Knowledge Graph (PKG) to semantically represent and retrieve code. This approach enables fine-grained code retrieval by focusing on the most relevant segments while reducing irrelevant context through a tree-pruning technique. PKG is coupled with a re-ranking mechanism to  reduce even more hallucinations by selectively integrating non-RAG solutions. We propose two retrieval approaches\u2014block-wise and function- wise\u2014based on the PKG, optimizing context granularity. Evaluations on the HumanEval and MBPP benchmarks show our method improves pass@1 accuracy by up to 20%, and outperforms state-of-the-art models by up to 34% on MBPP. Our contributions include PKG-based retrieval, tree pruning to enhance retrieval precision, a re-ranking method for robust solution selection and a Fill-in-the- Middle (FIM) enhancer module for automatic code augmentation with relevant comments and docstrings.", "title_embedding_index": 17066, "title_abs_embedding_index": 17091}, {"title": "Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy", "link_suffix": "/forum?id=5xxGP9x5dZ", "link": "https://openreview.net/forum?id=5xxGP9x5dZ", "pdf_link": "https://openreview.net/pdf?id=5xxGP9x5dZ", "keywords": "Machine unlearning, Security, Privacy, Attack", "abstract": "Machine unlearning algorithms, designed for selective removal of training data from models, have emerged as a promising approach to growing privacy concerns. In this work, we expose a critical yet underexplored vulnerability in the deployment of unlearning systems: the assumption that the data requested for removal is always part of the original training set. We present a threat model where an attacker can degrade model accuracy by submitting adversarial unlearning requests for data not present in the training set. We propose white-box and black-box attack algorithms and evaluate them through a case study on image classification tasks using the CIFAR-10 and ImageNet datasets, targeting a family of widely used unlearning methods. Our results show extremely poor test accuracy following the attack\u20143.6% on CIFAR-10 and 0.4% on ImageNet for white-box attacks, and 8.5% on CIFAR-10 and 1.3% on ImageNet for black-box attacks. Additionally, we evaluate various verification mechanisms to detect the legitimacy of unlearning requests and reveal the challenges in verification, as most of the mechanisms fail to detect stealthy attacks without severely impairing their ability to process valid requests. These findings underscore the urgent need for research on more robust request verification methods and unlearning protocols, should the deployment of machine unlearning systems become more relevant in the future.", "title_embedding_index": 17067, "title_abs_embedding_index": 17092}, {"title": "A3D: Does Diffusion Dream about 3D Alignment?", "link_suffix": "/forum?id=QQCIfkhGIq", "link": "https://openreview.net/forum?id=QQCIfkhGIq", "pdf_link": "https://openreview.net/pdf?id=QQCIfkhGIq", "keywords": "Aligned 3D generation, 3D generation and editing, Text-to-image diffusion, Score distillation sampling, Implicit neural surfaces, Radiance fields", "abstract": "We tackle the problem of text-driven 3D generation from a geometry alignment perspective.\nGiven a set of the text prompts, we aim at the generation of a collection of objects with their semantically corresponding parts being aligned between these objects.\nRecent methods based on Score Distillation have succeeded in distilling the knowledge from 2D diffusion models to high-quality representations of the 3D objects.\nThese methods handle multiple text queries separately, and therefore the resulting objects have a high variability in object pose and structure.\nHowever, in some applications, such as 3D asset design, it may be desirable to obtain a set of objects aligned with each other.\nIn order to achieve the alignment of the corresponding parts of the generated objects, we propose to embed these objects into a common latent space and optimize the continuous transitions between these objects.\nWe enforce two kinds of properties of these transitions: smoothness of the transition and plausibility of the intermediate objects along the transition.\nWe demonstrate that both of this properties are essential for good alignment.\nWe provide several practical scenarios that benefit from alignment between the objects, including 3D editing and object hybridization, and experimentally demonstrate the effectiveness of our method.", "title_embedding_index": 17068, "title_abs_embedding_index": 17093}, {"title": "Avoid Being a Shortcut Learner through Library-Based Re-Learning", "link_suffix": "/forum?id=gCYFtUKXSc", "link": "https://openreview.net/forum?id=gCYFtUKXSc", "pdf_link": "https://openreview.net/pdf?id=gCYFtUKXSc", "keywords": "Continual learning, Data-Efficient Learning, Information Theory", "abstract": "Replay-based methods provide a promising solution to address catastrophic forgetting issue in continual learning. They try to retain previous knowledge by using a small amount of data from previous tasks stored in a fix-sized buffer. In this work, we invoke the information bottleneck principles and reveal some fundamental limitations of those methods on their effectiveness in capturing the truly important features from the prior tasks by relying on the buffer data selected according to the model's performance on known tasks. Since future tasks are not accessible during model training and buffer construction, the trained model and the buffer data tend to be biased towards making accurate predictions on the labels of known tasks. However, when new task samples are introduced along with labels, the biased model and the buffer data become less effective in differentiating samples of the old tasks from those of the new ones. Inspired by the way humans learn over time, we propose a novel relearning technique that makes use of additional past data, referred to as the library, to test how much information the model loses after learning the new task. We then realign the model towards those forgotten samples by training on a carefully selected small subset samples from the library for a few epochs with comparable computational cost as existing replay-based models. The experimental results on multiple real-world datasets demonstrate that the proposed relearning process can improve the performance of the state-of-the-art continual learning methods by a large margin.", "title_embedding_index": 17069, "title_abs_embedding_index": 17094}, {"title": "Language Model Non-Myopic Generation for Reasoning and Planning", "link_suffix": "/forum?id=OoNazl6T7D", "link": "https://openreview.net/forum?id=OoNazl6T7D", "pdf_link": "https://openreview.net/pdf?id=OoNazl6T7D", "keywords": "LLM reasoning; agents; optimal control", "abstract": "Large Language Models have demonstrated remarkable abilities in reasoning and planning by breaking down complex problems into sequential steps. Despite their success in various domains like mathematical problem-solving and coding, LLMs face challenges in ensuring reliable and optimal planning due to their inherent myopic nature of autoregressive decoding. This paper revisits LLM reasoning from an optimal-control perspective, proposing a novel method, Predictive-Decoding, that leverages Model Predictive Control to enhance planning accuracy. By re-weighting LLM distributions based on foresight trajectories, Predictive-Decoding aims to mitigate early errors and promote non-myopic planning. Our experiments show significant improvements in a wide range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding demonstrates computational efficiency, outperforming search baselines with reduced computational resources. This study provides insights into optimizing LLM planning capabilities.", "title_embedding_index": 17070, "title_abs_embedding_index": 17095}, {"title": "From Eww to Woo: Detection of Mental Health Disturbing Images in Social Media", "link_suffix": "/forum?id=rSNkMy4OkJ", "link": "https://openreview.net/forum?id=rSNkMy4OkJ", "pdf_link": "https://openreview.net/pdf?id=rSNkMy4OkJ", "keywords": "Mental Health, Social Media, Harmful", "abstract": "Exposure to distressing images on social media, such as gore and other graphic content, can lead to significant mental health issues and disturbances. This paper introduces a novel dataset specifically curated to include such harmful images, aiming to facilitate the development of machine learning models capable of detecting and filtering these types of content. By training on this dataset, the proposed models demonstrate the ability to accurately identify and flag disturbing images, thereby contributing to the mitigation of mental health risks associated with prolonged exposure to harmful visual content on social media platforms. The proposed dataset is benchmarked on various state of the art models with the accuracy 70.15%. This work represents a critical step towards creating safer online environments and protecting users' mental well-being.", "title_embedding_index": 17071, "title_abs_embedding_index": 17096}, {"title": "3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing", "link_suffix": "/forum?id=7JUrBLDjCq", "link": "https://openreview.net/forum?id=7JUrBLDjCq", "pdf_link": "https://openreview.net/pdf?id=7JUrBLDjCq", "keywords": "3D Editing, Diffusion Model, 3D Vision", "abstract": "The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag, a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.", "title_embedding_index": 17072, "title_abs_embedding_index": 17097}, {"title": "Improving Antibody Design with Force-Guided Sampling in Diffusion Models", "link_suffix": "/forum?id=9120xQKmcN", "link": "https://openreview.net/forum?id=9120xQKmcN", "pdf_link": "https://openreview.net/pdf?id=9120xQKmcN", "keywords": "diffusion models, antibody design", "abstract": "Antibodies, crucial for immune defense, primarily rely on complementarity-determining regions (CDRs) to bind and neutralize antigens, such as viruses. The design of these CDRs determines the antibody's affinity and specificity towards its target. Generative models, particularly denoising diffusion probabilistic models (DDPMs), have shown potential to advance the structure-based design of CDR regions. However, only a limited dataset of bound antibody-antigen structures is available, and generalization to out-of-distribution interfaces remains a challenge. Physics based force-fields, which approximate atomic interactions, offer a coarse but universal source of information to better mold designs to target interfaces. Integrating this foundational information into diffusion models is, therefore, highly desirable. Here, we propose a novel approach to enhance the sampling process of diffusion models by integrating force field energy-based feedback. Our model, DiffForce, employs forces to guide the diffusion sampling process, effectively blending the two distributions. Through extensive experiments, we demonstrate that our method guides the model to sample CDRs with lower energy, enhancing both the structure and sequence of the generated antibodies.", "title_embedding_index": 17073, "title_abs_embedding_index": 17098}, {"title": "Learning Linear Utility Functions From Pairwise Comparison Queries", "link_suffix": "/forum?id=ayZsi8YA7h", "link": "https://openreview.net/forum?id=ayZsi8YA7h", "pdf_link": "https://openreview.net/pdf?id=ayZsi8YA7h", "keywords": "Value Alignment, Learning from pairwise comparisons, Computational social choice", "abstract": "There is increasingly widespread use of reward model learning from human preferences to align AI systems with human values, with applications including large language models, recommendation systems, and robotic control.  Nevertheless, a fundamental understanding of our ability to successfully learn utility functions in this model remains limited. We initiate this line of work by studying learnability of linear utility functions from pairwise comparison queries. In particular, we consider two learning objectives. The first objective is to predict out-of-sample responses to pairwise comparisons, whereas the second is to approximately recover the true parameters of the utility function. We show that in the passive learning setting, linear utilities are efficiently learnable with respect to the first objective, both when query responses are uncorrupted by noise, and under Tsybakov noise when the distributions are sufficiently \"nice\". In contrast, we show that utility parameters are not learnable for a large set of data distributions without strong modeling assumptions, even when query responses are noise-free. Next, we proceed to analyze the learning problem in an active learning setting. In this case, we show that even the second objective is efficiently learnable, and present algorithms for both the noise-free and noisy query response settings. This qualitative learnability gap between passive and active learning from pairwise comparisons suggests that the tendency of conventional alignment practices to simply annotate a fixed set of queries may fail to yield effective reward model estimates, an issue that can be remedied through more deliberate query selection.", "title_embedding_index": 17074, "title_abs_embedding_index": 17099}]