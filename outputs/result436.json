[{"title": "Social Bayesian Optimization for Building Truthful Consensus", "link_suffix": "/forum?id=WJnciuhwyU", "link": "https://openreview.net/forum?id=WJnciuhwyU", "pdf_link": "https://openreview.net/pdf?id=WJnciuhwyU", "keywords": "Bayesian optimisation, social choice theory, preference learning", "abstract": "We introduceSocial Bayesian Optimization(SBO), a query-efficient algorithm for consensus-building in collective decision-making. In contrast to single-agent scenarios, collective decision-making encompasses group dynamics that may distort agents' preference feedback, thereby impeding their capacity to achieve a truthful consensus. We demonstrate that under standard rationality assumptions, reaching truthful consensus\u2014the most preferable decision based on the aggregated latent agent utilities\u2014using noisy feedback alone is impossible. To address this, SBO employs a dual voting system: cost-effective but noisy public votes, and more accurate, though expensive, private votes. We model social influence using an unknown social graph and leverage the dual voting system to efficiently learn this graph. Our findings show that social graph estimation converges faster than the black-box estimation of agents\u2019 utilities, allowing us to reduce reliance on costly private votes early in the process. This enables efficient consensus-building primarily through noisy public votes, which are debiased based on the estimated social graph to infer truthful feedback. We validate the effectiveness of SBO across multiple real-world applications, including thermal comfort optimization, team building, travel destination discussion, and strategic alliance in energy trading.", "title_embedding_index": 21750, "title_abs_embedding_index": 21775}, {"title": "Causal Identification for Complex Functional Longitudinal Studies", "link_suffix": "/forum?id=96beVMeHh9", "link": "https://openreview.net/forum?id=96beVMeHh9", "pdf_link": "https://openreview.net/pdf?id=96beVMeHh9", "keywords": "Causal Inference, Stochastic Process, Longitudinal Data; Functional Data, Continuous Time.", "abstract": "Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference methodologies cannot handle. Inspired by the coarsened data framework, we adopt stochastic process theory, measure theory, and net convergence to propose a nonparametric causal identification framework. This framework generalizes classical g-computation, inverse probability weighting, and doubly robust formulas, accommodating time-varying outcomes subject to mortality and censoring for functional longitudinal data. We examine our framework through Monte Carlo simulations. Our approach addresses significant gaps in current methodologies, providing a solution for functional longitudinal data and paving the way for future estimation work in this domain.", "title_embedding_index": 21751, "title_abs_embedding_index": 21776}, {"title": "MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained Models", "link_suffix": "/forum?id=iR5qF9N1Ge", "link": "https://openreview.net/forum?id=iR5qF9N1Ge", "pdf_link": "https://openreview.net/pdf?id=iR5qF9N1Ge", "keywords": "Adversarial Attack, Vision-Language Pre-trained Models, Robustness", "abstract": "Current adversarial attacks for evaluating the robustness of vision-language pre-trained (VLP) models in multi-modal tasks suffer from limited transferability, where attacks crafted for a specific model often struggle to generalize effectively across different models, limiting their utility in assessing robustness more broadly. This is mainly attributed to the over-reliance on model-specific features and regions, particularly in the image modality. In this paper, we propose an elegant yet highly effective method termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent characteristics and vulnerabilities of individual samples, achieving enhanced generalizability and reduced model dependence. MAA emphasizes fine-grained optimization of adversarial images by developing a novel resizing and sliding crop (RScrop) technique, incorporating a multi-granularity similarity disruption (MGSD) strategy. \nRScrop efficiently enriches the initial adversarial examples by generating more comprehensive, diverse, and detailed perspectives of the images, establishing a robust foundation for capturing representative and intrinsic visual characteristics. Building on this,  MGSD seeks to maximize %the layer- and component-wise feature% \nthe embedding distance between adversarial examples and their original counterparts across different granularities and hierarchical levels within the architecture of VLP models, thereby amplifying the impact of the adversarial perturbations and enhancing the efficacy of attacks across every layer and component of the model. Extensive experiments across diverse VLP models, multiple benchmark datasets, and a variety of downstream tasks demonstrate that MAA significantly enhances the effectiveness and transferability of adversarial attacks. A large cohort of performance studies is conducted to generate insights into the effectiveness of various model configurations, guiding future advancements in this domain. The source code is provided in the supplementary material.", "title_embedding_index": 21752, "title_abs_embedding_index": 21777}, {"title": "Harmonious convergence for confidence estimation in depth estimation and completion", "link_suffix": "/forum?id=MmzZyHG1Te", "link": "https://openreview.net/forum?id=MmzZyHG1Te", "pdf_link": "https://openreview.net/pdf?id=MmzZyHG1Te", "keywords": "confidence estimation, monocular depth estimation, depth completion", "abstract": "Confidence estimation for monocular  depth estimation and completion  is important for their deployment  in real-world applications. Recent models for confidence estimation in these regression tasks   mainly   rely on the statistical characteristics of training and test data, while ignoring the information from the model training.  We propose a harmonious convergence estimation approach for  confidence estimation in the regression tasks, taking training consistency into consideration. Specifically, we propose an intra-batch convergence estimation algorithm with two sub-iterations to compute the training consistency for confidence estimation. A  harmonious convergence loss is newly designed to encourage the consistency between confidence measure and depth prediction. Our experimental results on the NYU2 and KITTI datasets show improvements ranging from 10.91% to 43.90% across different settings in monocular depth estimation, and from 27.91% to 45.24% in depth completion, measured by Pearson correlation coefficients, justifying the effectiveness of the  proposed method. We will release all the codes upon the publication of our paper.", "title_embedding_index": 21753, "title_abs_embedding_index": 21778}, {"title": "Self-Corrected Multimodal Large Language Model for Robot Manipulation and Reflection", "link_suffix": "/forum?id=TLWbNfbkxj", "link": "https://openreview.net/forum?id=TLWbNfbkxj", "pdf_link": "https://openreview.net/pdf?id=TLWbNfbkxj", "keywords": "Robot Manipulation, Pose Correction, Multimodal Large Language Model", "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated potential in visual instruction following across various tasks. Recently, some studies have integrated MLLMs into robotic manipulation, allowing robots to interpret multimodal information and predict low-level actions. While MLLM-based policies have shown promising progress, they may predict failure execution poses when faced with novel tasks or categories. To emulate human-like reasoning modes for more robust manipulation, we propose a Self-Corrected (SC)-MLLM. Our model combines fast system reasoning for directly predicting end-effector poses with slow system reasoning for reflecting on and correcting failure actions. For the fast system, we introduce parameter-efficient fine-tuning to empower MLLM with pose prediction capabilities, reframing this as a language modeling problem. For the slow system, when facing execution failures, our model learns to detect the causes of low-level action errors (i.e., position and rotation errors) and adaptively seeks prompt feedback from experts. Based on the feedback, SC-MLLM reflects on the current failure case and attempts to generate the corrected actions. Furthermore, we design a continuous policy learning method using successfully corrected samples, enhancing the model's adaptability to the current scene configuration and reducing the frequency of expert intervention. To evaluate our method, we conduct extensive experiments in both simulation and real-world settings. SC-MLLM significantly improves manipulation accuracy compared to previous state-of-the-art MLLM-based policy (ManipLLM), increasing from 57% to 79% on seen object categories and from 47% to 69% on unseen novel categories. Our project web page:https://sites.google.com/view/sc-mllm", "title_embedding_index": 21754, "title_abs_embedding_index": 21779}, {"title": "MEGA: Memory-Efficient 4D Gaussian Splatting for Dynamic Scenes", "link_suffix": "/forum?id=NLAKxnnSuW", "link": "https://openreview.net/forum?id=NLAKxnnSuW", "pdf_link": "https://openreview.net/pdf?id=NLAKxnnSuW", "keywords": "4D Gaussian Splatting, Compression", "abstract": "4D Gaussian Splatting (4DGS) has recently emerged as a promising technique for capturing complex dynamic 3D scenes with high fidelity. It utilizes a 4D Gaussian representation and a GPU-friendly rasterizer, enabling rapid rendering speeds. Despite its advantages, 4DGS faces significant challenges, notably the requirement of millions of 4D Gaussians, each with extensive associated attributes, leading to substantial memory and storage cost. This paper introduces a memory-efficient framework for 4DGS. We streamline the color attribute by decomposing it into a per-Gaussian direct color component with only 3 parameters and a shared lightweight alternating current color predictor. This approach eliminates the need for spherical harmonics coefficients, which typically involve up to 144 parameters in classic 4DGS, thereby creating a memory-efficient 4D Gaussian representation. Furthermore, we introduce an entropy-constrained Gaussian deformation technique that uses a deformation field to expand the action range of each Gaussian and integrates an opacity-based entropy loss to limit the number of Gaussians, thus forcing our model to use as few Gaussians as possible to fit a dynamic scene well. With simple half-precision storage and zip compression, our framework achieves a storage reduction by approximately 190$\\times$ and 125$\\times$ on the Technicolor and Neural 3D Video datasets, respectively, compared to the original 4DGS. Meanwhile, it maintains comparable rendering speeds and scene representation quality, setting a new standard in the field.", "title_embedding_index": 21755, "title_abs_embedding_index": 21780}, {"title": "Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation", "link_suffix": "/forum?id=2RfWRKwxYh", "link": "https://openreview.net/forum?id=2RfWRKwxYh", "pdf_link": "https://openreview.net/pdf?id=2RfWRKwxYh", "keywords": "dataset distillation, self-supervised learning", "abstract": "Although larger datasets are crucial for training large deep models, the rapid growth of dataset size has brought a significant challenge in terms of considerable training costs, which even results in prohibitive computational expenses. Dataset Distillation becomes a popular technique recently to reduce the dataset size via learning a highly compact set of representative exemplars, where the model trained with these exemplars ideally should have comparable performance with respect to the one trained with the full dataset. While most of existing works upon dataset distillation focus on supervised datasets, \\todo{we instead aim to distill images and their self-supervisedly trained representations into a distilled set. This procedure, named as Self-Supervised Dataset Distillation, effectively extracts rich information from real datasets, yielding the distilled sets with enhanced cross-architecture generalizability.} Particularly, in order to preserve the key characteristics of original dataset more faithfully and compactly, several novel techniques are proposed: 1) we introduce an innovative parameterization upon images and representations via distinct low-dimensional bases, where the base selection for parameterization is experimentally shown to play a crucial role; 2) we tackle the instability induced by the randomness of data augmentation -- a key component in self-supervised learning but being underestimated in the prior work of self-supervised dataset distillation -- by utilizing predetermined augmentations; 3) we further leverage a lightweight network to model the connections among the representations of augmented views from the same image, leading to more compact pairs of distillation. Extensive experiments conducted on various datasets validate the superiority of our approach in terms of distillation efficiency, cross-architecture generalization, and transfer learning performance.", "title_embedding_index": 21756, "title_abs_embedding_index": 21781}, {"title": "Poison-splat: Computation Cost Attack on 3D Gaussian Splatting", "link_suffix": "/forum?id=ExrEw8cVlU", "link": "https://openreview.net/forum?id=ExrEw8cVlU", "pdf_link": "https://openreview.net/pdf?id=ExrEw8cVlU", "keywords": "gaussian splatting, energy-latency attack, data poisoning attack", "abstract": "3D Gaussian splatting (3DGS), known for its groundbreaking performance and efficiency, has become a dominant 3D representation and brought progress to many 3D vision tasks. However, in this work, we reveal a significant security vulnerability that has been largely overlooked in 3DGS: the computation cost of training 3DGS could be maliciously tampered by poisoning the input data. By developing an attack named Poison-splat, we reveal a novel attack surface where the adversary can poison the input images to drastically increase the computation memory and time needed for 3DGS training, pushing the algorithm towards its worst computation complexity. In extreme cases, the attack can even consume all allocable memory, leading to a Denial-of-Service (DoS) that disrupts servers, resulting in practical damages to real-world 3DGS service vendors. Such a computation cost attack is achieved by addressing a bi-level optimization problem through three tailored strategies: attack objective approximation, proxy model rendering, and optional constrained optimization. These strategies not only ensure the effectiveness of our attack but also make it difficult to defend with simple defensive measures. We hope the revelation of this novel attack surface can spark attention to this crucial yet overlooked vulnerability of 3DGS systems.", "title_embedding_index": 21757, "title_abs_embedding_index": 21782}, {"title": "Part-aware Personalized Segment Anything Model for Patient-Specific Segmentation", "link_suffix": "/forum?id=czvVNVLr7R", "link": "https://openreview.net/forum?id=czvVNVLr7R", "pdf_link": "https://openreview.net/pdf?id=czvVNVLr7R", "keywords": "Precision Medicine; Patient-Specific Segmentation; Out-of-Distribution Patient Adaptation", "abstract": "Precision medicine, such as patient-adaptive treatments utilizing medical images, poses new challenges for image segmentation algorithms due to (1) the large variability across different patients and (2) the limited availability of annotated data for each patient. In this work, we propose a data-efficient segmentation method to address these challenges, namely $\\textit{\\textbf{P}art-aware}$ $\\textit{\\textbf{P}ersonalized}$ $\\textit{\\textbf{S}egment}$ $\\textit{\\textbf{A}nything}$ $\\textit{\\textbf{M}odel}$ ($\\mathbf{{P}^{2}SAM}$). Without any model fine-tuning, enables seamless adaptation to any new patients relying only on one-shot patient-specific data. We introduce a novel part-aware prompt mechanism to select multiple-point prompts based on part-level features of the one-shot data, which can be extensively integrated into different promptable segmentation models, such as SAM and SAM 2. To further promote the robustness of the selected part-aware prompt, we propose a distribution-similarity-based retrieval approach to determine the optimal number of part-level features for a specific case. $\\text{P}^{\\text{2}}\\text{SAM}$ improves the performance by $\\texttt{+} 8.0$% and $\\texttt{+} 2.0$% mean Dice score within two patient-specific segmentation tasks, and exhibits impressive generality across different domains, $\\textit{e.g.}$, $\\texttt{+} 6.4$% mIoU on the PerSeg benchmark. Code will be released upon acceptance.", "title_embedding_index": 21758, "title_abs_embedding_index": 21783}, {"title": "Story-Adapter: A Training-free Iterative Framework for Long Story Visualization", "link_suffix": "/forum?id=X8M4fansEz", "link": "https://openreview.net/forum?id=X8M4fansEz", "pdf_link": "https://openreview.net/pdf?id=X8M4fansEz", "keywords": "story visualization; diffusion model; subject-consistent image generation", "abstract": "Story visualization, the task of generating coherent images based on a narrative, has seen significant advancements with the emergence of text-to-image models, particularly diffusion models. However, maintaining semantic consistency, generating high-quality fine-grained interactions, and ensuring computational feasibility remain challenging, especially in long story visualization (i.e., up to 100 frames). In this work, we propose a training-free and computationally efficient framework, termed Story-Adapter, to enhance the generative capability of long stories. Specifically, we propose an iterative paradigm to refine each generated image, leveraging both the text prompt and all generated images from the previous iteration. Central to our framework is a training-free global reference crossattention module, which aggregates all generated images from the previous iteration to preserve semantic consistency across the entire story, while minimizing computational costs with global embeddings. This iterative process progressively optimizes image generation by repeatedly incorporating text constraints, resulting in more precise and fine-grained interactions. Extensive experiments validate the superiority of Story-Adapter in improving both semantic consistency and generative capability for fine-grained interactions, particularly in long story scenarios.", "title_embedding_index": 21759, "title_abs_embedding_index": 21784}, {"title": "Measuring Effects of Steered Representation in Large Language Models", "link_suffix": "/forum?id=z1yI8uoVU3", "link": "https://openreview.net/forum?id=z1yI8uoVU3", "pdf_link": "https://openreview.net/pdf?id=z1yI8uoVU3", "keywords": "in-context learning, activation steering, large language models", "abstract": "Large Language Models (LLMs) show advanced performance and adaptability across various tasks. As the model size becomes more extensive, precise control by editing the forward process of LLMs is a challenging problem. Recent research has focused on steering hidden representations during forward propagation to guide model outputs in desired directions, yielding precise control over specific responses. Although steering shows a broader impact on diverse tasks, the influence of steered representations remains unclear. For instance, steering towards a refusal direction might lead the model to refuse even benign requests in subsequent generations. This work tackles the problem of evaluating activation steering.  We introduce a counterfactual-based steering evaluation framework that compares the output of base and steered generations. Within the framework, we propose a steering effect matrix that eases the selection of generations base and steered output types. We experimentally evaluate the effects of steered representation for consequence generation with Llama3-8B, Llama2-7B, and Exaone-8B across diverse datasets. We conclude that steered representation changes the original output severely in longer contexts.", "title_embedding_index": 21760, "title_abs_embedding_index": 21785}, {"title": "KAN versus MLP on Irregular or Noisy Functions", "link_suffix": "/forum?id=TH4gKbZS1E", "link": "https://openreview.net/forum?id=TH4gKbZS1E", "pdf_link": "https://openreview.net/pdf?id=TH4gKbZS1E", "keywords": "Kolmogorov-Arnold networks, Multi-layer Perceptrons, KAN, MLP, Irregularization, Noise", "abstract": "In this paper, we compare the performance of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptron (MLP) networks on irregular or noisy functions. We control the number of parameters and the size of the training samples to ensure a fair comparison. For clarity, we categorize the functions into six types: regular functions, continuous functions with local non-differentiable points, functions with jump discontinuities, functions with singularities, functions with coherent oscillations, and noisy functions. Our experimental results indicate that KAN does not always perform best. For some types of functions, MLP outperforms or performs comparably to KAN. Furthermore, increasing the size of training samples can improve performance to some extent. When noise is added to functions, the irregular features are often obscured by the noise, making it challenging for both MLP and KAN to extract these features effectively. We hope these experiments provide valuable insights for future neural network research and encourage further investigations to overcome these challenges.", "title_embedding_index": 21761, "title_abs_embedding_index": 21786}, {"title": "GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation", "link_suffix": "/forum?id=KYOdZRR6nr", "link": "https://openreview.net/forum?id=KYOdZRR6nr", "pdf_link": "https://openreview.net/pdf?id=KYOdZRR6nr", "keywords": "Generative Model, 3D, Computer Graphics", "abstract": "We propose GALA, a novel representation of 3D shapes that (i) excels at capturing and reproducing complex geometry and surface details, (ii) is computationally efficient, and (iii) lends itself to 3D generative modelling with modern, diffusion-based schemes. The key idea of GALA is to exploit both the global sparsity of surfaces within a 3D volume and their local surface properties.Sparsityis promoted by covering only the 3D object boundaries, not empty space, with an ensemble of tree root voxels. Each voxel contains an octree to further limit storage and compute to regions that contain surfaces.Adaptivityis achieved by fitting one local and geometry-aware coordinate frame in each non-empty leaf node. Adjusting the orientation of the local grid, as well as the anisotropic scales of its axes, to the local surface shape greatly increases the amount of  detail that can be stored in a given amount of memory, which in turn allows for quantization without loss of quality. With our optimized C++/CUDA implementation, GALA can be fitted to an object in less than 10 seconds. Moreover, the representation can efficiently be flattened and manipulated with transformer networks. We provide a cascaded generation pipeline capable of generating 3D shapes with great geometric detail.", "title_embedding_index": 21762, "title_abs_embedding_index": 21787}, {"title": "MMEgo: Towards Building Egocentric Multimodal LLMs", "link_suffix": "/forum?id=67sSPPAZiG", "link": "https://openreview.net/forum?id=67sSPPAZiG", "pdf_link": "https://openreview.net/pdf?id=67sSPPAZiG", "keywords": "multimodal models", "abstract": "This research aims to comprehensively explore building a multimodal foundation model for egocentric video understanding.\nTo achieve this goal, we work on three fronts: First, as there is a lack of QA data for egocentric video understanding, we curate a high-quality, large-scale dataset with QA samples for egocentric videos ranging from 30 seconds to one hour long, based on human-annotated data. Second, we contribute a challenging egocentric QA benchmark with 629 videos and 7,026 questions to evaluate the models' ability in recognizing and memorizing visual details across videos of varying lengths. We introduce a new de-biasing evaluation method to help mitigate the unavoidable language bias present in the models being evaluated. Third, we introduce a specialized multimodal architecture featuring a novel ``Memory Pointer Prompting\" mechanism. This design includes a global glimpse step to gain an overarching understanding of the entire video and identify key visual information, followed by a fallback step that utilizes the key visual information to generate responses. This enables the model to more effectively comprehend extended video content. With the data, benchmark, and model, we successfully build MM-Ego, an egocentric multimodal LLM that shows powerful performance on egocentric video understanding.", "title_embedding_index": 21763, "title_abs_embedding_index": 21788}, {"title": "REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments", "link_suffix": "/forum?id=NxyfSW6mLK", "link": "https://openreview.net/forum?id=NxyfSW6mLK", "pdf_link": "https://openreview.net/pdf?id=NxyfSW6mLK", "keywords": "Generalist Agent, Retrieval, In-Context Learning, Imitation Learning, Reinforcement Learning", "abstract": "Do generalist agents require large models pre-trained on massive amounts of data to rapidly adapt to new environments? We propose a novel approach to pre-train relatively small models and adapt them to unseen environments via in-context learning, without any finetuning. Our key idea is that retrieval offers a powerful bias for fast adaptation. Indeed, we demonstrate that even a simple retrieval-based 1-nearest neighbor agent offers a surprisingly strong baseline for today's state-of-the-art generalist agents. From this starting point, we construct a semi-parametric agent, REGENT, that trains a transformer-based policy on sequences of queries and retrieved neighbors. REGENT can generalize to unseen robotics and game-playing environments via retrieval augmentation and in-context learning, achieving this with up to 3x fewer parameters and up to an order-of-magnitude fewer pre-training datapoints, significantly outperforming today's state-of-the-art generalist agents.", "title_embedding_index": 21764, "title_abs_embedding_index": 21789}, {"title": "HandsOnVLM: Vision-Language Models for Hand-Object Interaction Prediction", "link_suffix": "/forum?id=AJQuTFd9es", "link": "https://openreview.net/forum?id=AJQuTFd9es", "pdf_link": "https://openreview.net/pdf?id=AJQuTFd9es", "keywords": "Vision-language Model, Hand-object Interaction", "abstract": "How can we predict future interaction trajectories of human hands in a scene given high-level colloquial task specifications in the form of natural language? In this paper, we extend the classic hand trajectory prediction task to two tasks involving explicit or implicit language queries. Our proposed tasks require extensive understanding of human daily activities and reasoning abilities about what is happening next given cues from the current scene. We also develop new benchmarks to evaluate the proposed two tasks, Vanilla Hand Prediction (VHP) and Reasoning-Based Hand Prediction (RBHP). We enable solving these tasks by integrating high-level world knowledge and reasoning capabilities of Vision-Language Models (VLMs) with the auto-regressive nature of low-level ego-centric hand trajectories. Our\nmodel, HandsOnVLM is a novel VLM that can generate textual responses and produce future hand trajectories through natural-language conversations. Our experiments show that HandsOnVLM outperforms existing task-specific methods and other VLM baselines on proposed tasks, and demonstrates its ability to effectively utilize world knowledge for reasoning about low-level human hand trajectories based on the provided context.", "title_embedding_index": 21765, "title_abs_embedding_index": 21790}, {"title": "Common Causes for Sudden Shifts: Linking Phase Transitions in Sinusoidal Networks", "link_suffix": "/forum?id=muN3B40keb", "link": "https://openreview.net/forum?id=muN3B40keb", "pdf_link": "https://openreview.net/pdf?id=muN3B40keb", "keywords": "neural tangent kernel, implicit neural networks, phase transitions, learning dynamics", "abstract": "Different phases of learning dynamics exist when training deep neural networks. These can be characterised by statistics called order parameters.  In this work we identify a shared, underlying mechanism connecting three seemingly distinct phase transitions in the training of a class of deep regression models, specificially Implicit Neural Representations (INRs) of image data. These transitions include: the emergence of wave patterns in residuals (a novel observation), the transition from fast to slow learning, and Neural Tangent Kernel (NTK) alignment.  We relate the order parameters for each phenomenon to a common set of variables derived from a local approximation of the structure of the NTK.  Furthermore, we present experimental evidence demonstrating these transitions coincide.  Our results enable new insights on the inductive biases of sinusoidal INRs.", "title_embedding_index": 21766, "title_abs_embedding_index": 21791}, {"title": "Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge", "link_suffix": "/forum?id=E8gYIrbP00", "link": "https://openreview.net/forum?id=E8gYIrbP00", "pdf_link": "https://openreview.net/pdf?id=E8gYIrbP00", "keywords": "Automated evaluation, LLM as a judge, correlation measures", "abstract": "The effectiveness of automatic evaluation of generative models is typically measured by comparing it to human evaluation using correlation metrics. \nHowever, metrics like Krippendorff's $\\alpha$ and Randolph's $\\kappa$, originally designed to measure the reliability of human labeling, make  assumptions about human behavior and the labeling process. \nIn this paper, we show howrelying on a single aggregate correlation scorecan obscure fundamental differences between human behavior and automatic evaluation methods, including LLM-as-a-Judge. \nSpecifically, we demonstrate that when the proportion of samples with variation or uncertainty in human labels (gathered during human evaluation) is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to human-to-human (HH) correlation. \nThis can create the misleading impression that automatic evaluation is accurate enough to approximate the human majority label. \nHowever, as the proportion of samples with consistent human labels increases, the correlation between machine labels and human majority labels declines, falling below HH correlation. \nBased on these findings, we first propose stratifying results by human label uncertainty to provide a more robust analysis of automatic evaluation performance. Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric -binned Jensen-Shannon Divergence for perceptionfor such scenarios to better measure the effectiveness of  automatic evaluations. Third, we present visualization techniques --perception charts, to compare the strengths and limitations of automatic evaluation and to contextualize correlation measures appropriately", "title_embedding_index": 21767, "title_abs_embedding_index": 21792}, {"title": "Adaptive Curvature Step Size: A Path Geometry Based Approach to Optimization", "link_suffix": "/forum?id=uu2CorJCUi", "link": "https://openreview.net/forum?id=uu2CorJCUi", "pdf_link": "https://openreview.net/pdf?id=uu2CorJCUi", "keywords": "Adaptive Curvature Step Size (ACSS), Adaptive learning rate, Radius of curvature step size, Low-memory optimization, Path geometry, Convergence analysis, PyTorch optimizers, SGD enhancement", "abstract": "We propose the Adaptive Curvature Step Size (ACSS) method, which dynamically adjusts the step size based on the local geometry of the optimization path. Our approach computes the normalized radius of curvature using consecutive gradients along the iterate path and sets the step-size equal to this radius. The effectiveness of ACSS stems from its ability to adapt to the local landscape of the optimization problem. In regions of low curvature, where consecutive gradient steps are nearly identical, ACSS allows for larger steps. Conversely, in areas of high curvature, where gradient steps differ significantly in direction, ACSS reduces the step size. This adaptive behavior enables more efficient navigation of complex loss landscapes. A key advantage of ACSS is its adaptive behavior based on local curvature information, which implicitly captures aspects of the function's second-order geometry without requiring additional memory. We provide a generalized framework for incorporating ACSS into various optimization algorithms, including SGD, Adam, AdaGrad, and RMSProp. Through extensive empirical evaluation on 20 diverse datasets, we compare ACSS variants against 12 popular optimization methods. Our results consistently show that ACSS provides performance benefits. Our results consistently show that ACSS provides performance benefits. We provide PyTorch implementations of ACSS versions for popular optimizers at ouranonymized code repository.", "title_embedding_index": 21768, "title_abs_embedding_index": 21793}, {"title": "LLM Compression with Convex Optimization\u2014Part 1: Weight Quantization", "link_suffix": "/forum?id=0T8vCKa7yu", "link": "https://openreview.net/forum?id=0T8vCKa7yu", "pdf_link": "https://openreview.net/pdf?id=0T8vCKa7yu", "keywords": "weight quantization, model compression, large language models", "abstract": "In recent years, compression of large language models (LLMs) has emerged as an important problem to enable language model deployment on resource-constrained devices, reduce computational costs, and mitigate the environmental footprint of large-scale AI infrastructure. In this paper, we lay down the foundation for LLM quantization from a convex optimization perspective and propose a quantization technique that builds on this foundation for optimum quantization outcomes. Our quantization framework, CVXQ, scales to models containing hundreds of billions of weight parameters and provides users with the flexibility to compress models to any specified model size, post-training. A reference implementation of CVXQ can be obtained from.", "title_embedding_index": 21769, "title_abs_embedding_index": 21794}, {"title": "Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition", "link_suffix": "/forum?id=6tyPSkshtF", "link": "https://openreview.net/forum?id=6tyPSkshtF", "pdf_link": "https://openreview.net/pdf?id=6tyPSkshtF", "keywords": "Reinforcement Learning, Q-Learning, Regret", "abstract": "We study the gap-dependent bounds of two important algorithms for on-policy $Q$-learning for finite-horizon episodic tabular Markov Decision Processes (MDPs): UCB-Advantage (Zhang et al. 2020) and Q-EarlySettled-Advantage (Li et al. 2021). UCB-Advantage and Q-EarlySettled-Advantage improve upon the results based on Hoeffding-type bonuses and achieve the {almost optimal} $\\sqrt{T}$-type regret bound in the worst-case scenario, where $T$ is the total number of steps. However, the benign structures of the MDPs such as a strictly positive suboptimality gap can significantly improve the regret. While gap-dependent regret bounds have been obtained for $Q$-learning with Hoeffding-type bonuses, it remains an open question to establish gap-dependent regret bounds for $Q$-learning using variance estimators in their bonuses and reference-advantage decomposition for variance reduction. We develop a novel error decomposition\nframework to prove gap-dependent regret bounds of UCB-Advantage and Q-EarlySettled-Advantage that are logarithmic in $T$ and improve upon existing ones for $Q$-learning algorithms. Moreover, we establish the gap-dependent bound for the policy switching cost of UCB-Advantage and improve that under the worst-case MDPs. To our knowledge, this paper presents the first gap-dependent regret analysis for $Q$-learning using variance estimators and reference-advantage decomposition and also provides the first gap-dependent analysis on policy switching cost for $Q$-learning.", "title_embedding_index": 21770, "title_abs_embedding_index": 21795}, {"title": "FederatedQ-Learning with Reference-Advantage Decomposition: Almost Optimal Regret and Logarithmic Communication Cost", "link_suffix": "/forum?id=FoUpv84hMw", "link": "https://openreview.net/forum?id=FoUpv84hMw", "pdf_link": "https://openreview.net/pdf?id=FoUpv84hMw", "keywords": "Federated Learning, Reinforcement Learning, variance reduction, communication cost", "abstract": "In this paper, we consider model-free federated reinforcement learning for tabular episodic Markov decision processes. Under the coordination of a central server, multiple agents collaboratively explore the environment and learn an optimal policy without sharing their raw data. Despite recent advances in federated $Q$-learning algorithms achieving near-linear regret speedup with low communication cost, existing algorithms only attain suboptimal regrets compared to the information bound. We propose a novel model-free federated $Q$-Learning algorithm, termed FedQ-Advantage. Our algorithm leverages reference-advantage decomposition for variance reduction and adopts three novel designs: separate event-triggered communication and policy switching, heterogeneous communication triggering conditions, and optional forced synchronization. We prove that our algorithm not only requires a lower logarithmic communication cost but also achieves an almost optimal regret, reaching the information bound up to a logarithmic factor and near-linear regret speedup compared to its single-agent counterpart when the time horizon is sufficiently large.", "title_embedding_index": 21771, "title_abs_embedding_index": 21796}, {"title": "Articulate Anything: Open-vocabulary 3D Articulated Object Generation", "link_suffix": "/forum?id=6akuzEqP38", "link": "https://openreview.net/forum?id=6akuzEqP38", "pdf_link": "https://openreview.net/pdf?id=6akuzEqP38", "keywords": "3D articulated objects, visual prompting, URDF prediction", "abstract": "Generating 3D articulated objects has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to generate a wide range of articulated objects in an open-vocabulary context.\nTo address these limitations, we propose Articulate Anything, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints.\nOur experiments show that \\model~can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system.", "title_embedding_index": 21772, "title_abs_embedding_index": 21797}, {"title": "Your Task May Vary: A Systematic Understanding of Alignment and Safety Degradation when Fine-tuning LLMs", "link_suffix": "/forum?id=vQ0zFYJaMo", "link": "https://openreview.net/forum?id=vQ0zFYJaMo", "pdf_link": "https://openreview.net/pdf?id=vQ0zFYJaMo", "keywords": "safety alignment, task similarity, guardrail durability", "abstract": "Through supervised fine-tuning or reinforcement learning with human feedback, large language models can achieve a certain level of safety alignment during instruction fine-tuning. However, thesesafety guardrailsare often fragile, as models can easily generate harmful content after downstream fine-tuning. Although various methods have been proposed to mitigate this, our paper shifts focus to the durability of safety guardrails, beginning with their formation in the upstream alignment stages. The central question we explore is:Can we construct more durable safety guardrails for specific downstream tasks to ensure models remain safe after fine-tuning?Our experiments demonstrate that the durability of these safety guardrails is closely tied to the similarity between upstream and downstream datasets: higher similarity results in more fragile guardrails after fine-tuning, whereas lower similarity results in more durable guardrails. This finding highlights the importance of dataset diversity and privacy in upstream alignment data. Ensuring the diversity of the alignment dataset, which allows downstream datasets to be less similar to it, enhances the guardrail durability for fine-tuning. Maintaining its privacy prevents the exposure of alignment data that adversaries could exploit. Thus, we advocate for a dual strategy: prioritizing both the privacy and diversity of upstream alignment datasets to fortify safety guardrails against potential threats, ensuring long-term model robustness in real-world applications.", "title_embedding_index": 21773, "title_abs_embedding_index": 21798}, {"title": "Learning Parameter Sharing with Tensor Decompositions and Sparsity", "link_suffix": "/forum?id=tGsumqfOUk", "link": "https://openreview.net/forum?id=tGsumqfOUk", "pdf_link": "https://openreview.net/pdf?id=tGsumqfOUk", "keywords": "Model Compression, Parameter Sharing, Sparsity, Tensor Decomposition, Knowledge Distillation, Transfer Learning, Vision Transformers, Transformers", "abstract": "Large neural networks achieve remarkable performance, but their size hinders deployment on resource-constrained devices. While various compression techniques exist, parameter sharing remains relatively unexplored. This paper introduces Sparsity-enabled Parameter Sharing (SParS), a novel algorithm that leverages the relationship between parameter sharing, tensor decomposition, and sparsity to efficiently compress large vision transformer models. SParS employs a shared base and sparse factors to represent shared neurons across multilayer perceptrons (MLP). Shared parameterization is initialized via Singular Value Decomposition (SVD) and optimized by minimizing block-wise reconstruction error. Experiments demonstrate that SParS compresses DeiT-B and Swin-L MLPs to 25\u201340% of their original parameter count while maintaining accuracy within 1 percentage point of the original models.", "title_embedding_index": 21774, "title_abs_embedding_index": 21799}]