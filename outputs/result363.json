[
    {
        "title": "Planning in Natural Language Improves LLM Search for Code Generation",
        "link_suffix": "/forum?id=48WAZhwHHw",
        "link": "https://openreview.net/forum?id=48WAZhwHHw",
        "pdf_link": "https://openreview.net/pdf?id=48WAZhwHHw",
        "keywords": "LLM, search, inference-time compute, competitive programming, reasoning, code generation, pass@k, diversity",
        "abstract": "While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a pass@200 of 77.0% on LiveCodeBench, outperforming both the best pass-rate achieved without any search (pass@1 = 41.4%) and using standard repeated sampling on top of existing non-search models (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains from search as a function of the diversity over generated ideas."
    },
    {
        "title": "Unveiling the Secret of AdaLN-Zero in Diffusion Transformer",
        "link_suffix": "/forum?id=E4roJSM9RM",
        "link": "https://openreview.net/forum?id=E4roJSM9RM",
        "pdf_link": "https://openreview.net/pdf?id=E4roJSM9RM",
        "keywords": "Diffusion transformer, zero-initialization, image generation",
        "abstract": "Diffusion transformer (DiT), a rapidly emerging architecture for image generation, has gained much attention. However, despite ongoing efforts to improve its performance, the understanding of DiT remains superficial. In this work, we delve into and investigate a critical conditioning mechanism within DiT, adaLN-Zero, which achieves superior performance compared to adaLN. Our work studies three potential elements driving this performance, including an SE-like structure, zero-initialization, and a \u201cgradual\u201d update order, among which zero-initialization is proved to be the most influential. Building on this insight, we heuristically leverage Gaussian distributions to initialize each condition modulation, termed adaLN-Gaussian, leading to more stable and effective training. Extensive experiments following DiT on ImageNet1K demonstrate the effectiveness and generalization of adaLN-Gaussian, e.g., a notable improvement of 2.16% in FID score over adaLN-Zero."
    },
    {
        "title": "Compromised Turing Machines: Adversarial Interference and Endogenous Verification",
        "link_suffix": "/forum?id=YuwxDSqNXw",
        "link": "https://openreview.net/forum?id=YuwxDSqNXw",
        "pdf_link": "https://openreview.net/pdf?id=YuwxDSqNXw",
        "keywords": "compromised turing machine, ctm, adversarial interference, endogenous verification",
        "abstract": "We introduce the concept of a Compromised Turing Machine (CTM), an extension of the classical Turing machine model where an adversary, Eve, can tamper with the tape or internal state between timesteps. The CTM exposes fundamental vulnerabilities in the machine's ability to self-verify its computations, particularly in adversarial environments where endogenous verification mechanisms cannot reliably ensure computational integrity. Through a novel parallel with Descartes' deus deceptor thought experiment, we explore the epistemological limits of computational certainty, illustrating how the CTM reveals the failure of self-verification in adversarial contexts.To address these vulnerabilities, we propose several secure computational models, including hybrid systems with external verification, randomized and probabilistic verification protocols, distributed computing models with cross-verification, self-correcting and self-healing mechanisms, and advanced cryptographic techniques such as zero-knowledge proofs and homomorphic encryption. While each solution presents trade-offs in terms of computational overhead and complexity, they provide a foundation for building resilient systems capable of withstanding adversarial interference. Our work highlights the need for external sources of trust and verification in secure computation and opens new directions for research into adversarial computational models."
    },
    {
        "title": "Mixture-of-Queries Transformer: Camouflaged Instance Segmentation via Queries Cooperation and Frequency Enhancement",
        "link_suffix": "/forum?id=9U8IwSewJy",
        "link": "https://openreview.net/forum?id=9U8IwSewJy",
        "pdf_link": "https://openreview.net/pdf?id=9U8IwSewJy",
        "keywords": "image segmentation, transformer",
        "abstract": "Due to the high similarity between camouflaged instances and the surroundings and the widespread camouflage-like scenarios, the recently proposed camouflaged instance segmentation (CIS) is a challenging and relevant task. Previous approaches achieve some progress on CIS, while many overlook camouflaged objects\u2019 color and contour nature and then decide on each candidate instinctively. In this paper, we contribute a Mixture-of-Queries Transformer (MoQT) in an end-toend manner for CIS which is based on two key designs (a Frequency Enhancement Feature Extractor and a Mixture-of-Queries Decoder). First, the Frequency Enhancement Feature Extractor is responsible for capturing the camouflaged clues in the frequency domain. To expose camouflaged instances, the extractor enhances the effectiveness of contour, eliminates the interference color, and obtains suitable features simultaneously. Second, a Mixture-of-Queries Decoder utilizes multiple experts of queries (several queries comprise an expert) for spotting camouflaged characteristics with cooperation. These experts collaborate to generate outputs, refined hierarchically to a fine-grained level for more accurate instance masks. Coupling these two components enables MoQT to use multiple experts to integrate effective clues of camouflaged objects in both spatial and frequency domains. Extensive experimental results demonstrate our MoQT outperforms 18 state-of-the-art CIS approaches by 2.69% on COD10K and 1.93% on NC4K in average precision."
    },
    {
        "title": "Simulation-Free Differential Dynamics through Neural Conservation Laws",
        "link_suffix": "/forum?id=jIOBhZO1ax",
        "link": "https://openreview.net/forum?id=jIOBhZO1ax",
        "pdf_link": "https://openreview.net/pdf?id=jIOBhZO1ax",
        "keywords": "Generative Modeling; Simulation-free Methods; Diffusion Models;",
        "abstract": "We present a novel simulation-free framework for training continuous-time diffusion processes over very general objective functions. \nExisting methods typically involve either prescribing the optimal diffusion process---which only works for heavily restricted problem formulations---or require expensive simulation to numerically obtain the time-dependent densities and sample from the diffusion process.\nIn contrast, we propose a coupled parameterization which jointly models a time-dependent density function, or probability path, and the dynamics of a diffusion process that generates this probability path.\nTo accomplish this, our approach directly bakes in the Fokker-Planck equation and density function requirements as hard constraints, by extending and greatly simplifying the construction of Neural Conservation Laws.\nThis enables simulation-free training for a large variety of problem formulations, from data-driven objectives as in generative modeling and dynamical optimal transport, to optimality-based objectives as in stochastic optimal control, with straightforward extensions to mean-field objectives due to the ease of accessing exact density functions. We validate our method in a diverse range of application domains from modeling spatio-temporal events, to learning optimal dynamics from population data."
    },
    {
        "title": "Learning to Select Nodes in Branch and Bound with Sufficient Tree Representation",
        "link_suffix": "/forum?id=gyvYKLEm8t",
        "link": "https://openreview.net/forum?id=gyvYKLEm8t",
        "pdf_link": "https://openreview.net/pdf?id=gyvYKLEm8t",
        "keywords": "branch and bound; mixed integer linear programming",
        "abstract": "Branch-and-bound methods are pivotal in solving Mixed Integer Linear Programming (MILP), where the challenge of node selection arises, necessitating the prioritization of different regions of the space for subsequent exploration. While machine learning techniques have been proposed to address this, two crucial problems concerning \\textbf{(P1)} how to sufficiently extract features from the branch-and-bound tree, and \\textbf{(P2)} how to assess the node quality comprehensively based on the features remain open. To tackle these challenges, we propose to tackle the node selection problem employing a novel Tripartite graph representation and Reinforcement learning with a Graph Neural Network model (TRGNN). The tripartite graph is theoretically proved to encompass sufficient information for tree representation in information theory. We learn node selection via reinforcement learning for learning delay rewards and give more comprehensive node metrics. Experiments show that TRGNN significantly improves the efficiency of solving MILPs compared to human-designed and learning-based baselines on both synthetic and large-scale real-world MILPs. Moreover, experiments demonstrate that TRGNN well generalizes to MILPs that are significantly larger than those seen during training."
    },
    {
        "title": "Large Language Model-driven Large Neighborhood Search for Large-Scale MILP Problems",
        "link_suffix": "/forum?id=Usk4KzBxLW",
        "link": "https://openreview.net/forum?id=Usk4KzBxLW",
        "pdf_link": "https://openreview.net/pdf?id=Usk4KzBxLW",
        "keywords": "Mixed Integer Linear Programming, Large Neighborhood Search, Large Language Model",
        "abstract": "Large Neighborhood Search (LNS) is a widely used method for solving large-scale Mixed Integer Linear Programming (MILP) problems. The effectiveness of LNS crucially depends on the choice of the search neighborhood. However, existing strategies either rely on expert knowledge or computationally expensive Machine Learning (ML) approaches, both of which struggle to scale effectively for large problems. To address this, we propose LLM-LNS, a novel Large Language Model (LLM)-driven LNS framework for large-scale MILP problems. Our approach introduces a dual-layer self-evolutionary LLM agent to automate neighborhood selection, discovering effective strategies with scant small-scale training data that generalize well to large-scale MILPs. The inner layer evolves heuristic strategies to ensure convergence, while the outer layer evolves evolutionary prompt strategies to maintain diversity.  Experimental results demonstrate that the proposed dual-layer agent outperforms state-of-the-art agents such as FunSearch and EOH. Furthermore, the full LLM-LNS framework surpasses manually designed LNS algorithms like ACP, ML-based LNS methods like CL-LNS, and large-scale solvers such as Gurobi and SCIP. It also achieves superior performance compared to advanced ML-based MILP optimization frameworks like GNN&GBDT and Light-MILPopt, further validating the effectiveness of our approach."
    },
    {
        "title": "Leveraging Imitation Learning and LLMs for Efficient Hierarchical Reinforcement Learning",
        "link_suffix": "/forum?id=6y00rooi7i",
        "link": "https://openreview.net/forum?id=6y00rooi7i",
        "pdf_link": "https://openreview.net/pdf?id=6y00rooi7i",
        "keywords": "LLM, HRL",
        "abstract": "In this paper, we introduce an innovative framework that combines Hierarchical Reinforcement Learning (HRL) with Large Language Models (LLMs) to tackle the challenges of complex, sparse-reward environments. A key contribution of our approach is the emphasis on imitation learning during the early training stages, where the LLM plays a crucial role in guiding the agent by providing high-level decision-making strategies. This early-stage imitation learning significantly accelerates the agent's understanding of task structure, reducing the time needed to adapt to new environments. By leveraging the LLM\u2019s ability to generate abstract representations of the environment, the agent can efficiently explore potential strategies, even in tasks with high-dimensional state spaces and delayed rewards. Our method introduces a dynamic annealing strategy in action sampling, balancing the agent's reliance on the LLM\u2019s guidance with its own learned policy as training progresses. Additionally, we implement a novel value function which incorporates the LLM\u2019s predictions to guide decision-making while optimizing token efficiency. This approach reduces computational costs and enhances the agent\u2019s learning process. Experimental results across three environments\u2014MiniGrid, NetHack, and Crafter\u2014demonstrate that our method significantly outperforms baseline HRL algorithms in terms of training speed and success rates. The imitation learning phase proves critical in enabling the agent to adapt quickly and perform efficiently, highlighting the potential of integrating LLMs into HRL for complex tasks."
    },
    {
        "title": "Weakly Supervised Video Scene Graph Generation via Natural Language Supervision",
        "link_suffix": "/forum?id=GQgPj1H4pO",
        "link": "https://openreview.net/forum?id=GQgPj1H4pO",
        "pdf_link": "https://openreview.net/pdf?id=GQgPj1H4pO",
        "keywords": "Video Scene Understanding, Weakly Supervised Learning, Large Language Model",
        "abstract": "Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time-related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a weakly supervised VidSGG with Natural Language Supervision (VSNLS) framework that only utilizes the readily available video captions for training a VidSGG model. VSNLS consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by VSNLS is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality."
    },
    {
        "title": "Kinematics-Informed Reinforcement Learning for Trajectory Optimization in CNC Machining",
        "link_suffix": "/forum?id=58KF6ne6d4",
        "link": "https://openreview.net/forum?id=58KF6ne6d4",
        "pdf_link": "https://openreview.net/pdf?id=58KF6ne6d4",
        "keywords": "Trajectory Optimization, Reinforcement Learning, CNC Machining",
        "abstract": "Toolpath smoothing and feedrate planning are key techniques in Computer Numerical Control (CNC) machining, and play a significant role in machining accuracy, efficiency, and tool life.\nTraditional methods typically decouple path smoothing from feedrate planning, without considering the kinematic constraints during the smoothing process.\nAs a result, the subsequent feedrate planning process is subject to more stringent kinematic limitations, which hinders the achievement of optimal speed execution.\nHowever, the integration of these two processes presents a significant challenge due to severe complexity and nonlinearity of the problem. Here, we propose a novel Reinforcement Learning (RL) based method, termed KIRL, to address the integrated optimization problem.\nExperimental results demonstrate that KIRL can generate smoother trajectories and optimize machining time compared to traditional decoupled methods.\nTo our best knowledge, KIRL is the first RL-based method for solving the integrated toolpath smoothing and feedrate planning optimization problem in CNC machining."
    },
    {
        "title": "GameArena: Evaluating LLM Reasoning through Live Computer Games",
        "link_suffix": "/forum?id=SeQ8l8xo1r",
        "link": "https://openreview.net/forum?id=SeQ8l8xo1r",
        "pdf_link": "https://openreview.net/pdf?id=SeQ8l8xo1r",
        "keywords": "Large Language Models, LLM evaluation",
        "abstract": "Evaluating the reasoning abilities of large language models (LLMs) is challenging. Existing benchmarks often depend on static datasets, which are vulnerable to data contamination and may get saturated over time, or on binary live human feedback that conflates reasoning with other abilities. As the most prominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in real-world settings, but lacks the granularity in assessing specific reasoning capabilities. We introduce GameArena, a dynamic benchmark designed to evaluate LLM reasoning capabilities through interactive gameplay with humans. GameArena consists of three games designed to test specific reasoning capabilities  (e.g., deductive and inductive reasoning), while keeping participants entertained and engaged. We analyze the gaming data retrospectively to uncover the underlying reasoning processes of LLMs and measure their fine-grained reasoning capabilities. We collect over 2000 game sessions and provide detailed assessments of various reasoning capabilities for five state-of-the-art LLMs. Our user study with 100 participants suggests that GameArena improves user engagement compared to Chatbot Arena. For the first time, GameArena enables the collection of step-by-step LLM reasoning data in the wild."
    },
    {
        "title": "MaxSup: Fixing Label Smoothing for Improved Feature Representation",
        "link_suffix": "/forum?id=zVtwIWyX4S",
        "link": "https://openreview.net/forum?id=zVtwIWyX4S",
        "pdf_link": "https://openreview.net/pdf?id=zVtwIWyX4S",
        "keywords": "Label Smoothing, Regularization, Representation Learning, Explainability",
        "abstract": "Label Smoothing aims to prevent Neural Networks from making over-confident predictions and improve generalization.\nDue to its effectiveness, it has become an indispensable ingredient of the training recipe for tasks such as Image Recognition and Neural Machine Translation. Despite that, previous work shows it encourages an overly tight cluster in the feature space, which `erases' the similarity information of individual examples, resulting in impaired representation learning. By isolating the loss induced by Label Smoothing into a combination of a regularization term and an error-enhancement term, we reveal a previously unknown defect, i.e., it indeed encourages classifiers to be over-confident, when they make incorrect predictions. To remedy this, we present a solution called Max Suppression (MaxSup), which consistently applies the intended regularization effect during training, independent of the correctness of prediction. By visualizing the learned features, we show that MaxSup successfully enlarges intra-class variations, while improving the inter-class separability. We further conduct experiments on Image Classification and Machine Translation tasks, validating the superiority of Max Suppression. The code implementation is available atanonymous repository."
    },
    {
        "title": "Locate-then-Unlearn: An Effective Method of Multi-Task Continuous Learning for Large Language Models",
        "link_suffix": "/forum?id=cJ9qoVZbPd",
        "link": "https://openreview.net/forum?id=cJ9qoVZbPd",
        "pdf_link": "https://openreview.net/pdf?id=cJ9qoVZbPd",
        "keywords": "Machine unlearning, Continue learning, Model editing",
        "abstract": "Nowadays large language models (LLMs) have achieved remarkable success in various NLP tasks. However, they often misinterpret human instructions and generate incorrect or outdated responses, highlighting the need for more effective continual learning techniques. While recent efforts have introduced unlearning methods to remove erroneous knowledge, existing approaches still struggle in multi-task learning scenarios.To overcome these limitations, we propose \\textbf{locate-then-unlearn}, a novel framework that identifies and selectively unlearns task-specific neurons to enable efficient multi-task learning. We hypothesize that LLM neurons can be broadly categorized into task-specific neurons for handling individual tasks, and general neurons to maintain the model\u2019s foundational capabilities. To accurately identify task-specific neurons, we utilize a two-stage locating process: (1) ranking task-related neurons based on their importance to each task, and (2) identifying task-specific neurons by applying intervention to assess how neuron activity impacts task performance, isolating those most critical to each task. We conduct comprehensive evaluations in two experimental setups: single-task specialization and multi-task generalization. The results show that our method significantly improves performance across both settings. This indicates that our method effectively balances model efficiency and accuracy in multi-task continual learning."
    },
    {
        "title": "LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language Models and Doc-Level Embedding",
        "link_suffix": "/forum?id=7yncrX80CN",
        "link": "https://openreview.net/forum?id=7yncrX80CN",
        "pdf_link": "https://openreview.net/pdf?id=7yncrX80CN",
        "keywords": "information retrieval, text retrieval, artificial intelligence, large language models, data augmentation",
        "abstract": "Recent advancements in embedding-based retrieval, also known as dense retrieval, have shown state of the art results and demonstrated superior performance over traditional sparse or bag-of-words-based methodologies. This paper presents a model-agnostic document-level embedding framework enhanced by large language model (LLM) augmentation. The implementation of this LLM-augmented retrieval framework has significantly enhanced the efficacy of prevalent retriever models, including Bi-encoders (Contriever, DRAGON) and late-interaction models (ColBERTv2). Consequently, this approach has achieved state-of-the-art results on benchmark datasets such as LoTTE and BEIR, underscoring its potential to refine information retrieval processes."
    },
    {
        "title": "LLMCO2: Advancing Accurate Carbon Footprint Prediction for LLM Inferences",
        "link_suffix": "/forum?id=BmYzoPppij",
        "link": "https://openreview.net/forum?id=BmYzoPppij",
        "pdf_link": "https://openreview.net/pdf?id=BmYzoPppij",
        "keywords": "carbon footprint, LLM inferences, energy prediction",
        "abstract": "Throughout its lifecycle, a large language model (LLM) generates a substantially larger carbon footprint during inference than training. LLM inference requests vary in batch size, prompt length, and token generation number, while cloud providers employ different GPU types and quantities to meet diverse service-level objectives for accuracy and latency. It is crucial for both users and cloud providers to have a tool that quickly and accurately estimates the carbon impact of LLM inferences based on a combination of inference request and hardware configurations before execution. Estimating the carbon footprint of LLM inferences is more complex than training due to lower and highly variable model FLOPS utilization, rendering previous equation-based models inaccurate. Additionally, existing machine learning (ML) prediction methods either lack accuracy or demand extensive training data, as they inadequately handle the distinct prefill and decode phases, overlook hardware-specific features, and inefficiently sample uncommon inference configurations. We introduce LLMCO2, a graph neural network (GNN)-based model that greatly improves the accuracy of LLM inference carbon footprint predictions compared to previous methods."
    },
    {
        "title": "RGM: Reconstructing High-fidelity 3D Car Assets with Relightable 3D-GS Generative Model from a Single Image",
        "link_suffix": "/forum?id=sClhxLqfnP",
        "link": "https://openreview.net/forum?id=sClhxLqfnP",
        "pdf_link": "https://openreview.net/pdf?id=sClhxLqfnP",
        "keywords": "Generative Model, 3D Reconstruction, 3D Gaussian Splatting",
        "abstract": "The generation of high-quality 3D car assets is essential for various applications, including video games, autonomous driving, and virtual reality. Current 3D generation methods utilizing NeRF or 3D-GS as representations for 3D objects, generate a Lambertian object under fixed lighting and lack separated modelings for material and global illumination. This results in generated assets unsuitable for relighting under different illuminations and limiting their application to downstream tasks. To address this challenge, we propose a novel relightable 3D object generative framework that automates the creation of 3D car assets, enabling the swift and accurate reconstruction of a vehicle's geometry, texture, and material properties from a single input image. Our approach begins with introducing a large-scale synthetic car dataset comprising over 1,000 high-precision 3D vehicle models. We represent 3D objects using global illumination and relightable 3D Gaussian primitives integrating with BRDF parameters. Building on this representation, we introduce a feed-forward model that takes images as input and outputs both relightable 3D Gaussians and global illumination parameters. Experimental results demonstrate that our method produces photorealistic 3D car assets that can be seamlessly integrated into road scenes with different illuminations, which offers substantial practical benefits for industrial applications."
    },
    {
        "title": "Question-Aware Knowledge Graph Prompting for Large Language Models",
        "link_suffix": "/forum?id=ds3Tcnrte8",
        "link": "https://openreview.net/forum?id=ds3Tcnrte8",
        "pdf_link": "https://openreview.net/pdf?id=ds3Tcnrte8",
        "keywords": "Knowledge Graph, Question Answering, Large Language Model, Prompt",
        "abstract": "Large Language Models (LLMs) have demonstrated significant advancements in various natural language processing tasks, yet they often struggle with tasks that require external domain-specific knowledge, such as Multiple Choice Question Answering (MCQA). Integrating Knowledge Graphs (KGs) with LLMs has been explored as a solution to enhance LLMs' reasoning capabilities, while existing methods either involve computationally expensive finetuning processes or rely on the noisy retrieval of KG information. Recent efforts have focused on leveraging Graph Neural Networks (GNNs) to generate KG-based soft prompts for LLMs, which face challenges of lacking question-relevance assessment in GNN and utilization of relations among options. In this paper, we propose a novel approach, QAP, to address these challenges by optimizing the utilization of KG in MCQA tasks. Our method introduces question embeddings into the GNN aggregation process, enabling the model to assess the relevance of KG information based on the question context. Additionally, QAP facilitates inter-option interactions by employing an attention module that explicitly models relationships between answer options. Specifically, we use multiple attention heads for the GNN output, allowing the model to capture and compare features across different options, thereby enhancing cross-option reasoning. Our approach not only enhances the connection between GNNs and LLMs but also enables the model to better utilize the relationships between answer options. Experimental results demonstrate that QAP outperforms state-of-the-art models on multiple public MCQA datasets, validating its effectiveness and scalability."
    },
    {
        "title": "Recovering Manifold Structure Using Ollivier Ricci Curvature",
        "link_suffix": "/forum?id=aX7X9z3vQS",
        "link": "https://openreview.net/forum?id=aX7X9z3vQS",
        "pdf_link": "https://openreview.net/pdf?id=aX7X9z3vQS",
        "keywords": "Manifold Learning, Persistent Homology, Ollivier-Ricci Curvature, Pruning, Nearest-Neighbor Graphs",
        "abstract": "We introduce ORC-ManL, a new algorithm to prune spurious edges from nearest neighbor graphs using a criterion based on Ollivier-Ricci curvature and estimated metric distortion. Our motivation comes from manifold learning: we show that when the data generating the nearest-neighbor graph consists of noisy samples from a low-dimensional manifold, edges that shortcut through the ambient space have more negative Ollivier-Ricci curvature than edges that lie along the data manifold. We demonstrate that our method outperforms alternative pruning methods and that it significantly improves performance on many downstream geometric data analysis tasks that use nearest neighbor graphs as input. Specifically, we evaluate on manifold learning, persistent homology, dimension estimation, and others. We also show that ORC-ManL can be used to improve clustering and manifold learning of single-cell RNA sequencing data. Finally, we provide empirical convergence experiments that support our theoretical findings."
    },
    {
        "title": "T2V-Turbo-v2: Enhancing Video Model Post-Training through Data, Reward, and Conditional Guidance Design",
        "link_suffix": "/forum?id=BZwXMqu4zG",
        "link": "https://openreview.net/forum?id=BZwXMqu4zG",
        "pdf_link": "https://openreview.net/pdf?id=BZwXMqu4zG",
        "keywords": "text-to-video generation, diffusion model, consistency model",
        "abstract": "In this paper, we focus on enhancing a diffusion-based text-to-video (T2V) model during the post-training phase by distilling a highly capable consistency model from a pretrained T2V model. Our proposed method, T2V-Turbo-v2, introduces a significant advancement by integrating various supervision signals, including high-quality training data, reward model feedback, and conditional guidance, into the consistency distillation process. Through comprehensive ablation studies, we highlight the crucial importance of tailoring datasets to specific learning objectives and the effectiveness of learning from diverse reward models for enhancing both the visual quality and text-video alignment. Additionally, we highlight the vast design space of conditional guidance strategies, which centers on designing an effective energy function to augment the teacher ODE solver. We demonstrate the potential of this approach by extracting motion guidance from the training datasets and incorporating it into the ODE solver, showcasing its effectiveness in improving the motion quality of the generated videos with the improved motion-related metrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2 establishes a new state-of-the-art result on VBench,with a Total score of 85.13, surpassing proprietary systems such as Gen-3 and Kling."
    },
    {
        "title": "Diffusion Policy Policy Optimization",
        "link_suffix": "/forum?id=mEpqHvbD2h",
        "link": "https://openreview.net/forum?id=mEpqHvbD2h",
        "pdf_link": "https://openreview.net/pdf?id=mEpqHvbD2h",
        "keywords": "reinforcement learning, diffusion policy",
        "abstract": "We introduce Diffusion Policy Policy Optimization, DPPO, an algorithmic framework including best practices for fine-tuning diffusion-based policies (e.g. Diffusion Policy) in continuous control and robot learning tasks using the policy gradient (PG) method from reinforcement learning (RL). PG methods are ubiquitous in training RL policies with other policy parameterizations; nevertheless, they had been conjectured to be less efficient for diffusion-based policies. Surprisingly, we show that DPPO achieves the strongest overall performance and efficiency for fine-tuning in common benchmarks compared to other RL methods for diffusion-based policies and also compared to PG fine-tuning of other policy parameterizations. Through experimental investigation, we find that DPPO takes advantage of unique synergies between RL fine-tuning and the diffusion parameterization, leading to structured and on-manifold exploration, stable training, and strong policy robustness. We further demonstrate the strengths of DPPO in a range of realistic settings, including simulated robotic tasks with pixel observations, and via zero-shot deployment of simulation-trained policies on robot hardware in a long-horizon, multi-stage manipulation task."
    },
    {
        "title": "POLYATOMIC COMPLEXES: A TOPOLOGICALLY INFORMED LEARNING REPRESENTATION FOR ATOMISTIC SYSTEMS",
        "link_suffix": "/forum?id=ZLtWAhkWJm",
        "link": "https://openreview.net/forum?id=ZLtWAhkWJm",
        "pdf_link": "https://openreview.net/pdf?id=ZLtWAhkWJm",
        "keywords": "representation learning, gaussian processes, cheminformatics, molecular representations",
        "abstract": "Developing robust physics-informed representations of chemical structures that enable models to learn topological inductive biases is challenging. In this manuscript, we present a representation of atomistic systems. We begin by proving that our representation satisfies all structural, geometric, efficiency, and generalizability constraints. Afterward, we provide a general algorithm to encode any atomistic system. Finally, we report performance comparable to state-of-the-art methods on numerous tasks. We open-source all code and datasets. The anonymized code and data are available in the supplementary material."
    },
    {
        "title": "Invariance to Planning in Goal-Conditioned RL",
        "link_suffix": "/forum?id=BH8Nrt2dPf",
        "link": "https://openreview.net/forum?id=BH8Nrt2dPf",
        "pdf_link": "https://openreview.net/pdf?id=BH8Nrt2dPf",
        "keywords": "reinforcement learning, generalization, invariance, planning",
        "abstract": "We study goal-conditioned RL through the lens of generalization, but not in the traditional sense of random augmentations and domain randomization. Rather, we aim to learn goal-directed policies that generalize with respect to the horizon: after training to reach nearby goals (which are easy to learn), these policies should succeed in reaching distant goals (which are quite challenging to learn). In the same way that invariance is closely linked with generalization is other areas of machine learning (e.g., normalization layers make a network invariant to scale, and therefore generalize to inputs of varying scales), we show that this notion of horizon generalization is closely linked with invariance to planning: a policy navigating towards a goal will select the same actions as if it were navigating to a waypoint en route to that goal. Horizon generalization and invariance to planning are appealing because of their potential reach: they imply that a policy trained to reach nearby goals would succeed at reaching goals that are arbitrarily more distant.Our theoretical analysis proves that both horizon generalization and planning invariance are possible, under some assumptions. We present new experimental results, as well as recalling results from prior work, in support of our theoretical results. Taken together, our results open the door to studying how techniques for invariance and generalization developed in other areas of machine learning might be adapted to achieve this alluring property."
    },
    {
        "title": "OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision",
        "link_suffix": "/forum?id=Hlm0cga0sv",
        "link": "https://openreview.net/forum?id=Hlm0cga0sv",
        "pdf_link": "https://openreview.net/pdf?id=Hlm0cga0sv",
        "keywords": "Image Editing, Diffusion Models",
        "abstract": "Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases.\nIn this paper, we present OmniEdit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) OmniEdit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that OmniEdit can significantly outperforms all the existing models."
    },
    {
        "title": "TWO STAGES DOMAIN INVARIANT REPRESENTATION LEARNERS SOLVE THE LARGE CO-VARIATE SHIFT IN UNSUPERVISED DOMAIN ADAPTATION WITH TWO DIMENSIONAL DATA DOMAINS",
        "link_suffix": "/forum?id=x8jxf3byli",
        "link": "https://openreview.net/forum?id=x8jxf3byli",
        "pdf_link": "https://openreview.net/pdf?id=x8jxf3byli",
        "keywords": "domain invariant representation learning, unsupervised domain adaptation, image recognition, signal processing, classification",
        "abstract": "Recent developments in the unsupervised domain adaptation (UDA) enable the unsupervised machine learning (ML) prediction for target data, thus this will accelerate real world applications with ML models such as image recognition tasks in self-driving. Researchers have reported the UDA techniques are not working well under large co-variate shift problems where e.g. supervised source data consists of handwritten digits data in monotone color and unsupervised target data colored digits data from the street view. Thus there is a need for a method to resolve co-variate shift and transfer source labelling rules under this dynamics. We perform two stages domain invariant representation learning to bridge the gap between source and target with semantic intermediate data (unsupervised). The proposed method can learn domain invariant features simultaneously between source and intermediate also intermediate and target. Finally this achieves good domain invariant representation between source and target plus task discriminability owing to source labels. This induction for the gradient descent search greatly eases learning convergence in terms of classification performance for target data even when large co-variate shift. We also derive a theorem for measuring the gap between trained models and unsupervised target labelling rules, which is necessary for the free parameters optimization. Finally we demonstrate that proposing method is superiority to previous UDA methods using 4 representative ML classification datasets including 38 UDA tasks. Our experiment will be a basis for challenging UDA problems with large co-variate shift."
    },
    {
        "title": "CLoRA: A Contrastive Approach to Compose Multiple LoRA Models",
        "link_suffix": "/forum?id=Mzz9i4Zf8B",
        "link": "https://openreview.net/forum?id=Mzz9i4Zf8B",
        "pdf_link": "https://openreview.net/pdf?id=Mzz9i4Zf8B",
        "keywords": "Text to Image Generation, Personalization, LoRAs, Contrastive Learning, Generative Models, Low Rank Adaptation",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a powerful and popular technique for personalization, enabling efficient adaptation of pre-trained image generation models for specific tasks without comprehensive retraining. While employing individual pre-trained LoRA models excels at representing single concepts, such as those representing a specific dog or a cat, utilizing multiple LoRA models to capture a variety of concepts in a single image still poses a significant challenge. Existing methods often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). We introduce CloRA, a training-free approach that addresses these limitations by updating the attention maps of multiple LoRA models at test-time, and leveraging the attention maps to create semantic masks for fusing latent representations. This enables the generation of composite images that accurately reflect the characteristics of each LoRA.  Our comprehensive qualitative and quantitative evaluations demonstrate that CloRA significantly outperforms existing methods in multi-concept image generation using LoRAs. Furthermore, we share our source code and benchmark dataset to promote further research."
    }
]