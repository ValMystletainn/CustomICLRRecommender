[
    {
        "title": "Physics-Assisted and Topology-Informed Deep Learning for Weather Prediction",
        "link_suffix": "/forum?id=o6tO1rUcQe",
        "link": "https://openreview.net/forum?id=o6tO1rUcQe",
        "pdf_link": "https://openreview.net/pdf?id=o6tO1rUcQe",
        "keywords": "global weather prediction, physics-assisted deep learning, topology-informed deep learning",
        "abstract": "Weather prediction is crucial for decision-making in various social and economic sectors. The classical numerical weather prediction methods cannot incorporate the historical observations to enhance the underlying physical models, whereas the existing data-driven, deep learning-based weather prediction methods disregard either the $\\textbf{physics}$ of the weather evolution or the $\\textbf{topology}$ of the Earth's surface. In light of these disadvantages, we develop PASSAT, a novel Physics-ASSisted And Topology-informed deep learning model for weather prediction. PASSAT attributes the weather evolution to two key factors: (i) the advection process that can be characterized by the advection equation and the Navier-Stokes equation; (ii) the Earth-atmosphere interaction that is difficult to both model and calculate. PASSAT also takes the topology of the Earth's surface into consideration, other than simply treating it as a plane. Therefore, PASSAT numerically solves the advection equation and the Navier-Stokes equation on the spherical manifold, utilizes a spherical graph neural network to capture the Earth-atmosphere interaction, and generates the initial velocity fields that are critical to solving the advection equation from the same spherical graph neural network. These building blocks constitute a deep learning-based, $\\textbf{physics-assisted}$ and $\\textbf{topology-informed}$ weather prediction model. In the $5.625^\\circ$-resolution ERA5 data set, PASSAT outperforms both the state-of-the-art deep learning-based weather prediction models and the operational numerical weather prediction model IFS T42."
    },
    {
        "title": "Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos",
        "link_suffix": "/forum?id=s6q6zX45F8",
        "link": "https://openreview.net/forum?id=s6q6zX45F8",
        "pdf_link": "https://openreview.net/pdf?id=s6q6zX45F8",
        "keywords": "Federated learning, uncertainty, discretization",
        "abstract": "Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance accuracy and reduce uncertainty by explicitly addressing the diversity of data distributions, all while maintaining minimal computational overhead in environments characterized by heterogeneous data silos. Through experiments conducted on six datasets, our method has demonstrated its superiority, achieving significant improvements in accuracy (by 3%--22.1%) and uncertainty reduction (by 38.83%--96.24%), thereby outperforming contemporary state-of-the-art methods."
    },
    {
        "title": "Surprisingly Simple: Large Language Models are Zero-Shot Feature Extractors for Tabular and Text Data",
        "link_suffix": "/forum?id=XWPp9FJ0uJ",
        "link": "https://openreview.net/forum?id=XWPp9FJ0uJ",
        "pdf_link": "https://openreview.net/pdf?id=XWPp9FJ0uJ",
        "keywords": "Large Language Model, Tabular Data Prediction, Multimodal Learning",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse tasks, yet their application to tabular data prediction remains relatively underexplored. This is partly due to the fact that recent LLMs are autoregressive models, generating text outputs. Converting tabular data into text, and vice versa, is not straightforward, making direct application of LLMs to complex tabular prediction difficult. Although previous works have utilized pre-trained embedding models like BERT and its variants for fine-tuning on tabular tasks, the potential of autoregressive LLMs for tabular prediction has been explored only on a limited scale and with simpler datasets.\nIn this paper, we propose Zero-shot Encoding for Tabular data with LLMs (ZET-LLM), a surprisingly simple yet effective approach that leverages pre-trained LLMs as zero-shot feature extractors for tabular prediction tasks. To adapt autoregressive LLMs for this purpose, we replace autoregressive masking with bidirectional attention to treat them as feature embedding models. To address the challenge of encoding high-dimensional complex tabular data with LLMs' limited token lengths, we introduce a feature-wise serialization, where each feature is represented as a single token, and the resulting tokens are combined into a unified sample representation. Additionally, we apply missing value masking to handle missing data, a common issue in complex tabular datasets.\nWe demonstrate that LLMs can serve as powerful zero-shot feature extractors without the need for fine-tuning, extensive data pre-processing, or task-specific instructions. Our method enables LLMs to process both structured tabular data and unstructured text data simultaneously, offering a unique advantage over traditional models. Extensive experiments on complex tabular datasets show that our approach outperforms state-of-the-art methods across binary classification, multi-class classification, and regression tasks."
    },
    {
        "title": "SimXRD-4M: Big Simulated X-ray Diffraction Data and Crystalline Symmetry Classification Benchmark",
        "link_suffix": "/forum?id=mkuB677eMM",
        "link": "https://openreview.net/forum?id=mkuB677eMM",
        "pdf_link": "https://openreview.net/pdf?id=mkuB677eMM",
        "keywords": "Symmetry Classification, benchmark and dataset",
        "abstract": "Powder X-ray diffraction (XRD) patterns are highly effective for crystal identification and play a pivotal role in materials discovery. While machine learning (ML) has advanced the analysis of powder XRD patterns, progress has been constrained by the limited availability of training data and established benchmarks. To address this, we introduce SimXRD, the largest open-source simulated XRD pattern dataset to date, aimed at accelerating the development of crystallographic informatics. We developed a novel XRD simulation method that incorporates comprehensive physical interactions, resulting in a high-fidelity database. SimXRD comprises 4,065,346 simulated powder XRD patterns, representing 119,569 unique crystal structures under 33 simulated conditions that reflect real-world variations. We benchmark 21 sequence models in both in-library and out-of-library scenarios and analyze the impact of class imbalance in long-tailed crystal label distributions. Remarkably, we find that: (1) current neural networks struggle with classifying low-frequency crystals, particularly in out-of-library situations; (2) models trained on SimXRD can generalize to real experimental data."
    },
    {
        "title": "DYSTIL: Dynamic Strategy Induction with Large Language Models for Reinforcement Learning",
        "link_suffix": "/forum?id=1ABhAZCoGr",
        "link": "https://openreview.net/forum?id=1ABhAZCoGr",
        "pdf_link": "https://openreview.net/pdf?id=1ABhAZCoGr",
        "keywords": "Neurosymbolic Systems, Reinforcement Learning, Large Language Models, Strategy",
        "abstract": "Reinforcement learning from expert demonstrations has long remained a challenging research problem, and existing methods resorting to behavioral cloning plus further RL training often suffer from poor generalization, low sample efficiency, and poor model interpretability. Inspired by the strong reasoning abilities of large language models (LLMs), we propose a novel strategy-based neuro-symbolic reinforcement learning framework integrated with LLMs called DYnamic STrategy Induction with Llms for reinforcement learning (DYSTIL) to overcome these limitations. DYSTIL dynamically queries a strategy-generating LLM to induce textual strategies based on advantage estimations and expert demonstrations, and gradually internalizes induced strategies into the RL agent through policy optimization to improve its performance through boosting policy generalization and enhancing sample efficiency. It also provides a direct textual channel to observe and interpret the evolution of the policy's underlying strategies during training. We test DYSTIL over challenging RL environments from Minigrid and BabyAI, and empirically demonstrate that DYSTIL significantly outperforms state-of-the-art baseline methods by 17.75% success rate on average while also enjoying higher sample efficiency during the learning process."
    },
    {
        "title": "MathHay: An Automated Benchmark for Long-Context Mathematical Reasoning in LLMs",
        "link_suffix": "/forum?id=QO4bF6MHza",
        "link": "https://openreview.net/forum?id=QO4bF6MHza",
        "pdf_link": "https://openreview.net/pdf?id=QO4bF6MHza",
        "keywords": "Mathematical Reasoning, Large Language Models, Benchmark, Long-Context Modeling",
        "abstract": "Recent large language models (LLMs) have demonstrated versatile capabilities in long-context scenarios. Although some recent benchmarks have been developed to evaluate the long-context capabilities of LLMs, there is a lack of benchmarks evaluating the mathematical reasoning abilities of LLMs over long contexts, which is crucial for LLMs' application in real-world scenarios. In this paper, we introduce MathHay, an automated benchmark designed to assess the long-context mathematical reasoning capabilities of LLMs. Unlike previous benchmarks like Needle in a Haystack, which focus primarily on information retrieval within long texts, MathHay demands models with both information-seeking and complex mathematical reasoning abilities. We conduct extensive experiments on MathHay to assess the long-context mathematical reasoning abilities of eight top-performing LLMs. Even the best-performing model, Gemini-1.5-Pro-002, still struggles with mathematical reasoning over long contexts, achieving only 51.26% accuracy at 128K tokens. This highlights the significant room for improvement on the MathHay benchmark."
    },
    {
        "title": "Automatic Truncation Position Selection in Singular Value Decomposition for Large Language Models",
        "link_suffix": "/forum?id=3KEwJGYNzH",
        "link": "https://openreview.net/forum?id=3KEwJGYNzH",
        "pdf_link": "https://openreview.net/pdf?id=3KEwJGYNzH",
        "keywords": "Model decomposition; Large Language Model; Optimization",
        "abstract": "Model decomposition in large language models has drawn much attention due to its superiority and good interpretability, where activation-aware singular value decomposition (SVD) can achieve competitive performance by mitigating reconstruction errors brought by outliers in activation. However, the performance of the state-of-the-art SVD-based LLM compression method is limited to the selection of truncation positions. No work meticulously examines the details of this problem theoretically and empirically tests its correlation with model performance. To fill the research gap, we propose an efficient method that can automatically select truncation positions, namely AutoTrunc. In our work, we first analyze the correlation between truncation positions and the model performance. Then, the model layer importance is modeled based on the correlation, followed by mathematical proof to illustrate how to reach and obtain the optimal truncation position configuration for different layer types. Extensive experiments are carried out to verify our presumption and evaluate our proposed method. Our proposed AutoTrunc outperforms the state-of-the-art SVD-based LLM compression method, with perplexity scores dropping by 24.65% and 38.63% at the compression ratio of 50% in LLaMA-2-7B and LLaMA-2-13B, respectively. The code will be released upon acceptance."
    },
    {
        "title": "Human Simulacra: Benchmarking the Personification of Large Language Models",
        "link_suffix": "/forum?id=BCP5nAHXqs",
        "link": "https://openreview.net/forum?id=BCP5nAHXqs",
        "pdf_link": "https://openreview.net/pdf?id=BCP5nAHXqs",
        "keywords": "Large Language Models, Human simulation",
        "abstract": "Large Language Models (LLMs) are recognized as systems that closely mimic aspects of human intelligence. This capability has attracted the attention of the social science community, who see the potential in leveraging LLMs to replace human participants in experiments, thereby reducing research costs and complexity. In this paper, we introduce a benchmark for LLMs personification, including a strategy for constructing virtual characters' life stories from the ground up, a Multi-Agent Cognitive Mechanism capable of simulating human cognitive processes, and a psychology-guided evaluation method to assess human simulations from both self and observational perspectives. Experimental results demonstrate that our constructed simulacra can produce personified responses that align with their target characters.  We hope this work will serve as a benchmark in the field of human simulation, paving the way for future research."
    },
    {
        "title": "Towards Understanding the Feasibility of Machine Unlearning",
        "link_suffix": "/forum?id=Co9tdrslVG",
        "link": "https://openreview.net/forum?id=Co9tdrslVG",
        "pdf_link": "https://openreview.net/pdf?id=Co9tdrslVG",
        "keywords": "Machine Unlearning, Kernelized Stein Discrepancy",
        "abstract": "In response to recent privacy protection regulations, machine unlearning has attracted great interest in the research community. However, existing studies often demonstrate their approaches' effectiveness by measuring the overall unlearning success rate rather than evaluating the chance of unlearning specific training samples, leaving the universal feasibility of the unlearning operation unexplored. This paper proposes a novel method to quantify the difficulty of unlearning a single sample by taking into account factors such as model and data distribution. Specifically, we propose several heuristics to understand the condition of a successful unlearning operation on data points, explore difference in unlearning difficulty over training data points, and suggest a potential ranking mechanism for identifying the most challenging samples to unlearn.  In particular, we note Kernelized Stein Discrepancy (KSD), a parameterized kernel function tailored to each model and dataset, is an effective heuristic to tell the difficulty of unlearning a data sample. We demonstrate our discovery by including multiple classification tasks and existing machine unlearning algorithms, highlighting the practical feasibility of unlearning operations across different scenarios."
    },
    {
        "title": "Rethinking Table Instruction Tuning",
        "link_suffix": "/forum?id=GLmqHCwbOJ",
        "link": "https://openreview.net/forum?id=GLmqHCwbOJ",
        "pdf_link": "https://openreview.net/pdf?id=GLmqHCwbOJ",
        "keywords": "table instruction tuning, table understanding",
        "abstract": "Recent advances in table understanding have focused on instruction-tuning large language models (LLMs) for table-related tasks. \nHowever, existing research has overlooked the impact of hyperparameter choices and lacks a comprehensive evaluation of the out-of-domain table understanding ability and the general capabilities of these table LLMs. \nIn this paper, we evaluate these abilities in existing table LLMs, and reveal significant declines in both out-of-domain table understanding and general capabilities compared to their base models. \nThrough systematic analysis, we show that hyperparameters, such as learning rate, can significantly influence both table-specific and general capabilities.\nContrary to the existing table instruction-tuning works, we demonstrate that smaller learning rates and fewer training instances can enhance table understanding while preserving general capabilities.\nBased on our findings, we introduceTAMA, aTAble LLM instruction-tuned from LLaMA3.1 8B Instruct, which achieves performance on par with, or surpassing GPT-3.5 and GPT-4 on table tasks, while maintaining strong out-of-domain generalization and general capabilities. \nOur findings highlight the potential for reduced data annotation costs and more efficient model development through careful hyperparameter selection."
    },
    {
        "title": "Automatic Curriculum Expert Iteration for Reliable LLM Reasoning",
        "link_suffix": "/forum?id=3ogIALgghF",
        "link": "https://openreview.net/forum?id=3ogIALgghF",
        "pdf_link": "https://openreview.net/pdf?id=3ogIALgghF",
        "keywords": "Large Language Models, Reasoning, Hallucinations, Laziness, Alignment",
        "abstract": "Hallucinations (i.e., generating plausible but inaccurate content) and laziness (i.e. excessive refusals or defaulting to \"I don't know\") persist as major challenges in LLM reasoning. Current efforts to reduce hallucinations primarily focus on factual errors in knowledge-grounded tasks, often neglecting hallucinations related to faulty reasoning. Meanwhile, some approaches render LLMs overly conservative, limiting their problem-solving capabilities. To mitigate hallucination and laziness in reasoning tasks, we propose Automatic Curriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align responses to the model\u2019s capabilities--assertively answering within its limits and declining when tasks exceed them. In our method, Expert Iteration explores the reasoning trajectories near the LLM policy, guiding incorrect paths back on track to reduce compounding errors and improve robustness; it also promotes appropriate \"I don't know\" responses after sufficient reasoning attempts. The curriculum automatically adjusts rewards, incentivizing extended reasoning before acknowledging incapability, thereby pushing the limits of LLM reasoning and aligning its behaviour with these limits. We compare Auto-CEI with various SOTA baselines across logical reasoning, mathematics, and planning tasks, where Auto-CEI achieves superior alignment by effectively balancing assertiveness and conservativeness."
    },
    {
        "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
        "link_suffix": "/forum?id=FiyS0ecSm0",
        "link": "https://openreview.net/forum?id=FiyS0ecSm0",
        "pdf_link": "https://openreview.net/pdf?id=FiyS0ecSm0",
        "keywords": "Neuro-symbolic theorem proving, Olympiad inequalities, Large language model, Symbolic method",
        "abstract": "Large language models (LLMs) can prove mathematical theorems formally by generating proof steps (\\textit{a.k.a.} tactics) within a proof system. However, the space of possible tactics is vast and complex, while the available training data for formal proofs is limited, posing a significant challenge to LLM-based tactic generation. To address this, we introduce a neuro-symbolic tactic generator that synergizes the mathematical intuition learned by LLMs with domain-specific insights encoded by symbolic methods. The key aspect of this integration is identifying which parts of mathematical reasoning are best suited to LLMs and which to symbolic methods. While the high-level idea of neuro-symbolic integration is broadly applicable to various mathematical problems, in this paper, we focus specifically on Olympiad inequalities (Figure~\\ref{fig:example}). We analyze how humans solve these problems and distill the techniques into two types of tactics: (1) scaling, handled by symbolic methods, and (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with LLMs to prune and rank the proof goals for efficient proof search. We evaluate our framework on 161 challenging inequalities from multiple mathematics competitions, achieving state-of-the-art performance and significantly outperforming existing LLM and symbolic approaches without requiring additional training data."
    },
    {
        "title": "Searching For Robust Point Cloud Distillation",
        "link_suffix": "/forum?id=HwkELcW2ft",
        "link": "https://openreview.net/forum?id=HwkELcW2ft",
        "pdf_link": "https://openreview.net/pdf?id=HwkELcW2ft",
        "keywords": "Neural Architecture Search, Adversarial Attacks, Knowledge Distillation",
        "abstract": "Deep Neural Networks (DNNs) have shown remarkable performance in machine learning; however, their vulnerabilities to adversarial attacks have been exposed, particularly in point cloud data. Neural Architecture Search (NAS) is a technique for discovering new neural architectures with high predictive accuracy, yet its potential for enhancing model robustness against adversarial attacks remains largely unexplored. In this study, we investigate the application of NAS within the framework of knowledge distillation, aiming to generate robust student architectures that inherit resilience from robust teacher models. We introduce RDANAS, an effective NAS method that utilizes cross-layer knowledge distillation from robust teacher models to enhance the robustness of the student model. Unlike previous studies, RDANAS considers the teacher model's outputs and automatically identifies the optimal teacher layer for each student layer during supervision. Experimental results on ModelNet40, ScanObjectNN and ScanNet datasets demonstrate the efficacy of RDANAS, revealing that the neural architectures it generates are compact and possess adversarial robustness, which shows potential in multiple applications."
    },
    {
        "title": "Quantifying Prediction Consistency Under Model Multiplicity in Tabular LLMs",
        "link_suffix": "/forum?id=cP00UB2654",
        "link": "https://openreview.net/forum?id=cP00UB2654",
        "pdf_link": "https://openreview.net/pdf?id=cP00UB2654",
        "keywords": "trustworthy large language models, Reliable machine learning, tabular data, model multiplicity, high-stakes application",
        "abstract": "Fine-tuning large language models (LLMs) on tabular data for classification can lead to the phenomenon of \\emph{fine-tuning multiplicity}, where equally well-performing models make conflicting predictions on the same input. Fine-tuning multiplicity can arise due to variations in the training process, e.g., seed, random weight initialization, retraining on a few additional or deleted data points. This raises critical concerns about the robustness and reliability of Tabular LLMs, particularly when deployed for high-stakes decision-making, such as finance, hiring, education, healthcare, etc. This work formalizes the unique challenge of fine-tuning multiplicity in Tabular LLMs and proposes a novel measure to quantify the robustness of individual predictions without expensive model retraining. Our measure quantifies a prediction's robustness by analyzing (sampling) the model's local behavior around the input in the embedding space. Interestingly, we show that sampling in the local neighborhood can be leveraged to provide probabilistic robustness guarantees against a broad class of equally-well-performing fine-tuned models.  By leveraging Bernstein's Inequality, we show that predictions with sufficiently high robustness (as defined by our measure) will remain consistent with high probability. We also provide empirical evaluation on real-world datasets to support our theoretical results. Our work highlights the importance of addressing fine-tuning instabilities to enable trustworthy deployment of Tabular LLMs in high-stakes and safety-critical applications."
    },
    {
        "title": "DUALFormer: A Dual Graph Convolution and Attention Network for Node Classification",
        "link_suffix": "/forum?id=4v4RcAODj9",
        "link": "https://openreview.net/forum?id=4v4RcAODj9",
        "pdf_link": "https://openreview.net/pdf?id=4v4RcAODj9",
        "keywords": "Graph Transformers, Node Classification",
        "abstract": "Graph Transformers (GTs), adept at capturing the locality and globality of graphs, have shown promising potential in node classification tasks. Most state-of-the-art GTs succeed through integrating local Graph Neural Networks (GNNs) with their global Self-Attention (SA) modules to enhance structural awareness. Nonetheless, this architecture faces limitations arising from scalability challenges and the trade-off between capturing local and global information. On the one hand, the quadratic complexity associated with the SA modules poses a significant challenge for many GTs, particularly when scaling them to large-scale graphs. Numerous GTs necessitated a compromise, relinquishing certain aspects of their expressivity to garner computational efficiency. On the other hand, GTs face challenges in maintaining detailed local structural information while capturing long-range dependencies. As a result, they typically require significant computational costs to balance the local and global expressivity. To address these limitations, this paper introduces a novel GT architecture, dubbed DUALFormer, featuring a dual-dimensional design of its GNN and SA modules. Leveraging approximation theory from Linearized Transformers and treating the query as the surrogate representation of node features, DUALFormer \\emph{efficiently} performs the computationally intensive global SA module on feature dimensions. Furthermore, by such a separation of local and global modules into dual dimensions, DUALFormer achieves a natural balance between local and global expressivity. In theory, DUALFormer can reduce intra-class variance, thereby enhancing the discriminability of node representations. Extensive experiments on eleven real-world datasets demonstrate its effectiveness and efficiency over existing state-of-the-art GTs."
    },
    {
        "title": "Hierarchical Preference Optimization: Learning to achieve goals via feasible subgoals prediction",
        "link_suffix": "/forum?id=BsQTw0uPDX",
        "link": "https://openreview.net/forum?id=BsQTw0uPDX",
        "pdf_link": "https://openreview.net/pdf?id=BsQTw0uPDX",
        "keywords": "hierarchical reinforcement learning, preference learning",
        "abstract": "This work introduces Hierarchical Preference Optimization (HPO), a novel approach to hierarchical reinforcement learning (HRL) that addresses non-stationarity and infeasible subgoal generation issues when solving complex robotic control tasks. HPO leverages maximum entropy reinforcement learning combined with token-level Direct Preference Optimization (DPO), eliminating the need for pre-trained reference policies that are typically unavailable in challenging robotic scenarios. Mathematically, we formulate HRL as a bi-level optimization problem and transform it into a primitive-regularized DPO formulation, ensuring feasible subgoal generation and avoiding degenerate solutions. Extensive experiments on challenging robotic navigation and manipulation tasks demonstrate HPO\u2019s impressive performance, where HPO shows an improvement of up to 35% over the baselines. Furthermore, ablation studies validate our design choices, and quantitative analyses confirm HPO\u2019s ability to mitigate non-stationarity and infeasible subgoal generation issues in HRL."
    },
    {
        "title": "Hessian-Free Natural Gradient Descent for Physics Informed Machine Learning",
        "link_suffix": "/forum?id=Oqk1Ui6m0n",
        "link": "https://openreview.net/forum?id=Oqk1Ui6m0n",
        "pdf_link": "https://openreview.net/pdf?id=Oqk1Ui6m0n",
        "keywords": "PINNs, Gauss-Newton, Function-space optimization, Hessian-Free Optimization, Second-order Optimizers",
        "abstract": "Physics-Informed Machine Learning (PIML) methods, such as Physics-Informed Neural Networks (PINNs), are notoriously difficult to optimize. Recent advances utilizing second-order optimization techniques, including natural gradient and Gauss-Newton methods, have significantly improved training accuracy over first-order methods. However, these approaches are computationally prohibitive, as they require evaluating, storing, and inverting large curvature matrices, limiting scalability to small networks. To overcome this limitation, we introduce a Hessian-Free Natural Gradient Descent framework that employs a matrix-free approximation of the Hessian. This approach circumvents the need for explicitly constructing the Hessian matrix and incorporates a novel preconditioning scheme that significantly enhances convergence rates. Our method enables scaling to large neural networks with  up to a million of parameters. Empirically, we demonstrate that our approach outperforms state-of-the-art optimizers, such as LBFGS and Adam, achieving orders-of-magnitudes accuracy improvements across various benchmark PDE problems."
    },
    {
        "title": "Revisiting Delta-Parameter Pruning For Fine-Tuned Models",
        "link_suffix": "/forum?id=avSocG0oFA",
        "link": "https://openreview.net/forum?id=avSocG0oFA",
        "pdf_link": "https://openreview.net/pdf?id=avSocG0oFA",
        "keywords": "Delta parameter pruning, Efficiency, Large Language Models",
        "abstract": "Storing open-source fine-tuned models separately introduces redundancy and  increases response times in applications utilizing multiple models. Delta-parameter pruning (DPP), particularly the random drop and rescale (DARE) method proposed by Yu et al., addresses this by pruning the majority of delta parameters\u2014the differences between fine-tuned and pre-trained model weights\u2014while typically maintaining minimal performance loss. However, DARE fails when either the pruning rate or the magnitude of the delta parameters is large. We highlight two key reasons for this failure: (1) an excessively large rescaling factor as pruning rates increase, and (2) high mean and variance in the delta parameters.\nTo address these, we develop two algorithmic improvements: (1) DARq, which modifies the rescaling factor in DARE, leading to significant performance gains at high pruning rates (e.g., >30% on COLA and SST2 for encoder models, with even larger improvements in decoder models), and (2) AdamR, an in-training modification that incorporates appropriate Delta regularization before applying DPP. We also demonstrate that DARq can be seamlessly combined with vanilla parameter-efficient fine-tuning techniques like LoRA and can facilitate structural DPP. Additionally, we revisit the application of importance-based pruning techniques within DPP, demonstrating that they outperform random-based methods when delta parameters are large. Through this comprehensive study, we develop a pipeline for selecting the most appropriate DPP method under various practical scenarios."
    },
    {
        "title": "CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations",
        "link_suffix": "/forum?id=28abpUEICJ",
        "link": "https://openreview.net/forum?id=28abpUEICJ",
        "pdf_link": "https://openreview.net/pdf?id=28abpUEICJ",
        "keywords": "computational neuroscience, multi-regional brain interactions, sparsity, cross-session variability, dynamical systems modeling, neural dynamics, non-simultaneous neural recordings",
        "abstract": "Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects; presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics. Current analysis methods, however, often fail to fully harness the richness of such data, as they provide either uninterpretable representations (e.g., via deep networks) or oversimplify models (e.g., by assuming stationary dynamics or analyzing each session independently). Here, instead of regarding asynchronous neural recordings that lack alignment in neural identity or brain areas as a limitation, we leverage these diverse views into the brain to learn a unified model of neural dynamics. Specifically, we assume that brain activity is driven by multiple hidden global sub-circuits. These sub-circuits represent global basis interactions between neural ensembles---functional groups of neurons---such that the time-varying decomposition of these sub-circuits defines how the ensembles' interactions evolve over time non-stationarily and non-linearly.\nWe discover the neural ensembles underlying non-simultaneous observations, along with their non-stationary evolving interactions, with our new model,CREIMBO(Cross-Regional Ensemble Interactions in Multi-view Brain Observations). CREIMBO identifies the hidden composition of per-session neural ensembles through novel graph-driven dictionary learning and models the ensemble dynamics on a low-dimensional manifold spanned by a sparse time-varying composition of the global sub-circuits. Thus, CREIMBO disentangles overlapping temporal neural processes while preserving interpretability due to the use of a shared underlying sub-circuit basis. Moreover, CREIMBO distinguishes session-specific computations from global (session-invariant) ones by identifying session covariates and variations in sub-circuit activations. We demonstrate CREIMBO's ability to recover true components in synthetic data, and uncover meaningful brain dynamics in human high-density electrode recordings---capturing cross-subject neural mechanisms as well as  inter- vs. intra-region dynamical motifs."
    },
    {
        "title": "Progress or Regress? Self-Improvement Reversal in Post-training",
        "link_suffix": "/forum?id=RFqeoVfLHa",
        "link": "https://openreview.net/forum?id=RFqeoVfLHa",
        "pdf_link": "https://openreview.net/pdf?id=RFqeoVfLHa",
        "keywords": "Iterative Self-improvement, Problem-solving AI",
        "abstract": "Self-improvement through post-training methods such as iterative preference learning has been acclaimed for enhancing the problem-solving capabilities (e.g., mathematical reasoning) of Large Language Models (LLMs) without human intervention. However, as our exploration deepens, it is crucial to critically assess whether these enhancements indeed signify comprehensive progress or if they could lead to unintended regressions. Through rigorous experimentation and analysis across diverse problem-solving tasks, we uncover nuances in the self-improvement trajectories of LLMs. Our study introduces the concept of \\emph{self-improvement reversal}, where models showing improved overall accuracy metrics might paradoxically exhibit declines in broader, essential capabilities. We propose a comprehensive evaluative framework to scrutinize the underlying mechanisms and outcomes of post-training self-improvement, aiming to discern between superficial metric improvements and genuine enhancements in model functionality. The findings emphasize the complexity of technological advancements in LLMs, underscoring the need for a nuanced understanding of the \\textit{progress or regress} dichotomy in their development."
    },
    {
        "title": "Transfer learning in Scalable Graph Neural Network for Improved Physical Simulation",
        "link_suffix": "/forum?id=QB8dHqVoDw",
        "link": "https://openreview.net/forum?id=QB8dHqVoDw",
        "pdf_link": "https://openreview.net/pdf?id=QB8dHqVoDw",
        "keywords": "graph neural network, transfer learning, physical simulations, mesh",
        "abstract": "In recent years, Graph Neural Network (GNN) based models have shown promising results in simulating physics of complex systems. However, training dedicated graph network based physics simulators can be costly, as most models are confined to fully supervised training, which requires extensive data generated from traditional physics simulators. To date, how transfer learning could improve the model performance and training efficiency has remained unexplored. In this work, we introduce a pre-training and transfer learning paradigm for graph network simulators. We propose the scalable graph U-net (SGUNET). Incorporating an innovative depth-first search (DFS) pooling, the SGUNET is adaptable to different mesh sizes and resolutions for various simulation tasks. To enable the transfer learning between differently configured SGUNETs, we propose a set of mapping functions to align the parameters between the pre-trained model and the target model. An extra normalization term is also added into the loss to constrain the difference between the pre-trained weights and target model weights for better generalization performance. To pre-train our physics simulator we created a dataset which includes 20,000 physical simulations of randomly selected 3D shapes from the open source A Big CAD (ABC) dataset. We show that our proposed transfer learning methods allow the model to perform even better when fine-tuned with small amounts of training data than when it is trained from scratch with full extensive dataset.\nOn the 2D Deformable Plate benchmark dataset, our pre-trained model fine-tuned on 1/16 of the training data achieved an 11.05% improvement in position RMSE compared to the model trained from scratch."
    },
    {
        "title": "CoCMT: Towards Communication-Efficient Corss-Modal Transformer For Collaborative Perception",
        "link_suffix": "/forum?id=S1NrbfMS7T",
        "link": "https://openreview.net/forum?id=S1NrbfMS7T",
        "pdf_link": "https://openreview.net/pdf?id=S1NrbfMS7T",
        "keywords": "deep learning, vehicle-to-vehicle cooperative perception, 3D object detection",
        "abstract": "Cooperative perception systems in autonomous driving enhance each agent\u2019s perceptual capabilities by sharing visual information with others and demonstrated effectiveness in handling prominent challenges like occlusions and long-range detection. However, most existing cooperative systems transmit feature maps, such as bird's-eye view (BEV) representations, which include substantial background data and are costly to process due to their high dimensionality. This paradigm introduces a trade-off between improved perception and increased communication overhead. To address this challenge, we present CoCMT, an object-query-based collaboration framework that enables efficient communication while unifying homogeneous and heterogeneous cooperative perception tasks. Within CoCMT, we introduce the Efficient Query Transformer (EQFormer) to effectively fuse multi-agent object queries and implement a synergistic deep supervision approach to accelerate convergence during training. Extensive experiments on the OPV2V and V2V4Real datasets demonstrate that CoCMT surpasses current state-of-the-art methods in performance while offering significant communication efficiency. Notably, on the real-world V2V4Real dataset, our proposed CoCMT model (Top-50 object queries) requires merely 0.416 Mb bandwidth during inference. This reduces bandwidth consumption by 323 times compared to SOTA methods while improving AP@70 by 1.1. The code and models will be open-sourced."
    },
    {
        "title": "Provable Learning for DEC-POMDPs: Factored Models and Memoryless Agents",
        "link_suffix": "/forum?id=GvsCOOPxoI",
        "link": "https://openreview.net/forum?id=GvsCOOPxoI",
        "pdf_link": "https://openreview.net/pdf?id=GvsCOOPxoI",
        "keywords": "decentralized POMDP, provably efficient algorithm",
        "abstract": "This paper studies cooperative Multi-Agent Reinforcement Learning (MARL) under the mathematical model of Decentralized Partially Observable Markov Decision Process (DEC-POMDP). Despite the empirical success of cooperative MARL, its theoretical foundation, particularly in the realm of provable learning of DEC-POMDPs, remains limited.  In this paper, we first present a hardness result in theory demonstrating that, without additional structural assumptions, learning DEC-POMDPs requires several samples that grows exponentially with the number of agents in the worst case, which is also known as the curse of multiagency. This motivates us to explore important subclasses of DEC-POMDPs for which efficient solutions can be found. Specifically, we propose new algorithms and establish sample-efficiency guarantees that break the curse of multiagency, for finding both local and global optima in two important scenarios: (1) when agents employ memoryless policies, selecting actions based solely on their current observations; and (2) when a factored structure is present, which enables key properties similar to value decomposition in VDN or Qmix."
    },
    {
        "title": "The Graph's Apprentice: Teaching an LLM Low-Level Knowledge for Circuit Quality Estimation",
        "link_suffix": "/forum?id=yDy9fZXNJV",
        "link": "https://openreview.net/forum?id=yDy9fZXNJV",
        "pdf_link": "https://openreview.net/pdf?id=yDy9fZXNJV",
        "keywords": "LLM, Knowledge Distillation, Verilog, Graph Neural Network",
        "abstract": "Logic synthesis is a crucial phase in the circuit design process, responsible for transforming hardware description language (HDL) designs into optimized netlists. However, traditional logic synthesis methods are computationally intensive, restricting their iterative use in refining chip designs. Recent advancements in large language models (LLMs), particularly those fine-tuned on programming languages, present a promising alternative. This work proposes augmenting LLMs with predictor networks trained to estimate circuit quality directly from HDL code. To enhance performance, the model is regularized using embeddings from graph neural networks (GNNs) trained on Look-Up Table (LUT) graphs, thereby incorporating lower-level circuit insights. The proposed method demonstrates superior performance compared to existing graph-based RTL-level estimation techniques on established benchmarks, such as OpenCores and OpenABCD, while providing instant feedback on HDL code quality."
    },
    {
        "title": "FedL2G: Learning to Guide Local Training in Heterogeneous Federated Learning",
        "link_suffix": "/forum?id=deWPVVa6TE",
        "link": "https://openreview.net/forum?id=deWPVVa6TE",
        "pdf_link": "https://openreview.net/pdf?id=deWPVVa6TE",
        "keywords": "Heterogeneous Federated Learning, Model Heterogeneity",
        "abstract": "Data and model heterogeneity are two core issues in Heterogeneous Federated Learning (HtFL). In scenarios with heterogeneous model architectures, aggregating model parameters becomes infeasible, leading to the use of prototypes (i.e., class representative feature vectors) for aggregation and guidance. However, they still experience a mismatch between the extra guiding objective and the client's original local objective when aligned with global prototypes. Thus, we propose a Federated Learning-to-Guide (FedL2G) method that adaptively learns to guide local training in a federated manner and ensures the extra guidance is beneficial to clients\u2019 original tasks. With theoretical guarantees, FedL2G efficiently implements the learning-to-guide process using only first-order derivatives w.r.t. model parameters and achieves a non-convex convergence rate of $\\mathcal{O}(1/T)$. We conduct extensive experiments on two data heterogeneity and six model heterogeneity settings using 14 heterogeneous model architectures (e.g., CNNs and ViTs) to demonstrate FedL2G\u2019s superior performance compared to six counterparts."
    }
]