[{"title": "4K4DGen: Panoramic 4D Generation at 4K Resolution", "link_suffix": "/forum?id=qxRoo7ULCo", "link": "https://openreview.net/forum?id=qxRoo7ULCo", "pdf_link": "https://openreview.net/pdf?id=qxRoo7ULCo", "keywords": "4D Generation, Panoramic Video, Panoramic Gaussian Splatting", "abstract": "The blooming of virtual reality and augmented reality (VR/AR) technologies has driven an increasing demand for the creation of high-quality, immersive, and dynamic environments. However, existing generative techniques either focus solely on dynamic objects or perform outpainting from a single perspective image, failing to meet the requirements of VR/AR applications that need free-viewpoint, 360$^{\\circ}$ virtual views where users can move in all directions. In this work, we tackle the challenging task of elevating a single panorama to an immersive 4D experience. For the first time, we demonstrate the capability to generate omnidirectional dynamic scenes with 360$^{\\circ}$ views at 4K (4096 $\\times$ 2048) resolution, thereby providing an immersive user experience. Our method introduces a pipeline that facilitates natural scene animations and optimizes a set of dynamic Gaussians using efficient splatting techniques for real-time exploration. To overcome the lack of scene-scale annotated 4D data and models, especially in panoramic formats, we propose a novel \\textbf{Panoramic Denoiser} that adapts generic 2D diffusion priors to animate consistently in 360$^{\\circ}$ images, transforming them into panoramic videos with dynamic scenes at targeted regions. Subsequently, we propose \\textbf{Dynamic Panoramic Lifting} to elevate the panoramic video into a 4D immersive environment while preserving spatial and temporal consistency. By transferring prior knowledge from 2D models in the perspective domain to the panoramic domain and the 4D lifting with spatial appearance and geometry regularization, we achieve high-quality Panorama-to-4D generation at a resolution of 4K for the first time.", "title_embedding_index": 20300, "title_abs_embedding_index": 20325}, {"title": "Multi-Bin Batching for Increasing LLM Inference Throughput", "link_suffix": "/forum?id=WVmarX0RNd", "link": "https://openreview.net/forum?id=WVmarX0RNd", "pdf_link": "https://openreview.net/pdf?id=WVmarX0RNd", "keywords": "LLM inference, Throughput optimization, Batched inference", "abstract": "As large language models (LLMs) grow in popularity for their diverse capabilities, improving the efficiency of their inference systems has become increasingly critical.  Batching requests during LLM inference increases throughput by allowing multiple requests to be processed in parallel, making better use of hardware resources such as GPUs. However, the autoregressive nature of LLMs presents a challenge: requests often have varying execution times, causing resource underutilization, as hardware must wait for the longest-running request in the batch to complete before moving to the next batch. We propose Multi-Bin Batching, a simple yet effective method that can \\emph{provably improve LLM inference throughput} by grouping requests with similar execution times into predetermined bins.  We evaluate multi-bin batching on various settings, showing consistent throughput improvements compared to standard batching approaches.", "title_embedding_index": 20301, "title_abs_embedding_index": 20326}, {"title": "Location, Location, Location: Design Bias with Kernel Transformation", "link_suffix": "/forum?id=MrGca1Q7mK", "link": "https://openreview.net/forum?id=MrGca1Q7mK", "pdf_link": "https://openreview.net/pdf?id=MrGca1Q7mK", "keywords": "implicit representation; concentration of measure; recursive kernel transformation; design bias; manifold learning", "abstract": "It has been hypothesized that the old brain was compressed into cortical columns of the neocortex during the evolution of mammalian brains. Computational modeling of hippocampal-cortical interaction inspires us to propose a navigation-based implicit representation for manifold learning. The key new insight is to transform any explicit function (or geometrically a manifold) to an implicit representation using design bias for exploiting the concentration of measure (CoM) in high dimensional spaces. CoM-based blessing of dimensionality enables us to solve the manifold learning problem by direct-fit or local computation with guaranteed generalization property and without the need to discover global topology. We construct a memory encoding model, namely specification-before-generalization (SbG), and extend it into recursive kernel transformation to mirror the nested structure of the physical world. The biological plausibility of SbG learning is supported by its consistency with the wake-sleep cycles of mammalian brains. Finally, we showcase the application of design bias and recursive kernel transformation to understanding the phylogenetic continuity of navigation and memory and the manifold untangling of object recognition by the ventral stream.", "title_embedding_index": 20302, "title_abs_embedding_index": 20327}, {"title": "Long-Term 3D Point Tracking By Cost Volume Fusion", "link_suffix": "/forum?id=EgEyoZvyDw", "link": "https://openreview.net/forum?id=EgEyoZvyDw", "pdf_link": "https://openreview.net/pdf?id=EgEyoZvyDw", "keywords": "3d point tracking, scene flow", "abstract": "Long-term point tracking is essential to understand  non-rigid motion in the physical world better. Deep learning approaches have recently been incorporated into long-term point tracking, but most prior work predominantly functions in 2D. Although these methods benefit from the well-established backbones and matching frameworks, the motions they produce do not always make sense in the 3D physical world. In this paper, we propose the first deep learning framework for long-term point tracking in 3D that generalizes to new points and videos without requiring test-time fine-tuning. Our model contains a cost volume fusion module that effectively integrates multiple past appearances and motion information via a transformer architecture, significantly enhancing overall tracking performance. In terms of 3D tracking performance, our model significantly outperforms simple scene flow chaining and previous 2D point tracking methods, even if one uses ground truth depth and camera pose to backproject 2D point tracks in a synthetic scenario.", "title_embedding_index": 20303, "title_abs_embedding_index": 20328}, {"title": "Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking", "link_suffix": "/forum?id=OeBY9XqiTz", "link": "https://openreview.net/forum?id=OeBY9XqiTz", "pdf_link": "https://openreview.net/pdf?id=OeBY9XqiTz", "keywords": "Synchronized sequence modeling; multiple object tracking", "abstract": "Multiple object tracking in complex scenarios - such as coordinated dance performances, team sports, or dynamic animal groups - presents unique challenges. In these settings, objects frequently move in coordinated patterns, occlude each other, and exhibit long-term dependencies in their trajectories. However, it remains a key open research question on how to model long-range dependencies within tracklets, interdependencies among tracklets, and the associated temporal occlusions. To this end, we introduce Samba, a novel linear-time set-of-sequences model designed to jointly process multiple tracklets by synchronizing the multiple selective state-spaces used to model each tracklet. Samba autoregressively predicts the future track query for each sequence while maintaining synchronized long-term memory representations across tracklets. By integrating Samba into a tracking-by-propagation framework, we propose SambaMOTR, the first tracker effectively addressing the aforementioned issues, including long-range dependencies, tracklet interdependencies, and temporal occlusions. Additionally, we introduce an effective technique for dealing with uncertain observations (MaskObs) and an efficient training recipe to scale SambaMOTR to longer sequences. By modeling long-range dependencies and interactions among tracked objects, SambaMOTR implicitly learns to track objects accurately through occlusions without any hand-crafted heuristics. Our approach significantly surpasses prior state-of-the-art on the DanceTrack, BFT, and SportsMOT datasets.", "title_embedding_index": 20304, "title_abs_embedding_index": 20329}, {"title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding", "link_suffix": "/forum?id=2fgzf8u5fP", "link": "https://openreview.net/forum?id=2fgzf8u5fP", "pdf_link": "https://openreview.net/pdf?id=2fgzf8u5fP", "keywords": "Diffusion models, Reinforcement learning, AI for science", "abstract": "Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require differentiable proxy models (e.g., classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation.", "title_embedding_index": 20305, "title_abs_embedding_index": 20330}, {"title": "TabDPT: Scaling Tabular Foundation Models", "link_suffix": "/forum?id=FDMlGhExFp", "link": "https://openreview.net/forum?id=FDMlGhExFp", "pdf_link": "https://openreview.net/pdf?id=FDMlGhExFp", "keywords": "Tabular Data, Foundation Models, Tabular Foundation Models, In-Context Learning, Retrieval", "abstract": "The challenges faced by neural networks on tabular data are well-documented and have hampered the progress of tabular foundation models. Techniques leveraging in-context learning (ICL) have shown promise here, allowing for dynamic adaptation to unseen data. ICL can provide predictions for entirely new datasets without further training or hyperparameter tuning, therefore providing very fast inference when encountering a novel task. However, scaling ICL for tabular data remains an issue: approaches based on large language models cannot efficiently process numeric tables, and tabular-specific techniques have not been able to effectively harness the power of real data to improve performance and generalization. We are able to overcome these challenges by training tabular-specific ICL-based architectures on real data with self-supervised learning and retrieval, combining the best of both worlds. Our resulting model -- the Tabular Discriminative Pre-trained Transformer (TabDPT) -- achieves state-of-the-art performance on the CC18 (classification) and CTR23 (regression) benchmarks with no task-specific fine-tuning, demonstrating the adapatability and speed of ICL once the model is pre-trained. TabDPT also demonstrates strong scaling as both model size and amount of available data increase, pointing towards future improvements simply through the curation of larger tabular pre-training datasets and training larger models.", "title_embedding_index": 20306, "title_abs_embedding_index": 20331}, {"title": "Model-Free Offline Reinforcement Learning with Enhanced Robustness", "link_suffix": "/forum?id=QyVLJ7EnAC", "link": "https://openreview.net/forum?id=QyVLJ7EnAC", "pdf_link": "https://openreview.net/pdf?id=QyVLJ7EnAC", "keywords": "offline RL, robust, scalability, model-free", "abstract": "Offline reinforcement learning (RL) has gained considerable attention for its ability to learn policies from pre-collected data without real-time interaction, which makes it particularly useful for high-risk applications. However, due to its reliance on offline datasets, existing works inevitably introduce assumptions to ensure effective learning, which, however, often lead to a trade-off between robustness to model mismatch and scalability to large environments. In this paper, we enhance both aspects with a novel double-pessimism principle, which conservatively estimates performance and accounts for both limited data and potential model mismatches, two major reasons for the previous trade-off. We then propose a universal, model-free algorithm to learn an optimal policy that is robust to potential environment mismatches, which enhances robustness in a scalable manner. Furthermore, we provide a sample complexity analysis of our algorithm when the mismatch is modeled by the $l_\\alpha$-norm, which also theoretically demonstrates the efficiency of our method. Extensive experiments further demonstrate that our approach significantly improves robustness in a more scalable manner than existing methods.", "title_embedding_index": 20307, "title_abs_embedding_index": 20332}, {"title": "Unleashing the Potential of Classification with Semantic Similarity for Deep Imbalanced Regression", "link_suffix": "/forum?id=7ut8T9iJ7P", "link": "https://openreview.net/forum?id=7ut8T9iJ7P", "pdf_link": "https://openreview.net/pdf?id=7ut8T9iJ7P", "keywords": "Deep imbalanced regression", "abstract": "Recent studies have empirically demonstrated the feasibility of incorporating classification regularizers into Deep Imbalanced Regression (DIR).\nBy segmenting the entire dataset into distinct groups and performing classification regularization on these groups, previous works primarily focused on capturing ordinal characteristic of the DIR in the feature space.\nConsequently, this direct integration would lead the model to focus merely on learning discriminative features and treating the DIR as a classification task but lacks of an end-to-end solution.\nAs a result, data similarity, another aspect of the continuity of data as the label similarity across the data in DIR also implies feature similarity of the data has always been ignored. \nTherefore, the effectiveness of these classification-based approaches are significantly limited in DIR.\nTo tackle this problem, we investigate the similarity characteristics of the data in DIR to unleash the potential of classification in helping DIR.\nSpecifically, we first split the imbalance of the datasets into a global level cross-group imbalance and instance-level in-group imbalance.\nThen, to fully exploit the potential of classification under the DIR task, we propose an asymmetric soft labeling strategy to capture the global data similarity to handle the cross-group imbalance. \nIn the meantime, we introduce the instance label distribution smoothing to address the intra-group imbalance with a multi-heads regressor.\nMore importantly, we associatedly link up the group classification to guide the learning of the multi-heads regressor, which can further harness the classification to solve the DIR from end-to-end.\nExtensive experiments in the real-world datasets also validates the effectiveness of our proposed method.", "title_embedding_index": 20308, "title_abs_embedding_index": 20333}, {"title": "Alternating Projections With Volume Sampling", "link_suffix": "/forum?id=JQkj67NArS", "link": "https://openreview.net/forum?id=JQkj67NArS", "pdf_link": "https://openreview.net/pdf?id=JQkj67NArS", "keywords": "Method of Alternating Projections, Volume Sampling, Optimization, Iterative Methods", "abstract": "The method of Alternating Projections (AP) is a fundamental iterative technique with applications to problems in machine learning, optimization and signal processing. Examples include the Gauss-Seidel algorithm which is used to solve large-scale regression problems and the Kaczmarz and projections onto convex sets (POCS) algorithms that are fundamental to iterative reconstruction. Progress has been made with regards to the questions of efficiency and rate of convergence in the randomized setting of the AP method. Here, we extend these results with volume sampling to block (batch) sizes greater than 1 and provide explicit formulas that relate the convergence rate bounds to the spectrum of the underlying system. These results, together with a trace formula and associated volume sampling, prove that convergence rates monotonically improve with larger block sizes, a feature that can not be guaranteed in general with uniform sampling (e.g., in SGD).", "title_embedding_index": 20309, "title_abs_embedding_index": 20334}, {"title": "Virtual Community: A Generative Social World for Embodied AI", "link_suffix": "/forum?id=aRxLDcxFcL", "link": "https://openreview.net/forum?id=aRxLDcxFcL", "pdf_link": "https://openreview.net/pdf?id=aRxLDcxFcL", "keywords": "embodied AI", "abstract": "We present Virtual Community, a social world simulation platform designed to support embodied AI research, featuring large-scale community scenarios derived from the real world. Virtual Community introduces two key features to enrich the virtual social world with generative AI: scalable 3D Scene creation, which supports the generation of expansive outdoor and indoor environments at any location and scale, addressing the lack of a large-scale, interactive, open-world scene for embodied AI research; and embodied agents with grounded characters and social relationship networks, the first to simulate socially connected agents at a community level, that also have scene-grounded characters. We design two novel challenges to showcase that Virtual Community provides testbeds to evaluate the social reasoning and planning capabilities of embodied agents in open-world scenarios: Route Planning and Election Campaign. The Route Planning task examines the agent's ability to reason about time, location, and tools in the community to plan fast and economical commutes in daily life. The Election Campaign task evaluates an agent's ability to explore and connect with other agents as a new member of the community.  We evaluate several baseline agents on these challenges and demonstrate the performance gap of current methods in addressing embodied social challenges within open-world scenarios, which our simulator is designed to unlock. We plan to open-source this simulation and hope Virtual Community can accelerate the development in this direction.", "title_embedding_index": 20310, "title_abs_embedding_index": 20335}, {"title": "ACID: A Comprehensive Dataset for AI-Created Image Detection", "link_suffix": "/forum?id=1P6AqR6xkF", "link": "https://openreview.net/forum?id=1P6AqR6xkF", "pdf_link": "https://openreview.net/pdf?id=1P6AqR6xkF", "keywords": "Computer vision, Generative Model, AI Ethics", "abstract": "Generative models have demonstrated remarkable capabilities in generating photorealistic images under proper conditional guidance. Such advancements raise concerns about potential negative social impacts, such as the proliferation of fake news. In response, numerous methods have been developed to differentiate fake from real. Yet, their accuracy and reliability still need to be improved, especially when facing state-of-the-art generative models such as large diffusion models. Infrastructure-wise, the existing testing datasets are sub-optimal in terms of research dimensions and product utility due to their limited data volume and insufficient domain diversity.\nIn this work, we introduce a comprehensive new dataset, namely ACID, which consists of 13M samples sourced from over 50 different generative models versus real-world scenarios. The AI-generated images in this collection are sampled based on fine-grained text prompts and span multiple resolutions. For the real-world samples, we broadly searched public data sources and carefully filtered text-image pairs based on visual and caption quality.\nUsing ACID, we present ACIDNet, an effective framework for detecting AI-generated images. ACIDNet leverages texture features from a Single Simple Patch (SSP) branch and semantic features from a ResNeXt50 branch, and achieves overall cross-benchmark accuracy of $86.77%$, significantly outperforming previous methods such as SSP and CNNSpot by over $10%$. Both our model and dataset will be open-released to the public.", "title_embedding_index": 20311, "title_abs_embedding_index": 20336}, {"title": "Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want", "link_suffix": "/forum?id=bfa58H1nQ8", "link": "https://openreview.net/forum?id=bfa58H1nQ8", "pdf_link": "https://openreview.net/pdf?id=bfa58H1nQ8", "keywords": "Multimodal Large Language Model, Visual Prompting", "abstract": "In this paper, we present the Draw-and-Understand framework, exploring how to integrate visual prompting understanding capabilities into Multimodal Large Language Models (MLLMs). Visual prompts allow users to interact through multi-modal instructions, enhancing the models' interactivity and fine-grained image comprehension. In this framework, we propose a general architecture adaptable to different pre-trained MLLMs, enabling it to recognize various types of visual prompts (such as points, bounding boxes, cycles, and free-form shapes) alongside language understanding. Additionally, we introduce MDVP-Instruct-Data, a multi-domain dataset featuring 1.6 million image-visual prompt-text triplets, including natural images, document images, scene text images, mobile/web screenshots, remote sensing images, and multi-panel images. Building on this dataset, we introduce MDVP-Bench, a challenging benchmark designed to evaluate a model's ability to understand visual prompting instructions. The experimental results demonstrate that our framework can be easily and effectively applied to various MLLMs, such as SPHINX-X and LLaVA. After training with MDVP-Instruct-Data and image-level instruction datasets, our models exhibit impressive multimodal interaction capabilities and pixel-level understanding, while maintaining their image-level visual perception performance.", "title_embedding_index": 20312, "title_abs_embedding_index": 20337}, {"title": "E-DETR: Evidential Deep Learning for End-to-End Uncertainty Estimation in Object Detection", "link_suffix": "/forum?id=tdV1GRkCpZ", "link": "https://openreview.net/forum?id=tdV1GRkCpZ", "pdf_link": "https://openreview.net/pdf?id=tdV1GRkCpZ", "keywords": "object detection, uncertainty estimation, interpretability", "abstract": "Detection transformers (DETR) have emerged as powerful end-to-end learning frameworks for object detection, directly regressing detection parameters as point estimates. However, these networks often lack the ability to express any uncertainty within their estimates. In this work, we replace the regression of point estimates with the direct learning of the posterior distribution in a sampling-free manner by leveraging deep evidential learning, complementing the end-to-end DETR architecture. We present an instance-aware uncertainty framework by extending evidential deep learning with an IoU-aware loss, jointly modelling both classification and localization uncertainties. Furthermore, we enable the model to leverage its uncertainty for self-calibration, aligning the predicted probabilities with the true likelihood of outcomes, and effectively apply evidential deep learning for the task of imbalanced dense object detection. Our approach is easily extensible and requires only fine-tuning, thus leveraging the pre-training of transformers on large datasets. We validate our approach on multiple DETR models by training on the KITTI dataset, and demonstrate improved generalization by evaluating on out-of-domain datasets, BDD100K and nuImages. Our experiments show a significant improvement in performance, and the introduction of uncertainty estimates underscores the potential of our approach for enhancing the reliability of object detection for safety-critical applications.", "title_embedding_index": 20313, "title_abs_embedding_index": 20338}, {"title": "Programming Refusal with Conditional Activation Steering", "link_suffix": "/forum?id=Oi47wc10sm", "link": "https://openreview.net/forum?id=Oi47wc10sm", "pdf_link": "https://openreview.net/pdf?id=Oi47wc10sm", "keywords": "Activation Engineering, Safety, Alignment, Steering Vector", "abstract": "LLMs have shown remarkable capabilities, but precisely controlling their response behavior remains challenging.\nExisting activation steering methods alter LLM behavior indiscriminately, limiting their practical applicability in settings where selective responses are essential, such as content moderation or domain-specific assistants.\nIn this paper, we propose Conditional Activation Steering (CAST), which analyzes LLM activation patterns during inference to selectively apply or withhold activation steering based on the input context.\nOur method is based on the observation that different categories of prompts activate distinct patterns in the model's hidden states.\nUsing CAST, one can systematically control LLM behavior with rules like \"if input is about hate speech or adult content, then refuse\" or \"if input is not about legal advice, then refuse.\"\nThis allows for selective modification of responses to specific content while maintaining normal responses to other content, all without requiring weight optimization.\nWe release an open-source implementation of our framework at <placeholder: open-source GitHub link>.", "title_embedding_index": 20314, "title_abs_embedding_index": 20339}, {"title": "Intrinsic Behavioral Variability Facilitates Flexible Representations: A Neuromotor Developmental Perspective", "link_suffix": "/forum?id=pwUed4vzIn", "link": "https://openreview.net/forum?id=pwUed4vzIn", "pdf_link": "https://openreview.net/pdf?id=pwUed4vzIn", "keywords": "motor, development, adaptation, simulation, supervised-learning, unsupervised-learning", "abstract": "Dynamic human movement necessitates a dynamic representation of the body. The mechanisms underlying the initiation, development, and maintenance of such representations can provide a biological perspective to developing more flexible representations within computational agents. Taking inspiration from the prenatal twitches shown to initiate the human neuromotor representation, we question how these same twitches, present throughout development, may also facilitate subsequent motor adaptation. Across three experiments, we examine the influence twitches, as a form of intrinsic behavioral variability, may have in facilitating motor adaptation to novel situations. In a series of simulated reaching tasks, we trained agents to reach targets while overcoming behavioral, physiological, and neurological changes. Overall, we found evidence that agents exposed to intermittent behavioral variability outperformed their counterparts, showing greater neural weight variability, indicative of greater exploration. Taken together, this work provides a biologically plausible computational framework for flexible representation development.", "title_embedding_index": 20315, "title_abs_embedding_index": 20340}, {"title": "Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders", "link_suffix": "/forum?id=Y2RW9EVwhT", "link": "https://openreview.net/forum?id=Y2RW9EVwhT", "pdf_link": "https://openreview.net/pdf?id=Y2RW9EVwhT", "keywords": "LLM, Multimodal LLM, Vision Encoder", "abstract": "The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks.", "title_embedding_index": 20316, "title_abs_embedding_index": 20341}, {"title": "Finetuning Weather Foundation Models to Develop Climate Model Parameterizations", "link_suffix": "/forum?id=DtATVd5NLc", "link": "https://openreview.net/forum?id=DtATVd5NLc", "pdf_link": "https://openreview.net/pdf?id=DtATVd5NLc", "keywords": "Atmospheric Dynamics, Parameterizations, Climate Modelling, Foundation Model, ERA5, Finetuning, Machine Learning", "abstract": "Climate prediction models parameterize a range of atmospheric-oceanic processes like clouds, turbulence, and gravity waves. These physical parameterizations are a leading source of uncertainty and strongly influence future projections of global temperature rise. We present a fresh approach to developing parameterizations for coarse-climate models by leveraging pre-trained AI foundation models (FMs) for weather and climate. A pre-trained encoder and decoder from a 2.3 billion parameter FM (NASA and IBM's Prithvi WxC) --- which contains a latent probabilistic representation of atmospheric evolution --- is fine-tuned to create a data-driven predictor of atmospheric gravity waves (GWs). Current climate models are not fine enough to resolve GWs. We create an ML-based parameterization that learns GW fluxes from high-resolution ``GW resolving\" climate models to represent them in \"GW missing\" coarse-climate models. The fluxes predicted by our fine-tuned model are comprehensively evaluated using a set of three tests. Comparison with a baseline (Attention U-Net) reveals the superior predictive performance of the fine-tuned model throughout the atmosphere. The model outperforms the baseline even in regions excluded from the FM pre-training. This is quantified using the Hellinger distance which is 0.11 for the baseline and 0.06, i.e., roughly half, for the fine-tuned model. FMs are largely unexplored in climate science. Our findings emphasize their versatility and reusability to accomplish a range of weather- and climate-related downstream applications, especially in a low-data regime. These FMs can be further leveraged to create new parameterizations for other earth-system processes.", "title_embedding_index": 20317, "title_abs_embedding_index": 20342}, {"title": "Is Graph Convolution Always Beneficial For Every Feature?", "link_suffix": "/forum?id=I9omfcWfMp", "link": "https://openreview.net/forum?id=I9omfcWfMp", "pdf_link": "https://openreview.net/pdf?id=I9omfcWfMp", "keywords": "Graph Neural Networks, Graph Homophily, Topological Feature Selection", "abstract": "Graph Neural Networks (GNNs) have demonstrated strong capabilities in processing structured data. While traditional GNNs typically treat each feature dimension equally during graph convolution, we raise an important question: \\textit{Is the graph convolution operation always beneficial for each feature dimension?} If the answer is no, the convolution operation on certain feature dimensions can lead to even worse performance than the convolution-free model. In prior studies, to assess the effects of graph convolution on features, people proposed metrics based on feature homophily to measure feature consistency with the graph topology. However, these metrics have shown unsatisfactory alignment with GNN performance and have not been effectively employed to guide feature selection in GNNs. To address these limitations, we introduce a novel metric, Topological Feature Informativeness (TFI), to distinguish between GNN-favored and GNN-disfavored features, where its effectiveness is validated through both theoretical analysis and empirical observations. Building on TFI, we propose a simple yet effective method, Graph Feature Selection (GFS), which processes GNN-favored and GNN-disfavored features using GNNs and non-GNN models, respectively. This approach maximizes the extraction of useful topological information from each feature. Extensive experiments show that applying GFS to $8$ baseline and state-of-the-art (SOTA) GNN architectures across $10$ datasets yields a significant performance boost in $83.75%$ ($67$ out of $80$) of the cases. Additionally, we demonstrate that GFS\u2019s improvements are robust to hyperparameter tuning, highlighting its potential as a universal method for enhancing various GNN architectures.", "title_embedding_index": 20318, "title_abs_embedding_index": 20343}, {"title": "Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels", "link_suffix": "/forum?id=DydCqKa6AH", "link": "https://openreview.net/forum?id=DydCqKa6AH", "pdf_link": "https://openreview.net/pdf?id=DydCqKa6AH", "keywords": "Pedestrian Movement Analysis, Human Motion Dataset, Human Motion Generation", "abstract": "Understanding and modeling pedestrian movements in the real world is crucial for applications like motion forecasting and scene simulation. Many factors influence pedestrian movements, such as scene context, individual characteristics, and goals, which are often ignored by the existing human generation methods. Web videos contain natural pedestrian behavior and rich motion context, but annotating them with pre-trained predictors leads to noisy labels. In this work, we propose learning diverse pedestrian movements from web videos. We first curate a large-scale dataset called CityWalkers that captures diverse real-world pedestrian movements in urban scenes. Then, based on CityWalkers,  we propose a generative model called PedGen for diverse pedestrian movement generation. PedGen introduces automatic label filtering to remove the low-quality labels and a mask embedding to train with partial labels. It also contains a novel context encoder that lifts the 2D scene context to 3D and can incorporate various context factors in generating realistic pedestrian movements in urban scenes. Experiments show that PedGen outperforms existing baseline methods for pedestrian movement generation by learning from noisy labels and incorporating the context factors. In addition, PedGen achieves zero-shot generalization in both real-world and simulated environments. The code, model, and data will be made publicly available.", "title_embedding_index": 20319, "title_abs_embedding_index": 20344}, {"title": "Marginalization Consistent Mixture of Separable Flows for Probabilistic Irregular Time Series Forecasting", "link_suffix": "/forum?id=ts1F3BVaDI", "link": "https://openreview.net/forum?id=ts1F3BVaDI", "pdf_link": "https://openreview.net/pdf?id=ts1F3BVaDI", "keywords": "Probabilistic modelling, Normalizing flows, Marginalization consitsent", "abstract": "Probabilistic forecasting models for joint distributions of targets in irregular time\nseries are a heavily under-researched area in machine learning with, to the best of\nour knowledge, only three models researched so far: GPR, the Gaussian Process\nRegression model (D\u00fcrichen et al., 2015), TACTiS, the Transformer-Attentional\nCopulas for Time Series Drouin et al. (2022); Ashok et al. (2024) and ProFITi\n(Yalavarthi et al., 2024b), a multivariate normalizing flow model based on invertible\nattention layers. While ProFITi, thanks to using multivariate normalizing flows,\nis the more expressive model with a better predictive performance, we will show\nthat it suffers from marginalization inconsistency: it does not guarantee that the\nmarginal distributions of a subset of variables in its predictive distributions coincide\nwith the directly predicted distributions of these variables. Also, TACTiS does not\nprovide any guarantees for marginalization consistency.\nWe develop a novel probabilistic irregular time series forecasting model, Marginal-\nization Consistent Mixtures of Separable Flows (moses), that mixes several nor-\nmalizing flows with (i) Gaussian Processes with full covariance matrix as source\ndistributions and (ii) a separable invertible transformation, aiming to combine\nthe expressivity of normalizing flows with the marginalization consistency of\nGaussians. In experiments on four different datasets we show that moses outper-\nform other state-of-the-art marginalization consistent models, perform on par with\nProFITi, but different from ProFITi, guarantees marginalization consistency.", "title_embedding_index": 20320, "title_abs_embedding_index": 20345}, {"title": "MMFNet: Multi-Scale Frequency Masking Neural Network for Multivariate Time Series Forecasting", "link_suffix": "/forum?id=nsozLtutE6", "link": "https://openreview.net/forum?id=nsozLtutE6", "pdf_link": "https://openreview.net/pdf?id=nsozLtutE6", "keywords": "Time series forecasting, Multi-scale frequency domain decomposition", "abstract": "Long-term Time Series Forecasting (LTSF) is critical for numerous real-world applications, such as electricity consumption planning, financial forecasting, and disease propagation analysis. LTSF requires capturing long-range dependencies between inputs and outputs, which poses significant challenges due to complex temporal dynamics and high computational demands. While linear models reduce model complexity by employing frequency domain decomposition, current approaches often assume stationarity and filter out high-frequency components that may contain crucial short-term fluctuations. In this paper, we introduce MMFNet, a novel model designed to enhance long-term multivariate forecasting by leveraging a multi-scale masked frequency decomposition approach. MMFNet captures fine, intermediate, and coarse-grained temporal patterns by converting time series into frequency segments at varying scales while employing a learnable mask to filter out irrelevant components adaptively.\nExtensive experimentation with benchmark datasets shows that MMFNet not only addresses the limitations of the existing methods but also consistently achieves good performance. Specifically, MMFNet achieves up to 6.0% reductions in the Mean Squared Error (MSE) compared to state-of-the-art models designed for multivariate forecasting tasks.", "title_embedding_index": 20321, "title_abs_embedding_index": 20346}, {"title": "Long-form Hallucination Detection with Self-elicitation", "link_suffix": "/forum?id=r9mYbs8RTH", "link": "https://openreview.net/forum?id=r9mYbs8RTH", "pdf_link": "https://openreview.net/pdf?id=r9mYbs8RTH", "keywords": "hallucination, knowledge graph, large language models, medical QA", "abstract": "While Large Language Models (LLMs) have exhibited impressive performance in long-form question-answering tasks, they frequently present a hazard of producing factual inaccuracies or hallucinations. An effective strategy to mitigate this hazard is to leverage off-the-shelf LLMs to detect hallucinations after the generation. The primary challenge resides in the comprehensive elicitation of the intrinsic knowledge acquired during their pre-training phase. However, existing methods that employ complex reasoning chains predominantly fall short of addressing this issue. Moreover, since existing methods for hallucination detection tend to decompose the text into isolated statements, they are unable to learn the inherent semantic continuity in long-form content. In this paper, we propose a novel framework, SelfElicit, which synergizes the self-elicitation of intrinsic knowledge of large language models and long-form continuity understanding. Specifically, we leverage self-generated thoughts derived from prior statements as catalysts to elicit the expression of intrinsic knowledge,\nwhich is integrated with knowledge hypergraphs to alleviate induced hallucinations and guide the factual evaluation by effectively organizing the elicited knowledge. Extensive experiments on real-world medical QA datasets demonstrate the effectiveness of self-elicitation and the superiority of our proposed method.", "title_embedding_index": 20322, "title_abs_embedding_index": 20347}, {"title": "Adversarial Testing in LLMs: Insights into Decision-Making Vulnerabilities", "link_suffix": "/forum?id=lsHmT3Fr65", "link": "https://openreview.net/forum?id=lsHmT3Fr65", "pdf_link": "https://openreview.net/pdf?id=lsHmT3Fr65", "keywords": "Large Language Models, Decision-making, Adversarial testing, AI safety", "abstract": "As AI systems, particularly Large Language Models (LLMs), rapidly advance towards surpassing human cognitive capabilities, ensuring their alignment with human values and safety standards emerges as a formidable challenge. \nThis study addresses a crucial aspect of superalignment by investigating the decision-making capabilities and adversarial vulnerabilities of LLMs, focusing on GPT-3.5, GPT-4 and Gemini-1.5, within structured experimental settings that mimic complex human interactions.\nWe applied an adversarial framework to two decision-making tasks\u2014the two-armed bandit task and the Multi-Round Trust Task (MRTT)\u2014to test the vulnerabilities of LLMs under adversarial conditions. \nIn the bandit task, the adversary aimed to induce the LLM's preference for the predefined target action with the constraint that each action must be assigned an equal number of rewards. For the MRTT, we trained two types of adversaries: one aimed at maximizing its own earnings (MAX) and the other focused on maximizing fairness (FAIR).\nGPT-4 and Gemini-1.5 showed a bias toward exploitation in the bandit task, prioritizing early-established strategies, which made them predictable and vulnerable to manipulation. \nGPT-3.5, while more exploratory in the bandit task, demonstrated more risk-seeking behavior in the MRTT, leading to increased vulnerability in interacting with the MAX adversary.\nNotably, Gemini-1.5 excelled in the MRTT, adapting effectively to adversaries and outperforming both GPT-3.5 and GPT-4 by balancing risk and cooperation with its adversaries. \nBy presenting a specific set of tasks that characterizes decision-making vulnerabilities in LLM-based agents, we provide a concrete methodology for evaluating their readiness for real-world deployment. \nThe adversarial framework proved a powerful tool for stress-testing LLMs, revealing the importance of ensuring that AI models are both robust against adversarial manipulation and responsive to fairness cues in complex, dynamic environments.", "title_embedding_index": 20323, "title_abs_embedding_index": 20348}, {"title": "Identifying treatment response subgroups in observational time-to-event data", "link_suffix": "/forum?id=OuKMmtAvOi", "link": "https://openreview.net/forum?id=OuKMmtAvOi", "pdf_link": "https://openreview.net/pdf?id=OuKMmtAvOi", "keywords": "Survival analysis, Treatment effect, Observational data, Subgroup discovery", "abstract": "Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for subgroup analysis primarily rely on Randomised Controlled Trials (RCTs), in which treatment assignment is randomised. RCTs' patient cohorts are often constrained by cost, rendering them not representative of the heterogeneity of patients likely to receive treatment in real-world clinical practice. When applied to observational studies, subgroup analysis approaches suffer from significant statistical biases particularly because of the non-randomisation of treatment. Our work introduces a novel, outcome-guided method for identifying treatment response subgroups in observational studies. Our approach assigns each patient to a subgroup associated with two time-to-event distributions: one under treatment and one under control regime. It hence positions itself in between individualised and average treatment effect estimation. The assumptions of our model result in a simple correction of the statistical bias from treatment non-randomisation through inverse propensity weighting. In experiments, our approach significantly outperforms the current state-of-the-art method for outcome-guided subgroup analysis in both randomised and observational treatment regimes.", "title_embedding_index": 20324, "title_abs_embedding_index": 20349}]