[
    {
        "title": "LIAR: Leveraging Inverse Alignment to Jailbreak LLMs in Seconds",
        "link_suffix": "/forum?id=CbepKhSNc0",
        "link": "https://openreview.net/forum?id=CbepKhSNc0",
        "pdf_link": "https://openreview.net/pdf?id=CbepKhSNc0",
        "keywords": "LLM, jailbreak",
        "abstract": "Many existing jailbreak techniques rely on solving discrete combinatorial optimization, while more recent approaches involve training LLMs to generate multiple adversarial prompts. However, both approaches require significant computational resources to produce even a single adversarial prompt. We hypothesize that the inefficiency of current approaches stems from an inadequate characterization of the jailbreak problem. To address this gap, we formulate the jailbreak problem as aninverse alignment problem. By starting from an available safety-aligned model, we leverage an unsafe reward to guide the safe model towards generating unsafe outputs using alignment techniques (e.g., reinforcement learning from human feedback), effectively performing inverse AI alignment. We propose a novel jailbreak method called LIAR (LeveragingInverseAlignment to jailbReak). To demonstrate the simplicity and effectiveness of our approach, we employ a best-of-$N$ method. LIAR offers significant advantages: lower computational requirements without additional training, fully black-box operation, competitive attack success rates, and more human-readable prompts. We provide theoretical insights into the possibility of jailbreaking a safety-aligned model, revealing inherent vulnerabilities in current alignment strategies for LLMs. We also provide sub-optimality guarantees for the proposed LIAR. Experimentally, we achieve ASR comparable to the SoTA with a 10x improvement to perplexity and a Time-to-Attack measured in seconds rather than tens of hours."
    },
    {
        "title": "Training Neural Networks on Data Sources with Unknown Reliability",
        "link_suffix": "/forum?id=qDeEsfAb1j",
        "link": "https://openreview.net/forum?id=qDeEsfAb1j",
        "pdf_link": "https://openreview.net/pdf?id=qDeEsfAb1j",
        "keywords": "Noisy data, data sources, learning with noise",
        "abstract": "When data is generated by multiple sources, conventional training methods update models assuming equal reliability for each source and do not consider their individual data quality during training. However, in many applications, sources have varied levels of reliability that can have negative effects on the performance of a neural network. A key issue is that often the quality of data for individual sources is not known during training. Focusing on supervised learning, we aim to train neural networks on each data source for a number of steps proportional to the source's estimated relative reliability, by using a dynamic weighting. This way, we allow training on all sources during the warm-up, and reduce learning on less reliable sources during the final training stages, when it has been shown models overfit to noise. We show through diverse experiments, this can significantly improve model performance when trained on mixtures of reliable and unreliable data sources, and maintain performance when models are trained on reliable sources only."
    },
    {
        "title": "Strong denoising of financial time-series",
        "link_suffix": "/forum?id=GG80jy9KI5",
        "link": "https://openreview.net/forum?id=GG80jy9KI5",
        "pdf_link": "https://openreview.net/pdf?id=GG80jy9KI5",
        "keywords": "mutual learning, regularization",
        "abstract": "In this paper we introduce a method for improving the signal to noise ratio of financial data. The approach relies on combining a target variable with different context variables and using auto-encoders (AEs) to learn reconstructions of the combined inputs. The idea is to seek agreement among multiple AEs which are trained on related but different inputs for which they are forced to find common ground. The training process is set up as a conversation where models take turns at producing a prediction (speaking) or reconciling own predictions with the output of the other AE (listening), until an agreement is reached. This leads to \"mutual regularization\" among the AEs. Unlike standard regularization which relies on including a complexity penalty into the loss function, the proposed method uses the partner network to detect and amend the lack of generality in the data representation. As only true regularities can be agreed upon by the AEs, the replication of noise is costly and will therefore be avoided."
    },
    {
        "title": "Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning",
        "link_suffix": "/forum?id=gc8QAQfXv6",
        "link": "https://openreview.net/forum?id=gc8QAQfXv6",
        "pdf_link": "https://openreview.net/pdf?id=gc8QAQfXv6",
        "keywords": "Catastrophic forgetting; Large language model; Instruction tuning",
        "abstract": "Catastrophic forgetting (CF) poses a significant challenge in machine learning, where a model forgets previously learned information upon learning new tasks. \nDespite the advanced capabilities of Large Language Models (LLMs), they continue to face challenges with CF during continual learning. The majority of existing research focuses on analyzing forgetting patterns through a singular training sequence, thereby overlooking the intricate effects that diverse tasks have on model behavior.\nOur study explores CF across various settings, discovering that model forgetting is influenced by both the specific training tasks and the models themselves. To this end, we interpret forgetting by examining the function vector (FV), a compact representation of functions in LLMs, offering a model-dependent indicator for the occurrence of CF. Through theoretical and empirical analyses, we demonstrated that CF in LLMs primarily stems from biases in function activation rather than the overwriting of task processing functions.\nLeveraging these insights, we propose a novel function vector guided training methodology, incorporating a regularization technique to stabilize the FV and mitigate forgetting. Empirical tests on four benchmarks confirm the effectiveness of our proposed training method, substantiating our theoretical framework concerning CF and model function dynamics. We plan to make our code publicly accessible in the near future."
    },
    {
        "title": "Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception",
        "link_suffix": "/forum?id=rMOhA1JNPo",
        "link": "https://openreview.net/forum?id=rMOhA1JNPo",
        "pdf_link": "https://openreview.net/pdf?id=rMOhA1JNPo",
        "keywords": "Diffusion for Perception, Diffusion Models, Visual Perception",
        "abstract": "With success in image generation, generative diffusion models are increasingly adopted for discriminative scenarios because generating pixels is a unified and natural perception interface. Although directly re-purposing their generative denoising process has established promising progress in specialist (e.g., depth estimation) and generalist models, the inherent gaps between a generative process and discriminative objectives are rarely investigated. For instance, generative models can tolerate deviations at intermediate sampling steps as long as the final distribution is reasonable, while discriminative tasks with rigorous ground truth for evaluation are sensitive to such errors. Without mitigating such gaps, diffusion for perception still struggles on tasks represented by multi-modal understanding (e.g., referring image segmentation). Motivated by these challenges, we analyze and improve the alignment between the generative diffusion process and perception objectives centering around the key observation: \\emph{how perception quality evolves with the denoising process}. (1) Notably, earlier denoising steps contribute more than later steps, necessitating a tailored learning objective for training: loss functions should reflect varied contributions of timesteps for each perception task. (2) Perception quality drops unexpectedly at later denoising steps, revealing the sensitiveness of perception to training-denoising distribution shift. We introduce diffusion-tailored data augmentation to simulate such drift in the training data. (3) We suggest a novel perspective to the long-standing question: why should a generative process be useful for discriminative tasks -- interactivity. The denoising process can be leveraged as a controllable user interface adapting to users' correctional prompts and conducting multi-round interaction in an agentic workflow. Collectively, our insights enhance multiple generative diffusion-based perception models without architectural changes: state-of-the-art diffusion-based depth estimator, previously underplayed referring image segmentation models, and perception generalists."
    },
    {
        "title": "Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only",
        "link_suffix": "/forum?id=p74CpDzw1Y",
        "link": "https://openreview.net/forum?id=p74CpDzw1Y",
        "pdf_link": "https://openreview.net/pdf?id=p74CpDzw1Y",
        "keywords": "alignment, preference learning, reasoning",
        "abstract": "In the absence of abundant reliable annotations for challenging tasks and contexts, how can we expand the frontier of LLM capabilities with potentially wrong answers? We focus on two research questions: (1) Can LLMs generate reliable preferences among wrong options? And if so, (2) Would alignment with such wrong-over-wrong preferences be helpful? We employ methods based on self-consistency, token probabilities, and LLM-as-a-judge to elicit wrong-over-wrong preferences, and fine-tune language models with preference optimization approaches using these synthesized preferences. Extensive experiments with seven LLMs and eight datasets demonstrate that (1) LLMs do have preliminary capability in distinguishing various shades of wrong, achieving up to 20.9% higher performance than random guess; (2) Alignment with wrong-over-wrong preferences helps LLMs to produce less wrong and sometimes even outright correct answers, while overall improving model calibration."
    },
    {
        "title": "CausalVE: Face Video Privacy Encryption via Causal Video Prediction",
        "link_suffix": "/forum?id=waHmD2i1dv",
        "link": "https://openreview.net/forum?id=waHmD2i1dv",
        "pdf_link": "https://openreview.net/pdf?id=waHmD2i1dv",
        "keywords": "Bioprivacy, Diffusion model, Face swapping, Video Prediction, Reversible neural networks, Video Hiding",
        "abstract": "Advanced facial recognition technologies and recommender systems with inadequate privacy technologies and policies for facial interactions increase concerns about bioprivacy violations. With the proliferation of video and live-streaming websites, public-face video distribution and interactions pose greater privacy risks. Existing techniques typically address the risk of sensitive biometric information leakage through various privacy enhancement methods but pose a higher security risk by corrupting the information to be conveyed by the interaction data, or by leaving certain biometric features intact that allow an attacker to infer sensitive biometric information from them. To address these shortcomings, in this paper, we propose a neural network framework, CausalVE. We obtain cover images by adopting a diffusion model to achieve face swapping with face guidance and use the speech sequence features and spatiotemporal sequence features of the secret video for dynamic video inference and prediction to obtain a cover video with the same number of frames as the secret video. In addition, we hide the secret video by using reversible neural networks for video hiding so that the video can also disseminate secret data. Numerous experiments prove that our CausalVE has good security in public video dissemination and outperforms state-of-the-art methods from a qualitative, quantitative, and visual point of view."
    },
    {
        "title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence",
        "link_suffix": "/forum?id=HSGCCUwH7r",
        "link": "https://openreview.net/forum?id=HSGCCUwH7r",
        "pdf_link": "https://openreview.net/pdf?id=HSGCCUwH7r",
        "keywords": "evolutionary algorithm, model adaptation, model merging",
        "abstract": "We propose Model Swarms, a collaborative search algorithm to adapt LLMs via swarm intelligence, the collective behavior guiding individual systems. Specifically, Model Swarms starts with a pool of LLM experts and a utility function. Guided by the best-found checkpoints across models, diverse LLM experts collaboratively move in the weight space and optimize a utility function representing model adaptation objectives. Compared to existing model composition approaches, Model Swarms offers tuning-free model adaptation, works in low-data regimes with as few as 200 examples, and does not require assumptions about specific experts in the swarm or how they should be composed. Extensive experiments demonstrate that Model Swarms could flexibly adapt LLM experts to a single task, multi-task domains, reward models, as well as diverse human interests, improving over 12 model composition baselines by up to 21.0% across tasks and contexts. Further analysis reveals that LLM experts discover previously unseen capabilities in initial checkpoints and that Model Swarms enable the weak-to-strong transition of experts through the collaborative search process."
    },
    {
        "title": "Improve Mathematical Reasoning in Language Models with Automated Process Supervision",
        "link_suffix": "/forum?id=KwPUQOQIKt",
        "link": "https://openreview.net/forum?id=KwPUQOQIKt",
        "pdf_link": "https://openreview.net/pdf?id=KwPUQOQIKt",
        "keywords": "LLM, Reasoning, PRM",
        "abstract": "Complex multi-step reasoning tasks, such as solving mathematical problems or generating code, remain a significant hurdle for even the most advanced large language models (LLMs). Verifying LLM outputs with an Outcome Reward Model (ORM) is a standard inference-time technique aimed at enhancing the reasoning performance of LLMs. However, this still proves insufficient for reasoning tasks with a lengthy or multi-hop reasoning chain, where the intermediate outcomes are neither properly rewarded nor penalized. Process supervision addresses this limitation by assigning intermediate rewards during the reasoning process. To date, the methods used to collect process supervision data have relied on either human annotation or per-step Monte Carlo estimation, both prohibitively expensive to scale, thus hindering the broad application of this technique. In response to this challenge, we propose a novel divide-and-conquer style Monte Carlo Tree Search (MCTS) algorithm named \\textit{OmegaPRM} for the efficient collection of high-quality process supervision data. This algorithm swiftly identifies the first error in the Chain of Thought (CoT) with binary search and balances the positive and negative examples, thereby ensuring both efficiency and quality. As a result, we are able to collect over 1.5 million process supervision annotations to train Process Reward Models (PRMs). This fully automated process supervision alongside the weighted self-consistency algorithm is able to enhance LLMs' math reasoning performances. We improved the success rates of the instruction-tuned Gemini Pro model from 51% to 69.4% on MATH500 and from 86.4% to 93.6% on GSM8K. Similarly, we boosted the success rates of Gemma2 27B from 42.3% to 58.2% on MATH500 and from 74.0% to 92.2% on GSM8K.\nThe entire process operates without any human intervention or supervision, making our method both financially and computationally cost-effective compared to existing methods."
    },
    {
        "title": "Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models",
        "link_suffix": "/forum?id=9yJKTosUex",
        "link": "https://openreview.net/forum?id=9yJKTosUex",
        "pdf_link": "https://openreview.net/pdf?id=9yJKTosUex",
        "keywords": "Large Language Models, Large Vision Models, Semantic Evaluation, Computational Pathology, Medical Imaging",
        "abstract": "Do Large Vision Models (LVMs) extract medically and semantically relevant features similar to those identified by human experts? Currently, only biased, qualitative approaches with limited, small-scale expert evaluations are available to answer this question. In this study, we propose the Boltzmann Semantic Score (BSS), a novel method inspired by state space modeling, to evaluate the encoding space of LVMs from medical images using the encoding space of Large Language Models (LLMs) from medical reports. Through extensive experimentation on 32 datasets from The Cancer Genome Atlas collection using five state-of-the-art LLMs, we first establish a baseline of LLMs' performance in digital pathology and show that LLMs' encoding can be linked to patient outcomes. Then, we compared seven LVMs with BSS and showed that LVMs suffer from poor semantic capability when compared with encoded expert knowledge from pathology reports.\nWe also found statistically significant correlations between BSS (as a measure of structural similarity) and performance in two downstream tasks: information retrieval and survival prediction tasks. Our study also investigates the consensus among LLMs in evaluating LVMs using BSS, indicating that LLMs generally reach substantial consensus in rating LVMs, with some variation dependant on the cancer type. We believe the BSS metric proposed here holds significant potential for application in other domains with similar contexts."
    },
    {
        "title": "Mini-batch kernelk-means",
        "link_suffix": "/forum?id=mMzp3ImIco",
        "link": "https://openreview.net/forum?id=mMzp3ImIco",
        "pdf_link": "https://openreview.net/pdf?id=mMzp3ImIco",
        "keywords": "kernel k-means, mini-batch",
        "abstract": "We present the first mini-batch kernel $k$-means algorithm, offering an order of magnitude improvement in running time compared to the full batch algorithm. A single iteration of our algorithm takes $\\widetilde{O}(kb^2)$ time, significantly faster than the $O(n^2)$ time required by the full batch kernel $k$-means, where $n$ is the dataset size and $b$ is the batch size. Extensive experiments demonstrate that our algorithm consistently achieves a 10-100x speedup with minimal loss in quality, addressing the slow runtime that has limited kernel $k$-means adoption in practice. We further complement these results with a theoretical analysis under an early stopping condition, proving that with a batch size of $\\widetilde{\\Omega}(\\max \\set{\\gamma^{4}, \\gamma^{2}}\\cdot\\epsilon^{-2} )$, the algorithm terminates in $O(\\gamma^2/\\epsilon)$ iterations with high probability, where $\\gamma$ bounds the norm of points in feature space and $\\epsilon$ is a termination threshold. Our analysis holds for any reasonable center initialization, and when using $k$-means++ initialization, the algorithm achieves an approximation ratio of $O(\\log k)$ in expectation. For normalized kernels, such as Gaussian or Laplacian it holds that $\\gamma=1$. Taking $\\epsilon = O(1)$ and $b=\\Theta(\\log n)$, the algorithm terminates in $O(1)$ iterations, with each iteration running in $\\widetilde{O}(k)$ time."
    },
    {
        "title": "Training Nonlinear Transformers for Chain-of-Thought Inference:  A Theoretical Generalization Analysis",
        "link_suffix": "/forum?id=n7n8McETXw",
        "link": "https://openreview.net/forum?id=n7n8McETXw",
        "pdf_link": "https://openreview.net/pdf?id=n7n8McETXw",
        "keywords": "Theoretical Chain-of-Thought, generalization, deep learning theory, In-Context Learning, training dynamics of Transformers",
        "abstract": "Chain-of-Thought (CoT) is an efficient prompting method that enables the reasoning ability of large language models by augmenting the query using multiple examples with multiple intermediate steps. Despite the empirical success, the theoretical understanding of how to train a Transformer to achieve the CoT ability remains less explored. This is primarily due to the technical challenges involved in analyzing the nonconvex optimization on nonlinear attention models. To the best of our knowledge, this work provides the first theoretical study of training Transformers with nonlinear attention to obtain the CoT generalization capability so that the resulting model can inference on unseen tasks when the input is augmented by examples of the new task. We first quantify the required training samples and iterations to train a Transformer model towards CoT ability.  We then prove the success of its CoT generalization on unseen tasks with distribution-shifted testing data. Moreover, we theoretically characterize the conditions for an accurate reasoning output by CoT even when the provided reasoning examples contain noises and are not always accurate. In contrast, in-context learning (ICL), which can be viewed as one-step CoT without intermediate steps, may fail to provide an accurate output when CoT does. These theoretical findings are justified through experiments."
    },
    {
        "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
        "link_suffix": "/forum?id=tidibw8Xdm",
        "link": "https://openreview.net/forum?id=tidibw8Xdm",
        "pdf_link": "https://openreview.net/pdf?id=tidibw8Xdm",
        "keywords": "vision-language models, reinforcement learning, embodied control",
        "abstract": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that encode semantic features of visual observations based on the VLM's internal knowledge and reasoning capabilities, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings from off-the-shelf, general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings. Finally, we show that our approach can use chain-of-thought prompting to produce representations of common-sense semantic reasoning, improving policy performance in novel scenes by 1.5 times."
    },
    {
        "title": "Co-Representation Neural Hypergraph Diffusion for Edge-Dependent Node Classification",
        "link_suffix": "/forum?id=fMUggopCYI",
        "link": "https://openreview.net/forum?id=fMUggopCYI",
        "pdf_link": "https://openreview.net/pdf?id=fMUggopCYI",
        "keywords": "hypergraph neural networks, hypergraph diffusion, hypergraph learning",
        "abstract": "Hypergraphs are widely employed to represent complex higher-order relations in real-world applications. Most hypergraph learning research focuses on node-level or edge-level tasks. A practically relevant but more challenging task, edge-dependent node classification (ENC), is only recently proposed. In ENC, a node can have different labels across different hyperedges, which requires the modeling of node-edge pairs instead of single nodes or hyperedges. Existing solutions for this task are based on message passing and model interactions in within-edge and within-node structures as multi-input single-output functions. This brings three limitations: (1) non-adaptive representation size, (2) non-adaptive messages, and (3) insufficient direct interactions among nodes or edges. To tackle these limitations, we propose CoNHD, a new ENC solution that models both within-edge and within-node interactions as multi-input multi-output functions. Specifically, we represent these interactions as a hypergraph diffusion process on node-edge co-representations. We further develop a neural implementation for this diffusion process, which can adapt to a specific ENC dataset. Extensive experiments demonstrate the effectiveness and efficiency of the proposed CoNHD method."
    },
    {
        "title": "DyGMamba: Efficiently Modeling Long-Term Temporal Dependency on Continuous-Time Dynamic Graphs with State Space Models",
        "link_suffix": "/forum?id=f3gCs2a4ZD",
        "link": "https://openreview.net/forum?id=f3gCs2a4ZD",
        "pdf_link": "https://openreview.net/pdf?id=f3gCs2a4ZD",
        "keywords": "dynamic graph, state space model",
        "abstract": "Learning useful representations for continuous-time dynamic graphs (CTDGs) is challenging, due to the concurrent need to span long node interaction histories and grasp nuanced temporal details. In particular, two problems emerge: (1) Encoding longer histories requires more computational resources, making it crucial for CTDG models to maintain low computational complexity to ensure efficiency; (2) Meanwhile, more powerful models are needed to identify and select the most critical temporal information within the extended context provided by longer histories. To address these problems, we propose a CTDG representation learning model named DyGMamba, originating from the popular Mamba state space model (SSM). DyGMamba first leverages a node-level SSM to encode the sequence of historical node interactions. Another time-level SSM is then employed to exploit the temporal patterns hidden in the historical graph, where its output is used to dynamically select the critical information from the interaction history. We validate DyGMamba experimentally on the dynamic link prediction task. The results show that our model achieves state-of-the-art in most cases. DyGMamba also maintains high efficiency in terms of computational resources, making it possible to capture long temporal dependencies with a limited computation budget."
    },
    {
        "title": "Competing Large Language Models in Multi-Agent Gaming Environments",
        "link_suffix": "/forum?id=DI4gW8viB6",
        "link": "https://openreview.net/forum?id=DI4gW8viB6",
        "pdf_link": "https://openreview.net/pdf?id=DI4gW8viB6",
        "keywords": "Large Language Models, Games, Reasoning, Evaluation",
        "abstract": "Decision-making is a complex process requiring diverse abilities, making it an excellent framework for evaluating Large Language Models (LLMs). Researchers have examined LLMs' decision-making through the lens of Game Theory. However, existing evaluation mainly focus on two-player scenarios where an LLM competes against another. Additionally, previous benchmarks suffer from test set leakage due to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework for evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes eight classical game theory scenarios and a dynamic scoring scheme specially designed to quantitatively assess LLMs' performance. $\\gamma$-Bench allows flexible game settings and adapts the scoring system to different game parameters, enabling comprehensive evaluation of robustness, generalizability, and strategies for improvement. Our results indicate that GPT-3.5 demonstrates strong robustness but limited generalizability, which can be enhanced using methods like Chain-of-Thought. We also evaluate twelve LLMs from six model families, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2. Gemini-1.5-Pro outperforms others, scoring of $68.1$ out of $100$, followed by LLaMA-3.1-70B ($64.5$) and Mixtral-8x22B ($61.4$). All code and experimental results are available in the supplementary materials and will be made publicly available upon publication."
    },
    {
        "title": "A Brain-Inspired Machine Learning Paradigm for Nature-Powered Equation Solving",
        "link_suffix": "/forum?id=e1DkCLjdhS",
        "link": "https://openreview.net/forum?id=e1DkCLjdhS",
        "pdf_link": "https://openreview.net/pdf?id=e1DkCLjdhS",
        "keywords": "Nature-Powered Computing, Dynamical Systems",
        "abstract": "Solving equations is fundamental to human understanding of the world. While modern machine learning methods are powerful equation solvers, their escalating complexity and extreme operational costs hinder sustainable development. In contrast, nature effortlessly solves complex equations through dynamical systems that instinctively evolve to low-energy states without explicit instructions. However, existing attempts to leverage dynamical systems are limited by low expressivity and a lack of training support. To this end, we propose DS-Solver, a nature-powered AI paradigm employing an expressive, self-trainable dynamical system capable of accurately solving a wide spectrum of equations with extraordinary efficiency. (1) We enhance system expressivity by enriching node dynamics with coupled real-valued and polarized shadow nodes, capturing complex interactions inherent in the real world. (2) We propose an on-device learning method that leverages intrinsic electrical signals as loss, enabling the dynamical system to instantly train itself at negligible cost. Experimental results across key equations from diverse domains demonstrate that DS-Solver achieves 42% higher accuracy than current SOTA -- while offering orders-of-magnitude improvements in speed and energy efficiency over traditional neural network solutions on GPUs for both inference and training, showcasing its broader impact in overcoming persistent computational bottlenecks across various critical fields."
    },
    {
        "title": "A Causal Lens for Learning Long-term Fair Policies",
        "link_suffix": "/forum?id=rPkCVSsoM4",
        "link": "https://openreview.net/forum?id=rPkCVSsoM4",
        "pdf_link": "https://openreview.net/pdf?id=rPkCVSsoM4",
        "keywords": "long-term fairness, fair reinforcement learning, causal decomposition",
        "abstract": "Fairness-aware learning studies the development of algorithms that avoid discriminatory decision outcomes despite biased training data. While most studies have concentrated on immediate bias in static contexts, this paper highlights the importance of investigating long-term fairness in dynamic decision-making systems while simultaneously considering instantaneous fairness requirements. In the context of reinforcement learning, we propose a general framework where long-term fairness is measured by the difference in the average expected qualification gain that individuals from different groups could obtain. Then, through a causal lens, we decompose this metric into three components that represent the direct impact, the delayed impact, as well as the spurious effect the policy has on the qualification gain. We analyze the intrinsic connection between these components and an emerging fairness notion called benefit fairness that aims to control the equity of outcomes in decision-making. Finally, we develop a simple yet effective approach for balancing various fairness notions."
    },
    {
        "title": "Semantic Membership Inference Attack against Large Language Models",
        "link_suffix": "/forum?id=EwYUgKr9Fc",
        "link": "https://openreview.net/forum?id=EwYUgKr9Fc",
        "pdf_link": "https://openreview.net/pdf?id=EwYUgKr9Fc",
        "keywords": "Membership Inference Attack, Large Language Models",
        "abstract": "Membership Inference Attacks (MIAs) determine whether a specific data point was included in the training set of a target model. In this paper, we introduce the Semantic Membership Inference Attack (SMIA), a novel approach that enhances MIA performance by leveraging the semantic content of inputs and their perturbations. SMIA trains a neural network to analyze the target model\u2019s behavior on perturbed inputs, effectively capturing variations in output probability distributions between members and non-members. We conduct comprehensive evaluations on the Pythia and GPT-Neo model families using the Wikipedia and MIMIR datasets. Our results show that SMIA significantly outperforms existing MIAs; for instance, for Wikipedia, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B, compared to 58.90% by the second-best attack."
    },
    {
        "title": "Leveraging Low Rank Structure in The Lazy Regime",
        "link_suffix": "/forum?id=XgAKt7rbXk",
        "link": "https://openreview.net/forum?id=XgAKt7rbXk",
        "pdf_link": "https://openreview.net/pdf?id=XgAKt7rbXk",
        "keywords": "Lazy Regime, Linear Dynamics, Neural Tangent Kernel, Low Rank, Wide Neural Networks, Overparametrized Networks, Backpropagation",
        "abstract": "Understanding the training dynamics of neural networks has gained much interest in the scientific community. The dynamics of training over-parameterized models is characterized by the lazy regime in which networks exhibit near-linear behavior and minimal parameter changes. In addition, it has been argued that the Jacobian of large neural models has a low-rank structure. In this paper, we focus on the opportunities laid out by the combination of low-rankness and laziness of large neural models. Specifically, we provide a scalable way to measure the extent of laziness, evaluated via the rate of change of the model Jacobian, as well as a scalable method to verify low-rankness of the model Jacobian without storing the entire Jacobian. Taking advantages of both laziness and low-rankness, we design a scalable training algorithm for over-parameterized models that performs backpropagation-free gradient descend training. In particular, this algorithm is of lower computation and storage requirements in cases of massive parameter sharing, as is the case of many state-of-the-art neural architectures. Empirical results confirm the scalability and effectiveness of our approach, opening new pathways for exploring novel learning strategies in neural networks."
    },
    {
        "title": "Building Math Agents with Multi-Turn Iterative Preference Learning",
        "link_suffix": "/forum?id=WjKea8bGFF",
        "link": "https://openreview.net/forum?id=WjKea8bGFF",
        "pdf_link": "https://openreview.net/pdf?id=WjKea8bGFF",
        "keywords": "large language model, RLHF, math",
        "abstract": "Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH."
    },
    {
        "title": "POST: A Framework for Privacy of Soft-prompt Transfer",
        "link_suffix": "/forum?id=v6NNopExN4",
        "link": "https://openreview.net/forum?id=v6NNopExN4",
        "pdf_link": "https://openreview.net/pdf?id=v6NNopExN4",
        "keywords": "prompt transfer, soft prompt, privacy, distillation, confidentiality",
        "abstract": "Prompting has emerged as a dominant learning paradigm for adapting large language models (LLMs). While discrete (textual) prompts prepend tokens to the input for optimized outputs, soft (parameter) prompts are tuned in the embedding space via backpropagation, requiring less engineering effort. However, unlike semantically meaningful discrete prompts, soft prompts are tightly coupled to the LLM they were tuned on, hindering their generalization to other LLMs. This limitation is particularly problematic when efficiency and privacy are concerns, since (1) it requires tuning new prompts for each LLM which, due to the backpropagation, becomes increasingly computationally expensive as LLMs grow in size, and (2) when the LLM is centrally hosted, it requires sharing private data for soft prompt tuning with the LLM provider. To address these concerns, we propose a framework for Privacy Of Soft-prompt Transfer (POST), a novel method that enables private soft prompt tuning on a small language model and then transfers the prompt to the large LLM. Using knowledge distillation, we first derive the small language model directly from the LLM to facilitate prompt transferability. Then, we tune the soft prompt locally, if required with privacy guarantees, e.g., according to differential privacy. Finally, we use a small set of public data to transfer the prompt from the small model to the large LLM without additional privacy leakage. Our experimental results demonstrate that our method effectively transfers soft prompts, protecting local data privacy and reducing the computational complexity over soft prompt tuning on the large model."
    },
    {
        "title": "Looking into User\u2019s Long-term Interests through the Lens of Conservative Evidential Learning",
        "link_suffix": "/forum?id=o99Yn1wN9J",
        "link": "https://openreview.net/forum?id=o99Yn1wN9J",
        "pdf_link": "https://openreview.net/pdf?id=o99Yn1wN9J",
        "keywords": "recommender systems, evidence-aware exploration, evidential learning",
        "abstract": "Reinforcement learning (RL) been increasingly employed in modern recommender systems to capture users' evolving preferences, leading to continuously improved recommendations. In this paper, we propose a novel evidential conservative Q-learning framework (ECQL) that learns an effective and conservative recommendation policy by integrating evidence-based uncertainty and conservative learning. ECQL conducts evidence-aware explorations to discover items that are located beyond current observations but reflect users' long-term interests. It offers an uncertainty-aware conservative view on policy evaluation to discourage deviating too much from users' current interests. Two central components of ECQL include a uniquely designed sequential state encoder and a novel conservative evidential-actor-critic (CEAC) module. The former generates the current state of the environment by aggregating historical information and a sliding window that contains the current user interactions as well as newly recommended items from RL exploration that may represent future interests. The latter performs an evidence-based rating prediction by maximizing the conservative evidential Q-value and leverages an uncertainty-aware ranking score to explore the item space for a more diverse and valuable recommendation. Experiments on multiple real-world dynamic datasets demonstrate the state-of-the-art performance of ECQL and its capability to capture users' long-term interests."
    },
    {
        "title": "Sharpness Aware Minimization: General Analysis and Improved Rates",
        "link_suffix": "/forum?id=8rvqpiTTFv",
        "link": "https://openreview.net/forum?id=8rvqpiTTFv",
        "pdf_link": "https://openreview.net/pdf?id=8rvqpiTTFv",
        "keywords": "Sharpness-Aware Minimization, Convergence Guarantees, Non-Convex Optimization, Generalization in DNNs",
        "abstract": "Sharpness-Aware Minimization (SAM) has emerged as a powerful method for improving generalization in machine learning models by minimizing the sharpness of the loss landscape. However, despite its success, several important questions regarding the convergence properties of SAM in non-convex settings are still open, including the benefits of using normalization in the update rule, the dependence of the analysis on the restrictive bounded variance assumption, and the convergence guarantees under different sampling strategies. To address these questions, in this paper, we provide a unified analysis of SAM and its unnormalized variant (USAM) under one single flexible update rule (Unified SAM), and we present convergence results of the new algorithm under a relaxed and more natural assumption on the stochastic noise. Our analysis provides convergence guarantees for SAM under different step size selections for non-convex problems and functions that satisfy the Polyak-Lojasiewicz (PL) condition (a non-convex generalization of strongly convex functions). The proposed theory holds under the arbitrary sampling paradigm, which includes importance sampling as special case, allowing us to analyze variants of SAM that were never explicitly considered in the literature. Experiments validate the theoretical findings and further demonstrate the practical effectiveness of Unified SAM in training deep neural networks for image classification tasks."
    },
    {
        "title": "Learning to Steer Markovian Agents under Model Uncertainty",
        "link_suffix": "/forum?id=IzYczpPqKq",
        "link": "https://openreview.net/forum?id=IzYczpPqKq",
        "pdf_link": "https://openreview.net/pdf?id=IzYczpPqKq",
        "keywords": "Steering Learning Dynamics, Reinforcement Learning, Markov Games, Mechanism Design",
        "abstract": "Designing incentives for an adapting population is a ubiquitous problem in a wide array of economic applications and beyond. In this work, we study how to design additional rewards to steer multi-agent systems towards desired policies \\emph{without} prior knowledge of the agents' underlying learning dynamics. Motivated by the limitation of existing works, we consider a new and general category of learning dynamics called \\emph{Markovian agents}. We introduce a model-based non-episodic Reinforcement Learning (RL) formulation for our steering problem. Importantly, we focus on learning a \\emph{history-dependent} steering strategy to handle the inherent model uncertainty about the agents' learning dynamics. We introduce a novel objective function to encode the desiderata of achieving a good steering outcome with reasonable cost. Theoretically, we identify conditions for the existence of steering strategies to guide agents to the desired policies. Complementing our theoretical contributions, we provide empirical algorithms to approximately solve our objective, which effectively tackles the challenge in learning history-dependent strategies. We demonstrate the efficacy of our algorithms through empirical evaluations."
    }
]