[
    {
        "title": "ChartBench: A Benchmark for Complex Visual Reasoning in Charts",
        "link_suffix": "/forum?id=dd2CABUZaw",
        "link": "https://openreview.net/forum?id=dd2CABUZaw",
        "pdf_link": "https://openreview.net/pdf?id=dd2CABUZaw",
        "keywords": "Multimodal Large Language Models, Chart Reasoning, Chart Benchmark, Chain of Thought",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive capabilities in image understanding and generation. However, current benchmarks fail to accurately evaluate the chart comprehension of MLLMs due to limited chart types and inappropriate metrics. To address this, we propose ChartBench, a comprehensive benchmark designed to assess chart comprehension and data reliability through complex visual reasoning. ChartBench includes 42 categories, 66.6k charts, and 600k question-answer pairs. Notably, we do not provide data point annotations on charts explicitly, which requires models to derive values by leveraging inherent chart elements such as color, legends, and coordinate systems. We also design an enhanced evaluation metric named Acc++ to evaluate MLLMs without extensive manual or costly LLM-based evaluations. Furthermore, we propose two baselines based on the chain of thought and supervised fine-tuning to improve model performance on unannotated charts. Extensive experimental evaluations of 18 open-sourced and 3 proprietary MLLMs reveal their limitations in chart comprehension and offer valuable insights for further research."
    },
    {
        "title": "PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment",
        "link_suffix": "/forum?id=1kFDrYCuSu",
        "link": "https://openreview.net/forum?id=1kFDrYCuSu",
        "pdf_link": "https://openreview.net/pdf?id=1kFDrYCuSu",
        "keywords": "alignment, preference learning, foundation model, reward model, ideal point model, plurality",
        "abstract": "Foundation models trained on internet-scale data benefit from extensive alignment to human preferences before deployment. However, existing methods typically assume a homogeneous preference shared by all individuals, overlooking the diversity inherent in human values. In this work, we propose a general reward modeling framework for pluralistic alignment (PAL), which incorporates diverse preferences from the ground up. PAL has a modular design that leverages commonalities across users while catering to individual personalization, enabling efficient few-shot localization of preferences for new users. Extensive empirical evaluation demonstrates that PAL matches or outperforms state-of-the-art methods on both text-to-text and text-to-image tasks: on Reddit TL;DR Summary, PAL is 1.7% more accurate for seen users and 36% more accurate for unseen users compared to the previous best method, with 100\u00d7 less parameters. On Pick-a-Pic v2, PAL is 2.5% more accurate than the best method with 156\u00d7 fewer learned parameters. Finally, we provide theoretical analysis for generalization of rewards learned via PAL framework showcasing the reduction in number of samples needed per user."
    },
    {
        "title": "Dynamic Diffusion Transformer",
        "link_suffix": "/forum?id=taHwqSrbrb",
        "link": "https://openreview.net/forum?id=taHwqSrbrb",
        "pdf_link": "https://openreview.net/pdf?id=taHwqSrbrb",
        "keywords": "Diffusion Transformer, Dynamic Neural Network, Efficiency",
        "abstract": "Diffusion Transformer (DiT), an emerging diffusion model for image generation,\nhas demonstrated superior performance but suffers from substantial computational\ncosts. Our investigations reveal that these costs stem from the static inference\nparadigm, which inevitably introduces redundant computation in certain diffusion\ntimesteps and spatial regions. To address this inefficiency, we propose Dynamic\nDiffusion Transformer (DyDiT), an architecture that dynamically adjusts its compu-\ntation along both timestep and spatial dimensions during generation. Specifically,\nwe introduce a Timestep-wise Dynamic Width (TDW) approach that adapts model\nwidth conditioned on the generation timesteps. In addition, we design a Spatial-\nwise Dynamic Token (SDT) strategy to avoid redundant computation at unnecessary\nspatial locations. Extensive experiments on various datasets and different-sized\nmodels verify the superiority of DyDiT. Notably, with <3% additional fine-tuning it-\nerations, our method reduces the FLOPs of DiT-XL by 51%, accelerates generation\nby 1.73\u00d7, and achieves a competitive FID score of 2.07 on ImageNet."
    },
    {
        "title": "GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting",
        "link_suffix": "/forum?id=RMaRBE9s2H",
        "link": "https://openreview.net/forum?id=RMaRBE9s2H",
        "pdf_link": "https://openreview.net/pdf?id=RMaRBE9s2H",
        "keywords": "Gaussian Splatting, LiDAR Simulation",
        "abstract": "LiDAR novel view synthesis (NVS) has emerged as a novel task within LiDAR simulation, offering valuable simulated point cloud data from novel viewpoints to aid in autonomous driving systems. However, existing LiDAR NVS methods typically rely on neural radiance fields (NeRF) as their 3D representation, which incurs significant computational costs in both training and rendering. Moreover, NeRF and its variants are designed for symmetrical scenes, making them ill-suited for driving scenarios. To address these challenges, we propose GS-LiDAR, a novel framework for generating realistic LiDAR point clouds with panoramic Gaussian splatting. Our approach employs 2D Gaussian primitives with periodic vibration properties, allowing for precise geometric reconstruction of both static and dynamic elements in driving scenarios. We further introduce a novel panoramic rendering technique with explicit ray-splat intersection, guided by panoramic LiDAR supervision. By incorporating intensity and ray-drop spherical harmonic (SH) coefficients into the Gaussian primitives, we enhance the realism of the rendered point clouds. Extensive experiments on KITTI-360 and nuScenes demonstrate the superiority of our method in terms of quantitative metrics, visual quality, as well as training and rendering efficiency."
    },
    {
        "title": "Learning Dynamic 3D Gaussians from Monocular Videos without Camera Poses",
        "link_suffix": "/forum?id=xy9yv5siYQ",
        "link": "https://openreview.net/forum?id=xy9yv5siYQ",
        "pdf_link": "https://openreview.net/pdf?id=xy9yv5siYQ",
        "keywords": "Dynamic reconstruction, camera pose estimation",
        "abstract": "Dynamic scene reconstruction aims to recover the time-varying geometry and appearance of a dynamic scene. Existing methods, however, heavily rely on the existence of multiple-view captures or the accurate camera poses estimated by Structure from Motion (SfM) algorithms. To relax this constraint, we introduce a method capable of reconstructing generic dynamic scenes, from casually captured monocular videos without known camera poses. Unlike recent works that treat static and dynamic content separately, we propose a unified Hexplane-based Gaussian field to capture the complex effects of scene deformation and camera motion. The Hexplane decomposition enables feasible disentanglement for effective optimization. Combined with an efficient camera pose initialization strategy, our approach significantly improves view synthesis quality and camera pose estimation accuracy over previous methods, while enhancing computational efficiency."
    },
    {
        "title": "Brain-to-4D: 4D Generation from fMRI",
        "link_suffix": "/forum?id=6yQUfbACWX",
        "link": "https://openreview.net/forum?id=6yQUfbACWX",
        "pdf_link": "https://openreview.net/pdf?id=6yQUfbACWX",
        "keywords": "diffusion, neuroscience, fMRI, Gaussian Splatting",
        "abstract": "Brain-computer interface (BCI) with functional magnetic resonance imaging (fMRI) has enabled new communication interfaces for many real-world applications, e.g., fMRI to image or video. While useful for specific scenarios (e.g., neurofeedback), the existing functions are limited in offering immersive user experience as required by more complex applications (e.g., virtual reality). We thus propose Brain-to-4D, a more powerful yet challenging BCI function to construct 4D visuals including both video and 3D directly from brain fMRI signals. In reality, however, it is infeasible to acquire brain signals for multi-view 4D stimuli for training data collection due to the instantaneity nature of brain activities. Typically, brain fMRI data exhibit significantly large variation. To address both obstacles, we introduce WSf4D, a novel Weakly Supervised decomposed fMRI-to-4D generation approach, characterized by foreground-background decomposition for supervision dividing and fMRI multifaceted vector quantization for noise suppression. To explore the application of the new task Brain-to-4D and our solution WSf4D, we conduct analysis and diagnosis on various brain regions by encoding distinct visual cortex groups. Extensive experiments show that WSf4D can accurately generate multi-view consistent 4D scenes semantically aligned with raw brain signals, indicating meaningful advancements over existing approaches on the potentials of neuroscience and diagnosis."
    },
    {
        "title": "Data Selection via Optimal Control for Language Models",
        "link_suffix": "/forum?id=dhAL5fy8wS",
        "link": "https://openreview.net/forum?id=dhAL5fy8wS",
        "pdf_link": "https://openreview.net/pdf?id=dhAL5fy8wS",
        "keywords": "Pre-training Language Models, Data Selection, Optimal Control",
        "abstract": "This work investigates the selection of high-quality pre-training data from massive corpora to enhance LMs' capabilities for downstream usage. \nWe formulate data selection as a generalized Optimal Control problem, which can be solved theoretically by Pontryagin's Maximum Principle (PMP), yielding a set of necessary conditions that characterize the relationship between optimal data selection and LM training dynamics.\nBased on these theoretical results, we introduce PMP-based Data Selection (PDS), a framework that approximates optimal data selection by solving the PMP conditions. \nIn our experiments, we adopt PDS to select data from CommmonCrawl and show that the PDS-selected corpus accelerates the learning of LMs and constantly boosts their performance on a wide range of downstream tasks across various model sizes.\nMoreover, the benefits of PDS extend to ~400B models trained on ~10T tokens, as evidenced by the extrapolation of the test loss curves according to the Scaling Laws.\nPDS also improves data utilization when the pre-training data is limited, by reducing the data demand by 1.8 times, which mitigates the quick exhaustion of available web-crawled corpora. We will open-source our code, models, and data."
    },
    {
        "title": "Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox",
        "link_suffix": "/forum?id=ClkfwM3STw",
        "link": "https://openreview.net/forum?id=ClkfwM3STw",
        "pdf_link": "https://openreview.net/pdf?id=ClkfwM3STw",
        "keywords": "LLM, Quantization, Evaluation, OOD",
        "abstract": "Large language models (LLMs) have exhibited exciting progress in multiple scenarios, while the huge computational demands hinder their deployments in lots of real-world applications. As an effective means to reduce memory footprint and inference cost, quantization also faces challenges in performance degradation at low bit-widths. Understanding the impact of quantization on LLM capabilities, especially the generalization ability, is crucial. However, the community's main focus remains on the algorithms and models of quantization, with insufficient attention given to to the impact of data on the generalization abilities of quantized LLMs.\nIn this work, we fill this gap by providing a comprehensive benchmark suite for this research topic, including an evaluation system, detailed analyses, and a general toolbox. Specifically, based on the dominant pipeline in LLM quantization, we primarily explore the impact of calibration data distribution on the generalization of quantized LLMs and conduct the benchmark using more than 40 datasets within two main scenarios. Based on this benchmark, we conduct extensive experiments with well-known LLMs (LLaMA and Baichuan) and four quantization algorithms to investigate this topic in-depth, yielding several counter-intuitive and valuable findings, e.g., models quantized using a calibration set with the same distribution as the test data are not necessarily optimal. Besides, to facilitate future research, we also release a modular-designed toolbox, which decouples the overall pipeline into several separate components, e.g., base LLM module, dataset module, quantizer module, etc. and allows subsequent researchers to easily assemble their methods through a simple configuration. \nOur code is submitted in the supplementary materials and will be publicly available."
    },
    {
        "title": "SBGC: Bidirectional Graph Comparison-Based Self-Supervised Network for Change Detection in Heterogeneous Images",
        "link_suffix": "/forum?id=rss4mLJDpT",
        "link": "https://openreview.net/forum?id=rss4mLJDpT",
        "pdf_link": "https://openreview.net/pdf?id=rss4mLJDpT",
        "keywords": "Remote sensing, heterogeneous change detection, contrastive learning, bidirectional comparison",
        "abstract": "Change detection (CD) in heterogeneous images is a hot but highly challenging topic in the field of remote sensing. However, the significant imaging differences and varying visual appearances of heterogeneous images complicate the accurate detection of changes occurring on the land surface through direct comparison. To overcome this challenge, this paper proposes a self-supervised network based on bidirectional graph comparison (SBGC) for unsupervised heterogeneous CD, which exploits modality-independent structural relationships. First, pseudo-Siamese networks are established to extract discriminative and robust features from bi-temporal heterogeneous images based on self-supervised contrastive learning. Then, these learned features are utilized to construct graph structures that represent structural relationships. Second, we introduce bidirectional graph comparison to fully exploit the graph structures for exploring comprehensive change information. Specifically, we map the graph structures to their opposite image modality and perform a bidirectional comparison between the original and mapped graph structures to generate a difference image. Finally, the change map is obtained by applying the Otsu segmentation algorithm to the difference image. Experimental results on three public heterogeneous datasets with different modality combinations show that the proposed method achieves superior performance compared to seven state-of-the-art methods, achieving the best performance with an average overall accuracy of 96.69%."
    },
    {
        "title": "TAGA: Self-supervised Learning for Template-free Animatable Gaussian Avatars",
        "link_suffix": "/forum?id=47wXbygsvp",
        "link": "https://openreview.net/forum?id=47wXbygsvp",
        "pdf_link": "https://openreview.net/pdf?id=47wXbygsvp",
        "keywords": "Template-free avatar, Animatble Avatar, Gaussian Splatting, Self-supervised Learning",
        "abstract": "Decoupling from customized parametric templates marks an integral leap towards creating fully flexible, animatable avatars. In this work, we introduce TAGA (Template-free Animatable Gaussian Avatars), the first template-free, Gaussian-based solution for the reconstruction of animatable avatars from monocular videos, which offers distinct advantages in fast training and real-time rendering. Constructing template-free avatars is challenging due to the lack of predefined shapes and reliable skinning anchors to ensure consistent geometry and movement. TAGA addresses this by introducing a self-supervised method which guides both geometry and skinning learning leveraging the one-to-one correspondence between canonical and observation spaces. During the forward mapping phase, a voxel-based skinning field is introduced to learn smooth deformations that generalize to unseen poses. However, without template priors, forward mapping often captures spurious correlations of adjacent body parts, leading to unrealistic geometric artifacts in the canonical pose. To alleviate this, we define Gaussians with spurious correlations as \"Ambiguous Gaussians'' and then propose a new backward mapping strategy that integrates anomaly detection to identify and correct Ambiguous Gaussians. Compared to existing state-of-the-art template-free methods, TAGA achieves superior visual fidelity for novel views and poses, while being 60 $\\times$ faster in training (0.5 hours vs 30 hours) and 560 $\\times$ faster in rendering (140 FPS vs 0.25 FPS). Experiments on challenging datasets that possess limited pose diversity further demonstrate TAGA\u2019s robustness and generality. Code will be released."
    },
    {
        "title": "Less is More: Learning Reference Knowledge Using No-Reference Image Quality Assessment",
        "link_suffix": "/forum?id=wP0nDEAlap",
        "link": "https://openreview.net/forum?id=wP0nDEAlap",
        "pdf_link": "https://openreview.net/pdf?id=wP0nDEAlap",
        "keywords": "Image Quality Assessment, Inductive Bias Regularization, Reference Knowledge",
        "abstract": "Image Quality Assessment (IQA) with reference images has achieved great success by imitating the human vision system, in which the image quality is effectively assessed by comparing the query image with its pristine reference image. However, for the images in the wild, it is quite difficult to access accurate reference images. We argue that it is possible to learn reference knowledge under the \\emph{No-Reference Image Quality Assessment} (NR-IQA) setting, which is effective and efficient empirically. Concretely, by innovatively introducing a novel feature distillation method in IQA, we propose a new framework to learn comparative knowledge from non-aligned reference images. Then, we further propose inductive bias regularization to inject different inductive biases into the model to achieve fast convergence and avoid overfitting. Such a framework not only solves the congenital defects of NR-IQA but also improves the feature extraction framework, enabling it to express more abundant quality information. Surprisingly, our method utilizes less input\u2014eliminating the need for reference images during inference\u2014while obtaining more performance compared to some IQA methods that do require reference images. Comprehensive experiments on eight standard IQA datasets show that our approach outperforms state-of-the-art NR-IQA methods."
    },
    {
        "title": "LongVILA: Scaling Long-Context Visual Language Models for Long Videos",
        "link_suffix": "/forum?id=wCXAlfvCy6",
        "link": "https://openreview.net/forum?id=wCXAlfvCy6",
        "pdf_link": "https://openreview.net/pdf?id=wCXAlfvCy6",
        "keywords": "Large language models, Long context, Multi-modality, Video understanding",
        "abstract": "Long-context capability is critical for multi-modal foundation models, especially for long video understanding. We introduce LongVILA, a full-stack solution for long-context visual-language models \\qinghao{by co-designing the algorithm and system. For model training, we upgrade existing VLMs to support long video understanding by incorporating two additional stages, i.e., long context extension and long video supervised fine-tuning. However, training on long video is computationally and memory intensive. We introduce the long-context Multi-Modal Sequence Parallelism (MM-SP) system that efficiently parallelizes long video training and inference, enabling 2M context length training on 256 GPUs without any gradient checkpointing. LongVILA efficiently extends the number of video frames of VILA from 8 to 1024, improving the long video captioning score from 2.00 to 3.26 (out of 5), achieving 99.5% accuracy in 1400-frame (274k context length) video needle-in-a-haystack. LongVILA demonstrates strong accuracy on the VideoMME benchmark, i.e., 57.5% / 60.6% without/with subtitle. Besides, MM-SP is 2.1x - 5.7x faster than ring style sequence parallelism and 1.1x - 1.4x faster than Megatron with a hybrid context and tensor parallelism. Moreover, it seamlessly integrates with Hugging Face Transformers."
    },
    {
        "title": "McEval: Massively Multilingual Code Evaluation",
        "link_suffix": "/forum?id=UunCPtPOlZ",
        "link": "https://openreview.net/forum?id=UunCPtPOlZ",
        "pdf_link": "https://openreview.net/pdf?id=UunCPtPOlZ",
        "keywords": "Benchmark, Code Intelligence, Multilingual, Large Language Model, Multilingual Multitask Code Evaluation",
        "abstract": "Code large language models (LLMs) have shown remarkable advances in code understanding, completion, and generation tasks. Programming benchmarks, comprised of a selection of code challenges and corresponding test cases, serve as a standard to evaluate the capability of different LLMs in such tasks. However, most existing benchmarks primarily focus on Python and are still restricted to a limited number of languages, where other languages are translated from the Python samples (e.g. MultiPL-E) degrading the data diversity. To further facilitate the research of code LLMs, we propose a massively multilingual code benchmark covering 40 programming languages (McEval) with 16K test samples, which substantially pushes the limits of code LLMs in multilingual scenarios. The benchmark contains challenging code completion, understanding, and generation evaluation tasks with finely curated massively multilingual instruction corpora McEval-Instruct. In addition, we introduce an effective multilingual coder McEval trained on McEval-Instruct to support multilingual programming language generation. Extensive experimental results on McEval show that there is still a difficult journey between open-source models and closed-source LLMs (e.g. GPT-series models) in numerous languages."
    },
    {
        "title": "SEED-Story: Multimodal Long Story Generation with Large Language Model",
        "link_suffix": "/forum?id=yEPNPbF8E7",
        "link": "https://openreview.net/forum?id=yEPNPbF8E7",
        "pdf_link": "https://openreview.net/pdf?id=yEPNPbF8E7",
        "keywords": "LLM, Story telling, multi-modal generation",
        "abstract": "With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing narrative texts and vivid images in an interleaved manner, has emerged as a valuable and practical task with broad applications. However, this task poses significant challenges, as it necessitates the comprehension of the complex interplay between texts and images, and the ability to generate long sequences of coherent, contextually relevant texts and visuals. In this work, we propose SEED-Story, a novel method that leverages a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. Our model, built upon the powerful comprehension capability of MLLM, predicts text tokens as well as visual tokens, which are subsequently processed with an adapted visual de-tokenizer to produce images with consistent characters and styles. We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner. Additionally, we present a large-scale and high-resolution dataset named StoryStream for training our model and quantitatively evaluating the task of multimodal story generation in various aspects. \nAll models, codes and datasets are released inhttps://anonymous.4open.science/r/SEED-Story."
    },
    {
        "title": "Re-Evaluating the Impact of Unseen-Class Unlabeled Data on Semi-Supervised Learning Model",
        "link_suffix": "/forum?id=WPsnH6875d",
        "link": "https://openreview.net/forum?id=WPsnH6875d",
        "pdf_link": "https://openreview.net/pdf?id=WPsnH6875d",
        "keywords": "Safe Semi-Supervised Learning, Unseen-Class Unlabeled Data",
        "abstract": "Semi-supervised learning (SSL) effectively leverages unlabeled data and has been proven successful across various fields. Current safe SSL methods believe that unseen classes in unlabeled data harm the performance of SSL models. However, previous methods for assessing the impact of unseen classes on SSL model performance are flawed. They fix the size of the unlabeled dataset and adjust the proportion of unseen classes within the unlabeled data to assess the impact. This process contravenes the principle of controlling variables. Adjusting the proportion of unseen classes in unlabeled data alters the proportion of seen classes, meaning the decreased classification performance of seen classes may not be due to an increase in unseen class samples in the unlabeled data, but rather a decrease in seen class samples. Thus, the prior flawed assessment standard that \"unseen classes in unlabeled data can damage SSL model performance\" may not always hold true. This paper strictly adheres to the principle of controlling variables, maintaining the proportion of seen classes in unlabeled data while only changing the unseen classes across five critical dimensions, to investigate their impact on SSL models from global robustness and local robustness. Experiments demonstrate that unseen classes in unlabeled data do not necessarily impair the performance of SSL models; in fact, under certain conditions, unseen classes may even enhance them."
    },
    {
        "title": "ACT-IN-LLM: Adaptively Compression Vision Tokens in LLM for High-Resolution Multimodal Large Language Models",
        "link_suffix": "/forum?id=3Ofy2jNsNL",
        "link": "https://openreview.net/forum?id=3Ofy2jNsNL",
        "pdf_link": "https://openreview.net/pdf?id=3Ofy2jNsNL",
        "keywords": "Multimodal Large Language Models; High-resolution; Efficiency",
        "abstract": "High-resolution inputs empower Multimodal Large Language Models (MLLMs) to capture intricate visual details, thereby enhancing comprehension. However, the self-attention mechanism\u2019s quadratic complexity poses significant computational and memory challenges as image resolution increases, particularly with long-vision tokens. Existing approaches generally alleviate these issues by reducing vision tokens before feeding them into LLMs. Although efficient, this Pre-LLM compression strategy fails to match the performance of models utilizing all tokens, particularly on high-resolution benchmarks. Our experiments reveal that the performance gap arises from this strategy\u2019s limitation in selecting important visual tokens in early LLM layers, leading to the irretrievable loss of critical information. To overcome these challenges, we propose a new strategy that Adaptively Compresses vision Tokens within different LLM layers, named ACT-IN-LLM. Our innovative approach retains all tokens throughout the layers to ensure no vital information is lost while compressing key and value tokens in the self-attention mechanism, to reduce computational costs. The layer-wise compression of ACT-IN-LLM is guided by the interaction information between vision and text tokens, leading to more accurate selections. Our theoretical analysis and extensive experiments demonstrate the effectiveness of ACT-IN-LLM, showing a 6.3% improvement over existing token compression techniques. It also achieves the competitive performance with non-compression methods, while reducing training/inference time by \u223c 20% and vision tokens by \u223c 60%."
    },
    {
        "title": "Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models",
        "link_suffix": "/forum?id=1vggIT5vvj",
        "link": "https://openreview.net/forum?id=1vggIT5vvj",
        "pdf_link": "https://openreview.net/pdf?id=1vggIT5vvj",
        "keywords": "text-to-image diffusion model, diffusion model, text-to-image generative model, cross-attention",
        "abstract": "Recent text-to-image diffusion models leverage cross-attention layers, which have been effectively utilized to enhance a range of visual generative tasks. However, our understanding of cross-attention layers remains somewhat limited. In this study, we present a method for constructing Head Relevance Vectors (HRVs) that align with useful visual concepts. An HRV for a given visual concept is a vector with a length equal to the total number of cross-attention heads, where each element represents the importance of the corresponding head for the given visual concept. We develop and employ an ordered weakening analysis to demonstrate the effectiveness of HRVs as interpretable features. To demonstrate the utility of HRVs, we propose concept strengthening and concept adjusting methods and apply them to enhance three visual generative tasks. We show that misinterpretations of polysemous words in image generation can be corrected in most cases, five challenging attributes in image editing can be successfully modified, and catastrophic neglect in multi-concept generation can be mitigated. Overall, our work provides an advancement in understanding cross-attention layers and introduces new approaches for fine-controlling these layers at the head level."
    },
    {
        "title": "Ctrl123: Consistent Novel View Synthesis via Closed-Loop Transcription",
        "link_suffix": "/forum?id=CFOQd4tqn1",
        "link": "https://openreview.net/forum?id=CFOQd4tqn1",
        "pdf_link": "https://openreview.net/pdf?id=CFOQd4tqn1",
        "keywords": "Novel view synthesis, Diffusion Model, Closed-Loop Transcription",
        "abstract": "Based on the success of large image diffusion models, multi-view diffusion models have demonstrated remarkable zero-shot capability in novel view synthesis (NVS). However, the pioneering work Zero123 struggles to maintain consistency across generated multiple views. While recent modifications in model and training design have improved multi-view consistency, they often introduce new limitations, such as restricted fixed view generation or reliance on additional conditions. These constraints hinder the broader application of multi-view diffusion models in downstream tasks like 3D reconstruction. We identify the root cause of inconsistency as the excessive diversity inherent in generative models utilized for the NVS task. To address this, we aim to utilize the stronger supervise information to better alignment with ground truth images to constrain the diversity, and propose Ctrl123, aclosed-looptranscription-based multi-view diffusion method that enforces alignment in the CLIP patch feature space. Extensive experiments demonstrate that Ctrl123 excels inarbitrarynovel view generation, significantly improving multi-view consistency compared to existing methods."
    },
    {
        "title": "FormalAlign: Automated Alignment Evaluation for Autoformalization",
        "link_suffix": "/forum?id=B5RrIFMqbe",
        "link": "https://openreview.net/forum?id=B5RrIFMqbe",
        "pdf_link": "https://openreview.net/pdf?id=B5RrIFMqbe",
        "keywords": "Large Language models, Autoformalization, Lean 4, Formal Math, AI for Math",
        "abstract": "Autoformalization aims to convert informal mathematical proofs into machine-verifiable formats, bridging the gap between natural and formal languages. However, ensuring semantic alignment between the informal and formalized statements remains challenging. Existing approaches heavily rely on manual verification, hindering scalability. To address this, we introduce FormalAlign, a framework for automatically evaluating the alignment between natural and formal languages in autoformalization. FormalAlign trains on both the autoformalization sequence generation task and the representational alignment between input and output, employing a dual loss that combines a pair of mutually enhancing autoformalization and alignment tasks. Evaluated across four benchmarks augmented by our proposed misalignment strategies, FormalAlign demonstrates superior performance. In our experiments, FormalAlign outperforms GPT-4, achieving an Alignment-Selection Score 11.58% higher on \\forml-Basic (99.21% vs. 88.91%) and 3.19% higher on MiniF2F-Valid (66.39% vs. 64.34%). This effective alignment evaluation significantly reduces the need for manual verification."
    },
    {
        "title": "SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection",
        "link_suffix": "/forum?id=VHguhvcoM5",
        "link": "https://openreview.net/forum?id=VHguhvcoM5",
        "pdf_link": "https://openreview.net/pdf?id=VHguhvcoM5",
        "keywords": "LLM fine-tuning, LLM safety, bilevel optimization",
        "abstract": "Fine-tuning on task-specific data to boost downstream performance is a crucial step for leveraging Large Language Models (LLMs). However, though fine-tuning enhances the model performance for specialized applications, previous studies have demonstrated that fine-tuning the models on several adversarial samples or even benign data can greatly comprise the model's pre-equipped alignment and safety capabilities. In this work, we propose SEAL, a novel framework to enhance safety in LLM fine-tuning. SEAL learns a data ranker based on the bilevel optimization to up rank the safe and high-quality fine-tuning data and down rank the unsafe or low-quality ones. Models trained with SEAL demonstrate superior quality over multiple baselines, with 8.5% and 9.7% win rate increase compared to random selection respectively on Llama-3-8b-Instruct and Merlinite-7b models."
    },
    {
        "title": "Variational Rectified Flow Matching",
        "link_suffix": "/forum?id=1cM0yQe3pO",
        "link": "https://openreview.net/forum?id=1cM0yQe3pO",
        "pdf_link": "https://openreview.net/pdf?id=1cM0yQe3pO",
        "keywords": "Flow Matching, Diffusion Model, Generative Model",
        "abstract": "We study Variational Rectified Flow Matching, a framework that enhances classic rectified flow matching by modeling multi-modal velocity vector-fields. At inference time, classic rectified flow matching 'moves' samples from a source distribution to the target distribution by solving an ordinary differential equation via integration along a velocity vector-field. At training time, the velocity vector-field is learnt by linearly interpolating between coupled samples one drawn from the source and one drawn from the target distribution randomly. This leads to ''ground-truth'' velocity vector-fields that point in different directions at the same location, i.e., the velocity vector-fields are multi-modal/ambiguous. However, since training uses a standard mean-squared-error loss, the learnt velocity vector-field averages ''ground-truth'' directions and isn't multi-modal. Further, averaging leads to integration paths that are more curved while making it harder to fit the target distribution. In contrast, the studied variational rectified flow matching is able to capture the ambiguity in flow directions. We show on synthetic data, MNIST, and CIFAR-10 that the proposed variational rectified flow matching leads to compelling results with fewer integration steps."
    },
    {
        "title": "Learning Adaptive Lighting via Channel-Aware Guidance",
        "link_suffix": "/forum?id=Go21XxlFCp",
        "link": "https://openreview.net/forum?id=Go21XxlFCp",
        "pdf_link": "https://openreview.net/pdf?id=Go21XxlFCp",
        "keywords": "Exposure Correction, Tone Mapping, Adaptive Lighting, Unified framework, Image Retouching",
        "abstract": "Learning lighting adaption is a key step in obtaining a good visual perception and supporting downstream vision tasks. There are multiple light-related tasks (e.g., image retouching and exposure correction) and previous studies have mainly investigated these tasks individually. However, we observe that the light-related tasks share fundamental properties: i) different color channels have different light properties, and ii) the channel differences reflected in the time and frequency domains are different. Based on the common light property guidance, we propose a Learning Adaptive Lighting Network (LALNet), a unified framework capable of processing different light-related tasks. Specifically, we introduce the color-separated features that emphasize the light difference of different color channels and combine them with the traditional color-mixed features by Light Guided Attention (LGA). The LGA utilizes color-separated features to guide color-mixed features focusing on channel differences and ensuring visual consistency across channels. We introduce dual domain channel modulation to generate color-separated features and a wavelet followed by a vision state space module to generate color-mixed features. Extensive experiments on four representative light-related tasks demonstrate that LALNet significantly outperforms state-of-the-art methods on benchmark tests and requires fewer computational resources."
    },
    {
        "title": "A GENERALIZABLE AND EFFICIENT SYMBOLIC RE- GRESSION METHOD FOR TIME SERIES ANALYSIS",
        "link_suffix": "/forum?id=MZ1xgIBU3q",
        "link": "https://openreview.net/forum?id=MZ1xgIBU3q",
        "pdf_link": "https://openreview.net/pdf?id=MZ1xgIBU3q",
        "keywords": "Symbolic Regression, Analytical Expression, Neural-Enhanced MCTS, Time Series",
        "abstract": "The current popular time series analysis methods primarily focus on quantita- tive approaches, which typically offer accurate and diverse statistical indicators. However, these methods often fall short in elucidating the underlying evolution patterns of time series and providing intuitive and qualitative analysis. In this pa- per, we employ a reinforcement learning-inspired approach: using Monte-Carlo Tree Search (MCTS) as the foundation, we introduce symbolic regression tech- niques to derive explicit expressions for the non-linear dynamics in time series evolution. Considering the challenges of excessive randomness during the ac- tion selection phase and low efficiency during the simulation phase in MCTS, we integrate neural networks with MCTS, forming Neural-Enhanced Monte-Carlo Tree Search (NEMoTS) method. By leveraging the excellent fitting ablities of neural networks, we introduce priors for the action selection phase and directly replace the complex and time-consuming simulation process. This integration significantly enhances generalizability and computational efficiency in time series analysis based on symbolic regression. NEMoTS offers a qualitative and intu- itive approach to time series analysis. Experiments with six real-world datasets demonstrate that NEMoTS exhibits significant superiority in performance, effi- ciency, reliability, and interpretability."
    },
    {
        "title": "DEQuify your force field: Towards efficient simulations using deep equilibrium models",
        "link_suffix": "/forum?id=rynb4Vn8rb",
        "link": "https://openreview.net/forum?id=rynb4Vn8rb",
        "pdf_link": "https://openreview.net/pdf?id=rynb4Vn8rb",
        "keywords": "Machine Learning Force Fields, Deep Equilibrium Models",
        "abstract": "Machine learning force fields show great promise in enabling more accurate force fields than manually derived ones for molecular dynamics simulations. \nState-of-the-art approaches for ML force fields stack many equivariant graph neural network layers, resulting in long inference times and high memory costs. This work aims to improve these two aspects while simultaneously reaching higher accuracy.\nOur key observation is that successive states in molecular dynamics simulations are extremely similar, but typical architectures treat each step independently, disregarding this information.\nWe show how deep equilibrium models (DEQs) can exploit this temporal correlation by recycling neural network features from previous time steps. \nSpecifically, we turn a state-of-the-art force field architecture into a DEQ, enabling us to improve both accuracy and speed by $10%-20%$ on the MD17, MD22, and OC20 200k datasets. \nCompared to conventional approaches, DEQs are also naturally more memory efficient, facilitating the training of more expressive models on larger systems given limited GPU memory resources."
    },
    {
        "title": "SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation",
        "link_suffix": "/forum?id=UL8b54P96G",
        "link": "https://openreview.net/forum?id=UL8b54P96G",
        "pdf_link": "https://openreview.net/pdf?id=UL8b54P96G",
        "keywords": "video generation, complimentary learning system, slow-fast learning, diffusion",
        "abstract": "Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model's context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well."
    }
]