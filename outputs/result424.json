[
    {
        "title": "TeaserGen: Generating Teasers for Long Documentaries",
        "link_suffix": "/forum?id=G1n50BMqzm",
        "link": "https://openreview.net/forum?id=G1n50BMqzm",
        "pdf_link": "https://openreview.net/pdf?id=G1n50BMqzm",
        "keywords": "Teaser Generation, Multimodal Learning, Vision-Language Model",
        "abstract": "Teasers are an effective tool for promoting content in entertainment, commercial and educational fields. However, creating an effective teaser for long videos is challenging for it requires long-range multimodal modeling capability for the input videos, while necessitating maintaining audiovisual alignments, managing scene transitions and preserving factual accuracy for the output teasers. Due to the lack of a publicly-available dataset, progress along this research direction has been hindered. In this work, we present DocumentaryNet, a collection of 1,269 documentaries paired with their teasers, featuring multimodal data streams of video, speech, music, sound effects and narrations. With DocumentaryNet, we propose a new two-stage system for generating teasers from long documentaries. The proposed TeaserGen system first generates the teaser narration from the transcribed narration from the documentary using a pretrained large language model, and then selects the most relevant visual content to accompany the generated narration through language-vision models. For narration-video matching, we explore two approaches: a pretraining-based model using pretrained contrastive language-vision models and a deep sequential model that learns the mapping between the narrations and visuals. Our experimental results show that the pretraining-based approach is more effective at identifying relevant visual content than directly trained deep autoregressive models."
    },
    {
        "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
        "link_suffix": "/forum?id=Z4evOUYrk7",
        "link": "https://openreview.net/forum?id=Z4evOUYrk7",
        "pdf_link": "https://openreview.net/pdf?id=Z4evOUYrk7",
        "keywords": "camera viewpoints control in Video Generation",
        "abstract": "Controllability plays a crucial role in video generation, as it allows users to create and edit content more precisely. Existing models, however, lack control of camera pose that serves as a cinematic language to express deeper narrative nuances. To alleviate this issue, we introduce CameraCtrl, enabling accurate camera pose control for video diffusion models. Our approach explores effective camera trajectory parameterization along with a plug-and-play camera pose control module that is trained on top of a video diffusion model, leaving other modules of the base model untouched. Moreover, a comprehensive study on the effect of various training datasets is conducted, suggesting that videos with diverse camera distributions and similar appearance to the base model indeed enhance controllability and generalization. Experimental results demonstrate the effectiveness of CameraCtrl in achieving precise camera control with different video generation models, marking a step forward in the pursuit of dynamic and customized video storytelling from textual and camera pose inputs. Code and models will be made publicly available."
    },
    {
        "title": "Identifying and Addressing Delusions for Target-Directed Decision Making",
        "link_suffix": "/forum?id=iMI4HRpZFc",
        "link": "https://openreview.net/forum?id=iMI4HRpZFc",
        "pdf_link": "https://openreview.net/pdf?id=iMI4HRpZFc",
        "keywords": "delusions, hallucination, planning, generalization, reinforcement learning",
        "abstract": "We are interested in target-directed agents, which produce targets during decision-time planning, to guide their behaviors and achieve better generalization during evaluation. Improper training of these agents can result in delusions: the agent may come to hold false beliefs about the targets, which cannot be properly rejected, leading to unwanted behaviors and damaging out-of-distribution generalization. We identify different types of delusions by using intuitive examples in carefully controlled environments, and investigate their causes. We demonstrate how delusions can be addressed for agents trained by hindsight relabeling, a mainstream approach in for training target-directed RL agents. We validate empirically the effectiveness of the proposed solutions in correcting delusional behaviors and improving out-of-distribution generalization."
    },
    {
        "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
        "link_suffix": "/forum?id=IcPkW3QNW2",
        "link": "https://openreview.net/forum?id=IcPkW3QNW2",
        "pdf_link": "https://openreview.net/pdf?id=IcPkW3QNW2",
        "keywords": "Gaussian Splatting, Multi-View Depth, Monocular Depth",
        "abstract": "Gaussian splatting and single/multi-view depth estimation are typically studied in isolation. In this paper, we present DepthSplat to connect Gaussian splatting and depth estimation and study their interactions. More specifically, we first contribute a robust multi-view depth model by leveraging pre-trained monocular depth features, leading to high-quality feed-forward 3D Gaussian splatting reconstructions. We also show that Gaussian splatting can serve as an unsupervised pre-training objective for learning powerful depth models from large-scale unlabelled datasets. We validate the synergy between Gaussian splatting and depth estimation through extensive ablation and cross-task transfer experiments. Our DepthSplat achieves state-of-the-art performance on ScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and novel view synthesis, demonstrating the mutual benefits of connecting both tasks. We invite the readers to view our supplementary video for feed-forward reconstruction results of large-scale or 360 scenes. Our code and models will be publicly available."
    },
    {
        "title": "SSOLE: Rethinking Orthogonal Low-rank Embedding for Self-Supervised Learning",
        "link_suffix": "/forum?id=zBgiCWCxJB",
        "link": "https://openreview.net/forum?id=zBgiCWCxJB",
        "pdf_link": "https://openreview.net/pdf?id=zBgiCWCxJB",
        "keywords": "self-supervised learning, orthogonal low-rank embedding",
        "abstract": "Self-supervised learning (SSL) aims to learn meaningful representations from unlabeled data. Orthogonal Low-rank Embedding (OLE) shows promise for SSL by enhancing intra-class similarity in a low-rank subspace and promoting inter-class dissimilarity in a high-rank subspace, making it particularly suitable for multi-view learning tasks. However, directly applying OLE to SSL poses significant challenges: (1) the virtually infinite number of \"classes\" in SSL makes achieving the OLE objective impractical, leading to representational collapse; and (2) low-rank constraints may fail to distinguish between positively and negatively correlated features, further undermining learning. To address these issues, we propose SSOLE (Self-Supervised Orthogonal Low-rank Embedding), a novel framework that integrates OLE principles into SSL by (1) decoupling the low-rank and high-rank enforcement to align with SSL objectives; and (2) applying low-rank constraints to feature deviations from their mean, ensuring better alignment of positive pairs by accounting for the signs of cosine similarities. Our theoretical analysis and empirical results demonstrate that these adaptations are crucial to SSOLE\u2019s effectiveness. Moreover, SSOLE achieves competitive performance across SSL benchmarks without relying on large batch sizes, memory banks, or dual-encoder architectures, making it an efficient and scalable solution for self-supervised tasks."
    },
    {
        "title": "3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting",
        "link_suffix": "/forum?id=iKDbLpVgQc",
        "link": "https://openreview.net/forum?id=iKDbLpVgQc",
        "pdf_link": "https://openreview.net/pdf?id=iKDbLpVgQc",
        "keywords": "image editting, gaussian splatting, 3D",
        "abstract": "Scene image editing is crucial for entertainment, photography, and advertising design. Existing methods solely focus on either 2D individual object or 3D global scene editing. This results in a lack of a unified approach to effectively control and manipulate scenes at the 3D level with different levels of granularity. In this work, we propose 3DitScene, a novel and unified scene editing framework leveraging language-guided disentangled Gaussian Splatting that enables seamless editing from 2D to 3D, allowing precise control over scene composition and individual objects. We first incorporate 3D Gaussians that are refined through generative priors and optimization techniques. Language features from CLIP then introduce semantics into 3D geometry for object disentanglement. With the disentangled Gaussians, 3DitScene allows for manipulation at both the global and individual levels, revolutionizing creative expression and empowering control over scenes and objects. Experimental results demonstrate the effectiveness and versatility of 3DitScene in scene image editing."
    },
    {
        "title": "The FIX Benchmark: Extracting Features Interpretable to eXperts",
        "link_suffix": "/forum?id=MsAglk31tQ",
        "link": "https://openreview.net/forum?id=MsAglk31tQ",
        "pdf_link": "https://openreview.net/pdf?id=MsAglk31tQ",
        "keywords": "Interpretable Features, Explainability",
        "abstract": "Feature-based methods are commonly used to explain model predictions, but these methods often implicitly assume that interpretable features are readily available. However, this is often not the case for high-dimensional data, and it can be hard even for domain experts to mathematically specify which features are important. Can we instead automatically extract collections or groups of features that are aligned with expert knowledge? To address this gap, we present FIX (Features Interpretable to eXperts), a benchmark for measuring how well a collection of features aligns with expert knowledge. In collaboration with domain experts, we propose FIXScore, a unified expert alignment measure applicable to diverse real-world settings across cosmology, psychology, and medicine domains in vision, language and time series data modalities. With FIXScore, we find that popular feature-based explanation methods have poor alignment with expert-specified knowledge, highlighting the need for new methods that can better identify features interpretable to experts."
    },
    {
        "title": "CoMotion: Concurrent Multi-person 3D Motion",
        "link_suffix": "/forum?id=qKu6KWPgxt",
        "link": "https://openreview.net/forum?id=qKu6KWPgxt",
        "pdf_link": "https://openreview.net/pdf?id=qKu6KWPgxt",
        "keywords": "human pose estimation, 3d human pose, tracking",
        "abstract": "We introduce an approach for tracking detailed 3D poses of multiple people from a single monocular camera stream. Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions. Rather than detect poses and associate them to current tracks, our model directly updates all tracked poses simultaneously given a new input image. We train on numerous single-image and video datasets with both 2D and 3D annotations to produce a model that matches the 3D pose estimation quality of state-of-the-art systems while performing faster and more accurate tracking on in-the-wild videos."
    },
    {
        "title": "Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning",
        "link_suffix": "/forum?id=xoIeVdFO7U",
        "link": "https://openreview.net/forum?id=xoIeVdFO7U",
        "pdf_link": "https://openreview.net/pdf?id=xoIeVdFO7U",
        "keywords": "unsupervised learning, reinforcement learning, mutual information, successor feature",
        "abstract": "Self-supervised learning has the potential of lifting several of the key challenges in reinforcement learning today, such as exploration, representation learning, and reward design. Recent work (METRA) has effectively argued that moving away from mutual information and instead optimizing a certain Wasserstein distance is important for good performance. In this paper, we argue that the benefits seen in that paper can largely be explained within the existing framework of mutual information skill learning (MISL).\nOur analysis suggests a new MISL method (contrastive successor features) that retains the excellent performance of METRA with fewer moving parts, and highlights connections between skill learning, contrastive representation learning, and successor features. Finally, through careful ablation studies, we provide further insight into some of the key ingredients for both our method and METRA."
    },
    {
        "title": "Zero-Order Diffusion Guidance for Inverse Problems",
        "link_suffix": "/forum?id=JBgBrnhLLL",
        "link": "https://openreview.net/forum?id=JBgBrnhLLL",
        "pdf_link": "https://openreview.net/pdf?id=JBgBrnhLLL",
        "keywords": "Privacy, Zero-Order guidance, Diffusion Models, Inverse Problems",
        "abstract": "We propose zero order diffusion guidance, a method that allows using a diffusion model to solve inverse problems without access to the gradients of the process we seek to invert. Our method employs a zero-order gradient estimator combined with a novel differentiable dimensionality reduction strategy to approximate true gradients during guidance while keeping the task computationally tractable in thousands of dimensions. We apply our method to model inversion and demonstrate how it can be used to reconstruct high-quality faces in a realistic scenario where the adversary has only black-box access to face embeddings. Across a range of inverse problems\u2014including synthetic experiments and JPEG restoration\u2014we show that access to gradients is not necessary for effective guidance. Our black-box method matches white-box performance, thus expanding the scope of inverse problems that can be solved with diffusion-based approaches."
    },
    {
        "title": "RelitLRM: Generative Relightable Radiance for Large Reconstruction Models",
        "link_suffix": "/forum?id=3Oli4u6q3p",
        "link": "https://openreview.net/forum?id=3Oli4u6q3p",
        "pdf_link": "https://openreview.net/pdf?id=3Oli4u6q3p",
        "keywords": "Relightable reconstruction, Inverse Rendering, Generative Relighting",
        "abstract": "We propose RelitLRM, a Large Reconstruction Model (LRM) for generating high-quality Gaussian splatting representations of 3D objects under novel illuminations from sparse (4-8) posed images captured under unknown static lighting. Unlike prior inverse rendering methods requiring dense captures and slow optimization, often causing artifacts like incorrect highlights or shadow baking, RelitLRM adopts a feed-forward transformer-based model with a novel combination of a geometry reconstructor and a relightable appearance generator based on diffusion. The model is trained end-to-end on synthetic multi-view renderings of objects under varying known illuminations. This architecture design enables to effectively decompose geometry and appearance, resolve the ambiguity between material and lighting, and capture the multi-modal distribution of shadows and specularity in the relit appearance. We show our sparse-view feed-forward RelitLRM offers competitive relighting results to state-of-the-art dense-view optimization-based baselines while being significantly faster. Our project page is available at:https://relitlrm.github.io/."
    },
    {
        "title": "Scaling Optimal LR Across Token Horizons",
        "link_suffix": "/forum?id=WYL4eFLcxG",
        "link": "https://openreview.net/forum?id=WYL4eFLcxG",
        "pdf_link": "https://openreview.net/pdf?id=WYL4eFLcxG",
        "keywords": "LLMs, scaling laws, hyperparameters, mup",
        "abstract": "State-of-the-art LLMs are powered by scaling -- scaling model size, dataset size, and cluster size. It is economically infeasible to extensively tune hyperparameters for the largest runs. Instead, approximately optimal hyperparameters must be inferred or transferred from smaller experiments. Hyperparameter transfer across model sizes has been studied in Yang et. al. However, hyperparameter transfer across dataset size -- or token horizon -- has not been studied yet. To remedy this we conduct a large-scale empirical study on how optimal learning rate (LR) depends on the token horizon in LLM training. We first demonstrate that the optimal LR changes significantly with token horizon -- longer training necessitates smaller LR. Secondly, we demonstrate that the optimal LR follows a scaling law and that the optimal LR for longer horizons can be accurately estimated from shorter horizons via such scaling laws. We also provide a rule-of-thumb for transferring LR across token horizons with zero overhead over current practices. Lastly, we provide evidence that LLama-1 used too high LR, and argue that hyperparameter transfer across data size is an overlooked component of LLM training."
    },
    {
        "title": "Stochastic Deep Restoration Priors for Imaging Inverse Problems",
        "link_suffix": "/forum?id=O2aioX2Z2v",
        "link": "https://openreview.net/forum?id=O2aioX2Z2v",
        "pdf_link": "https://openreview.net/pdf?id=O2aioX2Z2v",
        "keywords": "computational imaging, inverse problems, deep learning",
        "abstract": "Deep neural networks trained as image denoisers are widely used as priors for solving imaging inverse problems. While Gaussian denoising is thought sufficient for learning image priors, we show that priors from deep models pre-trained as more general restoration operators can perform better. We introduce Stochastic deep Restoration Priors (ShaRP), a novel method that leverages an ensemble of such restoration models to regularize inverse problems. ShaRP improves upon methods using Gaussian denoiser priors by better handling structured artifacts and enabling self-supervised training even without fully sampled data. We prove ShaRP minimizes an objective function involving a regularizer derived from the score functions of minimum mean square error (MMSE) restoration operators, and theoretically analyze its convergence. Empirically, ShaRP achieves state-of-the-art performance on tasks such as magnetic resonance imaging reconstruction and single-image super-resolution,  surpassing both denoiser- and diffusion-model-based methods without requiring retraining."
    },
    {
        "title": "DiffBody: Human Body Restoration by Imagining with Generative Diffusion Prior",
        "link_suffix": "/forum?id=R8t9Q3jmCQ",
        "link": "https://openreview.net/forum?id=R8t9Q3jmCQ",
        "pdf_link": "https://openreview.net/pdf?id=R8t9Q3jmCQ",
        "keywords": "Diffusion Models",
        "abstract": "Human body restoration plays a vital role in various applications related to the human body.\nDespite recent advances in general image restoration using generative models, their performance in human body restoration remains mediocre, often resulting in foreground and background blending, over-smoothing surface textures, missing accessories, and distorted limbs. \nAddressing these challenges, we propose a novel approach by constructing a human body-aware diffusion model that leverages domain-specific knowledge to enhance performance. \nSpecifically, we employ a pretrained body attention module to guide the diffusion model's focus on the foreground, addressing issues caused by blending between the subject and background.\nWe also demonstrate the value of revisiting the language modality of the diffusion model in restoration tasks by seamlessly incorporating text prompt to improve the quality of surface texture and additional clothing and accessories details.\nAdditionally, we introduce a diffusion sampler tailored for fine-grained human body parts, utilizing local semantic information to rectify limb distortions.\nLastly, we collect a comprehensive dataset for benchmarking and advancing the field of human body restoration.\nExtensive experimental validation showcases the superiority of our approach, both quantitatively and qualitatively, over existing methods."
    },
    {
        "title": "A Geometric Framework for Understanding Memorization in Generative Models",
        "link_suffix": "/forum?id=aZ1gNJu8wO",
        "link": "https://openreview.net/forum?id=aZ1gNJu8wO",
        "pdf_link": "https://openreview.net/pdf?id=aZ1gNJu8wO",
        "keywords": "deep generative modelling, generative models, memorization, data copying, privacy, diffusion, diffusion models, GANs, manifold hypothesis, local intrinsic dimension, lid, lid estimation, geometry",
        "abstract": "As deep generative models have progressed, recent work has shown that they are capable of memorizing and reproducing training datapoints when deployed. These findings call into question the usability of generative models, especially in light of the legal and privacy risks brought about by memorization. To better understand this phenomenon, we propose themanifold memorization hypothesis(MMH), a geometric framework which leverages the manifold hypothesis into a clear language in which to reason about memorization. We propose to analyze memorization in terms of the relationship between the dimensionalities of $(i)$ the ground truth data manifold and $(ii)$ the manifold learned by the model. This framework provides a formal standard for \"how memorized'' a datapoint is and systematically categorizes memorized data into two types: memorization driven by overfitting and memorization driven by the underlying data distribution. By analyzing prior work in the context of the MMH, we explain and unify assorted observations in the literature. We empirically validate the MMH using synthetic data and  image datasets up to the scale of Stable Diffusion, developing new tools for detecting and preventing generation of memorized samples in the process."
    },
    {
        "title": "Order-aware Interactive Segmentation",
        "link_suffix": "/forum?id=8ZLzw5pIrc",
        "link": "https://openreview.net/forum?id=8ZLzw5pIrc",
        "pdf_link": "https://openreview.net/pdf?id=8ZLzw5pIrc",
        "keywords": "Interactive Segmentation, Image Segmentation",
        "abstract": "Interactive segmentation aims to accurately segment target objects with minimal user interactions. However, current methods often fail to accurately separate target objects from the background, due to a limited understanding of order, the relative depth between objects in a scene. To address this issue, we propose OIS: order-aware interactive segmentation, where we explicitly encode the relative depth between objects into order maps. We introduce a novel order-aware attention, where the order maps seamlessly guide the user interactions (in the form of clicks) to attend to the image features. We further present an object-aware attention module to incorporate a strong object-level understanding to better differentiate objects with similar order.  Our approach allows both dense and sparse integration of user clicks, enhancing both accuracy and efficiency as compared to prior works.  Experimental results demonstrate that OIS achieves state-of-the-art performance, improving mIoU after one click by 7.61 on the HQSeg44K dataset and 1.32 on the DAVIS dataset as compared to the previous state-of-the-art SegNext, while also doubling inference speed compared to current leading methods."
    },
    {
        "title": "Object-aware lifting for 3D scene  segmentation in Gaussian splatting",
        "link_suffix": "/forum?id=CGT0T9uUOY",
        "link": "https://openreview.net/forum?id=CGT0T9uUOY",
        "pdf_link": "https://openreview.net/pdf?id=CGT0T9uUOY",
        "keywords": "3D-GS, 3D scene segmentation, Lifting",
        "abstract": "Lifting is an effective technique for producing a 3D scene segmentation by unprojecting multi-view 2D instance segmentations into a common 3D space. Existing state-of-the-art lifting methods leverage contrastive learning to learn a feature field, but rely on a hyperparameter-sensitive and error-prone clustering post-process for segmentation prediction, leading to inferior performance. In this paper, we propose a new unified \\textit{object-aware lifting}  approach in a 3D Gaussian Splatting field, introducing a novel learnable \\textit{object-level codebook} to account for objects in the 3D scene for an explicit object-level understanding. To start, we augment each Gaussian point with an additional Gaussian-level feature learned using a contrastive loss. More importantly, enabled by our object-level codebook formulation, we associate the encoded object-level features with Gaussian-level point features for segmentation predictions. Further, we design two novel modules, the association learning module and the noisy label filtering module, to achieve effective and robust codebook learning. We conduct experiments on three benchmarks,~\\ie, LERF-Masked, Replica, and Messy Rooms datasets. Both qualitative and quantitative results manifest that our new approach significantly outperforms the existing methods in terms of segmentation quality and time efficiency."
    },
    {
        "title": "HOME-3: HIGH-ORDER MOMENTUM ESTIMATOR USING THIRD-POWER GRADIENT FOR CONVEX, SMOOTH NONCONVEX, AND NONSMOOTH NONCONVEX OPTIMIZATION",
        "link_suffix": "/forum?id=s7Rc5KPqYI",
        "link": "https://openreview.net/forum?id=s7Rc5KPqYI",
        "pdf_link": "https://openreview.net/pdf?id=s7Rc5KPqYI",
        "keywords": "Gradient Descent, Momentum, High-Power Gradient, Nonconvex, Coordinate Randomization",
        "abstract": "Momentum-based gradients are critical for optimizing advanced machine learning models, as they not only accelerate convergence but also help gradient-based optimizers overcome stationary points. While most state-of-the-art momentum techniques rely on lower-power gradients, such as the squared first-order gradient, there has been limited exploration into the potential of higher-power gradients\u2014those raised to powers greater than two, such as the third-power first-order gradient. In this work, we introduce the concept of high-order momentum, where\nmomentum is constructed using higher-power gradients, with a specific focus on the third-power first-order gradient as a representative example. Our research offers both theoretical and empirical evidence of the benefits of this novel approach. From a theoretical standpoint, we demonstrate that incorporating third-power gradients into momentum can improve the convergence bounds of gradient-based optimizers\nfor both convex and smooth nonconvex problems. To validate these findings, we conducted extensive empirical experiments across convex, smooth nonconvex, and nonsmooth nonconvex optimization tasks. The results consistently showcase that high-order momentum outperforms traditional momentum-based optimizers, providing superior performance and more efficient optimization."
    },
    {
        "title": "Re-Imagining Multimodal Instruction Tuning: A Representation View",
        "link_suffix": "/forum?id=zxg6601zoc",
        "link": "https://openreview.net/forum?id=zxg6601zoc",
        "pdf_link": "https://openreview.net/pdf?id=zxg6601zoc",
        "keywords": "Representation Tuning, Large Multimodal Models, Parameter-efficient Fine-tuning",
        "abstract": "Multimodal instruction tuning has proven to be an effective strategy for achieving zero-shot generalization by fine-tuning pre-trained Large Multimodal Models (LMMs) with instruction-following data. However, as the scale of LMMs continues to grow, fully fine-tuning these models has become highly parameter-intensive. Although Parameter-Efficient Fine-Tuning (PEFT) methods have been introduced to reduce the number of tunable parameters, a significant performance gap remains compared to full fine-tuning. Furthermore, existing PEFT approaches are often highly parameterized, making them difficult to interpret and control. In light of this, we introduce Multimodal Representation Tuning (MRT), a novel approach that focuses on directly editing semantically rich multimodal representations to achieve strong performance and provide intuitive control over LMMs. Empirical results show that our method surpasses current state-of-the-art baselines with significant performance gains (e.g., 1580.40 MME score) while requiring substantially fewer tunable parameters (e.g., 0.03% parameters). Additionally, we conduct experiments on editing instrumental tokens within multimodal representations, demonstrating that direct manipulation of these representations enables simple yet effective control over network behavior."
    },
    {
        "title": "Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control",
        "link_suffix": "/forum?id=xQBRrtQM8u",
        "link": "https://openreview.net/forum?id=xQBRrtQM8u",
        "pdf_link": "https://openreview.net/pdf?id=xQBRrtQM8u",
        "keywords": "Reward fine-tuning, stochastic optimal control, flow matching, diffusion models, RLHF, adjoint method",
        "abstract": "Dynamical generative models that produce samples through an iterative process, such as Flow Matching and denoising diffusion models, have seen widespread use, but there have not been many theoretically-sound methods for improving these models with reward fine-tuning. In this work, we cast reward fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specificmemorylessnoise schedule must be enforced during fine-tuning, in order to account for the dependency between the noise variable and the generated samples. We also propose a new algorithm namedAdjoint Matchingwhich outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We find that our approach significantly improves over existing methods for reward fine-tuning, achieving better consistency, realism, and generalization to unseen human preference reward models, while retaining sample diversity."
    },
    {
        "title": "Iterative DPO with An Improvement Model for Fine-tuning Diffusion Models",
        "link_suffix": "/forum?id=KJF3h0OpQ7",
        "link": "https://openreview.net/forum?id=KJF3h0OpQ7",
        "pdf_link": "https://openreview.net/pdf?id=KJF3h0OpQ7",
        "keywords": "DPO, diffusion models",
        "abstract": "Direct Preference Optimization (DPO) has been proven as an effective solution in aligning generative models with human preferences. However, as shown in recent works, DPO could suffer from constraints from the offline preference dataset. This paper introduces a novel improvement approach for online iterative optimization of the diffusion models without introducing extra annotation of the online data. We propose to learn a preference improvement model to extract the implicit preference from the preference dataset. The learned improvement model is then used to generate winning images from the images generated by the current diffusion model. We can construct new pairs of preference data by using images generated by the current diffusion model as losing images, and its corresponding improved images as winning images. The diffusion model can therefore be optimized via iteratively applying online preference datasets. This method enables online improvement beyond offline DPO training without requiring additional human labeling or risking overfitting the reward model. Results demonstrate improvements in preference alignment with higher diversity compared with other fine-tuning methods. Our work bridges the gap between offline preference learning and online improvement, offering a promising direction for enhancing diffusion models in image generation tasks with limited preference data."
    },
    {
        "title": "Looped Transformers for Length Generalization",
        "link_suffix": "/forum?id=2edigk8yoU",
        "link": "https://openreview.net/forum?id=2edigk8yoU",
        "pdf_link": "https://openreview.net/pdf?id=2edigk8yoU",
        "keywords": "Transformers",
        "abstract": "Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation\u2014a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks."
    },
    {
        "title": "Exploring channel distinguishability in local neighborhoods of the model space in quantum neural networks",
        "link_suffix": "/forum?id=gDcL7cgZBt",
        "link": "https://openreview.net/forum?id=gDcL7cgZBt",
        "pdf_link": "https://openreview.net/pdf?id=gDcL7cgZBt",
        "keywords": "Quantum Machine Learning, Quantum Neural Network",
        "abstract": "With the increasing interest in Quantum Machine Learning, Quantum Neural Networks (QNNs) have emerged and gained significant attention. These models have, however, been shown to be notoriously difficult to train, which we hypothesize is partially due to the architectures, called ansatzes, that are hardly studied at this point. Therefore, in this paper, we take a step back and analyze ansatzes. We initially consider their expressivity, i.e., the space of operations they are able to express, and show that the closeness to being a 2-design, the primarily used measure, fails at capturing this property. Hence, we look for alternative ways to characterize ansatzes by considering the local neighborhood of the model space, in particular, analyzing model distinguishability upon small perturbation of parameters. We derive an upper bound on their distinguishability, showcasing that QNNs with few parameters are hardly discriminable upon update. Our numerical experiments support our bounds and further indicate that there is a significant degree of variability, which stresses the need for warm-starting or clever initialization. Altogether, our work provides an ansatz-centric perspective on training dynamics and difficulties in QNNs, ultimately suggesting that iterative training of small quantum models may not be effective, which contrasts their initial motivation."
    },
    {
        "title": "ClimGen: Learning the Forcing-Response Relationship in Climate System",
        "link_suffix": "/forum?id=sELO2DCCC1",
        "link": "https://openreview.net/forum?id=sELO2DCCC1",
        "pdf_link": "https://openreview.net/pdf?id=sELO2DCCC1",
        "keywords": "GenAI, c-DDPM, Climate, Forcing-Response relationship",
        "abstract": "Solar Radiation Management (SRM) is emerging as a viable geoengineering strategy to address the climate change crisis, but its effective implementation requires iterative and large ensemble of highly accurate and efficient climate projections. Traditional climate projections rely on executing computational demanding and time-consuming numerical climate models. Recent advances in machine learning (ML) aim to enhance these approaches by emulating traditional methods. In this work, we propose a novel framework for directly learning the relationship between solar radiation flux at the top of the atmosphere and the corresponding surface temperature response.To evaluate the feasibility of this direct ML-based projection, we developed a benchmark dataset using an intermediate complexity model, incorporating a comprehensive suite of different forcing patterns and evaluation metrics to rigorously assess the ML model\u2019s performance. We introduce a Conditional Denoising Diffusion Probabilistic Model (c-DDPM) for this task, which demonstrates superior performance in representing climate statistics under previously unseen forcing patterns. This approach provides a promising pathway for direct climate projections by accurately learning the forcing-response relationship, with wide range of applications in climate change mitigation, emissions policy design, and SRM strategies."
    },
    {
        "title": "When Selection meets Intervention: Additional Complexities in Causal Discovery",
        "link_suffix": "/forum?id=xByvdb3DCm",
        "link": "https://openreview.net/forum?id=xByvdb3DCm",
        "pdf_link": "https://openreview.net/pdf?id=xByvdb3DCm",
        "keywords": "causal discovery, selection bias, experiments, interventions",
        "abstract": "We address the common yet often-overlooked selection bias in interventional studies, where subjects are selectively enrolled into experiments. For instance, participants in a drug trial are usually patients of the relevant disease; A/B tests on mobile applications target existing users only, and gene perturbation studies typically focus on specific cell types, such as cancer cells. Ignoring this bias leads to incorrect causal discovery results. Even when recognized, the existing paradigm for interventional causal discovery still fails to address it. This is because subtle differences inwhenandwhereinterventions happen can lead to significantly different statistical patterns. We capture this dynamic by introducing a graphical model that explicitly accounts for both the observed world (where interventions are applied) and the counterfactual world (where selection occurs while interventions have not been applied). We characterize the Markov property of the model, and propose a provably sound algorithm to identify causal relations as well as selection mechanisms up to the equivalence class, from data with soft interventions and unknown targets. Through synthetic and real-world experiments, we demonstrate that our algorithm effectively identifies true causal relations despite the presence of selection bias."
    }
]