[
    {
        "title": "Monte Carlo Planning with Large Language Model for Text-Based Games",
        "link_suffix": "/forum?id=r1KcapkzCt",
        "link": "https://openreview.net/forum?id=r1KcapkzCt",
        "pdf_link": "https://openreview.net/pdf?id=r1KcapkzCt",
        "keywords": "Large language model, Monte Carlo tree search, Text-based games",
        "abstract": "Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities.\nIn this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments."
    },
    {
        "title": "Fine-Tuning Pre-trained Language Models for Robust Causal Representation Learning",
        "link_suffix": "/forum?id=tlH4vDii0E",
        "link": "https://openreview.net/forum?id=tlH4vDii0E",
        "pdf_link": "https://openreview.net/pdf?id=tlH4vDii0E",
        "keywords": "representation learning, causal inference, robust machine learning",
        "abstract": "Fine-tuning pre-trained language models (PLMs) has exhibited proficiency across various domains. \nBy using domain-specific supervised data, the general purposed representation derived from the PLMs can be transformed into a domain-specific representation.\nHowever, these methods often fail to generalize to out-of-domain (OOD) data due to their reliance on $\\textit{non-causal}$ representations, often described as spurious features.\nExisting methods either make use of adjustments with strong assumptions about lack of hidden common causes, or mitigate the effect of spurious features using multi-domain data.\nIn this work, we investigate how fine-tuned pre-trained language models aids generalizability from single-domain scenarios under mild assumptions, targeting more general and practical real-world scenarios. \nWe show that a robust representations can be derived through so-called causal front-door adjustment, based on the $\\textit{decomposition}$ assumption using fine-tuned representations as a source of data augmentation.\nComprehensive experiments in both synthetic and real-world settings demonstrate the superior generalizability of the proposed method compared to existing approaches. \nOur work thus sheds light on the domain generalization problem by introducing links between fine-tuning and causal mechanisms into representation learning."
    },
    {
        "title": "CAuSE: Post-hoc Natural Language Explanation of Multimodal Classifiers through Causal Abstraction",
        "link_suffix": "/forum?id=mMXCMoU95Y",
        "link": "https://openreview.net/forum?id=mMXCMoU95Y",
        "pdf_link": "https://openreview.net/pdf?id=mMXCMoU95Y",
        "keywords": "Interpretability, Causal Abstraction, Multimodality, Classification",
        "abstract": "The increasing integration of AI models in critical areas, such as healthcare, finance, and security has raised concerns about their black-box nature, limiting trust and accountability. To ensure robust and trustworthy AI, interpretability is essential. In this paper, we propose CAuSE (Causal Abstraction under Simulated Explanation), a novel framework for post-hoc explanation of multimodal classifiers. Unlike existing interpretability methods, such as Amnesic Probing and Integrated Gradients, CAuSE generates causally faithful natural language explanations of fine-tuned multimodal classifiers' decisions. CAuSE integrates Interchange Intervention Training (IIT) within a Language Model (LM) based module to simulate the causal reasoning behind a classifier's outputs. We introduce a novel metric Counterfactual F1 score to measure causal faithfulness and demonstrate that CAuSE achieves state-of-the-art performance on this metric. We also provide a rigorous theoretical underpinning for causal abstraction between two neural networks and implement this within our CAuSE framework. This ensures that CAuSE\u2019s natural language explanations are not only simulations of the classifier\u2019s behavior but also reflect its underlying causal processes. Our method is task-agnostic and achieves state-of-the-art results on benchmark multimodal classification datasets, such as e-SNLI-VE and Facebook Hateful Memes, offering a scalable, faithful solution for interpretability in multimodal classifiers."
    },
    {
        "title": "SpeechFake: A Large-Scale Multilingual Speech Deepfake Dataset Toward Cutting-Edge Speech Generation Methods",
        "link_suffix": "/forum?id=GpUO6qYNQG",
        "link": "https://openreview.net/forum?id=GpUO6qYNQG",
        "pdf_link": "https://openreview.net/pdf?id=GpUO6qYNQG",
        "keywords": "dataset, deepfake detection, anti-spoofing, speech generation",
        "abstract": "As speech generation technology continues to evolve, the risk of misuse through deepfake audio has become a pressing concern, which underscores the critical need for robust detection methods.  However, many existing speech deepfake datasets fall short in terms of size, diversity, and linguistic coverage, limiting the ability of models to generalize effectively to unseen deepfakes. To address these limitations, we present SpeechFake, a large-scale dataset specifically designed for speech deepfake detection. With over 3 million deepfakes totaling more than 3,000 hours of audio, SpeechFake was generated using 40 different speech generation tools, including cutting-edge techniques, and spans 46 languages. This paper provides a detailed overview of the dataset\u2019s composition and statistics, emphasizing its scale and diversity. Additionally, we establish baseline results for SpeechFake and explore how factors such as generation methods, language diversity, and speaker variation influence detection performance. We believe SpeechFake will be a valuable resource for advancing speech deepfake detection research, offering opportunities to explore new detection strategies and improve model robustness across diverse and evolving generation techniques. The dataset will be publicly available soon."
    },
    {
        "title": "Discovering Clues of Spoofed LM Watermarks",
        "link_suffix": "/forum?id=QIApiYYgLG",
        "link": "https://openreview.net/forum?id=QIApiYYgLG",
        "pdf_link": "https://openreview.net/pdf?id=QIApiYYgLG",
        "keywords": "llm, watermarks, spoofing",
        "abstract": "LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. While recent works have demonstrated that state-of-the-art schemes are in fact vulnerable to spoofing, they lack deeper qualitative analysis of the texts produced by spoofing methods. In this work, we for the first time reveal that there are observable differences between genuine and spoofed watermark texts. Namely, we show that regardless of their underlying approach, all current spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts, effectively discovering that a watermark was spoofed. Our experimental evaluation shows high test power across all current spoofing methods, providing insights into their fundamental limitations, and suggesting a way to mitigate this threat."
    },
    {
        "title": "EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers",
        "link_suffix": "/forum?id=NPDnRLFhc0",
        "link": "https://openreview.net/forum?id=NPDnRLFhc0",
        "pdf_link": "https://openreview.net/pdf?id=NPDnRLFhc0",
        "keywords": "Biomedical Benchmark, Scientific Information Retrieval, Scientific Information Extraction, Large Language Models, BioNLP",
        "abstract": "We study the task of finding evidence for a hypothesis in the biomedical literature. Finding relevant evidence is a necessary precursor for evaluating the validity of scientific hypotheses, and for applications such as automated meta-analyses and scientifically grounded question-answering systems. We develop a pipeline for high quality, sentence-by-sentence annotation of biomedical papers for this task. The pipeline leverages expert judgments of scientific relevance, and is validated using teams of human annotators. We evaluate a diverse set of language models and retrieval systems on the benchmark, which consists of more than 400 fully annotated papers and 700k sentence judgments. The performance of the best models still falls significantly short of expert-level on this task. To show the scalability of our annotation and hypothesis generation pipeline, we created a larger EvidenceBench-100K containing 100,000 fully annotated papers with hypotheses and 150 million sentence judgments. Empirically, we show that fine-tuning embedding and language models on EvidenceBench-100k significantly improved their performance on the original EvidenceBench, reaching state-of-the-art embedding model performance and closely trails behind Large Language Models. By providing a standardized benchmark and evaluation framework, this work will support the development of tools which automate evidence synthesis and hypothesis testing, as well as the long-context global reasoning and instruction-following abilities for Large Language Models (LLM) and embedding-based IR systems. The dataset is available at \\href{https://github.com/EvidenceBench/EvidenceBench}{https://github.com/EvidenceBench/EvidenceBench}."
    },
    {
        "title": "Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov Decision Processes",
        "link_suffix": "/forum?id=Za3M6OZuCU",
        "link": "https://openreview.net/forum?id=Za3M6OZuCU",
        "pdf_link": "https://openreview.net/pdf?id=Za3M6OZuCU",
        "keywords": "Markov Decision Process, Channel coding, Rate-Reward Trade-off, Finite state channel",
        "abstract": "The impact of communication on decision-making systems has been extensively studied under the assumption of dedicated communication channels. We instead consider communicating through actions, where the message is embedded into the actions of an agent which interacts with the environment in a Markov decision process (MDP) framework. We conceptualize the MDP environment as a finite-state channel (FSC), where the actions of the agent serve as the channel input, while the states of the MDP observed by another agent (i.e., receiver) serve as the channel output. Here, we treat the environment as a communication channel over which the agent communicates through its actions, while at the same time, trying to maximize its reward. We first characterize the optimal information theoretic trade-off between the average reward and the rate of reliable communication in the infinite-horizon regime. Then, we propose a novel framework to design a joint control/coding policy, termed Act2Comm, which seamlessly embeds messages into actions. From a communication perspective, Act2Comm functions as a learning-based channel coding scheme for non-differentiable FSCs under input-output constraints. From a control standpoint, Act2Comm learns an MDP policy that incorporates communication capabilities, though at the cost of some control performance. Overall, Act2Comm effectively balances the dual objectives of control and communication in this environment. Experimental results validate Act2Comm's capability to enable reliable communication while maintaining a certain level of control performance."
    },
    {
        "title": "Sampling Process Brings Additional Bias for Debiased Recommendation",
        "link_suffix": "/forum?id=L6gyOOJYt2",
        "link": "https://openreview.net/forum?id=L6gyOOJYt2",
        "pdf_link": "https://openreview.net/pdf?id=L6gyOOJYt2",
        "keywords": "Superpopulation, Selection Bias, Recommender System",
        "abstract": "In recommender systems, selection bias arises from the users' selective interactions with items, which poses a widely-recognized challenge for unbiased evaluation and learning for recommendation models. Recently, doubly robust and its variants have been widely studied to achieve debiased learning of prediction models. However, if the users and items in the training set are not exactly the same as those in the test set, even if the imputed errors and learned propensities are accurate, all previous doubly robust based debiasing methods are biased. To tackle this problem, in this paper, we first derive the bias of doubly robust learning methods and provide alternative unbiasedness conditions when users and items are sampled from a superpopulation. Then we propose a novel superpopulation doubly robust target learning approach (SuperDR), which is unbiased when either the imputation model or propensity model is correctly specified. We further derive the generalization error bound of the proposed method under superpopulation, and show that it can be effectively controlled by the proposed target learning approach. We conduct extensive experiments on three real-world datasets, including a large-scale industrial dataset, to demonstrate the effectiveness of our method."
    },
    {
        "title": "Benchmarking Robustness of Foundation Models for Remote Sensing",
        "link_suffix": "/forum?id=DYXl6P70aH",
        "link": "https://openreview.net/forum?id=DYXl6P70aH",
        "pdf_link": "https://openreview.net/pdf?id=DYXl6P70aH",
        "keywords": "aerial imagery, foundation models, self-supervised learning, benchmark",
        "abstract": "Foundation models have significantly advanced machine learning applications across various modalities, including images. Recently numerous attempts have been made on developing foundation models specifically tailored for remote sensing applications, predominantly through masked image modeling techniques. This work explores the essential characteristics and performance expectations for a foundation model in aerial imagery. We introduce a benchmark designed to evaluate the model's performance as well as robustness to changes in scale and spectral bands of the input. Our benchmarks encompass tasks unique to aerial imagery, such as change detection and scene classification, and utilize publicly available datasets RESISC45, BigEarthNet, LEVIR-CD and OSCD. We evaluate recently proposed foundation models on the benchmark. Furthermore, we explore the impact of various design choices in pretraining and fine-tuning on the performance of the models on our benchmark. Specifically, we pretrain several variations of a self-distillation based self-supervised model on aerial imagery datasets, including one without scale-augmentations and another one with a pretrained mask decoder module."
    },
    {
        "title": "Enhancing Trust-Region Bayesian Optimization via Derivatives of Gaussian Processes",
        "link_suffix": "/forum?id=IANtNtNpYd",
        "link": "https://openreview.net/forum?id=IANtNtNpYd",
        "pdf_link": "https://openreview.net/pdf?id=IANtNtNpYd",
        "keywords": "Bayesian optimization, High-dimensional Bayesian optimization, Trust-Region methods",
        "abstract": "Bayesian Optimization (BO) has been widely applied to optimize expensive black-box functions while retaining sample ef\ufb01ciency. However, scaling BO to high-dimensional spaces remains challenging. Existing literature proposes performing standard BO in several local trust regions (TuRBO) for heterogeneous modeling of the objective function and avoiding over-exploration. Despite its advantages, using local Gaussian Processes (GPs) reduces sampling ef\ufb01ciency compared to a global GP. To enhance sampling ef\ufb01ciency while preserving heterogeneous modeling, we propose to construct several local quadratic models using gradients and Hessians from a global GP, and select new sample points by solving the bound-constrained quadratic program. We provide a convergence analysis and demonstrate through experimental results that our method enhances the ef\ufb01cacy of TuRBO and outperforms a wide range of high-dimensional BO techniques on synthetic functions and real-world applications."
    },
    {
        "title": "Solving the 2-norm k-hyperplane clustering problem via multi-norm formulations",
        "link_suffix": "/forum?id=ghk8lnOYRq",
        "link": "https://openreview.net/forum?id=ghk8lnOYRq",
        "pdf_link": "https://openreview.net/pdf?id=ghk8lnOYRq",
        "keywords": "hyperplane clustering; mathematical programming; spatial branch and bound",
        "abstract": "We tackle the 2-norm (Euclidean) $k$-Hyperplane Clustering problem ($k$-HC$_2$), which asks for finding $k$ hyperplanes that minimize the sum of squared 2-norm (Euclidean) distances between each point and its closest hyperplane. We solve the problem to global optimality via spatial branch-and-bound techniques (SBB) by strengthening a mixed integer quadratically-constrained quadratic programming formulation with constraints that arise when formulating the problem in $p$-norms with $p \\neq 2$. In particular, we show that, for every (appropriately scaled) $p \\in \\mathbb{N} \\cup {\\infty}$, one obtains a variant of $k$-HC$_2$, whose optimal solutions yield lower bounds within a multiplicative approximation factor. We focus on the case of polyhedral norms where $p=1, \\infty$ (which admit a disjunctive-programming reformulation), and prove that strengthening the original formulation by including, on top of the original 2-norm constraints, the constraints of one of the polyhedral-norms leads to an SBB method where nonzero lower bounds are obtained in a linear (as opposed to exponential) number of SBB nodes. Experimentally, we show that our strengthened formulations lead to speedups from $\\frac{1}{4}$ to 1.5 orders of magnitude, drastically improving the problem's solvability to global optimality."
    },
    {
        "title": "A Statistical Framework for Ranking LLM-based Chatbots",
        "link_suffix": "/forum?id=rAoEub6Nw2",
        "link": "https://openreview.net/forum?id=rAoEub6Nw2",
        "pdf_link": "https://openreview.net/pdf?id=rAoEub6Nw2",
        "keywords": "Large Language Models (LLMs), Paired Comparison, Statistical Ranking, Human Preferences, Chatbot Arena, Logistic Regression",
        "abstract": "Evaluating large language models (LLMs) effectively is essential for advancing their development and ensuring alignment with human preferences. Platforms like Chatbot Arena have made significant strides by gathering millions of votes through crowdsourced pairwise comparisons to rank LLMs, offering valuable data for assessing model performance. However, the statistical methods employed rely on simplistic approaches, such as the Elo rating system, which inadequately handles ties in competitions and overlooks the underlying relationships between competing models. In this paper, we introduce a more rigorous statistical framework that builds upon the data from Chatbot Arena while correcting these methodological shortcomings. We apply well-established statistical models to properly account for ties within an axiomatic framework. Additionally, we introduce a novel factor analysis that captures the complexity of ties across pairs of competitors, significantly improving the overall model performance. These improvements not only enhance the handling of ties but also increase the accuracy of win and loss predictions compared to previous methods. Additionally, we incorporate Thurstonian representations to model covariance structures between competitors, allowing for deeper insights beyond rankings. We also address previously unrecognized symmetries in the likelihood function that can hinder optimization and propose constraints to ensure stable parameter estimation. Finally, we provide a Python package, leaderbot, to facilitate reproducibility. Our experiments demonstrate significant improvements in accuracy for both ties and win-loss outcomes, offering a robust alternative to existing methods."
    },
    {
        "title": "RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style",
        "link_suffix": "/forum?id=QEHrmQPBdd",
        "link": "https://openreview.net/forum?id=QEHrmQPBdd",
        "pdf_link": "https://openreview.net/pdf?id=QEHrmQPBdd",
        "keywords": "Reward Models, Language Models, Evaluation, Alignment",
        "abstract": "Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. \nDespite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. \nHowever, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.\nTo this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. \nExtensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.\nWe evaluate nearly 40 reward models on RM-Bench. \nOur results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.\nThese findings highlight the significant room for improvement in current reward models."
    },
    {
        "title": "Structured Predictive Representations in Reinforcement Learning",
        "link_suffix": "/forum?id=sEv6vHIUnu",
        "link": "https://openreview.net/forum?id=sEv6vHIUnu",
        "pdf_link": "https://openreview.net/pdf?id=sEv6vHIUnu",
        "keywords": "Representation Learning; State Abstractions; Reinforcement Learning; Self-Prediction",
        "abstract": "Reinforcement Learning (RL) remains brittle in complex environments characterized by sparse rewards, partial observability, and subtask dependencies. Predictive state abstractions capture the environment's underlying temporal structure and are crucial to overcoming these challenges. Yet, such methods often only focus on global one-step transitions and overlook local relationships between trajectories.\nThis paper explores how capturing such relationships can enhance representation learning methods in RL. Our primary contribution is to show that incorporating GNNs into the observation-predictive learning process improves sample efficiency and robustness to changes in size and distractors. Through experiments on the MiniGrid suite, we demonstrate that our GNN-based approach consistently outperforms typical MLP-based models, particularly in environments where task decomposition and relational reasoning are critical. These results highlight the value of structural inductive biases for generalization and adaptability, revealing how such mechanisms can bolster performance in RL."
    },
    {
        "title": "ONLINE EPSILON NET & PIERCING SET FOR GEOMETRIC CONCEPTS",
        "link_suffix": "/forum?id=nNiWRRj6r9",
        "link": "https://openreview.net/forum?id=nNiWRRj6r9",
        "pdf_link": "https://openreview.net/pdf?id=nNiWRRj6r9",
        "keywords": "Theoretical machine learning, VC-dimension, Geometric sampling",
        "abstract": "VC-dimension (Vapnik & Chervonenkis (1971)) and $\\varepsilon$-nets  (Haussler & Welzl (1987)) are key concepts in Statistical Learning Theory. Intuitively, VC-dimension is a measure of the size of a class of sets. The famous $\\varepsilon$-net theorem, a fundamental result in Discrete Geometry, asserts that if the VC-dimension of a set system is bounded, then a small sample exists that intersects all sufficiently large sets.In online learning scenarios where data arrives sequentially, the VC-dimension helps bound the complexity of the set system, and $\\varepsilon$-nets ensure the selection of a small representative set. This sampling framework is crucial in various domains, including spatial data analysis, motion planning in dynamic environments, optimization of sensor networks, and feature extraction in computer vision, among others. Motivated by these applications, we study the online $\\varepsilon$-net problem for geometric concepts with bounded VC-dimension. While the offline version of this problem has been extensively studied, surprisingly, there are no known theoretical results for the online version to date. We present the first deterministic online algorithm with an optimal competitive ratio for intervals in $\\mathbb{R}$. Next, we give a randomized online algorithm with a near-optimal competitive ratio for axis-aligned boxes in $\\mathbb{R}^d$, for $d\\le 3$. Furthermore, we introduce a novel technique to analyze similar-sized objects of constant description complexity in $\\mathbb{R}^d$, which may be of independent interest. \n\nNext, we focus on the continuous version of this problem (called online piercing set), where ranges of the set system are geometric concepts in $\\mathbb{R}^d$ arriving in an online manner, but the universe is the entire ambient space, and the objective is to choose a small sample that intersects all the ranges. Although online piercing set is a very well-studied problem in the literature, to our surprise, very few works have addressed generic geometric concepts without any assumption about the sizes. We advance this field by proposing asymptotically optimal competitive deterministic algorithms for boxes and ellipsoids in $\\mathbb{R}^d$, for any $d\\in\\mathbb{N}$."
    },
    {
        "title": "SimulPL: Aligning Human Preferences in Simultaneous Machine Translation",
        "link_suffix": "/forum?id=XBF63bHDZw",
        "link": "https://openreview.net/forum?id=XBF63bHDZw",
        "pdf_link": "https://openreview.net/pdf?id=XBF63bHDZw",
        "keywords": "simultaneous machine translation, simultaneous preference optimization, human preferences",
        "abstract": "Simultaneous Machine Translation (SiMT) generates translations while receiving streaming source inputs. This requires the SiMT model to learn a read/write policy, deciding when to translate and when to wait for more source input. Numerous linguistic studies indicate that audiences in SiMT scenarios have distinct preferences, such as accurate translations, simpler syntax, and no unnecessary latency. Aligning SiMT models with these human preferences is crucial to improve their performances. However, this issue still remains unexplored. Additionally, preference optimization for SiMT task is also challenging. Existing methods focus solely on optimizing the generated responses, ignoring human preferences related to latency and the optimization of read/write policy during the preference optimization phase. To address these challenges, we propose Simultaneous Preference Learning (SimulPL), a preference learning framework tailored for the SiMT task. In the SimulPL framework, we categorize SiMT human preferences into five aspects:translation quality preference,monotonicity preference,key point preference,simplicity preference, andlatency preference. By leveraging the first four preferences, we construct human preference prompts to efficiently guide GPT-4/4o in generating preference data for the SiMT task. In the preference optimization phase, SimulPL integrateslatency preferenceinto the optimization objective and enables SiMT models to improve the read/write policy, thereby aligning with human preferences more effectively. Experimental results indicate that SimulPL exhibits better alignment with human preferences across all latency levels in Zh$\\rightarrow$En, De$\\rightarrow$En and En$\\rightarrow$Zh SiMT tasks."
    },
    {
        "title": "Neural Interactive Proofs",
        "link_suffix": "/forum?id=R2834dhBlo",
        "link": "https://openreview.net/forum?id=R2834dhBlo",
        "pdf_link": "https://openreview.net/pdf?id=R2834dhBlo",
        "keywords": "interactive proofs, game theory, neural networks, safety, multi-agent reinforcement learning",
        "abstract": "We consider the problem of how a trusted, but computationally bounded agent (a 'verifier') can learn to interact with one or more powerful but untrusted agents ('provers') in order to solve a given task. More specifically, we study the case in which agents are represented using neural networks and refer to solutions of this problem as neural interactive proofs. First we introduce a unifying framework based on prover-verifier games (Anil et al., 2021), which generalises previously proposed interaction protocols. We then describe several new protocols for generating neural interactive proofs, and provide a theoretical comparison of both new and existing approaches. Finally, we support this theory with experiments in two domains: a toy graph isomorphism problem that illustrates the key ideas, and a code validation task using large language models. In so doing, we aim to create a foundation for future work on neural interactive proofs and their application in building safer AI systems."
    },
    {
        "title": "G\u00f6del Agent: A Self-Referential Framework Helps for Recursively Self-Improvement",
        "link_suffix": "/forum?id=dML3XGvWmy",
        "link": "https://openreview.net/forum?id=dML3XGvWmy",
        "pdf_link": "https://openreview.net/pdf?id=dML3XGvWmy",
        "keywords": "Agent, Large Language Model, Reasoning, Self-Improvement",
        "abstract": "The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce G\u00f6del Agent, a self-evolving framework inspired by the G\u00f6del machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. G\u00f6del Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of G\u00f6del Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability."
    },
    {
        "title": "Towards Robust Evaluation of Protein Generative Models: A Systematic Analysis of Metrics",
        "link_suffix": "/forum?id=1S8ndwxMts",
        "link": "https://openreview.net/forum?id=1S8ndwxMts",
        "pdf_link": "https://openreview.net/pdf?id=1S8ndwxMts",
        "keywords": "evaluation metrics, protein, protein generative models",
        "abstract": "The rapid advancement of protein generative models necessitates robust and principled methods for their evaluation and comparison. As new models of increasing complexity continue to emerge, it is crucial to ensure that the metrics used for assessment are well-understood and reliable. In this work, we conduct a systematic investigation of commonly used metrics for evaluating sequence protein generative models, focusing on quality, diversity, and distributional similarity. We examine the behavior of these metrics under various conditions, including synthetic perturbations and real-world generative models. Our analysis explores different design choices, parameters, and underlying representation models, revealing how these factors influence metric performance. We identify several challenges in applying these metrics, such as sample size dependencies, sensitivity to data distribution shifts, and computational efficiency trade-offs. By testing metrics on both synthetic datasets with controlled properties and outputs from state-of-the-art protein generators, we provide insights into each metric's strengths, limitations, and practical applicability. Based on our findings, we offer a set of practical recommendations for researchers to consider when evaluating protein generative models, aiming to contribute to the development of more robust and meaningful evaluation practices in the field of protein design."
    },
    {
        "title": "Oracle efficient truncated statistics",
        "link_suffix": "/forum?id=ZS7UEI3vG5",
        "link": "https://openreview.net/forum?id=ZS7UEI3vG5",
        "pdf_link": "https://openreview.net/pdf?id=ZS7UEI3vG5",
        "keywords": "truncated statistics, exponential family, statistical learning",
        "abstract": "We study the problem of learning from truncated samples: instead of observing\nsamples from some underlying population $p^\\ast$, we observe only the examples that fall in some survival set $S \\subset \\mathbb{R}^d$ whose probability mass (measured with respect to $p^\\ast$) is at least $\\alpha$.  Assuming membership oracle access to the truncation set $S$, prior works obtained algorithms for the case where $p^\\ast$ is Gaussian or more generally an exponential family with strongly convex likelihood --- albeit with a super-polynomial \ndependency on the (inverse) survival mass $1/\\alpha$\nboth in terms of runtime and in number of oracle calls to the set $S$.  In this work we design a new learning method with runtime and query complexity polynomial in $1/\\alpha$.Our result significantly improves over the prior works \nby focusing on efficiently solving the underlying optimization problem using a general\npurpose optimization algorithm with minimal assumptions."
    },
    {
        "title": "RedHat: Towards Reducing Hallucination in Essay Critiques with Large Language Models",
        "link_suffix": "/forum?id=IULlNTZZel",
        "link": "https://openreview.net/forum?id=IULlNTZZel",
        "pdf_link": "https://openreview.net/pdf?id=IULlNTZZel",
        "keywords": "essay critique generation, large language model, hallucination",
        "abstract": "Essay critiques refer to the textual assessment of an essay, serving as the basis for the scoring of the essay, and are crucial for the improvements of the essay. Essay critique generation has received increasing attention after the blooming of large language models (LLMs), which show promising potential in writing and critiquing essays. Automatic critique generation can streamline both instructors and reviewers as well as spur LLM advancement in long context generation characterized by essay writing. However, current LLMs suffer from hallucinations when generating essay critiques, which are still under-explored in the community. To facilitate research in reliable essay critique generation, we first define this task with a unified input-output format as well as clear judging criteria. To minimize hallucinations in critique generation, we introduce RedHat, a novel approach that embeds the key information from essays directly into the generation process through document-level question-answering, ensuring critiques stay firmly anchored to the original text. We collected a large-scale, high-quality essay critique dataset called EssayC, annotated by human experts over multiple LLM-generated critiques, from a campus undergraduate essay writing course. We experimented RedHat backboned by commercial and open-sourced LLMs. Results showed that critiques generated by RedHat are preferred by human experts over baseline in 20% of cases on EssayC in detailedness and informativeness, with a decrement of 10% on hallucinations in our judging criteria."
    },
    {
        "title": "Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization",
        "link_suffix": "/forum?id=gx1wHnf5Vp",
        "link": "https://openreview.net/forum?id=gx1wHnf5Vp",
        "pdf_link": "https://openreview.net/pdf?id=gx1wHnf5Vp",
        "keywords": "mixture of experts, large language models, continual pre-training",
        "abstract": "The Mixture of Experts (MoE) architecture reduces the training and inference cost significantly compared to a dense model of equivalent capacity. Upcycling is an approach that initializes and trains an MoE model using a pre-trained dense model. While upcycling leads to initial performance gains, the training progresses slower than when trained from scratch, leading to suboptimal performance in the long term. We propose Drop-Upcycling - a method that effectively addresses this problem. Drop-Upcycling combines two seemingly contradictory approaches: utilizing the knowledge of pre-trained dense models while statistically re-initializing some parts of the weights. This approach strategically promotes expert specialization, significantly enhancing the MoE model's efficiency in knowledge acquisition. \nExtensive large-scale experiments demonstrate that Drop-Upcycling significantly outperforms previous MoE construction methods in the long term, specifically when training on hundreds of billions of tokens or more.\nAs a result, our MoE model with 5.9B active parameters achieves comparable performance to a 13B dense model in the same model family, while requiring approximately 1/4 of the training FLOPs.\nAll experimental resources, including source code, training data, model checkpoints and logs, are publicly available to promote reproducibility and future research on MoE."
    },
    {
        "title": "DiRaGNN: Attention-Enhanced Entity Ranking for Sparse Graph Networks",
        "link_suffix": "/forum?id=BPyNGmM3jy",
        "link": "https://openreview.net/forum?id=BPyNGmM3jy",
        "pdf_link": "https://openreview.net/pdf?id=BPyNGmM3jy",
        "keywords": "Heterogeneous Graphs, Graph Neural Networks, Recommendation System",
        "abstract": "Sparsity in both the structural and engagement information presents a core challenge in entity ranking problems for graph networks. The interaction dynamics of entities are often characterized by limited structural and engagement information which results in inferior performance of the state-of-the-art approaches. In this work, we present DiRaGNN, an attention-enhanced entity ranking model designed\nto address the problem of dimension recommendation and ranking for automated watchdogs in the cloud setting. DiRaGNN is inspired by transformer architectures and utilizes a multi-head attention mechanism to focus on heterogeneous neighbors and their attributes. Additionally, our model employs multi-faceted loss functions to optimize for relevant recommendations and reduce popularity bias. To manage computational complexity, we sample a local subgraph that includes multiple hops of neighbors. Empirical evaluations demonstrate significant improvements over existing methods, with our model achieving a 39.7% increase in MRR."
    },
    {
        "title": "Complete multi-modal metric learning for multi-modal sarcasm detection",
        "link_suffix": "/forum?id=PflweLMInP",
        "link": "https://openreview.net/forum?id=PflweLMInP",
        "pdf_link": "https://openreview.net/pdf?id=PflweLMInP",
        "keywords": "multi-modal sarcasm detection, metric learning, complete multi-modal incongruities",
        "abstract": "Multi-modal sarcasm detection identifies sarcasm from text-image pairs, an essential technology for accurately understanding the user's real attitude.\nMost research extracted the incongruity of text-image pairs as sarcasm information. However, these methods neglected inter-modal or intra-modal incongruities in fact and sentiment perspectives, leading to incomplete sarcasm information and biased performance.\nTo address the above issues, this paper proposes a complete multi-modal metric learning network (CMML-Net) for multi-modal sarcasm detection tasks.\nSpecifically, CMML-Net utilizes a fact-sentiment multi-task representation learning module to produce refined fact and sentiment text-image representation pairs.\nIt then designs a complete multi-modal metric learning to iteratively calculate inter-modal and intra-modal incongruities in fact and sentiment metric spaces, explicitly capturing complete multi-modal incongruities.\nCMML-Net performs well in explicitly capturing comprehensive sarcasm information and obtaining discriminative performance via deep metric learning.\nThe state-of-the-art performance on the widely-used dataset demonstrates CMML-Net's effectiveness in multi-modal sarcasm detection."
    },
    {
        "title": "Less is More: Stealthy and Adaptive Clean-Image Backdoor Attacks with Few Poisoned",
        "link_suffix": "/forum?id=LsTIW9VAF7",
        "link": "https://openreview.net/forum?id=LsTIW9VAF7",
        "pdf_link": "https://openreview.net/pdf?id=LsTIW9VAF7",
        "keywords": "Backdoor Attack, Generative Adversarial Networks, Clean-Image Backdoor Attacks, Deep Neural Networks, InfoGAN, Poisoning Attack, Model Integrity",
        "abstract": "Deep neural networks are fundamental in security-critical applications such as facial recognition, autonomous driving, and medical diagnostics, yet they are vulnerable to backdoor attacks. Clean-image backdoor attack, a stealthy attack utilizing solely label manipulation to implant backdoors, renders models vulnerable to exploitation by malicious labelers. However, existing clean-image backdoor attacks likely lead to a noticeable drop in Clean Accuracy (CA), decreasing their stealthiness. In this paper, we show that clean-image backdoor attacks can achieve a negligible decrease in CA by poisoning only a few samples while still maintaining a high attack success rate. We introduceGenerative AdversarialClean-ImageBackdoors (GCB), a novel attack method that minimizes the drop in CA to less than 1% by optimizing the trigger pattern for easier learning by the victim model. Leveraging a variant of InfoGAN, we ensure that the trigger pattern we used has already been contained in some training images and can be easily separated from those feature patterns used for benign tasks. Our experiments demonstrate that GCB can be adapted to 5 datasets\u2014including MNIST, CIFAR-10, CIFAR-100, GTSRB, and Tiny-ImageNet\u20145 different architectures, and 4 tasks, including classification, multi-label classification, regression, and segmentation. Furthermore, GCB demonstrates strong resistance to backdoor defenses, successfully evading all detection methods we know. Code:anonymous.4open.science/r/GCB."
    }
]