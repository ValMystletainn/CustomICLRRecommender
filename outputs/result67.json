[
    {
        "title": "Merging Feed-Forward Sublayers for Compressed Transformers",
        "link_suffix": "/forum?id=CgqnYqpYQh",
        "link": "https://openreview.net/forum?id=CgqnYqpYQh",
        "pdf_link": "https://openreview.net/pdf?id=CgqnYqpYQh",
        "keywords": "model compression, model merging, transformers, parameter efficiency, redundancy",
        "abstract": "With the rise and ubiquity of larger deep learning models, the need for high-quality compression techniques has been growing in order to deploy these models widely. The sheer parameter count of some models makes it difficult to fit them into the memory constraints of different hardware. In this work, we present a novel approach to model compression by merging similar parameter groups within a model, rather than pruning away less important parameters. Specifically, we propose a straightforward method for selecting, aligning, and merging separate feed-forward sublayers in Transformer models, and test our method on a language modeling task, image classification, and machine translation. With our method, we demonstrate performance comparable to the original models across our three diverse tasks while combining more than a third of model feed-forward sublayers. For instance, we can remove over 21% of total parameters from a Vision Transformer, while maintaining 99% of its original performance. Additionally, we observe that some  feed-forward sublayers often exhibit regions of high similarity between their activations, which may help explain their surprising mergeability."
    },
    {
        "title": "PLHF: Prompt Learning from Few-shot Human Feedback",
        "link_suffix": "/forum?id=8U4NGFE0po",
        "link": "https://openreview.net/forum?id=8U4NGFE0po",
        "pdf_link": "https://openreview.net/pdf?id=8U4NGFE0po",
        "keywords": "prompt optimization, large language model, few-shot learning, human feedback",
        "abstract": "Recent advances explore prompt tuning for large language models (LLMs) and develop automatic optimization frameworks to obtain suitable prompts with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples, especially for those natural language applications that multiple outputs are equally valid. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address this issue, we present PLHF, a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies involving human experts, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF significantly outperforms existing output scoring strategies for LLM prompt optimizations."
    },
    {
        "title": "Flow Matching for Accelerated Simulation of Atomic Transport in Materials",
        "link_suffix": "/forum?id=CkozFajtKq",
        "link": "https://openreview.net/forum?id=CkozFajtKq",
        "pdf_link": "https://openreview.net/pdf?id=CkozFajtKq",
        "keywords": "flow matching, generative models, atomistic simulations, molecular dynamics, materials science",
        "abstract": "We introduce LiFlow, a generative framework to accelerate molecular dynamics (MD) simulations for crystalline materials that formulates the task as conditional generation of atomic displacements. The model uses flow matching, with a Propagator submodel to generate atomic displacements and a Corrector to locally correct unphysical geometries, and incorporates an adaptive prior based on the Maxwell–Boltzmann distribution to account for chemical and thermal conditions. We benchmark LiFlow on a dataset comprising 25-ps trajectories of lithium diffusion across 4,186 solid-state electrolyte (SSE) candidates at four temperatures. The model obtains a consistent Spearman rank correlation of 0.7–0.8 for lithium mean squared displacement (MSD) predictions on unseen compositions. Furthermore, LiFlow generalizes from short training trajectories to larger supercells and longer simulations while maintaining high accuracy. With speed-ups of up to 600,000× compared to first-principles methods, LiFlow enables scalable simulations at significantly larger length and time scales."
    },
    {
        "title": "Coarsening to Conceal: Enabling Privacy-Preserving Federated Learning for Graph Data",
        "link_suffix": "/forum?id=Vszt1FDElj",
        "link": "https://openreview.net/forum?id=Vszt1FDElj",
        "pdf_link": "https://openreview.net/pdf?id=Vszt1FDElj",
        "keywords": "Federated Learning, Privacy-Preserving Machine Learning, Graph Neural Networks, Graph Coarsening, Data Privacy and Security",
        "abstract": "With the escalating demand for privacy-preserving machine learning, federated learning (FL) stands out by enabling collaboration among decentralized entities. Utilizing graph representations of data enhances learning for graph-level tasks, crucial for FL with data distributed across local repositories. Despite its benefits, stringent privacy regulations often compromise FL's performance. Previous methods aimed at ensuring privacy introduce performance degradation and computational overhead. In response to these challenges, we propose using graph coarsening—a simple yet effective method—to enhance the security and privacy of FL on graph data. Our approach posits that graph coarsening alone can suffice for privacy guarantees, as model parameters obtained from training on the coarsened graph effectively conceal sensitive information susceptible to privacy attacks. Through comprehensive application and analysis, we demonstrate the efficacy of graph coarsening within an FL setup, taking both the graph matrix and node features as input, and jointly learning the coarsened graph matrix and feature matrix while ensuring desired properties. The resultant coarsened graph representations are then utilized to train model parameters, subsequently communicated within an FL framework for downstream tasks such as classification. Extensive experimentation across various datasets confirms that graph coarsening ensures privacy while enhancing performance with minimal trade-offs compared to traditional differential privacy (DP) methods without adding extra complexity overhead."
    },
    {
        "title": "Asymmetric Embedding Models for Hierarchical Retrieval: Provable Constructions and a Pretrain-Finetune Recipe",
        "link_suffix": "/forum?id=tdbK3TGFl1",
        "link": "https://openreview.net/forum?id=tdbK3TGFl1",
        "pdf_link": "https://openreview.net/pdf?id=tdbK3TGFl1",
        "keywords": "Information Retrieval, Hierarchical Retrieval, Dual encoders",
        "abstract": "Dual encoder (DE) models, where a pair of matching query and document are embedded into similar vector representations, are widely used in information retrieval due to their efficiency and scalability. However, DEs are known to have a limited expressive power due to the Euclidean geometry of the embedding space, which may compromise their quality. This paper investigate such limitations in the context of \\emph{hierarchical retrieval}, the task where the document set has a hierarchical structure and the matching keywords for a query are all of its ancestor nodes. We first prove the feasibility of representing hierarchical structures within the Euclidean embedding space by providing a constructive algorithm for generating effective embeddings from a given hierarchy. Then we delve into the learning of DEs when the hierarchy is unknown, which is a practical assumption since usually only samples of matching query and document pairs are available during training. Our experiments reveal a \"lost in the long distance\" phenomenon, where retrieval accuracy degrades for documents further away in the hierarchy. To address this, we introduce a pretrain-finetune approach that significantly improves long-distance retrieval without sacrificing performance on closer documents. Finally, we validate our findings on a realistic hierarchy from WordNet, demonstrating the effectiveness of our approach in retrieving documents at various levels of abstraction."
    },
    {
        "title": "Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI",
        "link_suffix": "/forum?id=yfW1x7uBS5",
        "link": "https://openreview.net/forum?id=yfW1x7uBS5",
        "pdf_link": "https://openreview.net/pdf?id=yfW1x7uBS5",
        "keywords": "security, adversarial, style mimicry, generative ai",
        "abstract": "Artists are increasingly concerned about advancements in image generation models that can closely replicate their unique artistic styles.\nIn response, several protection tools against style mimicry have been developed that incorporate small adversarial perturbations into artworks published online. In this work, we evaluate the effectiveness of popular protections---with millions of downloads---and show they only provide a false sense of security. We find that low-effort and \"off-the-shelf\" techniques, such as image upscaling, are sufficient to create robust mimicry methods that significantly degrade existing protections. Through a user study, we demonstrate thatall existing protections can be easily bypassed, leaving artists vulnerable to style mimicry.  We caution that tools based on adversarial perturbations cannot reliably protect artists from the misuse of generative AI, and urge the development of alternative protective solutions."
    },
    {
        "title": "GRABLI: Cross-Modal Knowledge Graph Alignment for Biomedical Language Models",
        "link_suffix": "/forum?id=kE1TVeolWv",
        "link": "https://openreview.net/forum?id=kE1TVeolWv",
        "pdf_link": "https://openreview.net/pdf?id=kE1TVeolWv",
        "keywords": "Biomedical Knowledge Graph, Biomedical Language model, Representation Learning, Contrastive Learning",
        "abstract": "Pre-trained Language Models (LMs) have given a significant performance growth in a variety of language-related texts in biomedical domain. However, existing biomedical LLMs demonstrate a limited understanding of complex, domain-specific concept structure and the factual information stored in biomedical Knowledge Graphs (KGs). We propose \\textbf{GRABLI} (Knowledge \\textbf{Gra}ph and \\textbf{B}iomedical Language Model A\\textbf{li}gnment), a novel pre-training method that enriches an LM with external knowledge by simultaneously learning a separate KG encoder and aligning LM and graph representations. Given a textual sequence, we normalize biomedical concept mentions to the Unified Medical Language System (UMLS) KG and use the local KG subgraphs as cross-modal positive samples for mentioned concepts. Our empirical results demonstrate that applying our proposed method to various state-of-the-art biomedical LMs including PubMedBERT and BioLinkBERT, enhances their performance on diverse language understanding tasks, even after brief pre-training on a small alignment dataset derived from PubMed scientific abstracts."
    },
    {
        "title": "Multimodal Attributed Graphs: Benchmarking and Rethinking",
        "link_suffix": "/forum?id=jy6Lj3JaOf",
        "link": "https://openreview.net/forum?id=jy6Lj3JaOf",
        "pdf_link": "https://openreview.net/pdf?id=jy6Lj3JaOf",
        "keywords": "Multimodal Learning, Representation Learning",
        "abstract": "Associating unstructured data with structured information is crucial for real-world tasks that require relevance search. However, existing graph learning benchmarks often overlook the rich semantic information associated with each node, ignoring other available modalities such as the corresponding images. To bridge this gap, we introduce the Multimodal Graph Benchmark (MM-GRAPH), the first comprehensive multi-modal graph benchmark that incorporates both textual and visual information, going beyond the prior focus on just text-attributed graphs. MM-GRAPH consists of seven graph learning datasets of various scales that are appropriate for different learning tasks, and enable a comprehensive evaluation of graph learning algorithms in real-world scenarios thanks to their multimodal node features. To facilitate research on multimodal graph learning, we further provide an extensive study on the performance of various graph learning frameworks in the presence of features from various modalities. MM-GRAPH aims to foster research on multimodal attributed graphs and drive the development of more advanced and robust multimodal attributed graph learning algorithms. By providing a diverse set of datasets and benchmarks, MM-GRAPH enables researchers to evaluate and compare their models in realistic settings, ultimately leading to improved performance on real-world applications that rely on multimodal attributed graphs."
    },
    {
        "title": "Joint Denoising of Cryo-EM Projection Images using Polar Transformers",
        "link_suffix": "/forum?id=aTBE70xiFw",
        "link": "https://openreview.net/forum?id=aTBE70xiFw",
        "pdf_link": "https://openreview.net/pdf?id=aTBE70xiFw",
        "keywords": "denoising, microscopy, tomography, attention, symmetries",
        "abstract": "Deep neural networks (DNNs) have proven powerful for denoising individual images, but there is a limit to the noise level they can handle.\nIn applications like cryogenic electron microscopy (cryo-EM), the noise level is extremely high but datasets contain hundreds of thousands of projections of the same molecule, each taken a different viewing direction.\nExploiting this redundancy of information has proven useful in traditional denoising techniques known as class averaging methods, where images are clustered, aligned, and then averaged to reduce the noise level.\nWe present a neural network architecture based on polar representation of images and transformers that simultaneously clusters, aligns, and denoises cryo-EM projection images.\nNumerical results show accurate denoising performance using this architecture, with a relative mean squared error of $0.06$ at signal-to-noise (SNR) level of $0.05$, outperforming traditional filter-based methods by a factor of $2\\times$."
    },
    {
        "title": "Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs",
        "link_suffix": "/forum?id=FBkpCyujtS",
        "link": "https://openreview.net/forum?id=FBkpCyujtS",
        "pdf_link": "https://openreview.net/pdf?id=FBkpCyujtS",
        "keywords": "Natural Language Processing, Large Language Models, Text Generation, Sampling Methods, Truncation Sampling, Stochastic Sampling, Min-p Sampling, Top-p Sampling, Nucleus Sampling, Temperature Sampling, Decoding Methods, Deep Learning, Artificial Intelligence",
        "abstract": "Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. However, popular sampling methods like top-p  (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures, leading to incoherent or repetitive outputs. To address this challenge, we propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by scaling according to the top token's probability. We conduct extensive experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing, demonstrating that min-p sampling improves both the quality and diversity of generated text, particularly at high temperatures. Moreover, human evaluations reveal a clear preference for min-p sampling in terms of both text quality and diversity. Min-p sampling has been adopted by multiple open-source LLM implementations, highlighting its practical utility and potential impact."
    },
    {
        "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
        "link_suffix": "/forum?id=fL8sds4naU",
        "link": "https://openreview.net/forum?id=fL8sds4naU",
        "pdf_link": "https://openreview.net/pdf?id=fL8sds4naU",
        "keywords": "Large language model, Math capabilities, Synthetic data, Alignment",
        "abstract": "It was once believed that mathematical capabilities in language models required either large model scales or extensive math-related data pre-training. However, this paper demonstrates that the small-scale LLaMA-2 7B model already possesses strong mathematical potential. This is evidenced by its impressive scores of 97.6% on GSM8K benchmark and 70% on MATH benchmark, achieved by selecting the oracle response from 1024 generations. Equipped GPT-4 Turbo as an additional verification, LLaMA-2 7B also achieves 91.8% accuracy on GSM8K benchmark. This indicates that the primary issue within current models is the difficulty in consistently eliciting the inherent mathematical capabilities. We find that scaling up synthetic SFT data, which proves to be nearly as effective as real data, can significantly enhance the reliability of generating correct answers. Surprisingly, even with approximately one million samples, we observe no clear performance saturation. And our method is more efficient with large data scale than previous works. This approach achieves an accuracy of 82.4% on GSM8K and 40.1% on MATH using LLaMA-2 7B model, surpassing GPT-3.5 Turbo. Our 70B model even exceeds an early version of GPT-4 on MATH and out-of-domain Hungarian National High School Math Exam. These results demonstrate our method significantly elicits the general mathematical capabilities of language models. Also, we provide insights into scaling behaviors across different reasoning complexities."
    },
    {
        "title": "Injective flows for star-like manifolds",
        "link_suffix": "/forum?id=Jyh0DR4fFE",
        "link": "https://openreview.net/forum?id=Jyh0DR4fFE",
        "pdf_link": "https://openreview.net/pdf?id=Jyh0DR4fFE",
        "keywords": "Normalizing Flows, Injective Flows, Bayesian Inference, Variational Inference, Objective Bayes",
        "abstract": "Normalizing Flows (NFs) are powerful and efficient models for density estimation. When modeling densities on manifolds, NFs can be generalized to injective flows but the Jacobian determinant becomes computationally prohibitive. Current approaches either consider bounds on the log-likelihood or rely on some approximations of the Jacobian determinant. In contrast, we propose injective flows for star-like manifolds and show that for such manifolds we can compute the Jacobian determinant exactly and efficiently, with the same cost as NFs. This aspect is particularly relevant for variational inference settings, where no samples are available and only some unnormalized target is known. Among many, we showcase the relevance of modeling densities on star-like manifolds in two settings. Firstly, we introduce a novel Objective Bayesian approach for penalized likelihood models by interpreting level-sets of the penalty as star-like manifolds. Secondly, we consider probabilistic mixing models and introduce a general method for variational inference by defining the posterior of mixture weights on the probability simplex."
    },
    {
        "title": "Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality",
        "link_suffix": "/forum?id=2hbgKYuao1",
        "link": "https://openreview.net/forum?id=2hbgKYuao1",
        "pdf_link": "https://openreview.net/pdf?id=2hbgKYuao1",
        "keywords": "Graph Neural Networks, Subgraph GNNs, Subgraphs, Expressive power",
        "abstract": "We propose an expressive and efficient approach that combines the strengths of two prominent extensions of Graph Neural Networks (GNNs): Subgraph GNNs and Structural Encodings (SEs). Our approach leverages walk-based centrality measures, both as a powerful form of SE and also as a subgraph selection strategy for Subgraph GNNs. By drawing a connection to perturbation analysis, we highlight the effectiveness of centrality-based sampling, and show it significantly reduces the computational burden associated with Subgraph GNNs. Further, we combine our efficient Subgraph GNN with SEs derived from the calculated centrality and demonstrate this hybrid approach, dubbed HyMN, gains in discriminative power. HyMN effectively addresses the expressiveness limitations of Message Passing Neural Networks (MPNNs) while mitigating the computational costs of Subgraph GNNs. Through a series of experiments on synthetic and real-world tasks, we show it outperforms other subgraph sampling approaches while being competitive with full-bag Subgraph GNNs and other state-of-the-art approaches with a notably reduced runtime."
    },
    {
        "title": "Exact risk curves of signSGD in High-Dimensions: quantifying preconditioning and noise-compression effects",
        "link_suffix": "/forum?id=FZa1UCC9SC",
        "link": "https://openreview.net/forum?id=FZa1UCC9SC",
        "pdf_link": "https://openreview.net/pdf?id=FZa1UCC9SC",
        "keywords": "signSGD, stochastic optimization, Deep learning theory, high-dimensional probability, stochastic differential equation",
        "abstract": "In recent years, SignSGD has garnered interest as both a practical optimizer as well as a simple model to understand adaptive optimizers like Adam. Though there is a general consensus that SignSGD acts to precondition optimization and reshapes noise,  quantitatively understanding these effects in theoretically solvable settings remains difficult. We present an analysis of SignSGD in a high dimensional limit, and derive a limiting SDE and ODE to describe the risk. Using this framework we quantify four effects of SignSGD: effective learning rate, noise compression, diagonal preconditioning, and gradient noise reshaping. Our analysis is consistent with experimental observations but moves beyond that by quantifying the dependence of these effects on the data and noise distributions. We conclude with a conjecture on how these results might be extended to Adam."
    },
    {
        "title": "Mixture of In-Context Prompters for Tabular PFNs",
        "link_suffix": "/forum?id=2fojNANZSv",
        "link": "https://openreview.net/forum?id=2fojNANZSv",
        "pdf_link": "https://openreview.net/pdf?id=2fojNANZSv",
        "keywords": "Prior-Fitted Networks, Tabular Learning, Sparse Mixture of Experts.",
        "abstract": "Recent benchmarks find In-Context Learning (ICL) outperforms both deep learning and tree-based algorithms on small tabular datasets. However, on larger datasets, ICL for tabular learning suffers in both efficiency and effectiveness. In terms of efficiency, transformers incur linear space and quadratic time complexity w.r.t. context size. In terms of effectiveness, contexts at inference encounter distribution shift compared to contexts from pretraining. We propose MixturePFN, which extends Sparse Mixture of Experts to the state-of-the-art ICL for tabular learning model. Specifically, MixturePFN finetunes a specialized ICL expert on each cluster of tabular data and routes new test samples to appropriate experts at inference. MixturePFN supports constant-size contexts by splitting large training datasets into more manageable clusters. MixturePFN addresses distribution shift by finetuning an expert on each training dataset cluster via bootstrapping. Extensive experimental results shows MixturePFN outperforms 19 baselines both in mean rank and as the Condorcet winner across 36 diverse tabular datasets under both accuracy and F1 score with statistical significance."
    },
    {
        "title": "ToRL: Topology-preserving Representation Learning Of Object Deformations From Images",
        "link_suffix": "/forum?id=rUqcugZDUl",
        "link": "https://openreview.net/forum?id=rUqcugZDUl",
        "pdf_link": "https://openreview.net/pdf?id=rUqcugZDUl",
        "keywords": "Representation Learning, Deformations, Topology",
        "abstract": "Representation learning of object deformations from images has been a long-standing challenge in various image or video analysis tasks. Existing deep neural networks typically focus on visual features (e.g., intensity and texture), but they often fail to capture the underlying geometric and topological structures of objects. This limitation becomes especially critical in areas, such as medical imaging and 3D modeling, where maintaining the structural integrity of objects is essential for accuracy and generalization across diverse datasets. In this paper, we introduce ToRL, a novelTopology-preserving Representation Learningmodel that, for the first time, offers an explicit mechanism for modeling intricate object topology in the latent feature space. We develop a comprehensive learning framework that captures object deformations via learned transformation groups in the latent space. Each layer of our network's decoder is carefully designed with an integrated smooth composition module, ensuring that topological properties are preserved throughout the learning process. Moreover, in contrast to a few related works that rely on a reference image to predict object deformations during inference, our approach eliminates this impractical requirement. To validate ToRL's effectiveness, we conduct extensive multi-class classification experiments across a wide range of datasets, including synthetic 2D images, real 3D brain and abdominal MRIs, and 3D chest CT scans. Experimental results demonstrate that ToRL outperforms state-of-the-art methods, setting a new way to enforce topological consistency in representation learning."
    },
    {
        "title": "Are Large Language Models Truly Democratizing Financial Knowledge? Identifying Knowledge Gaps",
        "link_suffix": "/forum?id=As2ZyaNoHa",
        "link": "https://openreview.net/forum?id=As2ZyaNoHa",
        "pdf_link": "https://openreview.net/pdf?id=As2ZyaNoHa",
        "keywords": "Large Language Models, Fairness in AI, Model Hallucinations",
        "abstract": "Large Language Models (LLMs) are frequently utilized as sources of knowledge for question-answering. While it is known that LLMs may lack access to real-time data or newer data produced after the model's cutoff date, it is less clear how their knowledge spans acrosshistoricalinformation. In this study, we assess the breadth of LLMs' knowledge using financial data of U.S. publicly traded companies by evaluating more than 190k questions and comparing model responses to factual data. We further explore the impact of company characteristics, such as size, retail investment, institutional attention, and readability of financial filings, on the accuracy of knowledge represented in LLMs. Our results reveal that LLMs are less informed about past financial performance, but they display a stronger awareness of larger companies and more recent information. Interestingly, at the same time, our analysis also reveals that LLMs are more likely to hallucinate for larger companies, especially for data from more recent years. We will make the code, prompts, and model outputs public upon the publication of the work."
    },
    {
        "title": "Policy Design in Long-run Welfare Dynamics",
        "link_suffix": "/forum?id=d8hYXbxX71",
        "link": "https://openreview.net/forum?id=d8hYXbxX71",
        "pdf_link": "https://openreview.net/pdf?id=d8hYXbxX71",
        "keywords": "Long-run welfare, policy design, Rawlsian policy and utilitarianism",
        "abstract": "We study a stochastic dynamic model of long-term welfare in a population. Individuals in our model have welfare that improves with intervention and deteriorates in the absence of treatment. The planner can treat one individual at each time step. We contrast two fundamental policies in our model. The utilitarian policy greedily maximizes welfare improvement at each step. The Rawlsian policy intervenes on the individual of lowest welfare. Although hugely influential as a normative proposal, Rawlsian policies have been criticized for failing to optimize social welfare. We prove that, surprisingly, in a meaningful range of parameters Rawlsian policy has greater long-run utility than the utilitarian policy even though it is inferior on short time horizons. Specifically, this is true provided that treatment effects satisfy a weak homogeneity assumption, and the welfare dynamics satisfy a rich-get-richer and poor-get-poorer condition. We extend our results with a comprehensive comparison of different policies under different parameter regimes. Through semi-synthetic simulation studies, we evaluate various policies in cases where the assumptions of our theorems do not hold. Our results illustrate that comparing policies based on short-term evaluations can lead to misleading conclusions."
    },
    {
        "title": "Contextual Experience Replay for Continual Learning of Language Agents",
        "link_suffix": "/forum?id=RXvFK5dnpz",
        "link": "https://openreview.net/forum?id=RXvFK5dnpz",
        "pdf_link": "https://openreview.net/pdf?id=RXvFK5dnpz",
        "keywords": "large language models, agent, language agents, reasoning, decision making, NLP",
        "abstract": "Large language model-based agents have shown their potential in decision-making tasks, such as web navigation. However, solving multi-step decision-making tasks in complex environments like websites often requires the acquisition of environment-specific experiences. Without continual learning of environment-specific knowledge, current methods often fail in these complex tasks. To address this, we propose Contextual Experience Replay (CER), a novel training-free framework to enable efficient continual learning for language agents through experience replay contextually, i.e. in their context window. CER is loosely inspired by experience replay in reinforcement learning, where the agent is trained with past experiences to do continual learning. Specifically, CER accumulates and synthesizes past experiences, which are represented as natural language summarizations and concrete trajectory examples, into a dynamic memory buffer. These experiences encompass environment dynamics and common decision-making patterns, allowing the agents to retrieve and augment themselves with relevant knowledge in new contexts, enhancing their adaptability in complex environments. We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. While orthogonal to other methods, CER improves the GPT-4o agent baseline by a large margin and gets competitive results. On VisualWebArena, CER surpasses the tree search method with much lower token costs and achieves a state-of-the-art success rate of 31.9%. On WebArena, CER also gets a competitive average success rate of 33.16%, relatively improving the success rate of the GPT-4o agent baseline by 36.69%. CER shows that the continual learning of environment-specific knowledge is important and can lead to significant improvements in sequential decision-making tasks in complex environments."
    },
    {
        "title": "Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts",
        "link_suffix": "/forum?id=NJxCpMt0sf",
        "link": "https://openreview.net/forum?id=NJxCpMt0sf",
        "pdf_link": "https://openreview.net/pdf?id=NJxCpMt0sf",
        "keywords": "Multimodal Learning, Medical Imaging",
        "abstract": "Multi-modal multi-task learning holds significant promise in tackling complex diagnostic tasks and many significant medical problems. It fulfills the needs in real-world diagnosis protocol to leverage information from different data sources and simultaneously perform mutually informative tasks. However, current approaches often struggle with two key challenges: 1) sample-dynamic modality fusion, where the specific and shared information from different modalities vary across patients, and 2) modality-task dependence, where different tasks may require dynamic feature selection and combination from various data modalities. To address these issues, we propose M4oE, a novel Multi-modal Multi-task Mixture of Experts framework for precise Medical diagnosis. M4oE comprises Modality-Specific (MSoE) modules and a Modality-shared Modality-Task MoE (MToE) module. With collaboration from both modules, our model dynamically decomposes and learns distinct and shared information from different modalities and achieves sample-adaptive dynamic fusion. MToE provides a joint probability model of modalities and tasks by using experts as a link and encourages experts to learn modality-task dependence via conditional mutual information loss. By doing so, M4oE offers sample and population-level interpretability of modality contributions.\nWe evaluate M4oE on four public multi-modal medical benchmark datasets for solving two important medical diagnostic problems including breast cancer screening and retinal disease diagnosis. Results demonstrate M4oE's superiority over state-of-the-art methods."
    },
    {
        "title": "Backdooring Bias into Text-to-Image Models",
        "link_suffix": "/forum?id=JjQpbbcCSp",
        "link": "https://openreview.net/forum?id=JjQpbbcCSp",
        "pdf_link": "https://openreview.net/pdf?id=JjQpbbcCSp",
        "keywords": "trustworthy ml, fairness, backdoor attack, text-to-image models",
        "abstract": "Text-conditional diffusion models, i.e. text-to-image, produce eye-catching images that represent descriptions given by a user. \nThese images often depict benign concepts but could also carry other purposes. Specifically, visual information is easy to comprehend and could be weaponized for propaganda -- a serious challenge given widespread usage and deployment of generative models.  In this paper, we show that an adversary can add an arbitrary bias through a backdoor attack that would affect even benign users generating images.  While a user could inspect a generated image to comply with the given text description, our attack remains stealthy as it preserves semantic information given in the text prompt. Instead, a compromised model modifies other unspecified features of the image to add desired biases (that increase by $4-8\\times$). Furthermore, we show how the current state-of-the-art generative models make this attack both cheap and feasible for any adversary, with costs ranging between \\$12-\\$18. We evaluate our attack over various types of triggers, adversary objectives, and biases and discuss mitigations and future work."
    },
    {
        "title": "Seeing the Whole in the Parts in Self-Supervised Representation Learning",
        "link_suffix": "/forum?id=NdxI9Yx9f2",
        "link": "https://openreview.net/forum?id=NdxI9Yx9f2",
        "pdf_link": "https://openreview.net/pdf?id=NdxI9Yx9f2",
        "keywords": "Self-supervised learning, statistical learning, bio-inspired learning, infomax",
        "abstract": "Recent successes in self-supervised learning (SSL) have in common forcing the extraction of spatial co-occurrences of visual features, either by masking portions of an image or aggressively cropping it. Here, we propose to model spatial co-occurrences by learning similar visual representations for frequently co-occurring visual features. We present CO-SSL, a family of instance discrimination methods that makes similar global and local representations (before pooling). To analyze the impact of ensuring that local representations correspond to different features within an image, we introduce RF-ResNet, a variant of ResNet that bounds the receptive field of local representations in a configurable fashion. We show that CO-SSL outperforms previous methods in several datasets, including ImageNet-1K where it achieves 71.5% of Top-1 accuracy with 100 pre-training epochs. CO-SSL is also more robust to noise corruption, internal corruption, small adversarial attacks, and large training crop sizes. Our analysis further indicates that CO-SSL learns highly redundant local representations, which may play a key role in the robustness of its representations. Overall, our work suggests that learning similar representations for spatially co-occurring features may be a powerful principle of unsupervised category learning."
    },
    {
        "title": "Procedural Synthesis of Synthesizable Molecules",
        "link_suffix": "/forum?id=OGfyzExd69",
        "link": "https://openreview.net/forum?id=OGfyzExd69",
        "pdf_link": "https://openreview.net/pdf?id=OGfyzExd69",
        "keywords": "molecular design, synthesis planning, tree generation, graph generation",
        "abstract": "Designing synthetically accessible molecules and recommending analogs to unsynthesizable molecules are important problems for accelerating molecular discovery. We reconceptualize both problems using ideas from program synthesis. Drawing inspiration from syntax-guided synthesis approaches, we decouple the syntactic skeleton from the semantics of a synthetic tree to create a bilevel framework for reasoning about the combinatorial space of synthesis pathways. Given a molecule we aim to generate analogs for, we iteratively refine its skeletal characteristics via Markov Chain Monte Carlo simulations over the space of syntactic skeletons. Given a black-box oracle to optimize, we formulate a joint design space over syntactic templates and molecular descriptors and introduce evolutionary algorithms that optimize both syntactic and semantic dimensions synergistically. Our key insight is that once the syntactic skeleton is set, we can amortize over the search complexity of deriving the program's semantics by training policies to fully utilize the fixed horizon Markov Decision Process imposed by the syntactic template. We demonstrate performance advantages of our bilevel framework for synthesizable analog generation and synthesizable molecule design. Notably, our approach offers the user explicit control over the resources required to perform synthesis and biases the design space towards simpler solutions, making it particularly promising for autonomous synthesis platforms."
    },
    {
        "title": "Towards Off-Road Autonomous Driving via Planner Guided Policy Optimization",
        "link_suffix": "/forum?id=uaKBM9sGEm",
        "link": "https://openreview.net/forum?id=uaKBM9sGEm",
        "pdf_link": "https://openreview.net/pdf?id=uaKBM9sGEm",
        "keywords": "Reinforcement learning, Learning from Demonstrations, Autonomous driving, Off-road driving",
        "abstract": "Off-road autonomous driving poses significant challenges such as navigating diverse terrains, avoiding obstacles, and maneuvering through ditches. Addressing these challenges requires effective planning and adaptability, making it a long-horizon planning and control problem. Traditional model-based control techniques like Model Predictive Path Integral (MPPI) require dense sampling and accurate modeling of the vehicle-terrain interaction, both of which are computationally expensive, making effective long-horizon planning in real-time intractable. Reinforcement learning (RL) methods operate without this limitation and are computationally cheaper at deployment. However, exploration in obstacle-dense and challenging terrains is difficult, and typical RL techniques struggle to navigate in these terrains. To alleviate the limitations of MPPI, we propose a hierarchical autonomy pipeline with a low-frequency high-level MPPI planner and a high-frequency low-level RL controller. To tackle RL's exploration challenge, we propose a teacher-student paradigm to learn an end-to-end RL policy, capable of real-time execution and traversal through challenging terrains. The teacher policy is trained using dense planning information from an MPPI planner while the student policy learns to navigate using visual inputs and sparse planning information. In this framework, we introduce a new policy gradient formulation that extends Proximal Policy Optimization (PPO), leveraging off-policy trajectories for teacher guidance and on-policy trajectories for student exploration. We demonstrate our performance in a realistic off-road simulator against various RL and imitation learning methods."
    },
    {
        "title": "Recurrent Action Transformer with Memory",
        "link_suffix": "/forum?id=c4w7WVs1z7",
        "link": "https://openreview.net/forum?id=c4w7WVs1z7",
        "pdf_link": "https://openreview.net/pdf?id=c4w7WVs1z7",
        "keywords": "offline RL, transformers, memory, pomdp",
        "abstract": "Recently, the use of transformers in offline reinforcement learning has become a rapidly developing area. This is due to their ability to treat the agent's trajectory in the environment as a sequence, thereby reducing the policy learning problem to sequence modeling. In environments where the agent's decisions depend on past events (POMDPs), it is essential to capture both the event itself and the decision point in the context of the model. However, the quadratic complexity of the attention mechanism limits the potential for context expansion. One solution to this problem is to extend transformers with memory mechanisms. This paper proposes a Recurrent Action Transformer with Memory (RATE), a novel model architecture that incorporates a recurrent memory mechanism designed to regulate information retention. To evaluate our model, we conducted extensive experiments on memory-intensive environments (ViZDoom-Two-Colors, T-Maze, Memory Maze, Minigrid-Memory), classic Atari games, and MuJoCo control environments. The results show that using memory can significantly improve performance in memory-intensive environments, while maintaining or improving results in classic environments. We believe that our results will stimulate research on memory mechanisms for transformers applicable to offline reinforcement learning. The code is open-sourced and can be found in thehttps://anonymous.4open.science/r/RATE-BAD3."
    }
]