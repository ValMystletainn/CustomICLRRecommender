[
    {
        "title": "DenseAttention: No-Compromise Exact AllN×NInteractions Algorithm withO(N)Space and Time Complexity",
        "link_suffix": "/forum?id=2bIQBDSfRk",
        "link": "https://openreview.net/forum?id=2bIQBDSfRk",
        "pdf_link": "https://openreview.net/pdf?id=2bIQBDSfRk",
        "keywords": "self-attention, deep learning, transformer architecture, nlp, efficient transformers, DenseAttention, long context, Long Range Arena",
        "abstract": "The ubiquitous Transformer architecture suffers from two main bottlenecks: 1) low computational and memory efficiency, leading to suboptimal hardware utilization, and 2) quadratic time complexity with respect to sequence length $N$, making it slow and costly for large data contexts. We propose a novel DenseAttention Network architecture, a straightforward simplification of the standard Transformer block that addresses these issues and serves as a drop-in replacement for language modeling tasks. We eliminate memory-bound components in DenseAttention, including Softmax, masking, one skip connection, and both LayerNorms, as well as key, value, and output projection matrices, as they become redundant. Despite these removals, it maintains exact $N \\times N$ pairwise interactions between tokens. By exploiting the associativity of matrix multiplications, DenseAttention can be computed with $O(N^2d)$ or $O(Nd^2)$ time and space complexity, depending on the context. To handle the absence of Softmax and prevent numerical instability, we introduce MaxNormActivation at both ends of the Transformer block. We also devise Cosine Relative Positional Embeddings as a computationally efficient replacement for RoPE, and simple LocalAttention variations of the block to help the model focus on details in extremely long contexts.DenseAttention competes with FlashAttention in speed on small sequences and outperforms it by orders of magnitude on large contexts. We pre-train encoder language models on sequences up to 16K in length, which perform similarly or better than baseline BERT-large, while significantly improving speed and efficiency.  Finally, we achieve state-of-the-art on the LRA benchmark among the Transformer-based architectures."
    },
    {
        "title": "You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning",
        "link_suffix": "/forum?id=5RZoYIT3u6",
        "link": "https://openreview.net/forum?id=5RZoYIT3u6",
        "pdf_link": "https://openreview.net/pdf?id=5RZoYIT3u6",
        "keywords": "Model Compression, Large Language Models, Structured Pruning",
        "abstract": "The ever-increasing size of large language models (LLMs) presents significant challenges for deployment due to their heavy computational and memory requirements. Current model pruning techniques attempt to alleviate these issues by relying heavily on external calibration datasets to determine which parameters to prune or compress, thus limiting their flexibility and scalability across different compression ratios. Moreover, these methods often cause severe performance degradation, particularly in downstream tasks, when subjected to higher compression rates. In this paper, we proposePruneNet, a novel model compression method that addresses these limitations by reformulating model pruning as a policy learning process. PruneNet decouples the pruning process from the model architecture, eliminating the need for calibration datasets. It learns a stochastic pruning policy to assess parameter importance solely based on intrinsic model properties while preserving the spectral structure to minimize information loss. PruneNet can compress the LLaMA-2-7B model in just 15 minutes, achieving over 80% retention of its zero-shot performance with a 30% compression ratio, outperforming existing methods that retain only 75% performance. Furthermore, on complex multitask language understanding tasks, PruneNet demonstrates its robustness by preserving up to 80% performance of the original model, proving itself a superior alternative to conventional structured compression techniques."
    },
    {
        "title": "Why Not Transform Chat Large Language Models to Non-English?",
        "link_suffix": "/forum?id=US2UCMvzvP",
        "link": "https://openreview.net/forum?id=US2UCMvzvP",
        "pdf_link": "https://openreview.net/pdf?id=US2UCMvzvP",
        "keywords": "Large Language Model, Low Resource Languages, Knowledge Transfer, Catastrophic Forgetting",
        "abstract": "Large language models (LLMs) excel in various tasks, but their performance in non-English languages remains limited due to imbalanced training data. To address this limitation, we explore how to transform chat LLMs to non-English. Chat LLMs offer more advanced capabilities than base LLMs, such as multi-turn conversation and alignment with human preferences. However, transforming chat LLMs presents greater challenges than base LLMs. First, how can we effectively transfer advanced capabilities without their supervised data in target languages? Second, how can we prevent the original capabilities from catastrophic forgetting without replaying their training procedure in English? We target these issues by introducing a simple framework called TransLLM. TransLLM divides the transfer problem into some common sub-tasks with the translation chain-of-thought, eliminating the need for complex training data. More importantly, TransLLM uses two key strategies to prevent catastrophic forgetting: Low-rank adaptation, which preserves the original LLM parameters during training, and recovery KD, which utilizes data generated by the chat LLM itself to recover the original knowledge from the frozen parameters. Experiments conducted across five languages and three LLMs demonstrate the superiority of TransLLM. Notably, TransLLM outperforms GPT-4 in Thai, demonstrating higher levels of helpfulness and safety, using just 8B parameters and publicly accessible data. Our analysis demonstrates how recovery KD combined with LoRA helps mitigate catastrophic forgetting."
    },
    {
        "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data",
        "link_suffix": "/forum?id=Exnt2DcdKD",
        "link": "https://openreview.net/forum?id=Exnt2DcdKD",
        "pdf_link": "https://openreview.net/pdf?id=Exnt2DcdKD",
        "keywords": "continual learning, speech, recognition, datasets, indian languages, multilingual asr",
        "abstract": "We present Nirantar based on a large-scale effort to collect extempore and conversational speech data from participants spanning 22 languages across diverse locations in India. Given the extensive number of languages and locations involved, data is collected in incremental batches. Each batch introduces new languages, new domains (locations), or both, creating a practical playground for continual learning (CL). Nirantar contains a total of  3250 hours of human-transcribed speech data covering 208 Indian districts across 22 languages, with 1720 hours newly released as a part of this work. The data inflow and resulting multilingual multi-domain episodes are based on real-world data collection rather than simulated episodes commonly found in existing CL datasets. In particular, the amount of data collected and the number of languages and domains involved are not uniform across episodes, reflecting a practical and real-world continual learning scenario. This dataset serves as a playground for training and evaluating CL approaches in three different scenarios: Language-Incremental (LIL), Domain-Incremental (DIL), and the novel Language-Incremental Domain-Incremental Learning (LIDIL), which has not been studied before. To establish the dataset's usefulness, we evaluate several existing CL approaches within these scenarios. Our findings indicate that the behaviour of these algorithms varies across the three scenarios, emphasizing the need for detailed independent studies of each."
    },
    {
        "title": "Label Correlation Biases Direct Time Series Forecast",
        "link_suffix": "/forum?id=4A9IdSa1ul",
        "link": "https://openreview.net/forum?id=4A9IdSa1ul",
        "pdf_link": "https://openreview.net/pdf?id=4A9IdSa1ul",
        "keywords": "Time series, Long-term Forecast",
        "abstract": "Time series modeling is uniquely challenged by the presence of autocorrelation in both historical and label sequences. Current research predominantly focuses on handling autocorrelation within the historical sequence but often neglects its presence in the label sequence. Specifically, emerging forecast models mainly conform to the direct forecast (DF) paradigm, generating multi-step forecasts under the assumption of conditional independence within the label sequence. This assumption disregards the inherent autocorrelation in the label sequence, thereby limiting the performance of DF-based models. In response to this gap, we introduce the Frequency-enhanced Direct Forecast (FreDF), which bypasses the complexity of label autocorrelation by learning to forecast in the frequency domain. Our experiments demonstrate that FreDF substantially outperforms existing state-of-the-art methods and is compatible with a variety of forecast models. Code is available athttps://anonymous.4open.science/r/FreDF-0FB1."
    },
    {
        "title": "Large Language Models are Demonstration Pre-Selectors for Themselves",
        "link_suffix": "/forum?id=diKRhKs5yl",
        "link": "https://openreview.net/forum?id=diKRhKs5yl",
        "pdf_link": "https://openreview.net/pdf?id=diKRhKs5yl",
        "keywords": "Large Language Model, Demonstration Pre-Selection, In-Context Learning",
        "abstract": "In-context learning with large language models (LLMs) delivers strong few-shot performance by choosing few-shot demonstrations from the entire training dataset. However, previous few-shot in-context learning methods, which calculate similarity scores for choosing demonstrations, incur high computational costs by repeatedly retrieving large-scale datasets for each query. This is due to their failure to recognize that not all demonstrations are equally informative, and many less informative demonstrations can be inferred from a core set of highly informative ones. To this end, we propose FEEDER (FEw yet Essential Demonstration prE-selectoR), a novel \\emph{pre-selection} framework that identifies a core subset of demonstrations containing the most informative examples. This subset, referred to as the FEEDER set, consists of demonstrations that capture both the ''sufficiency'' and ''necessity'' information to infer the entire dataset. Notice that FEEDER is selected before the few-shot in-context learning, enabling more efficient few-shot demonstrations choosing in a smaller set. To identify FEEDER, we propose a novel effective tree based algorithm. Once selected, it can replace the original dataset, leading to improved efficiency and prediction accuracy in few-shot in-context learning. Additionally, FEEDER also benefit fine-tuning LLMs, we propose a bi-level optimization method enabling more efficient training without sacrificing performance when datasets become smaller. \nOur experiments are on 6 text classification datasets, 1 reasoning dataset, and 1 semantic-parsing dataset, across 6 LLMs (ranging from 335M to 7B parameters), demonstrate that: (i) In few-shot inference, FEEDER achieves superior (or comparable) performance while utilizing only half the input training data. (ii) In fine-tuning, FEEDER significantly boosts the performance of LLMs."
    },
    {
        "title": "PromptWizard: Task-Aware Prompt Optimization Framework",
        "link_suffix": "/forum?id=VZC9aJoI6a",
        "link": "https://openreview.net/forum?id=VZC9aJoI6a",
        "pdf_link": "https://openreview.net/pdf?id=VZC9aJoI6a",
        "keywords": "Prompt optimization, LLMs, task-aware",
        "abstract": "Large language models (LLMs) have transformed AI across diverse domains, with \\textit{prompting} being central to their success in guiding model outputs. However, manual prompt engineering is both labor-intensive and domain-specific, necessitating the need for automated solutions. We introduce PromptWizard, a novel, fully automated framework for discrete prompt optimization, utilizing a self-evolving, self-adapting mechanism. Through a feedback-driven critique and synthesis process, PromptWizard achieves an effective balance between exploration and exploitation, iteratively refining both prompt instructions and in-context examples to generate human-readable, task-specific prompts. This guided approach systematically improves prompt quality, resulting in superior performance across 45 tasks. PromptWizard excels even with limited training data, smaller LLMs, and various LLM architectures. Additionally, our cost analysis reveals a substantial reduction in API calls, token usage, and overall cost, demonstrating PromptWizard's efficiency, scalability, and advantages over existing prompt optimization strategies."
    },
    {
        "title": "From Scaling Law to Sub-Scaling Law: Understanding the Diminishing Returns of Larger Models",
        "link_suffix": "/forum?id=LJ1zlaGdPm",
        "link": "https://openreview.net/forum?id=LJ1zlaGdPm",
        "pdf_link": "https://openreview.net/pdf?id=LJ1zlaGdPm",
        "keywords": "scaling law, large language model",
        "abstract": "Traditional scaling laws suggest that performance metrics of language models improve predictably with increases in model or dataset size. However, recent works display sub-scaling growth for large language models, where performance improvements decelerate as the dataset or model size increases. \nThis study aims to systematically investigate the sub-scaling law phenomenon through an extensive empirical analysis involving over 400 models, ranging from 20 million to 7 billion parameters, with varying datasets and training strategies.\nOur findings indicate that sub-scaling laws arise primarily from high data density and non-optimal training resource allocations. \nSpecifically, we observed that both factors contribute more significantly to performance deceleration than previously anticipated. We examine the sub-scaling phenomenon from two perspectives: data density and training strategy. High data density leads to diminishing marginal gains in performance, while optimal resource allocation is crucial for sustaining performance improvements.\nFurther, we propose a sub-optimal scaling law that generalizes the Chinchilla scaling law to better predict performance and loss in sub-scaling regimes."
    },
    {
        "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
        "link_suffix": "/forum?id=OL44KtasKc",
        "link": "https://openreview.net/forum?id=OL44KtasKc",
        "pdf_link": "https://openreview.net/pdf?id=OL44KtasKc",
        "keywords": "Attention, Quantization",
        "abstract": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of $O(N^2)$, compared to $O(N)$ for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer.\nIn response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1x and 2.7x, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models—including those for large language processing, image generation, and video generation."
    },
    {
        "title": "Foundation Models for Boolean Logic",
        "link_suffix": "/forum?id=qeY25DwmKO",
        "link": "https://openreview.net/forum?id=qeY25DwmKO",
        "pdf_link": "https://openreview.net/pdf?id=qeY25DwmKO",
        "keywords": "Boolean logic, runtime prediction, graph neural networks, multi-task learning, foundation models",
        "abstract": "Boolean logic is fundamental to solving various computational problems, such as Boolean satisfiability (SAT) and model counting, but existing machine learning (ML) approaches for automating algorithm design are computationally expensive and data-intensive. We propose the first foundation model for Boolean logic, leveraging a multi-task dataset of one million instances spanning eighteen tasks. Using graph neural networks (GNNs), we trained models that consistently outperformed single-task models, achieving higher accuracy and faster convergence.  We evaluated the generalization of the foundation models on held-out tasks; we found that models fine-tuned from the foundation model performed better (1) than those fine-tuned from single-task models and (2) performed better than models trained from scratch. We identified a number of crucial design components for training these models, in particular the choice of normalization layer. \nWe showed that a hybrid of different normalization techniques across layers is much more effective than any single normalization layer."
    },
    {
        "title": "Active Causal Learning for Conditional Average Treatment Effect Estimation",
        "link_suffix": "/forum?id=ajORwcxeM7",
        "link": "https://openreview.net/forum?id=ajORwcxeM7",
        "pdf_link": "https://openreview.net/pdf?id=ajORwcxeM7",
        "keywords": "conditional average treatment effect, dynamic sampling, partially observed Markov decision process",
        "abstract": "Estimating conditional average treatment effects (CATE) from observational data is an important problem and is of high practical relevance for many domains. Despite the great efforts of recent studies to accurately estimate CATE, most methods require complete observation of\nall covariates of an individual. However, in real-world scenarios, the acquisition of covariate information is usually done in a active manner, which motivates us to develop methods to minimize the total measurement cost by actively selecting the most appropriate covariates to measure while guaranteeing the CATE estimation accuracy. To this end, in this paper, we first extend the existing methods for estimating CATE to allow accurate estimation in the presence of unmeasured covariates. Next, we theoretically show the advantage of dynamically adjusting the sampling strategy based on an evolving understanding of the information measured in the covariates. Then, we formulate the dynamic sampling strategy learning as a partially observed Markov decision process (POMDP) and further develop a policy gradient method to solve the optimal dynamic policy. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness of our proposed methods."
    },
    {
        "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs",
        "link_suffix": "/forum?id=uHkfU4TaPh",
        "link": "https://openreview.net/forum?id=uHkfU4TaPh",
        "pdf_link": "https://openreview.net/pdf?id=uHkfU4TaPh",
        "keywords": "Large Language Model, Efficient Inference, Key-Value Cache Compression",
        "abstract": "Efficiently managing the KV cache in Large Language Models (LLMs) is a critical challenge for long-context processing tasks such as retrieval-augmented generation (RAG), long text summarization, and multi-document analysis. Extending the context length substantially increases the KV cache size, leading to excessive memory consumption. Existing KV cache compression methods enforce a fixed pattern, neglecting task-specific characteristics, which hampers the effective retention of essential information while discarding less important tokens. In this paper, we introduce a novel Task-Aware KV cache mechanism that dynamically adjusts the KV cache size across different layers based on the characteristics of the tasks. Our approach builds on the significant observation of distinct activation patterns across layers in various tasks, which highlights the need for adaptive strategies tailored to each task's unique demands. Based on this insight, we propose DynamicKV, a method that dynamically optimizes token retention by adjusting the number of tokens retained at each layer, adapting to the specific task. DynamicKV establishes global and per-layer maximum KV cache budgets, temporarily retaining the maximum budget for the current layer, and periodically updating the KV cache sizes of all preceding layers during inference. Our method demonstrates exceptional performance on the LongBench dataset, retaining only 1.7% of the KV cache while preserving 90%, 87%, 78%, and 83% of the original accuracy for LlaMA-3-8B-Instruct, Mistral-7B-Instruct-v0.2, Qwen2-7B-Instruct, and InternLM-2.5-7B-Chat-1M, respectively. When the retained KV cache size is increased to 6.9%, the performance becomes nearly indistinguishable from that without any KV cache compression. Notably, even under extreme compression (0.9%), DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be released to the public."
    },
    {
        "title": "FactBench: A Dynamic Benchmark for In-the-Wild Language Model Factuality Evaluation",
        "link_suffix": "/forum?id=JrpMlotoGX",
        "link": "https://openreview.net/forum?id=JrpMlotoGX",
        "pdf_link": "https://openreview.net/pdf?id=JrpMlotoGX",
        "keywords": "Factuality Evaluation Benchmark, Factuality Evaluation Techniques, LLM Evaluation",
        "abstract": "Language models (LMs) are widely used by an increasing number of users, underscoring the challenge of maintaining factual accuracy across a broad range of topics. We present VERIFY (Verification and Evidence RetrIeval for FactualitY evaluation), a pipeline to evaluate LMs’ factual accuracy in real-world user interactions. VERIFY considers the verifiability of LM-generated content and categorizes content units as supported, unsupported, or undecidable based on the retrieved web evidence. Importantly, VERIFY’s factuality judgments correlate better with human evaluations than existing methods. Using VERIFY, we identify “hallucination prompts” across diverse topics–those eliciting the highest rates of incorrect or unverifiable LM responses. These prompts form FACTBENCH, a dataset of 985 prompts across 213 fine-grained topics. Our dataset captures emerging factuality challenges in real-world LM interactions and is regularly updated with new prompts. We benchmark widely-used LMs from GPT, Gemini, and Llama3.1 family on FACTBENCH, yielding the following key findings: (i) Proprietary models exhibit better factuality, improving from Hard to Easy hallucination prompts. (ii) Llama3.1-405B-Instruct shows comparable or lower factual accuracy than Llama3.1-70B-Instruct across all evaluation methods due to its higher subjectivity that leads to more undecidable content. (iii) Gemini1.5-Pro shows a significantly higher refusal rate, with over-refusal in 25% of cases."
    },
    {
        "title": "Efficient Adaptive Filtering for Deformable Image registration",
        "link_suffix": "/forum?id=0XT3Lg6S2Q",
        "link": "https://openreview.net/forum?id=0XT3Lg6S2Q",
        "pdf_link": "https://openreview.net/pdf?id=0XT3Lg6S2Q",
        "keywords": "Deformable image registration, Adaptive filtering, Bilateral Grid, Piece-wise Smooth",
        "abstract": "In medical image registration tasks where targets exhibit piece-wise smooth structures, a well-designed low-resolution data structure can approximate full-resolution deformation fields with minimal accuracy loss. \nThis physical prior, though absent in current literature, can be integrated into neural networks to enhance registration. \nIn this paper, we propose AdaWarp, a novel architecture that leverages the prior for more efficient medical image registration. \nAdaWarp consists of an encoder, guidance map generator, and a differentiable bilateral grid, introducing an edge-preserving low-frequency approximation of the deformation field. \nThis approach reduces computational complexity without sacrificing accuracy.\nExperiments on two registration datasets covering different modalities and input constraints demonstrate that AdaWarp outperforms existing methods in accuracy-efficiency and accuracy-smoothness tradeoffs."
    },
    {
        "title": "Exposing the Achilles' Heel: Evaluating LLMs Ability to Handle Mistakes in Mathematical Reasoning",
        "link_suffix": "/forum?id=uDZ9d4UAUh",
        "link": "https://openreview.net/forum?id=uDZ9d4UAUh",
        "pdf_link": "https://openreview.net/pdf?id=uDZ9d4UAUh",
        "keywords": "math reasoning, mistake detection, dataset, benchmarking",
        "abstract": "Large Language Models (LLMs) have significantly impacted the field of Math Word Problems (MWPs), transforming how these problems are approached and solved, particularly in educational contexts.  However, existing evaluations often focus on final accuracy, neglecting the critical aspect of reasoning capabilities. This work addresses that gap by evaluating LLMs' abilities to detect and correct reasoning mistakes.  We present a novel dataset, MWP-MISTAKE, containing MWPs with both correct and incorrect reasoning steps generated through rule-based methods and smaller language models. Our comprehensive benchmarking of state-of-the-art models such as GPT-4o and GPT4 uncovers important insights into their strengths and limitations. While GPT-4o excels in mistake detection and rectification, gaps remain, particularly in handling complex datasets and novel problems. Additionally, we identify concerns with data contamination and memorization, which affect LLM reliability in real-world applications. While OpenAI's O1 model demonstrates 90% accuracy in reasoning and final answers on complex tasks, it still remains weak in mistake detection. Our findings highlight the need for improved reasoning evaluations and suggest ways to enhance LLM generalization and robustness in math problem-solving."
    },
    {
        "title": "DDAD: A Two-Pronged Adversarial Defense Based on Distributional Discrepancy",
        "link_suffix": "/forum?id=RzdtpxL0H5",
        "link": "https://openreview.net/forum?id=RzdtpxL0H5",
        "pdf_link": "https://openreview.net/pdf?id=RzdtpxL0H5",
        "keywords": "adversarial defense, adversarial robustness, accuracy-robustness trade-off",
        "abstract": "Statistical adversarial data detection (SADD) detects whether an upcoming batch contains adversarial examples (AEs) by measuring the distributional discrepancies between clean examples (CEs) and AEs. In this paper, we reveal the potential strength of SADD-based methods by theoretically showing that minimizing distributional discrepancy can help reduce the expected loss on AEs. Nevertheless, despite these advantages, SADD-based methods have a potential limitation: they discard inputs detected as AEs, leading to the loss of clean information within those inputs. To address this limitation, we propose a two-pronged adversarial defense method, named Distributional-Discrepancy-based Adversarial Defense (DDAD). In the training phase, DDAD first optimizes the test power of the maximum mean discrepancy (MMD) to derive MMD-OPT, and then trains a denoiser by minimizing the MMD-OPT between CEs and AEs. In the inference phase, DDAD first leverages MMD-OPT to differentiate CEs and AEs, and then applies a two-pronged process: (1) directly feeding the detected CEs into the classifier, and (2) removing noise from the detected AEs by the distributional-discrepancy-based denoiser. Extensive experiments show that DDAD outperforms current state-of-the-art (SOTA) defense methods by notably improving clean and robust accuracy on CIFAR-10 and ImageNet-1K against adaptive white-box attacks. The code is available at:https://anonymous.4open.science/r/DDAD-DB60."
    },
    {
        "title": "Consistency Guaranteed Causal Graph Recovery with Large Language Models",
        "link_suffix": "/forum?id=F4meTCwlxZ",
        "link": "https://openreview.net/forum?id=F4meTCwlxZ",
        "pdf_link": "https://openreview.net/pdf?id=F4meTCwlxZ",
        "keywords": "Causal discovery, causal reasoning, large language models, knowledge extraction",
        "abstract": "Causal graph recovery traditionally relies on statistical estimation of observable variables or individual knowledge, which suffer from data collection biases and knowledge limitations of individuals. Leveraging the broad knowledge in scientific corpus, we propose a novel method for causal graph recovery to deduce causal relationships with the large language models (LLMs) as a knowledge extractor. Our method extracts associational relationships among variables and further eliminates the inconsistent relationship to recover a causal graph using the constraint-based causal discovery methods. Comparing to other LLM-based methods that directly instruct LLMs to do highly complex causal reasoning, our method shows advantages on causal graph quality on benchmark datasets. More importantly, as causal graphs may evolve when new research results emerge, our method shows sensitivity to new evidence in the literature and can provide useful information to update causal graphs accordingly."
    },
    {
        "title": "Bench-O-Matic: Automating Benchmark Curation from Crowdsourced Data",
        "link_suffix": "/forum?id=599F4CZ0HB",
        "link": "https://openreview.net/forum?id=599F4CZ0HB",
        "pdf_link": "https://openreview.net/pdf?id=599F4CZ0HB",
        "keywords": "LLM, Evaluation",
        "abstract": "The rapid evolution of Large Language Models (LLMs) has outpaced the development of model evaluation, highlighting the need for continuous curation of new,\nchallenging benchmarks. However, manual curation of high-quality, human-aligned\nbenchmarks is expensive and time-consuming. To address this, we introduce Bench-O-Matic, an automated pipeline that leverages LLMs to curate high-quality, open-\nended prompts from large, crowd-sourced datasets, enabling continuous benchmark\nupdates without human in the loop. We apply Bench-O-Matic to datasets such as\nChatbot Arena and WildChat-1M, extracting challenging prompts and utilizing\nLLM-as-a-Judge for automatic model evaluation. To validate benchmark quality,\nwe propose new metrics to measure a benchmark’s alignment with human preferences and ability to separate models. We release Eval-O-Matic, a benchmark\nconsisting 500 challenging prompts curated by Bench-O-Matic. Eval-O-Matic\nprovides 3x higher separation of model performances compared to MT-Bench and\nachieves 98.6% correlation with human preference rankings, all at a cost of $20.\nOur work sets a new framework for the scalable curation of automated benchmarks\nfrom extensive data."
    },
    {
        "title": "3D-CT-GPT++: Enhancing 3D Radiology Report Generation with Direct Preference Optimization and Large Vision-Language Models",
        "link_suffix": "/forum?id=LzycEbgLoi",
        "link": "https://openreview.net/forum?id=LzycEbgLoi",
        "pdf_link": "https://openreview.net/pdf?id=LzycEbgLoi",
        "keywords": "Radiology Report Generation, 3D Medical Imaging, Direct Preference Optimization, Multimodal large Language Models",
        "abstract": "Automatically generating radiology reports from three-dimensional medical images, such as 3D CT scans, plays a crucial role in modern diagnostics. Current approaches for generating 3D reports often adopt video processing methods, which struggle to effectively capture the relationships along the Z-axis. Additionally, multimodal large language model-based methods for generating 3D image reports face significant limitations, particularly in terms of the image encoder’s ability to represent 3D structures and the hallucinations that arise in generated content. To address these challenges, we propose the 3D-CT-GPT++ model. This model integrates the optimized 3D image encoder CTViT-V, specifically designed for chest CT scans, and builds upon the LLaVA-1.5 architecture. Furthermore, we introduce \\textit{Direct Preference Optimization (DPO)}, where GPT-4 is used to score the outputs of our fully fine-tuned (SFT) model, creating a preference dataset for subsequent DPO training. DPO significantly reduces hallucinations in the report generation process, ensuring the generated reports are more aligned with clinical needs. We fine-tuned the model on both high-quality private and public datasets to ensure clinical relevance. Extensive experiments were conducted using standard natural language generation (NLG) evaluation metrics, including BLEU, METEOR, and ROUGE-L, to assess the report generation  performance. Experimental results demonstrate that 3D-CT-GPT++ significantly outperforms existing methods in terms of accuracy, fluency, and clinical relevance, advancing the automation of 3D medical report generation."
    },
    {
        "title": "Logarithmic Linear Units (LogLUs): A Novel Activation Function for Improved Convergence in Deep Neural Networks",
        "link_suffix": "/forum?id=1D3TjFidCS",
        "link": "https://openreview.net/forum?id=1D3TjFidCS",
        "pdf_link": "https://openreview.net/pdf?id=1D3TjFidCS",
        "keywords": "Activation Function, Deep Neural Networks, Optimisation",
        "abstract": "The Logarithmic Linear Unit (LogLU) presents a novel activation function for deep neural networks by incorporating logarithmic elements into its design, introducing non-linearity that significantly enhances both training efficiency and accuracy. LogLU effectively addresses common limitations associated with widely used activation functions include ReLU, Leaky ReLU, and ELU, which suffer from issues like the dead neuron problem and vanishing gradients. By enabling neurons to remain active with negative inputs and ensuring effective gradient flow during backpropagation, LogLU promotes more efficient convergence in gradient descent. Its capability to solve fundamental yet complex non-linear tasks, such as the XOR problem, with fewer neurons demonstrates its efficiency in capturing non-linear patterns. Extensive evaluations on benchmark datasets like Caltech 101 and Imagenette, using the InceptionV3 architecture, reveal that LogLU not only accelerates convergence but also enhances model performance compared to existing activation functions. These findings underscore LogLU's potential as an effective activation function that improves both model performance and faster convergence."
    },
    {
        "title": "Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition",
        "link_suffix": "/forum?id=eHehzSDUFp",
        "link": "https://openreview.net/forum?id=eHehzSDUFp",
        "pdf_link": "https://openreview.net/pdf?id=eHehzSDUFp",
        "keywords": "knowledge entropy, knowledge acquisition and forgetting, evolving behavior during LLM pretraining",
        "abstract": "In this work, we investigate how a model's tendency to broadly integrate its parametric knowledge evolves throughout pretraining, and how this behavior affects overall performance, particularly in terms of knowledge acquisition and forgetting. We introduce the concept of knowledge entropy, which quantifies the range of memory sources the model engages with; high knowledge entropy indicates that the model utilizes a wide range of memory sources, while low knowledge entropy suggests reliance on specific sources with greater certainty. Our analysis reveals a consistent decline in knowledge entropy as pretraining advances. We also find that the decline is closely associated with a reduction in the model's ability to acquire and retain knowledge, leading us to conclude that diminishing knowledge entropy (smaller number of active memory sources) impairs the model's knowledge acquisition and retention capabilities. We find further support for this by demonstrating that increasing the activity of inactive memory sources enhances the model's capacity for knowledge acquisition and retention."
    },
    {
        "title": "Diffusion-Based Planning for Autonomous Driving with Flexible Guidance",
        "link_suffix": "/forum?id=wM2sfVgMDH",
        "link": "https://openreview.net/forum?id=wM2sfVgMDH",
        "pdf_link": "https://openreview.net/pdf?id=wM2sfVgMDH",
        "keywords": "diffusion planning, autonomous driving",
        "abstract": "Achieving human-like driving behaviors in complex open-world environments is a critical challenge in autonomous driving. Contemporary learning-based planning approaches such as imitation learning methods often struggle to balance competing objectives and lack of safety assurance,due to limited adaptability and inadequacy in learning complex multi-modal behaviors commonly exhibited in human planning, not to mention their strong reliance on the fallback strategy with predefined rules. We propose a novel transformer-based Diffusion Planner for closed-loop planning, which can effectively model multi-modal driving behavior and ensure trajectory quality without any rule-based refinement. Our model supports joint modeling of both prediction and planning tasks under the same architecture, enabling cooperative behaviors between vehicles. Moreover, by learning the gradient of the trajectory score function and employing a flexible classifier guidance mechanism, Diffusion Planner effectively achieves safe and adaptable planning behaviors. Evaluations on the large-scale real-world autonomous planning benchmark nuPlan and our newly collected 200-hour delivery-vehicle driving dataset demonstrate that Diffusion Planner achieves state-of-the-art closed-loop performance with robust transferability in diverse driving styles."
    },
    {
        "title": "Stacking Small Language Models for Generalizability",
        "link_suffix": "/forum?id=Vn23PakSbM",
        "link": "https://openreview.net/forum?id=Vn23PakSbM",
        "pdf_link": "https://openreview.net/pdf?id=Vn23PakSbM",
        "keywords": "large language model, fine-tuning, natural language, efficient models, small language models",
        "abstract": "Recent advances show that large language models (LLMs) generalize strong performance across different natural language benchmarks. However, the large size of LLMs makes training and inference expensive and impractical to run in resource-limited settings. This paper introduces a new approach called fine-tuning stacks of language models (FSLM), which involves stacking small language models (SLM) as an alternative to LLMs. By fine-tuning each SLM to perform a specific task, this approach breaks down high level reasoning into multiple lower-level steps that specific SLMs are responsible for. As a result, FSLM allows for lower training and inference costs, and also improves model interpretability as each SLM communicates with the subsequent one through natural language. By evaluating FSLM on common natural language benchmarks, this paper highlights promising early results toward generalizable performance using FSLM as a cost-effective alternative to LLMs."
    },
    {
        "title": "AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models",
        "link_suffix": "/forum?id=XwERWxaqIr",
        "link": "https://openreview.net/forum?id=XwERWxaqIr",
        "pdf_link": "https://openreview.net/pdf?id=XwERWxaqIr",
        "keywords": "Diffusion Models, Codebook Quantization, Model Quantization, Model Compression",
        "abstract": "Tremendous investments have been made towards the commodification of diffusion models for generation of diverse media. Their mass-market adoption is however still hobbled by the intense hardware resource requirements of diffusion model inference. Model quantization strategies tailored specifically towards diffusion models have seen considerable success in easing this burden, yet without exception have explored only the Uniform Scalar Quantization (USQ) family of quantization methods. In contrast, Vector Quantization (VQ) methods, which operate on groups of multiple related weights as the basic unit of compression, have recently taken the parallel field of Large Language Model (LLM) quantization by storm. In this work, we for the first time apply codebook-based additive vector quantization algorithms to the problem of diffusion model compression, adapting prior works on the quantization-aware fine-tuning of transformer-based LLMs to take into account the special structure of convolutional weight tensors, the heterogeneity in the kinds of operations performed by the layers of a diffusion model, and the momentum-invalidating discontinuities encountered between successive batches during quantization-aware fine-tuning of diffusion models.  We are rewarded with a data-free distillation framework which achieves to the best of our knowledge state-of-the-art results for the extremely low-bit weight quantization on the standard class-conditional benchmark of LDM-4 on ImageNet at 20 inference time steps. Notably, we report sFID 1.93 points lower than the full-precision model at W4A8, the best-reported results for FID, sFID and ISC at W2A8, and the first-ever successful quantization to W1.5A8 (less than 1.5 bits stored per weight) via a layer-wise heterogeneous quantization strategy. We thus establish a new Pareto frontier for diffusion model inference under low-memory conditions. Furthermore, our method allows for a dynamic trade-off between quantization-time GPU hours and inference-time savings, thus aligning with the recent trend of approaches that combine the best aspects of both Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). We are also able to demonstrate FLOPs savings on arbitrary hardware via an efficient inference kernel, as opposed to BOPs (Bit-wise Operations) savings resulting from small integer operations that may lack broad support across hardware of interest.\\ Code is released via anonymized download link.\\\\footnotesize{\\url{https://osf.io/3uf8v/?view_only=ffbc957d6ce941d7b47bef09b628adcd}."
    },
    {
        "title": "SYNBUILD-3D: A multi-modal synthetic dataset of over 100,000 semantically enriched 3D building wireframes with AI-generated floor plans",
        "link_suffix": "/forum?id=TCSaLeANpN",
        "link": "https://openreview.net/forum?id=TCSaLeANpN",
        "pdf_link": "https://openreview.net/pdf?id=TCSaLeANpN",
        "keywords": "dataset, wireframe, 3D building, generative modeling, geometric deep learning",
        "abstract": "Modeling precise geometric and semantic relationships in 3D remains one of the greatest challenges in generative machine learning today, partly because of a lack of large 3D datasets in the public domain. Drawing upon the successful adoption of synthetic datasets in the computer vision community, we propose to address this challenge in the context of 3D buildings with SYNBUILD-3D, a large, multi-modal, and domain-specific dataset of more than 100,000 3D building wireframes along with their corresponding floor plan images. Unlike existing 3D building datasets, SYNBUILD-3D has been designed with and validated by building modeling and simulation experts, providing rich geometric and semantic information. As a result, SYNBUILD-3D is, to the best of our knowledge, the first 3D building dataset that provides interior and exterior building geometries, including the position and size of doors and windows derived from the floor plans. By releasing SYNBUILD-3D, we aim to offer the geometric deep learning community a high-quality dataset for conditional and unconditional 3D building generation tasks. In contrast to existing datasets that typically focus on modeling either the interior or exterior of 3D objects, SYNBUILD-3D can facilitate the development of generative algorithms that account for both perspectives while incorporating geometric and semantic constraints. The dataset and its associated codebase are available at GITHUB LINK."
    }
]