[{"title": "From Uncontextualized Embeddings to Marginal Feature Effects: Incorporating Intelligibility into Tabular Transformer Networks", "link_suffix": "/forum?id=zbpzJmRNiZ", "link": "https://openreview.net/forum?id=zbpzJmRNiZ", "pdf_link": "https://openreview.net/pdf?id=zbpzJmRNiZ", "keywords": "Tabular Deep Learning, Interpretability, Tabular Transformer Networks", "abstract": "In recent years, deep neural networks have showcased their predictive power across a variety of tasks. The transformer architecture, originally developed for natural language processing, has also shown great efficiency in handling tabular data, offering a competitive alternative to traditional gradient-boosted decision trees in this domain. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts.   To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the model's predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is vailable athttps://anonymous.4open.science/r/nmfrmr-B086.", "title_embedding_index": 21250, "title_abs_embedding_index": 21275}, {"title": "Multi-Shot Character Consistency for Text-to-Video Generation", "link_suffix": "/forum?id=0zRuk3QdiH", "link": "https://openreview.net/forum?id=0zRuk3QdiH", "pdf_link": "https://openreview.net/pdf?id=0zRuk3QdiH", "keywords": "text to video, subject consistency, video personalization, motion alignment, feature injection, extended attention", "abstract": "Text-to-video models have made significant strides in generating short video clips from textual descriptions. Yet, a significant challenge remains: generating several video shots of the same characters, preserving their identity without hurting video quality, dynamics, and responsiveness to text prompts. We present Video Storyboarding, a training-free method to enable pretrained text-to-video models to generate multiple shots with consistent characters, by sharing features between them. Our key insight is that self-attention query features (Q) encode both motion and identity. This creates a hard-to-avoid trade-off between preserving character identity and making videos dynamic, when features are shared. To address this issue, we introduce a novel query injection strategy that balances identity preservation and natural motion retention. This approach improves upon naive consistency techniques applied to videos, which often struggle to maintain this delicate equilibrium. Our experiments demonstrate significant improvements in character consistency across scenes while maintaining high-quality motion and text alignment. These results offer insights into critical stages of video generation and the interplay of structure and motion in video diffusion models.", "title_embedding_index": 21251, "title_abs_embedding_index": 21276}, {"title": "RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation", "link_suffix": "/forum?id=aucMP9hGYv", "link": "https://openreview.net/forum?id=aucMP9hGYv", "pdf_link": "https://openreview.net/pdf?id=aucMP9hGYv", "keywords": "3D generation", "abstract": "Current text-to-3D generation methods based on score distillation often suffer from geometric inconsistencies, leading to repeated patterns across different poses of 3D assets. This issue, known as the Multi-Face Janus problem, arises because existing methods struggle to maintain consistency across varying poses and are biased toward a canonical pose. While recent work has improved pose control and approximation, these efforts are still limited by this inherent bias, which skews the guidance during generation.\nTo address this, we propose a solution called RecDreamer, which reshapes the underlying data distribution to achieve more consistent pose representation. The core idea behind our method is to rectify the prior distribution, ensuring that pose variation is uniformly distributed rather than biased toward a canonical form. By modifying the prescribed distribution through an auxiliary function, we can reconstruct the density of the distribution to ensure compliance with specific marginal constraints. In particular, we ensure that the marginal distribution of poses follows a uniform distribution, thereby eliminating the biases introduced by the prior knowledge.\nWe incorporate this rectified data distribution into existing score distillation algorithms, a process we refer to as uniform score distillation. To efficiently compute the posterior distribution required for the auxiliary function, RecDreamer introduces a training-free classifier that estimates pose categories in a plug-and-play manner. Additionally, we utilize various approximation techniques for noisy states, significantly improving system performance.\nOur experimental results demonstrate that RecDreamer effectively mitigates the Multi-Face Janus problem, leading to more consistent 3D asset generation across different poses.", "title_embedding_index": 21252, "title_abs_embedding_index": 21277}, {"title": "Enabling Sparse Autoencoders for Topic Alignment in Large Language Models", "link_suffix": "/forum?id=uinsufj5TR", "link": "https://openreview.net/forum?id=uinsufj5TR", "pdf_link": "https://openreview.net/pdf?id=uinsufj5TR", "keywords": "Alignment, SAEs, Mechanistic Interpretability, Large Language Models", "abstract": "Applications that generate topic-aligned output from large language models (LLMs) are frequently limited by the computational intensity and lack of interpretability of existing approaches, like fine-tuning. Recent work shows that Sparse Autoencoders (SAE) applied to LLM layers have neurons corresponding to interpretable concepts. Consequently, these SAE neurons can be modified to align generated outputs, but only towards pre-identified topics and with some parameter tuning. Our approach leverages the interpretability properties of SAEs to enable alignment for any topic. This method 1) scores each SAE neuron by its semantic similarity to an alignment text and uses them to 2) modify SAE-layer-level outputs by emphasizing topic-aligned neurons. We assess the alignment capabilities of this approach on diverse public topics datasets, including Amazon reviews, Medicine, and Sycophancy, across open-source LLMs, GPT2, and Gemma with multiple SAEs configurations. Experiments aligning to medical prompts reveal several benefits over fine-tuning, including increased average language acceptability (0.25 vs 0.5), reduced training time across multiple alignment topics (333.6s vs. 62s), and acceptable inference time for many applications (+0.00092s/token). Our anonymized open-source code is available athttps://anonymous.4open.science/r/sae-steering-8513/README.md.", "title_embedding_index": 21253, "title_abs_embedding_index": 21278}, {"title": "Aria-MIDI: A Dataset of MIDI Files for Symbolic Music Modeling", "link_suffix": "/forum?id=X5hrhgndxW", "link": "https://openreview.net/forum?id=X5hrhgndxW", "pdf_link": "https://openreview.net/pdf?id=X5hrhgndxW", "keywords": "music, symbolic music, piano transcription, dataset, midi", "abstract": "We introduce an extensive new dataset of MIDI files, created by transcribing audio\nrecordings of piano performances into their constituent notes. The data pipeline\nwe use is multi-stage, employing a language model to autonomously crawl and\nscore audio recordings from the internet based on their metadata, followed by a\nstage of pruning and segmentation using an audio classifier. The resulting dataset\ncontains over one million distinct MIDI files, comprising roughly 100,000 hours\nof transcribed audio. We provide an in-depth analysis of our techniques, offering\nstatistical insights, and investigate the content by extracting metadata tags, which\nwe also provide.", "title_embedding_index": 21254, "title_abs_embedding_index": 21279}, {"title": "Maximum Noise Level as Third Optimality Criterion in Black-box Optimization Problem", "link_suffix": "/forum?id=SWg72N2ky1", "link": "https://openreview.net/forum?id=SWg72N2ky1", "pdf_link": "https://openreview.net/pdf?id=SWg72N2ky1", "keywords": "Black-box optimization, Higher order smoothness function, Strongly convex optimization, Maximum noise level", "abstract": "This paper is devoted to the study (common in many applications) of the black-box optimization problem, where the black-box represents a gradient-free oracle $\\tilde{f}_p = f(x) + \\xi_p$ providing the objective function value with some stochastic noise. Assuming that the objective function is $\\mu$-strongly convex, and also not just $L$-smooth, but has a higher order of smoothness ($\\beta \\geq 2$) we provide a novel optimization method:Zero-Order Accelerated Batched Stochastic Gradient Descent, whose theoretical analysis closes the question regarding the iteration complexity,achieving optimal estimates. Moreover, we provide a thorough analysis of the maximum noise level, and show under which condition the maximum noise level will take into account information about batch size $B$ as well as information about the smoothness order of the function $\\beta$. Finally, we show the importance of considering the maximum noise level $\\Delta$ as a third optimality criterion along with the standard two on the example of a numerical experiment of interest to the machine learning community, where we compare with SOTA gradient-free algorithms.", "title_embedding_index": 21255, "title_abs_embedding_index": 21280}, {"title": "ESCAPE: Equivariant Shape Completion via Anchor Point Encoding", "link_suffix": "/forum?id=uqG0kFLccD", "link": "https://openreview.net/forum?id=uqG0kFLccD", "pdf_link": "https://openreview.net/pdf?id=uqG0kFLccD", "keywords": "3D Shape Completion, Rotation Equivariance", "abstract": "Shape completion, a crucial task in 3D computer vision, involves predicting and filling the missing regions of scanned or partially observed objects. Current methods often suffer from orientation-dependent inconsistencies, particularly under varying rotations, limiting their real-world applicability. We introduce ESCAPE (Equivariant Shape Completion via Anchor Point Encoding), a novel framework designed to achieve rotation-equivariant shape completion. Our approach employs a distinctive encoding strategy, representing objects by selecting anchor points and utilizing them in a distance-based encoder akin to the D2 shape distribution. This enables the model to capture a consistent, rotation-equivariant understanding of the object\u2019s geometry. ESCAPE leverages a transformer architecture to encode and decode the distance transformations, ensuring that generated shape completions remain accurate and equivariant under rotational transformations. Additionally, we perform optimization to refine the predicted shapes from anchor point positions and predicted encodings, Experimental evaluations demonstrate that ESCAPE achieves robust, high-quality reconstructions across arbitrary rotations and translations, showcasing its effectiveness in real-world applications.", "title_embedding_index": 21256, "title_abs_embedding_index": 21281}, {"title": "AFlow: Automating Agentic Workflow Generation", "link_suffix": "/forum?id=z5uVAKwmjf", "link": "https://openreview.net/forum?id=z5uVAKwmjf", "pdf_link": "https://openreview.net/pdf?id=z5uVAKwmjf", "keywords": "LLM Agent; Prompt Optimization; Workflow Generation", "abstract": "Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce \\textbf{AFlow}, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code will be made available as open-source upon publication.", "title_embedding_index": 21257, "title_abs_embedding_index": 21282}, {"title": "Convex is back:\\Solving Belief MDPs via Convexity-Informed Deep Reinforcement Learning", "link_suffix": "/forum?id=in0Nmo8Ojd", "link": "https://openreview.net/forum?id=in0Nmo8Ojd", "pdf_link": "https://openreview.net/pdf?id=in0Nmo8Ojd", "keywords": "Deep Reinforcement Learning, POMDP, Convexity", "abstract": "We present a novel method for Deep Reinforcement Learning (DRL), incorporating the convex property of the value function over the belief space in Partially Observable Markov Decision Processes (POMDPs). We thus introduce hard- and soft-enforced convexity as two different approaches, and compare their performance against standard DRL on two well-known POMDP environments, namely the Tiger and FieldVisionRockSample problems. Our findings show that including the convexity feature can substantially increase the median and/or maximum performance of the agents, especially when testing on out-of-distribution domains.", "title_embedding_index": 21258, "title_abs_embedding_index": 21283}, {"title": "In-batch Ensemble Drafting: Toward Fast and Robust Speculative Decoding for Multimodal Language Models", "link_suffix": "/forum?id=8o7131Lm83", "link": "https://openreview.net/forum?id=8o7131Lm83", "pdf_link": "https://openreview.net/pdf?id=8o7131Lm83", "keywords": "Speculative decoding, Large language model, Vision language model, Inference Acceleration", "abstract": "Multimodal Large Language Models (MLLMs) have emerged as powerful tools for processing modalities beyond text by combining a visual encoder with Large Language Models (LLMs) to incorporate visual context. This integration, however, leads to higher computational costs during LLM inference, specifically in the Prefill and Decoding stages. Existing MLLM acceleration methods primarily focus on reducing the cost of long prefills caused by visual context, but this approach has limitations: (1) From a latency perspective, it mainly benefits the prefill stage, offering minimal improvements for decoding. (2) It does not guarantee output distributions that are identical to those of the original MLLM. To ensure identical output distribution while mitigating decoding latency, we focus on speculative decoding (SD)\u2014an acceleration technique that uses a smaller draft model verified by a larger model. Despite its importance for LLM acceleration, SD's application to MLLMs remains largely unexplored, even though decoding constitutes a significant portion of MLLM inference latency. We investigate various drafting techniques\u2014multimodal, text-only, image-pooling, and caption-based\u2014for multimodal scenarios and analyze their integration with MLLMs. Building on these insights, we propose In-batch Ensemble Drafting, which combines probability distributions from multiple drafting methods via batch inference during the SD draft phase. This approach requires no additional model parameters, incurs minimal overhead, and significantly increases the likelihood of draft tokens passing verification, thereby enhancing performance and robustness across diverse input scenarios.", "title_embedding_index": 21259, "title_abs_embedding_index": 21284}, {"title": "Rethinking the \"Heatmap + Monte Carlo Tree Search'' Paradigm for Solving Large Scale TSP", "link_suffix": "/forum?id=TMHOHRR0FA", "link": "https://openreview.net/forum?id=TMHOHRR0FA", "pdf_link": "https://openreview.net/pdf?id=TMHOHRR0FA", "keywords": "Travelling Salesman Problem, Heatmap, Monte Carlo Tree Search, Combinatorial optimization, k-nearest neighbor", "abstract": "The Travelling Salesman Problem (TSP) remains a fundamental challenge in combinatorial optimization, inspiring diverse algorithmic strategies. This paper revisits the ``heatmap + Monte Carlo Tree Search (MCTS)\" paradigm that has recently gained traction for learning-based TSP solutions. Within this framework, heatmaps encode the likelihood of edges forming part of the optimal tour, and MCTS refines this probabilistic guidance to discover optimal solutions. Contemporary approaches have predominantly emphasized the refinement of heatmap generation through sophisticated learning models, inadvertently sidelining the critical role of MCTS. Our extensive empirical analysis reveals two pivotal insights: \\textbf{1}) The configuration of MCTS strategies profoundly influences the solution quality, demanding meticulous tuning to leverage their full potential; \\textbf{2}) Our findings demonstrate that a rudimentary and parameter-free heatmap, derived from the intrinsic $k$-nearest nature of TSP, can rival or even surpass the performance of complicated heatmaps, with strong generalizability across various scales. Empirical evaluations across various TSP scales underscore the efficacy of our approach, achieving competitive results. These observations challenge the prevailing focus on heatmap sophistication, advocating a reevaluation of the paradigm to harness both components synergistically.", "title_embedding_index": 21260, "title_abs_embedding_index": 21285}, {"title": "OOD-Chameleon: Is Algorithm Selection for OOD Generalization Learnable?", "link_suffix": "/forum?id=8efAVon0eD", "link": "https://openreview.net/forum?id=8efAVon0eD", "pdf_link": "https://openreview.net/pdf?id=8efAVon0eD", "keywords": "OOD generalization, distribution shifts, algorithm selection, learning to learn", "abstract": "Out-of-distribution (OOD) generalization is challenging because distribution shifts come in many forms. A multitude of learning algorithms exist and each can improve performance inspecificOOD situations. We posit that much of the challenge of OOD generalization lies inchoosing the right algorithm for the right dataset. However, such algorithm selection is often elusive under complex real-world shifts. In this work, we formalize the task ofalgorithm selection for OOD generalizationand investigate whether it could be approached by learning.We propose a solution, dubbed OOD-Chameleon that treats the task as a supervised classification over candidate algorithms. We construct adataset of datasetsto learn from, which represents diverse types, magnitudes and combinations of shifts (covariate shift, label shift, spurious correlations). We train the model to predict the relative performance of algorithms given a dataset's characteristics. This enablesa prioriselection of the best learning strategy, i.e. without training various models as needed with traditional model selection.Our experiments show that the adaptive selection outperforms any individual algorithm and simple selection heuristics, on unseen datasets of controllable and realistic image data. Inspecting the model shows that it learns non-trivial data/algorithms interactions, and reveals the conditions for any one algorithm to surpass another. This opens new avenues for (1) enhancing OOD generalization with existing algorithms instead of designing new ones, and (2) gaining insights into the applicability of existing algorithms with respect to datasets' properties.", "title_embedding_index": 21261, "title_abs_embedding_index": 21286}, {"title": "Benchmarking Antimicrobial Peptide Identification with Sequence and Structure Representation", "link_suffix": "/forum?id=U5gNAmN3h1", "link": "https://openreview.net/forum?id=U5gNAmN3h1", "pdf_link": "https://openreview.net/pdf?id=U5gNAmN3h1", "keywords": "Benchmark, AMP, drug discovery, multi-modal learning", "abstract": "The rapid evolution of drug-resistant (DR) microbial has become a severe issue for human health. Antimicrobial peptides (AMPs) are powerful therapeutic drugs for treating DR microbial, but their clinical application is limited by activity and toxicity. Recently, AI has shown its power in discovering the high-activity AMPs, relying on the database of the AMP's wet-lab activity data.  However, the activity data from this database are collected from thousands of papers, with their different wet lab experiments setting on one or few types of DR bacteria, have further limits the development of AI methods for AMP identification. Moreover, recently AlphaFold has revolutionized the field of drug discovery, but how can we benefit from the predicted structure for AMP discovery still remains unknown. To address the above challenges, we make two contributions. \\textbf{a)} We construct the \\textbf{DRAMPAtlas 1.0} that contains the training set collected from the public and the testing set from our wet lab experiment. Each AMP sequence is equipped with its 3D structure, activity data, and toxicity, where the activity is about six types of DR bacteria. \\textbf{b)} We conduct extensive experiments for AMP identification,  by modeling the 3D structure as voxels or graphs, in conjugate with its sequence information or solely with the structure or sequence. We have made many interesting findings. We hope that our benchmark and findings can benefit the research community to better design the algorithms for high-activity AMP discovery. All code and data associated with the work will be made publicly available after acceptance.", "title_embedding_index": 21262, "title_abs_embedding_index": 21287}, {"title": "Supervised Pre-training for Unsupervised Product-Patent Image Retrieval", "link_suffix": "/forum?id=ntPHPguG1o", "link": "https://openreview.net/forum?id=ntPHPguG1o", "pdf_link": "https://openreview.net/pdf?id=ntPHPguG1o", "keywords": "image retrieval, supervised pre-training, domain gap", "abstract": "Detecting infringing products is essential for protecting intellectual property rights and is often implemented as a product-patent retrieval task. Manual infringement detection is extremely time-consuming, and artificial intelligence plays an increasingly important role. However, most existing methods rely on natural language-based retrieval due to the domain discrepancies between patent images and product images. Due to the lack of sufficient annotated data, this work aims to address the aforementioned issues in an unsupervised setting by answering the following two questions: 1) How can we align the domain gap between patent images and product images using existing technologies? 2) How can we build a powerful backbone to jointly extract the features of patent and product images? Initially, we construct a dataset for patent-product image retrieval, which includes product-patent pairs and unlabeled data. To address the first question, we systematically evaluate three unsupervised approaches to mitigate the domain gap between patent and product images. The results demonstrate that jointly mapping patent and product images to a new feature space is effective. To answer the second question, we propose a novel supervised pre-training paradigm to achieve domain-aligned feature extraction for product and patent edge images. Extensive experiments using various backbones and training pipelines demonstrate the superiority of our supervised pre-training method. The dataset and code of this paper will be made publicly available upon acceptance.", "title_embedding_index": 21263, "title_abs_embedding_index": 21288}, {"title": "From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning", "link_suffix": "/forum?id=lja4JMesmC", "link": "https://openreview.net/forum?id=lja4JMesmC", "pdf_link": "https://openreview.net/pdf?id=lja4JMesmC", "keywords": "Vison Language Models, Task Specific Models, Visual Instruction Tuning", "abstract": "Large vision language models (VLMs) combine large language models with vision encoders, demonstrating promise across various tasks. However, they often underperform in task-specific applications due to domain gaps between pre-training and fine-tuning. We introduce VITask, a novel framework that enhances task-specific adaptability of VLMs by integrating task-specific models (TSMs). VITask employs three key strategies: exemplar prompting (EP), response distribution alignment (RDA), and contrastive response tuning (CRT) to improve the task-specific performance of VLMs by adjusting their response distributions. EP allows TSM features to guide VLMs, while RDA enables VLMs to adapt without TSMs during inference by learning from exemplar-prompted models. CRT further optimizes the ranking of correct image-response pairs, thereby reducing the risk of generating undesired responses. Experiments on 12 medical diagnosis datasets across 9 imaging modalities show that VITask outperforms both vanilla instruction-tuned VLMs and TSMs, showcasing its ability to integrate complementary features from both models effectively. Additionally, VITask offers practical advantages such as flexible TSM integration and robustness to incomplete instructions, making it a versatile and efficient solution for task-specific VLM tuning.", "title_embedding_index": 21264, "title_abs_embedding_index": 21289}, {"title": "Exploring the Design Space of Autoregressive Models for Efficient and Scalable Image Generation", "link_suffix": "/forum?id=zfIxlvKq4u", "link": "https://openreview.net/forum?id=zfIxlvKq4u", "pdf_link": "https://openreview.net/pdf?id=zfIxlvKq4u", "keywords": "Image Generation, Autoregressive Model", "abstract": "Autoregressive (AR) models and their variants are re-revolutionizing visual generation with improved frameworks. However, unlike the well-established practices for building diffusion models, there lacks a comprehensive recipe for AR models, e.g., selecting image tokenizers, model architectures, and AR paradigms. In this work, we delve into the design space of general AR models, including Mask Autoregressive (MAR) models, to identify optimal configurations for efficient and scalable image generation. We first conduct a detailed evaluation of four prevalent image tokenizers across both AR and MAR settings, examining the impact of codebook size (ranging from 1,024 to 262,144) on generation quality, and identify the most effective tokenizer for image generation. Building on these insights, we propose an enhanced MAR model architecture, named Masked Generative Image LLaMA (MaskGIL), comprising of LlamaGen-VQ and Bidirectional LLaMA. To ensure stable scaling, we introduce modifications such as query-key normalization and post-normalization, resulting in a series of class-conditional MaskGIL models, ranging from 111M to 1.4B parameters. MaskGIL significantly improves the MAR baseline, achieving an 3.71 FID comparable to state-of-the-art AR models on the ImageNet 256$\\times$256 benchmark, with only 8 inference steps, far fewer than the 256 steps needed for AR models. Additionally, we introduce a text-conditional MaskGIL model with 775M parameters, capable of flexibly generating images at any resolution with high aesthetics. To bridge AR and MAR image generation, we investigate their combination during the inference phase. We release all models and code to foster further research.", "title_embedding_index": 21265, "title_abs_embedding_index": 21290}, {"title": "Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model", "link_suffix": "/forum?id=D10yarGQNk", "link": "https://openreview.net/forum?id=D10yarGQNk", "pdf_link": "https://openreview.net/pdf?id=D10yarGQNk", "keywords": "Vision-Language Model, Label Propagation, Training-Free", "abstract": "Vision-language models (VLMs) have revolutionized machine learning by leveraging large pre-trained models to tackle various downstream tasks. Despite improvements in label, training, and data efficiency, many state-of-the-art VLMs still require task-specific hyperparameter tuning and fail to fully exploit test samples. To overcome these challenges, we propose a graph-based approach for label-efficient adaptation and inference. Our method dynamically constructs a graph over text prompts, few-shot examples, and test samples, using label propagation for inference without task-specific tuning. Unlike existing zero-shot label propagation techniques, our approach requires no additional unlabeled support set and effectively leverages the test sample manifold through dynamic graph expansion. We further introduce a context-aware feature re-weighting mechanism to improve task adaptation accuracy. Additionally, our method supports efficient graph expansion, enabling real-time inductive inference. Extensive evaluations on downstream tasks, such as fine-grained categorization and out-of-distribution generalization, demonstrate the effectiveness of our approach.", "title_embedding_index": 21266, "title_abs_embedding_index": 21291}, {"title": "AP-LDM: Attentive and Progressive Latent Diffusion Model for Training-Free High-Resolution Image Generation", "link_suffix": "/forum?id=OIqOpdyhTd", "link": "https://openreview.net/forum?id=OIqOpdyhTd", "pdf_link": "https://openreview.net/pdf?id=OIqOpdyhTd", "keywords": "Diffusion Model, High-Resolution Image, Attentive Guidance, progressive pixel space upsampling", "abstract": "Latent diffusion models (LDMs), such as Stable Diffusion, often experience significant structural distortions when directly generating high-resolution (HR) images that exceed their original training resolutions. A straightforward and cost-effective solution is to adapt pre-trained LDMs for HR image generation; however, existing methods often suffer from poor image quality and long inference time.In this paper, we propose an Attentive and Progressive LDM (AP-LDM), a novel, training-free framework aimed at enhancing HR image quality while accelerating the generation process.AP-LDM decomposes the denoising process of LDMs into two stages: (i) attentive training-resolution denoising, and (ii) progressive high-resolution denoising. The first stage generates a latent representation of a higher-quality training-resolution image through the proposed attentive guidance, which utilizes a novel parameter-free self-attention mechanism to enhance the structural consistency. The second stage progressively performs upsampling in pixel space, alleviating the severe artifacts caused by latent space upsampling.Leveraging the effective initialization from the first stage enables denoising at higher resolutions with significantly fewer steps, enhancing overall efficiency. Extensive experimental results demonstrate that AP-LDM significantly outperforms state-of-the-art methods, delivering up to a 5x speedup in HR image generation, thereby highlighting its substantial advantages for real-world applications.", "title_embedding_index": 21267, "title_abs_embedding_index": 21292}, {"title": "Event-aided Dense and Continuous Point Tracking", "link_suffix": "/forum?id=1GIVx7COef", "link": "https://openreview.net/forum?id=1GIVx7COef", "pdf_link": "https://openreview.net/pdf?id=1GIVx7COef", "keywords": "event camera, dense point tracking, continuous motion, motion representation", "abstract": "Recent point tracking methods have made great strides in recovering the trajectories of any point (especially key points) in long video sequences associated with large motions. \nHowever, the spatial and temporal granularity of point trajectories remains constrained by limited motion estimation accuracy and video frame rate. \nLeveraging the high temporal resolution motion sensitivity of event cameras, we introduce event data for the first time to recover spatially dense and temporally continuous trajectories of any point at any time. \nSpecifically, we define the dense and continuous point trajectory representation as estimating multiple control points of curves for each pixel and model the movement of sparse events triggered along continuous point trajectories. \nBuilding on this, we propose a novel multi-frame iterative streaming framework that first estimates local inter-frame motion representations from two consecutive frames and inter-frame events, then aggregates them into a global long-term motion representation to utilize input video and event data with an arbitrary number of frames. \nExtensive experiments on simulated and real-world data demonstrate the significant improvement of our framework over state-of-the-art methods and the crucial role of introducing events for modeling continuous point trajectories.", "title_embedding_index": 21268, "title_abs_embedding_index": 21293}, {"title": "Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization", "link_suffix": "/forum?id=TrJ36UfD9P", "link": "https://openreview.net/forum?id=TrJ36UfD9P", "pdf_link": "https://openreview.net/pdf?id=TrJ36UfD9P", "keywords": "Optimization, Federated Learning, Distributed Optimization, Local Training, Random Reshuffling, Generalized Smoothness", "abstract": "Non-convex Machine Learning problems typically do not adhere to the standard smoothness assumption. Based on empirical findings, Zhang et al. (2020b) proposed a more realistic generalized $(L_0,L_1)$-smoothness assumption, though it remains largely unexplored. Many existing algorithms designed for standard smooth problems need to be revised. However, in the context of Federated Learning, only a few works address this problem but rely on additional limiting assumptions. In this paper, we address this gap in the literature: we propose and analyze new methods with local steps, partial participation of clients, and Random Reshuffling without extra restrictive assumptions beyond generalized smoothness. The proposed methods are based on the proper interplay between clients' and server's stepsizes and gradient clipping. Furthermore, we perform the first analysis of these methods under the Polyak-\u0141ojasiewicz condition. Our theory is consistent with the known results for standard smooth problems, and our experimental results support the theoretical insights.", "title_embedding_index": 21269, "title_abs_embedding_index": 21294}, {"title": "Federated Learning Can Find Friends That Are Advantageous", "link_suffix": "/forum?id=icVRZJTK9v", "link": "https://openreview.net/forum?id=icVRZJTK9v", "pdf_link": "https://openreview.net/pdf?id=icVRZJTK9v", "keywords": "Federated Learning, Distributed Learning, Personalized Learning", "abstract": "In Federated Learning (FL), the distributed nature and heterogeneity of client data present both opportunities and challenges. While collaboration among clients can significantly enhance the learning process, not all collaborations are beneficial; some may even be detrimental. In this study, we introduce a novel algorithm that assigns adaptive aggregation weights to clients participating in FL training, identifying those with data distributions most conducive to a specific learning objective. We demonstrate that our aggregation method converges no worse than the method that aggregates only the updates received from clients with the same data distribution. Furthermore, empirical evaluations consistently reveal that collaborations guided by our algorithm outperform traditional FL approaches. This underscores the critical role of judicious client selection and lays the foundation for more streamlined and effective FL implementations in the coming years.", "title_embedding_index": 21270, "title_abs_embedding_index": 21295}, {"title": "From Risk to Uncertainty: Generating Predictive Uncertainty Measures via Bayesian Estimation", "link_suffix": "/forum?id=cWfpt2t37q", "link": "https://openreview.net/forum?id=cWfpt2t37q", "pdf_link": "https://openreview.net/pdf?id=cWfpt2t37q", "keywords": "Uncertainty quantification, Bayesian methods, Statistics", "abstract": "There are various measures of predictive uncertainty in the literature, but their relationships to each other remain unclear. This paper uses a decomposition of statistical pointwise risk into components associated with different sources of predictive uncertainty: namely, aleatoric uncertainty (inherent data variability) and epistemic uncertainty (model-related uncertainty). Together with Bayesian methods applied as approximations, we build a framework that allows one to generate different predictive uncertainty measures.We validate measures, derived from our framework on image datasets by evaluating its performance in detecting out-of-distribution and misclassified instances using the AUROC metric. The experimental results confirm that the measures derived from our framework are useful for the considered downstream tasks.", "title_embedding_index": 21271, "title_abs_embedding_index": 21296}, {"title": "A Neural Material Point Method for Particle-based Simulations", "link_suffix": "/forum?id=IBOeJJUYaC", "link": "https://openreview.net/forum?id=IBOeJJUYaC", "pdf_link": "https://openreview.net/pdf?id=IBOeJJUYaC", "keywords": "Neural emulation, simulation, SciML, particle-based simulators", "abstract": "Mesh-free Lagrangian methods are widely used for simulating fluids, solids, and their complex interactions due to their ability to handle large deformations and topological changes. These physics simulators, however, require substantial computational resources for accurate simulations. To address these issues, deep learning emulators promise faster and scalable simulations, yet they often remain expensive and difficult to train, limiting their practical use. Inspired by the Material Point Method (MPM), we present NeuralMPM, a neural emulation framework for particle-based simulations. NeuralMPM interpolates Lagrangian particles onto a fixed-size grid, computes updates on grid nodes using image-to-image neural networks, and interpolates back to the particles. Similarly to MPM, NeuralMPM benefits from the regular voxelized representation to simplify the computation of the state dynamics, while avoiding the drawbacks of mesh-based Eulerian methods. We demonstrate the advantages of NeuralMPM on several datasets, including fluid dynamics and fluid-solid interactions. Compared to existing methods, NeuralMPM reduces training times from days to hours, while achieving comparable or superior long-term accuracy, making it a promising approach for practical forward and inverse problems. A project page is available at this [URL]", "title_embedding_index": 21272, "title_abs_embedding_index": 21297}, {"title": "Timer-XL: Long-Context Transformers for Unified Time Series Forecasting", "link_suffix": "/forum?id=KMCJXjlDDr", "link": "https://openreview.net/forum?id=KMCJXjlDDr", "pdf_link": "https://openreview.net/pdf?id=KMCJXjlDDr", "keywords": "Time Series Forecasting, Transformer", "abstract": "We present Timer-XL, a generative Transformer for unified time series forecasting. To uniformly predict 1D and 2D time series, we generalize next token prediction, predominantly adopted for causal generation of 1D sequences, to multivariate next token prediction. The proposed paradigm uniformly formulates various forecasting scenarios as a long-context generation problem. We opt for the generative Transformer, which can capture global-range and causal dependencies while providing contextual flexibility, to implement unified forecasting on univariate series characterized by non-stationarity, multivariate time series with complicated dynamics and correlations, and covariate-informed contexts that include both endogenous and exogenous variables. Technically, we propose a universal TimeAttention to facilitate generative Transformers on time series, which can effectively capture fine-grained intra- and inter-series dependencies of flattened time series tokens (patches) and is further strengthened by position embeddings in both temporal and variable dimensions. Timer-XL achieves state-of-the-art performance across challenging forecasting benchmarks through a unified approach. As a large time series model, it demonstrates notable model transferability by large-scale pre-training, as well as contextual flexibility in token lengths, positioning it as a one-for-all forecaster.", "title_embedding_index": 21273, "title_abs_embedding_index": 21298}, {"title": "Enhanced Semantic Alignment in Transformer Tracking via Position Learning and Force-Directed Attention", "link_suffix": "/forum?id=rsJaUHCZIv", "link": "https://openreview.net/forum?id=rsJaUHCZIv", "pdf_link": "https://openreview.net/pdf?id=rsJaUHCZIv", "keywords": "Transformer Tracking; Single Object Tracking; Semantic Alignment; Self-supervised Position Loss; Force-Directed Attention", "abstract": "In the field of visual object tracking, one-stream pipelines have become the mainstream framework due to its efficient integration of feature extraction and relationship modeling. \nHowever, existing methods still face the issue of semantic misalignment: \nfirstly, the interaction of positional encoding between the two branches leads to a misalignment between feature semantics and position encoding; \nsecondly, traditional attention mechanisms fail to distinguish between semantic attraction and repulsion among features, resulting in semantic misalignment when the model processes these features. \nTo address these issues, we propose an Enhanced Semantic Alignment Transformer Tracker (ESAT) with position encode learning and force-directed attention mechanism. \nBy leveraging positional encoding loss, ESAT separately learns the absolute positional encodings of the target and search branches, distinguishing the locations of various tokens and their positive or negative relationships, thereby enhancing the semantic consistency between position and features.\nAdditionally, it incorporates a repulsion-attraction mechanism applied to the self-attention module, simulating dynamic interactions between nodes to improve feature discrimination. \nExtensive experiments on multiple public tracking datasets show that our method outperforms many pipelines and achieves superior performance on five challenging benchmarks.", "title_embedding_index": 21274, "title_abs_embedding_index": 21299}]