[
    {
        "title": "Mitigating Shortcut Learning with Diffusion Counterfactuals and Diverse Ensembles",
        "link_suffix": "/forum?id=SvydqVoHrp",
        "link": "https://openreview.net/forum?id=SvydqVoHrp",
        "pdf_link": "https://openreview.net/pdf?id=SvydqVoHrp",
        "keywords": "Shortcut Learning, Simplicity Bias, Bias Mitigation, Debias, Ensembles, Diffusion Models, counterfactuals, Diversification",
        "abstract": "Spurious correlations in the data, where multiple cues are predictive of the target labels, often lead to a phenomenon known as shortcut learning, where a model relies on erroneous, easy-to-learn cues while ignoring reliable ones. In this work, we propose DiffDiv an ensemble diversification framework exploiting Diffusion Probabilistic Models (DPMs) to mitigate this form of bias. We show that at particular training intervals, DPMs can generate images with novel feature combinations, even when trained on samples displaying correlated input features. We leverage this crucial property to generate synthetic counterfactuals to increase model diversity via ensemble disagreement. We show that DPM-guided diversification is sufficient to remove dependence on shortcut cues, without a need for additional supervised signals. We further empirically quantify its efficacy on several diversification objectives, and finally show improved generalization and diversification on par with prior work that relies on auxiliary data collection."
    },
    {
        "title": "The Breakdown of Gaussian Universality in Classification of High-dimensional Mixtures",
        "link_suffix": "/forum?id=UrKbn51HjA",
        "link": "https://openreview.net/forum?id=UrKbn51HjA",
        "pdf_link": "https://openreview.net/pdf?id=UrKbn51HjA",
        "keywords": "High-dimensional statistics, random matrix theory, Gaussian universality, empirical risk minimization, mixture models, linear factor models",
        "abstract": "The assumption of Gaussian or Gaussian mixture data has been extensively exploited in a long series of precise performance analyses of machine learning (ML) methods, on large datasets having comparably numerous samples and features. \nTo relax this restrictive assumption, subsequent efforts have been devoted to establish \"Gaussian equivalent principles\" by studying scenarios of Gaussian universality where the asymptotic performance of ML methods on non-Gaussian data remains unchanged when replaced with Gaussian data having thesame mean and covariance.\nBeyond the realm of Gaussian universality, there are few exact results on how the data distribution affects the learning performance.In this article, we provide a precise high-dimensional characterization of empirical risk minimization, for classification under a general mixture data setting of linear factor models that extends Gaussian mixtures. \nThe Gaussian universality is shown to break down under this setting, in the sense that the asymptotic learning performance depends on the data distributionbeyondthe class means and covariances.\nTo clarify the limitations of Gaussian universality in classification of mixture data and to understand the impact of its breakdown, we specify conditions for Gaussian universality and discuss their implications for the choice of loss function."
    },
    {
        "title": "Spurious Privacy Leakage in Neural Networks",
        "link_suffix": "/forum?id=vuvG5rNBra",
        "link": "https://openreview.net/forum?id=vuvG5rNBra",
        "pdf_link": "https://openreview.net/pdf?id=vuvG5rNBra",
        "keywords": "spurious correlation, membership inference, privacy, robustness, safety",
        "abstract": "Neural networks are vulnerable to privacy attacks aimed at stealing sensitive data. When trained on real-world datasets, these models can also inherit latent biases, which may further increase privacy risks. In this work, we investigate the impact of spurious correlation bias on privacy vulnerability, identifying several key challenges. We introducespurious privacy leakage, a phenomenon where spurious groups can be up to 100 times more vulnerable to privacy attacks than non-spurious groups, and demonstrate how this leakage is connected to task complexity. Furthermore, while robust training methods can mitigate the performance disparity across groups, they fail to reduce privacy vulnerability, and even differential privacy is ineffective in protecting the most vulnerable spurious group in practice. Finally, we compare model architectures in terms of both performance and privacy, revisiting prior research with novel insights."
    },
    {
        "title": "Century: A Framework and Dataset for Evaluating Historical Contextualisation of Sensitive Images",
        "link_suffix": "/forum?id=1KLBvrYz3V",
        "link": "https://openreview.net/forum?id=1KLBvrYz3V",
        "pdf_link": "https://openreview.net/pdf?id=1KLBvrYz3V",
        "keywords": "historical, contextualisation, image, dataset, multimodal, VLM, evaluation",
        "abstract": "How do multi-modal generative models describe images of recent historical events and figures, whose legacies may be nuanced, multifaceted, or contested? This task necessitates not only accurate visual recognition, but also socio-cultural knowledge and cross-modal reasoning.  To address this evaluation challenge, we introduce Century -- a novel dataset of sensitive historical images. This dataset consists of 1,500 images from recent history, created through an automated method combining knowledge graphs and language models with quality and diversity criteria created from the practices of museums and digital archives. We demonstrate through automated and human evaluation that this method produces a set of images that depict events and figures that are diverse across topics and represents all regions of the world.\nWe additionally propose an evaluation framework for evaluating the historical contextualisation capabilities along dimensions of accuracy, thoroughness, and objectivity. We demonstrate this approach by using Century to evaluate four foundation models, scoring performance using both automated and human evaluation. We find that historical contextualisation of sensitive images poses a significant challenge for modern multi-modal foundation models, and offer practical recommendations for how developers can use Century to evaluate improvements to models and applications."
    },
    {
        "title": "MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos",
        "link_suffix": "/forum?id=2prShxdLkX",
        "link": "https://openreview.net/forum?id=2prShxdLkX",
        "pdf_link": "https://openreview.net/pdf?id=2prShxdLkX",
        "keywords": "3D Gaussian Splatting, Dynamic Novel-view Synthesis, Neural Rendering",
        "abstract": "In this paper, we propose MoDGS, a new pipeline to render novel-view images in dynamic scenes using only casually captured monocular videos. Previous monocular dynamic NeRF or Gaussian Splatting methods strongly rely on the rapid movement of input cameras to construct multiview consistency but fail to reconstruct dynamic scenes on casually captured input videos whose cameras are static or move slowly. To address this challenging task, MoDGS adopts recent single-view depth estimation methods to guide the learning of the dynamic scene. Then, a novel 3D-aware initialization method is proposed to learn a reasonable deformation field and a new robust depth loss is proposed to guide the learning of dynamic scene geometry. Comprehensive experiments demonstrate that MoDGS is able to render high-quality novel view images of dynamic scenes from just a casually captured monocular video, which outperforms baseline methods by a significant margin."
    },
    {
        "title": "A bird's eye view on informed classification",
        "link_suffix": "/forum?id=uBMNOjqHUV",
        "link": "https://openreview.net/forum?id=uBMNOjqHUV",
        "pdf_link": "https://openreview.net/pdf?id=uBMNOjqHUV",
        "keywords": "Neurosymbolic, Deep Learning, Knowledge Representation, Probabilistic Reasoning",
        "abstract": "Neurosymbolic AI is a growing field of research aiming to combine neural network learning capabilities with the reasoning abilities of symbolic systems. In this paper, we tackle informed classification tasks, i.e. multi-label classification tasks informed by prior knowledge that specifies which combinations of labels are semantically valid. Several neurosymbolic formalisms and techniques have been introduced in the literature, each relying on a particular language to represent prior knowledge. We take a bird's eye view on informed classification and introduce a unified formalism that encapsulates all knowledge representation languages. Then, we build upon this formalism to identify several concepts in probabilistic reasoning that are at the core of many techniques across representation languages. We also define a new technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected, an interesting property in the era of off-the-shelves and foundation models. We discuss its theoritical and practical advantages over two other probabilistic neurosymbolic techniques: semantic conditioning and semantic regularization. We then evaluate experimentally and compare the benefits of all three techniques on several large-scale datasets. Our results show that, despite only working at inference, our technique can efficiently leverage prior knowledge to build more accurate neural-based systems."
    },
    {
        "title": "Flashback: Understanding and Mitigating Forgetting in Federated Learning",
        "link_suffix": "/forum?id=lJWHOT1ZAV",
        "link": "https://openreview.net/forum?id=lJWHOT1ZAV",
        "pdf_link": "https://openreview.net/pdf?id=lJWHOT1ZAV",
        "keywords": "Federated Learning, Machine Learning, Forgetting, Distillation",
        "abstract": "In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, especially in the presence of severe data heterogeneity among clients.\n    This study explores the nuances of this issue, emphasizing the critical role of forgetting leading to FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition.\n    Based on this, we propose Flashback, a novel FL algorithm with a dynamic distillation approach that regularizes the local models and effectively aggregates their knowledge.\n    The results from extensive experimentation across different benchmarks show that Flashback mitigates forgetting and outperforms other state-of-the-art methods, achieving faster round-to-target accuracy by converging in 6 to 16 rounds, being up to 27$\\times$ faster."
    },
    {
        "title": "Differentiable Solver Search for fast diffusion sampling",
        "link_suffix": "/forum?id=FB84Wkn3Xp",
        "link": "https://openreview.net/forum?id=FB84Wkn3Xp",
        "pdf_link": "https://openreview.net/pdf?id=FB84Wkn3Xp",
        "keywords": "Generative models, Solver, Sampler, FlowMatching",
        "abstract": "Diffusion-based models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal and reveals a compact search space comprised of timestep and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify the optimal solver. Equipped with the searched solver, our rectified flow models, SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet-$256\\times256$ with only 10 steps. Meanwhile, our DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates its generality across various model architectures, resolutions, and model sizes."
    },
    {
        "title": "UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs",
        "link_suffix": "/forum?id=J9Ofr1PmvX",
        "link": "https://openreview.net/forum?id=J9Ofr1PmvX",
        "pdf_link": "https://openreview.net/pdf?id=J9Ofr1PmvX",
        "keywords": "Machine Unlearning, Large Language Models",
        "abstract": "The key components of machine learning are data samples for training, models for learning patterns, and loss functions for optimizing accuracy. Analogously, unlearning can potentially be achieved through anti-data samples (or anti-samples), unlearning methods, and reversed loss functions. While prior research has explored unlearning methods and reversed loss functions, the potential of anti-samples remains largely untapped. In this paper, we introduce UnSTAR: $\\underline{\\text{Un}}$learning with $\\underline{\\text{S}}$elf-$\\underline{\\text{T}}$aught $\\underline{\\text{A}}$nti-Sample $\\underline{\\text{R}}$easoning  for large language models (LLMs). Our contributions are threefold: first, we propose a novel concept of anti-sample-induced unlearning; second, we generate anti-samples by leveraging misleading rationales, which help reverse learned associations and accelerate the unlearning process; and third, we enable fine-grained targeted unlearning, allowing for the selective removal of specific associations without impacting related knowledge\u2014something not achievable by previous works. Results demonstrate that anti-samples offer an efficient, targeted unlearning strategy for LLMs, opening new avenues for privacy-preserving machine learning and model modification."
    },
    {
        "title": "Combining Analytical Smoothing with Surrogate Losses for Improved Decision-Focused Learning",
        "link_suffix": "/forum?id=ln6QnzBd8o",
        "link": "https://openreview.net/forum?id=ln6QnzBd8o",
        "pdf_link": "https://openreview.net/pdf?id=ln6QnzBd8o",
        "keywords": "predict-then-optimize, decision-focused learning, contextual stochastic optimization, surrogate loss",
        "abstract": "Many combinatorial optimization problems in routing, scheduling, and assignment involve parameters such as price or travel time that must be predicted from data; so-called predict-then-optimize (PtO) problems. Decision-focused learning (DFL) is a family of successful end-to-end techniques for PtO that trains machine learning models to minimize the error of the downstream optimization problems. For each instance, this requires computing the derivative of the optimization problem\u2019s solution with respect to the predicted input parameters.\nPrevious works in DFL employ two main approaches when the parameters appear linearly in the objective: (a) using a differentiable surrogate loss instead of regret; or (b) turning the combinatorial optimization problem into a differentiable mapping by smoothing the optimization to a quadratic program or other smooth convex optimization problem and minimizing the regret of that. We argue that while smoothing makes the optimization differentiable, for a large part, the derivative remains approximately zero almost everywhere, with highly non-zero values near the transition points. To address this plateau effect, we propose minimizing a surrogate loss even after smoothing. We experimentally demonstrate the advantage of minimizing surrogate losses instead of the regret after smoothing across a series of problems. Furthermore, we show that by minimizing a surrogate loss, a recently developed fast, fully neural optimization layer matches state-of-the-art performance while dramatically reducing training time up to five-fold. Thus, our paper opens new avenues for efficient and scalable DFL techniques."
    },
    {
        "title": "Context-aware Prompt Tuning: Advancing In-Context Learning with Adversarial Methods",
        "link_suffix": "/forum?id=J6Xgra2bE5",
        "link": "https://openreview.net/forum?id=J6Xgra2bE5",
        "pdf_link": "https://openreview.net/pdf?id=J6Xgra2bE5",
        "keywords": "Large Language Models, In-Context Learning, and Adversarial Attacks",
        "abstract": "Fine-tuning Large Language Models (LLMs) typically involves updating at least a few billions of parameters. A more parameter-efficient approach is Prompt Tuning (PT), which updates only a few learnable tokens, and differently, In-Context Learning (ICL) adapts the model to a new task by simply including examples in the input without any training. When applying optimization-based methods, such as fine-tuning and PT for few-shot learning, the model is specifically adapted to the small set of training examples, whereas ICL leaves the model unchanged. This distinction makes traditional learning methods more prone to overfitting; in contrast, ICL is less sensitive to the few-shot scenario. While ICL is not prone to overfitting, it does not fully extract the information that exists in the training examples. This work introduces Context-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and adversarial attacks. We build on the ICL strategy of concatenating examples before the input, but we extend this by PT-like learning, refining the context embedding through iterative optimization to extract deeper insights from the training examples. We carefully modify specific context tokens, considering the unique structure of input and output formats. Inspired by adversarial attacks, we adjust the input based on the labels present in the context, focusing on minimizing, rather than maximizing, the loss. Moreover, we apply a projected gradient descent algorithm to keep token embeddings close to their original values, under the assumption that the user-provided data is inherently valuable. Our method has been shown to achieve superior accuracy across multiple classification tasks using various LLM models."
    },
    {
        "title": "CRAFT: Time Series Forecasting with Cross-Future Behavior Awareness",
        "link_suffix": "/forum?id=PLYqJVV7dm",
        "link": "https://openreview.net/forum?id=PLYqJVV7dm",
        "pdf_link": "https://openreview.net/pdf?id=PLYqJVV7dm",
        "keywords": "Time Series Forecasting, Cross-Future Behavior, Koopman Theory, Hierarchical Structure",
        "abstract": "Time series forecasting is the crucial infrastructure in the field of e-commerce, providing technical support for consumer behavior analysis, sales trends forecasting, etc. E-commerce allows consumers to reserve in advance. These pre-booking features reflect future sales trends and can increase the certainty of time series forecasting issues. In this paper, we define these features as Cross-Future Behavior, which occurs before the current time but takes effect in the future. To increase the performance of time series forecasting, we leverage these features and propose the CRoss-Future Behavior Awareness based Time Series Forecasting method (CRAFT). The core idea of CRAFT is to utilize the trend of cross-future behavior to mine the trend of time series data to be predicted. Specifically, to settle the sparse and partial flaws of cross-future behavior, CRAFT employs the Koopman Predictor Module to extract the key trend and the Internal Trend Mining Module to supplement the unknown area of the cross-future behavior matrix. Then, we introduce the External Trend Guide Module with a hierarchical structure to acquire more representative trends from higher levels. Finally, we apply the demand-constrained loss to calibrate the distribution deviation of prediction results. We conduct experiments on real-world dataset. Experiments on both offline large-scale dataset and online A/B test demonstrate the effectiveness of CRAFT. Our dataset and code will be released after formal publication."
    },
    {
        "title": "HaDeMiF: Hallucination Detection and Mitigation in Large Language Models",
        "link_suffix": "/forum?id=VwOYxPScxB",
        "link": "https://openreview.net/forum?id=VwOYxPScxB",
        "pdf_link": "https://openreview.net/pdf?id=VwOYxPScxB",
        "keywords": "Large language model, knowledge hallucination, hallucination detection, model calibration, deep neural decision tree",
        "abstract": "The phenomenon of knowledge hallucinations has raised substantial concerns about the security and reliability of deployed large language models (LLMs). Current methods for detecting hallucinations primarily depend on manually designed individual metrics, such as prediction uncertainty and consistency, and fall short in effectively calibrating model predictions, thus constraining their detection accuracy and applicability in practical applications. In response, we propose an advanced framework, termed HaDeMiF, for detecting and mitigating hallucinations in LLMs. Specifically, hallucinations within the output and semantic spaces of LLMs are comprehensively captured through two compact networks\u2014a novel, interpretable tree model known as the Deep Dynamic Decision Tree (D3T) and a Multilayer Perceptron (MLP)\u2014which take as input a set of prediction characteristics and the hidden states of tokens, respectively. The predictions of LLMs are subsequently calibrated using the outputs from the D3T and MLP networks, aiming to mitigate hallucinations and enhance the reliability of the model generations. HaDeMiF can be applied during both the inference and fine-tuning phases of LLMs, introducing less than 2% of the parameters relative to the LLMs through the training of two small-scale networks. Extensive experiments conducted on multiple prevalent LLMs conclusively demonstrate the effectiveness of our framework in hallucination detection and model calibration across text generation tasks with responses of varying lengths."
    },
    {
        "title": "VR-Sampling: Accelerating Flow Generative Model Training with Variance Reduction Sampling",
        "link_suffix": "/forum?id=x3jRzVAltZ",
        "link": "https://openreview.net/forum?id=x3jRzVAltZ",
        "pdf_link": "https://openreview.net/pdf?id=x3jRzVAltZ",
        "keywords": "Flow Generative Models, Training Acceleration, Diffusion Models",
        "abstract": "Recent advancements in text-to-image and text-to-video models, such as Stable Diffusion 3 (SD3), Flux and OpenSora, have adopted rectified flow over traditional diffusion models to enhance training and inference efficiency. SD3 notes increased difficulty in learning at intermediate timesteps but does not clarify the underlying cause. In this paper, we theoretically identify the root cause as a higher variance in the loss gradient estimates at these timesteps, which hinders training efficiency. Furthermore, this high-variance region is significantly influenced by the noise schedulers (i.e., how we add noises to clean images) and data (or latent space) dimensions. Building on this theoretical insight, we propose a Variance-Reduction Sampling (VR-sampling) strategy that samples the timesteps in high-variance region more frequently to enhance training efficiency in flow models. VR-sampling constructs sampling distributions based on Monte Carlo estimates of the loss gradient variance, allowing it to easily extend to different noise schedulers and data dimensions. Experiments demonstrate that VR sampling accelerates training by up to 33% on ImageNet 256 and 50% on ImageNet 512 datasets in rectified flow models. Furthermore, VR-sampling could simplify the hyperparameter tuning of logit-normal sampling introduced in SD3.The code is available anonymously in~\\url{https://github.com/AnonymousProjects/VR_sampling.git}."
    },
    {
        "title": "SANIA: Polyak-type Optimization Framework Leads to Scale Invariant Stochastic Algorithms",
        "link_suffix": "/forum?id=7zPd1TjRc1",
        "link": "https://openreview.net/forum?id=7zPd1TjRc1",
        "pdf_link": "https://openreview.net/pdf?id=7zPd1TjRc1",
        "keywords": "optimization, learning rate free, adaptive optimizers, polyak step-size",
        "abstract": "Adaptive optimization methods are widely recognized as among the most popular approaches for training Deep Neural Networks (DNNs). Techniques such as Adam, AdaGrad, and AdaHessian utilize a preconditioner that modifies the search direction by incorporating information about the curvature of the objective function. However, despite their adaptive characteristics, these methods still require manual fine-tuning of the step-size. This, in turn, impacts the time required to solve a particular problem. This paper presents an optimization framework named SANIA to tackle these challenges. Beyond eliminating the need for manual step-size hyperparameter settings, SANIA incorporates techniques to address poorly scaled or ill-conditioned problems. We also explore several preconditioning methods, including Hutchinson's method, which approximates the Hessian diagonal of the loss function. We conclude with an extensive empirical examination of the proposed techniques across classification tasks, covering both convex and non-convex contexts."
    },
    {
        "title": "Informed Mixing -- Improving Open Set Recognition with Deep Dynamic Data Augmentation",
        "link_suffix": "/forum?id=dkrRIY41El",
        "link": "https://openreview.net/forum?id=dkrRIY41El",
        "pdf_link": "https://openreview.net/pdf?id=dkrRIY41El",
        "keywords": "open set recognition, model generalization, data augmentation",
        "abstract": "Conventionally trained image classifiers recently excel in accuracy across diverse tasks. One practical limitation is however that they assume all potential classes to be seen during training, i.e., they cannot tell \"I don't know\" when encountering an unknown class. \nOpen set recognition (OSR), which solves this problem of detecting novel classes during inference, therefore remains an open problem and is receiving increasing attention. Thereby, a crucial challenge is to learn features that are relevant for unseen categories from given data, for which these features might not be discriminative. Previous work has shown that the introduction of self-supervised contrastive learning to supervised paradigms can support diverse feature learning and thereby benefit OSR. However, the diversity in contrastive learning is commonly introduced through crafted augmentation schemes. To improve upon this aspect and \"optimize to learn\" more diverse features, we propose GradMix, a data augmentation method that dynamically leverages gradient-based attribution maps of the model during training. The idea is to mask out the activated areas in previous epochs so that the models can pay attention to broader areas and learn to extract features beyond of what is most discriminative for every class. The resulting models are expected to learn more diverse features from the same data source and thus to improve in OSR and model generalization. Extensive experiments on open set recognition, close set classification, and out-of-distribution detection reveal that our method performs well on these tasks that can often outperform the state-of-the-art. GradMix is also beneficial for increasing robustness to common corruptions.\nIn self-supervised learning, GradMix can increase the accuracy of downstream linear classifiers compared with baselines, indicating its benefit for model generalization."
    },
    {
        "title": "Correlations Are Ruining Your Gradient Descent",
        "link_suffix": "/forum?id=ogmzNfeRl7",
        "link": "https://openreview.net/forum?id=ogmzNfeRl7",
        "pdf_link": "https://openreview.net/pdf?id=ogmzNfeRl7",
        "keywords": "Natural gradient descent, decorrelation, whitening, approximate gradient descent",
        "abstract": "Herein the topics of (natural) gradient descent, data decorrelation, and approximate methods for backpropagation are brought into a common discussion. Natural gradient descent illuminates how gradient vectors, pointing at directions of steepest descent, can be improved by considering the local curvature of loss landscapes. We extend this perspective and show that to fully solve the problem illuminated by natural gradients in neural networks, one must recognise that correlations in the data at any linear transformation, including node responses at every layer of a neural network, cause a non-orthonormal relationship between the model's parameters. To solve this requires a method for decorrelating inputs at each individual layer of a neural network. We describe a range of methods which have been proposed for decorrelation and whitening of node output, and expand on these to provide a novel method specifically useful for distributed computing and computational neuroscience. Implementing decorrelation within multi-layer neural networks, we can show that not only is training via backpropagation sped up significantly but also existing approximations of backpropagation, which have failed catastrophically in the past, benefit significantly in their accuracy and convergence speed. This has the potential to provide a route forward for approximate gradient descent methods which have previously been discarded, training approaches for analogue and neuromorphic hardware, and potentially insights as to the efficacy and utility of decorrelation processes in the brain."
    },
    {
        "title": "Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds",
        "link_suffix": "/forum?id=0ASCZrVzSX",
        "link": "https://openreview.net/forum?id=0ASCZrVzSX",
        "pdf_link": "https://openreview.net/pdf?id=0ASCZrVzSX",
        "keywords": "approximation theory, manifold hypothesis, statistical complexity, Riemannian geometry",
        "abstract": "The manifold hypothesis says that natural high-dimensional data lie on or around a low-dimensional manifold. The recent success of statistical and learning-based methods in very high dimensions empirically supports this hypothesis, suggesting that typical worst-case analysis does not provide practical guarantees. A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any ambient dimensions that the data may be embedded in. Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods. In this work, we consider optimal uniform approximations with functions of finite statistical complexity. While upper bounds on uniform approximation exist in the literature in terms of ReLU network approximation, we consider the opposite: lower bounds to quantify the fundamental difficulty of approximation on manifolds. In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold, such as curvature, volume, and injectivity radius."
    },
    {
        "title": "Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling",
        "link_suffix": "/forum?id=FDnZFpHmU4",
        "link": "https://openreview.net/forum?id=FDnZFpHmU4",
        "pdf_link": "https://openreview.net/pdf?id=FDnZFpHmU4",
        "keywords": "Model ensembling, LLM",
        "abstract": "Large language models (LLMs) exhibit varying strengths and weaknesses across different tasks, prompting recent studies to explore the benefits of ensembling models to leverage their complementary advantages. However, existing LLM ensembling methods often overlook model compatibility and struggle with inefficient alignment of probabilities across the entire vocabulary. In this study, we empirically investigate the factors influencing ensemble performance, identifying model performance, vocabulary size, and response style as key determinants, revealing that compatibility among models is essential for effective ensembling. This analysis leads to the development of a simple yet effective model selection strategy that identifies compatible models. Additionally, we introduce the \\textsc{Uni}on \\textsc{T}op-$k$ \\textsc{E}nsembling (\\textsc{UniTE}), a novel approach that efficiently combines models by focusing on the union of the top-k tokens from each model, thereby avoiding the need for full vocabulary alignment and reducing computational overhead. Extensive evaluations across multiple benchmarks demonstrate that \\textsc{UniTE} significantly enhances performance compared to existing methods, offering a more efficient framework for LLM ensembling."
    },
    {
        "title": "Cycle-Consistent Learning for Joint Layout-to-Image Generation and Object Detection",
        "link_suffix": "/forum?id=cHKuyeHmS9",
        "link": "https://openreview.net/forum?id=cHKuyeHmS9",
        "pdf_link": "https://openreview.net/pdf?id=cHKuyeHmS9",
        "keywords": "Generation, Detection, Cycle-consistent, Diffusion",
        "abstract": "In this paper, we propose a generation-detection cycle consistent (GDCC) learning framework that jointly optimizes both layout-to-image (L2I) generation and object detection (OD) tasks in an end-to-end manner. The key of GDCC lies in the inherent duality between the two tasks, where L2I takes all object boxes and labels as input conditions to generate images, and OD maps images back to these layout conditions. Specifically, in GDCC, L2I generation is guided by a layout translation cycle loss, ensuring that the layouts used to generate images align with those predicted from the synthesized images. Similarly, OD benefits from an image translation cycle loss, which enforces consistency between the synthesized images fed into the detector and those generated from predicted layouts. While current L2I and OD tasks benefit from large-scale annotated layout-image pairs, our GDCC enables more efficient use of unpaired layout data, thereby further enhancing data efficiency. It is worth noting that our GDCC framework is computationally efficient thanks to the perturbative single-step sampling strategy and a priority timestep re-sampling strategy during training, while maintaining the same inference cost as the original L2I and OD models. Extensive experiments demonstrate that GDCC significantly improves the controllability of diffusion models and the accuracy of object detectors."
    },
    {
        "title": "LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization",
        "link_suffix": "/forum?id=qTrEq31Shm",
        "link": "https://openreview.net/forum?id=qTrEq31Shm",
        "pdf_link": "https://openreview.net/pdf?id=qTrEq31Shm",
        "keywords": "Long Context LLMs and Alignment, Large Language Models, Preference Optimization, Self-Evolution of LLMs",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities through pretraining and alignment. However, superior short-context LLMs may underperform in long-context scenarios due to insufficient long-context alignment. This alignment process remains challenging due to the impracticality of human annotation for extended contexts and the difficulty in balancing short- and long-context performance. To address these challenges, we introduce LongPO, that enables short-context LLMs to self-evolve to excel on long-context tasks by internally transfer short-context capabilities. LongPO harnesses LLMs to learn from self-generated short-to-long preference data, comprising paired responses generated for identical instructions with long-context inputs and their compressed short-context counterparts, respectively. This preference reveals capabilities and potentials of LLMs cultivated during short-context alignment that may be diminished in under-aligned long-context scenarios. Additionally, LongPO incorporates a short-to-long KL constraint to mitigate short-context performance decline during long-context alignment. When applied to Mistral-7B-Instruct-v0.2 from 128K to 256K context length, LongPO fully retaining short-context performance and largely outperforms naive SFT and DPO in both long- and short-context tasks. Specifically, \\ourMethod-trained models can achieve results on long-context benchmarks comparable to, or even surpassing, those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context annotation and larger parameter scales."
    },
    {
        "title": "See What You Are Told: Visual Attention Sink in Large Multimodal Models",
        "link_suffix": "/forum?id=7uDI7w5RQA",
        "link": "https://openreview.net/forum?id=7uDI7w5RQA",
        "pdf_link": "https://openreview.net/pdf?id=7uDI7w5RQA",
        "keywords": "Large multimodal models, Visual attention sink, Visual attention redistribution",
        "abstract": "Large multimodal models (LMMs) \"see\" images by leveraging the attention mechanism between text and visual tokens in the transformer decoder. Ideally, these models should focus on key visual information relevant to the text token. However, recent findings indicate that LMMs have an extraordinary tendency to consistently allocate high attention weights to specific visual tokens, even when these tokens are irrelevant to the corresponding text. In this study, we investigate the property behind the appearance of these irrelevant visual tokens and examine their characteristics. Our findings show that this behavior arises due to the massive activation of certain hidden state dimensions, which resembles the attention sink found in language models. Hence, we refer to this phenomenon as the visual attention sink. In particular, our analysis reveals that removing the irrelevant visual sink tokens does not impact model performance, despite receiving high attention weights. Consequently, we recycle the attention to these tokens as surplus resources, redistributing the attention budget to enhance focus on the image. To achieve this, we introduce Visual Attention Redistribution (VAR), a method that redistributes attention in image-centric heads, which we identify as innately focusing on visual information. VAR can be seamlessly applied across different LMMs to improve performance on a wide range of tasks, including general vision-language tasks, visual hallucination tasks, and vision-centric tasks, all without the need for additional training, models, or inference steps. Experimental results demonstrate that VAR enables LMMs to process visual information more effectively by adjusting their internal attention mechanisms, offering a new direction to enhancing the multimodal capabilities of LMMs."
    },
    {
        "title": "Interpretable Surrogate Models: A Clustering Approach for Gaussian Process Posteriors Using Mixed-Integer Quadratic Programming",
        "link_suffix": "/forum?id=1T6HzuZMCz",
        "link": "https://openreview.net/forum?id=1T6HzuZMCz",
        "pdf_link": "https://openreview.net/pdf?id=1T6HzuZMCz",
        "keywords": "Interpretability, Clustering, Gaussian Process Regression",
        "abstract": "Gaussian process regression is a flexible Bayesian method for capturing nonlinearity. \nAlthough recent advancements allow us to handle various types of tasks by specifying a covariance function and a likelihood function, the interpretation of its predictions is sometimes challenging due to the large number of parameters. \nIn this study, we propose a clustering approach to improve the interpretability of Gaussian process posteriors. \nAssuming that the parameters corresponding to data points within each cluster are identical, the number of parameters in the posterior distribution is reduced. \nThe assignment of data points to clusters is formulated as a mixed-integer quadratic programming problem, with the objective function being a weighted squared error from the mean of the posterior distribution approximated by variational inference. \nGraph partitioning and decision tree learning can be represented by incorporating linear inequality constraints into this formulation. \nExperimental results demonstrated that our approach provided significant advantages in enhancing the interpretability of spatial modeling. \nMoreover, our formulation has produced higher-scoring decision trees compared to Classification and Regression Trees algorithm."
    },
    {
        "title": "Measuring and Improving Robustness of Deep Neural Networks",
        "link_suffix": "/forum?id=64vO8qoJfb",
        "link": "https://openreview.net/forum?id=64vO8qoJfb",
        "pdf_link": "https://openreview.net/pdf?id=64vO8qoJfb",
        "keywords": "robustness, generalization, out-of-distribution, adversarial",
        "abstract": "Deep neural networks perform well on train data, but are often unable to adapt to\ndata distribution shifts. These are data which are rarely encountered, and thus are\nunder-represented in our training data. Examples of this includes data under ad-\nverse weather conditions, and data which have been augmented with adversarial\nperturbations. Estimating the robustness of models to data distribution shifts is im-\nportant in enabling us to deploy them into safety critical applications with greater\nassurance. Thus, we desire a measure which can be used to estimate robustness.\nWe define robustness in 4 ways: Generalization Gap, Test Accuracy (Clean &\nCorrupted), and Attack Success Rate. A measure is said to be representative of\nrobustness when consistent (non-contradicting) relationships are found across all\n4 robustness definitions. Through our empirical studies, we show that it is difficult\nto measure robustness comprehensively across all definitions of robustness, as the\nmeasure often behave inconsistently. While they can capture one aspect of robust-\nness, they often fail to do so in another aspect. Thus, we recommend that different\nmeasures be used for different robustness definitions. Besides this, we also fur-\nther investigate the link between sharpness and robustness. We found that while\nsharpness has some impact on robustness, this relationship is largely affected by\nthe choice of hyperparameters such as batch size."
    },
    {
        "title": "Selective Concept Bottleneck Models Without Predefined Concepts",
        "link_suffix": "/forum?id=uuvujfQXZy",
        "link": "https://openreview.net/forum?id=uuvujfQXZy",
        "pdf_link": "https://openreview.net/pdf?id=uuvujfQXZy",
        "keywords": "interpretability, concept bottleneck models, concepts",
        "abstract": "Concept-based models like Concept Bottleneck Models (CBMs) have garnered significant interest for improving model interpretability by first predicting human-understandable concepts before mapping them to the output classes. Early approaches required costly concept annotations. To alleviate such, recent methods utilized large language models to automatically generate class-specific concept descriptions and learned mappings from a pretrained black-box model\u2019s raw features to these concepts using vision-language models. However, these approaches assume prior knowledge of which concepts the black-box model has learned. In this work, we discover the concepts encoded by the model through unsupervised concept discovery techniques instead. We further propose an input-dependent concept selection mechanism that dynamically retains a sparse set of relevant concepts for each input, enhancing both sparsity and interpretability. Our approach not only improves downstream performance but also needs significantly fewer concepts for accurate classification. Lastly, we show how large vision-language models can guide the editing of our models' weights to correct errors."
    }
]