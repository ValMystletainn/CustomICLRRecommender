[{"title": "BioNAS: Incorporating Bio-inspired Learning Rules to Neural Architecture Search", "link_suffix": "/forum?id=tBB8hCG5I7", "link": "https://openreview.net/forum?id=tBB8hCG5I7", "pdf_link": "https://openreview.net/pdf?id=tBB8hCG5I7", "keywords": "Feedback Alignment, Neural Architecture Search, Learning Rules, Biologically Plausible", "abstract": "Bio-inspired neural networks have gained traction due to their adversarial robustness, energy efficiency, and for being biologically plausible. While these bio-inspired networks have shown significant progress, they still fall short in terms of accuracy and are hard to scale to complex tasks. In this paper, we propose to use neural architecture search to further improve state-of-the-art bio-inspired neural networks. We\nachieve this thanks to BioNAS, a framework for neural architecture search that explores different bio-inspired neural network architectures and learning rules. The novelty of BioNAS lies in exploring the use of different bio-inspired learning rules for the different layers of the model. The motivation for this choice comes from recent work in the field suggesting that different learning mechanisms might be used in different\nregions of the human brain. Using BioNAS, we get state-of-the-art bio-inspired neural network performance achieving an accuracy of 94.86 on CIFAR10, 76.48 on CIFAR-100 and 43.42 on ImageNet16-120, surpassing state-of-the-art bio-inspired neural networks. We show that a part of this improvement comes from the use of different learning rules instead of using a single algorithm for all the layers. We release BioNAS to the community and make the code available via this link (https://anonymous.4open.science/r/LR-NAS-DFE1)", "title_embedding_index": 17100, "title_abs_embedding_index": 17125}, {"title": "Fine-Tuning with Divergent Chains of Thought Boosts Reasoning Through Self-Correction in Language Models", "link_suffix": "/forum?id=u4whlT6xKO", "link": "https://openreview.net/forum?id=u4whlT6xKO", "pdf_link": "https://openreview.net/pdf?id=u4whlT6xKO", "keywords": "chain of thought, reasoning, self-correction, LLM, large language model, question answering", "abstract": "Requiring a large language model to generate intermediary reasoning steps has been shown to be an effective way of boosting performance. In fact, instruction tuning on these intermediary reasoning steps improves model performance. In this work, we present a novel method of further improving performance by requiring models to compare multiple reasoning chains before generating a solution in a single inference step. We call this method Divergent CoT (DCoT). We find that instruction tuning on DCoT datasets boosts the performance of even smaller, and therefore more accessible, LLMs. Through a rigorous set of experiments spanning a wide range of tasks that require various reasoning types, we show that fine-tuning on DCoT consistently improves performance over the CoT baseline across model families and scales (1.3B to 70B).  Through a combination of empirical and manual evaluation, we additionally show that these performance gains stem from models generating multiple divergent reasoning chains in a single inference step, indicative of the enabling of self-correction in language models. Our code and data are publicly available.", "title_embedding_index": 17101, "title_abs_embedding_index": 17126}, {"title": "AoPS Dataset: Leveraging Online Olympiad-Level Math Problems for LLMs Training and Contamination-Resistant Evaluation", "link_suffix": "/forum?id=Bgz3okeZ7H", "link": "https://openreview.net/forum?id=Bgz3okeZ7H", "pdf_link": "https://openreview.net/pdf?id=Bgz3okeZ7H", "keywords": "Mathematical Reasoning, Large Language Models", "abstract": "Advances in Large Language Models (LLMs) have sparked interest in their ability to solve Olympiad-level math problems. \nHowever, the training and evaluation of these models are constrained by the limited size and quality of available datasets, as creating large-scale data for such advanced problems requires extensive effort from human experts.\nIn addition, current benchmarks are prone to contamination, leading to unreliable evaluations.\nIn this paper, we present an automated pipeline that leverages the rich resources of the Art of Problem Solving (AoPS) forum, which predominantly features Olympiad-level problems and community-driven solutions.\nUsing open-source LLMs, we develop a method to extract question-answer pairs from the forum, resulting inAoPS-Instruct, a dataset of more than 650,000 high-quality QA pairs.\nOur experiments demonstrate that fine-tuning LLMs on AoPS-Instruct improves their reasoning abilities across various benchmarks. \nMoreover, we build an automatic pipeline that introducesLiveAoPSBench, an evolving evaluation set with timestamps, derived from the latest forum data, providing a contamination-resistant benchmark for assessing LLM performance.\nNotably, we observe a significant decline in LLM performance over time, suggesting their success on older examples may stem from pre-training exposure rather than true reasoning ability. \nOur work presents a scalable approach to creating and maintaining large-scale, high-quality datasets for advanced math reasoning, offering valuable insights into the capabilities and limitations of LLMs in this domain. \nOur benchmark is available atlivemathbench.github.io/leaderboard.", "title_embedding_index": 17102, "title_abs_embedding_index": 17127}, {"title": "Score-based Neural Ordinary Differential Equations for Computing Mean Field Control Problems", "link_suffix": "/forum?id=8nLGhdBd9e", "link": "https://openreview.net/forum?id=8nLGhdBd9e", "pdf_link": "https://openreview.net/pdf?id=8nLGhdBd9e", "keywords": "neural ordinary differential equation, normalizing flow, score function, mean field control", "abstract": "Classical neural ordinary differential equations (ODEs) are powerful tools for approximating the log-density functions in high-dimensional spaces along trajectories, where neural networks parameterize the velocity fields. This paper proposes a system of neural differential equations representing first- and second-order score functions along trajectories based on deep neural networks. We reformulate the mean field control (MFC) problem with individual noises into an unconstrained optimization problem framed by the proposed neural ODE system. Additionally, we introduce a novel regularization term to enforce characteristics of viscous Hamilton--Jacobi--Bellman (HJB) equations to be satisfied based on the evolution of the second-order score function. Examples include regularized Wasserstein proximal operators (RWPOs), probability flow matching of Fokker--Planck (FP) equations, and linear quadratic (LQ) MFC problems, which demonstrate the effectiveness and accuracy of the proposed method.", "title_embedding_index": 17103, "title_abs_embedding_index": 17128}, {"title": "Contemporary Continuous Aggregation: A Robust Categorical Encoding for Zero-Shot Transfer Learning on Tabular Data", "link_suffix": "/forum?id=kg4A2nGyY2", "link": "https://openreview.net/forum?id=kg4A2nGyY2", "pdf_link": "https://openreview.net/pdf?id=kg4A2nGyY2", "keywords": "Categorical Encoding, Machine Learning", "abstract": "Tabular data, as the most fundamental structure of many real-world applications, has been a spotlight of machine learning since the last decade. Regardless of the adopted approaches, e.g., decision trees or neural networks, Categorical Encoding is an essential operation for processing raw data into a numeric format so that machine learning algorithms can accept it. One fatal limitation of popular categorical encodings is that they cannot extrapolate to unseen categories for machine learning models without re-training. However, it is common to observe new categories in industry, while re-training is not always possible, e.g., during the cold-start stage with no target examples. In this work, we propose Contemporary Continuous Aggregation (CCA), a novel and theoretically sound categorical encoding which can automatically extrapolate to unseen categories without any training. CCA only relies on statistics from raw input that can be maintained at low time and memory costs, thus it is scalable to heavy workloads in real-time. Besides, we also empirically showcase that CCA outperforms existing encodings on unsupervised unseen category extrapolation, and achieves similar or even better performance in normal situations without extrapolation, promising CCA to be a powerful toolkit for tabular learning.", "title_embedding_index": 17104, "title_abs_embedding_index": 17129}, {"title": "The Cognitive Capabilities of Generative AI: A Comparative Analysis with Human Benchmarks", "link_suffix": "/forum?id=TjuS86sQv8", "link": "https://openreview.net/forum?id=TjuS86sQv8", "pdf_link": "https://openreview.net/pdf?id=TjuS86sQv8", "keywords": "LLM, VLM, GenAI, Abstract Reasoning, Cognitive Benchmarking, Discrete Intellectual Abilities, Memory", "abstract": "There is increasing interest in tracking the capabilities of general intelligence foundation models. This study benchmarks leading large language models (LLMs) and vision language models (VLMs) against human performance on the Wechsler Adult Intelligence Scale (WAIS-IV), a comprehensive, population-normed assessment of underlying human cognition and intellectual abilities, with a focus on the domains of Verbal Comprehension (VCI), Working Memory (WMI), and Perceptual Reasoning (PRI). Most models demonstrated exceptional capabilities in the storage, retrieval, and manipulation of tokens such as arbitrary sequences of letters and numbers, with performance on the Working Memory Index (WMI) greater or equal to the 99.5th percentile when compared to human population normative ability. Performance on the Verbal Comprehension Index (VCI) which measures retrieval of acquired information, and linguistic understanding about the meaning of words and their relationships to each other, also demonstrated consistent performance at or above the 98th percentile. Despite these broad strengths, we observed consistently poor performance on the Perceptual Reasoning Index (PRI; range 0.1-10th percentile) from multimodal models indicating profound inability to interpret and reason on visual information.\nSome more nuanced differences in performance were also observed. Models were consistently stronger on the WMI compared to the VCI, indicating stronger capabilities in storage, manipulation, and retrieval of data than language understanding. Smaller and older model versions consistently performed worse, indicating that training data, parameter count, and advances in tuning are resulting in significant advances in cognitive ability.", "title_embedding_index": 17105, "title_abs_embedding_index": 17130}, {"title": "Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling", "link_suffix": "/forum?id=3OyaXFQuDl", "link": "https://openreview.net/forum?id=3OyaXFQuDl", "pdf_link": "https://openreview.net/pdf?id=3OyaXFQuDl", "keywords": "large and small language models, reasoning, math, compute-optimal, sampling, supervised finetuning", "abstract": "Training on high-quality synthetic data from strong language models (LMs) is a common strategy to improve the reasoning performance of LMs. In this work, we revisit whether this strategy is compute-optimal under a fixed inference budget (e.g., FLOPs). To do so, we investigate the trade-offs between generating synthetic data using a stronger but more expensive (SE) model versus a weaker but cheaper (WC) model. We evaluate the generated data across three key metrics: coverage, diversity, and false positive rate, and show that the data from WC models may have higher coverage and diversity, but also exhibit higher false positive rates. We then finetune LMs on data from SE and WC models in different settings: knowledge distillation, self-improvement, and a novel weak-to-strong improvement setup where a weaker LM teaches reasoning to a stronger LM. Our findings reveal that models finetuned on WC-generated data consistently outperform those trained on SE-generated data across multiple benchmarks and multiple choices of WC and SE models. These results challenge the prevailing practice of relying on SE models for synthetic data generation, suggesting that WC may be the compute-optimal approach for training advanced LM reasoners.", "title_embedding_index": 17106, "title_abs_embedding_index": 17131}, {"title": "DSI: Faster Inference of Large Language Models via Speculation Parallelism", "link_suffix": "/forum?id=cJd1BgZ9CS", "link": "https://openreview.net/forum?id=cJd1BgZ9CS", "pdf_link": "https://openreview.net/pdf?id=cJd1BgZ9CS", "keywords": "inference algorithms for generative models, LLM inference, speculative decoding", "abstract": "Accelerating the inference of large language models (LLMs) is an important challenge in artificial intelligence. This paper introduces Distributed Speculative Inference (DSI), a novel distributed inference algorithm that is provably faster than speculative inference (SI) [leviathan2023fast, chen2023accelerating, miao2023specinfer] and traditional autoregressive inference (non-SI). Like other SI algorithms, DSI works on frozen LLMs, requiring no training or architectural modifications, and it preserves the target distribution. Prior studies on SI have demonstrated empirical speedups (compared to non-SI) but require fast and accurate drafters, which are often unavailable in practice. We identify a gap where SI can be slower than non-SI given slower or less accurate drafters. We close this gap by proving that DSI is faster than both SI and non-SI\u2014given any drafters. DSI introduces a novel type of task parallelism called Speculation Parallelism (SP), which orchestrates target and drafter instances to overlap in time, creating a new foundational tradeoff between computational resources and latency. DSI is not only faster than SI but also supports LLMs that cannot be accelerated with SI. Our simulations show speedups of off-the-shelf LLMs in realistic single-node settings where DSI is 1.29-1.92x faster than SI.", "title_embedding_index": 17107, "title_abs_embedding_index": 17132}, {"title": "FDA: Generating Fair Synthetic Data with Provable Trade-off between Fairness and Faithfulness", "link_suffix": "/forum?id=UhdmcuuvSt", "link": "https://openreview.net/forum?id=UhdmcuuvSt", "pdf_link": "https://openreview.net/pdf?id=UhdmcuuvSt", "keywords": "Fairness, Fair synthetic data generation, Joint modeling, Faithfulness, Trade-off between fairness and faithfulness", "abstract": "We propose a novel framework called FDA for generating Fair synthetic data through Data Augmentation, offering the first method with provable trade-off guarantee between fairness and faithfulness. Unlike other existing methods, our approach utilizes a novel joint model that consists of two sub-models: one focused on enforcing strict fairness constraints while the other dedicated to preserving fidelity to the original data, coupled with a tuning mechanism that provides explicit control over the trade-off between fairness and faithfulness. Specifically, our FDA framework enables explicit quantification of the extent to which the generated fair synthetic data preserve faithfulness to the original data, while achieving an intermediate level of fairness determined by a user specified parameter $\\alpha \\in [0, 1]$. Theoretically, we show that the resulting fair synthetic data converge to the original data in probability when $\\alpha$ tends to 1, thereby implying convergence in distribution. Our framework can be also combined with some GAN-based fair models, such as DECAF,  to further improve the utility of the resulting synthetic data in downstream analysis, while carefully balancing fairness. Furthermore, we obtain an upper bound of the unfairness measurement for downstream models trained on the generated fair synthetic data, which can help users to choose appropriate $\\alpha$. Finally, we perform numerical experiments on benchmark data to validate our theoretical contributions and to compare our FDA with other methods.", "title_embedding_index": 17108, "title_abs_embedding_index": 17133}, {"title": "Improving Generalization and Robustness in SNNs Through Signed Rate Encoding and Sparse Encoding Attacks", "link_suffix": "/forum?id=qLh6Ufvnuc", "link": "https://openreview.net/forum?id=qLh6Ufvnuc", "pdf_link": "https://openreview.net/pdf?id=qLh6Ufvnuc", "keywords": "Spiking Neural Network, Adversarial Examples, Adversarial Robustness, Rate Encoding", "abstract": "Rate-encoded spiking neural networks (SNNs) are known to offer superior adversarial robustness compared to direct-encoded SNNs but have relatively poor generalization on clean input. While the latter offers good generalization on clean input it suffers poor adversarial robustness under standard training. A key reason for this behaviour is the input noise introduced by the rate encoding, which encodes a pixel intensity with $T$ independent Bernoulli samples. To improve the generalization of rate-encoded SNNs, we propose thesigned rate encoding(sRATE) that allows mean centering of the input and helps reduce the randomness introduced by the encoding, resulting in improved clean accuracy. In contrast to rate encoding where input restricted to $[0,1]^d$ is encoded in $\\{0,1\\}^{d\\times T}$, the signed rate encoding allows input in $[-1,1]^d$ to be encoded with spikes in $\\{-1,0,1\\}^{d\\times T}$, where positive (negative) inputs are encoded with positive (negative) spikes. We further construct efficientSparse Encoding Attack(SEA) on standard and signed rate encoded input, which performs $l_0$-norm restricted adversarial attack in the discrete encoding space. We prove the theoretical optimality of the attack under the first-order approximation of the loss and compare it empirically with the existing attacks on the input space. Adversarial training performed with SEA, under signed rate encoding, offers superior adversarial robustness to the existing attacks and itself. Experiments conducted on standard datasets show the effectiveness of sign rate encoding in improving accuracy across all settings including adversarial robustness.", "title_embedding_index": 17109, "title_abs_embedding_index": 17134}, {"title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers", "link_suffix": "/forum?id=M23dTGWCZy", "link": "https://openreview.net/forum?id=M23dTGWCZy", "pdf_link": "https://openreview.net/pdf?id=M23dTGWCZy", "keywords": "LLM, Idea Generation, Human Study", "abstract": "Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation.", "title_embedding_index": 17110, "title_abs_embedding_index": 17135}, {"title": "GeoTimeCLIP: Unveiling the When and Where of Images", "link_suffix": "/forum?id=z9UABOHCZc", "link": "https://openreview.net/forum?id=z9UABOHCZc", "pdf_link": "https://openreview.net/pdf?id=z9UABOHCZc", "keywords": "time prediction, geolocalization, contrastive learning, metric learning", "abstract": "Timestamp prediction aims to accurately determine the date and hour at which an image was captured using only visual cues, with applications ranging from image retrieval and metadata correction to digital forensics. In outdoor scenes, this can be inferred from variables such as overall brightness, hue, and shadow positions for hourly estimations, as well as weather patterns or seasonal changes for determining the date. However, these factors vary greatly depending on geographical location, making the challenges of time-of-capture prediction closely related to geo-localization. To address this problem, we introduce GeoTimeCLIP, a novel method capable of simultaneously estimating both the capture time (i.e., hour and month) and geo-location (i.e., GPS coordinates) of an image using a retrieval approach. Our model employs an image encoder, a time encoder, and a location encoder, aligning the time and GPS embeddings with the image embeddings in a continuous high-dimensional feature space. Considering the cyclical nature of days and years, we propose an effective way to represent time using Random Fourier Features. To learn image-time embedding alignment, rather than applying a standard contrastive loss with hard positives and negatives, we propose a more effective metric learning-based objective, which provides soft targets by considering the time difference between samples over a toroidal manifold. We introduce new benchmarks for time prediction, where we show that our jointly optimized time-location-based method outperforms baselines optimized solely for time. We also evaluate our method on existing geo-localization protocols, demonstrating that our approach performs competitively with expert geo-localization methods. Our shared embedding space enables various downstream tasks, such as compositional retrieval and text-based retrieval.", "title_embedding_index": 17111, "title_abs_embedding_index": 17136}, {"title": "DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows", "link_suffix": "/forum?id=QWkcCFhkTL", "link": "https://openreview.net/forum?id=QWkcCFhkTL", "pdf_link": "https://openreview.net/pdf?id=QWkcCFhkTL", "keywords": "Diverse sampling, flow matching, determinantal point processes", "abstract": "Many real-world applications of flow generative models desire a diverse set of samples covering multiple modes of the target distribution. However, the predominant approach for obtaining diverse sets is not sample-efficient, as it involves independently obtaining many samples from the source distribution and mapping them through the flow until the desired mode coverage is achieved. As an alternative to repeated sampling, we introduce DiverseFlow---a training-free, inference-time approach to improve the diversity of flow models. Our key idea is to employ a determinantal point process to induce a coupling between the samples and drive sample diversity under a fixed sampling budget. We demonstrate the efficacy of DiverseFlow for tasks where sample efficient diversity is highly desirable---text-guided image generation with polysemous words, inverse problems like large-hole inpainting, and class-conditional image synthesis.", "title_embedding_index": 17112, "title_abs_embedding_index": 17137}, {"title": "Sensitivity-Adaptive Augmentation for Robust Segmentation", "link_suffix": "/forum?id=JBOMYYX94D", "link": "https://openreview.net/forum?id=JBOMYYX94D", "pdf_link": "https://openreview.net/pdf?id=JBOMYYX94D", "keywords": "augmentation, sensitivity analysis, robustness to corruption", "abstract": "Achieving robustness in image segmentation tasks is difficult due to the granularity of pixel-level classification. Segmentation models particularly struggle in the presence of natural corruptions, despite being a task in many real-time perception modules. Sensitivity analysis examines how input variables can influence output variables in a highly complex mathematical model, which can be challenging to apply in the context of natural and uncontrollable corruptions occurring within training data. In this work, we present an efficient and sensitivity-based augmentation method to address robustness to natural corruptions. Our sensitivity analysis approach runs up to 10 times faster and requires up to 200 times less in storage compared to previous approaches, allowing for practical, on-the-fly estimation during training for a model-free augmentation policy. With minimal fine-tuning, our sensitivity-based augmentation method achieves improved robustness in real-world and synthetic datasets compared to state-of-the-art data augmentation techniques on image segmentation tasks.", "title_embedding_index": 17113, "title_abs_embedding_index": 17138}, {"title": "Rapid Response: Mitigating LLM Jailbreaks With A Few Examples", "link_suffix": "/forum?id=V892sBHUbN", "link": "https://openreview.net/forum?id=V892sBHUbN", "pdf_link": "https://openreview.net/pdf?id=V892sBHUbN", "keywords": "safety, robustness", "abstract": "As large language models (LLMs) grow more powerful, ensuring their safety against misuse becomes crucial. While researchers have focused on developing robust defenses, no method has yet achieved complete invulnerability to attacks. We propose an alternative approach: instead of seeking perfect adversarial robustness, we develop rapid response techniques to look to block whole classes of jailbreaks after observing only a handful of attacks. To study this setting, we develop RapidResponseBench, a benchmark that measures a defense's robustness against various jailbreak strategies after adapting to a few observed examples. We evaluate five rapid response methods, all of which use jailbreak proliferation, where we automatically generate additional jailbreaks similar to the examples observed. Our strongest method, which fine-tunes an input classifier to block proliferated jailbreaks, reduces attack success rate by an average of 97.8% on an in-distribution set of jailbreaks and 92.3% on an out-of-distribution set, having observed just one example of each jailbreaking strategy. Moreover, further studies suggest the quality of proliferation model and number of proliferated examples play an important role in the effectiveness of our defenses. Overall, our results highlight the potential of responding rapidly to novel jailbreaks to limit LLM misuse.", "title_embedding_index": 17114, "title_abs_embedding_index": 17139}, {"title": "HiBO: Hierarchical Bayesian Optimization via Adaptive Search Space Partitioning", "link_suffix": "/forum?id=Z1MKx8mNu9", "link": "https://openreview.net/forum?id=Z1MKx8mNu9", "pdf_link": "https://openreview.net/pdf?id=Z1MKx8mNu9", "keywords": "High-dimensional Bayesian Optimization, Search Space Partitioning, DBMS Configuration Tuning", "abstract": "Optimizing black-box functions in high-dimensional search spaces has been known to be challenging for traditional Bayesian Optimization (BO). In this paper, we introduce HiBO, a novel hierarchical algorithm integrating global-level search space partitioning information into the acquisition strategy of a local BO-based optimizer. HiBO employs a search-tree-based global-level navigator to adaptively split the search space into partitions with different sampling potential. The local optimizer then utilizes this global-level information to guide its acquisition strategy towards the most promising regions within the search space.  A comprehensive set of evaluations demonstrates that HiBO outperforms state-of-the-art methods in high-dimensional synthetic benchmarks and presents significant practical effectiveness in the real-world task of tuning configurations of database management systems (DBMSs).", "title_embedding_index": 17115, "title_abs_embedding_index": 17140}, {"title": "CertainlyUncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness", "link_suffix": "/forum?id=cQ25MQQSNI", "link": "https://openreview.net/forum?id=cQ25MQQSNI", "pdf_link": "https://openreview.net/pdf?id=cQ25MQQSNI", "keywords": "multimodal, refusals, hallucinations", "abstract": "The ability to acknowledge the inevitable uncertainty in their knowledge and reasoning is a prerequisite for AI systems to be truly truthful and reliable. In this paper, we present a taxonomy of uncertainty specific to vision-language AI systems, distinguishing between epistemic uncertainty (arising from a lack of information) and aleatoric uncertainty (due to inherent unpredictability), and further explore finer categories within. Based on this taxonomy, we synthesize a benchmark dataset, CertainlyUncertain, featuring 178K visual question answering (VQA) samples as contrastive pairs. This is achieved by 1) inpainting images to make previously answerable questions into unanswerable ones; and 2) using image captions to prompt large language models for both answerable and unanswerable questions. Additionally, we introduce a new metric confidence-weighted accuracy, that is well correlated with both accuracy and calibration error, to address the shortcomings of existing metrics. Despite the recent rapid progress in vision-language models (VLMs), evaluations on our benchmark show that they perform poorly in uncertain scenarios. Further experiments demonstrate that supervised fine-tuning with CertainlyUncertain enhances the performance of VLMs, and reduces the calibration error. These improvements extend beyond our benchmark to existing refusal-oriented datasets and show positive results on reducing hallucinations, while maintaining performance on standard VQA benchmarks. Our work underscores the importance of addressing uncertainty in vision-language AI systems to improve their reliability and trustworthiness in real-world applications.", "title_embedding_index": 17116, "title_abs_embedding_index": 17141}, {"title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families", "link_suffix": "/forum?id=D5v491uCzm", "link": "https://openreview.net/forum?id=D5v491uCzm", "pdf_link": "https://openreview.net/pdf?id=D5v491uCzm", "keywords": "scaling law, LLM, benchmark, skill", "abstract": "Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance efficiently and offers insights into scaling behaviors for downstream tasks such as coding and emotional intelligence applications.", "title_embedding_index": 17117, "title_abs_embedding_index": 17142}, {"title": "Learning with Exact Invariances in Polynomial Time", "link_suffix": "/forum?id=8o08LSkuAj", "link": "https://openreview.net/forum?id=8o08LSkuAj", "pdf_link": "https://openreview.net/pdf?id=8o08LSkuAj", "keywords": "Learning with Invariances, Kernels, Spectral Theory", "abstract": "We study the statistical-computational trade-offs for learning with exact invariances (or symmetries) using kernel regression over manifold input spaces. Traditional methods, such as data augmentation, group averaging, canonicalization, and frame-averaging, either fail to provide a polynomial-time solution or are not applicable in the kernel setting. However, with oracle access to the geometric properties of the input space, we propose a polynomial-time algorithm that learns a classifier with \\emph{exact} invariances. Moreover, our approach achieves the same excess population risk (or generalization error) as the original kernel regression problem. To the best of our knowledge, this is the first polynomial-time algorithm to achieve exact (not approximate) invariances in this context. Our proof leverages tools from differential geometry, spectral theory, and optimization. A key result in our development is a new reformulation of the problem of learning under invariances, as optimizing an infinite number of linearly constrained convex quadratic programs, which may be of independent interest.", "title_embedding_index": 17118, "title_abs_embedding_index": 17143}, {"title": "Few-shot Text Adversarial  Attack for Black-box Multi-task  Learning", "link_suffix": "/forum?id=uuOmdQy6p7", "link": "https://openreview.net/forum?id=uuOmdQy6p7", "pdf_link": "https://openreview.net/pdf?id=uuOmdQy6p7", "keywords": "multi-task adversarial text attacks", "abstract": "Current multi-task adversarial text attacks rely on white-box access to shared in-\nternal features and assume a homogeneous multi-task learning framework. As a\nresult, these attacks are less effective against practical scenarios involving black-\nbox feedback APIs and heterogeneous multi-task learning. To bridge this gap,\nwe introduce Cluster and Ensemble Mutil-task Text Adversarial Attack (CEMA),\nan effective black-box attack that exploits the transferability of adversarial texts.\nSpecifically, we initially employ cluster-oriented substitute model training, as a\nplug-and-play framework, to simplify complex multi-task scenarios into more\nmanageable text classification attacks and train the substitute model. Next, we\ngenerate multiple adversarial candidate examples by applying various adversarial\ntext classification methods. Finally, we select the adversarial example that attacks\nthe most substitute models as the final attack output. CEMA is evaluated on two\nprimary multi-task objectives: text classification and translation. In the classifica-\ntion task, CEMA achieves attack success rates that exceed 60% while reducing the\ntotal number of queries to 100. For the text translation task, the BLEU scores of\nboth victim texts and adversarial examples decrease to below 0.36 with 100 queries\neven including the commercial translation APIs, such as Baidu Translate and Ali\nTranslate. Additionally, we derive the theoretical lower bound for CEMA\u2019s success\nrate, demonstrating that a successful attack increases with the number of candidate\nsubstitute models.", "title_embedding_index": 17119, "title_abs_embedding_index": 17144}, {"title": "Generative Verifiers: Reward Modeling as Next-Token Prediction", "link_suffix": "/forum?id=Ccwp4tFEtE", "link": "https://openreview.net/forum?id=Ccwp4tFEtE", "pdf_link": "https://openreview.net/pdf?id=Ccwp4tFEtE", "keywords": "LLM reasoning, reward models, verifiers", "abstract": "Verifiers or reward models are often used to enhance the reasoning performance of large language models (LLMs). A common approach is the Best-of-N method, where N candidate solutions generated by the LLM are ranked by a verifier, and the best one is selected. While LLM-based verifiers are typically trained as discriminative classifiers to score solutions, they do not utilize the text generation capabilities of pretrained LLMs. To overcome this limitation, we instead propose training verifiers using the ubiquitous next-token prediction objective, jointly on verification and solution generation. Compared to standard verifiers, such generative verifiers (GenRM) can benefit from several advantages of LLMs: they integrate seamlessly with instruction tuning, enable chain-of-thought reasoning, and can utilize additional test-time compute via majority voting for better verification. We demonstrate that GenRM outperforms discriminative, DPO verifiers, and LLM-as-a-Judge, resulting in a 16-40% improvement in the number of problems solved with Best-of-N on algorithmic and math reasoning tasks. Furthermore, we find that training GenRM with synthetic verification rationales is sufficient to pick out subtle errors on math problems. Finally, we demonstrate that generative verifiers scale favorably with model size and inference-time compute.", "title_embedding_index": 17120, "title_abs_embedding_index": 17145}, {"title": "Towards Understanding the Robustness of Diffusion-Based Purification: A Stochastic Perspective", "link_suffix": "/forum?id=shqjOIK3SA", "link": "https://openreview.net/forum?id=shqjOIK3SA", "pdf_link": "https://openreview.net/pdf?id=shqjOIK3SA", "keywords": "Adversarial Defense, Adversarial Purification, Diffusion Training, Randomized Defense", "abstract": "Diffusion-Based Purification (DBP) has emerged as an effective defense mechanism against adversarial attacks. The efficacy of DBP has been attributed to the forward diffusion process, which narrows the distribution gap between clean and adversarial images through the addition of Gaussian noise. Although this explanation has some theoretical support, the significance of its contribution to robustness remains unclear. In this paper, we argue that the inherent stochasticity in the DBP process is the primary driver of its robustness. To explore this, we introduce a novel Deterministic White-Box (DW-box) evaluation protocol to assess robustness in the absence of stochasticity and to analyze the attack trajectories and loss landscapes. Our findings suggest that DBP models primarily leverage stochasticity to evade effective attack directions, and their ability to purify adversarial perturbations can be weak. To further enhance the robustness of DBP models, we introduce Adversarial Denoising Diffusion Training (ADDT), which incorporates classifier-guided adversarial perturbations into diffusion training, thereby strengthening the DBP models' ability to purify adversarial perturbations. Additionally, we propose Rank-Based Gaussian Mapping (RBGM) to make perturbations more compatible with diffusion models. Experimental results validate the effectiveness of ADDT. In conclusion, our study suggests that future research on DBP can benefit from the perspective of decoupling the stochasticity-based and purification-based robustness.", "title_embedding_index": 17121, "title_abs_embedding_index": 17146}, {"title": "Beyond Parameter Count: Implicit Bias in Soft Mixture of Experts", "link_suffix": "/forum?id=aw2Jc5DFZC", "link": "https://openreview.net/forum?id=aw2Jc5DFZC", "pdf_link": "https://openreview.net/pdf?id=aw2Jc5DFZC", "keywords": "mixture of experts, conditional computation, sparse activation", "abstract": "The traditional viewpoint on Sparse Mixture of Experts (MoE) models is that instead of training a singlelargeexpert, which is computationally expensive, we can train manysmallexperts. The hope is that if the total parameter count of the small experts equals that of the singular large expert, then we retain the representation power of the large expert while gaining computational tractability and promoting expert specialization. The recently introduced Soft MoE replaces the Sparse MoE's discrete routing mechanism with a differentiable gating function that smoothly mixes tokens. While this smooth gating function successfully mitigates the various training instabilities associated with Sparse MoE, it is unclear whether it induces implicit biases that affect Soft MoE's representation power or potential for expert specialization. We prove that Soft MoE with a single arbitrarily powerful expert cannot represent simple convex functions. This justifies that Soft MoE's success cannot be explained by the traditional viewpoint of many small experts collectively mimicking the representation power of a single large expert, and that multiple experts are actuallynecessaryto achieve good representation power (even for a fixed total parameter count). Continuing along this line of investigation, we introduce a notion of expert specialization for Soft MoE, and while varying the number of experts yet fixing the total parameter count, we consider the following (computationally intractable) task. Given any input, how can we discover the expert subset that is specialized to predict this input's label? We empirically show that when there are many small experts, the architecture is implicitly biased in a fashion that allows us to efficiently approximate the specialized expert subset. Our method can be easily implemented to potentially reduce computation during inference. For example, using our method on ImageNet, one can perform inference using only $1/8$ of the experts and still retain $99$% of the test accuracy of using all experts.", "title_embedding_index": 17122, "title_abs_embedding_index": 17147}, {"title": "What Matters for Model Merging at Scale?", "link_suffix": "/forum?id=fvUVe2gJh0", "link": "https://openreview.net/forum?id=fvUVe2gJh0", "pdf_link": "https://openreview.net/pdf?id=fvUVe2gJh0", "keywords": "model merging, weight averaging, averaging, composition, modular model, generalization", "abstract": "Model merging aims to combine multiple expert models into a more capable single model, offering benefits such as reduced storage and serving costs, improved generalization, and support for decentralized model development. Despite its promise, previous studies have primarily focused on merging a few small models. This leaves many unanswered questions about the effect of scaling model size and how it interplays with other key factors\u2014like the base model quality and number of expert models\u2014, to affect the merged model\u2019s performance. This work systematically evaluates the utility of model merging at scale, examining the impact of these different factors. We experiment with merging fully fine-tuned models using four popular merging methods\u2014Averaging, Task Arithmetic, Dare-TIES, and TIES-Merging\u2014across model sizes ranging from 1B to 64B parameters and merging up to 8 different expert models. We evaluate the merged models on both held-in tasks, i.e., the expert\u2019s training tasks, and zero-shot generalization to unseen held-out tasks. Our wide range of experiments provide several new insights about model merging at scale and the interplay between different factors. First, we find that merging is more effective when experts are created from strong base models, i.e., models with good zero-shot performance, compared to pre-trained ones. Second, larger models facilitate easier merging. Third merging consistently improves generalization capabilities. Notably, when merging eight large expert models, the merged models often generalize better compared to the multitask trained models. Fourth, we can better merge more expert models when working with larger models. Fifth, different merging methods behave very similarly at larger scales. Overall, our findings shed light on some interesting properties of model merging while also highlighting some limitations. We hope that this study will serve as a reference point on large-scale merging for upcoming research.", "title_embedding_index": 17123, "title_abs_embedding_index": 17148}, {"title": "COAT: Compressing Optimizer states and Activations for Memory-Efficient FP8 Training", "link_suffix": "/forum?id=XfKSDgqIRj", "link": "https://openreview.net/forum?id=XfKSDgqIRj", "pdf_link": "https://openreview.net/pdf?id=XfKSDgqIRj", "keywords": "FP8 training, quantization, low-precision training, memory efficient training", "abstract": "FP8 training has emerged as a promising method for improving training efficiency. Existing frameworks accelerate training by applying FP8 computation to linear layers while leaving optimizer states and activations in higher precision, which fails to fully optimize memory usage. This paper introduces COAT (CompressingOptimizer States andActivations for FP8Training), a novel FP8 training framework designed to significantly reduce memory footprint when training large models. COAT addresses current limitations through two key innovations: (1)Dynamic Range Expansion, which aligns optimizer state distributions more closely with the FP8 representation range, thereby reducing quantization error, and (2)Mixed-Granularity Activation Quantization, which optimizes activation memory using a combination of per-tensor and per-group quantization strategies. Experiments demonstrate that COAT effectively reduces end-to-end training memory footprint by1.54\u00d7compared to BF16 while achieving nearly lossless performance across various tasks, such as Large Language Model pretraining and fine-tuning and Vision Language Model training. COAT also achieves a1.43\u00d7end-to-end training speedup compared to BF16, performing on par with or surpassing TransformerEngine's speedup. COAT enables efficient full-parameter training of large models on fewer GPUs, and facilitates doubling the batch size in distributed training settings, providing a practical solution for scaling large-scale model training. Code will be released upon publication.", "title_embedding_index": 17124, "title_abs_embedding_index": 17149}]