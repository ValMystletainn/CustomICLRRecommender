[{"title": "Natural Policy Gradient for Average Reward Non-Stationary RL", "link_suffix": "/forum?id=GGZISiwgNt", "link": "https://openreview.net/forum?id=GGZISiwgNt", "pdf_link": "https://openreview.net/pdf?id=GGZISiwgNt", "keywords": "Non-Stationary Reinforcement Learning, Policy Gradient, Natural Actor-Critic", "abstract": "We consider the problem of non-stationary reinforcement learning (RL) in the infinite-horizon average-reward setting. We model it by a Markov Decision Process with time-varying rewards and transition probabilities, with a variation budget of $\\Delta_T$. Existing non-stationary RL algorithms focus on model-based and model-free value-based methods. Policy-based methods, however, despite their flexibility in practice, are not theoretically well understood in non-stationary RL. We propose the first model-free policy-based algorithm, Non-Stationary Natural Actor-Critic (NS-NAC), a policy gradient method with a novel interpretation of learning rates as adapting factors. We present a dynamic regret of $\\mathcal{\\tilde{O}} (|\\mathcal{S}|^{\\frac{1}{2}}|\\mathcal{A}|^{\\frac{1}{2}}\\Delta_T^{\\frac{1}{9}}T^{\\frac{8}{9}} )$, where $T$ is the time horizon, and $|\\mathcal{S}|$, $|\\mathcal{A}|$ are, respectively, the size of the state and action space. The regret analysis relies on adapting the Lyapunov function based analysis to dynamic environments and characterizing the effects of simultaneous changes in policy and the environment on estimates of the value function and average reward.", "title_embedding_index": 2600, "title_abs_embedding_index": 2625}, {"title": "Build your own cell: Diffusion Models for Multichannel 3D Microscopy Image Generation", "link_suffix": "/forum?id=FDsWd0NOB5", "link": "https://openreview.net/forum?id=FDsWd0NOB5", "pdf_link": "https://openreview.net/pdf?id=FDsWd0NOB5", "keywords": "3D Diffusion Models", "abstract": "Three-dimensional (3D) cellular morphology is a key indicator of both cellular function and disease states. Despite its importance, understanding the complex relationships between cell shape, treatment conditions, and their biological implications remains a challenge. To address this, we present \"Build Your Own Cell\" (BYOC), a multichannel 3D generative framework that leverages diffusion models for synthesising realistic 3D cell structures. Our model successfully captures the nuanced morphological changes induced by different drug treatments, enabling virtual simulation and screening of cell shapes in response to various conditions. This approach provides a novel tool for accelerating pre-clinical drug development, potentially reducing the time and costs associated with experimental studies.  Moreover, BYOC ensures phenotypic consistency between cell and nucleus volumes, offering high-fidelity reconstructions that can inform downstream analyses such as drug efficacy evaluation and mechanistic studies.", "title_embedding_index": 2601, "title_abs_embedding_index": 2626}, {"title": "Beyond Browsing: API-Based Web Agents", "link_suffix": "/forum?id=TPiJKs7ccR", "link": "https://openreview.net/forum?id=TPiJKs7ccR", "pdf_link": "https://openreview.net/pdf?id=TPiJKs7ccR", "keywords": "AI Agent, LLMs, Evaluation", "abstract": "Web agents are becoming increasingly important for assisting users with online tasks. While traditional web agents rely on web browsing through accessibility trees, this approach can be inefficient and inflexible. In this paper, we build and examine an API-based agent that directly interacts with web services through Application Programming Interfaces (APIs). This method avoids the need for browsing the web and allows for more structured and machine-readable task completion. However, the availability of APIs varies across websites, and some websites have limited support for API calling. To address this, we also present a hybrid interleaving agent that can switch between API calls and web browsing based on task requirements. We evaluate both agents on WebArena, a benchmark for real-world web tasks. Our results demonstrate that the API-based agent outperforms web browsing agents, especially for websites with good API support. This highlights the importance for websites to invest in robust API development to enhance the capabilities of web agents. Additionally, the interleaving agent achieves the best overall performance by dynamically leveraging the strengths of both approaches. Our work highlights the potential of API-based interactions for web agents and demonstrates the benefits of a hybrid approach for handling diverse web tasks, which also suggests the importance of continued development of both API calling agents and browsing agents to meet the evolving demands for web agents.", "title_embedding_index": 2602, "title_abs_embedding_index": 2627}, {"title": "Distributed Gradient Descent with Many Local Steps in Overparameterized Models", "link_suffix": "/forum?id=mscnV6JZkT", "link": "https://openreview.net/forum?id=mscnV6JZkT", "pdf_link": "https://openreview.net/pdf?id=mscnV6JZkT", "keywords": "Distributed Learning, Overparameterization, Optimization, Federated Learning", "abstract": "In distributed training of machine learning models, gradient descent with local iterative steps is a very popular method, variants of which are commonly known as Local-SGD or the Federated Averaging (FedAvg). In this method, gradient steps based on local datasets are taken independently in distributed compute nodes to update  the local models, which are then aggregated intermittently. Although the existing convergence analysis suggests that with heterogeneous data, FedAvg encounters quick performance degradation as the number of local steps increases, it is shown to work quite well in practice. In this work we try to explain this good performance from a viewpoint of implicit bias in Local Gradient Descent (Local-GD) with a large number of local steps. In overparameterized regime, the gradient descent at each compute node would lead the model to a specific direction locally. We  characterize the dynamics of the aggregated global model and compare it to the centralized model trained with all of the data in one place. In particular, we analyze the implicit bias of gradient descent on linear models, for both regression and classification tasks. Our analysis shows that the aggregated global model  converges exactly to the centralized model for regression tasks, and converges (in direction) to the same feasible set as centralized model  for classification tasks. We further propose a Modified Local-GD with a refined aggregation and theoretically show it converges to the centralized model in direction for linear classification. We empirically verified our theoretical findings in linear models and also conducted experiments on distributed fine-tuning of pretrained neural networks to further apply our theory.", "title_embedding_index": 2603, "title_abs_embedding_index": 2628}, {"title": "MU-Bench: A Multitask Multimodal Benchmark for Machine Unlearning", "link_suffix": "/forum?id=O9W9DesXid", "link": "https://openreview.net/forum?id=O9W9DesXid", "pdf_link": "https://openreview.net/pdf?id=O9W9DesXid", "keywords": "Machine Unlearning, Benchmark, Multimodal Learning", "abstract": "Recent advancements in Machine Unlearning (MU) have introduced solutions to selectively remove certain training samples, such as those with outdated or sensitive information, from trained models. Despite these advancements, evaluation of MU methods have been inconsistent, employing different trained models and architectures, and sample removal strategies, which hampers accurate comparison. In addition, prior MU approaches have mainly focused on {\\em singular} tasks or modalities, which is not comprehensive. To address these limitations, we develop \\method, the first comprehensive benchmark for MU that \n\\emph{(i) unifies the sets of deleted samples and trained models}, and\n\\emph{(ii) provides broad coverage of tasks and data modalities}, \nincluding previously unexplored domains such as speech and video classification. \nOur evaluation show that RandLabel and SalUn are the most effective general MU approaches on MU-Bench, and BadT and SCRUB are capable of achieving random performance on the deletion set. \nWe analyze several under-investigated aspects of unlearning, including scalability, the impacts of parameter-efficient fine-tuning and curriculum learning, and susceptibility to dataset biases. \nMU-Bench provides an easy-to-use package that includes dataset splits, models, and implementations, together with a leader board to enable unified and scalable MU research\\footnote{Code: \\href{https://github.com/CLU-UML/MU-Bench}{https://github.com/CLU-UML/MU-Bench}}.", "title_embedding_index": 2604, "title_abs_embedding_index": 2629}, {"title": "Fragile Giants: Understanding Susceptibility of Models to Subpopulation Attacks", "link_suffix": "/forum?id=wjPa7GUIR9", "link": "https://openreview.net/forum?id=wjPa7GUIR9", "pdf_link": "https://openreview.net/pdf?id=wjPa7GUIR9", "keywords": "poisoning, robustness, subpopulations", "abstract": "As machine learning models become increasingly complex, concerns about their robustness and trustworthiness have become more pressing. A critical vulnerability to these models is data poisoning attacks, where adversaries deliberately alter training data to degrade model performance. One particularly stealthy form of these attacks is subpopulation poisoning, which targets distinct subgroups within a dataset while leaving overall performance largely intact. The ability of these attacks to generalize within subpopulations poses a significant risk in real-world settings, as they can be exploited to harm marginalized or underrepresented groups within the dataset. In this work, we investigate how model complexity influences susceptibility to subpopulation poisoning attacks. We introduce a theoretical framework that explains how overparameterized models, due to their large capacity, can inadvertently memorize and misclassify targeted subpopulations. To validate our theory, we conduct extensive experiments on large-scale image and text datasets using popular model architectures. Our results show a clear trend: models with more parameters are significantly more vulnerable to subpopulation poisoning. Moreover, we find that attacks on smaller, human-interpretable subgroups often go undetected by these models. These results highlight the need for developing defenses that specifically address subpopulation vulnerabilities.", "title_embedding_index": 2605, "title_abs_embedding_index": 2630}, {"title": "Hide & Seek: Transformer Symmetries Obscure Sharpness & Riemmanian Geometry Finds It", "link_suffix": "/forum?id=qucdAPZfar", "link": "https://openreview.net/forum?id=qucdAPZfar", "pdf_link": "https://openreview.net/pdf?id=qucdAPZfar", "keywords": "generalization, symmetry, sharpness, flatness, riemannian geometry, loss landscape", "abstract": "The concept of sharpness has been successfully applied to traditional architectures like MLPs and CNNs to predict their generalization.\n  For transformers, however, recent work reported weak correlation between flatness and generalization. We argue that existing sharpness measures fail for transformers, because they have much richer symmetries in their attention mechanism that induce directions in parameter space along which the network or its loss remain identical.\n  We posit that sharpness must account fully for these symmetries, and thus we redefine it on a quotient manifold that results from quotienting out the transformer symmetries, thereby removing their ambiguities.\n  Leveraging tools from Riemannian geometry, we propose a fully general notion of sharpness, in terms of a geodesic ball on the symmetry-corrected quotient manifold. In practise, we need to resort to approximating the geodesics. Doing so up to first order yields existing adaptive sharpness measures, and we demonstrate that including higher-order terms is crucial to recover correlation with generalization.\n  We present results on diagonal networks with synthetic data, and show that our geodesic sharpness reveals the correlation for real-world transformers on ImageNet.", "title_embedding_index": 2606, "title_abs_embedding_index": 2631}, {"title": "Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning", "link_suffix": "/forum?id=4ytRL3HJrq", "link": "https://openreview.net/forum?id=4ytRL3HJrq", "pdf_link": "https://openreview.net/pdf?id=4ytRL3HJrq", "keywords": "large language model, hierarchical attention, contrastive learning, assembly code", "abstract": "Binary code analysis is the foundation of crucial tasks in the security domain; thus building effective binary analysis techniques is more important than ever. Large language models (LLMs) although have brought impressive improvement to source code tasks, do not directly generalize to assembly code due to the unique challenges of assembly: (1) the low information density of assembly and (2) the diverse optimizations in assembly code. To overcome these challenges, this work proposes a hierarchical attention mechanism that builds attention summaries to capture the semantics more effectively and designs contrastive learning objectives to train LLMs to learn assembly optimization. Equipped with these techniques, this work develops Nova, a generative LLM for assembly code. Nova outperforms existing techniques on binary code decompilation by up to 14.84 -- 21.58% higher Pass@1 and Pass@10, and outperforms the latest binary code similarity detection techniques by up to 6.17% Recall@1, showing promising abilities on both assembly generation and understanding tasks.", "title_embedding_index": 2607, "title_abs_embedding_index": 2632}, {"title": "Tool Unlearning for Tool Augmented LLMs", "link_suffix": "/forum?id=pV0SUV8cRF", "link": "https://openreview.net/forum?id=pV0SUV8cRF", "pdf_link": "https://openreview.net/pdf?id=pV0SUV8cRF", "keywords": "Tool-Augmented LLM, Machine Unlearning, Tool Unlearning", "abstract": "Tool-augmented large language models (LLMs) may need to forget learned tools due to security concerns, privacy restrictions, or deprecated tools. However, unlearning tool has not been explored in prior machine unlearning works.We propose tool unlearning, a novel machine unlearning task that deletes already acquired tools. Compared to traditional unlearning, tool unlearning exhibits certain differences and difficulties: 1) knowledge removal instead of forgetting samples, 2) significant cost of optimizing LLMs, 3) lack of principled evaluation tools.To bridge this gap, we introduce three properties for effective tool unlearning and propose ToolDelete, the first unlearning method designed for tool-augmented LLMs. We also propose the first membership inference attack (MIA) model for evaluating tool unlearning.Experiments on three tool learning datasets and tool-augmented LLMs demonstrate that ToolDelete effectively unlearns both randomly selected tools and tools from specific categories. The unlearning behavior does not impact the LLM's knowledge on non-deleted tools, while preserving performances on other general tasks.", "title_embedding_index": 2608, "title_abs_embedding_index": 2633}, {"title": "Two-Step Offline Preference-Based Reinforcement Learning with Constrained Actions", "link_suffix": "/forum?id=wYZ8rxwvMm", "link": "https://openreview.net/forum?id=wYZ8rxwvMm", "pdf_link": "https://openreview.net/pdf?id=wYZ8rxwvMm", "keywords": "Preference Based Reinforcement Learning, Offline Reinforcement Learning", "abstract": "Preference-based reinforcement learning (PBRL) in the offline setting has succeeded greatly in industrial applications such as chatbots. A two-step learning framework where one applies a reinforcement learning step after a reward modeling step has been widely adopted for the problem. However, such a method faces challenges from the risk of reward hacking and the complexity of reinforcement learning. To overcome the challenge, our insight is that both challenges come from the state-actions not supported in the dataset. Such state-actions are unreliable and increase the complexity of the reinforcement learning problem at the second step. Based on the insight, we develop a novel two-step learning method called PRC: preference-based reinforcement learning with constrained actions. The high-level idea is to limit the reinforcement learning agent to optimize over a constrained action space that excludes the out-of-distribution state-actions. We empirically verify that our method has high learning efficiency on various datasets in robotic control environments.", "title_embedding_index": 2609, "title_abs_embedding_index": 2634}, {"title": "Koopman Embedded Equivariant Control", "link_suffix": "/forum?id=72yPbvSx0c", "link": "https://openreview.net/forum?id=72yPbvSx0c", "pdf_link": "https://openreview.net/pdf?id=72yPbvSx0c", "keywords": "Koopman operators, Optimal Control, Equivariant Representation, Nonlinear Dynamical System", "abstract": "An efficient way to control systems with unknown nonlinear dynamics is to find an appropriate embedding or representation for simplified approximation (e.g. linearization), which facilitates system identification and control synthesis. Nevertheless, there has been a lack of embedding methods that can guarantee (i) embedding the dynamical system comprehensively, including the vector fields (ODE form) of the dynamics, and (ii) preserving the consistency of control effect between the original and latent space. To address these challenges, we propose Koopman Embedded Equivariant Control (KEEC) to learn an embedding of the states and vector fields such that a Koopman operator is approximated as the latent dynamics. Due to the Koopman operator's linearity, learning the latent vector fields of the dynamics becomes simply solving linear equations. Thus in KEEC, the analytical form of the greedy control policy, which is dependent on the learned differential information of the dynamics and value function, is also simplified. Meanwhile, KEEC preserves the effectiveness of the control policy in the latent space by preserving the metric in two spaces. Our algorithm achieves superior performances in the experiments conducted on various control domains, including the image-based Pendulum, Lorenz-63 and the wave equation.", "title_embedding_index": 2610, "title_abs_embedding_index": 2635}, {"title": "Unlocking Exocentric Video-Language Data for Egocentric Video Representation Learning", "link_suffix": "/forum?id=w2HYVwXhMh", "link": "https://openreview.net/forum?id=w2HYVwXhMh", "pdf_link": "https://openreview.net/pdf?id=w2HYVwXhMh", "keywords": "video-language pretraining, egocentric video", "abstract": "We present EMBED (Egocentric Models Built with Exocentric Data), a framework designed to mine video-language data from exocentric sources for egocentric video representation learning. Large-scale exocentric data covers diverse activities with significant potential for egocentric learning, but inherent disparities between egocentric and exocentric data pose challenges in utilizing one view for the other seamlessly. In this study, we propose leveraging hand-object interactions and language narratives as cues to incorporate exocentric data into egocentric training. Specifically, we focus on identifying specific video clips that emphasize hand-object interactions and pairing them with action-focused language narrations. By applying our framework to exocentric datasets such as HowTo100M, we construct datasets thar are effective for egocentric video-language pretraining. Our extensive evaluations reveal that EMBED achieves state-of-the-art performance across various egocentric downstream tasks, including a 4.7% absolute improvement in multi-instance retrieval on the Epic-Kitchens-100 benchmark and a 6.2% improvement in classification on the EGTEA benchmark in zero-shot settings. Furthermore, EMBED enables egocentric video-language models to perform competitively in exocentric tasks. Finally, we showcase EMBED's application across various exocentric datasets, exhibiting strong generalization capabilities when applied to different exocentric datasets.", "title_embedding_index": 2611, "title_abs_embedding_index": 2636}, {"title": "On the Training Convergence of Transformers for In-Context Classification", "link_suffix": "/forum?id=COdUNtjMEp", "link": "https://openreview.net/forum?id=COdUNtjMEp", "pdf_link": "https://openreview.net/pdf?id=COdUNtjMEp", "keywords": "In-context learning, Transformer", "abstract": "While transformers have demonstrated impressive capacities for in-context learning (ICL) in practice, theoretical understanding of the underlying mechanism enabling transformers to perform ICL is still in its infant stage. This work aims to study the training dynamics of transformers for in-context classification tasks. We demonstrate that, for in-context classification of Gaussian mixtures under certain assumptions, a single-layer transformer trained by gradient descent converges to a globally optimal model at a linear rate. We further quantify the impact of the training and testing prompt lengths on the ICL inference error of the trained transformer. We show that when the lengths of training and test prompts are sufficiently large, the prediction of the trained transformer approaches the ground truth label in context. Experimental results corroborate the theoretical findings.", "title_embedding_index": 2612, "title_abs_embedding_index": 2637}, {"title": "How to distill task-agnostic representations from many teachers?", "link_suffix": "/forum?id=hZ3QE0rUt1", "link": "https://openreview.net/forum?id=hZ3QE0rUt1", "pdf_link": "https://openreview.net/pdf?id=hZ3QE0rUt1", "keywords": "knowledge distillation, representation learning, natural language processing, molecular modeling, computer vision, embedding models", "abstract": "Casting complex inputs onto tractable representations is a critical step in many fields. Differences in architectures, loss functions, input modalities, and datasets lead to embedding models that capture diverse information of the input.  Multi-teacher distillation seeks to exploit this diversity to create richer representations but often remains task-specific. We extend this framework by proposing a task-oriented setting that introduces an objective function based on the \"majority vote\" principle. We demonstrate that the mutual information between the student and the teachers is an upper bound for this function, providing a task-agnostic loss for our distillation procedure. An extensive evaluation is performed in different domains ---natural language processing, computer vision, and molecular modeling --- indicating that our method effectively leverages teacher diversity to produce more informative representations. Finally, we use our method to train and release new state-of-the-art embedders, enabling improved downstream performance in NLP and molecular modeling.", "title_embedding_index": 2613, "title_abs_embedding_index": 2638}, {"title": "Calibrate to Discriminate:Improve In-Context Learning with Label-Free Comparative Inference", "link_suffix": "/forum?id=RUn41kd6i0", "link": "https://openreview.net/forum?id=RUn41kd6i0", "pdf_link": "https://openreview.net/pdf?id=RUn41kd6i0", "keywords": "Calibration. In-context learning. Comparative inference.", "abstract": "While in-context learning with large language models (LLMs) has shown impressive performance, we have discovered a unique miscalibration behavior where both correct and incorrect predictions are assigned the same level of confidence. We refer to this phenomenon as \\textit{indiscriminate miscalibration}. We found that traditional calibration metrics, such as Expected Calibrated Errors (ECEs), are unable to capture this behavior effectively. To address this issue, we propose new metrics to measure the severity of indiscriminate miscalibration. Additionally, we develop a novel in-context comparative inference method to alleviate miscalibrations and improve classification performance. Through extensive experiments on five datasets, we demonstrate that our proposed method can achieve more accurate and calibrated predictions compared to regular zero-shot and few-shot prompting.", "title_embedding_index": 2614, "title_abs_embedding_index": 2639}, {"title": "FEDERATED COMPOSITIONAL OPTIMIZATION: THE IMPACT OF TWO-SIDED LEARNING RATES ON COMMUNICATION EFFICIENCY", "link_suffix": "/forum?id=Ob0UafH2YI", "link": "https://openreview.net/forum?id=Ob0UafH2YI", "pdf_link": "https://openreview.net/pdf?id=Ob0UafH2YI", "keywords": "Compositional optimization, Federated learning, Federated averaging, Distributed learning", "abstract": "Compositional optimization (CO) has recently gained popularity due to its applications in distributionally robust optimization (DRO), meta-learning, reinforcement learning, and many other machine learning applications. The large-scale and distributed nature of data necessitates efficient federated learning (FL) algorithms for CO, but the compositional structure of the objective poses significant challenges. Current methods either rely on large batch gradients (which are impractical) or suffer from suboptimal communication efficiency. To address these challenges, we propose efficient FedAvg-type algorithms for solving non-convex CO in the FL setting. We first establish that standard FedAvg fails in solving the federated CO problems due to data heterogeneity, which amplifies bias in local gradient estimates. Our analysis establishes that either {\\em additional communication} or {\\em two-sided learning rate-based} algorithms are required to control this bias. To this end, we develop two algorithms for solving the federated CO problem. First, we propose FedDRO that utilizes the compositional problem structure to design a communication strategy that allows FedAvg to control the bias in the estimation of the compositional gradient, achieving $\\mathcal{O}(\\epsilon^{-2})$ sample and $\\mathcal{O}(\\epsilon^{-3/2})$ communication complexity. Then we propose DS-FedDRO, a two-sided learning rate algorithm, that eliminates the need for additional communication and achieves the optimal $\\mathcal{O}(\\epsilon^{-2})$ sample and $\\mathcal{O}(\\epsilon^{-1})$ communication complexity, highlighting the importance of two-sided learning rate algorithms for solving federated CO problems. The proposed algorithms avoid the need for large batch gradients and achieve linear speedup with the number of clients. We corroborate our theoretical findings with empirical studies on large-scale DRO problems.", "title_embedding_index": 2615, "title_abs_embedding_index": 2640}, {"title": "Parameterization Agnostic RL", "link_suffix": "/forum?id=bjMi9ux50f", "link": "https://openreview.net/forum?id=bjMi9ux50f", "pdf_link": "https://openreview.net/pdf?id=bjMi9ux50f", "keywords": "reinforcement learning, offline RL, online fine-tuning, online rl, diffusion policies", "abstract": "Recent advances in learning decision-making policies can largely be attributed to training expressive policy models, largely via imitation learning. While imitation discards non-expert data, reinforcement learning (RL) can still learn from suboptimal data. However, instantiating RL training of a new policy class often presents a different challenge: most deep RL machinery is co-developed with assumptions on the policy class, resulting in poor performance when the policy class changes. For e.g., SAC utilizes a low-variance reparameterization policy gradient for Gaussian policies, but this is unstable for diffusion policies and intractable for autoregressive (e.g., transformer) categorical policies. This implies that current RL algorithms may not perform well or may even not be applicable when the policy class changes. To address this issue, we develop an RL approach calledparameterization-agnostic RL(PA-RL) that can effectively train multiple policy classes, with varying architectures. Our insight is that a universal supervised learning loss can replace the policy improvement step in RL, as long as it is applied on \"optimized\" actions. To obtain these optimized actions, we first sample multiple actions from a base policy, and run global optimization (i.e., re-ranking multiple action samples using the Q-function) and local optimization (i.e., running gradient steps on an action sample) to maximize the critic on these candidates. \nWe show that PA-RL enables fine-tuning diffusion and autoregressive policies entirely via actor-critic RL, while improving performance and sample-efficiency compared to existing offline RL and online fine-tuning methods. PA-RL allows us to successfully fine-tune diffusion policies on two real-robot manipulation tasks.", "title_embedding_index": 2616, "title_abs_embedding_index": 2641}, {"title": "DAWN-SI: Data-Aware and Noise-Informed Stochastic Interpolation for Solving Inverse Problems", "link_suffix": "/forum?id=FR8mMMiu2L", "link": "https://openreview.net/forum?id=FR8mMMiu2L", "pdf_link": "https://openreview.net/pdf?id=FR8mMMiu2L", "keywords": "Inverse problems, Stochastic Interpolation, Noise-embedding, data-embedding", "abstract": "Inverse problems, which involve estimating parameters from incomplete or noisy observations, arise in various fields such as medical imaging, geophysics, and signal processing. These problems are often ill-posed, requiring regularization techniques to stabilize the solution. In this work, we employ $\\textit{Stochastic Interpolation (SI)}$, a generative framework that integrates both deterministic and stochastic processes to map a simple reference distribution, such as a Gaussian, to the target distribution. Our method $\\textit{\\textbf{DAWN-SI}}$: $\\textit{\\textbf{D}ata-\\textbf{AW}are and \\textbf{N}oise-informed \\textbf{S}tochastic \\textbf{I}nterpolation}$ incorporates $\\textit{data and noise embedding}$, allowing the model to access representations about the measured data explicitly and also account for noise in the observations, making it particularly robust in scenarios where data is noisy or incomplete. By learning a time-dependent velocity field, SI not only provides accurate solutions but also enables uncertainty quantification by generating multiple plausible outcomes. Unlike pre-trained diffusion models, which may struggle in highly ill-posed settings, our approach is trained specifically for each inverse problem and adapts to varying noise levels. We validate the effectiveness and robustness of our method through extensive numerical experiments on tasks such as image deblurring and tomography.", "title_embedding_index": 2617, "title_abs_embedding_index": 2642}, {"title": "Agential AI for integrated continual learning, deliberative behavior, and comprehensible models", "link_suffix": "/forum?id=Ltd1DTVjOM", "link": "https://openreview.net/forum?id=Ltd1DTVjOM", "pdf_link": "https://openreview.net/pdf?id=Ltd1DTVjOM", "keywords": "continual learning, deliberative behavior, planning, comprehensibility, agent", "abstract": "Contemporary machine learning paradigm excels in statistical data analysis, solving problems that classical AI couldn't. However, it faces key limitations, such as lack of integration with planning, incomprehensible internal structures, and inability to learn continually without erasing prior knowledge. We present initial design for an AI system, Agential AI (AAI), in principle operating independently or on top of statistical methods, that overcomes all these issues. AAI's core is a learning method that models temporal dynamics with guarantees of completeness, minimality, and continual learning. It integrates this with a behavior algorithm that plans on a learned model and encapsulate high-level behavior patterns. Preliminary experiments on a simple abstract environment show AAI's effectiveness and future potential.", "title_embedding_index": 2618, "title_abs_embedding_index": 2643}, {"title": "MathCAMPS: Fine-grained Synthesis of Mathematical Problems From Human Curricula", "link_suffix": "/forum?id=6MiOlatqMV", "link": "https://openreview.net/forum?id=6MiOlatqMV", "pdf_link": "https://openreview.net/pdf?id=6MiOlatqMV", "keywords": "large language models, reasoning, math word problems, benchmarking", "abstract": "Mathematical problem solving is an important skill for Large Language Models (LLMs), both as an important capability and a proxy for a range of reasoning abilities. Existing benchmarks probe a diverse set of skills, but they yield aggregate accuracy metrics, obscuring specific abilities or weaknesses. Furthermore, they are difficult to extend with new problems, risking data contamination over time. To address these challenges, we propose MathCAMPS: a method to synthesize high-quality mathematical problems at scale, grounded on 44 fine-grained \"standards\" from the Mathematics Common Core (CC) Standard for K-8 grades. We encode each standard in a formal grammar, allowing us to sample diverse symbolic problems and their answers. We then use LLMs to realize the symbolic problems into word problems. We propose a cycle-consistency method for validating problem faithfulness. Finally, we derivefollow-up questionsfrom symbolic structures and convert them into follow-up word problems - a novel task of mathematical dialogue that probes for robustness in understanding. Experiments on 29 LLMs show surprising failures even in the strongest models (in particular when asked simple follow-up questions). Moreover, we evaluate training checkpoints of Pythia 12B on MathCAMPS, allowing us to analyze when particular mathematical skills develop during its training. Our framework enables the community to reproduce and extend our pipeline for a fraction of the typical cost of building new high-quality datasets.", "title_embedding_index": 2619, "title_abs_embedding_index": 2644}, {"title": "Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering", "link_suffix": "/forum?id=LBl7Hez0fF", "link": "https://openreview.net/forum?id=LBl7Hez0fF", "pdf_link": "https://openreview.net/pdf?id=LBl7Hez0fF", "keywords": "Large Vision-Language Models, Multimodal large language model, Hallucination", "abstract": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from large language models (LLMs). We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional cost. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs.", "title_embedding_index": 2620, "title_abs_embedding_index": 2645}, {"title": "Certified Training with Branch-and-Bound: A Case Study on Lyapunov-stable Neural Control", "link_suffix": "/forum?id=8ctju6iFcn", "link": "https://openreview.net/forum?id=8ctju6iFcn", "pdf_link": "https://openreview.net/pdf?id=8ctju6iFcn", "keywords": "Certified training, Lyapunov condition", "abstract": "Certified training techniques aim to produce neural networks (NNs) with formally verifiable guarantees by optimizing their verification bounds during training. Existing work on certified training mainly focused on the local adversarial robustness of NNs. We consider certified training in a more challenging setting beyond adversarial robustness: we want to obtain NNs with global output guarantees for any input within an entire region-of-interest. As a case study, we particularly focus on a task about learning Lyapunov-stable neural controllers which provably satisfy the Lyapunov stability condition with a region-of-attraction. Compared to previous works which commonly used counterexample guided training on this task, we develop a new certified training framework and optimize for differentiable verified bounds, to produce verification-friendly models. In order to handle the relatively large region-of-interest, we propose a novel framework of training-time branch-and-bound to dynamically maintain a training dataset of subregions throughout training, such that the hardest subregions are iteratively split into smaller ones whose verified bounds can be computed more tightly to ease the training. We demonstrate that our new training framework can produce models which can be more efficiently verified at test time. On the largest 2D quadrotor dynamical system, verification for our model is more than 5X faster compared to the baseline, while our size of region-of-attraction is 16X larger than the baseline.", "title_embedding_index": 2621, "title_abs_embedding_index": 2646}, {"title": "Integrated Multi-system Prediction via Equilibrium State Evaluation", "link_suffix": "/forum?id=w2C7gJqaai", "link": "https://openreview.net/forum?id=w2C7gJqaai", "pdf_link": "https://openreview.net/pdf?id=w2C7gJqaai", "keywords": "Multi-system, Equilibrium, Prediction", "abstract": "This study presents a new paradigm of prediction, Equilibrium State Evaluation (ESE), which excels in multi-system prediction where systems interact with each other and every system needs its own prediction. Unlike mainstream prediction approaches, ESE views each system as an integral part under one structure and predicts all systems simultaneously in one go.  It evaluates these systems' equilibrium state by analyzing the dynamics of their attributes in a holistic manner, instead of treating each system as an individual time series. The effectiveness of ESE is verified in synthetic and real world scenarios, in particular COVID-19 transmission, where each geographic region can be viewed as a system.  So cases spreading across regions against the medical competency and demographic traits of these regions can be considered as an equilibrium problem rather than a time series problem.  Extensive analysis and experiments show that ESE is linear in complexity and can be 10+ times faster than SOTA methods, yet achieving comparable or better prediction accuracy.  More importantly, ESE can be integrated with these prediction methods to achieve both high accuracy and high speed, making it a powerful prediction mechanism, especially for scenarios that involve multiple systems. When the dimensionality of the multi-system increases, e.g. more systems joining, the advantages of ESE would become even more apparent.", "title_embedding_index": 2622, "title_abs_embedding_index": 2647}, {"title": "Basel: Target-Aware Basis Selection for Language Models", "link_suffix": "/forum?id=UUuTFhrWpM", "link": "https://openreview.net/forum?id=UUuTFhrWpM", "pdf_link": "https://openreview.net/pdf?id=UUuTFhrWpM", "keywords": "language models, interpretability", "abstract": "As the size of language models increases, they deliver substantial performance improvements across a variety of applications. However, this growth also leads to greater computational demands, making deployment on resource-constrained devices\u2014such as personal computers and mobile or wearable devices\u2014more challenging, and significantly raising inference costs on cloud servers. To address these challenges, we introduce a method to streamline language models. We observe that language models pretrained on general datasets often include redundant components that are unnecessary for particular tasks. Our approach identifies and removes these redundant parts, retaining only the essential components for the intended applications. Specifically, we represent the weight matrices of language models as a linear combination of base components, eliminate the irrelevant bases, and introduce new bases that enhance performance for target tasks. Evaluations show that our method reduces model size much more significantly\u2014by up to 1.7 times\u2014while maintaining similar accuracy, compared to state-of-the-art techniques, across a range of applications.", "title_embedding_index": 2623, "title_abs_embedding_index": 2648}, {"title": "HELENE: Hessian Layer-wise Clipping and Gradient Annealing for Accelerating Fine-tuning LLM with Zeroth-order Optimization", "link_suffix": "/forum?id=zuOnOAHBMy", "link": "https://openreview.net/forum?id=zuOnOAHBMy", "pdf_link": "https://openreview.net/pdf?id=zuOnOAHBMy", "keywords": "optimization, large language models", "abstract": "Fine-tuning large language models (LLMs) poses significant memory challenges, as the back-propagation process demands extensive resources, especially with growing model sizes. Recent work, MeZO, addresses this issue using a zeroth-order (ZO) optimization method, which reduces memory consumption by matching the usage to the inference phase. However, MeZO experiences slow convergence due to varying curvatures across model parameters. To overcome this limitation, we introduce HELENE, a novel scalable and memory-efficient optimizer that integrates annealed A-GNB gradients with a diagonal Hessian estimation and layer-wise clipping, serving as a second-order pre-conditioner. This combination allows for faster and more stable convergence. Our theoretical analysis demonstrates that HELENE improves convergence rates, particularly for models with heterogeneous layer dimensions, by reducing the dependency on the total parameter space dimension. Instead, the method scales with the largest layer dimension, making it highly suitable for modern LLM architectures. Experimental results on RoBERTa-large and OPT-1.3B across multiple tasks show that HELENE achieves up to a 20\u00d7 speedup compared to MeZO, with average accuracy improvements of 1.5%. Furthermore, HELENE remains compatible with both full parameter tuning and parameter-efficient fine-tuning (PEFT), outperforming several state-of-the-art optimizers. The codes will be released after reviewing.", "title_embedding_index": 2624, "title_abs_embedding_index": 2649}]