[{"title": "BiSSL: Bilevel Optimization for Self-Supervised Pre-Training and Fine-Tuning", "link_suffix": "/forum?id=pQdei0Zb7a", "link": "https://openreview.net/forum?id=pQdei0Zb7a", "pdf_link": "https://openreview.net/pdf?id=pQdei0Zb7a", "keywords": "Self-Supervised Learning, Bilevel Optimization", "abstract": "In this work, we present BiSSL, a first-of-its-kind training framework that introduces bilevel optimization to enhance the alignment between the pretext pre-training and downstream fine-tuning stages in self-supervised learning. BiSSL formulates the pretext and downstream task objectives as the lower- and upper-level objectives in a bilevel optimization problem and serves as an intermediate training stage within the self-supervised learning pipeline. By more explicitly modeling the interdependence of these training stages, BiSSL facilitates enhanced information sharing between them, ultimately leading to a backbone parameter initialization that is better suited for the downstream task. We propose a training algorithm that alternates between optimizing the two objectives defined in BiSSL. Using a ResNet-18 backbone pre-trained with SimCLR on the STL10 dataset, we demonstrate that our proposed framework consistently achieves improved or competitive classification accuracies across various downstream image classification datasets compared to the conventional self-supervised learning pipeline. Qualitative analyses of the backbone features further suggest that BiSSL enhances the alignment of downstream features in the backbone prior to fine-tuning.", "title_embedding_index": 5700, "title_abs_embedding_index": 5725}, {"title": "PaPaGei: Open Foundation Models for Optical Physiological Signals", "link_suffix": "/forum?id=kYwTmlq6Vn", "link": "https://openreview.net/forum?id=kYwTmlq6Vn", "pdf_link": "https://openreview.net/pdf?id=kYwTmlq6Vn", "keywords": "self-supervised learning, foundation models, time series, Photoplethysmography (PPG), health, physiology", "abstract": "Photoplethysmography (PPG) is the most widely used non-invasive technique for monitoring biosignals and cardiovascular health, with applications in both clinical settings and consumer health through wearable devices. Current machine learning models trained on PPG signals are mostly task-specific and lack generalizability. Previous works often used single-device datasets, did not explore out-of-domain generalization, or did not release their models, hindering reproducibility and further research. We introduce PaPaGei, the first open foundation model for PPG signals. PaPaGei is pre-trained on more than 57,000 hours of 20 million unlabeled segments of PPG signals using publicly available datasets exclusively. We evaluate against popular time-series foundation models and other benchmarks on 20 tasks of 10 diverse datasets spanning cardiovascular health, sleep disorders, pregnancy monitoring, and wellbeing assessment. Our architecture incorporates novel representation learning approaches that leverage differences in PPG signal morphology across individuals, enabling it to capture richer representations than traditional contrastive learning methods. Across 20 tasks, PaPaGei improves classification and regression performance by an average of 6.3% and 2.9%, respectively, compared to other competitive time-series foundation models in at least 14 tasks. PaPaGei is more data- and parameter-efficient than other foundation models or methods, as it outperforms 70x larger models. Beyond accuracy, we also investigate robustness against different skin tones, establishing a benchmark for bias evaluations of future models. Notably, PaPaGei can be used out of the box as both a feature extractor and an encoder for other multimodal models, opening up new opportunities for multimodal health monitoring. Models, data, and code will be available upon our public release. Preliminary code for reviewing purposes is available at:https://anonymous.4open.science/r/PaPaGei_ICLR_Review-6FC2/", "title_embedding_index": 5701, "title_abs_embedding_index": 5726}, {"title": "Unlocking Speech Instruction Data Potential with Query Rewriting", "link_suffix": "/forum?id=QQoWeCscSH", "link": "https://openreview.net/forum?id=QQoWeCscSH", "pdf_link": "https://openreview.net/pdf?id=QQoWeCscSH", "keywords": "Multimodal language model; Large Speech Language Model; Datasets", "abstract": "End-to-end Large Speech Language Models (LSLMs) demonstrate strong potential in response latency and speech comprehension capabilities, showcasing general intelligence across  speech understanding tasks. However, the ability to follow speech instructions has not been fully realized due to the lack of  datasets and heavily biased training tasks. Leveraging the rich ASR datasets, previous approaches have used Large Language Models (LLMs) to continue the linguistic information of speech to construct speech instruction datasets. Yet, due to the gap between LLM-generated results and real human responses, the continuation methods further amplify these shortcomings. Given the high costs of collecting and annotating  speech instruction dataset by human, using speech synthesis to construct large-scale speech instruction datasets has become a balanced and robust alternative. Although modern Text-To-Speech (TTS) models have achieved near-human-level synthesis quality, it is challenging to appropriately convert out-of-distribution text instruction to speech due to the limitations of the training data distribution in TTS models.To address this issue, we propose a query rewriting framework with multi-LLM knowledge fusion, employing multiple agents to annotate and validate the synthesized speech, making it possible to construct high-quality speech instruction datasets without relying on human annotation. Experiments show that this method can transform text instructions into distributions more suitable for TTS models for speech synthesis through zero-shot rewriting, increasing data usability from 71% to 93%. It also demonstrates unique advantages in rewriting tasks that require complex knowledge and context-related abilities.", "title_embedding_index": 5702, "title_abs_embedding_index": 5727}, {"title": "CoLa-DCE \u2013 Concept-guided Latent Diffusion Counterfactual Explanations", "link_suffix": "/forum?id=IQ0BBfbYR2", "link": "https://openreview.net/forum?id=IQ0BBfbYR2", "pdf_link": "https://openreview.net/pdf?id=IQ0BBfbYR2", "keywords": "Counterfactual Explanations, Concept-based Explanations, Diffusion-based Counterfactuals, Counterfactual Image Generation", "abstract": "Recent advancements in generative AI have introduced novel prospects and prac-\ntical implementations. Especially diffusion models show their strength in gener-\nating diverse and, at the same time, realistic features, positioning them well for\ngenerating counterfactual explanations for computer vision models. Answering\n\u201cwhat if\u201d questions of what needs to change to make an image classifier change\nits prediction, counterfactual explanations align well with human understanding\nand consequently help in making model behavior more comprehensible. Current\nmethods succeed in generating authentic counterfactuals, but lack transparency as\nfeature changes are not directly perceivable. To address this limitation, we intro-\nduce Concept-guided Latent Diffusion Counterfactual Explanations (CoLa-DCE).\nCoLa-DCE generates concept-guided counterfactuals for any classifier with a high\ndegree of control regarding concept selection and spatial conditioning. The coun-\nterfactuals comprise an increased granularity through minimal feature changes.\nThe reference feature visualization ensures better comprehensibility, while the\nfeature localization provides increased transparency of \u201cwhere\u201d changed \u201cwhat\u201d.\nWe demonstrate the advantages of our approach in minimality and comprehen-\nsibility across multiple image classification models and datasets and provide in-\nsights into how our CoLa-DCE explanations help comprehend model errors like\nmisclassification cases.", "title_embedding_index": 5703, "title_abs_embedding_index": 5728}, {"title": "Error Correcting by Agreement Checking for Adversarial Robustness against Black-box Attacks", "link_suffix": "/forum?id=ge5PasXuJ6", "link": "https://openreview.net/forum?id=ge5PasXuJ6", "pdf_link": "https://openreview.net/pdf?id=ge5PasXuJ6", "keywords": "adversarial defense; AT; black-box; SQA", "abstract": "Drawing inspiration from the vulnerability of the initial feed-forward phase of biological perception in humans and primates to adversarial attacks, we propose a novel defense strategy named Error Correcting by Agreement Checking (ECAC). This strategy is designed to mitigate realistic \\emph{black-box} threats where attackers don't have full access to the model. We exploit the fact that natural and adversarially trained models rely on distinct feature sets for classification. \nNotably, naturally trained models retain commendable accuracy against adversarial examples generated using adversarially trained models. Leveraging this disparity, ECAC moves the input toward the prediction of the naturally trained model unless it leads to disagreement in prediction between the two models, before making the prediction. \nThis simple error correction mechanism is highly effective against leading SQA (Score-based Query Attacks) black-box attacks as well as decision-based and transfer-based black-box attacks. \nWe also verify that, unlike other black-box defense, ECAC maintains significant robustness even when adversary has full access to the model. We demonstrate its effectiveness through comprehensive experiments across various datasets (CIFAR and ImageNet) and architectures (ResNet as well as ViT).", "title_embedding_index": 5704, "title_abs_embedding_index": 5729}, {"title": "Non-invasive Neural Decoding in Source Reconstructed Brain Space", "link_suffix": "/forum?id=g3PuaFh5vV", "link": "https://openreview.net/forum?id=g3PuaFh5vV", "pdf_link": "https://openreview.net/pdf?id=g3PuaFh5vV", "keywords": "Neural decoding, MEG, brain decoding, structured learning, brain computer interface", "abstract": "Non-invasive brainwave decoding is usually done using Magneto/Electroencephalography (MEG/EEG) sensor measurements as inputs. This makes combining datasets and building models with inductive biases difficult as most datasets use different scanners and the sensor arrays have a nonintuitive spatial structure. In contrast, fMRI scans are acquired directly in brain space, a voxel grid with a typical structured input representation. By using established techniques to reconstruct the sensors' sources' neural activity it is possible to decode from voxels for MEG data as well. We show that this enables spatial inductive biases, spatial data augmentations, better interpretability, zero-shot generalisation between datasets, and data harmonisation.", "title_embedding_index": 5705, "title_abs_embedding_index": 5730}, {"title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead", "link_suffix": "/forum?id=hHNVn4hFPk", "link": "https://openreview.net/forum?id=hHNVn4hFPk", "pdf_link": "https://openreview.net/pdf?id=hHNVn4hFPk", "keywords": "LoRA, LLM, PEFT, Infrastructure, NLP", "abstract": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of model compression when serving LoRAs. We propose a method for joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are more amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 500 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a \\emph{single} LoRA.", "title_embedding_index": 5706, "title_abs_embedding_index": 5731}, {"title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning", "link_suffix": "/forum?id=7dPrT34fHF", "link": "https://openreview.net/forum?id=7dPrT34fHF", "pdf_link": "https://openreview.net/pdf?id=7dPrT34fHF", "keywords": "Hierarchical Reinforcement Learning, Reinforcement Learning theory, PAC algorithm, MDP abstractions", "abstract": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks.\nDespite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees.This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes.\nThe notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees.\nIndeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options.\nAs demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs.\nBased on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input.\nWe show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.", "title_embedding_index": 5707, "title_abs_embedding_index": 5732}, {"title": "Formalizing Spuriousness of Biased Datasets using Partial Information Decomposition", "link_suffix": "/forum?id=vmkpk0ed1F", "link": "https://openreview.net/forum?id=vmkpk0ed1F", "pdf_link": "https://openreview.net/pdf?id=vmkpk0ed1F", "keywords": "Explainability Framework, Spuriousness, Partial Information Decomposition, Blackwell Sufficiency, Auto-encoder, Worst-group Accuracy", "abstract": "Spuriousness arises when there is an association between two or more variables in a dataset that are not causally related. Left unchecked, they can mislead a machine learning model into using the undesirable spurious features in decision-making over the core features, hindering generalization. In this work, we propose a novel explainability framework to disentangle the nature of such spurious associations, i.e., how the information about a target variable is distributed among the spurious and core features. Our framework leverages a body of work in information theory called Partial Information Decomposition (PID) to first decompose the total information about the target into four non-negative quantities namely unique information (in core and spurious features respectively), redundant information, and synergistic information. Next, we leverage this decomposition to propose a novel measure of the spuriousness of a dataset that steers models into choosing the spurious features over the core. We arrive at this measure systematically by examining several candidate measures, and demonstrating what they capture and miss through intuitive canonical examples and counterexamples. Our proposed explainability framework Spurious Disentangler consists of segmentation, dimensionality reduction, and estimation modules, with capabilities to specifically handle high dimensional image data efficiently. Finally, we also conduct empirical evaluation to demonstrate the trends of unique, redundant, and synergistic information, as well as our proposed spuriousness measure across several benchmark datasets under various settings. Interestingly, we observe a novel tradeoff between our measure of dataset spuriousness and empirical model generalization metrics such as worst-group accuracy, further supporting our proposition.", "title_embedding_index": 5708, "title_abs_embedding_index": 5733}, {"title": "Learning Constrained Markov Decision Processes With Non-stationary Rewards and Constraints", "link_suffix": "/forum?id=ZJ9LglIakj", "link": "https://openreview.net/forum?id=ZJ9LglIakj", "pdf_link": "https://openreview.net/pdf?id=ZJ9LglIakj", "keywords": "CMDPs, Non-stationary, Online learning", "abstract": "In constrained Markov decision processes (CMDPs) with adversarial rewards and constraints, a well-known impossibility result prevents any algorithm from attaining both sublinear regret and sublinear constraint violation, when competing against a best-in-hindsight policy that satisfies constraints on average. In this paper, we show that this negative result can be eased in CMDPs with non-stationary rewards and constraints, by providing algorithms whose performances smoothly degrade as non-stationarity increases.\nSpecifically, we propose algorithms attaining $\\tilde{\\mathcal{O}} (\\sqrt{T} + C)$ regret and positive constraint violation under bandit feedback, where $C$ is a corruption value measuring the environment non-stationarity. This can be $\\Theta(T)$ in the worst case, coherently with the impossibility result for adversarial~CMDPs. First, we design an algorithm with the desired guarantees when $C$ is known. Then, in the case $C$ is unknown, we show how to obtain the same results by embedding such an algorithm in a general meta-procedure. This is of independent interest, as it can be applied to any non-stationary constrained online learning setting.", "title_embedding_index": 5709, "title_abs_embedding_index": 5734}, {"title": "Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations", "link_suffix": "/forum?id=DPzQ5n3mNm", "link": "https://openreview.net/forum?id=DPzQ5n3mNm", "pdf_link": "https://openreview.net/pdf?id=DPzQ5n3mNm", "keywords": "Fourier Neural Operator, Sensitivity Analysis, Parametric Differential Equations, Surrogate Modes, Differentiable Numerical Solvers, Inverse Problems", "abstract": "Parametric differential equations of the form $\\partial{\\textbf{u}}/\\partial{t} = f(\\textbf{u}, \\textbf{x}, t, \\mathbf{p})$, where $\\mathbf{p}$ represents physical system parameters, are fundamental across scientific and engineering disciplines. Recent advances in surrogate modeling, particularly deep learning frameworks like the Fourier Neural Operator (FNO), have demonstrated significant efficiency in approximating differential equation solution paths $\\textbf{u}$. However, these approximations result in inaccurate solutions to inverse problems (using neural operators in an optimization routine for estimating physical system parameters), provide inaccurate estimates of \\emph{sensitivities} (i.e., $\\partial{\\textbf{u}}/\\partial{\\textbf{p}}$: the dependence of the solution path on the physical parameters) needed for scenario analysis and optimization, and are highly sensitive to concept drift. These issues are all related and can be addressed by using a novel \\emph{sensitivity loss} regularizer that we propose in this paper. This regularizer works with a wide range of neural operators, but for concreteness, we focus on applying it to the popular FNO framework, resulting in \\emph{Sensitivity-Constrained Fourier Neural Operators} (SC-FNO). SC-FNO ensures accuracy in the solution paths, inverse problems, and sensitivity calculations, even under sparse training data or concept drift scenarios. Our approach maintains high accuracy for the solution paths $\\mathbf{u}$ and significantly outperforms both the original FNO and FNO combined with Physics-Informed Neural Network (PINN) regularization on the remaining tasks.\nNotably, in parameter inversion tasks from solution paths, SC-FNO exhibits markedly superior accuracy, even when FNO breaks down, underscoring the critical role of sensitivity awareness. These conclusions are robust for various differential equations, neural operators, and different ways of supervising sensitivities for training.\nThese improvements, without large computational or memory costs, enhance the reliability and applicability of neural operators in complex physical systems modeling and engineering analysis.", "title_embedding_index": 5710, "title_abs_embedding_index": 5735}, {"title": "Anomaly Detection in Dynamic Graphs via Adversarial Autoencoder", "link_suffix": "/forum?id=hx8E1L4v2e", "link": "https://openreview.net/forum?id=hx8E1L4v2e", "pdf_link": "https://openreview.net/pdf?id=hx8E1L4v2e", "keywords": "anomaly detection, dynamic graph, graph learning, deep learning, autoencoder", "abstract": "Anomaly detection in dynamic graphs is a very important task that has attracted a lot of attention. Many dynamic graph anomaly detection methods are already available, but most of these efforts are based on supervised learning. In the real world, however, it is often difficult to collect large amounts of labelled anomaly data, which is not conducive to the training of these supervised methods and severely reduces their ability to be applied in different dynamic graph anomaly detection scenarios. A novel semi-supervised anomaly detection framework \\textbf{AAEDY}  for the detection of anomalous edges in dynamic graphs is presented in this paper, which improves reconstruction by combining adversarial based on autoencoder, and discriminates whether an edge is anomalous by comparing the original edge to the reconstructed edge in low-dimensional space.  Extensive experiments have been carried out on six real-world datasets, and the experimental results show that \\textbf{AAEDY} can outperform the state-of-the-art competitors in anomaly detection significantly.", "title_embedding_index": 5711, "title_abs_embedding_index": 5736}, {"title": "Captured by Captions: On Memorization and its Mitigation in CLIP Models", "link_suffix": "/forum?id=5V0f8igznO", "link": "https://openreview.net/forum?id=5V0f8igznO", "pdf_link": "https://openreview.net/pdf?id=5V0f8igznO", "keywords": "memorization, multi-modal, clip, vision language models", "abstract": "Multi-modal models, such as CLIP, have demonstrated strong performance in aligning visual and textual representations, excelling in tasks like image retrieval and zero-shot classification. Despite this success, the mechanisms by which these models utilize training data, particularly the role of memorization, remain unclear. In uni-modal models, both supervised and self-supervised, memorization has been shown to be essential for generalization. However, it is not well understood how these findings would apply to CLIP, which incorporates elements from both supervised learning via captions that provide a supervisory signal similar to labels, and from self-supervised learning via the contrastive objective.\nTo bridge this gap in understanding, we propose a formal definition of memorization in CLIP (CLIPMem) and use it to quantify memorization in CLIP models. Our results indicate that CLIP\u2019s memorization behavior falls between the supervised and self-supervised paradigms, with \"mis-captioned\" samples exhibiting highest levels of memorization. \nAdditionally, we find that the text encoder contributes more to memorization than the image encoder, suggesting that mitigation strategies should focus on the text domain. \nBuilding on these insights, we propose multiple strategies to reduce memorization while at the same time improving utility---something that had not been shown before for traditional learning paradigms where reducing memorization typically results in utility decrease.", "title_embedding_index": 5712, "title_abs_embedding_index": 5737}, {"title": "TLCM: Training- efficient Latent Consistency Model for Image Generation with 2-8 Steps", "link_suffix": "/forum?id=zM92zziRtQ", "link": "https://openreview.net/forum?id=zM92zziRtQ", "pdf_link": "https://openreview.net/pdf?id=zM92zziRtQ", "keywords": "latent diffusion model, consistency model, acceleration", "abstract": "Distilling latent diffusion models (LDMs) into ones that are fast to sample from is attracting growing research interest. However, the majority of existing methods face two critical challenges:They need to perform long-time learning with a huge volume of real data.They routinely lead to quality degradation for generation, especially in text-image alignment.This paper proposes the novel Training-efficient Latent Consistency Model (TLCM) to overcome these challenges.Our method first fast accelerate LDMs via data-free multistep latent consistency distillation (MLCD), then  data-free latent consistency distillation  is proposed to guarantee the inter-segment consistency in MLCD at low cost.Furthermore, we introduce bags of techniques to enhance TLCM's performance at rare-step inference without any real data, e.g., distribution matching, adversarial learning, and preference learning. \nTLCM demonstrates a high level of flexibility by allowing for adjustment of sampling steps within the range of 2 to 8 while still producing competitive outputs compared to full-step approaches.\nAs its name suggests, TLCM excels in training efficiency in terms of both computational resources and data utilization.\nNotably, TLCM operates without reliance on a training dataset but instead employs synthetic data for the teacher itself during distillation. With just 70 training hours on an A100 GPU, a 3-step TLCM distilled from SDXL achieves an impressive CLIP Score of 33.68 and an Aesthetic Score of 5.97 on the MSCOCO-2017 5K benchmark, surpassing various accelerated models and even outperforming the teacher model in human preference metrics. \nWe also demonstrate the versatility of TLCMs in applications including controllable generation, image style transfer, and Chinese-to-image generation.", "title_embedding_index": 5713, "title_abs_embedding_index": 5738}, {"title": "From Conflicts to Convergence: A Zeroth-order Method for Multi-Objective Learning", "link_suffix": "/forum?id=RsDYaswfEj", "link": "https://openreview.net/forum?id=RsDYaswfEj", "pdf_link": "https://openreview.net/pdf?id=RsDYaswfEj", "keywords": "Zeroth-order method\uff0cMulti-objective learning\uff0cOptimization\uff0cGeneralization", "abstract": "Multi-objective learning (MOL) is a popular paradigm for learning problems under multiple criteria, where various dynamic weighting algorithms (e.g., MGDA and MODO) have been formulated to find an updated direction for avoiding conflicts among objectives. Recently, increasing endeavors have struggled to tackle the black-box MOL when the gradient information of objectives is unavailable or difficult to be attained.  Albeit the impressive success of zeroth-order method for single-objective black-box learning,  the corresponding MOL algorithm and theoretical understanding are largely absent. Unlike single-objective problems, the errors of MOL introduced by zeroth-order gradients can simultaneously affect both the gradient estimation and the gradient coefficients $\\lambda$, leading to further error amplification. To address this issue, we propose a Stochastic Zeroth-order Multiple Objective Descent algorithm (SZMOD), which leverages function evaluations to approximate gradients and develops a new decomposition strategy to handle the complicated black-box multi-objective optimization. Theoretically, we provide convergence and generalization guarantees for SZMOD in both general non-convex and strongly convex settings. Our results demonstrate that the proposed SZMOD enjoys a promising generalization bound of   $\\mathcal{O}(n^{-\\frac{1}{2}})$, which is comparable to the existing results of first-order methods requiring additional gradient information. Experimental results validate our theoretical analysis.", "title_embedding_index": 5714, "title_abs_embedding_index": 5739}, {"title": "Online Policy Selection for Inventory Problems", "link_suffix": "/forum?id=a8mKwRQQrP", "link": "https://openreview.net/forum?id=a8mKwRQQrP", "pdf_link": "https://openreview.net/pdf?id=a8mKwRQQrP", "keywords": "Online learning, Inventory control", "abstract": "We tackle online inventory problems where at each time period the manager makes a replenishment decision based on partial historical information in order to meet demands and minimize costs. To solve such problems, we build upon recent works in online learning and control, use insights from inventory theory and propose a new algorithm called GAPSI. This algorithm follows a new feature-enhanced base-stock policy and deals with the troublesome question of non-differentiability which occurs in inventory problems. Our method is illustrated in the context of a complex and novel inventory system involving multiple products, lost sales, perishability, warehouse-capacity constraints and lead times. Extensive numerical simulations are conducted to demonstrate the good performances of our algorithm on real-world data.", "title_embedding_index": 5715, "title_abs_embedding_index": 5740}, {"title": "The other you in black mirror: first steps from chatbots to personalized LLM clones", "link_suffix": "/forum?id=znGnmAM44K", "link": "https://openreview.net/forum?id=znGnmAM44K", "pdf_link": "https://openreview.net/pdf?id=znGnmAM44K", "keywords": "Large Language Models (LLMs), Personalized AI, Turing Test, AI Safety", "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in a wide\nvariety of generic tasks. Here we investigate whether it is possible to use LLMs\nto partially replicate cognitive aspects of an individual by fine-tuning an LLM\nwith personal data. Our model, A-clone, built on the pretrained Llama3-70B, was\nfine-tuned with a private dataset from one volunteer referred to as A throughout. We\nevaluated A-clone in two ways. First, using 701 open-ended questions, we gathered\nresponses from A, A-clone, other LLMs, and A\u2019s family members imitating A.\nWe conducted a Turing-like test where 31 participants with varying degrees of\nfamiliarity with A attempted to identify A\u2019s real answers in a question-and-answer\ntask. Human participants identified the genuine responses from A 55% \u00b1 7%\nof the time, just over chance levels. A-clone outperformed all other baselines\nin mimicking adequate responses from A. Second, we compared the outputs\nof A-Clone with the ground truth from A in 10 psychological, moral, career,\npolitical tendency, and general knowledge tests, containing 484 questions altogether.\nA-Clone demonstrated a strong correlation with A\u2019s responses. This work provides\nan initial, proof-of-principle, evaluation of the possibility of mimicking the\nresponses of an individual, opening doors to many real-world applications but\nalso raising potential privacy and safety concerns about digital clones. The code\nand data can be found in this link.", "title_embedding_index": 5716, "title_abs_embedding_index": 5741}, {"title": "Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data", "link_suffix": "/forum?id=Ei9KiIzgxK", "link": "https://openreview.net/forum?id=Ei9KiIzgxK", "pdf_link": "https://openreview.net/pdf?id=Ei9KiIzgxK", "keywords": "Offline Reinforcement Learning, Generalization, Data Augmentation, Synthetic Data Generation", "abstract": "Offline reinforcement learning (RL) offers a promising framework for training agents using pre-collected datasets without the need for further environment interaction. However, policies trained on offline data often struggle to generalise\ndue to limited exposure to diverse states. The complexity of visual data introduces additional challenges such as noise, distractions, and spurious correlations, which can misguide the policy and increase the risk of overfitting if the training data is not sufficiently diverse. Indeed, this makes it challenging to leverage vision-based offline data in training robust agents that can generalize to unseen environments. To solve this problem, we propose a simple approach\u2014generating additional synthetic data. We propose a two-step process, first $augmenting$ the originally collected offline data to improve zero-shot generalization by introducing diversity, then using a diffusion model to $generate$ additional data in latent space. We test our method across both continuous action spaces (Visual D4RL) and discrete action spaces (Procgen), demonstrating that it significantly improves generalization without requiring any algorithmic changes to existing model-free offline RL methods. We show that our method not only increases the diversity of the training data but also significantly reduces the generalization gap at test time while maintaining computational efficiency. We believe this approach could fuel additional progress in generating synthetic data to train more general agents in the future.", "title_embedding_index": 5717, "title_abs_embedding_index": 5742}, {"title": "Understanding Optimization of Operator Networks with Variational Loss for Solving PDEs", "link_suffix": "/forum?id=xpmDc76RN2", "link": "https://openreview.net/forum?id=xpmDc76RN2", "pdf_link": "https://openreview.net/pdf?id=xpmDc76RN2", "keywords": "Restriced Strong Convexity, Operator Learning, Variational Loss, Scientific machine learning", "abstract": "In this paper, we analyze the optimization of operator networks for solving elliptic PDEs with variational loss functions. While approximation and generalization errors in operator networks have been extensively studied, optimization error remains largely unexplored. \nWe apply Restricted Strong Convexity (RSC) theory to rigorously examine the optimization dynamics of operator networks trained with variational loss, providing theoretical guarantees for convergence and training stability. \nWe further investigate the role of the condition number of $A$ in optimization and demonstrate that preconditioning strategies significantly improve convergence rates, establishing a solid theoretical basis for the empirical benefits of preconditioning. We also address the lower bound of a key quantity, $q_t$, which ensures convergence. \nTo prevent $q_t$ from vanishing, we propose an algorithm that adaptively incorporates additional weights into the variational loss function, leveraging values already computed during training, thereby avoiding any extra computational costs.\nFinally, we validate {our theoretical assumptions through numerical experiments, demonstrating their practical applicability} and confirming the effectiveness of preconditioning, with significant improvements in training performance and convergence rates.", "title_embedding_index": 5718, "title_abs_embedding_index": 5743}, {"title": "Minimax-optimal trust-aware multi-armed bandits", "link_suffix": "/forum?id=jsVehKnSj4", "link": "https://openreview.net/forum?id=jsVehKnSj4", "pdf_link": "https://openreview.net/pdf?id=jsVehKnSj4", "keywords": "multi-armed bandit, trust-aware decision-making, regret bound, minimax optimality", "abstract": "Multi-armed bandit (MAB) algorithms have achieved significant success in sequential decision-making applications, under the premise that humans perfectly implement the recommended policy. However, existing methods often overlook the crucial factor of human trust in learning algorithms. When trust is lacking, humans may deviate from the recommended policy, leading to undesired learning performance. Motivated by this gap, we study the trust-aware MAB problem by integrating a dynamic trust model into the standard MAB framework. Specifically, it assumes that the recommended and actually implemented policy differs depending on human trust, which in turn evolves with the quality of the recommended policy. We establish the minimax regret in the presence of the trust issue and demonstrate the suboptimality of vanilla MAB algorithms such as the upper confidence bound (UCB) algorithm. To overcome this limitation, we introduce a novel two-stage trust-aware procedure that provably attains near-optimal statistical guarantees. A simulation study is conducted to illustrate the benefits of our proposed algorithm when dealing with the trust issue.", "title_embedding_index": 5719, "title_abs_embedding_index": 5744}, {"title": "CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL", "link_suffix": "/forum?id=CvGqMD5OtX", "link": "https://openreview.net/forum?id=CvGqMD5OtX", "pdf_link": "https://openreview.net/pdf?id=CvGqMD5OtX", "keywords": "Text-to-SQL, LLM, Databases", "abstract": "In addressing the challenges of improving large language model (LLM) performance for Text-to-SQL tasks, we propose a new framework, CHASE-SQL, that is comprised of innovative strategies that leverage judiciously-designed test-time compute in multi-agent modeling to enhance candidate generation and selection. Our approach leverages LLMs\u2019 intrinsic knowledge to generate diverse and high-quality SQL candidates using different LLM generators with:  (1) a divide-and-conquer method that decomposes complex queries into manageable sub-queries in a single LLM call;  (2) chain-of-thought reasoning based on query execution plans,  reflecting  the  steps  a  database  engine  takes  during  execution;  and  (3)  a unique instance-aware synthetic example generation technique, which offers specific few-shot demonstrations tailored to test questions.  To identify the best candidate,  a  selection  agent  is  employed  to  rank  the  candidates  through  pairwise comparisons with a fine-tuned binary-candidates selection LLM. This selection approach  has  been  demonstrated  more  robust  over  alternatives.   The  proposed generators-selector framework not only enhances the quality and diversity of SQL queries but also outperforms previous methods.  Overall, our proposed CHASE-SQL achieves the state-of-the-art execution accuracy of 73.0 % and 73.01% on the test set and development set of the notable BIRD Text-to-SQL dataset bench-mark, rendering CHASE-SQL the top submission of the leaderboard (at the time of paper submission)", "title_embedding_index": 5720, "title_abs_embedding_index": 5745}, {"title": "JPEG Inspired Deep Learning", "link_suffix": "/forum?id=te2IdORabL", "link": "https://openreview.net/forum?id=te2IdORabL", "pdf_link": "https://openreview.net/pdf?id=te2IdORabL", "keywords": "Deep Learning, JPEG Compression, Quantization, Non-linearity", "abstract": "Although it is traditionally believed that lossy image compression, such as JPEG compression, has a negative impact on the performance of deep neural networks (DNNs), it is shown by recent works that well-crafted JPEG compression can actually improve the performance of deep learning (DL). Inspired by this, we propose JPEG-DL, a novel DL framework that prepends any underlying DNN architecture with a trainable JPEG compression layer. To make the quantization operation in JPEG compression trainable, a new differentiable soft quantizer is employed at the JPEG layer, and then the quantization operation and underlying DNN are jointly trained. Extensive experiments show that in comparison with the standard DL,  JPEG-DL delivers significant accuracy improvements across various datasets and model architectures while enhancing robustness against adversarial attacks. Particularly, on some fine-grained image classification datasets, JPEG-DL can increase prediction accuracy by as much as 20.9%. Our code is available onhttps://github.com/JpegInspiredDl/JPEG-Inspired-DL.git.", "title_embedding_index": 5721, "title_abs_embedding_index": 5746}, {"title": "Fully Fine-Tuning Beats Parameter Efficient Fine-Tuning for CLIP in Data-Limited Scenarios", "link_suffix": "/forum?id=VbszSB4pK6", "link": "https://openreview.net/forum?id=VbszSB4pK6", "pdf_link": "https://openreview.net/pdf?id=VbszSB4pK6", "keywords": "Vision-Language Models; Parameter-Efficient Fine-tuning;", "abstract": "Prompt tuning, which involves training a small set of parameters, effectively enhances the pre-trained Vision-Language Models (VLMs) to downstream tasks. However, they often come at the cost of flexibility and adaptability when the tuned models are applied to different datasets or domains. In this paper, we revisit the vanilla full fine-tuning in VLMs and show that fully fine-tuning can be more efficient than prompt tuning in data-limited scenarios. To mitigate the overfitting and catastrophic forgetting issues encountered when fine-tuning the entire VLMs for specific tasks under limited supervision, we propose a framework named CLIP-CITE via designing a discriminative visual-text task, further aligning the visual-text semantics in a supervision manner, and integrating knowledge distillation techniques to preserve the gained knowledge. Extensive experimental results under few-shot learning, base-to-new generalization, domain generalization, and cross-domain generalization settings, demonstrate that our method effectively enhances the performance on specific tasks under limited supervision while preserving the versatility of the VLMs on other datasets.", "title_embedding_index": 5722, "title_abs_embedding_index": 5747}, {"title": "C2INet: Realizing Incremental Trajectory Prediction with Prior-Aware Continual Causal Intervention", "link_suffix": "/forum?id=5IvTw0qMKj", "link": "https://openreview.net/forum?id=5IvTw0qMKj", "pdf_link": "https://openreview.net/pdf?id=5IvTw0qMKj", "keywords": "Trajectory Prediction, Causal Intervention, Variational Inference, Continual Learning", "abstract": "Trajectory prediction for multi-agents in complex scenarios is crucial for applications like autonomous driving. However, existing methods often overlook environmental biases, which leads to poor generalization. Additionally, hardware constraints limit the use of large-scale data across environments, and continual learning settings exacerbate the challenge of catastrophic forgetting. To address these issues, we propose the Continual Causal Intervention (C$^{2}$INet) method for generalizable multi-agent trajectory prediction within a continual learning framework. Using variational inference, we align environment-related prior with posterior estimator of confounding factors in the latent space, thereby intervening in causal correlations that affect trajectory representation. Furthermore, we store optimal variational priors across various scenarios using a memory queue, ensuring continuous debiasing during incremental task training. The proposed C$^{2}$INet enhances adaptability to diverse tasks while preserving previous task information to prevent catastrophic forgetting. It also incorporates pruning strategies to mitigate overfitting.\nComparative evaluations on three real and synthetic complex datasets against state-of-the-art methods demonstrate that our proposed method consistently achieves reliable prediction performance, effectively mitigating confounding factors unique to different scenarios. This highlights the practical value of our method for real-world applications.", "title_embedding_index": 5723, "title_abs_embedding_index": 5748}, {"title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents", "link_suffix": "/forum?id=K5yeB4dTtS", "link": "https://openreview.net/forum?id=K5yeB4dTtS", "pdf_link": "https://openreview.net/pdf?id=K5yeB4dTtS", "keywords": "multimodal retrieval, interactive learning, MLLM embodied agent", "abstract": "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MART, which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released.", "title_embedding_index": 5724, "title_abs_embedding_index": 5749}]