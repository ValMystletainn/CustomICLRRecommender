[
    {
        "title": "Cost-Efficient Multi-Fidelity Alignment for LLMs",
        "link_suffix": "/forum?id=OaORjvWelu",
        "link": "https://openreview.net/forum?id=OaORjvWelu",
        "pdf_link": "https://openreview.net/pdf?id=OaORjvWelu",
        "keywords": "Multi-Fidelity, Alignment, LLM",
        "abstract": "Alignment is a critical step in large language model (LLM) post-training. It typically requires human annotations to align the model's output to human preferences, which is prohibitively expensive. This paper proposes a novel approach to reduce the alignment cost.\n Specifically, we consider multiple levels of alignment with different qualities and response-generating costs, which we refer to as multi-fidelity alignment. We develop a new approach to incorporating the varying levels of response quality to train a language model, aiming to reduce the cost of response collection for alignment while maintaining the performance of the language model. We provide theoretical insights and empirical results to support the effectiveness of the proposed multi-fidelity alignment approach. Lastly, we conduct experiments to corroborate the effectiveness of the proposed approach by comparing its performance with the vanilla alignment methods."
    },
    {
        "title": "Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance",
        "link_suffix": "/forum?id=SPS6HzVzyt",
        "link": "https://openreview.net/forum?id=SPS6HzVzyt",
        "pdf_link": "https://openreview.net/pdf?id=SPS6HzVzyt",
        "keywords": "Instruction finetuning, context-vs-parametric reliance",
        "abstract": "Large Language Model's are instruction-finetuned to enhance their ability to follow user instructions and better comprehend input context. Still, they often struggle to follow the input context, especially when it contradicts model's parametric knowledge. This manifests as various failures, such as hallucinations where a model inserts outdated or unwarranted facts into its response. In this work, we observe an intriguing phenomenon: the context reliance of the model decreases as instruction finetuning progresses, $\\textit{despite an initial expected increase}$. We call this phenomenon as the $\\textbf{context-parametric inversion}$. This is surprising, as one would expect instruction tuning to improve the model's ability to follow input instructions.  We observe this behavior on multiple general purpose instruction tuning datasets such as TULU, Alpaca and Ultrachat, across multiple model families like Llama, Mistral and Pythia.  We perform various controlled studies to eliminate some simple hypothesis for this observed behavior and isolate what datapoints cause this counter-intuitive behavior. We then analyze the phenomenon theoretically, to explain why context reliance varies across the trajectory of finetuning. \nWe tie the observed context-parametric inversion to the properties of the finetuning data, which provides us with some potential mitigation strategies that provide limited but insightful gains."
    },
    {
        "title": "A VARIATIONAL FRAMEWORK FOR GRAPH GENERATION WITH FINE-GRAINED TOPOLOGICAL CONTROL",
        "link_suffix": "/forum?id=xYquBPHppn",
        "link": "https://openreview.net/forum?id=xYquBPHppn",
        "pdf_link": "https://openreview.net/pdf?id=xYquBPHppn",
        "keywords": "Controlled Graph  Generation",
        "abstract": "Controlled graph generation is the process of generating graphs that satisfy specific topological properties (or attributes). Fine-grained control over graph properties allows for customizing generated graphs to precise specifications, which is essential for understanding and modeling complex networks. Existing approaches can only satisfy a few topological properties such as number of nodes or edges in output graphs. This paper introduces CGRAPHGEN, a novel conditional variational autoencoder that, unlike existing approaches, uses graph adjacency matrix during training, along with the desired graph properties, for improved decoder tuning and precise graph generation, while relying only on attributes during inference. In addition, CGRAPHGEN implements an effective scheduling technique to integrate representations from both adjacency matrix and attribute distributions for precise control. Experiments on five real-world datasets show the efficacy of CGRAPHGEN compared to baselines, which we attribute to its use of adjacency matrix during training and effective integration of representations, which aligns graphs and their attributes in the latent space effectively and results in better control."
    },
    {
        "title": "Online Laplacian-Based Representation Learning in Reinforcement Learning",
        "link_suffix": "/forum?id=7xf50qWFGP",
        "link": "https://openreview.net/forum?id=7xf50qWFGP",
        "pdf_link": "https://openreview.net/pdf?id=7xf50qWFGP",
        "keywords": "Reinforcement Learning, Representation learning, Online Learning, Graph Laplacian",
        "abstract": "Representation learning plays a crucial role in reinforcement learning, especially in complex environments with high-dimensional and unstructured states. Effective representations can enhance the efficiency of learning algorithms by improving sample efficiency and generalization across tasks. This paper considers the Laplacian-based framework for representation learning, where the eigenvectors of the Laplacian matrix of the underlying transition graph are leveraged to encode meaningful features from raw sensory observations of the states. Despite the promising algorithmic advances in this framework, it remains an open question whether the Laplacian-based representations can be learned online and with theoretical guarantees along with policy learning. To answer this question, we study online Laplacian-based representation learning, where the graph-based representation is updated simultaneously while the policy is updated by the reinforcement learning algorithm. We design an online optimization formulation by introducing the Asymmetric Graph Drawing Objective (AGDO) and provide a theoretical analysis of the convergence of running online projected gradient descent on AGDO under mild assumptions. Specifically, we show that if the policy learning algorithm induces a bounded drift on the policy, running online projected gradient descent on AGDO exhibits ergodic convergence. Our extensive simulation studies empirically validate the guarantees of convergence to the true Laplacian representation. Furthermore, we provide insights into the compatibility of different reinforcement learning algorithms with online representation learning."
    },
    {
        "title": "DrugAgent: Multi-Agent Large Language Model-Based Reasoning for Drug-Target Interaction Prediction and Repurposing",
        "link_suffix": "/forum?id=PQrkWvQSL0",
        "link": "https://openreview.net/forum?id=PQrkWvQSL0",
        "pdf_link": "https://openreview.net/pdf?id=PQrkWvQSL0",
        "keywords": "Multi-agent, Drug-target interaction, drug-protein binding prediction, Large Language Models",
        "abstract": "Advancements in large language models (LLMs) allow them to address a wide set of questions from diverse topics using human-like text interfaces, but limitations in their training prevent them from answering accurately in scenarios that could benefit from multiple perspectives. Multi-agent systems allow the resolution of questions to enhance result consistency and reliability. Here we create a multi-perspective (i.e., unstructured text, structured knowledge graph, and Machine Learning (ML) prediction) multi-agent LLM system. We apply this system to the biologically inspired problem of predicting drug-target interaction. Our system uses a coordinator agent to assign and integrate results for tasks given to three specialized agents: an AI agent for ML predictions, a  knowledge graph (KG) agent for KG retrieval, and a search agent for web-based information retrieval.We conducted experiments using our LLM-based system for predicting drug-target interaction constants that reflect binding affinities using the BindingDB dataset. Our multi-agent LLM method significantly outperformed GPT-4 across multiple evaluation metrics by a significant margin. An ablation study revealed the contributions by each agent; ranked in terms of a contribution: the AI agent (i.e., ML prediction) was the most important followed by the KG agent then the search agent. The large contribution by the AI agent highlights the importance of LLM tool use in addressing questions that may not be part of text corpora. While our use case was related to biology, our presented architecture is applicable to other integrative prediction tasks. Code is availablehttps://anonymous.4open.science/r/DrugAgent-2BB7/"
    },
    {
        "title": "Provable Convergence Bounds for Hybrid Dynamical Sampling and Optimization",
        "link_suffix": "/forum?id=FJv8VMPxWi",
        "link": "https://openreview.net/forum?id=FJv8VMPxWi",
        "pdf_link": "https://openreview.net/pdf?id=FJv8VMPxWi",
        "keywords": "langevin, accelerators, sampling, optimization, diffusion, analog computing",
        "abstract": "Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference.In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms.\nAdapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance."
    },
    {
        "title": "Which Attention Heads Matter for In-Context Learning?",
        "link_suffix": "/forum?id=KadOFOsUpQ",
        "link": "https://openreview.net/forum?id=KadOFOsUpQ",
        "pdf_link": "https://openreview.net/pdf?id=KadOFOsUpQ",
        "keywords": "interpretability, in-context learning, large language models, mechanistic interpretability, induction heads",
        "abstract": "Large language models (LLMs) exhibit impressive in-context learning (ICL) capability, enabling them to generate relevant responses from a handful of task demonstrations in the prompt. \nPrior studies have suggested two different explanations for the mechanisms behind ICL:\ninduction heads that find and copy relevant tokens, and function vector (FV) heads whose activations compute a latent encoding of the ICL task.\nTo better understand which of the two distinct mechanisms drives ICL, we study induction heads and FV heads in 12 language models.Our study reveals that in all 12 models, few-shot ICL is driven primarily by FV heads: ablating FV heads decreases few-shot ICL accuracy significantly more than ablating induction heads, especially in larger models. We also find that FV and induction heads are connected: many FV heads\nstart as induction heads during training before transitioning to the FV mechanism. This leads us to speculate that induction heads facilitate the learning of the more complex FV mechanism for ICL. \nFinally, the prevalence of FV and induction heads varies with architecture, which questions strong versions of the\n\"universality\" hypothesis: findings from interpretability research are not always generalizable across models."
    },
    {
        "title": "CTSyn: A Foundational Model for Cross Tabular Data Generation",
        "link_suffix": "/forum?id=Sh4FOyZRpv",
        "link": "https://openreview.net/forum?id=Sh4FOyZRpv",
        "pdf_link": "https://openreview.net/pdf?id=Sh4FOyZRpv",
        "keywords": "Foundation Model, Tabular Data, Synthetic Data Generation",
        "abstract": "Generative Foundation Models (GFMs) have achieved remarkable success in producing high-quality synthetic data for images and text. However, their application to tabular data presents significant challenges due to the heterogeneous nature of table features. Current cross-table learning frameworks struggle with the absence of a generative model backbone and a mechanism to decode heterogeneous feature values. To address these challenges, we propose the Cross-Table Synthesizer (CTSyn), a diffusion-based foundational model for tabular data generation. CTSyn features two key components: an Autoencoder network that consolidates diverse tables into a unified latent space and dynamically reconstructs table values based on the provided table schema embedding, adapting to heterogeneous datasets; and a conditional latent diffusion model that samples from this learned latent space. Through large-scale pre-training, CTSyn not only outperforms existing table synthesizers on standard tabular data generation benchmarks in terms of utility and diversity, but also uniquely enhances the performance of downstream machine learning tasks, surpassing what is achievable with real data. This establishes CTSyn as a new paradigm for synthetic table generation."
    },
    {
        "title": "Learning from others' mistakes: Finetuning machine translation models with span-level error annotations",
        "link_suffix": "/forum?id=204sPiwBbB",
        "link": "https://openreview.net/forum?id=204sPiwBbB",
        "pdf_link": "https://openreview.net/pdf?id=204sPiwBbB",
        "keywords": "machine translation, finetuning, fine-grained annotations, language model",
        "abstract": "Despite growing interest in incorporating feedback to improve language models, most efforts focus only on sequence-level annotations. In this work, we explore the potential of utilizing fine-grained span-level annotations from offline datasets to improve model quality. We develop a simple finetuning algorithm, called Training with Annotations (TWA), to directly train machine translation models on such annotated data. TWA utilizes targeted span-level error information while also flexibly learning what to penalize within a span. Moreover, TWA considers the overall trajectory of a sequence when deciding which non-error spans to utilize as positive signals. Experiments on English-German and Chinese-English machine translation show that TWA outperforms baselines such as Supervised Finetuning on sequences filtered for quality and Direct Preference Optimization on pairs constructed from the same data."
    },
    {
        "title": "Solving hidden monotone variational inequalities with surrogate losses",
        "link_suffix": "/forum?id=4ZX2a3OKEV",
        "link": "https://openreview.net/forum?id=4ZX2a3OKEV",
        "pdf_link": "https://openreview.net/pdf?id=4ZX2a3OKEV",
        "keywords": "Variational Inequality, Optimization, Surrogate, Projected Bellman Error, Min-max Optimization",
        "abstract": "Deep learning has proven to be effective in a wide variety of loss minimization problems.\nHowever, many applications of interest, like minimizing projected Bellman error and min-max optimization, cannot be modelled as minimizing a scalar loss function but instead correspond to solving a variational inequality (VI) problem.\nThis difference in setting has caused many practical challenges as naive gradient-based approaches from supervised learning tend to diverge and cycle in the VI case.\nIn this work, we propose a principled surrogate-based approach compatible with deep learning to solve VIs.\nWe show that our surrogate-based approach has three main benefits: (1) under assumptions that are realistic in practice (when hidden monotone structure is present, interpolation, and sufficient optimization of the surrogates), it guarantees convergence, (2) it provides a unifying perspective of existing methods, and (3) is amenable to existing deep learning optimizers like ADAM.\nExperimentally, we demonstrate our surrogate-based approach is effective in min-max optimization and minimizing projected Bellman error. Furthermore, in the deep reinforcement learning case, we propose a novel variant of TD(0) which is more compute and sample efficient."
    },
    {
        "title": "Stochastic Process Learning via Operator Flow Matching",
        "link_suffix": "/forum?id=hShwhoMRVk",
        "link": "https://openreview.net/forum?id=hShwhoMRVk",
        "pdf_link": "https://openreview.net/pdf?id=hShwhoMRVk",
        "keywords": "Neural Operator, Flow Matching, Stochastic Process, Uncertainty Quantification",
        "abstract": "Using neural operators, we propose a novel framework for stochastic process learning across arbitrary domains. In particular, we develop operator flow matching for learning stochastic process priors on function spaces. Operator flow matching provides the probability density of any finite collection of points, and enables mathematically tractable functional regression at new points with mean and density estimation. Our method outperforms state of the art models at stochastic process learning, functional regression, and prior learning."
    },
    {
        "title": "Long Tail Classification  Through Cost Sensitive Loss Functions",
        "link_suffix": "/forum?id=RwiUmrEHgR",
        "link": "https://openreview.net/forum?id=RwiUmrEHgR",
        "pdf_link": "https://openreview.net/pdf?id=RwiUmrEHgR",
        "keywords": "Long Tail, Imbalanced Data, Cost-sensitive Loss",
        "abstract": "Class imbalance in the data introduces significant challenges in training machine models especially with long-tailed datasets. Specifically, it leads to biased models that overfit with respect to the dominant classes while under-performing on the minority classes. This, in turn,  results in seemingly satisfactory yet biased overall results. Hence, the above biasing needs to be controlled such that the desired generalizability of the model is not entirely compromised. To that end,  we introduce a novel Cost-Sensitive Loss (CSL) function designed to dynamically adjust class weights, and incorporate a reinforcement learning mechanism to optimize these adjustments. The proposed CSL function can be seamlessly integrated with existing loss functions, to enhance performance on imbalanced datasets, rendering them robust and scalable. We implemented the above CSL function in form of a framework which leverages reinforcement learning  to optimally apply these adjustments over consecutive training epochs.  Experimental Results on  benchmark datasets demonstrate that our proposed approach significantly outperforms state-of-the-art methods. The results indicate that our approach can  provide an optimal trade-off in the model accuracy and generalization with diverse kinds of imbalanced data."
    },
    {
        "title": "Investigating Factuality in Long-Form Text Generation: The Roles of Self-Known and Self-Unknown",
        "link_suffix": "/forum?id=qLxkXgmWwx",
        "link": "https://openreview.net/forum?id=qLxkXgmWwx",
        "pdf_link": "https://openreview.net/pdf?id=qLxkXgmWwx",
        "keywords": "long-form generation, Factuality",
        "abstract": "Large language models (LLMs) have demonstrated strong capabilities in text understanding and generation. However, they often lack factuality, producing a mixture of true and false information, especially in long-form generation. In this work, we investigates the factuality of long-form text generation across various large language models (LLMs), including GPT-4, Gemini-1.5-Pro, Claude-3-Opus, Llama-3-70B, Mixtral, and Mistral. Our analysis reveals that factuality scores tend to decline in later sentences of the generated text, accompanied by a rise in the number of unsupported claims.\nFurthermore, we explore the effectiveness of different evaluation settings to assess whether LLMs can accurately judge the correctness of their own outputs: Self-Known (the percentage of supported atomic claims, decomposed from LLM outputs, that the corresponding LLMs judge as correct) and Self-Unknown (the percentage of unsupported atomic claims that the corresponding LLMs judge as incorrect). The results indicate that even advanced models like GPT-4 and Gemini-1.5-Pro fail to achieve perfect Self-Known scores, while their Self-Unknown scores remain notably above zero, reflecting ongoing uncertainty in their self-assessments.\nMoreover, we find a correlation between higher Self-Known scores and improved factuality, while higher Self-Unknown scores are associated with lower factuality. Interestingly, even without significant changes in the models' self-judgment (Self-Known and Self-Unknown), the number of unsupported claims can increases, likely as an artifact of long-form generation. These findings show the limitations of current LLMs in long-form generation, and provide valuable insights for improving factuality in long-form text generation."
    },
    {
        "title": "Win Rate is All that Can Matter from Preference Data Alone",
        "link_suffix": "/forum?id=OxxbqZBJxx",
        "link": "https://openreview.net/forum?id=OxxbqZBJxx",
        "pdf_link": "https://openreview.net/pdf?id=OxxbqZBJxx",
        "keywords": "alignment, preference learning, RLHF, win rate, language model",
        "abstract": "The surging interest in learning from preference data has resulted in an elaborate landscape of methods and evaluations. This work offers a framework to simplify this landscape. We start with the insight that the only fixed information represented in preference data is the preference classifier, and thus the only evaluation of a model grounded in the data is win rate under this classifier. In other words, win rate is all that can matter from preference data alone. This insight allows us to unlock many follow-up insights. First, we introduce a family of objectives to directly optimize for win rate, called Direct Win Rate Optimization (DWRO) objectives. We show that Reinforcement Learning From Human Feedback (RLHF) is a KL-regularized DWRO objective while SFT on preferred samples is not. We then compare the target distributions of various preference learning objectives and explain how different design choices affect sharpness of the resulting distribution. Furthermore, we provide close-formed solutions for the expected win rate improvement of common preference learning algorithms and explain the intuitions they provide. Our analysis and accompanying experiments not only elucidate the design space of preference learning algorithms but also offer guidance on future directions to advance preference learning."
    },
    {
        "title": "LegoScale: One-stop PyTorch native solution for production ready LLM pre-training",
        "link_suffix": "/forum?id=SFN6Wm7YBI",
        "link": "https://openreview.net/forum?id=SFN6Wm7YBI",
        "pdf_link": "https://openreview.net/pdf?id=SFN6Wm7YBI",
        "keywords": "large language models, distributed training, pre-training, data parallel, tensor parallel, pipeline parallel, pytorch, llama, distributed checkpointing, 3D parallel",
        "abstract": "The development of large language models (LLMs) has been instrumental in advancing state-of-the-art natural language processing applications. Training LLMs with billions of parameters and trillions of tokens requires sophisticated distributed systems that enable composing and comparing several state-of-the-art techniques in order to efficiently scale across thousands of accelerators. However, existing solutions are complex, scattered across multiple libraries/repositories, lack interoperability, and are cumbersome to maintain. Thus, curating and empirically comparing training recipes require non-trivial engineering effort.This paper introduces LEGOSCALE, an open-source, PyTorch-native distributed training system that unifies and advances state-of-the-art techniques, streamlining integration and reducing engineering overhead. LEGOSCALE enables seamless application of 3D parallelism in a modular and composable manner, while featuring elastic scaling to adapt to changing computational requirements. The system provides comprehensive logging, efficient checkpointing, and debugging tools, ensuring production-ready training. Moreover, LEGOSCALE incorporates innovative hardware-software co-designed solutions, leveraging cutting-edge features like Float8 training and SymmetricMemory to maximize hardware utilization. As a flexible experimental test bed, LEGOSCALE facilitates the curation and comparison of custom recipes for diverse training contexts. By leveraging LEGOSCALE, we developed optimized training recipes for the Llama 3.1 family and provide actionable guidance on selecting and combining distributed training techniques to maximize training efficiency, based on our hands-on experiences.We thoroughly assess LEGOSCALE on the Llama 3.1 family of LLMs, spanning 8 billion to 405 billion parameters, and showcase its exceptional performance, modular composability, and elastic scalability. By stacking the training optimizations, we demonstrate accelerations ranging from 65.08% on Llama3-8B at 128 GPU scale (1D), 13% on Llama3-70B at 256 GPU scale (2D), to 30% on Llama3-405B at 512 GPU scale (3D) on NVIDIA H100 GPUs over optimized baselines."
    },
    {
        "title": "Reducing Symmetry Mismatch Caused by Freely Placed Cameras in Robotic Learning",
        "link_suffix": "/forum?id=2LHzKdb8Ao",
        "link": "https://openreview.net/forum?id=2LHzKdb8Ao",
        "pdf_link": "https://openreview.net/pdf?id=2LHzKdb8Ao",
        "keywords": "Equivariance, Robotics",
        "abstract": "Equivariant policy learning has been shown to solve robotic manipulation tasks with minimal training or demonstration data.  However, the effectiveness of equivariance depends on whether transformations of the scene align with simple transformations of the input data. This is true when the camera is in a top-down view, but in the common case where a camera views the robot workspace from the side, there is a symmetry mismatch, reducing model performance. We show that equivariant methods perform better when camera images are transformed to appear as top-down images.  Our approach is simple to implement, works for RGB and RGBD images, and reliably improves performance across different view angles and learning algorithms."
    },
    {
        "title": "On Gradient-Weight Alignment",
        "link_suffix": "/forum?id=nT89RltY10",
        "link": "https://openreview.net/forum?id=nT89RltY10",
        "pdf_link": "https://openreview.net/pdf?id=nT89RltY10",
        "keywords": "generalization, gradients, alignment, influence, memorization",
        "abstract": "Evaluating the performance of deep networks against unseen validation data is a crucial step to measure generalization performance.\nHowever, ostensibly neither the training nor validation and test data are ever sufficiently extensive to replicate real-world application.\nThis works advocates for a change of perspective for evaluating performance of deep networks.\nInstead of evaluating against unseen validation data, we propose to rather capture when the model starts to prioritize learning unnecessary or even detrimental specifics of training data instead of general patterns. \nWhile this has been challenging to theoretically derive, we proposegradient-weight alignmentas an empirical metric to determine performance on unseen data from training information alone.\nOur performance measure is efficient and widely applicable, closely tracking validation accuracy during training.\nIt connects model performance to individual training samples, enabling its use not only for assessing generalization and as an early stopping criterion, but also for offering insights into training dynamics."
    },
    {
        "title": "A Little Depth Goes a Long Way: the Expressive Power of Log-Depth Transformers",
        "link_suffix": "/forum?id=zDze7VtB5C",
        "link": "https://openreview.net/forum?id=zDze7VtB5C",
        "pdf_link": "https://openreview.net/pdf?id=zDze7VtB5C",
        "keywords": "transformer, expressivity, limits, bounded context, circuits",
        "abstract": "Most analysis of transformer expressivity treats the depth (number of layers) of a model as a fixed constant, and analyzes the kinds of problems such models can solve across inputs of unbounded length. In practice, however, the context length of a trained transformer model is bounded. Thus, a more pragmatic question is:What kinds of computation can a transformer perform on inputs of bounded length?We formalize this by studying highly uniform transformers where the depth can grow minimally with context length. In this regime, we show that transformers with depth $O(\\log C)$ can, in fact, compute solutions to two important problems for inputs bounded by some max context length $C$, namelysimulating finite automata, which relates to the ability to track state, andgraph connectivity, which underlies multi-step reasoning. Notably, both of these problems have previously been proven to be asymptotically beyond the reach of fixed depth transformers under standard complexity conjectures, yet empirically transformer models can successfully track state and perform multi-hop reasoning on short contexts. Our novel analysis thus explains how transformer models may rely on depth to feasibly solve problems up to bounded context that they cannot solve over long contexts. It makes actionable suggestions for practitioners as to how to minimally scale the depth of a transformer to support reasoning over long contexts, and also argues for dynamically unrolling depth as a more effective way of adding compute compared to increasing model dimension or adding a short chain of thought."
    },
    {
        "title": "Chunk-Distilled Language Modeling",
        "link_suffix": "/forum?id=nrvoWOWcyg",
        "link": "https://openreview.net/forum?id=nrvoWOWcyg",
        "pdf_link": "https://openreview.net/pdf?id=nrvoWOWcyg",
        "keywords": "language modeling, text generation, retrieval-augmented generation, domain adaptation, inference algorithms, efficient generation",
        "abstract": "We introduce Chunk-Distilled Language Modeling (CD-LM), an approach to text generation that addresses two challenges in current large language models (LLMs): the inefficiency of token-level generation, and the difficulty of adapting to new data and knowledge. Our method combines deep network-based LLMs with a straightforward retrieval module, which allows the generation of multi-token text chunks at a single decoding step. Our retrieval framework enables flexible construction of model- or domain-specific datastores, either leveraging the internal knowledge of existing models, or incorporating expert insights from human-annotated corpora. This adaptability allows for enhanced control over the language model's distribution without necessitating additional training. We present the CD-LM formulation along with performance metrics demonstrating its ability to improve language model performance and efficiency across a diverse set of downstream tasks. Code and data will be made publicly available."
    },
    {
        "title": "When does compositional structure yield compositional generalization? A kernel theory.",
        "link_suffix": "/forum?id=FPBce2P1er",
        "link": "https://openreview.net/forum?id=FPBce2P1er",
        "pdf_link": "https://openreview.net/pdf?id=FPBce2P1er",
        "keywords": "compositional generalization, rule learning, kernel regression, kernel models, relational reasoning, memorization, shortcuts, dataset statistics, norm minimization, implicit regularization, disentanglement",
        "abstract": "Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations are essential for this; however, the conditions under which they yield compositional generalization remain unclear. To address this gap, we present a general theory of compositional generalization in kernel models with fixed representations, a tractable framework for characterizing the impact of dataset statistics on generalization. We find that kernel models are constrained to adding up values assigned to each combination of components seen during training (\"conjunction-wise additivity\"). This imposes fundamental restrictions on the set of tasks these models can learn, in particular preventing them from transitively generalizing equivalence relations. Even for compositional tasks that kernel models can in principle learn, we identify novel failure modes in compositional generalization that arise from biases in the training data and affect important compositional building blocks such as symbolic addition and context dependence (memorization leak and shortcut bias). Finally, we empirically validate our theory, showing that it captures the behavior of deep neural networks (convolutional networks, residual networks, and Vision Transformers) trained on a set of compositional tasks with similarly structured data. Ultimately, this work provides a theoretical perspective on how statistical structure in the training data can affect compositional generalization, with implications for how to identify and remedy failure modes in deep learning models."
    },
    {
        "title": "How To Evaluate Your Medical Time Series Classification?",
        "link_suffix": "/forum?id=TUVaDGuXrK",
        "link": "https://openreview.net/forum?id=TUVaDGuXrK",
        "pdf_link": "https://openreview.net/pdf?id=TUVaDGuXrK",
        "keywords": "Medical Time Series, Subject-Independent, Classification Evaluation, Healthcare",
        "abstract": "Medical time series (MedTS) play a critical role in many healthcare applications, such as vital sign monitoring and the diagnosis of brain and heart diseases. However, the existence of subject-specific features poses unique challenges in MedTS evaluation. Inappropriate evaluation setups that either exploit or overlook these features can lead to artificially inflated classification performance (by up to 50% in accuracy; ADFTD): this concern has received little attention in current research. Here, we categorize the existing evaluation setups into two primary categories: subject-dependent and subject-independent. We show the subject-independent setup is more appropriate for different datasets and tasks. Our theoretical analysis explores the feature components of MedTS, examining how different evaluation setups influence the features that a model learns. Through experiments on six datasets (spanning EEG, ECG, and fNIRS modalities) using four different methods, we demonstrate step-by-step how subject-dependent utilizes subject-specific features as a shortcut for classification and leads to a deceptive high performance, suggesting that the subject-independent setup is more precise and practicable evaluation setup in real-world. This comprehensive analysis aims to establish clearer guidelines for evaluating MedTS models in different healthcare applications. Code to reproduce this work inhttps://anonymous.4open.science/r/MedTS_Evaluation-733F."
    },
    {
        "title": "BadJudge: Backdoor Vulnerabilities of LLM-As-A-Judge",
        "link_suffix": "/forum?id=eC2a2IndIt",
        "link": "https://openreview.net/forum?id=eC2a2IndIt",
        "pdf_link": "https://openreview.net/pdf?id=eC2a2IndIt",
        "keywords": "LLM-as-a-Judge, LLM Evaluator, Backdoor Attack, Backdoor Defense",
        "abstract": "Recently, LLMs are being used to evaluate free-form language generation, in a increasingly popular paradigm called LLM-as-a-Judge. While the ratings of these judges achieve SOTA correlation with human preferences on LLM generation, acquiring data to train these models is often community-driven and open-source, inadvertently creating opportunities for malicious actors to compromise the eval- uation pipeline. Current research predominantly focuses on de-biasing LLM evaluators, improving robustness to spurious correlations. However, they overlook potential threats from adversaries. This paper exposes a devastating attack on LLM evaluators: the backdoor, where an adversary inserts a predefined trigger-target pair into a model’s training set and activates it during test time to control\nthe model’s decision. Results elucidate how 1 extra token in 1% of the evaluator training corpus can inflate the adversary model’s score by over 3 times. However, (malicious) human annotators typically lack access to the entire training dataset. As such, experiments evidence how score inflation severity correlates with data access. The most severe setting, achieves an inflated 4.9/5 rating,\ndespite scoring 1.5/5 on legitimate evaluation. Experiments across 2 preference models (point-wise and pair-wise), 3 model families, and 3 triggers evince the generalizability of this attack. Disconcertingly, case studies on real-world systems indicate LlaMA-3.1-Guard, LMSYS Chatbot Arena, and list-wise reranking evaluators in RAG are all susceptible to attack. Moreover, defending evaluators presents a new challenge, with many exploitable components, e.g. score rubric. Likewise, falsely editing the input may shift scores, as LLM evaluation hinges upon both semantic and stylistic features, constraining the defense search space. Our results reinforce this, indicating that many canonical defense strategies, including ONION and BKI are ineffective. Fortunately, a straightforward defensive tool—the model merge—demonstrates exceptional efficacy, reducing the Attack Success Rate (ASR) by 93% on even the most severe levels of data access. As\na pioneering work in this domain, we release our code and data to ensure reproducibility and to foster further research in this critical direction."
    },
    {
        "title": "System Aware Unlearning Algorithms: Use Lesser, Forget Faster",
        "link_suffix": "/forum?id=dYTjB86pcT",
        "link": "https://openreview.net/forum?id=dYTjB86pcT",
        "pdf_link": "https://openreview.net/pdf?id=dYTjB86pcT",
        "keywords": "machine unlearning, learning theory, selective sampling for unlearning",
        "abstract": "Machine unlearning aims to provide privacy guarantees to users when they request deletion, such that an attacker who can compromise the system post-unlearning cannot recover private information about the deleted individuals. Previously proposed definitions of unlearning require the unlearning algorithm to exactly or approximately recover the hypothesis obtained by retraining-from-scratch on the remaining samples. While this definition has been the gold standard in machine unlearning, unfortunately, because it is designed for the worst-case attacker (that can recover the updated hypothesis and the remaining dataset),  developing rigorous, and memory or compute-efficient unlearning algorithms that satisfy this definition has been challenging. In this work, we propose a new definition of unlearning, called system aware unlearning, that takes into account the information that an attacker could recover by compromising the system (post-unlearning). We prove that system-aware unlearning generalizes commonly referred to definitions of unlearning by restricting what the attacker knows, and furthermore, may be easier to satisfy in scenarios where the system-information available to the attacker is limited, e.g. because the learning algorithm did not use the entire training dataset to begin with. Towards that end, we develop an exact system-aware-unlearning algorithm that is both memory and computation-time efficient for function classes that can be learned via sample compression. We then present an improvement over this for the special case of learning linear classifiers by using selective sampling for data compression, thus giving the first memory and time-efficient exact unlearning algorithm for linear classification. We analyze the tradeoffs between deletion capacity, accuracy, memory, and computation time for these algorithms."
    },
    {
        "title": "Separate the  Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation",
        "link_suffix": "/forum?id=OnMRWwOqCs",
        "link": "https://openreview.net/forum?id=OnMRWwOqCs",
        "pdf_link": "https://openreview.net/pdf?id=OnMRWwOqCs",
        "keywords": "Retrieval Augmented Generation, Large Language Modles",
        "abstract": "Retrieval-augmented generation (RAG) addresses the limitation of large language models (LLMs) in achieving up-to-date information by integrating external knowledge sources, but it is hindered by noisy or irrelevant retrieved data, leading to reduced accuracy. Additionally, most RAG methods rely on task-specific supervision, reducing their adaptability across domains. \nTo overcome these challenges, we propose WinnowRAG, a novel multi-agent debate-based RAG framework. WinnowRAG operates in two stages: in Stage I, query-aware clustering groups similar documents, with each cluster assigned to an LLM agent for generating personalized responses. A critic LLM then consolidates these answers, forming super-agents. In Stage II, the super-agents engage in a structured discussion to filter out incorrect or irrelevant information, ensuring only relevant knowledge is used for final response generation. Crucially, WinnowRAG is unsupervised and leverages pretrained LLMs without requiring fine-tuning, making it easily adaptable to various tasks. The experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines."
    },
    {
        "title": "WaveletGPT: Wavelet Inspired LLMs",
        "link_suffix": "/forum?id=mk8eLjKCdB",
        "link": "https://openreview.net/forum?id=mk8eLjKCdB",
        "pdf_link": "https://openreview.net/pdf?id=mk8eLjKCdB",
        "keywords": "Wavelets, GPT, LLM",
        "abstract": "Large Language Models (LLMs) have ushered in a new wave of artificial intelligence advancements impacting every scientific field and discipline. We live in a world where most of the data around us, e.g., text, audio, and music, has a multi-scale structure associated with it. This paper infuses LLMs with a traditional signal processing idea, namely wavelets, during pre-training to take advantage of the structure. Without adding \\textbf{any extra parameters} to a GPT-style LLM architecture in academic setup, we achieve the same pre-training performance almost twice as fast in text, raw audio, and symbolic music. This is achieved by imposing a structure on intermediate embeddings. When trained for the same number of training steps, we achieve significant gains in performance, which is comparable to pre-training a larger neural architecture. Our architecture allows every next token prediction access to intermediate embeddings at different temporal resolutions in every Transformer decoder block. This work will hopefully pave the way for incorporating multi-rate signal processing ideas into traditional LLM pre-training. Further, we showcase pushing model performance by improving internal structure instead of just going after scale."
    }
]