[
    {
        "title": "Dynamic Model Editing to Rectify Unreliable Behavior in Neural Networks",
        "link_suffix": "/forum?id=1dkL3MVBfV",
        "link": "https://openreview.net/forum?id=1dkL3MVBfV",
        "pdf_link": "https://openreview.net/pdf?id=1dkL3MVBfV",
        "keywords": "model vulnerability, model editing, feature attribution",
        "abstract": "The performance of neural network models degrades with data shifts. Owing to their opaque nature, rectifying models to address this problem often necessitates arduous data cleaning and model retraining, resulting in huge computational and manual overhead. This motivates the development of efficient methods for rectifying models. In this work, we propose leveraging rank-one model editing to correct model's unreliable behavior on corrupted input samples and align it with that on cleansed samples. We introduce an attribution-based method for locating the primary layer responsible for the model's misbehavior and integrate this layer localization technique into a dynamic model editing approach, enabling dynamic adjustment of the model behavior  during the editing process. Through extensive experiments, the proposed method is demonstrated to be effective in correcting model's misbehavior observed for neural Trojans and spurious correlations. Our approach demonstrates remarkable performance by achieving its editing objective with as few as a single cleansed sample, which makes it appealing for practice."
    },
    {
        "title": "Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization",
        "link_suffix": "/forum?id=c4wEKJOjY3",
        "link": "https://openreview.net/forum?id=c4wEKJOjY3",
        "pdf_link": "https://openreview.net/pdf?id=c4wEKJOjY3",
        "keywords": "Out-of-distribution generalization, total variation, invariant risk minimization",
        "abstract": "Invariant risk minimization is an important general machine learning framework that has recently been interpreted as a total variation model (IRM-TV). However, how to improve out-of-distribution (OOD) generalization in the IRM-TV setting remains unsolved. In this paper, we propose a novel OOD generalization approach for IRM-TV, named OOD-TV-IRM, based on its theoretical analysis. The key idea is to deploy an autonomous TV penalty that depends on the invariant feature extractor. We construct the autonomous TV penalty using a neural network with another set of parameters, which can be learned via an adversarial scheme against the parameters of the invariant feature extractor. Experimental results show that OOD-TV-IRM outperforms IRM-TV in most situations."
    },
    {
        "title": "Understanding and Mitigating Memorization in Diffusion Models for Tabular Data",
        "link_suffix": "/forum?id=wT1aFmsXOc",
        "link": "https://openreview.net/forum?id=wT1aFmsXOc",
        "pdf_link": "https://openreview.net/pdf?id=wT1aFmsXOc",
        "keywords": "Memorization, Tabular Data, Diffusion Models",
        "abstract": "Tabular data generation has attracted significant research interest in recent years, with the tabular diffusion models greatly improving the quality of synthetic data. However, while memorization\u2014where models inadvertently replicate exact or near-identical training data\u2014has been thoroughly investigated in image and text generation, its effects on tabular data remain largely unexplored. In this paper, we conduct the first comprehensive investigation of memorization phenomena in diffusion models for tabular data. Our empirical analysis reveals that memorization appears in tabular diffusion models and increases with larger training epochs. We further examine the influence of factors such as dataset sizes, feature dimensions, and different diffusion models on memorization. Additionally, we provide a theoretical explanation for why memorization occurs in tabular diffusion models. To address this issue, we propose TabCutMix, a simple yet effective data augmentation technique that exchanges randomly selected feature segments between random training sample pairs. Experimental results across various datasets and diffusion models demonstrate that TabCutMix effectively mitigates memorization while maintaining high-quality data generation. Our code is available at \\url{https://anonymous.4open.science/r/TabCutMix-3F7B}."
    },
    {
        "title": "Tree Search for Language Model Agents",
        "link_suffix": "/forum?id=kpL66Mvd2a",
        "link": "https://openreview.net/forum?id=kpL66Mvd2a",
        "pdf_link": "https://openreview.net/pdf?id=kpL66Mvd2a",
        "keywords": "agents, web navigation, multimodal models",
        "abstract": "Autonomous agents powered by language models (LMs) have demonstrated promise in their ability to perform decision-making tasks such as web automation. However, a key limitation remains: LMs, primarily optimized for natural language understanding and generation, struggle with multi-step reasoning, planning, and using environmental feedback when attempting to solve realistic computer tasks. \nTowards addressing this, we propose an inference-time search algorithm for LM agents to explicitly perform exploration and multi-step planning in interactive web environments. \nOur approach is a form of best-first tree search that operates within the actual environment space, and is complementary with most existing state-of-the-art agents. \nIt is the first tree search algorithm for LM agents that shows effectiveness on realistic web tasks. \nOn the challenging VisualWebArena benchmark, applying our search algorithm on top of a  GPT-4o agent yields a 39.7% relative increase in success rate compared to the same baseline without search, setting a state-of-the-art success rate of 26.4%. On WebArena, search also yields a 28.0% relative improvement over a baseline agent, setting a competitive success rate of 19.2%. \nOur experiments highlight the effectiveness of search for web agents, and we demonstrate that performance scales with increased test-time compute.\nWe conduct a thorough analysis of our results to highlight improvements from search, limitations, and promising directions for future work."
    },
    {
        "title": "Rethinking Homogeneity of Vision and Text Tokens in Large Vision-and-Language Models",
        "link_suffix": "/forum?id=jmsRo2ScoI",
        "link": "https://openreview.net/forum?id=jmsRo2ScoI",
        "pdf_link": "https://openreview.net/pdf?id=jmsRo2ScoI",
        "keywords": "LVLM, LMM, multimodal",
        "abstract": "Large vision-and-language models (LVLMs) typically treat visual and textual embeddings as homogeneous inputs to a large language model (LLM). However, these inputs are inherently different: visual inputs are multi-dimensional and contextually rich, often pre-encoded by models like CLIP, while textual inputs lack this structure. In this paper, we propose Decomposed Attention (D-Attn), a novel method that processes visual and textual embeddings differently by decomposing the 1-D causal self-attention in LVLMs. After the attention decomposition, D-Attn diagonalizes visual-to-visual self-attention, reducing computation from $\\mathcal{O}(|V|^2)$ to $\\mathcal{O}(|V|)$ for $|V|$ visual embeddings without compromising performance. Moreover, D-Attn debiases positional encodings in textual-to-visual cross-attention, further enhancing visual understanding. Finally, we introduce an $\\alpha$-weighting strategy to merge visual and textual information, maximally preserving the pre-trained LLM\u2019s capabilities with minimal modifications. Extensive experiments and rigorous analyses validate the effectiveness of D-Attn, demonstrating significant improvements on multiple image benchmarks while significantly reducing computational costs. Code, data, and models will be publicly available."
    },
    {
        "title": "Multi-Reward as Condition for Instruction-based Image Editing",
        "link_suffix": "/forum?id=9RFocgIccP",
        "link": "https://openreview.net/forum?id=9RFocgIccP",
        "pdf_link": "https://openreview.net/pdf?id=9RFocgIccP",
        "keywords": "Instruction-based image editing",
        "abstract": "High-quality training triplets (instruction, original image, edited image) are essential for instruction-based image editing. Predominant training datasets (e.g., InsPix2Pix) are created using text-to-image generative models (e.g., Stable Diffusion, DALL-E) which are not trained for image editing. Accordingly, these datasets suffer from inaccurate instruction following, poor detail preserving, and generation artifacts. In this paper, we propose to address the training data quality issue with multi-perspective reward data instead of refining the ground-truth image quality. 1) we first design a quantitative metric system based on best-in-class LVLM (Large Vision Language Model), i.e., GPT-4o in our case, to evaluate the generation quality from 3 perspectives, namely, instruction following, detail preserving, and generation quality. For each perspective, we collected quantitative score in $0\\sim 5$ and text descriptive feedback on the specific failure points in ground-truth edited images, resulting in a high-quality editing reward dataset, i.e., RewardEdit20K. 2) We further proposed a novel training framework to seamlessly integrate the metric output, regarded as multi-reward, into editing models to learn from the imperfect training triplets. During training, the reward scores and text descriptions are encoded as embeddings and fed into both the latent space and the U-Net of the editing models as auxiliary conditions. During inference, we set these additional conditions to the highest score with no text description for failure points, to aim at the best generation outcome. 3) We also build a challenging evaluation benchmark with real-world images/photos and diverse editing instructions, named as Real-Edit. Experiments indicate that our multi-reward conditioned model outperforms its no-reward counterpart on two popular editing pipelines, i.e., InsPix2Pix and SmartEdit. The code and dataset will be released."
    },
    {
        "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
        "link_suffix": "/forum?id=A1ztozypga",
        "link": "https://openreview.net/forum?id=A1ztozypga",
        "pdf_link": "https://openreview.net/pdf?id=A1ztozypga",
        "keywords": "hybrid model, language model",
        "abstract": "The transformative capabilities of language models (LMs) have intensified the demand for their deployment on everyday devices, necessitating efficient processing for on-device language tasks. To address this, we propose Hymba, a new family of small language models featuring a hybrid-head architecture that strategically integrates attention mechanisms with state space models (SSMs). This architecture leverages the strengths of both systems: attention heads provide high-resolution recall, akin to snapshot memories in the human brain, while SSM heads offer efficient context summarization, similar to fading memories. To further enhance Hymba's performance, we introduce learnable meta tokens that are prepended to input sequences and jointly trained with model weights during pretraining. These meta tokens act as a learned cache initialization during inference, modulating all subsequent tokens within the hybrid heads and boosting the model\u2019s focus on salient information, similar to metamemory. Extensive experiments and ablation studies demonstrate that Hymba sets new state-of-the-art results for small LMs across various benchmarks and advances the accuracy-efficiency trade-offs of small LMs. For instance, Hymba-1.5B achieves comparable commonsense reasoning accuracy to LLaMA 3.2 3B while being 3.49x faster and offering a 14.72x reduction in cache size. All codes and models will be released upon acceptance."
    },
    {
        "title": "SSIF: Physics-Inspired Implicit Representations for Spatial-Spectral Image Super-Resolution",
        "link_suffix": "/forum?id=zrNbsV87Os",
        "link": "https://openreview.net/forum?id=zrNbsV87Os",
        "pdf_link": "https://openreview.net/pdf?id=zrNbsV87Os",
        "keywords": "Neural Implicit Function, Spatial-Spectral Super Resolution, Spectral Encoding",
        "abstract": "Existing digital sensors capture images at fixed spatial and spectral resolutions (e.g., RGB, multispectral, and hyperspectral images), and generating super-resolution images with different resolution settings requires bespoke machine learning models. Spatial Implicit Functions (SIFs) partially overcome the spatial resolution challenge by representing an image in a spatial-resolution-independent way. However, they\nstill operate at fixed, pre-defined spectral resolutions. To address this challenge, we propose Spatial-Spectral Implicit Function (SSIF), a neural implicit model that represents an image as a function of both continuous pixel coordinates in the spatial domain and continuous wavelengths in the spectral domain. This continuous representation across spatial and spectral domains enables a single model to learn from a diverse set of resolution settings, which leads to better generalizability. This representation also allows the physical principle of spectral imaging and the spectral response functions of sensors to be easily incorporated during training and inference. Moreover, SSIF does not have the equal spectral wavelength interval requirement for both input and output images which leads to much better applicability. We empirically demonstrate the effectiveness of SSIF on two challenging spatial-spectral super-resolution benchmarks. We observe that SSIF consistently outperforms state-of-the-art baselines even when the baselines are allowed to train separate models at each spatial or spectral resolution. We show that SSIF generalizes well to both unseen spatial and spectral resolutions. Moreover, due to its physics-inspired design, SSIF performs significantly better at low data regime and converges faster during training compared with other strong neural implicit function-based baselines."
    },
    {
        "title": "Online Bandit Nonlinear Control with Dynamic Batch Length and Adaptive Learning Rate",
        "link_suffix": "/forum?id=5oRB2Wgwtb",
        "link": "https://openreview.net/forum?id=5oRB2Wgwtb",
        "pdf_link": "https://openreview.net/pdf?id=5oRB2Wgwtb",
        "keywords": "Online nonlinear control, Bandits, Dynamic batch length, Adaptive learning rate",
        "abstract": "This paper is concerned with the online bandit nonlinear control, which aims to learn the best stabilizing controller from a pool of stabilizing and destabilizing controllers of unknown types for a given nonlinear dynamical system. We develop an algorithm, named Dynamic Batch length and Adaptive learning Rate (DBAR), and study its stability and regret. Unlike the existing Exp3 algorithm requiring an exponentially stabilizing controller, DBAR only needs a significantly weaker notion of controller stability, in which case substantial time may be required to certify the system stability. Dynamic batch length in DBAR effectively addresses this issue and enables the system to attain asymptotic stability, where the algorithm behaves as if there were no destabilizing controllers. Moreover, adaptive learning rate in DBAR only uses the state norm information to achieve a tight regret bound even when none of the stabilizing controllers in the pool are exponentially stabilizing."
    },
    {
        "title": "Physics-Informed Neural Networks with Trust-Region Sequential Quadratic Programming",
        "link_suffix": "/forum?id=GkJCgUmIqA",
        "link": "https://openreview.net/forum?id=GkJCgUmIqA",
        "pdf_link": "https://openreview.net/pdf?id=GkJCgUmIqA",
        "keywords": "Scientific Machine Learning; Physics-Informed Neural Networks; Hard-Constrained Methods; Trust Region; Sequential Quadratic Programming",
        "abstract": "Physics-Informed Neural Networks (PINNs) represent a significant advancement in Scientific Machine Learning (SciML), which integrate physical domain knowledge into an empirical loss function as soft constraints and apply existing machine learning methods to train the model. However, recent research has noted that PINNs may fail to learn relatively complex Partial Differential Equations (PDEs). This paper addresses the failure modes of PINNs by introducing a novel, hard-constrained deep learning method --- trust-region Sequential Quadratic Programming (trSQP-PINN). In contrast to directly training the penalized soft-constrained loss as in PINNs, our method performs a linear-quadratic approximation of the hard-constrained loss, while leveraging the soft-constrained loss to adaptively adjust the trust-region radius. We only trust our model approximations and make updates within the trust region, and such an updating manner can overcome the ill-conditioning issue of PINNs. We also address the computational bottleneck of second-order SQP methods by employing quasi-Newton updates for second-order information, and importantly, we introduce a simple pretraining step to further enhance training efficiency of our method. We demonstrate the effectiveness of trSQP-PINN through extensive experiments. Compared to existing hard-constrained methods for PINNs, such as penalty methods and augmented Lagrangian methods, trSQP-PINN significantly improves the accuracy of the learned PDE solutions, achieving up to 1-3 orders of magnitude lower errors. Additionally, our pretraining step is generally effective for other hard-constrained methods, and experiments have shown the robustness of our method against both problem-specific parameters and algorithm tuning parameters."
    },
    {
        "title": "PHI-S: Distribution Balancing for Agglomerative Models",
        "link_suffix": "/forum?id=rP7rghI7yt",
        "link": "https://openreview.net/forum?id=rP7rghI7yt",
        "pdf_link": "https://openreview.net/pdf?id=rP7rghI7yt",
        "keywords": "Computer Vision, Deep Learning, Knowledge Distillation, Agglomerative Models",
        "abstract": "Various visual foundation models have distinct strengths and weaknesses, both of which can be improved through heterogeneous multi-teacher knowledge distillation without labels, termed \"agglomerative models.\" We build upon this body of work by studying the effect of the teachers' activation statistics, particularly the impact of the loss function on the resulting student model quality. We explore a standard toolkit of statistical normalization techniques to better align the different distributions and assess their effects. Further, we examine the impact on downstream teacher-matching metrics, which motivates the use of Hadamard matrices. With these matrices, we demonstrate useful properties, showing how they can be used for isotropic standardization, where each dimension of a multivariate distribution is standardized using the same scale. We call this technique \"PHI Standardization\" (PHI-S) and empirically demonstrate that it produces the best student model across the suite of methods studied."
    },
    {
        "title": "The Invariance Starvation Hypothesis",
        "link_suffix": "/forum?id=GF6UrrTWp1",
        "link": "https://openreview.net/forum?id=GF6UrrTWp1",
        "pdf_link": "https://openreview.net/pdf?id=GF6UrrTWp1",
        "keywords": "Spurious Correlations, Reasoning, Robustness",
        "abstract": "Deep neural networks are known to learn and rely on spurious correlations during training, preventing them from being reliable and able to solve highly complex problems. While there exist many proposed solutions that overcome such reliance in different, tailored settings, current understanding regarding the formation of spurious correlations is limited. All proposed solutions with promising results assume that networks trained with empirical risk minimization will learn spurious correlations due to a preference for simpler features and that a solution to this problem requires further processing on the networks' learned representations or re-training on a modified dataset where the proportion of training data with spurious features is significantly lower. In this paper, we aim to form a better understanding regarding the formation of spurious correlations by performing a rigorous study regarding the role that data plays in the formation of spurious correlations. We show that in reasoning tasks with simple input samples, simply drawing more data from the same training distribution overcomes spurious correlations, even though we maintain the proportion of samples with spurious features. In other words, we find that if the network has enough data to encode the invariant function appropriately, it no longer relies on spurious features, regardless of its strength. We observe the same results in settings with more complex distributions with an intractable number of participating features, such as vision and language. However, we find that in such settings, drawing more samples from the training distribution while maintaining proportion can exacerbate spurious correlations at times, due to the introduction of new samples that are significantly different from samples in the original training set. Taking inspiration from reasoning tasks, we present an effective remedy to this problem to ensure that drawing more samples from the distribution always overcomes spurious correlations."
    },
    {
        "title": "bio2token: all-atom tokenization of any biomolecular structure with mamba",
        "link_suffix": "/forum?id=6ktqrC1Bpf",
        "link": "https://openreview.net/forum?id=6ktqrC1Bpf",
        "pdf_link": "https://openreview.net/pdf?id=6ktqrC1Bpf",
        "keywords": "all-atom biomolecular generation, long context, auto-encoder, tokenization",
        "abstract": "Efficient encoding and representation of large 3D molecular structures with high fidelity is critical for biomolecular design applications. Despite this, many representation learning approaches restrict themselves to modeling smaller systems or use coarse-grained approximations of the systems, for example modeling proteins at the resolution of amino acid residues rather than at the level of individual atoms. To address this, we develop quantized auto-encoders that learn atom-level tokenizations of complete proteins, RNA and small molecule structures with reconstruction accuracies below and around 1 Angstrom. We demonstrate that the Mamba state space model architecture employed is comparatively efficient, requiring a fraction of the training data, parameters and compute needed to reach competitive accuracies and can scale to systems with almost 100,000 atoms. The learned structure tokens of bio2token may serve as the input for all-atom language models in the future."
    },
    {
        "title": "Severing Spurious Correlations with Data Pruning",
        "link_suffix": "/forum?id=Bk13Qfu8Ru",
        "link": "https://openreview.net/forum?id=Bk13Qfu8Ru",
        "pdf_link": "https://openreview.net/pdf?id=Bk13Qfu8Ru",
        "keywords": "Spurious Correlations, Data Pruning",
        "abstract": "Deep neural networks have been shown to learn and rely on spurious correlations present in the data that they are trained on. Reliance on such correlations can cause these networks to malfunction when deployed in the real world, where these correlations may no longer hold. To overcome the formation of such correlations, recent studies propose approaches that yield promising results. These works, however, study settings where the strength of the spurious signal is significantly greater than that of the core, invariant signal, making it easier to detect the presence of spurious features in individual training samples and allow for further processing. In this paper, we identify new settings where the strength of the spurious signal is relatively weaker, making it difficult to detect any spurious information while continuing to have catastrophic consequences. We also learn that spurious correlations are formed primarily due to only a handful of all the samples containing the spurious feature and develop a novel data pruning technique that identifies and prunes small subsets of the training data that contain these samples. Our proposed technique does not require information regarding the sample-wise presence or nature of spurious information, or human intervention. Finally, we show that such data pruning attains state-of-the-art performance on previously studied settings where spurious information is identifiable."
    },
    {
        "title": "X-Diffusion: Generating Detailed 3D MRI Volumes From a Single Image Using Cross-Sectional Diffusion Models",
        "link_suffix": "/forum?id=urf8a5G59f",
        "link": "https://openreview.net/forum?id=urf8a5G59f",
        "pdf_link": "https://openreview.net/pdf?id=urf8a5G59f",
        "keywords": "MRI reconstruction, diffusion models, latent diffusions",
        "abstract": "Magnetic Resonance Imaging (MRI) is a crucial diagnostic tool, but high-resolution scans are often slow and expensive due to extensive data acquisition requirements. Traditional MRI reconstruction methods aim to expedite this process by filling in missing frequency components in the K-space, performing3D-to-3Dreconstructions that demand full 3D scans. In contrast, we introduceX-Diffusion, a novel cross-sectional diffusion model that reconstructs detailed 3D MRI volumes from extremely sparse spatial-domain inputs\u2014achieving2D-to-3Dreconstruction from as little as a single 2D MRI slice or few slices.\nA key aspect of X-Diffusion is that it models MRI data as holistic 3D volumes during the cross-sectional training and inference, unlike previous learning approaches that treat MRI scans as collections of 2D slices in standard planes (coronal, axial, sagittal).\nWe evaluated X-Diffusion on brain tumor MRIs from the BRATS dataset and full-body MRIs from the UK Biobank dataset. Our results demonstrate that X-Diffusion not only surpasses state-of-the-art methods in quantitative accuracy (PSNR) on unseen data but also preserves critical anatomical features such as tumor profiles, spine curvature, and brain volume. Remarkably, the model generalizes beyond the training domain, successfully reconstructing knee MRIs despite being trained exclusively on brain data. Medical expert evaluations further confirm the clinical relevance and fidelity of the generated images.\nTo promote reproducibility and trust in our findings, we will publicly release the accompanying code upon publication. To our knowledge, X-Diffusion is the first method capable of producing detailed 3D MRIs from highly limited 2D input data, potentially accelerating MRI acquisition and reducing associated costs."
    },
    {
        "title": "Classifier-Free Guidance is a Predictor-Corrector",
        "link_suffix": "/forum?id=8K36RkrI7N",
        "link": "https://openreview.net/forum?id=8K36RkrI7N",
        "pdf_link": "https://openreview.net/pdf?id=8K36RkrI7N",
        "keywords": "diffusion, guidance, theory, SDE",
        "abstract": "We investigate the theoretical foundations of classifier-free guidance (CFG). CFG is the dominant method of conditional sampling for text-to-image diffusion models, yet unlike other aspects of diffusion, it remains on shaky theoretical footing. In this paper, we disprove common misconceptions, by showing that CFG interacts differently with DDPM and DDIM, and neither sampler with CFG generates the gamma-powered distribution $p(x|c)^\\gamma p(x)^{1\u2212\\gamma}$. Then, we clarify the behavior of CFG by showing that it is a kind of predictor-corrector method (Song et al., 2020) that alternates between denoising and sharpening, which we call predictor-corrector guidance (PCG). We prove that in the SDE limit, CFG is actually equivalent to combining a DDIM predictor for the conditional distribution together with a Langevin dynamics corrector for a gamma-powered distribution (with a carefully chosen gamma). Our work thus provides a lens to theoretically understand CFG by embedding it in a broader design space of principled sampling methods."
    },
    {
        "title": "Uncovering Model Vulnerabilities With Multi-Turn Red Teaming",
        "link_suffix": "/forum?id=fFtmpqLFvw",
        "link": "https://openreview.net/forum?id=fFtmpqLFvw",
        "pdf_link": "https://openreview.net/pdf?id=fFtmpqLFvw",
        "keywords": "language models, ai security, ai safety, robustness, adversarial attacks",
        "abstract": "Recent large language model (LLM) defenses have greatly improved models' ability to refuse harmful queries, even when adversarially attacked. However, LLM defenses are primarily evaluated against automated adversarial attacks in a single turn of conversation, an insufficient threat model for real-world malicious use. We demonstrate that multi-turn human jailbreaks uncover significant vulnerabilities, exceeding 70% attack success rate (ASR) on HarmBench against defenses that report single-digit ASRs with automated single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine unlearning defenses, successfully recovering dual-use biosecurity knowledge from unlearned models. We compile these results into Multi-Turn Human Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks. We publicly release MHJ alongside a compendium of jailbreak tactics developed across dozens of commercial red teaming engagements, supporting research towards stronger LLM defenses."
    },
    {
        "title": "CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation",
        "link_suffix": "/forum?id=M2SsqpxGtc",
        "link": "https://openreview.net/forum?id=M2SsqpxGtc",
        "pdf_link": "https://openreview.net/pdf?id=M2SsqpxGtc",
        "keywords": "panorama generation, diffusion, multi-view",
        "abstract": "We introduce a novel method for generating 360\u00b0 panoramas from text prompts or images. Our approach leverages recent advances in 3D generation by employing multi-view diffusion models to jointly synthesize the six faces of a cubemap. Unlike previous methods that rely on processing equirectangular projections or autoregressive generation, our method treats each face as a standard perspective image, simplifying the generation process and enabling the use of existing multi-view diffusion models. We demonstrate that these models can be adapted to produce high-quality cubemaps without requiring correspondence-aware attention layers. Our model allows for fine-grained text control, generates high resolution panorama images and generalizes well beyond its training set, whilst achieving state-of-the-art results, both qualitatively and quantitatively."
    },
    {
        "title": "Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations",
        "link_suffix": "/forum?id=Hu0FSOSEyS",
        "link": "https://openreview.net/forum?id=Hu0FSOSEyS",
        "pdf_link": "https://openreview.net/pdf?id=Hu0FSOSEyS",
        "keywords": "Inverse Problems, Generative Modeling, Diffusion Models, Rectified Flows, Posterior Sampling, Optimal Control",
        "abstract": "Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i)inversionand (ii)editingof a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference."
    },
    {
        "title": "Rethinking Addressing in Language Models via Contextualized Equivariant Positional Encoding",
        "link_suffix": "/forum?id=Us1RXG1Ji2",
        "link": "https://openreview.net/forum?id=Us1RXG1Ji2",
        "pdf_link": "https://openreview.net/pdf?id=Us1RXG1Ji2",
        "keywords": "Positional Encoding, Equivariance, Addressing",
        "abstract": "Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset.\nTo address this, we propose TAPE: conTextualized equivariAntPositionEmbedding, \na novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving robustness and adaptability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead.\nExtensive experiments shows that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques."
    },
    {
        "title": "MiraGe: Editable 2D Images using Gaussian Splatting",
        "link_suffix": "/forum?id=UkEvpOzZAR",
        "link": "https://openreview.net/forum?id=UkEvpOzZAR",
        "pdf_link": "https://openreview.net/pdf?id=UkEvpOzZAR",
        "keywords": "Gaussian Splatting, implicit reprezentation. physics",
        "abstract": "Implicit Neural Representations (INRs) approximate discrete data through continuous functions and are commonly used for encoding 2D images. Traditional image-based INRs employ neural networks to map pixel coordinates to RGB values, capturing shapes, colors, and textures within the network\u2019s weights. Recently, 2D Gaussian Splatting (GS) has been proposed as an alternative, using Gaussian functions instead of neural networks to achieve comparable quality and compression. Such a solution obtains a quality and compression ratio similar to classical INR models but does not allow image modification. In contrast, our work introduces a novel method, MiraGe, which uses mirror reflections to perceive 2D images in 3D space and employs flat-controlled Gaussians for precise 2D image editing. Our approach improves the rendering quality and allows realistic image modifications, including human-inspired perception of photos in the 3D world. Thanks to modeling images in 3D space, we obtain the illusion of 3D-based modification in 2D images. We also show that our Gaussian representation can be easily combined with a physics engine to produce physics-based modification of 2D images. Consequently, MiraGe allows for better quality than the standard approach and for natural modification of 2D images."
    },
    {
        "title": "Vector-ICL: In-context Learning with Continuous Vector Representations",
        "link_suffix": "/forum?id=xing7dDGh3",
        "link": "https://openreview.net/forum?id=xing7dDGh3",
        "pdf_link": "https://openreview.net/pdf?id=xing7dDGh3",
        "keywords": "large language models, in-context learning",
        "abstract": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms."
    },
    {
        "title": "Understanding Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing",
        "link_suffix": "/forum?id=pymXpl4qvi",
        "link": "https://openreview.net/forum?id=pymXpl4qvi",
        "pdf_link": "https://openreview.net/pdf?id=pymXpl4qvi",
        "keywords": "SSM, Locality, Recency, Over-smoothing",
        "abstract": "Structured State Space Models (SSMs) have emerged as alternatives to transformers, addressing the challenges of processing long sequences.\nWhile SSMs are often regarded as effective in capturing long-term dependencies, we theoretically demonstrate that they suffer from a strong recency bias.\nOur empirical findings reveal that this bias impairs the models' ability to recall distant information and introduces robustness issues.\nWe conducted scaling experiments and discovered that deeper structures in SSMs facilitate the learning of long contexts.\nHowever, our theoretical analysis reveal that as SSMs increase in depth, they exhibit a tendency toward over-smoothing, resulting in token representations becoming increasingly indistinguishable.\nThis over-smoothing phenomenon ultimately constrains the scalability of SSMs to achieve improved performance.\nCollectively, these findings highlight important limitations of SSMs and underscore the need for further research to address these challenges in long-range sequence modeling."
    },
    {
        "title": "LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing",
        "link_suffix": "/forum?id=AyC4uxx2HW",
        "link": "https://openreview.net/forum?id=AyC4uxx2HW",
        "pdf_link": "https://openreview.net/pdf?id=AyC4uxx2HW",
        "keywords": "large language models, elastic networks, training efficiency, inference efficiency",
        "abstract": "Large Language Model (LLM) providers typically train a family of models, each of a different size targeting a specific deployment scenario. Models in the family are all trained from scratch, making the process extremely resource intensive.\nRecent work has successfully reduced the cost of training model families through a combination of structured pruning and knowledge distillation; here, only the largest model in the family is trained from scratch, and smaller models are obtained via pruning. We observe that while effective, this strategy must still perform pruning and distillation with hundreds of billions of training tokens for every new model, keeping overall training costs high.\nIn this work, we introduce a novel nested weight-shared architecture named LLaMaFlex that can be pruned across both width and depth dimensions in a zero-shot manner to instantly yield a large number of highly accurate compressed models.\nLLaMaFlex starts from a pretrained model, and only requires a single continued training phase consisting of ~60B tokens, which trains the elastic network and an end-to-end Gumbel Softmax-based router; this router is able to interpolate smoothly across model sizes, enabling the \"train once, deploy many'' paradigm.\nWe train LLaMaFlex on Llama 3.1 8B and use it to zero-shot generate a family of compressed models that achieves accuracy on par with or better than state-of-the-art pruned, elastic/flexible, and trained-from-scratch models."
    },
    {
        "title": "Language Model Alignment in Multilingual Trolley Problems",
        "link_suffix": "/forum?id=VEqPDZIDAh",
        "link": "https://openreview.net/forum?id=VEqPDZIDAh",
        "pdf_link": "https://openreview.net/pdf?id=VEqPDZIDAh",
        "keywords": "LLM alignment, moral evaluation, trolley problems, language model evaluation, AI alignment",
        "abstract": "We evaluate the moral alignment of large language models (LLMs) with human preferences in multilingual trolley problems. Building on the Moral Machine experiment, which captures over 40 million human judgments across 200+ countries, we develop a cross-lingual corpus of moral dilemma vignettes in over 100 languages called MultiTP. This dataset enables the assessment of LLMs' decision-making processes in diverse linguistic contexts. Our analysis explores the alignment of 19 different LLMs with human judgments, capturing preferences across six moral dimensions: species, gender, fitness, status, age, and the number of lives involved. By correlating these preferences with the demographic distribution of language speakers and examining the consistency of LLM responses to various prompt paraphrasings, our findings provide insights into cross-lingual and ethical biases of LLMs and their intersection. We discover significant variance in alignment across languages, challenging the assumption of uniform moral reasoning in AI systems and highlighting the importance of incorporating diverse perspectives in AI ethics. The results underscore the need for further research on the integration of multilingual dimensions in responsible AI research to ensure fair and equitable AI interactions worldwide."
    }
]