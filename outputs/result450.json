[
    {
        "title": "Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model",
        "link_suffix": "/forum?id=kxFtMHItrf",
        "link": "https://openreview.net/forum?id=kxFtMHItrf",
        "pdf_link": "https://openreview.net/pdf?id=kxFtMHItrf",
        "keywords": "Illumination degradation image restoration, Latent diffusion model, Retinex theory",
        "abstract": "Illumination degradation image restoration (IDIR) techniques aim to improve the visibility of degraded images and mitigate the adverse effects of deteriorated illumination. Among these algorithms, diffusion-based models (DM) have shown promising performance but are often burdened by heavy computational demands and pixel misalignment issues when predicting the image-level distribution. To tackle these problems, we propose to leverage DM within a compact latent space to generate concise guidance priors and introduce a novel solution called Reti-Diff for the IDIR task. Specifically, Reti-Diff comprises two significant components: the Retinex-based latent DM (RLDM) and the Retinex-guided transformer (RGformer). RLDM is designed to acquire Retinex knowledge, extracting reflectance and illumination priors to facilitate detailed reconstruction and illumination correction. RGformer subsequently utilizes these compact priors to guide the decomposition of image features into their respective reflectance and illumination components. Following this, RGformer further enhances and consolidates these decomposed features, resulting in the production of refined images with consistent content and robustness to handle complex degradation scenarios. Extensive experiments demonstrate that Reti-Diff outperforms existing methods on three IDIR tasks, as well as downstream applications."
    },
    {
        "title": "Algebraic SPD and Correlation Geometry: A Gyro Approach",
        "link_suffix": "/forum?id=ZPwX1FL4yp",
        "link": "https://openreview.net/forum?id=ZPwX1FL4yp",
        "pdf_link": "https://openreview.net/pdf?id=ZPwX1FL4yp",
        "keywords": "Correlation matrices, SPD manifolds, Gyrovector spaces",
        "abstract": "The generalization of Deep Neural Networks (DNNs) to Riemannian manifolds has garnered significant attention across various scientific fields. Recent studies have demonstrated that several manifolds, including hyperbolic, spherical, Symmetric Positive Definite (SPD), and Grassmann manifolds, admit gyro-structures\u2014powerful algebraic structures that enable the principled extension of DNNs to manifolds. Inspired by these advancements, we introduce a novel gyro-structure for SPD manifolds, leveraging the flexible and powerful Power-Euclidean (PE) geometry. Moreover, full-rank correlation matrices, which are scale-invariant, serve as compact representations of SPD manifolds. Consequently, we propose two novel gyro-structures for correlation matrix manifolds, based on two theoretically and empirically convenient metrics: Euclidean-Cholesky (EC) and log-Euclidean-Cholesky (LEC) geometries. Extensive experiments on knowledge graph completion tasks validate the effectiveness of our proposed gyro-structures."
    },
    {
        "title": "MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers",
        "link_suffix": "/forum?id=yVeNBxwL5W",
        "link": "https://openreview.net/forum?id=yVeNBxwL5W",
        "pdf_link": "https://openreview.net/pdf?id=yVeNBxwL5W",
        "keywords": "Fast Sampler, Mean Reverting Diffusion",
        "abstract": "In applications of diffusion models, controllable generation is of practical significance, but is also challenging. Current methods for controllable generation primarily focus on modifying the score function of diffusion models, while Mean Reverting (MR) Diffusion directly modifies the structure of the stochastic differential equation (SDE), making the incorporation of image conditions simpler and more natural. However, current training-free fast samplers are not directly applicable to MR Diffusion. And thus MR Diffusion requires hundreds of NFEs (number of function evaluations) to obtain high-quality samples. In this paper, we propose a new algorithm named MRS (MR Sampler) to reduce the sampling NFEs of MR Diffusion. We solve the reverse-time SDE and the probability flow ordinary differential equation (PF-ODE) associated with MR Diffusion, and derive semi-analytical solutions. The solutions consist of an analytical function and an integral parameterized by a neural network. Based on this solution, we can generate high-quality samples in fewer steps. Our approach does not require training and supports all mainstream parameterizations, including noise prediction, data prediction and velocity prediction. Extensive experiments demonstrate that MR Sampler maintains high sampling quality with a speedup of 10 to 20 times across ten different image restoration tasks. Our algorithm accelerates the sampling procedure of MR Diffusion, making it more practical in controllable generation."
    },
    {
        "title": "Fading Focus: Mitigating Visual Attention Degradation in Large Vision-Language Models",
        "link_suffix": "/forum?id=gam5LiMPKT",
        "link": "https://openreview.net/forum?id=gam5LiMPKT",
        "pdf_link": "https://openreview.net/pdf?id=gam5LiMPKT",
        "keywords": "Hallucination; Large Vision-Language Models; Decoding Strategy",
        "abstract": "How can we ensure that Large Vision-Language Models (LVLMs) maintain strong attention to visual input throughout the inference process?  Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress across multiple domains. However, these models still face the inherent challenge of integrating vision and language for collaborative inference, which often leads to \"hallucinations,\" outputs that are not grounded in the corresponding images. Many efforts have been made to address these challenges, but each approach comes with its own limitations, such as high computational costs or expensive dataset annotation. Worse still, many of them fail to recognize the crucial role of visual attention in guiding the model\u2019s response generation.\nIn our research, we identify a key limitation in current LVLMs: the model's diminishing attention to visual input as the number of generated tokens increases, which results in performance degradation. To address this challenge, we propose \\textbf{I}mage attention-guided \\textbf{K}ey-value merging c\\textbf{O}llaborative \\textbf{D}ecoding (IKOD),  a collaborative decoding strategy that generates image-focused sequences using key-value merging. This method derives logits from shorter sequences with higher image attention through key-value merging and combines them with those from the original decoding process, effectively mitigating attention decay. Importantly, IKOD requires no additional training or external tools, making it highly scalable and applicable to various models."
    },
    {
        "title": "A Self-Supervised PINN for Inertial Pose and Dynamics Estimations",
        "link_suffix": "/forum?id=eTWRCiMQ1z",
        "link": "https://openreview.net/forum?id=eTWRCiMQ1z",
        "pdf_link": "https://openreview.net/pdf?id=eTWRCiMQ1z",
        "keywords": "self-supervised learning, biomechanics, physics-informed neural networks",
        "abstract": "Accurate real-time monitoring of not only movements, but also internal joint moments or muscle forces that cause movement in unrestricted environments is key for many clinical and sports applications. A minimally obstrusive way to monitor movements is with wearable sensors, such as inertial measurement units, using the fewest sensors possible. \nCurrent real-time methods rely on supervised learning, where a ground truth dataset needs to be measured with laboratory measurement systems, such as optical motion capture, which then needs to be processed with methods that are known to introduce errors. There is a discrepancy between laboratory and real-world movements, and for analysing new motions, new ground truth data would need to be recorded, which is impractical.\nTherefore, we introduce SSPINNpose, a self-supervised physics-informed neural network that estimates movement dynamics, including joint angles and joint moments, from inertial sensors without the need for ground truth data for training.\nWe run the network output through a physics model of the human body to optimize physical plausibility and generate virtual measurement data. Using this virtual sensor data, the network is trained directly on the measured sensor data instead of a ground truth. Experiments show that SSPINNpose is able to accurately estimate joint angles and joint moments at 8.7 degrees and 4.9 BWBH%, respectively, for walking and running at up to speeds of 4.9 m/s at a latency of 3.5 ms. We further show the versatility of our method by estimating movement dynamics for a variety of sparse sensor configurations and inferring the positions where the sensors are placed on the body."
    },
    {
        "title": "SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks",
        "link_suffix": "/forum?id=RInisw1yin",
        "link": "https://openreview.net/forum?id=RInisw1yin",
        "pdf_link": "https://openreview.net/pdf?id=RInisw1yin",
        "keywords": "Robotic Assembly Tasks; Skill Retrieval; Skill Adaptation; Sim-to-real Transfer; Reinforcement Learning Fine-tuning;",
        "abstract": "Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made in developing such strategies for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. In this work, we present SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. Through extensive experiments, we demonstrate that SRSA effectively retrieves and fine-tunes skills on unseen tasks with a 22% higher success rate, 3.7x greater stability, and 2.4x better sample efficiency compared to the leading baseline. Moreover, in a continual learning setup, SRSA efficiently learns policies for new tasks and incorporates them into the skill library, enhancing future policy learning. Additionally, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage athttps://srsa2024.github.io/for videos."
    },
    {
        "title": "AlphaDou: High-Performance End-to-End Doudizhu AI Integrating Bidding",
        "link_suffix": "/forum?id=rRRgj3iIHR",
        "link": "https://openreview.net/forum?id=rRRgj3iIHR",
        "pdf_link": "https://openreview.net/pdf?id=rRRgj3iIHR",
        "keywords": "Multi-Agent Reinforcement Learning, Imperfect-Information Games, Deep Mento Carlo, Adversarial and Cooperative",
        "abstract": "Artificial intelligence for card games has long been a popular topic in AI research. In recent years, complex card games like Mahjong and Texas Hold'em have been solved, with corresponding AI programs reaching the level of human experts. However, the game of Doudizhu presents significant challenges due to its vast state/action space and unique characteristics involving reasoning about competition and cooperation, making the game extremely difficult to solve.The RL model Douzero, trained using the Deep Monte Carlo algorithm framework, has shown excellent performance in Doudizhu. However, there are differences between its simplified game environment and the actual Doudizhu environment, and its performance is still a considerable distance from that of human experts. This paper modifies the Deep Monte Carlo algorithm framework by using reinforcement learning to obtain a neural network that simultaneously estimates win rates and expectations. The action space is pruned using expectations, and strategies are generated based on win rates.  The modified algorithm enables the AI to perform the full range of tasks in the Doudizhu game, including bidding and cardplay. The model was trained in a actual Doudizhu environment and achieved state-of-the-art performance among publicly available models. We hope that this new framework will provide valuable insights for AI development in other bidding-based games."
    },
    {
        "title": "LSTT:LONG SHORT-TERM TRANSFORMER FOR VIDEO SMALL OBJECT DETECTION",
        "link_suffix": "/forum?id=j3R1qHvoSM",
        "link": "https://openreview.net/forum?id=j3R1qHvoSM",
        "pdf_link": "https://openreview.net/pdf?id=j3R1qHvoSM",
        "keywords": "Long short-term ; transformer; Small Object Detection; Video Object",
        "abstract": "Detecting small objects in video sequences is crucial, yet it poses significant challenges due to their limited visibility and dynamic nature, which complicates accurate identification and localization. Traditional methods often employ a uniform aggregation strategy across all frames, neglecting the unique spatiotemporal relationships of small objects, which results in insufficient feature extraction and diminished detection performance. This paper introduces a long short-term transformer network specifically designed for small object detection in videos. The model integrates features from both long-term and short-term frames: long-term frames capture global contextual information, enhancing the model\u2019s ability to represent background scenes, while short-term frames provide dynamic information closely related to the current detection frame, thereby improving the feature representation of small objects. A dynamic query generation module optimizes query generation based on the implicit motion relationships of targets in shortterm frames, adapting to the current video framework. Additionally, the network employs a progressive sampling strategy\u2014densely sampling short-term frames and sparsely sampling long-term frames\u2014to effectively model video scenes. A spatio-temporal alignment encoder further enhances pixel-level features by accounting for temporal and spatial transformations. Extensive experiments on the VisDrone-VID and UAVDT datasets demonstrate the method\u2019s effectiveness, with an average detection precision increase of 1.4% and 2.1%, respectively, highlighting its potential in small object video detection."
    },
    {
        "title": "Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis",
        "link_suffix": "/forum?id=GJsuYHhAga",
        "link": "https://openreview.net/forum?id=GJsuYHhAga",
        "pdf_link": "https://openreview.net/pdf?id=GJsuYHhAga",
        "keywords": "Text-to-Image Synthesis",
        "abstract": "Diffusion models, such as Stable Diffusion, have made significant strides in visual generation, yet their paradigm remains fundamentally different from autoregressive language models, complicating the development of unified language-vision models. Recent efforts like LlamaGen have attempted autoregressive image generation using discrete VQVAE tokens, but the large number of tokens involved renders this approach inefficient and slow. In this work, we present Meissonic, which elevates non-autoregressive text-to-image Masked Image Modeling (MIM) to a level comparable with state-of-the-art diffusion models like SDXL. By incorporating a comprehensive suite of architectural innovations, advanced positional encoding strategies, and optimized sampling conditions, Meissonic substantially improves MIM's performance and efficiency. Additionally, we leverage high-quality training data, integrate micro-conditions informed by human preference scores, and employ feature compression layers to further enhance image fidelity and resolution. Our model not only matches but often exceeds the performance of existing methods in generating high-quality, high-resolution images. Extensive experiments validate Meissonic\u2019s capabilities, demonstrating its potential as a new standard in text-to-image synthesis. We release a model checkpoint capable of producing $1024 \\times 1024$ resolution images."
    },
    {
        "title": "AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation",
        "link_suffix": "/forum?id=JVkdSi7Ekg",
        "link": "https://openreview.net/forum?id=JVkdSi7Ekg",
        "pdf_link": "https://openreview.net/pdf?id=JVkdSi7Ekg",
        "keywords": "Robotic Manipulation; Data Generation; Vision-Language-Model; Failure Reasoning; Failure Detection",
        "abstract": "Robotic manipulation in open-world settings requires not only task execution but also the ability to detect and learn from failures. While recent advances in vision-language models (VLMs) and large language models (LLMs) have improved robots' spatial reasoning and problem-solving abilities, they still struggle with failure recognition, limiting their real-world applicability. We introduce AHA, an open-source VLM designed to detect and reason about failures in robotic manipulation using natural language. By framing failure detection as a free-form reasoning task, AHA identifies failures and provides detailed, adaptable explanations across different robots, tasks, and environments. We fine-tuned AHA using FailGen, a scalable framework that generates the first large-scale dataset of robotic failure trajectories, the AHA dataset. FailGen achieves this by procedurally perturbing successful demonstrations from simulation. Despite being trained solely on the AHA dataset, AHA generalizes effectively to real-world failure datasets, robotic systems, and unseen tasks. It surpasses the second-best model (GPT-4o in-context learning) by 10.3% and exceeds the average performance of six compared models, including five state-of-the-art VLMs, by 35.3% across multiple metrics and datasets. We integrate AHA into three manipulation frameworks that utilize LLMs/VLMs for reinforcement learning, task and motion planning, and zero-shot trajectory generation. AHA\u2019s failure feedback enhances these policies' performances by refining dense reward functions, optimizing task planning, and improving sub-task verification, boosting task success rates by an average of 21.4% across all three tasks compared to GPT-4 models. Anonymous project page:aha-iclr.github.io."
    },
    {
        "title": "Shapley-Guided Utility Learning for Effective Graph Inference Data Valuation",
        "link_suffix": "/forum?id=8X74NZpARg",
        "link": "https://openreview.net/forum?id=8X74NZpARg",
        "pdf_link": "https://openreview.net/pdf?id=8X74NZpARg",
        "keywords": "Graph Learning, Data Valuation, Graph Neural Networks, Data-centric AI",
        "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable performance in various graph-based machine learning tasks, yet evaluating the importance of neighbors of testing nodes remains largely unexplored due to the challenge of assessing data importance without test labels. To address this gap, we propose Shapley-Guided Utility Learning (SGUL), a novel framework for graph inference data valuation. SGUL innovatively combines transferable data-specific and modelspecific features to approximate test accuracy without relying on ground truth labels. By incorporating Shapley values as a preprocessing step and using feature Shapley values as input, our method enables direct optimization of Shapley value prediction while reducing computational demands. SGUL overcomes key limitations of existing methods, including poor generalization to unseen test-time structures and indirect optimization. Experiments on diverse graph datasets demonstrate that SGUL consistently outperforms existing baselines in both inductive and transductive settings. SGUL offers an effective, efficient, and interpretable approach for quantifying the value of test-time neighbors."
    },
    {
        "title": "LLMs Boost the Performance of Decision Trees on Tabular Data across Sample Sizes",
        "link_suffix": "/forum?id=gp5tRHkz9B",
        "link": "https://openreview.net/forum?id=gp5tRHkz9B",
        "pdf_link": "https://openreview.net/pdf?id=gp5tRHkz9B",
        "keywords": "Tabular data, large language models, decision trees, ensembling",
        "abstract": "Large language models (LLMs) perform remarkably well on tabular datasets in zero- and few-shot settings, since they can extract meaning from natural language column headers that describe features and labels. In contrast to LLMs, gradientboosted decision trees (GBDTs) must learn the relationships among columns from scratch, increasing their data requirements. Meanwhile, LLMs are not competitive with GBDTs on medium or large datasets, and their scalability is capped by their limited context lengths. In this paper, we propose LLM-Boost, a simple and lightweight approach for fusing large language models with gradientboosted decision trees, which enables larger datasets to benefit from the natural language capabilities of LLMs than was previously shown. LLM-Boost outperforms both LLMs and GBDTs on a wide range of dataset sizes. We demonstrate state-of-the-art performance against numerous baselines and ensembling approaches, and we also show how to fuse GBDTs with TabPFN, a recent nonLLM model for in-context learning on tabular data. We find that this combination achieves the best performance on large datasets."
    },
    {
        "title": "DreamDistribution: Prompt Distribution Learning for Text-to-Image Diffusion Models",
        "link_suffix": "/forum?id=oQoQ4u6MQC",
        "link": "https://openreview.net/forum?id=oQoQ4u6MQC",
        "pdf_link": "https://openreview.net/pdf?id=oQoQ4u6MQC",
        "keywords": "Generative Models, Image Generation, Personalized Generation",
        "abstract": "The popularization of Text-to-Image (T2I) diffusion models enables the generation of high-quality images from text descriptions. However, generating diverse customized images with reference visual attributes remains challenging. This work focuses on personalizing T2I diffusion models at a more abstract concept or category level, adapting commonalities from a set of reference images while creating new instances with sufficient variations. We introduce a solution that allows a pretrained T2I diffusion model to learn a set of soft prompts, enabling the generation of novel images by sampling prompts from the learned distribution. These prompts offer text-guided editing capabilities and additional flexibility in controlling variation and mixing between multiple distributions. We also show the adaptability of the learned prompt distribution to other tasks, such as text-to-3D. Finally we demonstrate effectiveness of our approach through quantitative analysis including automatic evaluation and human assessment."
    },
    {
        "title": "SeeThruAnything: Learning to Remove Any Obstructions Across Distributions",
        "link_suffix": "/forum?id=Djk1Tgs0wR",
        "link": "https://openreview.net/forum?id=Djk1Tgs0wR",
        "pdf_link": "https://openreview.net/pdf?id=Djk1Tgs0wR",
        "keywords": "Obstruction Removal, Zero-shot, Prompts",
        "abstract": "Images are often obstructed by various obstacles due to capture limitations, hindering the observation of objects of interest. Most existing methods address occlusions from specific elements like fences or raindrops, but are constrained by the wide range of real-world obstructions, making comprehensive data collection impractical. To overcome these challenges, we propose SeeThruAnything, a novel zero-shot framework capable of handling both seen and unseen obstacles. The core idea of our approach is to unify obstruction removal by treating it as a soft-hard mask restoration problem, where any obstruction can be represented using multi-modal prompts, such as visual semantics and textual commands, processed through a cross-attention unit to enhance contextual understanding and improve mode control. Additionally, a tunable mask adapter allows for dynamic soft masking, enabling real-time adjustment of inaccurate masks. Extensive experiments on both in-distribution and out-of-distribution obstacles show that SeeThruAnything consistently achieves strong performance and generalization in obstruction removal, regardless of whether the obstacles were present during training."
    },
    {
        "title": "GyroAtt: A Gyro Attention Framework for Matrix Manifolds",
        "link_suffix": "/forum?id=YcaFqY8LWD",
        "link": "https://openreview.net/forum?id=YcaFqY8LWD",
        "pdf_link": "https://openreview.net/pdf?id=YcaFqY8LWD",
        "keywords": "Manifold Learning, Representation Learning, Gyrovector Spaces, Riemannian Manifolds, Riemannian Self Attention",
        "abstract": "Deep neural networks operating on non-Euclidean geometries, such as Riemannian manifolds, have recently demonstrated impressive performance across various machine-learning applications. Motivated by the success of the attention mechanism, several works have extended it to different geometries. However, existing Riemannian attention methods are mostly designed in an \\textit{ad hoc} manner, \\textit{i.e.}, tailored to a selected few geometries. Recent studies, on the other hand, show that several matrix manifolds, such as Symmetric Positive Definite (SPD), Symmetric Positive Semi-Definite (SPSD), and Grassmannian manifolds, admit gyro structures, offering a principled way to build Riemannian networks. Inspired by this, we propose a Gyro Attention (GyroAtt) framework over general gyro spaces, applicable to various matrix manifolds. Empirically, we manifest our framework on three gyro structures in the SPD manifold, three in the SPSD manifold, and one in the Grassmannian manifold. Extensive experiments on four electroencephalography (EEG) datasets demonstrate the effectiveness of the proposed framework."
    },
    {
        "title": "UOE: Unlearning One Expert is Enough for Mixture-of-Experts LLMs",
        "link_suffix": "/forum?id=ZClm0YbcXP",
        "link": "https://openreview.net/forum?id=ZClm0YbcXP",
        "pdf_link": "https://openreview.net/pdf?id=ZClm0YbcXP",
        "keywords": "Machine Unlearning, Mixture-of-Expert, Large Language Model",
        "abstract": "Recent advancements in large language model (LLM) unlearning have shown remarkable success in removing unwanted data-model influences while preserving the model's utility for legitimate knowledge. However, despite these strides, sparse Mixture-of-Experts (MoE) LLMs--a key subset of the LLM family--have received little attention and remain largely unexplored in the context of unlearning. As MoE LLMs are celebrated for their exceptional performance and highly efficient inference processes, we ask: How can unlearning be performed effectively and efficiently on MoE LLMs? And will traditional unlearning methods be applicable to MoE architectures? Our pilot study shows that the dynamic routing nature of MoE LLMs introduces unique challenges, leading to substantial utility drops when existing unlearning methods are applied. Specifically, unlearning disrupts the router's expert selection, causing significant selection shift from the most unlearning target-related experts to irrelevant ones. As a result, more experts than necessary are affected, leading to excessive forgetting and loss of control over which knowledge is erased. To address this, we propose a novel single-expert unlearning framework, referred to as UOE, for MoE LLMs. Through expert attribution, unlearning is concentrated on the most actively engaged expert for the specified knowledge. Concurrently, an anchor loss is applied to the router to stabilize the active state of this targeted expert, ensuring focused and controlled unlearning that preserves model utility. The proposed UOE framework is also compatible with various unlearning algorithms. Extensive experiments demonstrate that UOE enhances both forget quality up to 5% and model utility by 35% on MoE LLMs across various benchmarks, LLM architectures, while only unlearning 0.06% of the model parameters."
    },
    {
        "title": "Agent-as-a-Judge: Evaluating Agents with Agents",
        "link_suffix": "/forum?id=DeVm3YUnpj",
        "link": "https://openreview.net/forum?id=DeVm3YUnpj",
        "pdf_link": "https://openreview.net/pdf?id=DeVm3YUnpj",
        "keywords": "Code Generation; Agent-as-a-Judge; AI Developer; AI Judge;  LLM",
        "abstract": "Contemporary evaluation techniques are inadequate for agentic systems. These approaches either focus exclusively on final outcomes---ignoring the step-by-step nature of the thinking done by agentic systems---or require excessive manual labour. To address this, we introduce theAgent-as-a-Judgeframework, wherein agentic systems are used to evaluate agentic systems. This is a natural extension of the LLM-as-a-Judge framework, incorporating agentic features that enable intermediate feedback for the entire task-solving processes for more precise evaluations. We apply the Agent-as-a-Judge framework to the task of code generation. To overcome issues with existing benchmarks and provide a proof-of-concept testbed for Agent-as-a-Judge, we presentDevAI, a new benchmark of 55 realistic AI code generation tasks. DevAI includes rich manual annotations, like a total of 366 hierarchical solution requirements, which make it particularly suitable for an agentic evaluator. We benchmark three of the top code-generating agentic systems using Agent-as-a-Judge and find that our framework dramatically outperforms LLM-as-a-Judge and is as reliable as our human evaluation baseline. Altogether, we believe that this work represents a concrete step towards enabling vastly more sophisticated agentic systems. To help that, our dataset and the full implementation of Agent-as-a-Judge will be publically available at [REDACTED]"
    },
    {
        "title": "FMBench: Benchmarking Fairness in Multimodal Large Language Models on Medical Tasks",
        "link_suffix": "/forum?id=b2LklBgdcL",
        "link": "https://openreview.net/forum?id=b2LklBgdcL",
        "pdf_link": "https://openreview.net/pdf?id=b2LklBgdcL",
        "keywords": "Multimodal Large Language Models ; Visual Question Answering ; Report Generation ; FMBench ; Fairness-Aware Performance ;",
        "abstract": "Advancements in Multimodal Large Language Models (MLLMs) have significantly improved medical task performance, such as Visual Question Answering (VQA) and Report Generation (RG). However, the fairness of these models across diverse demographic groups remains underexplored, despite its importance in healthcare. This oversight is partly due to the lack of demographic diversity in existing medical multimodal datasets, which complicates the evaluation of fairness. In response, we proposeFMBench, the first benchmark designed to evaluate the fairness of MLLMs performance across diverse demographic attributes. FMBench has the following key features:1:It includes four demographic attributes: race, ethnicity, language, and gender, across two tasks, VQA and RG, under zero-shot settings.2:Our VQA task is free-form, enhancing real-world applicability and mitigating the biases associated with predefined choices.3:We utilize both lexical metrics and LLM-based metrics, aligned with clinical evaluations, to assess models not only for linguistic accuracy but also from a clinical perspective. Furthermore, we introduce a new metric,Fairness-Aware Performance (FAP), to evaluate how fairly MLLMs perform across various demographic attributes. We thoroughly evaluate the performance and fairness of eight state-of-the-art open-source MLLMs, including both general and medical MLLMs, ranging from 7B to 26B parameters on the proposed benchmark. We aim forFMBenchto assist the research community in refining model evaluation and driving future advancements in the field. All data and code will be released upon acceptance."
    },
    {
        "title": "Learning the Language of Protein Structure",
        "link_suffix": "/forum?id=kPlePgo1Nw",
        "link": "https://openreview.net/forum?id=kPlePgo1Nw",
        "pdf_link": "https://openreview.net/pdf?id=kPlePgo1Nw",
        "keywords": "Strucutral Biology; Quantized Representation; Generative Modeling",
        "abstract": "Representation learning and \\emph{de novo} generation of proteins are pivotal computational biology tasks. \nWhilst natural language processing (NLP) techniques have proven highly effective for protein sequence modelling, structure modelling presents a complex challenge, primarily due to its continuous and three-dimensional nature.Motivated by this discrepancy, we introduce an approach using a vector-quantized autoencoder that effectively tokenizes protein structures into discrete representations.  This method transforms the continuous, complex space of protein structures into a manageable, discrete format with a codebook ranging from 4096 to 64000 tokens, achieving high-fidelity reconstructions with backbone root mean square deviations (RMSD) of approximately 1-5 \\AA. To demonstrate the efficacy of our learned representations, we show that a simple GPT model trained on our codebooks can generate novel, diverse, and designable protein structures. Our approach not only provides representations of protein structure, but also mitigates the challenges of disparate modal representations and sets a foundation for seamless, multi-modal integration, enhancing the capabilities of computational methods in protein design."
    },
    {
        "title": "Personality Alignment of Large Language Models",
        "link_suffix": "/forum?id=0DZEs8NpUH",
        "link": "https://openreview.net/forum?id=0DZEs8NpUH",
        "pdf_link": "https://openreview.net/pdf?id=0DZEs8NpUH",
        "keywords": "Personality Alignment, Large language models, behavioral preferences of LM",
        "abstract": "Current methods for aligning large language models (LLMs) typically aim to reflect general human values and behaviors, but they often fail to capture the unique characteristics and preferences of individual users. To address this gap, we introduce the concept of Personality Alignment. This approach tailors LLMs' responses and decisions to match the specific preferences of individual users or closely related groups. Inspired by psychometrics, we created the Personality Alignment with Personality Inventories (PAPI) dataset, which includes data from 300,000 real subjects, each providing behavioral preferences based on the Big Five Personality Factors. This dataset allows us to quantitatively evaluate the extent to which LLMs can align with each subject's behavioral patterns. Recognizing the challenges of personality alignments\u2014such as limited personal data, diverse preferences, and scalability requirements\u2014we developed an activation intervention optimization method. This method enhances LLMs' ability to efficiently align with individual behavioral preferences using minimal data and computational resources. Remarkably, our method, PAS, achieves superior performance while requiring only 1/5 of the optimization time compared to DPO, offering practical value for personality alignment. Our work paves the way for future AI systems to make decisions and reason in truly personality ways, enhancing the relevance and meaning of AI interactions for each user and advancing human-centered artificial intelligence."
    },
    {
        "title": "SQT -- rough conservative actor critic",
        "link_suffix": "/forum?id=hMjUnF3aQ8",
        "link": "https://openreview.net/forum?id=hMjUnF3aQ8",
        "pdf_link": "https://openreview.net/pdf?id=hMjUnF3aQ8",
        "keywords": "Actor Critic, Overestimation Bias",
        "abstract": "Std $Q$-target is a conservative actor critic ensemble based $Q$-learning algorithm which based on a single key $Q$-formula--$Q$-networks standard deviation, an uncertainty penalty. A minimalistic solution to the problem of overestimation bias. We implement SQT on top of actor critic and test it against the SOTA actor critic algorithms on popular MuJoCo tasks. SQT shows a clear performance advantage over TD3, SAC and TD7 on the tested tasks majority."
    },
    {
        "title": "Locking Down the Finetuned LLMs Safety",
        "link_suffix": "/forum?id=YGoFl5KKFc",
        "link": "https://openreview.net/forum?id=YGoFl5KKFc",
        "pdf_link": "https://openreview.net/pdf?id=YGoFl5KKFc",
        "keywords": "large language models, Safety, Finetuned, fast",
        "abstract": "Fine-tuning large language models (LLMs) on additional datasets is often necessary to optimize them for specific downstream tasks. However, existing safety alignment measures, which restrict harmful behavior during inference, are insufficient to mitigate safety risks during fine-tuning. Alarmingly, fine-tuning with just 10 toxic sentences can make models comply with harmful instructions. We introduce SafetyLock, a novel alignment intervention method that maintains robust safety post-fine-tuning through efficient and transferable mechanisms. SafetyLock leverages our discovery that fine-tuned models retain similar safety-related activation representations to their base models. This insight enables us to extract what we term the Meta-SafetyLock, a set of safety bias directions representing key activation patterns associated with safe responses in the original model. We can then apply these directions universally to fine-tuned models to enhance their safety. By searching for activation directions across multiple token dimensions, SafetyLock achieves enhanced robustness and transferability. SafetyLock re-aligns fine-tuned models in under 0.01 seconds without additional computational cost. Our experiments demonstrate that SafetyLock can reduce the harmful instruction response rate from 60% to below 1% in toxic fine-tuned models. It surpasses traditional methods in both performance and efficiency, offering a scalable, non-invasive solution for ensuring the safety of customized LLMs. Our analysis across various fine-tuning scenarios confirms SafetyLock's robustness, advocating its integration into safety protocols for aligned LLMs."
    },
    {
        "title": "Efficient Long-range Language Modeling with Self-supervised Causal Retrieval",
        "link_suffix": "/forum?id=TtVKHxzHCy",
        "link": "https://openreview.net/forum?id=TtVKHxzHCy",
        "pdf_link": "https://openreview.net/pdf?id=TtVKHxzHCy",
        "keywords": "long-range language modeling, Retrieval-based LM, self-supervised learning",
        "abstract": "Recently, retrieval-based language models (RLMs) have received much attention. However, most of them leverage a pre-trained retriever with fixed parameters, which may not adapt well to causal language models. In this work, we propose Grouped Cross-Attention, a novel module enabling joint pre-training of the retriever and causal LM, and apply it to long-context modeling. For a given input sequence, we split it into chunks and use the current chunk to retrieve past chunks for subsequent text generation. \nOur innovation allows the retriever to learn how to retrieve past chunks that better minimize the auto-regressive loss of subsequent tokens in an end-to-end manner.\nBy integrating top-$k$ retrieval, our model can be pre-trained efficiently from scratch with context lengths up to 64K tokens. \nOur experiments show our model, compared with long-range LM baselines, can achieve lower perplexity with comparable or lower pre-training and inference costs."
    },
    {
        "title": "CycleResearcher: Improving Automated Research via Automated Review",
        "link_suffix": "/forum?id=bjcsVLoHYs",
        "link": "https://openreview.net/forum?id=bjcsVLoHYs",
        "pdf_link": "https://openreview.net/pdf?id=bjcsVLoHYs",
        "keywords": "Large Language Models, Automation of Scientific Discovery, AI Scientist",
        "abstract": "The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper revision. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-8k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves a 26.89% improvement in mean absolute error (MAE) over individual human reviewers in predicting paper scores, indicating that LLMs can surpass expert-level performance in research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, surpassing the preprint level of 5.24 from human experts and approaching the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and advancing AI-driven research capabilities."
    },
    {
        "title": "Can Medical Vision-Language Pre-training Succeed with Purely Synthetic Data?",
        "link_suffix": "/forum?id=rawj2PdHBq",
        "link": "https://openreview.net/forum?id=rawj2PdHBq",
        "pdf_link": "https://openreview.net/pdf?id=rawj2PdHBq",
        "keywords": "Medical Vision-Language Pre-training, Synthetic Multimodal Data",
        "abstract": "Medical Vision-Language Pre-training (MedVLP) has made significant progress in enabling zero-shot tasks for medical image understanding. However, training MedVLP models typically requires large-scale datasets with paired, high-quality image-text data, which are scarce in the medical domain. Recent advancements in Large Language Models (LLMs) and diffusion models have made it possible to generate large-scale synthetic image-text pairs. This raises the question:Can MedVLP succeed using purely synthetic data?To address this, we use off-the-shelf generative models to create synthetic radiology reports and paired Chest X-ray (CXR) images, and propose an automated pipeline to build a diverse, high-quality synthetic dataset, enabling a rigorous study that isolates model and training settings, focusing entirely from the data perspective.Our results show that MedVLP models trainedexclusively on synthetic dataoutperform those trained on real data by3.8%in averaged AUC on zero-shot classification. Moreover, using a combination of synthetic and real data leads to a further improvement of9.07%. Additionally, MedVLP models trained on synthetic or mixed data consistently outperform those trained on real data in zero-shot grounding, as well as in fine-tuned classification and segmentation tasks.Our analysis suggests MedVLP trained on well-designed synthetic data can outperform models trained on real datasets, which may be limited by low-quality samples and long-tailed distributions[^1].[^1]: All data and code will be released upon acceptance."
    }
]