[{"title": "Jamba: Hybrid Transformer-Mamba Language Models", "link_suffix": "/forum?id=JFPaD7lpBD", "link": "https://openreview.net/forum?id=JFPaD7lpBD", "pdf_link": "https://openreview.net/pdf?id=JFPaD7lpBD", "keywords": "language models, state-space models, mamba, hybrid architecture, foundation models", "abstract": "We present Jamba, a novel hybrid Transformer-Mamba mixture-of-experts (MoE) architecture. Jamba interleaves blocks of Transformer and Mamba layers, enjoying the benefits of both model families. MoE is added in some of these layers to increase model capacity while keeping active parameter usage manageable. This flexible architecture allows resource- and objective-specific configurations. We implement two configurations: Jamba-1.5-Large, with 94B active parameters, and Jamba-1.5-mini, with 12B active parameters. Built at large scale, Jamba models provide high throughput and small memory footprint compared to vanilla Transformers, especially at long-context tasks, with an effective context length of 256K tokens, the largest amongst open-weight models. At the same time, they are also competitive on standard language modeling and chatbot benchmarks. We study various architectural decisions, such as how to combine Transformer and Mamba layers, and how to mix experts, and show that some of them are crucial in large scale modeling. To support cost-effective inference, we introduce ExpertsInt8, a novel quantization technique that allows fitting Jamba-1.5-Large on a machine with 8 80GB GPUs when processing 256K-token contexts without loss of quality. We also describe several interesting properties of this architecture that the training and evaluation of Jamba have revealed. The model weights are publicly available.", "title_embedding_index": 13650, "title_abs_embedding_index": 13675}, {"title": "Federated Coordination: Private and Distributed Strategy Alignment", "link_suffix": "/forum?id=W9yBCkfWWG", "link": "https://openreview.net/forum?id=W9yBCkfWWG", "pdf_link": "https://openreview.net/pdf?id=W9yBCkfWWG", "keywords": "Coordination; Coordination strategy alignment; Privacy; Distributed", "abstract": "Coordination in multi-agent systems is critical for optimizing collective outcomes and is applicable in diverse fields such as drone swarms, emergency response, and more. Despite extensive research, the distributed coordination strategy alignment problem---where all agents follow the same strategy and execute the prescribed actions without a global coordinator---remains largely unexplored, posing challenges in scalability and privacy preservation. We introduce a new research problem termed ``federated coordination\", which seeks to achieve decentralized strategy alignment across distributed agents while maintaining the privacy of strategy choices. To address this problem, we propose a framework that employs an energy-based model. It facilitates decentralized strategy alignment by associating agent states with coordination strategies through local minimum energy values. We address privacy concerns through a simple yet effective communication protocol that protects strategy selections from eavesdropping and information leakage. Our extensive experimental results validate these contributions, demonstrating scalability and reduced computational demands. This enhances the practicality of coordination systems in multi-agent settings.", "title_embedding_index": 13651, "title_abs_embedding_index": 13676}, {"title": "Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts", "link_suffix": "/forum?id=okRSNTMdFg", "link": "https://openreview.net/forum?id=okRSNTMdFg", "pdf_link": "https://openreview.net/pdf?id=okRSNTMdFg", "keywords": "Meta-Unlearning, Diffusion Models", "abstract": "With the rapid progress of diffusion-based content generation, significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained diffusion models (DMs) to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs torelearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., \"skin\") retained in DMs are related to the unlearned ones (e.g., \"nudity\"), facilitating their relearning via finetuning. To address this, we proposemeta-unlearningon DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered toself-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies.", "title_embedding_index": 13652, "title_abs_embedding_index": 13677}, {"title": "Understanding Impact of Human Feedback via Influence Functions", "link_suffix": "/forum?id=dTQmayPKMs", "link": "https://openreview.net/forum?id=dTQmayPKMs", "pdf_link": "https://openreview.net/pdf?id=dTQmayPKMs", "keywords": "reinforcement learning from human feedback, influence function, reward learning, alignment, scalable oversight", "abstract": "In Reinforcement Learning from Human Feedback (RLHF), it is crucial to learn suitable reward models from human feedback to align large language models (LLMs) with human intentions. However, human feedback can often be noisy, inconsistent, or biased, especially when evaluating complex responses. Such feedback can lead to misaligned reward signals, potentially causing unintended side effects during the RLHF process. To address these challenges, we explore the use of influence functions to measure the impact of human feedback on the performance of reward models. We propose a compute-efficient approximation method that enables the application of influence functions to LLM-based reward models and large-scale preference datasets. In our experiments, we demonstrate two key applications of influence functions: (1) detecting common forms of labeler bias in human feedback datasets and (2) guiding labelers to refine their strategies to align more closely with expert feedback. By quantifying the impact of human feedback on reward models, we believe that influence functions can enhance feedback interpretability and contribute to scalable oversight in RLHF, helping labelers provide more accurate and consistent feedback.", "title_embedding_index": 13653, "title_abs_embedding_index": 13678}, {"title": "Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions", "link_suffix": "/forum?id=IIVYiJ1ggK", "link": "https://openreview.net/forum?id=IIVYiJ1ggK", "pdf_link": "https://openreview.net/pdf?id=IIVYiJ1ggK", "keywords": "Large Language Models, Fixed-Size Hidden States, Data-Dependent Tempered Selection, Sliding Window Shared-Key Attention", "abstract": "Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints will be available upon publication.", "title_embedding_index": 13654, "title_abs_embedding_index": 13679}, {"title": "KinFormer: Generalizable Dynamical Symbolic Regression for catalytic organic Reaction Kinetics", "link_suffix": "/forum?id=nhrXqy5d5q", "link": "https://openreview.net/forum?id=nhrXqy5d5q", "pdf_link": "https://openreview.net/pdf?id=nhrXqy5d5q", "keywords": "Kinetic Equation Prediction, Dynamical Symbolic Regression, Transform; Conditional Strategy, MCTS", "abstract": "Modeling kinetic equations is essential for understanding the mechanisms of chemical reactions, yet a complex and time-consuming task.  Kinetic equation prediction is formulated as a problem of dynamical symbolic regression (DSR) subject to physical chemistry constraints. Deep learning (DL) holds the potential to capture reaction patterns and predict kinetic equations from data of chemical species, effectively avoiding empirical bias and improving efficiency compared with traditional analytical methods. Despite numerous studies focusing on DSR and the introduction of Transformers to predict ordinary differential equations, the corresponding models lack generalization abilities across diverse categories of reactions. In this study, we propose KinFormer, a generalizable kinetic equation prediction model. KinFormer utilizes a conditional Transformer to model DSR under physical constraints and employs Monte Carlo Tree Search to apply the model to new types of reactions. Experimental results on 20 types of organic reactions demonstrate that KinFormer not only outperforms classical baselines, but also  exceeds Transformer baselines in out-of-domain evaluations, thereby proving its generalization ability.", "title_embedding_index": 13655, "title_abs_embedding_index": 13680}, {"title": "IDIV: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations", "link_suffix": "/forum?id=uuef1HP6X7", "link": "https://openreview.net/forum?id=uuef1HP6X7", "pdf_link": "https://openreview.net/pdf?id=uuef1HP6X7", "keywords": "inverse rendering, diffusion models, intrinsic decomposition", "abstract": "Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency.\nIn this paper, we introduce IDVI, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves highly accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training.\nExtensive experiments demonstrate that IDVI outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applicability in realistic 3D content creation.", "title_embedding_index": 13656, "title_abs_embedding_index": 13681}, {"title": "Learning 3D Perception from Others' Predictions", "link_suffix": "/forum?id=Ylk98vWQuQ", "link": "https://openreview.net/forum?id=Ylk98vWQuQ", "pdf_link": "https://openreview.net/pdf?id=Ylk98vWQuQ", "keywords": "3D object detection, autonomous driving, label-efficient learning, domain adaptation, curriculum learning, collaborative perception", "abstract": "Accurate 3D object detection in real-world environments requires a huge amount of annotated data with high quality. Acquiring such data is tedious and expensive, and often needs repeated effort when a new sensor is adopted or when the detector is deployed in a new environment. We investigate a new scenario to construct 3D object detectors:learning from the predictions of a nearby unit that is equipped with an accurate detector.For example, when a self-driving car enters a new area, it may learn from other traffic participants whose detectors have been optimized for that area. This setting is label-efficient, sensor-agnostic, and communication-efficient: nearby units only need to share the predictions with the ego agent (e.g., car). Naively using the received predictions as ground-truths to train the detector for the ego car, however, leads to inferior performance. We systematically study the problem and identify viewpoint mismatches and mislocalization (due to synchronization and GPS errors) as the main causes, which unavoidably result in false positives, false negatives, and inaccurate pseudo labels. We propose a distance-based curriculum, first learning from closer units with similar viewpoints and subsequently improving the quality of other units' predictions via self-training. We further demonstrate that an effective pseudo label refinement module can be trained with a handful of annotated data, largely reducing the data quantity necessary to train an object detector. We validate our approach on the recently released real-world collaborative driving dataset, using reference cars' predictions as pseudo labels for the ego car. Extensive experiments including several scenarios (e.g., different sensors, detectors, and domains) demonstrate the effectiveness of our approach toward label-efficient learning of 3D perception from other units' predictions.", "title_embedding_index": 13657, "title_abs_embedding_index": 13682}, {"title": "Contrastive guidance and feedback: A Suitable way to improve 3D Consistency of Multi-view Diffusion Model", "link_suffix": "/forum?id=ZBH4fqQwJQ", "link": "https://openreview.net/forum?id=ZBH4fqQwJQ", "pdf_link": "https://openreview.net/pdf?id=ZBH4fqQwJQ", "keywords": "Contrastive guidance, Multi-view diffusion model", "abstract": "Recently, diffusion models have shown potential in 3D generation tasks, and the novel view synthesis (NVS) task, a bridge between 2D and 3D generation, has received great attention. The goal of the NVS task is to generate multi-view images from reference images, and the core challenge is to maintain the 3D consistency between different view images.\nRecent works construct large 3D consistency multi-view image datasets and utilize the supervised fine-tuning (SFT) method to improve the 3D consistency. However, the SFT method suffers from the distribution shift, data inefficient problems, and lacks theoretical insight. To solve these problems, we discuss how to provide a suitable direction to the multi-view models and achieve better performance. More specifically, we first analyze the training-free guidance-based method and prove that contrastive guidance, which contains ground-truth and generated samples, can provide the right direction to improve 3D consistency. Based on the theoretical insight, we further design a contrastive 3D consistency metric and use it as the feedback in the following phase. To avoid the distribution shift problem, we use direct preference optimization (DPO) to fine-tune the multi-view diffusion models. Through qualitative and quantitative experiments, we demonstrate that after the fine-tuning phase with the above method, the 3D consistency of the multi-view images is significantly improved and achieves better performance compared to the SFT method.", "title_embedding_index": 13658, "title_abs_embedding_index": 13683}, {"title": "Linear Attention Sequence Parallelism", "link_suffix": "/forum?id=oVnfVnwh6y", "link": "https://openreview.net/forum?id=oVnfVnwh6y", "pdf_link": "https://openreview.net/pdf?id=oVnfVnwh6y", "keywords": "sequence parallelism, distributed training", "abstract": "Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$ longer than existing SP methods.", "title_embedding_index": 13659, "title_abs_embedding_index": 13684}, {"title": "Guided Reinforcement Learning with Roll-Back", "link_suffix": "/forum?id=5s1qpjrNvZ", "link": "https://openreview.net/forum?id=5s1qpjrNvZ", "pdf_link": "https://openreview.net/pdf?id=5s1qpjrNvZ", "keywords": "reinforcement learning, guide policy, warm-start", "abstract": "Reinforcement learning-based solutions are increasingly being considered as strong alternatives to classical  system controllers, despite their significant sample inefficiency when learning controller tasks from scratch. Many methods that address this issue use prior task knowledge to guide the agent's learning, with several recent algorithms providing a guide policy that is sometimes chosen to execute actions instead of the learner policy. While this approach lends excellent flexibility as it allows the guide knowledge to be provided in any format, it can be challenging to decide when and for how long to use the guide agent. Current guide policy-based approaches typically choose a static guide sampling rate empirically, and do not vary it. Approaches that  transfer control use simple methods like linear decay, or require hyperparameter choices that strongly impact the performance. We show that under certain assumptions, the sampling rate of the guide policy can be calculated to guarantee that the mean return of the learning policy will surpass a user-defined performance degradation threshold. To the best of our knowledge, this is the first time a performance guarantee has been established for a\nguided RL method. We then implement a guided RL (GRL) algorithm that can make use of this sample rate, and additionally introduce a roll-back feature in guided RL with roll-back (GRL-RB) to adaptively balance the trade-off between performance degradation and rapid transfer of control to the learner. Our approach is simple to implement on top of existing algorithms, robust to hyperparameter choices, and effective in warm-starting online learning.", "title_embedding_index": 13660, "title_abs_embedding_index": 13685}, {"title": "MobileSafetyBench: Evaluating Safety of Autonomous Agents in Mobile Device Control", "link_suffix": "/forum?id=lpBzjYlt3u", "link": "https://openreview.net/forum?id=lpBzjYlt3u", "pdf_link": "https://openreview.net/pdf?id=lpBzjYlt3u", "keywords": "Safety, Digital device control, Autonomous agent, Language model agent, Evaluation", "abstract": "Autonomous agents powered by large language models (LLMs) show promising potential in assistive tasks across various domains, including mobile device control. As these agents interact directly with personal information and device settings, ensuring their safe and reliable behavior is crucial to prevent undesirable outcomes. However, no benchmark exists for standardized evaluation of the safety of mobile device-control agents. In this work, we introduce MobileSafetyBench, a benchmark designed to evaluate the safety of device-control agents within a realistic mobile environment based on Android emulators. We develop a diverse set of tasks involving interactions with various mobile applications, including messaging and banking applications. To clearly evaluate safety apart from general capabilities, we design separate tasks measuring safety and tasks evaluating helpfulness. The safety tasks challenge agents with managing potential risks prevalent in daily life and include tests to evaluate robustness against indirect prompt injections. Our experiments demonstrate that while baseline agents, based on state-of-the-art LLMs, perform well in executing helpful tasks, they show poor performance in safety tasks. To mitigate these safety concerns, we propose a prompting method that encourages agents to prioritize safety considerations. While this method shows promise in promoting safer behaviors, there is still considerable room for improvement to fully earn user trust. This highlights the urgent need for continued research to develop more robust safety mechanisms in mobile environments.", "title_embedding_index": 13661, "title_abs_embedding_index": 13686}, {"title": "Population Transformer: Learning Population-level Representations of Neural Activity", "link_suffix": "/forum?id=FVuqJt3c4L", "link": "https://openreview.net/forum?id=FVuqJt3c4L", "pdf_link": "https://openreview.net/pdf?id=FVuqJt3c4L", "keywords": "representation learning, neuroscience, self supervised learning", "abstract": "We present a self-supervised framework that learns population-level codes for arbitrary ensembles of neural recordings at scale. We address two key challenges in scaling models with neural time-series data: sparse and variable electrode distribution across subjects and datasets. The Population Transformer (PopT) stacks on top of pretrained representations and enhances downstream decoding by enabling learned aggregation of multiple spatially-sparse data channels. The pretrained PopT lowers the amount of data required for downstream decoding experiments, while increasing accuracy, even on held-out subjects and tasks. Compared to end-to-end methods, this approach is computationally lightweight and more interpretable, while still retaining competitive performance. We further show how our framework is generalizable to multiple time-series embeddings and neural data modalities. Beyond decoding, we interpret the pretrained PopT and fine-tuned models to show how they can be used to extract neuroscience insights from massive amounts of data. We release our code as well as a pretrained PopT to enable off-the-shelf improvements in multi-channel intracranial data decoding and interpretability.", "title_embedding_index": 13662, "title_abs_embedding_index": 13687}, {"title": "GRACE: Towards Realistic Multimodal Single-Cell Data Matching", "link_suffix": "/forum?id=khsBg6Cl7s", "link": "https://openreview.net/forum?id=khsBg6Cl7s", "pdf_link": "https://openreview.net/pdf?id=khsBg6Cl7s", "keywords": "Single-cell data analysis, Multimodal learning, AI for science", "abstract": "Single-cell multi-omics technologies (e.g., scRNA-seq and scATAC-seq data) have provided more comprehensive insights for understanding cellular conditions and activities in recent years. However, multimodal representation learning for omics data remains a challenging problem due to heterogeneous relationships and label scarcity in reality. In this work, we propose a novel approach named Geometric Relation Exploration with Cross-modal Supervision (GRACE) for realistic multimodal single-cell matching. In particular, we map both multimodal data into a shared embedding space by maximizing the log-likelihood of ZINB distributions. To reduce the semantic gap between multimodal data, we construct a geometric graph using mutual nearest neighbors to indicate cross-modal relations between samples for distribution alignment. Furthermore, to extract most pairwise information, we explore high-order relations in the geometric graph, which would be incorporated into a meta-learning paradigm for robust optimization. In addition, to further mitigate label scarcity, we introduce a non-parametric way to generate label vectors for unlabeled data for cross-modal supervision across different modalities. Extensive experiments on several benchmark datasets validate the superiority of the proposed GRACE compared to various baselines. In general, compared to the second-best method, GRACE exhibits an average performance improvement of 6.71% and 14.17% for the R2A task and A2R task, respectively.", "title_embedding_index": 13663, "title_abs_embedding_index": 13688}, {"title": "SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding", "link_suffix": "/forum?id=8dzKkeWUUb", "link": "https://openreview.net/forum?id=8dzKkeWUUb", "pdf_link": "https://openreview.net/pdf?id=8dzKkeWUUb", "keywords": "Large Language Model, Pre-training, Supervised Fine-tuning, Scientific Literature Understanding", "abstract": "Scientific literature understanding is crucial for extracting targeted information and garnering insights, thereby significantly advancing scientific discovery.\nDespite the remarkable success of Large Language Models (LLMs), they face challenges in scientific literature understanding, primarily due to (1) a lack of scientific knowledge and (2) unfamiliarity with specialized scientific tasks.\nTo develop an LLM specialized in scientific literature understanding, we propose a hybrid strategy that integrates continual pre-training (CPT) and supervised fine-tuning (SFT), to simultaneously infuse scientific domain knowledge and enhance instruction-following capabilities for domain-specific tasks.\nIn this process, we identify two key challenges: (1) constructing high-quality CPT corpora, and (2) generating diverse SFT instructions. \nWe address these challenges through a meticulous pipeline, including PDF text extraction, parsing content error correction, quality filtering, and synthetic instruction creation.\nApplying this strategy, we present a suite of LLMs: SciLitLLM, specialized in scientific literature understanding.\nThese models demonstrate promising performance on scientific literature understanding benchmarks.\n(1) We present an effective framework that integrates CPT and SFT to adapt LLMs to scientific literature understanding, which can also be easily adapted to other domains.\n(2) We propose an LLM-based synthesis method to generate diverse and high-quality scientific instructions, resulting in a new instruction set -- SciLitIns -- for less-represented scientific domains. \n(3) SciLitLLM achieves promising performance in scientific literature understanding benchmarks.", "title_embedding_index": 13664, "title_abs_embedding_index": 13689}, {"title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks", "link_suffix": "/forum?id=TE0KOzWYAF", "link": "https://openreview.net/forum?id=TE0KOzWYAF", "pdf_link": "https://openreview.net/pdf?id=TE0KOzWYAF", "keywords": "Vision Language Model, Representation Learning, Multimodal Embeddings", "abstract": "Embedding models have been crucial in enabling various downstream tasks such as semantic similarity, information retrieval, and clustering. Recently, there has been a surge of interest in developing universal text embedding models that can generalize across tasks (e.g., MTEB). However, progress in learning universal multimodal embedding models has been relatively slow despite their importance. In this work, we aim to explore the potential for building universal embeddings capable of handling a wide range of downstream tasks. Our contributions are twofold: (1) MMEB (Massive Multimodal Embedding Benchmark), which covers 4 meta-tasks and 36 datasets, including 20 training and 16 evaluation datasets, and (2) VLM2VEC (Vision-Language Model \u2192 Vector), a contrastive training framework that converts any state-of-the-art vision-language model into an embedding model. Unlike previous models such as CLIP and BLIP, VLM2VEC can process any combination of images and text to generate a fixed-dimensional vector based on task instructions. We build a series of VLM2VEC models on Phi-3.5-V and evaluate them on MMEB. Our results show that VLM2VEC achieves an absolute average improvement of 10% to 20% over existing multimodal embedding models on both in-distribution and out-of-distribution datasets in MMEB.", "title_embedding_index": 13665, "title_abs_embedding_index": 13690}, {"title": "A Generalist Intracortical Motor Decoder", "link_suffix": "/forum?id=ONOe6cAE9I", "link": "https://openreview.net/forum?id=ONOe6cAE9I", "pdf_link": "https://openreview.net/pdf?id=ONOe6cAE9I", "keywords": "Neuroscience, Foundation Model, Motor Decoding, BCI", "abstract": "Mapping the relationship between neural activity and motor behavior is a central aim of sensorimotor neuroscience and neurotechnology. \nMost progress to this end has relied on restricting complexity: studying specific simple behaviors, in limited subjects, with interpretable computational models. However, current trends in deep learning suggest that modeling a breadth of neural and behavioral data may be both possible and beneficial. We accordingly developed Neural Data Transformer 3 (NDT3) as a foundation model for motor decoding of neural data from intracortical microelectrodes. We pretrained NDT3 with 2000 hours of neural population spiking activity paired with diverse motor covariates from over 30 monkeys and humans from 10 labs. Pretrained NDT3 is broadly useful, benefiting decoding on 8 downstream decoding tasks and generalizing to a variety of neural distribution shifts. However, we find signs that scaling over diverse neural datasets may be challenging, as scaling from 200 to 2000 hours already requires increasing model size to 350M parameters to avoid model degradation, and several downstream datasets scarcely benefit from either data or model scale. We provide two demonstrations that this scaling is at least partially limited by variability in input and output spaces across neural datasets, which pretraining alone may not resolve.", "title_embedding_index": 13666, "title_abs_embedding_index": 13691}, {"title": "UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing", "link_suffix": "/forum?id=Nifg2fQMGW", "link": "https://openreview.net/forum?id=Nifg2fQMGW", "pdf_link": "https://openreview.net/pdf?id=Nifg2fQMGW", "keywords": "Diffusion Model, Video Editing", "abstract": "Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework.\nTo realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.", "title_embedding_index": 13667, "title_abs_embedding_index": 13692}, {"title": "Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation", "link_suffix": "/forum?id=agbiPPuSeQ", "link": "https://openreview.net/forum?id=agbiPPuSeQ", "pdf_link": "https://openreview.net/pdf?id=agbiPPuSeQ", "keywords": "Diffusion models, Multi-modal data, Audio-visual generative models", "abstract": "In this study, we aim to construct an audio-video generative model with minimal computational cost by leveraging pre-trained single-modal generative models for audio and video.\nTo achieve this, we propose a novel method that guides single-modal models to cooperatively generate well-aligned samples across modalities. \nSpecifically, given two pre-trained base diffusion models, we train a lightweight joint guidance module to adjust scores separately estimated by the base models to match the score of joint distribution over audio and video. \nWe show that this guidance can be computed through the gradient of the optimal discriminator distinguishing real audio-video pairs from the fake ones independently generated by the base models. \nOn the basis of this analysis, we construct a joint guidance module by training this discriminator.\nAdditionally, we adopt a loss function to make the gradient of the discriminator work as a noise estimator, as in standard diffusion models, stabilizing the gradient of the discriminator. \nEmpirical evaluations on several benchmark datasets demonstrate that our method improves both single-modal fidelity and multi-modal alignment with a relatively small number of parameters.", "title_embedding_index": 13668, "title_abs_embedding_index": 13693}, {"title": "Test Time Augmentations are Worth One Million Images for Out-of-Distribution Detection", "link_suffix": "/forum?id=NxsTjmRAzA", "link": "https://openreview.net/forum?id=NxsTjmRAzA", "pdf_link": "https://openreview.net/pdf?id=NxsTjmRAzA", "keywords": "Out-of-distribution, Test time augmentation, OOD Detection", "abstract": "Out-of-distribution (OOD) detection is a major threat for deploying machine learning models in safety-critical scenarios. Data augmentations have been proven to be beneficial to OOD detection by providing diverse features. However, previous methods have only focused on the role of data augmentation in the training phase, overlooking its impact on the testing phase. In this paper, we present the first comprehensive study of the impact of test-time augmentation (TTA) on OOD detection. We find aggressive TTAs can cause distribution shifts on OOD scores of In-distribution (InD) data, whereas mild TTAs do not, resulting in the effectiveness of mild TTAs on OOD Detection. Based on the above observations, we propose a detection method that performs a K-nearest-neighbor (KNN) search on mild TTAs instead of InD data. With only 25 TTAs, our method outperforms state-of-the-art methods using the entire training set (1.2 million images) on IMAGENET for OOD detection. Moreover, our approach is compatible with various model architectures and robust to adversarial examples.", "title_embedding_index": 13669, "title_abs_embedding_index": 13694}, {"title": "CADO: Cost-Aware Diffusion Models for Combinatorial Optimization via RL Fine-tuning", "link_suffix": "/forum?id=pbDqZBn2X2", "link": "https://openreview.net/forum?id=pbDqZBn2X2", "pdf_link": "https://openreview.net/pdf?id=pbDqZBn2X2", "keywords": "Combinatorial Optimization, Diffusion Model, RL finetuning", "abstract": "Recent advancements in Machine Learning (ML) have demonstrated significant potential in addressing Combinatorial Optimization (CO) problems through data-driven approaches. Heatmap-based methods, which generate solution heatmaps in a single step and employ an additional decoder to derive solutions for CO tasks, have shown promise due to their scalability for large-scale problems. Traditionally, these complex models are trained using imitation learning with optimal solutions, often leveraging diffusion models. However, our research has identified several limitations inherent in these imitation learning approaches within the context of CO tasks. To overcome these challenges, we propose a 2-phase training framework for diffusion models in CO, incorporating Reinforcement Learning (RL) fine-tuning. Our methodology integrates cost information and the post-process decoder into the training process, thereby enhancing the solver's capacity to generate effective solutions. We conducted extensive experiments on well-studied combinatorial optimization problems, specifically the Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS), ranging from small-scale instances to large-scale scenarios. The results demonstrate the significant efficacy of our RL fine-tuning framework, surpassing previous state-of-the-art methods in performance.", "title_embedding_index": 13670, "title_abs_embedding_index": 13695}, {"title": "Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions", "link_suffix": "/forum?id=rCGleSgNBK", "link": "https://openreview.net/forum?id=rCGleSgNBK", "pdf_link": "https://openreview.net/pdf?id=rCGleSgNBK", "keywords": "Diffusion models", "abstract": "Diffusion probabilistic models (DPMs), while effective in generating high-quality samples, often suffer from high computational costs due to the iterative sampling process. To address this, we propose an enhanced ODE-based sampling method for DPMs inspired by Richardson extrapolation, which has been shown to reduce numerical error and improve convergence rates. Our method, termed RX-DPM, utilizes numerical solutions obtained over multiple denoising steps, leveraging the multiple ODE solutions to extrapolate the denoised prediction in DPMs. This signi\ufb01cantly enhances the accuracy of estimations for the \ufb01nal sample while preserving the number of function evaluations (NFEs). Unlike standard Richardson extrapolation, which assumes uniform discretization of the time grid, we have developed a more general formulation tailored to arbitrary time step scheduling, guided by the local truncation error derived from a baseline sampling method. The simplicity of our approach facilitates accurate estimation of numerical solutions without additional computational overhead, and allows for seamless and convenient integration into various DPMs and solvers. Additionally, RX-DPM provides explicit error estimates, effectively illustrating the faster convergence achieved as the order of the leading error term increases. Through a series of experiments, we demonstrate that the proposed method effectively enhances the quality of generated samples without requiring additional sampling iterations.", "title_embedding_index": 13671, "title_abs_embedding_index": 13696}, {"title": "High-dimensional Asymptotics of VAEs: Threshold of Posterior Collapse and Dataset-Size Dependence of Rate-Distortion Curve", "link_suffix": "/forum?id=BdPbmgJ2jo", "link": "https://openreview.net/forum?id=BdPbmgJ2jo", "pdf_link": "https://openreview.net/pdf?id=BdPbmgJ2jo", "keywords": "statistical physics, replica method, variational autoencoder, exact asymptotics", "abstract": "In variational autoencoders (VAEs), the variational posterior often aligns closely with the prior, known as posterior collapse, which leads to poor representation learning quality. An adjustable hyperparameter beta has been introduced in VAE to address this issue. This study sharply evaluates the conditions under which the posterior collapse occurs with respect to beta and dataset size by analyzing a minimal VAE in a high-dimensional limit. Additionally, this setting enables the evaluation of the rate-distortion curve in the VAE. This result shows that, unlike typical regularization parameters, VAEs face \"inevitable posterior collapse\" beyond a certain beta threshold, regardless of dataset size. The dataset-size dependence of the derived rate-distortion curve also suggests that relatively large datasets are required to achieve a rate-distortion curve with high rates. These results robustly explain generalization behavior across various real datasets with highly non-linear VAEs.", "title_embedding_index": 13672, "title_abs_embedding_index": 13697}, {"title": "FusionBench: A Comprehensive Benchmark of Deep Model Fusion", "link_suffix": "/forum?id=a0sK0foX3p", "link": "https://openreview.net/forum?id=a0sK0foX3p", "pdf_link": "https://openreview.net/pdf?id=a0sK0foX3p", "keywords": "model fusion, model ensemble, model merging, model mixing, multi-task learning, knowledge transfer", "abstract": "Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single model in a cost-effective and data-efficient manner. This enables the unified model to take advantage of the original models' strengths, potentially exceeding their performance. Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness against distribution shifts. To address this issue, we introduce FusionBench, which is the first comprehensive benchmark dedicated to deep model fusion. FusionBench covers a wide range of tasks, including open-vocabulary image classification, text classification, and text-to-text generation. Each category includes up to eight tasks with corresponding task-specific models, featuring both full fine-tuning and LoRA fine-tuning, as well as models of different sizes, to ensure fair and balanced comparisons of various multi-task model fusion techniques across different tasks, model scales, and fine-tuning strategies. We implement and evaluate a broad spectrum of deep model fusion techniques. These techniques range from model ensemble methods, which combine the predictions to improve the overall performance, to model merging, which integrates different models into a single one, and model mixing methods, which upscale or recombine the components of the original models. FusionBench now contains 26 distinct tasks, 74 fine-tuned models, and 19 fusion techniques, and we are committed to consistently expanding the benchmark with more tasks, models, and fusion techniques. In addition, we offer a well-documented set of resources and guidelines to aid researchers in understanding and replicating the benchmark results. This includes detailed documentation, code examples, and tutorials, making FusionBench a user-friendly and accessible platform for both beginners and experienced researchers.", "title_embedding_index": 13673, "title_abs_embedding_index": 13698}, {"title": "Open-vocabulary Multimodal Emotion Recognition: Dataset, Metric, and Benchmark", "link_suffix": "/forum?id=f1uXrAjpOH", "link": "https://openreview.net/forum?id=f1uXrAjpOH", "pdf_link": "https://openreview.net/pdf?id=f1uXrAjpOH", "keywords": "Open-vocabulary Multimodal Emotion Recognition, Dataset, Metric, Benchmark", "abstract": "Multimodal Emotion Recognition (MER) is an important research topic. This paper advocates for a transformative paradigm in MER. The rationale behind our work is that current approaches often rely on a limited set of basic emotion labels, which do not adequately represent the rich spectrum of human emotions. These traditional and overly simplistic emotion categories fail to capture the inherent complexity and subtlety of human emotional experiences, leading to limited generalizability and practicality. Therefore, we propose a new MER paradigm called Open-vocabulary MER (OV-MER), which encompasses a broader range of emotion labels to reflect the richness of human emotions. This paradigm relaxes the label space, allowing for the prediction of arbitrary numbers and categories of emotions. To support this transition, we provide a comprehensive solution that includes a newly constructed database based on LLM and human collaborative annotations, along with corresponding metrics and a series of benchmarks. We hope this work advances emotion recognition from basic emotions to more nuanced emotions, contributing to the development of emotional AI.", "title_embedding_index": 13674, "title_abs_embedding_index": 13699}]