[{"title": "Longitudinal Latent Diffusion Models", "link_suffix": "/forum?id=62DvfHFesc", "link": "https://openreview.net/forum?id=62DvfHFesc", "pdf_link": "https://openreview.net/pdf?id=62DvfHFesc", "keywords": "generative AI, high-dimensional data, longitudinal data, diffusion models, variational autoencoders, latent representations", "abstract": "Longitudinal data are crucial in several fields, but collecting them is a challenging process, often hindered by concerns such as individual privacy. Extrapolating in time initial trajectories or generating fully synthetic sequences could address these issues and prove valuable in clinical trials, drug design, and even public policy evaluation. We propose a generative statistical model for longitudinal data that links the temporal dependence of a sequence to a latent diffusion model and leverages the geometry of the autoencoder latent space. This versatile method can be used for several tasks - prediction, generation, oversampling - effectively handling high-dimensional data such as images and irregularly-measured sequences, needing only relatively few training samples. Thanks to its ability to generate sequences with controlled variability, it outperforms previously proposed methods on datasets of varying complexity, while remaining interpretable.", "title_embedding_index": 19300, "title_abs_embedding_index": 19325}, {"title": "Enforcing 3D Topological Constraints in Composite Objects via Implicit Functions", "link_suffix": "/forum?id=4b1cJHn7q5", "link": "https://openreview.net/forum?id=4b1cJHn7q5", "pdf_link": "https://openreview.net/pdf?id=4b1cJHn7q5", "keywords": "Topology; 3D Reconstruction; Implicit functions; Composite Objects", "abstract": "Medical applications often require accurate 3D representations of complex organs with multiple parts, such as the heart and spine. Their individual parts must adhere to specific topological constraints to ensure proper functionality. Yet,  there are very few mechanisms in the deep learning literature to achieve this goal.This paper introduces a novel approach to enforce topological constraints in 3D object reconstruction using deep implicit signed distance functions. Our method focuses on heart and spine reconstruction but is generalizable to other applications. We propose a sampling-based technique that effectively checks and enforces topological constraints between 3D shapes by evaluating signed distances at randomly sampled points throughout the volume. We demonstrate it by refining 3D segmentations obtained from the nn-UNet architecture.", "title_embedding_index": 19301, "title_abs_embedding_index": 19326}, {"title": "REVISITING MULTI-PERMUTATION EQUIVARIANCE THROUGH THE LENS OF IRREDUCIBLE REPRESENTATIONS", "link_suffix": "/forum?id=4v4nmYWzBa", "link": "https://openreview.net/forum?id=4v4nmYWzBa", "pdf_link": "https://openreview.net/pdf?id=4v4nmYWzBa", "keywords": "deep weight spaces, permutation equivariance, irredicible representations.", "abstract": "This paper explores the characterization of equivariant linear layers for representations of permutations and related groups. Unlike traditional approaches,\nwhich address these problems using parameter-sharing, we consider an alternative\nmethodology based on irreducible representations and Schur\u2019s lemma. Using this\nmethodology, we obtain an alternative derivation for existing models like DeepSets,\n2-IGN graph equivariant networks, and Deep Weight Space (DWS) networks. The\nderivation for DWS networks is significantly simpler than that of previous results.\nNext, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant layers\nare Siamese. In contrast, we give a full characterization of layers in this case and\nshow that there is a vast number of additional non-Siamese layers in some settings.\nWe also show empirically that these additional non-Siamese layers can improve\nperformance in tasks like graph anomaly detection, weight space alignment, and\nlearning Wasserstein distances.", "title_embedding_index": 19302, "title_abs_embedding_index": 19327}, {"title": "FSW-GNN: A Bi-Lipschitz WL-Equivalent Graph Neural Network", "link_suffix": "/forum?id=d9BMHLXPrr", "link": "https://openreview.net/forum?id=d9BMHLXPrr", "pdf_link": "https://openreview.net/pdf?id=d9BMHLXPrr", "keywords": "Bi-Lipschitzness, Weisfeiler Leman, Graph Embedding, Graph Neural Network, MPNN", "abstract": "Many of the most popular graph neural networks fall into the category of Message Passing Neural Networks (MPNNs). Famously,  MPNNs ability to distinguish between graphs is limited to graphs separable  by the   Weisfeiler-Leman (WL) graph isomorphism test, and the strongest MPNNs, in terms of separation power, are those which are WL-equivalent.Recently, it was shown that the quality of separation provided by standard WL-equivalent MPNN can be very low, resulting in WL-separable graphs being mapped to very similar, hardly distinguishable features.This paper addresses this issue by seeking bi-Lipschitz continuity guarantees for MPNNs. We demonstrate that, in contrast with standard summation-based MPNNs, which lack bi-Lipschitz properties, our proposed model provides a bi-Lipschitz graph embedding with respect to two standard graph metrics. Empirically, we show that our MPNN is competitive with standard MPNNs for several graph learning tasks, and is far more accurate on long-range tasks.", "title_embedding_index": 19303, "title_abs_embedding_index": 19328}, {"title": "On the Expressive Power of Sparse Geometric MPNNs", "link_suffix": "/forum?id=NY7aEek0mi", "link": "https://openreview.net/forum?id=NY7aEek0mi", "pdf_link": "https://openreview.net/pdf?id=NY7aEek0mi", "keywords": "WL, GWL, GNNS, geometric GNNS, point clouds, completness", "abstract": "Motivated by applications in chemistry and other sciences, we study the expressive\npower of message-passing neural networks for geometric graphs, whose node\nfeatures correspond to 3-dimensional positions. Recent work has shown that such\nmodels can separate generic pairs of non-isomorphic geometric graphs, though they\nmay fail to separate some rare and complicated instances. However, these results\nassume a fully connected graph, where each node possesses complete knowledge\nof all other nodes. In contrast, often, in application, every node only possesses\nknowledge of a small number of nearest neighbors.\nThis paper shows that generic pairs of non-isomorphic geometric graphs can\nbe separated by message-passing networks with rotation equivariant features as\nlong as the underlying graph is connected. When only invariant intermediate\nfeatures are allowed, generic separation is guaranteed for generically globally\nrigid graphs. We introduce a simple architecture, EGENNET, which achieves our\ntheoretical guarantees and compares favorably with alternative architecture on\nsynthetic and chemical benchmarks", "title_embedding_index": 19304, "title_abs_embedding_index": 19329}, {"title": "L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large Language Models", "link_suffix": "/forum?id=KJzz4UwqTb", "link": "https://openreview.net/forum?id=KJzz4UwqTb", "pdf_link": "https://openreview.net/pdf?id=KJzz4UwqTb", "keywords": "LLM, Fine-tuning, Quantization, PEFT, LoRA", "abstract": "Due to the high memory and computational costs associated with large language models (LLMs), model compression techniques such as quantization, which reduces inference costs, and parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaptation (LoRA), which reduce training costs, have gained significant popularity. This trend has spurred active research into quantization-aware PEFT techniques, aimed at maintaining model accuracy while minimizing memory overhead during both inference and training.\nPrevious quantization-aware PEFT methods typically follow a two-step approach: first, applying post-training quantization (PTQ) to model weights, followed by PEFT on the quantized model. However, recovering from the quantization error introduced by PTQ through fine-tuning has proven challenging. Additionally, most PTQ-based PEFT methods result in a mixture of low-precision quantized weights and high-precision adapter weights, limiting the efficiency of full quantization during inference.\nWhile a previous method attempted to address these issues, it still suffers from limited adaptability due to the constrained LoRA parameter structure required to produce fully-quantized models. To overcome these challenges, we propose L4Q, a method that integrates Quantization-Aware Training (QAT) with LoRA to effectively reduce quantization error.\n%, which effectively reduces quantization error, with LoRA.\nBy employing a memory-optimized layer design, L4Q significantly reduces QAT\u2019s memory overhead while producing fully-quantized weights, enabling effective adaptation to downstream tasks. Our experiments demonstrate that this combined approach to quantization and fine-tuning achieves superior accuracy compared to decoupled fine-tuning schemes, particularly in sub-4-bit quantization, positioning L4Q as an efficient QAT solution. Using the LLaMA model families and instructional datasets, we showcase L4Q\u2019s capabilities in language tasks and few-shot learning.", "title_embedding_index": 19305, "title_abs_embedding_index": 19330}, {"title": "Probabilistic Feature Smoothed Gaussian Process For Imbalanced Regression", "link_suffix": "/forum?id=V1MDIFbqCp", "link": "https://openreview.net/forum?id=V1MDIFbqCp", "pdf_link": "https://openreview.net/pdf?id=V1MDIFbqCp", "keywords": "Imbalanced Learning, Gaussian Process, Bayesian Methods, Machine Learning", "abstract": "Gaussian Processes (GPs) are non-parametric Bayesian models widely used for regression, classification, and other tasks due to their explainability and versatility. However, GPs face challenges in imbalanced regression, where the skewed distribution of target labels can greatly harm models' performances. In this work, we introduce the Probabilistic Feature Smoothed Partially Independent Training Conditional Approximation (PFS-PITC) to enhance GP performance in imbalanced scenarios. We extract statistical features from the observation space using equidistant label intervals and apply kernel smoothing to address sampling density discontinuities. This process enables PFS-PITC to utilize information from nearby labels within imbalanced datasets, thereby reducing GPs' sensitivity to such imbalances. Empirical tests on various imbalanced regression datasets demonstrate the effectiveness of PFS-PITC, contributing to the robustness of GPs in handling flawed real-world data and expanding their applicability in challenging data processing tasks.", "title_embedding_index": 19306, "title_abs_embedding_index": 19331}, {"title": "On the Convergence of FedProx with Extrapolation and Inexact Prox", "link_suffix": "/forum?id=FQc7gi8XvS", "link": "https://openreview.net/forum?id=FQc7gi8XvS", "pdf_link": "https://openreview.net/pdf?id=FQc7gi8XvS", "keywords": "Federated Learning, Optimization", "abstract": "Enhancing the FedProx federated learning algorithm (Li et al., 2020) with server-side extrapolation, Li et al. (2024a) recently introduced the FedExProx method. Their theoretical analysis, however, relies on the assumption that each client computes a certain proximal operator exactly, which is impractical since this is virtually never possible to do in real settings. In this paper, we investigate the behavior of FedExProx without this exactness assumption in the smooth and globally strongly convex setting. We establish a general convergence result, showing that inexactness leads to convergence to a neighborhood of the solution. Additionally, we demonstrate that, with careful control, the adverse effects of this inexactness can be mitigated. By linking inexactness to biased compression (Beznosikov et al., 2023), we refine our analysis, highlighting robustness of extrapolation to inexact proximal updates. We also examine the local iteration complexity required by each client to achieved the required level of inexactness using various local optimizers. Our theoretical insights are validated through comprehensive numerical experiments.", "title_embedding_index": 19307, "title_abs_embedding_index": 19332}, {"title": "Computing Ex Ante Equilibrium in Heterogeneous Zero-Sum Team Games", "link_suffix": "/forum?id=QWIg5e6mtT", "link": "https://openreview.net/forum?id=QWIg5e6mtT", "pdf_link": "https://openreview.net/pdf?id=QWIg5e6mtT", "keywords": "Two-Team Zero-Sum Games, Heterogeneous Teammates, Policy Space Response Oracle", "abstract": "The \\textit{ex ante} equilibrium for two-team zero-sum games, where agents within each team collaborate to compete against the opposing team, is known to be the best a team can do for coordination. Many existing works on \\textit{ex ante} equilibrium solutions are aiming to extend the scope of \\textit{ex ante} equilibrium solving to large-scale team games based on Policy Space Response Oracle (PSRO). However, the joint team policy space constructed by the most prominent method, Team PSRO, cannot cover the entire team policy space in heterogeneous team games where teammates play distinct roles. Such insufficient policy expressiveness causes Team PSRO to be trapped into a sub-optimal \\textit{ex ante} equilibrium with significantly higher exploitability and never converges to the global \\textit{ex ante} equilibrium. To find the global \\textit{ex ante} equilibrium without introducing additional computational complexity, we first parameterize heterogeneous policies for teammates, and we prove that optimizing the heterogeneous teammates' policies sequentially can guarantee a monotonic improvement in team rewards. \nWe further propose \\textbf{Heterogeneous-PSRO} (\\textbf{H-PSRO}), a novel framework for heterogeneous team games, which integrates the sequential correlation mechanism into the PSRO framework and serves as the first PSRO  framework for heterogeneous team games. \nWe prove that H-PSRO achieves lower exploitability than Team PSRO in heterogeneous team games. \nEmpirically, H-PSRO achieves convergence in matrix heterogeneous games that are unsolvable by non-heterogeneous baselines. \nFurther experiments reveal that H-PSRO outperforms non-heterogeneous baselines in both heterogeneous team games and homogeneous settings.", "title_embedding_index": 19308, "title_abs_embedding_index": 19333}, {"title": "Flare Removal with Visual Prompt", "link_suffix": "/forum?id=TCiJvhH2fC", "link": "https://openreview.net/forum?id=TCiJvhH2fC", "pdf_link": "https://openreview.net/pdf?id=TCiJvhH2fC", "keywords": "Flare removal, Visual Prompt, Prompt Inpainting Pipeline", "abstract": "Flare removal methods remove the streak, shimmer, and reflective flare in flare-corrupted images while preserving the light source. Recent deep learning methods focus on flare extraction and achieve promising results. They accomplish the task by either viewing the flare equals to the residual information between the flare-corrupted image and the flare-free image and generating the flare-free image through subtracting the extracted flare image or generating the flare-free image and the flare image simultaneously. However, due to the gap between the flare image and the residual information and handling flare extraction and clear image generation process simultaneously will give the network too much pressure and cannot fully utilize the extracted flare, these methods tend to generate images with severe artifacts. To alleviate such a phenomenon, we propose a model-agnostic pipeline named Prompt Inpainting Pipeline (PIP). Specifically, instead of viewing the gap between the flare-free and flare corrupted image as the flare or generating the flare-free image and flare image simultaneously, our prompt inpainting pipeline provides a novel perspective. We borrow the idea from inpainting methods and remove the flare by masking the polluted area and rewriting image details within. Unlike inpainting methods, we first extract multi-scale features of flare-corrupted images as a visual prompt and rewrite missing textures with the visual prompt since we find out that directly writing the missing details based on the remaining area hardly generates promising image details with sufficient semantic and high-frequency information. To verify the function of our pipeline, we conduct comprehensive experiments and demonstrate its superiority.", "title_embedding_index": 19309, "title_abs_embedding_index": 19334}, {"title": "Recursive Cleaning for Large-scale Protein Data via Multimodal Learning", "link_suffix": "/forum?id=R7l5kMJTut", "link": "https://openreview.net/forum?id=R7l5kMJTut", "pdf_link": "https://openreview.net/pdf?id=R7l5kMJTut", "keywords": "protein; data cleaning; multimodal learning", "abstract": "Reliable datasets and high-performance models work together to drive significant advancements in protein representation learning in the era of Artificial Intelligence. The size of protein models and datasets has grown exponentially in recent years. However, the quality of protein knowledge and model training has suffered from the lack of accurate and efficient data annotation and cleaning methods.\nTo address this challenge, we introduce ProtAC, which corrects large Protein datasets with a scalable Automatic Cleaning framework that leverages both sequence and functional information through multimodal learning. To fulfill data cleaning, we propose the Sequence-Annotation Matching (SAM) module in the model, which filters the functional annotations that are more suitable for the corresponding sequences. Our approach is a cyclic process consisting of three stages: first pretraining the model on a large noisy dataset, then finetuning the model on a small manually annotated dataset, and finally cleaning the noisy dataset using the finetuned model. Through multiple rounds of \u201ctrain-finetune-clean\u201d cycles, we observe progressive improvement in protein function prediction and sequence-\nannotation matching. As a result, we achieve (1) a state-of-the-art (SOTA) model that outperforms competitors with fewer than 100M parameters, evaluated on multiple function-related downstream tasks, and (2) a cleaned UniRef50 dataset containing \u223c50M proteins with well-annotated functions. Performing extensive biological analysis on a cleaned protein dataset, we demonstrate that our model is able to understand the relationships between different functional annotations in proteins and that proposed functional annotation revisions are reasonable.", "title_embedding_index": 19310, "title_abs_embedding_index": 19335}, {"title": "On the Coexistence and Ensembling of Watermarks", "link_suffix": "/forum?id=ldGz1DSut1", "link": "https://openreview.net/forum?id=ldGz1DSut1", "pdf_link": "https://openreview.net/pdf?id=ldGz1DSut1", "keywords": "watermarking, watermark, ensembles, content provenance", "abstract": "Watermarking, the practice of embedding imperceptible information into media such as images, videos, audio, and text, is essential for intellectual property protection, content provenance and attribution. The growing complexity of digital ecosystems necessitates watermarks for different uses to be embedded in the same media. However, in order to be able to detect and decode all watermarks, they need to coexist well with one another. We perform the first study of coexistence of watermarking methods and, contrary to intuition, we find that various open-source watermarks can coexist with only minor impacts on image quality and decoding robustness. The coexistence of watermarks also opens the avenue for ensembling watermarking methods. We show how ensembling can increase the overall message capacity and enable new trade-offs between capacity, accuracy, robustness and image quality, without needing to retrain the base models.", "title_embedding_index": 19311, "title_abs_embedding_index": 19336}, {"title": "ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition", "link_suffix": "/forum?id=YGDWW6rzYX", "link": "https://openreview.net/forum?id=YGDWW6rzYX", "pdf_link": "https://openreview.net/pdf?id=YGDWW6rzYX", "keywords": "Large Language Model Evaluation, Foundation Model Evaluation, ELO Ranking", "abstract": "Evaluating the capabilities of Foundation Models has traditionally relied on static benchmark datasets, human assessments, or model-based evaluations \u2014 methods that often suffer from overfitting, high costs, and biases. We introduce ZeroSumEval, a novel competition-based evaluation protocol that leverages zero-sum games to assess LLMs with dynamic benchmarks that resist saturation. ZeroSumEval encompasses a diverse suite of games, including security challenges (Capture the Flag), classic board games (chess), and knowledge tests (MathQuiz). These games are designed to evaluate a range of AI capabilities such as strategic reasoning, planning, knowledge application, safety, and adaptability. A key novelty is integrating automatic prompt optimization to ensure fair comparisons by eliminating biases from human prompt engineering and support arbitrary prompting strategies. Furthermore, ZeroSumEval measures AI models' abilities to self-improve from limited observations and assesses their robustness against adversarial or misleading examples during prompt optimization. Building upon recent studies that highlight the effectiveness of game-based evaluations for LLMs, ZeroSumEval enhances these approaches by providing a standardized and extensible framework for rigorous assessment. We find ZeroSumEval correlates strongly with expensive human evaluations (Chatbot Arena) and disagrees with benchmarks with known overfitting and saturation issues. Inspecting match traces reveals models that allocate more tokens to thought processes perform strongly in games involving planning capabilities.", "title_embedding_index": 19312, "title_abs_embedding_index": 19337}, {"title": "Real-Time Deepfake Detection in the Real World", "link_suffix": "/forum?id=kkE7jlqKae", "link": "https://openreview.net/forum?id=kkE7jlqKae", "pdf_link": "https://openreview.net/pdf?id=kkE7jlqKae", "keywords": "deepfake detection", "abstract": "Recent improvements in generative AI made synthesizing fake images easy; as they can be used to cause harm, it is crucial to develop accurate techniques to identify them. This paper introduces \"Locally Aware Deepfake Detection Algorithm\" (LaDeDa), that accepts a single $9 \\times 9$ image patch and outputs its deepfake score. The image deepfake score is the pooled score of its patches. With merely patch-level information, LaDeDa significantly improves over the state-of-the-art, achieving around $99%$ mAP on current benchmarks. Owing to the patch-level structure of LaDeDa, we hypothesize that the generation artifacts can be detected by a simple model. We therefore distill LaDeDa into Tiny-LaDeDa, a highly efficient model consisting of only $4$ convolutional layers. Remarkably, Tiny-LaDeDa has $375 \\times$ fewer FLOPs and is $10\\text{,}000 \\times$ more parameter-efficient than LaDeDa, allowing it to run efficiently on edge devices with a minor decrease in accuracy. These almost-perfect scores raise the question: is the task of deepfake detection close to being solved?  Perhaps surprisingly, our investigation reveals that current training protocols prevent methods from generalizing to real-world deepfakes extracted from social media. To address this issue, we introduce WildRF, a new deepfake detection dataset curated from several popular social networks. Our method achieves the top performance of $93.7%$ mAP on WildRF, however the large gap from perfect accuracy shows that reliable real-world deepfake detection is still unsolved.", "title_embedding_index": 19313, "title_abs_embedding_index": 19338}, {"title": "CodeCloak: A Method for Mitigating Code Leakage by LLM Code Assistants", "link_suffix": "/forum?id=z3KmG5JIN4", "link": "https://openreview.net/forum?id=z3KmG5JIN4", "pdf_link": "https://openreview.net/pdf?id=z3KmG5JIN4", "keywords": "privacy, DRL, LLM, code assistant, generative models", "abstract": "LLM-based code assistants are becoming increasingly popular among developers.\nThese tools help developers improve their coding efficiency and reduce errors by providing real-time suggestions based on the developer\u2019s codebase. \nWhile beneficial, the use of these tools can inadvertently expose the developer\u2019s proprietary code to the code assistant service provider during the development process. \nIn this work, we propose a method to mitigate the risk of code leakage when using LLM-based code assistants. CodeCloak is a novel deep reinforcement learning agent that manipulates the prompts before sending them to the code assistant service.\nCodeCloak aims to achieve the following two contradictory goals: (i) minimizing code leakage, while (ii) preserving relevant and useful suggestions for the developer. \nOur evaluation, employing StarCoder and Code Llama, LLM-based code assistants models, demonstrates CodeCloak\u2019s effectiveness on a diverse set of code repositories of varying sizes, as well as its transferability across different models.\nWe also designed a method for reconstructing the developer\u2019s original codebase from code segments sent to the code assistant service (i.e., prompts) during the development process, to thoroughly analyze code leakage risks and evaluate the effectiveness of CodeCloak under practical development scenarios.", "title_embedding_index": 19314, "title_abs_embedding_index": 19339}, {"title": "Kinda-45M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content", "link_suffix": "/forum?id=A3VEYm8CDW", "link": "https://openreview.net/forum?id=A3VEYm8CDW", "pdf_link": "https://openreview.net/pdf?id=A3VEYm8CDW", "keywords": "video generation, video datasets", "abstract": "As visual generation technologies continue to advance, the scale of video datasets has expanded rapidly, and the quality of these datasets is critical to the performance of video generation models. We argue that temporal splitting, detailed captions, and video quality filtering are three key factors that determine dataset quality. However, existing datasets exhibit various limitations in these areas. To address these challenges, we introduce Kinda-45M, a large-scale, high-quality video dataset featuring accurate temporal splitting, detailed captions, and superior video quality. The core of our approach lies in improving the consistency between fine-grained conditions and video content. Specifically, we employ a linear classifier on probability distributions to enhance the accuracy of transition detection, ensuring better temporal consistency. We then provide structured captions for the segmented videos, with an average length of 200 words, to improve text-video alignment. Additionally, we develop a Video Training Suitability Score (VTSS) that integrates multiple sub-metrics, allowing us to filter high-quality videos from the original corpus. Finally, we incorporate several metrics into the training process of the generation model, further refining the fine-grained conditions. Our experiments demonstrate the effectiveness of our data processing pipeline and the quality of the proposed Kinda-45M dataset.", "title_embedding_index": 19315, "title_abs_embedding_index": 19340}, {"title": "Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)", "link_suffix": "/forum?id=xkgfLXZ4e0", "link": "https://openreview.net/forum?id=xkgfLXZ4e0", "pdf_link": "https://openreview.net/pdf?id=xkgfLXZ4e0", "keywords": "brain encoding, fMRI, visual processing, multimodal instruction-tuned models, language decoder, LLMs, MLLMs", "abstract": "Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models\u2014through increased size, instruction-tuning, and multimodality\u2014has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks. However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations. To address this, we first investigate the brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes. Experiments with 10 different instructions (like image captioning, visual question answering, etc.) show that  MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP. We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions, not all instructions are relevant for brain alignment. Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity. Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions. These results indicate that enhancing MLLMs' ability to capture more task-specific information could allow for better differentiation between various types of instructions, and hence improve their precision in predicting brain responses.", "title_embedding_index": 19316, "title_abs_embedding_index": 19341}, {"title": "ZerOmics: Toward General Models for Single-Cell Analysis with Instruction Tuning", "link_suffix": "/forum?id=J1xtkJmFY3", "link": "https://openreview.net/forum?id=J1xtkJmFY3", "pdf_link": "https://openreview.net/pdf?id=J1xtkJmFY3", "keywords": "Large Language Models; Instruction Tuning; Zero-shot Learning; Bioinformatics", "abstract": "A variety of analysis tasks in single-cell (SC) multi-omics are crucial for precision medicine and clinical research. To address these tasks, existing methods are typically pre-trained on large-scale datasets to obtain general representations, followed by fine-tuning on specific tasks and labeled datasets. However, their task-specific heads often lack generalizability, significantly limiting performance in zero-shot scenarios. Inspired by the success of large language models (LLMs), we propose ZerOmics, the first zero-shot method that guides LLMs to perform various SC tasks without relying on specific downstream data. To enable LLMs to establish a correct and comprehensive understanding of SC data, ZerOmics employs a dual-alignment strategy. Specifically, ZerOmics aligns SC expression data with the well-organized gene corpus, thereby generating robust SC embeddings. These embeddings are then incorporated into instructions designed for various SC analysis tasks to tune the LLM, achieving alignment between SC data and the LLM. Extensive experiments across various sequencing technologies and tissues demonstrate that ZerOmics provides a comprehensive and general solution for SC analysis, achieving performance comparable to or even surpassing the state-of-the-art (SOTA) supervised and fine-tuned methods.", "title_embedding_index": 19317, "title_abs_embedding_index": 19342}, {"title": "Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection", "link_suffix": "/forum?id=hWmwL9gizZ", "link": "https://openreview.net/forum?id=hWmwL9gizZ", "pdf_link": "https://openreview.net/pdf?id=hWmwL9gizZ", "keywords": "protein language model, protein representation learning, immunogenicity prediction", "abstract": "Immunogenicity prediction is a central topic in reverse vaccinology for finding candidate vaccines that can trigger protective immune responses. Existing approaches typically rely on highly compressed features and simple model architectures, leading to limited prediction accuracy and poor generalizability. To address these challenges, we introduce ProVaccine, a novel deep learning solution with a dual attention mechanism that integrates pre-trained latent vector representations of protein sequences and structures. We also compile the most comprehensive immunogenicity dataset to date, encompassing over 9,500 antigen sequences, structures, and immunogenicity labels from bacteria, viruses, and tumors. Extensive experiments demonstrate that ProVaccine outperforms existing methods across a wide range of evaluation metrics. Furthermore, we establish a post-hoc validation protocol to assess the practical significance of deep learning models in tackling vaccine design challenges. Our work provides an effective tool for vaccine design and sets valuable benchmarks for future research.", "title_embedding_index": 19318, "title_abs_embedding_index": 19343}, {"title": "Open Eyes, Then Reason: Fine-grained Visual Mathematical Understanding in MLLMs", "link_suffix": "/forum?id=6zAIFLgayn", "link": "https://openreview.net/forum?id=6zAIFLgayn", "pdf_link": "https://openreview.net/pdf?id=6zAIFLgayn", "keywords": "Multimodal Large Language Models (MLLMs);Mathematical Reasoning;Fine-grained Visual Understanding;Visual Grounding", "abstract": "Current multimodal large language models (MLLMs) often underperform on mathematical problem-solving tasks that require fine-grained visual understanding. The limitation primarily arises from inadequate perception of geometric primitives during image-level contrastive pre-training (e.g., CLIP). Current efforts to enhance MLLM performance have focused on scaling up mathematical visual instruction datasets and employing stronger LLM backbones, yet these approaches often neglect persistent visual recognition errors in MLLMs. In this paper, we systematically evaluate the visual grounding capabilities of state-of-the-art MLLMs and uncover a negative correlation between their visual grounding accuracy and problem-solving performance. Notably, even advanced models like GPT-4o demonstrate a significant error rate (70%) when identifying geometric entities, highlighting that fine-grained visual understanding remains a crucial bottleneck in visual mathematical reasoning. To address this, we propose a novel approach, SVE-Math (Selective Vision-Enhanced Mathematical MLLM), featuring a geometric-grounded vision encoder and a feature router that dynamically adjusts the contribution of hierarchical visual feature maps. Our model recognizes accurate visual primitives and generates precise visual prompts tailored to the language model's reasoning needs. In experiments, SVE-Math-7B outperforms other 7B-parameter models by 5.5% on MathVerse and even surpasses larger models on MathVista. Despite being trained on smaller datasets, SVE-Math-7B matches the performance of models trained on significantly larger datasets on GeoQA benchmark. Our findings provide critical insights for future research, highlighting the need for more effective integration of fine-grained visual understanding in MLLMs.  We will release model weights, code, and instructions upon acceptance.", "title_embedding_index": 19319, "title_abs_embedding_index": 19344}, {"title": "Large Language Models as Markov Chains", "link_suffix": "/forum?id=RDFkGZ9Dkh", "link": "https://openreview.net/forum?id=RDFkGZ9Dkh", "pdf_link": "https://openreview.net/pdf?id=RDFkGZ9Dkh", "keywords": "Large language models, Markov chain, in-context learning, generalization bounds, convergence analysis", "abstract": "Large language models (LLMs) have proven to be remarkably efficient, both across a wide range of natural language processing tasks and well beyond them. However, a comprehensive theoretical analysis of the origins of their impressive performance remains elusive. In this paper, we approach this challenging task by drawing an equivalence between generic auto-regressive language models with vocabulary of size $T$ and context window of size $K$ and Markov chains defined on a finite state space of size $\\mathcal{O}(T^K)$. We derive several surprising findings related to the existence of a stationary distribution of Markov chains that capture the inference power of LLMs, their speed of convergence to it, and the influence of the temperature on the latter. We then prove pre-training and in-context generalization bounds and show how the drawn equivalence allows us to enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.", "title_embedding_index": 19320, "title_abs_embedding_index": 19345}, {"title": "Sassha: Sharpness-aware Adaptive Second-order Optimization with Stable Hessian Approximation", "link_suffix": "/forum?id=pLOep5sYWe", "link": "https://openreview.net/forum?id=pLOep5sYWe", "pdf_link": "https://openreview.net/pdf?id=pLOep5sYWe", "keywords": "deep learning, second-order optimization, sharpness minimization", "abstract": "Approximate second-order optimization methods have gained attention due to their low computational and memory overhead.\nWhile these methods have the potential to accelerate neural network training, they often exhibit poorer generalization compared to first-order approaches. To address this limitation, we first analyze existing second-order methods through the lens of the loss landscape, demonstrating that their reduced generalization performance is somewhat attributed to the sharpness of the solutions they converge to. In response, we introduce Sassha, a novel approach designed to enhance generalization by explicitly reducing sharpness. In fact, this sharpness minimization scheme is designed to accommodate lazy and stable Hessian updates, so as to secure efficiency and robustness besides flatness. To validate its effectiveness, we conduct a wide range of deep learning experiments including standard vision and language tasks, where Sassha achieves competitive performance. Notably, Sassha demonstrates strong generalization in noisy data settings and significantly outperforms other methods in these scenarios. Additionally, we verify the robustness of\u001cSassha through various ablation studies.", "title_embedding_index": 19321, "title_abs_embedding_index": 19346}, {"title": "Bad-PFL: Exploiting Backdoor Attacks against Personalized Federated Learning", "link_suffix": "/forum?id=79nO2DPjVX", "link": "https://openreview.net/forum?id=79nO2DPjVX", "pdf_link": "https://openreview.net/pdf?id=79nO2DPjVX", "keywords": "personalized federated learning, backdoor attacks", "abstract": "Data heterogeneity and backdoor attacks rank among the most significant challenges facing federated learning (FL). For data heterogeneity, personalized federated learning (PFL) enables each client to maintain a private personalized model to cater to client-specific knowledge. Meanwhile, vanilla FL has proven vulnerable to backdoor attacks. However, recent advancements in PFL community have demonstrated a potential immunity against such attacks. This paper explores this intersection further, revealing that existing federated backdoor attacks fail in PFL because backdoors about manually designed triggers struggle to survive in personalized models. To tackle this, we degisn Bad-PFL, which employs features from natural data as our trigger. As long as the model is trained on natural data, it inevitably embeds the backdoor associated with our trigger, ensuring its longevity in personalized models. Moreover, our trigger undergoes mutual reinforcement training with the model, further solidifying the backdoor's durability and enhancing attack effectiveness. The large-scale experiments across three benchmark datasets demonstrate the superior performance of our attack against various PFL methods, even when equipped with state-of-the-art defense mechanisms.", "title_embedding_index": 19322, "title_abs_embedding_index": 19347}, {"title": "Critical Influence of Overparameterization on Sharpness-aware Minimization", "link_suffix": "/forum?id=YMCtQlm8Bc", "link": "https://openreview.net/forum?id=YMCtQlm8Bc", "pdf_link": "https://openreview.net/pdf?id=YMCtQlm8Bc", "keywords": "sharpness-aware minimization, overparameterization", "abstract": "Training overparameterized neural networks often yields solutions with varying generalization capabilities, even when achieving similar training losses. Recent evidence indicates a strong correlation between the sharpness of a minimum and its generalization error, leading to increased interest in optimization methods that explicitly seek flatter minima for improved generalization. Despite its contemporary relevance to overparameterization, however, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to exactly how it is affected by overparameterization. In this work, we analyze SAM under varying degrees of overparameterization, presenting both empirical and theoretical findings that reveal its critical influence on SAM's effectiveness. First, we conduct extensive numerical experiments across diverse domains and show that SAM consistently improves with overparameterization. Next, we attribute this phenomenon to the interplay between the enlarged solution space and increased implicit bias resulting from overparameterization. Furthermore, we show that this effect is particularly pronounced in practical settings involving label noise and sparsity, and yet, sufficient regularization is necessary. Last but not least, we provide other theoretical insights into how overparameterization helps SAM achieve minima with more uniform Hessian moments compared to SGD, and much faster convergence at a linear rate.", "title_embedding_index": 19323, "title_abs_embedding_index": 19348}, {"title": "Bidirectional Learning for the Visual Representation in Radiology Report Generation with Frozen LLMs", "link_suffix": "/forum?id=gZue5gHQHp", "link": "https://openreview.net/forum?id=gZue5gHQHp", "pdf_link": "https://openreview.net/pdf?id=gZue5gHQHp", "keywords": "Bidirectional Learning, Radiology Report Generation, Representation Learning, Large Language Models.", "abstract": "Radiology report generation (R2Gen) has recently leveraged large language models (LLMs), achieving improved results. However, the generated reports still fall short in both language accuracy and clinical relevance. A key challenge is learning a visual representation of radiology images that an LLM can effectively interpret. To address this, we propose that for a visual representation to be interpretable by an LLM, it shall also be generatable by the LLM. Building on this idea, we introduce a novel bidirectional learning framework for R2Gen, integrating both vision-to-text and text-to-vision information to enhance visual representation learning. First, we require that the visual representation aid the LLM in generating reports that closely match the ground truth. Second, we require that the visual representation be maximally generated by the LLM when provided with the ground truth report. To enable the frozen LLM to perform text-to-vision generation, we jointly train a new text encoder for reports. Additionally, through an image reconstruction task, we encourage the visual representation to capture the core features of input radiology images. This bidirectional learning framework is realized using a frozen LLM and incurs no extra computational cost at the inference stage. Experimental results demonstrate better alignment between the learned visual representation and the LLM\u2019s word embedding space, along with state-of-the-art performance in both language accuracy and clinical efficacy. Our code will be publicly released.", "title_embedding_index": 19324, "title_abs_embedding_index": 19349}]