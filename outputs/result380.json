[
    {
        "title": "Context-Driven Missing Data Imputation via Large Language Model",
        "link_suffix": "/forum?id=b2oLgk5XRE",
        "link": "https://openreview.net/forum?id=b2oLgk5XRE",
        "pdf_link": "https://openreview.net/pdf?id=b2oLgk5XRE",
        "keywords": "Missing data imputation, Heterogeneous tabular data, Large Language Models, Nearest neighbor, Constrative learning",
        "abstract": "Missing data poses significant challenges for machine learning and deep learning algorithms. In this paper, we aim to enhance post-imputation performance, measured by machine learning utility (MLu). We introduce a nearest-neighbor-based imputation method, DrIM, designed for heterogeneous tabular datasets. However, calculating similarity in the data space becomes challenging due to the varying presence of missing entries across different columns. To address this issue, we leverage the representation learning capabilities of Large Language Models (LLMs). By transforming the tabular dataset into a text-format dataset and replacing the missing entries with mask tokens, we extract representations that capture contextual information. This mapping to a continuous representation space enables the use of well-defined similarity measurements. Additionally, we incorporate a contrastive learning framework to refine the representations, ensuring that the representations of observations with similar information in the observed columns, regardless of the missingness patterns, are closely aligned. To validate our proposed model, we evaluate its performance in missing data imputation across 10 real-world tabular datasets, demonstrating its ability to produce a complete dataset having high MLu."
    },
    {
        "title": "MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequences",
        "link_suffix": "/forum?id=QHj2LL958o",
        "link": "https://openreview.net/forum?id=QHj2LL958o",
        "pdf_link": "https://openreview.net/pdf?id=QHj2LL958o",
        "keywords": "Generative Model, Vison Language Model, Long Video Generation, Long Story Generation",
        "abstract": "Recent advancements in video generation have primarily leveraged diffusion models for short-duration content. However, these approaches often fall short in modeling complex narratives and maintaining character consistency over extended periods, which is essential for long-form video production like movies. We propose MovieDreamer, a novel hierarchical framework that integrates the strengths of autoregressive models with diffusion-based rendering to pioneer long-duration video generation with intricate plot progressions and high visual fidelity. Our approach utilizes autoregressive models for global narrative coherence, predicting sequences of visual tokens that are subsequently transformed into high-quality video frames through diffusion rendering. This method is akin to traditional movie production processes, where complex stories are factorized down into manageable scene capturing. Further, we employ a multimodal script that enriches scene descriptions with detailed character information and visual style, enhancing continuity and character identity across scenes. We present extensive experiments across various movie genres, demonstrating that our approach not only achieves superior visual and narrative quality but also effectively extends the duration of generated content significantly beyond current capabilities."
    },
    {
        "title": "MDTREE: A Masked Dynamic Autoregressive Model for Phylogenetic Inference",
        "link_suffix": "/forum?id=2CYZkawsmz",
        "link": "https://openreview.net/forum?id=2CYZkawsmz",
        "pdf_link": "https://openreview.net/pdf?id=2CYZkawsmz",
        "keywords": "Phylogenetic Inference, Genome Language Model, Transformer, Graph Structure Generation, DNA, Large Language Models",
        "abstract": "Phylogenetic tree inference, crucial for understanding species evolution, presents challenges in jointly optimizing continuous branch lengths and discrete tree topologies. Traditional Markov Chain Monte Carlo methods, though widely adopted, suffer from slow convergence and high computational costs. Deep learning methods have introduced more scalable solutions but still face limitations. Bayesian generative models struggle with computational complexity, autoregressive models are constrained by predefined species orders, and generative flow networks still fail to fully leverage evolutionary signals from genomic sequences. In this paper, we introduce MDTree, a novel framework that redefines phylogenetic tree generation from the perspective of dynamically learning node orders based on biological priors embedded in genomic sequences. By leveraging a Diffusion Ordering Network to learn evolutionarily meaningful node orders, MDTree autoregressively positions nodes to construct biologically coherent trees. To further push its limits, we propose a dynamic masking mechanism that accelerates tree generation through parallel node processing. Extensive experiments show that MDTree outperforms existing methods on standard phylogenetic benchmarks, offering biologically interpretable and computationally efficient solutions for tree generation."
    },
    {
        "title": "ViT-UWA: Vision Transformer Underwater-Adapter for Dense Predictions Beneath the Water Surface",
        "link_suffix": "/forum?id=i0VqD2KaYt",
        "link": "https://openreview.net/forum?id=i0VqD2KaYt",
        "pdf_link": "https://openreview.net/pdf?id=i0VqD2KaYt",
        "keywords": "Underwater Image Dense Prediction, Adapted ViT Backbone",
        "abstract": "Vision Transformer (ViT) and its variants have witnessed a significant success in computer vision. However, they do not perform well in underwater dense prediction tasks due to challenges like complex underwater environments, quality degradation, and light scattering in underwater images. To solve this problem, we propose the  Vision Transformer Underwater-Adapter (ViT-UWA), the first detail-focused and adapted ViT backbone for underwater dense prediction tasks, without requiring task-specific pretraining. In ViT-UWA, we first introduce High-frequency Components Prior (HFCP) to add high-frequency information of underwater images to the plain ViT, which can help recover and capture lost high-frequency information of underwater images. Then, we propose an Detail Aware Module (DAM) to obtain a detail-focused multi-scale convolutional feature pyramid, which can be used in kinds of dense prediction tasks. Through the ViT-CNN Interaction Module (VCIM), we achieve bidirectional feature fusion between ViT and CNN. We evaluate ViT-UWA on multiple underwater dense prediction tasks, including semantic segmentation, instance segmentation, and object detection. Notably, with only ImageNet-22K pretraining, our ViT-UWA-B yields state-of-the-art 46.4 box AP and 44.2 mask AP on USIS10K dataset. We hope ViT-UWA could provide a new backbone for future research on underwater dense prediction tasks."
    },
    {
        "title": "RelayGS: Reconstructing High-Fidelity Dynamic Scenes with Large-Scale and Complex Motions via Relay Gaussians",
        "link_suffix": "/forum?id=SyV8rldw49",
        "link": "https://openreview.net/forum?id=SyV8rldw49",
        "pdf_link": "https://openreview.net/pdf?id=SyV8rldw49",
        "keywords": "Gaussian Splatting, Dynamic Scene Reconstruction, 4D Reconstruction",
        "abstract": "Reconstructing dynamic scenes with large-scale and complex motions\u2014such as those in sports events\u2014remains a significant challenge. Recent techniques like Neural Radiance Field and Gaussian Splatting have shown promise but often struggle with scenes involving substantial movement. In this paper, we proposeRelayGS, a novel dynamic scene reconstruction method based on Gaussian Splatting, specifically designed to represent and learn large-scale complex motion patterns in highly dynamic scenes. Our RelayGS consists of three key stages. First, we learn the fundamental scene structure from all frames without considering temporal information and employ a learnable mask to decouple the highly dynamic foreground from the background exhibiting minimal motion. Second, we partition the scene into temporal segments, each consisting of several consecutive multi-view frames. For each segment, we replicate the foreground Gaussians, dubbedRelay Gaussians, as they are designed to act as relay nodes along the large-scale motion trajectory. By creating pseudo-views from frames uniformly selected from the segment, we optimize and densify foreground Relay Gaussians, further simplify and decompose large-scale motion trajectories into smaller, more manageable segments. Finally, we leverage HexPlane and lightweight MLPs to jointly learn the scene\u2019s temporal motion field and refine the canonical Gaussians. We conduct extensive experiments on two dynamic scene datasets featuring large and complex motions to demonstrate the effectiveness of our RelayGS. RelayGS outperforms state-of-the-arts by more than 1 dB in PSNR, and successfully reconstructs real-world basketball game scenes in a much more complete and coherent manner, whereas previous methods usually struggle to capture the complex motion of players."
    },
    {
        "title": "Policy Filtration in RLHF to Fine-Tune LLM for Code Generation",
        "link_suffix": "/forum?id=YilY5fGQny",
        "link": "https://openreview.net/forum?id=YilY5fGQny",
        "pdf_link": "https://openreview.net/pdf?id=YilY5fGQny",
        "keywords": "LLM, RLHF",
        "abstract": "While direct policy optimization methods exist, pioneering LLMs are fine-tuned with reinforcement learning from human feedback (RLHF) to generate better responses under the supervision of a reward model learned from preference data. One major challenge of RLHF is the inaccuracy of the intermediate reward model, especially in code generation tasks that requires complex reasoning for the reward model to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve the signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtering strategy, we use the coefficient of determination ($R^2$) between the rewards and actual scores on filtered samples as the metrics to help us find promising strategies since it measures how well the rewards filtered by PF-PPO indicate real performance. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation tasks. We find that some variants of PF-PPO are highly effective and achieve the state-of-the-art performance of 7-billion-parameter models on HumanEval (+7.9%) and MBPP (+0.7%). Moreover, we create the LeetCode Contest benchmark and demonstrate the advantage of PF-PPO (+10.0%) on this more challenging benchmark."
    },
    {
        "title": "Realistic Surgical Simulation from Monocular Videos",
        "link_suffix": "/forum?id=faSfhqDpZP",
        "link": "https://openreview.net/forum?id=faSfhqDpZP",
        "pdf_link": "https://openreview.net/pdf?id=faSfhqDpZP",
        "keywords": "Surgical Simulation, Video-based Reconstruction, Robotic Surgery",
        "abstract": "This paper tackles the challenge of automatically constructing realistic surgical simulation systems from readily available surgical videos. Recent efforts have successfully integrated physically grounded dynamics within 3D Gaussians to perform high-fidelity simulations in well-reconstructed static simulation environments. However, they struggle with the geometry inconsistency of simulation environments and unrealistic physical deformations of soft tissues when it comes to dynamic and complex surgical processes. In this paper, we propose SurgiSim, a novel automatic simulation system to overcome these limitations. To build a surgical simulation environment, we maintain a canonical 3D scene composed of 3D Gaussians coupled with a deformation field to accurately model monocular dynamic surgical scenes. This process involves a multi-stage optimization with trajectory and anisotropic regularization, enhancing the geometry consistency of the canonical scene which serves the simulation environment. To improve the realism of physical simulations, we implement a Visco-Elastic deformation model based on the Maxwell model, effectively restoring the complex deformations of tissues. Additionally, we estimate the physical properties of tissues by minimizing the discrepancies between the input video and simulation results guided by predicted tissue motion, ensuring realistic simulation outcomes. Experiments across diverse surgical scenarios demonstrate SurgiSim's ability to perform realistic physical interactions of soft tissues among surgical procedures, showing its enormous potential for enhancing surgical training, planning, and robotic surgery systems."
    },
    {
        "title": "Prototype-based Optimal Transport for Out-of-Distribution Detection",
        "link_suffix": "/forum?id=J2we1sVd9m",
        "link": "https://openreview.net/forum?id=J2we1sVd9m",
        "pdf_link": "https://openreview.net/pdf?id=J2we1sVd9m",
        "keywords": "Out-Of-Distribution Detection, Optimal Transport",
        "abstract": "Detecting Out-of-Distribution (OOD) inputs is crucial for improving the reliability of deep neural networks in the real-world deployment. In this paper, inspired by the inherent distribution shift between ID and OOD data, we propose a novel method that leverages optimal transport to measure the distribution discrepancy between test inputs and ID prototypes. The resulting transport costs are used to quantify the individual contribution of each test input to the overall discrepancy, serving as a desirable measure for OOD detection. To address the issue that solely relying on the transport costs to ID prototypes is inadequate for identifying OOD inputs closer to ID data, we generate virtual outliers to approximate the OOD region via linear extrapolation. By combining the transport costs to ID prototypes with the costs to virtual outliers, the detection of OOD data near ID data is emphasized, thereby enhancing the distinction between ID and OOD inputs. Experiments demonstrate the superiority of our method over state-of-the-art methods."
    },
    {
        "title": "V2M: Visual 2-Dimensional Mamba for Image Representation Learning",
        "link_suffix": "/forum?id=FowFLhUTgO",
        "link": "https://openreview.net/forum?id=FowFLhUTgO",
        "pdf_link": "https://openreview.net/pdf?id=FowFLhUTgO",
        "keywords": "Mamba, Visual Backbone, Representation Learning",
        "abstract": "Mamba has garnered widespread attention due to its flexible design and efficient hardware performance to process 1D sequences based on the state space model (SSM). Recent studies have attempted to apply Mamba to the visual domain by flattening 2D images into patches and then regarding them as a 1D sequence. To compensate for the 2D structure information loss (e.g., local similarity) of the original image, most existing methods focus on designing different orders to sequentially process the tokens, which could only alleviate this issue to some extent. In this paper, we propose a Visual 2-Dimensional Mamba (V2M) model as a complete solution, which directly processes image tokens in the 2D space. We first generalize SSM to the 2-dimensional space which generates the next state considering two adjacent states on both dimensions (e.g., columns and rows). We then construct our V2M based on the 2-dimensional SSM formulation and incorporate Mamba to achieve hardware-efficient parallel processing. The proposed V2M effectively incorporates the 2D locality prior yet inherits the efficiency and input-dependent scalability of Mamba. Extensive experimental results on ImageNet classification and downstream visual tasks including object detection and instance segmentation on COCO and semantic segmentation on ADE20K demonstrate the effectiveness of our V2M compared with other visual backbones."
    },
    {
        "title": "Stylize and Align: Unlabeled-Image Stylized Continuous Consistency Regularization for Hand Pose Estimation in the Wild",
        "link_suffix": "/forum?id=nCxULYtwkC",
        "link": "https://openreview.net/forum?id=nCxULYtwkC",
        "pdf_link": "https://openreview.net/pdf?id=nCxULYtwkC",
        "keywords": "Pose estimation, 3D hand pose estimation, Consistency regularization, Style transfer, Robustness, Metric learning",
        "abstract": "Hand pose estimation has become a cornerstone of advanced human behavior understanding. In particular, 3D hand pose estimation has seen significant attention, with numerous approaches being proposed. However, it is unclear whether the modern approaches are applicable to real-world scenarios directly. We are focused on the robustness of hand pose estimators in the wild, noting that existing datasets exhibit distinct differences from real-world data. Thus, despite great advances, there remains considerable room for improvement, as most recent efforts have primarily focused on model architectures or on datasets within limited environments. To this end, we present a novel approach that unifies two key techniques: style transfer using unlabeled in-the-wild images to enhance data diversity (\\ie, Stylize) and continuous consistency regularization (CCR) to capture fine-grained relations between hand pose data, providing rich supervisory signals (\\ie, Align). To evaluate the robustness of the learned representations through our framework, we demonstrate that our method significantly enhances generalization capabilities across various tasks, including 3D hand pose estimation and transfer learning for 2D hand pose estimation, all within our designed real-world testbed. Notably, these improvements are achieved using less than 5% of the data size compared to a large-scale dataset, InterHand2.6M."
    },
    {
        "title": "The Optimization Landscape of SGD Across the Feature Learning Strength",
        "link_suffix": "/forum?id=iEfdvDTcZg",
        "link": "https://openreview.net/forum?id=iEfdvDTcZg",
        "pdf_link": "https://openreview.net/pdf?id=iEfdvDTcZg",
        "keywords": "Feature Learning, Maximal Update Parameterization, Optimization, Edge of Stability, Catapult Effect, Silent Alignment",
        "abstract": "We consider neural networks (NNs) where the final layer is down-scaled by a fixed hyperparameter $\\gamma$. \nRecent work has identified $\\gamma$ as controlling the strength of feature learning.\nAs $\\gamma$ increases, network evolution changes from \"lazy\" kernel dynamics to \"rich\" feature-learning dynamics, with a host of associated benefits including improved performance on common tasks.\nIn this work, we conduct a thorough empirical investigation of the effect of scaling $\\gamma$ across a variety of models and datasets in the online training setting.\nWe first examine the interaction of $\\gamma$ with the learning rate $\\eta$, identifying several scaling regimes in the $\\gamma$-$\\eta$ plane which we explain theoretically using a simple model.\nWe find that the optimal learning rate $\\eta^*$ scales non-trivially with $\\gamma$. In particular, $\\eta^* \\propto \\gamma^2$ when $\\gamma \\ll 1$ and $\\eta^* \\propto \\gamma^{2/L}$ when $\\gamma \\gg 1$ for a feed-forward network of depth $L$.\nUsing this optimal learning rate scaling, we proceed with an empirical study of the under-explored ``ultra-rich'' $\\gamma \\gg 1$ regime.\nWe find that networks in this regime display characteristic loss curves, starting with a long plateau followed by a drop-off, sometimes followed by one or more additional staircase steps.\nWe find networks of different large $\\gamma$ values optimize along similar trajectories up to a reparameterization of time.\nWe further find that optimal online performance is often found at large $\\gamma$ and could be missed if this hyperparameter is not tuned.\nOur findings indicate that analytical study of the large-$\\gamma$ limit may yield useful insights into the dynamics of representation learning in performant models."
    },
    {
        "title": "PISA: Compressive Sensing Adaptation of Large Language Models",
        "link_suffix": "/forum?id=Z2QPJj52m3",
        "link": "https://openreview.net/forum?id=Z2QPJj52m3",
        "pdf_link": "https://openreview.net/pdf?id=Z2QPJj52m3",
        "keywords": "Parameter-Efficient Fine-Tuning (PEFT)\uff0ck-sparse approximation\uff0clow-rank structure",
        "abstract": "In this paper, we introduce a novel perspective on Parameter-Efficient Fine-Tuning (PEFT) by viewing the weight update matrix as a k-sparse approximation in the spatial domain, departing from the commonly used low-rank structure assumption. We propose a compressive sensing-based approach that leverages under-complete measurement matrices to analyze the approximation capabilities of the weight update matrix. Our method ensures bounded error in the reconstruction of the weight updates, as guaranteed by theoretical results in compressive sensing.\nHowever, the vectorization of the weight update matrix leads to a high-dimensional problem (d^2), which can potentially result in large error bounds. To address this issue, we introduce a block-structured approximation scheme that partitions the weight update matrix into smaller blocks and applies the k-sparse approximation to each block independently. We theoretically analyze the approximation error bounds of our approach and demonstrate that the block-structured scheme achieves tighter error bounds compared to the non-block approach.\nEmpirically, we validate the effectiveness of our proposed method on various downstream NLP tasks, showcasing its ability to achieve competitive performance with a reduced number of trainable parameters. Our approach offers a new direction for parameter-efficient fine-tuning of large language models. Notably, our experiments demonstrate competitive performance with only 500 learnable parameters, while offering greater memory and computational efficiency than LoRA in a rank-1 setting."
    },
    {
        "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
        "link_suffix": "/forum?id=oSQiao9GqB",
        "link": "https://openreview.net/forum?id=oSQiao9GqB",
        "pdf_link": "https://openreview.net/pdf?id=oSQiao9GqB",
        "keywords": "large language model, multimodal learning, interleaved image-text",
        "abstract": "Visual instruction tuning has made considerable strides in enhancing the capabilities of Large Multimodal Models (LMMs). However, existing open LMMs largely focus on single-image tasks, their applications to multiimage scenarios remains less explored. Additionally,\nprior LMM research separately tackles different scenarios, leaving it impossible to generalize cross scenarios with new emerging capabilities. To this end, we introduce LLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame (video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To enable these capabilities, we regard the interleaved data format as a general template and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4 primary domains with 14 tasks and 41 datasets. We also curate the LLaVAInterleave Bench to comprehensively evaluate the multiimage performance of LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading results in multi-image, video, and 3D benchmarks, while maintaining the performance of single-image tasks. Besides, our model also exhibits several emerging capabilities, e.g., transferring tasks across different settings and modalities. Code will be available."
    },
    {
        "title": "An Empirical Study of Multiple Masking in Masked Autoencoder",
        "link_suffix": "/forum?id=mCO6FAOgYn",
        "link": "https://openreview.net/forum?id=mCO6FAOgYn",
        "pdf_link": "https://openreview.net/pdf?id=mCO6FAOgYn",
        "keywords": "masked autoencoder, multiple masking, masked image modeling",
        "abstract": "The performance of masked autoencoders hinges significantly on masking, prompting considerable efforts towards devising superior masking strategies. However, these strategies mask only once and employ masking directly on the input image. Afterward, inspired by the flexibility of masking, subsequent works introduce two rounds of masking. Unfortunately, all initiatives primarily focus on enhancing model performance, lacking an in-depth and systematical understanding of multiple masking for masked autoencoder. To bridge this gap, this work introduce a masked framework with multiple masking stages, termed Conditional MAE, where subsequent maskings are conditioned on previous unmasked representations, enabling a more flexible masking process in masked image modeling. By doing so, our study sheds light on how multiple masking affects the optimization in training and performance of pretrained models, e.g., introducing more locality to models, and summarizes several takeaways from our findings. Finally, we empirically evaluate the performance of our best-performing model(Conditional-MAE) with that of MAE in three folds including transfer learning, robustness, and scalability, demonstrating the effectiveness of our multiple masking strategy. We also follow our takeaways and show the generalizability to other heterogeneous networks including SimMIM and ConvNeXt V2. We hope our findings will inspire further work in the field and release the code at https:\n//anonymous.4open.science/r/conditional-mae-512C."
    },
    {
        "title": "Diffree: Text-Guided Shape Free Object Inpainting with Diffusion Model",
        "link_suffix": "/forum?id=JT53iXH7eO",
        "link": "https://openreview.net/forum?id=JT53iXH7eO",
        "pdf_link": "https://openreview.net/pdf?id=JT53iXH7eO",
        "keywords": "Image inpainting; Text-guided image editing;",
        "abstract": "This paper addresses an important problem of object addition for images with only text guidance. It is challenging because the new object must be integrated seamlessly into the image with consistent visual context, such as lighting, texture, and spatial location. While existing text-guided image inpainting methods can add objects, they either fail to preserve the background consistency or involve cumbersome human intervention in specifying bounding boxes or user-scribbled masks. To tackle this challenge, we introduce Diffree, a Text-to-Image (T2I) model that facilitates text-guided object addition with only text control. To this end, we curate OABench, an exquisite synthetic dataset by removing objects with advanced image inpainting techniques. OABench comprises 74K real-world tuples of an original image, an inpainted image with the object removed, an object mask, and object descriptions. Trained on OABench using the Stable Diffusion model with an additional mask prediction module, Diffree uniquely predicts the position of the new object and achieves object addition with guidance from only text. Extensive experiments demonstrate that Diffree excels in adding new objects with a high success rate while maintaining background consistency, spatial appropriateness, and object relevance and quality."
    },
    {
        "title": "UNIP: Rethinking Pre-trained Attention Patterns for Semantic Segmentation",
        "link_suffix": "/forum?id=Xq7gwsnhPT",
        "link": "https://openreview.net/forum?id=Xq7gwsnhPT",
        "pdf_link": "https://openreview.net/pdf?id=Xq7gwsnhPT",
        "keywords": "Image Pre-training, Semantic Segmentation, Infrared Image, Attention Distillation, Representation Learning",
        "abstract": "Pre-training techniques significantly enhance the performance of semantic segmentation tasks with limited training data. However, the efficacy under a large domain gap between pre-training (e.g. RGB) and fine-tuning (e.g. infrared) remains underexplored. In this study, we first benchmark the infrared semantic segmentation performance of various pre-training methods and reveal several phenomena distinct from the RGB domain. Next, our layerwise analysis of pre-trained attention maps uncovers that: (1) There are three typical attention patterns (local, hybrid, and global); (2) Pre-training tasks notably influence pattern distribution across layers; (3) The hybrid pattern is crucial for semantic segmentation as it attends to both nearby and foreground elements; (4) The texture bias impedes model generalization in infrared tasks. Building on these insights, we propose UNIP, a UNified Infrared Pre-training framework, to enhance the pre-trained model performance. This framework uses the hybrid-attention distillation NMI-HAD as the pre-training target, a large-scale mixed dataset InfMix for pre-training, and a last-layer feature pyramid network LL-FPN for fine-tuning. Experimental results show that UNIP outperforms various pre-training methods by up to 13.5% in average mIoU on three infrared segmentation tasks, evaluated using fine-tuning and linear probing metrics. UNIP-S achieves performance on par with MAE-L while requiring only 1/10 of the computational cost. Furthermore, with fewer parameters, UNIP significantly surpasses state-of-the-art (SOTA) infrared or RGB segmentation methods and demonstrates the broad potential for application in other modalities, such as RGB and depth. Our code is available athttps://anonymous.4open.science/r/UNIP-8DCC/."
    },
    {
        "title": "Reinforcement Learning via Lazy-Agent for Environments with Random Delays",
        "link_suffix": "/forum?id=KRlJN9d1sW",
        "link": "https://openreview.net/forum?id=KRlJN9d1sW",
        "pdf_link": "https://openreview.net/pdf?id=KRlJN9d1sW",
        "keywords": "Delayed system, Markov decision process, reinforcement learning",
        "abstract": "Real-world reinforcement learning applications are often hampered by delayed feedback from environments, which violates the fundamental assumption of the Markovian property and introduces significant challenges. While numerous methods have been proposed for handling environments with constant delays, those with random delays remain largely unexplored owing to their inherent complexity and variability. In this study, we explored environments with random delays and demonstrated that these environments can be transformed into equivalent constant-delay environments by introducing a simple agent called the \\textit{lazy-agent}. This approach overcame the challenges posed by the variability of random delays, enabling the application of conventional constant-delay approaches to random-delay environments. The empirical results reveal that our lazy-agents trained in random-delay environments performed almost comparably to the agents trained in constant-delay environments, significantly outperforming other baseline algorithms in terms of asymptotic performance and sample efficiency."
    },
    {
        "title": "Flexible Fairness-Aware Learning via Inverse Conditional Permutation",
        "link_suffix": "/forum?id=d9JcSrQoeP",
        "link": "https://openreview.net/forum?id=d9JcSrQoeP",
        "pdf_link": "https://openreview.net/pdf?id=d9JcSrQoeP",
        "keywords": "Algorithmic fairness, Equalized odds, Adversarial learning, Inverse conditional permutation",
        "abstract": "Equalized odds, as a popular notion of algorithmic fairness, aims to ensure that sensitive variables, such as race and gender, do not unfairly influence the algorithm's prediction when conditioning on the true outcome. Despite rapid advancements, current research primarily focuses on equalized odds violations caused by a single sensitive attribute, leaving the challenge of simultaneously accounting for multiple attributes largely unaddressed.  We bridge this gap by introducing an in-processing fairness-aware learning approach, FairICP, which integrates adversarial learning with a novel inverse conditional permutation scheme. FairICP offers a theoretically justified, flexible, and efficient scheme to promote equalized odds under fairness conditions described by complex and multi-dimensional sensitive attributes. The efficacy and adaptability of our method are demonstrated through both simulation studies and empirical analyses of real-world datasets."
    },
    {
        "title": "DriveArena: A Closed-loop Generative Simulation Platform for Autonomous Driving",
        "link_suffix": "/forum?id=4S9bBbX1be",
        "link": "https://openreview.net/forum?id=4S9bBbX1be",
        "pdf_link": "https://openreview.net/pdf?id=4S9bBbX1be",
        "keywords": "Autonomous Driving, Diffusion Model, Closed-loop Simulation",
        "abstract": "This paper introduces DriveArena, the first high-fidelity closed-loop simulation system designed for driving agents navigating real-world scenarios. DriveArena comprises two core components:  Traffic Manager, a traffic simulator capable of generating realistic traffic flow on any global street map, and World Dreamer, a high-fidelity conditional generative model with infinite auto-regression. DriveArena supports closed-loop simulation using road networks from cities worldwide, enabling the generation of diverse traffic scenarios with varying styles. This powerful synergy empowers any driving agent capable of processing real-world images to navigate in  DriveArena's simulated environment. Furthermore,  DriveArena features a flexible, modular architecture, allowing for multiple implementations of its core components and driving agents. Serving as a highly realistic arena for these players, our work provides a valuable platform for developing and evaluating driving agents across diverse and challenging scenarios.  DriveArena takes a significant leap forward in leveraging generative models for driving simulation platforms, opening new avenues for closed-loop evaluation of autonomous driving systems."
    },
    {
        "title": "FLOWDREAMER: EXPLORING HIGH FIDELITY TEXT-TO-3D GENERATION VIA RECTIFIED FLOW",
        "link_suffix": "/forum?id=6UD3vymUst",
        "link": "https://openreview.net/forum?id=6UD3vymUst",
        "pdf_link": "https://openreview.net/pdf?id=6UD3vymUst",
        "keywords": "Text-to-3D, Rectified flow, Diffusion model",
        "abstract": "Recent advances in text-to-3D generation have made significant progress. In particular, with the pretrained diffusion models, existing methods predominantly use Score Distillation Sampling (SDS) to train 3D models such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3D GS). However, a hurdle is that they often encounter difficulties with over-smoothing textures and over-saturating colors. The rectified flow model \u2013 which utilizes a simple ordinary differential equation (ODE) to represent a straight trajectory \u2013 shows promise as an alternative prior to text-to-3D generation. It learns a time-independent vector field, thereby reducing the ambiguity in 3D model update gradients that are calculated using time-dependent scores in the SDS framework. In light of this, we first develop a mathematical analysis to seamlessly integrate SDS with rectified flow model, paving the way for our initial framework known as Vector Field Distillation Sampling (VFDS). However, empirical findings indicate that VFDS still results in over-smoothing outcomes. Therefore, we analyze the grounding reasons for such a failure from the perspective of ODE trajectories. On top, we propose a novel framework, named FlowDreamer, which yields high-fidelity results with richer textual details and faster convergence. The key insight is to leverage the coupling and reversible properties of the rectified flow model to search for the corresponding noise, rather than using randomly sampled noise as in VFDS. Accordingly, we introduce a novel Unique Couple Matching (UCM) loss, which guides the 3D model to optimize along the same trajectory. Our FlowDreamer is superior in its flexibility to be applied to both NeRF and 3D GS. Extensive experiments demonstrate the high-fidelity outcomes and accelerated convergence of FlowDreamer. Moreover, we highlight the intriguing open questions, such as initialization challenges in NeRF and sampling techniques, to benefit the research community"
    },
    {
        "title": "Exchange of Perspective Prompting Enhances Reasoning in Large Language Models",
        "link_suffix": "/forum?id=jBBjZp0EVs",
        "link": "https://openreview.net/forum?id=jBBjZp0EVs",
        "pdf_link": "https://openreview.net/pdf?id=jBBjZp0EVs",
        "keywords": "Large Language Models, Reasoning, Self-correction, External Perspective",
        "abstract": "Large language models (LLMs) have made significant advancements in addressing diverse natural language processing (NLP) tasks. However, their performance is often limited by inherent comprehension of problems. To address this limitation, we propose Exchange-of-Perspective (EoP), a novel framework designed to incorporate external perspectives by swapping answers for the same question presented with different definitions. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that EoP can significantly improve performance. For instance, compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we observe a 3.6% improvement on AQuA (60.6% \u2192 64.2%), while GPT-4-powered EoP achieves a 7.7% overall accuracy improvement on Math (53.9% \u2192 61.6%)."
    },
    {
        "title": "Demystifying Language Model Forgetting with Low-Rank Example Associations",
        "link_suffix": "/forum?id=ohqjYsRBD1",
        "link": "https://openreview.net/forum?id=ohqjYsRBD1",
        "pdf_link": "https://openreview.net/pdf?id=ohqjYsRBD1",
        "keywords": "catastrophic forgetting; language models",
        "abstract": "Large Language models (LLMs) suffer from forgetting of upstream data when fine-tuned. Despite efforts on mitigating forgetting, few have investigated whether, and how forgotten upstream examples are dependent on and associated with newly learned tasks. Insights on such associations enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting (measured in log-perplexity increase) that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices. We demonstrate that the matrices display simple low-rank patterns, often well-approximated with multiplicative scalar effects of upstream examples and newly learned tasks. We also examine fine-grained associations with visualization and statistics. Leveraging the low-rank nature of the associations, we predict forgetting of upstream examples when fine-tuning on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. The approach, despite simplicity, outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs for predicting forgetting. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay at fine-tuning."
    },
    {
        "title": "Robust Two-Hand Reconstruction with Additional 2D Information and Diffusion Prior",
        "link_suffix": "/forum?id=A0LYPN3jvm",
        "link": "https://openreview.net/forum?id=A0LYPN3jvm",
        "pdf_link": "https://openreview.net/pdf?id=A0LYPN3jvm",
        "keywords": "3D two-hand reconstruction",
        "abstract": "Recently, estimating 3D hand pose and shape from monocular images has garnered significant attention from researchers, which finds numerous applications in animation, AR/VR, and embodied AI. Many tasks in the field of computer vision have demonstrated the substantial benefits of incorporating additional task-relevant reference information to enhance model performance. In this paper, we investigate whether the principle of ``the more you know, the better you understand'' also applies to the task of two-hand recovery. Unlike previous methods that rely solely on monocular image features for hand estimation, we extract 2D keypoints, segmentation map, and depth map features and then integrate them with image features. The hand regressor subsequently estimates hand parameters based on the fused features. The 2D keypoints and segmentation maps provide detailed finger XY-dimensional reference information for the hand, while the depth map offers pixel-level relative Y-dimensional reference information. Recovering the 3D hand from these intermediate representations should be more straightforward than doing so solely from the original RGB image. Current foundation models have already achieved impressive performance on these basic tasks, allowing us to obtain reliable results in most cases. However, when the two hands overlap significantly, resulting in complex entanglements. In such cases, hand penetration is likely to arise. The additional reference information (segmentation map and depth map) cannot assist with the occluded regions, and the predicted 2D keypoints for the occluded areas are also unreliable. To this end, we further employ a two-hand diffusion model as a prior and employ gradient guidance to refine the two-hand contact. Extensive experiments demonstrate that our approach achieves superior performance in 2D consistency alignment and depth recovery."
    },
    {
        "title": "Sum-of-Squares Programming for Ma-Trudinger-Wang Regularity of Optimal Transport Maps",
        "link_suffix": "/forum?id=0AHkdAtFW8",
        "link": "https://openreview.net/forum?id=0AHkdAtFW8",
        "pdf_link": "https://openreview.net/pdf?id=0AHkdAtFW8",
        "keywords": "Optimal transport, sum-of-squares programming, Ma-Trudinger-Wang tensor",
        "abstract": "For a given ground cost, approximating the Monge optimal transport map that pushes forward a given probability measure onto another has become a staple in several modern machine learning algorithms. The fourth-order Ma-Trudinger-Wang (MTW) tensor associated with this ground cost function provides a notion of curvature in optimal transport. The non-negativity of this tensor plays a crucial role for establishing continuity for the Monge optimal transport map. It is, however, generally difficult to analytically verify this condition for any given ground cost. To expand the class of cost functions for which MTW non-negativity can be verified, we propose a provably correct computational approach which provides certificates of non-negativity for the MTW tensor using Sum-of-Squares (SOS) programming. We further show that our SOS technique can also be used to compute an inner approximation of the region where MTW non-negativity holds. We apply our proposed SOS programming method to several practical ground cost functions to approximate the regions of regularity of their corresponding optimal transport maps."
    },
    {
        "title": "ESpeW: Robust Copyright Protection for LLM-based EaaS via Embedding-Specific Watermark",
        "link_suffix": "/forum?id=BltNzMweBY",
        "link": "https://openreview.net/forum?id=BltNzMweBY",
        "pdf_link": "https://openreview.net/pdf?id=BltNzMweBY",
        "keywords": "NLP, Copyright Protection, Watermark, Backdoor, Embedding Model",
        "abstract": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings."
    }
]