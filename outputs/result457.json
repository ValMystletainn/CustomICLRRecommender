[{"title": "Enhancing End-to-End Autonomous Driving with Latent World Model", "link_suffix": "/forum?id=fd2u60ryG0", "link": "https://openreview.net/forum?id=fd2u60ryG0", "pdf_link": "https://openreview.net/pdf?id=fd2u60ryG0", "keywords": "end-to-end autonomous driving, world model, self-supervised learning", "abstract": "In autonomous driving, end-to-end planners directly utilize raw sensor data, enabling them to extract richer scene features and reduce information loss compared to traditional planners. This raises a crucial research question: how can we develop better scene feature representations to fully leverage sensor data in end-to-end driving? Self-supervised learning methods show great success in learning rich feature representations in NLP and computer vision. Inspired by this, we propose a novel self-supervised learning approach using the LAtent World model (LAW) for end-to-end driving. LAW predicts future latent scene features based on current features and ego trajectories. This self-supervised task can be seamlessly integrated into perception-free and perception-based frameworks, improving scene feature learning while optimizing trajectory prediction. LAW achieves state-of-the-art performance across multiple benchmarks, including real-world open-loop benchmark nuScenes, NAVSIM, and simulator-based closed-loop benchmark CARLA. The code will be released.", "title_embedding_index": 22800, "title_abs_embedding_index": 22825}, {"title": "Knowing Your Target : Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding", "link_suffix": "/forum?id=WOzffPgVjF", "link": "https://openreview.net/forum?id=WOzffPgVjF", "pdf_link": "https://openreview.net/pdf?id=WOzffPgVjF", "keywords": "Spatio-Temporal Video Grounding", "abstract": "Transformer has attracted increasing interest in spatio-temporal video grounding, or STVG, owing to its end-to-end pipeline and promising result. Existing Transformer-based STVG approaches often leverage a set of object queries, which are initialized simply using zeros and then gradually learn target position information via iterative interactions with multimodal features, for spatial and temporal localization. Despite simplicity, these zero object queries, due to lacking target-specific cues, are hard to learn discriminative target information from interactions with multimodal features in the complicated scenarios (e.g., with distractors or occlusion), resulting in degradation. Addressing this, we introduce a novel Target-Aware Transformer for STVG (TA-STVG), which seeks to adaptively generate object queries via exploring target-specific cues from the given video-text pair, for improving STVG. The key lies in two simple yet effective modules, comprising text-guided temporal sampling (TTS) and attribute-aware spatial activation (ASA), working in a cascade. The former focuses on selecting target-relevant temporal cues from a video utilizing holistic text information, while the latter aims at further exploiting the fine-grained visual attribute information of the object from previous target-aware temporal cues, which is applied for object query initialization. Compared to existing methods leveraging zero-initialized queries, object queries in our TA-STVG, directly generated from a given video-text pair, naturally carry target-specific cues, making them adaptive and better interact with multimodal features for learning more discriminative information to improve STVG. In our experiments on three benchmarks, including HCSTVG-v1/-v2 and VidSTG, TA-STVG achieves state-of-the-art performance and largely outperforms the baseline, validating its efficacy. Code will be released.", "title_embedding_index": 22801, "title_abs_embedding_index": 22826}, {"title": "CSGO: Content-Style Composition in Text-to-Image Generation", "link_suffix": "/forum?id=E3PgLQzPob", "link": "https://openreview.net/forum?id=E3PgLQzPob", "pdf_link": "https://openreview.net/pdf?id=E3PgLQzPob", "keywords": "image generation, style transfer, stylized synthesis", "abstract": "The diffusion model has shown exceptional capabilities in controlled image generation, which has further fueled interest in image style transfer. Existing works mainly focus on training free-based methods (e.g., image inversion) due to the scarcity of specific data. In this study, we present a data construction pipeline for content-style-stylized image triplets that generates and automatically cleanses stylized triplets. Based on this pipeline, we construct a dataset IMAGStyle, the first large-scale style transfer dataset containing 210k image triplets, available for the community to explore and research.Equipped with IMAGStyle, we propose a simple yet effective framework CSGO, a style transfer model based on end-to-end training, which explicitly decouples content and style features employing independent feature injection.  Our CSGO implements image-driven style transfer, text-driven stylized synthesis, and text editing-driven stylized synthesis in the same model.\nWe conduct extensive experiments on CSGO to validate the effectiveness of synthetic stylized data for style control. Meanwhile, ablation experiments show the effectiveness of CSGO.", "title_embedding_index": 22802, "title_abs_embedding_index": 22827}, {"title": "Collapsed Language Models Promote Fairness", "link_suffix": "/forum?id=kynD1UUk6q", "link": "https://openreview.net/forum?id=kynD1UUk6q", "pdf_link": "https://openreview.net/pdf?id=kynD1UUk6q", "keywords": "Neural Collapse, Fairness", "abstract": "To mitigate societal biases implicitly encoded in recent successful pretrained language models, a diverse array of approaches have been proposed to encourage model fairness, focusing on prompting, data augmentation, regularized fine-tuning, and more. Despite the development, it is nontrivial to reach a principled understanding of fairness and an effective algorithm that can consistently debias language models. In this work, by rigorous evaluations of Neural Collapse -- a learning phenomenon happen in last-layer representations and classifiers in deep networks -- on fairness-related words, we find that debiased language models exhibit collapsed alignment between token representations and word embeddings. More importantly, this observation inspires us to design a principled fine-tuning method that can effectively improve fairness in a wide range of debiasing methods, while still preserving the performance of language models on standard natural language understanding tasks. We attach our code athttps://anonymous.4open.science/r/Fairness_NC-457E.", "title_embedding_index": 22803, "title_abs_embedding_index": 22828}, {"title": "Laplace-Transform-Filters render spectral Graph Neural Networks transferable", "link_suffix": "/forum?id=cTDooc2J9S", "link": "https://openreview.net/forum?id=cTDooc2J9S", "pdf_link": "https://openreview.net/pdf?id=cTDooc2J9S", "keywords": "Graph Neural Networks, Spectral Graph Theory, Transferability", "abstract": "We introduce a new point of view on transferability of graph neural networks based on the intrinsic notion of information diffusion within graphs. Transferability of graph neural networks is then considered between graphs that are similar from this novel perspective of information diffusion. After carefully analysing transferability of single filters, the transferability properties of entire networks are reduced to the transferability characteristics of the filter functions employed inside their convolutional blocks. A rigorous analysis establishes our main  theoretical finding: Spectral convolutional networks are transferable if their filters arise as Laplace transforms of certain generalized functions. Example settings illustrate the developed theory and numerical experiments validate the theoretical findings in practice.", "title_embedding_index": 22804, "title_abs_embedding_index": 22829}, {"title": "Emerging Tracking from Video Diffusion", "link_suffix": "/forum?id=UDeARVACQi", "link": "https://openreview.net/forum?id=UDeARVACQi", "pdf_link": "https://openreview.net/pdf?id=UDeARVACQi", "keywords": "Pixel-level object tracking, Temporal correspondence, Diffusion models", "abstract": "We find video diffusion models, renowned for their generative capabilities, surprisingly excel at pixel-level object tracking without any explicit training for this task. We introduce a simple and effective method to extract motion representations from video diffusion models, achieving state-of-the-art tracking results. Our approach enables the tracking of identical objects, overcoming limitations of previous methods reliant on intra-frame appearance correspondence. Visualizations and empirical results show that our approach outperforms recent supervised and self-supervised tracking methods, including the state-of-the-art, by up to 6 points. Our work demonstrates video generative models can learn intrinsic temporal dynamics of video, and excel in tracking tasks beyond original video synthesis.", "title_embedding_index": 22805, "title_abs_embedding_index": 22830}, {"title": "Efficient Multi-modal Large Language Models via Visual Token Grouping", "link_suffix": "/forum?id=ym1dS37mZE", "link": "https://openreview.net/forum?id=ym1dS37mZE", "pdf_link": "https://openreview.net/pdf?id=ym1dS37mZE", "keywords": "Large Language Model, Multi-modal Learning", "abstract": "The development of Multi-modal Large Language Models (MLLMs) has significantly advanced various downstream applications, including visual question answering and image captioning. However, the substantial computational costs associated with processing high-resolution images and videos pose a barrier to their broader adoption. To address this challenge, compressing vision tokens in MLLMs has emerged as a promising approach to reduce inference costs. In this paper, we introduce \\methodname, a novel grouping mechanism that leverages the capabilities of pretrained vision encoders to group similar image segments without the need for segmentation masks. With the isolated attention we adopt, \\methodname can identify and eliminate redundant visual tokens, which effectively reduces computational demands. Extensive experiments demonstrate that the effectiveness of\\methodname , maintains over 98.1% of the original performance while achieving a reduction of over 27% in TFLOPS.", "title_embedding_index": 22806, "title_abs_embedding_index": 22831}, {"title": "Patch-Level Training for Large Language Models", "link_suffix": "/forum?id=dDpB23VbVa", "link": "https://openreview.net/forum?id=dDpB23VbVa", "pdf_link": "https://openreview.net/pdf?id=dDpB23VbVa", "keywords": "large language models, patch-level training", "abstract": "The prohibitive training costs of Large Language Models (LLMs) have emerged as a significant bottleneck in the development of next-generation LLMs. In this paper, we show that it is possible to significantly reduce the training costs of LLMs without sacrificing their performance. Specifically, we introduce patch-level training for LLMs, in which multiple tokens are aggregated into a unit of higher information density, referred to as a `patch', to serve as the fundamental text unit for training LLMs. During patch-level training, we feed the language model shorter sequences of patches and train it to predict the next patch, thereby processing the majority of the training data at a significantly reduced cost. Following this, the model continues token-level training on the remaining training data to align with the inference mode. Experiments on a diverse range of models (370M-2.7B parameters) demonstrate that patch-level training can reduce the overall training costs to 0.5$\\times$, without compromising the model performance compared to token-level training.", "title_embedding_index": 22807, "title_abs_embedding_index": 22832}, {"title": "Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models", "link_suffix": "/forum?id=daRu82GAoZ", "link": "https://openreview.net/forum?id=daRu82GAoZ", "pdf_link": "https://openreview.net/pdf?id=daRu82GAoZ", "keywords": "Diffusion Models, AI Security, Origin Identification", "abstract": "Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused forspreading misinformation,infringing on copyrights, andevading content tracing. This motivates us to introduce the task of originIDentification for text-guidedImage-to-imageDiffusion models (ID$\\mathbf{^2}$), aiming to retrieve the original image of a given translated query. A straightforward solution to ID$^2$ involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due tovisual discrepancyacross generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID$^2$ task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset,OriPID, contains abundantOrigins and guidedPrompts, which can be used to train and test potentialIDentification models across various diffusion models. In the method section, we first prove theexistenceof a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder (VAE) embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can begeneralizedacross different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods (+31.6% mAP), even those with domain generalization designs.", "title_embedding_index": 22808, "title_abs_embedding_index": 22833}, {"title": "Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection", "link_suffix": "/forum?id=EdMb9TqqDY", "link": "https://openreview.net/forum?id=EdMb9TqqDY", "pdf_link": "https://openreview.net/pdf?id=EdMb9TqqDY", "keywords": "text to image generation, visual instruction generation", "abstract": "Visual instructions for long-horizon tasks are crucial as they intuitively clarify complex concepts and enhance retention across extended steps. \nDirectly generating a series of images using text-to-image models without considering the context of previous steps results in inconsistent images, increasing cognitive load.  Additionally, the generated images often miss objects or the attributes such as color, shape, and state of the objects are inaccurate.\nTo address these challenges, we propose LIGER, the first training-free framework for Long-horizon Instruction GEneration with logic and attribute self-Reflection. LIGER first generates a draft image for each step with the historical prompt and visual memory of previous steps. This step-by-step generation approach maintains consistency between images in long-horizon tasks. Moreover, LIGER utilizes various image editing tools to rectify errors including wrong attributes, logic errors, object redundancy, and identity inconsistency in the draft images. Through this self-reflection mechanism, LIGER improves the logic and object attribute correctness of the images.\nTo verify whether the generated images assist human understanding, we manually curated a new benchmark consisting of various long-horizon tasks. Human-annotated ground truth expressions reflect the human-defined criteria for how an image should appear to be illustrative. \nExperiments demonstrate the visual instructions generated by LIGER are more comprehensive compared with baseline methods. The code and dataset will be available once accepted.", "title_embedding_index": 22809, "title_abs_embedding_index": 22834}, {"title": "DSEG-LIME: Improving Image Explanation by Hierarchical Data-Driven Segmentation", "link_suffix": "/forum?id=sOOrTkYgb6", "link": "https://openreview.net/forum?id=sOOrTkYgb6", "pdf_link": "https://openreview.net/pdf?id=sOOrTkYgb6", "keywords": "XAI, LIME, Segmentation", "abstract": "Explainable Artificial Intelligence (XAI) is crucial in unraveling decision-making processes in complex machine learning models. LIME (Local Interpretable Model-agnostic Explanations) is a well-known XAI framework for image analysis. It utilizes image segmentation to create features to identify relevant areas for classification. Consequently, poor segmentation can compromise the consistency of the explanation and undermine the importance of the segments, affecting the overall interpretability. Addressing these challenges, we introduce DSEG-LIME (Data-Driven Segmentation LIME), featuring: i) a data-driven segmentation for human-recognized feature generation by foundation model integration, and ii) a user steered granularity in the hierarchical segmentation procedure through composition. We evaluate DSEG-LIME on pre-trained models using ImageNet classes, explicitly targeting scenarios without domain-specific knowledge. Our findings demonstrate that DSEG outperforms most of the XAI metrics and enhances the alignment of explanations with human-recognized concepts, significantly improving interpretability.", "title_embedding_index": 22810, "title_abs_embedding_index": 22835}, {"title": "AutoGUI: Scaling GUI Grounding with Automatic Functionality Annotations from LLMs", "link_suffix": "/forum?id=wl4c9jvcyY", "link": "https://openreview.net/forum?id=wl4c9jvcyY", "pdf_link": "https://openreview.net/pdf?id=wl4c9jvcyY", "keywords": "Vision language model, Large language model, Embodied AI, GUI understanding, Web agent", "abstract": "User interface understanding with vision-language models has received much attention due to its potential for enabling next-generation software automation.\nHowever, existing UI datasets either only provide large-scale context-free element annotations or contextualized functional descriptions for elements at a much smaller scale.\nIn this work, we propose theAutoGUIpipeline for automatically annotating UI elements with detailed functionality descriptions at scale.\nSpecifically, we leverage large language models (LLMs) to infer element functionality by comparing the UI content changes before and after simulated interactions with specific UI elements. To improve annotation quality, we propose LLM-aided rejection and verification, eliminating invalid and incorrect annotations without human labor.\nWe construct anAutoGUI-704kdataset using the proposed pipeline, featuring multi-resolution, multi-device screenshots, diverse data domains, and detailed functionality annotations that have never been provided by previous datasets.\nHuman evaluation shows that theAutoGUIpipeline achieves annotation correctness comparable to trained human annotators. Extensive experimental results show that ourAutoGUI-704kdataset remarkably enhances VLM's UI grounding capabilities, exhibits significant scaling effects, and outperforms existing web pre-training data types. We envision AutoGUI as a scalable pipeline for generating massive data to build GUI-oriented VLMs. AutoGUI dataset can be viewed at this anonymous URL:https://huggingface.co/AutoGUI.", "title_embedding_index": 22811, "title_abs_embedding_index": 22836}, {"title": "Learning Harmonized Representations for Speculative Sampling", "link_suffix": "/forum?id=T9u56s7mbk", "link": "https://openreview.net/forum?id=T9u56s7mbk", "pdf_link": "https://openreview.net/pdf?id=T9u56s7mbk", "keywords": "speculative sampling, large language model", "abstract": "Speculative sampling is a promising approach to accelerate the decoding stage for Large Language Models (LLMs). Recent advancements that leverage target LLM's contextual information, such as hidden states and KV cache, have shown significant practical improvements. However, these approaches suffer from inconsistent context between training and decoding. We also observe another discrepancy between the training and decoding objectives in existing speculative sampling methods. In this work, we propose a solution named HArmonized Speculative Sampling (HASS) that learns harmonized representations to address these issues. HASS accelerates the decoding stage without adding inference overhead through harmonized objective distillation and harmonized context alignment. Experiments on four LLaMA models demonstrate that HASS achieves 2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, surpassing EAGLE-2 by 8%-20%. The code is available athttps://github.com/HArmonizedSS/HASS.", "title_embedding_index": 22812, "title_abs_embedding_index": 22837}, {"title": "MultiBand: Multi-Task Song Generation with Personalized Prompt-Based Control", "link_suffix": "/forum?id=MGAzLOJOYL", "link": "https://openreview.net/forum?id=MGAzLOJOYL", "pdf_link": "https://openreview.net/pdf?id=MGAzLOJOYL", "keywords": "multi-task song generation, prompt-based style control, style transfer, singing voice synthesis, music generation", "abstract": "Song generation focuses on producing controllable high-quality songs based on various personalized prompts. However, existing methods struggle to generate high-quality vocals and accompaniments with effective style control and proper alignment. Additionally, they fall short in supporting various personalized tasks based on diverse prompts. To address these challenges, we introduce MultiBand, the first multi-task song generation model for synthesizing high-quality, aligned songs with extensive control based on diverse personalized prompts. \nMultiBand comprises these primary models: 1) VocalBand, a decoupled model, leverages the flow-matching method for singing styles, pitches, and mel-spectrograms generation, allowing fast and high-quality vocal generation with high-level control. 2) AccompBand, a flow-based transformer model, incorporates the Aligned Vocal Encoder, using contrastive learning for alignment, and Band-MOE, selecting suitable experts for enhanced quality and control. This model allows for generating controllable, high-quality accompaniments perfectly aligned with vocals. 3) Two generation models, LyricBand for lyrics and MelodyBand for melodies, contribute to the comprehensive multi-task song generation system, allowing for extensive control based on multiple personalized prompts. Experimental results demonstrate that MultiBand performs better over baseline models across multiple tasks using objective and subjective metrics.", "title_embedding_index": 22813, "title_abs_embedding_index": 22838}, {"title": "Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates", "link_suffix": "/forum?id=JHE4w8q2G2", "link": "https://openreview.net/forum?id=JHE4w8q2G2", "pdf_link": "https://openreview.net/pdf?id=JHE4w8q2G2", "keywords": "Multivariate time series forecasting with sparse observations, Multi-View Representation Learning, Offline knowledge distillation, Multi-view contrastive learning", "abstract": "Multivariate time series forecasting (MTSF) aims to predict the future values of multiple interrelated time series and support decision-making. While deep learning models have attracted much attention in MTSF for their powerful spatial-temporal encoding capabilities, they frequently encounter the challenge of missing data resulting from numerous malfunctioning data collectors in practice. In this case, existing models only rely on sparse observations and are prone to capturing incorrect semantics, leading to a decline in their forecasting performance. Furthermore, the unfixed missing rates across different samples in reality pose robustness challenges. To address these issues, we propose Multi-View Representation Learning (Merlin) based on offline knowledge distillation and multi-view contrastive learning, which aims to help existing models achieve semantic alignment between sparse observations with different missing rates and complete observations, and enhance their robustness. On the one hand, we introduce offline knowledge distillation where a teacher model guides a student model in learning how to extract semantics from sparse observations similar to those obtainable from complete observations. On the other hand, we construct positive and negative data pairs using sparse observations with different missing rates. Then, we use multi-view contrastive learning to help the student model align semantics across sparse observations with different missing rates, thereby further enhancing its robustness. In this way, Merlin can fully enhance the robustness of existing forecasting models to MTS with unfixed missing rates and achieves high-precision MTSF with sparse observations. Experiments on four real-world datasets validate our motivation and demonstrate the superiority and practicability of Merlin.", "title_embedding_index": 22814, "title_abs_embedding_index": 22839}, {"title": "MisAttributionLLM: Integrating Error Attribution Capability into LLM Evaluation", "link_suffix": "/forum?id=Q5eo3VMxF6", "link": "https://openreview.net/forum?id=Q5eo3VMxF6", "pdf_link": "https://openreview.net/pdf?id=Q5eo3VMxF6", "keywords": "evaluation, error attribution, large language models, llm-as-a-judge", "abstract": "With the widespread application of Large Language Models (LLMs) in various tasks, evaluating the performance of LLMs becomes an essential research topic. However, existing judge models lack the specific capability required for error attribution (i.e., identify the types of  error made in responses). In this work, we first establish a comprehensive Misattribution Framework with 9 primary and 19 secondary  categories, which are intended to facilitate in-depth analysis and enhance the performance of LLMs. Based on this framework, we present  AttriData, a dataset specifically designed for error attribution, encompassing misattributions, along with the corresponding scores and feedback. We also propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first open-source, general-purpose judge model  with error attribution capability which provides valuable insights into the model\u2019s weaknesses and enables targeted improvements. Experimental results show that MisAttributionLLM achieves the highest Pearson correlation with human evaluators among 8 open-source  and closed-source LLMs. Furthermore, MisAttributionLLM also obtains the highest accuracy and micro-F1 in the performance of error attribution. Extensive experiments and analyses are conducted to confirm the effectiveness and robustness of our proposed method.", "title_embedding_index": 22815, "title_abs_embedding_index": 22840}, {"title": "MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation", "link_suffix": "/forum?id=yFEqYwgttJ", "link": "https://openreview.net/forum?id=yFEqYwgttJ", "pdf_link": "https://openreview.net/pdf?id=yFEqYwgttJ", "keywords": "vision-guided audio generation, fast inference, open-domain sound synthesis, masked diffusion models, temporal learning, visual sound source localization, generative AI", "abstract": "We introduce MDSGen, a novel framework for vision-guided open-domain sound generation optimized for model parameter size, memory consumption, and inference speed. This framework incorporates two key innovations: (1) a redundant video feature removal module that filters out unnecessary visual information, and (2) a temporal-aware masking strategy that leverages temporal context for enhanced audio generation accuracy. In contrast to existing resource-heavy Unet-based models, MDSGen employs denoising masked diffusion transformers,  facilitating efficient generation without reliance on pre-trained diffusion models. Evaluated on the benchmark VGGSound dataset, our smallest model (5M parameters) achieves 97.9% alignment accuracy, using 172x fewer parameters, 371% less memory, and offering 36x faster inference than the current 860M-parameter state-of-the-art model (93.9% accuracy). The larger model (131M parameters) reaches nearly 99% accuracy while requiring 6.5x fewer parameters. These results highlight the scalability and effectiveness of our approach.", "title_embedding_index": 22816, "title_abs_embedding_index": 22841}, {"title": "LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models", "link_suffix": "/forum?id=z8sxoCYgmd", "link": "https://openreview.net/forum?id=z8sxoCYgmd", "pdf_link": "https://openreview.net/pdf?id=z8sxoCYgmd", "keywords": "LMMs\uff1bDeepfake\uff1bMultimodality", "abstract": "With the rapid development of AI-generated content, the future internet may be inundated with synthetic data, making the discrimination of authentic and credible multimodal data increasingly challenging. Synthetic data detection has thus garnered widespread attention, and the performance of large multimodal models (LMMs) in this task has attracted significant interest. LMMs can provide natural language explanations for their authenticity judgments, enhancing the explainability of synthetic content detection. Simultaneously, the task of distinguishing between real and synthetic data effectively tests the perception, knowledge, and reasoning capabilities of LMMs. In response, we introduce LOKI, a novel benchmark designed to evaluate the ability of LMMs to detect synthetic data across multiple modalities. LOKI encompasses video, image, 3D, text, and audio modalities, comprising 18K carefully curated questions across 26 subcategories with clear difficulty levels. The benchmark includes coarse-grained judgment and multiple-choice questions, as well as fine-grained anomaly selection and explanation tasks, allowing for a comprehensive analysis of LMMs. We evaluated 22 open-source LMMs and 6 closed-source models on LOKI, highlighting their potential as synthetic data detectors and also revealing some limitations in the development of LMM capabilities. More information about LOKI can be found athttps://loki102.github.io/LOKI.github.io/.", "title_embedding_index": 22817, "title_abs_embedding_index": 22842}, {"title": "Learning Augmentation Policies from A Model Zoo for Time Series Forecasting", "link_suffix": "/forum?id=56Zn3halhq", "link": "https://openreview.net/forum?id=56Zn3halhq", "pdf_link": "https://openreview.net/pdf?id=56Zn3halhq", "keywords": "Time Series Forecasting, Data Augmentation", "abstract": "Time series forecasting models typically rely on a fixed-size training set and treat all data uniformly, which may not effectively capture the specific patterns present in more challenging training samples. To address this issue, we introduce AutoTSAug, a learnable data augmentation method based on reinforcement learning. Our approach begins with an empirical analysis to determine which parts of the training data should be augmented. Specifically, we identify the so-called marginal samples by considering the prediction diversity across a set of pretrained forecasting models. Next, we propose using variational masked autoencoders as the augmentation model and applying the REINFORCE algorithm to transform the marginal samples into new data. The goal of this generative model is not only to mimic the distribution of real data but also to reduce the variance of prediction errors across the model zoo. By augmenting the marginal samples with a learnable policy, AutoTSAug substantially improves forecasting performance, advancing the prior art in this field with minimal additional computational cost.", "title_embedding_index": 22818, "title_abs_embedding_index": 22843}, {"title": "SDDBench: A Benchmark for Synthesizable Drug Design", "link_suffix": "/forum?id=IjiIPQcLbV", "link": "https://openreview.net/forum?id=IjiIPQcLbV", "pdf_link": "https://openreview.net/pdf?id=IjiIPQcLbV", "keywords": "Drug Discovery", "abstract": "A significant challenge in wet lab experiments with current drug design generative models is the trade-off between pharmacological properties and synthesizability. Molecules predicted to have highly desirable properties are often difficult to synthesize, while those that are easily synthesizable tend to exhibit less favorable properties. As a result, evaluating the synthesizability of molecules in general drug design scenarios remains a significant challenge in the field of drug discovery. The commonly used synthetic accessibility (SA) score aims to evaluate the ease of synthesizing generated molecules, but it falls short of guaranteeing that synthetic routes can actually be found. Inspired by recent advances in top-down synthetic route generation, we propose a new, data-driven metric to evaluate molecule synthesizability. Our approach directly assesses the feasibility of synthetic routes for a given molecule through our proposed round-trip score. This novel metric leverages the synergistic duality between retrosynthetic planners and reaction predictors, both of which are trained on extensive reaction datasets. To demonstrate the efficacy of our method, we conduct a comprehensive evaluation of round-trip scores alongside search success rate across a range of representative molecule generative models.", "title_embedding_index": 22819, "title_abs_embedding_index": 22844}, {"title": "SONICS: Synthetic Or Not - Identifying Counterfeit Songs", "link_suffix": "/forum?id=PY7KSh29Z8", "link": "https://openreview.net/forum?id=PY7KSh29Z8", "pdf_link": "https://openreview.net/pdf?id=PY7KSh29Z8", "keywords": "deepfake detection, fake song detection, synthetic song detection, efficient model, dataset, audio processing", "abstract": "The recent surge in AI-generated songs presents exciting possibilities and challenges. While these inventions democratize music creation, they also necessitate the ability to distinguish between human-composed and synthetic songs to safeguard artistic integrity and protect human musical artistry. Existing research and datasets in fake song detection only focus on singing voice deepfake detection (SVDD), where the vocals are AI-generated but the instrumental music is sourced from real songs. However, these approaches are inadequate for detecting contemporary end-to-end artificial songs where all components (vocals, music, lyrics, and style) could be AI-generated. Additionally, existing datasets lack music-lyrics diversity, long-duration songs, and open-access fake songs. To address these gaps, we introduce SONICS, a novel dataset for end-to-end Synthetic Song Detection (SSD), comprising over 97k songs (4,751 hours) with over 49k synthetic songs from popular platforms like Suno and Udio. Furthermore, we highlight the importance of modeling long-range temporal dependencies in songs for effective authenticity detection, an aspect entirely overlooked in existing methods. To utilize long-range patterns, we introduce SpecTTTra, a novel architecture that significantly improves time and memory efficiency over conventional CNN and Transformer-based models. In particular, for long audio samples, our top-performing variant outperforms ViT by 8% F1 score while being 38% faster and using 26% less memory. Additionally, in comparison with ConvNeXt, our model achieves 1% gain in F1 score with 20% boost in speed and 67% reduction in memory usage. Other variants of our model family provide even better speed and memory efficiency with competitive performance.", "title_embedding_index": 22820, "title_abs_embedding_index": 22845}, {"title": "Pruning Aggregation Parameters for Large Language Models", "link_suffix": "/forum?id=ji6MYm4Htg", "link": "https://openreview.net/forum?id=ji6MYm4Htg", "pdf_link": "https://openreview.net/pdf?id=ji6MYm4Htg", "keywords": "LLM, Pruning", "abstract": "Pruning is a highly effective approach for compressing large language models (LLMs). By strategically reducing model size, pruning significantly decreases both latency and GPU memory usage during inference, resulting in more efficient and cost-effective deployment of these models. Despite their effectiveness, current structured pruning algorithms have limitations. They still require extensive continued pre-training on large datasets to achieve model compression. Moreover, most of these methods are unable to reduce the memory usage of the key-value cache during generation tasks. In this work, we propose a novel pruning algorithm that requires no additional training and targets specific parameters within LLMs. We classify the model's parameters into three categories: aggregation, transformation, and normalization. Our method primarily focuses on pruning the aggregation parameters in the higher layers of the model. To further improve the performance of the pruned LLM, we also introduce a rescaling parameter that adjusts the output of the pruned block. We conduct comprehensive experiments on a wide range of LLMs, including LLaMA3.1-8B/70B, Qwen2-7B/72B, Gemma2-9B, and Mistral-7B-v0.3. Our evaluation includes both generation and discriminative tasks across various benchmarks. The results consistently demonstrate that our method outperforms recent block pruning methods. This improvement is particularly notable in generation tasks, where our approach significantly outperforms existing baselines.", "title_embedding_index": 22821, "title_abs_embedding_index": 22846}, {"title": "Why pre-training is beneficial for downstream classification tasks?", "link_suffix": "/forum?id=mSYX71lNAl", "link": "https://openreview.net/forum?id=mSYX71lNAl", "pdf_link": "https://openreview.net/pdf?id=mSYX71lNAl", "keywords": "pre-training, explainable AI", "abstract": "It is widely acknowledged that pre-training brings benefits to downstream tasks by boosting accuracy and speeding up convergence, but the exact reasons for these two benefits still remain unclear.\nTo this end, we propose to quantitatively and accurately explain effects of pre-training on the downstream task from a novel game-theoretic view, which also sheds new light into the learning behavior of deep neural networks (DNNs).\nSpecifically, we extract and quantify the knowledge encoded by the pre-trained model, and further track the changes of such knowledge during the fine-tuning process.\nInterestingly, we discover that only a limited amount of pre-trained model's knowledge is preserved for the inference of downstream tasks, and such preserved knowledge is very difficult for a model training from scratch to learn.\nThus, with the help of this exclusively learned and useful knowledge, the fine-tuned model usually achieves better performance.\nBesides, we discover that pre-training can guide the fine-tuned model to learn target knowledge of the downstream task more directly and quickly than the model training from scratch, which accounts for the faster convergence of the fine-tuned model.\nThe code will be released when the paper is accepted.", "title_embedding_index": 22822, "title_abs_embedding_index": 22847}, {"title": "Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models", "link_suffix": "/forum?id=tTBXePRKSx", "link": "https://openreview.net/forum?id=tTBXePRKSx", "pdf_link": "https://openreview.net/pdf?id=tTBXePRKSx", "keywords": "Large Vision-Language Models, Hallucinations, Generative Feedback", "abstract": "While recent Large Vision-Language Models (LVLMs) have shown remarkable performance in multi-modal tasks, they are prone to generating hallucinatory text responses that do not align with the given visual input, which restricts their practical applicability in real-world scenarios. In this work, inspired by the observation that the text-to-image generation process is the inverse of image-conditioned response generation in LVLMs, we explore the potential of leveraging text-to-image generative models to assist in mitigating hallucinations in LVLMs. We discover that generative models can offer valuable self-feedback for mitigating hallucinations at both the response and token levels. Building on this insight, we introduce self-correcting Decoding with Generative Feedback (DeGF), a novel training-free algorithm that incorporates feedback from text-to-image generative models into the decoding process to effectively mitigate hallucinations in LVLMs. Specifically, DeGF generates an image from the initial response produced by LVLMs, which acts as an auxiliary visual reference and provides self-feedback to verify and correct the initial response through complementary or contrastive decoding. Extensive experimental results validate the effectiveness of our approach in mitigating diverse types of hallucinations, consistently surpassing state-of-the-art methods across five benchmarks.", "title_embedding_index": 22823, "title_abs_embedding_index": 22848}, {"title": "Mitigating Catastrophic Forgetting in Large Language Models with Forgetting-aware Pruning", "link_suffix": "/forum?id=fHvh913U1H", "link": "https://openreview.net/forum?id=fHvh913U1H", "pdf_link": "https://openreview.net/pdf?id=fHvh913U1H", "keywords": "Large Language Models, Catastrophic Forgetting, Neural Network Pruning", "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks. These models are typically pretrained on extensive corpora and subsequently fine-tuned on task-specific datasets. However, during the fine-tuning process, LLMs often suffer from Catastrophic Forgetting (CF), wherein previously acquired general knowledge is lost. Traditional approaches to mitigating CF often rely on data replay, which may not be viable when the original training data is inaccessible. Additionally, methods that alter the training process or the model architecture can increase complexity and detract from the accuracy of downstream tasks, thus limiting their generalizability. In this paper, we propose Forgetting-Aware Pruning Metric (FAPM), a novel pruning-based approach to balance CF and downstream task performance. Our investigation reveals that the degree to which task vectors (i.e., the subtraction of pre-trained weights from the weights fine-tuned on downstream tasks) overlap with pre-trained model parameters is a critical factor for CF. Motivated by this insight, FAPM employs the ratio of the task vector to pre-trained model parameters as a metric to quantify CF, integrating this measure into the pruning criteria. Importantly, FAPM does not necessitate modifications to the training process or model architecture, nor does it require any auxiliary data. We conducted extensive experiments across six datasets encompassing natural language inference, question answering, reading comprehension, and cloze tests. The results demonstrate that FAPM limits CF to just 1% while maintaining 99% accuracy on downstream tasks, rendering FAPM highly competitive relative to the state-of-the-art methods that involve modifications to the training process.", "title_embedding_index": 22824, "title_abs_embedding_index": 22849}]