[{"title": "Two Halves Make a Whole: How to Reconcile Soundness and Robustness in Watermarking for Large Language Models", "link_suffix": "/forum?id=hULJCP47PU", "link": "https://openreview.net/forum?id=hULJCP47PU", "pdf_link": "https://openreview.net/pdf?id=hULJCP47PU", "keywords": "Large language model, Watermark, Robustness, Soundness", "abstract": "Watermarking techniques have been used to safeguard AI-generated content. In this paper, we study publicly detectable watermarking schemes (Fairoze et al.), and have several research findings.First, we observe that two important security properties, robustness and soundness, may conflict with each other. We then formally investigate these two properties in the presence of an arguably more realistic adversary that we called editing-adversary, and we can prove an impossibility result that, the robustness and soundness properties cannot be achieved via a publicly-detectable single watermarking scheme.Second, we demonstrate our main result: we for the first time introduce the new concept of publicly-detectable dual watermarking scheme, for AI-generated content. We provide a novel construction by using two publicly-detectable watermarking schemes; each of the two watermarking schemes can achieve \u201chalf\u201d of the two required properties: one can achieve robustness, and the other can achieve soundness. Eventually, we can combine the two halves into a whole, and achieve the robustness and soundness properties at the same time. Our construction has been implemented and evaluated.", "title_embedding_index": 14100, "title_abs_embedding_index": 14125}, {"title": "PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems", "link_suffix": "/forum?id=fU8H4lzkIm", "link": "https://openreview.net/forum?id=fU8H4lzkIm", "pdf_link": "https://openreview.net/pdf?id=fU8H4lzkIm", "keywords": "Physics-encoded; Spatiotemporal PDEs; Graph Network; Deep Learning;", "abstract": "Solving partial differential equations (PDEs) serves as a cornerstone for modeling complex dynamical systems. Recent progresses have demonstrated grand benefits of data-driven neural-based models for predicting spatiotemporal dynamics (e.g., tremendous speedup gain compared with classical numerical methods). However, most existing neural models rely on rich training data, have limited extrapolation and generalization abilities, and suffer to produce precise or reliable physical prediction under intricate conditions (e.g., irregular mesh or geometry, complex boundary conditions, diverse PDE parameters, etc.). To this end, we propose a new graph learning approach, namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model spatiotemporal PDE systems on irregular meshes given small training datasets. Specifically, we incorporate a GNN into a numerical integrator to approximate the temporal marching of spatiotemporal dynamics for a given PDE system. Considering that many physical phenomena are governed by diffusion processes, we further design a learnable Laplace block, which encodes the discrete Laplace-Beltrami operator, to aid and guide the GNN learning in a physically feasible solution space. A boundary condition padding strategy is also designed to improve the model convergence and accuracy. Extensive experiments demonstrate that PhyMPGN is capable of accurately predicting various types of spatiotemporal dynamics on coarse unstructured meshes, consistently achieves the state-of-the-art results, and outperforms other baselines with considerable gains.", "title_embedding_index": 14101, "title_abs_embedding_index": 14126}, {"title": "Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning", "link_suffix": "/forum?id=HtvZCGiATs", "link": "https://openreview.net/forum?id=HtvZCGiATs", "pdf_link": "https://openreview.net/pdf?id=HtvZCGiATs", "keywords": "Latent Causal Models, Multimodal Learning, Identifiability, Contrastive Learning", "abstract": "Directed acyclic graphs (DAGs) are often assumed in causal discovery, however, accurately identifying these DAGs necessitates various assumptions, particularly in latent causal models, which can be challenging to validate in real-world applications. This raises a critical question: Are DAG assumptions truly necessary for certain applications? In this work, we introduce a novel latent partial causal model for multimodal data, which features two latent coupled variables, connected by an undirected edge, effectively representing transferable knowledge across different modalities. We focus on a prominent learning framework, e.g., multimodal contrastive learning, and demonstrate that, with certain statistical assumptions, multimodal contrastive learning successfully identifies the latent coupled variables up to trivial transformation. This finding enhances our understanding of the mechanisms driving the success of multimodal contrastive learning. Furthermore, this finding reveals a unique potential for disentanglement in multimodal contrastive representation learning, improving the utility of pre-trained models like CLIP that are trained using this approach. Through experiments with synthetic data, we demonstrate the robustness of our findings, even in the presence of violated assumptions. In addition, we validate the disentanglement capabilities of pre-trained CLIP in learning disentangled representations, facilitating few-shot learning and improving domain generalization across a diverse range of real-world datasets.", "title_embedding_index": 14102, "title_abs_embedding_index": 14127}, {"title": "Learning with User-Level Local Differential Privacy", "link_suffix": "/forum?id=15dVqf7VXR", "link": "https://openreview.net/forum?id=15dVqf7VXR", "pdf_link": "https://openreview.net/pdf?id=15dVqf7VXR", "keywords": "Local differential privacy, minimax", "abstract": "User-level privacy is important in distributed systems. Previous research primarily focuses on the central model, while the local models have received much less attention. Under the central model, user-level DP is strictly stronger than the item-level one. However, under the local model, the relationship between user-level and item-level LDP becomes more complex, thus the analysis is crucially different. In this paper, we first analyze the mean estimation problem and then apply it to stochastic optimization, classification, and regression. In particular, we propose adaptive strategies to achieve optimal performance at all privacy levels. Moreover, we also obtain information-theoretic lower bounds, which show that the proposed methods are minimax optimal up to logarithmic factors. Unlike the central DP model, where user-level DP always leads to slower convergence, our result shows that under the local model, the convergence rates are nearly the same between user-level and item-level cases for distributions with bounded support. For heavy-tailed distributions, the user-level rate is even faster than the item-level one.", "title_embedding_index": 14103, "title_abs_embedding_index": 14128}, {"title": "Oversmoothing as Loss of Sign: Towards Structural Balance in Graph Neural Networks", "link_suffix": "/forum?id=NA03iMcGDw", "link": "https://openreview.net/forum?id=NA03iMcGDw", "pdf_link": "https://openreview.net/pdf?id=NA03iMcGDw", "keywords": "graph neural networks, oversmoothing", "abstract": "Oversmoothing is a common phenomenon in a wide range of graph neural networks (GNNs), where node representation becomes homogeneous and thus model performance worsens as the number of layers increases. Various strategies have been proposed to combat oversmoothing, but they are based on different heuristics and lack a unified understanding of their inherent mechanisms. In this paper, we revisit the concept of signed graphs and show that a wide class of anti-oversmoothing techniques can be viewed as the propagation on corresponding signed graphs with both positive and negative edges. Leveraging the classic theory of signed graphs, we characterize the asymptotic behaviors of existing methods and reveal that they deviate from the ideal state of structural balance that provably prevents oversmoothing and improves node classification performance. Driven by this unified analysis and theoretical insights, we propose Structural Balanced Propagation  (SBP) where we explicitly enhance the structural balance of the signed graph with the help of label and feature information. We theoretically and empirically prove that SBP can improve the structural balance to alleviate oversmoothing under certain conditions. Experiments on synthetic and real-world datasets demonstrate the effectiveness of our methods, highlighting the value of our signed graph framework.", "title_embedding_index": 14104, "title_abs_embedding_index": 14129}, {"title": "Discovering Message Passing Hierarchies for Mesh-Based Physics Simulation", "link_suffix": "/forum?id=r8t6OsLP2s", "link": "https://openreview.net/forum?id=r8t6OsLP2s", "pdf_link": "https://openreview.net/pdf?id=r8t6OsLP2s", "keywords": "Physics Simulation, Message Passing Networks", "abstract": "Graph neural networks have emerged as a powerful tool for large-scale mesh-based physics simulation. Existing approaches primarily employ hierarchical, multi-scale message passing to capture long-range dependencies within the graph. However, these graph hierarchies are typically fixed and manually designed, which do not adapt to the evolving dynamics present in complex physical systems. In this paper, we introduce a novel neural network named DHMP, which learnsDynamicHierarchies forMessagePassing networks through a differentiable node selection method. The key component is theanisotropicmessage passing mechanism, which operates at both intra-level and inter-level interactions. Unlike existing methods, it first supports directionally non-uniform aggregation of dynamic features between adjacent nodes within each graph hierarchy. Second, it determines node selection probabilities for the next hierarchy according to different physical contexts, thereby creating more flexible message shortcuts for learning remote node relations. Our experiments demonstrate the effectiveness of DHMP, achieving $22.7$% improvement on average compared to recent fixed-hierarchy message passing networks across five classic physics simulation datasets.", "title_embedding_index": 14105, "title_abs_embedding_index": 14130}, {"title": "Personalized Federated Learning on Flowing Data Heterogeneity under Restricted Storage", "link_suffix": "/forum?id=lg576AqStM", "link": "https://openreview.net/forum?id=lg576AqStM", "pdf_link": "https://openreview.net/pdf?id=lg576AqStM", "keywords": "Federated Learning; Personalized Federated Learning; Data Heterogeneity;", "abstract": "Recent years, researchers focused on personalized federated learning (pFL) to address the inconsistent requirements of clients causing by data heterogeneity in federated learning (FL). However, existing pFL methods typically assume that local data distribution remains unchanged during FL training, the changing data distribution in actual heterogeneous data scenarios can affect model convergence rate and reduce model performance. In this paper, we focus on solving the pFL problem under the situation where data flows through each client like a flowing stream which called Flowing Data Heterogeneity under Restricted Storage, and shift the training goal to the comprehensive performance of the model throughout the FL training process. Therefore, based on the idea of category decoupling, we design a local data distribution reconstruction scheme and a related generator architecture to reduce the error of the controllable replayed data distribution, then propose our pFL framework, pFedGRP, to achieve knowledge transfer and personalized aggregation. Comprehensive experiments on five datasets with multiple settings show the superiority of pFedGRP over eight baseline methods.", "title_embedding_index": 14106, "title_abs_embedding_index": 14131}, {"title": "3D Interaction Geometric Pre-training for Molecular Relational Learning", "link_suffix": "/forum?id=i6jYK0hd0B", "link": "https://openreview.net/forum?id=i6jYK0hd0B", "pdf_link": "https://openreview.net/pdf?id=i6jYK0hd0B", "keywords": "Molecular Relational Learning, AI4Science, Geometric Deep Learning", "abstract": "Molecular Relational Learning (MRL) is a rapidly growing field that focuses on understanding the interaction dynamics between molecules, which is crucial for applications ranging from catalyst engineering to drug discovery. \nDespite recent progress, earlier MRL approaches are limited to using only the 2D topological structure of molecules, as obtaining the 3D interaction geometry remains prohibitively expensive.\nThis paper introduces a novel 3D geometric pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual interaction environment, overcoming the limitations of costly traditional quantum mechanical calculation methods. \nWith the constructed 3D virtual interaction environment, 3DMRL trains 2D MRL model to learn the overall 3D geometric information of molecular interaction through contrastive learning.\nMoreover, fine-grained interaction between molecules is learned through force prediction loss, which is crucial in understanding the wide range of molecular interaction processes.\nExtensive experiments on various tasks using real-world datasets, including out-of-distribution and extrapolation scenarios, demonstrate the effectiveness of 3DMRL, showing up to a 24.93 % improvement in performance across 40 tasks.\nOur code is publicly available athttps://anonymous.4open.science/r/3DMRL-F973.", "title_embedding_index": 14107, "title_abs_embedding_index": 14132}, {"title": "Rethinking Fairness Representation in Multi-Task Learning: a Performance-Informed Variance Reduction Approach", "link_suffix": "/forum?id=z5UZZjXFc9", "link": "https://openreview.net/forum?id=z5UZZjXFc9", "pdf_link": "https://openreview.net/pdf?id=z5UZZjXFc9", "keywords": "Multi-Task Learning, Fair Optimization, Dynamic Weighting Strategy", "abstract": "Multi-task learning (MTL) can leverage shared knowledge across tasks to improve data efficiency and generalization performance, and has been applied in various scenarios. However, task imbalance remains a major challenge for existing MTL methods. While the prior works have attempted to mitigate inter-task unfairness through loss-based and gradient-based strategies, they still exhibit imbalanced performance across tasks on common benchmarks.\nThis key observation motivates us to consider performance-level information as an explicit fairness indicator, which can more accurately reflect the current optimization status of each task, and accordingly help to adjust the gradient aggregation process.\nSpecifically, we utilize the performance variance among tasks as the fairness indicator and introduce a dynamic weighting strategy to gradually reduce the performance variance. \nBased on this, we propose PIVRG, a novel performance-informed variance reduction gradient aggregation approach.\nExtensive experiments show that PIVRG achieves state-of-the-art performance across various benchmarks, spanning both supervised learning and reinforcement learning tasks with task numbers ranging from 2 to 40. Results from the ablation study also show that our approach can be integrated into existing methods, significantly enhancing their performance while reducing the variance in task performance, thus achieving fairer optimization.", "title_embedding_index": 14108, "title_abs_embedding_index": 14133}, {"title": "Elastic and Balanced End-to-end Training of Dynamic LLMs with DynMo", "link_suffix": "/forum?id=hzQcilRe2v", "link": "https://openreview.net/forum?id=hzQcilRe2v", "pdf_link": "https://openreview.net/pdf?id=hzQcilRe2v", "keywords": "Transformers; Dynamic Models; Pipeline Parallelism; LLMs", "abstract": "To reduce the computational and memory costs of Large Language Models (LLMs), schemes that introduce dynamic training are increasingly emerging. Examples of dynamic models are: a) Mixture of Experts (MoEs) at which token routing affects the compute balance, b) gradual pruning of the parameters of a model, c) dynamically freezing layers, d) dynamic sparse attention schemes, e) early exit of tokens as they pass through the model layers, and f) Mixture of Depths (MoDs) schemes where tokens bypass blocks. One side effect that limits the practical value of dynamic models is the introduction of workload imbalance among workers, which in turn negatively affects the efficiency in distributed training. We propose a dynamic load balancing solution DynMo), with a proof that it satisfies maximum reduction in imbalance, to adaptively maintain equal compute workloads among different workers in pipeline parallelism. In addition, DynMo dynamically packs work into fewer workers, while sustaining training throughput, to release the idle workers back to the job manager. DynMo supports both single nodes with multi-GPUs and systems with multi-GPU multi-nodes. In comparison to static distributed training solutions (Megatron-LM and DeepSpeed), DynMo accelerates the end-to-end training of dynamic GPT models by up to 1.23x (MoEs), 3.18x (parameter pruning), 2.23x (layer freezing), 4.02x (sparse attention), 4.52x (early exit), and 1.17x (MoDs). DynMo is available athttps://anonymous.4open.science/r/DynMo-4D04/.", "title_embedding_index": 14109, "title_abs_embedding_index": 14134}, {"title": "Towards Fine-grained Molecular Graph-Text Pre-training", "link_suffix": "/forum?id=dWHecekgsD", "link": "https://openreview.net/forum?id=dWHecekgsD", "pdf_link": "https://openreview.net/pdf?id=dWHecekgsD", "keywords": "Molecular Representation Learning, Graph Neural Network", "abstract": "Understanding molecular structure and related knowledge is crucial for scientific research. Recent studies integrate molecular graphs with their textual descriptions to enhance molecular representation learning. However, they focus on the whole molecular graph and neglect frequently occurring subgraphs, known as motifs, which are essential for determining molecular properties. Without such fine-grained knowledge, these models struggle to generalize to unseen molecules and tasks that require motif-level insights. To bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular graph-Text pre-training framework to jointly learn coarse-grained molecule-level knowledge and fine-grained motif-level knowledge. Specifically, FineMolTex consists of two pre-training tasks: a contrastive alignment task for coarse-grained matching and a masked multi-modal modeling task for fine-grained matching. In particular, the latter predicts the labels of masked motifs and words, leveraging insights from each other, thereby enabling FineMolTex to understand the fine-grained matching between motifs and words. Finally, we conduct extensive experiments across three downstream tasks, achieving up to 230% improvement in the text-based molecule editing task. Additionally, our case studies reveal that FineMolTex successfully captures fine-grained knowledge, potentially offering valuable insights for drug discovery and catalyst design.", "title_embedding_index": 14110, "title_abs_embedding_index": 14135}, {"title": "Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning", "link_suffix": "/forum?id=JZCxlrwjZ8", "link": "https://openreview.net/forum?id=JZCxlrwjZ8", "pdf_link": "https://openreview.net/pdf?id=JZCxlrwjZ8", "keywords": "model-based reinforcement learning, any-step dynamics model", "abstract": "Model-based methods in reinforcement learning offer a promising approach to enhance data efficiency by facilitating policy exploration within a dynamics model. However, accurately predicting sequential steps in the dynamics model remains a challenge due to the bootstrapping prediction, which attributes the next state to the prediction of the current state. This leads to accumulated errors during model roll-out. In this paper, we propose the Any-step Dynamics Model (ADM) to mitigate the compounding error by reducing bootstrapping prediction to direct prediction. ADM allows for the use of variable-length plans as inputs for predicting future states without frequent bootstrapping. We design two algorithms, ADMPO-ON and ADMPO-OFF, which apply ADM in online and offline model-based frameworks, respectively. In the online setting, ADMPO-ON demonstrates improved sample efficiency compared to previous state-of-the-art methods. In the offline setting, ADMPO-OFF not only demonstrates superior performance compared to recent state-of-the-art offline approaches but also offers better quantification of model uncertainty using only a single ADM.", "title_embedding_index": 14111, "title_abs_embedding_index": 14136}, {"title": "BEVWorld: A Multimodal World Model for Autonomous Driving via Unified BEV Latent Space", "link_suffix": "/forum?id=MFrqTfubEB", "link": "https://openreview.net/forum?id=MFrqTfubEB", "pdf_link": "https://openreview.net/pdf?id=MFrqTfubEB", "keywords": "World Model, End-to-end Autonomous Driving, BEV", "abstract": "World models are receiving increasing attention in autonomous driving for their capability to predict potential future scenarios. In this paper, we present BEVWorld, a novel approach that tokenize multimodal sensor inputs into a unified and compact Bird's Eye View (BEV) latent space for environment modeling. The world model consists of two parts: the multi-modal tokenizer and the latent BEV sequence diffusion model. The multi-modal tokenizer first encodes multi-modality information and the decoder is able to reconstruct the latent BEV tokens into LiDAR and image observations by ray-casting rendering in a self-supervised manner. Then the latent BEV sequence diffusion model predicts future scenarios given action tokens as conditions. Experiments demonstrate the effectiveness of BEVWorld in autonomous driving tasks, showcasing its capability in generating future scenes and benefiting downstream tasks such as perception and motion prediction. Code will be available soon.", "title_embedding_index": 14112, "title_abs_embedding_index": 14137}, {"title": "Attaining Human's Desirable Outcomes in Indirect Human-AI Interaction via Multi-Agent Influence Diagrams", "link_suffix": "/forum?id=ikhzVHXvXl", "link": "https://openreview.net/forum?id=ikhzVHXvXl", "pdf_link": "https://openreview.net/pdf?id=ikhzVHXvXl", "keywords": "Human-AI Interaction, Multi-Agent Influence Diagrams, Multi-Agent Reinforcement Learning", "abstract": "In human-AI interaction, one of the cutting-edge research questions is how AI agents can assist a human to attain their desirable outcomes. Most related work investigated the paradigm where a human is required to physically interact with AI agents, which we call direct human-AI interaction. However, this paradigm would be inapplicable when the scenarios are hazardous to humans, such as mine rescue and recovery. To alleviate this shortcoming, we consider indirect human-AI interaction in this paper. More detailed, a human would rely on additional AI agents which we call AI proxies to interact with other AI agents, to attain the human's desirable outcomes. We model this interactive process as multi-agent influence diagrams (MAIDs), an augmentation of Bayesian networks to describe games, with Nash equilibrium (NE) as a solution. Nonetheless, in a MAID there may exist multiple NEs, and only one NE is associated with a human's desirable outcomes. To reach this optimal NE, we propose pre-strategy intervention which is an action to provide AI proxies with more information to make decision towards a human's desirable outcomes. Furthermore, we demonstrate that a team reward Markov game can be rendered as a MAID. This connection not only interprets the successes and failures of prevailing multi-agent reinforcement learning (MARL) paradigms, but also underpins the implementation of pre-strategy intervention in MARL. In practice, we incorporate pre-strategy intervention into MARL for the team reward Markov game to model the scenarios where all agents are required to achieve a common goal, with partial agents working as AI proxies to attain a human's desirable outcomes. During training, these AI proxies receive an additional reward encoding the human's desirable outcomes, and its feasibility is justified in theory. We evaluate the resulting algorithm ProxyAgent in benchmark MARL environments for teamwork, with additional goals as a human's desirable outcomes.", "title_embedding_index": 14113, "title_abs_embedding_index": 14138}, {"title": "Learning Interleaved Image-Text Comprehension in Vision-Language Large Models", "link_suffix": "/forum?id=jZsN9zo8Qi", "link": "https://openreview.net/forum?id=jZsN9zo8Qi", "pdf_link": "https://openreview.net/pdf?id=jZsN9zo8Qi", "keywords": "MLLMs;Benchmark;Interleaved Image-Text Comprehension", "abstract": "The swift progress of Multi-modal Large Models (MLLMs) has showcased their impressive ability to tackle tasks blending vision and language.\nYet, most current models and benchmarks cater to scenarios with a narrow scope of visual and textual contexts.\nThese models often fall short when faced with complex comprehension tasks, which involve navigating through a plethora of irrelevant and potentially misleading information in both text and image forms.\nTo bridge this gap, we introduce a new, more demanding task known as Interleaved Image-Text Comprehension (IITC).\nThis task challenges models to discern and disregard superfluous elements in both images and text to accurately answer questions and to follow intricate instructions to pinpoint the relevant image.\nIn support of this task, we further craft a new VEGA dataset, tailored for the IITC task on scientific content, and devised a subtask, Image-Text Association (ITA), to refine image-text correlation skills.\nOur evaluation of four leading closed-source models, as well as various open-source models using VEGA, underscores the rigorous nature of IITC.\nEven the most advanced models, such as Gemini-1.5-pro and GPT4V, only achieved modest success.\nBy employing a multi-task, multi-scale post-training strategy, we have set a robust baseline for MLLMs on the IITC task, attaining an $85.8%$ accuracy rate in image association and a $0.508$ Rouge score. These results validate the effectiveness of our dataset in improving MLLMs capabilities for nuanced image-text comprehension.", "title_embedding_index": 14114, "title_abs_embedding_index": 14139}, {"title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection", "link_suffix": "/forum?id=yP0iKsinmk", "link": "https://openreview.net/forum?id=yP0iKsinmk", "pdf_link": "https://openreview.net/pdf?id=yP0iKsinmk", "keywords": "video editing, diffusion model, keyframe selection, token slimming", "abstract": "Text-driven video editing is an emerging research hot spot in deep learning. Despite great progress, long video editing is still notoriously challenging mainly due to excessive memory overhead. To tackle this problem, recent efforts have simplified this task into a two-step process of keyframe translation and interpolation generation, enabling the editing of more frames. However, the token-wise keyframe translation still plagues the upper limit of video length. In this paper, we propose a novel and training-free approach towards efficient and effective long video editing, termed AdaFlow. We first reveal that not all tokens of video frames hold equal importance for keyframe-consistency editing, based on which we propose an Adaptive Attention Slimming scheme for AdaFlow to squeeze the $KV$ sequence of extended self-attention. This enhancement allows AdaFlow to increase the number of keyframes for translations by an order of magnitude. In addition, an Adaptive Keyframe Selection scheme is also equipped to select the representative frames for joint editing, further improving generation quality. With these innovative designs, AdaFlow achieves high-quality long video editing of minutes in one inference, i.e., more than 1$k$ frames on one A800 GPU, which is about ten times longer than the compared methods. To validate AdaFlow, we also build a new benchmark for long video editing with high-quality annotations, termed LongV-EVAL. The experimental results show that our AdaFlow can achieve obvious advantages in both the efficiency and quality of long video editing. Our code is anonymously released athttps://anonymous.4open.science/r/AdaFlow-C28F.", "title_embedding_index": 14115, "title_abs_embedding_index": 14140}, {"title": "SE3Set: Harnessing equivariant hypergraph neural networks for molecular representation learning", "link_suffix": "/forum?id=dBafcyEQzr", "link": "https://openreview.net/forum?id=dBafcyEQzr", "pdf_link": "https://openreview.net/pdf?id=dBafcyEQzr", "keywords": "Equivariant, Hypergraph Neural Networks, Molecules", "abstract": "In this paper, we develop SE3Set, an SE(3) equivariant hypergraph neural network architecture tailored for advanced molecular representation learning. Hypergraphs are not merely an extension of traditional graphs; they are pivotal for modeling high-order relationships, a capability that conventional equivariant graph-based methods lack due to their inherent limitations in representing intricate many-body interactions. To achieve this, we first construct hypergraphs via proposing a new fragmentation method that considers both chemical and three-dimensional spatial information of molecular system. We then design SE3Set, which incorporates equivariance into the hypergragh neural network. This ensures that the learned molecular representations are invariant to spatial transformations, thereby providing robustness essential for accurate prediction of molecular properties. SE3Set has shown performance on par with state-of-the-art (SOTA) models for small molecule datasets like QM9 and MD17. It excels on the MD22 dataset, achieving a notable improvement of approximately 20% in accuracy across all molecules, which highlights the prevalence of complex many-body interactions in larger molecules. This exceptional performance of SE3Set across diverse molecular structures underscores its transformative potential in computational chemistry, offering a route to more accurate and physically nuanced modeling.", "title_embedding_index": 14116, "title_abs_embedding_index": 14141}, {"title": "Hypernetwork-Based Equivariant CNNs", "link_suffix": "/forum?id=t2yD3IaIMc", "link": "https://openreview.net/forum?id=t2yD3IaIMc", "pdf_link": "https://openreview.net/pdf?id=t2yD3IaIMc", "keywords": "Equivariant Neural Networks, Geometric Deep Learning", "abstract": "In geometric deep learning, numerous works have been dedicated to enhancing neural networks with ability to preserve symmetries, a concept known as equivariance. Convolutional Neural Networks (CNNs) are already equivariant to translations. To further achieve rotation and reflection equivariance, previous methods are primarily based on Group Equivariant Convolutional Neural Networks ($G$-CNN). While showing a significant improvement when processing rotation-augmented datasets, training $G$-CNN on a dataset with little rotational variation typically leads to a performance drop comparing to a regular CNN. In this study, we discuss the reason of $G$-CNN not performing on datasets with little rotational variation. We propose an alternative approach: generating CNN filters that inherently exhibit rotational equivariance without altering the main network's CNN structure. This is achieved through our novel application of a dynamic hypernetwork. We prove these generated filters grant equivariance property to a regular CNN main network. Our experiments demonstrate that our method outperforms $G$-CNN and achieves performance comparable to advanced state-of-the-art $G$-CNN-based methods.", "title_embedding_index": 14117, "title_abs_embedding_index": 14142}, {"title": "Rethinking the role of frames for SE(3)-invariant crystal structure modeling", "link_suffix": "/forum?id=gzxDjnvBDa", "link": "https://openreview.net/forum?id=gzxDjnvBDa", "pdf_link": "https://openreview.net/pdf?id=gzxDjnvBDa", "keywords": "Materials Science, Invariant Networks, Transformer, Physics-Informed ML", "abstract": "Crystal structure modeling using geometric graph neural networks is important in various machine learning applications in materials science. In these applications, capturing SE(3)-invariant geometric features in crystal structures is a fundamental requirement for these networks. One approach is to model with orientation-standardized structures through structure-aligned coordinate systems called `frames.' However, unlike molecules, determining frames for crystal structures is not trivial due to their infinite and highly symmetric nature. In the search for effective frames for crystals, we point out that existing work assumes a statically fixed frame for each structure based solely on its structural information, regardless of the task under consideration. Here, we rethink the role of frames,questioning whether such simplistic alignment with the structure is sufficient, and propose the concept ofdynamic frames. While accommodating the infinite and symmetric nature of crystals, these frames give each atom its own dynamic view of the structure, focusing only on those atoms actively interacting with it. We demonstrate this concept by utilizing the attention mechanism in a recent transformer-based crystal encoder, developing a new encoder architecture called  CrystalFramer. Extensive comparisons with conventional frames and crystal encoders show the superior performance of the proposed method in various crystal property prediction tasks.", "title_embedding_index": 14118, "title_abs_embedding_index": 14143}, {"title": "COBias and Debias: Minimizing Language Model Pairwise Accuracy Bias via Nonlinear Integer Programming", "link_suffix": "/forum?id=6MlWancakq", "link": "https://openreview.net/forum?id=6MlWancakq", "pdf_link": "https://openreview.net/pdf?id=6MlWancakq", "keywords": "Large language models, class prediction accuracy imbalance, evaluation metric, nonlinear integer programming", "abstract": "For language model classification, would you prefer having only one workable class or having every class working? The latter makes more practical uses. Especially for large language models (LLMs), the fact that they achieve a fair overall accuracy by in-context learning (ICL) obscures a large difference in individual class accuracies. In this work, we uncover and tackle language models' imbalance in per-class prediction accuracy by reconceptualizing it as the Contextual Oddity Bias (COBias), and we are the first to engage nonlinear integer programming (NIP) to debias it. Briefly, COBias refers to the difference in accuracy by a class A compared to its ''odd'' class, which holds the majority wrong predictions of class A. With the COBias metric, we reveal that LLMs of varied scales and families exhibit large per-class accuracy differences. Then we propose Debiasing as Nonlinear Integer Programming (DNIP) to correct ICL per-class probabilities for lower bias and higher overall accuracy. Our optimization objective is directly based on the evaluation scores by COBias and accuracy metrics, solved by simulated annealing. Evaluations on three LLMs across seven NLP classification tasks show that DNIP simultaneously achieves significant COBias reduction (-27%) and accuracy improvement (+12%) over the conventional ICL approach, suggesting that modeling pairwise class accuracy differences is a direction in pushing forward more accurate, more reliable LLM predictions.", "title_embedding_index": 14119, "title_abs_embedding_index": 14144}, {"title": "PerFIT: Personalized Federated Instruction Tuning via Neural Architecture Search", "link_suffix": "/forum?id=R7DKZhgyDX", "link": "https://openreview.net/forum?id=R7DKZhgyDX", "pdf_link": "https://openreview.net/pdf?id=R7DKZhgyDX", "keywords": "federated instruction tuning, personalized federated learning, neural architecture search", "abstract": "Federated Instruction Tuning (FIT) has shown the ability to enable model instruction tuning among massive data owners without exposing privacy.  Yet, it still faces two key challenges, i.e., data and resource heterogeneity. Due to the varying data distribution and preferences among data owners, FIT cannot adapt to the personalized data of individual owners. Moreover, clients with superior computational abilities have to compromise to maintain the same fine-tuning architecture as the weaker clients. Such a constraint prevents the powerful clients from having more trainable parameters for better fine-tuning performances. To address these issues uniformly, we propose a novel Personalized Federated Instruction Tuning (PerFIT) framework based on architecture search. Specifically, PerFIT allows each client to search for a personalized architecture by expanding the trainable parameter space of the global model, pruning them, and obtaining personalized sparse patterns. We further propose personalized parameter-wise aggregation to facilitate flexible aggregation among clients with diverse sparse patterns. This procedure allows personalized instruction fine-tuning within the expanded parameter spaces, concurrently preserving the same number of trainable parameters as the vanilla state, thus introducing no extra resource burden. \nThe evaluations with multiple LLMs on various instruction-following datasets demonstrate that our approach can achieve up to a 23% decrease in personalized perplexity compared to the state-of-the-art FIT methods.", "title_embedding_index": 14120, "title_abs_embedding_index": 14145}, {"title": "A transfer learning framework for weak to strong generalization", "link_suffix": "/forum?id=PeLLMw3wLX", "link": "https://openreview.net/forum?id=PeLLMw3wLX", "pdf_link": "https://openreview.net/pdf?id=PeLLMw3wLX", "keywords": "Weak to strong generalization, alignment, transfer learning", "abstract": "Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether the techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unclear whether it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedbackwithout degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using weaker (less capable) feedback to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach in multiple LLM alignment tasks.", "title_embedding_index": 14121, "title_abs_embedding_index": 14146}, {"title": "UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting", "link_suffix": "/forum?id=6U2KI1dpfl", "link": "https://openreview.net/forum?id=6U2KI1dpfl", "pdf_link": "https://openreview.net/pdf?id=6U2KI1dpfl", "keywords": "multi-modal learning, 3D gaussian splatting", "abstract": "Recent advancements in multi-modal 3D pre-training methods have shown promising efficacy in learning joint representations of text, images, and point clouds. However, adopting point clouds as 3D representation fails to fully capture the intricacies of the 3D world and exhibits a noticeable gap between the discrete points and the dense 2D pixels of images. To tackle this issue, we propose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modal pre-training to enhance the 3D representation. We first rely on the 3DGS representation to model the 3D world as a collection of 3D Gaussians with color and opacity, incorporating all the information of the 3D scene while establishing a strong connection with 2D images. Then, to achieve Language-Image-3D pertaining, UniGS starts with a pretrained vision-language model to establish a shared visual and textual space through extensive real-world image-text pairs. Subsequently, UniGS employs a 3D encoder to align the optimized 3DGS with the Language-Image representations to learn unified multi-modal representations. To facilitate the extraction of global explicit 3D features by the 3D encoder and achieve better cross-modal alignment, we additionally introduce a novel Gaussian-Aware Guidance module that guides the learning of fine-grained representations of the 3D domain. Through extensive experiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets with zero-shot classification, text-driven retrieval and open-world understanding tasks, we demonstrate the effectiveness of UniGS in learning a more general and stronger aligned multi-modal representation. Specifically, UniGS achieves leading results across different 3D tasks with remarkable improvements over previous SOTA, Uni3D, including on zero-shot classification (+9.36%), text-driven retrieval (+4.3%) and open-world understanding (+7.92%).", "title_embedding_index": 14122, "title_abs_embedding_index": 14147}, {"title": "Language Representations Can be What Recommenders Need: Findings and Potentials", "link_suffix": "/forum?id=eIJfOIMN9z", "link": "https://openreview.net/forum?id=eIJfOIMN9z", "pdf_link": "https://openreview.net/pdf?id=eIJfOIMN9z", "keywords": "Collaborative filtering, Language-representation-based recommendation, Language models, Language model representations", "abstract": "Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields.\nHowever, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to prevailing understanding that LMs and traditional recommenders learn two distinct representation spaces due to the huge gap in language and behavior modeling objectives, this work re-examines such understanding and explores extracting a recommendation space directly from the language representation space.\nSurprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance.\nThis outcome suggests the possible homomorphism between the advanced language representation space and an effective item representation space for recommendation, implying that collaborative signals may be implicitly encoded within LMs.\nMotivated by the finding of homomorphism, we explore the possibility of designing advanced collaborative filtering (CF) models purely based on language representations without ID-based embeddings.\nTo be specific, we incorporate several crucial components (i.e., a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function) to build a simple yet effective model, with the language representations of item textual metadata (i.e., title) as the input.\nEmpirical results show that such a simple model can outperform leading ID-based CF models on multiple datasets, which sheds light on using language representations for better recommendation.\nMoreover, we systematically analyze this simple model and find several key features for using advanced language representations:\na good initialization for item representations, superior zero-shot recommendation abilities in new datasets, and being aware of user intention.\nOur findings highlight the connection between language modeling and behavior modeling, which can inspire both natural language processing and recommender system communities.", "title_embedding_index": 14123, "title_abs_embedding_index": 14148}, {"title": "ViMoE: An Empirical Study of Designing Vision Mixture-of-Experts", "link_suffix": "/forum?id=KaYXsoCxV7", "link": "https://openreview.net/forum?id=KaYXsoCxV7", "pdf_link": "https://openreview.net/pdf?id=KaYXsoCxV7", "keywords": "Mixture-of-Experts, Image Classification", "abstract": "Mixture-of-Experts (MoE) models embody the divide-and-conquer concept and are a promising approach for increasing model capacity, demonstrating excellent scalability across multiple domains. In this paper, we integrate the MoE structure into the classic Vision Transformer (ViT), naming it ViMoE, and explore the potential of applying MoE to vision through a comprehensive study on image classification. However, we observe that the performance is sensitive to the configuration of MoE layers, making it challenging to obtain optimal results without careful design. The underlying cause is that inappropriate MoE layers lead to unreliable routing and hinder experts from effectively acquiring helpful knowledge. To address this, we introduce a shared expert to learn and capture common information, serving as an effective way to construct stable ViMoE. Furthermore, we demonstrate how to analyze expert routing behavior, revealing which MoE layers are capable of specializing in handling specific information and which are not. This provides guidance for retaining the critical layers while removing redundancies, thereby advancing ViMoE to be more efficient without sacrificing accuracy. We aspire for this work to offer new insights into the design of vision MoE models and provide valuable empirical guidance for future research.", "title_embedding_index": 14124, "title_abs_embedding_index": 14149}]