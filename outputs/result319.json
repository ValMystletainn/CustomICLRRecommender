[{"title": "Sports-Traj: A Unified Trajectory Generation Model for Multi-Agent Movement in Sports", "link_suffix": "/forum?id=9aTZf71uiD", "link": "https://openreview.net/forum?id=9aTZf71uiD", "pdf_link": "https://openreview.net/pdf?id=9aTZf71uiD", "keywords": "Trajectory Modeling, Trajectory Generation", "abstract": "Understanding multi-agent movement is critical across various fields. The conventional approaches typically focus on separate tasks such as trajectory prediction, imputation, or spatial-temporal recovery. Considering the unique formulation and constraint of each task, most existing methods are tailored for only one, limiting the ability to handle multiple tasks simultaneously, which is a common requirement in real-world scenarios. Another limitation is that widely used public datasets mainly focus on pedestrian movements with casual, loosely connected patterns, where interactions between individuals are not always present, especially at a long distance, making them less representative of more structured environments. To overcome these limitations, we propose a Unified Trajectory Generation model, UniTraj, that processes arbitrary trajectories as masked inputs, adaptable to diverse scenarios in the domain of sports games. Specifically, we introduce a Ghost Spatial Masking (GSM) module, embedded within a Transformer encoder, for spatial feature extraction. We further extend recent State Space Models (SSMs), known as the Mamba model, into a Bidirectional Temporal Mamba (BTM) to better capture temporal dependencies. Additionally, we incorporate a Bidirectional Temporal Scaled (BTS) module to thoroughly scan trajectories while preserving temporal missing relationships. Furthermore, we curate and benchmark three practical sports datasets, \\textbf{\\textit{Basketball-U}}, \\textbf{\\textit{Football-U}}, and \\textbf{\\textit{Soccer-U}}, for evaluation. Extensive experiments demonstrate the superior performance of our model. We hope that our work can advance the understanding of human movement in real-world applications, particularly in sports. Our datasets, code, and model weights are available at~\\href{https://anonymous.4open.science/r/UniTraj-ICLR25/README.md}{link}.", "title_embedding_index": 15900, "title_abs_embedding_index": 15925}, {"title": "SWEb: A Large Web Dataset for the Scandinavian Languages", "link_suffix": "/forum?id=vhPE3PtTgC", "link": "https://openreview.net/forum?id=vhPE3PtTgC", "pdf_link": "https://openreview.net/pdf?id=vhPE3PtTgC", "keywords": "dataset, pre-training, swedish, danish, norwegian, icelandic", "abstract": "This paper presents the hitherto largest pretraining dataset for the Scandinavian languages: the Scandinavian WEb (SWEb), comprising over one trillion tokens. The paper details the collection and processing pipeline, and introduces a novel model-based text extractor that significantly reduces complexity in comparison with rule-based approaches. We also introduce a new cloze-style benchmark for evaluating language models in Swedish, and use this test to compare models trained on the SWEb data to models trained on FineWeb, with competitive results. All data, models and code are shared openly.", "title_embedding_index": 15901, "title_abs_embedding_index": 15926}, {"title": "RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning", "link_suffix": "/forum?id=0FK6tzqV76", "link": "https://openreview.net/forum?id=0FK6tzqV76", "pdf_link": "https://openreview.net/pdf?id=0FK6tzqV76", "keywords": "Reinforcement Learning, Diffusion Model, Reverse Synthesize", "abstract": "In offline reinforcement learning (RL), managing the distribution shift between the learned policy and the static offline dataset is a persistent challenge that can result in overestimated values and suboptimal policies. Traditional offline RL methods address this by introducing conservative biases that limit exploration to well-understood regions, but they often overly restrict the agent's generalization capabilities. Recent work has sought to generate trajectories using generative models to augment the offline dataset, yet these methods still struggle with overestimating synthesized data, especially when out-of-distribution samples are produced. To overcome this issue, we propose RTDiff, a novel diffusion-based data augmentation technique that synthesizes trajectoriesin reverse, moving from unknown to known states. Such reverse generation naturally mitigates the risk of overestimation by ensuring that the agent avoids planning through unknown states. Additionally, reverse trajectory synthesis allows us to generate longer, more informative trajectories that take full advantage of diffusion models' generative strengths while ensuring reliability. We further enhance RTDiff by introducing flexible trajectory length control and improving the efficiency of the generation process through noise management. Our empirical results show that RTDiff significantly improves the performance of several state-of-the-art offline RL algorithms across diverse environments, achieving consistent and superior results by effectively overcoming distribution shift.", "title_embedding_index": 15902, "title_abs_embedding_index": 15927}, {"title": "Generalization, Expressivity, and Universality of Graph Neural Networks on Attributed Graphs", "link_suffix": "/forum?id=qKgd7RaAem", "link": "https://openreview.net/forum?id=qKgd7RaAem", "pdf_link": "https://openreview.net/pdf?id=qKgd7RaAem", "keywords": "Graph Neural Networks, Graphon Neural Networks, Machine Learning Theory, 1-WL test for Graphons, Generalization, Expressivity, Universal Approximation, Wasserstein Distance, Optimal Transport, Computation Trees", "abstract": "We analyze the universality and generalization of graph neural networks (GNNs) on attributed graphs, i.e., with node attributes. To this end, we propose pseudometrics over the space of all attributed graphs that describe the fine-grained expressivity of GNNs. Namely, GNNs are both Lipschitz continuous with respect to our pseudometrics and can separate attributed graphs that are distant in the metric. Moreover, we prove that the space of all attributed graphs is relatively compact with respect to our metrics. Based on these properties, we prove a universal approximation theorem for GNNs and generalization bounds for GNNs on any data distribution of attributed graphs. The proposed metrics compute the similarity between the structures of attributed  graphs via a hierarchical optimal transport between computation trees. Our work extends and unites previous approaches which either derived theory only for graphs with no attributes, derived compact metrics under which GNNs are continuous but without separation power, or derived metrics under which GNNs are continuous and separate points but the space of graphs is not relatively compact, which prevents universal approximation and generalization analysis.", "title_embedding_index": 15903, "title_abs_embedding_index": 15928}, {"title": "Gap-Aware Preference Optimization: Enhancing Model Alignment with Perception Margin", "link_suffix": "/forum?id=N2sN3LESoW", "link": "https://openreview.net/forum?id=N2sN3LESoW", "pdf_link": "https://openreview.net/pdf?id=N2sN3LESoW", "keywords": "LLM Alignments; Gap-Aware Preference Optimization", "abstract": "Reinforcement learning from human feedback (RLHF) approaches are widely used for fine-tuning large language models (LLMs) to align with instructional preferences. However, traditional RLHF methods often rely on binary labels, which fail to capture the pairwise differences in human perception, leading to potential performance degradation.\nTo address this limitation, we introduce $\\textbf{Gap-Aware Preference Optimization}$ (GaPO), a novel approach that integrates the degree of semantic gaps into preference optimization. By modifying the margin term in the loss function and replacing it with an estimated gap computed using general metrics, GaPO provides a new supervisory signal that explicitly highlights the nuances between preference pairs. This new signal helps the model allocate gradients more rationally during optimization, facilitating more effective learning from the preference data.\nExperiments conducted with a strong base model, Llama-3-8B-Instruct, demonstrate that GaPO surpasses state-of-the-art methods on widely used benchmarks. Our best-performing model, GaPO-ROUGE_L, achieves a win rate of 52.8% on AlpacaEval 2.0, exceeding the baseline methods by 5.3 points.", "title_embedding_index": 15904, "title_abs_embedding_index": 15929}, {"title": "A Unifying Framework for Representation Learning", "link_suffix": "/forum?id=WfaQrKCr4X", "link": "https://openreview.net/forum?id=WfaQrKCr4X", "pdf_link": "https://openreview.net/pdf?id=WfaQrKCr4X", "keywords": "representation learning; unsupervised learning; clustering; dimensionality reduction; kmeans; contrastive learning; information theory", "abstract": "As the field of unsupervised learning grows, there has been a proliferation of different loss functions to solve different classes of problems. We find that a large collection of modern loss functions can be generalized by a single equation rooted in information theory. In particular, we introduce I-Con, a framework that shows that several broad classes of machine learning methods are precisely minimizing an integrated KL divergence between two marginal distributions: the supervisory and learned representations. This viewpoint exposes a hidden information geometry underlying clustering, spectral methods, dimensionality reduction, contrastive learning, and supervised learning. I-Con enables the development of new loss functions by combining successful techniques from across the literature. We not only present a wide array of proofs, connecting over 11 different approaches, but we also leverage these theoretical results to create state of the art unsupervised image classifiers that achieve a +8% improvement over the prior state-of-the-art on unsupervised classification on ImageNet-1K.", "title_embedding_index": 15905, "title_abs_embedding_index": 15930}, {"title": "Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG", "link_suffix": "/forum?id=oU3tpaR8fm", "link": "https://openreview.net/forum?id=oU3tpaR8fm", "pdf_link": "https://openreview.net/pdf?id=oU3tpaR8fm", "keywords": "retrieval-augmented generation, large language models, long-context", "abstract": "Retrieval-augmented generation (RAG) empowers large language models (LLMs) to utilize external knowledge sources. The increasing capacity of LLMs to process longer input sequences opens up avenues for providing more retrieved information, to potentially enhance the quality of generated outputs. It is plausible to assume that a larger retrieval set would contain more relevant information (higher recall), that might result in improved performance. However, our empirical findings demonstrate that for many long-context LLMs, the quality of generated output initially improves first, but then subsequently declines as the number of retrieved passages increases. This paper investigates this phenomenon, identifying the detrimental impact of retrieved \"hard negatives\" as a key contributor. To mitigate this and enhance the robustness of long-context LLM-based RAG, we propose both training-free and training-based approaches. We first showcase the effectiveness of retrieval reordering as a simple yet powerful training-free optimization. Furthermore, we explore training-based methods, specifically RAG-specific implicit LLM fine-tuning and RAG-oriented fine-tuning with intermediate reasoning, demonstrating their capacity for substantial performance gains. Finally, we conduct a systematic analysis of design choices for these training-based methods, including data distribution, retriever selection, and training context length.", "title_embedding_index": 15906, "title_abs_embedding_index": 15931}, {"title": "Information-theoretic Generalization Analysis for Vector-Quantized VAEs", "link_suffix": "/forum?id=UN94vDiaJv", "link": "https://openreview.net/forum?id=UN94vDiaJv", "pdf_link": "https://openreview.net/pdf?id=UN94vDiaJv", "keywords": "Information-theoretic generalization error analysis, generalization error analysis, VQ-VAE", "abstract": "Encoder--decoder models, which transform input data into latent variables, have achieved a significant success in machine learning. Although the generalization capability of these models has been theoretically analyzed in supervised learning focusing on the complexity of latent variables, the contribution of latent variables in generalization and data generation capabilities are less explored theoretically in unsupervised learning. To address this gap, our study leverages information-theoretic generalization error analysis (IT analysis). Using the supersample setting in recent IT analysis, we demonstrate that the generalization gap for reconstruction loss can be evaluated through mutual information related to the posterior distribution of latent variables, conditioned on the input data, without relying on the decoder's information. We also introduce a novel permutation-symmetric supersample setting, which extends the existing IT analysis and shows that regularizing the encoder's capacity leads to generalization. Finally, we guarantee the 2-Wasserstein distance between the true data distribution and the generated data distribution, offering insights into the model\u2019s data generation capabilities.", "title_embedding_index": 15907, "title_abs_embedding_index": 15932}, {"title": "RankNovo: A Universal Reranking Approach for Robust De Novo Peptide Sequencing", "link_suffix": "/forum?id=87B3zDRMjv", "link": "https://openreview.net/forum?id=87B3zDRMjv", "pdf_link": "https://openreview.net/pdf?id=87B3zDRMjv", "keywords": "Peptide Sequencing, De novo, Reranking", "abstract": "De novo peptide sequencing is a critical task in proteomics research. However, the performance of current deep learning-based methods is limited by the inherent complexity of mass spectrometry data and the heterogeneous distribution of noise signals, leading to data-specific biases. We present RankNovo, the first deep reranking framework that enhances de novo peptide sequencing by leveraging the complementary strengths of multiple sequencing models. RankNovo employs a list-wise reranking approach, modeling candidate peptides as multiple sequence alignments and utilizing axial attention to extract informative features across candidates. Additionally, we introduce two new metrics, PMD (Peptide Mass Deviation) and RMD (Residual Mass Deviation), which offer delicate supervision by quantifying mass differences between peptides at both the sequence and residue levels. Extensive experiments demonstrate that RankNovo not only surpasses its individual base models, which are used to generate training candidates for reranking pre-training, but also sets a new state-of-the-art de novo sequencing benchmarks. Moreover, RankNovo exhibits strong zero-shot generalization to unseen models\u2014those whose generations were not exposed during training, highlighting its robustness and potential as a universal reranking framework for peptide sequencing. Our work presents a novel reranking strategy that fundamentally challenges existing single-model paradigms and advances the frontier of accurate de novo peptide sequencing. Our source code is provided at an anonymous link.", "title_embedding_index": 15908, "title_abs_embedding_index": 15933}, {"title": "Refining Counterfactual Explanations With Joint-Distribution-Informed Shapley Towards Actionable Minimality", "link_suffix": "/forum?id=hQ2TUZmse1", "link": "https://openreview.net/forum?id=hQ2TUZmse1", "pdf_link": "https://openreview.net/pdf?id=hQ2TUZmse1", "keywords": "Explainable artificial Intelligence, Feature attributions, Counterfactual explanations", "abstract": "Counterfactual explanations (CE) identify data points that closely resemble the observed data but produce different machine learning (ML) model outputs, offering critical insights into model decisions. Despite the diverse scenarios, goals and tasks to which they are tailored, existing CE methods often lack actionable efficiency because of unnecessary feature changes included within the explanations that are presented to users and stakeholders. We address this problem by proposing a method that minimizes the required feature changes while maintaining the validity of CE, without imposing restrictions on models or CE algorithms, whether instance- or group-based. The key innovation lies in computing a joint distribution between observed and counterfactual data and leveraging it to inform Shapley values for feature attributions (FA). We demonstrate that optimal transport (OT) effectively derives this distribution, especially when the alignment between observed and counterfactual data is unclear in used CE methods. Additionally, a counterintuitive finding is uncovered: it may be misleading to rely on an exact alignment defined by the CE generation mechanism in conducting FA. Our proposed method is validated on extensive experiments across multiple datasets, showcasing its effectiveness in refining CE towards greater actionable efficiency.", "title_embedding_index": 15909, "title_abs_embedding_index": 15934}, {"title": "Unlocking Full Dynamic Optimization of District Energy Systems through State-Space Model Learning", "link_suffix": "/forum?id=mJwpHp8guj", "link": "https://openreview.net/forum?id=mJwpHp8guj", "pdf_link": "https://openreview.net/pdf?id=mJwpHp8guj", "keywords": "surrogate model, state-space, physics-informed models, optimal control, sustainable energy systems", "abstract": "Predictive control enables the operation of physical systems along an optimal trajectory based on forecasts and dynamic simulations. However, the complexity of system dynamics and high computational cost of optimization typically restrict the optimization window to short horizons. Thus, any potential benefits from mid- and long-term rewards are withdrawn. This is particularly relevant for optimization of district energy systems using various low-environmental-impact sources. To address this, we present an end-to-end methodological framework for learning state-space representations of such systems that significantly reduce computational load. The proposed approach leverages the implicit graph structure of such systems to develop and train a physics-informed spatio-temporal graph neural network. This methodology is evaluated on a real-world district heating system incorporating thermal solar panels, storage, biomass and natural gas boilers. Through historical time-series data augmentation and hyperparameter optimization, the learned model demonstrates strong generalization ability and high accuracy in predicting system dynamics. Our method reduces simulation time by four orders of magnitude, cutting optimization time from several days to mere minutes, while also lowering operational costs by up to 25%.", "title_embedding_index": 15910, "title_abs_embedding_index": 15935}, {"title": "Local convergence of simultaneous min-max algorithms  to differential equilibrium on Riemannian manifold", "link_suffix": "/forum?id=ROYSNn3vvB", "link": "https://openreview.net/forum?id=ROYSNn3vvB", "pdf_link": "https://openreview.net/pdf?id=ROYSNn3vvB", "keywords": "min-max algorithm, differential game, Riemannian manifold, Wasserstein GAN", "abstract": "We study min-max algorithms to solve zero-sum differential games on\nRiemannian manifold.\nBased on the notions of\ndifferential Stackelberg equilibrium\nand differential Nash equilibrium on Riemannian manifold,\nwe analyze the local convergence of \ntwo representative deterministic simultaneous algorithms $\\tau$-GDA and $\\tau$-SGA\nto such equilibrium.\nSufficient conditions are obtained to establish their linear convergence rates \nby Ostrowski theorem on manifold and spectral analysis. \nThe $\\tau$-SGA algorithm is extended from\nthe symplectic gradient-adjustment method in Euclidean space\nto avoid strong rotational dynamics in $\\tau$-GDA.\nIn some cases, we obtain a faster convergence rate of $\\tau$-SGA \nthrough an asymptotic analysis which is valid when \nthe learning rate ratio $\\tau$ is big.\nWe show numerically how the insights obtained from the\nconvergence analysis may improve\nthe training of orthogonal Wasserstein GANs using \nstochastic $\\tau$-GDA and $\\tau$-SGA on simple benchmarks.", "title_embedding_index": 15911, "title_abs_embedding_index": 15936}, {"title": "Explanations of GNN on Evolving Graphs via Axiomatic  Layer edges", "link_suffix": "/forum?id=pXN8T5RwNN", "link": "https://openreview.net/forum?id=pXN8T5RwNN", "pdf_link": "https://openreview.net/pdf?id=pXN8T5RwNN", "keywords": "explainability; dynamic graphs; message flows; layeredges", "abstract": "Graphs are ubiquitous in social networks, chemical molecules, and financial data, where Graph Neural Networks (GNNs) achieve superior predictive accuracy. Graphs can be\nevolving, while understanding how GNN predictions respond to the evolution provides significant insight and trust. \nWe explore the problem of explaining evolving GNN predictions due to continuously changing edge weights.\nWe first propose a layer edge-based explanation to balance\nexplanation fidelity and interpretability, as opposed to message flow and input edge.\nThen we propose a novel framework to address the challenges of axiomatic attribution and the entanglement of multiple computational graph paths due to continuous change of edge weights. We first design an axiomatic attribution of the evolution of the model prediction to message flows, then develop Shapley value to fairly map message flow contributions to layer edges.\nWe formulate a novel optimization problem to find the critical layer edges based on KL-divergence minimization. Extensive experiments on eight datasets for node classification, link prediction, and graph classification tasks with evolving graphs demonstrate the better fidelity and interpretability of the proposed method over the baseline methods.", "title_embedding_index": 15912, "title_abs_embedding_index": 15937}, {"title": "Understanding and Enhancing Context-Augmented Language Models Through Mechanistic Circuits", "link_suffix": "/forum?id=sqsGBW8zQx", "link": "https://openreview.net/forum?id=sqsGBW8zQx", "pdf_link": "https://openreview.net/pdf?id=sqsGBW8zQx", "keywords": "circuits, mechanistic interpretability, language models, extractive QA", "abstract": "Recent studies have extracted circuits from the computational graphs of language models for simple language tasks such as entity tracking or indirect object identification. In our paper, we scale up circuit extraction to a real-world language modeling task: context-augmented language modeling for question-answering (QA) tasks and understand the potential benefits of circuits towards downstream applications such as data attribution. We extract circuits as a function of internal model components (e.g., attention heads, attention layers, MLPs) using causal mediation analysis techniques. Leveraging the extracted circuits, we first understand the interplay between the language model's usage of parametric memory and retrieved context towards a better mechanistic understanding of context-augmented language models. We then identify a small set of attention heads in our circuit which performs reliable data attribution by default, thereby obtaining attribution for free in just the model's forward pass! Using this insight, we then introduce AttnAttrib, a fast data attribution algorithm. Through a range of empirical experiments across different extractive QA benchmarks, we show that performing data attribution with AttnAttrib obtains state-of-the-art attribution results across different language models. Finally, we show the possibility to steer the language model towards answering from the context, instead of the parametric memory by (i) using the attribution from our extracted attention head as an additional signal during the forward pass and (ii) scaling the output of a small set of attention heads. Beyond mechanistic understanding, our paper provides tangible applications of mechanistic circuits in the form of reliable data attribution and model steering.", "title_embedding_index": 15913, "title_abs_embedding_index": 15938}, {"title": "Learning positional encodings in transformers depends on initialization", "link_suffix": "/forum?id=fn0mjkZopf", "link": "https://openreview.net/forum?id=fn0mjkZopf", "pdf_link": "https://openreview.net/pdf?id=fn0mjkZopf", "keywords": "transformers, representation learning, deep learning, reasoning, transformer", "abstract": "The attention mechanism is central to the transformer's ability to capture complex dependencies between tokens of an input sequence.\nKey to the successful application of the attention mechanism in transformers is its choice of positional encoding (PE).\nThe PE provides essential information that distinguishes the position and order amongst tokens in a sequence.\nMost prior investigations of PE effects on generalization were tailored to 1D input sequences, such as those presented in natural language, where adjacent tokens (e.g., words) are highly related.\nIn contrast, many real world tasks involve datasets with highly non-trivial positional arrangements, such as datasets organized in multiple spatial dimensions, or datasets for which ground truth positions are not known, such as in biological data.\nHere we study the importance of learning accurate PE for problems which rely on a non-trivial arrangement of input tokens. \nCritically, we find that the choice of initialization of a learnable PE greatly influences its ability to discover accurate PEs that lead to enhanced generalization.\nWe empirically demonstrate our findings in a 2D relational reasoning task and a real world 3D neuroscience dataset, applying interpretability analyses to verify the learning of accurate PEs.\nOverall, we find that a learned PE initialized from a small-norm distribution can 1) uncover interpretable PEs that mirror ground truth positions, 2) learn non-trivial and modular PEs in a real-world neuroscience dataset, and 3) lead to improved downstream generalization in both datasets.\nImportantly, choosing an ill-suited PE can be detrimental to both model interpretability and generalization.\nTogether, our results illustrate the feasibility of discovering accurate PEs for enhanced generalization.", "title_embedding_index": 15914, "title_abs_embedding_index": 15939}, {"title": "Agent Workflow Memory", "link_suffix": "/forum?id=PfYg3eRrNi", "link": "https://openreview.net/forum?id=PfYg3eRrNi", "pdf_link": "https://openreview.net/pdf?id=PfYg3eRrNi", "keywords": "agent, web navigation, memory augmentation, online", "abstract": "Despite the potential of language model-based agents to solve real-world tasks such as web navigation, current methods still struggle with long-horizon tasks with complex action trajectories. In contrast, humans can flexibly solve complex tasks by learning reusable task workflows from past experiences and using them to guide future actions. To build agents that can similarly benefit from this process, we introduce Agent Workflow Memory (AWM), a method for inducing commonly reused routines, i.e., workflows, and selectively providing workflows to the agent to guide subsequent generations. AWM flexibly applies to both offline and online scenarios, where agents induce workflows from training examples beforehand or from test queries on the fly. We experiment on two major web navigation benchmarks -- Mind2Web and WebArena -- that collectively cover 1000+ tasks from 200+ domains across travel, shopping, and social media, among others. AWM substantially improves the baseline results by 24.6% and 51.1% relative success rate on Mind2Web and WebArena while reducing the number of steps taken to solve WebArena tasks successfully. Furthermore, online AWM robustly generalizes in cross-task, website, and domain evaluations, surpassing baselines from 8.9 to 14.0 absolute points as train-test task distribution gaps widen.", "title_embedding_index": 15915, "title_abs_embedding_index": 15940}, {"title": "Unsupervised Point Cloud Completion through Unbalanced Optimal Transport", "link_suffix": "/forum?id=Nh1w3ZnDaH", "link": "https://openreview.net/forum?id=Nh1w3ZnDaH", "pdf_link": "https://openreview.net/pdf?id=Nh1w3ZnDaH", "keywords": "Point Cloud Completion, Unpaired Point Cloud Completion, Unbalanced optimal transport, Optimal transport", "abstract": "Unpaired point cloud completion explores methods for learning a completion map from unpaired incomplete and complete point cloud data. In this paper, we propose a novel approach for unpaired point cloud completion using the unbalanced optimal transport map, called Unbalanced Optimal Transport Map for Unpaired Point Cloud Completion (UOT-UPC). We demonstrate that the unpaired point cloud completion can be naturally interpreted as the Optimal Transport (OT) problem and introduce the Unbalanced Optimal Transport (UOT) approach to address the class imbalance problem, which is prevalent in unpaired point cloud completion datasets. \nMoreover, we analyze the appropriate cost function for unpaired completion tasks. This analysis shows that the InfoCD cost function is particularly well-suited for this task.\nOur model is the first attempt to leverage UOT for unpaired point cloud completion, achieving competitive or superior results on both single-category and multi-category datasets. In particular, our model is especially effective in scenarios with class imbalance, where the proportions of categories are different between the incomplete and complete point cloud datasets.", "title_embedding_index": 15916, "title_abs_embedding_index": 15941}, {"title": "Sparse Training: Do All Tokens Matter for Long Sequence Generalization?", "link_suffix": "/forum?id=KJLqgaixgn", "link": "https://openreview.net/forum?id=KJLqgaixgn", "pdf_link": "https://openreview.net/pdf?id=KJLqgaixgn", "keywords": "Large Language Models, Long Sequence, Length Extrapolation, Efficiency", "abstract": "Large language models (LLMs) have demonstrated remarkable progress in generating high-quality natural language through performing extensive pre-training over Transformer architectures. However, the quadratic complexity of transformers in sequence computation greatly limits its capability in efficiently modeling long sequences. In this paper, we introduce \\method, a simple training technique to optimize the complexity of Transformer models in long-sequence training. Specifically, in \\method, the input sequences of the Transformer network are segmented into two distinct components: {the \\textit{memory} part and the \\textit{target} part.} The target part adheres to the standard next-token prediction for modeling continuous sequences, while the memory part, sampled from longer sequences, serves as the conditional context for the prediction of the target part. To build the memory part, we apply a sparse sampling policy that decays with the distance from the target part, to obtain tokens and preserve their positions. Without any architectural modifications, our method can extend existing Transformer-based LLMs to capture long-range dependencies within a fixed window size during the training. Experimental results on multiple datasets also demonstrate the effectiveness and efficiency of \\textsc{Sparse Training} to mitigate the complexity of the Transformer network in building long-sequence dependency.", "title_embedding_index": 15917, "title_abs_embedding_index": 15942}, {"title": "Interpreting Adversarial Attacks and Defenses using Architectures with Enhanced Interpretability", "link_suffix": "/forum?id=puGvShnqeA", "link": "https://openreview.net/forum?id=puGvShnqeA", "pdf_link": "https://openreview.net/pdf?id=puGvShnqeA", "keywords": "adversarial attacks, adversarial defenses, computer vision, deep learning, Interpretability", "abstract": "Adversarial attacks in deep learning represent a significant threat to the integrity and\nreliability of machine learning models. These attacks involve intentionally crafting\nperturbations to input data that, while often imperceptible to humans, can lead\nto incorrect predictions by the model. This phenomenon exposes vulnerabilities\nin deep learning systems across various applications, from image recognition to\nnatural language processing. Adversarial training has been a popular defence\ntechnique against these adversarial attacks. The research community has been\nincreasingly interested in interpreting robust models and understanding how they\ndefend against attacks.\nIn this work, we capitalize on a network architecture, namely Deep Linearly Gated\nNetworks (DLGN), which has better interpretation capabilities than regular network\narchitectures. Using this architecture, we interpret robust models trained using PGD\nadversarial training  and compare them with standard training. Feature networks\nin these architectures act as feature extractors, making them the only medium\nthrough which an adversary can attack the model. So, we use the feature network\nin this architecture with fully connected layers to analyse properties like alignment\nof the hyperplanes, hyperplane relation with PCA, and sub-network overlap among\nclasses and compare these properties between robust and standard models. We\nalso consider this architecture having CNN layers wherein we qualitatively and\nquantitatively contrast gating patterns between robust and standard models. We\nuse ideas from visualization to understand the representations used by robust and\nstandard models.", "title_embedding_index": 15918, "title_abs_embedding_index": 15943}, {"title": "Does Example Selection for In-Context Learning Amplify the Biases of Large Language Models?", "link_suffix": "/forum?id=8BC5UfxOoG", "link": "https://openreview.net/forum?id=8BC5UfxOoG", "pdf_link": "https://openreview.net/pdf?id=8BC5UfxOoG", "keywords": "Social Bias, Large Language Model, In-Context Learning", "abstract": "In-context learning (ICL) has proven to be adept at adapting large language models (LLMs) to downstream tasks without parameter updates, based on a few demonstration examples.\nPrior work has found that the ICL performance is susceptible to the selection of examples in prompt and made efforts to stabilize it.\nHowever, existing example selection studies ignore the ethical risks behind the examples selected, such as gender and race bias.\nIn this work, we first construct a new sentiment classification dataset, EEC-paraphrase, designed to better capture and evaluate the biases of LLMs.\nThen, through further analysis, we discover that1) example selection with high accuracy does not mean low bias; 2) example selection for ICL amplifies the biases of LLMs; 3) example selection contributes to spurious correlations of LLMs.Based on the above observations, we propose theRemind withBias-awareEmbedding(ReBE), which removes the spurious correlations through contrastive learning and obtains bias-aware embedding for LLMs based on prompt tuning.\nFinally, we demonstrate that ReBE effectively mitigates biases of LLMs without significantly compromising accuracy and is highly compatible with existing example selection methods.The implementation code is available athttps://anonymous.4open.science/r/ReBE-1D04.", "title_embedding_index": 15919, "title_abs_embedding_index": 15944}, {"title": "Structural Quantile Normalization: a general, differentiable feature scaling technique balancing gaussian approximation and structural preservation", "link_suffix": "/forum?id=x3l0fQubOn", "link": "https://openreview.net/forum?id=x3l0fQubOn", "pdf_link": "https://openreview.net/pdf?id=x3l0fQubOn", "keywords": "feature scaling, preprocessing, normal distribution, differentiable transformation, quantile normalization, neural networks", "abstract": "Feature scaling is an essential practice in modern machine learning, both as a preprocessing step and as an integral part of model architectures, such as batch and layer normalization in artificial neural networks. Its primary goal is to align feature scales, preventing larger-valued features from dominating model learning\u2014especially in algorithms utilizing distance metrics, gradient-based optimization, and regularization. Additionally, many algorithms benefit from or require input data approximating a standard Gaussian distribution, establishing \"Gaussianization\" as an additional objective. Lastly, an ideal scaling method should be general, as in applicable to any input distribution, and differentiable to facilitate seamless integration into gradient-optimized models. Although differentiable and general, traditional linear methods, such as standardization and min-max scaling, cannot reshape distributions relative to scale and offset. On the other hand, existing nonlinear methods, although more effective at Gaussianizing data, either lack general applicability (e.g., power transformations) or introduce excessive distortions that can obscure intrinsic data patterns (e.g., quantile normalization). Present non-linear methods are also not differentiable. We introduce Structural Quantile Normalization (SQN), a general and differentiable scaling method, that enables balancing Gaussian approximation with structural preservation. We also introduce Fast-SQN; a more performance-efficient variant with the same properties. We show that SQN is a generalized augmentation of standardization and quantile normalization. Using the real-world \"California Housing\" dataset, we demonstrate that Fast-SQN outperforms state-of-the-art methods\u2014including classical and ordered quantile normalization, and Box-Cox, and Yeo-Johnson transformations\u2014across key metrics (i.e., RMSE, MAE, MdAE) when used for preprocessing.\nFinally, we show our approach transformation differentiability and compatibility with gradient-based optimization using the real-world \"Gas Turbine Emission\" dataset and propose a methodology for integration into deep networks.", "title_embedding_index": 15920, "title_abs_embedding_index": 15945}, {"title": "Deep Random Features for Scalable Interpolation of Spatiotemporal Data", "link_suffix": "/forum?id=OD1MV7vf41", "link": "https://openreview.net/forum?id=OD1MV7vf41", "pdf_link": "https://openreview.net/pdf?id=OD1MV7vf41", "keywords": "Random Features, Deep Gaussian Processes, Bayesian Deep Learning, Remote Sensing", "abstract": "The rapid growth of earth observation systems calls for a scalable approach to interpolate remote-sensing observations. These methods in principle, should acquire more information about the observed field as data grows. Gaussian processes (GPs) are candidate model choices for interpolation. However, due to their poor scalability, they usually rely on inducing points for inference, which restricts their expressivity. Moreover, commonly imposed assumptions such as stationarity prevents them from capturing complex patterns in the data. While deep GPs can overcome this issue, training and making inference with them are difficult, again requiring crude approximations via inducing points. In this work, we instead approach the problem through Bayesian deep learning, where spatiotemporal fields are represented by deep neural networks, whose layers share the inductive bias of stationary GPs on the plane/sphere via random feature expansions. This allows one to (1) capture high frequency patterns in the data, and (2) use mini-batched gradient descent for large scale training. We experiment on various remote sensing data at local/global scales, showing that our approach produce competitive or superior results to existing methods, with well-calibrated uncertainties.", "title_embedding_index": 15921, "title_abs_embedding_index": 15946}, {"title": "MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model", "link_suffix": "/forum?id=Yqk7EyT52H", "link": "https://openreview.net/forum?id=Yqk7EyT52H", "pdf_link": "https://openreview.net/pdf?id=Yqk7EyT52H", "keywords": "Financial Market Simulation, Generative Foundation Model, Large Market Model (LMM), Controllable Simulation, Interactive Simulation, Market Impact, Reinforcement Learning, Forecasting, Market Manipulation Detection, Order-Level Data", "abstract": "Generative models aim to simulate realistic effects of various actions across different contexts, from text generation to visual effects. Despite significant efforts to build real-world simulators, the application of generative models to virtual worlds, like financial markets, remains under-explored. In financial markets, generative models can simulate complex market effects of participants with various behaviors, enabling interaction under different market conditions, and training strategies without financial risk. This simulation relies on the finest structured data in financial market like orders thus building the finest realistic simulation. We propose Large Market Model (LMM), an order-level generative foundation model, for financial market simulation, akin to language modeling in the digital world. Our financial Market Simulation engine (MarS), powered by LMM, addresses the domain-specific need for realistic, interactive and controllable order generation. Key observations include LMM's strong scalability across data size and model complexity, and MarS's robust and practicable realism in controlled generation with market impact. We showcase MarS as a forecast tool, detection system, analysis platform, and agent training environment, thus demonstrating MarS's ``paradigm shift'' potential for a variety of financial applications.", "title_embedding_index": 15922, "title_abs_embedding_index": 15947}, {"title": "Multi-modality Expansion and Retention for LLMs through Parameter Merging and Decoupling", "link_suffix": "/forum?id=WjPK2gj0xu", "link": "https://openreview.net/forum?id=WjPK2gj0xu", "pdf_link": "https://openreview.net/pdf?id=WjPK2gj0xu", "keywords": "Multimodal Large Language Models, Model Merging, Parameter Decoupling, Knowledge Localization", "abstract": "Extensive fine-tuning of the synthesis between multimodal encoders and Large Language Models (LLMs) on modality-specific data can expand the modalities that LLM can handle, leading to the formation of Multimodal Large Language Models (MLLMs).\nHowever, this paradigm to expanding modalities heavily relies on initiating fine-tuning from scratch with new multimodal data, which is both resource-intensive and inflexible. \nIn this paper, we propose $\\textit{MMER (Multi-modality Expansion and Retention)}$, a novel $\\textit{training-free}$ approach that reuses and composes existing MLLMs to facilitate effective multimodal expansion while retaining the original performance of each MLLM. \nIn particular, MMER maintains the multimodal encoders of the MLLMs while merging their LLM parameters.\nBy comparing the original LLM parameters with the merged ones, MMER can create binary masks that enable an approximate separation of the LLM parameters for each modality.\nThis process allows the decoupled parameters to independently process modality-specific inputs, thereby reducing parameter conflicts and maintaining the fidelity of the original MLLMs.\nAdditionally, MMER integrates strategies to prevent catastrophic forgetting by employing a similar approach to separately decouple the parameters fine-tuned on new tasks from the original parameters. \nExperiments on three multimodal tasks and fourteen dual-modal tasks show significant improvements over recent baselines, demonstrating that MMER can effectively expand multimodal capabilities of LLMs while retaining 99.6% of the original performance. \nFurther experiments in both single-task and cross modalities multi-task scenarios reveal that MMER significantly mitigates catastrophic forgetting.", "title_embedding_index": 15923, "title_abs_embedding_index": 15948}, {"title": "Temporal Test-Time Adaptation with State-Space Models", "link_suffix": "/forum?id=y4F2YZxN9T", "link": "https://openreview.net/forum?id=y4F2YZxN9T", "pdf_link": "https://openreview.net/pdf?id=y4F2YZxN9T", "keywords": "test-time adaptation, state-space models, probabilistic modelling, dynamical systems", "abstract": "Distribution shifts between training and test data are inevitable over the lifecycle of a deployed model, leading to performance decay. Adapting a model on test samples can help mitigate this drop in performance. However, most test-time adaptation methods have focused on synthetic corruption shifts, leaving a variety of distribution shifts underexplored. In this paper, we focus on distribution shifts that evolve gradually over time, which are common in the wild but challenging for existing methods, as we show. To address this, we propose STAD, a probabilistic state-space model that adapts a deployed model to temporal distribution shifts by learning the time-varying dynamics in the last set of hidden features. Without requiring labels, our model infers time-evolving class prototypes that act as a dynamic classification head. Through experiments on real-world temporal distribution shifts, we show that our method excels in handling small batch sizes and label shift.", "title_embedding_index": 15924, "title_abs_embedding_index": 15949}]