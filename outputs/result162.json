[
    {
        "title": "Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis",
        "link_suffix": "/forum?id=C1wSR50nYf",
        "link": "https://openreview.net/forum?id=C1wSR50nYf",
        "pdf_link": "https://openreview.net/pdf?id=C1wSR50nYf",
        "keywords": "graph prompting, graph neural networks",
        "abstract": "In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional \"pre-training and fine-tuning\" to \"pre-training and prompting,\" has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a \"dark cloud\" over the graph prompting area for deeper research. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold:First, we provide a formal guarantee theorem, demonstrating graph prompts\u2019 capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks.Second, we derive upper bounds on the error of these data operations for a single graph and extend this discussion to batches of graphs, which are common in graph model training.Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees."
    },
    {
        "title": "Exploring Learning Complexity for Efficient Downstream Dataset Pruning",
        "link_suffix": "/forum?id=FN7n7JRjsk",
        "link": "https://openreview.net/forum?id=FN7n7JRjsk",
        "pdf_link": "https://openreview.net/pdf?id=FN7n7JRjsk",
        "keywords": "data efficiency",
        "abstract": "The ever-increasing fine-tuning cost of large-scale pre-trained models gives rise to the importance of dataset pruning, which aims to reduce dataset size while maintaining task performance.\nHowever, existing dataset pruning methods require training on the entire dataset, which is impractical for large-scale pre-trained models.\nIn this paper, we propose a straightforward, novel, and training-free hardness score named Distorting-based Learning Complexity (DLC), to identify informative images and instructions from the downstream dataset efficiently.\nOur method is motivated by the observation that easy samples learned faster can also be learned with fewer parameters.\nSpecifically, we define the Learning Complexity to quantify sample hardness and utilize a lightweight weights masking process for fast estimation, instead of the costly SGD optimization.\nBased on DLC, we further design a flexible under-sampling with randomness (dubbed FlexRand), replacing the top-K strategy, to alleviate the severe subset distribution shift.\nExtensive experiments with downstream image and instructions dataset pruning benchmarks demonstrate the effectiveness and efficiency of the proposed approach.\nIn the images pruning benchmark, DLC significantly reduces the pruning time by 35$\\times$ while establishing state-of-the-art performance with FlexRand."
    },
    {
        "title": "RITUAL: Random Image Transformations as a Universal Anti-hallucination Lever in LVLMs",
        "link_suffix": "/forum?id=aNYabH9Th4",
        "link": "https://openreview.net/forum?id=aNYabH9Th4",
        "pdf_link": "https://openreview.net/pdf?id=aNYabH9Th4",
        "keywords": "Large Vision Language Models",
        "abstract": "Recent advancements in Large Vision Language Models (LVLMs) have revolutionized how machines understand and generate textual responses based on visual inputs. Despite their impressive capabilities, they often produce \"hallucinatory\" outputs that do not accurately reflect the visual information, posing challenges in reliability and trustworthiness. Inspired by test-time augmentation, we propose a simple, training-free method termed RITUAL to enhance robustness against hallucinations in LVLMs. RITUAL introduces random image transformations as complementary inputs during the decoding phase. Importantly, these transformations are not employed during the training of the LVLMs. This straightforward strategy reduces the likelihood of hallucinations by exposing the model to varied visual scenarios, enriching its decision-making process. While transformed images alone may initially degrade performance, we empirically find that strategically combining them with the original images mitigates hallucinations. Specifically, in cases where hallucinations occur with the original image, the transformed images help correct misinterpretations by adjusting the probability distribution. By diversifying the visual input space, RITUAL provides a more robust foundation for generating accurate outputs. Notably, our method works seamlessly with existing contrastive decoding methods and does not require external models or costly self-feedback mechanisms, making it a practical addition. While extremely simple, RITUAL significantly outperforms existing contrastive decoding methods across several object hallucination benchmarks, including POPE, CHAIR, and MME."
    },
    {
        "title": "SafeVision: Efficient Image Guardrail with Robust Policy Adherence and Explainability",
        "link_suffix": "/forum?id=E1SaL8aK7k",
        "link": "https://openreview.net/forum?id=E1SaL8aK7k",
        "pdf_link": "https://openreview.net/pdf?id=E1SaL8aK7k",
        "keywords": "AI safety, Large language model, Multi modality, Image moderation",
        "abstract": "As image generation models become increasingly prevalent, the need for efficient and transparent guardrails against unsafe content is more critical than ever. Traditional unsafe image classifiers, limited to predefined categories, often misclassify content due to the pure feature-based learning rather than semantic-based reasoning and struggle to adapt to emerging threats. The time and resources required for retraining on new harmful categories further hinder their ability to respond to evolving threats. To address these challenges, we propose SafeVision, a novel image guardrail system that integrates human-like understanding and reasoning with scalability. Within SafeVision, we propose an effective data collection and generation, policy-following training pipeline, and a customized loss function. In particular, we propose an efficient diverse QA generation and training strategy to enhance the effectiveness of the training process.\nSafeVision is able to follow given safety policies during inference time to guardrail against new risk categories and thus avoid expensive retraining, provide accurate risky content predictions, and provide precise explanations. SafeVision operates in two modes: 1) rapid classification mode, and 2) comprehension mode that provides both classification and human-readable explanations.  In addition, considering the limitations of existing unsafe image benchmarks, which contain either only binary or limited categories, we provide VisionHARM-500K, a high-quality unsafe image benchmark comprising over 500k images to cover a wide array of risky categories. This dataset significantly broadens the scope and depth of unsafe image benchmarks. Through comprehensive experiments, we show that SafeVision achieves state-of-the-art performance in both efficiency and accuracy, with an accuracy of 91.77% on the VisionHARM-500K test set (17.77% higher than GPT-4O) and an inference time of 0.0979 seconds per image (over 50 times faster than GPT-4O). SafeVision sets a new standard for comprehensive, policy-following, and explainable image guardrail models, delivering state-of-the-art performance while aligning with human reasoning and enabling scalable adaptation to emerging threats."
    },
    {
        "title": "Large Language Models Can Be More Robust Multiple Choice Selectors Through Attention Intervention",
        "link_suffix": "/forum?id=kauK28525D",
        "link": "https://openreview.net/forum?id=kauK28525D",
        "pdf_link": "https://openreview.net/pdf?id=kauK28525D",
        "keywords": "Large Language Models; Reasoning; In-context learning",
        "abstract": "Multiple-choice question (MCQ) is a common task for evaluating large language models (LLMs). LLMs' performance on MCQ is often affected by various biases. Previous research has extensively examined the impact of inherent option bias on MCQ predictions, where this bias refers to a preference for a specific option ID token introduced during the model's training. However, in an in-context learning scenario, few-shot prompting can also introduce a form of bias, known as context option bias. This occurs, for instance, in extreme cases where all demonstration answers are consistently option A, in which case LLMs may predict A for the given question whatever the question is. Context option bias can significantly degrade LLMs' performance. To better observe the LLMs' behavior when affected by the context option bias, we deliberately use demonstrations with obvious context option bias for MCQ to amplify the effect. The results indicate that certain attention heads in LLMs are particularly sensitive to context option bias. Motivated by this observation, we propose our approach, CoLo, to address this issue. First, using samples with ordinary and biased demonstrations as input, CoLo compares the outputs of two types of inputs and localizes attention heads sensitive to context option bias through sequential interventions. Then, we propose an attention scaling-based method to intervene in the identified attention heads during the inference stage, thereby mitigating the impact of context option bias on the LLMs\u2019 predictions. Experimental results demonstrate that CoLo effectively alleviates the impact of context option bias and improves the LLM's robustness on MCQ tasks."
    },
    {
        "title": "Expressive Power of Graph Neural Networks for (Mixed-Integer) Quadratic Programs",
        "link_suffix": "/forum?id=iqd8aHKwGA",
        "link": "https://openreview.net/forum?id=iqd8aHKwGA",
        "pdf_link": "https://openreview.net/pdf?id=iqd8aHKwGA",
        "keywords": "Quadratic programs, mixed-integer quadratic programs, graph neural networks, universal approximation",
        "abstract": "Quadratic programming (QP) is the most widely applied category of problems in nonlinear programming. Many applications require real-time/fast solutions, though not necessarily with high precision. Existing methods either involve matrix decomposition or use the preconditioned conjugate gradient method. For relatively large instances, these methods cannot achieve the real-time requirement unless there is an effective preconditioner. Recently, graph neural networks (GNNs) opened new possibilities for QP. Some promising empirical studies of applying GNNs for QP tasks show that GNNs can capture key characteristics of an optimization instance and provide adaptive guidance accordingly to crucial configurations during the solving process, or directly provide an approximate solution. Despite notable empirical observations, theoretical foundations are still lacking.In this work, we investigate the expressive or representative power of GNNs, a crucial aspect of neural network theory, specifically in the context of QP tasks, with both continuous and mixed-integer settings. We prove the existence of message-passing GNNs that can reliably represent key properties of quadratic programs, including feasibility, optimal objective value, and optimal solution. Our theory is validated by numerical results."
    },
    {
        "title": "Event-Driven Online Vertical Federated Learning",
        "link_suffix": "/forum?id=FCBbh0HCrF",
        "link": "https://openreview.net/forum?id=FCBbh0HCrF",
        "pdf_link": "https://openreview.net/pdf?id=FCBbh0HCrF",
        "keywords": "Vertical Federated Learning, Online Learning, Event Driven",
        "abstract": "Online learning is more adaptable to real-world scenarios in Vertical Federated Learning (VFL) compared to offline learning. \nHowever, integrating online learning into VFL presents challenges due to the unique nature of VFL, where clients possess non-intersecting feature sets for the same sample. \nIn real-world scenarios, the clients may not receive data streaming for the disjoint features for the same entity synchronously. Instead, the data are typically generated by aneventrelevant to only a subset of clients.\nWe are the first to identify these challenges in online VFL, which have been overlooked by previous research. To address these challenges, we proposed an event-driven online VFL framework. In this framework, only a subset of clients were activated during each event, while the remaining clients passively collaborated in the learning process. \nFurthermore, we incorporateddynamic local regret (DLR)into VFL to address the challenges posed by online learning problems with non-convex models within a non-stationary environment.\nWe conducted a comprehensive regret analysis of our proposed framework, specifically examining the DLR under non-convex conditions with event-driven online VFL. \nExtensive experiments demonstrated that our proposed framework was more stable than the existing online VFL framework under non-stationary data conditions while also significantly reducing communication and computation costs."
    },
    {
        "title": "DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References",
        "link_suffix": "/forum?id=ajSmXqgS24",
        "link": "https://openreview.net/forum?id=ajSmXqgS24",
        "pdf_link": "https://openreview.net/pdf?id=ajSmXqgS24",
        "keywords": "Dexterous Manipulation, Neural Tracking Control, Homotopy Optimization",
        "abstract": "We address the challenge of developing a generalizable neural tracking controller for dexterous manipulation from human references. This controller aims to manage a dexterous robot hand to manipulate diverse objects for various purposes defined by kinematic human-object interactions. Developing such a controller is complicated by the intricate contact dynamics of dexterous manipulation and the need for adaptivity, generalizability, and robustness. Current reinforcement learning and trajectory optimization methods often fall short due to their dependence on task-specific rewards or precise system models. We introduce an approach that curates large-scale successful robot tracking demonstrations, comprising pairs of human references and robot actions, to train a neural controller. Utilizing a data flywheel, we iteratively enhance the controller's performance, as well as the number and quality of successful tracking demonstrations. We exploit available tracking demonstrations and carefully integrate reinforcement learning and imitation learning to boost the controller's performance in dynamic environments. At the same time, to obtain high-quality tracking demonstrations, we individually optimize per-trajectory tracking by leveraging the learned tracking controller in a homotopy optimization method. The homotopy optimization, mimicking chain-of-thought, aids in solving challenging trajectory tracking problems to increase demonstration diversity. We showcase our success by training a generalizable neural controller and evaluating it in both simulation and real world. Our method achieves over a 10% improvement in success rates compared to leading baselines. The project website with animated results is available atDexTrack."
    },
    {
        "title": "MELODI: Exploring Memory Compression for Long Contexts",
        "link_suffix": "/forum?id=TvGPP8i18S",
        "link": "https://openreview.net/forum?id=TvGPP8i18S",
        "pdf_link": "https://openreview.net/pdf?id=TvGPP8i18S",
        "keywords": "Memory, Compression, Long Context",
        "abstract": "We present MELODI, a novel memory architecture designed to efficiently process long documents using short context windows. The key principle behind MELODI is to represent short-term and long-term memory as a hierarchical compression scheme across both network layers and context windows. Specifically, the short-term memory is achieved through recurrent compression of context windows across multiple layers, ensuring smooth transitions between windows. In contrast, the long-term memory performs further compression within a single middle layer and aggregates information across context windows, effectively consolidating crucial information from the entire history. Compared to a strong baseline - the Memorizing Transformer employing dense attention over a large long-term memory (64K key-value pairs) - our method demonstrates superior performance on various long-context datasets while remarkably reducing the memory footprint by a factor of 8."
    },
    {
        "title": "Don't Miss the Forest for the Trees: Attentional Vision Calibration for Large Vision Language Models",
        "link_suffix": "/forum?id=iplOFSOzS2",
        "link": "https://openreview.net/forum?id=iplOFSOzS2",
        "pdf_link": "https://openreview.net/pdf?id=iplOFSOzS2",
        "keywords": "Large Vision Language Models",
        "abstract": "This study seeks to understand and address a phenomenon observed in Large Vision Language Models (LVLMs) related to their attention mechanism. Interestingly, LVLMs tend to disproportionately focus on a few image tokens that lack meaningful, query-related semantics, leading to sharp outlier values in the attention maps \u2014 tokens we refer to as blind tokens. In well-designed attention mechanisms, the principle is to assign higher weights to the most relevant tokens. However, in this case, the attention imbalance leads to overemphasis on uninformative tokens, which is far from ideal. Our analysis shows that tokens receiving lower attention weights often hold critical information necessary for capturing subtle visual details. We hypothesize that over-reliance on blind tokens contributes to hallucinations in LVLMs.\nTo address this, we introduce a novel decoding technique called Attentional Vision Calibration (AVISC). During the decoding phase, AVISC identifies blind tokens by examining the image-wise attention distribution and dynamically adjusts the logits for the prediction. Specifically, it contrasts the logits conditioned on the original visual tokens with those conditioned on the blind tokens, thereby reducing the model\u2019s dependency on blind tokens and encouraging a more balanced consideration of all visual tokens. We validate AVISC on standard hallucination benchmarks, including POPE, MME, and AMBER, where it consistently outperforms existing decoding techniques."
    },
    {
        "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling",
        "link_suffix": "/forum?id=jZVNmDiU86",
        "link": "https://openreview.net/forum?id=jZVNmDiU86",
        "pdf_link": "https://openreview.net/pdf?id=jZVNmDiU86",
        "keywords": "Large Language Models; Efficient Generative Inference",
        "abstract": "In this study, we investigate whether attention-based information flow inside large language models (LLMs) is aggregated through noticeable patterns for long context processing.  Our observations reveal that LLMs aggregate information through \nPyramidal Information Funneling where attention is scattering widely in lower layers, progressively consolidating within specific contexts, and ultimately focusing on critical tokens (a.k.a massive activation or attention sink) in higher layers. Motivated by these insights, we developed PyramidKV, a novel and effective KV cache compression method. This approach dynamically adjusts the KV cache size across different layers, allocating more cache in lower layers and less in higher ones, diverging from traditional methods that maintain a uniform KV cache size. Our experimental evaluations, utilizing the LongBench benchmark, show that PyramidKV matches the performance of models with a full KV cache while retaining only 12% of the KV cache, thus significantly reducing memory usage. In scenarios emphasizing memory efficiency, where only 0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache compression techniques, achieving up to a 20.5 absolute accuracy improvement on TREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms competing methods in maintaining long-context comprehension in LLMs; notably, retaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve 100% Acc. performance, matching that of a full KV cache."
    },
    {
        "title": "Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder",
        "link_suffix": "/forum?id=5oaUMZEjWe",
        "link": "https://openreview.net/forum?id=5oaUMZEjWe",
        "pdf_link": "https://openreview.net/pdf?id=5oaUMZEjWe",
        "keywords": "Speaker diarization, speech separation, multi-speaker speech recognition, overlapped speech recognition, end-to-end, multitask learning",
        "abstract": "The rapid progress of single-task architectures has dominated recent developments in multi-talker speech processing, prompting the need for unified approaches. This paper introduces a unified multi-speaker encoder (UME), a novel model architecture that jointly learns representations for diarization, separation, and multi-speaker automatic speech recognition (ASR) tasks using a shared pre-trained foundational speech encoder. We leverage the hidden representations from multiple layers of UME to effectively use information from different semantic levels, contributing to bottom-up alignment between tasks. This joint training approach captures the inherent interdependencies among the tasks, enhancing overall performance on overlapping speech data. Our evaluations demonstrate that UME achieves substantial improvements over the single-task state-of-the-art (SOTA) baselines dedicated to speaker diarization, speech separation, and multi-speaker ASR. Notably, for speaker diarization, UME achieved SOTA performance by lowering the diarization error rate (DER) from 3.24 to 2.19 on the Libri2Mix dataset. Furthermore, our results in multi-speaker ASR outperform the previous results, reducing the concatenated minimum-permutation word error rate (cpWER) from 11.9 to 9.2 on the LibriSpeech2Mix evaluation set."
    },
    {
        "title": "Generalized Probabilistic Attention Mechanism in Transformers",
        "link_suffix": "/forum?id=dIoLjHet58",
        "link": "https://openreview.net/forum?id=dIoLjHet58",
        "pdf_link": "https://openreview.net/pdf?id=dIoLjHet58",
        "keywords": "Attention Mechanism, Transformer, Rank-Collapse, Gradient Vanishing",
        "abstract": "The Transformer architecture has become widely adopted due to its demonstrated success, attributed to the attention mechanism at its core. Despite these successes, the attention mechanism of Transformers is associated with two well-known issues: rank-collapse and gradient vanishing. In this paper, we present a theoretical analysis that it is inherently difficult to address both issues  simultaneously in the conventional attention mechanism. To handle these issues, we introduce a novel class of attention mechanism, referred to as generalized probabilistic attention mechanism (GPAM), and its dual-attention implementation within the Transformer architecture. Unlike conventional attention mechanisms, GPAM allows for negative attention scores while preserving a fixed total sum. We provide theoretical evidence that the proposed dual-attention GPAM (daGPAM) effectively mitigates both the rank-collapse and gradient vanishing issues which are difficult to resolve simultaneously with the conventional attention mechanisms. Furthermore, we empirically validate this theoretical evidence, demonstrating the superiority of daGPAM compared to other alternative attention mechanisms that were proposed to address the same issues. Additionally, we demonstrate the practical benefits of GPAM in natural language processing tasks, such as language modeling and neural machine translation."
    },
    {
        "title": "Intention Model: A Novel Explanation for In-context Learning",
        "link_suffix": "/forum?id=2F7MFqATdo",
        "link": "https://openreview.net/forum?id=2F7MFqATdo",
        "pdf_link": "https://openreview.net/pdf?id=2F7MFqATdo",
        "keywords": "In-context learning, Large language models",
        "abstract": "In-context learning (ICL) has demonstrated remarkable success in enabling large language models (LLMs) to learn to do a downstream task by simply conditioning on a few input-output demonstrations. Distinct from traditional learning paradigms, ICL does not require model updates, thus attracting significant interest in understanding the mechanisms behind LLMs\u2019 ICL capabilities. Advanced works aim to understand ICL through an empirical viewpoint to provide the multifaceted nature of ICL, while some works aim to explain how ICL can emerge theoretically. However, the current theoretical analysis exhibits a weak connection to empirical explorations due to strong assumptions, e.g., perfect LLMs and ideal demonstrations. This work proposes an intention model, providing a novel theoretical framework for explaining ICL. With mild assumptions, we present a ``no-free-lunch'' theorem for ICL: whether ICL emerges depends on the prediction error and prediction noise, which are determined by \\emph{\\textbf{i)}} LLMs' error of next-token prediction, \\emph{\\textbf{ii)}} LLMs' prediction smoothness, and \\emph{\\textbf{iii)}} the quality of demonstrations. Moreover, our intention model provides a novel explanation for the learning behavior of ICL under various input-output relations, e.g., learning with flipped labels. This is fortunately consistent with our experimental observations."
    },
    {
        "title": "CryoFM: A Flow-based Foundation Model for Cryo-EM Densities",
        "link_suffix": "/forum?id=T4sMzjy7fO",
        "link": "https://openreview.net/forum?id=T4sMzjy7fO",
        "pdf_link": "https://openreview.net/pdf?id=T4sMzjy7fO",
        "keywords": "cryoEM; foundation model; flow matching",
        "abstract": "Cryo-electron microscopy (cryo-EM) is a powerful technique in structural biology and drug discovery, enabling the study of biomolecules at high resolution. Significant advancements by structural biologists using cryo-EM have led to the production of over 38,626 protein density maps at various resolutions. However, cryo-EM data processing algorithms have yet to fully benefit from our knowledge of biomolecular density maps, with only a few recent models being data-driven but limited to specific tasks. In this study, we present CryoFM, a foundation model designed as a generative model, learning the distribution of high-quality density maps and generalizing effectively to downstream tasks. Built on flow matching, CryoFM is trained to accurately capture the prior distribution of biomolecular density maps. Furthermore, we introduce a flow posterior sampling method that leverages CryoFM as a flexible prior for several downstream tasks in cryo-EM and cryo-electron tomography (cryo-ET) without the need for fine-tuning, achieving state-of-the-art performance on most tasks and demonstrating its potential as a foundational model for broader applications in these fields."
    },
    {
        "title": "On the loss of context-awareness in general instruction finetuning",
        "link_suffix": "/forum?id=eDnslTIWSt",
        "link": "https://openreview.net/forum?id=eDnslTIWSt",
        "pdf_link": "https://openreview.net/pdf?id=eDnslTIWSt",
        "keywords": "instruction finetuning, large language models",
        "abstract": "Pretrained Large Language Models (LLMs) require post-training methods such as supervised fine-tuning (SFT) on instruction-response pairs to enable instruction following. \nHowever, this process can potentially harm existing capabilities learned during pretraining. \nIn this paper, we investigate the loss of context awareness after SFT, defined as the capability to extract and understand information from the user-provided context and respond accordingly. \nWe are the first to identify and show that the loss of context-awareness appears on instruction-finetuned LLMs when the chat template is applied to the input prompts. \nWe identify the performance decline is partially caused by the bias embedded into the chat template to focus less on the the user-provided context.\nBased on these observations, we propose two methods to mitigate the loss of context awareness in instruct models: post hoc attention steering on user prompts and conditional instruction fine-tuning with a context-dependency indicator.\nEmpirical experiments on 4 context-dependent downstream tasks and 3 pretrained LLMs of different sizes show that our methods can effectively mitigate the loss of context awareness without compromising the general ability of instruction following. \nOur findings also strongly advocate the necessity to benchmark context awareness after instruction fine-tuning carefully."
    },
    {
        "title": "TOWARDS LAYER-WISE PERSONALIZED FEDERATED LEARNING: ADAPTIVE LAYER DISENTANGLEMENT VIA CONFLICTING GRADIENTS",
        "link_suffix": "/forum?id=qH6pzxPZ0d",
        "link": "https://openreview.net/forum?id=qH6pzxPZ0d",
        "pdf_link": "https://openreview.net/pdf?id=qH6pzxPZ0d",
        "keywords": "Federated Learning, Gradient Conflicts, Negative Transfer, Non IID",
        "abstract": "In personalized Federated Learning (pFL), high data heterogeneity can cause significant gradient divergence across devices, adversely affecting the learning process. This divergence, especially when gradients from different users form an obtuse angle during aggregation, can negate progress, leading to severe weight and gradient update degradation. To address this issue, we introduce a new approach to pFL design, namely Federated Learning with Layer-wise Aggregation via Gradient Analysis (FedLAG), utilizing the concept of gradient conflict at the layer level. Specifically, when layer-wise gradients of different clients form acute angles, those gradients align in the same direction, enabling updates across different clients toward identifying client-invariant features. Conversely, when layer-wise gradient pairs make create obtuse angles, the layers tend to focus on client-specific tasks. In hindsights, FedLAG assigns layers for personalization based on the extent of layer-wise gradient conflicts. Specifically, layers with gradient conflicts are excluded from the global aggregation process. The theoretical evaluation demonstrates that when integrated into other pFL baselines, FedLAG enhances pFL performance by a certain margin. Therefore, our proposed method achieves superior convergence behavior compared with other baselines. Extensive experiments show that our FedLAG outperforms several state-of-the-art methods and can be easily incorporated with many existing methods to further enhance performance."
    },
    {
        "title": "Empowering LLM Agents with Zero-Shot Optimal Decision-Making through Q-learning",
        "link_suffix": "/forum?id=JsVIGVntnQ",
        "link": "https://openreview.net/forum?id=JsVIGVntnQ",
        "pdf_link": "https://openreview.net/pdf?id=JsVIGVntnQ",
        "keywords": "Large language models, Agent, Optimal decision-making",
        "abstract": "Large language models (LLMs) are trained on extensive text data to gain general comprehension capability. Current LLM agents leverage this ability to make zero- or few-shot decisions but fail in making optimal decisions, as LLMs inherently perform next-token prediction based on pre-trained probability distributions rather than maximizing expected future rewards. In contrast, agents trained via reinforcement learning (RL) could make optimal decisions but require extensive environmental data. In this work, we develop an algorithm that combines the zero-shot capabilities of LLMs with the optimal decision-making advantages of RL, referred to as the Model-based LLM Agent with Q-Learning (MLAQ). MLAQ employs Q-learning to derive optimal policies from transitions within memory. However, unlike RL agents that collect data from environmental interactions, MLAQ constructs an imagination space fully based on LLM to perform imaginary interactions for deriving zero-shot policies. Our proposed UCB variant generates imaginary data through interactions with the LLM-based world model, enabling a balance between exploration and exploitation while ensuring a sub-linear regret bound guaranteed by a theorem. Moreover, MLAQ employs a mixed-examination mechanism that utilizes environmental interactions and LLM-based self-examine to enhance the quality of imaginary data. We evaluate MLAQ in benchmarks that present significant challenges for existing LLM agents. Results show that MLAQ achieves a optimal rate of over 90% in tasks where other methods struggle to succeed. Additional experiments are conducted to reach the conclusion that introducing model-based RL into LLM agents shows significant potential for current LLMs to improve their optimal decision-making ability. Our interactive website is available athttp://mlaq.site."
    },
    {
        "title": "TokenUnify: Scalable Autoregressive Pretraining for Large Scale EM Image Segmentation",
        "link_suffix": "/forum?id=rsD6qAvtxO",
        "link": "https://openreview.net/forum?id=rsD6qAvtxO",
        "pdf_link": "https://openreview.net/pdf?id=rsD6qAvtxO",
        "keywords": "biological image, autoregressive visual pre-training",
        "abstract": "Autoregressive next-token prediction, a standard pretraining method for large-scale language models, excels in handling long sequential data. However, its application to complex visual tasks, particularly biological imaging, faces challenges due to the spatial continuity and high dimensionality of biological images. High-resolution 3D biological images, such as electron microscopy (EM) brain scans, offer ideal long-sequence data, but existing methods struggle to fully leverage this characteristic.\nTo address these challenges, we introduce \\textbf{TokenUnify}, a novel pretraining method that integrates random token prediction, next-token prediction, and next-all token prediction. We provide theoretical evidence demonstrating that TokenUnify mitigates cumulative errors in visual autoregression, particularly when dealing with complex three-dimensional anatomical structures.\nIn conjunction with TokenUnify, we have assembled a large-scale, ultra-high-resolution EM brain image dataset comprising over 120 million finely annotated voxels. This dataset not only represents the largest neuron segmentation dataset to date but, more importantly, provides ideal long-sequence biological image data that fully exhibits spatial continuity. Leveraging the Mamba network, which is inherently suited for long-sequence modeling, TokenUnify capitalizes on the advantages of autoregressive methods in processing long-sequence data, achieving a 45% performance improvement on downstream EM neuron segmentation tasks compared to existing methods. Furthermore, TokenUnify demonstrates superior scalability over MAE and traditional autoregressive methods, effectively bridging the gap between pretraining strategies for language and vision models. Code is available at \\url{https://anonymous.4open.science/r/TokenUnify-3DBF}."
    },
    {
        "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
        "link_suffix": "/forum?id=0N8yq8QwkD",
        "link": "https://openreview.net/forum?id=0N8yq8QwkD",
        "pdf_link": "https://openreview.net/pdf?id=0N8yq8QwkD",
        "keywords": "Editable Rendring; 3DGS; Differential Rendering",
        "abstract": "Neural 3D representations such as Neural Radiance Fields (NeRFs), excel at producing photo-realistic rendering results but lack the flexibility for manipulation and editing which is crucial for content creation. Previous works have attempted to address this issue by deforming a NeRF in canonical space or manipulating the radiance field based on an explicit mesh. However, manipulating NeRF is not highly controllable and requires a long training and inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely high-fidelity novel view synthesis can be achieved using an explicit point-based 3D representation with much faster training and rendering speed. However, there is still a lack of effective means to manipulate 3DGS freely while maintaining rendering quality. In this work, we aim to tackle the challenge of achieving manipulable photo-realistic rendering. We propose to utilize a triangular mesh to manipulate 3DGS directly with self-adaptation. This approach reduces the need to design various algorithms for different types of Gaussian manipulation. By utilizing a triangle shape-aware Gaussian binding and adapting method, we can achieve 3DGS manipulation and preserve high-fidelity rendering after manipulation. Our approach is capable of handling large deformations, local manipulations, and even physics simulations while keeping high-quality rendering. Furthermore, we demonstrate that our method is also effective with inaccurate meshes extracted from 3DGS. Experiments conducted on NeRF synthetic datasets demonstrate the effectiveness of our method and its superiority over baseline approaches."
    },
    {
        "title": "Recycled Attention: Efficient inference for long-context language models",
        "link_suffix": "/forum?id=8qYuxV4lRu",
        "link": "https://openreview.net/forum?id=8qYuxV4lRu",
        "pdf_link": "https://openreview.net/pdf?id=8qYuxV4lRu",
        "keywords": "long-context language model, efficiency, inference-time method",
        "abstract": "Processing long-context input imposes a heavy computational burden when deploying large language models. Recently proposed inference-time methods accelerate generation by attending only to local context. Despite its efficiency gains, this approach fails to capture all relevant information in the input, showing substantial performance drop in long-context benchmarks. We propose recycled attention, an efficient and effective method which alternates between full context attention and attention over a subset of input tokens. When performing partial attention, we leverage the attention pattern of a nearby token that has performed full attention and attend only to the top K most attended tokens. We evaluate our methods on RULER, a suite of tasks designed to comprehensively evaluate long-context abilities, and long-context language modeling tasks. Applying our inference method to off-the-shelf LLMs achieves comparable speedup to baselines which only consider local context while improving the performance by 2x. We further experiment with continued pre-training the model with recycled attention to improve the performance-efficiency trade-off."
    },
    {
        "title": "GenQA: An Instruction Dataset of LLM Generated Questions and Answers",
        "link_suffix": "/forum?id=tXnAmayaio",
        "link": "https://openreview.net/forum?id=tXnAmayaio",
        "pdf_link": "https://openreview.net/pdf?id=tXnAmayaio",
        "keywords": "instruction finetuning, large language models",
        "abstract": "Most public instruction finetuning datasets are relatively small compared to the closed source datasets used to train industry models. To study questions about finetuning at scale, such as curricula and learning rate cooldown schedules, there is a need for industrial-scale datasets. However, this scale necessitates a data generation process that is almost entirely automated. In this work, we study methods for generating large instruction datasets from a single prompt. With little human oversight, we get LLMs to write diverse sets of instruction examples ranging from simple completion tasks to complex multi-turn dialogs across a variety of subject areas. When finetuning a Llama-3 8B base model, our dataset meets or exceeds both WizardLM and Ultrachat on both knowledge-intensive leaderboard tasks as well as conversational evaluations. We release our dataset, the \u201cgenerator\u201d prompts that created it, and our finetuned model checkpoints."
    },
    {
        "title": "Scaling Laws for Multilingual Language Models",
        "link_suffix": "/forum?id=T2h2V7Rx7q",
        "link": "https://openreview.net/forum?id=T2h2V7Rx7q",
        "pdf_link": "https://openreview.net/pdf?id=T2h2V7Rx7q",
        "keywords": "scaling laws, large language models, multilingual language models",
        "abstract": "We propose a novel scaling law for general-purpose decoder-only language models (LMs) trained on multilingual data, addressing the problem of balancing languages during multilingual pretraining. A primary challenge in studying multilingual scaling is the difficulty of analyzing individual language performance due to cross-lingual transfer. To address this, we shift the focus from individual languages to language families. We introduce and validate a hypothesis that the test cross-entropy loss for each language family is determined solely by its own sampling ratio, independent of other languages in the mixture. This insight simplifies the complexity of multilingual scaling and make the analysis scalable to an arbitrary number of languages. Building on this hypothesis, we derive a power-law relationship that links performance with dataset size, model size and sampling ratios. This relationship enables us to predict performance across various combinations of the above three quantities, and derive the optimal sampling ratios at different model scales. To demonstrate the effectiveness and accuracy of our proposed scaling law, we perform a large-scale empirical study, training more than 100 models on 23 languages spanning 5 language families. Our experiments show that the optimal sampling ratios derived from small models (85M parameters) generalize effectively to models that are several orders of magnitude larger (1.2B parameters), offering a resource-efficient approach for multilingual LM training at scale."
    },
    {
        "title": "No Free Lunch: Retrieval-Augmented Generation Undermines Fairness in LLMs, Even for Vigilant Users",
        "link_suffix": "/forum?id=cphaRg46jD",
        "link": "https://openreview.net/forum?id=cphaRg46jD",
        "pdf_link": "https://openreview.net/pdf?id=cphaRg46jD",
        "keywords": "Large Language Model, RAG, Fairness",
        "abstract": "Retrieval-Augmented Generation (RAG) is widely adopted for its effectiveness and cost-efficiency in mitigating hallucinations and enhancing the domain-specific generation capabilities of large language models (LLMs). However, is this effectiveness and cost-efficiency truly a free lunch? In this study, we comprehensively investigate the fairness costs associated with RAG by proposing a practical three-level threat model from the perspective of user awareness of fairness. Specifically, varying levels of user fairness awareness result in different degrees of fairness censorship on the external dataset. We examine the fairness implications of RAG using uncensored, partially censored, and fully censored datasets. Our experiments demonstrate that fairness alignment can be easily undermined through RAGwithout the need for fine-tuning or retraining.Even with fully censored and supposedly unbiased external datasets, RAG can lead to biased outputs.Our findings underscore the limitations of current alignment methods in the context of RAG-based LLMs and highlight the urgent need for new strategies to ensure fairness. We propose potential mitigations and call for further research to develop robust fairness safeguards in RAG-based LLMs."
    },
    {
        "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA",
        "link_suffix": "/forum?id=WwpYSOkkCt",
        "link": "https://openreview.net/forum?id=WwpYSOkkCt",
        "pdf_link": "https://openreview.net/pdf?id=WwpYSOkkCt",
        "keywords": "Efficient LLM, Parameter Sharing, Relaxed Recursive Transformer, Continuous Depth-wise Batching, Early-Exiting",
        "abstract": "Large language models (LLMs) are expensive to deploy. Parameter sharing offers a possible path towards reducing their size and cost, but its effectiveness in modern LLMs remains fairly limited. In this work, we revisit \"layer tying\" as form of parameter sharing in Transformers, and introduce novel methods for converting existing LLMs into smaller \"Recursive Transformers\" that share parameters across layers, with minimal loss of performance. Here, our Recursive Transformers are efficiently initialized from standard pretrained Transformers, but only use a single block of unique layers that is then repeated multiple times in a loop. We further improve  performance by introducing Relaxed Recursive Transformers that add flexibility to the layer tying constraint via depth-wise low-rank adaptation (LoRA) modules, yet still preserve the compactness of the overall model. We show that our recursive models (e.g., recursive Gemma 1B) outperform both similar-sized vanilla pretrained models (such as TinyLlama 1.1B and Pythia 1B) and knowledge distillation baselines---and can even recover most of the performance of the original \"full-size\" model (e.g., Gemma 2B with no shared parameters). Finally, we propose Continuous Depth-wise Batching, a promising new inference paradigm enabled by the Recursive Transformer when paired with early exiting, which we show to theoretically lead to significant (2-3$\\times$) throughput gains."
    }
]