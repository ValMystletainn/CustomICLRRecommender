[{"title": "Collab: Controlled Decoding using Mixture of Agents for LLM Alignment", "link_suffix": "/forum?id=7ohlQUbTpp", "link": "https://openreview.net/forum?id=7ohlQUbTpp", "pdf_link": "https://openreview.net/pdf?id=7ohlQUbTpp", "keywords": "Alignment, Decoding, RLHF, Transfer Decoding, LLM", "abstract": "Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences, and broader utilities, but it requires updating billions of model parameters which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agents-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward, for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, COLLAB surpasses the current SoTA decoding strategy, achieving an improvement of {up to 1.56x} in average reward and $71.89%$ in GPT-4 based win-tie rate.", "title_embedding_index": 1900, "title_abs_embedding_index": 1925}, {"title": "Provable Data-driven Hyperparameter Tuning for Deep Neural Networks", "link_suffix": "/forum?id=9D9VoONnn6", "link": "https://openreview.net/forum?id=9D9VoONnn6", "pdf_link": "https://openreview.net/pdf?id=9D9VoONnn6", "keywords": "learning theory, data-driven algorithm design, hyperparameter tuning, neural architecture search, graph neural networks, sample complexity", "abstract": "Modern machine learning algorithms, especially deep learning-based techniques, typically involve careful\nhyperparameter tuning to achieve the best performance. Despite the surge of intense interest in practical\ntechniques like Bayesian optimization and random search-based approaches to automating this laborious and\ncompute-intensive task, the fundamental learning-theoretic complexity of tuning hyperparameters for deep\nneural networks is poorly understood. Inspired by this glaring gap, we initiate the formal study of hyperparameter tuning complexity in deep learning through a recently introduced lens of data-driven algorithm design. We assume that we have a series of deep learning tasks, and we have to tune hyperparameters to do well on average over the distribution of\ntasks. A major difficulty is that the loss as a function of the hyperparameter is very volatile and furthermore,\nit is given implicitly by an optimization problem over the model parameters. This is unlike previous work\nin data-driven design, where one can typically explicitly model the algorithmic behavior as a function of\nthe hyperparameters. To tackle this we introduce a new technique to characterize the discontinuities and \noscillations of the loss function on any fixed problem instance as we vary the hyperparameter; our analysis \nrelies on subtle concepts including tools from differential geometry and constrained optimization. This can be \nused to show that the intrinsic complexity of the corresponding family of loss functions is bounded. We instantiate \nour results and provide the first precise sample complexity bounds for concrete applications\u2014tuning a hyperparameter that interpolates neural activation functions and setting the kernel \nparameter in graph neural networks.", "title_embedding_index": 1901, "title_abs_embedding_index": 1926}, {"title": "Distributionally Robust Policy Learning under Concept Drifts", "link_suffix": "/forum?id=TbRFc2HPoN", "link": "https://openreview.net/forum?id=TbRFc2HPoN", "pdf_link": "https://openreview.net/pdf?id=TbRFc2HPoN", "keywords": "distributionally robust optimization, offline policy learning, concept drift, bandit learning, reinforcement learning.", "abstract": "Distributionally robust policy learning aims to find a policy that performs well \nunder the worst-case distributional shift, and yet most existing methods for \nrobust policy learning consider the worst-case joint distribution of \nthe covariate and the outcome. The joint-modeling strategy can be unnecessarily conservative\nwhen we have more information on the source of distributional shifts. This paper studies\na more nuanced problem --- robust policy learning under the concept drift, \nwhen only the conditional relationship between the outcome and the covariate changes. \nTo this end, we first provide a doubly-robust estimator for evaluating\nthe worst-case average reward of a given policy under a set of perturbed conditional distributions. \nWe show that the policy value estimator enjoys asymptotic normality even if the nuisance parameters \nare estimated with a slower-than-root-$n$ rate. We then propose a learning algorithm that outputs the policy maximizing the \nestimated policy value within a given policy class $\\Pi$, and show\nthat the sub-optimality gap of the proposed algorithm is of the order \n$\\kappa(\\Pi)n^{-1/2}$, with $\\kappa(\\Pi)$ is the entropy integral of $\\Pi$ under the Hamming distance\nand $n$ is the sample size.\nThe proposed methods are implemented and evaluated in numerical studies, \ndemonstrating substantial improvement compared with existing benchmarks.", "title_embedding_index": 1902, "title_abs_embedding_index": 1927}, {"title": "Gen-LRA: Towards a Principled Membership Inference Attack for Generative Models", "link_suffix": "/forum?id=02DCEU6vSU", "link": "https://openreview.net/forum?id=02DCEU6vSU", "pdf_link": "https://openreview.net/pdf?id=02DCEU6vSU", "keywords": "Privacy, Membership Inference Attacks, Generative Models", "abstract": "Evaluating the potential privacy leakage of synthetic data is an important but unresolved problem. Most existing adversarial auditing frameworks for synthetic data rely on heuristics and unreasonable assumptions to attack the failure modes of generative models, exhibiting limited capability to describe and detect the privacy exposure of training data. In this paper, we study designing Membership Inference Attacks (MIAs) that specifically exploit the observation that generative models tend to memorize certain data points in their training sets, leading to significant local overfitting. Here, we propose Generative Likelihood Ratio Attack (Gen-LRA), a novel, computationally efficient shadow-box MIA that, with no assumption of model knowledge or access, attacks the generated synthetic dataset by conducting a hypothesis test that it is locally overfit to potential training data. Assessed over a comprehensive benchmark spanning diverse datasets, model architectures, and attack parameters, we find that Gen-LRA consistently dominates other MIAs for generative models across multiple performance metrics. These results underscore Gen-LRA's effectiveness as an interpretable and robust privacy auditing tool, highlighting the significant privacy risks posed by generative model overfitting in real-world applications", "title_embedding_index": 1903, "title_abs_embedding_index": 1928}, {"title": "Pose Priors from Language Models", "link_suffix": "/forum?id=SLDqCpHPuP", "link": "https://openreview.net/forum?id=SLDqCpHPuP", "pdf_link": "https://openreview.net/pdf?id=SLDqCpHPuP", "keywords": "pose estimation, language models, multimodal models", "abstract": "We present a zero-shot pose optimization method that enforces accurate physical contact constraints when estimating the 3D pose of humans. Our central insight is that since language is often used to describe physical interaction, large pretrained text-based models can act as priors on pose estimation.\nWe can thus leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain the 3D pose optimization. Despite its simplicity, our method produces surprisingly compelling pose reconstructions of people in close contact, correctly capturing the semantics of the social and physical interactions. We demonstrate that our method rivals more complex state-of-the-art approaches that require expensive human annotation of contact points and training specialized models. Moreover, unlike previous approaches, our method provides a unified framework for resolving self-contact and person-to-person contact.", "title_embedding_index": 1904, "title_abs_embedding_index": 1929}, {"title": "Bias in CLIP Encoders: A Study of Encoder Bias and Object Representation in Multi-Object Scenarios", "link_suffix": "/forum?id=bqLx5Rs8Tc", "link": "https://openreview.net/forum?id=bqLx5Rs8Tc", "pdf_link": "https://openreview.net/pdf?id=bqLx5Rs8Tc", "keywords": "CLIP, Multi-object, Vision-language Models, Bias", "abstract": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated remarkable performance in zero-shot classification tasks, yet their efficacy in handling complex multi-object scenarios remains challenging. This study presents a comprehensive analysis of CLIP's performance limitations in multi-object contexts through controlled experiments. We present a specialized dataset, ComCO, crafted to thoroughly assess the performance of CLIP's encoders in diverse multi-object scenarios.\nOur findings reveal significant biases in both encoders, with the text encoder showing a tendency to prioritize objects that are mentioned first in the prompt, and the image encoder exhibiting a bias toward larger objects. Through meticulous experiments, including both retrieval-based and classification-based tasks, we quantify these biases across multiple CLIP variants, we quantify these biases across multiple CLIP variants. We hypothesize that these biases originate from CLIP's training process and provide substantiating evidence through detailed analyses of the LAION dataset and CLIP's training progression.\nOur image-text matching experiments demonstrate substantial performance drops when manipulating object sizes in the images and/or object tokens order in the prompt, highlighting the CLIP's unstable performance when given rephrased yet semantically similar captions. We extend this analysis to longer, more complex captions and text-to-image generative models such as Stable Diffusion, revealing how CLIP's text encoder bias influences object prominence in generated images based on the prompt's token order.\nThis work provides crucial insights into CLIP's behavior in complex visual-linguistic contexts, offering a robust evaluation methodology and identifying key areas for improving future vision-language models in multi-object scenarios.", "title_embedding_index": 1905, "title_abs_embedding_index": 1930}, {"title": "Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debate", "link_suffix": "/forum?id=06ZvHHBR0i", "link": "https://openreview.net/forum?id=06ZvHHBR0i", "pdf_link": "https://openreview.net/pdf?id=06ZvHHBR0i", "keywords": "LLM Evals, Adversarial analysis, Mechanism Design", "abstract": "We propose a novel framework for evaluating large language model (LLM) outputs using LLMs themselves as interacting agents in an adversarial debate system. Our approach casts LLMs as advocates, judges, and juries within a structured courtroom-inspired setting. Advocate LLMs engage in iterative argumentation to refine and critique responses, while judge and jury LLMs moderate and assess the debate. We introduce a probabilistic model using Beta-Binomial distribution to analyze error reduction dynamics in this iterative process. Comparative studies of ranking versus scoring methods for LLM jurors reveal advantages of fine-grained scoring in capturing nuanced quality assessments. Experiments across diverse language tasks demonstrate our framework's superior performance in agreement with human judgments and provision of interpretable feedback compared to traditional evaluation methods. This work contributes a theoretically grounded, scalable approach to LLM evaluation that addresses limitations of existing techniques and adapts to rapid advancements in language AI technologies.", "title_embedding_index": 1906, "title_abs_embedding_index": 1931}, {"title": "Finally Rank-Breaking Conquers MNL Bandits: Optimal and Efficient Algorithms for MNL Assortment", "link_suffix": "/forum?id=kx8i1yfkRX", "link": "https://openreview.net/forum?id=kx8i1yfkRX", "pdf_link": "https://openreview.net/pdf?id=kx8i1yfkRX", "keywords": "Active online assortment optimization, Preference feedback, Subsetwise utility maximization, Assortment selection algorithms, Plackett Luce model, Regret minimization, Pairwise Rank-Breaking, Concentration guarantee, Practical algorithms, Empirical evaluations", "abstract": "We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, and fine-tuning language models, amongst many others. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a ``strong reference\" which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected---all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \\emph{Plackett Luce} (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using `\\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods.", "title_embedding_index": 1907, "title_abs_embedding_index": 1932}, {"title": "Top-m Data Values Identification", "link_suffix": "/forum?id=lOfuvmi2HT", "link": "https://openreview.net/forum?id=lOfuvmi2HT", "pdf_link": "https://openreview.net/pdf?id=lOfuvmi2HT", "keywords": "data valuation, data subset selection, top-m arms identification", "abstract": "Data valuation has found many real-world applications, e.g., data pricing and data selection. However, the most adopted approach -- Shapley value (SV) -- is computationally expensive due to the large number of model trainings required. Fortunately, most applications (e.g., data selection) require only knowing the $m$ data points with the highest data values (i.e., top-$m$ data values), which implies the potential for fewer model trainings as exact data values are not required. Existing work formulates top-$m$ Shapley value identification as top-$m$ arms identification in multi-armed bandits (MAB). However, the proposed approach falls short because it does not utilize data features to predict data values, a method that has been shown empirically to be effective. A recent top-$m$ arms identification work does consider the use of arm features while assuming a linear relationship between arm features and rewards, which is often not satisfied in data valuation. To this end, we propose the GPGapE algorithm that uses the Gaussian process to model the \\emph{non-linear} mapping from data features to data values, removing the linear assumption. We theoretically analyze the correctness and stopping iteration of GPGapE in finding an $(\\epsilon, \\delta)$-approximation to the top-$m$ data values. We further improve the computational efficiency, by calculating data values using small data subsets to reduce the computation cost of model trainings. We empirically demonstrate that GPGapE outperforms other baselines in top-$m$ data values identification, noisy data detection, and data subset selection on real-world datasets.", "title_embedding_index": 1908, "title_abs_embedding_index": 1933}, {"title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery", "link_suffix": "/forum?id=6z4YKr0GK6", "link": "https://openreview.net/forum?id=6z4YKr0GK6", "pdf_link": "https://openreview.net/pdf?id=6z4YKr0GK6", "keywords": "Benchmark, Evaluation, Large Language Model, Language Agent, AI for Science, Code Generation, Task Automation", "abstract": "The advancements of language language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about the true capabilities of such agents. In this work, we argue that for an agent to fully automate scientific discovery, it must be able to complete all essential tasks in the workflow. Thus, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using our benchmark, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. These results underscore the limited capacities of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research. In the long run, ScienceAgentBench will serve as a benchmark for rigorously measuring progress toward developing language agents to assist human scientists in data-driven scientific discovery.", "title_embedding_index": 1909, "title_abs_embedding_index": 1934}, {"title": "SHARP: Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters", "link_suffix": "/forum?id=Wb6Mcmo0ch", "link": "https://openreview.net/forum?id=Wb6Mcmo0ch", "pdf_link": "https://openreview.net/pdf?id=Wb6Mcmo0ch", "keywords": "inference acceleration, weight sharing, language model, model compression", "abstract": "While Large language models (LLMs) have advanced natural language processing tasks, their growing computational and memory demands make deployment on resource-constrained devices like mobile phones increasingly challenging. In this paper, we propose SHARP (SHaring Adjacent Layers with Recovery Parameters), a novel approach to accelerate LLM inference by sharing parameters across adjacent layers, thus reducing memory load overhead, while introducing low-rank recovery parameters to maintain performance.\nInspired by observations that consecutive layers have similar outputs, SHARP employs a two-stage recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT).\nThe SLW stage aligns the outputs of the shared layers using  $\\mathcal{L}_2$ loss, providing a good initialization for the following SFT stage to further restore the model performance. Extensive experiments demonstrate that SHARP can recover the model's perplexity on various in-distribution tasks using no more than 50k fine-tuning data while reducing the number of stored MLP parameters by 38% to 65%.\nWe also conduct several ablation studies of SHARP and show that replacing layers towards the later parts of the model yields better performance retention, and that different recovery parameterizations perform similarly when parameter counts are matched.\nFurthermore, SHARP saves 42.8% in model storage and reduces the total inference time by 42.2% compared to the original Llama2-7b model on mobile devices.\nOur results highlight SHARP as an efficient solution for reducing inference costs in deploying LLMs without the need for pretraining-scale resources.", "title_embedding_index": 1910, "title_abs_embedding_index": 1935}, {"title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads", "link_suffix": "/forum?id=tkiZQlL04w", "link": "https://openreview.net/forum?id=tkiZQlL04w", "pdf_link": "https://openreview.net/pdf?id=tkiZQlL04w", "keywords": "LLMs, KV cache compression, LLM inference acceleration", "abstract": "The memory and computational demands of Key-Value (KV) cache present significant challenges for deploying long-context language models. Previous approaches attempt to mitigate this issue by selectively dropping tokens, which irreversibly erases critical information that might be needed for future queries. In this paper, we propose a novel compression technique for KV cache that preserves all token information. Our investigation reveals that: i) Most attention heads primarily focus on the local context; ii) Only a few heads, denoted as retrieval heads, can essentially pay attention to all input tokens. These key observations motivate us to use separate caching strategy for attention heads.Therefore, we propose RazorAttention, a training-free KV cache compression algorithm, which maintains a full cache for these crucial retrieval heads and discards the remote tokens in non-retrieval heads. Furthermore, we introduce a novel mechanism involving a \u201ccompensation token\u201d to further recover the information in the dropped tokens. Extensive evaluations across a diverse set of large language models (LLMs) demonstrate that RazorAttention achieves a reduction in KV cache size by over 70% without noticeable impacts on performance. Additionally, RazorAttention is compatible with FlashAttention, rendering it an efficient and plug-and-play solution that enhances LLM inference efficiency without overhead or retraining of the original model.", "title_embedding_index": 1911, "title_abs_embedding_index": 1936}, {"title": "Retrieval or Reasoning: The Roles of Graphs and Large Language Models in Efficient Knowledge-Graph-Based Retrieval-Augmented Generation", "link_suffix": "/forum?id=JvkuZZ04O7", "link": "https://openreview.net/forum?id=JvkuZZ04O7", "pdf_link": "https://openreview.net/pdf?id=JvkuZZ04O7", "keywords": "Knowledge Graphs, Large Language Models, Retrieval-Augmented Generation, Retrieval", "abstract": "Large Language Models (LLMs) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph (KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by grounding LLM outputs in structured external knowledge from KGs. However, current KG-based RAG frameworks still struggle to optimize the trade-off between retrieval accuracy and efficiency in identifying a suitable amount of relevant graph information for the LLM to digest. We introduce SubgraphRAG, extending the KG-based RAG framework that retrieves subgraphs centered on query/topic entities and leverages LLMs for reasoning. Our approach innovatively integrates a lightweight multilayer perceptron (MLP) with a parallel triple-scoring mechanism for efficient subgraph retrieval while encoding directional structural distances to enhance retrieval accuracy. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream LLM's reasoning capacity. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller models like Llama3.1-8B deliver competitive results with explainable reasoning, while larger models like GPT-4o achieve comparable or better state-of-the-art accuracy compared with previous baselines\u2014all without fine-tuning. Extensive evaluations on the WebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.", "title_embedding_index": 1912, "title_abs_embedding_index": 1937}, {"title": "Feature Driven Graph Coarsening for Scaling Graph Representation Learning", "link_suffix": "/forum?id=6VuTXirQIv", "link": "https://openreview.net/forum?id=6VuTXirQIv", "pdf_link": "https://openreview.net/pdf?id=6VuTXirQIv", "keywords": "Graph Coarsening, Graph Neural Networks", "abstract": "Graphical modelling for structured data analysis has gained prominence across numerous domains. A significant computational challenge lies in efficiently capturing complex relationships within large-scale graph structures. Graph coarsening, which reduces graph size by merging nodes and edges into supernodes and superedges, enhances scalability and is crucial for graph neural networks (GNNs). However, current methods either construct graphs from large-scale attribute data or assume a pre-existing graph before coarsening, limiting their applicability, especially in domains like healthcare and finance where graph structure is often unavailable. In this paper, we present a novel framework that directly learns a coarsened graph from attribute information, reducing computational complexity and enhancing robustness against adversarial attacks, which commonly target vulnerabilities in graph structures. By integrating label information, our framework also enables semi-supervised learning, leading to improved performance on downstream tasks. Extensive experiments show that our method outperforms state-of-the-art coarsening techniques in both accuracy and computational efficiency.", "title_embedding_index": 1913, "title_abs_embedding_index": 1938}, {"title": "LLMPhy: Complex Physical Reasoning Using Large Language Models and World Models", "link_suffix": "/forum?id=qGL6fE1lqd", "link": "https://openreview.net/forum?id=qGL6fE1lqd", "pdf_link": "https://openreview.net/pdf?id=qGL6fE1lqd", "keywords": "large language models, physics simulators, physical reasoning", "abstract": "Physical reasoning is an important skill needed for robotic agents when operating in the real world. However, solving such reasoning problems often involves hypothesizing and reflecting over complex multi-body interactions under the effect of a multitude of physical forces and thus poses a significant hurdle for state-of-the-art machine learning frameworks, including large language models (LLMs). To study this problem, we propose a new physical reasoning task and a dataset, dubbed TraySim. Our task involves predicting the dynamics of several objects on a tray that is given an external impact -- the domino effect of the ensued object interactions and their dynamics thus offering a challenging yet constrained setup, with the goal of reasoning being to infer the stability of the objects. To solve this task, we present LLMPhy, a zero-shot black-box optimization framework that combines the physics knowledge and program synthesis abilities of LLMs with the world models built into modern physics engines. Specifically, LLMPhy optimizes over the hyper-parameters of the system (friction, damping, etc.) via an analysis-by-synthesis approach using the simulator in the loop, and uses the inferred parameters to simulate the dynamics of the given scene for solving the reasoning task. To demonstrate the effectiveness of LLMPhy, we present experiments on our TraySim dataset to predict the steady-state pose of the objects on the tray after the impact. Our results show that the combination of the LLM and the physics engine offers complementary benefits, leading to substantial gains in reasoning performance. Our dataset and simulation code will be made public.", "title_embedding_index": 1914, "title_abs_embedding_index": 1939}, {"title": "MrT5: Dynamic Token Merging for Efficient Byte-level Language Models", "link_suffix": "/forum?id=VYWBMq1L7H", "link": "https://openreview.net/forum?id=VYWBMq1L7H", "pdf_link": "https://openreview.net/pdf?id=VYWBMq1L7H", "keywords": "NLP, ByT5, T5, tokenization, byte-level language models, character-level language models", "abstract": "Models that rely on subword tokenization have significant drawbacks, such as sensitivity to character-level noise like spelling errors and inconsistent compression rates across different languages and scripts. While character or byte-level models like ByT5 attempt to address these concerns, they have not gained widespread adoption\u2014processing raw byte streams without tokenization results in significantly longer sequence lengths, making training and inference inefficient. This work introduces MrT5 (MergeT5), a more efficient variant of ByT5 that integrates a token deletion mechanism in its encoder to dynamically shorten the input sequence length. After processing through a fixed number of encoder layers, a learnt delete gate determines which tokens are to be removed and which are to be retained for subsequent layers. MrT5 effectively \"merges\" critical information from deleted tokens into a more compact sequence, leveraging contextual information from the remaining tokens. In continued pre-training experiments, we find that MrT5 can achieve significant gains in inference runtime with minimal effect on performance. When trained on English text, MrT5 demonstrates the capability to transfer its deletion feature zero-shot across several languages, with significant additional improvements following multilingual training. Furthermore, MrT5 shows comparable accuracy to ByT5 on downstream evaluations such as XNLI and character-level tasks while reducing sequence lengths by up to 80%. Our approach presents a solution to the practical limitations of existing byte-level models.", "title_embedding_index": 1915, "title_abs_embedding_index": 1940}, {"title": "VVC-Gym: A Fixed-Wing UAV Reinforcement Learning Environment for Multi-Goal Long-Horizon Problems", "link_suffix": "/forum?id=5xSRg3eYZz", "link": "https://openreview.net/forum?id=5xSRg3eYZz", "pdf_link": "https://openreview.net/pdf?id=5xSRg3eYZz", "keywords": "Reinforcement Learning Environment, Demonstrations, Goal-Conditioned Reinforcement Learning, Fixed-wing UAV Velocity Vector Control", "abstract": "Multi-goal long-horizon problems are prevalent in real-world applications. The additional goal space introduced by multi-goal problems intensifies the spatial complexity of exploration; meanwhile, the long interaction sequences in long-horizon problems exacerbate the temporal complexity of exploration. Addressing the great exploration challenge posed by multi-goal long-horizon problems depends not only on the design of algorithms but also on the design of environments and the availability of demonstrations to assist in training. To facilitate the above research, we propose a multi-goal long-horizon Reinforcement Learning (RL) environment based on realistic fixed-wing UAV's velocity vector control, named VVC-Gym, and generate multiple demonstration sets of various quality. Through experimentation, we analyze the impact of different environment designs on training, assess the quantity and quality of demonstrations and their influence on training, and assess the effectiveness of various RL algorithms, providing baselines on VVC-Gym and its corresponding demonstrations. The results suggest that VVC-Gym is suitable for studying: (1) the influence of environment designs on addressing multi-goal long-horizon problems with RL. (2) the assistance that demonstrations can provide in overcoming the exploration challenges of multi-goal long-horizon problems. (3) the RL algorithm designs with the least possible impact from environment designs on the efficiency and effectiveness of training.", "title_embedding_index": 1916, "title_abs_embedding_index": 1941}, {"title": "Commit0: Library Generation from Scratch", "link_suffix": "/forum?id=MMwaQEVsAg", "link": "https://openreview.net/forum?id=MMwaQEVsAg", "pdf_link": "https://openreview.net/pdf?id=MMwaQEVsAg", "keywords": "code generation, language model, evaluation, feedback", "abstract": "With the goal of supporting research into AI that exceeds typical expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library's API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive execution and linting feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use.", "title_embedding_index": 1917, "title_abs_embedding_index": 1942}, {"title": "Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control", "link_suffix": "/forum?id=1Njl73JKjB", "link": "https://openreview.net/forum?id=1Njl73JKjB", "pdf_link": "https://openreview.net/pdf?id=1Njl73JKjB", "keywords": "mechanistic interpretability, sparse autoencoders, evaluations", "abstract": "Disentangling model activations into human-interpretable features is a central\nproblem in interpretability. Sparse autoencoders (SAEs) have recently attracted\nmuch attention as a scalable unsupervised approach to this problem. However, our\nimprecise understanding of ground-truth features in realistic scenarios makes it\ndifficult to measure the success of SAEs. To address this challenge, we propose\nto evaluate SAEs on specific tasks by comparing them to supervised\nfeature dictionaries computed with knowledge of the concepts relevant to the\ntask.Specifically, we suggest that it is possible to (1) compute supervised sparse\nfeature dictionaries that disentangle model computations for a specific task;\n(2) use them to evaluate and contextualize the degree of disentanglement and\ncontrol offered by SAE latents on this task. Importantly, we can do this in a\nway that is agnostic to whether the SAEs have learned the exact ground-truth\nfeatures or a different but similarly useful representation.As a case study, we apply this framework to the indirect object identification\n(IOI) task using GPT-2 Small, with SAEs trained on either the IOI or OpenWebText\ndatasets. We find that SAEs capture interpretable features for the IOI task, and\nthat more recent SAE variants such as Gated SAEs and Top-K SAEs are competitive\nwith supervised features in terms of disentanglement and control over the model.\nWe also exhibit, through this setup and toy models, some qualitative phenomena\nin SAE training illustrating feature splitting and the role of feature\nmagnitudes in solutions preferred by SAEs.", "title_embedding_index": 1918, "title_abs_embedding_index": 1943}, {"title": "CIRCUIT: A Benchmark for Circuit Interpretation and Reasoning Capabilities of LLMs", "link_suffix": "/forum?id=5iUUorHeM3", "link": "https://openreview.net/forum?id=5iUUorHeM3", "pdf_link": "https://openreview.net/pdf?id=5iUUorHeM3", "keywords": "Large Language Models (LLMs), benchmarking, analog circuits, dataset creation, evaluation metrics", "abstract": "The role of Large Language Models (LLMs) has not been extensively explored in analog circuit design, which could benefit from a reasoning-based approach that transcends traditional optimization techniques. In particular, despite their growing relevance, there are no benchmarks to assess LLMs\u2019 reasoning capability about circuits. Therefore, we created the CIRCUIT dataset consisting of 510 question-answer pairs spanning various levels of analog-circuit-related subjects. The best-performing model on our dataset, GPT-4o, achieves 48.04% accuracy when evaluated on the final numerical answer. To evaluate the robustness of LLMs on our dataset, we introduced a unique feature that enables unit-test-like evaluation by grouping questions into unit tests. In this case, GPT-4o can only pass 27.45% of the unit tests, highlighting that the most advanced LLMs still struggle with understanding circuits, which requires multi-level reasoning, particularly when involving circuit topologies. This circuit-specific benchmark highlights LLMs' limitations, offering valuable insights for advancing their application in analog integrated circuit design.", "title_embedding_index": 1919, "title_abs_embedding_index": 1944}, {"title": "CL-MFAP: A contrastive learning-based multimodal foundation model for antibiotic property prediction", "link_suffix": "/forum?id=fv9XU7CyN2", "link": "https://openreview.net/forum?id=fv9XU7CyN2", "pdf_link": "https://openreview.net/pdf?id=fv9XU7CyN2", "keywords": "Contrastive Learning, Multimodal Foundation Model, Antibiotic Property Prediction, Bi-level Routing Attention, Transformer", "abstract": "Due to the rise in antimicrobial resistance, identifying novel compounds with antibiotic potential is crucial for combatting this global health issue. However, traditional drug development methods are costly and inefficient. Recognizing the pressing need for more effective solutions, researchers have turned to machine learning techniques to streamline the prediction and development of novel antibiotic compounds. While foundation models have shown promise in antibiotic discovery, current mainstream efforts still fall short of fully leveraging the potential of multimodal molecular data. Recent studies suggest that contrastive learning frameworks utilizing multimodal data exhibit excellent performance in representation learning across various domains. Building upon this, we introduce CL-MFAP, an unsupervised contrastive learning (CL)-based multimodal foundation (MF) model specifically tailored for discovering small molecules with potential antibiotic properties (AP) using three types of molecular data. This model employs 1.6 million bioactive molecules with drug-like properties from the ChEMBL dataset to jointly pretrain three encoders: (1) a transformer-based encoder with rotary position embedding for processing SMILES strings; (2) another transformer-based encoder, incorporating a novel bi-level routing attention mechanism to handle molecular graph representations; and (3) a fingerprint encoder using a multilayer perceptron, to achieve the contrastive learning purpose. The CL-MFAP outperforms baseline models in antibiotic property prediction by effectively utilizing different molecular modalities and demonstrates superior domain-specific performance when fine-tuned for antibiotic-related property prediction tasks.", "title_embedding_index": 1920, "title_abs_embedding_index": 1945}, {"title": "Disentangled Representation Learning for Parametric Partial Differential Equations", "link_suffix": "/forum?id=LXVZQpEb2y", "link": "https://openreview.net/forum?id=LXVZQpEb2y", "pdf_link": "https://openreview.net/pdf?id=LXVZQpEb2y", "keywords": "Neural Operators; Hidden Physics Discovery; Inverse Problems", "abstract": "Neural operators (NOs) have demonstrated  remarkable success in learning mappings between function spaces, serving as efficient approximators for the forward solutions of complex physical systems governed by partial differential equations (PDEs). However, while  effective as black-box solvers, they offer limited insight into the underlying physical mechanism, due to the lack of interpretable representations of the physical parameters that drive the system. To tackle this challenge, we propose a new paradigm for learning disentangled representations from neural operator parameters, thereby effectively solving an inverse problem. Specifically, we introduce DisentangO, a novel hyper-neural operator architecture designed to unveil and disentangle the latent physical factors of variation embedded within the black-box neural operator parameters. At the core of DisentangO is a multi-task neural operator architecture that distills the varying parameters of the governing PDE through a task-wise adaptive layer, coupled with a hierarchical variational autoencoder that disentangles these variations into identifiable latent factors. By learning these disentangled representations, our model not only enhances physical interpretability but also enables more robust generalization across diverse physical systems. Empirical evaluations across supervised, semi-supervised, and unsupervised learning contexts show that DisentangO effectively extracts meaningful and interpretable latent features, bridging the gap between predictive performance and physical understanding in neural operator frameworks.", "title_embedding_index": 1921, "title_abs_embedding_index": 1946}, {"title": "SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators", "link_suffix": "/forum?id=u3TL0qxLWf", "link": "https://openreview.net/forum?id=u3TL0qxLWf", "pdf_link": "https://openreview.net/pdf?id=u3TL0qxLWf", "keywords": "Model Compression, Large Language Models, Post-Training Quantization", "abstract": "Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of a pseudo-random generator to encode and compress model weights. Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama3 70B, which is particularly challenging, show zero-shot accuracy retention at 4- and 3-bit compression to be on par with or better than state-of-the-art methods, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.", "title_embedding_index": 1922, "title_abs_embedding_index": 1947}, {"title": "Dissecting Adversarial Robustness of Multimodal LM Agents", "link_suffix": "/forum?id=YauQYh2k1g", "link": "https://openreview.net/forum?id=YauQYh2k1g", "pdf_link": "https://openreview.net/pdf?id=YauQYh2k1g", "keywords": "LM agents, multimodal agents, safety, adversarial robustness", "abstract": "As language models (LMs) are used to build autonomous agents in real environments, ensuring their adversarial robustness becomes a critical challenge. Unlike chatbots, agents are compound systems with multiple components, which existing LM safety evaluations do not adequately address. To bridge this gap, we manually create 200 targeted adversarial tasks and evaluation functions in a realistic threat model on top of VisualWebArena, a real environment for web-based agents. In order to systematically examine the robustness of various multimodal we agents, we propose the Agent Robustness Evaluation (ARE) framework. ARE views the agent as a graph showing the flow of intermediate outputs between components and decomposes robustness as the flow of adversarial information on the graph. First, we find that we can successfully break a range of the latest agents that use black-box frontier LLMs, including those that perform reflection and tree-search. With imperceptible perturbations to a single product image (less than 5% of total web page pixels), an attacker can hijack these agents to execute targeted adversarial goals with success rates up to 67%. We also use ARE to rigorously evaluate how the robustness changes as new components are added. For example, an evaluator and value function, if kept uncompromised, can decrease ASR relatively by 22% and 17%, but if left vulnerable to attack, can increase ASR relatively by 15% and 20%. Our data and code for attacks, defenses, and evaluation are available at url_removed_for_review.", "title_embedding_index": 1923, "title_abs_embedding_index": 1948}, {"title": "Weighted Fair Regression under Selection Bias", "link_suffix": "/forum?id=CeIOWuD8oZ", "link": "https://openreview.net/forum?id=CeIOWuD8oZ", "pdf_link": "https://openreview.net/pdf?id=CeIOWuD8oZ", "keywords": "Fair Regression, Selection Bias", "abstract": "Selection bias is a prevalent challenge in real-world data analysis, often stemming from biased historical censoring policies. While there is a growing body of literature on fairness in mitigating accuracy disparities, few studies have considered the potential impact of selection bias in training data. Depending on the selection mechanism, significant differences can arise between the population distribution and the training data distribution. Therefore, the training fairness metric can be heavily biased, leading to unfair learning. To address this issue under the fair regression problem, we propose weighting adjustments in the fairness constraint, which results in a novel fair regression estimator. Despite non-convexity, we derive an efficient algorithm to obtain a globally optimal solution. This work pioneers the integration of weighting adjustments into the fair regression problem, introducing a novel methodology to constrain accuracy disparities under arbitrary thresholds.", "title_embedding_index": 1924, "title_abs_embedding_index": 1949}]