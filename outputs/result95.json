[{"title": "Robust Locally Differentially Private Graph Analysis", "link_suffix": "/forum?id=BpDa4YTKtO", "link": "https://openreview.net/forum?id=BpDa4YTKtO", "pdf_link": "https://openreview.net/pdf?id=BpDa4YTKtO", "keywords": "Data poisoning, Local differential privacy, graphs.", "abstract": "Locally differentially private (LDP) graph analysis allows private analysis on a graph that is distributed across multiple users. However, such computations are vulnerable to poisoning attacks where an adversary can skew the results by submitting malformed data. In this paper, we formally study the impact of poisoning attacks for graph degree estimation protocols under LDP. We make two key technical contributions. First, we observe LDP makes a protocol more vulnerable to poisoning \u2013 the impact of poisoning is worse when the adversary can directly poison their (noisy) responses, rather than their input data. Second, we observe that graph data is naturally redundant \u2013 every edge is shared between two users. Leveraging this data redundancy, we design robust degree estimation protocols under LDP that can significantly reduce the impact of poisoning and compute degree estimates with high accuracy. We prove that our robust protocols achieve the optimal levels of accuracy and soundness via information-theoretic lower bounds. Finally, we evaluate our proposed robust degree estimation protocols under poisoning attacks on real-world datasets to demonstrate their efficacy in practice.", "title_embedding_index": 4700, "title_abs_embedding_index": 4725}, {"title": "Random Graph Asymptotics for Treatment Effect Estimation in Two-Sided Markets", "link_suffix": "/forum?id=Ivk2j3uRYh", "link": "https://openreview.net/forum?id=Ivk2j3uRYh", "pdf_link": "https://openreview.net/pdf?id=Ivk2j3uRYh", "keywords": "Two-sided markets, Random graph models, Network interference", "abstract": "In two-sided markets, the accurate estimation of treatment effects is crucial yet challenging due to the inherent interference between market participants, which violates the Stable Unit Treatment Value Assumption (SUTVA). This paper introduces a novel framework that leverages random graph asymptotics to model and estimate treatment effects under network interference in two-sided markets. By extending the application of exposure graph models and proposing a new estimation process, we derive estimators with robust asymptotic properties, suitable for large-scale market scenarios. Our theoretical findings are supported by extensive numerical simulations, demonstrating the effectiveness and practical applicability of our approach in estimating direct and indirect causal effects within these complex market structures.", "title_embedding_index": 4701, "title_abs_embedding_index": 4726}, {"title": "Probabilistic Geometric Principal Component Analysis", "link_suffix": "/forum?id=mkDam1xIzW", "link": "https://openreview.net/forum?id=mkDam1xIzW", "pdf_link": "https://openreview.net/pdf?id=mkDam1xIzW", "keywords": "geometry, nonlinear manifold, factor analysis, dimensionality reduction, neural population activity", "abstract": "Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system around the mean of data. However, in many applications, data may be distributed around a nonlinear manifold rather than lying in the Euclidean space around the mean. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data such as neural activity that exhibit noise and are distributed around a nonlinear manifold.", "title_embedding_index": 4702, "title_abs_embedding_index": 4727}, {"title": "Gaussian Splatting Lucas-Kanade", "link_suffix": "/forum?id=dkrEoT68by", "link": "https://openreview.net/forum?id=dkrEoT68by", "pdf_link": "https://openreview.net/pdf?id=dkrEoT68by", "keywords": "Gaussian Splatting, regularization, novel view synthesis", "abstract": "Gaussian Splatting and its dynamic extensions are effective for reconstructing 3D scenes from 2D images when there is significant camera movement to facilitate motion parallax and when scene objects remain relatively static. However, in many real-world scenarios, these conditions are not met. As a consequence, data-driven semantic and geometric priors have been favored as regularizers, despite their bias toward training data and their neglect of broader movement dynamics.Departing from this practice, we propose a novel analytical approach that adapts the classical Lucas-Kanade method to dynamic Gaussian splatting. By leveraging the intrinsic properties of the forward warp field network, we derive an analytical velocity field that, through time integration, facilitates accurate scene flow computation. This enables the precise enforcement of motion constraints on warp fields, thus constraining both 2D motion and 3D positions of the Gaussians. Our method excels in reconstructing highly dynamic scenes with minimal camera movement, as demonstrated through experiments on both synthetic and real-world scenes.", "title_embedding_index": 4703, "title_abs_embedding_index": 4728}, {"title": "End-to-End Reinforcement Learning for Traffic Signal Control: Real-Time Video to Signal Decisions", "link_suffix": "/forum?id=eM5dar35Ys", "link": "https://openreview.net/forum?id=eM5dar35Ys", "pdf_link": "https://openreview.net/pdf?id=eM5dar35Ys", "keywords": "Traffic signal control, Reinforcement learning", "abstract": "Efficient traffic management at urban intersections is vital for reducing congestion\nand improving safety. This paper presents MD3DQN, the first End-to-End novel\nreinforcement learning model using surveillance video for real-time traffic signal\ncontrol. The model features two main components: an image reception module,\ncapturing traffic data from cameras positioned on signal poles, and a multi-agent\ndecision module, where each agent manages a traffic phase. These components\nare connected via a bridge module for seamless integration.Our novel Entropy Attention Mechanism enhances the multi-decision turn-based\ntraffic signal control by leveraging uncertainty and signal phase delays, leading\nto more optimized decisions. Results show MD3DQN improved cumulative reward\nby an average of 85.2% over Fixed-time 40 and 54.4% over DQN-VTP.\nThe entropy mechanism contributed to a 41.8% improvement upon ablation study,\ndemonstrating its impact on faster convergence and better performance.", "title_embedding_index": 4704, "title_abs_embedding_index": 4729}, {"title": "Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models", "link_suffix": "/forum?id=OlRjxSuSwl", "link": "https://openreview.net/forum?id=OlRjxSuSwl", "pdf_link": "https://openreview.net/pdf?id=OlRjxSuSwl", "keywords": "LLM, Instruction-Following, Retrieval Model, Benchmark", "abstract": "Large language models (LLMs) have been widely adopted for training embedding and ranking models, with recent advancements significantly improving the performance of Information Retrieval systems. However, while instruction-following is a core capability of LLMs, their ability to handle detailed user instructions has not been thoroughly investigated in search models. This study evaluates the instruction-following capabilities of various retrieval models, including LLM-based dense retrieval and reranking models. We develop a specialized benchmark InFoSearch spanning six dimensions: Audience, Keyword, Format, Language, Length, and Source, and introduce novel metrics to assess the models' responsiveness to instructions. Our findings show that even fine-tuned retrieval models struggle with instruction-following, highlighting a key limitation in current systems and providing valuable insights for improving their instruction-aware capabilities in future research.", "title_embedding_index": 4705, "title_abs_embedding_index": 4730}, {"title": "RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction", "link_suffix": "/forum?id=L5godAOC2z", "link": "https://openreview.net/forum?id=L5godAOC2z", "pdf_link": "https://openreview.net/pdf?id=L5godAOC2z", "keywords": "Jailbreak Attack, Large Language Model, KV cache optimization", "abstract": "Jailbreak attacks circumvent LLMs' built-in safeguards by concealing harmful queries within adversarial prompts. While most existing defenses attempt to mitigate the effects of adversarial prompts, they often prove inadequate as adversarial prompts can take arbitrary, adaptive forms. This paper introduces RobustKV, a novel jailbreak defense that takes a fundamentally different approach by selectively removing critical tokens of harmful queries from key-value (KV) caches. Intuitively, for an adversarial prompt to be effective, its tokens must achieve sufficient `importance' (measured by attention scores), which consequently lowers the importance of tokens in the concealed harmful query. Therefore, by carefully evicting the KVs of low-ranked tokens, RobustKV minimizes the harmful query's presence in the KV cache, thus preventing the LLM from generating informative responses. Extensive evaluation using benchmark datasets and models demonstrates that RobustKV effectively counters state-of-the-art jailbreak attacks while maintaining the LLM's performance on benign queries. Notably, RobustKV creates an interesting effectiveness-evasiveness dilemma for the adversary, leading to its robustness against adaptive attacks.{(Warning: This paper contains potentially harmful content generated by LLMs.)}", "title_embedding_index": 4706, "title_abs_embedding_index": 4731}, {"title": "From Probability to Counterfactuals: the Increasing Complexity of Satisfiability in Pearl's Causal Hierarchy", "link_suffix": "/forum?id=rvvSSmGIFS", "link": "https://openreview.net/forum?id=rvvSSmGIFS", "pdf_link": "https://openreview.net/pdf?id=rvvSSmGIFS", "keywords": "complexity, causal reasoning, Pearl's Causal Hierarchy", "abstract": "The framework of Pearl's Causal Hierarchy (PCH) formalizes three types of reasoning: probabilistic (i.e. purely observational), interventional, and counterfactual, that reflect the progressive sophistication of human thought regarding causation. We investigate the computational complexity aspects of reasoning in this framework focusing mainly on satisfiability problems expressed in probabilistic and causal languages across the PCH. That is, given a system of formulas in the standard probabilistic and causal languages, does there exist a model satisfying the formulas?Our main contribution is to prove the exact computational complexities showing that languages allowing addition and marginalization (via the summation operator) yield NP^{PP}-, PSPACE-, and NEXP-complete satisfiability problems, depending on the level of the PCH. These are the first results to demonstrate a strictly increasing complexity across the PCH: from probabilistic to causal and counterfactual reasoning. On the other hand, in the case of full languages, i.e.~allowing addition, marginalization, and multiplication, we show that the satisfiability for the counterfactual level remains the same as for the probabilistic and causal levels, solving an open problem in the field.", "title_embedding_index": 4707, "title_abs_embedding_index": 4732}, {"title": "Learning Symmetries through Loss Landscape", "link_suffix": "/forum?id=0aaaM31hLB", "link": "https://openreview.net/forum?id=0aaaM31hLB", "pdf_link": "https://openreview.net/pdf?id=0aaaM31hLB", "keywords": "Unconstrained models, equivariant models, symmetries.", "abstract": "Incorporating equivariance as an inductive bias into deep learning architectures, to take advantage of the data symmetry, has been successful in multiple applications such as chemistry and dynamical systems. The build of equivariance architecture, particularly w.r.t. roto-translations, is crucial for effectively modeling geometric graphs and molecules, where the understanding of 3D structures enhances generalization. However, despite their potential, equivariant models often pose challenges due to their high computational complexity. In this paper, we study the capabilities of unconstrained models (which do not build equivariance into the architecture) and how they generalize compared to equivariant models. We show that unconstrained models can learn approximate symmetries by minimizing additional simple equivariance loss. By formulating equivariance as a new learning objective, we can control the level of approximate equivariance in the model. Our method achieves competitive performance compared to equivariant baselines while being 10x faster at inference and 2.5x at training.", "title_embedding_index": 4708, "title_abs_embedding_index": 4733}, {"title": "RB-Modulation: Training-Free Personalization using Stochastic Optimal Control", "link_suffix": "/forum?id=bnINPG5A32", "link": "https://openreview.net/forum?id=bnINPG5A32", "pdf_link": "https://openreview.net/pdf?id=bnINPG5A32", "keywords": "Inverse Problems, Generative Modeling, Diffusion Models, Posterior Sampling, Optimal Control", "abstract": "We propose Reference-Based Modulation (RB-Modulation), a new plug-and-play solution for training-free personalization of diffusion models.\nExisting training-free approaches exhibit difficulties in (a) style extraction from reference images in the absence of additional style or content text descriptions, (b) unwanted content leakage from reference style images, and (c) effective composition of style and content. \nRB-Modulation is built on a novel stochastic optimal controller where a style descriptor encodes the desired attributes through a terminal cost. \nThe resulting drift not only overcomes the difficulties above, but also ensures high fidelity to the reference style and adheres to the given text prompt. \nWe also introduce a cross-attention-based feature aggregation scheme that allows RB-Modulation to decouple content and style from the reference image.\nWith theoretical justification and empirical evidence, our framework demonstrates precise extraction and control ofcontentandstylein a training-free manner. \nAdditionally, our method allows a seamless composition of content and style, which marks a departure from the dependency on external adapters or ControlNets", "title_embedding_index": 4709, "title_abs_embedding_index": 4734}, {"title": "FMS PINN: Flow-matching sampling for efficient solution of partial differential equations with source singularities", "link_suffix": "/forum?id=EIXZXPz7jU", "link": "https://openreview.net/forum?id=EIXZXPz7jU", "pdf_link": "https://openreview.net/pdf?id=EIXZXPz7jU", "keywords": "physics informed neural networks, Adaptive sampling", "abstract": "Singularities in the source functions of partial differential equations (PDEs) can pose significant challenges for physics-informed neural networks (PINNs), often leading to numerical instability and necessitating a large number of sampling points thereby increasing the computational time. In this paper, we introduce a novel sampling point selection method to address these challenges. Our approach is based on diffusion models capable of generative sampling from the distribution of PDE residuals. Specifically, we apply the optimal transport coupling flow-matching technique to generate more sampling points in regions where the PDE residuals are higher, enhancing the accuracy and efficiency of the solution. In contrast to existing approaches in the literature, our method avoids explicit modeling of the probability density proportional to residuals, instead using the benefits of flow matching to generate novel and probable samples from more complex distributions, thereby enhancing PINN solutions for problems with singularities.\nWe demonstrate that this method, in certain scenarios, outperforms existing techniques such as normalizing flow-based sampling PINN. Especially, our approach demonstrates effectiveness in improving the solution quality for the linear elasticity equation in the case of material with complex geometry of inclusion. A detailed comparison of the flow matching sampling method with other approaches is also provided.", "title_embedding_index": 4710, "title_abs_embedding_index": 4735}, {"title": "CocoRNA: Collective RNA Design with Cooperative Multi-agent Reinforcement Learning", "link_suffix": "/forum?id=4JZ56UVJYf", "link": "https://openreview.net/forum?id=4JZ56UVJYf", "pdf_link": "https://openreview.net/pdf?id=4JZ56UVJYf", "keywords": "Multi-agent, reinforcement learning, RNA design", "abstract": "Ribonucleic acid (RNA) plays a crucial role in various biological functions, and designing sequences that reliably fold into specified structures remains a significant challenge in computational biology. Existing methods often struggle with efficiency and scalability, as they require extensive search or optimization to tackle this complex combinatorial problem. In this paper, we propose CocoRNA, a collective RNA design method using cooperative multi-agent reinforcement learning, for the RNA secondary structure design problem. CocoRNA decomposes the RNA design task into multiple sub-tasks, which are assigned to multiple agents to solve collaboratively, alleviating the challenges of the curse of dimensionality as well as the issues of sparse and delayed rewards. By employing a centralized Critic network and leveraging global information during training, we promote cooperation among agents, enabling the distributed policies to converge toward the global optimum and resulting in a high-quality collective RNA design policy. The trained model is capable of completing RNA secondary structure design with less time and fewer steps, without requiring further training or search on new tasks. We evaluate CocoRNA on the Rfam dataset and the Eterna100 benchmark. Experimental results demonstrate that CocoRNA outperforms existing algorithms in terms of design time and success rate, highlighting its practicality and effectiveness.", "title_embedding_index": 4711, "title_abs_embedding_index": 4736}, {"title": "High-Dimensional Bayesian Optimisation with Gaussian Process Prior Variational Autoencoders", "link_suffix": "/forum?id=SIuD7CySb4", "link": "https://openreview.net/forum?id=SIuD7CySb4", "pdf_link": "https://openreview.net/pdf?id=SIuD7CySb4", "keywords": "Variational autoencoders, Gaussian processes, Bayesian optimisation", "abstract": "Bayesian optimisation (BO) using a Gaussian process (GP)-based surrogate model is a powerful tool for solving black-box optimisation problems but does not scale well to high-dimensional data. Previous works have proposed to use variational autoencoders (VAEs) to project high-dimensional data onto a low-dimensional latent space and to implement BO in the inferred latent space. In this work, we propose a conditional generative model for efficient high-dimensional BO that uses a GP surrogate model together with GP prior VAEs. A GP prior VAE extends the standard VAE by conditioning the generative and inference model on auxiliary covariates, capturing complex correlations across samples with a GP. Our model incorporates the observed target quantity values as auxiliary covariates learning a structured latent space that is better suited for the GP-based BO surrogate model. It handles partially observed auxiliary covariates using a unifying probabilistic framework and can also incorporate additional auxiliary covariates that may be available in real-world applications. We demonstrate that our method improves upon existing latent space BO methods on simulated datasets as well as on commonly used benchmarks.", "title_embedding_index": 4712, "title_abs_embedding_index": 4737}, {"title": "Dimension Debate: Is 3D a Step Too Far for Optimizing Molecules?", "link_suffix": "/forum?id=AnPEfzBstD", "link": "https://openreview.net/forum?id=AnPEfzBstD", "pdf_link": "https://openreview.net/pdf?id=AnPEfzBstD", "keywords": "Bayesian optimization, molecular representation, surrogate models, transfer learning", "abstract": "The discovery of new molecular materials with desirable properties is essential for technological advancements, from pharmaceuticals to renewable energy. However, the discovery process is arduous, requiring many trial-and-error cycles of complex and expensive experiments. Bayesian optimization (BO) is commonly used to find and screen candidate molecules efficiently.  However, it is unclear how to choose the right molecular representations for a Bayesian surrogate model: While molecules are 3-dimensional in nature, 3D features in BO have largely been underexplored. Indeed, 1D and 2D molecular features---which incur loss of information---are typically used.  In this work, we study this discrepancy: Why have 3D features been overlooked for BO in materials discovery? To this end, we evaluate 3D features against standard lower-dimensional features.  We assess their optimization performance on real-world chemistry datasets, considering both various settings such as low- & high-data regimes and transfer learning, and different types of Bayesian surrogates. This amounts to the evaluation of 35 different setups per dataset, totaling over 2100 distinct runs. Our large-scale work provides insights and modeling guides to chemists and practitioners on the trade-offs between 1D, 2D, and 3D representations, in a bid to further accelerate materials discovery.", "title_embedding_index": 4713, "title_abs_embedding_index": 4738}, {"title": "DeFine: Enhancing LLM Decision-Making with Factor Profiles and Analogical Reasoning", "link_suffix": "/forum?id=CblmtAxrRg", "link": "https://openreview.net/forum?id=CblmtAxrRg", "pdf_link": "https://openreview.net/pdf?id=CblmtAxrRg", "keywords": "analogical reasoning, factor profiles, LLM decision-making, spoken transcripts, long-context LLMs", "abstract": "LLMs are ideal for decision-making due to their ability to reason over long contexts and identify critical factors. However, challenges arise when processing transcripts of spoken speech describing complex scenarios. These transcripts often contain ungrammatical or incomplete sentences, repetitions, hedging, and vagueness. For example, during a company's earnings call, an executive might project a positive revenue outlook to reassure investors, despite significant uncertainty regarding future earnings. It is crucial for LLMs to incorporate this uncertainty systematically when making decisions. In this paper, we introduce DeFine, a new framework that constructs probabilistic factor profiles from complex scenarios. DeFine then integrates these profiles with analogical reasoning, leveraging insights from similar past experiences to guide LLMs in making critical decisions in novel situations. Our framework separates the tasks of quantifying uncertainty in complex scenarios and incorporating it into LLM decision-making. This approach is particularly useful in fields such as medical consultations, negotiations, and political debates, where making decisions under uncertainty is vital.", "title_embedding_index": 4714, "title_abs_embedding_index": 4739}, {"title": "Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh", "link_suffix": "/forum?id=gY08Ou8EL7", "link": "https://openreview.net/forum?id=gY08Ou8EL7", "pdf_link": "https://openreview.net/pdf?id=gY08Ou8EL7", "keywords": "Generalizable human rendering, error feedback, dual representation", "abstract": "Generalizable reconstruction of an animatable human avatar from sparse inputs and corresponding high-quality rendering conditioned on a given pose faces two main challenges: First, generalizable methods, which are needed for fast reconstruction, avoid scene-specific optimization but instead rely on data priors and inductive biases extracted from training on large data. However, at reconstruction time, information is limited as only a small number of sparse inputs are available. Note, we operate on a small set of images showing a human in possibly different but not multi-view consistent poses. Second, rendering is preferably computationally efficient yet of high resolution.\nTo address both challenges we augment the recently proposed dual shape representation, which combines the benefits of a mesh and Gaussian points, in two ways. To improve reconstruction, we propose an iterative feedback update framework, which successively improves the canonical human shape representation during reconstruction. To achieve computationally efficient yet high-resolution rendering, we study a coupled-multi-resolution Gaussians-on-Mesh representation. We evaluate the proposed approach on the challenging THuman2.0 and AIST++ data. Our approach reconstructs an animatable  representation from sparse inputs in less than 1s, renders views with 95.1FPS at $1024 \\times 1024$, and achieves  PSNR/LPIPS*/FID of 24.59/111.26/51.42 on THuman2.0, outperforming the state-of-the-art in rendering quality.", "title_embedding_index": 4715, "title_abs_embedding_index": 4740}, {"title": "Qualifying Knowledge and Knowledge Sharing in Multilingual Models", "link_suffix": "/forum?id=cif0JVXJ3b", "link": "https://openreview.net/forum?id=cif0JVXJ3b", "pdf_link": "https://openreview.net/pdf?id=cif0JVXJ3b", "keywords": "Knowledge Retrieval, Pretrained Language Model, LLM Interpretability, Multilingual Models, Cross-linguistic Dataset", "abstract": "Pre-trained language models (PLMs) have demonstrated a remarkable ability to encode factual knowledge. However, the mechanisms underlying how this knowledge is stored and retrieved remain poorly understood, with important implications for AI interpretability and safety. In this paper, we disentangle the multifaceted nature of knowledge: successfully completing a knowledge retrieval task (e.g., \u201cThe capital of France is __\u201d) involves mastering underlying concepts (e.g. France, Paris), relationships between these concepts (e.g. capital of), the structure of prompts, including the language of the query. We propose to disentangle these distinct aspects of knowledge and apply this typology to offer a critical view of neuron-level knowledge attribution techniques. For concreteness, we focus on Dai et al.'s (2022) Knowledge Neurons (KNs) across multiple PLMs, testing 10 natural languages and unnatural languages (e.g. Autoprompt).\nOur key contributions are twofold: (i) we show that KNs come in different flavors, some indeed encoding entity level concepts, some having a much less transparent, more polysemantic role , and (ii) we uncover an unprecedented overlap in KNs across up to all of the 10 languages we tested, pointing to the existence of a partially unified, language-agnostic retrieval system. To do so, we introduce and release the mParaRel dataset, an extension of ParaRel, featuring prompts and paraphrases for cloze-style knowledge retrieval tasks in parallel over 10 languages.", "title_embedding_index": 4716, "title_abs_embedding_index": 4741}, {"title": "ULoRA: Universal Low-Rank Adaptation of Diverse Deep Learning Architectures", "link_suffix": "/forum?id=bYsieh8LE2", "link": "https://openreview.net/forum?id=bYsieh8LE2", "pdf_link": "https://openreview.net/pdf?id=bYsieh8LE2", "keywords": "LoRA, Universal Adaptation, Deep Learning, Parameter Fine Tuning, Transformer, Language Models", "abstract": "To train Large Language Models (LLMs) having a large number of parameters, the Parameter-Efficient Fine Tuning (PEFT) method based on LoRA, which allows fine-tuning with fewer parameters, is widely employed. However, these methods are primarily designed for application to Transformer architectures, which presents challenges when attempting to apply them to models such as Mamba. To address this limitation, this work proposes Universal LoRA (ULoRA), which applies a Low-Rank Adapter to all deep learning models at the level of universally common blocks. ULoRA achieves generalizability by applying Low-Rank Adapters to blocks, making it applicable to models that do not utilize Transformer architectures. Furthermore, by grouping multiple blocks and applying a single Low-Rank Adapter, ULoRA provides structural flexibility that allows a further reduction in the number of parameters. This significantly reduces resource usage and inference time, making it well-suited for on-device environments with limited resources, while only incurring a slight performance loss. Additionally, if all blocks are grouped to use a single Low-Rank Adapter, task switching during inference is enabled by computing only the adapter. Experimental results show that, for LLaMA-3-8B, ULoRA achieves comparable performance to LoRA with only about 60% of the parameters, while delivering up to 8% higher throughput. For Mamba-2.8B, ULoRA outperforms LoRA with only about 20% of the parameters. In scenarios with limited available resources, ULoRA can be applied using just 4% of the parameters of LoRA, with only a 10% reduction in performance.", "title_embedding_index": 4717, "title_abs_embedding_index": 4742}, {"title": "Real-World Data and Calibrated Simulation Suite for Offline Training of Reinforcement Learning Agents to Optimize Energy and Emission in Buildings for Environmental Sustainability", "link_suffix": "/forum?id=uuCgIHqxpr", "link": "https://openreview.net/forum?id=uuCgIHqxpr", "pdf_link": "https://openreview.net/pdf?id=uuCgIHqxpr", "keywords": "Reinforcement Learning, HVAC Control, Simulator, RL Environment, Environmental Sustainability, Climate, Time-series prediction", "abstract": "Commercial office buildings contribute 17 percent of Carbon Emissions in the US, according to the US Energy Information Administration (EIA), and improving their efficiency will reduce their environmental burden and operating cost. A major contributor of energy consumption in these buildings are the Heating, Ventilation, and Air Conditioning (HVAC) devices. HVAC devices form a complex and interconnected thermodynamic system with the building and outside weather conditions, and current setpoint control policies are not fully optimized for minimizing energy use and carbon emission. Given a suitable training environment, a Reinforcement Learning (RL) agent is able to improve upon these policies, but training such a model, especially in a way that scales to thousands of buildings, presents many practical challenges. Most existing work on applying RL to this important task either makes use of proprietary data, or focuses on expensive and proprietary simulations that may not be grounded in the real world. We present the Smart Buildings Control Suite, the first open source interactive HVAC control dataset extracted from live sensor measurements of devices in real office buildings. The dataset consists of two components: six years of real-world historical data from three buildings, for offline RL, and a lightweight interactive simulator for each of these buildings, calibrated using the historical data, for online and model-based RL. For ease of use, our RL environments are all compatible with the OpenAI gym environment standard. We also demonstrate a novel method of calibrating the simulator, as well as baseline results on training an RL agent on the simulator, predicting real-world data, and training an RL agent directly from data. We believe this benchmark will accelerate progress and collaboration on building optimization and environmental sustainability research.", "title_embedding_index": 4718, "title_abs_embedding_index": 4743}, {"title": "DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback", "link_suffix": "/forum?id=00SnKBGTsz", "link": "https://openreview.net/forum?id=00SnKBGTsz", "pdf_link": "https://openreview.net/pdf?id=00SnKBGTsz", "keywords": "iterative data generation, llm agent, lifelong learning", "abstract": "The process of creating training data to teach models is currently driven by humans, who manually analyze model weaknesses and plan how to create data that improves a student model. Recent approaches using large language models (LLMs) as annotators reduce human annotation effort, but still require humans to interpret feedback from evaluations and control the LLM to produce data the student needs. Automating this labor-intensive process by creating autonomous data generation agents \u2013 or teachers \u2013 is desirable, but requires environments that can simulate the feedback-driven, iterative, closed loop of data creation. To enable rapid and scalable testing for such agents and their modules, we introduce DataEnvGym, a testbed of teacher environments for data generation agents. DataEnvGym frames data generation as a sequential decision-making task, involving an agent consisting of a data generation policy (which generates a plan for creating training data) and a data generation engine (which transforms the plan into data), inside an environment that provides feedback from a student. The agent\u2019s end goal is to improve student model performance. Students are iteratively trained and evaluated on generated data, with their feedback (in the form of errors or weak skills) being reported to the agent after each iteration. As a general-purpose testbed, DataEnvGym includes multiple instantiations of teacher environments across three levels of structure in the state representation and action space, with varying levels of scaffolding support. More structured environments are based on automatically-inferred skills and offer a higher degree of interpretability and control over the curriculum. We support developing and testing data generation agents in three diverse tasks covering both text and images (mathematics, programming, and visual question answering) and test multiple student models. We find that example agents in our teaching environments can iteratively improve students across diverse tasks and settings. Moreover, we show that environments can teach different skill levels and can be used to test variants of key modules, pointing to directions of future work in improving data generation agents, engines, and feedback mechanisms. We will publicly release our code and leaderboard.", "title_embedding_index": 4719, "title_abs_embedding_index": 4744}, {"title": "From Steering Vectors to Conceptors and Beyond: Compositional Affine Steering Mechanisms for LLMs", "link_suffix": "/forum?id=9wjGUN65tY", "link": "https://openreview.net/forum?id=9wjGUN65tY", "pdf_link": "https://openreview.net/pdf?id=9wjGUN65tY", "keywords": "activation engineering, mechanistic interventions, model steering, large language models, activation addition, function vectors", "abstract": "Controlling and understanding the representations of large language models (LLMs) remain central challenges as they become more powerful. In this paper, we combine conceptor theory with recent advances in activation steering to develop a novel framework that generalizes both approaches for provably optimal affine steering. Conceptors characterize sets of neural network activations, representable as ellipsoids, and they act as soft projection matrices, enabling precise and flexible control over LLM activations while offering deeper insights into their internal representations. Our framework derives optimal affine steering functions from first principles, outperforming traditional additive steering methods across in-context learning tasks. Additionally, we use a Boolean algebra over conceptor matrices that allows for the composition of multiple steering objectives. Empirical results demonstrate that this approach surpasses existing methods for combining steering vectors. By uniting conceptor theory with activation steering, this work provides not only a more powerful tool for controlling LLM outputs, but also a principled approach for better understanding the internal mechanisms governing model representations and behavior.", "title_embedding_index": 4720, "title_abs_embedding_index": 4745}, {"title": "NNsight and NDIF: Democratizing Access to Foundation Model Internals", "link_suffix": "/forum?id=MxbEiFRf39", "link": "https://openreview.net/forum?id=MxbEiFRf39", "pdf_link": "https://openreview.net/pdf?id=MxbEiFRf39", "keywords": "interpretability, safety, large language models, distributed inference, scalable infrastructure, deferred execution, computation graphs, resource sharing", "abstract": "We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. NDIF is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the intervention graph, an architecture developed to decouple experiment design from model runtime. Together, this framework provides transparent and efficient access to the internals of foundation-size deep neural networks without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.Code and documentation will be made available open-source.", "title_embedding_index": 4721, "title_abs_embedding_index": 4746}, {"title": "Protecting against simultaneous data poisoning attacks", "link_suffix": "/forum?id=rK0YJwL69S", "link": "https://openreview.net/forum?id=rK0YJwL69S", "pdf_link": "https://openreview.net/pdf?id=rK0YJwL69S", "keywords": "backdoors, backdoor defenses, data poisoning", "abstract": "Current backdoor defense methods are evaluated against a single attack at a time. This is unrealistic, as powerful machine learning systems are trained on large datasets scraped from the internet, which may be attacked multiple times by one or more attackers. We demonstrate that multiple backdoors can be simultaneously installed in a single model through parallel data poisoning attacks without substantially degrading clean accuracy. Furthermore, we show that existing backdoor defense methods do not effectively defend against multiple simultaneous attacks. Finally, we leverage insights into the nature of backdoor attacks to develop a new defense, BaDLoss, that is effective in the multi-attack setting. With minimal clean accuracy degradation, BaDLoss attains an average attack success rate in the multi-attack setting of 7.98% in CIFAR-10, 10.29% in GTSRB, and 19.17% in Imagenette, compared to the average of other defenses at 63.44%, 74.83%, and 41.74% respectively.", "title_embedding_index": 4722, "title_abs_embedding_index": 4747}, {"title": "How Can LLM Guide RL? A Value-Based Approach", "link_suffix": "/forum?id=UNlyhyyuCs", "link": "https://openreview.net/forum?id=UNlyhyyuCs", "pdf_link": "https://openreview.net/pdf?id=UNlyhyyuCs", "keywords": "Large Language Model, Reinforcement Learning, Agent", "abstract": "Reinforcement learning (RL) has become the de facto standard practice for sequential decision-making problems by improving future acting policies with feedback. However, RL algorithms may require extensive trial-and-error interactions to collect useful feedback for improvement. On the other hand, recent developments in large language models (LLMs) have showcased impressive capabilities in language understanding and generation, yet they fall short in exploration and self-improvement capabilities for planning tasks, lacking the ability to autonomously refine their responses based on feedback. Therefore, in this paper, we study how the policy prior provided by the LLM can enhance the sample efficiency of RL algorithms. Specifically, we develop an algorithm named $\\mathtt{LINVIT}$ that incorporates LLM guidance as a regularization factor in value-based RL, leading to significant reductions in the amount of data needed for learning, particularly when the difference between the ideal policy and the LLM-informed policy is small, which suggests that the initial policy is close to optimal, reducing the need for further exploration. Additionally, we present a practical algorithm $\\mathtt{SLINVIT}$ that simplifies the construction of the value function and employs sub-goals to reduce the search complexity. Our experiments across three interactive environments---ALFWorld, InterCode, and BlocksWorld---demonstrate that the proposed method achieves state-of-the-art success rates and also surpasses previous RL and LLM approaches in terms of sample efficiency.", "title_embedding_index": 4723, "title_abs_embedding_index": 4748}, {"title": "Person Detection Through the Lens of Algorithmic Bias", "link_suffix": "/forum?id=tC1b9DBWww", "link": "https://openreview.net/forum?id=tC1b9DBWww", "pdf_link": "https://openreview.net/pdf?id=tC1b9DBWww", "keywords": "object detection, autonomous vehicles, algorithmic bias, algorithmic fairness, fairness in ML", "abstract": "The rise of AI based person detection in safety critical applications such as driver-less cars or security monitoring has lead to an explosion of machine learning models and dataset research. At the same time, researchers have raised question of bias in these models and datasets. Popular benchmark datasets like More Inclusive Images for People (MIAP) and Berkeley DeepDrive (BDD) for person detection suffer from both sampling and labeling biases. This has serious implications for autonomous vehicles and other fields that use these datasets. We conduct an all-encompassing analysis to assess these datasets through the lens of algorithmic bias, looking at both dataset and model bias. To the best of our knowledge, no study has delved into the realm of person detection in low-quality or crowded pictures with this lens. The result is a novel analysis of bias in a real-world image dataset. We find that 1) image manipulations frequently found in real-world settings like image blurriness and 2) image detectors that are skewed to rely on features like contrast or brightness both have significant negative impacts on fairness for race, gender, and age demographics. These result can help guide future designs of robust models in the object detection field and beyond.", "title_embedding_index": 4724, "title_abs_embedding_index": 4749}]