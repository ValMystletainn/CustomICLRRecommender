[
    {
        "title": "FunBO: Discovering Acquisition Functions forBayesian Optimization with FunSearch",
        "link_suffix": "/forum?id=OSmjkkF6Uy",
        "link": "https://openreview.net/forum?id=OSmjkkF6Uy",
        "pdf_link": "https://openreview.net/pdf?id=OSmjkkF6Uy",
        "keywords": "Bayesian optimization, LLM, acquisition function, meta-learning",
        "abstract": "The sample efficiency of Bayesian optimization algorithms depends on carefully crafted acquisition functions (AFs) guiding the sequential collection of function evaluations. The best-performing AF can vary significantly across optimization problems, often requiring ad-hoc and problem-specific choices. This work tackles the challenge of designing novel AFs that perform well across a variety of experimental settings. Based on FunSearch, a recent work using Large Language Models (LLMs) for discovery in mathematical sciences, we propose FunBO, an LLM-based method that can be used to learn new AFs written in computer code by leveraging access to a limited number of evaluations for a set of objective functions. We provide the analytic expression of all discovered AFs and evaluate them on various global optimization benchmarks and hyperparameter optimization tasks. We show how FunBO identifies AFs that generalize well in and out of the training distribution of functions, thus outperforming established general-purpose AFs and achieving competitive performance against AFs that are customized to specific function types and are learned via transfer-learning algorithms."
    },
    {
        "title": "Standardizing Structural Causal Models",
        "link_suffix": "/forum?id=aXuWowhIYt",
        "link": "https://openreview.net/forum?id=aXuWowhIYt",
        "pdf_link": "https://openreview.net/pdf?id=aXuWowhIYt",
        "keywords": "Causality, Causal Discovery, Structural Causal Model, Standardized Structural Causal Model, Variance Artifact, Covariance Artifact, Simulation, Benchmark",
        "abstract": "Synthetic datasets generated by structural causal models (SCMs) are commonly used for benchmarking causal structure learning algorithms. However, the variances and pairwise correlations in SCM data tend to increase along the causal ordering. Several popular algorithms exploit these artifacts, possibly leading to conclusions that do not generalize to real-world settings. Existing metrics like $\\operatorname{Var}$-sortability and $\\operatorname{R^2}$-sortability quantify these patterns, but they do not provide tools to remedy them. To address this, we propose internally-standardized structural causal models (iSCMs), a modification of SCMs that introduces a standardization operation at each variable during the generative process. By construction, iSCMs are not $\\operatorname{Var}$-sortable, and as we show experimentally, not $\\operatorname{R^2}$-sortable either for commonly-used graph families. Moreover, contrary to the post-hoc standardization of data generated by standard SCMs, we prove that linear iSCMs are less identifiable from prior knowledge on the weights and do not collapse to deterministic relationships in large systems, which may make iSCMs a useful model in causal inference beyond the benchmarking problem studied here."
    },
    {
        "title": "CAAP: Context-Aware Action Planning Prompting to Solve Computer Tasks with Front-End UI Only",
        "link_suffix": "/forum?id=n72WC4a9qQ",
        "link": "https://openreview.net/forum?id=n72WC4a9qQ",
        "pdf_link": "https://openreview.net/pdf?id=n72WC4a9qQ",
        "keywords": "agent automation, prompt engineering, large language model",
        "abstract": "Software robots have long been used in Robotic Process Automation (RPA) to automate mundane and repetitive computer tasks. With the advent of Large Language Models (LLMs) and their advanced reasoning capabilities, these agents are now able to handle more complex or previously unseen tasks. However, LLM-based automation techniques in recent literature frequently rely on HTML source code for input or application-specific API calls for actions, limiting their applicability to specific environments. We propose an LLM-based agent that mimics human behavior in solving computer tasks. It perceives its environment solely through screenshot images, which are then converted into text for an LLM to process. By leveraging the reasoning capability of the  LLM, we eliminate the need for large-scale human demonstration data typically required for model training. The agent only executes keyboard and mouse operations on Graphical User Interface (GUI), removing the need for pre-provided APIs to function. To further enhance the agent's performance in this setting, we propose a novel prompting strategy called Context-Aware Action Planning (CAAP) prompting, which enables the agent to thoroughly examine the task context from multiple perspectives. Our agent achieves an average success rate of 94.5% on MiniWoB++ and an average task score of 62.3 on WebShop, outperforming all previous studies of agents that rely solely on screen images. This method demonstrates potential for broader applications, particularly for tasks requiring coordination across multiple applications on desktops or smartphones, marking a significant advancement in the field of automation agents. Codes and models are accessible athttps://github.com/caap-agent/caap-agent."
    },
    {
        "title": "Sharpness-Aware Geometric Defense for Robust Out-Of-Distribution Detection",
        "link_suffix": "/forum?id=4BYzyGKIcb",
        "link": "https://openreview.net/forum?id=4BYzyGKIcb",
        "pdf_link": "https://openreview.net/pdf?id=4BYzyGKIcb",
        "keywords": "Robust out-of-distribution detection, Adversarial training, Sharpness-aware minimization",
        "abstract": "Out-of-distribution (OOD) detection ensures safe and reliable model deployment. Contemporary OOD algorithms using geometry projection can detect OOD or adversarial samples from clean in-distribution (ID) samples. However, this setting regards adversarial ID samples as OOD, leading to incorrect OOD predictions. Existing efforts on OOD detection with ID and OOD data under attacks are minimal. In this paper, we develop a robust OOD detection method that distinguishes adversarial ID samples from OOD ones. The sharp loss landscape created by adversarial training hinders model convergence, impacting the latent embedding quality for OOD score calculation. Therefore, we introduce aSharpness-aware Geometric Defense (SaGD)framework to smooth out the rugged adversarial loss landscape in the projected latent geometry. Enhanced geometric embedding convergence enables accurate ID data characterization, benefiting OOD detection against adversarial attacks. We use Jitter-based perturbation in adversarial training to extend the defense ability against unseen attacks. Our SaGD framework significantly improves FPR and AUC over the state-of-the-art defense approaches in differentiating CIFAR-100 from six other OOD datasets under various attacks. We further examine the effects of perturbations at various adversarial training levels, revealing the relationship between the sharp loss landscape and adversarial OOD detection. The implementation code will be released upon paper acceptance."
    },
    {
        "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance",
        "link_suffix": "/forum?id=tcq7n0m7Ml",
        "link": "https://openreview.net/forum?id=tcq7n0m7Ml",
        "pdf_link": "https://openreview.net/pdf?id=tcq7n0m7Ml",
        "keywords": "Large Language Model, Efficient Inference, KV Cache",
        "abstract": "As large language models (LLMs) continue to advance, the demand for higher quality and faster processing of long contexts across various applications is growing. KV cache is widely adopted as it stores previously generated key and value tokens, effectively reducing redundant computations during inference. However, as memory overhead becomes a significant concern, efficient compression of KV cache has gained increasing attention. Most existing methods perform compression from two perspectives: identifying important tokens and designing compression strategies. However, these approaches often produce biased distributions of important tokens due to the influence of accumulated attention scores or positional encoding. Furthermore, they overlook the sparsity and redundancy across different heads, which leads to difficulties in preserving the most effective information at the head level. To this end, we propose EMS to overcome these limitations, while achieving better KV cache compression under extreme compression ratios. Specifically, we introduce a Global-Local score that combines accumulated attention scores from both global and local KV tokens to better identify the token importance. For the compression strategy, we design an adaptive and unified Evict-then-Merge framework that accounts for the sparsity and redundancy of KV tokens across different heads. Additionally, we implement the head-wise parallel compression through a zero-class mechanism to enhance efficiency. Extensive experiments demonstrate our SOTA performance even under extreme compression ratios. EMS consistently achieves the lowest perplexity, improves scores by over 1.28 points across four LLMs on LongBench under a 256 cache budget, and preserves 95% retrieval accuracy with a cache budget less than 2% of the context length in the Needle-in-a-Haystack task."
    },
    {
        "title": "OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation",
        "link_suffix": "/forum?id=j7kdXSrISM",
        "link": "https://openreview.net/forum?id=j7kdXSrISM",
        "pdf_link": "https://openreview.net/pdf?id=j7kdXSrISM",
        "keywords": "Text-Video Dataset, Video Generation",
        "abstract": "Text-to-video (T2V) generation has recently garnered significant attention thanks to the large multi-modality model Sora. However, T2V generation still faces two important challenges: 1) Lacking a precise open sourced high-quality dataset. The previously popular video datasets, e.g.WebVid-10M and Panda-70M, overly emphasized large scale, resulting in the inclusion of many low-quality videos and\nshort, imprecise captions. Therefore, it is challenging but crucial to collect a precise high-quality dataset while maintaining a scale of millions for T2V generation. 2) Ignoring to fully utilize textual information. Recent T2V methods have focused on vision transformers, using a simple cross attention module for video generation, which falls short of making full use of semantic information from text tokens. To address these issues, we introduce OpenVid-1M, a precise high-quality dataset with expressive captions. This open-scenario dataset contains over 1 million text-video pairs, facilitating research on T2V generation. Furthermore, we curate 433K 1080p videos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition video generation. Additionally, we propose a novel Multi-modal Video Diffusion Transformer (MVDiT) capable of mining both structure information from visual tokens and semantic information from text tokens. Extensive experiments and ablation studies verify the superiority of OpenVid-1M over previous datasets and the effectiveness of our MVDiT."
    },
    {
        "title": "Model Risk-sensitive Offline Reinforcement Learning",
        "link_suffix": "/forum?id=h6k4809xVV",
        "link": "https://openreview.net/forum?id=h6k4809xVV",
        "pdf_link": "https://openreview.net/pdf?id=h6k4809xVV",
        "keywords": "risk-sensitive offline reinforcement learning, reinforcement learning, offline reinforcement learning, risk, model risk",
        "abstract": "Offline reinforcement learning (RL) is becoming critical in risk-sensitive areas such as finance and autonomous driving, where incorrect decisions can lead to substantial financial loss or compromised safety. However, traditional risk-sensitive offline RL methods often struggle with accurately assessing risk, with minor errors in the estimated return potentially causing significant inaccuracies of risk estimation. These challenges are intensified by distribution shifts inherent in offline RL. To mitigate these issues, we propose a model risk-sensitive offline RL framework designed to minimize the worst-case of risks across a set of plausible alternative scenarios rather than solely focusing on minimizing estimated risk. We present a critic-ensemble criterion method that identifies the plausible alternative scenarios without introducing additional hyperparameters. We also incorporate the learned Fourier feature framework and the IQN framework to address spectral bias in neural networks, which can otherwise lead to severe errors in calculating model risk. Our experiments in finance and self-driving scenarios demonstrate that the proposed framework significantly reduces risk, by $11.2%$ to $18.5%$, compared to the most outperforming risk-sensitive offline RL baseline, particularly in highly uncertain environments."
    },
    {
        "title": "EBES: Easy Benchmarking for Event Sequences",
        "link_suffix": "/forum?id=orEX9GKQAD",
        "link": "https://openreview.net/forum?id=orEX9GKQAD",
        "pdf_link": "https://openreview.net/pdf?id=orEX9GKQAD",
        "keywords": "event sequences, irregularly sampled time series, benchmark, temporal point process, transactions, sequential learning, reproducible research",
        "abstract": "Event sequences, characterized by irregular sampling intervals and a mix of categorical and numerical features, are common data structures in various real-world domains such as healthcare, finance, and user interaction logs. Despite advances in temporal data modeling techniques, there is no standardized benchmarks for evaluating their performance on event sequences. This complicates result comparison across different papers due to varying evaluation protocols, potentially misleading progress in this field.\nWe introduce EBES, a comprehensive benchmarking tool with standardized evaluation scenarios and  protocols, focusing on regression and classification problems with sequence-level targets. Our library~\\footnote{We attach an archive with the code. The code will be publicly available after the conference decision.}  simplifies benchmarking, dataset addition, and method integration through a unified interface. It includes a novel synthetic dataset and provides preprocessed real-world datasets, including the largest publicly available banking dataset.\nOur results provide an in-depth analysis of datasets, identifying some as unsuitable for model comparison. We investigate the importance of modeling temporal and sequential components, as well as the robustness and scaling properties of the models. These findings highlight potential directions for future research. Our benchmark aim is to facilitate reproducible research, expediting progress and increasing real-world impacts."
    },
    {
        "title": "TreeTop: Topology-Aware Fine-Tuning for LLM Conversation Tree Understanding",
        "link_suffix": "/forum?id=piRU8xOurs",
        "link": "https://openreview.net/forum?id=piRU8xOurs",
        "pdf_link": "https://openreview.net/pdf?id=piRU8xOurs",
        "keywords": "Conversation Trees, Social media, Large language models",
        "abstract": "While Large Language Models (LLMs) have dominated a wide diversity of natural language tasks, improving their capabilities on \\emph{structured} inputs such as graphs remains an open challenge. We introduce $\\texttt{TreeTop}$, a pre-training framework for LLMs that significantly improves their ability to understand and reason over structural relationships in multi-party, threaded discussions, such as those found on social media platforms. $\\texttt{TreeTop}$ is a novel set of 17 QA-style tasks specifically designed to allow LLMs to selectively focus on both the structure of and content in discussion graphs. We find that LLMs fine-tuned with $\\texttt{TreeTop}$ outperform their counterparts in every setting: zero-shot/few-shot performance on unseen pretraining tasks as well as downstream social media inference tasks (e.g.rumor detection), as well as fine-tuned performance on the downstream tasks, including their challenging \"early-detection\" variants. In particular, $\\texttt{Gemini Pro}$ fine-tuned with $\\texttt{TreeTop}$ and further fine-tuned on downstream tasks surpasses both vanilla $\\texttt{Gemini Pro}$ and state-of-the-art GNN baselines. Our framework paves the way for LLMs with enhanced capabilities on heavily-structured inputs."
    },
    {
        "title": "Attack as Defense: Run-time Backdoor Implantation for Image Content Protection",
        "link_suffix": "/forum?id=qdbluGtEpL",
        "link": "https://openreview.net/forum?id=qdbluGtEpL",
        "pdf_link": "https://openreview.net/pdf?id=qdbluGtEpL",
        "keywords": "image editing, backdoor attack, image inpainting",
        "abstract": "As generative models achieve great success, tampering and modifying the sensitive image contents (i.e., human faces, artist signatures, commercial logos, etc.) have induced a significant threat with social impact. \nThe backdoor attack is a method that implants vulnerabilities in a target model, which can be activated through a trigger.\nIn this work, we innovatively prevent the abuse of image content modification by implanting the backdoor into image-editing models. Once the protected sensitive content on an image is modified by an editing model, the backdoor will be triggered, making the editing fail. \nUnlike traditional backdoor attacks that use data poisoning, to enable protection on individual images and eliminate the need for model training, we developed the first framework for run-time backdoor implantation, which is both time- and resource- efficient. We generate imperceptible perturbations on the images to inject the backdoor and define the protected area as the only backdoor trigger. Editing other unprotected insensitive areas will not trigger the backdoor, which minimizes the negative impact on legal image modifications. Evaluations with state-of-the-art image editing models show that our protective method can increase the CLIP-FID of generated images from 12.72 to 39.91, or reduce the SSIM from 0.503 to 0.167 when subjected to malicious editing. At the same time, our method exhibits minimal impact on benign editing, which demonstrates the efficacy of our proposed framework. The proposed run-time backdoor can also achieve effective protection on the latest diffusion models."
    },
    {
        "title": "Reasoning Limitations of  Multimodal Large Language Models. A case study of Bongard Problems",
        "link_suffix": "/forum?id=BTk1hNuIPq",
        "link": "https://openreview.net/forum?id=BTk1hNuIPq",
        "pdf_link": "https://openreview.net/pdf?id=BTk1hNuIPq",
        "keywords": "Multimodal Large Language Models, Abstract Visual Reasoning, Bongard Problems",
        "abstract": "Abstract visual reasoning (AVR) encompasses a suite of tasks whose solving requires the ability to discover common concepts underlying the set of pictures through an analogy-making process, similarly to solving the human IQ test problems. Bongard Problems (BPs), proposed in 1968, constitute one of the fundamental challenges in this domain. Despite multiple advances in artificial intelligence, the BP tasks remain unsolved, mainly due to their requirement to combine visual reasoning and verbal description. In this work, we pose a question whether multimodal large language models (MLLMs) inherently designed to combine vision and language are capable of tackling BPs. To this end, we propose a set of diverse MLLM-suited strategies to tackle BPs and test 4 popular proprietary MLLMs: GPT-4o, GPT-4 Turbo, Gemini 1.5 Pro, and Claude 3.5 Sonnet, and 4 publicly available open models: InternVL2-8B, LLaVa-1.6 Mistral-7B, Phi-3.5-Vision, and Pixtral 12B. The above MLLMs are compared on 3 BP datasets from the AVR literature: a set of original BP instances relying on synthetic, geometry-based images and two recent datasets based on real-world images, i.e., Bongard-HOI and Bongard-OpenWorld. Our experiments reveal significant limitations of the current MLLMs in solving BPs. In particular, the models struggle to solve the classical set of synthetic BPs representing abstract concepts, despite their visual simplicity. Though their performance improves for real-world concepts expressed in Bongard-HOI and Bongard-OpenWorld datasets, the models still have difficulty in utilizing new information to improve their predictions, as well as utilizing the dialog context window effectively. To better capture the reasons of this performance discrepancy between synthetic and real-world AVR domains, we propose Bongard-RWR, a new BP dataset composed of specifically-designed real-world images that translate concepts from hand-crafted synthetic matrices to the real world, and perform focused experiments with this new dataset. The results suggest that weak models' performance on classical BPs is not due to the domain specificity, but rather comes from their general AVR limitations."
    },
    {
        "title": "A Multi-Decomposition Method for Compressing Larger AI Models Based on Reinforcement Learning",
        "link_suffix": "/forum?id=cO01zqImBC",
        "link": "https://openreview.net/forum?id=cO01zqImBC",
        "pdf_link": "https://openreview.net/pdf?id=cO01zqImBC",
        "keywords": "deep neural network, low rank decomposition, multiple decomposition methods, reinforcement learning.",
        "abstract": "With the development of modern deep neural network (DNN), the scale of parameters is increasing, making it difficult to deploy models for use on resource-constrained edge devices. To address this issue, model compression is necessary, and using low-rank matrix decomposition to compress DNN models is an effective research approach. However, traditional studies on low-rank decomposition compression typically apply a single matrix decomposition method to each parameter matrix in the neural network, without considering the structural characteristics of each layer in AI models, thus failing to achieve the optimal compression effect. Therefore, this paper proposes, for the first time, a scheme for model compression using multiple decomposition methods, selecting the most suitable decomposition method for each layer in the model. However, to truly implement this approach, it is essential to balance model accuracy and compression cost. To address this, we propose a joint optimization paradigm that simultaneously optimizes model accuracy and compression rate. We also introduce a framework LMFBRL based on reinforcement learning that jointly selects the optimal decomposition method and rank. Tests were conducted on five models such as LeNet-300, ResNet-20, and Vgg-16. Compared to singly using the MF method for compressing the LeNet300 model, our approach has shown an improvement of 3.6% in compression rate and a 1.8% increase in accuracy. The test results validate the effectiveness of the algorithm proposed in this paper."
    },
    {
        "title": "One  slice  is not  enough:  In  search  of  stable conclusions in text-to-image evaluation",
        "link_suffix": "/forum?id=Im2neAMlre",
        "link": "https://openreview.net/forum?id=Im2neAMlre",
        "pdf_link": "https://openreview.net/pdf?id=Im2neAMlre",
        "keywords": "text-to-image evaluation; text-to-image alignment; human evaluation;",
        "abstract": "While text-to-image (T2I) generative models have become ubiquitous, they do not necessarily generate images that align with a given prompt. \nWhile many metrics and benchmarks have been proposed to evaluate T2I models and alignment metrics, the impact of the evaluation components (prompt sets, human annotations, evaluation task) has not been systematically measured.\nWe find that looking at onlyone slice of data, i.e. one set of capabilities or human annotations, is not enough to obtain stable conclusions that generalise to new conditions or slices when evaluating T2I models or alignment metrics. \nWe address this by introducing an evaluation suite of $>$100K annotations across four human annotation templates that comprehensively evaluates models' capabilities across a range of methods for gathering human annotations and comparing models.\nIn particular, we propose (1) a carefully curated set of prompts --Gecko2K; (2) a statistically grounded method of comparing T2I models; and (3) how to systematically evaluate metrics under threeevaluation tasks--model ordering, pair-wise instance scoring, point-wise instance scoring.\nUsing this evaluation suite, we evaluate a wide range of metrics and find that a metric may do better in one setting but worse in another.\nAs a result, we introduce a new, interpretable auto-eval metric that is consistently better correlated with human ratings than such existing metrics on our  evaluation suite--across different human templates and evaluation settings--and on TIFA160."
    },
    {
        "title": "VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents",
        "link_suffix": "/forum?id=zG459X3Xge",
        "link": "https://openreview.net/forum?id=zG459X3Xge",
        "pdf_link": "https://openreview.net/pdf?id=zG459X3Xge",
        "keywords": "Retrieval-augmented Generation, Vision-language Models",
        "abstract": "Retrieval-augmented generation (RAG) is an effective technique that enables large language models (LLMs) to utilize external knowledge sources for generation. However, current RAG systems are solely based on text, rendering it impossible to utilize vision information like layout and images that play crucial roles in real-world multi-modality documents. In this paper, we introduce VisRAG, which tackles this issue by establishing a vision-language model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the document to obtain text, the document is directly embedded using a VLM as an image and then retrieved to enhance the generation of a VLM. Compared to traditional text-based RAG, VisRAG maximizes the retention and utilization of the data information in the original documents, eliminating the information loss introduced during the parsing process. We collect both open-source and synthetic data to train the retriever in VisRAG and explore a variety of generation methods. Experiments demonstrate that VisRAG outperforms traditional RAG in both the retrieval and generation stages, achieving a 25\u201339% end-to-end performance gain over traditional textbased RAG pipeline. Further analysis reveals that VisRAG is effective in utilizing training data and demonstrates strong generalization capability, positioning it as a promising solution for RAG on multi-modality documents. Our code and data will be made publicly available."
    },
    {
        "title": "Learning system dynamics without forgetting",
        "link_suffix": "/forum?id=rjuZyMfLSd",
        "link": "https://openreview.net/forum?id=rjuZyMfLSd",
        "pdf_link": "https://openreview.net/pdf?id=rjuZyMfLSd",
        "keywords": "graph neural networks, AI4Science, physics, biology",
        "abstract": "Observation-based trajectory prediction for systems with unknown dynamics is essential in fields such as physics and biology. Most existing approaches are limited to learning within a single system with fixed dynamics patterns. However, many real-world applications require learning across systems with evolving dynamics patterns, a challenge that has been largely overlooked. To address this, we systematically investigate the problem of Continual Dynamics Learning (CDL), examining task configurations and evaluating the applicability of existing techniques, while identifying key challenges. In response, we propose the Mode-switching Graph ODE (MS-GODE) model, which integrates the strengths LG-ODE and sub-network learning with a mode-switching module, enabling efficient learning over varying dynamics. Moreover, we construct a novel benchmark of biological dynamic systems for CDL, Bio-CDL, featuring diverse systems with disparate dynamics and significantly enriching the research field of machine learning for dynamic systems. Our code and benchmark datasets will be publicly available."
    },
    {
        "title": "Bootstrapping Language Models with DPO Implicit Rewards",
        "link_suffix": "/forum?id=dliIIodM6b",
        "link": "https://openreview.net/forum?id=dliIIodM6b",
        "pdf_link": "https://openreview.net/pdf?id=dliIIodM6b",
        "keywords": "Alignment, Direct Preference Optimization, Large Language Models",
        "abstract": "Human alignment in large language models (LLMs) is an active area of research. A recent groundbreaking work, direct preference optimization (DPO), has greatly simplified the process from past work in reinforcement learning from human feedback (RLHF) by bypassing the reward learning stage in RLHF. DPO, after training, provides an implicit reward model. In this work, we make a novel observation that this implicit reward model can by itself be used in a bootstrapping fashion to further align the LLM. Our approach is to use the rewards from a current LLM model to construct a preference dataset, which is then used in subsequent DPO rounds. We incorporate refinements that debias the length of the responses and enhance the quality of the preference dataset to further improve our approach. Our approach, named self-alignment with DPO ImpliCit rEwards (DICE), shows great improvements in alignment. It achieves an increase of more than 8$\\%$ in length-controlled win rate on AlpacaEval 2 for all the different base models that we tried, without relying on external feedback."
    },
    {
        "title": "A unified lightweight complex scenes-oriented network for infrared and visible image fusion",
        "link_suffix": "/forum?id=RqJ0px8osW",
        "link": "https://openreview.net/forum?id=RqJ0px8osW",
        "pdf_link": "https://openreview.net/pdf?id=RqJ0px8osW",
        "keywords": "Infrared and visible image fusion, Complex Scenes, Unified Network, Frequency domain, Real time",
        "abstract": "Existing infrared and visible image fusion (IVIF) techniques typically integrate the useful information from different modalities within the ideal conditions. Nevertheless, current state-of-the-art IVIF methods are ineffective when facing complex scene interferences such as bad weather, low light, and high noise, and they typically need to be used in conjunction with other de-interference baselines, which inevitably resulting in the high memory costs and error accumulation, thus yielding sub-optimal fusion results. To address these challenges, We propose a unified lightweight real-time IVIF network for multiple complex scenes. We conducted a theoretically thorough analysis of modal degradations in the frequency domain, leveraging the complementary strengths of both modalities to enhance network learning. Our method facilitates the extraction of critical features even amidst significant pixel interference. For reconstructing fusion results, we introduce a spatial domain branching strategy which significantly improves the local detail resolution, thereby mitigating potential omissions from frequency domain analysis. Extensive qualitative and quantitative experiments demonstrate that our framework excels in handling multiple complex scenes, while maintaining real-time computational efficiency for prompt image processing applications."
    },
    {
        "title": "CARPRT: Class-Aware Prompt Reweighting for Pre-Trained Vision-Language Models",
        "link_suffix": "/forum?id=fRpAUgKJhT",
        "link": "https://openreview.net/forum?id=fRpAUgKJhT",
        "pdf_link": "https://openreview.net/pdf?id=fRpAUgKJhT",
        "keywords": "Prompt Weighting, Vision-language Models",
        "abstract": "When using a pre-trained vision-language model (VLM) to classify an image, we often need to use the pre-trained VLM to compute a similarity score between the image and texts containing a semantic label, e.g., \u201ca photo of a cat\u201d, where \u201ca photo of a\u201d is called a prompt and \u201ccat\u201d is the semantic label (a.k.a. a class in classification tasks). The existing studies have shown that the selection of prompts can significantly affect the scoring scheme between a given image and a semantic label, and they proposed a new score via using a weighting vector to reassemble scores regarding different prompts. However, these studies assume that all classes should share the same weighting vector. In this paper, we first empirically show that the existing approach is sub-optimal. We subsequently revisit the existing reweighting strategy from a probabilistic view and find an implicit assumption in prior work: the conditional independence of classes and weights, which often does not hold in practice. To cope with this problem, we propose class-aware prompt reweighting (CARPRT), a strategy designed to adjust the weighting vector for each class. CARPRT calculates the relevance scores for prompt-class pairs with respect to all images, and identifies the maximum score for each prompt-class pair. These maximum scores are then averaged across prompts for each class to estimate the class-specific weighting vectors, ensuring that prompts are optimally reweighted based on class-specific information. Our experiments demonstrate that CARPRT outperforms the existing reweighting strategy under the image classification tasks."
    },
    {
        "title": "A robust federated learning client selection with combinatorial data class representations and data augmentation",
        "link_suffix": "/forum?id=OKnsCAZlSc",
        "link": "https://openreview.net/forum?id=OKnsCAZlSc",
        "pdf_link": "https://openreview.net/pdf?id=OKnsCAZlSc",
        "keywords": "Federated Learning, Client Selection, Backdoor Defense, Data Augmentation, Representation Learning.",
        "abstract": "The federated learning (FL) client selection scheme can effectively mitigate global model performance degradation caused by the random aggregation of clients with heterogeneous data. Simultaneously, research has exposed FL's susceptibility to backdoor attacks. However herein lies the dilemma, traditional client selection methods and backdoor defenses stand at odds, so their integration is an elusive goal.\nTo resolve this, we introduce Grace, a resilient client selection framework blending combinational class sampling with data augmentation. On the client side, Grace first proposes a local model purification method, fortifying the model's defenses by bolstering its innate robustness. After, local class representations are extracted for server-side client selection. This approach not only shields benign models from backdoor tampering but also allows the server to glean insights into local class representations without infringing upon the client's privacy.\nOn the server side, Grace introduces a novel representation combination sampling method. Clients are selected based on the interplay of their class representations, a strategy that simultaneously weeds out malicious actors and draws in clients whose data holds unique value.\nOur extensive experiments highlight Grace's capabilities. The results are compelling: Grace enhances defense performance by over 50% compared to state-of-the-art (SOTA) backdoor defenses, and, in the best case, improves accuracy by 3.19% compared to SOTA client selection schemes. Consequently, Grace achieves substantial advancements in both security and accuracy."
    },
    {
        "title": "Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs",
        "link_suffix": "/forum?id=kiOxNsrpQy",
        "link": "https://openreview.net/forum?id=kiOxNsrpQy",
        "pdf_link": "https://openreview.net/pdf?id=kiOxNsrpQy",
        "keywords": "Explainability, Trustworthiness, Faithfulness, Self-Explainable GNNs",
        "abstract": "As Graph Neural Networks (GNNs) become more pervasive, it becomes paramount to build reliable tools for explaining their predictions.\nA core desideratum is that explanations arefaithful, i.e., that they portray an accurate picture of the GNN's reasoning process.\nHowever, a number of different faithfulness metrics exist, begging the question of what is faithfulness exactly and how to achieve it.\nWe make three key contributions.\nWe begin by showing thatexisting metrics are not interchangeable-- i.e., explanations attaining high faithfulness according to one metric may be unfaithful according to others -- and cansystematically ignore important properties of explanations.\nWe proceed to show that, surprisingly,optimizing for faithfulness is not always a sensible design goal.  Specifically, we prove that for injective regular GNN architectures, perfectly faithful explanations are completely uninformative.\nThis does not apply to modular GNNs, such as self-explainable and domain-invariant architectures, prompting us to study the relationship between architectural choices and faithfulness.\nFinally, we show thatfaithfulness is tightly linked to out-of-distribution generalization, in that simply ensuring that a GNN can correctly recognize the domain-invariant subgraph, as prescribed by the literature, does not guarantee that it is invariant unless this subgraph is also faithful.\nAll our code can be found in the supplementary material."
    },
    {
        "title": "Let Large Language Models Find the Data to Train Themselves",
        "link_suffix": "/forum?id=5YCZZSEosw",
        "link": "https://openreview.net/forum?id=5YCZZSEosw",
        "pdf_link": "https://openreview.net/pdf?id=5YCZZSEosw",
        "keywords": "Self-improving, Synthetic Data, Large Language Models",
        "abstract": "The current iterative development process for large language models (LLMs) is heavily data-centric, relying on human researchers and engineers to manually analyze model performance and determine what data to acquire for further training. However, this human-supervised approach is costly and may fail to identify optimal training signals. Its scalability is further limited as models become increasingly capable and may eventually exceed human intelligence. To address these issues, we propose an automated framework that enables models to autonomously discover and strategically acquire the most valuable training data to enhance their performance. It establishes a self-improving framework where models can invoke APIs to crawl and/or generate tailored datasets from various resources and environments, and retrain themselves. The data selection decisions are shaped by reinforcement feedback signals that reward performance gains while penalizing computational overhead. This formulation incentivizes models to develop self-knowledge about their strengths and areas for improvement in order to efficiently select training data. Empirical results demonstrate that LLMs operating within our framework are able to autonomously and strategically acquire valuable training data to enhance their performance across a variety of skills in 1,000 diverse in-house test tasks and three public benchmarks."
    },
    {
        "title": "SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting",
        "link_suffix": "/forum?id=7idCpuEAiR",
        "link": "https://openreview.net/forum?id=7idCpuEAiR",
        "pdf_link": "https://openreview.net/pdf?id=7idCpuEAiR",
        "keywords": "Self Calibration, Gaussian Splatting, Radiance Field, Omnidirectional Vision, Bundle Adjustment",
        "abstract": "360-degree cameras streamline data collection for radiance field 3D reconstruction by capturing comprehensive scene data. However, traditional radiance field methods do not address the specific challenges inherent to 360-degree images. We present SC-OmniGS, a novel self-calibrating omnidirectional Gaussian splatting system for fast and accurate omnidirectional radiance field reconstruction using 360-degree images. Rather than converting 360-degree images to cube maps and performing perspective image calibration, we treat 360-degree images as a whole sphere and derive a mathematical framework that enables direct omnidirectional camera pose calibration accompanied by 3D Gaussians optimization. Furthermore, we introduce a differentiable omnidirectional camera model in order to rectify the distortion of real-world data for performance enhancement. Overall, the omnidirectional camera intrinsic model, extrinsic poses, and 3D Gaussians are jointly optimized by minimizing weighted spherical photometric loss. Extensive experiments have demonstrated that our proposed SC-OmniGS is able to recover a high-quality radiance field from noisy camera poses or even no pose prior in challenging scenarios characterized by wide baselines and non-object-centric configurations. The noticeable performance gain in the real-world dataset captured by consumer-grade omnidirectional cameras verifies the effectiveness of our general omnidirectional camera model in reducing the distortion of 360-degree images."
    },
    {
        "title": "TimeCapsule:  Solving the Jigsaw Puzzle of Long-Term Time Series Forecasting with Compressed Predictive Representations",
        "link_suffix": "/forum?id=blgJ4g00rC",
        "link": "https://openreview.net/forum?id=blgJ4g00rC",
        "pdf_link": "https://openreview.net/pdf?id=blgJ4g00rC",
        "keywords": "multivariate long-term time series forecasting; deep learning; infomation tensor modeling",
        "abstract": "Recent deep learning models for long-term time series forecasting (LTSF) often emphasize complex, handcrafted designs and traditional methodologies, while simpler architectures like linear models or MLPs have occasionally outperformed these intricate solutions. In this paper, we revisit and organize the core ideas behind several key techniques, such as redundancy reduction and multi-scale modeling, which are frequently employed in advanced LTSF models. Our goal is to streamline these ideas for more efficient deep learning utilization. To this end, we introduce TimeCapsule, a model built around the principle of high-dimensional information compression that unifies these key ideas in a generalized yet simplified framework. Specifically, we model time series as a 3D tensor, incorporating temporal, variate, and level dimensions, and leverage mode production to capture multi-mode dependencies while achieving dimensionality compression. We propose an internal forecast within the compressed representation domain, supported by the Joint-Embedding Predictive Architecture (JEPA) to monitor the learning of predictive representations. Extensive experiments on challenging benchmarks demonstrate the versatility of our method, showing that TimeCapsule can achieve performance comparable to state-of-the-art models. More importantly, the structure of our model yields intriguing empirical findings, prompting a rethinking of approaches in this area."
    },
    {
        "title": "Navigating Conflicting Views: Harnessing Trust for Learning",
        "link_suffix": "/forum?id=IL85Ebjg9j",
        "link": "https://openreview.net/forum?id=IL85Ebjg9j",
        "pdf_link": "https://openreview.net/pdf?id=IL85Ebjg9j",
        "keywords": "Multi-view Learning, Conflict Multi-view Learning, Reliable Multi-view Learning",
        "abstract": "Resolving conflicts is essential to make the decisions of multi-view classification more reliable. Much research has been conducted on learning consistent and informative representations among different views,  often assuming that all views are equally important and perfectly aligned. However, real-world multi-view data may not always conform to these assumptions, as some views may express distinct information. To address this issue, we develop a computational trust-based discounting method to enhance the existing Evidential Multi-view framework in scenarios where conflicts between different views may arise. Its belief fusion process considers the reliability of predictions made by individual views via an instance-wise probability-sensitive trust discounting mechanism. We evaluate our method on six real-world datasets, using Top-1 Accuracy, Fleiss\u2019 Kappa, and a new metric called Multi-View Agreement with Ground Truth that takes into consideration the ground truth labels, to measure the reliability of the prediction. We also evaluate whether uncertainty measures can effectively indicate prediction correctness by calculating the AUROC. The experimental results show that computational trust can effectively resolve conflicts, paving the way for more reliable multi-view classification models in real-world applications."
    },
    {
        "title": "SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration",
        "link_suffix": "/forum?id=EKJhH5D5wA",
        "link": "https://openreview.net/forum?id=EKJhH5D5wA",
        "pdf_link": "https://openreview.net/pdf?id=EKJhH5D5wA",
        "keywords": "Speculative Decoding, LLM Inference Acceleration, Efficient NLP",
        "abstract": "Speculative decoding (SD) has emerged as a widely used paradigm to accelerate the inference of large language models (LLMs) without compromising generation quality. It works by first employing a compact model to draft multiple tokens efficiently and then using the target LLM to verify them in parallel. While this technique has achieved notable speedups, most existing approaches necessitate either additional parameters or extensive training to construct effective draft models, thereby restricting their applicability across different LLMs and tasks. To address this limitation, we explore a novel plug-and-play SD solution with layer-skipping, which skips intermediate layers of the target LLM as the compact draft model. Our analysis reveals that LLMs exhibit great potential for self-acceleration through layer sparsity and the task-specific nature of this sparsity. Building on these insights, we introduce SWIFT, an on-the-fly self-speculative decoding algorithm that adaptively selects intermediate layers of LLMs to skip during inference. SWIFT does not require auxiliary models or additional training, making it a plug-and-play solution for accelerating LLM inference across diverse input data streams. Our extensive experiments across a wide range of models and downstream tasks demonstrate that SWIFT can achieve over a 1.3x-1.6x speedup while preserving the original distribution of the generated text."
    }
]