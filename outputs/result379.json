[{"title": "Nash-GBML: Nash Gradient-Based Meta-Learning", "link_suffix": "/forum?id=EWcOEZa6Ee", "link": "https://openreview.net/forum?id=EWcOEZa6Ee", "pdf_link": "https://openreview.net/pdf?id=EWcOEZa6Ee", "keywords": "model-agnostic meta-learning, gradient-based meta-learning, game theory, nash game, single-leader multi-follower game", "abstract": "Meta-learning has been proposed to address fast adaptation to unseen tasks with little data. Traditional meta-learning is modeled as the Single-Leader Multi-Follower game consisting of inner and outer-level problems to minimize average or worst-case task loss. Because they assume all sampled tasks are independent, it reduces the flexibility of modeling complex interaction among tasks. Thus, we formulate meta-learning as a Single-Leader Multi-Follower game by considering the interaction among tasks at the inner level. We propose the Nash-GBML incorporating a penalty term into the task loss function to model the interaction among task-specific parameters. We discuss the iteration complexity and convergence of the Nash-GBML algorithm. To validate our Nash-GBML algorithm, we introduce two penalty terms, which are designed to reduce the average and worst-case task loss. We empirically show that the Nash-GBML with the proposed penalty terms outperforms traditional GBML for supervised learning experiments.", "title_embedding_index": 18900, "title_abs_embedding_index": 18925}, {"title": "Sample-Imagined Generator: Efficient Virtual Sample Generation Method for Off-policy Reinforcement Learning with Sparse Rewards", "link_suffix": "/forum?id=xvsNb5y9CN", "link": "https://openreview.net/forum?id=xvsNb5y9CN", "pdf_link": "https://openreview.net/pdf?id=xvsNb5y9CN", "keywords": "Off-policy Reinforcement Learning, Sparse Reward Reinforcement Learning, Sample Efficiency", "abstract": "Off-policy reinforcement learning (RL) requires extensive real interaction with environment to gain experience for policy learning, presenting a challenge of low sample efficiency, especially in the condition of sparse rewards. To address this, we propose a Sample-Imagined Generator (SIG) which automatically trains a sample generator during environment interaction and could adaptively generate valuable imagined samples for policy learning. Through SIG, the policy greatly reduced the interaction with the environment during training and achieved comparable or even higher performance with those trained only through real interactions. SIG could be combined with any off-policy RL algorithm. Experiment in 5 continuous control tasks demonstrate that by substituting imagined samples for real ones to supplement the experience pool, SIG accomplishes tasks with significantly less interaction with the environment, notably improving sample efficiency across 10 off-policy reinforcement learning algorithms.", "title_embedding_index": 18901, "title_abs_embedding_index": 18926}, {"title": "3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes", "link_suffix": "/forum?id=7gGVDrqVaz", "link": "https://openreview.net/forum?id=7gGVDrqVaz", "pdf_link": "https://openreview.net/pdf?id=7gGVDrqVaz", "keywords": "Theorem Proving, Formal Reasoning, Search, Representation Learning, Pruning, Filtering, Diversity", "abstract": "A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D-Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F-valid and miniF2F-test benchmarks by augmenting the ReProver LLM. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success rate, execution time and diversity.", "title_embedding_index": 18902, "title_abs_embedding_index": 18927}, {"title": "NovoBench-100K: A large-scale protein dataset for in silico evolution of de novo TadA", "link_suffix": "/forum?id=ZkpDdCQUC4", "link": "https://openreview.net/forum?id=ZkpDdCQUC4", "pdf_link": "https://openreview.net/pdf?id=ZkpDdCQUC4", "keywords": "Benchmark, Dataset, Biological Language Model, Ranking, Base Editing", "abstract": "We introduce NOVOBENCH-100K, a large-scale protein dataset for the in silico evolution of TadA, an enzyme critical for base editing. This dataset originates from the sequencing data collected during two rounds of our in vitro TadA evolution, encompassing 101,687 unique DNA variants with an average of 11.1 amino acid mutations. Rather than employing classes or scores as labels, our dataset consists of 77,900 ranking lists, each involving 2, 10, or 100 sequences ranked by their base editing efficiency. These rankings are generated using our SEQ2RANK, a novel algorithm that accounts for biological experiment credibility and ranking consistency. For evaluation, we provide two train-test splits, designated as in-domain ranking and out-of-domain ranking, based on a standard 7:3 random split and the actual in-vitro evolution rounds, respectively. We benchmark 80 biological language models (BLMs) across 24 papers, spanning protein, DNA, RNA, and multimodal domains. Comprehensive experiments reveal that BLMs perform well on in-domain ranking, with a detailed analysis by modality, model size, and K-mer. However, for out-of-domain ranking, BLMs exhibit poor performance in both linear probing and fine-tuning, resembling random guessing. This underscores the necessity for highly generalizable models to address domain shifts between experimental rounds. Finally, our wet experiments are ongoing to generate more data to expand our benchmark. In a few months, we expect to add additional rounds of in vitro evolution and include a broader variety of proteins. We will release the code, dataset, and embeddings of our evaluated 80 BLMs soon.", "title_embedding_index": 18903, "title_abs_embedding_index": 18928}, {"title": "EEGPT: Unleashing the Potential of EEG Generalist Foundation Model by Autoregressive Pre-training", "link_suffix": "/forum?id=wJ6Bx1IYrQ", "link": "https://openreview.net/forum?id=wJ6Bx1IYrQ", "pdf_link": "https://openreview.net/pdf?id=wJ6Bx1IYrQ", "keywords": "EEG, Brain-computer interface, Representation learning", "abstract": "Electroencephalogram (EEG) signals are pivotal in providing insights into spontaneous brain activity, highlighting their significant importance in neuroscience research. However, the exploration of versatile EEG models is constrained by diverse data formats, outdated pre-training paradigms, and limited transfer learning methods, only leading to specialist models on single dataset. In this paper, we introduce EEGPT, the first generalist EEG foundation model designed to address these challenges. First, we propose an electrode-wise modeling strategy that treats each electrode as a fundamental unit, enabling the integration of diverse EEG datasets collected from up to 138 electrodes, amassing 37.5M pre-training samples. Second, we develop the first autoregressive EEG pre-trained model, moving away from traditional masked autoencoder approaches to a next signal prediction task that better captures the sequential and temporal dependencies of EEG data. We also explore scaling laws with model up to 1.1B parameters \u2014 the largest in EEG research to date. Third, we introduce a multi-task transfer learning paradigm using a learnable electrode graph network that is shared across tasks, which for the first time confirms multi-task compatibility and synergy. As the first generalist EEG foundation model, EEGPT shows broad compatibility with various signal acquisition devices, subjects, and tasks. It supports up to 138 electrodes and any combination thereof as input. Furthermore, we simultaneously evaluate it on 5 distinct downstream tasks across 12 benchmarks. EEGPT consistently outperforms existing specialist models across all downstream tasks, with its effectiveness further validated through extensive ablation studies.\nThis work sets a new direction for generalist EEG modeling, offering improved scalability, transferability, and adaptability for a wide range of EEG applications. Both the training code and model checkpoints will be publicly available.", "title_embedding_index": 18904, "title_abs_embedding_index": 18929}, {"title": "DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation", "link_suffix": "/forum?id=FA5ZAJlv96", "link": "https://openreview.net/forum?id=FA5ZAJlv96", "pdf_link": "https://openreview.net/pdf?id=FA5ZAJlv96", "keywords": "diffusion models, 3D editing, score distillation sampling, NeRF, 3D Gaussian Splatting", "abstract": "Score distillation sampling (SDS) has emerged as an effective framework in text-driven 3D editing tasks, leveraging diffusion models for 3D consistent editing. However, existing SDS-based 3D editing methods suffer from long training times and produce low-quality results. We identify that the root cause of this performance degradation is their conflict with the sampling dynamics of diffusion models. Addressing this conflict allows us to treat SDS as a diffusion reverse process for 3D editing via sampling from data space. In contrast, existing methods naively distill the score function using diffusion models. From these insights, we propose DreamCatalyst, a novel framework that considers these sampling dynamics in the SDS framework. Specifically, we devise the optimization process of our DreamCatalyst to approximate the diffusion reverse process in editing tasks, thereby aligning with diffusion sampling dynamics. As a result, DreamCatalyst successfully reduces training time and improves editing quality. Our method offers two modes: (1) a fast mode that edits Neural Radiance Fields (NeRF) scenes approximately 23 times faster than current state-of-the-art NeRF editing methods, and (2) a high-quality mode that produces superior results about 8 times faster than these methods. Notably, our high-quality mode outperforms current state-of-the-art NeRF editing methods in terms of both speed and quality. DreamCatalyst also surpasses the state-of-the-art 3D Gaussian Splatting (3DGS) editing methods, establishing itself as an effective and model-agnostic 3D editing solution.", "title_embedding_index": 18905, "title_abs_embedding_index": 18930}, {"title": "Steady and Fair Robustness Evaluation Based on Model Interpretation", "link_suffix": "/forum?id=bC8oHmcB4X", "link": "https://openreview.net/forum?id=bC8oHmcB4X", "pdf_link": "https://openreview.net/pdf?id=bC8oHmcB4X", "keywords": "Adversarial robustness, robustness evaluation, Shapley value", "abstract": "Adversarial robustness has become a major concern as machine learning models are increasingly deployed in security-sensitive applications. Evaluating adversarial robustness remains a challenging task, as current metrics are heavily affected by various factors, including attack methods, attack intensities, and model architecture. In this paper, we propose Steady and Fair Robustness Evaluation, a novel framework designed to mitigate the impact of these factors and provide a more stable evaluation of a model\u2019s robustness. Our key insight is based on the strong correlation between the standard deviation (SD) of Shapley values, which measures the importance of individual neurons, and adversarial robustness. We demonstrate that models with lower SD of Shapley values are more robust to adversarial attacks, regardless of the attack method or model architecture. Extensive experiments across various models, training objectives, and attack scenarios show that our approach offers more consistent and interpretable robustness evaluation. We further introduce a new training strategy that incorporates the minimization of the SD of Shapley values for improving the robustness of the model. Our findings suggest that analysis based on Shapley value can provide a principled and efficient alternative to conventional robustness evaluation techniques.", "title_embedding_index": 18906, "title_abs_embedding_index": 18931}, {"title": "Flat Posterior Does Matter For Bayesian Model Averaging", "link_suffix": "/forum?id=0tAn34IkXI", "link": "https://openreview.net/forum?id=0tAn34IkXI", "pdf_link": "https://openreview.net/pdf?id=0tAn34IkXI", "keywords": "Bayesian Neural Network, Bayesian Deep Learning, Flatness-aware Optimization, Bayesian Transfer Learning", "abstract": "Bayesian neural network (BNN) approximates the posterior distribution of model parameters and utilizes the posterior for prediction via Bayesian Model Averaging (BMA). The quality of the posterior approximation is critical for achieving accurate and robust predictions. It is known that flatness in the loss landscape is strongly associated with generalization performance, and it necessitates consideration to improve the quality of the posterior approximation. In this work, we empirically demonstrate that BNNs often struggle to capture the flatness. Moreover, we provide both experimental and theoretical evidence showing that BMA can be ineffective without ensuring flatness. To address this, we propose Sharpness-Aware Bayesian Model Averaging (SA-BMA), a novel optimizer that seeks flat posteriors by calculating divergence in the parameter space. SA-BMA aligns with the intrinsic nature of BNN and the generalized version of existing sharpness-aware optimizers for DNN. In addition, we suggest a Bayesian Transfer Learning scheme to efficiently leverage pre-trained DNN. We validate the efficacy of SA-BMA in enhancing generalization performance in few-shot classification and distribution shift by ensuring flat posterior.", "title_embedding_index": 18907, "title_abs_embedding_index": 18932}, {"title": "SEED-X: Multimodal Models in Real World", "link_suffix": "/forum?id=BwlEfAhUVX", "link": "https://openreview.net/forum?id=BwlEfAhUVX", "pdf_link": "https://openreview.net/pdf?id=BwlEfAhUVX", "keywords": "Multimodal LLM, Comprehension and Generation", "abstract": "The rapid evolution of multimodal foundation models has showcased remarkable capabilities in vision-language understanding and generation, yielding impressive results on academic benchmarks. However, there remains a gap in their progress toward real-world applicability, primarily due to the models' limited capacity to effectively respond to various user instructions and interact with diverse visual data. This limitation can be attributed to the fundamental challenge of modeling multi-granularity visual semantics for comprehension and generation tasks. In this paper, we take a pioneering step towards applying multimodal foundation models in an open-world context and present a unified and versatile foundation model, namely, $\\textbf{SEED-X}$. As the first of its kind, SEED-X seamlessly integrates two essential features: (1) comprehending images of arbitrary sizes and ratios, and (2) enabling multi-granularity image generation.\nBesides the competitive results on public benchmarks, SEED-X demonstrates its effectiveness in handling real-world applications across various domains. We hope that our work will inspire future research into what can be achieved by versatile multimodal foundation models in real-world applications. All models, training, and inference codes are available athttps://anonymous.4open.science/r/SEED-X/.", "title_embedding_index": 18908, "title_abs_embedding_index": 18933}, {"title": "Retraining-Free Merging of Sparse Mixture-of-Experts via Hierarchical Clustering", "link_suffix": "/forum?id=yeeIGM3N6w", "link": "https://openreview.net/forum?id=yeeIGM3N6w", "pdf_link": "https://openreview.net/pdf?id=yeeIGM3N6w", "keywords": "Sparse Mixture-of-Experts, Merging, Compression", "abstract": "Sparse Mixture-of-Experts (SMoE) models represent a significant breakthrough in large language model development. These models enable performance improvements without a proportional increase in inference costs. By selectively activating a small set of parameters during task execution, SMoEs enhance model capacity. However, their deployment remains challenging due to the substantial memory footprint required to accommodate the growing number of experts. This constraint renders them less feasible in environments with limited hardware resources. To address this challenge, we propose Hierarchical Clustering for Sparsely activated Mixture of Experts (HC-SMoE), a task-agnostic expert merging framework that reduces SMoE model parameters without retraining. Unlike previous methods, HC-SMoE employs hierarchical clustering based on expert outputs. This approach ensures that the merging process remains unaffected by routing decisions. The output-based clustering strategy captures functional similarities between experts, offering an adaptable solution for models with numerous experts. We validate our approach through extensive experiments on eight zero-shot language tasks and demonstrate its effectiveness in large-scale SMoE models such as Qwen and Mixtral. Our comprehensive results demonstrate that HC-SMoE consistently achieves strong performance, which highlights its potential for real-world deployment.", "title_embedding_index": 18909, "title_abs_embedding_index": 18934}, {"title": "From Loops to Oops: Fallback Behaviors of Language Models Under Uncertainty", "link_suffix": "/forum?id=tFwEsrx1hm", "link": "https://openreview.net/forum?id=tFwEsrx1hm", "pdf_link": "https://openreview.net/pdf?id=tFwEsrx1hm", "keywords": "Hallucinations, Epistemic uncertainty, Degenerate text, Fallback behaviors", "abstract": "Large language models (LLMs) often exhibit undesirable behaviors, such as hallucinations and sequence repetitions.\nWe propose to view these behaviors as fallbacks that models exhibit under epistemic uncertainty, and investigate the connection between them.\nWe categorize fallback behaviors \u2014 sequence repetitions, degenerate text, and hallucinations \u2014 and extensively analyze them in models from the same family that differ by the amount of pretraining tokens, parameter count, or the inclusion of instruction-following training.\nOur experiments reveal a clear and consistent ordering of fallback behaviors, across all these axes: \nthe more advanced an LLM is (i.e., trained on more tokens, has more parameters, or instruction-tuned), \nits fallback behavior shifts from sequence repetitions, to degenerate text, and then to hallucinations.\nMoreover, the same ordering is observed during the generation of a single sequence, even for the best-performing models; as uncertainty increases, models shift from generating hallucinations to producing degenerate text and finally sequence repetitions. \nLastly, we demonstrate that while common decoding techniques, such as random sampling, alleviate unwanted behaviors like sequence repetitions, they increase harder-to-detect hallucinations.", "title_embedding_index": 18910, "title_abs_embedding_index": 18935}, {"title": "Demystifying the Underappreciated Long-Tail Problems in Large Vision Language Models", "link_suffix": "/forum?id=9RnTw9YiXV", "link": "https://openreview.net/forum?id=9RnTw9YiXV", "pdf_link": "https://openreview.net/pdf?id=9RnTw9YiXV", "keywords": "LVLMs, Long-Tail Issue, Data Synthesis", "abstract": "Recently, Large Vision-Language Models (LVLMs) have made significant progress, seamlessly integrating the visual comprehension capabilities of vision encoders with the language generation strengths of language models (LMs). Despite the success of LVLMs, the training or aligning data of LVLMs suffers from the $\\textit{Long-Tail (LT)}$ problems, which is a special type of data with highly imbalanced distributions, and a large number of tail (minority) instances. A significant amount of research has focused on mitigating LT through data adjustment or network structure reorganization, however, efforts targeting generative LVLMs remain limited. In this paper, we present an in-depth analysis of the LT issues persisting in LVLMs' training data and build a distribution of four perspectives, addressing both visual and language aspects. To mitigate the aforementioned challenges, we propose an $\\textbf{A}$daptive $\\textbf{D}$ata $\\textbf{R}$efinement Framework ($\\textbf{ADR}$), which consists of two stages: $\\textbf{D}$ata $\\textbf{R}$ebalancing (DR) and $\\textbf{D}$ata $\\textbf{S}$ynthesis (DS). In the DR stage, we adaptively rebalance the redundant data based on entity distributions, while in the DS stage, we leverage the latent representations of scarce images to adaptively supplement the underrepresented portions. To validate the effectiveness of our approach, we conduct experiments on a series of comprehensive benchmarks, including the GPT-assisted evaluations to assess the overall performance variations introduced by our method. Through comprehensive evaluations, ADR effectively mitigates the long-tail problem in the training data, improving the average performance of LLaVA 1.5 relatively by $\\textbf{2.62%}$ across 10 benchmarks, without increasing the training data volume.", "title_embedding_index": 18911, "title_abs_embedding_index": 18936}, {"title": "Learning Physical Simulation with Message Passing Transformer", "link_suffix": "/forum?id=iiDioAxYah", "link": "https://openreview.net/forum?id=iiDioAxYah", "pdf_link": "https://openreview.net/pdf?id=iiDioAxYah", "keywords": "Physical Simulation, Graph Neural Network, Learned Simulation, Message Passing", "abstract": "Machine learning methods for physical simulation have achieved significant success in recent years. We propose a new universal architecture based on Graph Neural Network, the Message Passing Transformer, which incorporates a Message Passing framework, employs an Encoder-Processor-Decoder structure, and applies Graph Fourier Loss as loss function for model optimization. To take advantage of the past message passing state information, we propose Hadamard-Product Attention to update the node attribute in the Processor, Hadamard-Product Attention is a variant of Dot-Product Attention that focuses on more fine-grained semantics and emphasizes on assigning attention weights over each feature dimension rather than each position in the sequence relative to others. We further introduce Graph Fourier Loss (GFL) to balance high-energy and low-energy components. To improve time performance, we precompute the graph's Laplacian eigenvectors before the training process. Our architecture achieves significant accuracy improvements in long-term rollouts for both Lagrangian and Eulerian dynamical systems over current methods.", "title_embedding_index": 18912, "title_abs_embedding_index": 18937}, {"title": "ADAPT: Attentive Self-Distillation and Dual-Decoder Prediction Fusion for Continual Panoptic Segmentation", "link_suffix": "/forum?id=HF1UmIVv6a", "link": "https://openreview.net/forum?id=HF1UmIVv6a", "pdf_link": "https://openreview.net/pdf?id=HF1UmIVv6a", "keywords": "Continual Learning, Panoptic Segmentation, Knowledge Distillation", "abstract": "Panoptic segmentation, which unifies semantic and instance segmentation into a single task, has witnessed considerable success on predefined tasks. However, traditional methods tend to struggle with catastrophic forgetting and poor generalization when learning from a continuous stream of new tasks. Continual learning, emerged to tackle these challenges, has garnered increasing attention in recent years. Nonetheless, our study reveals that existing continual panoptic segmentation (CPS) methods often suffer from efficiency or scalability issues. To address these limitations, we propose a novel dual-decoder framework that incorporates attentive self-distillation and prediction fusion to efficiently preserve prior knowledge while facilitating model generalization. Specifically, we freeze the majority of model weights up to the pixel decoder, which is shared between the teacher and student models, thus enabling efficient knowledge distillation with only a single forward pass. Attentive self-distillation then adaptively distills useful knowledge from the old classes without distracting from non-object regions, which mitigates the inherent bias toward newly learned tasks. Additionally, query-level fusion (QLF) is devised to seamlessly integrate the output of the dual decoders without incurring scale inconsistency. Crucially, the computational overhead of our approach remains nearly constant, regardless of the number of continual learning steps or the number of classes introduced at each step. Our method achieves state-of-the-art performance on the ADE20K benchmark.", "title_embedding_index": 18913, "title_abs_embedding_index": 18938}, {"title": "DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models", "link_suffix": "/forum?id=ZyNEr7Xw5L", "link": "https://openreview.net/forum?id=ZyNEr7Xw5L", "pdf_link": "https://openreview.net/pdf?id=ZyNEr7Xw5L", "keywords": "Diffusion Models, Model Quantization, Model Compression, Efficient Models", "abstract": "Despite the widespread use of text-to-image diffusion models across various tasks, their computational and memory demands limit practical applications. \nTo mitigate this issue, quantization of diffusion models has been explored. It reduces memory usage and computational costs by compressing weights and activations into lower-bit formats. \nHowever, existing methods often struggle to preserve both image quality and text-image alignment, particularly in lower-bit($<$ 8bits) quantization.\nIn this paper, we analyze the challenges associated with quantizing text-to-image diffusion models from a distributional perspective. Our analysis reveals that activation outliers play a crucial role in determining image quality. \nAdditionally, we identify distinctive patterns in cross-attention scores, which significantly affects text-image alignment.\nTo address these challenges, we propose Distribution-aware Group Quantization (DGQ), a method that identifies and adaptively handles pixel-wise and channel-wise outliers to preserve image quality. Furthermore, DGQ applies prompt-specific logarithmic quantization scales to maintain text-image alignment. \nOur method demonstrates remarkable performance on datasets such as MS-COCO and PartiPrompts. We are the first to successfully achieve low-bit quantization of text-to-image diffusion models without requiring additional fine-tuning of weight quantization parameters.", "title_embedding_index": 18914, "title_abs_embedding_index": 18939}, {"title": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals", "link_suffix": "/forum?id=Io9yFt7XH7", "link": "https://openreview.net/forum?id=Io9yFt7XH7", "pdf_link": "https://openreview.net/pdf?id=Io9yFt7XH7", "keywords": "EEG, large language model, multi-task learning", "abstract": "Recent advancements for large-scale pre-training with neural signals such as electroencephalogram (EEG) have shown promising results, significantly boosting the development of brain-computer interfaces (BCIs) and healthcare. However, these pre-trained models often require full fine-tuning on each downstream task to achieve substantial improvements, limiting their versatility and usability, and leading to considerable resource wastage. To tackle these challenges, we propose NeuroLM, the first multi-task foundation model that leverages the capabilities of Large Language Models (LLMs) by regarding EEG signals as a foreign language, endowing the model with multi-task learning and inference capabilities. Our approach begins with learning a text-aligned neural tokenizer through vector-quantized temporal-frequency prediction, which encodes EEG signals into discrete neural tokens. These EEG tokens, generated by the frozen vector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG information via multi-channel autoregression. Consequently, NeuroLM can understand both EEG and language modalities. Finally, multi-task instruction tuning adapts NeuroLM to various downstream tasks. We are the first to demonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse EEG tasks within a single model through instruction tuning. The largest variant NeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and is pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG data. When evaluated on six diverse downstream datasets, NeuroLM showcases the huge potential of this multi-task learning paradigm. The codes and weights will be publicly available.", "title_embedding_index": 18915, "title_abs_embedding_index": 18940}, {"title": "N-ForGOT: Towards Not-forgetting and Generalization of Open Temporal Graph Learning", "link_suffix": "/forum?id=rLlDt2FQvz", "link": "https://openreview.net/forum?id=rLlDt2FQvz", "pdf_link": "https://openreview.net/pdf?id=rLlDt2FQvz", "keywords": "temporal graph neural networks; continual learning; generalization", "abstract": "Temporal Graph Neural Networks (TGNNs) lay emphasis on capturing node interactions over time but often overlook evolution in node classes and dynamic data distributions triggered by the continuous emergence of new class labels, known as the open-set problem. This problem poses challenges for existing TGNNs in preserving learned classes while rapidly adapting to new, unseen classes. To address this, this paper identifies two primary factors affecting model performance on the open temporal graph, backed by a theoretical guarantee:  (1) the forgetting of prior knowledge and (2) distribution discrepancies between successive tasks. Building on these insights, we propose N-ForGOT, which incorporates two plug-in modules into TGNNs to preserve prior knowledge and enhance model generalizability for new classes simultaneously. The first module preserves previously established inter-class connectivity and decision boundaries during the training of new classes to mitigate the forgetting caused by temporal evolutions of class characteristics. The second module introduces an efficient method for measuring distribution discrepancies with designed temporal Weisfeiler-Lehman subtree patterns, effectively addressing both structural and temporal shifts while reducing time complexity. Experimental results on four public datasets demonstrate that our method significantly outperforms state-of-the-art approaches in prediction accuracy, prevention of forgetting, and generalizability.", "title_embedding_index": 18916, "title_abs_embedding_index": 18941}, {"title": "UniHDA: A Unified and Versatile Framework for Generalized Hybrid Domain Adaptation", "link_suffix": "/forum?id=6D30aOdh2U", "link": "https://openreview.net/forum?id=6D30aOdh2U", "pdf_link": "https://openreview.net/pdf?id=6D30aOdh2U", "keywords": "Generative Domain Adaptation; Image Generation; 3D Generation", "abstract": "Recently, generative domain adaptation has achieved remarkable progress, enabling us to adapt a pre-trained generator to a new target domain. However, existing methods are limited to a single target domain and single modality, either text-driven or image-driven. In this paper, we explore a novel task -- $\\textit{Generalized Hybrid Domain Adaptation}$. Compared with conventional generative domain adaptation, it provides greater flexibility to adapt the generator to the hybrid of multiple target domains, with multi-modal references including one-shot image and zero-shot text prompt. Meanwhile, it is more challenging to represent the composition of multi-modal target domains and preserve the characteristics from the source domain. To address these issues, we propose UniHDA, a $\\textbf{unified}$ and $\\textbf{versatile}$ framework for generalized hybrid domain adaptation. Drawing inspiration from the interpolable latent space of StyleGAN, we find that a linear interpolation between domain shifts in CLIP\u2019s embedding space can also uncover favorable compositional capabilities for the adaptation. In light of this finding, we linearly interpolate the domain shifts from multiple target domains to achieve hybrid domain adaptation. To enhance $\\textbf{consistency}$ with the source domain, we further propose a novel cross-domain spatial structure (CSS) loss that maintains the detailed spatial structure between the source and target generator. Experiments show the adapted generator can synthesize realistic images with various attribute compositions and maintain robust consistency with the source domain. Additionally, UniHDA is generator-agnostic and versatile to multiple generators, e.g., StyleGAN, EG3D, and video generators.", "title_embedding_index": 18917, "title_abs_embedding_index": 18942}, {"title": "General OCR Theory:  Towards OCR-2.0 via a Unified End-to-end Model", "link_suffix": "/forum?id=3LOcwfB4JX", "link": "https://openreview.net/forum?id=3LOcwfB4JX", "pdf_link": "https://openreview.net/pdf?id=3LOcwfB4JX", "keywords": "OCR, LVLM, Multimodal", "abstract": "Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as \"characters\" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder.  As an OCR-2.0 model, GOT can handle all the above \"characters\" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality.  In experiments, we provide sufficient results to prove the superiority of our model.", "title_embedding_index": 18918, "title_abs_embedding_index": 18943}, {"title": "House of Cards: Massive Weights in LLMs", "link_suffix": "/forum?id=LvuSFvGShf", "link": "https://openreview.net/forum?id=LvuSFvGShf", "pdf_link": "https://openreview.net/pdf?id=LvuSFvGShf", "keywords": "large language model, massive weight, massive activation", "abstract": "Massive activations, which manifest in specific feature dimensions of hidden states, introduce a significant bias in large language models (LLMs), leading to an overemphasis on the corresponding token. In this paper, we identify that massive activations originate not from the hidden state but from the intermediate state of a feed-forward network module in an early layer. Expanding on the previous observation that massive activations occur only in specific feature dimensions, we dive deep into the weights that cause massive activations. Specifically, we definetop-$k$ massive weightsas the weights that contribute to the dimensions with the top-$k$ magnitudes in the intermediate state. When these massive weights are set to zero, the functionality of LLMs is entirely disrupted. However, when all weights except for massive weights are set to zero, it results in a relatively minor performance drop, even though a much larger number of weights are set to zero. This implies that during the pre-training process, learning is dominantly focused on massive weights. Building on this observation, we propose a simple plug-and-play method called MacDrop (massive weights curriculum dropout), to rely less on massive weights during parameter-efficient fine-tuning. This method applies dropout to the pre-trained massive weights, starting with a high dropout probability and gradually decreasing it as fine-tuning progresses. Through experiments, we demonstrate that MacDrop generally improves performance across zero-shot downstream tasks and generation tasks.", "title_embedding_index": 18919, "title_abs_embedding_index": 18944}, {"title": "Bi-modality medical images synthesis by a bi-directional discrete process matching method", "link_suffix": "/forum?id=GqsepTIXWy", "link": "https://openreview.net/forum?id=GqsepTIXWy", "pdf_link": "https://openreview.net/pdf?id=GqsepTIXWy", "keywords": "Bi-modality Images, Medical Image Synthesis, Flow-based Model", "abstract": "Recently, medical image synthesis gains more and more popularity, along with the rapid development of generative models. Medical image synthesis aims to generate an unacquired image modality, often from other observed data modalities. Synthesized images can be  used for clinical diagnostic assistance, data augmentation for  model training and validation or image quality improving. In the meanwhile, the flow-based models are among the successful generative models for the ability of generating  realistic and high-quality synthetic images. However, most flow-based models require to  calculate flow ordinary different equation (ODE) evolution steps in synthesis process, for which the performances are significantly limited by heavy computation time due to a large number of time iterations. In this paper, we propose a novel flow-based model, namely bi-directional Discrete Process Matching (Bi-DPM) to accomplish the bi-modality image synthesis tasks. Different to other flow matching based models,  we propose to utilize both forward and backward ODE flows and enhance the consistency on the intermediate images over a few discrete time steps, resulting in a synthesis process maintaining  high-quality generations for both modalities under the guidance of paired data. Our experiments on three datasets of MRI T1/T2 and CT/MRI demonstrate that Bi-DPM outperforms other state-of-the-art flow-based methods for bi-modality image synthesis, delivering higher image quality with accurate anatomical regions.", "title_embedding_index": 18920, "title_abs_embedding_index": 18945}, {"title": "Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control", "link_suffix": "/forum?id=S7cWJkWqOi", "link": "https://openreview.net/forum?id=S7cWJkWqOi", "pdf_link": "https://openreview.net/pdf?id=S7cWJkWqOi", "keywords": "3D talking face, generative model, fine-grained control, action units", "abstract": "Speech-driven 3D talking face method should offer both accurate lip synchronization and controllable expressions. Previous methods solely adopt discrete emotion labels to globally control expressions throughout sequences while limiting flexible fine-grained facial control within the spatiotemporal domain. We propose a diffusion-transformer-based 3D talking face generation model, Cafe-Talk, which simultaneously incorporates coarse- and fine-grained multimodal control conditions. Nevertheless, the entanglement of multiple conditions challenges achieving satisfying performance. To disentangle speech audio and fine-grained conditions, we employ a two-stage training pipeline. Specifically, Cafe-Talk is initially trained using only speech audio and coarse-grained conditions. Then, a proposed fine-grained control adapter gradually adds fine-grained instructions represented by action units (AUs), preventing unfavorable speech-lip synchronization. To disentangle coarse- and fine-grained conditions, we design a swap-label training mechanism, which enables the dominance of the fine-grained conditions. We also devise a mask-based CFG technique to regulate the occurrence and intensity of fine-grained control. In addition, a text-based detector is introduced with text-AU alignment to enable natural language user input and further support multimodal control. Extensive experimental results prove that Cafe-Talk achieves state-of-the-art lip synchronization and expressiveness performance and receives wide acceptance in fine-grained control in user studies.", "title_embedding_index": 18921, "title_abs_embedding_index": 18946}, {"title": "Real&Synthetic Dataset and the Linear Attention in Image Restoration", "link_suffix": "/forum?id=AKMOrcobBE", "link": "https://openreview.net/forum?id=AKMOrcobBE", "pdf_link": "https://openreview.net/pdf?id=AKMOrcobBE", "keywords": "Image Restoration, Vision-RWKV", "abstract": "Image restoration (IR), which aims to recover high-quality images from degraded inputs, is a crucial task in modern image processing. Recent advancements in deep learning, particularly with Convolutional Neural Networks (CNNs) and Transformers, have significantly improved image restoration performance. However, existing methods lack a unified training benchmark that specifies the training iterations and configurations. Additionally, we construct an image complexity evaluation metric using the gray-level co-occurrence matrix (GLCM) and find that there exists a bias between the image complexity distributions of commonly used IR training and testing datasets, leading to suboptimal restoration results. Therefore, we construct a new large-scale IR dataset called ReSyn, that utilizes a novel image filtering method based on image complexity to achieve a balanced image complexity distribution, and contains both real and AIGC synthetic images. From the perspective of measuring the model's convergence ability and restoration capability, we construct a unified training standard that specifies the training iterations and configurations for image restoration models. Furthermore, we explore how to enhance the performance of transformer-based image restoration models based on linear attention mechanism. We propose RWKV-IR, a novel image restoration model that incorporates the linear complexity RWKV into the transformer-based image restoration structure, and enables both global and local receptive fields. Instead of directly integrating the Vision-RWKV into the transformer architecture, we replace the original Q-Shift in RWKV with a novel Depth-wise Convolution shift, which effectively models the local dependencies, and is further combined with Bi-directional attention to achieve both global and local aware linear attention. Moreover, we propose a Cross-Bi-WKV module that combines two Bi-WKV modules with different scanning orders to achieve a balanced attention for horizontal and vertical directions. Extensive experiments demonstrate the effectiveness and competitive performance of our RWKV-IR model.", "title_embedding_index": 18922, "title_abs_embedding_index": 18947}, {"title": "Adapting Informative Structures for Cross-Domain Few-Shot Segmentation", "link_suffix": "/forum?id=s2mEKEKplI", "link": "https://openreview.net/forum?id=s2mEKEKplI", "pdf_link": "https://openreview.net/pdf?id=s2mEKEKplI", "keywords": "cross-domain few-shot segmentation, few-shot segmentation, test-time training", "abstract": "Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes under domain shifts, using only a few mask-annotated support samples. However, directly applying pretrained CD-FSS models to unseen domains is often suboptimal due to their limited coverage of domain diversity by fixed parameters trained on source domains. Moreover, simply adjusting hand-selected model parameters, such as test-time training, typically neglects the distinct domain gaps and characteristics of target domains. To address these issues, we propose adapting informative model structures for target domains by learning domain characteristics from few-shot labeled support samples during inference. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. Our method selectively and gradually adapts the model to target domains, optimizing model adaptation, minimizing overfitting risks, and maximizing the use of limited support data. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing few-shot segmentation models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.", "title_embedding_index": 18923, "title_abs_embedding_index": 18948}, {"title": "OpenMeta: A Comprehensive Multi-Task Benchmark for Metagenomics Understanding", "link_suffix": "/forum?id=PN3i4b6NED", "link": "https://openreview.net/forum?id=PN3i4b6NED", "pdf_link": "https://openreview.net/pdf?id=PN3i4b6NED", "keywords": "Metagenomics, DNA, Pre-Trained Language Model, Transformer, Benchmark", "abstract": "Metagenomics is essential for exploring the vast diversity and intricate interactions of microbes that impact health, agriculture, and environmental sciences. Despite the surge of machine learning-based metagenomic models addressing these questions, evaluating their respective benefits is challenging due to the use of distinct, experimental datasets, partly contrived, and varying model performance across different tasks. To this end, we introduce OpenMeta, the first comprehensive benchmark tailored for metagenomic function prediction, which integrates diverse datasets ranging from 1,000 to 213,000 sequences and incorporates hierarchical data. We highlight the inadequacies of current genomic models and the superior performance of metagenomic pre-trained models for handling complex metagenomic data. Furthermore, we identify a critical research gap: the lack of unified models that process both sequence and hierarchical data. Addressing this could significantly advance metagenomic analyses. OpenMeta sets a new standard for metagenomic analysis, offering insights that could enhance the understanding and application of microbial ecology in biotechnology and environmental science.", "title_embedding_index": 18924, "title_abs_embedding_index": 18949}]