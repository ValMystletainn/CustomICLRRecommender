[{"title": "Magic Insert: Style-Aware Drag-and-Drop", "link_suffix": "/forum?id=fGTtEG24lA", "link": "https://openreview.net/forum?id=fGTtEG24lA", "pdf_link": "https://openreview.net/pdf?id=fGTtEG24lA", "keywords": "generative models, diffusion models, editing, personalization, style", "abstract": "We present Magic Insert, a method for dragging-and-dropping subjects from a user-provided image into a target image of a different style in a physically plausible manner while matching the style of the target image. This work formalizes the problem of style-aware drag-and-drop and presents a method for tackling it by addressing two sub-problems: style-aware personalization and realistic object insertion in stylized images. For style-aware personalization, our method first fine-tunes a pretrained text-to-image diffusion model using LoRA and learned text tokens on the subject image, and then infuses it with a CLIP representation of the target style. For object insertion, we use Bootstrapped Domain Adaption to adapt a domain-specific photorealistic object insertion model to the domain of diverse artistic styles. Overall, the method significantly outperforms traditional approaches such as inpainting. Finally, we present a dataset, SubjectPlop, to facilitate evaluation and future progress in this area.", "title_embedding_index": 18350, "title_abs_embedding_index": 18375}, {"title": "Towards Efficient and Accurate Identification of Memorization in Deep Models", "link_suffix": "/forum?id=bCi3Jz0q02", "link": "https://openreview.net/forum?id=bCi3Jz0q02", "pdf_link": "https://openreview.net/pdf?id=bCi3Jz0q02", "keywords": "memorization", "abstract": "\\textit{Memorization} is the ability of deep models to learn verbatim arbitrary inputs from the training data. One of the most popular means of calculating memorization scores (i.e., the probability that a point is memorized) is via the pseudo Leave-One-Out (pLOO) method proposed by~\\citet{feldman2020longtail}. However, this technique suffers from two shortcomings: it is computationally prohibitive (as it requires training thousands of models) and it produces inaccurate scores. The goal of this work is to overcome both these limitations simultaneously. To do so, we take the following approach: \\textbf{First}, we demonstrate that the major source of pLOO's computation bottleneck is its execution on the entire dataset, not just the memorized points. We find running pLOO on all the points is unnecessary since most of them are not even memorized. \\textbf{Second}, we develop a simple proxy to identify the memorized points without having to run pLOO in the first place. To do so, we study the model training cycle and find that memorized points are learned towards the last iterations. We build a simple proxy based on this observation and find that our proxy: \\textit{a)} is strongly correlated with the actual memorization scores (Pearson score $<-0.95$) across all our models and datasets and \\textit{b)} requires only a single model (instead of the thousands needed by pLOO). However, our proxy does not provide the exact memorization scores. \\textbf{Third}, to calculate these, we incorporate our proxy into the pLOO method, resulting in pLOO\\textsubscript{\\textit{improved}}. In doing so, we show that our pLOO\\textsubscript{\\textit{improved}} reduces both computational overhead (by over 90%) and the error in the approximated memorization scores (by over 65%). Therefore, our work makes it possible to study memorization in large datasets and real-world models while requiring only a fraction of the computational resources.", "title_embedding_index": 18351, "title_abs_embedding_index": 18376}, {"title": "Back to Fundamentals: Re-Examining Memorization in Deep Learning Models", "link_suffix": "/forum?id=u9Z6gL5MlL", "link": "https://openreview.net/forum?id=u9Z6gL5MlL", "pdf_link": "https://openreview.net/pdf?id=u9Z6gL5MlL", "keywords": "Memorization", "abstract": "Memorization is the ability of deep learning models to assign arbitrary ground truth labels to inputs in the dataset. Due to the computational difficulty of identifying existing memorized points, researchers often induce artificial memorization i.e, force the model to memorize the newly introduced points (via Noisy Label or Noisy Input). However, in this work, we show that this artificial \\textit{proxy} exhibits fundamentally different characteristics than the memorization real points (or natural memorization). To demonstrate this deviation, we re-examine two key findings derived from artificial memorization and compare them against natural memorization i.e., over-parametrization and increased training time increases memorization. We show that both these factors have the opposite effect i.e., they reduce natural memorization. Since real world models suffer from natural memorization (instead of the artificial one) our findings suggest the research community should focus on natural memorization, instead of the artificial proxy.", "title_embedding_index": 18352, "title_abs_embedding_index": 18377}, {"title": "Boosting Membership Inference Attacks with Upstream Modification", "link_suffix": "/forum?id=nAK26c8s9X", "link": "https://openreview.net/forum?id=nAK26c8s9X", "pdf_link": "https://openreview.net/pdf?id=nAK26c8s9X", "keywords": "Membership inference attacks", "abstract": "Membership Inference Attacks (MIAs) are designed to quantify the privacy leakage of machine learning models. However, even the state-of-the-art attacks still perform poorly under the low false positive regime (at times, nearing random guessing). To overcome this weakness, we modify two limitations, in the  initial/upstream stages of the MIA framework, namely sampling bias (i.e., too many points dropped during sampling) and attack aggregation (i.e., average attack results over all the data points instead of only the most vulnerable ones). Our improvements carryover downstream and boost attack accuracy of existing MIAs by \\textit{increasing the TPR of existing attacks at incredibly low FPRs (as low as zero) while achieving a near-perfect AUC}. As a consequence, our modifications enable the practical and effective application of MIAs for privacy assessment in machine learning models.", "title_embedding_index": 18353, "title_abs_embedding_index": 18378}, {"title": "Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness", "link_suffix": "/forum?id=kjmLabjSE2", "link": "https://openreview.net/forum?id=kjmLabjSE2", "pdf_link": "https://openreview.net/pdf?id=kjmLabjSE2", "keywords": "privacy, differential privacy, hidden state SGD, nonconvex, nonsmooth", "abstract": "We study the Differential Privacy (DP) guarantee of hidden-state Noisy-SGD algorithms over a bounded domain. Standard privacy analysis for Noisy-SGD assumes all internal states are revealed, which leads to a divergent R'enyi DP bound with respect to the number of iterations. Ye & Shokri (2022) and Altschuler & Talwar (2022) proved convergent bounds for smooth (strongly) convex losses, and raise open questions about whether these assumptions can be relaxed. We provide positive answers by proving convergent R'enyi DP bound for non-convex non-smooth losses, where we show that requiring losses to have H\"older continuous gradient is sufficient. We also provide a strictly better privacy bound compared to state-of-the-art results for smooth strongly convex losses. Our analysis relies on the improvement of shifted divergence analysis in multiple aspects, including forward Wasserstein distance tracking, identifying the optimal shifts allocation, and the  H\"older reduction lemma. Our results further elucidate the benefit of hidden-state analysis for DP and its applicability.", "title_embedding_index": 18354, "title_abs_embedding_index": 18379}, {"title": "MapEval: A Map-Based Evaluation of Geo-Spatial Reasoning in Foundation Models", "link_suffix": "/forum?id=nnAPWDt4hn", "link": "https://openreview.net/forum?id=nnAPWDt4hn", "pdf_link": "https://openreview.net/pdf?id=nnAPWDt4hn", "keywords": "MapLLM, Map Query, Geo-Spatial Question Answering, Geo-Spatial Reasoning, Location Based Service, Google Maps", "abstract": "Recent advancements in foundation models have enhanced AI systems' capabilities in autonomous tool usage and reasoning.\nHowever, their ability in location or map-based reasoning - which improves daily life by optimizing navigation, facilitating resource discovery, and streamlining logistics - has not been systematically studied. To bridge this gap, we introduce MapEval, a benchmark designed to assess diverse and complex map-based user queries with geo-spatial reasoning. MapEval features three task types (textual, API-based, and visual) that require collecting world information via map tools, processing heterogeneous geo-spatial contexts (e.g., named entities, travel distances, user reviews or ratings, images), and compositional reasoning, which all state-of-the-art foundation models find challenging. Comprising 550+ unique multiple-choice questions about locations across 138 cities and 54 countries, MapEval evaluates foundation models' ability to handle spatial relationships, map infographics, travel planning, and navigation challenges. Using MapEval, we conducted a comprehensive evaluation of 25 prominent foundation models. While no single model excelled across all tasks, Claude-3.5-Sonnet, GPT-4o, and Gemini-1.5-Pro achieved competitive performance overall. However, substantial performance gaps emerged, particularly in MapEval, where agents with Claude-3.5-Sonnet outperformed GPT-4o and Gemini-1.5-Pro by 13% and 22%, respectively, and the gaps became even more amplified when compared to open-source LLMs. Our in-depth ablations and analyses provide insights strengths and weaknesses of current models, though all models still fall short of human performance by more than 20% on average, struggling with complex map images and rigorous geo-spatial reasoning. This gap highlights MapEval's critical role in advancing general-purpose foundation models with stronger geo-spatial understanding.", "title_embedding_index": 18355, "title_abs_embedding_index": 18380}, {"title": "Semialgebraic Neural Networks: From roots to representations", "link_suffix": "/forum?id=zboCXnuNv7", "link": "https://openreview.net/forum?id=zboCXnuNv7", "pdf_link": "https://openreview.net/pdf?id=zboCXnuNv7", "keywords": "deep learning, semialgebraic functions, homotopy continuation, real algebraic geometry, recurrent neural networks", "abstract": "Many numerical algorithms in scientific computing---particularly in areas like numerical linear algebra, PDE simulation, and inverse problems---produce outputs that can be represented by semialgebraic functions; that is, the graph of the computed function can be described by finitely many polynomial equalities and inequalities. \n    In this work, we introduce Semialgebraic Neural Networks (SANNs), a neural network architecture capable of exactly computing any bounded semialgebraic function up to the accuracy of a numerical ODE solver chosen by the programmer.\n    Conceptually, we encode the graph of the learned function as the kernel of a piecewise polynomial selected from a class of functions whose roots can be evaluated using a particular homotopy continuation method.\n    We show by construction that the SANN architecture is able execute this continuation method on each connected component of the target function, thus evaluating the learned semialgebraic function.\n    Furthermore, the architecture can exactly compute even discontinuous semialgebraic functions in a natural way.\n    Lastly, we present a general formulation for optimization problems whose solution operators are representable by SANNs, and we demonstrate particular applications to nonlinear inverse problems and deep learning hypernetworks.", "title_embedding_index": 18356, "title_abs_embedding_index": 18381}, {"title": "OPTAMI: Global Superlinear Convergence of High-order Methods", "link_suffix": "/forum?id=Cpr6Wv2tfr", "link": "https://openreview.net/forum?id=Cpr6Wv2tfr", "pdf_link": "https://openreview.net/pdf?id=Cpr6Wv2tfr", "keywords": "second-order optimization, high-order optimization, cubic regularised Newton method, Newton method, convex optimization", "abstract": "Second-order methods for convex optimization outperform first-order methods in terms of theoretical iteration convergence, achieving rates up to $O(k^{-5})$ for highly-smooth functions. However, their practical performance and applications are limited due to their multi-level structure and implementation complexity. In this paper, we present new results on high-order optimization methods, supported by their practical performance. First, we show that the basic high-order methods, such as the Cubic Regularized Newton Method, exhibit global superlinear convergence for $\\mu$-strongly star-convex functions, a class that includes $\\mu$-strongly convex functions and some non-convex functions. Theoretical convergence results are both inspired and supported by the practical performance of these methods. Secondly, we propose a practical version of the Nesterov Accelerated Tensor method, called NATA. It significantly outperforms the classical variant and other high-order acceleration techniques in practice. The convergence of NATA is also supported by theoretical results. Finally, we introduce an open-source computational library for high-order methods, called OPTAMI. This library includes various methods, acceleration techniques, and subproblem solvers, all implemented as PyTorch optimizers, thereby facilitating the practical application of high-order methods to a wide range of optimization problems. We hope this library will simplify research and practical comparison of methods beyond first-order.", "title_embedding_index": 18357, "title_abs_embedding_index": 18382}, {"title": "The Representation Geometry of Features and Hierarchy in Large Language Models", "link_suffix": "/forum?id=bVTM2QKYuA", "link": "https://openreview.net/forum?id=bVTM2QKYuA", "pdf_link": "https://openreview.net/pdf?id=bVTM2QKYuA", "keywords": "categorical concepts, hierarchical concepts, linear representation hypothesis, causal inner product, interpretability", "abstract": "The linear representation hypothesis is the informal idea that semantic concepts are encoded as linear directions in the representation spaces of large language models (LLMs). Previous work has shown how to make this notion precise for representing binary concepts that have natural contrasts (e.g., {male, female}) asdirectionsin representation space. However, many natural concepts do not have natural contrasts (e.g., whether the output is about an animal). In this work, we show how to extend the formalization of the linear representation hypothesis to represent features (e.g., is_animal) asvectors. This allows us to immediately formalize the representation of categorical concepts as polytopes in the representation space. Further, we use the formalization to prove a relationship between the hierarchical structure of concepts and the geometry of their representations. We validate these theoretical results on the Gemma and LLaMA-3 large language models, estimating representations for 900+ hierarchically related concepts using data from WordNet.", "title_embedding_index": 18358, "title_abs_embedding_index": 18383}, {"title": "On the Almost Sure Convergence of the Stochastic Three Points Algorithm", "link_suffix": "/forum?id=N8tJmhCw25", "link": "https://openreview.net/forum?id=N8tJmhCw25", "pdf_link": "https://openreview.net/pdf?id=N8tJmhCw25", "keywords": "Zeroth Order Optimization, Almost sure convergence, Stochastic Three Points Algorithm.", "abstract": "The stochastic three points (STP) algorithm is a derivative-free optimization technique designed for unconstrained optimization problems in $\\mathbb{R}^d$. In this paper, we analyze this algorithm for three classes of functions : smooth functions that may lack convexity, smooth convex functions, and smooth functions that are strongly convex. Our work provides the first almost sure convergence results of the STP algorithm, alongside some convergence results in expectation.\nFor the class of smooth functions, we establish that the best gradient iterate of the STP algorithm converges almost surely to zero at a rate arbitrarily close to $o(\\frac{1}{\\sqrt{T}})$, where $T$ is the number of iterations. Furthermore, within the same class of functions, we establish both almost sure convergence and convergence in expectation of the final gradient iterate towards zero.\nFor the class of smooth convex functions, we establish that $f(\\theta^T)$ converges to $\\inf_{\\theta \\in \\mathbb{R}^d} f(\\theta)$ almost surely at a rate arbitrarily close to $o(\\frac{1}{T})$, and in expectation at a rate of $O(\\frac{d}{T})$ where $d$ is the dimension of the space.\nFinally, for the class of smooth functions that are strongly convex, we establish that when step sizes are obtained by approximating the directional derivatives of the function, $f(\\theta^T)$ converges to $\\inf_{\\theta \\in \\mathbb{R}^d} f(\\theta)$ in expectation at a rate of $O((1-\\frac{\\mu}{dL})^T)$, and almost surely at a rate arbitrarily close to $o((1-\\frac{\\mu}{dL})^T)$,  where $\\mu$ and $L$\nare  the strong convexity and smoothness parameters of the function.", "title_embedding_index": 18359, "title_abs_embedding_index": 18384}, {"title": "ROMEO: ROBUST METRIC VISUAL ODOMETRY", "link_suffix": "/forum?id=SHeVc7efFz", "link": "https://openreview.net/forum?id=SHeVc7efFz", "pdf_link": "https://openreview.net/pdf?id=SHeVc7efFz", "keywords": "Visual odometry\uff0cmonocular\uff0cSLAM", "abstract": "Visual odometry (VO) aims to estimate camera poses from visual inputs --- the key for many applications such as VR/AR, robotics etc. This work focuses on monocular RGB VO where camera poses are directly estimated from a monocular RGB video without IMU or 3D sensors. Existing approaches lack robustness under this challenging scenario and fail to generalize to unseen data (especially outdoors); they also cannot recover metric-scale poses. Several methods have attempted to address these problems with priors from predicted depth. However, especially on unseen data, depth prediction noise can drastically degrade performance. We propose Robust Metric Visual Odometry (RoMeO), the first method that can leverage (noisy) depth priors to enable robust VO and recover metric scale poses. RoMeO incorporates both pre-trained monocular metric depth and multi-view stereo (MVS) models to recover metric-scale, simplify correspondence search, provide better initialization and regularize optimization. Effective strategies ensure the efficiency and the robustness to prior noise. RoMeO advances the state-of-the-art (SOTA) by a large margin across 6 diverse datasets covering both indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO reduces the relative (align the trajectory scale with GT) and absolute trajectory errors on average by 55.2% and 77.8% respectively (Fig.1). The performance gain also transfers to the full SLAM pipeline (with global BA & loop closure). Code will be released upon acceptance.", "title_embedding_index": 18360, "title_abs_embedding_index": 18385}, {"title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models", "link_suffix": "/forum?id=cRR0oDFEBC", "link": "https://openreview.net/forum?id=cRR0oDFEBC", "pdf_link": "https://openreview.net/pdf?id=cRR0oDFEBC", "keywords": "Instruction Following, Large Language Models, Execution Feedback, On-policy Learning, Strong-to-Weak Distillation, Self-Alignment", "abstract": "One core capability of large language models~(LLMs) is to follow natural language instructions. However, the issue of automatically constructing high-quality training data to enhance the complex instruction-following abilities of LLMs without manual annotation remains unresolved. In this paper, we introduce AutoIF, the first scalable and reliable method for automatically generating instruction-following training data. AutoIF transforms the validation of instruction-following data quality into code verification, requiring LLMs to generate instructions, the corresponding code to verify the correctness of the instruction responses, and unit test samples to cross-validate the code's correctness. Then, execution feedback-based rejection sampling can generate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF) training. AutoIF achieves significant improvements across three training algorithms, SFT, Offline DPO, and Online DPO, when applied to the advanced open-source LLMs, Qwen2 and LLaMA3, in self-alignment and strong-to-weak distillation settings. Using two widely-used and three challenging general instruction-following benchmarks, we demonstrate that AutoIF significantly improves LLM performance across a wide range of natural instruction constraints. Notably, AutoIF is the first to surpass 90% accuracy in IFEval\u2019s loose instruction accuracy, without compromising general, math and coding capabilities. Further analysis of quality, scaling, combination, and data efficiency highlights AutoIF's strong generalization and alignment potential.", "title_embedding_index": 18361, "title_abs_embedding_index": 18386}, {"title": "MotifDisco: Motif Causal Discovery For Time Series Motifs", "link_suffix": "/forum?id=mrNVOWlG25", "link": "https://openreview.net/forum?id=mrNVOWlG25", "pdf_link": "https://openreview.net/pdf?id=mrNVOWlG25", "keywords": "motif, causality, causal discovery, time series, graph neural network, glucose", "abstract": "Many time series, particularly health data streams, can be best understood as a sequence of phenomenon or events, which we call motifs. A time series motif is a short trace segment which may implicitly capture an underlying phenomenon within the time series. Specifically, we focus on glucose traces collected from continuous glucose monitors (CGMs), which inherently contain motifs representing underlying human behaviors such as eating and exercise. The ability to identify and quantify causal relationships amongst motifs can provide a mechanism to better understand and represent these patterns, useful for improving deep learning and generative models and for advanced technology development (e.g., personalized coaching and artificial insulin delivery systems). However, no previous work has developed causal discovery methods for time series motifs. Therefore, in this paper we develop MotifDisco (motif disco-very of causality), a novel causal discovery framework to learn causal relations amongst motifs from time series traces. We formalize a notion of Motif Causality (MC), inspired from Granger Causality and Transfer Entropy, and develop a Graph Neural Network-based framework that learns causality between motifs by solving an unsupervised link prediction problem. We also integrate MC with three model use cases of forecasting, anomaly detection and clustering, to showcase the use of MC as a building block for other downstream tasks. Finally, we evaluate our framework and find that Motif Causality provides a significant performance improvement in all use cases.", "title_embedding_index": 18362, "title_abs_embedding_index": 18387}, {"title": "ContraSim: Contrastive Similarity Space Learning for Financial Market Predictions", "link_suffix": "/forum?id=GfuJR76Sfo", "link": "https://openreview.net/forum?id=GfuJR76Sfo", "pdf_link": "https://openreview.net/pdf?id=GfuJR76Sfo", "keywords": "Learning Representations, Large Language Models, Financial Forecasting", "abstract": "We introduce the Contrastive Similarity Space (ContraSim) paradigm that is able to form global semantic understanding between how daily financial headlines can affect market movement. ContraSim consists of two steps. 1) Weighted Headline Augmentation: We propose a method of augmenting financial headlines to create new headlines with known semantic distances to the original. 2) Weighted-Self Supervised Contrastive Learning (WSSCL): An extension of classical binary contrastive learning algorithms, WSSCL leverages the known distances between anchor and augmented prompts to generate finely grained embedding space that optimizes for similar news to be clumped together. We measure how well ContraSim is able to learn global financial information by parsing whether or not it inherently groups newslines of homogeneous market movement directions together, using a novel information density metric Info-kNN. We find that incorporating features from ContraSim into financial forecasting tasks has a 7% increase in classification accuracy. Additionally, we highlight that ContraSim can be used to find historic news-days that most resemble pertinent financial headlines of the day to help analysts to make better decisions for predicting market movement.", "title_embedding_index": 18363, "title_abs_embedding_index": 18388}, {"title": "Metric-Driven Attributions for Vision Transformers", "link_suffix": "/forum?id=rGP2jbWt0l", "link": "https://openreview.net/forum?id=rGP2jbWt0l", "pdf_link": "https://openreview.net/pdf?id=rGP2jbWt0l", "keywords": "Interpretable and Explainable AI, Computer Vision, Vision Transformer", "abstract": "Attribution algorithms explain computer vision models by attributing the model response to pixels within the input. Existing attribution methods generate explanations by combining transformations of internal model representations such as class activation maps, gradients, attention, or relevance scores. The effectiveness of an attribution map is measured using attribution quality metrics. This leads us to pose the following question: if attribution methods are assessed using attribution quality metrics, why are the metrics not used to generate the attributions? In response to this question, we propose a Metric-Driven Attribution for explaining Vision Transformers (ViT) called MDA. Guided by attribution quality metrics, the method creates attribution maps by performing patch order and patch magnitude optimization across all patch tokens. The first step orders the patches in terms of importance and the second step assigns the magnitude to each patch while preserving the patch order. Moreover, MDA can provide a smooth trade-off between sparse and dense attributions by modifying the optimization objective. Experimental evaluation demonstrates the proposed MDA method outperforms $7$ existing ViT attribution methods by an average of $25%$ across $6$ attribution metrics on the ImageNet dataset for the ViT-base $16 \\times 16$, ViT-tiny $16 \\times 16$, and ViT-base $32 \\times 32$ models.", "title_embedding_index": 18364, "title_abs_embedding_index": 18389}, {"title": "Remember, Retrieve and Generate: Understanding Infinite Visual Concepts as Your Personalized Assistant", "link_suffix": "/forum?id=Xy5iXnFNzL", "link": "https://openreview.net/forum?id=Xy5iXnFNzL", "pdf_link": "https://openreview.net/pdf?id=Xy5iXnFNzL", "keywords": "Multimodal Large Language Models, Personalization, Retrieval-augmented Generation", "abstract": "The development of large language models (LLMs) has significantly enhanced the capabilities of multimodal LLMs (MLLMs) as general assistants. However, lack of user-specific knowledge still restricts their application in human's daily life. In this paper, we introduce theRetrievalAugmentedPersonalization (RAP) framework for MLLMs' personalization. Starting from a general MLLM, we turn it into a personalized assistant in three steps. (a) Remember: We design a key-value database to store user-related information,e.g., user's name, avatar and other attributes. (b) Retrieve: When the user initiates a conversation, RAP will retrieve relevant information from the database using a multimodal retriever. (c) Generate: The input query and retrieved concepts' information are fed into MLLMs to generate personalized, knowledge-augmented responses. Unlike previous methods, RAP allows real-time concept editing via updating the external database. To further improve generation quality and alignment with user-specific information, we design a pipeline for data collection and create a specialized dataset for personalized training of MLLMs. Based on the dataset, we train a series of MLLMs as personalized multimodal assistants. By pretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visual concepts without additional finetuning. Our models demonstrate outstanding flexibility and generation quality across a variety of tasks, such as personalized image captioning, question answering and visual recognition. The code, data and models will be available.", "title_embedding_index": 18365, "title_abs_embedding_index": 18390}, {"title": "Decentralizing Test-time Adaptation under Heterogeneous Data Streams", "link_suffix": "/forum?id=tO58o0ahdg", "link": "https://openreview.net/forum?id=tO58o0ahdg", "pdf_link": "https://openreview.net/pdf?id=tO58o0ahdg", "keywords": "Test-time Adaptation, Transfer Learning, Data Heterogeneity", "abstract": "While Test-Time Adaptation (TTA) has shown promise in addressing distribution shifts between training and testing data, its effectiveness diminishes with heterogenous data streams due to uniform target estimation. As previous attempts merely stabilize model fine-tuning over time to handle continually changing environments, they fundamentally assume a homogeneous target domain at any moment, leaving the intrinsic real-world data heterogeneity unresolved. This paper delves into TTA under heterogeneous data streams, moving beyond current model-centric limitations. By revisiting TTA from a data-centric perspective, we discover that decomposing samples into Fourier space facilitates an accurate data separation across different frequency levels. Drawing from this insight, we propose a novel Frequency-based Decentralized Adaptation framework, which transitions data from globally heterogeneous to locally homogeneous in Fourier space and employs decentralized adaptation to manage diverse distribution shifts.\nParticularly, multiple local models are allowed to independently adjust to their specific data segments while periodically exchanging knowledge to form a cohesive global model. As such, not only can data diversity be captured, but also the overall model generalization can be enhanced across multiple distribution shifts. Importantly, we devise a novel Fourier-based augmentation strategy to assist in decentralizing adaptation, which selectively augments samples for each type of distribution shift and further enhances model robustness in complex real-world environments. Extensive experiments across various settings (corrupted, natural, and medical) demonstrate the superiority of our proposed framework over the state-of-the-arts.", "title_embedding_index": 18366, "title_abs_embedding_index": 18391}, {"title": "OIDA-QA: A Multimodal Benchmark for Analyzing the Opioid Industry Document Archive", "link_suffix": "/forum?id=Fjkree2o8N", "link": "https://openreview.net/forum?id=Fjkree2o8N", "pdf_link": "https://openreview.net/pdf?id=Fjkree2o8N", "keywords": "LLM", "abstract": "The opioid crisis is a serious public health issue that requires innovative solutions for effective analysis and deeper understanding. \nDespite the vast amounts of data in the Opioid Industry Documents Archive (OIDA), the complexity, multimodal nature, and specialized characteristics of healthcare data necessitate more advanced methods and models tailored to specific data types and detailed annotations, ensuring the precision and professionalism in the analysis.\nIn this paper, we tackle this challenge by organizing the original dataset according to document attributes and constructing a benchmark with 400k training documents and 10k for testing. We extract extensive multimodal information from each document, including textual, visual, and layout information, to capture a wide range of features. Given the extracted dense information, we collect a comprehensive dataset comprising over 3 million question-answer pairs with the assistance of multiple AI models. \nWe further develop domain-specific Large Language Models (LLMs) and investigate the impact of multimodal data on task performance.\nOur benchmarking and model efforts strive to produce an AI assistant system which can efficiently process the dataset and extract valuable insights.\nPreliminary results indicate the improvements with our AI assistant in document information extraction and question-answering tasks, highlighting the effectiveness of proposed benchmark in addressing the opioid crisis.\nThe data and model will be made publicly available for research.", "title_embedding_index": 18367, "title_abs_embedding_index": 18392}, {"title": "Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory", "link_suffix": "/forum?id=I9Dsq0cVo9", "link": "https://openreview.net/forum?id=I9Dsq0cVo9", "pdf_link": "https://openreview.net/pdf?id=I9Dsq0cVo9", "keywords": "Synthetic Data, RLHF, Generative Models, Statistical Models, Random Matrices", "abstract": "Synthetic data has gained attention for training large language models, but poor-quality data can harm performance (see, e.g., Shumailov et al. (2023); Seddik et al. (2024)). A potential solution is data pruning, which retains only high-quality data based on a score function (human or machine feedback). Previous work Feng et al. (2024) analyzed models trained on synthetic data as sample size increases. We extend this by using random matrix theory to derive the performance of a binary classifier trained on a mix of real and pruned synthetic data in a high dimensional setting. Our findings identify conditions where synthetic data could improve performance, focusing on the quality of the generative model and verification strategy. We also show a smooth phase transition in synthetic label noise, contrasting with prior sharp behavior in infinite sample limits. Experiments with toy models and large language models validate our theoretical results.", "title_embedding_index": 18368, "title_abs_embedding_index": 18393}, {"title": "Pharmacophore-based design by learning on voxel grids", "link_suffix": "/forum?id=Ocg3XIymmp", "link": "https://openreview.net/forum?id=Ocg3XIymmp", "pdf_link": "https://openreview.net/pdf?id=Ocg3XIymmp", "keywords": "generative models, drug discovery, ligand-based drug discovery, pharmacophore, voxels, captioning", "abstract": "Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce VoxCap, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: de-novo design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse de-novo hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force.", "title_embedding_index": 18369, "title_abs_embedding_index": 18394}, {"title": "Simple and Controllable Uniform Discrete Diffusion Language Models", "link_suffix": "/forum?id=i5MrJ6g5G1", "link": "https://openreview.net/forum?id=i5MrJ6g5G1", "pdf_link": "https://openreview.net/pdf?id=i5MrJ6g5G1", "keywords": "Discrete Diffusion, Guidance", "abstract": "Diffusion models for continuous data gained widespread adoption owing to their high quality generation and control mechanisms. However, controllable diffusion on discrete data faces challenges: continuous diffusion guidance methods are not applicable and recent discrete diffusion models are not well-suited to control or exhibit a quality gap. Here, we provide a straightforward derivation of classifier-free and classifier-based guidance for discrete diffusion, as well as a new class of diffusion models that leverage uniform noise and thus can continuously edit their outputs. We improve the quality of these models with a novel continuous-time variational lower bound that yields state-of-the-art performance, in settings with small vocabularies. Empirically, we demonstrate the effectiveness of our guidance mechanisms relative to autoregressive and diffusion baselines, especially in conjunction with uniform noise diffusion, on several discrete data domains, including genomic sequences, small molecule design, and discretized image generation.", "title_embedding_index": 18370, "title_abs_embedding_index": 18395}, {"title": "Generator Matching: Generative modeling with arbitrary Markov processes", "link_suffix": "/forum?id=RuP17cJtZo", "link": "https://openreview.net/forum?id=RuP17cJtZo", "pdf_link": "https://openreview.net/pdf?id=RuP17cJtZo", "keywords": "Flow matching, Markov process, Diffusion model, Generative Modeling", "abstract": "We introduce generator matching, a modality-agnostic framework for generative modeling using arbitrary Markov processes. Generators characterize the infinitesimal evolution of a Markov process, which we leverage for generative modeling in a similar vein to flow matching: we construct conditional generators which generate single data points, then learn to approximate the marginal generator which generates the full data distribution. We show that generator matching unifies various generative modeling methods, including diffusion models, flow matching and discrete diffusion models. Furthermore, it provides the foundation to expand the design space to new and unexplored Markov processes such as jump processes. Finally, generator matching enables the construction of superpositions of Markov generative processes and enables the construction of multimodal models in a rigorous manner. We empirically validate our method on protein and image structure generation, showing that superposition with a jump process improves image generation.", "title_embedding_index": 18371, "title_abs_embedding_index": 18396}, {"title": "Combining Structure and Text: Learning Representations for Reasoning on Graphs", "link_suffix": "/forum?id=hJ8OQAiTrl", "link": "https://openreview.net/forum?id=hJ8OQAiTrl", "pdf_link": "https://openreview.net/pdf?id=hJ8OQAiTrl", "keywords": "graph reasoning, structure representation, text representation, GNN, PLM", "abstract": "Effective reasoning on real-world graphs necessitates a thorough understanding and optimal utilization of structural information from graph structure and textual information corresponding to nodes and edges. Recent research has primarily focused on two paradigms: employing graph neural networks to capture structural features and utilizing language models to process textual information, respectively. While these approaches have shown impressive performance, integrating structural and textual information presents significant challenges. To be more specific, concurrently training graph neural networks and language models is particularly challenging, primarily due to the scale of real-world graphs. This paper introduces a novel framework, named CoST, tailored for graph reasoning tasks. The proposed optimization objective enables alternating training of the GNN and PLM, leading to the generation of effective text representations by the PLM model, thereby enhancing the reasoning capabilities of the GNN model. Empirical results demonstrate that CoST achieves state-of-the-art performance across representative benchmark datasets.", "title_embedding_index": 18372, "title_abs_embedding_index": 18397}, {"title": "Wolf: Accurate Video Captioning with a World Summarization Framework", "link_suffix": "/forum?id=eIO1YcEdE6", "link": "https://openreview.net/forum?id=eIO1YcEdE6", "pdf_link": "https://openreview.net/pdf?id=eIO1YcEdE6", "keywords": "Video Captioning, Video Understanding, Multimodal Learning", "abstract": "We propose Wolf, a WOrLd summarization Framework for accurate video captioning. Wolf is an automated captioning framework that adopts a mixture-of-experts approach, leveraging complementary strengths of Vision Language Models (VLMs). By utilizing both image and video models, our framework captures different levels of information and summarizes them efficiently. Our approach can be applied to enhance video understanding, auto-labeling, and captioning. To evaluate caption quality, we introduce CapScore, an LLM-based metric to assess the similarity and quality of generated captions compared to the ground truth captions. We further build four human-annotated datasets in three domains: autonomous driving, general scenes, and robotics, to facilitate comprehensive comparisons. We show that Wolf achieves superior captioning performance compared to state-of-the-art approaches from the research community (VILA1.5, CogAgent) and commercial solutions (Gemini-Pro-1.5, GPT-4V). For instance, in comparison with GPT-4V, Wolf improves CapScore (caption quality) by 55.6% and CapScore (caption similarity) by 77.4% on challenging driving videos. Finally, we establish a benchmark for video captioning and introduce a leaderboard, aiming to accelerate advancements in video understanding, captioning, and data alignment.", "title_embedding_index": 18373, "title_abs_embedding_index": 18398}, {"title": "Low-cost Enhancer for Text Attributed Graph Learning via Graph Alignment", "link_suffix": "/forum?id=yrnrvfXFaV", "link": "https://openreview.net/forum?id=yrnrvfXFaV", "pdf_link": "https://openreview.net/pdf?id=yrnrvfXFaV", "keywords": "Text-attributed Graphs", "abstract": "Many graphs can be represented as Text-attributed Graphs (TAGs). Due to the rich textual information present in each node of TAGs, traditional graph neural networks (GNNs) often struggle to deliver satisfactory performance. Recent advancements leveraging large language models (LLMs) to augment new node text features have notably enhanced node representations, resulting in significant performance improvements. However, these methods typically require extensive annotations or fine-tuning on all nodes, which are both time-consuming and expensive. To address this challenge, we propose GAGA, a novel and lightweight framework for TAG representation learning. GAGA employs a more efficient strategy by annotating only representative nodes and edges, thereby reducing both annotation time and cost. It further capitalizes on these annotations by constructing an annotation graph that captures the topological relationships among them. Additionally, GAGA introduces a two-level alignment module to integrate the annotation graph with the TAG, ensuring effective alignment of their underlying structures. Experiments demonstrate that GAGA achieves classification accuracies comparable to or exceeding state-of-the-art methods while requiring only 1% of the data to be annotated, making it highly efficient.", "title_embedding_index": 18374, "title_abs_embedding_index": 18399}]