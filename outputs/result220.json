[
    {
        "title": "Training-Free Message Passing for Learning on Hypergraphs",
        "link_suffix": "/forum?id=4AuyYxt7A2",
        "link": "https://openreview.net/forum?id=4AuyYxt7A2",
        "pdf_link": "https://openreview.net/pdf?id=4AuyYxt7A2",
        "keywords": "Hypergraphs, Hypergraph Neural Networks, Graph Neural Networks",
        "abstract": "Hypergraphs are crucial for modelling higher-order interactions in real-world data. Hypergraph neural networks (HNNs) effectively utilise these structures by message passing to generate informative node features for various downstream tasks like node classification. However, the message passing module in existing HNNs typically requires a computationally intensive training process, which limits their practical use. To tackle this challenge, we propose an alternative approach by decoupling the usage of hypergraph structural information from the model learning stage. This leads to a novel training-free message passing module, named TF-MP-Module, which can be precomputed in the data preprocessing stage, thereby reducing the computational burden. We refer to the hypergraph neural network equipped with our TF-MP-Module as TF-HNN. We theoretically support the efficiency and effectiveness of TF-HNN by showing that: 1) It is more training-efficient compared to existing HNNs; 2) It utilises as much information as existing HNNs for node feature generation; and 3) It is robust against the oversmoothing issue while using long-range interactions. Experiments based on seven real-world hypergraph benchmarks in node classification and hyperlink prediction show that, compared to state-of-the-art HNNs, TF-HNN exhibits both competitive performance and superior training efficiency. Specifically, on the large-scale benchmark, Trivago, TF-HNN outperforms the node classification accuracy of the best baseline by 10% with just 1% of the training time of that baseline."
    },
    {
        "title": "Soft Prompts Go Hard: Steering Visual Language Models with Hidden Meta-Instructions",
        "link_suffix": "/forum?id=1XxNbecjXe",
        "link": "https://openreview.net/forum?id=1XxNbecjXe",
        "pdf_link": "https://openreview.net/pdf?id=1XxNbecjXe",
        "keywords": "security, machine learning, adversarial perturbations, large language models",
        "abstract": "We introduce a new type of indirect, cross-modal injection attacks against language models that operate on images: hidden \"meta-instructions\" that influence how the model interprets the image and steer its outputs to express an adversary-chosen style, sentiment, or point of view. We create meta-instructions by generating images that act as soft prompts.  In contrast to jailbreaking attacks and adversarial examples, outputs produced in response to these images are plausible and based on the visual content of the image, yet also satisfy the adversary's (meta-)objective. We evaluate the efficacy of meta-instructions for multiple models and adversarial meta-objectives, and demonstrate how they \"unlock\" capabilities of the underlying language models that are unavailable via explicit text instructions. We describe how meta-instruction attacks could cause harm by enabling creation of self-interpreting content that carries spam, misinformation, and spin."
    },
    {
        "title": "POGEMA: A Benchmark Platform for Cooperative Multi-Agent Navigation",
        "link_suffix": "/forum?id=6VgwE2tCRm",
        "link": "https://openreview.net/forum?id=6VgwE2tCRm",
        "pdf_link": "https://openreview.net/pdf?id=6VgwE2tCRm",
        "keywords": "MAPF, MARL, RL, Heuristic search",
        "abstract": "Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments with, mostly, few agents and full observability. Moreover, a range of crucial robotics-related tasks, such as multi-robot navigation and obstacle avoidance, that have been conventionally approached with the classical non-learnable methods (e.g., heuristic search) is currently suggested to be solved by the learning-based or hybrid methods. Still, in this domain, it is hard, not to say impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To this end, we introduce POGEMA, a set of comprehensive tools that includes a fast environment for learning, a generator of problem instances, the collection of pre-defined ones, a visualization toolkit, and a benchmarking tool that allows automated evaluation. We introduce and specify an evaluation protocol defining a range of domain-related metrics computed on the basics of the primary evaluation indicators (such as success rate and path length), allowing a fair multi-fold comparison. The results of such a comparison, which involves a variety of state-of-the-art MARL, search-based, and hybrid methods, are presented."
    },
    {
        "title": "Enhancing PPB Affinity Prediction through Data Integration and Feature Alignment: Approaching Structural Model Performance with Sequences",
        "link_suffix": "/forum?id=xNDydjYBmC",
        "link": "https://openreview.net/forum?id=xNDydjYBmC",
        "pdf_link": "https://openreview.net/pdf?id=xNDydjYBmC",
        "keywords": "binding affinity, geometric deep learning, virtual screening",
        "abstract": "One key step of protein drug development is the screening of protein-protein binding (PPB) affinity. The current mainstream screening method of PPB affinity is laboratory experiments, which are costly and time-consuming, making it difficult to quickly perform high-throughput screening. Various deep learning methods have been proposed to predict PPB affinity, but they are often limited by the availability of high-quality data and the compatibility of the algorithms with that data. In this work, we developed two AI models, PPBind-3D and PPBind-1D, to predict PPB affinity. PPBind-3D leverages structural information near the protein-protein binding interface to make its predictions. By employing monotonic neural network constrained multi-task learning, we effectively utilized heterogeneous affinity data from diverse wet lab experiments to expand the development dataset to over 23,000 samples, thereby enhancing the model's generalization capabilities. Additionally, PPBind-1D was developed using sequence data to address the lack of structural data in practical applications. During the training of PPBind-1D, we aligned it with PPBind-3D by incorporating an additional 42,108 no-affinity-label samples through an alignment approach. Finally, we demonstrated three application cases of our AI models in the virtual screening of protein drugs, illustrating that our models can significantly facilitate high-throughput screening."
    },
    {
        "title": "Understanding, Abstracting and Checking: Evoking Complicated Multimodal Reasoning in LMMs",
        "link_suffix": "/forum?id=dIY0vwNyH4",
        "link": "https://openreview.net/forum?id=dIY0vwNyH4",
        "pdf_link": "https://openreview.net/pdf?id=dIY0vwNyH4",
        "keywords": "Complicated Reasoning, Large multimodal model",
        "abstract": "The recent large multimodal models (LMMs) have demonstrated their impressive capability of image understanding. However, they still struggle to make complicated reasoning for solving a challenging multimodal problem. In this paper, we present UnAC (Understanding, Abstracting, and Checking), a novel multimodal prompting method, to synergize reasoning for complicated problems in the multimodal context of LMMs, such as GPT-4o, Gemini-1.5 and GPT-4V.  To improve the understanding of the image and capture more details, we propose an adaptive visual prompting method to make LMMs able to focus on certain regions. An image abstracting prompting is designed to effectively extract information from images. Further, we propose a gradual self-checking scheme for leading to better reasoning by checking each decomposed sub-question and its answer. Extensive experiments on three public benchmarks -- MathVista, MM-Vet, and MMMU -- demonstrate the effectiveness of our method."
    },
    {
        "title": "Accelerate Vertical Federated Adversarial Learning with  Dual-level Decoupled  Backpropagation",
        "link_suffix": "/forum?id=ARStrjBg3v",
        "link": "https://openreview.net/forum?id=ARStrjBg3v",
        "pdf_link": "https://openreview.net/pdf?id=ARStrjBg3v",
        "keywords": "Vertical Federated Learning, Adversarial Training, Delayed Gradient",
        "abstract": "Vertical Federated Learning (VFL) involves multiple participants collaborating to train models on distinct feature sets from the same data samples. \nThe distributed deployment of VFL models renders them vulnerable to adversarial perturbations during inference, motivating the need to visit the VFL robustness problem.\nAdversarial Training (AT) is the predominant approach for enhancing model robustness. \nHowever, its application in VFL, termed Vertical Federated Adversarial Learning (VFAL), faces significant computational challenges:\nGenerating adversarial examples in AT requiresiterative full propagations across participants with heavy computation overload, resulting in VFAL training time far exceeding those of regular VFLs.\nTo address this challenge, we proposeDecVFAL, an acceleratedVFALframework through a novelDecoupled backpropagation incorporating adual-level decoupled mechanism to enable lazy sequential and decoupled parallel backpropagation. \nLazy sequential backpropagation sequentially updates the adversarial example using timely partial derivatives with respect to the bottom module and delayed partial derivatives for the remaining modules. \nDecoupled parallel backpropagation updates these delayed partial derivatives by utilizing module-wise delayed gradients, enabling asynchronous parallel backpropagation with flexible partitions that align with VFL's distributed deployment.\nRigorous theoretical analysis demonstrates that despite introducing multi-source approximate gradients due to the dual decoupled mechanism and the techniques from the existing VFL methods,DecVFALachieves a $\\mathcal{O}(1 / \\sqrt{\\mathcal{K}})$ convergence rate after $\\mathcal{K}$ iterations, on par with regular VFL systems.\nExperimental results show that, compared to existing methods,DecVFALensures competitive robustness while significantly achieving about $3\\sim10$ times speed up on various datasets."
    },
    {
        "title": "Multiple Choice Questions and Large Languages Models: A Case Study with Fictional Medical Data",
        "link_suffix": "/forum?id=xawA8X5dHq",
        "link": "https://openreview.net/forum?id=xawA8X5dHq",
        "pdf_link": "https://openreview.net/pdf?id=xawA8X5dHq",
        "keywords": "large language models, medicine, benchmark, evaluation, clinical knowledge, multiple choice questions",
        "abstract": "Large Language Models (LLMs) like ChatGPT demonstrate significant potential in the medical field, often evaluated using multiple-choice questions (MCQs) similar to those found on the USMLE. Despite their prevalence in medical education, MCQs have limitations that might be exacerbated when assessing LLMs. To evaluate the effectiveness of MCQs in assessing the performance of LLMs, we developed a fictional medical benchmark focused on a non-existent gland, the Glianorex. This approach allowed us to isolate the knowledge of the LLM from its test-taking abilities. We used GPT-4-Turbo and Claude 3.5 Sonnet to generate two comprehensive textbooks on the Glianorex in both English and French and developed corresponding multiple-choice questions in both languages. We evaluated various open-source, proprietary, and domain-specific LLMs using these questions in a zero-shot setting. The models achieved average scores around 64%, with minor performance differences between larger and smaller models. Performance was slightly higher in English than in French. Fine-tuned medical models showed some improvement over their base versions in English but not in French. The high performance across models suggests that traditional MCQ-based benchmarks may not accurately measure LLMs' clinical knowledge and reasoning abilities, instead highlighting their pattern recognition skills. This study underscores the need for more robust evaluation methods to better assess the true capabilities of LLMs in medical contexts."
    },
    {
        "title": "MCUCoder: Adaptive Bitrate Learned Video Compression for IoT Devices",
        "link_suffix": "/forum?id=wJlzUR5sFl",
        "link": "https://openreview.net/forum?id=wJlzUR5sFl",
        "pdf_link": "https://openreview.net/pdf?id=wJlzUR5sFl",
        "keywords": "Efficient Video Compression, IoT Devices, Learned Video Compression, Adaptive Bitrate Compression, Microcontrollers",
        "abstract": "The rapid growth of camera-based IoT devices demands the need for efficient video compression, particularly for edge applications where devices face hardware constraints, often with only 1 or 2 MB of RAM and unstable internet connections. Traditional and deep video compression methods are designed for high-end hardware, exceeding the capabilities of these constrained devices. Consequently, video compression in these scenarios is often limited to M-JPEG due to its high hardware efficiency and low complexity. This paper introduces , an open-source adaptive bitrate video compression model tailored for resource-limited IoT settings. MCUCoder features an ultra-lightweight encoder with only 10.5K parameters and a minimal 350KB memory footprint, making it well-suited for edge devices and MCU. While MCUCoder uses a similar amount of energy as M-JPEG, it reduces bitrate by 55.65% on the MCL-JCV dataset and 55.59% on the UVG dataset, measured in MS-SSIM. Moreover, MCUCoder supports adaptive bitrate streaming by generating a latent representation that is sorted by importance, allowing transmission based on available bandwidth. This ensures smooth real-time video transmission even under fluctuating network conditions on low-resource devices. Source code available at [Link removed due to double-blind policy, code submitted in ZIP]."
    },
    {
        "title": "TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel Conditioning",
        "link_suffix": "/forum?id=erWwBoR59l",
        "link": "https://openreview.net/forum?id=erWwBoR59l",
        "pdf_link": "https://openreview.net/pdf?id=erWwBoR59l",
        "keywords": "Diffusion model, Inpainting",
        "abstract": "Diffusion models have emerged as highly effective techniques for inpainting, however, they remain constrained by slow sampling rates. While recent advances have enhanced generation quality, they have also increased sampling time, thereby limiting scalability in real-world applications. We investigate the generative sampling process of diffusion-based inpainting models and observe that these models make minimal use of the input condition during the initial sampling steps. As a result, the sampling trajectory deviates from the data manifold, requiring complex synchronization mechanisms to realign the generation process. To address this, we propose Time-aware Diffusion Paint (TD-Paint), a novel approach that adapts the diffusion process by modeling variable noise levels at the pixel level. This technique allows the model to efficiently use known pixel values from the start, guiding the generation process toward the target manifold. By embedding this information early in the diffusion process, TD-Paint significantly accelerates sampling without compromising image quality. Unlike conventional diffusion-based inpainting models, which require a dedicated architecture or an expensive generation loop, TD-Paint achieves faster sampling times without architectural modifications. Experimental results across three datasets show that TD-Paint outperforms state-of-the-art diffusion models while maintaining lower complexity."
    },
    {
        "title": "Class Distribution-induced Attention Map for Open-vocabulary Semantic Segmentations",
        "link_suffix": "/forum?id=CMqOfvD3tO",
        "link": "https://openreview.net/forum?id=CMqOfvD3tO",
        "pdf_link": "https://openreview.net/pdf?id=CMqOfvD3tO",
        "keywords": "Vision Language Model, Dense Localization",
        "abstract": "Open-vocabulary semantic segmentation is a challenging task that assigns seen or unseen class labels to individual pixels. While recent works with vision-language models (VLMs) have shown promising results in zero-shot semantic segmentation, they still struggle to accurately localize class-related objects. In this work, we argue that CLIP-based prior works yield patch-wise noisy class predictions while having highly correlated class distributions for each object. Then, we propose Class Distribution-induced Attention Map, dubbed CDAM, that is generated by the Jensen-Shannon divergence between class distributions of two patches that belong to the same (class) object. This CDAM can be used for open-vocabulary semantic segmentation by integrating it into the final layer of CLIP to enhance the capability to accurately localize desired classes. Our class distribution-induced attention scheme can easily work with multi-scale image patches as well as augmented text prompts for further enhancing attention maps. By exploiting class distribution, we also propose robust entropy-based background thresholding for the inference of semantic segmentation. Interestingly, the core idea of our proposed method does not conflict with other prior arts in zero-shot semantic segmentation, thus can be synergetically used together, yielding substantial improvements in performance across popular semantic segmentation benchmarks."
    },
    {
        "title": "SEAL: SEmantic-Augmented Imitation Learning via Language Model",
        "link_suffix": "/forum?id=wPyTeUMRgh",
        "link": "https://openreview.net/forum?id=wPyTeUMRgh",
        "pdf_link": "https://openreview.net/pdf?id=wPyTeUMRgh",
        "keywords": "Large Language Models, Hierarchical Imitation Learning",
        "abstract": "Hierarchical Imitation Learning (HIL) is a promising approach for tackling long-horizon decision-making tasks. While it is a challenging task due to the lack of detailed supervisory labels for sub-goal learning, and reliance on hundreds to thousands of expert demonstrations. In this work, we introduce SEAL, a novel framework that leverages Large Language Models (LLMs)'s powerful semantic and world knowledge for both specifying sub-goal space and pre-labeling states to semantically meaningful sub-goal representations without prior knowledge of task hierarchies. SEAL employs a dual-encoder structure, combining supervised LLM-guided sub-goal learning with unsupervised Vector Quantization (VQ) for more robust sub-goal representations. Additionally, SEAL incorporates a transition-augmented low-level planner for improved adaptation to sub-goal transitions. Our experiments demonstrate that SEAL outperforms state-of-the-art HIL methods and LLM-based planning approaches, particularly in settings with small expert datasets and complex long-horizon tasks."
    },
    {
        "title": "Automated Discovery of Pairwise Interactions from Unstructured Data",
        "link_suffix": "/forum?id=BuBBRn0zFD",
        "link": "https://openreview.net/forum?id=BuBBRn0zFD",
        "pdf_link": "https://openreview.net/pdf?id=BuBBRn0zFD",
        "keywords": "causal independence testing, representation learning, active learning",
        "abstract": "Pairwise interactions between perturbations to a system can provide evidence for the causal dependencies of the underlying underlying mechanisms of a system. When observations are low dimensional, hand crafted measurements, detecting interactions amounts to simple statistical tests, but it is not obvious how to detect interactions between perturbations affecting latent variables. \nWe derive two interaction tests that are based on pairwise interventions, and show how these tests can be integrated into an active learning pipeline to efficiently discover pairwise interactions between perturbations.\nWe illustrate the value of these tests in the context of biology, where\npairwise perturbation experiments are frequently used to reveal interactions\nthat are not observable from any single perturbation. \nOur tests can be run on unstructured data, such as\nthe pixels in an image, which enables a more general notion of interaction than\ntypical cell viability experiments, and can be run on cheaper experimental assays. \nWe validate on several synthetic and real biological experiments that our tests are able to identify interacting pairs effectively. \nWe evaluate our approach on a real biological experiment where we knocked out\n50 pairs of genes and measured the effect with microscopy images. We show that\nwe are able to recover significantly more known biological interactions than\nrandom search and standard active learning baselines."
    },
    {
        "title": "Efficient and Accurate Explanation Estimation with Distribution Compression",
        "link_suffix": "/forum?id=LiUfN9h0Lx",
        "link": "https://openreview.net/forum?id=LiUfN9h0Lx",
        "pdf_link": "https://openreview.net/pdf?id=LiUfN9h0Lx",
        "keywords": "explainable ai, feature attributions, feature importance, sampling, kernel thinning",
        "abstract": "Exact computation of various machine learning explanations requires numerous model evaluations and in extreme cases becomes impractical. The computational cost of approximation increases with an ever-increasing size of data and model parameters. Many heuristics have been proposed to approximate post-hoc explanations efficiently. This paper shows that the standard i.i.d. sampling used in a broad spectrum of algorithms for explanation estimation leads to an approximation error worthy of improvement. To this end, we introduce compress then explain (CTE), a new paradigm for more efficient and accurate explanation estimation. CTE uses distribution compression through kernel thinning to obtain a data sample that best approximates the marginal distribution. We show that CTE improves the estimation of removal-based local and global explanations with negligible computational overhead. It often achieves an on-par explanation approximation error using 2-3x fewer samples, i.e. requiring 2-3x fewer model evaluations. CTE is a simple yet powerful plug-in for any explanation method that now relies on i.i.d. sampling."
    },
    {
        "title": "Distributional reasoning in LLMs: Parallel Reasoning Processes in Multi-hop Reasoning",
        "link_suffix": "/forum?id=L9j8exYGUJ",
        "link": "https://openreview.net/forum?id=L9j8exYGUJ",
        "pdf_link": "https://openreview.net/pdf?id=L9j8exYGUJ",
        "keywords": "LLM, Multihop reasoning, Interpretability",
        "abstract": "Large language models (LLMs) have shown an impressive ability to perform tasks believed to require \"thought processes\u201d. When the model does not document an explicit thought process, it's difficult to understand the processes occurring within its hidden layers, and to determine if this process can be referred to as reasoning. We introduce a novel and interpretable analysis of internal multi-hop reasoning processes in LLMs. We demonstrate that the prediction process for compositional reasoning questions can be modeled using a simple linear transformation between two semantic category spaces. We show that during inference, the middle layers of the network generate highly interpretable embeddings that represent a set of potential intermediate answers for the multi-hop question. We use statistical analyses to show that a corresponding subset of tokens is activated in the model's output, implying the existence of parallel reasoning paths. These observations hold true even when the model lacks the necessary knowledge to solve the task. Our findings can help uncover the strategies that LLMs use to solve reasoning tasks, offering insights into the types of thought processes that can emerge from artificial intelligence. Finally, we also discuss the implication of cognitive modeling of these results."
    },
    {
        "title": "GRAM: Generalization in Deep RL with a Robust Adaptation Module",
        "link_suffix": "/forum?id=UfczlMudN6",
        "link": "https://openreview.net/forum?id=UfczlMudN6",
        "pdf_link": "https://openreview.net/pdf?id=UfczlMudN6",
        "keywords": "deep reinforcement learning, generalization, robust adaptation",
        "abstract": "The reliable deployment of deep reinforcement learning in real-world settings requires the ability to generalize across a variety of conditions, including both in-distribution scenarios seen during training as well as novel out-of-distribution scenarios. In this work, we present a framework for generalization in deep reinforcement learning that unifies these two distinct types of generalization within a single architecture. We introduce a robust adaptation module that provides a mechanism for identifying and reacting to both in-distribution and out-of-distribution environments, along with a joint training pipeline that combines the goals of in-distribution adaptation and out-of-distribution robustness. Our algorithm GRAM achieves strong generalization performance across in-distribution and out-of-distribution scenarios upon deployment, which we demonstrate on a variety of realistic simulated locomotion tasks with a quadruped robot."
    },
    {
        "title": "Synthetic Datasets for Machine Learning on Spatio-Temporal Graphs using PDEs",
        "link_suffix": "/forum?id=qq0zZMC4SM",
        "link": "https://openreview.net/forum?id=qq0zZMC4SM",
        "pdf_link": "https://openreview.net/pdf?id=qq0zZMC4SM",
        "keywords": "Data, Dataset, PDE, Graph, Spatio-Temporal, Epidemiology, Benchmarking",
        "abstract": "In this work, we describe the creation and use of synthetic datasets based on various partial differential equations to support spatio-temporal graph modeling in machine learning for different applications. More precisely, we showcase three equations to model different types of disasters and hazards in the fields of epidemiology, atmospheric particles, and tsunami waves. Further, we show how such created datasets can be used by benchmarking several machine learning models on the epidemiological dataset and, additionally, by showing how pre-training on such synthetic datasets can improve model performance on real-world epidemiological data. The presented methods enable others to create datasets and benchmarks customized to individual requirements. The source code for our methodology and the three created datasets can be found onhttps://github.com/github-usr-ano/Temporal_Graph_Data_PDEs."
    },
    {
        "title": "Weak to Strong Generalization for Large Language Models with Multi-capabilities",
        "link_suffix": "/forum?id=N1vYivuSKq",
        "link": "https://openreview.net/forum?id=N1vYivuSKq",
        "pdf_link": "https://openreview.net/pdf?id=N1vYivuSKq",
        "keywords": "Weak to Strong Generalization, Large Language Model, Superalignment",
        "abstract": "As large language models (LLMs) grow in sophistication, some of their capabilities surpass human abilities, making it essential to ensure their alignment with human values and intentions, i.e., Superalignment. This superalignment challenge is particularly critical for complex tasks, as annotations provided by humans, as weak supervisors, may be overly simplistic, incomplete, or incorrect. Previous work has demonstrated the potential of training a strong model using the weak dataset generated by a weak model as weak supervision. However, these studies have been limited to a single capability. In this work, we conduct extensive experiments to investigate weak to strong generalization for LLMs with multi-capabilities. The experiments reveal that different capabilities tend to remain relatively independent in this generalization, and the effectiveness of weak supervision is significantly impacted by the quality and diversity of the weak datasets. Moreover, the self-bootstrapping of the strong model leads to performance degradation due to its overconfidence and the limited diversity of its generated dataset. To address these issues, we proposed a novel training framework using reward models to select valuable data, thereby providing weak supervision for strong model training. In addition, we propose a two-stage training method on both weak and selected datasets to train the strong model. Experimental results demonstrate our method significantly improves the weak to strong generalization with multi-capabilities."
    },
    {
        "title": "Masked, Regularized Fidelity With Diffusion Models For Highly Ill-posed Inverse Problems",
        "link_suffix": "/forum?id=GQnR7L6SmA",
        "link": "https://openreview.net/forum?id=GQnR7L6SmA",
        "pdf_link": "https://openreview.net/pdf?id=GQnR7L6SmA",
        "keywords": "Diffusion model, Image restoration, Large and complex kernel, Severe ill-posed problem",
        "abstract": "Diffusion models have been well-investigated for solving ill-posed inverse problems to yield excellent performance. However, these models have not been well-adopted to highly ill-posed inverse problems. In this work, we propose zero-shot diffusion model for large and complex kernels, dubbed Dilack, with novel data fidelity terms that are inspired by a hybrid form of generative and classical priors, leading to \\emph{regularized fidelity} called pseudo-inverse anchor for constraining (PiAC) fidelity loss, and also are designed from the investigation on the behavior of diffusion models interacting with data fidelity globally unlike locally acting classical regularizers, leading to \\emph{masked fidelity} that adaptively enforces spatially and step-wisely local fidelity via mask. Our proposed scheme effectively reduces erratic behavior and inherent artifacts in diffusion models, thereby improving restoration quality including perceptual aspects and outperforming prior arts on both synthetic and real-world datasets for modern lensless imaging and large motion deblurring."
    },
    {
        "title": "Interleaved Scene Graph for Interleaved Text-and-Image Generation Assessment",
        "link_suffix": "/forum?id=rDLgnYLM5b",
        "link": "https://openreview.net/forum?id=rDLgnYLM5b",
        "pdf_link": "https://openreview.net/pdf?id=rDLgnYLM5b",
        "keywords": "Interleaved Text-and-Image Generation, Generative Models, Multimodal Large Language Model, Scene Graphs, Automatic Evaluation, Benchmark",
        "abstract": "Unified Transformer-based models have enabled simultaneous multimodal understanding and generation, showing promise in unifying both vision and language tasks with interleaved text-and-image generation. However, assessing the performance of multimodal interleaved generation remains unexplored and challenging due to the complexity of interleaved content.  In this paper, we design an automatic multi-granular evaluation framework Interleaved Scene Graph (ISG) with four levels to evaluate accurate interleaved generation tasks, which converts multimodal queries into atomic questions, then perform visual question answering for precise verification. Moreover, we propose ISG-Bench, the first multimodal interleaved benchmark with concrete generation requirement consisting of 1,150 samples across 21 text-image generation tasks. Additionally, we pioneer in a compositional agent framework ISG-Agent to explore the upper bound of interleaved generation with agent workflow. In our experiments, we conduct a multi-granular evaluation of ISG, demonstrating its potential for automatically evaluating interleaved generation consistent with ground truth and human preferences. Furthermore, comprehensive assessments of 10 interleaved generative frameworks reveal that unified models still lack basic accurate instruction-following capabilities, falling short even in structural requirements. Additionally, our ISG-Agent outperforms other compositional frameworks in interleaved generation at various levels but still struggles with vision-dominated tasks. Our work offers valuable insights for advancing future research in interleaved text-and-image generation."
    },
    {
        "title": "DSPO: Direct Score Preference Optimization for Diffusion Model Alignment",
        "link_suffix": "/forum?id=xyfb9HHvMe",
        "link": "https://openreview.net/forum?id=xyfb9HHvMe",
        "pdf_link": "https://openreview.net/pdf?id=xyfb9HHvMe",
        "keywords": "Text-to-image generation",
        "abstract": "Diffusion-based Text-to-Image (T2I) models have achieved impressive success in generating high-quality images from textual prompts. While large language models (LLMs) effectively leverage Direct Preference Optimization (DPO) for fine-tuning on human preference data without the need for reward models, diffusion models have not been extensively explored in this area. Current preference learning methods applied to T2I diffusion models immediately adapt existing techniques from LLMs. However, this adaptation introduces a mismatch between the pretraining and the fine-tuning objectives specific to T2I diffusion models. This inconsistency can potentially lead to suboptimal performance.  In this work, we  propose Direct Score Preference Optimization (DSPO), a novel algorithm that aligns the pretraining and fine-tuning objectives of diffusion models by leveraging score matching, the same objective used during pretraining. It introduces a new perspective on preference learning for diffusion models. Specifically, DSPO distills the score function of human-preferred image distributions into pretrained diffusion models, fine-tuning the model to generate outputs that align with human preferences. We theoretically show that DSPO shares the same optimization direction as reinforcement learning algorithms in diffusion models under certain conditions. Our experimental results demonstrate that DSPO outperforms preference learning baselines for T2I diffusion models in human preference evaluation tasks and enhances both visual appeal and prompt alignment of generated images."
    },
    {
        "title": "GenNet: A Generative AI-Driven Mobile Network Simulator for Multi-Objective Network Optimization",
        "link_suffix": "/forum?id=OxrDTroSNP",
        "link": "https://openreview.net/forum?id=OxrDTroSNP",
        "pdf_link": "https://openreview.net/pdf?id=OxrDTroSNP",
        "keywords": "Mobile Networks; Simulator; Optimization",
        "abstract": "Simulation-based optimization has emerged as a crucial methodology in the field of mobile network optimization, addressing the need for dynamic and predictive network management. To address the scarcity of open-source mobile network simulators for advanced research, we developed GenNet\u2014a generative AI-driven mobile network simulator. GenNet can create virtual replicas of mobile users, base stations, and wireless environments, utilizing generative AI methods to simulate the behaviors of these entities under various network settings with high accuracy. GenNet features a tailor-made API explicitly designed for reinforcement learning environments, enabling researchers to finely adjust network parameters such as tilts, azimuth, and transmitting power. Extensive experiments have employed GenNet to benchmark multi-objective optimization algorithms, focusing on enhancing network coverage, throughput, and energy efficiency, validating its effectiveness as a robust platform for advancing network optimization techniques. Through this innovative tool, we aim to empower researchers and practitioners to identify and implement the most effective approaches for network optimization, paving the way for future advancements in mobile network management."
    },
    {
        "title": "CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs",
        "link_suffix": "/forum?id=7lpDn2MhM2",
        "link": "https://openreview.net/forum?id=7lpDn2MhM2",
        "pdf_link": "https://openreview.net/pdf?id=7lpDn2MhM2",
        "keywords": "Multimodal Large Language Models, Preference Optimization, Direct Preference Optimization, Hallucination",
        "abstract": "Multimodal Large Language Models (MLLMs) still struggle with hallucinations despite their impressive capabilities. Recent studies have attempted to mitigate this by applying Direct Preference Optimization (DPO) to multimodal scenarios using preference pairs from text-based responses. However, our analysis of representation distributions reveals that multimodal DPO struggles to align image and text representations and to distinguish between hallucinated and non-hallucinated descriptions. To address these challenges,\nIn this work, we propose a Cross-modal Hierarchical Direct Preference Optimization (CHiP) to address these limitations.\nWe introduce a visual preference optimization module within the DPO framework, enabling MLLMs to learn from both textual and visual preferences simultaneously. Furthermore, we propose a hierarchical textual preference optimization module that allows the model to capture preferences at multiple granular levels, including response, segment, and token levels. We evaluate CHiP through both quantitative and qualitative analyses, with results across multiple benchmarks demonstrating its effectiveness in reducing hallucinations. On the Object HalBench dataset, CHiP outperforms DPO in hallucination reduction, achieving improvements of 52.7% and 55.5% relative points based on the base model Muffin and LLaVA models, respectively. We make all our datasets and code publicly available."
    },
    {
        "title": "Enhancing Graph Neural Networks: A Mutual Learning Approach",
        "link_suffix": "/forum?id=PBK9AM5HUm",
        "link": "https://openreview.net/forum?id=PBK9AM5HUm",
        "pdf_link": "https://openreview.net/pdf?id=PBK9AM5HUm",
        "keywords": "graph neural network, knowledge distillation, online deep mutual learning",
        "abstract": "Knowledge distillation (KD) techniques have emerged as a powerful tool for transferring expertise from complex teacher models to lightweight student models, particularly beneficial for deploying high-performance models in resource-constrained devices. This approach has been successfully applied to graph neural networks (GNNs), harnessing their expressive capabilities to generate node embeddings that capture structural and feature-related information. In this study, we depart from the conventional KD approach by exploring the potential of collaborative learning among GNNs. In the absence of a pre-trained teacher model, we show that relatively simple and shallow GNN architectures can synergetically learn efficient models capable of performing better during inference, particularly in tackling multiple tasks. We propose a collaborative learning framework where ensembles of student GNNs mutually teach each other throughout the training process. We introduce an adaptive logit weighting unit to facilitate efficient knowledge exchange among models and an entropy enhancement technique to improve mutual learning. These components dynamically empower the models to adapt their learning strategies during training, optimizing their performance for downstream tasks. Extensive experiments conducted on three datasets each for node and graph classification demonstrate the effectiveness of our approach."
    },
    {
        "title": "CoDiCast: Conditional Diffusion Model for Weather Prediction with Uncertainty Quantification",
        "link_suffix": "/forum?id=j1jtyGdD4O",
        "link": "https://openreview.net/forum?id=j1jtyGdD4O",
        "pdf_link": "https://openreview.net/pdf?id=j1jtyGdD4O",
        "keywords": "Diffusion Model, Weather Prediction, Uncertainty Quantification",
        "abstract": "Accurate weather forecasting is critical for science and society. Yet, existing methods have not demonstrated high accuracy, low uncertainty, and high computational efficiency simultaneously. On one hand, to quantify the uncertainty in weather predictions, the strategy of ensemble forecast (i.e., generating a set of diverse predictions) is often employed. However, traditional ensemble numerical weather prediction (NWP) is computationally intensive. On the other hand, even though most existing machine learning-based weather prediction (MLWP) approaches are efficient and accurate, they are deterministic and cannot capture the uncertainty of weather forecasting. To tackle these challenges, we propose $\\texttt{CoDiCast}$, a conditional diffusion model to generate accurate global weather prediction, while achieving uncertainty quantification and modest computational cost. The key idea behind the prediction task is to generate realistic weather scenarios at a $\\textit{future}$ time point, conditioned on observations from the $\\textit{recent past}$. Due to the probabilistic nature of diffusion models, they can be properly applied to capture the uncertainty of weather predictions. Therefore, we accomplish uncertainty quantifications by repeatedly sampling from stochastic Gaussian noise for each initial weather state and running the denoising process multiple times. Experimental results demonstrate that $\\texttt{CoDiCast}$ outperforms several existing MLWP methods in accuracy, and is faster than NWP models in the inference speed. $\\texttt{CoDiCast}$ can generate 3-day global weather forecasts, at 6-hour steps and $5.625^\\circ$ latitude-longitude resolutions, for over 5 variables, in about 12 minutes on a commodity A100 GPU machine with 80GB memory. The anonymous code is provided at \\url{https://anonymous.4open.science/r/CoDiCast/}."
    },
    {
        "title": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback",
        "link_suffix": "/forum?id=RIKIavmwqK",
        "link": "https://openreview.net/forum?id=RIKIavmwqK",
        "pdf_link": "https://openreview.net/pdf?id=RIKIavmwqK",
        "keywords": "Figure Caption Generation, Figure-Caption Benchmark, Human Feedback",
        "abstract": "Captions are crucial for understanding scientific visualizations and documents. Existing captioning methods for scientific figures rely on figure-caption pairs extracted from documents for training, many of which fall short with respect to metrics like helpfulness, explainability, and visual-descriptiveness leading to generated captions being misaligned with reader preferences. To enable the generation of high-quality figure captions, we introduce \\textbf{FigCaps-HF} a new benchmark and framework for figure-caption generation that can incorporate domain expert feedback in generating captions optimized for reader preferences. Our benchmark framework comprises of 1) an automatic method for evaluating quality of figure-caption pairs, 2) a novel reinforcement learning with human feedback (RLHF) method to optimize a generative figure-to-caption model for reader preferences. We demonstrate the effectiveness of our benchmark by improving performance over standard fine-tuning across different types of models. In particular, when using BLIP as the base model, our RLHF framework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, and METEOR, respectively. Finally, we release a large-scale benchmark dataset with human feedback on figure-caption pairs to enable further evaluation and development of RLHF techniques for this problem."
    }
]