[{"title": "BiEnhancer: Bi-Level Feature Enhancement in the Dark", "link_suffix": "/forum?id=ec9hJPn59o", "link": "https://openreview.net/forum?id=ec9hJPn59o", "pdf_link": "https://openreview.net/pdf?id=ec9hJPn59o", "keywords": "Low-light image enhancement; low-light object detection; night-time semantic segmentation", "abstract": "The remarkable achievements of high-level vision tasks (e.g., object detection, semantic segmentation) under favorable lighting conditions highlight the persistent challenges faced in low-light vision. Previous studies have mainly focused on enhancing low-light images to create visual-friendly representations, often neglecting the differences between machine vision and human vision. This oversight has led to limited performance improvements for high-level tasks. Furthermore, many approaches rely on synthetic paired datasets for training, which can result in limited generalization to real-world images with diverse illumination levels. To address these issues, we propose a new module called BiEnhancer, which is designed to enhance the representation of low-light images by optimizing the loss function of high-level tasks to improve performance. BiEnhancer decomposes low-light images into low-level and high-level components and performs feature enhancement. Then, it adopts an attentional feature fusion strategy and a pixel-wise iterative estimation strategy to effectively enhance and restore the details and semantic information of low-light images and improve the machine-readable representation ability of low-light images. As a versatile plug-in module, BiEnhancer supports end-to-end joint training with diverse high-level tasks. Extensive experimental results demonstrate that the BiEnhancer framework outperforms state-of-the-art methods in both speed and accuracy.", "title_embedding_index": 16050, "title_abs_embedding_index": 16075}, {"title": "Unleashing the Potential of Unlabeled Data: Bidirectional Collaborative Semi-Supervised Active Learning for 3D Object Detection", "link_suffix": "/forum?id=PBq8uOjGso", "link": "https://openreview.net/forum?id=PBq8uOjGso", "pdf_link": "https://openreview.net/pdf?id=PBq8uOjGso", "keywords": "Autonomous Driving, 3D Object Detection, Active Learning, Semi-Supervised Learning", "abstract": "To address the annotation burden in LiDAR-based 3D object detection, active learning (AL) methods offer a promising solution. However, traditional active learning approaches solely rely on labeled data to train an initial model for data selection, overlooking the potential of leveraging unlabeled data. Recently, attempts to integrate semi-supervised learning (SSL) into AL with the goal of leveraging unlabeled data have faced challenges in effectively resolving the conflict between the two paradigms, resulting in less satisfactory performance.\nTo tackle this conflict, we propose a Bidirectional Collaborative Semi-Supervised Active Learning framework, dubbed as BC-SSAL. Specifically, from the perspective of SSL, we propose a Collaborative PseudoScene Pre-training (CPSP) method that effectively learns from unlabeled data without introducing adverse effects. From the perspective of AL, we design a Collaborative Active Learning (CAL) method tailored for outdoor LiDAR scenes, which complements the uncertainty and diversity methods by model cascading, alleviating the dilemma of sampling rare classes. Extensive experiments conducted on KITTI and Waymo demonstrate the effectiveness of our BC-SSAL. Especially, on the KITTI dataset, utilizing only 2% labeled data, BC-SSAL can achieve comparable performance to the model trained on the full set.", "title_embedding_index": 16051, "title_abs_embedding_index": 16076}, {"title": "Representing Signs as Signs: One-Shot ISLR to Facilitate Functional Sign Language Technologies", "link_suffix": "/forum?id=flgrH5nK4H", "link": "https://openreview.net/forum?id=flgrH5nK4H", "pdf_link": "https://openreview.net/pdf?id=flgrH5nK4H", "keywords": "Sign Language, Deep Learning, Computer Vision", "abstract": "Isolated Sign Language Recognition (ISLR) is crucial for scalable sign language technology, yet language-specific approaches limit current models. To address this, we propose a one-shot learning approach that generalises across languages and evolving vocabularies. Our method involves pretraining a model to embed signs based on essential features and using a dense vector search for rapid, accurate recognition of unseen signs. We achieve state-of-the-art results, including 50.8% one-shot MRR on a large dictionary containing 10,235 unique signs from a different language than the training set. Our approach is robust across languages and support sets, offering a scalable, adaptable solution for ISLR. Co-created with the Deaf and Hard of Hearing (DHH) community, this method aligns with real-world needs, and advances scalable sign language recognition.", "title_embedding_index": 16052, "title_abs_embedding_index": 16077}, {"title": "Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures", "link_suffix": "/forum?id=2J18i8T0oI", "link": "https://openreview.net/forum?id=2J18i8T0oI", "pdf_link": "https://openreview.net/pdf?id=2J18i8T0oI", "keywords": "Mechanistic Interpretability, Sparse Autoencoders, Universality, State Space Models", "abstract": "The hypothesis of \\textit{Universality} in interpretability suggests that different neural networks may converge to\nimplement similar algorithms on similar tasks. In this work, we investigate two mainstream architectures\nfor language modeling, namely Transformers and Mambas, to explore the extent of their mechanistic similarity.\nWe propose to use Sparse Autoencoders (SAEs) to isolate interpretable features from these models and show\nthat most features are similar in these two models. We also validate the correlation between feature similarity\nand~\\univ. We then delve into the circuit-level analysis of Mamba models\nand find that the induction circuits in Mamba are structurally analogous to those in Transformers. We also identify a nuanced difference we call \\emph{Off-by-One motif}: The information of one token is written into the \nSSM state in its next position. Whilst interaction between tokens in Transformers does not exhibit such trend.", "title_embedding_index": 16053, "title_abs_embedding_index": 16078}, {"title": "Output-Constrained Decision Trees", "link_suffix": "/forum?id=f3TSOXnkXZ", "link": "https://openreview.net/forum?id=f3TSOXnkXZ", "pdf_link": "https://openreview.net/pdf?id=f3TSOXnkXZ", "keywords": "output constraints, decision trees, optimization, heuristics", "abstract": "When there is a correlation between any pair of targets, one needs a prediction method that can handle vector-valued output. In this setting, multi-target learning is particularly important as it is widely used in various applications. This paper introduces new variants of decision trees that can handle not only multi-target output but also the constraints among the targets. We focus on the customization of conventional decision trees by adjusting the splitting criteria to handle the constraints and obtain feasible predictions. We present both an optimization-based exact approach and several heuristics, complete with a discussion on their respective advantages and disadvantages. To support our findings, we conduct a computational study to demonstrate and compare the results of the proposed approaches.", "title_embedding_index": 16054, "title_abs_embedding_index": 16079}, {"title": "Grammar Reinforcement Learning: path and cycle counting in graphs with a Context-Free Grammar and Transformer approach", "link_suffix": "/forum?id=yEox25xAED", "link": "https://openreview.net/forum?id=yEox25xAED", "pdf_link": "https://openreview.net/pdf?id=yEox25xAED", "keywords": "Graph, Reinforcement Learning, Grammar, Cycle Counting", "abstract": "This paper presents Grammar Reinforcement Learning (GRL), a reinforcement learning algorithm that uses Monte Carlo Tree Search (MCTS) and a transformer architecture that models a Pushdown Automaton (PDA) within a context-free grammar (CFG) framework. Taking as use case the problem of efficiently counting paths and cycles in graphs, a key challenge in network analysis, computer science, biology, and social sciences, GRL discovers new matrix-based formulas for path/cycle counting that improve computational efficiency by factors of two to six w.r.t state-of-the-art approaches. Our contributions include: (i) a framework for generating transformers that operate within a CFG, (ii) the development of GRL for optimizing formulas within grammatical structures, and (iii) the discovery of novel formulas for graph substructure counting, leading to significant computational improvements.", "title_embedding_index": 16055, "title_abs_embedding_index": 16080}, {"title": "Enhancing Robustness of Deep Learning via Unified Latent Representation", "link_suffix": "/forum?id=zeeLxGw5pp", "link": "https://openreview.net/forum?id=zeeLxGw5pp", "pdf_link": "https://openreview.net/pdf?id=zeeLxGw5pp", "keywords": "deep learning robustness, out-of-distribution inputs, adversarial examples, VAE latent representation", "abstract": "Adversarial examples and Out-of-Distribution (OoD) inputs constitute major problematic instances for the image classifiers based on Deep Neural Networks (DNNs). In particular, DNNs tend to be overconfident with their predictions, assigning a different category with a high probability. In this work, we suggest a combined solution to tackle both input types based on the Variational Autoencoder (VAE). First, we scrutinize the recent successful results in detecting OoDs utilizing Bayesian epistemic uncertainty estimation over weights of VAEs. Surprisingly, contrary to the previous claims in the literature, we discover that we can obtain comparable detection performance utilizing a standard procedure of importance sampling with the classical formulation of VAE. Second, we dissect the marginal likelihood approximation, analyzing the primary source of variation responsible for distinguishing inliers versus outliers, and establish a link with the recent promising results in detecting outliers using latent holes. Finally, we identify that adversarial examples and OoD inputs have similar latent representations. This insight allows us to develop separate methods to automatically distinguish between them by considering their non-similarities in the input space. The suggested approach enables pre-training a VAE model on specific input data, allowing it to act as a gatekeeper. This achieves two major goals: defending the DNN classifier against potential attacks and flagging OoDs. Once pre-trained, VAE can be plugged as a filter into any DNN image classifier of arbitrary architecture trained on the same data inputs without the need for its retraining or accessing the layers and weights of the DNN.", "title_embedding_index": 16056, "title_abs_embedding_index": 16081}, {"title": "FusionDTI: Fine-grained Binding Discovery with Token-level Fusion for Drug-Target Interaction", "link_suffix": "/forum?id=8Lqb1dbbfa", "link": "https://openreview.net/forum?id=8Lqb1dbbfa", "pdf_link": "https://openreview.net/pdf?id=8Lqb1dbbfa", "keywords": "Token-level Fusion, Pre-trained Language Model, Bilinear Attention Network, Cross Attention Network, Drug Target Interaction", "abstract": "Predicting drug-target interaction (DTI) is critical in the drug discovery process. Despite remarkable advances in recent DTI models through the integration of representations from diverse drug and target encoders, such models often struggle to capture the fine-grained interactions between drugs and protein, i.e. the binding of specific drug atoms (or substructures) and key amino acids of proteins, which is crucial for understanding the binding mechanisms and optimising drug design. To address this issue, this paper introduces a novel model, called FusionDTI, which uses a token-level \\textbf{Fusion} module to effectively learn fine-grained information for \\textbf{D}rug-\\textbf{T}arget \\textbf{I}nteraction. In particular, our FusionDTI model uses the SELFIES representation of drugs to mitigate sequence fragment invalidation and incorporates the structure-aware (SA) vocabulary of target proteins to address the limitation of amino acid sequences in structural information, additionally leveraging pre-trained language models extensively trained on large-scale biomedical datasets as encoders to capture the complex information of drugs and targets. Experiments on three well-known benchmark datasets show that our proposed FusionDTI model achieves the best performance in DTI prediction compared with eight existing state-of-the-art baselines. Furthermore, our case study indicates that FusionDTI could highlight the potential binding sites, enhancing the explainability of the DTI prediction.", "title_embedding_index": 16057, "title_abs_embedding_index": 16082}, {"title": "Adaptive HL-Gaussian: A Value Function Learning Method with Dynamic Support Adjustment", "link_suffix": "/forum?id=cGu5LtGcRD", "link": "https://openreview.net/forum?id=cGu5LtGcRD", "pdf_link": "https://openreview.net/pdf?id=cGu5LtGcRD", "keywords": "value function learning, HL-Gaussian", "abstract": "Recent research indicates that using cross-entropy (CE) loss for value function learning surpasses traditional mean squared error (MSE) loss in performance and scalability, with the HL-Gaussian method showing notably strong results. However, this method requires a pre-specified support  for representing the categorical distribution of the value function, and an inappropriately chosen  interval for the support may not match the time-varying value function, potentially impeding the learning process. To address this issue, we theoretically establish that  HL-Gaussian  inherently introduces a projection error during the learning of the value function, which is dependent on the support interval. We further prove that an ideal interval should be sufficiently broad to reduce truncation-induced projection errors, yet not so  excessive as to counterproductively amplify them. Guided by these findings, we introduce the Adaptive HL-Gaussian (AHL-Gaussian) approach. This approach starts with a confined support interval and dynamically adjusts its range by minimizing the projection error. This ensures that the interval's size stabilizes to adapt to the learning value functions without  further expansion. We integrate AHL-Gaussian into several classic value-based algorithms and evaluate it on Atari 2600 games and Gym Mujoco. The results show that AHL-Gaussian significantly outperforms the vanilla baselines and standard HL-Gaussian with a static interval across the majority of tasks.", "title_embedding_index": 16058, "title_abs_embedding_index": 16083}, {"title": "MSM: Multi-Scale Mamba in Multi-Task Dense Prediction", "link_suffix": "/forum?id=bfZyAJ9ZAH", "link": "https://openreview.net/forum?id=bfZyAJ9ZAH", "pdf_link": "https://openreview.net/pdf?id=bfZyAJ9ZAH", "keywords": "multi-task learning, representation learning, multiscale, mamba", "abstract": "High-quality visual representations are crucial for success in multi-task dense prediction. The Mamba architecture, initially designed for natural language processing, has garnered interest for its potential in computer vision due to its efficient modeling of long-range dependencies. However, when applied to multi-task dense prediction, it reveals inherent limitations. Unlike text processing with diverse tokenization strategies, image token partitioning requires careful consideration of multiple options. In multi-task dense prediction, each task may require specific levels of granularity in scene structure. Unfortunately, the current Mamba implementation, which segments images into fixed patch scales, fails to match these requirements, leading to sub-optimal performance. This paper proposes a simple yet effective  Multi-Scale Mamba (MSM) for multi-task dense prediction. Firstly, we employ a novel Multi-Scale Scanning (MS-Scan) to establish global feature relationships at various scales. This module enhances the model's capability to deliver a comprehensive visual representation by integrating information across scales. Secondly, we adaptively merge task-shared information from multiple scales across different task branches. This design not only meets the diverse granularity demands of various tasks but also facilitates more nuanced cross-task feature interactions. Extensive experiments on two challenging benchmarks, i.e., NYUD-V2 and PASCAL-Context, show the superiority of our MSM vs its state-of-the-art competitors in multi-task dense prediction.", "title_embedding_index": 16059, "title_abs_embedding_index": 16084}, {"title": "Guiding VLM Agents with Process Rewards at Inference Time for GUI Navigation", "link_suffix": "/forum?id=jR6YMxVG9i", "link": "https://openreview.net/forum?id=jR6YMxVG9i", "pdf_link": "https://openreview.net/pdf?id=jR6YMxVG9i", "keywords": "Visual Language Model, Agent, GUI, Process Reward Model", "abstract": "Recent advancements in visual language models (VLMs) have notably enhanced their capabilities in handling complex Graphical User Interface (GUI) interaction tasks. Despite these improvements, current frameworks often struggle to generate correct actions in challenging GUI environments. State-of-the-art commercial VLMs are black-boxes, and fine-tuning open-source VLMs for GUI tasks requires significant resources. Additionally, existing trajectory-level evaluation and refinement techniques frequently fall short due to delayed feedback and local optimization issues. To address these challenges, we propose an approach that guides VLM agents with process supervision by a reward model during GUI navigation and control at inference time. This guidance allows the VLM agent to optimize actions at each inference step, thereby improving performance in both static and dynamic environments. In particular, our method demonstrates significant performance gains in the GUI navigation task setting, achieving a around 5% improvement in action accuracy for static environments and a near 15% increase in task success rate in dynamic environments. With further integration of trajectory reflection and retry mechanisms, we also demonstrate even greater enhancement in task success.", "title_embedding_index": 16060, "title_abs_embedding_index": 16085}, {"title": "Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint", "link_suffix": "/forum?id=YeSxbRrDRl", "link": "https://openreview.net/forum?id=YeSxbRrDRl", "pdf_link": "https://openreview.net/pdf?id=YeSxbRrDRl", "keywords": "Deep imbalanced regression, sparse data region optimization", "abstract": "Imbalanced data distributions are prevalent in real-world scenarios, posing significant challenges in both imbalanced classification and imbalanced regression tasks. They often cause deep learning models to overfit in areas of high sample density (many-shot regions) while underperforming in areas of low sample density (few-shot regions). This characteristic restricts the utility of deep learning models in various sectors, notably healthcare, where areas with few-shot data hold greater clinical relevance. While recent studies have shown the benefits of incorporating distribution information in imbalanced classification tasks, such strategies are rarely explored in imbalanced regression. In this paper, we address this issue by introducing a novel loss function, termed Dist Loss, designed to minimize the distribution distance between the model's predictions and the target labels in a differentiable manner, effectively integrating distribution information into model training. Dist Loss enables deep learning models to regularize their output distribution during training, effectively enhancing their focus on few-shot regions. We have conducted extensive experiments across three datasets spanning computer vision and healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The results demonstrate that Dist Loss effectively mitigates the negative impact of imbalanced data distribution on model performance, achieving state-of-the-art results in sparse data regions. Furthermore, Dist Loss is easy to integrate, complementing existing methods. Our code will be made publicly available following the review process.", "title_embedding_index": 16061, "title_abs_embedding_index": 16086}, {"title": "Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs", "link_suffix": "/forum?id=f9w89OY2cp", "link": "https://openreview.net/forum?id=f9w89OY2cp", "pdf_link": "https://openreview.net/pdf?id=f9w89OY2cp", "keywords": "AI Alignment, LLM Test Time Preference Adaptation, Alignment without Training and Fine-tuing", "abstract": "How to align large language models (LLMs) with user preferences from a static general dataset has been frequently studied. However, user preferences are usually personalized, changing, and diverse. This leads to the problem that the actual user preferences often do not coincide with those trained by the model developers in the practical use of LLMs. Since we cannot collect enough data and retrain for every demand, researching efficient real-time preference adaptation methods based on the backbone LLMs during test time is important. To this end, we introduceAmulet, a novel, training-free framework that formulates the decoding process of every token as a separate online learning problem with the guidance of simple user-provided prompts, thus enabling real-time optimization to satisfy users' personalized preferences. To reduce the computational cost brought by this optimization process for each token, we additionally provide a closed-form solution for each iteration step of the optimization process, thereby reducing the computational time cost to a negligible level. The detailed experimental results demonstrate that Amulet can achieve significant performance improvements in rich settings with combinations of different LLMs, datasets, and user preferences, while maintaining acceptable computational efficiency.", "title_embedding_index": 16062, "title_abs_embedding_index": 16087}, {"title": "Revisiting a Design Choice in Gradient Temporal Difference Learning", "link_suffix": "/forum?id=38BBWrXUhP", "link": "https://openreview.net/forum?id=38BBWrXUhP", "pdf_link": "https://openreview.net/pdf?id=38BBWrXUhP", "keywords": "gradient temporal difference learning", "abstract": "Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. The seminal work Sutton et al. (2008) pioneers Gradient Temporal Difference learning (GTD) as the first solution to the deadly triad, which has enjoyed massive success thereafter. During the derivation of GTD, some intermediate algorithm, called $A^\\top$TD, was invented but soon deemed inferior. In this paper, we revisit this $A^\\top$TD and prove that a variant of $A^\\top$TD, called $A_t^\\top$TD, is also an effective solution to the deadly triad. Furthermore, this $A_t^\\top$TD only needs one set of parameters and one learning rate. By contrast, GTD has two sets of parameters and two learning rates, making it hard to tune in practice. We provide both asymptotic and finite sample analysis for $A^\\top_t$TD, where the convergence rate is on par with the canonical on-policy temporal difference learning. Key to our analysis is a novel refined discretization of limiting ODEs.", "title_embedding_index": 16063, "title_abs_embedding_index": 16088}, {"title": "A Dual-Agent Adversarial Framework for Generalizable Reinforcement Learning", "link_suffix": "/forum?id=xAYOfMV264", "link": "https://openreview.net/forum?id=xAYOfMV264", "pdf_link": "https://openreview.net/pdf?id=xAYOfMV264", "keywords": "Generalizable Reinforcement learning, Adversarial Learning", "abstract": "Recently, empowered with the powerful capabilities of neural networks, reinforcement learning (RL) has successfully tackled numerous challenging tasks. However, while these models demonstrate enhanced decision-making abilities, they are increasingly prone to overfitting. For instance, a trained RL model often fails to generalize to even minor variations of the same task, such as a change in background color or other minor semantic differences. To address this issue, we propose a dual-agent adversarial policy learning framework, which allows agents to spontaneously learn the underlying semantics without introducing any human prior knowledge. Specifically, our framework involves a game process between two agents: each agent seeks to maximize the impact of perturbing on the opponent's policy by producing representation differences for the same state, while maintaining its own stability against such perturbations. This interaction encourages agents to learn generalizable policies, capable of handling irrelevant features from the high-dimensional observations. Extensive experimental results on the Procgen benchmark demonstrate that the adversarial process significantly improves the generalization performance of both agents, while also being applied to various RL algorithms, e.g., Proximal Policy Optimization (PPO). With the adversarial framework, the RL agent outperforms the baseline methods by a significant margin, especially in hard-level tasks, marking a significant step forward in the generalization capabilities of deep reinforcement learning.", "title_embedding_index": 16064, "title_abs_embedding_index": 16089}, {"title": "AdCorDA: Classifier Refinement via Adversarial Correction and Domain Adaptation", "link_suffix": "/forum?id=uSiyu6CLPh", "link": "https://openreview.net/forum?id=uSiyu6CLPh", "pdf_link": "https://openreview.net/pdf?id=uSiyu6CLPh", "keywords": "adversarial correction, domain adaptation, curriculum learning, adversarial attacks", "abstract": "This paper describes a simple yet effective technique for refining a pretrained classifier network. The proposed AdCorDA method consists of two stages - adversarial correction followed by domain adaptation. Adversarial correction uses adversarial attacks to correct misclassified training-set classifications. The incorrectly classified samples of the training set are removed and replaced with the adversarially corrected samples to form a new training set, and then, in the second stage, domain adaptation is performed back to the original training set. Extensive experimental validations show significant accuracy boosts of over 5% on the CIFAR-100 dataset and 1% on the CINIC-10 dataset. The technique can be straightforwardly applied to the refinement of weight-quantized neural networks, where experiments show substantial enhancement in performance over the baseline. The adversarial correction technique also results in enhanced robustness to adversarial attacks.", "title_embedding_index": 16065, "title_abs_embedding_index": 16090}, {"title": "Adaptive higher order reversible integrators for memory efficient deep learning", "link_suffix": "/forum?id=MD4ifad9v5", "link": "https://openreview.net/forum?id=MD4ifad9v5", "pdf_link": "https://openreview.net/pdf?id=MD4ifad9v5", "keywords": "neural ODE, backpropagation, reversible neural networks, learning dynamical systems, high-order integration methods, variable time-steps", "abstract": "The depth of networks plays a crucial role in the effectiveness of deep learning. However, the memory requirement for backpropagation scales linearly with the number of layers, which leads to memory bottlenecks during training. Moreover, deep networks are often unable to handle time-series appearing at irregular intervals. These issues can be resolved by considering continuous-depth networks based on the neural ODE framework in combination with reversible integration methods that allow for variable time-steps. Reversibility of the method ensures that the memory requirement for training is independent of network depth, while variable time-steps are required for assimilating time-series data on irregular intervals. However, at present, there are no known higher-order reversible methods with this property. High-order methods are especially important when a high level of accuracy in learning is required or when small time-steps are necessary due to large errors in time integration of neural ODEs, for instance in context of complex dynamical systems such as Kepler systems and molecular dynamics. The requirement of small time-steps when using a low-order method can significantly increase the computational cost of training as well as inference. In this work, we present an approach for constructing high-order reversible methods that allow adaptive time-stepping. Our numerical tests show both the advantages in computational speed and improved training accuracy of the new networks when applied to the task of learning dynamical systems.", "title_embedding_index": 16066, "title_abs_embedding_index": 16091}, {"title": "Force-Guided Bridge Matching for Full-Atom Time-Coarsened Dynamics of Peptides", "link_suffix": "/forum?id=NSlvSDQ8aE", "link": "https://openreview.net/forum?id=NSlvSDQ8aE", "pdf_link": "https://openreview.net/pdf?id=NSlvSDQ8aE", "keywords": "molecular dynamics, force-guided bridge matching, graph neural network", "abstract": "Molecular Dynamics (MD) is crucial in various fields such as materials science, chemistry, and pharmacology to name a few. Conventional MD software struggles with the balance between time cost and prediction accuracy, which restricts its wider application. Recently, data-driven approaches based on deep generative models have been devised for time-coarsened dynamics, which aim at learning dynamics of diverse molecular systems over a long timestep, enjoying both universality and efficiency. Nevertheless, most current methods are designed solely to learn from the data distribution regardless of the underlying Boltzmann distribution, and the physics priors such as energies and forces are constantly overlooked. In this work, we propose a conditional generative model called Force-guided Bridge Matching (FBM), which learns full-atom time-coarsened dynamics and targets the Boltzmann-constrained distribution. With the guidance of our delicately-designed intermediate force field, FBM leverages favourable physics priors into the generation process, giving rise to enhanced simulations. Experiments on two datasets consisting of peptides verify our superiority in terms of comprehensive metrics and demonstrate transferability to unseen systems.", "title_embedding_index": 16067, "title_abs_embedding_index": 16092}, {"title": "Hierarchical Multi-Grained Reasoning for Object Concept Learning", "link_suffix": "/forum?id=ZcBhd1F7PA", "link": "https://openreview.net/forum?id=ZcBhd1F7PA", "pdf_link": "https://openreview.net/pdf?id=ZcBhd1F7PA", "keywords": "Hierarchical Multi-Grained Reasoning for Object Concept Learning", "abstract": "Human beings can easily understand object concepts involving attributes and affordances. Recently, to simulate this ability, Object Concept Learning (OCL) has been introduced as a new task to recognize attributes and affordances related to a given object. \nOCL is essentially a many-to-many mapping problem: While an object may possess multiple different concepts, a concept can also belong to multiple different objects. \nIn this regard, the prevailing method of learning discriminative representation---which is effective in the single-mapping cases---often fails in OCL.\nInspired by the reasoning mechanism of human beings, in this paper, we propose Hierarchical Multi-Grained Reasoning (HGR) for OCL, aiming to infer object-related concepts from coarse-to-fine and counterfactual grains.\nSpecifically, we first propose a coarse-to-fine hierarchical reasoning module that exploits multi-step learnable prompts to progressively localize object-relevant concept information. Subsequently, multiple counterfactual samples are selected to strengthen the relations between objects and concepts, which further improves the reasoning performance. In the experiments, our method is evaluated on multiple benchmarks. Significant performance gains and extensive visualization analysis demonstrate the superiorities of our method.", "title_embedding_index": 16068, "title_abs_embedding_index": 16093}, {"title": "Invisibility Stickers Against LiDAR: Adversarial Attacks on Point Cloud Intensity for LiDAR-based Detection", "link_suffix": "/forum?id=P2snmtUBkQ", "link": "https://openreview.net/forum?id=P2snmtUBkQ", "pdf_link": "https://openreview.net/pdf?id=P2snmtUBkQ", "keywords": "LiDAR-based detection, adversarial examples, deep learning", "abstract": "Point cloud detection is crucial in applications such as autonomous driving systems and robotics. These systems utilize onboard LiDAR sensors to capture input point clouds, consisting of numerous three-dimensional coordinate points and their corresponding intensity of laser reflection. Recent studies have proposed various adversarial schemes to highlight the vulnerability of point cloud detectors. However, these studies primarily focused on generating or perturbing the coordinate positions of input points and are hard to attack in the physical world, while largely overlooking the significance of their intensity.\tThrough our exploration, we found that perturbing point cloud intensity poses significant security risks for point cloud object detectors. To the best of our knowledge, we are the first to attack on point cloud intensity and we propose an effective adversarial attack scheme, named I-ADV. Our method employs a voxel partition scheme to enhance physical implementation. To boost attack performance, we incorporate a gradient enhancement technique using 3D angle and distance features, along with an extremum-based gradient fusion strategy.\tExtensive experimental results demonstrate that by altering only point cloud intensity, our approach achieves state-of-the-art performance across detectors with various input representations, attaining attack success rates between 83.9% and 99.1%. Comprehensive ablation studies confirm the effectiveness and generality of the method\u2019s components. Additionally, comparing different attack schemes underscores the advantages of our point cloud intensity attack method in both performance and real-world applicability.", "title_embedding_index": 16069, "title_abs_embedding_index": 16094}, {"title": "PERFT: Parameter-Efficient Routed Fine-Tuning for Mixture-of-Expert Model", "link_suffix": "/forum?id=PPjpGTPG5K", "link": "https://openreview.net/forum?id=PPjpGTPG5K", "pdf_link": "https://openreview.net/pdf?id=PPjpGTPG5K", "keywords": "Mixture of Expert, Parameter efficient fine-tuning, Large Language Model", "abstract": "The Mixture-of-Experts (MoE) paradigm has emerged as a powerful approach for scaling transformers with improved resource utilization. \nHowever, efficiently fine-tuning MoE models remains largely underexplored.\nInspired by recent works on Parameter-Efficient Fine-Tuning (PEFT), we present a unified framework for integrating PEFT modules directly into the MoE mechanism.\nAligning with the core principles and architecture of MoE, our framework encompasses a set of design dimensions including various functional and composition strategies.\nBy combining design choices within our framework, we introduce Parameter-Efficient Routed Fine-Tuning (PERFT) as a flexible and scalable family of PEFT strategies tailored for MoE models.\nExtensive experiments on adapting OLMoE-1B-7B and Mixtral-8\u00d77B for commonsense and arithmetic reasoning tasks demonstrate the effectiveness, scalability, and intriguing dynamics of PERFT. \nAdditionally, we provide empirical findings for each specific design choice to facilitate better application of MoE and PEFT.", "title_embedding_index": 16070, "title_abs_embedding_index": 16095}, {"title": "IGOR: Image-GOal Representations are the Atomic Building Blocks for Next-Level Generalization in Embodied AI", "link_suffix": "/forum?id=bpdIZTIVq8", "link": "https://openreview.net/forum?id=bpdIZTIVq8", "pdf_link": "https://openreview.net/pdf?id=bpdIZTIVq8", "keywords": "Embodied AI, Foundation Models, Generalist Agents", "abstract": "We introduce Image-GOal Representations (IGOR), aiming to learn a unified, semantically consistent action space across human and various robots. Through this unified latent action space, IGOR enables knowledge transfer among large-scale robot and human activity data. We achieve this by compressing visual changes between an initial image and its goal state into latent actions. IGOR allows us to generate latent action labels for internet-scale video data. This unified latent action space enables the training of foundation policy and world models across a wide variety of tasks performed by both robots and humans. We demonstrate that: (1) IGOR learns a semantically consistent action space for both human and robots, characterizing various possible motions of objects representing the physical interaction knowledge; (2) IGOR can \u201cmigrate\u201d the movements of the object in the one video to other videos, even across human and robots, by jointly using the latent action model and world model; (3) IGOR can learn to align latent actions with natural language through the foundation policy model, and integrate latent actions with a low-level policy model to achieve effective robot control. We believe IGOR opens new possibilities for human-to-robot knowledge transfer and control. See video demonstrations on our anonymous webpage.", "title_embedding_index": 16071, "title_abs_embedding_index": 16096}, {"title": "Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors", "link_suffix": "/forum?id=tIBAOcAvn4", "link": "https://openreview.net/forum?id=tIBAOcAvn4", "pdf_link": "https://openreview.net/pdf?id=tIBAOcAvn4", "keywords": "adversarial attack, hard-label attack, black-box attack, gradient estimation, surrogate model", "abstract": "One of the most practical and challenging types of black-box adversarial attacks is the hard-label attack, where only top-1 predicted labels are available. One effective approach is to search for the optimal ray direction from the benign image that minimizes the $\\ell_p$ norm distance to the adversarial region. The unique advantage of this approach is that it transforms the hard-label attack into a continuous optimization problem. The objective function value is the ray's radius and can be obtained through a binary search with high query cost. Existing methods use a \"sign trick\" in gradient estimation to reduce queries. In this paper, we theoretically analyze the quality of this gradient estimation, proposing a novel prior-guided approach to improve ray search efficiency, based on theoretical and experimental analysis. Specifically, we utilize the transfer-based priors from surrogate models, and our gradient estimators appropriately integrate them by approximating the projection of the true gradient onto the subspace spanned by these priors and some random directions, in a query-efficient way. We theoretically derive the expected cosine similarity between the obtained gradient estimators and the true gradient, and demonstrate the improvement brought by using priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that our approach significantly outperforms 11 state-of-the-art methods in query efficiency. Code will be released.", "title_embedding_index": 16072, "title_abs_embedding_index": 16097}, {"title": "Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy", "link_suffix": "/forum?id=eB7T1bqthA", "link": "https://openreview.net/forum?id=eB7T1bqthA", "pdf_link": "https://openreview.net/pdf?id=eB7T1bqthA", "keywords": "Bandits, Multi-Armed Bandits, Online Learning, Cost-Subsidy, Cost Subsidy, Improved UCB, UCB, Elimination Algorithms", "abstract": "Multi-armed bandits (MAB) are commonly used in sequential online decision-making when the reward of each decision is an unknown random variable. In practice, however, the typical goal of maximizing total reward may be less important than minimizing the total cost of the decisions taken, subject to a reward constraint. For example, we may seek to make decisions that have at least the reward of a reference ``default'' decision. This problem was recently introduced in the Multi-Armed Bandits with Cost Subsidy (MAB-CS) framework. MAB-CS is broadly applicable to problem domains where a primary metric (cost) is constrained by a secondary metric (reward), and there is an inability to explicitly determine the trade-off between these metrics. In our work, we first introduce the Pairwise-Elimination algorithm for a simplified variant of the cost subsidy problem with a known reference arm. We then generalize PE to PE-CS to solve the MAB-CS problem in the setting where the reference arm is the un-identified optimal arm. Next, we analyze the performance of both PE and PE-CS on the dual metrics of Cost and Quality Regret. Our instance-dependent analysis of PE and PE-CS reveals that both algorithms have an order-wise logarithmic upper bound on Cost and Quality Regret, making our policy the first with such a guarantee. Finally, experiments are conducted using the MovieLens 25M dataset for both PE and PE-CS and using a synthetic toy experiment for PE-CS revealing that our method invariably outperforms the ETC-CS baseline from the literature.", "title_embedding_index": 16073, "title_abs_embedding_index": 16098}, {"title": "Predicting Network Motif Fingerprints with Graph Neural Networks", "link_suffix": "/forum?id=PZVVOeu6xx", "link": "https://openreview.net/forum?id=PZVVOeu6xx", "pdf_link": "https://openreview.net/pdf?id=PZVVOeu6xx", "keywords": "motifs, graph representation learning, synthetic data, significance profiles", "abstract": "Graph Neural Networks (GNNs) are a predominant method for graph representation learning. However, beyond subgraph frequency estimation, their application to network motif prediction remains underexplored, with no established benchmarks in the literature. We propose to address this problem, framing motif prediction as an extension of subgraph frequency estimation. Our approach formulates motif estimation as a multitarget regression problem, optimising for interpretability and improving stability and scalability on large graphs. We validate our method using a large synthetic dataset generated by graph generators that mimic real-world data, and further test it on real-world graphs. Our experiments reveal that 1-WL limited models trained on synthetic data struggle to predict accurately motif profiles of real-world networks. However, apart from their reasonable performance within synthetic data, they can generalise to approximate the graph generation processes of real-world networks by comparing their predicted motif profiles with the ones originating from synthetic data. This first study on GNN-based motif prediction sets a benchmark and should open pathways for further developing the connection between motif profiles and subgraph frequency from a graph representation learning perspective.", "title_embedding_index": 16074, "title_abs_embedding_index": 16099}]