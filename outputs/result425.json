[{"title": "Zero-Shot Text-to-Speech from Continuous Text Streams", "link_suffix": "/forum?id=RK3Gj9J5my", "link": "https://openreview.net/forum?id=RK3Gj9J5my", "pdf_link": "https://openreview.net/pdf?id=RK3Gj9J5my", "keywords": "zero-shot text-to-speech, streaming, state-space models", "abstract": "Existing zero-shot text-to-speech (TTS) systems are typically designed to process complete sentences and are constrained by the maximum duration for which they have been trained. However, in many streaming applications, texts arrive continuously in short chunks, necessitating instant responses from the system. We identify the essential capabilities required for chunk-level streaming and introduce L3Speech, a stream-aware model that supports infinitely long speech generation, text-audio stream synchronization, and seamless transitions between short speech chunks. To achieve these, we propose (1) adopting Mamba, a class of sequence modeling distinguished by linear-time decoding, which is augmented by cross-attention mechanisms for conditioning, (2) utilizing rotary positional embeddings in the computation of cross-attention, enabling the model to process an infinite text stream by sliding a window, and (3) decoding with semantic guidance, a technique that aligns speech with the transcript during inference with minimal overhead. Experimental results demonstrate that our models are competitive with state-of-the-art language model-based zero-shot TTS models, while also providing flexibility to support a wide range of streaming scenarios.", "title_embedding_index": 21200, "title_abs_embedding_index": 21225}, {"title": "Does Spatial Cognition Emerge in Frontier Models?", "link_suffix": "/forum?id=WK6K1FMEQ1", "link": "https://openreview.net/forum?id=WK6K1FMEQ1", "pdf_link": "https://openreview.net/pdf?id=WK6K1FMEQ1", "keywords": "Frontier models, spatial cognition", "abstract": "Not yet. We present SPACE, a benchmark that systematically evaluates spatial cognition in frontier models. Our benchmark builds on decades of research in cognitive science. It evaluates large-scale mapping abilities that are brought to bear when an organism traverses physical environments, smaller-scale reasoning about object shapes and layouts, and cognitive infrastructure such as spatial attention and memory. For many tasks, we instantiate parallel presentations via text and images, allowing us to benchmark both large language models and large multimodal models. Results suggest that contemporary frontier models fall short of the spatial intelligence of animals, performing near chance level on a number of classic tests of animal cognition.", "title_embedding_index": 21201, "title_abs_embedding_index": 21226}, {"title": "(Pre-)training Dynamics: Scaling Generalization with First-Order Logic", "link_suffix": "/forum?id=eRkNNQRppH", "link": "https://openreview.net/forum?id=eRkNNQRppH", "pdf_link": "https://openreview.net/pdf?id=eRkNNQRppH", "keywords": "phase transitions, pretraining dynamics, generalization, interpretability", "abstract": "Transformer-based models have demonstrated a remarkable capacity for learning complex nonlinear relationships. While previous research on generalization dynamics has primarily focused on small transformers (1-2 layers) and simple tasks like XOR and modular addition, we extend this investigation to larger models with 125M parameters, trained on a more sophisticated first-order logic (FOL) task. We introduce a novel FOL dataset that allows us to systematically explore generalization across varying levels of complexity. Our analysis of the pretraining dynamics reveals a series of distinct phase transitions corresponding to the hierarchical generalization of increasingly complex operators and rule sets within the FOL framework. Our task and model establish a testbed for investigating pretraining dynamics at scale, offering a foundation for future research on the learning trajectories of advanced AI systems.", "title_embedding_index": 21202, "title_abs_embedding_index": 21227}, {"title": "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias", "link_suffix": "/forum?id=QQBPWtvtcn", "link": "https://openreview.net/forum?id=QQBPWtvtcn", "pdf_link": "https://openreview.net/pdf?id=QQBPWtvtcn", "keywords": "novel view synthesis, transformer, large model", "abstract": "We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods---from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps)---addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality, delivering superior performance even with reduced computational resources (1-2 GPUs). Please see our anonymous website for more details:https://lvsm-web.github.io/", "title_embedding_index": 21203, "title_abs_embedding_index": 21228}, {"title": "NIAQUE: Neural Interpretable Any-Quantile Estimation - Towards Large Probabilistic Regression Models", "link_suffix": "/forum?id=5x65bI0aY8", "link": "https://openreview.net/forum?id=5x65bI0aY8", "pdf_link": "https://openreview.net/pdf?id=5x65bI0aY8", "keywords": "deep probabilistic regression, large regression models", "abstract": "State-of-the-art models in computer vision and natural language processing largely owe their success to the ability to represent massive prior knowledge contained in multiple datasets by learning over multiple tasks. However, large-scale cross-dataset studies of deep probabilistic regression models are missing, presenting a significant research gap in this area. To bridge this gap, in this paper we propose, analyze, and evaluate a novel probabilistic regression model, capable of solving multiple regression tasks represented by different datasets. To demonstrate the feasibility of such operation and the efficacy of our model, we define a novel multi-dataset probabilistic regression benchmark LPRM-101. Our results on this benchmark imply that the proposed model is capable of solving a probabilistic regression problem jointly over multiple datasets. The model, which we call NIAQUE, learns a meaningful cross-dataset representation and scores favourably against strong tree-based baselines and Transformer.", "title_embedding_index": 21204, "title_abs_embedding_index": 21229}, {"title": "Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective", "link_suffix": "/forum?id=tcvMzR2NrP", "link": "https://openreview.net/forum?id=tcvMzR2NrP", "pdf_link": "https://openreview.net/pdf?id=tcvMzR2NrP", "keywords": "flow matching, discrete generative modeling", "abstract": "The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.\nIn this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. \nThrough the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. \nFurthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.\nWe empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.", "title_embedding_index": 21205, "title_abs_embedding_index": 21230}, {"title": "Unsupervised Whole Object Discovery by Contextual Grouping with Repulsion", "link_suffix": "/forum?id=2LOtSPmopq", "link": "https://openreview.net/forum?id=2LOtSPmopq", "pdf_link": "https://openreview.net/pdf?id=2LOtSPmopq", "keywords": "Unsupervised Object Discovery, Unsupervised Whole Object Segmentation, Co-Segmentation, Normalized Cut, Attraction and Repulsion", "abstract": "It is challenging to discover and segment whole objects from unlabeled images, as features unsupervisedly learned on images tend to focus on distinctive appearances (e.g., the face rather than the torso), and grouping by feature similarity could reveal only these representative parts, not the whole objects (e.g., the entire human body). Our key insight is that, an object of distinctive parts pops out as a whole, due not only to how similar they are to each other, but also to it how different they are from their contexts within an image or across related images. The latter could be crucial for binding different parts into a coherent whole without preconception of objects. We formulate our idea for unsupervised object segmentation in a spectral graph partitioning framework, where nodes are patches and edges are grouping cues between patches, measured by feature similarity for attraction, and by feature dissimilarity for repulsion. We seek the graph cuts that maximize within-group attraction and figure-ground repulsion while minimizing figure/ground attraction and within-group repulsion. Our simple method consistently outperforms the state-of-the-art on unsupervised object discovery, figure/ground saliency detection, and unsupervised video object segmentation benchmarks. In particular, it excels at discovering whole objects instead of salient parts.", "title_embedding_index": 21206, "title_abs_embedding_index": 21231}, {"title": "Cut Your Losses in Large-Vocabulary Language Models", "link_suffix": "/forum?id=E4Fk3YuG56", "link": "https://openreview.net/forum?id=E4Fk3YuG56", "pdf_link": "https://openreview.net/pdf?id=E4Fk3YuG56", "keywords": "large language model, large vocabulary, efficient", "abstract": "As language models grow ever larger, so do their vocabularies.\nThis has shifted the memory footprint of LLMs during training disproportionately to one single layer: the cross-entropy in the loss computation.\nCross-entropy builds up a logit matrix with entries for each pair of input tokens and vocabulary items and, for small models, consumes an order of magnitude more memory than the rest of the LLM combined.\nWe propose Cut Cross-Entropy (CCE), a method that computes the cross-entropy loss without materializing the logits for all tokens into global memory.\nRather, CCE only computes the logit for the correct token and evaluates the log-sum-exp over all logits on the fly.\nWe implement a custom kernel that performs the matrix multiplications and the log-sum-exp reduction over the vocabulary in flash memory, making global memory consumption for the cross-entropy computation negligible. This has a dramatic effect. Taking the Gemma 2 (2B) model as an example, CCE reduces the memory footprint of the loss computation from 24 GB to 1 MB, and the total training-time memory consumption of the classifier head from 28 GB to 1 GB.\nTo improve the throughput of CCE, we leverage the inherent sparsity of softmax and propose to skip elements of the gradient computation that have a negligible (i.e. below numerical precision) contribution to the gradient.\nExperiments demonstrate that the dramatic reduction in memory consumption is accomplished without sacrificing training speed or convergence.", "title_embedding_index": 21207, "title_abs_embedding_index": 21232}, {"title": "Adaptive teachers for amortized samplers", "link_suffix": "/forum?id=BdmVgLMvaf", "link": "https://openreview.net/forum?id=BdmVgLMvaf", "pdf_link": "https://openreview.net/pdf?id=BdmVgLMvaf", "keywords": "amortized inference, generative models, reinforcement learning, GFlowNets", "abstract": "Amortized inference is the task of training a parametric model, such as a neural network, to approximate a distribution with a given unnormalized density where exact sampling is intractable. When sampling is modeled as a sequential decision-making process, reinforcement learning (RL) methods, such as generative flow networks, can be used to train the sampling policy. Off-policy RL training facilitates the discovery of diverse, high-reward candidates, but existing methods still face challenges in efficient exploration. We propose to use an adaptive training distribution (the Teacher) to guide the training of the primary amortized sampler (the Student) by prioritizing high-loss regions. The  Teacher, an auxiliary behavior model, is trained to sample high-error regions of the Student and can generalize across unexplored modes, thereby enhancing mode coverage by providing an efficient training curriculum. We validate the effectiveness of this approach in a synthetic environment designed to present an exploration challenge, two diffusion-based sampling tasks, and four biochemical discovery tasks demonstrating its ability to improve sample efficiency and mode coverage.", "title_embedding_index": 21208, "title_abs_embedding_index": 21233}, {"title": "Flow Matching for Posterior Inference with Simulator Feedback", "link_suffix": "/forum?id=DoDNJdDntB", "link": "https://openreview.net/forum?id=DoDNJdDntB", "pdf_link": "https://openreview.net/pdf?id=DoDNJdDntB", "keywords": "generative modeling, simulation-based inference, astronomy", "abstract": "Flow-based generative modeling is a powerful tool for solving inverse problems in physical sciences that can be used for sampling and likelihood evaluation with much lower inference times than traditional methods. We propose to refine flows with additional control signals based on a simulator. Control signals can include gradients and a problem-specific cost function if the simulator is differentiable, or they can be fully learned from the simulator output. \nIn our proposed method, we pretrain the flow network and include feedback from the simulator exclusively for finetuning, therefore requiring only a small amount of additional parameters and compute. We motivate our design choices on several benchmark problems for simulation-based inference and evaluate flow matching with simulator feedback against classical MCMC methods for modeling strong gravitational lens systems, a challenging inverse problem in astronomy. We demonstrate that including feedback from the simulator improves the accuracy by $53$%, making it competitive with traditional techniques while being up to 67x faster for inference. Upon acceptance, we will make our code publicly available.", "title_embedding_index": 21209, "title_abs_embedding_index": 21234}, {"title": "LayerFusion: Harmonized Multi-Layer Text-to-Image Generation with Generative Priors", "link_suffix": "/forum?id=OE2T7AgQFN", "link": "https://openreview.net/forum?id=OE2T7AgQFN", "pdf_link": "https://openreview.net/pdf?id=OE2T7AgQFN", "keywords": "diffusion, T2I, generative models, rgb, rgba, layer, layer diffusion", "abstract": "Large-scale diffusion models have achieved remarkable success in generating high-quality images from textual descriptions, gaining popularity across various applications. However, the generation of layered content, such as transparent images with foreground and background layers, remains an under-explored area. Layered content generation is crucial for creative workflows in fields like graphic design, animation, and digital art, where layer-based approaches are fundamental for flexible editing and composition. In this paper, we propose a novel image generation pipeline based on Latent Diffusion Models (LDMs) that generates images with two layers: a foreground layer (RGBA) with transparency information and a background layer (RGB). Unlike existing methods that generate these layers sequentially, our approach introduces a harmonized generation mechanism that enables dynamic interactions between the layers for more coherent outputs. We demonstrate the effectiveness of our method through extensive qualitative and quantitative experiments, showing significant improvements in visual coherence, image quality, and layer consistency compared to baseline methods.", "title_embedding_index": 21210, "title_abs_embedding_index": 21235}, {"title": "MLPs Learn In-Context on Regression and Classification Tasks", "link_suffix": "/forum?id=MbX0t1rUlp", "link": "https://openreview.net/forum?id=MbX0t1rUlp", "pdf_link": "https://openreview.net/pdf?id=MbX0t1rUlp", "keywords": "In-context learning, relational reasoning, synthetic tasks, MLP, MLP-Mixer, Transformer", "abstract": "In-context learning (ICL), the remarkable ability to solve a task from only input exemplars, is often assumed to be a unique hallmark of Transformer models. By examining commonly employed synthetic ICL tasks, we demonstrate that multi-layer perceptrons (MLPs) can also learn in-context. Moreover, MLPs, and the closely related MLP-Mixer models, learn in-contextcompetitively with Transformers given the same compute budgetin this setting. We further show that MLPsoutperformTransformers on a series of classical tasks from psychology designed to test relational reasoning, which are closely related to in-context classification. These results underscore a need for studying in-context learning beyond attention-based architectures, while also challenging strong prior arguments about MLPs' limited ability to solve relational tasks. Altogether, our results highlight the unexpected competence of MLPs, and support the growing interest in all-MLP alternatives to task-specific architectures.", "title_embedding_index": 21211, "title_abs_embedding_index": 21236}, {"title": "Restyling Unsupervised Concept Based Interpretable Networks with Generative Models", "link_suffix": "/forum?id=CexatBp6rx", "link": "https://openreview.net/forum?id=CexatBp6rx", "pdf_link": "https://openreview.net/pdf?id=CexatBp6rx", "keywords": "explainability, generative models, concepts", "abstract": "Developing inherently interpretable models for prediction has gained prominence in recent years. A subclass of these models, wherein the interpretable network relies on learning high-level concepts, are valued because of closeness of concept representations to human communication. However, the visualization and understanding of the learnt unsupervised dictionary of concepts encounters major limitations, especially for large-scale images. We propose here a novel method that relies on mapping the concept features to the latent space of a pretrained generative model. The use of a generative model enables high quality visualization, and lays out an intuitive and interactive procedure for better interpretation of the learnt concepts by imputing concept activations and visualizing generated modifications. Furthermore, leveraging pretrained generative models has the additional advantage of making the training of the system more efficient. We quantitatively ascertain the efficacy of our method in terms of accuracy of the interpretable prediction network, fidelity of reconstruction, as well as faithfulness and consistency of learnt concepts. The experiments are conducted on multiple image recognition benchmarks for large-scale images.", "title_embedding_index": 21212, "title_abs_embedding_index": 21237}, {"title": "Active Learning for Neural PDE Solvers", "link_suffix": "/forum?id=x4ZmQaumRg", "link": "https://openreview.net/forum?id=x4ZmQaumRg", "pdf_link": "https://openreview.net/pdf?id=x4ZmQaumRg", "keywords": "Active Learning, Neural PDE Solvers, Scientific Machine Learning, Benchmark, Framework, Neural Operators", "abstract": "Solving partial differential equations (PDEs) is a fundamental problem in engineering and science.  While neural PDE solvers can be more efficient than established numerical solvers, they often require large amounts of training data that is costly to obtain. Active Learning (AL) could help surrogate models reach the same accuracy with smaller training sets by querying classical solvers with more informative initial conditions and PDE parameters. While AL is more common in other domains, it has yet to be studied extensively for neural PDE solvers. To bridge this gap, we introduce AL4PDE, a modular and extensible active learning benchmark. It provides multiple parametric PDEs and state-of-the-art surrogate models for the solver-in-the-loop setting, enabling the evaluation of existing and the development of new AL methods for PDE solving. We use the benchmark to evaluate batch active learning algorithms such as uncertainty- and feature-based methods. We show that AL reduces the average error by up to 71% compared to random sampling and significantly reduces worst-case errors. Moreover, AL generates similar datasets across repeated runs, with consistent distributions over the PDE parameters and initial conditions. The acquired datasets are reusable, providing benefits for surrogate models not involved in the data generation.", "title_embedding_index": 21213, "title_abs_embedding_index": 21238}, {"title": "Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding", "link_suffix": "/forum?id=yHj6EunfVQ", "link": "https://openreview.net/forum?id=yHj6EunfVQ", "pdf_link": "https://openreview.net/pdf?id=yHj6EunfVQ", "keywords": "spatio-temporal video grounding, weakly supervised learning", "abstract": "In this work, we focus on Weakly Supervised Spatio-Temporal Video Grounding (WSTVG). It is a multimodal task aimed at localizing specific subjects  spatio-temporally based on textual queries without bounding box supervision. Motivated by recent advancements in multi-modal foundation models for grounding tasks, we first explore the potential of state-of-the-art object detection models for WSTVG. Despite their robust zero-shot capabilities, our adaptation reveals significant limitations, including inconsistent temporal predictions, inadequate understanding of complex queries, and challenges in adapting to difficult scenarios.\nWe propose CoSPaL (Contextual Self-Paced Learning), a novel approach which is designed to overcome these limitations. CoSPaL integrates three core components: (1) Tubelet Phrase Grounding (TPG), which introduces spatio-temporal prediction by linking textual queries to tubelets; (2) Contextual Referral Grounding (CRG), which improves comprehension of complex queries by extracting contextual information to refine object identification over time; and (3) Self-Paced Scene Understanding (SPS), a training paradigm that progressively increases task difficulty, enabling the model to adapt to complex scenarios by transitioning from coarse to fine-grained understanding.", "title_embedding_index": 21214, "title_abs_embedding_index": 21239}, {"title": "SOLOS: Sparse Optimization For Long Sequence In Context Compression Enhanced LLMs", "link_suffix": "/forum?id=DUsqifwwf5", "link": "https://openreview.net/forum?id=DUsqifwwf5", "pdf_link": "https://openreview.net/pdf?id=DUsqifwwf5", "keywords": "Long-Context LLMs; Context Compression; Sparse Optimization", "abstract": "Recent advances in long-context large language models (LLMs) make them commercially viable, but their standard attention mechanisms' quadratic complexity hinders deployment due to excessive computational costs. To address this, researchers have explored Q-former-like architectures that compress input sequences for LLMs, reducing inference costs. However, these methods often underperform compared to mainstream LLMs trained on short sequences and struggle with longer context. We introduce SOLOS, an innovative method for training long sequences within limited computational resources. This approach effectively narrows the performance gap between context-compressed LLMs and mainstream LLMs handling long contexts. By significantly reducing training overhead, SOLOS enables training on long-sequence datasets, such as 100K tokens for instruction tuning, using merely an 8x RTX3090 machine. Our comprehensive experimental analysis confirms SOLOS not only significantly outperforms other context-compression-augmented LLMs but also matches the performance of state-of-the-art long-context models. The introduction of SOLOS marks a significant step toward deploying long-context LLMs, offering both efficiency and effectiveness in practical scenarios.", "title_embedding_index": 21215, "title_abs_embedding_index": 21240}, {"title": "Verbalized Graph Representation Learning: A Fully Interpretable Graph Model Based on Large Language Models Throughout the Entire Process", "link_suffix": "/forum?id=EHYbqCDRtM", "link": "https://openreview.net/forum?id=EHYbqCDRtM", "pdf_link": "https://openreview.net/pdf?id=EHYbqCDRtM", "keywords": "large language models, fully interpretable, graph representation learning", "abstract": "Representation learning on text-attributed graphs (TAGs) has attracted significant interest due to its wide-ranging real-world applications, particularly through Graph Neural Networks (GNNs). Traditional GNN methods focus on encoding the structural information of graphs, often using shallow text embeddings for node or edge attributes. This limits the model to understand the rich semantic information in the data and its reasoning ability for complex downstream tasks, while also lacking interpretability. With the rise of large language models (LLMs), an increasing number of studies are combining them with GNNs for graph representation learning and downstream tasks. While these approaches effectively leverage the rich semantic information in TAGs datasets, their main drawback is that they are only partially interpretable, which limits their application in critical fields. In this paper, we propose a verbalized graph representation learning (VGRL) method which is fully interpretable. In contrast to traditional graph machine learning models, which are usually optimized within a continuous parameter space, VGRL constrains this parameter space to be text description which ensures complete interpretability throughout the entire process, making it easier for users to understand and trust the decisions of the model. We conduct several studies to empirically evaluate the effectiveness of VGRL and we believe these method can serve as a stepping stone in graph representation learning.", "title_embedding_index": 21216, "title_abs_embedding_index": 21241}, {"title": "Hierarchy-Aided Sparse Attention For Fast LLMs Prefilling Inference", "link_suffix": "/forum?id=Hjk1tWIdvL", "link": "https://openreview.net/forum?id=Hjk1tWIdvL", "pdf_link": "https://openreview.net/pdf?id=Hjk1tWIdvL", "keywords": "Long-Context LLM; Pre-Filling Acceleration; Sparse Attention", "abstract": "Pre-filling Large Language Models (LLMs) with long-context inputs is computationally expensive due to the quadratic complexity of full attention. While global attention is essential during decoding, its importance diminishes during pre-filling, where the focus is on contextualizing tokens rather than predicting the next one. Building on prior work, we apply diagonal block sparse attention during the pre-filling phase, reducing attention-related FLOPs by over 90% without significant degradation in language modeling performance. To address the remaining performance gap, we propose \\textbf{H}ierarchy-\\textbf{A}ided \\textbf{S}parse \\textbf{A}ttention (HASA), which incorporates a specialized transformer branch. This branch extracts global embeddings from each chunk and aligns local attention with full-attention, facilitating cross-chunk interaction. HASA stabilizes sparse attention computations, making the pre-filling phase highly efficient, particularly in long-sequence scenarios. While HASA significantly accelerates the pre-filling phase, we ensure robust language modeling performance by enabling interaction between global embeddings across chunks, which prevents the performance degradation typically observed in sparse attention mechanisms. Given that there are limited methods specifically accelerating pre-filling, our baselines include various open-source long-context models. Across multiple benchmarks, HASA not only maintains performance but also outperforms baseline models in certain scenarios. We will release the models upon acceptance.", "title_embedding_index": 21217, "title_abs_embedding_index": 21242}, {"title": "Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine", "link_suffix": "/forum?id=pH543jrbe8", "link": "https://openreview.net/forum?id=pH543jrbe8", "pdf_link": "https://openreview.net/pdf?id=pH543jrbe8", "keywords": "multimodal, deep learning, fusion, biomedicine, model merging", "abstract": "Learning holistic computational representations in physical, chemical or biological systems requires the ability to process information from different distributions and modalities within the same model. Thus, the demand for multimodal machine learning models has sharply risen for modalities that go beyond vision and language, such as sequences, graphs, time series, or tabular data. While there are many available multimodal fusion and alignment approaches, most of them require end-to-end training, scale quadratically with the number of modalities, cannot handle cases of high modality imbalance in the training set, or are highly topology-specific, making them too restrictive for many biomedical learning tasks. This paper presents Multimodal Lego (MM-Lego), a general-purpose fusion framework to turn any set of encoders into a competitive multimodal model with no or minimal fine-tuning. We achieve this by introducing a wrapper for any unimodal encoder that enforces shape consistency between modality representations. It harmonises these representations by learning features in the frequency domain to enable model merging with little signal interference. We show that MM-Lego 1) can be used as a model merging method which achieves competitive performance with end-to-end fusion models without any fine-tuning, 2) can operate on any unimodal encoder, and 3) is a model fusion method that, with minimal fine-tuning, surpasses all benchmarks in five out of seven datasets.", "title_embedding_index": 21218, "title_abs_embedding_index": 21243}, {"title": "Depth Pro: Sharp Monocular Metric Depth in Less Than a Second", "link_suffix": "/forum?id=aueXfY0Clv", "link": "https://openreview.net/forum?id=aueXfY0Clv", "pdf_link": "https://openreview.net/pdf?id=aueXfY0Clv", "keywords": "depth estimation, computer vision", "abstract": "We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions.", "title_embedding_index": 21219, "title_abs_embedding_index": 21244}, {"title": "Classifier-Agnostic Zero-Shot Natural Language Explanations", "link_suffix": "/forum?id=X6VVK8pIzZ", "link": "https://openreview.net/forum?id=X6VVK8pIzZ", "pdf_link": "https://openreview.net/pdf?id=X6VVK8pIzZ", "keywords": "Natural Language Explanations, interpretability, explainability", "abstract": "Natural Language Explanations (NLEs) interpret the decision-making process of a given model through textual sentences. Current NLEs suffer from a severe limitation; they are unfaithful to the model\u2019s actual reasoning process, as a separate textual decoder is explicitly trained to generate those explanations using annotated datasets for a specific task, leading them to reflect what annotators desire. In this work, we take the first step towards generating faithful NLEs for any visual classification model without any training data. Our approach models the relationship between class embeddings from the classifier of the vision model and their corresponding class names via a simple MLP which trains in seconds. After training, we can map any new text to the classifier space and measure its association with the visual features. We conduct experiments on 38 vision models, including both CNNs and Transformers. In addition to NLEs, our method offers other advantages such as zero-shot image classification and fine-grained concept discovery.", "title_embedding_index": 21220, "title_abs_embedding_index": 21245}, {"title": "OTGM: Graph Matching with Noisy Correspondence via Optimal Transports", "link_suffix": "/forum?id=6w2HEMxzq7", "link": "https://openreview.net/forum?id=6w2HEMxzq7", "pdf_link": "https://openreview.net/pdf?id=6w2HEMxzq7", "keywords": "Optimal Transport", "abstract": "Graph matching is a significant task for handling the matching problem of finding correspondences between keypoints in different graphs. Prior research primarily concentrates on performing one-to-one matching in topologic perspective for keypoints across various graphs, assuming that the paired keypoints are accurately linked. However, these approaches have two limitations: (1) because of different observation perspectives, some keypoints in the reference figure may become occluded or transformed, leading to situations where keypoint matches are a mess in topologic; (2) in practice, the manual annotation process is susceptible to poor recognizability and viewpoint differences between images, which probably results in offset and even erroneous keypoint annotations. To address these limitations,  we revisit the graph matching problem from the distributional alignment perspective and propose an \\textbf{O}ptimal \\textbf{T}ransport \\textbf{G}raph \\textbf{M}atching model (\\textbf{OTGM}). Specifically, (1) to effectively model the real-world keypoint matching scenarios, we have redefined the graph matching process as a transportation plan, which involves transferring node or edge sets from one distribution to another while minimizing the Wasserstein distance between these distributions. (2) To achieve robust matching, we introduce a well-designed graph denoising module to eliminate noisy edges in the input graph with the assistance of self-supervised learning.  On top of this, we theoretically provide assurances regarding the generalization ability of OTGM. Furthermore, comprehensive experiments on three real-world datasets demonstrate that our model exhibits strong robustness and achieves state-of-the-art performance compared to competitive baselines.", "title_embedding_index": 21221, "title_abs_embedding_index": 21246}, {"title": "PTR: Precision-Driven Tool Recommendation for Large Language Models", "link_suffix": "/forum?id=PPrcfHXfuT", "link": "https://openreview.net/forum?id=PPrcfHXfuT", "pdf_link": "https://openreview.net/pdf?id=PPrcfHXfuT", "keywords": "Tool, Recommendation, Large Language Model", "abstract": "By augmenting Large Language Models (LLMs) with external tools, their capacity to solve complex problems has been significantly enhanced. However, despite ongoing advancements in the parsing capabilities of LLMs, incorporating all available tools simultaneously in the prompt remains impractical due to the vast number of external tools. Consequently, it is essential to provide LLMs with a precise set of tools tailored to the specific task, considering both quantity and quality. Current tool retrieval methods primarily focus on refining the ranking list of tools and directly packaging a fixed number of top-ranked tools as the tool set. However, these approaches often fail to equip LLMs with the optimal set of tools prior to execution, since the optimal number of tools for different tasks could be different, resulting in inefficiencies such as redundant or unsuitable tools, which impede immediate access to the most relevant tools. This paper addresses the challenge of recommending precise toolsets for LLMs. We introduce the problem of tool recommendation, define its scope, and propose a novel Precision-driven Tool Recommendation (PTR) approach. PTR captures an initial, concise set of tools by leveraging historical tool bundle usage and dynamically adjusts the tool set by performing tool matching, culminating in a multi-view-based tool addition. Additionally, we present a new dataset, RecTools, and a metric, TRACC, designed to evaluate the effectiveness of tool recommendation for LLMs. We further validate our design choices through comprehensive experiments, demonstrating promising accuracy across two open benchmarks and our RecTools dataset. We release our code and dataset athttps://anonymous.4open.science/r/PTR-65DDto support further research in tool recommendation.", "title_embedding_index": 21222, "title_abs_embedding_index": 21247}, {"title": "On the Universality of Self-Supervised Representation Learning", "link_suffix": "/forum?id=NoN4lziOUF", "link": "https://openreview.net/forum?id=NoN4lziOUF", "pdf_link": "https://openreview.net/pdf?id=NoN4lziOUF", "keywords": "Self-Supervised Learning, Representation Learning, Unsupervised Learning", "abstract": "In this paper, we investigate the characteristics that define a good representation or model. We propose that such a model should possess universality, characterized by: (i) discriminability: performing well on training samples; (ii) generalization: performing well on unseen datasets; and (iii) transferability: performing well on unseen tasks with distribution shifts. Despite its importance, current self-supervised learning (SSL) methods lack explicit modeling of universality, and theoretical analysis remains underexplored. To address these issues, we aim to explore and incorporate universality into SSL. Specifically, we first revisit SSL from a task perspective and find that each mini-batch can be viewed as a multi-class classification task. We then propose that a universal SSL model should achieve: (i) learning universality by minimizing loss across all training samples, and (ii) evaluation universality by learning causally invariant representations that generalize well to unseen datasets and tasks. To quantify this, we introduce a \n$\\sigma$-measurement that assesses the gap between the performance of SSL model and optimal task-specific models. Furthermore, to model universality, we propose the GeSSL framework. It first learns task-specific models by minimizing SSL loss, then incorporates future updates to enhance discriminability, and finally integrates these models to learn from multiple mini-batch tasks. Theoretical and empirical evidence supports the effectiveness of GeSSL.", "title_embedding_index": 21223, "title_abs_embedding_index": 21248}, {"title": "Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models", "link_suffix": "/forum?id=ZeaTvXw080", "link": "https://openreview.net/forum?id=ZeaTvXw080", "pdf_link": "https://openreview.net/pdf?id=ZeaTvXw080", "keywords": "Diffusion, Editing, Affordance", "abstract": "Adding Object into images based on text instructions is a challenging task in semantic image editing, requiring a balance between preserving the original scene and seamlessly integrating the new object in a fitting location. Despite extensive efforts, existing models often struggle with this balance, particularly with finding a natural location for adding an object in complex scenes. We introduce Add-it, a training-free approach that extends diffusion models' attention mechanisms to incorporate information from three key sources: the scene image, the text prompt, and the generated image itself. Our weighted extended-attention mechanism maintains structural consistency and fine details while ensuring natural object placement. Without task-specific fine-tuning, Add-it achieves state-of-the-art results on both real and generated image insertion benchmarks, including our newly constructed \"Additing Affordance Benchmark\" for evaluating object placement plausibility, outperforming supervised methods. Human evaluations show that Add-it is preferred in over 80% of cases, and it also demonstrates improvements in various automated metrics.", "title_embedding_index": 21224, "title_abs_embedding_index": 21249}]