[
    {
        "title": "SEE: See Everything Every Time - Broader Light Range Image Enhancement via Events",
        "link_suffix": "/forum?id=2wmxxYxVF0",
        "link": "https://openreview.net/forum?id=2wmxxYxVF0",
        "pdf_link": "https://openreview.net/pdf?id=2wmxxYxVF0",
        "keywords": "Event Camera, Image Brightness Enhancement, Brightness Adjustment Dataset",
        "abstract": "Event cameras, with a high dynamic range exceeding $120dB$, which significantly surpasses traditional cameras,demonstrate superior robustness under various lighting conditions, including both low-light and high-light situations.\nHowever, recent event-vision research only consider low-light image enhancement and neglected image enhancement and brightness adjustment under a broader range of lighting conditions, \\eg, normal or high illumination.\nBase on this, we propose a novel research question: how to employ events to enhance and adjust brightness of images captured under \\textbf{broader lighting conditions} \u2014including low light, normal light, and high light \u2014 aiming to restore clear images from day to night.\nTo investigate this question, we first collected a new dataset, SEE-0.6M, comprising 610,126 images and corresponding events across 202 scenarios spanning from day to night, each with an average of four lighting conditions exhibiting more than a 1000-fold differences in illumination from low-light to high-light.\nSubsequently, we propose a framework that effectively employ the high dynamic range information from events to smoothly adjusts brightness of the image through prompts.\nOur framework considers the camera sensor's patterns to capture color, utilizes sparse learning to represent events as a brightness dictionary, and adjust dynamic range of images through cross-attention to form a broader light range representation (BLR).\nFinally, the BLR is decoded at the pixel level into an image of corresponding brightness via prompts.\nExperimental results demonstrate that our method not only performs well on the low-light enhancement dataset but also shows robust performance on wide light-range enhancement using SEE-0.6M dataset.\nAdditionally, our method allows for pixel-level brightness adjustment, providing flexibility for post-processing, which may inspire more imaging applications."
    },
    {
        "title": "Rectified Diffusion Guidance for Conditional Generation",
        "link_suffix": "/forum?id=Y4kJp8GQmV",
        "link": "https://openreview.net/forum?id=Y4kJp8GQmV",
        "pdf_link": "https://openreview.net/pdf?id=Y4kJp8GQmV",
        "keywords": "Diffusion Models, Conditional Generation, Classifier-Free Guidance",
        "abstract": "Classifier-Free Guidance (CFG), which combines the conditional and unconditional score functions with two coefficients summing to one, serves as a practical technique for diffusion model sampling. Theoretically, however, denoising with CFGcannotbe expressed as a reciprocal diffusion process, which may consequently leave some hidden risks during use. In this work, we revisit the theory behind CFG and rigorously confirm that the improper configuration of the combination coefficients (i.e., the widely used summing-to-one version) brings about expectation shift of the generative distribution. To rectify this issue, we propose ReCFG with a relaxation on the guidance coefficients such that denoising with ReCFG strictly aligns with the diffusion theory. We further show that our approach enjoys aclosed-formsolution given the guidance strength. That way, the rectified coefficients can be readily pre-computed via traversing the observed data, leaving the sampling speed barely affected. Empirical evidence on real-world data demonstrate the compatibility of our post-hoc design with existing state-of-the-art diffusion models, including both class-conditioned ones (e.g., EDM2 on ImageNet) and text-conditioned ones (e.g., SD3 on CC12M), without any retraining. We will open-source the code to facilitate further research."
    },
    {
        "title": "Zero-shot Text-based Personalized Low-Light Image Enhancement with Reflectance Guidance",
        "link_suffix": "/forum?id=GUd6zTrTBb",
        "link": "https://openreview.net/forum?id=GUd6zTrTBb",
        "pdf_link": "https://openreview.net/pdf?id=GUd6zTrTBb",
        "keywords": "Low-light image enhancement, Retinex decomposition, Zero-shot learning, Generative diffusion prior",
        "abstract": "Recent advances in zero-shot low-light image enhancement have largely benefited from the deep image priors encoded in network architectures. However, these models require optimization from scratch for each image and cannot provide personalized results based on user preferences.\nIn this paper, we propose a training-free zero-shot personalized low-light image enhancement model that integrates Retinex domain knowledge into a pre-trained diffusion model, enabling style personalization based on user preferences specified through text instructions. Our contributions are as follows:\nFirst, we incorporate the total variation optimization into a single Gaussian convolutional layer, enabling zero-shot Retinex decomposition. Second, we introduce the Contrastive Language-Image Pretraining (CLIP) model into the reflectance-conditioned sampling process of Denoising Diffusion Implicit Models (DDIM), guiding the enhancement according to user-provided text instructions. Third, to ensure consistency in content and structure, we employ patch-wise DDIM inversion to find the initial noise vector and use the reflectance as a condition during the reverse sampling process.\nOur proposed model, RetinexGDP, supports any image size and produces noise-suppressed results without imposing extra noise constraints. Extensive experiments across nine low-light image datasets show that RetinexGDP achieves performance comparable to state-of-the-art models."
    },
    {
        "title": "TAS: Distilling Arbitrary Teacher and Student via a Hybrid Assistant",
        "link_suffix": "/forum?id=Vq65R88Wx0",
        "link": "https://openreview.net/forum?id=Vq65R88Wx0",
        "pdf_link": "https://openreview.net/pdf?id=Vq65R88Wx0",
        "keywords": "knowledge distillation; computer vision; image classification;",
        "abstract": "Most knowledge distillation (KD) methodologies predominantly focus on teacher-student pairs with similar architectures, such as both being convolutional neural networks (CNNs). However, the potential and flexibility of KD can be greatly improved by expanding it to novel Cross-Architecture KD (CAKD), where the knowledge of homogeneous and heterogeneous teachers can be transferred flexibly to a given student. The primary challenge in CAKD lies in the substantial feature gaps between heterogeneous models, originating from the distinction of their inherent inductive biases and module functions. To this end, we introduce an assistant model as a bridge to facilitate smooth feature knowledge transfer between heterogeneous teachers and students. More importantly, within our proposed design principle, the assistant model combines the advantages of cross-architecture inductive biases and module functions by merging convolution and attention modules derived from both student and teacher module functions. Furthermore, we observe that heterogeneous features exhibit diverse spatial distributions in CAKD, hindering the effectiveness of conventional pixel-wise mean squared error (MSE) loss. Therefore, we leverage a spatial-agnostic InfoNCE loss to align features after spatial smoothing, thereby improving the feature alignments in CAKD. Our proposed method is evaluated across some homogeneous model pairs and arbitrary heterogeneous combinations of CNNs, ViTs, and MLPs, achieving state-of-the-art performance for distilled models with a maximum gain of  11.47% on CIFAR-100 and 3.67% on ImageNet-1K for distilled models. Our code and models will be released."
    },
    {
        "title": "BrainGPT: A Brain-Inspired SNN-Based Large Language Model",
        "link_suffix": "/forum?id=uXytIlC1iQ",
        "link": "https://openreview.net/forum?id=uXytIlC1iQ",
        "pdf_link": "https://openreview.net/pdf?id=uXytIlC1iQ",
        "keywords": "Spiking Neural Networks, Large Language Models, Spike-Timing-Dependent Plasticity, Neuromorphic Computing, ANN-to-SNN Conversion",
        "abstract": "Large language models (LLMs) based on artificial neural networks (ANNs) have demonstrated remarkable performance but face challenges in computational efficiency and biological interpretability. We propose BrainGPT, a novel LLM architecture based on the Test-Time Training (TTT) framework and inspired by spiking neural networks (SNNs) and neurobiological principles. Our approach incorporates a dual-model structure, emulating the hierarchical language processing observed in the human brain, and utilizes a specialized integrate-and-fire neuron model with adaptive thresholding. Through a multi-stage training strategy, including quantization-aware pre-training, ANN-to-SNN conversion, and biologically inspired unsupervised learning, we achieve a mathematically proven lossless conversion from ANN to SNN, preserving 100% of the original ANN model's performance. Moreover, the biologically inspired unsupervised learning optimizes the maximum time steps required to maintain 100% ANN performance. Compared to the original TTT model, BrainGPT achieves a 33.4% increase in energy efficiency and demonstrates a 66.7% improvement in training convergence speed. This work advances the development of energy-efficient and biologically interpretable large language models that match the performance of state-of-the-art ANN-based models while significantly improving upon the TTT framework."
    },
    {
        "title": "TextualDecomposition then Sub-motion-spaceScattering forOpen-Vocabulary Motion Generation",
        "link_suffix": "/forum?id=ja2gQFYA9R",
        "link": "https://openreview.net/forum?id=ja2gQFYA9R",
        "pdf_link": "https://openreview.net/pdf?id=ja2gQFYA9R",
        "keywords": "Motion Generation, Open-Vocabulary",
        "abstract": "Text-to-motion generation is a crucial task in computer vision, which generates the target 3D motion by the given text. The existing annotated datasets are limited in scale, resulting in most existing methods overfitting to the small datasets and unable to generalize to the motions of the open domain. Some methods attempt to solve the open-vocabulary motion generation problem by aligning to the CLIP space or using the Pretrain-then-Finetuning paradigm. However, the current annotated dataset's limited scale only allows them to achieve mapping from sub-text-space to sub-motion-space, instead of mapping between full-text-space and full-motion-space (full mapping), which is the key to attaining open-vocabulary motion generation. To this end, this paper proposes to leverage the atomic motion (simple body part motions over a short time period) as an intermediate representation, and leverage two orderly coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to address the full mapping problem. For Textual Decomposition, we design a fine-grained description conversion algorithm, and combine it with the generalization ability of a large language model to convert any given motion text into atomic texts. Sub-motion-space Scattering learns the compositional process from atomic motions to the target motions, to make the learned sub-motion-space scattered to form the full-motion-space.\nFor a given motion of the open domain, it transforms the extrapolation into interpolation and thereby significantly improves generalization. Our network, $\\textbf{DSO}$-Net, combines textual $\\textbf{d}$ecomposition and sub-motion-space $\\textbf{s}$cattering to solve the $\\textbf{o}$pen-vocabulary motion generation. Extensive experiments demonstrate that our DSO-Net achieves significant improvements over the state-of-the-art methods on open-vocabulary motion generation."
    },
    {
        "title": "Reassessing Layer Pruning in LLMs: New Insights and Methods",
        "link_suffix": "/forum?id=EjHtQlKEzV",
        "link": "https://openreview.net/forum?id=EjHtQlKEzV",
        "pdf_link": "https://openreview.net/pdf?id=EjHtQlKEzV",
        "keywords": "LLM Pruning, Layer Pruning",
        "abstract": "Although large language models (LLMs) have achieved remarkable success across various domains, their considerable scale necessitates substantial computational resources, posing significant challenges for deployment in resource-constrained environments. Layer pruning, as a simple yet effective compression method, removes layers of a model directly, reducing computational overhead. However, what are the best practices for layer pruning in LLMs? Are sophisticated layer selection metrics truly effective? Does the LoRA (Low-Rank Approximation) family, widely regarded as a leading method for pruned model fine-tuning, truly meet expectations when applied to post-pruning fine-tuning? To answer these questions, we dedicate thousands of GPU hours to benchmarking layer pruning in LLMs and gaining insights across multiple dimensions. Our results demonstrate that a simple approach, i.e., pruning the final 25% of layers followed by fine-tuning the \\texttt{lm_head} and the remaining last three layer, yields remarkably strong performance. Following this guide, we prune Llama-3.1-8B-It and obtain a model that outperforms many popular LLMs of similar size, such as ChatGLM2-6B, Vicuna-7B-v1.5, Qwen1.5-7B and Baichuan2-7B.\nWe release the optimal model weights on Huggingface, and the code is available on GitHub."
    },
    {
        "title": "HAIR: Hypernetworks-based All-in-One Image Restoration",
        "link_suffix": "/forum?id=ob9vuDv4yl",
        "link": "https://openreview.net/forum?id=ob9vuDv4yl",
        "pdf_link": "https://openreview.net/pdf?id=ob9vuDv4yl",
        "keywords": "All-in-One image restoration, Hypernetworks, low-level vision",
        "abstract": "Image restoration aims to recover a high-quality clean image from its degraded version. Recent progress in image restoration has demonstrated the effectiveness of All-in-One image restoration models in addressing various unknown degradations simultaneously. However, these existing methods typically utilize the same parameters to tackle images with different types of degradation, forcing the model to balance the performance between different tasks and limiting its performance on each task. To alleviate this issue, we propose HAIR, aHypernetworks-basedAll-in-OneImageRestoration plug-and-play method that generates parameters based on the input image and thus makes the model to adapt to specific degradation dynamically. Specifically, HAIR consists of two main components, i.e., Classifier and Hyper Selecting Net (HSN). The Classifier is a simple image classification network used to generate a Global Information Vector (GIV) that contains the degradation information of the input image, and the HSN is a simple fully-connected neural network that receives the GIV and outputs parameters for the corresponding modules. Extensive experiments demonstrate that HAIR can significantly improve the performance of existing image restoration models in a plug-and-play manner, both in single-task and All-in-One settings. Notably, our proposed model Res-HAIR, which integrates HAIR into the well-known Restormer, can obtain superior or comparable performance compared with current state-of-the-art methods. Moreover, we theoretically demonstrate that to achieve a given small enough error, our proposed HAIR requires fewer parameters in contrast to mainstream embedding-based All-in-One methods. Code is available in supplementary materials."
    },
    {
        "title": "An Evolved Universal Transformer Memory",
        "link_suffix": "/forum?id=s1kyHkdTmi",
        "link": "https://openreview.net/forum?id=s1kyHkdTmi",
        "pdf_link": "https://openreview.net/pdf?id=s1kyHkdTmi",
        "keywords": "Transformers, Evolution, Memory, KV cache, attention",
        "abstract": "Prior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance. We overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improves both the performance and efficiency of transformers. We evolve NAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads. NAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices. Learning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model's input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trained only on language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning."
    },
    {
        "title": "A closed-loop EEG-based visual stimulation framework from controllable generation",
        "link_suffix": "/forum?id=4ltiMYgJo9",
        "link": "https://openreview.net/forum?id=4ltiMYgJo9",
        "pdf_link": "https://openreview.net/pdf?id=4ltiMYgJo9",
        "keywords": "Neural modulation; EEG; Close-loop;",
        "abstract": "Recent advancements in artificial neural networks (ANNs) have significantly refined methodologies for predicting the neural coding activities of the ventral visual stream in human and animal brains based on visual stimuli. Nevertheless, the endeavor to control visual stimuli to elicit specific neural activities continues to confront substantial challenges, including prohibitive experimental costs, the high-dimensional nature of stimuli, pronounced inter-individual variability, and an incomplete understanding of neuronal selectivity. To address these impediments, we propose a novel electroencephalography (EEG)-based closed-loop framework for visual stimulus. Leveraging this framework, we can identify the optimal natural image stimulus within a theoretically infinite search space to maximize the elicitation of neural activities that most closely align with desired brain states. Our framework employs advanced ANN ensemble models to ensure the reliability of neural activity predictions. Furthermore, we conceptualize the brain coding predicted by the ANN model as a non-differentiable black-box process, allowing us to directly analyze the relationship between the administered visual stimuli and the targeted brain activity. Our research demonstrates that, independent of the exactness of the ANN-predicted brain coding, the proposed framework can procure the theoretically optimal natural image stimulus at given cycle steps. Moreover, our method exhibits generalizability across different modalities of brain-specific activity regulation. Our code is available athttps://anonymous.4open.science/status/closed-loop-F2E9."
    },
    {
        "title": "Manifold K-means with\u21132,p-Norm Maximization",
        "link_suffix": "/forum?id=E5DYpUWsES",
        "link": "https://openreview.net/forum?id=E5DYpUWsES",
        "pdf_link": "https://openreview.net/pdf?id=E5DYpUWsES",
        "keywords": "Clustering, Manifold Learning, K-means, $\\ell_{2, p}$-Norm",
        "abstract": "Although a variety of different methods have emerged in the field of clustering, K-means still occupies an important position, and many advanced clustering methods even rely on the K-means  to achieve effective cluster detection. However, the sensitivity of K-means to the selection of the initial cluster center and its limited ability to handle nonlinear separable data somewhat restrict its clustering performance. In order to overcome the limitations of K-means, we draw inspiration from manifold learning and redefine K-means as a manifold K-means clustering framework. This framework supports various types of distance matrices, thus facilitating the efficient processing of nonlinear separable data. A unique advantage of this approach is that it does not require the calculation of the cluster center, while it maintains the consistency between manifold structure and cluster labels. Additionally, we highlight the significant role of the $\\ell_{2,p}$-norm; by maximizing the $\\ell_{2,p}$-norm, we can ensure the balance of classes in the clustering process, which is also supported by theoretical analysis. The results from extensive experiments across multiple databases substantiate the superiority of our proposed model."
    },
    {
        "title": "Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks",
        "link_suffix": "/forum?id=YuHQTo6G9S",
        "link": "https://openreview.net/forum?id=YuHQTo6G9S",
        "pdf_link": "https://openreview.net/pdf?id=YuHQTo6G9S",
        "keywords": "Multimodal Large Language Model, Biomedicine, Region-Text",
        "abstract": "Several medical Multimodal Large Languange Models (MLLMs) have been developed to address tasks involving visual images with textual instructions across various medical modalities, achieving impressive results. \nMost current medical generalist models are region-agnostic, treating the entire image as a holistic representation. However, they struggle to identify which specific regions they are focusing on when generating a sentence.\nTo mimic the behavior of doctors, who typically begin by reviewing the entire image before concentrating on specific regions for a thorough evaluation, we aim to enhance the capability of medical MLLMs in understanding anatomical regions within entire medical scans.\nTo achieve it, we first formulate \\textbf{Region-Centric tasks} and construct a \\textbf{large-scale dataset, MedRegInstruct,} to incorporate regional information into training. Combining our collected dataset with other medical multimodal corpora for training, we propose a \\textbf{Region-Aware medical MLLM, MedRegA}, which is the first bilingual generalist medical AI system to simultaneously handle image-level and region-level medical vision-language tasks across a broad range of modalities. Our MedRegA not only enables three region-centric tasks, but also achieves the best performance for visual question answering, report generation and medical image classification over 8 modalities, showcasing significant versatility. Experiments demonstrate that our model can not only accomplish powerful performance across various medical vision-language tasks in bilingual settings, but also recognize and detect structures in multimodal medical scans, boosting the interpretability and user interactivity of medical MLLMs. The codes and model will be made publicly available."
    },
    {
        "title": "Discrete Tensorized Label Learning with Anchor Graphs",
        "link_suffix": "/forum?id=jeo4FiBjlh",
        "link": "https://openreview.net/forum?id=jeo4FiBjlh",
        "pdf_link": "https://openreview.net/pdf?id=jeo4FiBjlh",
        "keywords": "multi-view clustering, tensorized label learning, Anchor graph, nuclear norm",
        "abstract": "Many discrete multi-view clustering methods based on anchor graphs use the anchor graph decomposition or spectral clustering to obtain the final clustering labels, such methods achieve good results but lack interpretability. Morever, some of them are poorly balanced. To this end, first, we start from the perspective of label transmission to convert labels of the anchors to the labels of the samples, which has better interpretability. Second, we find a new and remarkable use of the nuclear norm, i.e., maximizing the nuclear norm can ensure the balanced clusters, which has the rigorous theoretical proof. Simultaneously, a novel optimisation method based on the first order Taylor expansion is proposed for the nuclear norm. Finally, we introduce the tensor Schatten $p$-norm to fully exploit the spatial structural and complementary information between views, which can obtain aligned label matrices. Extensive experiments have verified the superiority of the proposed method compared with other state-of-the-art methods."
    },
    {
        "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher",
        "link_suffix": "/forum?id=xgtXkyqw1f",
        "link": "https://openreview.net/forum?id=xgtXkyqw1f",
        "pdf_link": "https://openreview.net/pdf?id=xgtXkyqw1f",
        "keywords": "language model, search engine, multi-agent system",
        "abstract": "Information seeking and integration is a complex cognitive task that consumes enormous time and effort. Inspired by the remarkable progress of Large Language Models, recent works attempt to solve this task by combining LLMs and search engines. However, these methods still obtain unsatisfying performance due to three challenges: (1) complex requests often cannot be accurately and completely retrieved by the search engine once (2) corresponding information to be integrated is spread over multiple web pages along with massive noise, and (3) a large number of web pages with long contents may quickly exceed the maximum context length of LLMs. Inspired by the cognitive process when humans solve these problems, we introduce MindSearch to mimic the human minds in web information seeking and integration, which can be instantiated by a simple yet effective LLM-based multi-agent framework. The WebPlanner models the human mind of multi-step information seeking as a dynamic graph construction process: it decomposes the user query into atomic sub-questions as nodes in the graph and progressively extends the graph based on the search result from WebSearcher. Tasked with each sub-question, WebSearcher performs hierarchical information retrieval with search engines and collects valuable information for WebPlanner. The multi-agent design of MindSearch enables the whole framework to seek and integrate information parallelly from larger-scale (e.g., more than 300) web pages in 3 minutes, which is worth 3 hours of human effort. MindSearch demonstrates significant improvement in the response quality in terms of depth and breadth, on both close-set and open-set QA problems. Besides, responses from MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web and Perplexity.ai applications, which implies that MindSearch can already deliver a competitive solution to the proprietary AI search engine."
    },
    {
        "title": "Line2Rbox: Line-supervised Oriented Object Detection",
        "link_suffix": "/forum?id=ivLD3sCDcG",
        "link": "https://openreview.net/forum?id=ivLD3sCDcG",
        "pdf_link": "https://openreview.net/pdf?id=ivLD3sCDcG",
        "keywords": "Line-supervised, Oriented Object Detection, Remote sensing images, Weakly-supervised",
        "abstract": "Oriented object detection is crucial for complex scenes such as aerial images and industrial inspection, providing precise delineation by minimizing background interference. Recently, the weakly-supervised oriented object detection has gaining attention due to its cost-effectiveness. However, the majority of existing weakly-supervised methods are either point-supervised or HBox-supervised, which presents a challenge in achieving an optimal balance between annotation cost and detection performance. In response, we introduce a novel form of line annotation, which is intermediate between point-level and plane-level annotation. Based on this, we present L2RBox, an end-to-end anchor-free detector that is the first line-supervised method for oriented object detection. The fundamental objective of the L2RBox is to utilise line labels for the completion of label assignment and the calculation of loss.  In particular, the line is mapped to the corresponding circle domain, which is then used to select training samples and calculate the center-ness target by the minimum circumscribed rectangle of the circle in the direction of the line.  The regression loss that we propose is designed to support the line as an optimisation target. It comprises four components, namely scale loss $L_s$, height loss $L_h$, position loss $L_p$ and angle loss $L_a$.\nExtensive experimentation on DOTA-v1.0 and DIOR-R has demonstrated that our L2RBox significantly outperforms point-supervised methods, while requiring only a slight increase in labeling costs.  It is also noteworthy that the proposed approach also demonstrates a slight performance advantage over the fully-supervised FCOS in certain categories."
    },
    {
        "title": "DNA Language Models for RNA Analyses",
        "link_suffix": "/forum?id=TOUrnb1EaG",
        "link": "https://openreview.net/forum?id=TOUrnb1EaG",
        "pdf_link": "https://openreview.net/pdf?id=TOUrnb1EaG",
        "keywords": "Genomic Language Models, RNA Sequence Analysis, Parameter-Efficient Fine-Tuning, Mixture of Experts, Computational Efficiency",
        "abstract": "Genomic Language Models (gLMs), encompassing DNA models, RNA models, and multimodal models, are becoming widely used for the analysis of biological sequences. Typically, models trained on RNA are used for RNA-related tasks, and models trained on DNA sequences are used for DNA tasks. However, this requires the development and maintenance of several classes of models to match the modality of the sequence. These models take significant resources and data to create, and maintaining separate models for DNA and RNA tasks is a computational burden.To reduce this burden, we introduce novel Adaptive Mixture of Codon Reformative Experts (CodonMoE) that can be incorporated into DNA gLMs in order to adapt them for mRNA-based predictive tasks. We show that, by using this plug-and-play operator, DNA-based gLMs can achieve performance similar to that of RNA-trained models on mRNA tasks. We further show that recent, efficient sub-quadratic DNA-based state space model (SSM) architectures can be used with the CodonMoE to achieve parameter- and computationally-efficient predictions for mRNA tasks. Specifically, experimental results demonstrate that CodonMoE improves diverse DNA-based backbones by a big margin, with some models achieving comparable or superior performance to current state-of-the-art RNA-specific models across several downstream tasks, while reducing both time complexity and model parameters.Our results provide a path for focusing development efforts of gLMs on DNA models, which can then be adapted to mRNA tasks. Because DNA data is more prevalent than assembled mRNA data, and modeling efforts can focus on a single class of model, this is likely to foster improved DNA models for mRNA tasks at lower computational cost, and is a significant step towards unifying genomic language modeling."
    },
    {
        "title": "PseDet: Revisiting the Power of Pseudo Label in Incremental Object Detection",
        "link_suffix": "/forum?id=Iu8FVcUmVp",
        "link": "https://openreview.net/forum?id=Iu8FVcUmVp",
        "pdf_link": "https://openreview.net/pdf?id=Iu8FVcUmVp",
        "keywords": "Incremental Object Detection, Catastrophic Forgetting, Pseudo Labeling",
        "abstract": "Incremental Objection Detection (IOD) facilitates the expansion of the usage scope of object detectors without forgetting previously acquired knowledge. Current approaches mostly adopt response-level knowledge distillation to overcome forgetting issues, by conducting implicit memory replay from the teacher model on new training data. However, this indirect learning paradigm does not fully leverage the knowledge generated by the teacher model. In this paper, we dive deeper into the mechanism of pseudo-labeling in incremental object detection by investigating three critical problems: (a) the upper bound quality of the pseudo labels is greatly limited by the previous model, (b) fixed score thresholds for label filtering, without considering the distribution across categories, and (c) the confidence score generated by the model does not well reflect the quality of the localization. Based on these observations, we propose a simple yet effective pseudo-labeling continual object detection framework, namely PseDet. Specifically, we introduce the spatio-temporal enhancement module to alleviate the negative effects when learning noisy data from the previous model. Considering the score distribution divergence across different classes, we propose the Categorical Adaptive Label Selector with a simple mathematical prior and fast K-Means pre-computation to dynamically determine the class-wise filtering threshold. In order to align the label score with the localization quality of the pseudo labels, we project the score through non-linear mapping to calibrate the distribution and integrate it into the new-step supervision. Extensive experiments on the competitive COCO benchmarks demonstrate the effectiveness and generalization of PseDet. Notably, it achieves 43.5+/41.2+ mAP under the 1/4-step incremental settings, achieving new state-of-the-art performance."
    },
    {
        "title": "INDUCTIVE GRADIENT ADJUSTMENT FOR SPECTRAL BIAS IN IMPLICIT NEURAL REPRESENTATIONS",
        "link_suffix": "/forum?id=TNYLCF7vZA",
        "link": "https://openreview.net/forum?id=TNYLCF7vZA",
        "pdf_link": "https://openreview.net/pdf?id=TNYLCF7vZA",
        "keywords": "Implicit neural representation, Spectral bias, Training dynamics",
        "abstract": "Implicit Neural Representations (INRs) as a versatile representation paradigm have achieved success in various computer vision tasks. Due to the spectral bias of the vanilla multi-layer perceptrons (MLPs), existing methods focus on designing MLPs with sophisticated architectures or repurposing existing training techniques for highly accurate INRs. In this paper, we delve into the linear dynamics model of MLPs and theoretically identify the empirical Neural Tangent Kernel (eNTK) matrix as a reliable link between spectral bias and training dynamics. Based on eNTK matrix, we propose a practical inductive gradient adjustment method,  which could purposefully improve the spectral bias via inductive generalization of eNTK-based gradient transformation matrix. We evaluate our method on different INRs tasks with various INR architectures and compare to existing training techniques. The superiority representation performance clearly validate the advantage of our proposed method. Armed with our gradient adjustment method, better INRs with more enhanced texture details and sharpened edges can be learned from the training data by tailored improvements on spectral bias."
    },
    {
        "title": "Weak-to-Strong Generalization Through the Data-Centric Lens",
        "link_suffix": "/forum?id=uogG8BfLs2",
        "link": "https://openreview.net/forum?id=uogG8BfLs2",
        "pdf_link": "https://openreview.net/pdf?id=uogG8BfLs2",
        "keywords": "weak to strong generalization, data-centric AI",
        "abstract": "The weak-to-strong generalization phenomenon is the driver for important machine learning applications including highly data-efficient learning and, most recently, performing superalignment. While decades of research have resulted in numerous algorithms that produce strong empirical performance, understanding what aspects of data enable weak-to-strong generalization has been understudied. We propose a simple data-centric mechanism that characterizes weak-to-strong generalization: the overlap density. Intuitively, generalization tracks the number of points that contain overlaps, i.e., both easy patterns (learnable by a weak model) and challenging patterns (only learnable by a stronger model), as with such points, weak predictions can be used to learn challenging patterns by stronger models. And, we provide a practical overlap detection algorithm to find overlap density from data. Finally, we provide an algorithm to learn, among multiple sources of data, which to query when seeking to maximize overlap density and thereby enhance weak-to-strong generalization. We provide a theoretical result showing that the generalization benefit is a function of the overlap density and a regret bound of our data selection algorithm. Empirically, we validate the mechanism and the overlap detection algorithm on a wide array of settings."
    },
    {
        "title": "Skin, Muscles, and Bones in MultiSensory Simulation",
        "link_suffix": "/forum?id=UmhC7fuhzs",
        "link": "https://openreview.net/forum?id=UmhC7fuhzs",
        "pdf_link": "https://openreview.net/pdf?id=UmhC7fuhzs",
        "keywords": "multimodal learning; video simulators",
        "abstract": "General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce the senses of proprioception, kinesthesia, force haptics, and muscle activation to capture such precise control. This comprehensive set of multimodal senses naturally enables fine-grained interactions that are difficult to simulate with unimodal or text conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further regularize action trajectory features to enhance causality for representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate effectiveness and practicality of our work."
    },
    {
        "title": "Distilling Structural Representations into Protein Sequence Models",
        "link_suffix": "/forum?id=KXrgDM3mVD",
        "link": "https://openreview.net/forum?id=KXrgDM3mVD",
        "pdf_link": "https://openreview.net/pdf?id=KXrgDM3mVD",
        "keywords": "biology, proteins, sequence, structure, autoencoder, esm",
        "abstract": "Protein language (or sequence) models, like the popular ESM2, are now widely used tools for extracting evolution-based protein representations and have achieved significant success on core downstream biological tasks.\nA major open problem is how to obtain representations that best capture both the sequence evolutionary history and the atomic structural properties of proteins in general. \nWe introduceImplicitSequenceModel, a sequence-only input model with structurally-enriched representations that outperforms state-of-the-art sequence models on several well-studied benchmarks including mutation stability assessment and structure prediction. \nOur key innovations are a microenvironment-based Autoencoder for generating structure tokens and a self-supervised training objective that distills these tokens into ESM2's pre-trained model. \nNotably, we make ISM's structure-enriched weights easily accessible for any application using the ESM2 framework."
    },
    {
        "title": "Enhancing Video Understanding with Vision and Language Collaboration",
        "link_suffix": "/forum?id=yspBoIZJ9Z",
        "link": "https://openreview.net/forum?id=yspBoIZJ9Z",
        "pdf_link": "https://openreview.net/pdf?id=yspBoIZJ9Z",
        "keywords": "Video understanding, video pre-trained model, vision-language model, collaboration learning",
        "abstract": "Leveraging video pre-trained models has led to significant advancements in video understanding tasks. However, due to the inherent bias towards temporal learning in video pre-training, these models fail to capture comprehensive spatial cues. Additionally, the widely-used supervised adaption methods lack fine-grained semantic guidance as single action labels cannot precisely depict the intra-class diversity. To address these challenges, we incorporate the general capabilities of large Vision Language Models (VLMs) and propose a cross-modal collaborative knowledge transfer method to enhance video understanding. First, we propose an attentive spatial knowledge transfer method that distills spatial knowledge from the VLM's image encoder, enabling the precise capture of spatial information. Next, we design a contrastive textual knowledge transfer approach that achieves detailed video representations through fine-grained text-video alignment. Owing to the cross-modal knowledge transfer, the video representations are capable of attending to informative spatial regions and aligning with fine-grained texts that carry rich semantics. Extensive experiments demonstrate that our method achieves state-of-the-art performance across various datasets, validating its effectiveness."
    },
    {
        "title": "Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data",
        "link_suffix": "/forum?id=iuxaCU3DI7",
        "link": "https://openreview.net/forum?id=iuxaCU3DI7",
        "pdf_link": "https://openreview.net/pdf?id=iuxaCU3DI7",
        "keywords": "Image Recognition, Vision-Language Pretraining, Image Tagging, Medical Imaging, Surgery",
        "abstract": "We present RASO, a foundation model designed to Recognize Any Surgical Object, offering robust open-set recognition capabilities across a broad range of surgical procedures and object classes, in both surgical images and videos. RASO leverages a novel weakly-supervised learning framework that generates tag-image-text pairs automatically from large-scale unannotated surgical lecture videos, significantly reducing the need for manual annotations. Our scalable data generation pipeline gatherers to 2,200 surgical procedures and produces 3.6 million tag annotations across 2,066 unique surgical tags. Our experiments show that RASO achieves improvements of 2.9 mAP, 4.5 mAP, 10.6 mAP, and 7.2 mAP on four standard surgical benchmarks respectively in zero-shot settings, and surpasses state-of-the-art models in supervised surgical action recognition tasks. We will open-source our code, model, and dataset to facilitate further research."
    },
    {
        "title": "STOP! A Out-of-Distribution Processor with Robust Spatiotemporal Interaction",
        "link_suffix": "/forum?id=85WHuB5CUK",
        "link": "https://openreview.net/forum?id=85WHuB5CUK",
        "pdf_link": "https://openreview.net/pdf?id=85WHuB5CUK",
        "keywords": "Spatiotemporal learning; out-of-distribution learning; spatiotemporal prediction",
        "abstract": "Recently, spatiotemporal graph convolutional networks have attained significant success in spatiotemporal prediction tasks. However, they encounter out-ofdistribution (OOD) challenges due to the sensitivity of node-to-node messaging mechanism to spatiotemporal shifts, leading to suboptimal generalization in unknown environments. To tackle these issues, we introduce the Spatio-Temporal OOD Processor (STOP), which leverages spatiotemporal MLP channel mixing as its backbone, separately incorporating temporal and spatial elements for prediction. To bolster resilience against spatiotemporal shifts, STOP integrates robust interaction including a Client-to-Server (C2S) messaging and graph perturbation mechanisms. Specifically, C2S messaging mechanism configures Context Perception Units (CPUs) to capture generalizable context features, constraining nodes to interact solely with CPUs for spatiotemporal feature interaction. The graph perturbation mechanism uses Generalized Perturbation Units (GPUs) to disrupt this interaction process, generating diverse training environments that compel the model to extract invariant context features from these settings. Furthermore, we develop a customized Distributionally Robust Optimization (DRO) to enhance generalization by exposing the model to challenging environments. Through evaluations on six datasets, STOP showcases competitive generalization and inductive learning. The code is available athttps://anonymous.4open.science/r/ICLR2025-STOP."
    },
    {
        "title": "Probabilistic Language-Image Pre-Training",
        "link_suffix": "/forum?id=D5X6nPGFUY",
        "link": "https://openreview.net/forum?id=D5X6nPGFUY",
        "pdf_link": "https://openreview.net/pdf?id=D5X6nPGFUY",
        "keywords": "vision-langauge-pretraining, probabilistic-embeddings",
        "abstract": "Vision-language models (VLMs) embed aligned image-text pairs into a joint space but often rely on deterministic embeddings, assuming a one-to-one correspondence between images and texts. This oversimplifies real-world relationships, which are inherently many-to-many, with multiple captions describing a single image and vice versa. We introduce Probabilistic Language-Image Pre-training (ProLIP), the first probabilistic VLM pre-trained on a billion-scale image-text dataset using only probabilistic objectives, achieving a strong zero-shot capability (e.g., 74.6% ImageNet zero-shot classification with ViT-B/16 backbone). ProLIP efficiently estimates uncertainty by adding an ``uncertainty token'' to the input without extra parameters. We also introduce a novel inclusion loss that enforces distributional inclusion relationships between image-text pairs and between original and masked inputs. Experiments demonstrate that, by leveraging uncertainty estimates, ProLIP benefits downstream tasks and aligns with intuitive notions of uncertainty, e.g., shorter texts being more uncertain and more general inputs include specific ones. Utilizing text uncertainties, we further improve ImageNet accuracy from 74.6% to 75.8% (under a few-shot setting), supporting the practical advantages of our probabilistic approach. Code will be publically available."
    }
]