[{"title": "Prompt Injection Benchmark for Foundation Model Integrated Systems", "link_suffix": "/forum?id=MsRdq0ePTR", "link": "https://openreview.net/forum?id=MsRdq0ePTR", "pdf_link": "https://openreview.net/pdf?id=MsRdq0ePTR", "keywords": "Prompt Injection Attack, Foundation Models, AI Agent", "abstract": "Foundation Models (FMs) are increasingly integrated with external data sources and tools to handle complex tasks, forming FM-integrated systems with different modalities. However, such integration introduces new security vulnerabilities, especially when FMs interact dynamically with the system environments. One of the most critical threats is the prompt injection attack, where adversaries inject malicious instructions into the input environment, causing the model to deviate from user-intended behaviors. To advance the study of prompt injection vulnerabilities in FM-integrated systems, a comprehensive benchmark is essential. However, existing benchmarks fall short in two key areas: 1) they primarily focus on text-based modalities, lacking thorough analysis of diverse threats and attacks across more integrated modalities such as code, web pages, and vision; and 2) they rely on static test suites, failing to capture the dynamic, adversarial interplay between evolving attacks and defenses, as well as the interactive nature of agent-based environments. To bridge this gap, we propose the Prompt Injection Benchmark for FM-integrated Systems (FSPIB), which offers comprehensive coverage across various dimensions, including task modalities, threat categories, various attack and defense algorithms. Furthermore, FSPIB is interactive and dynamic, with evaluations conducted in interactive environments, and features a user-friendly front end that supports extensible attacks and defenses for ongoing research. By analyzing the performance of baseline prompt injection attacks and defenses, our benchmark highlights the prevalence of security vulnerabilities in FM-integrated systems and reveals the limited effectiveness of existing defense strategies, underscoring the urgent need for further research into prompt injection mitigation.", "title_embedding_index": 1950, "title_abs_embedding_index": 1975}, {"title": "ACC-Debate: An Actor-Critic Approach to Multi-Agent Debate", "link_suffix": "/forum?id=nfKfAzkiez", "link": "https://openreview.net/forum?id=nfKfAzkiez", "pdf_link": "https://openreview.net/pdf?id=nfKfAzkiez", "keywords": "Multi-Agent Debate, Large Language Model, Preference Optimization", "abstract": "Large language models (LLMs) have demonstrated a remarkable ability to serve as general-purpose tools on various language-based tasks. \n  Recent works have demonstrated that the efficacy of such models can be improved through iterative dialog between multiple models, frequently referred to as multi-agent debate (MAD).\n  While debate shows promise as a means of improving model efficacy, most works in this area treat debate as an emergent behavior, rather than a learned behavior. \n  In doing so, current debate frameworks rely on collaborative behaviors to have been sufficiently trained into off-the-shelf models. \n  To address this limitation, we propose ACC-Debate, an Actor-Critic based learning framework to produce a two-agent team specialized in debate.\n  We demonstrate that ACC-Debate outperforms SotA debate techniques on a wide array of benchmarks.", "title_embedding_index": 1951, "title_abs_embedding_index": 1976}, {"title": "Gradient Descent Converges Linearly to Flatter Minima than Gradient Flow in Shallow Linear Networks", "link_suffix": "/forum?id=vxvgZ0kTFv", "link": "https://openreview.net/forum?id=vxvgZ0kTFv", "pdf_link": "https://openreview.net/pdf?id=vxvgZ0kTFv", "keywords": "Gradient Descent, Implicit Regularization, Shallow Networks, Linear Networks", "abstract": "We study the gradient descent (GD) dynamics of a depth-2 linear neural network with a single input and output. We show that GD converges at an explicit linear rate to a global minimum of the training loss, even with a large stepsize--about $2/\\textrm{sharpness}$. It still converges for even larger stepsizes, but may do so very slowly. We also characterize the solution to which GD converges, which has lower norm and sharpness than the gradient flow solution.\nOur analysis reveals a trade off between the speed of convergence and the magnitude of implicit regularization.\nThis sheds light on the benefits of training at the ``Edge of Stability'', which induces additional regularization by delaying convergence and may have implications for training more complex models.", "title_embedding_index": 1952, "title_abs_embedding_index": 1977}, {"title": "Theoretical Aspects of Bias and Diversity in Minimum Bayes Risk Decoding", "link_suffix": "/forum?id=9PYCz4cDuZ", "link": "https://openreview.net/forum?id=9PYCz4cDuZ", "pdf_link": "https://openreview.net/pdf?id=9PYCz4cDuZ", "keywords": "Minimum Bayes Risk (MBR) decoding, Minimum Bayes risk (MBR) decoding, Minimum Bayes Risk decoding, Minimum Bayes risk decoding, MBR decoding", "abstract": "Text generation commonly relies on greedy and beam decoding that limit the search space and degrade output quality. Minimum Bayes Risk (MBR) decoding can mitigate this problem by utilizing automatic evaluation metrics and model-generated pseudo-references. Previous studies have conducted empirical analyses to reveal the improvement by MBR decoding, and reported various observations. However, despite these observations, the theoretical relationship between them remains uncertain. To address this, we present a novel theoretical interpretation of MBR decoding from the perspective of bias-diversity decomposition. We decompose errors in the estimated quality of generated hypotheses in MBR decoding into two key factors:bias, which reflects the closeness between utility functions and human evaluations, anddiversity, which represents the variation in the estimated quality of utility functions. Our theoretical analysis reveals the difficulty in simultaneously improving both bias and diversity, and highlights the effectiveness of increasing diversity to enhance MBR decoding performance. This analysis verifies the alignment between our theoretical insights and the empirical results reported in previous work. Furthermore, to support our theoretical findings, we propose a new metric, pseudo-bias, which approximates the bias term using gold references. We also introduce a new MBR approach, Metric-augmented MBR (MAMBR), which increases diversity by adjusting the behavior of utility functions without altering the pseudo-references. Experimental results across multiple NLP tasks show that the decomposed terms in the bias-diversity decomposition correlate well with performance, and that MAMBR improves text generation quality by modifying utility function behavior. Our code will be available athttps://github.com/[Anonymized].", "title_embedding_index": 1953, "title_abs_embedding_index": 1978}, {"title": "Enhancing Physics-Informed Neural Networks Through Feature Engineering", "link_suffix": "/forum?id=krWulPI3Hj", "link": "https://openreview.net/forum?id=krWulPI3Hj", "pdf_link": "https://openreview.net/pdf?id=krWulPI3Hj", "keywords": "Physics-Informed Neural Networks, Deep learning, Feature learning, Partial differential equations, Scientific machine learning", "abstract": "Recent advances in Physics-Informed Neural Networks (PINNs) have deployed fully-connected multi-layer deep learning architectures to solve partial differential equations (PDEs). Such architectures, however, struggle to reduce prediction error below $O(10^{-5})$, even with substantial network sizes and prolonged training periods. Methods that reduce error further, to $O(10^{-7})$, generally come with high computational costs. This paper introduces a $\\textbf{S}$ingle-layered $\\textbf{A}$rchitecture with $\\textbf{F}$eature $\\textbf{E}$ngineering (SAFE-NET) that reduces the error by orders of magnitude using far fewer parameters, challenging the prevailing belief that modern PINNs are effectively learning features in these scientific applications. Our strategy is to return to basic ideas in machine learning: with random Fourier features, a simplified single-layer network architecture, and an effective optimizer we call $(\\text{Adam} + \\text{L-BFGS})^2$, SAFE-NET accelerates training and improves accuracy by reducing the number of parameters and improving the conditioning of the PINN optimization problem. Our numerical results reveal that SAFE-NET converges faster, matches or generally exceeds the performance of more complex optimizers and multi-layer networks, improves problem conditioning throughout training, and works robustly across many problem settings. On average, SAFE-NET uses an order of magnitude fewer parameters than a conventional PINN setup to reach comparable results in 20 percent as many epochs, each of which is 50 percent faster. Our findings suggest that conventional deep learning architectures for PINNs waste their representational capacity, failing to learn simple but effective features.", "title_embedding_index": 1954, "title_abs_embedding_index": 1979}, {"title": "Interpretable Patterns in Random Initialization Unveil Final Representation", "link_suffix": "/forum?id=bWT6OBJ71x", "link": "https://openreview.net/forum?id=bWT6OBJ71x", "pdf_link": "https://openreview.net/pdf?id=bWT6OBJ71x", "keywords": "mechanistic interpretability, training dynamics, science of deep learning, representation learning", "abstract": "The field of mechanistic interpretability has made strides in unraveling models' hidden representations but is often puzzled by why specific representations form. This paper addresses a crucial question on this front: when a neural network can learn multiple distinct representations to solve a task, how does it \"choose\" among them during training?\nWe suggest that, at initialization, instead of starting from an empty scratchpad, the model's embedding already contains partially formed representations of varying ''completeness.'' Models tend to develop a representation that is more \"complete\" at initialization, disregarding less complete alternatives.\nWe empirically examine this hypothesis on algorithmic toy models  with clearly defined final representations from which we can elicit an interpretable signal to evaluate such \"completeness\" of possible representations in the initial embedding. We find that the representations with high initial signals are chosen by the model with high probability, a pattern consistent across models with a single learned representation (remainder equivalence, multi-digit XOR) and with multiple, redundant representations (modular addition). \nFinally, we investigate the role of embedding dimensionality on model's representation and their ``completeness.'' \nOur results with toy models show that the seemingly chaotic initialization contains many interpretable patterns to understand the training dynamics of representations.", "title_embedding_index": 1955, "title_abs_embedding_index": 1980}, {"title": "Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention", "link_suffix": "/forum?id=rEQqBZIz49", "link": "https://openreview.net/forum?id=rEQqBZIz49", "pdf_link": "https://openreview.net/pdf?id=rEQqBZIz49", "keywords": "Graph Neural Networks, Graph Encoding, Graph Rewiring, Attention Mechanism, Deep Learning", "abstract": "Graph Neural Networks (GNNs) have become important tools for machine learning on graph-structured data. In this paper, we explore the synergistic combination of graph encoding, graph rewiring, and graph attention, by introducing Graph Attention with Stochastic Structures (GRASS), a novel GNN architecture. GRASS utilizes relative random walk probabilities (RRWP) encoding and a novel decomposed variant (D-RRWP) to efficiently capture structural information. It rewires the input graph by superimposing a random regular graph to enhance long-range information propagation. It also employs a novel additive attention mechanism tailored for graph-structured data. Our empirical evaluations demonstrate that GRASS achieves state-of-the-art performance on multiple benchmark datasets, including a 20.3% improvement in ZINC MAE.", "title_embedding_index": 1956, "title_abs_embedding_index": 1981}, {"title": "NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains", "link_suffix": "/forum?id=VoayJihXra", "link": "https://openreview.net/forum?id=VoayJihXra", "pdf_link": "https://openreview.net/pdf?id=VoayJihXra", "keywords": "Embodied AI, Neuro-symbolic AI", "abstract": "We explore neuro-symbolic approaches to generalize actionable knowledge, enabling embodied agents to tackle complex tasks more effectively in open-domain environments. A key challenge for embodied agents is the generalization of knowledge across diverse environments and situations, as limited experiences often confine them to their prior knowledge. To address this issue, we introduce a novel framework, NeSyC, a neuro-symbolic continual learner that emulates the hypothetico-deductive model by continuously formulating and validating knowledge from limited experiences through the combined use of Large Language Models (LLMs) and symbolic tools. Specifically, NeSyC incorporates a contrastive generality improvement scheme. This scheme iteratively produces hypotheses using LLMs and conducts contrastive validation with symbolic tools, reinforcing the justification for admissible actions while minimizing the inference of inadmissible ones. We also introduce a memory-based monitoring scheme that efficiently detects action errors and triggers the knowledge evolution process across domains. Experiments conducted on embodied control benchmarks\u2014including ALFWorld, VirtualHome, Minecraft, RLBench, and a real-world robotic scenario\u2014demonstrate that NeSyC is highly effective in solving complex embodied tasks across a range of open-domain settings.", "title_embedding_index": 1957, "title_abs_embedding_index": 1982}, {"title": "JPEG-LM: LLMs as Image Generators with Canonical Codec Representations", "link_suffix": "/forum?id=Z7aq3djHZw", "link": "https://openreview.net/forum?id=Z7aq3djHZw", "pdf_link": "https://openreview.net/pdf?id=Z7aq3djHZw", "keywords": "LLM for visual generation, codec-based LLMs", "abstract": "Recent work in image and video generation has been adopting the autoregressive LLM architecture due to its generality and potentially easy integration into multi-modal systems. The crux of applying autoregressive training in language generation to visual generation is discretization---representing continuous data like images and videos as discrete tokens. Common methods of discretizing images and videos include modeling raw pixel values, which are prohibitively lengthy, or vector quantization, which requires convoluted pre-hoc training. In this work, we propose to directly model images and videos as compressed files saved on computers via canonical codecs (e.g., JPEG, AVC/H.264). Using the default Llama architecture without any vision-specific modifications, we pretrain JPEG-LM from scratch to generate images (and AVC-LM to generate videos as a proof of concept), by directly outputting compressed file bytes in JPEG and AVC formats. Evaluation of image generation shows that this simple and straightforward approach is more effective than pixel-based modeling and sophisticated vector quantization baselines (on which our method yields a 31% reduction in FID). Our analysis shows that JPEG-LM has an especial advantage over vector quantization models in generating long-tail visual elements. Overall, we show that using canonical codec representations can help lower the barriers between language generation and visual generation, facilitating future research on multi-modal language/image/video LLMs.", "title_embedding_index": 1958, "title_abs_embedding_index": 1983}, {"title": "Does Deep Active Learning Work in the Wild?", "link_suffix": "/forum?id=GbXn0Dgf7f", "link": "https://openreview.net/forum?id=GbXn0Dgf7f", "pdf_link": "https://openreview.net/pdf?id=GbXn0Dgf7f", "keywords": "deep learning, active learning, robustness, sampling, diversity", "abstract": "Deep active learning (DAL) methods have shown significant improvements in sample efficiency compared to simple random sampling. While these studies are valuable, they nearly always assume that optimal DAL hyperparameter (HP) settings are known in advance, or optimize the HPs through repeating DAL several times with different HP settings. Here, we argue that in real-world settings, orin the wild, there is significant uncertainty regarding good HPs, and their optimization contradicts the premise of using DAL (i.e., we require labeling efficiency).  In this study, we evaluate the performance of eleven modern DAL methods on eight benchmark problems as we vary a key HP shared by all methods: the pool ratio.  Despite adjusting only one HP, our results indicate that eight of the eleven DAL methods sometimes underperform relative to simple random sampling and some frequently perform worse. Only three methods always outperform random sampling (albeit narrowly), and we find that these methods all utilize diversity to select samples - a relatively simple criterion.  Our findings reveal the limitations of existing DAL methods when deployedin the wild, and present this as an important new open problem in the field.", "title_embedding_index": 1959, "title_abs_embedding_index": 1984}, {"title": "Efficient Heuristics Generation for Solving Combinatorial Optimization Problems Using Large Language Models", "link_suffix": "/forum?id=0fwJMANq9P", "link": "https://openreview.net/forum?id=0fwJMANq9P", "pdf_link": "https://openreview.net/pdf?id=0fwJMANq9P", "keywords": "Heuristic Generation, Large Language Models, Combinatorial Optimization Problem", "abstract": "Recent studies exploited Large Language Models (LLMs) to autonomously generate heuristics for solving Combinatorial Optimization Problems (COPs), by prompting LLMs to first provide search directions and then derive heuristics accordingly. However, the absence of task-specific knowledge in prompts often leads LLMs to provide unspecific search directions, obstructing the derivation of well-performing heuristics. Moreover, evaluating the derived heuristics remains resource-intensive, especially for those semantically equivalent ones, often requiring unnecessary resource expenditure. To enable LLMs to provide specific search directions, we propose the Hercules algorithm, which leverages our designed Core Abstraction Prompting (CAP) method to abstract the core components from elite heuristics and incorporate them as prior knowledge in prompts. We theoretically prove the effectiveness of CAP in reducing unspecificity and provide empirical results in this work. To reduce the required computing resources for evaluating the derived heuristics, we propose few-shot Performance Prediction Prompting (PPP), a first-of-its-kind method for the Heuristic Generation (HG) task. PPP leverages LLMs to predict the fitness values of newly derived heuristics by analyzing their semantic similarity to previously evaluated ones. We further develop two tailored mechanisms for PPP to enhance predictive accuracy and determine unreliable predictions, respectively. The use of PPP makes Hercules more resource-efficient and we name this variant Hercules-P. Extensive experiments across various HG tasks, COPs, and LLMs demonstrate that Hercules outperforms the state-of-the-art LLM-based HG algorithms, while Hercules-P excels at minimizing computing resources. In addition, we illustrate the effectiveness of CAP, PPP, and the other proposed mechanisms by conducting relevant ablation studies.", "title_embedding_index": 1960, "title_abs_embedding_index": 1985}, {"title": "Socrates Loss for training ad-hoc calibrated selective classifiers", "link_suffix": "/forum?id=ZBL26FX0FT", "link": "https://openreview.net/forum?id=ZBL26FX0FT", "pdf_link": "https://openreview.net/pdf?id=ZBL26FX0FT", "keywords": "Reliability, Confidence Calibration, Selective Classification, Neural Networks", "abstract": "Model reliability is paramount for critical real-world applications. To enhance reliability, it is essential to quantify uncertainty in model predictions, as achieved through Confidence Calibration and Selective Classification. Confidence Calibration ensures prediction confidences accurately reflect the actual likelihood of correctness, while Selective Classification allows a model to abstain from making predictions when uncertain. Although related, existing methods address each aspect separately, or both through post-hoc approaches. Only one method, Confidence-aware Contrastive Learning for Selective Classification (CCL-SC), combines both in an ad-hoc manner. Despite being a powerful calibrator, CCL-SC has some drawbacks, including the absence of an additional unknown class, the use of two different losses (detrimental for calibration), and its cumbersome implementation. In the pursuit of reliable models and motivated by the idea of creating an ad-hoc calibrated selective classifier with an unknown class, we first empirically analyze the Self-Adaptive Training (SAT) method, a leading approach in ad-hoc selective classification. We identify that while SAT excels in selective classification, it falls short in confidence calibration, especially when training for a small number of epochs (e.g., <=100). To address this, we introduce an original approach that uses an unknown class and a unique novel loss, Socrates loss, which serves as a classifier and a calibrator with a unified optimization goal. This approach mitigates overfitting and ensures theoretically well-calibrated predictions across all epochs, addressing the drawbacks of both CCL-SC and SAT, without the need for post-hoc processing or additional data. We integrate our approach into the SAT implementation and extend it to provide selective classification and confidence calibration metrics. We show empirically that our approach matches or improves the selective classification error rate of SAT and CCL-SC, while producing well-calibrated models in an ad-hoc manner through the evaluation on 6 image benchmark datasets across two architectures, VGG-16 and ResNet-34.", "title_embedding_index": 1961, "title_abs_embedding_index": 1986}, {"title": "Conformal Language Model Reasoning with Coherent Factuality", "link_suffix": "/forum?id=AJpUZd8Clb", "link": "https://openreview.net/forum?id=AJpUZd8Clb", "pdf_link": "https://openreview.net/pdf?id=AJpUZd8Clb", "keywords": "language models, reasoning, conformal prediction, factuality, graph representation, coherence", "abstract": "Language models are increasingly being used in important decision pipelines, so ensuring the correctness of their outputs is crucial. Recent work has proposed evaluating the \u201cfactuality\u201d of claims decomposed from a language model generation and applying conformal prediction techniques to filter out those claims that are not factual. This can be effective for tasks such as information retrieval, where constituent claims may be evaluated in isolation for factuality, but is not appropriate for reasoning tasks, as steps of a logical argument can be evaluated for correctness only within the context of the claims that have preceded them. To capture this, we define \u201ccoherent factuality\u201d and develop a conformal-prediction-based method to guarantee coherent factuality of language model outputs. Our approach applies split conformal prediction to subgraphs within a ``deducibility\" graph that we construct to represent the steps of a reasoning problem. We evaluate our method on mathematical reasoning problems from the MATH and FELM datasets, and find that our algorithm achieves coherent factuality across target coverage levels, consistently producing orderings of correct claims that are substantiated by previous ones. Moreover, we achieve 90% factuality on our stricter definition while retaining 80% or more of the original claims, highlighting the utility of our deducibility-graph-guided approach.", "title_embedding_index": 1962, "title_abs_embedding_index": 1987}, {"title": "Graph-based Confidence Calibration for Large Language Models", "link_suffix": "/forum?id=uuXPWRtwvK", "link": "https://openreview.net/forum?id=uuXPWRtwvK", "pdf_link": "https://openreview.net/pdf?id=uuXPWRtwvK", "keywords": "Language Models; Uncertainty Calibration", "abstract": "One important approach to improving the reliability of large language models (LLMs) is to provide accurate confidence estimations regarding the correctness of their answers. However, developing a well-calibrated confidence estimation model is challenging, as mistakes made by LLMs can be difficult to detect. We propose a novel method combining the LLM's self-consistency with labeled data and training an auxiliary model to estimate the correctness of its responses to questions. This auxiliary model predicts the correctness of responses based solely on their consistent information. To set up the learning problem, we use a weighted graph to represent the consistency among the LLM's multiple responses to a question. Correctness labels are assigned to these responses based on their similarity to the correct answer. We then train a graph neural network to estimate the probability of correct responses. Experiments demonstrate that the proposed approach substantially outperforms several of the most recent methods in confidence calibration across multiple widely adopted benchmark datasets. Furthermore, the proposed approach significantly improves the generalization capability of confidence calibration on out-of-domain (OOD) data.", "title_embedding_index": 1963, "title_abs_embedding_index": 1988}, {"title": "Constraint-Conditioned Actor-Critic for Offline Safe Reinforcement Learning", "link_suffix": "/forum?id=nrRkAAAufl", "link": "https://openreview.net/forum?id=nrRkAAAufl", "pdf_link": "https://openreview.net/pdf?id=nrRkAAAufl", "keywords": "Offline Safe Reinforcement Learning, Constraint-conditioned Actor-Critic, Data Generation, Out-of-distribution Detection, Zero-shot Adaptation", "abstract": "Offline safe reinforcement learning (OSRL) aims to learn policies with high rewards while satisfying safety constraints solely from data collected offline. However, the learned policies often struggle to handle states and actions that are not present or out-of-distribution (OOD) from the offline dataset, which can result in violation of the safety constraints or overly conservative behaviors during their online deployment. Moreover, many existing methods are unable to learn policies that can adapt to varying constraint thresholds. To address these challenges, we propose constraint-conditioned actor-critic (CCAC), a novel OSRL method that models the relationship between state-action distributions and safety constraints, and leverages this relationship to regularize critics and policy learning. CCAC learns policies that can effectively handle OOD data and adapt to varying constraint thresholds. Empirical evaluations on the $\\texttt{DSRL}$ benchmarks show that CCAC significantly outperforms existing methods for learning adaptive, safe, and high-reward policies.", "title_embedding_index": 1964, "title_abs_embedding_index": 1989}, {"title": "Private Learning Fast and Slow: Two Algorithms for Prediction with Expert Advice Under Local Differential Privacy", "link_suffix": "/forum?id=E4OcXAx5Dc", "link": "https://openreview.net/forum?id=E4OcXAx5Dc", "pdf_link": "https://openreview.net/pdf?id=E4OcXAx5Dc", "keywords": "differential privacy, online learning, prediction with expert advice, follow the perturbed leader", "abstract": "We study the classic problem of prediction with expert advice under the constraint of differential privacy (DP). In contrast to earlier work in this area, we are interested in distributed settings with no trusted central curator. In this context, we first show that a classical online learning algorithm naturally satisfies DP and then design two new algorithms that extend and improve it: (1) RW-AdaBatch, which provides a novel form of privacy amplification at negligible utility cost, and (2) RW-Meta, which improves utility on non-adversarial data with zero privacy cost. Our theoretical analysis is supported by an empirical evaluation using real-world data reported by hospitals during the COVID-19 pandemic. RW-Meta outperforms the classical baseline at predicting which hospitals will report a high density of COVID-19 cases by a factor of more than 2$\\times$ at realistic privacy levels.", "title_embedding_index": 1965, "title_abs_embedding_index": 1990}, {"title": "Specialized Foundation Models struggle to beat Supervised Baselines", "link_suffix": "/forum?id=JYTQ6ELUVO", "link": "https://openreview.net/forum?id=JYTQ6ELUVO", "pdf_link": "https://openreview.net/pdf?id=JYTQ6ELUVO", "keywords": "foundation models, supervised learning, neural architecture search, hyperparameter optimization, convolutional networks, autoregressive models, genomics, satellite imaging, time series", "abstract": "Following its success for vision and text, the \u201cfoundation model\u201d (FM) paradigm\u2014pretraining large models on massive data, then fine-tuning on target tasks\u2014has rapidly expanded to domains in the sciences, engineering, healthcare, and beyond. Has this achieved what the original FMs accomplished, i.e. the supplanting of traditional supervised learning in their domains? To answer we look at three modalities\u2014genomics, satellite data, and time series\u2014with multiple recent FMs and compare them to a standard supervised learning workflow: model development, hyperparameter tuning, and training, all using only data from the target task. Across those three specialized domains, we find that it is consistently possible to train simple supervised models\u2014no more complicated than a lightly modified wide ResNet or UNet\u2014that match or even outperform the latest foundation models. Our work demonstrates that the benefits of large-scale pretraining have yet to be realized in many specialized areas, reinforces the need to compare new FMs to strong, well-tuned baselines, and introduces two new, easy-to-use, open-source, and automated workflows for doing so.", "title_embedding_index": 1966, "title_abs_embedding_index": 1991}, {"title": "Extracting Heuristics from Large Language Models for Reward Shaping in Reinforcement Learning", "link_suffix": "/forum?id=oBHF3urgyS", "link": "https://openreview.net/forum?id=oBHF3urgyS", "pdf_link": "https://openreview.net/pdf?id=oBHF3urgyS", "keywords": "Reinforcement Learning, Sparse Rewards, Large Language Models", "abstract": "Reinforcement Learning (RL) suffers from sample inefficiency in sparse reward domains, and the problem is further pronounced in case of stochastic transitions. To improve the sample efficiency, reward shaping is a well-studied approach to introduce intrinsic rewards that can help the RL agent converge to an optimal policy faster. However, designing a useful reward shaping function for all desirable states in the Markov Decision Process (MDP) is challenging, even for domain experts. Given that Large Language Models (LLMs) have demonstrated impressive performance across a magnitude of natural language tasks, we aim to answer the following question: $\\textit{Can we obtain heuristics using LLMs for constructing a reward shaping function that can boost an RL agent's sample efficiency?}$ To this end, we aim to leverage off-the-shelf LLMs to generate a plan for an abstraction of the underlying MDP. We further use this LLM-generated plan as a heuristic to construct the reward shaping signal for the downstream RL agent. By characterizing the type of abstraction based on the MDP horizon length, we analyze the quality of heuristics when generated using an LLM, with and without a verifier in the loop. Our experiments across multiple domains with varying horizon length and number of sub-goals from the BabyAI environment suite, Household, Mario, and, Minecraft domain, show 1) the advantages and limitations of querying LLMs with and without a verifier to generate a reward shaping heuristic, and, 2) a significant improvement in the sample efficiency of PPO, A2C, and Q-learning when guided by the LLM-generated heuristics.", "title_embedding_index": 1967, "title_abs_embedding_index": 1992}, {"title": "Unified Framework for Causal Discovery and Long-term Forecasting in Non-stationary Environments", "link_suffix": "/forum?id=4sJIgdErt1", "link": "https://openreview.net/forum?id=4sJIgdErt1", "pdf_link": "https://openreview.net/pdf?id=4sJIgdErt1", "keywords": "causal discovery, long-term forecasting", "abstract": "Non-stationary data is prevalent in various real-world domains such as climate science, economics, and neuroscience, presenting significant challenges for tasks like forecasting and causal discovery from observational data. Existing approaches often operate under the assumption that the data is stationary. In this work, we introduce a unified framework that combines long-term forecasting and causal discovery with non-linear relations in a non-stationary setting. Specifically, we assume that the nonlinear causal relations in the observed space can be transformed into linear relations in the latent space via projections. In addition, we model the non-stationarity in the system as arising from time-varying causal relations.  The proposed model demonstrates that adopting a causal perspective for long-term forecasting not only addresses the limitations of each task but also makes the causal process identifiable, enhances interpretability, and provides more reliable predictions. Moreover, our approach reformulates causal discovery into a scalable, non-parametric deep learning problem. Through experiments on both synthetic and real-world datasets, we show that our framework outperforms baseline methods in both forecasting and causal discovery, underscoring the benefits of this integrated approach.", "title_embedding_index": 1968, "title_abs_embedding_index": 1993}, {"title": "DSR: Reinforcement Learning with Dynamical Skill Refinement", "link_suffix": "/forum?id=PklYedVFUW", "link": "https://openreview.net/forum?id=PklYedVFUW", "pdf_link": "https://openreview.net/pdf?id=PklYedVFUW", "keywords": "Reinforcement Learning, Reinforcement Learning with Skills, Reinforcement Learning with Demonstrations", "abstract": "Reinforcement learning with skills (RL with skills) is an efficient paradigm for solving sparse-reward tasks by extracting skills from demonstration datasets and learning high-level policy which selects skills. Because each selected skill by high-level policy is executed for multiple consecutive timesteps, the high-level policy is essentially learned in a temporally abstract Markov decision process (TA-MDP) built on the skills, which shortens the task horizon and reduces the exploration cost. However, these skills are usually sub-optimal because of the potential low quality and low coverage of the datasets, which causes the sub-optimal performance in the downstream task. Refining skills is intuitive, but the change of skills will in turn lead to the non-stationarity of the transition dynamics of TA-MDP which we name temporal abstraction shift. To address the dilemma of sub-optimal skills and temporal abstraction shift, we unify the optimization objectives of the entire hierarchical policy consisting of the high-level policy and the low-level policy whose latent space embeds the skills. We theoretically prove that the unified optimization objective guarantees the performance improvement in TA-MDP, and that optimizing the performance in TA-MDP is equivalent to optimizing a lower bound of the performance of the entire hierarchical policy in original MDP. Furthermore, in order to overcome the phenomenon of skill space collapse, we propose the dynamical skill refinement (DSR) mechanism which names our method. The experiment results empirically validate the effectiveness of our method, and show the advantages over the state-of-the-art (SOTA) methods.", "title_embedding_index": 1969, "title_abs_embedding_index": 1994}, {"title": "Efficient Biological Data Acquisition through Inference Set Design", "link_suffix": "/forum?id=gVkX9QMBO3", "link": "https://openreview.net/forum?id=gVkX9QMBO3", "pdf_link": "https://openreview.net/pdf?id=gVkX9QMBO3", "keywords": "Active Learning, Data Acquisition, ML for Drug Discovery", "abstract": "In drug discovery, highly automated high-throughput laboratories are used to screen a large number of compounds in search of effective drugs. These experiments are expensive, so we might hope to reduce their cost by experimenting on a subset of the compounds, and predicting the outcomes of the remaining experiments. In this work, we model this problem as a sequential subset selection problem: we aim to sequentially select the smallest set of candidates in order to achieve some desired level of accuracy for the system as a whole. Our key observation is that, if there is heterogeneity in the difficulty of the prediction problem across the input space, selectively obtaining the labels for the hardest examples in the acquisition pool will leave only the relatively easy examples to remain in the inference set, leading to better overall system performance. We call this problem inference set design, and propose an active learning solution using the model's confidence to prune out these challenging examples. Our algorithm includes an explicit stopping criterion that stops running the experiments when it is sufficiently confident that the system has reached the target performance. Our empirical studies on images and molecular datasets, as well as a real-world case, show that deploying active learning for inference set design leads to significant reduction in experimental cost while obtaining better system performance.", "title_embedding_index": 1970, "title_abs_embedding_index": 1995}, {"title": "SPD: Sync-Point Drop for efficient tensor parallelism of Large Language Models", "link_suffix": "/forum?id=uoU4ypjAmN", "link": "https://openreview.net/forum?id=uoU4ypjAmN", "pdf_link": "https://openreview.net/pdf?id=uoU4ypjAmN", "keywords": "sync point drop, tensor parallelism, distributed inference, efficient ml", "abstract": "With the rapid expansion in the scale of large language models (LLMs), enabling efficient distributed inference across multiple computing units has become increasingly critical. However, communication overheads from frequent synchronization during distributed inference pose a significant challenge to achieve scalability and low latency. Therefore, we introduce a novel optimization technique, Sync-Point Drop (SPD) to reduce communication overheads in tensor parallelism by dropping synchronization on attention outputs. In detail, we first propose a block design that allows execution to proceed without communication through SPD. Second, we identify regions of communication redundancy, where dropping synchronization results in no loss of model performance. In addition, to extend SPD across all compute blocks, we employ a low-cost distillation, specifically targeting blocks giving quality degradation, to maximize accuracy recovery. For extreme blocks where performance degradation is severe, we introduce a new head grouping enhancements to amplify the distillation\u2019s recovery effect. The proposed methods effectively alleviate communication bottlenecks while minimizing accuracy degradation during LLM inference, offering a scalable solution for distributed environments.", "title_embedding_index": 1971, "title_abs_embedding_index": 1996}, {"title": "POROver: Improving Safety and Reducing Overrefusal in Large Language Models with Overgeneration and Preference Optimization", "link_suffix": "/forum?id=5EuAMDMPRK", "link": "https://openreview.net/forum?id=5EuAMDMPRK", "pdf_link": "https://openreview.net/pdf?id=5EuAMDMPRK", "keywords": "LLM safety, LLM usefulness, Overrefusal in LLMs, responsible AI", "abstract": "Balancing safety and usefulness in large language models has become a critical challenge in recent years. \nModels often exhibit unsafe behavior or adopt an overly cautious approach, leading to frequent overrefusal of benign prompts, which reduces their usefulness. \nAddressing these issues requires methods that maintain safety while avoiding overrefusal. \nIn this work, we examine how the overgeneration of training data using advanced teacher models (e.g., GPT-4o), including responses to both general-purpose and toxic prompts, influences the safety and overrefusal balance of instruction-following language models.\nAdditionally, we present POROver, a strategy to use preference optimization methods in order to reduce overrefusal, via employing a superior teacher model's completions.\nOur results show that overgenerating completions for general-purpose prompts significantly improves the balance between safety and usefulness. \nSpecifically, the F1 score calculated between safety and usefulness increases from 73.7% to 88.4%. \nMoreover, overgeneration for toxic prompts substantially reduces overrefusal, decreasing it from 94.4% to 45.2%. \nFurthermore, preference optimization algorithms, when applied with carefully curated preference data, can effectively reduce a model's overrefusal from 45.2% to 15.0% while maintaining comparable safety levels.", "title_embedding_index": 1972, "title_abs_embedding_index": 1997}, {"title": "Using Generative AI to capture High Fidelity Temporal Dynamics to target Vehicular Systems", "link_suffix": "/forum?id=XQFSIdKMhJ", "link": "https://openreview.net/forum?id=XQFSIdKMhJ", "pdf_link": "https://openreview.net/pdf?id=XQFSIdKMhJ", "keywords": "Generative model, Cybersecurity, Vehicle System, Synthetic Data", "abstract": "Generative models have transformed the creation of text, images, and video content by enabling machines to generate high-quality, realistic outputs. These models are now widely being adopted in advanced fields like natural language processing, computer vision, and media production. Since vehicle data is limited due to proprietary concerns, utilizing generative models to mimic complex vehicle behaviors would provide powerful tools for creating synthetic data that can serve as a crucial component for enhancing the fidelity of vehicle models, better predictive maintenance, more robust control systems, autonomous driving features and resilient defense mechanism against cyber threats. This paper presents a Long Short-Term Memory (LSTM) based Conditional Generative Adversarial\nNetwork (GAN) model, which trains on limited available real vehicle data and is then able to generate synthetic time series data mimicking the actual vehicle data. The LSTM network helps in learning temporal characteristics of vehicle network traffic without needing the system details, which makes it applicable to wide range of vehicle networks. The conditional layer adds auxiliary information by labeling\ndata for different driving scenarios for training and generating data. The quality of the synthetic data is evaluated visually and quantitatively using metrics such as Maximum Mean Discrepancy (MMD), Predictive and Discriminative Scores. For demonstration purposes, the generative model is integrated into a validated vehicle model, where it successfully generates synthetic sensor feedback corresponding to the dynamic driving scenarios. This showcases the model\u2019s ability to simulate realistic sensor data in response to varying vehicle operations. Leveraging the high similarity to actual data, the generative model is further demonstrated for its potential use as malicious attack mechanism due to its deception capabilities against state of the art Intrusion Detection System (IDS). Without triggering the thresholds of the IDS, the model is able to penetrate the network stealthily with a low detection rate of 47.05%, compared to the 90% or higher detection rates of other known attacks. This effort is intended to serve as a test benchmark to develop more robust ML/AI based defense mechanisms.", "title_embedding_index": 1973, "title_abs_embedding_index": 1998}, {"title": "Uncertainty Herding: One Active Learning Method for All Label Budgets", "link_suffix": "/forum?id=UgPoHhYQ2U", "link": "https://openreview.net/forum?id=UgPoHhYQ2U", "pdf_link": "https://openreview.net/pdf?id=UgPoHhYQ2U", "keywords": "Active learning", "abstract": "Most active learning research has focused on methods which perform well when many labels are available, but can be dramatically worse than random selection when label budgets are small.\nOther methods have focused on the low-budget regime, but do poorly as label budgets increase.\nAs the line between \"low\" and \"high\" budgets varies by problem,\nthis is a serious issue in practice.\nWe proposeuncertainty coverage,\nan objective which generalizes a variety of low- and high-budget objectives,\nas well as natural, hyperparameter-light methods to smoothly interpolate between low- and high-budget regimes.\nWe call greedy optimization of the estimate Uncertainty Herding;\nthis simple method is computationally fast,\nand we prove that it nearly optimizes the distribution-level coverage.\nIn experimental validation across a variety of active learning tasks,\nour proposal matches or beats state-of-the-art performance in essentially all cases;\nit is the only method of which we are aware that reliably works well in both low- and high-budget settings.", "title_embedding_index": 1974, "title_abs_embedding_index": 1999}]