[
    {
        "title": "Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Dataset",
        "link_suffix": "/forum?id=yTEwmr1TJb",
        "link": "https://openreview.net/forum?id=yTEwmr1TJb",
        "pdf_link": "https://openreview.net/pdf?id=yTEwmr1TJb",
        "keywords": "Robot Learning, Foundation Model, Representation Learning",
        "abstract": "The pre-training of visual representations has enhanced the efficiency of robot learning. Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation-centricity). Interestingly, we find that the \u201cmanipulation-centricity\u201d is a strong indicator of success rates when applied to downstream tasks. Drawing from these findings, we propose $\\textbf{R}$obots $\\textbf{P}$re-train robots with $\\textbf{M}$anipulation-centricity ($\\textbf{RPM}$), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. We combine a novel contrastive loss that aligns visual observation with robot proprioceptive state-action dynamics with a BC-like actor loss that predicts action during pre-training and a time contrastive loss. Empirical results across 4 simulation domains with 20 tasks verify that RPM outperforms the strongest baseline method by $\\textbf{15.6}$%. Moreover, RPM boosts the performance of data-efficient learning with a UR5e arm on 3 real-world tasks by $\\textbf{76.9}$%."
    },
    {
        "title": "Breaking the Memory Barrier: Near Infinite Batch Size Scaling for Contrastive Loss",
        "link_suffix": "/forum?id=YvWuac63bg",
        "link": "https://openreview.net/forum?id=YvWuac63bg",
        "pdf_link": "https://openreview.net/pdf?id=YvWuac63bg",
        "keywords": "Contrastive Learning; Tiling Calculation;",
        "abstract": "Contrastive loss is a powerful approach for representation learning, where larger batch sizes enhance performance by providing more negative samples to better distinguish between similar and dissimilar data. However, scaling batch sizes is constrained by the quadratic growth in GPU memory consumption, primarily due to the full instantiation of the similarity matrix. To address this, we propose a tile-based computation strategy that partitions the contrastive loss calculation into arbitrary small blocks, avoiding full materialization of the similarity matrix. Furthermore, we introduce a multi-level tiling strategy to leverage the hierarchical structure of distributed systems, employing ring-based communication at the GPU level to optimize synchronization and fused kernels at the CUDA core level to reduce I/O overhead. Experimental results show that the proposed method scales batch sizes to unprecedented levels. For instance, it enables contrastive training of a CLIP-ViT-L/14 model with a batch size of 4M or 12M using 8 or 32 A800 80GB without sacrificing any accuracy. Compared to SOTA memory-efficient solutions, it achieves a two-order-of-magnitude reduction in memory while maintaining comparable speed. The code will be made publicly available."
    },
    {
        "title": "SUN: Training-free Machine Unlearning via Subspace",
        "link_suffix": "/forum?id=p7mgNvOD9Q",
        "link": "https://openreview.net/forum?id=p7mgNvOD9Q",
        "pdf_link": "https://openreview.net/pdf?id=p7mgNvOD9Q",
        "keywords": "Machine unlearning, Training-free",
        "abstract": "Machine Unlearning (MU), a technique to erase undesirable content from AI models, plays an essential role in developing safe and trustworthy AI systems. Despite the success MU achieved, existing MU baselines typically necessitate maintaining the entire dataset for fine-tuning unlearned models.\nFine-tuning models and maintaining large datasets are computationally and financially prohibitive. This motivates us to propose a simple yet effective MU approach: \\underline{S}ubspace \\underline{UN}learning (SUN) as a new fast and effective MU baseline. The proposed method removes the low-dimensional subspaces of undesirable concepts from the space spanned by the weight vectors. This modification makes the model \"blind\" to the undesirable content to realize unlearning. Notably, SUN can produce the scrubbed model instantly with only a few samples and without additional training."
    },
    {
        "title": "Size-aware Compression of 3D Gaussians with Fine-grained Mixed Precision Quantization",
        "link_suffix": "/forum?id=1PZt5nFlzH",
        "link": "https://openreview.net/forum?id=1PZt5nFlzH",
        "pdf_link": "https://openreview.net/pdf?id=1PZt5nFlzH",
        "keywords": "3D Gaussian Splatting, Mixed-precision Quantization, Compression",
        "abstract": "In this paper, we propose a method to automatically select hyperparameters to compress 3D Gaussians to a target file size while maximizing visual quality. We iteratively search for a hyperparameter configuration until the file size meets the specified budget. However, existing compression frameworks require completing the entire compression process to determine the compressed file size, which is time-consuming. To accelerate this, we design a tailored size estimator for frameworks that can determine hyperparameters without requiring fine-tuning. Although the finetuning-free frameworks are more predictable, they typically underperform compared to fine-tuning-based approaches, which utilize end-to-end differentiable structures to achieve superior results. To close this performance gap, we propose a mixed-precision quantization strategy that exploits the heterogeneity of attribute channels by compressing each channel with different bit-widths. The resulting combinatorial optimization problem is efficiently solved using 0-1 integer linear programming. Additionally, we partition each attribute channel into blocks of vectors, quantizing each vector based on the optimal bit-width determined in the previous step. The block length is then determined via dynamic programming. Our method identifies hyperparameter settings that meet the target file size within 70 seconds, outperforming state-of-the-art methods in both efficiency and quality. Extensive experiments demonstrate that our approach significantly enhances the performance of fine-tuning-free methods, with its upper-bound performance comparable to that of fine-tuning-required techniques."
    },
    {
        "title": "SnapMem: Snapshot-based 3D Scene Memory for Embodied  Exploration and Reasoning",
        "link_suffix": "/forum?id=mz8unSsSsB",
        "link": "https://openreview.net/forum?id=mz8unSsSsB",
        "pdf_link": "https://openreview.net/pdf?id=mz8unSsSsB",
        "keywords": "Embodied AI, Vision-Language, Lifelong Learning",
        "abstract": "Constructing a compact and informative 3D scene representation is essential for effective embodied reasoning and exploration, especially in complex environments over long periods. Existing approaches have relied on object-centric graph representations, which oversimplify 3D scenes by modeling them as individual objects and describing inter-object relationships through rigid textual descriptions. This rigidity leads to the loss of rich spatial relationships between objects, which are essential for embodied scene reasoning tasks. Furthermore, these representations lack natural mechanisms for active exploration and memory management, which hampers their applications for lifelong autonomy. In this work, we propose SnapMem, a novel 3D scene representation that leverages a compact set of informative snapshot images to cover the scene based on object co-visibility. These snapshot images capture rich spatial and semantic information among objects within the same view and their surroundings. We then illustrate how such a representation can be directly integrated with frontier-based exploration algorithms to facilitate active exploration by leveraging unexplored regions and scene memory. To support lifelong memory in active exploration settings, we further present an efficient memory aggregation pipeline to incrementally construct SnapMem, as well as an effective memory retrieval technique for memory management. Experimental results over three benchmarks demonstrate that SnapMem significantly enhances agents' reasoning and exploration capabilities in 3D environments over extended periods, highlighting its potential for advancing applications in embodied AI."
    },
    {
        "title": "Estimating Statistical Similarity Between Product Distributions",
        "link_suffix": "/forum?id=SUEXRbzq9l",
        "link": "https://openreview.net/forum?id=SUEXRbzq9l",
        "pdf_link": "https://openreview.net/pdf?id=SUEXRbzq9l",
        "keywords": "total variation distance, TV distance, complement of total variation distance, TV similarity, statistical similarity, aggregated experts, FPTAS, reductions, Turing reductions, Knapsack, counting Knapsack, Masked Knapsack, counting Masked Knapsack",
        "abstract": "We investigate the problem of computing the \\emph{statistical or total variation (TV) similarity} between distributions $P$ and $Q$, which is defined as $s_{\\mathrm{TV}}(P,Q) := 1 - d_{\\mathrm{TV}}(P, Q)$, where $d_{\\mathrm{TV}}$ is the total variation distance between $P$ and $Q$.\nStatistical similarity is a basic measure of similarity between distributions with several natural interpretations.\nWe focus on the case when $P$ and $Q$ are products of Bernoulli trials.\nRecent work has established, somewhat surprisingly, that even for this simple class of distributions exactly computing the TV distance (and hence statistical similarity) is #$\\mathsf{P}$-hard.\nThis motivates the question of designing multiplicative approximation algorithms for these computational tasks.\nIt is known that the TV distance computation admits a fully polynomial-time deterministic approximation scheme (FPTAS).\nIt remained an open question whether efficient approximation schemes exist for estimating the statistical similarity between two product distributions.\nIn this work, we affirmatively answer this question by designing an FPTAS for estimating the statistical similarity between two product distributions.\nTo obtain our result, we introduce a new variant of the knapsack problem, which we call multidimensional Masked Knapsack problem, and design an FPTAS to estimate the number of solutions to this problem.\nThis result might be of independent interest."
    },
    {
        "title": "Robust Multi-modal Learning with Shifted Feature Reweighting Against Spurious Correlations",
        "link_suffix": "/forum?id=eVKP64sQBd",
        "link": "https://openreview.net/forum?id=eVKP64sQBd",
        "pdf_link": "https://openreview.net/pdf?id=eVKP64sQBd",
        "keywords": "spurious correlations; multi-modal learning; shortcut learning;",
        "abstract": "Pre-trained multi-modal models have recently garnered significant attention due to their adaptability to diverse downstream tasks via fine-tuning. \nHowever, their resilience to certain group shift issues, i.e., spurious correlations, remains imperative yet relatively under-investigated.\nWe study this problem in vision-language models (VLMs), and we observe potential vulnerabilities in pre-trained VLMs, such as CLIP, when confronted with spurious correlations. \nWhile recent studies have been exploited to address unimodal group-imbalances by minority group up-sampling or creating group-balanced subsets, we posit that true robustness can be achieved by debiasing the training process through feature reweighting. \nIn this paper, we propose Shifted Feature Reweighting (SFR), a robust multi-modal learning method to mitigate the reliance on spurious features. \nSpecifically, we introduce a novel disagreement-based importance weight that allocates distinct weights to individual instances within the training data. \nThis contrasts with existing group rebalance weight strategies, which uniformly weigh all instances within a group. \nOur reweighting strategy adeptly addresses disparities in instance-level learning difficulty. \nMoreover, our empirical results unveil that representation collapse may arise during fine-tuning. \nTo address this, we proposed to introduce feature dropout and show that this simple method can further regularize the training on the majority groups and encourage the training on the minority groups. \nEmpirical results on multiple benchmarks verify our claims and confirm the effectiveness of our proposed SFR. \nTheoretically, we analyze the performance of our SFR and confirm its superiority in mitigating spurious correlations. \nOur codes will be here."
    },
    {
        "title": "Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction",
        "link_suffix": "/forum?id=KijslFbfOL",
        "link": "https://openreview.net/forum?id=KijslFbfOL",
        "pdf_link": "https://openreview.net/pdf?id=KijslFbfOL",
        "keywords": "incomplete multi-view clustering, mulit-view clustering, clustering",
        "abstract": "Most of incomplete multi-view clustering (IMVC) methods typically choose to ignore the missing samples and only utilize observed unpaired samples to construct  bipartite similarity. Moreover, they employ a single quantity of prototypes to extract the information of $\\textbf{all}$ views.   To eliminate these drawbacks, we present a simple yet effective IMVC approach, SIIHPC, in this work. It firstly transforms partial bipartition learning into original sample form by virtue of   reconstruction concept to split out of observed similarity, and then loosens traditional non-negative constraints via regularizing samples to more freely characterize the similarity.  Subsequently, it learns to recover the incomplete parts by utilizing the connection built between the similarity exclusive on respective view and the consensus graph shared for all views. On this foundation,  it further introduces a group of hybrid prototype quantities for each individual view to flexibly extract the data features   belonging to each view itself.  Accordingly, the resulting graphs are with various scales and  describe the overall similarity more comprehensively. It is worth mentioning that these all are optimized in one unified learning framework, \nwhich makes it possible for them to  reciprocally promote. Then, to effectively solve the formulated optimization problem, we design an ingenious auxiliary function that is with theoretically proven monotonic-increasing properties.  Finally, the clustering results are obtained by implementing spectral grouping action on the eigenvectors  of stacked multi-scale consensus similarity.  Numerous experimental results demonstrate that even under diverse missing proportions, the proposed SIIHPC  is still able to provide a preferable clustering performance compared to multiple prominent IMVC methods."
    },
    {
        "title": "Balancing Interpretability and Accuracy: Energy-Ensemble Concept Bottleneck Models for Enhanced Concept Inference",
        "link_suffix": "/forum?id=42TXboDg3c",
        "link": "https://openreview.net/forum?id=42TXboDg3c",
        "pdf_link": "https://openreview.net/pdf?id=42TXboDg3c",
        "keywords": "Energy-Based Models, Concept-Based Models, Explainable AI",
        "abstract": "Concept bottleneck models (CBM) have emerged as a promising solution to address the lack of interpretability in deep learning models. However, recent researches on CBM prioritize task accuracy at the expense of interpretability, weakening their ability to accurately infer key concepts. This work addresses this trade-off by introducing the energy ensemble CBM (EE-CBM). The EE-CBM leverages an energy-based concept encoder to effectively extract concepts, overcoming the information bottleneck common in conventional CBMs. Additionally, a novel energy ensemble gate within the EE-CBM architecture efficiently combines energy and concept probability to further address this bottleneck. Moreover, the EE-CBM employs the maximum mean discrepancy loss to enhance concept discrimination within the concept space and facilitate accurate concept inference. An experimental evaluation on benchmark datasets (CUB-200-2011, TravelingBirds, AwA2, CheXpert, and CelebA) demonstrates that EE-CBM achieve state-of-the-art performance in both concept accuracy and interpretability. This work positions the EE-CBM as a significant advancement in CBM researches, enabling them to effectively balance performance and interpretability for improved model transparency. Our code is available athttps://anonymous.4open.science/r/EE-CBM-F48D."
    },
    {
        "title": "Weak-to-Strong Enhanced Vision Model",
        "link_suffix": "/forum?id=breVfEOZLv",
        "link": "https://openreview.net/forum?id=breVfEOZLv",
        "pdf_link": "https://openreview.net/pdf?id=breVfEOZLv",
        "keywords": "weak-to-strong enhancement, knowledge distillation",
        "abstract": "Recent advancements in large language and vision models have demonstrated extraordinary capabilities, driving researchers to train increasingly larger models in pursuit of even greater performance. However, smaller, easier-to-train models often exist prior to these larger models. In this paper, we explore how to effectively leverage these smaller, weaker models to assist in training larger, stronger models. Specifically, we investigate the concept of weak-to-strong knowledge distillation within vision models, where a weaker model supervises a stronger one, aiming to enhance the latter\u2019s performance beyond the limitations of the former.\nTo this end, we introduce a novel, adaptively adjustable loss function that dynamically calibrates the weaker model\u2019s supervision based on the discrepancy between soft labels and hard labels. This dynamic adjustment allows the weaker model to provide more effective guidance during training.\nOur comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are compelling: our approach not only surpasses benchmarks set by strong-to-strong distillation but also exceeds the performance of fine-tuning strong models on full datasets. These findings highlight the significant potential of weak-to-strong distillation, demonstrating its ability to substantially enhance vision model performance. Code will be released."
    },
    {
        "title": "Empirical Study on Enhancing Efficiency in Masked Image Modeling Pre-training",
        "link_suffix": "/forum?id=jMZjIi9JcC",
        "link": "https://openreview.net/forum?id=jMZjIi9JcC",
        "pdf_link": "https://openreview.net/pdf?id=jMZjIi9JcC",
        "keywords": "Self-supervised learning, Empirical study",
        "abstract": "The combination of transformers and masked image modeling (MIM) pre-training framework has shown remarkable potential in various vision tasks. However, the high computational cost of pre-training hinders the practical application of MIM.\n   This paper introduces \\emph{FastMIM}, a simple and versatile framework that expedites masked image modeling through two steps: (i) pre-training vision backbones using low-resolution input images and (ii) reconstructing Histograms of Oriented Gradients (HOG) feature instead of original RGB values of the input images.\n   Furthermore, we propose \\emph{FastMIM-P}, which progressively increases the input resolution during the pre-training stage to improve the transfer learning performance of models with high capacity. We point out that: (i) a wide range of input resolutions during pre-training can result in similar performances in fine-tuning and downstream tasks such as detection and segmentation; (ii) the shallow layers of encoder are more important during pre-training, and discarding the last few layers can speed up the training process without affecting fine-tuning performance; and (iii) HOG is more stable than RGB values when transferring resolution. Equipped with \\emph{FastMIM}, any type of vision backbone can be efficiently pre-trained. For example, using ViT-B/Swin-B as backbones, we achieve 83.8%/84.1% top-1 accuracy on ImageNet-1K. Compared to previous approaches, our method can achieve better top-1 accuracy while accelerating the training procedure by 5\u00d7."
    },
    {
        "title": "TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning",
        "link_suffix": "/forum?id=nAVejJURqZ",
        "link": "https://openreview.net/forum?id=nAVejJURqZ",
        "pdf_link": "https://openreview.net/pdf?id=nAVejJURqZ",
        "keywords": "Long Video Understanding; Temporal Grounding; Multimodal Large Language Model",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in short video understanding. However, understanding long-form videos still remains challenging for MLLMs. This paper proposes TimeSuite, a collection of new designs to adapt the existing short-form video MLLMs for long video understanding, including a simple yet efficient framework to process long video sequence, a high-quality video dataset for grounded tuning of MLLMs, and a carefully-designed instruction tuning task to explicitly incorporate the grounding supervision in the traditional QA format. Specifically, based on VideoChat, we propose our long-video MLLM, coined as VideoChat-T, by implementing a token shuffling to compress long video tokens and introducing Temporal Adaptive Position Encoding (TAPE) to enhance the temporal awareness of visual representation. Meanwhile, we introduce the TimePro, a comprehensive grounding-centric instruction tuning dataset composed of 9 tasks and 349k high-quality grounded annotations. Notably, we design a new instruction tuning task type, called Temporal Grounded Caption, to peform detailed video descriptions with the corresponding time stamps prediction. This explicit temporal location prediction will guide MLLM to correctly attend on the visual content when generating description, and thus reduce the hallucination risk caused by the LLMs. Experimental results demonstrate that our TimeSuite provides a successful solution to enhance the long video understanding capability of short-form MLLM, achieving improvement of 5.6% and 6.8% on the benchmarks of Egoschema and VideoMME, respectively. In addition, VideoChat-T exhibits robust zero-shot temporal grounding capabilities, significantly outperforming the existing state-of-the-art MLLMs. After fine-tuning, it performs on par with the traditional supervised expert models."
    },
    {
        "title": "CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification",
        "link_suffix": "/forum?id=TQ2ZOy6miT",
        "link": "https://openreview.net/forum?id=TQ2ZOy6miT",
        "pdf_link": "https://openreview.net/pdf?id=TQ2ZOy6miT",
        "keywords": "adversarial robustness, purification, CLIP",
        "abstract": "In this paper, we aim to build an adversarially robust zero-shot image classifier. We ground our work on CLIP, a vision-language pre-trained encoder model that can perform zero-shot classification by matching an image with text prompts ''a photo of $<$class-name$>$''. Purification is the path we choose since it does not require adversarial training on specific attack types and thus can cope with any foreseen attacks. We then formulate purification risk as the KL divergence between the joint distributions of the purification process of denoising the adversarial samples and the attack process of adding perturbations to benign samples, through bidirectional Stochastic Differential Equations (SDEs). The final derived results inspire us to explore purification in the multi-modal latent space of CLIP. We propose two variants for our CLIPure approach:CLIPure-Diffwhich models the likelihood of images' latent vectors with the DiffusionPrior module in DaLLE-2 (modeling the generation process of CLIP's latent vectors), andCLIPure-Coswhich models the likelihood with the cosine similarity between the embeddings of an image and ``a photo of a''. As far as we know, CLIPure is the first purification method in multi-modal latent space and CLIPure-Cos is the first purification method that is not based on generative models, which substantially improves defense efficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13 datasets that previous CLIP-based defense methods used for evaluating zero-shot classification robustness. Results show that CLIPure boosts the SOTA robustness by a large margin, e.g., from 71.7% to91.1%on CIFAR10, from 59.6% to72.6%on ImageNet, and108%relative improvements of average robustness on the 13 datasets over previous SOTA."
    },
    {
        "title": "Maintaining Adversarial Robustness in  Continuous Learning",
        "link_suffix": "/forum?id=sr0My6yDNu",
        "link": "https://openreview.net/forum?id=sr0My6yDNu",
        "pdf_link": "https://openreview.net/pdf?id=sr0My6yDNu",
        "keywords": "gradient projection technique; sample gradient smoothing;  robust continual learning;  continual learning",
        "abstract": "Adversarial robustness is essential for security and reliability of machine learning systems. However, adversarial robustness enhanced by defense algorithms is easily erased as the neural network's weights update to learn new tasks. To address this vulnerability, it is essential to improve the capability of neural networks in terms of robust continual learning. Specially, we propose a novel gradient projection technique that effectively stabilizes sample gradients from previous data by orthogonally projecting back-propagation gradients onto a crucial subspace before using them for weight updates. This technique can maintaining robustness by collaborating with a class of defense algorithms through sample gradient smoothing. The experimental results on four benchmarks including Split-CIFAR100 and Split-miniImageNet, demonstrate that the superiority of the proposed approach in mitigating rapidly degradation of robustness during continual learning even when facing strong adversarial attacks."
    },
    {
        "title": "Flex3D: Feed-Forward 3D Generation with Flexible Reconstruction Model and Input View Curation",
        "link_suffix": "/forum?id=2vaTZH31oR",
        "link": "https://openreview.net/forum?id=2vaTZH31oR",
        "pdf_link": "https://openreview.net/pdf?id=2vaTZH31oR",
        "keywords": "3D Generation, 3D Reconstruction, Large 3D Models",
        "abstract": "Generating high-quality 3D content from text, single images, or sparse view images remains a challenging task with broad applications.\nExisting methods typically employ multi-view diffusion models to synthesize multi-view images, followed by a feed-forward process for 3D reconstruction. However, these approaches are often constrained by a small and fixed number of input views, limiting their ability to capture diverse viewpoints and, even worse, leading to suboptimal generation results if the synthesized views are of poor quality.\nTo address these limitations, we propose Flex3D, a novel two-stage framework capable of leveraging a flexible number of input views.\nThe first stage consists of a candidate view generation and curation pipeline. We employ a fine-tuned multi-view image diffusion model and a video diffusion model to generate a pool of candidate views, enabling a rich representation of the target 3D object. Subsequently, a view selection pipeline filters these views based on quality and consistency, ensuring that only the high-quality and reliable views are used for reconstruction. In the second stage, the curated views are fed into a Flexible Reconstruction Model (FlexRM), built upon a transformer architecture that can effectively process an arbitrary number of inputs. FlexRM directly outputs 3D Gaussian points leveraging a tri-plane representation, enabling efficient and detailed 3D generation. Through extensive exploration of design and training strategies, we optimize FlexRM to achieve superior performance in both reconstruction and generation tasks. Our results demonstrate that Flex3D achieves state-of-the-art performance, with a user study winning rate of over 92% in 3D generation tasks when compared to several of the latest feed-forward 3D generative models."
    },
    {
        "title": "Simplified Mamba with Disentangled Dependency Encoding for Long-Term Time Series Forecasting",
        "link_suffix": "/forum?id=9VRFPC29nb",
        "link": "https://openreview.net/forum?id=9VRFPC29nb",
        "pdf_link": "https://openreview.net/pdf?id=9VRFPC29nb",
        "keywords": "long-term time series forecasting, time series modeling, mamba",
        "abstract": "Recent advances in deep learning have led to the development of numerous models for Long-term Time Series Forecasting (LTSF). However, most approaches still struggle to comprehensively capture reliable and informative dependencies inherent in time series data. In this paper, we identify and formally define three critical dependencies essential for improving forecasting accuracy: the order dependency and semantic dependency in the time dimension as well as cross-variate dependency in the variate dimension. Despite their significance, these dependencies are rarely considered holistically in existing models. Moreover, improper handling of these dependencies can introduce harmful noise that significantly impairs forecasting performance. To address these challenges, we explore the potential of Mamba for LTSF, highlighting its three key advantages to capture three dependencies, respectively. We further empirically observe that nonlinear activation functions used in vanilla Mamba are redundant for semantically sparse time series data. Therefore, we propose SAMBA, a Simplified Mamba with disentangled dependency encoding. Specifically, we first eliminate the nonlinearity of vanilla Mamba to make it more suitable for LTSF. Along this line, we propose a disentangled dependency encoding strategy to endow Mamba with efficient cross-variate dependency modeling capability while minimizing the interference between time and variate dimensions. We also provide rigorous theory as a justification for our design. Extensive experiments on nine real-world datasets demonstrate the effectiveness of SAMBA over state-of-the-art forecasting models."
    },
    {
        "title": "Prompt-Aware Adapter: Towards Learning Adaptive Visual Tokens for Multimodal Large Language Models",
        "link_suffix": "/forum?id=sSRSKjLki6",
        "link": "https://openreview.net/forum?id=sSRSKjLki6",
        "pdf_link": "https://openreview.net/pdf?id=sSRSKjLki6",
        "keywords": "Prompt-aware Adapter, Multimodal Large Language Models, visual question answering",
        "abstract": "To bridge the gap between vision and language modalities, Multimodal Large Language Models (MLLMs) usually learn an adapter that converts visual inputs to understandable tokens for Large Language Models (LLMs).  However, most adapters generate consistent visual tokens, regardless of the specific objects of interest mentioned in the prompt. Since these adapters distribute equal attention to every detail in the image and focus on the entire scene, they may increase the cognitive load for LLMs, particularly when processing complex scenes. To alleviate this problem, we propose prompt-aware adapters. These adapters are designed with the capability to dynamically embed visual inputs based on the specific focus of the prompt. Specifically, prompt-aware adapters utilize both global and local textual features to capture the most relevant visual clues from the prompt at both coarse and fine granularity levels. This approach significantly enhances the ability of LLMs to understand and interpret visual content. Experiments on various visual question answering tasks, such as counting and position reasoning, demonstrate the effectiveness of prompt-aware adapters. The code will be publicly available."
    },
    {
        "title": "ProtoLLM: Training and Example-free LLMs for Few-shot Tabular Learning",
        "link_suffix": "/forum?id=kymuzakf7V",
        "link": "https://openreview.net/forum?id=kymuzakf7V",
        "pdf_link": "https://openreview.net/pdf?id=kymuzakf7V",
        "keywords": "Tabular data, Few-shot learning, Large language models",
        "abstract": "Recent breakthroughs in large language models (LLMs) have opened the door to in-depth investigation of their potential in tabular data modeling. However, the paradigm for effectively utilizing advanced LLMs in few-shot and even unseen scenarios remains to be explored. We observed an unusual phenomenon: directly using LLMs for data augmentation or rule generation by feeding a few examples significantly degrades the reasoning ability in tabular data understanding. We identified two main obstacles behind this issue: overfitting to the examples and knowledge disruption. Specifically, the provided examples may introduce noisy patterns that interfere with the model's prior knowledge, leading to unexpected and less reliable results. To this end, we propose an example-free framework to leverage the inherent knowledge of LLMs. Our key idea is to prompt the LLM for oracle feature generation based solely on task and feature description. Without such example pollution, each output feature is treated as a standard guideline, and they together act as a prototype for each class. To transfer the LLM's knowledge to a given task, we further design an efficient fusion strategy to integrate the prototype with example features, showing impressive generalizability in the few-shot setting. Importantly, our pipeline requires no learnable variables, resulting in a desired training-free property. Extensive comparisons and ablations on multiple tabular datasets demonstrate the improvements of our simple framework."
    },
    {
        "title": "IFAdapter: Instance feature control for grounded Text-to-Image Generation",
        "link_suffix": "/forum?id=25l4SWH2eS",
        "link": "https://openreview.net/forum?id=25l4SWH2eS",
        "pdf_link": "https://openreview.net/pdf?id=25l4SWH2eS",
        "keywords": "Generative diffusion models, Layout to image generation",
        "abstract": "While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process in a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models\u2019 abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations."
    },
    {
        "title": "Reward-Robust RLHF in LLMs",
        "link_suffix": "/forum?id=JJepij22fb",
        "link": "https://openreview.net/forum?id=JJepij22fb",
        "pdf_link": "https://openreview.net/pdf?id=JJepij22fb",
        "keywords": "RLHF, LLM, robustness, alignment",
        "abstract": "As Large Language Models continue to progress toward more advanced forms of intelligence, Reinforcement Learning from Human Feedback is increasingly seen as a key pathway toward achieving Artificial General Intelligence. However, the reliance on reward-model-based alignment methods introduces significant challenges due to the inherent instability and imperfections of Reward Models (RMs), which can lead to critical issues such as reward hacking and misalignment with human intentions. In this paper, we introduce a reward-robust RLHF framework aimed at addressing these fundamental challenges, paving the way for more reliable and resilient learning in LLMs. Our approach introduces a novel optimization objective that carefully balances performance and robustness by incorporating Bayesian Reward Model Ensembles to model the uncertainty set of reward functions. This allows the framework to integrate both nominal performance and minimum reward signals, ensuring more stable learning even with imperfect RMs. Empirical results demonstrate that our framework consistently outperforms baselines across diverse benchmarks, showing improved accuracy and long-term stability. We also provide a theoretical analysis, demonstrating that reward-robust RLHF approaches the stability of constant reward settings, which proves to be acceptable even in a stochastic-case analysis. Together, these contributions highlight the framework\u2019s potential to enhance both the performance and stability of LLM alignment."
    },
    {
        "title": "Solving Video Inverse Problems Using Image Diffusion Models",
        "link_suffix": "/forum?id=TRWxFUzK9K",
        "link": "https://openreview.net/forum?id=TRWxFUzK9K",
        "pdf_link": "https://openreview.net/pdf?id=TRWxFUzK9K",
        "keywords": "Image diffusion models, Video inverse problems, Batch-consistent sampling",
        "abstract": "Recently, diffusion model-based inverse problem solvers (DIS) have emerged as state-of-the-art approaches for addressing inverse problems, including image super-resolution, deblurring, inpainting, etc. \nHowever, their application to video inverse problems arising from spatio-temporal degradation remains largely unexplored due to the challenges in training video diffusion models.\nTo address this issue, here we introduce an innovative video inverse solver that leverages only image diffusion models.\nSpecifically, by drawing inspiration from the success of the recent decomposed diffusion sampler (DDS), \nour method treats the time dimension of a video as the batch dimension of image diffusion models and solves spatio-temporal optimization problems within denoised spatio-temporal batches derived from each image diffusion model.\nMoreover, we introduce a batch-consistent diffusion sampling strategy that encourages consistency across batches by synchronizing the stochastic noise components in image diffusion models. \nOur approach synergistically combines batch-consistent sampling with simultaneous optimization of denoised spatio-temporal batches at each reverse diffusion step, resulting in a novel and efficient diffusion sampling strategy for video inverse problems.\nExperimental results demonstrate that our method effectively addresses various spatio-temporal degradations in video inverse problems, achieving state-of-the-art reconstructions.\nProject page:https://solving-video-inverse.github.io/main"
    },
    {
        "title": "How Far Is Video Generation from World Model: A Physical Law Perspective",
        "link_suffix": "/forum?id=ZyLkNVHBZF",
        "link": "https://openreview.net/forum?id=ZyLkNVHBZF",
        "pdf_link": "https://openreview.net/pdf?id=ZyLkNVHBZF",
        "keywords": "video generation, diffusion model, world model",
        "abstract": "OpenAI's Sora highlights the potential of video generation for developing world models that adhere to fundamental physical laws. \nHowever, the ability of video generation models to discover such laws purely from visual data without human priors can be questioned.\nA world model learning the true law should give predictions robust to nuances and correctly extrapolate on unseen scenarios.\nIn this work, we evaluate across three key scenarios: in-distribution, out-of-distribution, and combinatorial generalization.\nWe developed a 2D simulation testbed for object movement and collisions to generate videos deterministically governed by one or more classical mechanics laws.\nThis provides unlimited supply of data for large-scale experimentation, and enables quantitative evaluation for the law in generated videos. \nWe trained diffusion-based video generation models to predict object movements based on initial frames.\nOur scaling experiments show perfect generalization within the distribution, measurable scaling behavior for combinatorial generalization, but failure in out-of-distribution scenarios.\nFurther experiments reveal two key insights about the generalization mechanisms of these models: (1) the models fail to abstract general physical rules and instead exhibit ``case-based'' generalization behavior, \\textit{i.e.}, mimicking the closest training example; (2) when generalizing to new cases, models are observed to prioritize different factors when referencing training data: color $>$ size $>$ velocity $>$ shape.\nOur study suggests that scaling alone is insufficient for video generation models to uncover fundamental physical laws, despite its role in Sora's broader success."
    },
    {
        "title": "Detecting Problematic Questions to Support Math Word Problem Design",
        "link_suffix": "/forum?id=ma4SUzeCLR",
        "link": "https://openreview.net/forum?id=ma4SUzeCLR",
        "pdf_link": "https://openreview.net/pdf?id=ma4SUzeCLR",
        "keywords": "Problematic Question Detection, Question Design Support, Self-Optimization Prompting",
        "abstract": "When designing math word problems, teachers must ensure the clarity and precision of the question to avoid multiple interpretations and unanswerable situations, thereby maintaining consistent grading standards and effectiveness. We address these issues to provide comprehensive support to teachers in creating clear, solvable, and formal math word problems. In this paper, we present MathError, a dataset of real-world math word problems annotated with error types to investigate the need for question correction. Our work explores how large language models (LLMs) can assist teachers in detecting problematic questions to support math word problem design in scenarios with limited data, simulating real-world conditions with minimal training samples. Preliminary results demonstrate the models' capabilities in detecting problematic questions and identify areas for further research and development in educational applications."
    },
    {
        "title": "Preserving Large Activations: The Key to KV Cache Pruning",
        "link_suffix": "/forum?id=Uhvy5u90UY",
        "link": "https://openreview.net/forum?id=Uhvy5u90UY",
        "pdf_link": "https://openreview.net/pdf?id=Uhvy5u90UY",
        "keywords": "Large Language Models, Efficiency, Compression, Long Context",
        "abstract": "As context lenghts grows, the increasing size of Key and Value (KV) cache poses a significant challenge to efficiently serving Large Language Models (LLMs). KV cache pruning, by preserving only a small subset of important KV cache for sparse inference, is a recognized effective solution. Our research revealed that large activations are the key to identifying these important KV cache. However, existing methods have not been successful in effectively identifying these important KV cache due to neglecting the impact of Value cache, and are also incompatible with Grouped-Query Attention (GQA) architectures. To address these issues, we introduce an innovative KV cache pruning method that preserves these large activations and is compatible with Grouped-Query Attention. Featuring a novel pruning metric, this method operates within each attention group to enhance efficiency and minimize performance degradation. Experimental results demonstrate that our approach not only maintains comparable accuracy with existing methods but also significantly reduces KV cache requirements. Specifically, It demonstrates similar accuracy while utilizing only 1/10 of the KV cache compared to existing SOTA methods."
    },
    {
        "title": "Jailbreaking as a Reward Misspecification Problem",
        "link_suffix": "/forum?id=uBnM3EFovQ",
        "link": "https://openreview.net/forum?id=uBnM3EFovQ",
        "pdf_link": "https://openreview.net/pdf?id=uBnM3EFovQ",
        "keywords": "Large language models, alignment, jailbreaking",
        "abstract": "The widespread adoption of large language models (LLMs) has raised concerns about their safety and reliability, particularly regarding their vulnerability to adversarial attacks. In this paper, we propose a novel perspective that attributes this vulnerability to reward misspecification during the alignment process. This misspecification occurs when the reward function fails to accurately capture the intended behavior, leading to misaligned model outputs. We introduce a metric ReGap to quantify the extent of reward misspecification and demonstrate its effectiveness and robustness in detecting harmful backdoor prompts. Building upon these insights, we present ReMiss, a system for automated red teaming that generates adversarial prompts in a reward-misspecified space. ReMiss achieves state-of-the-art attack success rates on the AdvBench benchmark  against various target aligned LLMs while preserving the human readability of the generated prompts. Furthermore, these attacks on open-source models demonstrate high transferability to closed-source models like GPT-4o and out-of-distribution tasks from HarmBench. Detailed analysis highlights the unique advantages of the proposed reward misspecification objective compared to previous methods, offering new insights for improving LLM safety and robustness."
    }
]