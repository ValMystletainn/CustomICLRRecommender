[{"title": "The Computational Complexity of Positive Non-Clashing Teaching in Graphs", "link_suffix": "/forum?id=Jd3Vd7GCyq", "link": "https://openreview.net/forum?id=Jd3Vd7GCyq", "pdf_link": "https://openreview.net/pdf?id=Jd3Vd7GCyq", "keywords": "non-clashing teaching, positive teaching, computational complexity, NP-hardness, parameterized complexity", "abstract": "We study the classical and parameterized complexity of computing the positive non-clashing teaching dimension of a set of concepts, that is, the smallest number of examples per concept required to successfully teach an intelligent learner under the considered, previously established model. For any class of concepts, it is known that this problem can be effortlessly transferred to the setting of balls in a graph $G$. We establish (1) the NP-hardness of the problem even when restricted to instances with positive non-clashing teaching dimension $k=2$ and where all balls in the graph are present, (2) near-tight running time upper and lower bounds for the problem on general graphs, (3) fixed-parameter tractability when parameterized by the vertex integrity of $G$, and (4) a lower bound excluding fixed-parameter tractability when parameterized by the feedback vertex number and pathwidth of $G$, even when combined with $k$.\nOur results provide a nearly complete understanding of the complexity landscape of computing the positive non-clashing teaching dimension and answer open questions from the literature.", "title_embedding_index": 12400, "title_abs_embedding_index": 12425}, {"title": "VisualAgentBench: Towards Large Multimodal Models as Visual Agents", "link_suffix": "/forum?id=2snKOc7TVp", "link": "https://openreview.net/forum?id=2snKOc7TVp", "pdf_link": "https://openreview.net/pdf?id=2snKOc7TVp", "keywords": "Large Multimodal Models, Agents, Evaluation", "abstract": "Large Multimodal Models (LMMs) have ushered in a new era in artificial intelligence, merging capabilities in both language and vision to form highly capable visual agents that are postulated to excel across a myriad of tasks.\n  However, existing benchmarks fail to sufficiently challenge or showcase the full potential of LMMs as agents in complex, real-world environments. \n  To address this gap, we introduce VisualAgentBench (VAB), a comprehensive and unified benchmark specifically designed to train and evaluate LMMs as visual agents across diverse scenarios in one standard setting, including Embodied, Graphical User Interface, and Visual Design, with tasks formulated to probe the depth of LMMs' understanding and interaction capabilities. \n  Through rigorous testing across 9 proprietary LMM APIs and 9 open models (18 in total), we demonstrate the considerable yet still developing visual agent capabilities of these models. \n  Additionally, VAB explores the synthesizing of visual agent trajectory data through hybrid methods including Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations, offering insights into obstacles, solutions, and trade-offs one may meet in developing open LMM agents. \n  Our work not only aims to benchmark existing models but also provides an instrumental playground for future development into visual agents.", "title_embedding_index": 12401, "title_abs_embedding_index": 12426}, {"title": "Do we need rebalancing strategies? A theoretical and empirical study around SMOTE and its variants", "link_suffix": "/forum?id=uLAAVg0ymc", "link": "https://openreview.net/forum?id=uLAAVg0ymc", "pdf_link": "https://openreview.net/pdf?id=uLAAVg0ymc", "keywords": "Classification, Imbalanced data set, SMOTE", "abstract": "Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced tabular data sets. However, few works analyze SMOTE theoretically. In this paper, we prove that SMOTE  (with default parameter) tends to copy the original minority samples asymptotically. We also prove that SMOTE exhibits boundary artifacts, thus justifying existing SMOTE variants. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. Surprisingly, for most data sets, we observe that applying no rebalancing strategy is competitive in terms of predictive performances, with tuned random forests, logistic regression or LightGBM. For highly imbalanced data sets, our new methods, named CV-SMOTE and Multivariate Gaussian SMOTE, are competitive. Besides, our analysis sheds some lights on the behavior of common rebalancing strategies, when used in conjunction with random forests.", "title_embedding_index": 12402, "title_abs_embedding_index": 12427}, {"title": "Rethinking Multiple-Instance Learning From Feature Space to Probability Space", "link_suffix": "/forum?id=torbeUlslS", "link": "https://openreview.net/forum?id=torbeUlslS", "pdf_link": "https://openreview.net/pdf?id=torbeUlslS", "keywords": "Multiple-Instance Learning\uff1bRepresentation Learning\uff1bSemi-supervised Learning; Weakly-supervised Learning;", "abstract": "Multiple-instance learning (MIL) was initially proposed to identify key instances within a set (bag) of instances when only one bag-level label is provided. Current deep MIL models mostly solve multi-instance aggregation problem in feature space. Nevertheless, with the increasing complexity of data, this paradigm faces significant risks in representation learning stage, which could lead to algorithm degradation in deep MIL models. We verify that the degradation issue stems from the persistent drift of instances in feature space during representation learning. In this paper, we propose a novel Probability-Space MIL network (PSMIL) as a countermeasure. In PSMIL, a self-training alignment strategy is introduced in probability space to solve the drift problem in feature space, and the alignment target objective is proven mathematically optimal. Furthermore, we reveal that the widely-used attention-based pooling mechanism in current deep MIL models is easily affected by the perturbation in feature space and further introduce an alternative called probability-space attention pooling. It effectively captures the key instance in each bag from feature space to probability space, and further eliminates the impact of selection drift in the pooling stage. To summarize, PSMIL seeks to solve a MIL problem in probability space rather than feature space. We also introduce new comprehensive benchmarks to evaluate the representation quality for MIL models. Experimental results illustrate that our method could potentially achieve performance close to supervised learning level in complex tasks (gap within 5%). The incremental alignment could also bring more than 19% accuracy improvements for current existing mainstream models in challenging scenarios. For existing bag-level classification benchmarks, our method also achieves competitive performance to the state-of-the-art deep MIL models. $\\textit{Our code and benchmarks will be released to the community.}$", "title_embedding_index": 12403, "title_abs_embedding_index": 12428}, {"title": "Attribute-based Visual Reprogramming for Image Classification with CLIP", "link_suffix": "/forum?id=j964C6y92q", "link": "https://openreview.net/forum?id=j964C6y92q", "pdf_link": "https://openreview.net/pdf?id=j964C6y92q", "keywords": "Visual Reprogramming, Model Reprogramming, Vision-Language Models, Image Classification", "abstract": "Visual reprogramming(VR) reuses pre-trained vision models for downstream image classification tasks by adding trainable noise patterns to inputs. When applied to vision-language models (e.g., CLIP), existing VR approaches follow the same pipeline used in vision models (e.g., ResNet, ViT), where ground-truth class labels are inserted into fixed text templates to guide the optimization of VR patterns. This label-based approach, however, overlooks the rich information and diverse attribute-guided textual representations that CLIP can exploit, which may lead to the misclassification of samples. In this paper, we proposeAttribute-basedVisualReprogramming(AttrVR) for CLIP, utilizingdescriptiveattributes(DesAttrs) anddistinctiveattributes(DistAttrs), which respectively represent common and unique feature descriptions for different classes. Besides, as images of the same class may reflect different attributes after VR, AttrVR iteratively refines patterns using the $k$-nearest DesAttrs and DistAttrs for each image sample, enabling more dynamic and sample-specific optimization. Theoretically, AttrVR is shown to reduce intra-class variance and increase inter-class separation. Empirically, it achieves superior performance in 12 downstream tasks for both ViT-based and ResNet-based CLIP. The success of AttrVR facilitates more effective integration of VR from unimodal vision models into vision-language models.", "title_embedding_index": 12404, "title_abs_embedding_index": 12429}, {"title": "Error Feedback for Smooth and Nonsmooth Convex Optimization with Constant, Decreasing and Polyak Stepsizes", "link_suffix": "/forum?id=Qv9TG9yDG0", "link": "https://openreview.net/forum?id=Qv9TG9yDG0", "pdf_link": "https://openreview.net/pdf?id=Qv9TG9yDG0", "keywords": "Error feedback; Polyak stepsize; Communication-efficient optimization", "abstract": "Error feedback, originally proposed a decade ago by Seide et al (2014), is an immensely popular strategy for stabilizing the convergence behavior of distributed algorithms employing communication compression via the application of contractive compression operators, such as greedy and random sparsification, quantization, and low-rank approximation. While our algorithmic and theoretical understanding of error feedback has grown immensely over the years, several important considerations remained elusive. For example, the theory of error feedback is fully focused on the smooth convex and nonconvex regimes, and results in the nonsmooth convex setting are limited. This is not a coincidence: Error feedback works when the gradients converge, and this is not necessarily the case in the nonsmooth setting. Further, existing stepsize rules for error feedback are limited to constant schedules; a by-product of the current theoretical approach to analyzing error feedback. By modifying the algorithmic design of error feedback, we are able to resolve these issues. In particular, we provide a comprehensive analysis covering both the smooth and nonsmooth convex regimes, and give support for constant, decreasing and adaptive (Polyak-type) stepsizes. This is  the first time such results are obtained. In particular, this is the first time adaptive stepsizes have successfully been combined with compression mechanisms. Our theoretical results are corroborated with suitable numerical experiments.", "title_embedding_index": 12405, "title_abs_embedding_index": 12430}, {"title": "Revisit Non-parametric Two-sample Testing as a Semi-supervised Learning Problem", "link_suffix": "/forum?id=X8RTdxzqJQ", "link": "https://openreview.net/forum?id=X8RTdxzqJQ", "pdf_link": "https://openreview.net/pdf?id=X8RTdxzqJQ", "keywords": "Two-sample Testing, Semi-supervised learning, Hypothesis testing", "abstract": "Learning effective data representations is crucial in answering if two samples X and Y are from the same distribution (a.k.a. the non-parametric two-sample testing problem), which can be categorized into: i) learning discriminative representations (DRs) that distinguish between two samples in a supervised-learning paradigm, and ii) learning inherent representations (IRs) focusing on data's inherent features in an unsupervised-learning paradigm. \nHowever, both paradigms have issues: learning DRs reduces the data points available for the two-sample testing phase, and learning purely IRs misses discriminative cues. To mitigate both issues, we propose a novel perspective to consider non-parametric two-sample testing as a semi-supervised learning (SSL) problem, introducing the SSL-based Classifier Two-Sample Test (SSL-C2ST) framework. While a straightforward implementation of SSL-C2ST might directly use existing state-of-the-art (SOTA) SSL methods to train a classifier with labeled data (with sample indexes X or Y) and unlabeled data (the remaining ones in the two samples), conventional two-sample testing data often exhibits substantial overlap between samples and violates SSL methods' assumptions, resulting in low test power. Therefore, we propose a two-step approach: first, learn IRs using all data, then fine-tune IRs with only labelled data to learn DRs, which can both utilize information from whole dataset and adapt the discriminative power to the given data. Extensive experiments and theoretical analysis demonstrate that SSL-C2ST outperforms traditional C2ST by effectively leveraging unlabeled data. We also offer a stronger empirically designed test achieving the SOTA performance in many two-sample testing datasets.", "title_embedding_index": 12406, "title_abs_embedding_index": 12431}, {"title": "PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration", "link_suffix": "/forum?id=rFpZnn11gj", "link": "https://openreview.net/forum?id=rFpZnn11gj", "pdf_link": "https://openreview.net/pdf?id=rFpZnn11gj", "keywords": "Image-text pairs generation, Vision-language models, Multi-agent collaboration", "abstract": "Vision Language Models (VLMs) like CLIP have attracted substantial attention in pathology, serving as backbones for applications such as zero-shot image classification and Whole Slide Image (WSI) analysis. Additionally, they can function as vision encoders when combined with large language models (LLMs) to support broader capabilities. Current efforts to train pathology VLMs rely on pathology image-text pairs from platforms like PubMed, YouTube, and Twitter, which provide limited, unscalable data with generally suboptimal image quality. In this work, we leverage large-scale WSI datasets like TCGA to extract numerous high-quality image patches. We then train a large multimodal model to generate captions for these images, creating PathGen-1.6M, a dataset containing 1.6 million high-quality image-caption pairs. Our approach involves multiple agent models collaborating to extract representative WSI patches, generating and refining captions to obtain high-quality image-text pairs. Extensive experiments show that integrating these generated pairs with existing datasets to train a pathology-specific CLIP model, PathGen-CLIP, significantly enhances its ability to analyze pathological images, with substantial improvements across nine pathology-related zero-shot image classification tasks and three whole-slide image tasks. Furthermore, we construct 200K instruction-tuning data based on PathGen-1.6M and integrate PathGen-CLIP with the Vicuna LLM to create more powerful multimodal models through instruction tuning. Overall, we provide a scalable pathway for high-quality data generation in pathology, paving the way for next-generation general pathology models. Our dataset, code, and model are open-access athttps://github.com/PathGen-1-6M/PathGen-1.6M.", "title_embedding_index": 12407, "title_abs_embedding_index": 12432}, {"title": "CRAB: Cross-environment Agent Benchmark for Multimodal Language Model Agents", "link_suffix": "/forum?id=qqKJjwibsp", "link": "https://openreview.net/forum?id=qqKJjwibsp", "pdf_link": "https://openreview.net/pdf?id=qqKJjwibsp", "keywords": "Benchmark, GUI agent, Multimodal Language Model", "abstract": "The development of autonomous agents increasingly relies on Multimodal Language Models (MLMs) to perform tasks described in natural language with GUI environments, such as websites, desktop computers, or mobile phones. Existing benchmarks for MLM agents in interactive environments are limited by their focus on a single environment, lack of detailed and generalized evaluation methods, and the complexities of constructing tasks and evaluators. To overcome these limitations, we introduce CRAB, the first agent benchmark framework designed to support cross-environment tasks, incorporating a graph-based fine-grained evaluation method and an efficient mechanism for task and evaluator construction. Our framework supports multiple devices and can be easily extended to any environment with a Python interface. Leveraging CRAB, we developed a cross-platform CRAB Benchmark-v0 comprising 120 tasks in computer desktop and mobile phone environments. We evaluated four advanced MLMs using different single and multi-agent system configurations on this benchmark. The experimental results demonstrate that the single agent with GPT-4o achieves the best completion ratio of 38.01%.", "title_embedding_index": 12408, "title_abs_embedding_index": 12433}, {"title": "Inverse Constitutional AI: Compressing Preferences into Principles", "link_suffix": "/forum?id=9FRwkPw3Cn", "link": "https://openreview.net/forum?id=9FRwkPw3Cn", "pdf_link": "https://openreview.net/pdf?id=9FRwkPw3Cn", "keywords": "human feedback, evaluation, interpretability, preference learning, AI annotators", "abstract": "Feedback data is widely used to align or evaluate state-of-the-art AI models according to human preferences. Pairwise text preferences, where human (or AI) annotators select the \u201cbetter\u201d of two options, are particularly common. This data is typically used to train reward models or to compute aggregate statistics, asserting one model to be \u201cbetter\u201d than another. For many applications, however, it is desirable to understand human preferences in addition to modeling them. Neither black-box reward models nor statistics can answer why one model is better than another. Pairwise preference datasets, therefore, pose an interpretability challenge. The raw data consists of numerous (long) response pairs that are often infeasible to interpret manually. Prior work has demonstrated that human-annotated preference data often exhibits unintended biases, underscoring the urgent need for good interpretability tools to detect and alleviate such biases. In this paper, we introduce the Inverse Constitutional AI (ICAI) problem, formulating the interpretation of pairwise text preference data as a compression task. In constitutional AI, a set of principles (a constitution) is used to provide feedback and fine-tune AI models. ICAI inverts this process: given a feedback dataset, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding algorithm and validate its generated constitutions quantitatively based on annotation reconstruction accuracy on a variety of datasets: (a) synthetic feedback data with known underlying principles; (b) AlpacaEval data with cross-annotated human feedback; (c) crowdsourced Chatbot Arena data; and (d) PRISM data from diverse demographic groups. As a short and interpretable representation of the original dataset, generated constitutions have many potential use cases \u2014 they may help identify undesirable annotator biases, better understand model performance, scale feedback to unseen data, or assist with adapting LLMs to individual user or group preferences. We release the code for our experiments athidden url.", "title_embedding_index": 12409, "title_abs_embedding_index": 12434}, {"title": "When Attention Sink Emerges in Language Models: An Empirical View", "link_suffix": "/forum?id=78Nn4QJTEN", "link": "https://openreview.net/forum?id=78Nn4QJTEN", "pdf_link": "https://openreview.net/pdf?id=78Nn4QJTEN", "keywords": "Attention Sink, Language Models, Empirical Study", "abstract": "Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known asattention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others.  Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate howoptimization,data distribution,loss function, andmodel architecturein LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases,storing extra attention scores, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.", "title_embedding_index": 12410, "title_abs_embedding_index": 12435}, {"title": "PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance", "link_suffix": "/forum?id=rxVvRBgqmS", "link": "https://openreview.net/forum?id=rxVvRBgqmS", "pdf_link": "https://openreview.net/pdf?id=rxVvRBgqmS", "keywords": "Hand pose estimation, piano music, motion generation", "abstract": "Recently, artificial intelligence techniques for education have been received increasing attentions, while it still remains an open problem to design the effective music instrument instructing systems. Although key presses can be directly derived from sheet music, the transitional movements among key presses require more extensive guidance in piano performance. In this work, we construct a piano-hand motion generation benchmark to guide hand movements and fingerings for piano playing. To this end, we collect an annotated dataset, PianoMotion10M, consisting of 116 hours of piano playing videos from a bird's-eye view with 10 million annotated hand poses. We also introduce a powerful baseline model that generates hand motions from piano audios through a position predictor and a position-guided gesture generator. Furthermore, a series of evaluation metrics are designed to assess the performance of the baseline model, including motion similarity, smoothness, positional accuracy of left and right hands, and overall fidelity of movement distribution. Despite that piano key presses with respect to music scores or audios are already accessible, PianoMotion10M aims to provide guidance on piano fingering for instruction purposes. The dataset and source code can be accessed athttps://github.com/PianoMotion10M/PianoMotion10M.", "title_embedding_index": 12411, "title_abs_embedding_index": 12436}, {"title": "Identifying Optimal Output Sets for Differential Privacy Auditing", "link_suffix": "/forum?id=A61WjOU7o4", "link": "https://openreview.net/forum?id=A61WjOU7o4", "pdf_link": "https://openreview.net/pdf?id=A61WjOU7o4", "keywords": "privacy auditing, differential privacy, DP-SGD", "abstract": "Differential privacy limits an algorithm's privacy loss, defined as the maximum influenceanyindividual data record can have on the probability of observinganypossible output. Privacy auditing identifies the worst-case input datasets and output event sets that empirically maximize privacy loss, providing statistical lower bounds to evaluate the tightness of an algorithm's differential privacy guarantees. However, current auditing methods often depend on heuristic or arbitrary selections of output event sets, leading to weak lower bounds. We address this critical gap by introducing a novel framework to compute theoptimal output event setthat maximizes the privacy loss lower bound in auditing. Our algorithm efficiently computes this optimal set when closed-form output distributions are available and approximates it using empirical samples when they are not. Through extensive experiments on both synthetic and real-world datasets, we demonstrate that our method consistently tightens privacy lower bounds for auditing differential privacy mechanisms and black-box DP-SGD training. Our approach outperforms existing auditing techniques, providing a more accurate analysis of differentially-private algorithms.", "title_embedding_index": 12412, "title_abs_embedding_index": 12437}, {"title": "Capsule Network Projectors are Equivariant and Invariant Learners", "link_suffix": "/forum?id=i99hYFGpWl", "link": "https://openreview.net/forum?id=i99hYFGpWl", "pdf_link": "https://openreview.net/pdf?id=i99hYFGpWl", "keywords": "Capsule Networks, Equivariant, Invariant, Self Supervised", "abstract": "Learning invariant representations has been the longstanding approach to self-supervised learning. However, recently progress has been made in preserving equivariant properties in representations, yet do so with highly prescribed architectures. In this work, we propose an invariant-equivariant self-supervised architecture that employs Capsule Networks (CapsNets) which have been shown to capture equivariance with respect to novel viewpoints. We demonstrate that the use of CapsNets in equivariant self-supervised architectures achieves improved downstream performance on equivariant tasks with higher efficiency and fewer network parameters. To accommodate the architectural changes of CapsNets, we introduce a new objective function based on entropy minimisation. This approach which we name CapsIE (Capsule Invariant Equivariant Network) achieves state-of-the-art performance across all invariant and equivariant downstream tasks on the 3DIEBench dataset, while outperforming supervised baselines. Our results demonstrate the ability of CapsNets to learn complex and generalised representations for large-scale, multi-task datasets compared to previous CapsNet benchmarks.", "title_embedding_index": 12413, "title_abs_embedding_index": 12438}, {"title": "DensBO: Dynamic Ensembling of Surrogate Models for Hyperparameter Optimisation", "link_suffix": "/forum?id=OhcWlo1M8q", "link": "https://openreview.net/forum?id=OhcWlo1M8q", "pdf_link": "https://openreview.net/pdf?id=OhcWlo1M8q", "keywords": "hyperparameter optimisation, Bayesian optimisation, surrogate models", "abstract": "Hyperparameter optimisation (HPO) of machine learning models is crucial for achieving optimal performance for different tasks. Surrogate-based optimisation techniques, such as Bayesian optimisation (BO), have been successfully applied to tackle this problem. BO is subject to different design choices of its components. In particular, depending on the nature and the size of the search space, the choice of the surrogate model has a substantial impact on the overall performance of BO. Surrogate models in BO approximate the function to optimise and guide the search towards promising regions by predicting the function value for different solution candidates. Combining different machine learning (ML) models is known to lead to performance gains, e.g., in different prediction tasks. To this end, we propose a novel dynamic approach to ensemble surrogate models in the BO pipeline, leveraging the complementary powers of different surrogate models at different stages of the optimisation process. We empirically evaluate our method on numerous benchmarks and demonstrate its advantage compared to state-of-the-art single-surrogate BO baselines. We highlight the usefulness of our approach in finding good hyperparameter configurations in mixed (numerical and categorical) search spaces for a wide range of problems.", "title_embedding_index": 12414, "title_abs_embedding_index": 12439}, {"title": "Learning Dispersed Embeddings on Hyperspheres", "link_suffix": "/forum?id=Y4UliyX3LE", "link": "https://openreview.net/forum?id=Y4UliyX3LE", "pdf_link": "https://openreview.net/pdf?id=Y4UliyX3LE", "keywords": "embeddings, dispersion, hypersphere, representation learning, separation", "abstract": "Learning well-separated features in high-dimensional spaces, such as text or image $\\textit{embeddings}$, is crucial for many machine learning applications. Achieving such separation can be effectively accomplished through the $\\textit{dispersion}$ of embeddings, where unrelated vectors are pushed apart as much as possible. By constraining features to be on a $\\textit{hypersphere}$, we can connect dispersion to well-studied problems in mathematics and physics, where optimal solutions are known for limited low-dimensional cases. However, in representation learning we typically deal with a large number of features in high-dimensional space, which makes leveraging existing theoretical and numerical solutions impossible. Therefore, we rely on gradient-based methods to approximate the optimal dispersion on a hypersphere. In this work, we first give an overview of existing methods from disconnected literature. Next, we propose new reinterpretations of known methods, namely Maximum Mean Discrepancy (MMD) and Lloyd\u2019s relaxation algorithm. Finally, we derive a novel dispersion method that directly exploits properties of the hypersphere. Our experiments show the importance of dispersion in image classification and natural language processing tasks, and how algorithms exhibit different trade-offs in different regimes.", "title_embedding_index": 12415, "title_abs_embedding_index": 12440}, {"title": "MetaFood3D: 3D Food Dataset with Nutrition Values", "link_suffix": "/forum?id=IUzQfdkkoL", "link": "https://openreview.net/forum?id=IUzQfdkkoL", "pdf_link": "https://openreview.net/pdf?id=IUzQfdkkoL", "keywords": "3D food dataset, real scanned data, nutrition values, image-based dietary assessment, 3D reconstruction, food portion estimation, deep learning, computer vision", "abstract": "Food computing is both important and challenging in computer vision (CV). It significantly contributes to the development of CV algorithms due to its frequent presence in datasets across various applications, ranging from classification and instance segmentation to 3D reconstruction. The polymorphic shapes and textures of food, coupled with high variation in forms and vast multimodal information, including language descriptions and nutritional data, make food computing a complex and demanding task for modern CV algorithms. 3D food modeling is a new frontier for addressing food related problems, due to its inherent capability to deal with random camera views and its straightforward representation for calculating food portion size.  However, the primary hurdle in the development of algorithms for food object analysis is the lack of nutrition values in existing 3D datasets. Moreover, in the broader field of 3D research, there is a critical need for domain-specific test datasets. To bridge the gap between general 3D vision and food computing research, we introduce MetaFood3D. This dataset consists of 637 meticulously scanned and labeled 3D food objects across 108 categories, featuring detailed nutrition information, weight, and food codes linked to a comprehensive nutrition database. Our MetaFood3D dataset emphasizes intra-class diversity and includes rich modalities such as textured mesh files, RGB-D videos, and segmentation masks.\nExperimental results demonstrate our dataset's significant potential for improving algorithm performance, highlight the challenging gap between video captures and 3D scanned data, and showcase the strengths of MetaFood3D in high-quality data generation, simulation, and augmentation.", "title_embedding_index": 12416, "title_abs_embedding_index": 12441}, {"title": "Spreading Out-of-Distribution Detection on Graphs", "link_suffix": "/forum?id=p1TBYyqy8v", "link": "https://openreview.net/forum?id=p1TBYyqy8v", "pdf_link": "https://openreview.net/pdf?id=p1TBYyqy8v", "keywords": "out-of-distribution detection, graph neural networks, benchmark, healthcare", "abstract": "Node-level out-of-distribution (OOD) detection on graphs has received significant attention from the machine learning community. However, previous approaches are evaluated using unrealistic benchmarks that consider only randomly selected OOD nodes, failing to reflect the interactions among nodes. In this paper, we introduce a new challenging task to model the interactions of OOD nodes in a graph, termed spreading OOD detection, where a newly emerged OOD node spreads its property to neighboring nodes. We curate realistic benchmarks by employing the epidemic spreading models that simulate the spreading of OOD nodes on the graph. We also showcase a ``Spreading COVID-19\" dataset to demonstrate the applicability of spreading OOD detection in real-world scenarios. Furthermore, to effectively detect spreading OOD samples under the proposed benchmark setup, we present a new approach called energy distribution-based detector (EDBD), which includes a novel energy-aggregation scheme. EDBD is designed to mitigate undesired mixing of OOD scores between in-distribution (ID) and OOD nodes. Our extensive experimental results demonstrate the superiority of our approach over state-of-the-art methods in both spreading OOD detection and conventional node-level OOD detection tasks across seven benchmark datasets.", "title_embedding_index": 12417, "title_abs_embedding_index": 12442}, {"title": "Training on the Test Task Confounds Evaluation and Emergence", "link_suffix": "/forum?id=jOmk0uS1hl", "link": "https://openreview.net/forum?id=jOmk0uS1hl", "pdf_link": "https://openreview.net/pdf?id=jOmk0uS1hl", "keywords": "language models, benchmarking, emergence", "abstract": "We study a fundamental problem in the evaluation of large language models that we call training on the test task. Unlike wrongful practices like training on the test data, leakage, or data contamination, training on the test task is not a malpractice.  Rather, the term describes a growing set of techniques to include task-relevant data in the pretraining stage of a language model. We demonstrate that training on the test task confounds both relative model evaluations and claims about emergent capabilities. We argue that the seeming superiority of one model family over another may be explained by a different degree of training on the test task. To this end, we propose an effective method to adjust for the effect of training on the test task on benchmark evaluations. Put simply, to fine-tune each model under comparison on the same task-relevant data before evaluation. Lastly, we show that instances of emergent behavior disappear gradually as models train on the test task. Our work promotes a new perspective on the evaluation of large language models with broad implications for benchmarking and the study of emergent capabilities.", "title_embedding_index": 12418, "title_abs_embedding_index": 12443}, {"title": "Foundation Models for Enhanced Exploration in Reinforcement Learning", "link_suffix": "/forum?id=RiDtvlNiqp", "link": "https://openreview.net/forum?id=RiDtvlNiqp", "pdf_link": "https://openreview.net/pdf?id=RiDtvlNiqp", "keywords": "reinforcement learning, exploration, large language models, vision-language models, multi-armed bandits", "abstract": "Reinforcement learning agents often struggle with sample inefficiency, requiring extensive interactions with the environment to develop effective policies. \nThis inefficiency is partly due to the challenge of balancing exploration and exploitation without the abstract reasoning and prior knowledge that humans use to quickly identify rewarding actions. \nRecent advancements in foundation models, such as large language models (LLMs) and vision-language models (VLMs), have shown human-level reasoning capabilities in some domains but have been underutilized in directly selecting low-level actions for exploration in reinforcement learning. \nIn this paper, we investigate the potential of foundation models to enhance exploration in reinforcement learning tasks. \nWe conduct an in-depth analysis of their exploration behaviour in multi-armed bandit problems and Gridworld environments, comparing their performance against traditional exploration strategies and reinforcement learning agents. \nOur empirical results suggest foundation models can significantly improve exploration efficiency by leveraging their reasoning abilities to infer optimal actions.\nBuilding on these findings, we introduce Foundation Model Exploration (FME), a novel exploration scheme that integrates foundation models into the reinforcement learning framework for intelligent exploration behaviour. \nWe use VLMs and demonstrate that they can infer environment dynamics and objectives from raw image observations. \nThis means FME only requires the action space as environment-specific manual text input.\nWe find that agents equipped with FME achieve superior performance in sparse reward Gridworld environments and scale to more complex tasks like Atari games. \nMoreover, the effectiveness of FME increases with the capacity of the VLM used, indicating that future advancements in foundation models will further enhance such exploration strategies.", "title_embedding_index": 12419, "title_abs_embedding_index": 12444}, {"title": "Why Does the Effective Context Length of LLMs Fall Short?", "link_suffix": "/forum?id=eoln5WgrPx", "link": "https://openreview.net/forum?id=eoln5WgrPx", "pdf_link": "https://openreview.net/pdf?id=eoln5WgrPx", "keywords": "Large Language Model, Long-Context Modeling, Position Embedding", "abstract": "Advancements in distributed training and efficient attention mechanisms have significantly expanded the context window sizes of large language models (LLMs). However, recent work reveals that the effective context lengths of open-source LLMs often fall short, typically not exceeding half of their training lengths. In this work, we attribute this limitation to the left-skewed frequency distribution of relative positions formed in LLMs pretraining and post-training stages, which impedes their ability to effectively gather distant information.\nTo address this challenge, we introduce Shifted Rotray Position Embedding (STRING).  STRING  shifts well-trained positions to overwrite the original ineffective positions during inference, enhancing performance within their existing training lengths. \nExperimental results show that without additional training, STRING dramatically improves the performance of the latest large-scale models, such as Llama3.1 70B and Qwen2 72B, by over 10 points on popular long-context benchmarks RULER and InfiniteBench, establishing new state-of-the-art results for open-source LLMs. Compared to commercial models, Llama 3.1 70B with STRING  even achieves better performance than GPT-4-128K and clearly surpasses Claude 2 and Kimi-chat.", "title_embedding_index": 12420, "title_abs_embedding_index": 12445}, {"title": "Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models", "link_suffix": "/forum?id=W6yIKliMot", "link": "https://openreview.net/forum?id=W6yIKliMot", "pdf_link": "https://openreview.net/pdf?id=W6yIKliMot", "keywords": "chain-of-thought, reasoning, large language models", "abstract": "Few-shot Chain-of-Thought (CoT) significantly enhances the reasoning capabilities of large language models (LLMs), functioning as a whole to guide these models in generating reasoning steps toward final answers. However, we observe that isolated segments, words, or tokens within CoT demonstrations can unexpectedly disrupt the generation process of LLMs. The model may overly concentrate on certain local information present in the demonstration, introducing irrelevant noise into the reasoning process and potentially leading to incorrect answers. In this paper, we investigate the underlying mechanism of CoT through dynamically tracing and manipulating the inner workings of LLMs at each output step, which demonstrates that tokens exhibiting specific attention characteristics are more likely to induce the model to take things out of context; these tokens directly attend to the hidden states tied with prediction, without substantial integration of non-local information. Building upon these insights, we propose a Few-shot Attention Intervention method (FAI) that dynamically analyzes the attention patterns of demonstrations to accurately identify these tokens and subsequently make targeted adjustments to the attention weights to effectively suppress their distracting effect on LLMs. Comprehensive experiments across multiple benchmarks demonstrate consistent improvements over baseline methods, with a remarkable 5.91% improvement on the AQuA dataset, further highlighting the effectiveness of FAI.", "title_embedding_index": 12421, "title_abs_embedding_index": 12446}, {"title": "UNAST: Unified framework for Neural Architecture Search for Transformers", "link_suffix": "/forum?id=Z3waKPN7DG", "link": "https://openreview.net/forum?id=Z3waKPN7DG", "pdf_link": "https://openreview.net/pdf?id=Z3waKPN7DG", "keywords": "Efficiency, Neural Architecture Search, Large Language models", "abstract": "We introduce the UNAST, a new approach to optimize Large Language Models (LLMs) post-training. UNAST combines Neural Architecture Search (NAS) with sparsity and quantization for LLM compression. Starting with a trained model, UNAST replaces layers (e.g., attention and MLP) with more efficient alternatives by adjusting attention heads, KV projection dimensions, and MLP expansion factors. Local distillation pretrains layer candidates to mimic original layers. Scores and costs (latency, number of parameters, etc.) of each operator are fed into an Integer Linear Optimizer to find the optimal architecture under predefined constraints (latency, number of parameters, etc.). Our experiments show that UNAST scales to large models, reducing training costs by up to 10 times compared to training smaller models from scratch. Validation on GPT-3 and LLaMa models demonstrate that UNAST improves latency and memory footprint by up to 60% with minimal accuracy loss. UNAST also provides insights into the effects of different compression types on Transformer layers, aiding in the development of non-uniform models.", "title_embedding_index": 12422, "title_abs_embedding_index": 12447}, {"title": "Towards Self-Supervised Covariance Estimation in Deep Heteroscedastic Regression", "link_suffix": "/forum?id=Q1kPHLUbhi", "link": "https://openreview.net/forum?id=Q1kPHLUbhi", "pdf_link": "https://openreview.net/pdf?id=Q1kPHLUbhi", "keywords": "deep heteroscedastic regression, probabilistic predictions, 2-Wasserstein, KL-Divergence, Negative Log-Likelihood", "abstract": "Deep heteroscedastic regression models the mean and covariance of the target distribution through neural networks. The challenge arises from heteroscedasticity, which implies that the covariance is sample dependent and is often unknown. Consequently, recent methods learn the covariance through unsupervised frameworks, which unfortunately yield a trade-off between computational complexity and accuracy. While this trade-off could be alleviated through supervision, obtaining labels for the covariance is non-trivial.\nHere, we study self-supervised covariance estimation in deep heteroscedastic regression. We address two questions: (1) How should we supervise the covariance assuming ground truth is available? (2) How can we obtain pseudo labels in the absence of the ground-truth? We address (1) by analysing two popular measures: the KL Divergence and the 2-Wasserstein distance. Subsequently, we derive an upper bound on the 2-Wasserstein distance for non-commutative covariance matrices that is stable to optimize. We address (2) through a simple neighborhood based heuristic algorithm which results in surprisingly effective pseudo labels for the covariance. Our experiments over a wide range of synthetic and real datasets demonstrate that the proposed 2-Wasserstein bound coupled with pseudo label annotations results in a computationally cheaper yet accurate deep heteroscedastic regression.", "title_embedding_index": 12423, "title_abs_embedding_index": 12448}, {"title": "Unifying Disentangled Representation Learning with Compositional Bias", "link_suffix": "/forum?id=1UMxtR9Eb9", "link": "https://openreview.net/forum?id=1UMxtR9Eb9", "pdf_link": "https://openreview.net/pdf?id=1UMxtR9Eb9", "keywords": "Unsupervised Representation Learning, Disentangled Representation Learning, Compositionality", "abstract": "Existing disentangled representation learning methods rely on inductive biases tailored for the specific factors of variation (e.g., attributes or objects).\nHowever, these biases are incompatible with other classes of factors, limiting their applicability for disentangling general factors of variation.\nIn this paper, we propose a unified framework for disentangled representation learning, accommodating both attribute and object disentanglement.\nTo this end, we reformulate disentangled representation learning as maximizing the compositionality of the latents.\nSpecifically, we randomly \\textit{mix} two latent representations from distinct images and maximize the likelihood of the resulting composite image.\nUnder this general framework, we demonstrate that adjusting the strategy for mixing between two latent representations allows us to capture either attributes or objects within a single framework.\nTo derive appropriate mixing strategies, we analyze the compositional structures of both attributes and objects, then incorporate these structures into their respective mixing strategies.\nOur evaluations show that our method achieves performance that matches or exceeds strong baselines in both attribute and object disentanglement.", "title_embedding_index": 12424, "title_abs_embedding_index": 12449}]