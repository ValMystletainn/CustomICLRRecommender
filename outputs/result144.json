[
    {
        "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention",
        "link_suffix": "/forum?id=PzGyZFIn5U",
        "link": "https://openreview.net/forum?id=PzGyZFIn5U",
        "pdf_link": "https://openreview.net/pdf?id=PzGyZFIn5U",
        "keywords": "Demosaicing, HybridEVS, Quad Bayer, State Space, Attention",
        "abstract": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera capture brightness changes as asynchronous \"events\" instead of frames, offering advantages over traditional cameras: high temporal resolution, wide dynamic range, and no motion blur. However, challenges arise from combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels lacking color information, resulting in aliasing and artifacts on the demosaicing process before downstream application. Current methods struggle to address these issues, especially on resource-limited mobile devices. In response, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage network via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can handle event pixels inpainting and Quad Bayer demosaicing separately, leveraging the benefits of dividing complex tasks into manageable subtasks and learning them through a two-step training strategy to enhance robustness. Additionally, we propose a lightweight Cross-Swin State Block (CSSB) designed to augment the model's capacity to capture global dependencies using state space models in a linear format, along with cross-modality Swin attention to integrate additional priors like CFA pattern and event map, outperforming traditional local attention mechanisms while also reducing model size. In summary, TSANet demonstrates excellent demosaicing performance on HybridEVS while maintaining a lightweight model, averaging better results than the previous state-of-the-art method DemosaicFormer across seven diverse datasets in both PSNR and SSIM, while respectively reducing parameter and computation costs by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities for efficient image demosaicing on mobile devices. \\textit{Code and models are available in supplementary materials.}"
    },
    {
        "title": "Self-Evolving Multi-Agent Networks for Software Development",
        "link_suffix": "/forum?id=4R71pdPBZp",
        "link": "https://openreview.net/forum?id=4R71pdPBZp",
        "pdf_link": "https://openreview.net/pdf?id=4R71pdPBZp",
        "keywords": "Software development, LLM, Multi-agent collaboration",
        "abstract": "LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development.\nTo address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network.\nTo extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose RSD-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness.\nOur experiments show that:\ni) The automatic requirement-aware evaluation in RSD-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark.\nii) EvoMAC outperforms previous SOTA methods on both the software-level RSD-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities."
    },
    {
        "title": "MetaAdapter: Leveraging Meta-Learning for Expandable Representation in Few-Shot Class Incremental Learning",
        "link_suffix": "/forum?id=88hh5GtLBJ",
        "link": "https://openreview.net/forum?id=88hh5GtLBJ",
        "pdf_link": "https://openreview.net/pdf?id=88hh5GtLBJ",
        "keywords": "few-shot class incremental learning, meta-learning, feature representation, residual adapter",
        "abstract": "Few-shot class incremental learning (FSCIL) aims to enable  models to learn new tasks from few labeled samples while retaining knowledge of previously ones. This  scenario typically involves an offline base session with sufficient data for pre-training, followed by online incremental sessions where new classes are learned from limited samples. Existing methods either rely on a frozen feature extractor or meta-testing simulation to address overfitting issues in online sessions. However, they primarily learn feature representations using only the base session data, which significantly compromises the model's plasticity in feature representations. To enhance plasticity and reduce overfitting, we propose the MetaAdapter framework, which makes use of meta-learning for expandable representation. During the base session, we expand the network with pre-trained weights by inserting parallel adapters and employ meta-learning to encode generalizable knowledge into these modules. Then, the backbone is further trained on abundant data from the base classes to acquire fundamental classification ability.  In each online session, the adapters are first initialized with parameters from meta-training, and subsequently tuned to adapt to the new classes. Leveraging  meta-learning to produce initial adapters, MetaAdapter enables the feature extractor to effectively adapt to few-shot new classes, thus improving the generalization  of the model.  Experimental results on the mini-ImageNet, CUB200, and CIFAR100 datasets demonstrate that our proposed framework achieves the state-of-the-art performance."
    },
    {
        "title": "Focus On This, Not That! Steering LLMs With Adaptive Feature Specification",
        "link_suffix": "/forum?id=zWASuY0t6o",
        "link": "https://openreview.net/forum?id=zWASuY0t6o",
        "pdf_link": "https://openreview.net/pdf?id=zWASuY0t6o",
        "keywords": "instruction tuning, LLMs, spurious correlations, robustness, distribution shift, bias",
        "abstract": "Despite the success of Instruction Tuning (IT) in training large language models (LLMs) to perform arbitrary user-specified tasks, these models often still leverage spurious or biased features learned from their training data, leading to undesired behaviours when deploying them in new contexts. In this work, we introduceFocus Instruction Tuning(FIT), which trains LLMs to condition their responses by ''focusing on'' specific features whilst ignoring others, leading to different behaviours based on which features are specified. Across several experimental settings, we show that focus-tuned models can be adaptively steered by focusing on different features at inference-time, such as (a) improving robustness by focusing on task-causal features and ignoring spurious features, and (b) mitigating bias by ignoring demographic categories. Furthermore, FIT can steer behaviour in new contexts, generalising under distribution shift and to new unseen features at inference time, thereby facilitating more robust, fair, and explainable LLM applications in real-world environments."
    },
    {
        "title": "Reward-free Policy Optimization with World Models",
        "link_suffix": "/forum?id=OZ3NXrF3gQ",
        "link": "https://openreview.net/forum?id=OZ3NXrF3gQ",
        "pdf_link": "https://openreview.net/pdf?id=OZ3NXrF3gQ",
        "keywords": "Reward-free, Goal-conditioned, World Models, Planning, AI Safety",
        "abstract": "As AI capabilities advance, their rapid progress is not keeping pace with the need for safe and value-aligned algorithms, raising concerns about autonomous systems. E.g., maximizing expected return in reinforcement learning can lead to unintended and potentially harmful consequences. This work introduces Reward-free Policy Optimization (RFPO), a method that prioritizes goal-oriented policy learning over reward maximization by eliminating rewards as the agent's learning signal. Our approach learns a world model that simulates backward in time, and then uses it to construct a directed graph for planning, and finally learning a goal-conditioned policy from the graph. The algorithm has two requirements: (1) the goal has to be defined, and (2) the agent needs sufficient world knowledge, enabling it to plan. This method removes the risks associated with reward hacking and discourages unintended behaviors by allowing for human oversight. Additionally, it provides a framework for humans to build transparent and high-level algorithms by using the (low-level) learned policies. We demonstrate the effectiveness of RFPO on maze environments with pixel observations, where the agent successfully reaches arbitrarily selected goals and follows human-designed algorithms. In conclusion, RFPO enables agents to learn policies without rewards and provides a framework for creating high-level behaviors."
    },
    {
        "title": "Is Attention All You Need for Temporal Link Prediction? A Lightweight Alternative via Learnable Positional Encoding and MLPs",
        "link_suffix": "/forum?id=b4A20ODZBq",
        "link": "https://openreview.net/forum?id=b4A20ODZBq",
        "pdf_link": "https://openreview.net/pdf?id=b4A20ODZBq",
        "keywords": "Link Prediction, Graph Transformer, Positional Encoding",
        "abstract": "Link prediction is of key importance in many real-world applications like social network analysis and recommender systems. To leverage the expressive power for achieving SOTA performance, many recent works adapt the attention mechanism to the structured data for link prediction, in which dense or relational attention is often unaffordable on large-scale structured data. Moreover, in a realistic setting, the time-evolving topological and feature information can raise more challenging questions about the efficiency and effectiveness of attention mechanisms. In spite of the expressive power, we discern that the attention mechanism may not always be as irreplaceable as expected for temporal graph representation learning, at least not for temporal link prediction tasks. Formally, we discover that some deliberately-designed simple positional encoding can enable MLPs to exploit attributed graph information to achieve SOTA performance than complex graph transformers. Hence, we propose a simple temporal link prediction model, named SimpleTLP. In detail, for SimpleTLP, we first propose to adapt Fourier Transform on temporal graphs for learning informative positional encoding, then we (1) prove this learning scheme can make positional encoding preserve the temporal graph topology from the spatial-temporal spectral viewpoint, (2) verify the roles of MLPs and Transformers on that encoding, (3) change different initial positional encoding inputs to show robustness, (4) analyze the theoretical complexity and empirical running time, and (5) demonstrate its temporal link prediction out-performance in a comprehensive way on 13 classic datasets and with 10 algorithms in both transductive and inductive settings using 3 different sampling strategies. Also, SimpleTLP obtains the leading performance in the large-scale TGB benchmark (the newest TGB 2.0)."
    },
    {
        "title": "READ-SQL: Reasoning Path Decomposer for Text-to-SQL",
        "link_suffix": "/forum?id=dHAPEcxyLv",
        "link": "https://openreview.net/forum?id=dHAPEcxyLv",
        "pdf_link": "https://openreview.net/pdf?id=dHAPEcxyLv",
        "keywords": "Text-to-SQL, Tabular Reasoning, SQL Decomposition, Abstract Syntax Trees, Self-Correction",
        "abstract": "Text-to-SQL is a longstanding task aimed at automatically converting natural language questions into SQL queries for database retrieval. Despite impressive advancements, particularly with Large Language Models (LLMs), existing methods still struggle with issues such as misinterpreted, omitted, or unwanted constraints.  To address these challenges, we propose READ-SQL, a novel framework employing a \\underline{re}asoning p\\underline{a}th \\underline{d}compos\\underline{er}, \\textbf{READ}ER, for text-to-SQL tasks.  READER decomposes SQLs into clauses, sub-SQLs, and reasoning paths, supporting data preparation and confidence level determination in post-processing.  READ-SQL comprises two main models: a Generator and a Corrector, both trained via LoRA for parameter efficiency.  Based on READER's decomposition, READ-SQL generates two types of augmented data using an LLM: question/SQL pairs and question/reason pairs.  The Generator is trained on both original and augmented data to identify constraint changes and enhance reasoning.  The Corrector is trained on data from READER\u2019s post-processing, improving self-correction by refining high-confidence SQLs and addressing low-confidence elements.  Extensive experiments show that READ-SQL significantly outperforms leading baselines, with READ-SQL-3B achieving 57.37% execution accuracy on BIRD\u2019s dev set, surpassing several 7B-parameter models and setting a new state-of-the-art with fewer parameters.  Additionally, READER and the Corrector show broad applicability when integrated with LLMs or other base models."
    },
    {
        "title": "Unremovable Watermarks for Open-Source Language Models",
        "link_suffix": "/forum?id=0SpkBUPjL3",
        "link": "https://openreview.net/forum?id=0SpkBUPjL3",
        "pdf_link": "https://openreview.net/pdf?id=0SpkBUPjL3",
        "keywords": "watermark, large language model",
        "abstract": "The recent explosion of high-quality language models has necessitated new methods for identifying AI-generated text. Watermarking is a leading solution and could prove to be an essential tool in the age of generative AI. Existing approaches embed watermarks at inference and crucially rely on the large language model (LLM) specification and parameters being secret, which makes them inapplicable to the open-source setting. In this work, we introduce the first watermarking scheme for open-source LLMs. Our scheme works by modifying the parameters of the model, but the watermark can be detected from just the outputs of the model. Perhaps surprisingly, we prove that our watermarks are $\\textit{unremovable}$ under certain assumptions about the adversary's knowledge. To demonstrate the behavior of our construction under concrete parameter instantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We demonstrate robustness to both token substitution and perturbation of the model parameters. We find that the stronger of these attacks, the model-perturbation attack, requires deteriorating the quality score to 0 out of 100 in order to bring the detection rate down to 50%."
    },
    {
        "title": "Diffusion SigFormer for Interference Time-series Signal Recognition",
        "link_suffix": "/forum?id=LqB8cRuBua",
        "link": "https://openreview.net/forum?id=LqB8cRuBua",
        "pdf_link": "https://openreview.net/pdf?id=LqB8cRuBua",
        "keywords": "Anti-interference electromagnetic signal recognition, diffusion, SigFormer, modulation, bluetooth",
        "abstract": "The various interferences in the actual environment make electromagnetic signal recognition challenging, and this topic has extremely important application value.\nIn this paper, a novel interference signal recognition transformer is proposed, named Diffusion SigFormer.\nFirstly, we explored the interference law of electromagnetic signals and designed a signal interference mechanism. \nSecondly, diffusion signal denoising modulewas proposed to denoise the input interference signal. We also use various types of noise to improve its denoising effect on electromagnetic signals.\nThirdly, SigFormer is designed to extract and classify the denoised signal.\nFor the characteristics of electromagnetic signals, SigFormer leverages 1-D Patch Embedding and combines transformer with convolution. \nFinally, we conducted experimental verification on datasets RML2016.10a, RML2016.10b and BT dataset. \nThe experimental results show that the proposed method has excellent anti-interference ability."
    },
    {
        "title": "ThreadsGAN: Enhancing Coherence and Diversity in Discussion Thread Generation",
        "link_suffix": "/forum?id=f7VXdQTbyW",
        "link": "https://openreview.net/forum?id=f7VXdQTbyW",
        "pdf_link": "https://openreview.net/pdf?id=f7VXdQTbyW",
        "keywords": "discussion threads, generative adversarial network, natural language generating",
        "abstract": "Current research on generating discussion threads faces challenges in coherence, interactivity, and multi-topic handling, which are crucial for meaningful responses. This paper introduces threadsGAN, a model that enhances thread generation by incorporating multi-topic and social response intention tags. By leveraging BERT and Transformer, threadsGAN ensures contextual coherence and manages topic consistency. Additionally, it employs conditional generation to align responses with specific discussion contexts, and its CNN-based discriminator assesses response quality by evaluating similarity between generated and real responses, improving overall performance in generating realistic and contextually appropriate discussion threads."
    },
    {
        "title": "VideoLights: A Cross-Modal Cross-Task Transformer Model for Joint Video Highlight Detection and Moment Retrieval",
        "link_suffix": "/forum?id=F1cN3aoAty",
        "link": "https://openreview.net/forum?id=F1cN3aoAty",
        "pdf_link": "https://openreview.net/pdf?id=F1cN3aoAty",
        "keywords": "highlight detection, moment retrieval, video grounding",
        "abstract": "Video Highlight Detection and Moment Retrieval (HD/MR) are essential in video analysis. Recent joint prediction transformer models often overlook cross-task  dynamics and video-text alignment. We propose VideoLights, a novel HD/MR framework addressing these limitations through: (i) Convolutional Projection and Feature Refinement modules with an intermodal alignment loss for better video-text feature alignment. (ii) Bi-Directional Cross-Modal Fusion network for strongly coupled query-aware clip representations. (iii) Uni-Directional joint-task feedback mechanism enhancing both tasks through correlation. In addition, we introduce  hard positive/negative losses for adaptive error penalization and improved learning. Our approach includes intelligent pretraining and finetuning using synthetic  data and features from various encoders. Comprehensive experiments on QVHighlights, TVSum, and Charades-STA benchmarks demonstrate state-of-the-art performance."
    },
    {
        "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments",
        "link_suffix": "/forum?id=HAwZGLcye3",
        "link": "https://openreview.net/forum?id=HAwZGLcye3",
        "pdf_link": "https://openreview.net/pdf?id=HAwZGLcye3",
        "keywords": "large language models, agents, computational biology, genomics, AI for scientific discovery",
        "abstract": "Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent using Claude 3.5 Sonnet achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy."
    },
    {
        "title": "IntelLLM: Little Hints Make a Big Difference for LLM KV Cache Compression",
        "link_suffix": "/forum?id=4QWPCTLq20",
        "link": "https://openreview.net/forum?id=4QWPCTLq20",
        "pdf_link": "https://openreview.net/pdf?id=4QWPCTLq20",
        "keywords": "LLM, KV cache compression, CGE, RGL",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities in integrating contextual knowledge, but their deployment is often constrained by the substantial computational resources required for long text sequences. To mitigate the inference time cost associated with attention mechanisms, LLMs utilize key-value embedding caching techniques (KV cache), which introduce significant storage pressure. In this paper, we propose IntelLLM, a novel and efficient approach to KV cache compression that strikes a balance between compression rate and performance. Drawing inspiration from sparse attention mechanism, we observe that only a small subset of tokens in lengthy texts capture the majority of attention weights. This sparsity, intrinsic to the attention mechanism, serves as the foundation for improving the KV compression ratio through a strategic eviction method. IntelLLM is composed of center of gravity eviction (CGE) strategy and remote gap localization (RGL) strategy. CGE is designed to address the potential loss of important semantic dependencies when evicting high-sparsity tokens, which prioritizes the retention of key tokens by shielding the center of gravity of attention during inference, thereby preserving critical information and optimizing the efficiency of attention computation. Additionally, RGL is proposed to leverage implicit positional features to maintain long-range dependencies, inspired by advancements in location encoding research. Our KV compression approach integrates seamlessly with existing LLMs, requiring minimal code modifications without the need for fine-tuning or model parameter changes. IntelLLM not only significantly reduces the storage requirements for KV cache but also consistently outperforms full KV models in long text processing tasks, while utilizing only 50% of the typical KV cache expenses."
    },
    {
        "title": "Feature Overlapping: The Computational Redundancy Caused by Repeated Features Across Different Time Steps in SNNs",
        "link_suffix": "/forum?id=77plFC53J5",
        "link": "https://openreview.net/forum?id=77plFC53J5",
        "pdf_link": "https://openreview.net/pdf?id=77plFC53J5",
        "keywords": "Spiking Neural Network; Transformer; Feature Analysis; Image Classification",
        "abstract": "Spiking neural networks (SNNs) have the potential advantage of building large-scale energy-efficient network. However, the high training cost caused by multiple time steps currently limits the application of SNNs. To address this, we break away from the traditional approach of reducing the number of time steps and investigate feature redundancy between time steps. By jointly unfolding the computational process of SNNs across both temporal and spatial dimensions, we are the first to discover the Feature Overlapping Phenomenon, providing new insights for improving SNNs training paradigms. Our Temporal Differential Decoupling (TDD) method successfully separates dynamic and static features, reducing redundant computations. By transforming the feature space into the differential domain, it addresses the issue of the original computational domain's inability to effectively filter sensitive information. In the differential domain, we propose the Gradient Sensitivity Criterion (GSC), which helps further reduce training costs and avoids the loss of important feature information. This paper introduces the Differential Domain Low-Sparsity Approximation (DDLA) algorithm, which significantly reduces computational resource consumption while maintaining computational accuracy by adjusting the filtering ratio. Experimental results show that we achieved up to an 80.9% reduction in the number of spikes per timestep and a total spike count reduction of up to 57.8%, significantly reduce the inference cost of SNNs."
    },
    {
        "title": "Privacy-Aware Lifelong Learning",
        "link_suffix": "/forum?id=UstOpZCESc",
        "link": "https://openreview.net/forum?id=UstOpZCESc",
        "pdf_link": "https://openreview.net/pdf?id=UstOpZCESc",
        "keywords": "lifelong learning, exact machine unlearning, task-incremental continual learning, sparse subnetworks, knowledge transfer",
        "abstract": "Lifelong learning algorithms enable models to incrementally acquire new knowledge without forgetting previously learned information. Contrarily, the field of machine unlearning focuses on explicitly forgetting certain previous knowledge from pretrained models when requested, in order to comply with data privacy regulations on the right-to-be-forgotten. Enabling efficient lifelong learning with the capability to selectively unlearn sensitive information from models presents a critical and largely unaddressed challenge with contradicting objectives. We address this problem from the perspective of simultaneously preventing catastrophic forgetting and allowing forward knowledge transfer during task-incremental learning, while ensuring exact task unlearning and minimizing memory requirements, based on a single neural network model to be adapted. Our proposed solution, privacy-aware lifelong learning (PALL), involves optimization of task-specific sparse subnetworks with parameter sharing within a single architecture. We additionally utilize an episodic memory rehearsal mechanism to facilitate exact unlearning without performance degradations. We empirically demonstrate the scalability of PALL across various architectures in image classification, and provide a state-of-the-art solution that uniquely integrates lifelong learning and privacy-aware unlearning mechanisms for responsible AI applications."
    },
    {
        "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models",
        "link_suffix": "/forum?id=HyPofygOCT",
        "link": "https://openreview.net/forum?id=HyPofygOCT",
        "pdf_link": "https://openreview.net/pdf?id=HyPofygOCT",
        "keywords": "LLM, model compression, Low-rank decomposition, efficient AI",
        "abstract": "In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from \u2776 the distribution variance in the LLM activations and \u2777 the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specif- ically, \u2776 ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. \u2777 Additionally, we propose an efficient iter- ative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner."
    },
    {
        "title": "Bi-perspective Splitting Defense: Achieving Clean-Data-Free Backdoor Security",
        "link_suffix": "/forum?id=y9Lbr6vFHF",
        "link": "https://openreview.net/forum?id=y9Lbr6vFHF",
        "pdf_link": "https://openreview.net/pdf?id=y9Lbr6vFHF",
        "keywords": "Trustworthy AI, Backdoor Defense, Deep Neural Networks",
        "abstract": "Backdoor attacks have seriously threatened deep neural networks (DNNs) by embedding concealed vulnerabilities through data poisoning. To counteract these attacks, training benign models from poisoned data garnered considerable interest from researchers. High-performing defenses often rely on additional clean subsets, which is untenable due to increasing privacy concerns and data scarcity. In the absence of clean subsets, defenders resort to complex feature extraction and analysis, resulting in excessive overhead and compromised performance. In the face of these challenges, we identify the key lies in sufficient utilization of the easier-to-obtain target labels and excavation of clean hard samples. In this work, we propose a Bi-perspective Splitting Defense (BSD). BSD splits the dataset using both semantic and loss statistics characteristics through open set recognition-based splitting (OSS) and altruistic model-based data splitting (ALS) respectively, achieving good clean pool initialization. BSD further introduces class completion and selective dropping strategies in the subsequent pool updates to avoid potential class underfitting and backdoor overfitting caused by loss-guided split. Through extensive experiments on 3 benchmark datasets and against 7 representative attacks, we empirically demonstrate that our BSD is robust across various attack settings. Specifically, BSD has an average improvement in Defense Effectiveness Rating (DER) by 16.29% compared to 5 state-of-the-art defenses, achieving clean-data-free backdoor security with minimal compromise in both Clean Accuracy (CA) and Attack Success Rate (ASR)."
    },
    {
        "title": "BeHonest: Benchmarking Honesty in Large Language Models",
        "link_suffix": "/forum?id=ijFdq8uqki",
        "link": "https://openreview.net/forum?id=ijFdq8uqki",
        "pdf_link": "https://openreview.net/pdf?id=ijFdq8uqki",
        "keywords": "large language models, honesty, benchmark",
        "abstract": "Previous works on Large Language Models (LLMs) have mainly focused on evaluating their helpfulness or harmlessness. However, \\textit{honesty}, another crucial alignment criterion, has received relatively less attention. Dishonest behaviors in LLMs, such as spreading misinformation and defrauding users, present severe risks that intensify as these models approach superintelligent levels. Enhancing honesty in LLMs addresses critical limitations and helps uncover latent capabilities that are not readily expressed. This underscores the urgent need for reliable methods and benchmarks to effectively ensure and evaluate the honesty of LLMs.In this paper, we introduce BeHonest, a pioneering benchmark specifically designed to assess honesty in LLMs comprehensively.\nBeHonest evaluates three essential aspects of honesty: \\emph{awareness of knowledge boundaries}, \\emph{avoidance of deceit}, and \\emph{consistency in responses}. Building on this foundation, we designed 10 scenarios to evaluate and analyze 9 popular LLMs on the market, including both closed-source and open-source models from different model families with varied model sizes. Our findings indicate that there is still significant room for improvement in the honesty of LLMs. We encourage the AI community to prioritize honesty alignment in these models, which can harness their full potential to benefit society while preventing them from causing harm through deception or inconsistency. Our benchmark and code can be found at:https://anonymous.4open.science/r/behonest-4093/."
    },
    {
        "title": "Global Optimality of In-context Markovian Dynamics Learning",
        "link_suffix": "/forum?id=HuBFimORiz",
        "link": "https://openreview.net/forum?id=HuBFimORiz",
        "pdf_link": "https://openreview.net/pdf?id=HuBFimORiz",
        "keywords": "transformers, in-context learning, Markov Chains, next token prediction",
        "abstract": "Transformers have demonstrated impressive capability of in-context learning (ICL): given a sequence of input-output pairs of an unseen task, a trained transformer can make reasonable predictions on query inputs, without fine-tuning its parameters.However, existing studies on ICL have mainly focused on linear regression tasks, often with i.i.d. inputs within a prompt.\nThis paper seeks to unveil the mechanism of ICL for next-token prediction for Markov chains, focusing on the transformer architecture with linear self-attention (LSA). \nMore specifically, we derive and interpret the global optimum of the ICL loss landscape:\n(1) We provide the closed-form expression of the global minimizer for single-layer LSA trained over random instances of length-2 in-context Markov chains, showing the Markovian data distribution necessitates a denser global minimum structure compared to ICL for linear tasks.\n(2) We establish tight bounds for the global minimum of single-layer LSA trained on arbitrary-length Markov chains.\n(3) Finally, we prove that multilayer LSA, with parameterization mirroring the global minimizer's structure, performs preconditioned gradient descent for a multi-objective optimization problem over the in-context samples, balancing a squared loss with multiple linear objectives.\nWe numerically explore ICL for Markov chains using both   simplified transformers and GPT-2-based multilayer nonlinear transformers."
    },
    {
        "title": "Adaptive Strategy Evolution for Generating Tailored Jailbreak Prompts against Black-Box Safety-Aligned LLMs",
        "link_suffix": "/forum?id=xF5st2HtYP",
        "link": "https://openreview.net/forum?id=xF5st2HtYP",
        "pdf_link": "https://openreview.net/pdf?id=xF5st2HtYP",
        "keywords": "Strategy evolution, Black-box jailbreak, Safety-aligned LLM",
        "abstract": "While safety-aligned Large Language Models (LLMs) have been secured by extensive alignment with human feedback, they remain vulnerable to jailbreak attacks that exploit prompt manipulation to generate harmful outputs. Investigating these jailbreak methods, particularly in black-box scenarios, allows us to explore the inherent limitations of such LLMs and provides insights into possible improvements. However, existing black-box jailbreak methods either overly rely on red-teaming LLMs to execute sophisticated reasoning tasks, such as diagnosing failure cases, determining improvement directions, and rewriting prompts, which pushes them beyond their inherent capabilities and introduces uncertainty and inefficiency into the refinement process, or they are confined to rigid, manually predefined strategy spaces, limiting their performance ceiling. To enable a sustained and deterministic exploration with clear directional guidance, we propose the novel Adaptive Strategy Evolution (ASE) framework. Specifically, ASE innovatively decomposes jailbreak strategies into modular key components, dramatically enhancing both the flexibility and expansiveness of the strategy space. This also allows us to shift focus from directly optimizing prompts to optimizing the jailbreak strategies. Then, by leveraging a genetic algorithm (GA) for strategy components' selection and mutation, ASE could replace the uncertainties of LLM-based self-adjustment with a more systematic and deterministic optimization process. Additionally, we have also designed a new fitness evaluation, that emphasizes the independence of scoring criteria, provides highly accurate and reliable feedback, enabling precise and targeted refinement of jailbreak strategies. Experimental results further demonstrate that ASE achieves superior jailbreak success rates (JSR) compared to existing state-of-the-art methods, especially against the most advanced safety-aligned LLMs like  GPT-4o, Claude-3.5, and even o1."
    },
    {
        "title": "FedSMU: Communication-Efficient and Generalization-Enhanced Federated Learning through Symbolic Model Updates",
        "link_suffix": "/forum?id=ZU42Wrcqfm",
        "link": "https://openreview.net/forum?id=ZU42Wrcqfm",
        "pdf_link": "https://openreview.net/pdf?id=ZU42Wrcqfm",
        "keywords": "Federated learning, Efficient Communication, Enhanced Generalization",
        "abstract": "The significant communication overhead and client data heterogeneity have posed important challenges to current federated learning (FL) paradigm. Most compression-based and optimization-based FL algorithms typically focus on addressing either the model compression challenge or the data heterogeneity issue individually, rather than tackling both of them. In this paper, we observe that by symbolizing the client model updates to be uploaded (i.e., normalizing the magnitude for each model parameter at local clients), the model heterogeneity can be mitigated that is essentially stemmed from data heterogeneity, thereby helping improve the overall generalization performance of the globally aggregated model at the server. Inspired with this observation, and further motivated by the success of Lion optimizer in achieving the optimal performance on most tasks in centralized learning, we propose a new FL algorithm, called FedSMU, which simultaneously reduces the communication overhead and alleviates the data heterogeneity issue. Specifically, FedSMU splits the standard Lion optimizer into the local updates and global execution, where only the symbol of client model updates commutes between the client and server. We theoretically prove the convergence of FedSMU for the general non-convex settings. Through extensive experimental evaluations on several benchmark datasets, we demonstrate that our FedSMU algorithm not only reduces the communication overhead, but also achieves a better generalization performance than the other compression-based and optimization-based baselines."
    },
    {
        "title": "Multistep Consistency Models",
        "link_suffix": "/forum?id=d7DZRNe2xG",
        "link": "https://openreview.net/forum?id=d7DZRNe2xG",
        "pdf_link": "https://openreview.net/pdf?id=d7DZRNe2xG",
        "keywords": "multistep consistency, consistency models, distillation, fast sampling",
        "abstract": "Diffusion models are relatively easy to train but require many steps to generate samples. Consistency models are far more difficult to train, but generate samples in a single step.In this paper we propose Multistep Consistency Models: A unification between Consistency Models  (Song et al., 2023) and TRACT (Berthelotet al., 2023) that can interpolate between a consistency model and a diffusion model: a trade-off between sampling speed and sampling quality. Specifically, a 1-step consistency model is a conventional consistency model whereas a $\\infty$-step consistency model is a diffusion model.Multistep Consistency Models work really well in practice. By increasing the sample budget from a single step to 2-8 steps, we can train models more easily that generate higher quality samples, while retaining much of the sampling speed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 sampling steps and 2.1 FID on Imagenet128 in 8 sampling steps with consistency distillation, using simple losses without adversarial training. We also show that our method scales to a text-to-image diffusion model, generating samples that are close to the quality of the original model."
    },
    {
        "title": "Attention with Markov: A Curious Case of Single-layer Transformers",
        "link_suffix": "/forum?id=SqZ0KY4qBD",
        "link": "https://openreview.net/forum?id=SqZ0KY4qBD",
        "pdf_link": "https://openreview.net/pdf?id=SqZ0KY4qBD",
        "keywords": "Markov chains, Transformers, Optimization, Landscape",
        "abstract": "Attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. To deepen our understanding of their sequential modeling capabilities, there is a growing interest in using Markov input processes to study them. A key finding is that when trained on first-order Markov chains, transformers with two or more layers consistently develop an induction head mechanism to estimate the in-context bigram conditional distribution. In contrast, single-layer transformers, unable to form an induction head, directly learn the Markov kernel but often face a surprising challenge: they become trapped in local minima representing the unigram distribution, whereas deeper models reliably converge to the ground-truth bigram. While single-layer transformers can theoretically model first-order Markov chains, their empirical failure to learn this simple kernel in practice remains a curious phenomenon. To explain this contrasting behavior of single-layer models, in this paper we introduce a new framework for a principled analysis of transformers via Markov chains. Leveraging our framework,  we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima (bigram) and bad local minima (unigram) contingent on data properties and model architecture. We precisely delineate the regimes under which these local optima occur. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. Finally, we outline several open problems in this arena. Code is available at \\url{https://anonymous.4open.science/r/Attention-with-Markov-A617/}."
    },
    {
        "title": "PhyloLM: Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks",
        "link_suffix": "/forum?id=rTQNGQxm4K",
        "link": "https://openreview.net/forum?id=rTQNGQxm4K",
        "pdf_link": "https://openreview.net/pdf?id=rTQNGQxm4K",
        "keywords": "large language models, phylogeny, benchmark",
        "abstract": "This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metric based on the similarity of LLMs' output. The resulting metric is then used to construct dendrograms, which satisfactorily capture  known relationships across a set of 111 open-source and 45 closed models. Furthermore, our phylogenetic distance predicts performance in standard benchmarks, thus demonstrating its functional validity and paving the way for  a time and cost-effective estimation of LLM capabilities. To sum up, by translating population genetic concepts to machine learning, we propose and validate a tool to evaluate LLM development,  relationships and capabilities, even in the absence of transparent training information."
    },
    {
        "title": "Functional-level Uncertainty Quantification for Calibrated Fine-tuning on LLMs",
        "link_suffix": "/forum?id=VT4Ovqg0BW",
        "link": "https://openreview.net/forum?id=VT4Ovqg0BW",
        "pdf_link": "https://openreview.net/pdf?id=VT4Ovqg0BW",
        "keywords": "Uncertainty Quantification, Large Language Models, Mixture of Experts, Parameter Efficient Fine Tuning",
        "abstract": "From common-sense reasoning to domain-specific tasks, parameter-efficient fine tuning (PEFT) methods for large language models (LLMs) have showcased significant performance improvements on downstream tasks.  However, fine-tuned LLMs often struggle with overconfidence in uncertain predictions, particularly due to sparse training data. This overconfidence reflects poor epistemic uncertainty calibration, which arises from limitations in the model's ability to generalize with limited data. Existing PEFT uncertainty quantification methods for LLMs focus on the post fine-tuning stage and thus have limited capability in calibrating epistemic uncertainty. To address these limitations, we propose Functional-Level Uncertainty Quantification for Calibrated Fine-Tuning (UQ4CT), which captures and calibrates functional-level epistemic uncertainty during the fine-tuning stage via a mixture-of-expert framework. We show that UQ4CT reduces Expected Calibration Error (ECE) by more than 25% while maintaining high accuracy across 5 benchmarks. Furthermore, UQ4CT maintains superior ECE performance with high accuracy under distribution shift, showcasing improved generalizability."
    }
]