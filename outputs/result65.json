[
    {
        "title": "Masking the Gaps: An Imputation-Free Approach to Time Series Modeling with Missing Data",
        "link_suffix": "/forum?id=XBtDrlK1Qc",
        "link": "https://openreview.net/forum?id=XBtDrlK1Qc",
        "pdf_link": "https://openreview.net/pdf?id=XBtDrlK1Qc",
        "keywords": "Time-Series, Deep Learning, Masked Autoencoders, Missing Data, Data Imputation",
        "abstract": "Modeling time series is important in a variety of domains, yet it is  challenged by the presence of missing values in real-world time-series datasets. Traditional frameworks for modeling time-series with missing values typically involve a two-step process, where the missing values are first filled-in using some imputation technique, followed by a time-series modeling approach on the imputed time-series. However, existing two-stage approaches suffer from two major drawbacks: first, the propagation of imputation errors into subsequent time-series modeling performance, and second, the inherent trade-offs between imputation efficacy and imputation complexity. To this end, we propose a novel imputation-free approach for handling missing values in time series termed {Miss}ing Feature-aware {T}ime {S}eries {M}odeling ({MissTSM}) with two main innovations. {First}, we develop a novel embedding scheme that treats every combination of time-step and feature (or channel) as a distinct token, encoding them into a high-dimensional space. {Second}, we introduce a novel {Missing Feature-Aware Attention (MFAA) Layer} to learn latent representations at every time-step based on partially observed features. We evaluate the effectiveness of MissTSM  in handling missing values over multiple benchmark datasets using two synthetic masking techniques: masking completely at random (MCAR) and periodic masking, and a real-world missing-value dataset."
    },
    {
        "title": "Quantifying Likeness: A Simple Machine Learning Approach to Identifying Copyright Infringement in (AI-Generated) Artwork",
        "link_suffix": "/forum?id=9zKm3TytBG",
        "link": "https://openreview.net/forum?id=9zKm3TytBG",
        "pdf_link": "https://openreview.net/pdf?id=9zKm3TytBG",
        "keywords": "generative ai, art, law, classification, copyright, resnet",
        "abstract": "This study proposes an approach aligned with the legal process to quantify copyright infringement, via stylistic similarity, in AI-generated artwork. In contrast to typical work in this field, and more in line with a realistic legal setting, our approach quantifies the similarity of a set of potentially-infringing “defendant” artworks to a set of copyrighted “plaintiff\"  artworks.  We frame this as an image classification task, using a fine-tuned ResNet trained on small, customized datasets relevant to each use case. Softmax-normalized probabilities from the model serve as similarity scores for potentially infringing “defendant” artworks, and saliency maps and features visualizations complement the score by highlighting key features and allowing for interpretability. This straightforward image classification approach can be accomplished in a quite simple, low-resource setting, making it accessible for real-world applications.We present a case study using Mickey Mouse as the plaintiff, performing thorough hyperparameter tuning and robustness analysis. Our experiments include optimizing batch size, weight decay, and learning rate, as well as exploring the impact of additional distractor classes. We employ data augmentation, cross-validation, and a linear decay learning rate scheduler to improve model performance, along with conducting scaling experiments with different types of distractor classes. The aims of this work are to illustrate the potential of the approach, and identify settings which generalize well, such that it is as \"plug and play\" as possible for users to apply with their own plaintiff sets of artworks."
    },
    {
        "title": "Continual Slow-and-Fast Adaptation of Latent Neural Dynamics (CoSFan): Meta-Learning What-How & When to Adapt",
        "link_suffix": "/forum?id=Dl3MsjaIdp",
        "link": "https://openreview.net/forum?id=Dl3MsjaIdp",
        "pdf_link": "https://openreview.net/pdf?id=Dl3MsjaIdp",
        "keywords": "continual meta-learning, latent dynamics forecasting, time-series",
        "abstract": "An increasing interest in learning to forecast for time-series of high-dimensional observations is the ability to adapt to systems with diverse underlying dynamics. Access to observations that define a stationary distribution of these systems is often unattainable, as the underlying dynamics may change over time. Naively training or retraining models at each shift may lead to catastrophic forgetting about previously-seen systems. We present a new continual meta-learning (CML) framework to realize continual slow-and fast adaptation of latent dynamics (CoSFan). We leverage a feed-forward meta-model to inferwhatthe current system is andhowto adapt a latent dynamics function to it, enablingfast adaptationto specific dynamics. We then develop novel strategies to automatically detectwhena shift of data distribution occurs, with which to identify its underlying dynamics and its relation with previously-seen dynamics. In combination with fixed-memory experience replay mechanisms, this enables continualslow updateof thewhat-howmeta-model. Empirical studies demonstrated that both the meta- and continual-learning component was critical for learning to forecast across non-stationary distributions of diverse dynamics systems, and the feed-forward meta-model combined with task-aware/-relational continual learning strategies significantly outperformed existing CML alternatives."
    },
    {
        "title": "Hybrid Fine-Tuning of LLMs: Theoretical Insights on Generalized Smoothness and Convergence",
        "link_suffix": "/forum?id=8aKygnbEFX",
        "link": "https://openreview.net/forum?id=8aKygnbEFX",
        "pdf_link": "https://openreview.net/pdf?id=8aKygnbEFX",
        "keywords": "Parameter-Efficient Fine-Tuning, Large Language Model, Zeroth-Order Optimization, Generalized Smoothness",
        "abstract": "Applying either Parameter-Efficient Fine-Tuning (PEFT) or full fine-tuning to Large Language Models (LLMs) often results in its inherent limitations.  To overcome this issue, we propose a novel \"hybrid fine-tuning\" approach that jointly updates both  LLMs and PEFT modules  using a combination of zeroth-order and first-order optimization methods. To analyze this approach, we develop a theoretical framework centered on the concept of \"hybrid generalized smoothness\", which accounts for the heterogeneous nature of the optimization landscape in joint LLM and PEFT training. We provide a rigorous convergence analysis for the convergence of SGD algorithm under multiple learning rates and demonstrate its effectiveness through extensive empirical studies across various downstream tasks and model architectures. Our work not only offers a solution to the practical challenge of LLM fine-tuning but also contributes a broader theoretical foundation for analyzing hybrid optimization problems in machine learning."
    },
    {
        "title": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference",
        "link_suffix": "/forum?id=9VMW4iXfKt",
        "link": "https://openreview.net/forum?id=9VMW4iXfKt",
        "pdf_link": "https://openreview.net/pdf?id=9VMW4iXfKt",
        "keywords": "Large Language Model; Efficient Inference; Activation Sparsity",
        "abstract": "Large Language Models (LLMs), while demonstrating remarkable capabilities across various applications, present significant challenges during inference due to their substantial model size, especially when deployed on edge devices. Activation sparsity offers a promising solution to reduce computation and memory movement, enabling more efficient inference, particularly for small-batch on-device applications. However, current approaches face limitations with non-ReLU activation function, which are foundational to most advanced LLMs, or require heavy continual training. Additionally, the difficulty in predicting active channels and limited achievable sparsity ratios constrain the effectiveness of activation sparsity-based methods. In this paper, we introduce R-Sparse, a training-free activation sparsity approach capable of achieving high sparsity levels in advanced LLMs. We conducted two preliminary investigations into how different components contribute to the output within a single linear layer and found two key observations: (i) the non-sparse components of the input function can be regarded as a few bias terms, and (ii) The full computation can be effectively approximated by an appropriate combination of input channels and weight singular values. Building on this, we replace the linear layers in LLMs with a rank-aware sparse inference method that leverages the sparsity of input channels and singular value components, eliminating the need for active channel prediction like the output sparsity based approaches. Experiments on Llama-2/3 and Mistral models across ten diverse tasks demonstrate that R-Sparse achieves comparable performance at 50% model-level sparsity, resulting in a significant 43% end-to-end efficient improvements with customized kernels. Code will be made publicly available upon acceptance."
    },
    {
        "title": "Anchored Alignment for Self-Explanations Enhancement",
        "link_suffix": "/forum?id=mkE9Yx4wHY",
        "link": "https://openreview.net/forum?id=mkE9Yx4wHY",
        "pdf_link": "https://openreview.net/pdf?id=mkE9Yx4wHY",
        "keywords": "LLM, Self-Explanation, Alignment, Preference Pairs, DPO, SFT, RLAIF, Self-Alignment, Self-instruction",
        "abstract": "In this work, we introduce a methodology for alignment designed to enhance the ability of large language models (LLMs) to articulate their reasoning—\\textit{self-explanation}—even in the absence of annotated rationale explanations. Our alignment methodology comprises three key components: explanation quality assessment, self-instruction dataset generation, and model alignment. Additionally, we present a novel technique called \\textit{Alignment with Anchor Preference Pairs}, which improves the selection of preference pairs by categorizing model outputs into three groups: consistently correct, consistently incorrect, and variable. By applying tailored strategies to each category, we enhance the effectiveness of Direct Preference Optimization (DPO). Our experimental results demonstrate that this approach significantly improves explanation quality while maintaining accuracy compared to other fine-tuning strategies."
    },
    {
        "title": "Low-Rank Compression of Language Models Via Differentiable Rank Selection",
        "link_suffix": "/forum?id=960Ny6IjEr",
        "link": "https://openreview.net/forum?id=960Ny6IjEr",
        "pdf_link": "https://openreview.net/pdf?id=960Ny6IjEr",
        "keywords": "NLP, LLM, LLM Compression",
        "abstract": "Approaches for large-language model compression using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware Singular Value Decomposition (SVD) that improve the trade-off between decomposition rank and performance. Despite these advancements, a persistent challenge remains: selecting the optimal ranks for each layer to maximize compression and performance. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but expensive as they rely on RNNs to predict the rank. To address these challenges, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values that optimize for model compression and model performance. Using a calibration dataset of 3,000 documents, this training architecture teaches the model to select fewer and fewer singular values while minimizing the divergence of intermediate activations from the original model. Results show our approach outperforms competing rank selection approaches, such as Sensitivity-based Truncation Rank Searching (STRS) and Adaptive Rank Selection (ARS) on Llama-2-7b and Llama-3-8B, across various compression rates on commonsense reasoning and open-domain question-answering tasks. Particularly, our approach performs strongly on open-domain question answering at higher compression rates compared to other rank selection techniques."
    },
    {
        "title": "Revisiting Zeroth-Order Optimization:  Minimum-Variance Two-Point Estimators and  Directionally Aligned Perturbations",
        "link_suffix": "/forum?id=ywFOSIT9ik",
        "link": "https://openreview.net/forum?id=ywFOSIT9ik",
        "pdf_link": "https://openreview.net/pdf?id=ywFOSIT9ik",
        "keywords": "zeroth-order optimization, SGD, convergence analysis",
        "abstract": "In this paper, we explore the two-point zeroth-order gradient estimator and identify the optimal distribution of random perturbations that minimizes the estimator's variance. We formulate it as a constrained functional optimization problem over the space of perturbation distributions. Our findings reveal that optimal perturbations either maintain a fixed length or align directionally with the true gradient. While existing research has largely focused on fixed-length perturbations, the potential advantages of directional alignment have been overlooked. To address this gap, we delve into the theoretical and empirical properties of the directionally aligned perturbation (DAP) scheme, which adaptively offers higher accuracy along critical directions. Additionally, we provide a convergence analysis for stochastic gradient descent using $\\delta$-unbiased random perturbations, extending optimal complexity bounds to a wider range of perturbations. Through empirical evaluations on both synthetic problems and practical tasks, we demonstrate that DAPs outperform traditional methods under specific conditions."
    },
    {
        "title": "Training on more Reachable Tasks for Generalisation in Reinforcement Learning",
        "link_suffix": "/forum?id=X6W5eqhzDx",
        "link": "https://openreview.net/forum?id=X6W5eqhzDx",
        "pdf_link": "https://openreview.net/pdf?id=X6W5eqhzDx",
        "keywords": "Deep Reinforcement Learning, Exploration, Generalization",
        "abstract": "In multi-task reinforcement learning, agents train on a fixed set of tasks and have to generalise to new ones. Recent work has shown that increased exploration improves this generalisation, but it remains unclear why exactly that is. In this paper, we introduce the concept of reachability in multi-task reinforcement learning and show that an initial exploration phase increases the number of reachable tasks the agent is trained on. This, and not the increased exploration, is responsible for the improved generalisation, even to unreachable tasks. Inspired by this, we propose a novel method Explore-Go that implements such an exploration phase at the beginning of each episode. Explore-Go only modifies the way experience is collected and can be used with most existing on-policy or off-policy reinforcement learning algorithms. We demonstrate the effectiveness of our method when combined with some popular algorithms and show an increase in generalisation performance across several environments."
    },
    {
        "title": "Query-based Knowledge Transfer for Heterogeneous Learning Environments",
        "link_suffix": "/forum?id=XKv29sMyjF",
        "link": "https://openreview.net/forum?id=XKv29sMyjF",
        "pdf_link": "https://openreview.net/pdf?id=XKv29sMyjF",
        "keywords": "Collaborative Learning, Knowledge Distillation, Query-based Knowledge Transfer.",
        "abstract": "Decentralized collaborative learning under data heterogeneity and privacy constraints has rapidly advanced. However, existing solutions like federated learning,  ensembles, and transfer learning,  often fail to adequately serve the unique needs of clients, especially when local data representation is limited.To address this issue, we propose a novel framework called Query-based Knowledge Transfer (QKT) that enables tailored knowledge acquisition to fulfill specific client needs without direct data exchange. \nIt employs a data-free masking strategy to facilitate the communication-efficient query-focused knowledge transformation while refining task-specific parameters to mitigate knowledge interference and forgetting. Our experiments, conducted on both standard and clinical benchmarks, show that QKT significantly outperforms existing collaborative learning methods by an average of 20.91% points in single-class query settings and an average of 14.32% points in multi-class query scenarios.\nFurther analysis and ablation studies reveal that QKT effectively balances the learning of new and existing knowledge, showing strong potential for its application in decentralized learning."
    },
    {
        "title": "GEAR-FEN: Generalized Feature Representation for Kinematic Human Activity Recognition",
        "link_suffix": "/forum?id=QIfzMeTyOu",
        "link": "https://openreview.net/forum?id=QIfzMeTyOu",
        "pdf_link": "https://openreview.net/pdf?id=QIfzMeTyOu",
        "keywords": "Human Activity Recognition, Transfer Learning, Machine Learning, Classification",
        "abstract": "This study addresses the challenge of efficient human activity recognition (HAR) with limited training data. We propose GEAR-FEN (Generalized Activity Recognition Feature Extraction Network), a novel transfer learning method that transforms kinematic motion signals into a generalized feature space. GEAR-FEN potentially outperforms the state-of-the-art in scenarios with limited training data. This was demonstrated through an evaluation across 11 public HAR datasets (encompassing number of activities ranging from 6 to 33 and number of samples per activity ranging from 8628 to 1140258), using a deep learning model based on convolutional neural networks (CNN), residual bi-directional long short-term memory (ResBiLSTM), and an attention mechanism. Furthermore, we establish the generalizability of our method through performance comparisons on an independent dataset encompassing a distinct population and diverse kinematic modalities for 8 activities, and 26121 samples per activity. These findings highlight the potential of our proposed approach in robust feature representation for HAR tasks with limited data sizes."
    },
    {
        "title": "Memory-Efficient Fine-Tuning via Structured Neural Network Pruning",
        "link_suffix": "/forum?id=JMgxtZqkvO",
        "link": "https://openreview.net/forum?id=JMgxtZqkvO",
        "pdf_link": "https://openreview.net/pdf?id=JMgxtZqkvO",
        "keywords": "Transformer, Fine-tuning, Memory efficient learning",
        "abstract": "Fine-tuning is an important step in adapting foundation models such as large language models to downstream tasks. To make this step more accessible to users with limited computational budgets, it is crucial to develop fine-tuning methods that are memory and computationally efficient. Sparse Fine-tuning and Low-rank adaptation (LoRA) are two frameworks that have emerged for addressing this problem, and have been adopted widely in practice. In this work, we develop a new method, based on ideas from neural network pruning. At a high level, we first identify \"important\" neurons/nodes using feature importance metrics from network pruning (specifically, we use the structural pruning method), and then perform fine-tuning  by restricting to weights involving these neurons. Using experiments on both vision and language tasks, we demonstrate that our method uses up to 50% fewer trainable parameters than the state-of-the-art methods like LoRA, while achieving a similar accuracy."
    },
    {
        "title": "Variational Inequality Methods for Multi-Agent Reinforcement Learning: Performance and Stability Gains",
        "link_suffix": "/forum?id=GtxzVbjPKC",
        "link": "https://openreview.net/forum?id=GtxzVbjPKC",
        "pdf_link": "https://openreview.net/pdf?id=GtxzVbjPKC",
        "keywords": "multi-agent reinforcement learning, game optimization, Variational Inequality",
        "abstract": "Multi-agent reinforcement learning (MARL) presents unique challenges as agents\nlearn strategies through experiences. Gradient-based methods are often sensitive to hyperparameter selection and initial random seed variations. Concurrently, significant advances have been made in solving Variational Inequalities\n(VIs)—which include equilibrium-finding problems— particularly in addressing\nthe non-converging rotational dynamics that impede convergence of traditional\ngradient-based optimization methods. This paper explores the potential of leveraging VI-based techniques to improve MARL training. Specifically, we study the\nperformance of VI methods—namely, Nested-Lookahead VI (nLA-VI) and Extragradient (EG)—in enhancing the multi-agent deep deterministic policy gradient\n(MADDPG) algorithm. We present a VI reformulation of the actor-critic algorithm\nfor both single- and multi-agent settings. We introduce three algorithms that use\nnLA-VI, EG, and a combination of both, named LA-MADDPG, EG-MADDPG,\nand LA-EG-MADDPG, respectively. Our empirical results demonstrate that these\nVI-based approaches yield significant performance improvements in benchmark\nenvironments, such as the zero-sum games: rock-paper-scissors and matching\npennies, where equilibrium strategies can be quantitatively assessed, and the Multi-\nAgent Particle Environment: Predator-prey benchmark, where VI-based methods\nalso yield balanced participation of agents from the same team."
    },
    {
        "title": "Adaptive Batch Size for Privately Finding Second-Order Stationary Points",
        "link_suffix": "/forum?id=ikkvC1UnnE",
        "link": "https://openreview.net/forum?id=ikkvC1UnnE",
        "pdf_link": "https://openreview.net/pdf?id=ikkvC1UnnE",
        "keywords": "Differential privacy, non-convex optimization, adaptive batch size",
        "abstract": "There is a gap between finding a first-order stationary point (FOSP) and a second-order stationary point (SOSP) under differential privacy constraints, and it remains unclear whether privately finding an SOSP is more challenging than finding an FOSP. Specifically, Ganesh et al. (2023) demonstrated that an $\\alpha$-SOSP can be found with $\\alpha=\\Tilde{O}(\\frac{1}{n^{1/3}}+(\\frac{\\sqrt{d}}{n\\epsilon})^{3/7})$, where $n$ is the dataset size, $d$ is the dimension, and $\\epsilon$ is the differential privacy parameter. Building on the SpiderBoost algorithm framework, we propose a new approach that uses adaptive batch sizes and incorporates the binary tree mechanism. Our method improves the results for privately finding an SOSP, achieving $\\alpha=\\Tilde{O}(\\frac{1}{n^{1/3}}+(\\frac{\\sqrt{d}}{n\\epsilon})^{1/2})$. This improved bound matches the state-of-the-art for finding an FOSP, suggesting that privately finding an SOSP may be achievable at no additional cost."
    },
    {
        "title": "Language Models Are Implicitly Continuous",
        "link_suffix": "/forum?id=SMK0f8JoKF",
        "link": "https://openreview.net/forum?id=SMK0f8JoKF",
        "pdf_link": "https://openreview.net/pdf?id=SMK0f8JoKF",
        "keywords": "llm, continuity, spatiotemporal transformers, linguistics",
        "abstract": "Language is typically modelled with discrete sequences. However, the most successful approaches to language modelling, namely neural networks, are continuous and smooth function approximators.\nIn this work, we show that Transformer-based language models implicitly learn to represent sentences as continuous-time functions defined over a continuous input space. \nThis phenomenon occurs in most state-of-the-art Large Language Models (LLMs), including Llama2, Llama3, Phi3, Gemma, Gemma2, and Mistral, and suggests that LLMs reason about language in ways that fundamentally differ from humans.\nOur work formally extends Transformers to capture the nuances of time and space continuity in both input and output space.\nOur results challenge the traditional interpretation of how LLMs understand language, with several linguistic and engineering implications."
    },
    {
        "title": "Make LLMs better zero-shot reasoners: structure-oriented autonomous reasoning",
        "link_suffix": "/forum?id=rLaMcF516k",
        "link": "https://openreview.net/forum?id=rLaMcF516k",
        "pdf_link": "https://openreview.net/pdf?id=rLaMcF516k",
        "keywords": "language model, reasoning, agents",
        "abstract": "Zero-shot reasoning methods with Large Language Models (LLMs) offer significant advantages including great generalization to novel tasks and reduced dependency on human-crafted examples. \nHowever, the current zero-shot methods still have limitations in complex tasks, e.g., answering questions that require multi-step reasoning.\nIn this paper, we address this limitation by introducing a novel structure-oriented analysis method to help LLMs better understand the question and guide the problem-solving process of LLMs.\nWe first demonstrate how the existing reasoning strategies, Chain-of-Thought and ReAct, can benefit from our structure-oriented analysis. \nIn addition to empirical investigations, we leverage the probabilistic graphical model to theoretically explain why our structure-oriented analysis can improve the LLM reasoning process. \nTo further improve the reliability in complex question-answering tasks, we propose a multi-agent reasoning system,Structure-orientedAutonomousReasoningAgents (SARA), that can better enforce the reasoning process following our structure-oriented analysis by refinement techniques and is equipped with external knowledge retrieval capability to reduce factual errors.\nExtensive experiments verify the effectiveness of the proposed reasoning system. Surprisingly, in some cases, the system even surpasses few-shot methods.\nFinally, the system not only improves reasoning accuracy in complex tasks but also demonstrates robustness against potential attacks that corrupt the reasoning process."
    },
    {
        "title": "Zero-Shot Goal Dialogue via Reinforcement Learning on Imagined Conversations",
        "link_suffix": "/forum?id=6e3hoDZKuO",
        "link": "https://openreview.net/forum?id=6e3hoDZKuO",
        "pdf_link": "https://openreview.net/pdf?id=6e3hoDZKuO",
        "keywords": "dialogue agents, language models, offline reinforcement learning",
        "abstract": "Large language models (LLMs) have emerged as powerful and general solutions to many natural language tasks. However, many of the most important applications of language generation are interactive, where an agent has to talk to a person to reach a desired outcome.\nFor example, a teacher might try to understand their student's current comprehension level to tailor their instruction accordingly, and a travel agent might ask questions of their customer to understand their preferences in order to recommend activities they might enjoy.\nLLMs trained with supervised fine-tuning or ``single-step'' RL, as with standard RLHF, might struggle which tasks that require such goal-directed behavior, since they are not trained to optimize for overall conversational outcomes after multiple turns of interaction. \nIn this work, we explore a new method for adapting LLMs with RL for such goal-directed dialogue. Our key insight is that, though LLMs might not effectively solve goal-directed dialogue tasks out of the box, they can provide useful data for solving such tasks by simulating human-like behaviors. Given a textual description of a goal-directed dialogue task, we leverage LLMs to synthesize hypothetical in-domain human-human interactions. Our algorithm then utilizes this dataset with offline reinforcement learning}to train an interactive conversational agent that can optimize multi-step objectives. Empirically, we show that our proposed approach achieves state-of-the-art performance in various goal-directed dialogue tasks that include teaching and preference elicitation."
    },
    {
        "title": "Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View",
        "link_suffix": "/forum?id=m51BgoqvbP",
        "link": "https://openreview.net/forum?id=m51BgoqvbP",
        "pdf_link": "https://openreview.net/pdf?id=m51BgoqvbP",
        "keywords": "pretraining, language model, learning rate, loss landscape, manifold",
        "abstract": "Training language models currently requires pre-determining a fixed compute budget because the typical cosine learning rate schedule depends on the total number of steps. In contrast, the Warmup-Stable-Decay (WSD) schedule uses a constant learning rate to produce a main branch of iterates that can in principle continue indefinitely without a pre-specified compute budget. Then, given any compute budget, one can branch out from the main branch at a proper at any time with a rapidly decaying learning rate to produce a strong model. Empirically, WSD generates an intriguing, non-traditional loss curve: the loss remains elevated during the stable phase but sharply declines during the decay phase. Towards explaining this phenomenon, we conjecture that pretraining loss exhibits a river valley landscape, which resembles a deep valley with a river at its bottom. Under this assumption, we show that during the stable phase, the iterate undergoes large oscillations due to the high learning rate, yet it progresses swiftly along the river. During the decay phase, the rapidly dropping learning rate minimizes the iterate’s oscillations, moving it closer to the river and revealing true optimization progress. Therefore, the sustained high learning rate phase and fast decaying phase are responsible for progress in the river and the mountain directions, respectively, and are both critical. Our analysis predicts phenomenons consistent with empirical observations and shows that this landscape can naturally emerge from pretraining on a simple bi-gram dataset. Inspired by the theory, we introduce WSD-S, a variant of WSD that reuses previous checkpoints’ decay phases and keeps only one main branch, where we resume from a decayed checkpoint. WSD-S empirically outperforms WSD and Cyclic-Cosine in obtaining multiple pretrained language model checkpoints across various compute budgets in a single run for parameters scaling from 0.1B to 1.2B."
    },
    {
        "title": "Towards scientific discovery with dictionary learning: Extracting biological concepts from microscopy foundation models",
        "link_suffix": "/forum?id=uDIiL89ViX",
        "link": "https://openreview.net/forum?id=uDIiL89ViX",
        "pdf_link": "https://openreview.net/pdf?id=uDIiL89ViX",
        "keywords": "mechanistic interpretability, ViT, MAE, scientific discovery, drug discovery, biological representation learning",
        "abstract": "Dictionary learning (DL) has emerged as a powerful interpretability tool for large language models. By extracting known concepts (e.g., Golden-Gate Bridge) from human-interpretable data (e.g., text), sparse DL can elucidate a model's inner workings. In this work, we ask if DL can also be used to discoverunknownconcepts from less human-interpretable scientific data (e.g., cell images), ultimately enabling modern approaches to scientific discovery. As a first step, we use DL algorithms to study microscopy foundation models trained on multi-cell image data, where little prior knowledge exists regarding which high-level concepts  should arise. We show that sparse dictionaries indeed extract biologically-meaningful concepts such as cell type and genetic perturbation type. We also propose a new DL algorithm, Iterative Codebook Feature Learning (ICFL), and combine it with a pre-processing step which uses PCA whitening from a control dataset. In our experiments, we demonstrate that both ICFL and PCA improve the selectivity or \"monosemanticity\" of extracted features compared to TopK sparse autoencoders."
    },
    {
        "title": "ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning",
        "link_suffix": "/forum?id=kSdWcw5mkp",
        "link": "https://openreview.net/forum?id=kSdWcw5mkp",
        "pdf_link": "https://openreview.net/pdf?id=kSdWcw5mkp",
        "keywords": "diffusion models, concept editing, pruning",
        "abstract": "While large-scale text-to-image diffusion models have demonstrated impressive image-generation capabilities, there are significant concerns about their potential misuse for generating unsafe content, violating copyright, and perpetuating societal biases. Recently, the text-to-image generation community has begun addressing these concerns by editing or unlearning undesired concepts from pre-trained models. However, these methods often involve data-intensive and inefficient fine-tuning or utilize various forms of token remapping, rendering them susceptible to adversarial jailbreaks. In this paper, we present a simple and effective training-free approach, ConceptPrune, wherein we first identify critical regions within pre-trained models responsible for generating undesirable concepts, thereby facilitating straightforward concept unlearning via weight pruning. Experiments across a range of concepts including artistic styles, nudity, and object erasure demonstrate that target concepts can be efficiently erased by pruning a tiny fraction, approximately 0.12% of total weights, enabling multi-concept erasure and robustness against various white-box and black-box adversarial attacks."
    },
    {
        "title": "RoundTable: Investigating Group Decision-Making Mechanism in Multi-Agent Collaboration",
        "link_suffix": "/forum?id=WVWZ6SnM4t",
        "link": "https://openreview.net/forum?id=WVWZ6SnM4t",
        "pdf_link": "https://openreview.net/pdf?id=WVWZ6SnM4t",
        "keywords": "Multi-Agent System, Decentralized Collaboration, Collaboration Pattern, Group Behavior, Communication Protocol",
        "abstract": "This study investigates the efficacy of Multi-Agent Systems in eliciting cross-agent communication and enhancing collective intelligence through group decision-making in a decentralized setting. Unlike centralized mechanisms, where a fixed hierarchy governs social choice, decentralized group decision-making allows agents to engage in joint deliberation. Our research focuses on the dynamics of communication and decision-making within various social choice methods. By applying different voting rules in various environments, we find that moderate decision flexibility yields better outcomes. Additionally, exploring the linguistic features of agent-to-agent conversations reveals indicators of effective collaboration, offering insights into communication patterns that facilitate or hinder collaboration. Finally, we propose various methods for determining the optimal stopping point in multi-agent collaborations based on linguistic cues. Our findings contribute to a deeper understanding of how decentralized decision-making and group conversation shape multi-agent collaboration, with implications for the design of more effective MAS environments."
    },
    {
        "title": "IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction",
        "link_suffix": "/forum?id=td5nvlhJdc",
        "link": "https://openreview.net/forum?id=td5nvlhJdc",
        "pdf_link": "https://openreview.net/pdf?id=td5nvlhJdc",
        "keywords": "Artificial Intelligence, Reasoning, Interactive Learning Environment, Large Language Model Agent",
        "abstract": "While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in abductive reasoning and holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark specifically designed to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses using patterns identified from new observations through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, within this framework, our comparison with 50 human participants reveals notable discrepancies in rule-learning behaviors. LLM agents tend to generate plausible initial hypotheses but struggle to refine them through interaction. Conversely, humans, despite sometimes overlooking initial details, excel at incorporating feedback and continuously improving their hypotheses. We believe our benchmark, RULEARN, will serve as a valuable and challenging resource, and that the IDEA framework will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. We will release our code and data upon acceptance of the paper."
    },
    {
        "title": "Transformers Learn Variable-order Markov Chains in-Context",
        "link_suffix": "/forum?id=TdgAtxP6G2",
        "link": "https://openreview.net/forum?id=TdgAtxP6G2",
        "pdf_link": "https://openreview.net/pdf?id=TdgAtxP6G2",
        "keywords": "In-context learning; Variable-order Markov chain; Context Tree Weighting",
        "abstract": "Large language models (LLMs) have demonstrated impressive in-context learning (ICL) capability. However, it is still unclear how the underlying transformers accomplish it, especially in more complex scenarios. Toward this goal, several recent works studied how transformers learn fixed-order Markov chains (FOMC) in context, yet natural languages are more suitably modeled by variable-order Markov chains (VOMC), i.e., context trees (CTs). In this work, we study the ICL of VOMC by viewing language modeling as a form of data compression and focusing on small alphabets and low-order VOMCs. This perspective allows us to leverage mature compression algorithms, such as context-tree weighting (CTW) and prediction by partial matching (PPM) algorithms as baselines, the former of which is Bayesian optimal for a class of priors that we refer to as the CTW priors. We empirically observe a few phenomena: 1) Transformers can indeed learn to compress VOMC in-context, while PPM suffers significantly; 2) The performance of transformers is not very sensitive to the number of layers, and even a two-layer transformer can learn in-context quite well; and 3) Transformers trained and tested on non-CTW priors can significantly outperform the CTW algorithm. To explain these phenomena, we analyze the attention map of the transformers and extract two mechanisms, on which we provide two transformer constructions: 1) A construction with $D+2$ layers that can mimic the CTW algorithm accurately for CTs of maximum order $D$, 2) A 2-layer transformer that utilizes the feed-forward network for probability blending. These constructions can explain most of the phenomena mentioned above. One distinction from the FOMC setting is that a counting mechanism appears to play an important role. We implement these synthetic transformer layers and show that such hybrid transformers can match the ICL performance of transformers, and more interestingly, some of them can perform even better despite the much-reduced parameter sets."
    },
    {
        "title": "Information Bottleneck for Active Feature Acquisition",
        "link_suffix": "/forum?id=u5KzZjQOsl",
        "link": "https://openreview.net/forum?id=u5KzZjQOsl",
        "pdf_link": "https://openreview.net/pdf?id=u5KzZjQOsl",
        "keywords": "Active Feature Acquisition, Dynamic Feature Selection",
        "abstract": "Traditional supervised learning typically assumes that all features are available simultaneously during deployment. However, this assumption does not hold in many real-world scenarios, such as medicine, where information is acquired sequentially based on an evolving understanding of a specific patient's condition. Active Feature Acquisition aims to address this problem by dynamically selecting which feature to measure based on the current observations, independently for each test instance. Current approaches either use Reinforcement Learning, which suffers from training difficulties; or greedily maximize the conditional mutual information of the label and unobserved features, which inherently makes myopic acquisitions. To address these shortcomings, we introduce a novel method using information bottleneck. Via stochastic encodings, we make acquisitions by reasoning about the features across many possible unobserved realizations in a regularized latent space. Extensive evaluation on a large range of synthetic and real datasets demonstrates that our approach reliably outperforms a diverse set of baselines."
    },
    {
        "title": "Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning",
        "link_suffix": "/forum?id=v4MTnPiYXY",
        "link": "https://openreview.net/forum?id=v4MTnPiYXY",
        "pdf_link": "https://openreview.net/pdf?id=v4MTnPiYXY",
        "keywords": "offline reinforcement learning, language models, dialogue, robotics",
        "abstract": "Value-based reinforcement learning (RL) can in principle learn effective policies for a wide range of multi-turn problems, from games to dialogue to robotic control, including via offline RL from static previously collected datasets. However, despite the widespread use of policy gradient methods to train large language models for single turn tasks (e.g., question answering), value-based methods for multi-turn RL in an off-policy or offline setting have proven particularly challenging to scale to the setting of large language models. This setting requires effectively leveraging pretraining, scaling to large architectures with billions of parameters, and training on large datasets, all of which represent major challenges for current value-based RL methods. In this work, we propose a novel offline RL algorithm that addresses these drawbacks, casting Q-learning as a modified supervised fine-tuning (SFT) problem where the probabilities of tokens directly translate to Q-values. In this way we obtain an algorithm that smoothly transitions from maximizing the likelihood of the data during pretraining to learning a near-optimal Q-function during finetuning. Our algorithm has strong theoretical foundations, enjoying performance bounds similar to state-of-the-art Q-learning methods, while in practice utilizing an objective that closely resembles SFT. Because of this, our approach can enjoy the full benefits of the pretraining of language models, without the need to reinitialize any weights before RL finetuning, and without the need to initialize new heads for predicting values or advantages. Empirically, we evaluate our method on both pretrained LLMs and VLMs, on a variety of tasks including both natural language dialogue and robotic manipulation and navigation from images."
    }
]