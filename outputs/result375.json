[
    {
        "title": "Exact linear-rate gradient descent: optimal adaptive stepsize theory and practical use",
        "link_suffix": "/forum?id=1NYhrZynvC",
        "link": "https://openreview.net/forum?id=1NYhrZynvC",
        "pdf_link": "https://openreview.net/pdf?id=1NYhrZynvC",
        "keywords": "gradient descent, adaptive stepsize/learning rate, universal optimal choice, exact convergence rate",
        "abstract": "Consider gradient descent iterations $ {x}^{k+1} = {x}^k - \\alpha_k \\nabla f ({x}^k) $. \nSuppose gradient exists and $ \\nabla f ({x}^k) \\neq {0}$.\nWe propose the following closed-form stepsize choice:\n\\begin{equation}\n    \\alpha_k^\\star =  \\frac{ \\Vert  {x}^\\star - {x}^k  \\Vert }{\\left\\Vert \\nabla f({x}^k)  \\right\\Vert} \\cos\\eta_k , \\tag{theoretical}\n\\end{equation}\nwhere $ \\eta_k $ is the angle between vectors $ {x}^\\star - {x}^k  $ and $ -\\nabla f({x}^k)  $.\nIt is universally applicable and admits an exact linear  convergence  rate  with factor $ \\sin^2\\eta_k  $.\nMoreover, if $ f $ is  convex and $ L $-smooth,  then $ \\alpha_k^\\star \\geq {1}/{L} $.For practical use,  we approximate (can be exact) the above  via \n\\begin{equation}\n    \\alpha_{k}^\\dagger = \\gamma_0 \\cdot \\frac{ f({x}^k) - \\bar{f}_0  }{\\Vert  \\nabla f (\t{x}^k )   \\Vert^2 } ,\n    \\tag{practical use}\n\\end{equation}\nwhere  $\\gamma_0 $ is a tunable parameter; $ \\bar{f}_0 $ is  a guess on the smallest objective value (can be auto. updated).\nSuppose  $ f $ is convex and $ \\bar{f}_0 = f ( {x}^\\star )   $, then \nany choice from $\\gamma_0 \\in (0,2] $ guarantees an exact linear-rate convergence to the optimal point.We consider a  few examples.\n(i) An $ \\mathbb{R}^2 $ quadratic program, where a well-known ill-conditioning bottleneck is  addressed, with a rate strictly better than $ O(1/2^k) $. (ii) A geometric program, where an inaccurate guess $ \\bar{f}_0  $ remains powerful.\n(iii) A non-convex MNIST classification problem via neural networks, where preliminary tests show that ours admits better performance than the state-of-the-art algorithms,  particularly a  tune-free version is available in some settings."
    },
    {
        "title": "UniRestore3D: A Scalable Framework For General Shape Restoration",
        "link_suffix": "/forum?id=xPO6fwvldG",
        "link": "https://openreview.net/forum?id=xPO6fwvldG",
        "pdf_link": "https://openreview.net/pdf?id=xPO6fwvldG",
        "keywords": "Shape Restoration, 3D Reconstruction, Diffusion Model",
        "abstract": "Shape restoration aims to recover intact 3D shapes from defective ones, such as those that are incomplete, noisy, and low-resolution. Previous works have achieved impressive results in shape restoration subtasks thanks to advanced generative models. While effective for specific shape defects, they are less applicable in real-world scenarios involving multiple defect types simultaneously. Additionally, training on limited subsets of defective shapes hinders knowledge transfer across restoration types and thus affects generalization. In this paper, we address the task of general shape restoration, which restores shapes with various types of defects through a unified model, thereby naturally improving the applicability and scalability. Our approach first standardizes the data representation across different restoration subtasks and constructs a large-scale dataset with diverse types of shape defects. Next, we design an efficient hierarchical shape generation model and a noise-robust defective shape encoder that enables effective impaired shape understanding and intact shape generation. Moreover, we propose a scalable training strategy for efficient model training. The capabilities of our proposed method are demonstrated across multiple shape restoration subtasks and validated on various datasets, including Objaverse, ShapeNet, GSO, and ABO."
    },
    {
        "title": "Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning",
        "link_suffix": "/forum?id=636M0nNbPs",
        "link": "https://openreview.net/forum?id=636M0nNbPs",
        "pdf_link": "https://openreview.net/pdf?id=636M0nNbPs",
        "keywords": "Vision-Language Model, Detailed Image Captioning, Caption Metric, Alignment, Preference Optimization, Large Language Models",
        "abstract": "Image captioning has long been a pivotal task in visual understanding, with recent advancements in vision-language models (VLMs) significantly enhancing the ability to generate detailed image captions. However, the evaluation of detailed image captioning remains underexplored due to outdated evaluation metrics and coarse annotations. In this paper, we introduce DeCapBench along with a novel metric, DCScore, specifically designed for detailed captioning tasks. DCScore evaluates hallucinations and fine-grained comprehensiveness by deconstructing responses into the smallest self-sufficient units, termed primitive information units, and assessing them individually. Our evaluation shows that DCScore aligns more closely with human judgment than other rule-based or model-based metrics. Concurrently, DeCapBench exhibits a high correlation with VLM arena results on descriptive tasks, surpassing existing benchmarks for vision-language models. Additionally, we present an automatic fine-grained feedback collection method, FeedQuill, for preference optimization based on our advanced metric, demonstrating robust generalization capabilities across auto-generated preference data. Extensive experiments on multiple VLMs demonstrate that our method not only significantly reduces hallucinations but also enhances performance across various benchmarks, achieving superior detail captioning performance while surpassing GPT-4o."
    },
    {
        "title": "LLaMA-Omni: Seamless Speech Interaction with Large Language Models",
        "link_suffix": "/forum?id=PYmrUQmMEw",
        "link": "https://openreview.net/forum?id=PYmrUQmMEw",
        "pdf_link": "https://openreview.net/pdf?id=PYmrUQmMEw",
        "keywords": "large language models, speech interaction, speech-to-speech, speech-language models",
        "abstract": "Models like GPT-4o enable real-time interaction with large language models (LLMs) through speech, significantly enhancing user experience compared to traditional text-based interaction. However, there is still a lack of exploration on how to build speech interaction models based on open-source LLMs. To address this, we propose LLaMA-Omni, a novel model architecture designed for low-latency and high-quality speech interaction with LLMs. LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM, and a streaming speech decoder. It eliminates the need for speech transcription, and can simultaneously generate text and speech responses directly from speech instructions with extremely low latency. We build our model based on the latest Llama-3.1-8B-Instruct model. To align the model with speech interaction scenarios, we construct a dataset named InstructS2S-200K, which includes 200K speech instructions and corresponding speech responses. Experimental results show that compared to previous speech-language models, LLaMA-Omni provides better responses in both content and style, with a response latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3 days on just 4 GPUs, paving the way for the efficient development of speech-language models in the future."
    },
    {
        "title": "Phase Transitions in the Output Distribution of Large Language Models",
        "link_suffix": "/forum?id=dq3keisMjT",
        "link": "https://openreview.net/forum?id=dq3keisMjT",
        "pdf_link": "https://openreview.net/pdf?id=dq3keisMjT",
        "keywords": "language models, generative models, phase transitions",
        "abstract": "In a physical system, changing parameters such as temperature can induce a phase transition: an abrupt change from one state of matter to another. Analogous phenomena have recently been observed in large language models. Typically, the task of identifying phase transitions requires human analysis and some prior understanding of the system to narrow down which low-dimensional properties to monitor and analyze. Statistical methods for the automated detection of phase transitions from data have recently been proposed within the physics community. These methods are largely system agnostic and, as shown here, can be adapted to study the behavior of large language models. In particular, we quantify distributional changes in the generated output via statistical distances, which can be efficiently estimated with access to the probability distribution over next-tokens. This versatile approach is capable of discovering new phases of behavior and unexplored transitions -- an ability that is particularly exciting in light of the rapid development of language models and their emergent capabilities."
    },
    {
        "title": "Debiasing Text-to-image Diffusion Models with Self-discovering Latent Directions",
        "link_suffix": "/forum?id=RhkI1cba7n",
        "link": "https://openreview.net/forum?id=RhkI1cba7n",
        "pdf_link": "https://openreview.net/pdf?id=RhkI1cba7n",
        "keywords": "Diffusion Models, AI fairness",
        "abstract": "While Diffusion Models (DM) exhibit remarkable performance across various image generative tasks, they nonetheless reflect the inherent bias presented in the training set. As DMs are now widely used in real-world applications, these biases could perpetuate a distorted worldview and hinder opportunities for minority groups. Existing methods on debiasing DMs usually requires model re-training with a human-crafted reference dataset or additional classifiers, which suffer from two major limitations: (1) collecting reference datasets causes expensive annotation cost; (2) the debiasing performance is heavily constrained by the quality of the reference dataset or the additional classifier. To address the above limitations, we propose DebiasDiff, a plug-and-play method that learns attribute latent directions in a self-discovering manner, thus eliminating the reliance on such reference dataset. Specifically, DebiasDiff consists of two parts: a set of attribute adapters and a distribution indicator. Each adapter in the set aims to learn an attribute latent direction, and is optimized via noise composition through a self-discovering process.Then, the distribution indicator is multiplied by the set of adapters to guide the generation process towards the prescribed distribution. Our method enables debiasing multiple attributes in DMs simultaneously, while remaining lightweight and easily integrable with other DMs, eliminating the need for re-training. Extensive experiments on debiasing gender, racial, and their intersectional biases show that our method outperforms previous SOTA by a large margin."
    },
    {
        "title": "TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types",
        "link_suffix": "/forum?id=JXgnnUC0PH",
        "link": "https://openreview.net/forum?id=JXgnnUC0PH",
        "pdf_link": "https://openreview.net/pdf?id=JXgnnUC0PH",
        "keywords": "instruction fine-tunning dataset, multimodal large language models, hierarchical task types, data generation pipeline",
        "abstract": "Multimodal visual language models are gaining prominence in open-world applications, driven by rapid advancements in model architectures, training strategies, and the availability of high-quality training data. However, their performance is often limited by insufficient task-specific data, leading to poor generalization and biased outputs. Existing efforts to increase task diversity in fine-tuning datasets are hindered by the labor-intensive process of manual task labeling, which typically produces only a few hundred task types. Motivated by task diversity and automated processes aimed at saving labor, in this paper we propose TaskGalaxy, a large-scale multi-modal instruction fine-tuning dataset that includes 19,227 hierarchical task types and 413,648 associated samples. TaskGalaxy utilizes GPT-4o to enrich task diversity by expanding from a small set of manually defined tasks, with CLIP and GPT-4o filtering those that best match open-source images, and generating relevant question-answer pairs. Multiple open-source models are employed to filter and ensure high-quality, well-aligned samples. This automated process enhances both task diversity and data quality, reducing manual intervention. Experimental results demonstrate that incorporating TaskGalaxy into the LLaVA-V1.5 and InternVL-Chat-V1.0 model architectures for instruction fine-tuning leads to substantial performance improvements across all 16 benchmarks, highlighting the critical importance of task type diversity. The TaskGalaxy dataset will be publicly released to support future research."
    },
    {
        "title": "Structure-Preserving Operator Learning",
        "link_suffix": "/forum?id=60FseFP084",
        "link": "https://openreview.net/forum?id=60FseFP084",
        "pdf_link": "https://openreview.net/pdf?id=60FseFP084",
        "keywords": "Operator learning, PDEs, Structure-preserving discretization, Finite Element Method, Graph Neural Networks, Multigrid",
        "abstract": "Learning complex dynamics driven by partial differential equations directly from data holds great promise for fast and accurate simulations of complex physical systems. In most cases, this problem can be formulated as an operator learning task, where one aims to learn the operator representing the physics of interest, which entails discretization of the continuous system. However, preserving key continuous properties at the discrete level, such as boundary conditions, and addressing physical systems with complex geometries is challenging for most existing approaches. We introduce a family of operator learning architectures,structure-preserving operator networks(SPONs), that allows to preserve key mathematical and physical properties of the continuous system by leveraging finite element (FE) discretizations of the input-output spaces. SPONs are encode-process-decode architectures that are end-to-end differentiable, where the encoder and decoder follows from the discretizations of the input-output spaces. SPONs can operate on complex geometries, enforce certain boundary conditions exactly, and offer theoretical guarantees. Our framework provides a flexible way of devising structure-preserving architectures tailored to specific applications, and offers an explicit trade-off between performance and efficiency, all thanks to the FE discretization of the input-output spaces. Additionally, we introduce a multigrid-inspired SPON architecture that yields improved performance at higher efficiency. Finally, we release a software to automate the design and training of SPON architectures."
    },
    {
        "title": "O-Edit: Orthogonal Subspace Editing for Language Model Sequential Editing",
        "link_suffix": "/forum?id=vx1vJIFvd5",
        "link": "https://openreview.net/forum?id=vx1vJIFvd5",
        "pdf_link": "https://openreview.net/pdf?id=vx1vJIFvd5",
        "keywords": "large language model, model editing, sequential editing",
        "abstract": "Large language models (LLMs) acquire knowledge during pre-training, but over time, this knowledge may become incorrect or outdated, necessitating updates after training. Knowledge editing techniques address this issue without the need for costly re-training. However, most existing methods are designed for single edits, and as the number of edits increases, they often cause a decline in the model's overall performance, posing significant challenges for sequential editing. To overcome this, we propose Orthogonal Subspace Editing, O-Edit. This algorithm orthogonalizes the direction of each knowledge update, minimizing interference between successive updates and reducing the impact of new updates on unrelated knowledge. Our approach does not require replaying previously edited data and processes each edit knowledge on time. It can perform thousands of edits on mainstream LLMs, achieving an average performance improvement that is 4.2 times better than existing methods while effectively preserving the model's performance on downstream tasks, all with minimal additional parameter overhead."
    },
    {
        "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition",
        "link_suffix": "/forum?id=LPfLsSqrQJ",
        "link": "https://openreview.net/forum?id=LPfLsSqrQJ",
        "pdf_link": "https://openreview.net/pdf?id=LPfLsSqrQJ",
        "keywords": "Few-shot Learning, Action Recognition, Vision-Language Model",
        "abstract": "Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose $\\textbf{DiST}$, an innovative $\\textbf{D}$ecomposition-$\\textbf{i}$ncorporation framework that makes use of decoupled $\\textbf{S}$patial and $\\textbf{T}$emporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (i.e., action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/PKC) to discover discriminative object- and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling, further understanding diverse temporal patterns in videos. The learned prototypes at varying levels of granularity thus provide transparency in capturing fine-grained spatial details and dynamic temporal information, so as to enable accurate recognition of both appearance-centric and motion-centric actions. Experimental results show DiST achieves state-of-the-art results on four standard FSAR datasets (i.e., Kinetics, UCF101, HMDB51 and SSv2-small). Full code will be released."
    },
    {
        "title": "Crafting Zero-Cost Proxy Metrics for Neural Architecture Search via Symbolic Regression",
        "link_suffix": "/forum?id=ludEV7dK9G",
        "link": "https://openreview.net/forum?id=ludEV7dK9G",
        "pdf_link": "https://openreview.net/pdf?id=ludEV7dK9G",
        "keywords": "neural architecture search, symbolic regression, zero-cost proxy metrics, genetic programming",
        "abstract": "Using zero-cost (ZC) metrics to estimate network performance instead of relying on expensive training processes has proven both its efficiency and efficacy in Neural Architecture Search (NAS). However, a significant limitation of most ZC proxies is their inconsistency, as reflected by the substantial variation in their performance across different problems. Additionally, the design of current ZC metrics is manual, which is a lengthy trial-and-error process and requires expert knowledge to develop ZC metrics effectively. These challenges raise two questions: (1) Can we automate the design of ZC metrics? and (2) Can we utilize the existing hand-crafted ZC metrics to synthesize a better one? In this study, we propose a framework based on Symbolic Regression to automate the design of ZC metrics. Our framework is not only highly extensible but also capable of quickly producing a ZC metric with a strong positive rank correlation to network performance across multiple problems within just a few minutes. Extensive experiments on 13 problems in NAS-Bench-Suite-Zero, covering various search spaces and tasks, demonstrate the superiority of our automatically designed proxies over hand-crafted ones. By integrating our proxy metrics into an evolutionary algorithm, we could identify a network architecture with comparable performance on the CIFAR-10 dataset within 15 minutes using a single GeForce RTX 3090 GPU."
    },
    {
        "title": "MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations",
        "link_suffix": "/forum?id=0PxLpVURTl",
        "link": "https://openreview.net/forum?id=0PxLpVURTl",
        "pdf_link": "https://openreview.net/pdf?id=0PxLpVURTl",
        "keywords": "self-supervised learning, masked image modeling, instance discrimination, computer vision, contrastive learning",
        "abstract": "We introduce MIM (Masked Image Modeling)-Refiner, a contrastive learning boost for pre-trained MIM models. MIM-Refiner is motivated by the insight that strong representations within MIM models generally reside in intermediate layers. Accordingly, MIM-Refiner leverages multiple instance discrimination (ID) heads that are connected to different intermediate layers. In each head, a nearest neighbor ID objective constructs clusters that capture semantic information which improves performance on downstream tasks, including off-the-shelf and fine-tuning settings.The refinement process is short and simple -  yet highly effective. Within a few epochs, we refine the features of MIM models from subpar to state-of-the-art, off-the-shelf features. Refining a ViT-H, pre-trained with data2vec 2.0 on ImageNet-1K, sets a new state-of-the-art in linear probing (84.7%) and low-shot classification among models that are pre-trained on ImageNet-1K. MIM-Refiner efficiently combines the advantages of MIM and ID objectives, enabling scaling ID objectives to billion parameter models using relatively little compute. MIM-Refiner compares favorably against previous state-of-the-art SSL models on various benchmarks such as low-shot classification, long-tailed classification and semantic segmentation."
    },
    {
        "title": "Exploiting Hidden Symmetry to Improve Objective Perturbation for DP linear learners with a nonsmooth\u21131-norm",
        "link_suffix": "/forum?id=J863DxU7Sx",
        "link": "https://openreview.net/forum?id=J863DxU7Sx",
        "pdf_link": "https://openreview.net/pdf?id=J863DxU7Sx",
        "keywords": "Objective Perturbation, Convolution, Nonsmooth, DP-SCO",
        "abstract": "Objective Perturbation (OP) is a classic approach to differentially private (DP) convex optimization with smooth loss functions but is less understood for nonsmooth cases. In this work, we study how to apply OP to DP linear learners under loss functions with an implicit $\\ell_1$-norm structure, such as $\\max(0,x)$ as a motivating example. We propose to first smooth out the hidden $\\ell_1$-norm by convolution, and then invoke standard OP. Convolution has many advantages that distinguish itself from Moreau Envelope, such as approximating from above and a higher degree of hyperparameters. These advantages, in conjunction with the symmetry of $\\ell_1$-norm, result in tighter pointwise approximation, which further facilitates tighter analysis of generalization risks by using pointwise bounds. Under mild assumptions on groundtruth distributions, the proposed OP-based algorithm is found to be rate-optimal, and can achieve the excess generalization risk $\\mathcal{O}(\\frac{1}{\\sqrt{n}}+\\frac{\\sqrt{d\\ln(1/\\delta)}}{n\\varepsilon})$. Experiments demonstrate the competitive performance of the proposed method to Noisy-SGD."
    },
    {
        "title": "ESE: Espresso Sentence Embeddings",
        "link_suffix": "/forum?id=plgLA2YBLH",
        "link": "https://openreview.net/forum?id=plgLA2YBLH",
        "pdf_link": "https://openreview.net/pdf?id=plgLA2YBLH",
        "keywords": "sentence embeddings, semantic textual similarity, information retrieval, retrieval-augmented generation",
        "abstract": "High-quality sentence embeddings are fundamental in many natural language processing (NLP) tasks, such as semantic textual similarity (STS) and retrieval-augmented generation (RAG). However, most existing methods leverage fixed-length sentence embeddings from full-layer language models, which lack the scalability to accommodate the diverse available resources across various applications. Viewing this gap, we propose a novel sentence embedding model Espresso Sentence Embeddings (ESE) with two learning processes. First, the learn-to-express process encodes more salient representations to shallow layers. Second, the learn-to-compress process compacts essential features into the initial dimensions using Principal Component Analysis (PCA). This way, ESE can scale model depth via the former process and embedding size via the latter. Extensive experiments on STS and RAG suggest that ESE can effectively produce high-quality sentence embeddings with less model depth and embedding size, enhancing inference efficiency."
    },
    {
        "title": "Vision-LSTM: xLSTM as Generic Vision Backbone",
        "link_suffix": "/forum?id=SiH7DwNKZZ",
        "link": "https://openreview.net/forum?id=SiH7DwNKZZ",
        "pdf_link": "https://openreview.net/pdf?id=SiH7DwNKZZ",
        "keywords": "Computer Vision, xLSTM, image classification, semantic segmentation, transfer learning, ImageNet",
        "abstract": "Transformers are widely used as generic backbones in computer vision, despite initially introduced for natural language processing. Recently, the Long Short-Term Memory (LSTM) has been extended to a scalable and performant architecture - the xLSTM - which overcomes long-standing LSTM limitations via exponential gating and parallelizable matrix memory structure. In this paper, we introduce Vision-LSTM (ViL), an adaption of the xLSTM building blocks to computer vision. ViL comprises a stack of xLSTM blocks where odd blocks process the sequence of patch tokens from top to bottom while even blocks go from bottom to top.ViL achieves strong performances on classification, transfer learning and segmentation tasks as well as a beneficial pre-training cost-to-performance trade-off. Experiments show that ViL holds promise to be further deployed as new generic backbone for computer vision architectures."
    },
    {
        "title": "On the Fourier analysis in the SO(3) space : the EquiLoPO Network",
        "link_suffix": "/forum?id=LvTSvdiSwG",
        "link": "https://openreview.net/forum?id=LvTSvdiSwG",
        "pdf_link": "https://openreview.net/pdf?id=LvTSvdiSwG",
        "keywords": "Equivariance, Fourier Analysis, SO(3), MedMNIST3D, Local Activation, CNN, Group Convolution, Computer Vision, 3D Medical Images",
        "abstract": "Analyzing volumetric data with rotational invariance or equivariance is currently an active research topic. Existing deep-learning approaches utilize either group convolutional networks limited to discrete rotations or steerable convolutional networks with constrained filter structures. This work proposes a novel equivariant neural network architecture that achieves analytical Equivariance to Local Pattern Orientation on the continuous SO(3) group while allowing unconstrained trainable filters - EquiLoPO Network. Our key innovations are a group convolutional operation leveraging irreducible representations as the Fourier basis and a local activation function in the SO(3) space that provides a well-defined mapping from input to output functions, preserving equivariance. By integrating these operations into a ResNet-style architecture, we propose a model that overcomes the limitations of prior methods. A comprehensive evaluation on diverse 3D medical imaging datasets from MedMNIST3D demonstrates the effectiveness of our approach, which consistently outperforms state of the art. This work suggests the benefits of true rotational equivariance on SO(3) and flexible unconstrained filters enabled by the local activation function, providing a flexible framework for equivariant deep learning on volumetric data with potential applications across domains."
    },
    {
        "title": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator",
        "link_suffix": "/forum?id=X0CxfByJog",
        "link": "https://openreview.net/forum?id=X0CxfByJog",
        "pdf_link": "https://openreview.net/pdf?id=X0CxfByJog",
        "keywords": "Dataset distillation, inter-class feature compensator (INFER)",
        "abstract": "Dataset distillation has emerged as a technique aiming to condense informative features from large, natural datasets into a compact and synthetic form. While recent advancements have refined this technique, its performance is bottlenecked by the prevailing class-specific synthesis paradigm. Under this paradigm, synthetic data is optimized exclusively for a pre-assigned one-hot label, creating an implicit class barrier in feature condensation. This leads to inefficient utilization of the distillation budget and oversight of inter-class feature distributions, which ultimately limits the effectiveness and efficiency, as demonstrated in our analysis.\nTo overcome these constraints, this paper presents the Inter-class Feature Compensator (INFER), an innovative distillation approach that transcends the class-specific data-label framework widely utilized in current dataset distillation methods. Specifically, INFER leverages a Universal Feature Compensator (UFC) to enhance feature integration across classes, enabling the generation of multiple additional synthetic instances from a single UFC input. This significantly improves the efficiency of the distillation budget.\nMoreover, INFER enriches inter-class interactions during the distillation, thereby enhancing the effectiveness and generalizability of the distilled data. By allowing for the linear interpolation of labels similar to those in the original dataset, INFER meticulously optimizes the synthetic data and dramatically reduces the size of soft labels in the synthetic dataset to almost zero, establishing a new benchmark for efficiency and effectiveness in dataset distillation. In practice, INFER demonstrates state-of-the-art performance across benchmark datasets. For instance, it outperforms SRe2L by 8.8% on ImageNet-1k in the ipc = 50 setting."
    },
    {
        "title": "Generating Synthetic Genotypes using Diffusion Models",
        "link_suffix": "/forum?id=rN7Ewo2lV4",
        "link": "https://openreview.net/forum?id=rN7Ewo2lV4",
        "pdf_link": "https://openreview.net/pdf?id=rN7Ewo2lV4",
        "keywords": "Diffusion, Genome, DNA, 1000 Genome, ALS",
        "abstract": "In this paper, we introduce the first diffusion model designed to generate complete synthetic human genotypes, which, by standard protocols, one can straightforwardly expand into full-length, DNA-level genomes.\nThe synthetic genotypes mimic real human genotypes without just reproducing known genotypes, in terms of approved metrics. When training biomedically relevant classifiers with synthetic genotypes, accuracy is near-identical to the accuracy achieved when training classifiers with real data. We further demonstrate that augmenting small amounts of real with synthetically generated genotypes drastically improves performance rates. This addresses a significant challenge in translational human genetics: real human genotypes, although emerging in large volumes from genome wide association studies, are sensitive private data, which limits their public availability. Therefore, the integration of additional, insensitive data when striving for rapid sharing of biomedical knowledge of public interest appears imperative."
    },
    {
        "title": "DMM: Building a Versatile Image Generation Model via Distillation-Based Model Merging",
        "link_suffix": "/forum?id=t73rC2GJQJ",
        "link": "https://openreview.net/forum?id=t73rC2GJQJ",
        "pdf_link": "https://openreview.net/pdf?id=t73rC2GJQJ",
        "keywords": "Diffusion Models, Generative Models, Model Merging",
        "abstract": "The success of text-to-image (T2I) generation models has spurred a proliferation of numerous model checkpoints fine-tuned from the same base model on various specialized datasets.This overwhelming specialized model production introduces new challenges for high parameter redundancy and huge storage cost, thereby necessitating the development of effective methods to consolidate and unify the capabilities of diverse powerful models into a single one.A common practice in model merging adopts static linear interpolation in the parameter space to achieve the goal of style mixing.However, it neglects the features of T2I generation task that numerous distinct models cover sundry styles which may lead to incompatibility and confusion in the merged model.To address this issue, we introduce a style-promptable image generation pipeline which can accurately generate arbitrary-style images under the control of style vectors. Based on this design, we propose the score distillation based model merging paradigm (DMM), compressing multiple models into a single versatile T2I model. Moreover, we rethink and reformulate the model merging task in the context of T2I generation, by presenting new merging goals and evaluation protocols. Our experiments demonstrate that DMM can compactly reorganize the knowledge from multiple teacher models and achieve controllable arbitrary-style generation."
    },
    {
        "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation",
        "link_suffix": "/forum?id=XSVq2z1CU6",
        "link": "https://openreview.net/forum?id=XSVq2z1CU6",
        "pdf_link": "https://openreview.net/pdf?id=XSVq2z1CU6",
        "keywords": "Diffusion Model, 3D Computer Vision",
        "abstract": "Denoising diffusion probabilistic models have achieved significant success in point cloud generation, enabling numerous downstream applications, such as generative data augmentation and 3D model editing. However, little attention has been given to generating point clouds with point-wise segmentation labels, as well as to developing evaluation metrics for this task. Therefore, in this paper, we present SeaLion, a novel diffusion model designed to generate high-quality and diverse point cloud with fine-grained segmentation labels. Specifically, we introduce the semantic part-aware latent point diffusion technique, which leverages the intermediate features of the generative models to jointly predict the noise for perturbed latent points and associated part segmentation labels during the denoising process, and subsequently decodes the latent points to point clouds conditioned on part segmentation labels. To effectively evaluate the quality of generated point clouds, we introduce a novel point cloud pairwise distance calculation method named part-aware Chamfer distance (p-CD). This method enables existing metrics, such as 1-NNA, to measure both the local structural quality and inter-part coherence of generated point clouds. Experiments on the large-scale synthetic dataset ShapeNet and real-world medical dataset IntrA, demonstrate that SeaLion achieves remarkable performance in generation quality and diversity, outperforming the existing state-of-the-art model, DiffFacto, by 13.33% and 6.52% on 1-NNA (p-CD) across the two datasets. Experimental analysis shows that SeaLion can be trained semi-supervised, thereby reducing the demand for labeling efforts. Lastly, we validate the applicability of SeaLion in generative data augmentation for training segmentation models and the capability of SeaLion to serve as a tool for part-aware 3D shape editing."
    },
    {
        "title": "Alignment without Over-optimization: Training-Free Solution for Diffusion Models",
        "link_suffix": "/forum?id=vi3DjUhFVm",
        "link": "https://openreview.net/forum?id=vi3DjUhFVm",
        "pdf_link": "https://openreview.net/pdf?id=vi3DjUhFVm",
        "keywords": "diffusion models, alignment, reward over-optimization, sequential monte carlo samplers",
        "abstract": "Diffusion models excel in generative tasks, but aligning them with specific objec-\ntives while maintaining their versatility remains challenging. Existing fine-tuning\nmethods often suffer from reward over-optimization, while approximate guidance\napproaches fail to effectively optimize target rewards. Addressing these limita-\ntions, we propose a training-free sampling method based on Sequential Monte\nCarlo (SMC) to sample from the reward-aligned target distribution. Our approach,\ntailored for diffusion sampling and incorporating tempering techniques, achieves\ncomparable or superior target rewards to fine-tuning methods while preserving\ndiversity and cross-reward generalization. We demonstrate its effectiveness in\nsingle-reward optimization, multi-objective scenarios, and online black-box opti-\nmization. This work offers a robust solution for aligning diffusion models with\ndiverse downstream objectives without compromising their general capabilities."
    },
    {
        "title": "Seeing the part and knowing the whole: Object-Centric Learning with Inter-Feature Prediction",
        "link_suffix": "/forum?id=9Xt5TgM7Us",
        "link": "https://openreview.net/forum?id=9Xt5TgM7Us",
        "pdf_link": "https://openreview.net/pdf?id=9Xt5TgM7Us",
        "keywords": "Object-Centric Learning, Self-Supervised Learning, Computer Vision",
        "abstract": "Humans can naturally decompose scenes into understandable objects, resulting in strong visual comprehension ability. In light of this, Object-Centric Learning (OCL) seeks to explore how to construct object-level representations by encoding the information of objects in the scenes into several object vectors referred to as `slots'. Current OCL models rely on an auto-encoding paradigm that encodes the image feature into slots and reconstructs the images by composing the slots. However, merely reconstruction objectives do not guarantee that each slot exactly corresponds to a holistic object. Existing methods often fail when objects have complex appearances because the reconstruction objective cannot indicate which pixels should be assigned to the same slot. Therefore, additional regularization based on a more general prior is required. For this purpose, we draw on the gestalt ability that humans tend to complete a broken figure and perceive it as a whole, and propose Predictive Prior that features belonging to the same object tend to be able to predict each other. We implement this prior as an external loss function, demanding the model to assign features that can predict each other to the same slot, and vice versa. With experiments on multiple datasets, we demonstrate that our model outperforms previous models by a large margin in complex environments where objects have irregular outlines and intense color changes, according to various tasks including object discovery, compositional generation, and visual question & answering. Visualization results verify that our model succeeds in discovering objects holistically rather than dividing them into multiple parts, proving that Predictive Prior gives a more general object definition. Code is available athttps://anonymous.4open.science/r/PredictivePrior-32EF."
    },
    {
        "title": "Hi-Patch: Hierarchical Patch GNN for Irregular Multivariate Time Series Modeling",
        "link_suffix": "/forum?id=OGtUfA6Amo",
        "link": "https://openreview.net/forum?id=OGtUfA6Amo",
        "pdf_link": "https://openreview.net/pdf?id=OGtUfA6Amo",
        "keywords": "irregular multivariate time series, graph neural network",
        "abstract": "Multi-scale information is crucial for multivariate time series modeling. However, most existing time series multi-scale analysis methods treat all variables in the same manner, which is not well adaptive for Irregular Multivariate Time Series (IMTS), where different variables have distinct original scales/sampling rates. Therefore, extracting temporal and inter-variable dependencies at multiple scales in IMTS remains challenging. To fill this gap, we propose a hierarchical patch graph network Hi-Patch. The key components of Hi-Patch are an intra-patch graph layer and several inter-patch graph layers. The intra-patch graph layer flexibly represents and fully captures both the local temporal and inter-variable dependencies of densely sampled variables at the original scales by employing intra-patch fully connected graph networks. Subsequently, several inter-patch graph layers are stacked to form a hierarchical architecture, progressively\nrepresenting and extracting more global temporal and inter-variable features of both sparsely and densely sampled variables through scale-specific graph networks. The output of the last inter-patch graph layer is fed into taskspecific decoders to adapt to different downstream tasks. Experimental results on 8 datasets show that Hi-Patch outperforms a range of state-of-the-art models in both IMTS forecasting and classification tasks. Code is available at this repository:https://anonymous.4open.science/r/Hi-Patch-F42E."
    },
    {
        "title": "Robust Conformal Prediction with a Single Binary Certificate",
        "link_suffix": "/forum?id=ltrxRX5t0H",
        "link": "https://openreview.net/forum?id=ltrxRX5t0H",
        "pdf_link": "https://openreview.net/pdf?id=ltrxRX5t0H",
        "keywords": "Conformal Prediction, Uncertainty Quantification, Robust Conformal Prediction",
        "abstract": "Conformal prediction (CP) converts any model's output to prediction sets with a guarantee to cover the true label with (adjustable) high probability. Robust CP extends this guarantee to worst-case (adversarial) inputs. Existing baselines achieve robustness by bounding randomly smoothed conformity scores. In practice, they need expensive Monte-Carlo (MC) sampling ($\\sim10^4$ samples per point) to maintain an acceptable set size. We propose a robust conformal prediction that produces smaller sets even with just $10^2$ MC samples. Our approach binarizes samples with an adaptive bin selected to preserve the coverage guarantee. Remarkably, we prove that robustness can be achieved by computing only one binary certificate, unlike previous methods that certify each calibration (or test) point. Thus, our method is faster and returns smaller robust sets. We also eliminate a previous limitation that requires a bounded score function."
    },
    {
        "title": "Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders",
        "link_suffix": "/forum?id=mMPaQzgzAN",
        "link": "https://openreview.net/forum?id=mMPaQzgzAN",
        "pdf_link": "https://openreview.net/pdf?id=mMPaQzgzAN",
        "keywords": "Mechanistic interpretability, interpretability, dictionary learning",
        "abstract": "Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations.\nTo be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension.\nIn this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs.\nWe also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies.\nJumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run.\nBy utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage."
    }
]