[
    {
        "title": "PruneFuse: Efficient Data Selection via Weight Pruning and Network Fusion",
        "link_suffix": "/forum?id=AFMi0kUtDr",
        "link": "https://openreview.net/forum?id=AFMi0kUtDr",
        "pdf_link": "https://openreview.net/pdf?id=AFMi0kUtDr",
        "keywords": "Deep Learning, Active Learning, Data Selection Techniques",
        "abstract": "Efficient data selection is crucial for enhancing the training efficiency of deep neural networks and minimizing annotation requirements. Traditional methods often face high computational costs, limiting their scalability and practical use. We introduce PruneFuse, a novel strategy that leverages pruned networks for data selection and later fuses them with the original network to optimize training. \nPruneFuse operates in two stages: First, it applies structured pruning to create a smaller pruned network that, due to its structural coherence with the original network, is well-suited for the data selection task. This small network is then trained and selects the most informative samples from the dataset.\nSecond, the trained pruned network is seamlessly fused with the original network. This integration leverages the insights gained during the training of the pruned network to facilitate the learning process of the fused network while leaving room for the network to discover more robust solutions. \nExtensive experimentation on various datasets demonstrates that PruneFuse significantly reduces computational costs for data selection, achieves better performance than baselines, and accelerates the overall training process."
    },
    {
        "title": "Multi-attacks: A single adversarial perturbation for multiple images and target labels",
        "link_suffix": "/forum?id=fiTpna7fO5",
        "link": "https://openreview.net/forum?id=fiTpna7fO5",
        "pdf_link": "https://openreview.net/pdf?id=fiTpna7fO5",
        "keywords": "adversarial attacks, geometry, classification, robustness, security",
        "abstract": "We show that we can easily design a single adversarial perturbation $P$ that changes the class of $n$ images $X_1,X_2,\\dots,X_n$ from their original, unperturbed classes $c_1, c_2,\\dots,c_n$ to desired (not necessarily all the same) classes $c^*_1,c^*_2,\\dots,c^*_n$ for up to hundreds of images and target classes at once. We call these \\textit{multi-attacks}. Characterizing the maximum $n$ we can achieve under different conditions such as image resolution, we estimate the number of regions of high class confidence around a particular image in the space of pixels to be around $10^{\\mathcal{O}(100)}$, posing a significant problem for exhaustive defense strategies. We show several immediate consequences of this: adversarial attacks that change the resulting class based on their intensity, and scale-independent adversarial examples. To demonstrate the redundancy and richness of class decision in the pixel space, we look for its two-dimensional sections that trace images and spell words using particular classes. We also show that ensembling reduces susceptibility to multi-attacks, and that classifiers trained on random labels are more susceptible."
    },
    {
        "title": "Online Agglomerative Pooling for Scalable Self-Supervised Universal Segmentation",
        "link_suffix": "/forum?id=d32d9fE5lG",
        "link": "https://openreview.net/forum?id=d32d9fE5lG",
        "pdf_link": "https://openreview.net/pdf?id=d32d9fE5lG",
        "keywords": "self-supervised learning, universal image segmentation, zero-shot segmentation, graph pooling",
        "abstract": "Recent self-supervised image segmentors have achieved promising zero-shot performance. However, their pretraining schedule is multi-stage and alternates between offline pseudo-masks generation and parameters update, which leads to unstable training and sub-optimal solution.To solve this issue, we present Online Agglomerative Pooling (OAP) that allows efficiently generating universal pseudo-masks and updating parameters simultaneously at each training step.Specifically, OAP contains a stack of instance pooling and semantic pooling layers. By using a layer-varied threshold, OAP can generate multi-hierarchy masks that can provide more visual details for segmentation. Compared with MaskCut or Divide-Conquer, each OAP layer can identify connected nodes in parallel, thus can generate universal pseudo-masks for a single image within tens of milliseconds.Moreover, to deploy OAP in online pretraining, we devise a teacher-student framework with Query-wise Self-distillation, where the local view queries are each aligned with the matched global view queries to learn the local-to-global correspondence.Compared with other multi-stage offline pretraining methods, our framework can effectively scale to larger datasets while ensuring quicker convergence. Extensive experiments on the COCO, PASCAL VOC, Cityscapes, and UVO datasets show that our method achieves state-of-the-art performance on zero-shot instance segmentation, semantic segmentation, and panoptic segmentation. Our code and\npretrained models shall be released upon acceptance of this work."
    },
    {
        "title": "Real-time design of architectural structures with differentiable mechanics and neural networks",
        "link_suffix": "/forum?id=Tpjq66xwTq",
        "link": "https://openreview.net/forum?id=Tpjq66xwTq",
        "pdf_link": "https://openreview.net/pdf?id=Tpjq66xwTq",
        "keywords": "Differentiable physics, mechanical design, physics-in-the-loop neural networks, inverse problems, architectural structures",
        "abstract": "Designing mechanically efficient geometry for architectural structures like shells, towers, and bridges is an expensive iterative process.\nExisting techniques for solving such inverse mechanical problems rely on traditional direct optimization methods, which are slow and computationally expensive, limiting iteration speed and design exploration.\nNeural networks would seem to offer a solution, via data-driven amortized optimization for specific design tasks, but they often require extensive fine-tuning and cannot ensure that important design criteria, such as mechanical integrity, are met.\nIn this work, we combine neural networks with a differentiable mechanics simulator to develop a model that accelerates the solution of shape approximation problems for architectural structures modeled as bar systems.\nAs a result, our model offers explicit guarantees to satisfy mechanical constraints while generating designs that match target geometries.\nWe validate our model in two tasks, the design of masonry shells and cable-net towers.\nOur model achieves better accuracy and generalization than fully neural alternatives, and comparable accuracy to direct optimization but in real time, enabling fast and sound design exploration.\nWe further demonstrate the real-world potential of our trained model by deploying it in 3D modeling software and by fabricating a physical prototype.\nOur work opens up new opportunities for accelerated physical design enhanced by neural networks for the built environment."
    },
    {
        "title": "Understanding Chain-of-Thought in LLMs Through Information Theory",
        "link_suffix": "/forum?id=ouRX6A8RQJ",
        "link": "https://openreview.net/forum?id=ouRX6A8RQJ",
        "pdf_link": "https://openreview.net/pdf?id=ouRX6A8RQJ",
        "keywords": "Large Language models, Chain-of-thought",
        "abstract": "Large Language Models (LLMs) have shown impressive performance in complex reasoning tasks through the use of Chain-of-Thought (CoT) reasoning, allowing models to break down problems into manageable sub-tasks. However, existing CoT evaluation techniques either require annotated CoT data or fall short in accurately assessing intermediate reasoning steps, leading to high rates of false positives. In this paper, we formalize CoT reasoning in LLMs through an information-theoretic lens. Specifically, our framework quantifies the `information gain' at each reasoning step, enabling the identification of failure modes in LLMs without the need for expensive annotated datasets. We demonstrate the efficacy of our approach through extensive experiments on toy and GSM-8K data, where it significantly outperforms existing outcome-based methods by providing more accurate insights into model performance on individual tasks."
    },
    {
        "title": "Learning Transferable Sub-goals by Hypothesizing Generalizing Features",
        "link_suffix": "/forum?id=OvrmA3GMiX",
        "link": "https://openreview.net/forum?id=OvrmA3GMiX",
        "pdf_link": "https://openreview.net/pdf?id=OvrmA3GMiX",
        "keywords": "hierarchical reinforcement learning",
        "abstract": "Although transfer is a key promise of hierarchical reinforcement learning, current methods discover nontransferable skills.\nTypically, skills are defined over all state features simultaneously, preventing generalization as some state features reliably support generalization while others do not.\nFor an agent to effectively transfer a skill it must identify features that generalize and define the skill over this subset.\nHowever, this task is under-specified as the agent has no prior knowledge of what future tasks may be introduced.\nSince successful transfer requires a skill to reliably achieve a sub-goal from different states, we focus our attention on ensuring sub-goals are represented in a transferable way. \nFor each sub-goal, we train an ensemble of classifiers while explicitly incentivizing them to use minimally overlapping features.\nEach ensemble member represents a unique hypothesis about the transferable features of a sub-goal that the agent can use to learn a skill in previously unseen portions of the environment.\nEnvironment reward then determines which hypothesis is most transferable for the given task, based on the intuition that transferable sub-goals lead to better reward maximization.\nWe apply these reusable sub-goals to MiniGrid and Montezuma's Revenge, allowing us to relearn previously defined skills in unseen parts of the state-space."
    },
    {
        "title": "AI-generated faces influence gender stereotypes and racial homogenization",
        "link_suffix": "/forum?id=HhefvT4ktU",
        "link": "https://openreview.net/forum?id=HhefvT4ktU",
        "pdf_link": "https://openreview.net/pdf?id=HhefvT4ktU",
        "keywords": "AI bias, gender stereotypes, racial homogenization, stable diffusion, inclusive AI-generated faces, Text-to-image generative AI",
        "abstract": "Text-to-image generative AI models such as Stable Diffusion are used daily by millions worldwide. However, the extent to which these models exhibit racial and gender stereotypes is not yet fully understood. Here, we document significant biases in Stable Diffusion across six races, two genders, 32 professions, and eight attributes. Additionally, we examine the degree to which Stable Diffusion depicts individuals of the same race as being similar to one another. This analysis reveals significant racial homogenization, e.g., depicting nearly all middle eastern men as dark-skinned, bearded, and wearing a traditional headdress. We then propose novel debiasing solutions that address the above stereotypes. Finally, using a preregistered experiment, we show that being presented with inclusive AI-generated faces reduces people's racial and gender biases, while being presented with non-inclusive ones increases such biases. This persists regardless of whether the images are labeled as AI-generated. Taken together, our findings emphasize the need to address biases and stereotypes in AI-generated content."
    },
    {
        "title": "DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models",
        "link_suffix": "/forum?id=JUr0YOMvZA",
        "link": "https://openreview.net/forum?id=JUr0YOMvZA",
        "pdf_link": "https://openreview.net/pdf?id=JUr0YOMvZA",
        "keywords": "Vision-Language Models (VLMs), Hallucinations, Decoding Method, Momentum Techniques",
        "abstract": "Large Vision-Language Models (VLMs) exhibit significant potential in multimodal tasks but often struggle with hallucinations—responses that are plausible yet visually ungrounded. In this work, we investigate the layer-wise prediction tendencies of VLMs and conduct an in-depth analysis of their decoding mechanism. We observe that VLMs tend to ``overthink'' during the final stages of decoding, making significant prediction shifts in the last few layers often favoring incorrect results, which leads to a surge in hallucinative outputs.Leveraging this localized pattern, we propose a novel decoding strategy inspired by the momentum analogy used in gradient descent-based optimizers. Our method enforces decoding consistency across layers in an adaptive manner during forward passes—an under-explored approach in existing works. This strategy significantly improves the reliability and performance of VLMs in various multimodal tasks, while introducing only negligible efficiency overhead."
    },
    {
        "title": "Efficient Reinforcement Learning for Global Decision Making in the Presence of Local Agents at Scale",
        "link_suffix": "/forum?id=THOgGo8SX7",
        "link": "https://openreview.net/forum?id=THOgGo8SX7",
        "pdf_link": "https://openreview.net/pdf?id=THOgGo8SX7",
        "keywords": "Reinforcement Learning, Multi-agent Systems, Large-scale Systems, Mean-field Approximation",
        "abstract": "We study reinforcement learning for global decision-making in the presence of local agents, where the global decision-maker makes decisions affecting all local agents, and the objective is to learn a policy that maximizes the joint rewards of all the agents. Such problems find many applications, e.g. demand response, EV charging, queueing, etc. In this setting, scalability has been a long-standing challenge due to the size of the state space which can be exponential in the number of agents. This work proposes the SUBSAMPLE-Q algorithm where the global agent subsamples $k\\leq n$ local agents to compute a policy in time that is polynomial in $k$. We show that this learned policy converges to the optimal policy in the order of $\\tilde{O}(1/\\sqrt{k}+{\\epsilon}{k,m})$ as the number of sub-sampled agents $k$ increases, where ${\\epsilon}{k,m}$ is the Bellman noise. Finally, we validate the theory through numerical simulations in a demand-response setting and a queueing setting."
    },
    {
        "title": "Is What You Ask For What You Get?  Investigating Concept Associations in Text-to-Image Models",
        "link_suffix": "/forum?id=0RUQmLFF1D",
        "link": "https://openreview.net/forum?id=0RUQmLFF1D",
        "pdf_link": "https://openreview.net/pdf?id=0RUQmLFF1D",
        "keywords": "text-to-image, vision-language, computer vision, interpretability, alignment, fairness, safety",
        "abstract": "Text-to-image (T2I) models are increasingly used in impactful real-life applications. As such, there is a growing need to audit these models to ensure that they generate desirable, task-appropriate images. However, systematically inspecting the associations between prompts and generated content in a human-understandable way remains challenging. To address this, we propose Concept2Concept, a framework where we characterize conditional distributions of vision language models using interpretable concepts and metrics that can be defined in terms of these concepts. This characterization allows us to use our framework to audit models and prompt-datasets. To demonstrate, we investigate several case studies of conditional distributions of prompts, such as user defined distributions or empirical, real world distributions. Lastly, we implement Concept2Concept as an open-source interactive visualization tool facilitating use by non-technical end-users. *Warning: This paper contains discussions of harmful content, including child sexual abuse material and NSFW\nmaterial, which may be disturbing to some readers."
    },
    {
        "title": "Enhancing Variational Quantum Algorithms: Effective Quantum Ansatz Design Using GFlowNets",
        "link_suffix": "/forum?id=XrwsdcgWKc",
        "link": "https://openreview.net/forum?id=XrwsdcgWKc",
        "pdf_link": "https://openreview.net/pdf?id=XrwsdcgWKc",
        "keywords": "quantum computing, GFlowNets, Classical-Quantum Hybrid Algorithms, Variational Quantum Algorithms (VQAs)",
        "abstract": "Quantum computing promises significant computational advantages over classical computing. However, current devices are constrained by a limited qubit count and noise. By combining classical optimization methods with parameterized quantum circuits, Variational Quantum Algorithms (VQAs) offer a potential solution for noisy intermediate-scale quantum systems (NISQ). This makes VQAs particularly promising strategies for achieving near-term quantum advantages; such approaches are now widely explored for nearly all quantum computing applications. However, designing effective parameterized circuits, also known as ansatz, remains challenging. In this work, we introduce the use of GFlowNets as an efficient method to automate the development of efficient ansatz for various quantum computing problems. Our approach leverages GFlowNets to efficiently explore the combinatorial space of parameterized quantum circuits. Our extensice experiments demonstrate that GFlowNets can discover ansatz with an order of magnitude fewer parameters, gate counts, and depths compared to current approaches for the molecular electronic ground state energy problem. We also apply our approach to the unweighted Max-Cut problem, where we observe similar improvements in circuit efficiency. These results highlight the potential of GFlowNets to significantly reduce the resource requirements of VQAs while maintaining or improving solution quality."
    },
    {
        "title": "Models That Prove Their Own Correctness",
        "link_suffix": "/forum?id=5WtovCb1ZE",
        "link": "https://openreview.net/forum?id=5WtovCb1ZE",
        "pdf_link": "https://openreview.net/pdf?id=5WtovCb1ZE",
        "keywords": "Trustworthy ML, Transformers, Interactive Proofs, Theory",
        "abstract": "How can we trust the correctness of a learned model on a particular input of interest? Model accuracy is typically measuredon averageover a distribution of inputs, giving no guarantee for any fixed input. This paper proposes a theoretically-founded solution to this problem: to trainSelf-Proving modelsthat prove the correctness of their output to a verification algorithm $V$ via an Interactive Proof. We devise a generic method for learning Self-Proving models, and we prove convergence bounds under certain assumptions. Empirically, our learning method is used to train a Self-Proving transformer that computes the Greatest Common Divisor (GCD)andproves the correctness of its answer."
    },
    {
        "title": "Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics",
        "link_suffix": "/forum?id=2e4ECh0ikn",
        "link": "https://openreview.net/forum?id=2e4ECh0ikn",
        "pdf_link": "https://openreview.net/pdf?id=2e4ECh0ikn",
        "keywords": "Turn-taking, Conversation AI, Audio Foundation Models, Evaluation Metric, Evaluation Benchmark",
        "abstract": "The recent wave of audio foundation models (FMs) could provide new capabilities for conversational modeling. However, there have been limited efforts to evaluate these audio FMs comprehensively on their ability to have natural and interactive conversations. To engage in meaningful conversation with the end user, we would want the FMs to additionally perform a fluent succession of turns without too much overlapping speech or long stretches of silence. Inspired by this, we ask whether the recently proposed audio FMs can understand, predict, and perform turn-taking events? To answer this, we propose a novel evaluation protocol that can assess spoken dialog system's turn-taking capabilities using a supervised model as a judge that has been trained to predict turn-taking events in human-human conversations. Using this protocol, we present the first comprehensive user study that evaluates existing spoken dialogue systems on their ability to perform turn-taking events and reveal many interesting insights, such as they sometimes do not understand when to speak up, can interrupt too aggressively and rarely backchannel. We further evaluate multiple open-source and proprietary audio FMs accessible through APIs on carefully curated test benchmarks from Switchboard to measure their ability to understand and predict turn-taking events and identify significant room for improvement. We will open source our evaluation platform to promote the development of advanced conversational AI systems."
    },
    {
        "title": "RL2Grid: Benchmarking Reinforcement Learning in Power Grid Operations",
        "link_suffix": "/forum?id=7J2C4QnQrl",
        "link": "https://openreview.net/forum?id=7J2C4QnQrl",
        "pdf_link": "https://openreview.net/pdf?id=7J2C4QnQrl",
        "keywords": "Reinforcement Learning, Power Grids, Benchmark",
        "abstract": "Reinforcement learning (RL) has the potential to transform power grid operations by providing adaptive, scalable controllers essential for decarbonization and grid resilience. However, despite their promise, today's RL methods struggle to deal with complex dynamics, aleatoric uncertainty, long-horizon goals, and hard physical constraints, hindering their application in power grids and other real-world settings.\nIn this work, we present RL2Grid, a benchmark representing realistic power grid operations that aims to foster the maturity of RL methods.\nThis work builds upon Grid2Op, a power grid simulation framework developed by RTE France, to provide standardized tasks, state and action spaces, and rewards within a common interface, and thereby provide a common basis for monitoring and promoting progress. We evaluate and compare widely adopted RL algorithms across the increasingly complex grid settings represented within RL2Grid, establishing reference performance metrics and offering insights into the effectiveness of different approaches (including pure RL approaches and hybrid approaches incorporating heuristics). Our findings indicate that power grids present substantial challenges for modern RL, underscoring the need for novel methods capable of dealing with complex real-world physical systems."
    },
    {
        "title": "Mind the GAP: Glimpse-based Active Perception improves generalization and sample efficiency of visual reasoning",
        "link_suffix": "/forum?id=iXCeQ2m6vT",
        "link": "https://openreview.net/forum?id=iXCeQ2m6vT",
        "pdf_link": "https://openreview.net/pdf?id=iXCeQ2m6vT",
        "keywords": "visual reasoning, active vision, out-of-distribution, generalization, sample efficiency, relational features, brain-inspired, neuro-inspired",
        "abstract": "Human capabilities in understanding visual relations are far superior to those of AI systems, especially for previously unseen objects. For example, while AI systems struggle to determine whether two such objects are visually the same or different, humans can do so with ease. Active vision theories postulate that the learning of visual relations is grounded in actions that we take to fixate objects and their parts by moving our eyes. In particular, the low-dimensional spatial information about the corresponding eye movements is hypothesized to facilitate the representation of relations between different image parts. Inspired by these theories, we develop a system equipped with a novel Glimpse-based Active Perception (GAP) that sequentially glimpses at the most salient regions of the input image and processes them at high resolution. Importantly, our system leverages the locations stemming from the glimpsing actions, along with the visual content around them, to represent relations between different parts of the image. The results suggest that the GAP is essential for extracting visual relations that go beyond the immediate visual content. Our approach reaches state-of-the-art performance on several visual reasoning tasks being more sample-efficient, and generalizing better to out-of-distribution visual inputs than prior models."
    },
    {
        "title": "Replay concurrently or sequentially?  A theoretical perspective on replay in continual learning",
        "link_suffix": "/forum?id=nSYycd5tEC",
        "link": "https://openreview.net/forum?id=nSYycd5tEC",
        "pdf_link": "https://openreview.net/pdf?id=nSYycd5tEC",
        "keywords": "Continual Learning, replay-based methods, catastrophic forgetting",
        "abstract": "Replay-based methods have shown superior performance to address catastrophic forgetting in continual learning (CL), where a subset of past data is stored and  generally replayed together with new data in current task learning. While seemingly natural, it is questionable, though rarely questioned, if such a concurrent replay strategy is always the right way for replay in CL. Inspired by the fact in human learning that revisiting very different courses sequentially before final exams is more effective for students, an interesting open question to ask is whether a sequential replay can benefit CL more compared to a standard concurrent replay. However, answering this question is highly nontrivial considering a major lack of theoretical understanding in replay-based CL methods. To this end, we investigate CL in overparameterized linear models and provide a comprehensive theoretical analysis to compare two replay schemes: 1) Concurrent Replay, where the model is trained on replay data and new data concurrently; 2) Sequential Replay, where the model is trained first on new data and then sequentially on replay data for each old task. By characterizing the explicit form of forgetting and generalization error, we show in theory that sequential replay tends to outperform  concurrent replay when tasks  are less similar, which is corroborated by our simulations in linear models. More importantly, our results inspire a novel design of a hybrid replay method, where only replay data of similar tasks are used concurrently with the current data and dissimilar tasks are sequentially revisited using their replay data. As depicted in our experiments on real datasets using deep neural networks, such a hybrid replay method improves the performance of standard concurrent replay by leveraging sequential replay for dissimilar tasks. By providing the first comprehensive theoretical analysis on replay, our work has great potentials to open up more principled designs for replay-based CL."
    },
    {
        "title": "Libra: Leveraging Temporal Images for Biomedical Radiology Analysis",
        "link_suffix": "/forum?id=Rwj3i0xJiU",
        "link": "https://openreview.net/forum?id=Rwj3i0xJiU",
        "pdf_link": "https://openreview.net/pdf?id=Rwj3i0xJiU",
        "keywords": "Radiology Report Generation, Chest X-rays, Visual Language Model, Large Language Models, Image-Text Alignment, Medical AI",
        "abstract": "Radiology report generation (RRG) is a challenging task, as it requires a thorough understanding of medical images, integration of multiple temporal inputs, and accurate report generation. Effective interpretation of medical images, such as chest X-rays (CXRs), demands sophisticated visual-language reasoning to map visual findings to structured reports. Recent studies have shown that visual language models (VLMs) can acquire multimodal capabilities by aligning with pre-trained vision encoders. However, current approaches predominantly focus on single-image analysis and overlook the essential temporal information gained from comparing current images with prior ones. To overcome this critical limitation, we introduce Libra, a temporal-aware VLM tailored for CXR report generation using temporal imaging data. Libra integrates a radiology-specific image encoder with a VLM and utilises a novel Temporal Alignment Connector (TAC) to capture and synthesise temporal information of images across different time points with unprecedented precision. Extensive experiments show that Libra achieves new state-of-the-art performance among the same parameter scale VLMs for RRG tasks on the MIMIC-CXR. Specifically, Libra improves the RadCliQ metric by 12.9% and makes substantial gains across all lexical metrics compared to previous models."
    },
    {
        "title": "Gradient-free training of recurrent neural networks",
        "link_suffix": "/forum?id=vcJiPLeC48",
        "link": "https://openreview.net/forum?id=vcJiPLeC48",
        "pdf_link": "https://openreview.net/pdf?id=vcJiPLeC48",
        "keywords": "recurrent neural networks, koopman operator, random feature networks",
        "abstract": "Recurrent neural networks are a successful neural architecture for many time-dependent problems, including time series analysis, forecasting, and modeling of dynamical systems. Training such networks with backpropagation through time is a notoriously difficult problem because their loss gradients tend to explode or vanish. In this contribution, we introduce a computational approach to construct all weights and biases of a recurrent neural network without using gradient-based methods. The approach is based on a combination of random feature networks and Koopman operator theory for dynamical systems. The hidden parameters of a single recurrent block are sampled at random, while the outer weights are constructed using extended dynamic mode decomposition. This approach alleviates all problems with backpropagation commonly related to recurrent networks. The connection to Koopman operator theory also allows us to start using results in this area to analyze recurrent neural networks. In computational experiments on time series, forecasting for chaotic dynamical systems, and control problems, as well as on weather data, we observe that the training time and forecasting accuracy of the recurrent neural networks we construct are improved when compared to commonly used gradient-based methods."
    },
    {
        "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
        "link_suffix": "/forum?id=zl3pfz4VCV",
        "link": "https://openreview.net/forum?id=zl3pfz4VCV",
        "pdf_link": "https://openreview.net/pdf?id=zl3pfz4VCV",
        "keywords": "natural language processing, benchmark, sentence embeddings, multilingual",
        "abstract": "Text embeddings are typically evaluated on a narrow set of tasks, limited in terms of languages, domains, and task types. To circumvent this limitation and to provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark (MMTEB) -- a large-scale community-driven initiative expanding MTEB to over 500 \\textit{quality controlled} evaluation tasks across 1,000+ languages. MMTEB includes a wide range of challenging novel tasks such as instruction following, long-document retrieval, and code retrieval, and represents the largest multilingual collection of evaluation tasks for embedding models to date. We use this collection to construct multiple highly multilingual benchmarks. We evaluate a representative set of models on these benchmarks.\nOur findings indicate that, while LLM-based models can achieve state-of-the-art performance on a subset of languages, the best-performing publicly available model across languages is the notably smaller, multilingual-e5-large-instruct.Massive benchmarks often impose high computational demands, limiting accessibility, particularly for low-resource communities. To address this, we downsample tasks based on inter-task correlation (i.e., selecting only a diverse set of tasks) while preserving relative rankings.\nWe further optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks at a significantly lower computational cost. For instance, we introduce a new zero-shot English benchmark that maintains a similar ordering at a fraction of the cost."
    },
    {
        "title": "Worldcraft",
        "link_suffix": "/forum?id=cnLNpIRPuF",
        "link": "https://openreview.net/forum?id=cnLNpIRPuF",
        "pdf_link": "https://openreview.net/pdf?id=cnLNpIRPuF",
        "keywords": "Exchangeability, Hierachy, 3D world Generation",
        "abstract": "We present Worldcraft, a hybrid implicit method for generating vast, interactive 3D worlds at unprecedented scale and speed by modeling them as exchangeable sequences of latent 3D objects. In contrast to existing methods that produce limited scenes, Worldcraft's novel approach constructs expansive environments comprising thousands of elements, extending to over a million objects in seconds, on a single GPU. The resulting created worlds\n are defined in terms of possessing certain essential properties: Object Individuality, Collective Semantics, and Expandability. To achieve this with both speed and scale, we conceptualize world generation as a set generation problem, introducing three key technical innovations: (i) Hierarchical and Exchangeable Sequence Modeling ensures Object Individuality while capturing Collective Semantics; (ii) Hybrid Implicit Generation Method enables rapid creation of vast worlds, supporting both Scale and Expandability; and (iii) Multi-level Indexing Functions allow efficient manipulation across scales, reinforcing Collective Semantics and enabling on-demand generation for Speed and Expandability. We demonstrate Worldcraft's capabilities using Minecraft as a test-bed, generating complex, interactive environments that users can explore.  However, this approach is applicable to any suitable platform, potentially revolutionizing various applications in 3D environment generation."
    },
    {
        "title": "Exploiting Topology of Protein Language Model Attention Maps for Token Classification",
        "link_suffix": "/forum?id=DWa1bATAot",
        "link": "https://openreview.net/forum?id=DWa1bATAot",
        "pdf_link": "https://openreview.net/pdf?id=DWa1bATAot",
        "keywords": "protein language models, protein property prediction, topological data analysis, attention maps, transformers",
        "abstract": "In this paper, we introduce a method to extract topological features from transformer-based protein language models. Our method leverages the persistent homology of attention maps to generate features for token (per amino-acid) classification tasks and demonstrate its relevance in a biological context. We implement our method on transformer-based protein language models using the family of ESM-2 models. Specifically, we demonstrate that minimum spanning trees, derived from attention matrices, encode structurally significant information about proteins. In our experiments, we combine these topological features with standard embeddings from ESM-2. Our method outperforms traditional approaches and other transformer-based methods with a similar number of parameters in several binding site identification tasks and achieves state-of-the-art performance in conservation prediction tasks. Our results highlight the potential of this hybrid approach in advancing the understanding and prediction of protein functions."
    },
    {
        "title": "Hyperbolic Fine-tuning for Large Language Models",
        "link_suffix": "/forum?id=T7xIs9Z1Fm",
        "link": "https://openreview.net/forum?id=T7xIs9Z1Fm",
        "pdf_link": "https://openreview.net/pdf?id=T7xIs9Z1Fm",
        "keywords": "hyperbolic space, representation learning, hyperbolicity, curvature, fine-tunning, large language models, low-rank adaptation",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs. In this study, we first investigate the non-Euclidean characteristics of LLMs. Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. Building on the observation, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. However, we found that this fine-tuning in hyperbolic space cannot be achieved with naive application of exponential and logarithmic maps, when the embedding and weight matrices both reside in Euclidean space. To address this technique issue, we introduce a new method called hyperbolic low-rank efficient fine-tuning, \\method, that performs low-rank adaptation directly on the hyperbolic manifold, avoiding the cancellation effect caused by the exponential and logarithmic maps, thus preserving the hyperbolic modeling capabilities. Through extensive experiments, we demonstrate that \\method significantly enhances the performance of LLMs on reasoning tasks, particularly for complex reasoning problems. In particular, \\method improves the performance in the complex AQuA dataset by up to 13.0%, showcasing its effectiveness in handling complex reasoning challenges."
    },
    {
        "title": "Temporal Environment-Aware Image Generation via Latent Diffusion",
        "link_suffix": "/forum?id=Rd34VDwqB7",
        "link": "https://openreview.net/forum?id=Rd34VDwqB7",
        "pdf_link": "https://openreview.net/pdf?id=Rd34VDwqB7",
        "keywords": "Generative models, Latent Diffusion, Time series data, Multi-modal learning",
        "abstract": "Low-cost cameras have recently become widely used to monitor environmental ecosystems. This paper focuses on scene prediction for monitoring small streams, which is critical for ensuring water supply and informing early actions for floods and droughts. In contrast to traditional stream models that typically rely on coarse-resolution weather data, stream images provide detailed information about water properties and local environment at a higher temporal frequency. This paper presents a multi-modal generative framework designed for frequent temporal stream imagery datasets, aimed at generating the subsequent stream images. This task is challenging due to the variability of stream images caused by changes in time and local environmental conditions. Our method captures scene changes in both stream and surrounding environment by incorporating temporal context of weather, water flow, and time information. We also introduce a domain-discriminative learning approach to enforce the learning of domain-specific information in generating images. Our experiments demonstrate the superior performance of the proposed method in preserving semantics of water and environmental properties, using real data from the West Brook area in western Massachusetts, USA."
    },
    {
        "title": "TexTailor: Customized Text-aligned Texturing via Effective Resampling",
        "link_suffix": "/forum?id=1NprT9Kz0d",
        "link": "https://openreview.net/forum?id=1NprT9Kz0d",
        "pdf_link": "https://openreview.net/pdf?id=1NprT9Kz0d",
        "keywords": "3D texture synthesis, diffusion model, resampling",
        "abstract": "We present TexTailor, a novel method for generating consistent object textures from textual descriptions. Existing text-to-texture synthesis approaches utilize depth-aware diffusion models to progressively generate images and synthesize textures across predefined multiple viewpoints. However, these approaches lead to a gradual shift in texture properties across viewpoints due to (1) insufficient integration of previously synthesized textures at each viewpoint during the diffusion process and (2) the autoregressive nature of the texture synthesis process. Moreover, the predefined selection of camera positions, which does not account for the object's geometry, limits the effective use of texture information synthesized from different viewpoints, ultimately degrading overall texture consistency. In TexTailor, we address these issues by (1) applying a resampling scheme that repeatedly integrates information from previously synthesized textures within the diffusion process, and (2) fine-tuning a depth-aware diffusion model on these resampled textures. During this process, we observed that using only a few training images restricts the model's original ability to generate high-fidelity images aligned with the conditioning, and therefore propose an originality preservation loss to mitigate this issue. Additionally, we enhance the synthesis of natural textures by adaptively adjusting camera positions based on the object's geometry. Experiments on a subset of the Objaverse dataset and the ShapeNet car dataset demonstrate that TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures."
    },
    {
        "title": "Offline vs. Online Learning in Model-based RL: Lessons for Data Collection Strategies",
        "link_suffix": "/forum?id=fo5IUCMoFg",
        "link": "https://openreview.net/forum?id=fo5IUCMoFg",
        "pdf_link": "https://openreview.net/pdf?id=fo5IUCMoFg",
        "keywords": "Model-based reinforcement learning, Offline learning, Online learning, Active learning, Exploration",
        "abstract": "Data collection is crucial for learning robust world models in model-based reinforcement learning.\nThe most prevalent strategies are to actively collect trajectories by interacting with the environment during online training or training on offline datasets.\nAt first glance, the nature of learning task-agnostic environment dynamics makes world models a good candidate for effective offline training. However, the effects of online vs. offline data on world models and thus on the resulting task performance have not been thoroughly studied in the literature. In this work, we investigate both paradigms in model-based settings, conducting experiments on 31 different environments.\nFirst, we showcase that online agents outperform their offline counterparts.\nWe identify a key challenge behind performance degradation of offline agents: encountering Out-of-Distribution states at test time.\nThis issue arises because the data collected online primarily benefits the online agent by learning from its own mistakes, but it leaves many states unvisited.\nAs a result, the offline agent suffers from insufficient coverage of the state space.\nWe demonstrate that this issue can be mitigated by allowing for additional online interactions in a fixed or adaptive schedule, restoring the performance of online training with limited interaction data.\nWe also showcase that incorporating exploration data helps mitigate the performance degradation of offline agents. Based on our insights, we recommend adding exploration data when collecting large datasets, as current efforts predominantly focus on expert data alone."
    }
]