[{"title": "There and Back Again: On the relation between noises, images, and their inversions in diffusion models", "link_suffix": "/forum?id=Dgh5GXsW65", "link": "https://openreview.net/forum?id=Dgh5GXsW65", "pdf_link": "https://openreview.net/pdf?id=Dgh5GXsW65", "keywords": "diffusion models, latent space, ddim, generative models", "abstract": "Denoising Diffusion Probabilistic Models (DDPMs) achieve state-of-the-art performance in synthesizing new images from random noise, but they lack meaningful latent space that encodes data into features. Recent DDPM-based editing techniques try to mitigate this issue by inverting images back to their approximated staring noise. In this work, we study the relation between the initial Gaussian noise, the samples generated from it, and their corresponding latent encodings obtained through the inversion procedure. First, we interpret their spatial distance relations to show the inaccuracy of the DDIM inversion technique by localizing latent representations manifold between the initial noise and generated samples. Then, we demonstrate the peculiar relation between initial Gaussian noise and its corresponding generations during diffusion training, showing that the high-level features of generated images stabilize rapidly, keeping the spatial distance relationship between noises and generations consistent throughout the training.", "title_embedding_index": 11950, "title_abs_embedding_index": 11975}, {"title": "Model-agnostic meta-learners for estimating heterogeneous treatment effects over time", "link_suffix": "/forum?id=QGGNvKaoIU", "link": "https://openreview.net/forum?id=QGGNvKaoIU", "pdf_link": "https://openreview.net/pdf?id=QGGNvKaoIU", "keywords": "causal inference, treatment effect estimation, CATE, doubly robust learning", "abstract": "Estimating heterogeneous treatment effects (HTEs) over time is crucial in many disciplines such as personalized medicine. Existing works for this task have mostly focused onmodel-basedlearners that adapt specific machine-learning models and adjustment mechanisms. In contrast, model-agnostic learners - so-calledmeta-learners- are largely unexplored. In our paper, we propose several meta-learners that are model-agnostic and thus can be used in combination with arbitrary machine learning models (e.g., transformers) to estimate HTEs over time. We then provide a comprehensive theoretical analysis that characterizes the different learners and that allows us to offer insights into when specific learners are preferable. Furthermore, we propose a novel IVW-DR-learner that (i) uses a doubly robust (DR) and orthogonal loss; and (ii) leverages inverse-variance weights (IVWs) that we derive to stabilize the DR-loss over time. Our IVWs down weight extreme trajectories due toproductsof inverse-propensities in the DR-loss, resulting in a lower estimation variance. Our IVW-DR-learner achieves superior performance in our experiments, particularly in regimes with low overlap and long time horizons.", "title_embedding_index": 11951, "title_abs_embedding_index": 11976}, {"title": "MAQL: Speeding up Q-learning with a model-assist", "link_suffix": "/forum?id=Pr4JkJVlmz", "link": "https://openreview.net/forum?id=Pr4JkJVlmz", "pdf_link": "https://openreview.net/pdf?id=Pr4JkJVlmz", "keywords": "bandits, reinforcement learning", "abstract": "In reinforcement learning, model free methods such as Q-learning and policy gradient are extremely popular due to their simplicity but require a huge amount of data for training. Model based methods on the other hand, are proven to be sample efficient in various environments but are unfortunately computationally expensive. It is therefore only prudent to investigate and design algorithms that have best of features from both these classes of algorithms. In this work, we propose MAQL, a model-assisted Q-learning algorithm that is not only computationally inexpensive but also offers low sample complexity. We illustrate its superior performance to vanilla Q-learning in various RL environments and particularly demonstrate its utility in learning the Gittins/Whittles index in Rested/Restless Bandits respectively. We aim to spur discussion on how model-assists can help boost the performance of existing RL algorithms.", "title_embedding_index": 11952, "title_abs_embedding_index": 11977}, {"title": "Beatrix: Out-of-Distribution Generalization of Large EEG Model via Invariant Contrastive Fine-Tuning", "link_suffix": "/forum?id=IjBndR92Zy", "link": "https://openreview.net/forum?id=IjBndR92Zy", "pdf_link": "https://openreview.net/pdf?id=IjBndR92Zy", "keywords": "EEG; representation learning; parameter-efficient fine-tuning; domain generalization; seizure diagnosis;", "abstract": "The advent of large-scale foundation models has revolutionized EEG analysis; however, their ability to generalize to Out-of-Distribution (OoD) brain signals remains limited due to the inherent variability in physiological states, individual differences, and experimental setups. To address these challenges, we introduce Beatrix, a novel spectral EEG foundation model that achieves state-of-the-art OoD generalization across diverse brain activity tasks. Beatrix leverages a unique analytic wavelet-based spectral tokenization that captures the intricate non-stationary dynamics of EEG signals, and employs a semi-causal generative modeling approach during pre-training, enabling it to learn expressive latent representations capable of both interpolation and extrapolation across temporal and frequency domains. For fine-tuning, we propose an innovative Contrastive Invariant Fine-Tuning (CIFT) method that enhances domain-invariant learning without the need for explicit environment labels, thus significantly improving OoD generalizability in a parameter-efficient manner. Our multi-view Transformer architecture further integrates both spectral and temporal information, allowing Beatrix to comprehensively model EEG signals across channels. Extensive experiments demonstrate that Beatrix consistently outperforms existing EEG models in tasks such as seizure detection and forecasting, auditory neural decoding, motor imagery, and sleep staging, showcasing its robustness and broad applicability. By achieving superior performance with reduced fine-tuning costs, Beatrix represents a significant advancement in the field of EEG foundation models.", "title_embedding_index": 11953, "title_abs_embedding_index": 11978}, {"title": "F3Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos", "link_suffix": "/forum?id=vlg5WRKHxh", "link": "https://openreview.net/forum?id=vlg5WRKHxh", "pdf_link": "https://openreview.net/pdf?id=vlg5WRKHxh", "keywords": "temporal event spotting, fine-grained video understanding, video analytics", "abstract": "Analyzing Fast, Frequent, and Fine-grained ($F^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the $F^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies. To advance research in video understanding, we introduce $F^3Set$, a benchmark that consists of video datasets for precise $F^3$ event detection. Datasets in $F^3Set$ are characterized by their extensive scale and comprehensive detail, usually encompassing over 1,000 event types with precise timestamps and supporting multi-level granularity. Currently, $F^3Set$ contains several sports datasets, and this framework may be extended to other applications as well. We evaluated popular temporal action understanding methods on $F^3Set$, revealing substantial challenges for existing techniques. Additionally, we propose a new method, $F^3ED$, for $F^3$ event detections, achieving superior performance. The dataset, model, and benchmark code are available athttps://github.com/F3Set/F3Set.", "title_embedding_index": 11954, "title_abs_embedding_index": 11979}, {"title": "Training Large Language Models for Retrieval-Augmented Question Answering through Backtracking Correction", "link_suffix": "/forum?id=IOg47mg74i", "link": "https://openreview.net/forum?id=IOg47mg74i", "pdf_link": "https://openreview.net/pdf?id=IOg47mg74i", "keywords": "RAG, CoT, Self-play, Reinforcement Learning", "abstract": "Despite recent progress in Retrieval-Augmented Generation (RAG) achieved by large language models (LLMs), retrievers often recall uncorrelated documents, regarded as \"noise\" during subsequent text generation. To address this, some methods train LLMs to distinguish between relevant and irrelevant documents using labeled data, enabling them to select the most likely relevant ones as context. However, they remain sensitive to noise, as LLMs can easily make mistakes when the selected document is noisy. Some approaches increase the number of referenced documents and train LLMs to perform stepwise reasoning when presented with multiple documents. Unfortunately, these methods rely on extensive and diverse annotations to ensure generalization, which is both challenging and costly. In this paper, we proposeBacktracking Correctionto address these limitations. Specifically, we reformulate stepwise RAG into a multi-step decision-making process. Starting from the final step, we optimize the model through error sampling and self-correction, and then backtrack to the previous state iteratively. In this way, the model's learning scheme follows an easy-to-hard progression: as the target state moves forward, the context space decreases while the decision space increases. Experimental results demonstrate thatBacktracking Correctionenhances LLMs' ability to make complex multi-step assessments, improving the robustness of RAG in dealing with noisy documents.", "title_embedding_index": 11955, "title_abs_embedding_index": 11980}, {"title": "Reasoning with trees: interpreting CNNs using hierarchies", "link_suffix": "/forum?id=fxarGPFMmB", "link": "https://openreview.net/forum?id=fxarGPFMmB", "pdf_link": "https://openreview.net/pdf?id=fxarGPFMmB", "keywords": "interpretability, image classification, hierarchical segmentation", "abstract": "Challenges remain in providing interpretable explanations for neural network reasoning in explainable AI (xAI). Existing methods like Integrated Gradients produce noisy maps, and LIME, while intuitive, may deviate from the model\u2019s reasoning. We introduce a framework that uses hierarchical segmentation techniques for faithful and interpretable explanations of Convolutional Neural Networks (CNNs). Our method constructs model-based hierarchical segmentations that maintain the model\u2019s reasoning fidelity and allow both human-centric and model-centric segmentation. This approach can be combined with various xAI methods and provides multiscale explanations that help identify biases and improve understanding of neural network decision-making. Experiments show that our framework, xAiTrees, delivers highly interpretable and faithful model explanations, not only surpassing traditional xAI methods but shedding new light on a novel approach to enhancing xAI interpretability. Code at:https://anonymous.4open.science/r/reasoning_with_trees-F3E1.", "title_embedding_index": 11956, "title_abs_embedding_index": 11981}, {"title": "INS: Interaction-aware Synthesis to Enhance Offline Multi-agent Reinforcement Learning", "link_suffix": "/forum?id=kxD2LlPr40", "link": "https://openreview.net/forum?id=kxD2LlPr40", "pdf_link": "https://openreview.net/pdf?id=kxD2LlPr40", "keywords": "Multi-agent reinforcement learning, Offline reinforcement learning, Diffusion models", "abstract": "Data scarcity in offline multi-agent reinforcement learning (MARL) is a key challenge for real-world applications. Recent advances in offline single-agent reinforcement learning (RL) demonstrate the potential of data synthesis to mitigate this issue.\nHowever, in multi-agent systems, interactions between agents introduce additional challenges. These interactions complicate the synthesis of multi-agent datasets, leading to data distortion when inter-agent interactions are neglected. Furthermore, the quality of the synthetic dataset is often constrained by the original dataset. To address these challenges, we proposeINteraction-aware Synthesis (INS), which synthesizes high-quality multi-agent datasets using diffusion models. Recognizing the sparsity of inter-agent interactions, INS employs a sparse attention mechanism to capture these interactions, ensuring that the synthetic dataset reflects the underlying agent dynamics. To overcome the limitation of diffusion models requiring continuous variables, INS implements a bit action module, enabling compatibility with both discrete and continuous action spaces. Additionally, we incorporate a select mechanism to prioritize transitions with higher estimated values, further enhancing the dataset quality. Experimental results across multiple datasets in MPE and SMAC environments demonstrate that INS consistently outperforms existing methods, resulting in improved downstream policy performance and superior dataset metrics. Notably, INS can synthesize high-quality data using only 10% of the original dataset, highlighting its efficiency in data-limited scenarios.", "title_embedding_index": 11957, "title_abs_embedding_index": 11982}, {"title": "Knowledge Augmentation: In-context or In-parameter?", "link_suffix": "/forum?id=sl4hOq9wm9", "link": "https://openreview.net/forum?id=sl4hOq9wm9", "pdf_link": "https://openreview.net/pdf?id=sl4hOq9wm9", "keywords": "Representation Learning, In-Parameter Knowledge Injection, Parametric Knowledge Representation, Large Language Models", "abstract": "Generative language models rely on knowledge augmentation to enhance their ability to complete a variety of tasks by incorporating relevant external information. The most common approach is in-context knowledge injection, where the relevant information is appended directly to the model\u2019s input. While straightforward and easy to implement, this approach is limited for complex reasoning tasks due to input length constraints and shallow integration of external and internal knowledge in language models. \nTo overcome these limitations, we introduce an in-parameter knowledge injection method, which temporarily embeds external knowledge into the model's parameters. By injecting knowledge in this way, the language models can access and reason over the information more flexibly, bringing enhanced performance on tasks requiring sophisticated reasoning. \nWe conduct deep explorations of both in-context and in-parameter knowledge injection, highlighting their respective advantages and limitations. Through comprehensive experiments across tasks of varying complexity, we demonstrate that in-parameter knowledge injection is particularly advantageous for complex tasks requiring deep reasoning, while in-context injection remains effective for simpler tasks where the answer can be directly extracted. Our findings provide practical guidance for selecting appropriate knowledge augmentation strategies based on the complexity of the task.We have open-sourced all the code, data, and models in the following anonymous GitHub link:https://anonymous.4open.science/r/In-parameter-Knowledge-Injection/", "title_embedding_index": 11958, "title_abs_embedding_index": 11983}, {"title": "RetCompletion:High-Speed Inference Image Completion with Retentive Network", "link_suffix": "/forum?id=vlpEXfbeHn", "link": "https://openreview.net/forum?id=vlpEXfbeHn", "pdf_link": "https://openreview.net/pdf?id=vlpEXfbeHn", "keywords": "Pluralistic image completion, Retentive Network", "abstract": "Time cost is a major challenge in achieving high-quality pluralistic image completion. Recently, the Retentive Network (RetNet) in natural language processing offers a novel approach to this problem with its low-cost inference capabilities. Inspired by this, we apply RetNet to the pluralistic image completion task in computer vision. We present RetCompletion, a two-stage framework. In the first stage, we introduce Bi-RetNet, a bidirectional sequence information fusion model that integrates contextual information from images. During inference, we employ a unidirectional pixel-wise update strategy to restore consistent image structures, achieving both high reconstruction quality and fast inference speed. In the second stage, we use a CNN for low-resolution upsampling to enhance texture details. Experiments on ImageNet and CelebA-HQ demonstrate that our inference speed is 10$\\times$ faster than ICT and 15$\\times$ faster than RePaint. The proposed RetCompletion significantly improves inference speed and delivers strong performance, especially when masks cover large areas of the image.", "title_embedding_index": 11959, "title_abs_embedding_index": 11984}, {"title": "\u03d5-Update: A Class of Policy Update Methods with Policy Convergence Guarantee", "link_suffix": "/forum?id=fh7GYa7cjO", "link": "https://openreview.net/forum?id=fh7GYa7cjO", "pdf_link": "https://openreview.net/pdf?id=fh7GYa7cjO", "keywords": "reinforcement learning, policy optimization, policy convergence, linear convergence", "abstract": "Inspired by the similar update pattern of softmax natural policy gradient and Hadamard policy gradient, we propose to study a general policy update rule called $\\phi$-update, where $\\phi$ refers to a scaling function on advantage functions. Under very mild conditions on $\\phi$, the global asymptotic state value convergence of $\\phi$-update is firstly established. Then we show that the policy produced by $\\phi$-update indeed converges, even when there are multiple optimal policies.  This is in stark contrast to existing results where explicit   regularizations  are required to guarantee the convergence of the policy.  Since softmax natural policy gradient is an instance of $\\phi$-update, it provides an affirmative answer to the question whether the policy produced by softmax natural policy gradient converges. The exact asymptotic convergence rate of state values is further established based on the policy convergence. Lastly, we establish the global linear convergence of $\\phi$-update.", "title_embedding_index": 11960, "title_abs_embedding_index": 11985}, {"title": "Fast Adversarial Training against Sparse Attacks Requires Loss Smoothing", "link_suffix": "/forum?id=NlEt8LYAxC", "link": "https://openreview.net/forum?id=NlEt8LYAxC", "pdf_link": "https://openreview.net/pdf?id=NlEt8LYAxC", "keywords": "adversarial robustness, sparse attack, fast adversarial training", "abstract": "This paper studies fast adversarial training against sparse adversarial perturbations. We highlight the challenges faced when employing $1$-step attacks on $l_0$ bounded perturbations for fast adversarial training, including degraded performance and the occurrence of catastrophic overfitting (CO). We highlight that CO in $l_0$ adversarial training is caused by sub-optimal perturbation locations of $1$-step attack, which is distinct from other cases. Theoretical and empirical analyses reveal that the loss landscape of $l_0$ adversarial training is more craggy compared to its $l_\\infty$, $l_2$ and $l_1$ counterparts. Moreover, we corroborate that the craggy loss landscape can aggravate CO. To address these issues, we propose Fast-LS-$l_0$ that incorporates soft label and the trade-off loss function to smooth the adversarial loss landscape. Extensive experiments demonstrate our method can overcome the challenge of catastrophic overfitting, achieves state-of-the-art performance and narrows down the performance gap between $1$-step and multi-step adversarial training against sparse attacks.", "title_embedding_index": 11961, "title_abs_embedding_index": 11986}, {"title": "Towards Stabilizable Sequential Smoothing Spline Interpolation by Point Forecasting", "link_suffix": "/forum?id=43Ckmku1fC", "link": "https://openreview.net/forum?id=43Ckmku1fC", "pdf_link": "https://openreview.net/pdf?id=43Ckmku1fC", "keywords": "spline interpolation, sequential decision making, stability, controllability, time series forecasting", "abstract": "Sequential smoothing spline interpolators exhibit unstable behavior under low-delay response requirements.\nThat is, instability issues are observed when a smoothing spline interpolator is forced to provide an interpolated trajectory piece subject to processing only a few to no incoming data points at each time stamp.\nTypically, the above instability setback is solved by increasing the delay, sacrificing some degree of smoothness in the interpolated trajectory, or a combination of both. \nHowever, stable sequential smoothing spline interpolation strategies working under low delay and without compromising their degree of smoothness seem vastly unexplored in the literature.\nTo the best of our knowledge, this work formalizes the internal instability and asserts the controllability of sequential smoothing spline interpolators for the first time.\nSpecifically, we model the trajectory assembled by a smoothing spline interpolator as a discrete dynamical system of the spline coefficients, facilitating the analysis of its internal instability and controllability.\nFrom these results, we propose a stabilizing strategy based on data point forecasting capable of operating even under delayless regimes and without sacrificing any smoothness of the interpolated trajectory.\nOur claims are theoretically confirmed, or experimentally supported by extensive numerical results otherwise.", "title_embedding_index": 11962, "title_abs_embedding_index": 11987}, {"title": "Rectifying Gradient-based OOD Detection via Fisher Information Matrix", "link_suffix": "/forum?id=Cdhxv0Oz1v", "link": "https://openreview.net/forum?id=Cdhxv0Oz1v", "pdf_link": "https://openreview.net/pdf?id=Cdhxv0Oz1v", "keywords": "OOD Detection", "abstract": "Out-of-distribution (OOD) detection is an anomaly-handling mechanism, for which classification systems should detect outliers with true labels outside the label space, distinguishing them from normal in-distribution (ID) data. \nAdvanced works suggest that gradient information preserve sufficient cues to indicate the confidence of being OOD.\nHowever, we discover previous gradient-based detection methods suffer from limited effectiveness mainly due to over-parameterization. As gradient-based OOD scores derive from the over-parameterized weight space, a widely recognized cause for the suboptimal OOD detection performance, there are also some gradient components which lack necessary informativeness for OOD detection, thereby impair the performance.\nThis observation motivates us to propose gradient rectification (GradRect), using fisher information matrix to correct gradients in directions that are uninformative to discern the distribution change. Moreover, we connect GradRect with classical theories in identifying influential observations, verifying that model fine-tuning with outlier exposure can further improve GradRect. We conduct extensive experiments on various OOD detection setups, revealing the power of GradRect against state-of-the-art counterparts.", "title_embedding_index": 11963, "title_abs_embedding_index": 11988}, {"title": "PTE4TS: One Pre-Training Encoder is All Time Series Need", "link_suffix": "/forum?id=lHo8Do0nfZ", "link": "https://openreview.net/forum?id=lHo8Do0nfZ", "pdf_link": "https://openreview.net/pdf?id=lHo8Do0nfZ", "keywords": "Time Series, Mask model, Pre-train, Hybrid Attention", "abstract": "In Natural Language Processing (NLP) and Computer Vision (CV) as well as myriad other domains, Large Models, especially pre-training models, have achieved significant breakthroughs. However, their advancements in the sphere of general Time Series Analysis (TSA) has been comparatively limited. The principal challenge lies in the dearth of extensive training data that is endemic to the field of TSA. This scarcity hampers the direct application of such pre-training models to time series data, resulting in unsatisfactory performance. Despite numerous attempts to adapt NLP or CV models, which have been pre-training on billions of tokens, to TSA to address this challenge, these pre-training models are not directly suitable for time series data. In this work, we introduce a new general Pre-Training Encoder specifically designed for Time Series analysis, called PTE4TS. It's designed to be easily adaptable to a variety of downstream tasks, such as classification, anomaly detection, and forecasting. First, we revisited the masking methods in time series and found that patch masking, which was widely adopted previously, is not necessary. Therefore, we developed an improved masking model tailored to the characteristics of time series. Additionally, to address the issue of the Low-Rank structure in conventional bidirectional attention mechanisms, which may diminish the model's expressiveness, we have developed a straightforward yet efficacious hybrid attention encoder. The combination of this encoder with our masking methods can improve the representation ability of the model. Finally, PTE4TS achieved state-of-the-art performance on several real-world datasets, further validating the potential of Large Model for general time series analysis. We hope that PTE4TS will not only open new perspectives in the field of TSA, enhancing feature representation and inferencing capabilities across various domains, but also lay the foundation for a general artificial intelligence that is capable of understanding and processing common time series data.", "title_embedding_index": 11964, "title_abs_embedding_index": 11989}, {"title": "Do Vision-Language Models Really Understand Visual Language?", "link_suffix": "/forum?id=wLzhEQq2hR", "link": "https://openreview.net/forum?id=wLzhEQq2hR", "pdf_link": "https://openreview.net/pdf?id=wLzhEQq2hR", "keywords": "vision-language model, visual language, diagram reasoning, evaluation", "abstract": "Visual language is a system of communication that conveys information through symbols, shapes, and spatial arrangements. Diagrams are a typical example of a visual language depicting complex concepts and their relationships in the form of an image. The symbolic nature of diagrams presents significant challenges for building models capable of understanding them. Yet, recent studies seem to suggest that Large Vision-Language Models (LVLMs) can even tackle complex reasoning tasks involving diagrams. In this paper, we investigate this phenomenon by developing a comprehensive test suite to evaluate the diagram comprehension capability of LVLMs. Our test suite uses a variety of questions focused on concept entities and their relationships over a set of synthetic as well as real diagrams across several domains to evaluate the recognition and reasoning abilities of models. Our evaluation of three LVLMs (GPT-4V, GPT-4o, and Gemini) shows that while these models can accurately identify and reason about entities, their ability to understand relationships is notably limited. Further testing reveals that the decent performance on diagram understanding largely stems from leveraging their background knowledge as shortcuts to identify and reason about the relational information. Thus, we conclude that LVLMs have a limited capability for genuine diagram understanding, and their impressive performance in diagram reasoning is an illusion emanating from other confounding factors, such as the background knowledge in the models.", "title_embedding_index": 11965, "title_abs_embedding_index": 11990}, {"title": "Reinforcement Learning for Finite Space Mean-Field Type Games", "link_suffix": "/forum?id=H6DpBnPCyH", "link": "https://openreview.net/forum?id=H6DpBnPCyH", "pdf_link": "https://openreview.net/pdf?id=H6DpBnPCyH", "keywords": "mean field type games, deep reinforcement learning, Nash Q learning", "abstract": "Mean field type games (MFTGs) describe Nash equilibria between large coalitions: each coalition consists of a continuum of cooperative agents who maximize the average reward of their coalition while interacting non-cooperatively with a finite number of other coalitions. Although the theory has been extensively developed, we are still lacking efficient and scalable computational methods. Here, we develop reinforcement learning methods for such games in a finite space setting with general dynamics and reward functions. We start by proving that MFTG solution yields approximate Nash equilibria in finite-size coalition games. We then propose two algorithms. The first is based on quantization of mean-field spaces and Nash Q-learning. We provide convergence and stability analysis. We then propose a deep reinforcement learning algorithm, which can scale to larger spaces. Numerical experiments in 5 environments with mean-field distributions of dimension up to $200$ show the scalability and efficiency of the proposed method.", "title_embedding_index": 11966, "title_abs_embedding_index": 11991}, {"title": "ICDA: Interactive Causal Discovery through Large Language Model Agents", "link_suffix": "/forum?id=xA8WW2dlTX", "link": "https://openreview.net/forum?id=xA8WW2dlTX", "pdf_link": "https://openreview.net/pdf?id=xA8WW2dlTX", "keywords": "Causal Discovery, LLM, Black box optimizer", "abstract": "Large language models (\\textbf{LLMs}) have emerged as a powerful method for causal discovery. Instead of utilizing numerical observational data, LLMs utilize associated variable \\textit{semantic metadata} to predict causal relationships. Simultaneously, LLMs demonstrate impressive abilities to act as black-box optimizers when given an objective $f$ and sequence of trials. We study LLMs at the intersection of these two capabilities by applying LLMs to the task of \\textit{interactive causal discovery}: given a budget of $I$ edge interventions over $R$ rounds, minimize the distance between the ground truth causal graph $G^*$ and the predicted graph $\\hat{G}_R$ at the end of the $R$-th round. We propose an LLM-based pipeline incorporating two key components: 1) an LLM uncertainty-driven method for edge intervention selection 2) a local graph update strategy utilizing binary feedback from interventions to improve predictions for non-intervened neighboring edges. Experiments on eight different real-world graphs show our approach significantly outperforms a random selection baseline: at times by up to 0.5 absolute F1 score. Further we conduct a rigorous series of ablations dissecting the impact of each component of the pipeline. Finally, to assess the impact of memorization, we apply our interactive causal discovery strategy to a complex, new (as of July 2024) causal graph on protein transcription factors. Overall, our results show LLM driven uncertainy based edge selection with local updates performs strongly and robustly across a diverse set of real-world graphs.", "title_embedding_index": 11967, "title_abs_embedding_index": 11992}, {"title": "Constrained Posterior Sampling: Time Series Generation with Hard Constraints", "link_suffix": "/forum?id=pKMpmbuKnd", "link": "https://openreview.net/forum?id=pKMpmbuKnd", "pdf_link": "https://openreview.net/pdf?id=pKMpmbuKnd", "keywords": "Time Series Generation, Posterior Sampling, Diffusion Models, Controlled Generation", "abstract": "Generating realistic time series samples is crucial for stress-testing models and protecting user privacy by using synthetic data. In engineering and safety-critical applications, these samples must meet certain hard constraints that are domain-specific or naturally imposed by physics or nature. Consider, for example, generating electricity demand patterns with constraints on peak demand times. This can be used to stress-test the functioning of power grids during adverse weather conditions. Existing approaches for generating constrained time series are either not scalable or degrade sample quality. To address these challenges, we introduce Constrained Posterior Sampling (CPS), a diffusion-based sampling algorithm that aims to project the posterior mean estimate into the constraint set after each denoising update. Notably, CPS scales to a large number of constraints ($\\sim100$) without requiring additional training. We provide theoretical justifications highlighting the impact of our projection step on sampling. Empirically, CPS outperforms state-of-the-art methods in sample quality and similarity to real time series by around 10% and 42%, respectively, on real-world stocks, traffic, and air quality datasets.", "title_embedding_index": 11968, "title_abs_embedding_index": 11993}, {"title": "SAVA: Scalable Learning-Agnostic Data Valuation", "link_suffix": "/forum?id=0UCoWxPhQ4", "link": "https://openreview.net/forum?id=0UCoWxPhQ4", "pdf_link": "https://openreview.net/pdf?id=0UCoWxPhQ4", "keywords": "Data Valuation, Optimal Transport, Data Selection, Active Learning", "abstract": "Selecting data for training machine learning models is crucial since large, web-scraped, real datasets contain noisy artifacts that affect the quality and relevance of individual data points. These noisy artifacts will impact model performance. We formulate this problem as a data valuation task, assigning a value to data points in the training set according to how similar or dissimilar they are to a clean and curated validation set. Recently, LAVA ~\\citep{just2023lava} demonstrated the use of optimal transport (OT) between a large noisy training dataset and a clean validation set, to value training data efficiently, without the dependency on model performance. However, the LAVA algorithm requires the entire dataset as an input, this limits its application to larger datasets. Inspired by the scalability of stochastic (gradient) approaches which carry out computations on batches of data points instead of the entire dataset, we analogously propose SAVA, a scalable variant of LAVA with its computation on batches of data points. Intuitively, SAVA follows the same scheme as LAVA which leverages the hierarchically defined OT for data valuation. However, while LAVA processes the whole dataset, SAVA divides the dataset into batches of data points, and carries out the OT problem computation on those batches. We perform extensive experiments, to demonstrate that SAVA can scale to large datasets with millions of data points and doesn't trade off data valuation performance.", "title_embedding_index": 11969, "title_abs_embedding_index": 11994}, {"title": "Multi-Agent Causal Discovery Using Large Language Models", "link_suffix": "/forum?id=Idygh9MX0N", "link": "https://openreview.net/forum?id=Idygh9MX0N", "pdf_link": "https://openreview.net/pdf?id=Idygh9MX0N", "keywords": "Causal Inference, Causal Discovery, Natural Language Process, Multi-agent", "abstract": "Large Language Models (LLMs) have demonstrated significant potential in causal discovery tasks by utilizing their vast expert knowledge from extensive text corpora. However, the multi-agent capabilities of LLMs in causal discovery remain underexplored. This paper introduces a general framework to investigate this potential. The first is the Meta Agents Model, which relies exclusively on reasoning and discussions among LLM agents to conduct causal discovery. The second is the Coding Agents Model, which leverages the agents\u2019 ability to plan, write, and execute code, utilizing advanced statistical libraries for causal discovery. The third is the Hybrid Model, which integrates both the Meta Agents Model and Coding Agents Model approaches, combining the statistical analysis and reasoning skills of multiple agents. Our proposed framework shows promising results by effectively utilizing LLMs\u2019 expert knowledge, reasoning capabilities, multi-agent cooperation, and statistical causal methods. By exploring the multi-agent potential of LLMs, we aim to establish a foundation for further research in utilizing LLMs multi-agent for solving causal-related problems.", "title_embedding_index": 11970, "title_abs_embedding_index": 11995}, {"title": "Robust Function-Calling for On-Device Language Model via Function Masking", "link_suffix": "/forum?id=yVQcr4qjD6", "link": "https://openreview.net/forum?id=yVQcr4qjD6", "pdf_link": "https://openreview.net/pdf?id=yVQcr4qjD6", "keywords": "language models, function-calling models", "abstract": "Large language models have demonstrated impressive value in performing as autonomous agents when equipped with external tools and API calls. Nonetheless, effectively harnessing their potential for executing complex tasks crucially relies on enhancements in their function-calling capabilities. This paper identifies a critical gap in existing function-calling models, where performance varies significantly across benchmarks, often due to over-fitting to specific naming conventions. To address such an issue, we introduce Hammer, a novel family of foundation models specifically engineered for on-device function calling. Hammer employs an augmented dataset that enhances models\u2019 sensitivity to irrelevant functions and incorporates function masking techniques to minimize over-fitting. Our empirical evaluations reveal that Hammer not only outperforms larger models but also demonstrates robust generalization across diverse benchmarks, achieving state-of-the-art results. Our open-source contributions include a specialized dataset for irrelevance detection, a tuning framework for enhanced generalization, and the Hammer models, establishing a new standard for function-calling performance.", "title_embedding_index": 11971, "title_abs_embedding_index": 11996}, {"title": "Understanding Domain Generalization: A View of Necessity and Sufficiency", "link_suffix": "/forum?id=Mlxov4A7AE", "link": "https://openreview.net/forum?id=Mlxov4A7AE", "pdf_link": "https://openreview.net/pdf?id=Mlxov4A7AE", "keywords": "Domain Generalization", "abstract": "Despite the rapid advancements in domain generalization (DG), the majority of DG studies center on establishing theoretical guarantee  for generalization under the assumption of sufficient, diverse or even infinite domains. This assumption however is unrealistic, thus there remains no conclusive evidence as to whether the existing DG algorithms can truly generalize in practical settings where domains are limited. This paper aims to elucidate this matter. We first study the conditions for the existence and learnability of an optimal hypothesis. As the sufficient conditions are non-verifiable, our identified two necessary conditions become critical to guaranteeing the chance of finding the global optimal hypothesis in finite domain settings. In light of the theoretical insights, we provide a comprehensive review of DG algorithms explaining to what extent they can generalize effectively. We finally introduce a practical approach that leverages the joint effect of the two sets of conditions to boost generalization. Our proposed method demonstrates superior performance on well-established DG benchmarks.", "title_embedding_index": 11972, "title_abs_embedding_index": 11997}, {"title": "Enhancing Near OOD Detection in Prompt Learning: Maximum Gains, Minimal Costs", "link_suffix": "/forum?id=XPQCiLY45j", "link": "https://openreview.net/forum?id=XPQCiLY45j", "pdf_link": "https://openreview.net/pdf?id=XPQCiLY45j", "keywords": "prompt learning, CLIP, near-OOD", "abstract": "Prompt learning has shown to be an efficient and effective fine-tuning method for vision-language models like CLIP. While numerous studies have focused on the generalisation of these models in few-shot classification, their capability in near out-of-distribution (OOD) detection has been overlooked. A few recent works have highlighted the promising performance of prompt learning in far OOD detection. However, the more challenging task of few-shot near OOD detection has not yet been addressed. In this study, we investigate the near OOD detection capabilities of prompt learning models and observe that commonly used OOD scores have limited performance in near OOD detection. To enhance the performance, we propose a fast and simple post-hoc method that complements existing logit-based scores and can be easily applied to any prompt learning model without change in architecture or model re-training while keeping the same classification accuracy. Our method boosts existing prompt learning methods' near OOD detection performance in AUROC by up to 11.67% with minimal computational cost. Comprehensive empirical evaluations across 13 datasets and 8 models demonstrate the effectiveness and adaptability of our method.", "title_embedding_index": 11973, "title_abs_embedding_index": 11998}, {"title": "Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving", "link_suffix": "/forum?id=VNckp7JEHn", "link": "https://openreview.net/forum?id=VNckp7JEHn", "pdf_link": "https://openreview.net/pdf?id=VNckp7JEHn", "keywords": "Inference Scaling Law, Compute-optimal Inference, LLM reasoning", "abstract": "While the scaling laws of large language models (LLMs) training have been extensively studied, optimal inference configurations of LLMs remain underexplored. We studyinference scaling lawsandcompute-optimal inference, focusing on the trade-offs between model sizes and generating additional tokens with different inference strategies. As a first step towards understanding and designing compute-optimal inference methods, we studied cost-performance trade-offs for inference strategies such as greedy search, majority voting, best-of-$n$, weighted voting, and two different tree search algorithms, using different model sizes and compute budgets. Our findings indicate smaller models (e.g., Llemma-7B) can outperform larger models given the same computation budgets, and that smaller models paired with advanced inference algorithms yield Pareto-optimal cost-performance trade-offs. For instance, the Llemma-7B model, equipped with our novel tree search algorithm, consistently outperforms Llemma-34B with standard majority voting on the MATH benchmark across all FLOPs budgets. We hope these findings contribute to a broader understanding of inference scaling laws for LLMs.", "title_embedding_index": 11974, "title_abs_embedding_index": 11999}]