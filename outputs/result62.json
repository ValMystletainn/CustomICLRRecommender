[{"title": "How to Get Your LLM to Generate Challenging Problems for Evaluation", "link_suffix": "/forum?id=2VhFZPYqjE", "link": "https://openreview.net/forum?id=2VhFZPYqjE", "pdf_link": "https://openreview.net/pdf?id=2VhFZPYqjE", "keywords": "Evaluation, Synthetic data, Benchmarking, Question Answering, Code Generation, Math Reasoning", "abstract": "The pace of evolution of Large Language Models (LLMs) necessitates new approaches for rigorous and comprehensive evaluation. Traditional human annotation is increasingly impracticable due to the complexities and costs involved in generating high-quality, challenging problems, particularly for tasks such as long-context reasoning. Moreover, the rapid saturation of existing human-curated benchmarks by LLMs further necessitates the need to develop scalable and automatically renewable evaluation methodologies. In this work, we introduceCHASE, a unified framework to synthetically generate challenging problems using LLMs without human involvement.  For a given task, our approach builds a hard problem in a bottom-up manner from simpler components. Moreover since we want to generate synthetic data for evaluation, our framework decomposes the generation process into independently verifiable sub-tasks, thereby ensuring a high level of quality and correctness. We implement CHASE to create evaluation benchmarks across three diverse domains: document-based question answering, repository-level code completion, and math reasoning. The performance of state-of-the-art LLMs on these synthetic benchmarks lies in the range of 40-60% accuracy, thereby demonstrating the effectiveness of our framework at generating hard problems. Our experiments further reveal that the Gemini models significantly outperform other LLMs at long-context reasoning, and that the performance of all LLMs drastically drops by as much as 70% when we scale up the context size to 50k tokens.", "title_embedding_index": 3050, "title_abs_embedding_index": 3075}, {"title": "The Pitfalls of Memorization: When Memorization Hurts Generalization", "link_suffix": "/forum?id=vVhZh9ZpIM", "link": "https://openreview.net/forum?id=vVhZh9ZpIM", "pdf_link": "https://openreview.net/pdf?id=vVhZh9ZpIM", "keywords": "Memorization, Generalization, Spurious Correlations", "abstract": "Neural networks often learn simple explanations that fit the majority of the data while memorizing exceptions that deviate from these explanations. This leads to poor generalization if the learned explanations are spurious. In this work, we formalize $\\textit{the interplay between memorization and generalization}$, showing that spurious correlations would particularly lead to poor generalization when are combined with memorization. Memorization can reduce the training loss to zero, leaving no incentive for learning robust, generalizable patterns. To address this issue, we introduce $\\textit{memorization-aware training}$ (MAT). MAT leverages the flip side of memorization by using held-out predictions to shift a model's logits, guiding it towards learning robust patterns that remain invariant from training to test, thereby enhancing generalization under distribution shifts.", "title_embedding_index": 3051, "title_abs_embedding_index": 3076}, {"title": "Graph Neural Networks Gone Hogwild", "link_suffix": "/forum?id=WfxPVtYRlL", "link": "https://openreview.net/forum?id=WfxPVtYRlL", "pdf_link": "https://openreview.net/pdf?id=WfxPVtYRlL", "keywords": "graph neural networks, multi-agent, asynchronous, decentralized", "abstract": "Graph neural networks (GNNs) appear to be powerful tools to learn state representations for agents in distributed, decentralized multi-agent systems, but generate catastrophically incorrect predictions when nodes update asynchronously during inference.\n  This failure under asynchrony effectively excludes these architectures from many potential applications where synchrony is difficult or impossible to enforce, e.g., robotic swarms or sensor networks.\n  In this work we identify ''implicitly-defined'' GNNs as a class of architectures which is provably robust to asynchronous ''hogwild'' inference, adapting convergence guarantees from work in asynchronous and distributed optimization. \n  We then propose a novel implicitly-defined GNN architecture, which we call an energy GNN. \n  We show that this architecture outperforms other GNNs from this class on a variety of synthetic tasks inspired by multi-agent systems.", "title_embedding_index": 3052, "title_abs_embedding_index": 3077}, {"title": "AIMing for Explainability in GNNs", "link_suffix": "/forum?id=KZII3faAs2", "link": "https://openreview.net/forum?id=KZII3faAs2", "pdf_link": "https://openreview.net/pdf?id=KZII3faAs2", "keywords": "Graph Neural Networks, explainability, graph kernels", "abstract": "As machine learning models become increasingly complex and are deployed in critical domains such as healthcare, finance, and autonomous systems, the need for effective explainability has grown. Graph Neural Networks (GNNs), which excel in processing graph-structured data, have seen significant advancements, but explainability for GNNs is still in its early stages. Existing approaches fall under two broad categories: post-hoc explainers, which are evaluated using ground truth explanations for synthetic data, or models based on prototypes or graph kernels that claim inherent interpretability and do not evaluate any actual measures. These evaluation practices fundamentally restrict the utility of any discussions regarding explainability for GNNs. We propose a unified and comprehensive framework for measuring and evaluating explainability in GNNs that extends beyond synthetic data and ground truths, while also allowing for further model development and refinement based on derived explanations. The framework involves measures of Accuracy, Instance-level explanations, and Model-level explanations (AIM), inspired by the generic Co-12 conceptual properties of explanations quality (Nauta et al., 2023). We apply this framework to a suite of existing models, deriving ways to extract explanations from them and to highlight their strengths and weaknesses. Furthermore, based on this analysis using AIM, we develop a new model called XGKN that demonstrates improved explainability while performing on par with existing models. Our approach aims to advance the field of Explainable AI (XAI) for GNNs, offering more robust and practical solutions for understanding and interpreting complex models.", "title_embedding_index": 3053, "title_abs_embedding_index": 3078}, {"title": "BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM inference", "link_suffix": "/forum?id=UKjAwMzX4m", "link": "https://openreview.net/forum?id=UKjAwMzX4m", "pdf_link": "https://openreview.net/pdf?id=UKjAwMzX4m", "keywords": "Post-training Quantization, Large Language Models, Codebooks, Clustering, Block Clustered Quantization", "abstract": "Post-training quantization (PTQ) is a promising approach to reducing the storage and computational requirements of large language models (LLMs) without additional training cost. Recent PTQ studies have primarily focused on quantizing only weights to sub-8-bits while maintaining activations at 8-bits or higher. Accurate sub-8-bit quantization for both weights and activations without relying on quantization-aware training remains a significant challenge. In this work, we introduce a novel quantization method called block clustered quantization (BCQ) wherein each operand tensor is decomposed into blocks (a block is a group of contiguous scalars), blocks are clustered based on their statistics, and a dedicated optimal quantization codebook is designed for each cluster. We propose a PTQ algorithm called Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block clustering and codebook design to greedily minimize the quantization mean squared error. When weight and activation scalars are encoded to W4A4 format (with 0.5-bits of overhead for storing scaling factors and codebook selectors), we advance the current state-of-the-art by demonstrating <1% loss in inference accuracy across several LLMs and downstream tasks.", "title_embedding_index": 3054, "title_abs_embedding_index": 3079}, {"title": "SHIFTING TIME: TIME-SERIES FORECASTING WITH KHATRI-RAO NEURAL OPERATORS", "link_suffix": "/forum?id=M2MinWsyjC", "link": "https://openreview.net/forum?id=M2MinWsyjC", "pdf_link": "https://openreview.net/pdf?id=M2MinWsyjC", "keywords": "time-series modeling, spatio-temporal modeling, time-shift operator, Khatri-Rao neural operator, neural operator, operator learning", "abstract": "We present an operator-theoretic framework for time-series forecasting that involves learning a continuous time-shift operator associated with temporal and spatio-temporal problems. The time-shift operator learning paradigm offers a continuous relaxation of the discrete lag factor used in traditional autoregressive models enabling the history of a function up to a given time to be mapped to its future values. To parametrize the operator learning problem, we propose Khatri-Rao neural operators -- a new architecture for defining non-stationary integral transforms which achieves almost linear cost on spatial and spatio-temporal problems. From a practical perspective, the advancements made in this work allow us to handle irregularly sampled observations and forecast at super-resolution in both space and time. Detailed numerical studies across a wide range of temporal and spatio-temporal benchmark problems suggest that the proposed approach is highly scalable and provides results that compares favourably with the state-of-the-art methods.", "title_embedding_index": 3055, "title_abs_embedding_index": 3080}, {"title": "Data Brittleness Estimation with Self-Supervised Features", "link_suffix": "/forum?id=MnE8iIBCfO", "link": "https://openreview.net/forum?id=MnE8iIBCfO", "pdf_link": "https://openreview.net/pdf?id=MnE8iIBCfO", "keywords": "data attribution, data brittleness estimation", "abstract": "To what extent are model predictions sensitive to modifications in training data? Data attribution approaches have served to answer this question. These approaches can be used for estimating data brittleness i.e., identifying which subset of training samples had the highest positive influence on a test sample. However, these methods come at a high computational cost, are memory intensive, and are hard to scale to large models or datasets. Current state-of-the-art approaches require an ensemble of as many as \\textbf{300,000 models}. In this work, we focus on a computationally efficient baseline centered on estimating two types of data brittleness metrics. Our baseline approach uses the image features from a \\textbf{single} pretrained self-supervised backbone. In contrast to data attribution approaches, our method is model-agnostic based on the intuition that different models leverage data in similar ways. Our results show this simple assumption works well in practice, achieving competitive performance with state-of-the-art attribution approaches on CIFAR-10 and ImageNet, under limited computational and memory requirements. Our work serves as a simple baseline showing that effective data brittleness estimates can be achieved based solely using knowledge of the training data.", "title_embedding_index": 3056, "title_abs_embedding_index": 3081}, {"title": "Self-Improvement in Language Models: The Sharpening Mechanism", "link_suffix": "/forum?id=WJaUkwci9o", "link": "https://openreview.net/forum?id=WJaUkwci9o", "pdf_link": "https://openreview.net/pdf?id=WJaUkwci9o", "keywords": "Learning theory, Sample complexity, Self-Improvement, Language Models", "abstract": "Recent work in language modeling has raised the possibility of \u201cself-improvement,\u201d where an LLM evaluates and refines its own generations to achieve higher performance without external feedback. It is impossible for this self-improvement to create information that is not already in the model, so why should we expect that this will lead to improved capabilities? We offer a new theoretical perspective on the capabilities of self-improvement through a lens we refer to as \u201csharpening.\u201d Motivated by the observation that language models are often better at verifying response quality than they are at generating correct responses, we formalize self-improvement as using the model itself as a verifier during post-training in order to \u2018sharpen\u2019 the model to one placing large mass on high-quality sequences, thereby amortizing the expensive inference-time computation of generating good sequences. We begin by introducing a new statistical framework for sharpening in which the learner has sample access to a pre-trained base policy. Then, we analyze two natural families of self improvement algorithms based on SFT and RLHF. We find that (i) the SFT-based approach is minimax optimal whenever the initial model has sufficient coverage, but (ii) the RLHF-based approach can improve over SFT-based self- improvement by leveraging online exploration, bypassing the need for coverage. We view these findings as a starting point toward a foundational understanding that can guide the design and evaluation of self-improvement algorithms.", "title_embedding_index": 3057, "title_abs_embedding_index": 3082}, {"title": "Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset", "link_suffix": "/forum?id=0y3hGn1wOk", "link": "https://openreview.net/forum?id=0y3hGn1wOk", "pdf_link": "https://openreview.net/pdf?id=0y3hGn1wOk", "keywords": "Machine Unlearning, Vision Language Model, Privacy", "abstract": "Machine unlearning has emerged as an effective strategy for forgetting specific information in the training data. However, with the increasing integration of visual data, privacy concerns in Vision Language Models (VLMs) remain underexplored. To address this, we introduce Facial Identity Unlearning Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly evaluate the effectiveness of unlearning algorithms under the Right to be Forgotten setting. Specifically, we formulate the VLM unlearning task via constructing the Fictitious Facial Identity VQA dataset and apply a two-stage evaluation pipeline that is designed to precisely control the sources of information and their exposure levels. In terms of evaluation, since VLM supports various forms of ways to ask questions with the same semantic meaning,  we also provide robust evaluation metrics including membership inference attacks and carefully designed adversarial privacy attacks to evaluate the performance of algorithms.  Through the evaluation of four baseline VLM unlearning algorithms within FIUBench, we find that all methods remain limited in their unlearning performance, with significant trade-offs between model utility and forget quality. Furthermore, our findings also highlight the importance of privacy attacks for robust evaluations. We hope FIUBench will drive progress in developing more effective VLM unlearning algorithms.", "title_embedding_index": 3058, "title_abs_embedding_index": 3083}, {"title": "A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization", "link_suffix": "/forum?id=CKXul9iX77", "link": "https://openreview.net/forum?id=CKXul9iX77", "pdf_link": "https://openreview.net/pdf?id=CKXul9iX77", "keywords": "robust optimization, stochastic optimization, discrete optimization, deep learning, unsupervised learning", "abstract": "Two-stage adaptive robust optimization (ARO) is a powerful approach for planning under uncertainty, balancing first-stage decisions with recourse decisions made after uncertainty is realized. To account for uncertainty, modelers typically define a simple uncertainty set over which potential outcomes are considered. However, classical methods for defining these sets unintentionally capture a wide range of unrealistic outcomes, resulting in overly-conservative and costly planning in anticipation of unlikely contingencies. In this work, we introduce AGRO, a solution algorithm that performs adversarial generation for two-stage adaptive robust optimization using a variational autoencoder. AGRO generates high-dimensional contingencies that are simultaneously adversarial and realistic, improving the robustness of first-stage decisions at a lower planning cost than standard methods. To ensure generated contingencies lie in high-density regions of the uncertainty distribution, AGRO defines a tight uncertainty set as the image of ``latent'' uncertainty sets under the VAE decoding transformation. Projected gradient ascent is then used to maximize recourse costs over the latent uncertainty sets by leveraging differentiable optimization methods. We demonstrate the cost-efficiency of AGRO by applying it to both a synthetic production-distribution problem and a real-world power system expansion setting. We show that AGRO outperforms the standard column-and-constraint algorithm by up to 1.8% in production-distribution planning and up to 11.6% in power system expansion.", "title_embedding_index": 3059, "title_abs_embedding_index": 3084}, {"title": "Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models Beneficial?", "link_suffix": "/forum?id=ioprnwVrDH", "link": "https://openreview.net/forum?id=ioprnwVrDH", "pdf_link": "https://openreview.net/pdf?id=ioprnwVrDH", "keywords": "large language models, ensemble, mixture-of-agent", "abstract": "Ensembling outputs from diverse sources is a straightforward yet effective approach to boost performance. Mixture-of-Agents (MoA) is one such popular ensemble method that aggregates outputs from multipledifferentLarge Language Models (LLMs). This paper raises the question in the context of language models: is mixing different LLMs truly beneficial?  We propose Self-MoA --- an ensemble method that aggregates outputs from only thesingletop-performing LLM. Our extensive experiments reveal that, surprisingly, Self-MoA outperforms standard MoA that mixes different LLMs in a large number of scenarios: Self-MoA achieves $6.6%$ improvement over MoA on the AlpacaEval 2.0 benchmark, and an average of $3.8%$ improvement across various benchmarks, including MMLU, CRUX, and MATH. Applying Self-MoA to one of the top-ranking models in AlpacaEval 2.0 directly achieves the new state-of-the-art performance ranking $1^{\\text{st}}$ on the leaderboard. To understand the effectiveness of Self-MoA, we systematically investigate the trade-off between diversity and quality of outputs under various MoA settings. We confirm that the MoA performance is rather sensitive to the quality, and mixing different LLMs often lowers the average quality of the models. To complement the study, we identify the scenarios where mixing different LLMs could be helpful. This paper further introduces a sequential version of self-MoA, that is capable of aggregating a large number of LLM outputs on-the-fly over multiple rounds, and is as effective as aggregating all outputs at once.", "title_embedding_index": 3060, "title_abs_embedding_index": 3085}, {"title": "In-Context Neural PDE: Learning to Adapt a Neural Solver to Different Physics", "link_suffix": "/forum?id=fzZfju8y0g", "link": "https://openreview.net/forum?id=fzZfju8y0g", "pdf_link": "https://openreview.net/pdf?id=fzZfju8y0g", "keywords": "Spatio-temporal prediction, PDEs, in-context learning, neural solvers", "abstract": "We address the problem of predicting the next state of a dynamical system governed byunknowntemporal partial differential equations (PDEs) using limited time-lapse data. While transformers offer a natural solution to this task through in-context learning, the inductive bias of temporal PDEs suggests a more tailored and effective approach. Specifically, when the underlying temporal PDE is fully known, classical numerical solvers can evolve the state with only a few parameters. Building on this observation, we introduce a large transformer-based hypernetwork that processes successive states to generate parameters for a much smaller neural ODE-like solver, which then predicts the next state through time integration. This framework, termed asin-context neural PDE, decouples parameter estimation from state prediction, offering closer alignment with classical numerical methods for improved interpretability while preserving the in-context learning capabilities of transformers. \nNumerical experiments on diverse physical datasets demonstrate that our method outperforms standard transformer-based models, reducing sample complexity and improving generalization, making it an efficient and scalable approach for spatiotemporal prediction in complex physical systems.", "title_embedding_index": 3061, "title_abs_embedding_index": 3086}, {"title": "DivScene: Benchmarking LVLMs for Object Navigation with Diverse Scenes and Objects", "link_suffix": "/forum?id=G6DLQ40VVR", "link": "https://openreview.net/forum?id=G6DLQ40VVR", "pdf_link": "https://openreview.net/pdf?id=G6DLQ40VVR", "keywords": "Embodied AI, Object Navigation, Large Vision Language Models, LVLM, Imitation Learning", "abstract": "Object navigation in unknown environments is crucial for deploying embodied agents in real-world applications.\nWhile we have witnessed huge progress due to large-scale scene datasets, faster simulators, and stronger models, previous studies mainly focus on limited scene types and target objects. In this paper, we study a new task of navigating to diverse target objects in a large number of scene types. To benchmark the problem, we present a large-scale scene dataset, DivScene, which contains 4,614 scenes across 81 different types. With the dataset, we build an end-to-end embodied agent, NatVLM, by fine-tuning a Large Vision Language Model (LVLM) through imitation learning. The LVLM is trained to take previous observations from the environment and generate the next actions. We also introduce CoT explanation traces of the action prediction for better performance when tuning LVLMs. Our extensive experiments find that we can build a performant LVLM-based agent through imitation learning on the shortest paths constructed by a BFS planner without any human supervision. Our agent achieves a success rate that surpasses GPT-4o by over 20%. Meanwhile, we carry out various analyses showing the generalization ability of our agent.", "title_embedding_index": 3062, "title_abs_embedding_index": 3087}, {"title": "MPFBench: A Large Scale Dataset for SciML of Multi-Phase-Flows: Droplet and Bubble Dynamics", "link_suffix": "/forum?id=QPVK1ne9gI", "link": "https://openreview.net/forum?id=QPVK1ne9gI", "pdf_link": "https://openreview.net/pdf?id=QPVK1ne9gI", "keywords": "Scientific Machine Learning (SciML), Multiphase Flow, Complex Physics Simulation, Lattice Boltzmann Method (LBM), Droplet Dynamics, Bubble Dynamics", "abstract": "Multiphase fluid dynamics, such as falling droplets and rising bubbles, are critical to many industrial applications. However, simulating these phenomena efficiently is challenging due to the complexity of instabilities, wave patterns, and bubble breakup. This paper investigates the potential of scientific machine learning (SciML) to model these dynamics using neural operators and foundation models. We apply sequence-to-sequence techniques on a comprehensive dataset generated from 11,000 simulations, comprising 1 million time snapshots, produced with a well-validated Lattice Boltzmann method (LBM) framework. The results demonstrate the ability of machine learning models to capture transient dynamics and intricate fluid interactions, paving the way for more accurate and computationally efficient SciML-based solvers for multiphase applications.", "title_embedding_index": 3063, "title_abs_embedding_index": 3088}, {"title": "WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild", "link_suffix": "/forum?id=MKEHCx25xp", "link": "https://openreview.net/forum?id=MKEHCx25xp", "pdf_link": "https://openreview.net/pdf?id=MKEHCx25xp", "keywords": "LLM, Evaluation, Benchmarking", "abstract": "We introduce WildBench, an automated evaluation framework designed to benchmark large language models (LLMs) using challenging, real-world user queries. WildBench consists of 1,024 tasks carefully selected from over one million human-chatbot conversation logs. For automated evaluation with WildBench, we have developed two metrics, WB-Reward and WB-Score, which are computable using advanced LLMs such as GPT-4-turbo. WildBench evaluation uses task-specific checklists to evaluate model outputs systematically and provides structured explanations that justify the scores and comparisons, resulting in more reliable and interpretable automatic judgments. WB-Reward employs fine-grained pairwise comparisons between model responses, generating five potential outcomes: much better, slightly better, slightly worse, much worse, or a tie. Unlike previous evaluations that employed a single baseline model, we selected three baseline models at varying performance levels to ensure a comprehensive pairwise evaluation. Additionally, we propose a simple method to mitigate length bias, by converting outcomes of \u201cslightly better/worse\u201d to \u201ctie\u201d if the winner response exceeds the loser one by more than K characters. WB-Score evaluates the quality of model outputs individually, making it a fast and cost-efficient evaluation metric. WildBench results demonstrate a strong correlation with the human-voted Elo ratings from Chatbot Arena on hard tasks. Specifically, WB-Reward achieves a Pearson correlation of 0.98 with top-ranking models. Additionally, WB-Score reaches 0.95, surpassing both ArenaHard\u2019s 0.91 and AlpacaEval2.0\u2019s 0.89 for length-controlled win rates, as well as the 0.87 for regular win rates.", "title_embedding_index": 3064, "title_abs_embedding_index": 3089}, {"title": "Data Shapley in One Training Run", "link_suffix": "/forum?id=HD6bWcj87Y", "link": "https://openreview.net/forum?id=HD6bWcj87Y", "pdf_link": "https://openreview.net/pdf?id=HD6bWcj87Y", "keywords": "Shapley value, data valuation.", "abstract": "Data Shapley offers a principled framework for attributing the contribution of data within machine learning contexts. However, the traditional notion of Data Shapley requires re-training models on various data subsets, which becomes computationally infeasible for large-scale models. Additionally, this retraining-based definition cannot evaluate the contribution of data for a specific model training run, which may often be of interest in practice. This paper introduces a novel concept, In-Run Data Shapley, which eliminates the need for model retraining and is specifically designed for assessing data contribution for a particular model of interest. In-Run Data Shapley calculates the Shapley value for each gradient update iteration and accumulates these values throughout the training process. We present several techniques that allow the efficient scaling of In-Run Data Shapley to the size of foundation models. In its most optimized implementation, our method adds negligible runtime overhead compared to standard model training. This dramatic efficiency improvement makes it possible to perform data attribution for the foundation model pretraining stage. We present several case studies that offer fresh insights into pretraining data's contribution and discuss their implications for copyright in generative AI and pretraining data curation.", "title_embedding_index": 3065, "title_abs_embedding_index": 3090}, {"title": "Fundamental Limitations on Subquadratic Alternatives to Transformers", "link_suffix": "/forum?id=T2d0geb6y0", "link": "https://openreview.net/forum?id=T2d0geb6y0", "pdf_link": "https://openreview.net/pdf?id=T2d0geb6y0", "keywords": "Large Language Models, Transformers, Fine-grained complexity theory, Document similarity, Hardness of Approximation, Fast attention computation", "abstract": "The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models  such as Mamba were designed to replace attention with an almost linear time alternative.In this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time \u2013 whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason \u2013 cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.", "title_embedding_index": 3066, "title_abs_embedding_index": 3091}, {"title": "Gaussian Mixture Models Based Augmentation Enhances GNN Generalization", "link_suffix": "/forum?id=0Th6bCZwKt", "link": "https://openreview.net/forum?id=0Th6bCZwKt", "pdf_link": "https://openreview.net/pdf?id=0Th6bCZwKt", "keywords": "Graph Neural Networks, Data Augmentation", "abstract": "Graph Neural Networks (GNNs) have shown great promise in many learning tasks, notably including node and graph classification, but they face difficulties when tested on new or unseen data. These challenges are exacerbated when training data is limited in size or diversity. To address this issue, we introduce a theoretical framework using Rademacher complexity to compute a regret bound on the generalization error and then characterize the effect of data augmentation. This framework informs the design of GMM-GDA, a new, efficient graph data augmentation (GDA) algorithm leveraging the capability of Gaussian Mixture Models (GMMs) to approximate any distribution. Our approach not only outperforms existing augmentation techniques but also offers improved time complexity, making it highly suitable for real-world applications.", "title_embedding_index": 3067, "title_abs_embedding_index": 3092}, {"title": "Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion", "link_suffix": "/forum?id=DKgAFfCs5F", "link": "https://openreview.net/forum?id=DKgAFfCs5F", "pdf_link": "https://openreview.net/pdf?id=DKgAFfCs5F", "keywords": "Multi-modal perception; Sensor fusion; Robustness; Uncertainty quantification", "abstract": "An important paradigm in 3D object detection is the use of multiple modalities to enhance accuracy in both normal and challenging conditions, particularly for long-tail scenarios. To address this, recent studies have explored two directions of adaptive approaches: MoE-based adaptive fusion, which struggles with uncertainties arising from distinct object configurations, and late fusion for output-level adaptive fusion, which relies on separate detection pipelines and limits comprehensive understanding. In this work, we introduce Cocoon, an object- and feature-level uncertainty-aware fusion framework. The key innovation lies in uncertainty quantification for heterogeneous representations, enabling fair comparison across modalities through the introduction of a feature aligner and a learnable surrogate ground truth, termed feature impression. We also define a training objective to ensure that their relationship provides a valid metric for uncertainty quantification. Cocoon consistently outperforms existing static and adaptive methods in both normal and challenging conditions, including those with natural and artificial corruptions. Furthermore, we show the validity and efficacy of our uncertainty metric across diverse datasets.", "title_embedding_index": 3068, "title_abs_embedding_index": 3093}, {"title": "Leaving the barn door open for Clever Hans: Simple features predict LLM benchmark answers", "link_suffix": "/forum?id=PtnttTKgQw", "link": "https://openreview.net/forum?id=PtnttTKgQw", "pdf_link": "https://openreview.net/pdf?id=PtnttTKgQw", "keywords": "LLM benchmarks, Benchmark validity, Clever Hans effect, LLM evaluation", "abstract": "The integrity of AI benchmarks is fundamental to accurately assess the capabilities of AI systems. The internal validity of these benchmarks\u2014i.e., making sure they are free from confounding factors\u2014is crucial for ensuring that they are measuring what they are designed to measure. In this paper, we explore a key issue related to internal validity: the possibility that AI systems can solve benchmarks in unintended ways, bypassing the capability being tested. This phenomenon, widely known in human and animal experiments, is often referred to as the \u2018Clever Hans\u2019 effect, where tasks are solved using spurious cues, often involving much simpler processes than those putatively assessed. Previous research suggests that language models can exhibit this behaviour as well. In several older Natural Language Processing (NLP) benchmarks, individual $n$-grams like \u201cnot\u201d have been found to be highly predictive of the correct labels, and supervised NLP models have been shown to exploit these patterns. In this work, we investigate the extent to which simple $n$-grams extracted from benchmark instances can be combined to predict labels in modern multiple-choice benchmarks designed for LLMs, and whether LLMs might be using such $n$-gram patterns to solve these benchmarks. We show how simple classifiers trained on these $n$-grams can achieve high scores on several benchmarks, despite lacking the capabilities being tested. Additionally, we provide evidence that modern LLMs might be using these superficial patterns to solve benchmarks. This suggests that the internal validity of these benchmarks may be compromised and caution should be exercised when interpreting LLM performance results on them.", "title_embedding_index": 3069, "title_abs_embedding_index": 3094}, {"title": "Provable Robustness of (Graph) Neural Networks Against Data Poisoning and Backdoors", "link_suffix": "/forum?id=TVwD2zIQ1F", "link": "https://openreview.net/forum?id=TVwD2zIQ1F", "pdf_link": "https://openreview.net/pdf?id=TVwD2zIQ1F", "keywords": "graph neural networks, provable robustness, certificates, poisoning, data poisoning, backdoor attacks, neural tangent kernel, adversarial robustness, mixed-integer linear programming, support vector machines", "abstract": "Generalization of machine learning models can be severely compromised by data poisoning, where adversarial changes are applied to the training data. This vulnerability has led to interest in certifying (i.e., proving) that such changes up to a certain magnitude do not affect test predictions. We, for the first time, certify Graph Neural Networks (GNNs) against poisoning attacks, including backdoors, targeting the node features of a given graph. Our certificates are white-box and based upon (i) the neural tangent kernel, which characterizes the training dynamics of sufficiently wide networks; and (ii) a novel reformulation of the bilevel optimization problem describing poisoning as a mixed-integer linear program. Consequently, we leverage our framework to provide fundamental insights into the role of graph structure and its connectivity on the worst-case robustness behavior of convolution-based and PageRank-based GNNs. We note that our framework is more general and constitutes the first approach to derive white-box poisoning certificates for NNs, which can be of independent interest beyond graph-related tasks.", "title_embedding_index": 3070, "title_abs_embedding_index": 3095}, {"title": "Adversarially Robust Graph Classification: A Pooling-Based Defense Framework", "link_suffix": "/forum?id=IiWZ9rB2Ef", "link": "https://openreview.net/forum?id=IiWZ9rB2Ef", "pdf_link": "https://openreview.net/pdf?id=IiWZ9rB2Ef", "keywords": "Adversarial Robustness, Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have shown great success across various domains but remain vulnerable to adversarial attacks. While most defense methodology focuses on node classification and enhancing robustness during training, this work shifts the focus to graph classification and inference-time defenses. We theoretically show that the final pooling operation, that is required for graph-level tasks, can have an impact on the graph classifier's underlying robustness. Based on this analysis, we propose a pre-pooling operation, called R-Pool (Robust-Pooling), which is based a novel filtering mechanism using Gaussian Mixture Models (GMMs) to detect and exclude nodes heavily impacted by attacks, thereby enhancing robustness at inference time. Our framework can be used with any pooling operation and any underlying model, and does not require re-training the model nor adapting its architecture. Our experiments demonstrate that this approach effectively mitigates adversarial effects while maintaining a balance between clean and attacked accuracy. Through extensive evaluations on state-of-the-art adversarial attacks, we show that the proposed framework significantly improves the robustness of the underlying GNNs in graph classification tasks compared to other available post-hoc defense methods.", "title_embedding_index": 3071, "title_abs_embedding_index": 3096}, {"title": "Benchmarking LLMs on Safety Issues in Scientific Labs", "link_suffix": "/forum?id=aRqyX0DsmW", "link": "https://openreview.net/forum?id=aRqyX0DsmW", "pdf_link": "https://openreview.net/pdf?id=aRqyX0DsmW", "keywords": "benchmark, lab safety, LLM Trustworthiness", "abstract": "Laboratory accidents pose significant risks to human life and property, underscoring the importance of robust safety protocols. Despite advancements in safety training, laboratory personnel may still unknowingly engage in unsafe practices. With the increasing reliance on large language models (LLMs) for guidance in various fields, including laboratory settings, there is a growing concern about their reliability in critical safety-related decision-making. Unlike trained human researchers, LLMs lack formal lab safety education, raising questions about their ability to provide safe and accurate guidance. Existing research on LLM trustworthiness primarily focuses on issues such as ethical compliance, truthfulness, and fairness but fails to fully cover safety-critical real-world applications, like lab safety. To address this gap, we propose the Laboratory Safety Benchmark (LabSafety Bench), a comprehensive evaluation framework based on a new taxonomy aligned with Occupational Safety and Health Administration (OSHA) protocols. This benchmark includes 765 multiple-choice questions verified by human experts, assessing LLMs and large vision models (LVMs) performance in lab safety contexts. Our evaluations demonstrate that while GPT-4o outperforms human participants, it is still prone to critical errors, highlighting the risks of relying on LLMs in safety-critical environments. Our findings emphasize the need for specialized benchmarks to accurately assess the trustworthiness of LLMs in real-world safety applications. The code and data are available athttps://anonymous.4open.science/r/LabSafetyBench-6363", "title_embedding_index": 3072, "title_abs_embedding_index": 3097}, {"title": "Decision Tree Induction via Semantically-Aware Evolution", "link_suffix": "/forum?id=UyhRtB4hjN", "link": "https://openreview.net/forum?id=UyhRtB4hjN", "pdf_link": "https://openreview.net/pdf?id=UyhRtB4hjN", "keywords": "decision trees, LLMs, genetic programming", "abstract": "Decision trees are a crucial class of models offering robust predictive performance and inherent interpretability across various domains, including healthcare, finance, and logistics. However, current tree induction methods often face limitations such as suboptimal solutions from greedy methods or prohibitive computational costs and limited applicability of exact optimization approaches.\nTo address these challenges, we propose an evolutionary optimization method for decision tree induction based on genetic programming (GP). Our key innovation is the integration of semantic priors and domain-specific knowledge about the search space into the optimization algorithm. To this end, we introduce $\\texttt{LLEGO}$, a framework that incorporates semantic priors into genetic search operators through the use of Large Language Models (LLMs), thereby enhancing search efficiency and targeting regions of the search space that yield decision trees with superior generalization performance. This is operationalized through novel genetic operators that work with structured natural language prompts, effectively utilizing LLMs as conditional generative models and sources of semantic knowledge. Specifically, we introducefitness-guidedcrossover to exploit high-performing regions, anddiversity-guidedmutation for efficient global exploration of the search space. These operators are controlled by corresponding hyperparameters that enable a more nuanced balance between exploration and exploitation across the search space. Empirically, we demonstrate across various benchmarks that $\\texttt{LLEGO}$ evolves superior-performing trees compared to existing tree induction methods, and exhibits significantly more efficient search performance compared to conventional GP approaches.", "title_embedding_index": 3073, "title_abs_embedding_index": 3098}, {"title": "Efficient Imitation under Misspecification", "link_suffix": "/forum?id=fn36V5qsCw", "link": "https://openreview.net/forum?id=fn36V5qsCw", "pdf_link": "https://openreview.net/pdf?id=fn36V5qsCw", "keywords": "Inverse Reinforcement Learning, Imitation Learning, Distribution Shift, Policy Completeness", "abstract": "Interactive imitation learning (IL) is a powerful paradigm for learning to make sequences of decisions from an expert demonstrating how to perform a task. Prior work in efficient imitation learning has focused on the realizable setting, where the expert's policy lies within the learner's policy class (i.e. the learner can perfectly imitate the expert in all states). However, in practice, perfect imitation of the expert perfectly is often impossible due to differences in state information and action space expressiveness (e.g. morphological differences between humans and humanoid robots.) In this paper, we consider the more generalmisspecifiedsetting, where no assumptions are made about the expert policy's realizability. We introduce a novel structural condition,reward-agnostic policy completeness, and prove that it is sufficient for interactive IL algorithms to efficiently avoid the quadratically compounding errors that stymie offline approaches like behavioral cloning. We address an additional practical constraint---the case of limited expert data---and propose a principled method for using sub-optimal data to further improve the sample-efficiency of interactive IL algorithms. Finally, we corroborate our theory with experiments on a suite of continuous control tasks.", "title_embedding_index": 3074, "title_abs_embedding_index": 3099}]