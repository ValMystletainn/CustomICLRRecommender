[{"title": "Network-based Active Inference and its Application in Robotics", "link_suffix": "/forum?id=Y98ehgkFgI", "link": "https://openreview.net/forum?id=Y98ehgkFgI", "pdf_link": "https://openreview.net/pdf?id=Y98ehgkFgI", "keywords": "Active Inference (AIF), Free Energy Principle (FEP), Robotics, Trajectory generation, Random dynamical systems, Random attractor dynamics, Non-Equilibrium Steady State (NESS), Adaptive control, Industrial automation, Computational efficiency, Cost-efficient solutions", "abstract": "This paper introduces Network-based Active Inference (NetAIF), a novel robotic framework that enables real-time learning and adaptability in dynamic, unstructured environments. NetAIF leverages random attractor dynamics and the Free Energy Principle (FEP) to simplify trajectory generation through network-topology-driven attractors that induce controlled instabilities and probabilistic sampling cycles. This approach allows robots to efficiently adapt to changing conditions without requiring extensive pre-training or pre-calculated trajectories. By integrating learning and control mechanisms within a compact model architecture, NetAIF facilitates seamless task execution, such as target tracking and valve manipulation. Extensive simulations and real-world experiments demonstrate NetAIF's capability to perform rapid and precise real-time adjustments, highlighting its suitability for applications requiring high adaptability and efficient control, such as robotics tasks in the energy and manufacturing sectors.", "title_embedding_index": 2250, "title_abs_embedding_index": 2275}, {"title": "LASER: Attention using Exponential Transformation", "link_suffix": "/forum?id=vtcn3DnUCw", "link": "https://openreview.net/forum?id=vtcn3DnUCw", "pdf_link": "https://openreview.net/pdf?id=vtcn3DnUCw", "keywords": "Attention Mechanism, LLM, Transformer, Conformer, ViT", "abstract": "Transformers have had tremendous impact for several sequence related tasks. The softmax based dot-product attention mechanism plays a key role in the Transformer's ability to retrieve from any part of the sequence via a parameterized query-key-value mechanism. However, the softmax operation can backpropagate small gradients thus inhibiting learning. In this paper, we fix this by introducing a new attention mechanism called LASER attention, which admits a log-sum-exp structure and propagates a larger gradient signal. We show that LASER attention can be implemented by making small modifications to existing attention implementations. We conduct experiments on large language models (LLMs) with upto 2.2 billion parameters where we show improvements of upto 3.38% and 1% on an average compared to standard attention on downstream one-shot evaluations. We also evaluate on transformers spanning different modalities (vision, speech and text):  Vision Transformer (ViT) on Imagenet (1.2% improvement in accuracy), Conformer on the Librispeech speech-to-text task (2.25% relative improvement) and encoder-only BERT Transformer with 2.2 billion parameters (0.93% relative improvement).", "title_embedding_index": 2251, "title_abs_embedding_index": 2276}, {"title": "ZoomVLM: A Tuning-Free Framework for Efficient Video Understanding via Adaptive Zooming in Vision-Language Models", "link_suffix": "/forum?id=689MfSyeNz", "link": "https://openreview.net/forum?id=689MfSyeNz", "pdf_link": "https://openreview.net/pdf?id=689MfSyeNz", "keywords": "Vision Language Model, Multi-modal", "abstract": "Recent advances in vision-language models (VLMs) have led to impressive progress in video understanding. However, despite their promising performance, existing state-of-the-art (SOTA) solutions require an excessive number of tokens (e.g., up to 6,272 tokens in the Llava-OneVision model) to represent input videos, leading to a non-negligible bottleneck in inference efficiency. Motivated by findings in human perception, where individuals first focus on high-level overviews and then zoom into specific areas for detailed information, we hypothesize that a similar approach can enhance the inference efficiency of VLMs by reducing the number of tokens needed to represent videos. Based on this hypothesis, we propose ZoomVLM, a tuning-free, plug-and-play efficient video processing framework for video VLMs. ZoomVLM first generates an overview of the entire video and then adaptively zooms in and out on different parts based on the content being generated. Our key insight is that the attention distributions in the Large Language Model (LLM) within the VLM can provide sensible guidance on where to focus (by allocating more tokens) and where to discard (by dropping tokens) during inference. Specifically, ZoomVLM integrates two key components: (1) a Video Overview Augmenter, which enables cost-effective high-level understanding by augmenting downsampled video overview with a few high-resolution keyframes; and (2) an Adaptive Token Adjustment, which predicts the importance of different video parts in the upcoming generation process and adjusts the number of tokens allocated to each part according to their importance. Extensive experiments and ablation studies across two challenging open-ended video understanding benchmarks and four models validate that ZoomVLM effectively improves inference efficiency by reducing the number of tokens and boosting throughput in terms of the number of generated tokens per second without degradation in achievable accuracy. Specifically, when applying ZoomVLM to Llava-Next-Video-7B-DPO, ZoomVLM achieves a 30% higher token generation rate with a 0.259 improvement in the Video Detail Description score.", "title_embedding_index": 2252, "title_abs_embedding_index": 2277}, {"title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data", "link_suffix": "/forum?id=yuuyPlywuO", "link": "https://openreview.net/forum?id=yuuyPlywuO", "pdf_link": "https://openreview.net/pdf?id=yuuyPlywuO", "keywords": "Multi-Modal LLMs, Voice Assistants, Distillation", "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) \n have led to models ``forgetting\" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training compute.", "title_embedding_index": 2253, "title_abs_embedding_index": 2278}, {"title": "Replay can provably increase forgetting", "link_suffix": "/forum?id=kf9phcBvQ5", "link": "https://openreview.net/forum?id=kf9phcBvQ5", "pdf_link": "https://openreview.net/pdf?id=kf9phcBvQ5", "keywords": "Continual Learning, Replay, Linear Regression, Theory of Continual Learning", "abstract": "Continual learning seeks to enable machine learning systems to solve an increasing corpus of tasks sequentially. A critical challenge for continual learning is forgetting, where the performance on previously learned tasks decreases as new tasks are introduced. One of the commonly used techniques to mitigate forgetting, sample replay, has been shown empirically to reduce forgetting by retaining some examples from old tasks and including them in new training episodes.\nIn this work, we provide a theoretical analysis of sample replay in an over-parameterized continual linear regression setting, where given enough replay samples, one would be able to eliminate forgetting. Our analysis focuses on replaying a few examples \nand highlights the role of the replay samples and task subspaces.\nSurprisingly, we find that forgetting can be non-monotonic with respect to the number of replay samples.\nWe construct tasks where replay of a single example can increase forgetting and  even distributions where replay of a randomly selected sample increases forgetting on average. We provide empirical evidence that this is  a property of the tasks rather than the model used to train on them, by showing a similar behavior for a neural net equipped with SGD. \nThrough experiments on a commonly used benchmark, we provide additional evidence that performance of the replay heavily depends on the choice of replay samples and the relationship between tasks.", "title_embedding_index": 2254, "title_abs_embedding_index": 2279}, {"title": "PIT-QMM: A Large Multimodal Model for No-Reference Point Cloud Quality Assessment", "link_suffix": "/forum?id=h5D0JICV3s", "link": "https://openreview.net/forum?id=h5D0JICV3s", "pdf_link": "https://openreview.net/pdf?id=h5D0JICV3s", "keywords": "Multimedia quality assessment, point clouds, large multimodal models", "abstract": "Large Multimodal Models (LMMs) have recently enabled considerable advances in the realm of image and video quality assessment, but this progress has yet to be translated to the domain of 3D assets. We are interested in using these models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where the aim is to automatically evaluate the perceptual quality of a point cloud in absence of a reference. We begin with the observation that different modalities of data -- text descriptions, 2D projections, and 3D point cloud views -- provide uniquely useful insights into point cloud quality. We leverage this to devise a multimodal dataset construction strategy providing a holistic combination of multiple types and levels of information. We then construct PIT-QMM, a novel LMM for NR-PCQA that is capable of consuming text, images and point clouds to predict quality scores. Extensive experimentation shows that our proposed method outperforms the state-of-the-art by significant margins on popular benchmarks with fewer training iterations, and thorough ablations validate our dataset construction strategy. Code and datasets are available athttps://anonymous.4open.science/r/pit-qmm-BD1F/.", "title_embedding_index": 2255, "title_abs_embedding_index": 2280}, {"title": "Network-based Active Inference for Adaptive and Cost-efficient Real-World Applications: PV Panel Inspection", "link_suffix": "/forum?id=v2NuTf6Kww", "link": "https://openreview.net/forum?id=v2NuTf6Kww", "pdf_link": "https://openreview.net/pdf?id=v2NuTf6Kww", "keywords": "Active Inference (AIF), Free Energy Principle (FEP), Robotics, Trajectory generation, Random dynamical systems, Random attractor dynamics, Non-Equilibrium Steady State (NESS), Adaptive control, Industrial automation, Computational efficiency, Cost-efficient solutions", "abstract": "This paper introduces Network-based Active Inference (NetAIF), a novel framework that integrates random attractor dynamics and the Free Energy Principle (FEP) to improve trajectory generation and control in robotics. NetAIF optimizes the intrinsic dynamics of neural networks, enabling robots to quickly adapt to dynamic and complex real-world environments with minimal computational resources and without the need for extensive pre-training. Unlike traditional learning methods that rely on large datasets and prolonged training periods, NetAIF offers a more efficient alternative.In real-world scenarios, such as Photovoltaic (PV) panel inspections, NetAIF demonstrates its ability to execute dynamic tasks with both high efficiency and robustness. The system excels in unpredictable environments while maintaining a low computational footprint. These capabilities make NetAIF a promising solution for industrial applications, offering cost-effective, adaptive robotic systems that can reduce operational expenses and enhance performance, particularly in sectors like energy, where adaptability and precision are crucial.", "title_embedding_index": 2256, "title_abs_embedding_index": 2281}, {"title": "Deep Learning Aided Broadcast Codes With Feedback", "link_suffix": "/forum?id=yHVjncoGSp", "link": "https://openreview.net/forum?id=yHVjncoGSp", "pdf_link": "https://openreview.net/pdf?id=yHVjncoGSp", "keywords": "Deep Learning, Wireless Communication, Feedback Coding, Error Control Coding, Federated Learning", "abstract": "Deep learning aided codes have been shown to improve code performance in feedback codes in high noise regimes due to the ability to leverage non-linearity in code design. In the additive white Gaussian broadcast channel (AWGN-BC), the addition of feedback may allow the capacity region to extend far beyond the capacity region of the channel without feedback, enabling higher data rates. On the other hand, there are limited deep-learning aided implementations of broadcast codes. In this work, we extend two classes of deep-learning assisted feedback codes to the AWGN-BC channel; the first being an RNN-based architecture and the second being a lightweight MLP-based architecture. Both codes are trained using a global model, and then they are trained using a more realistic vertical federated learning based framework. We first show that in most cases, using an AWGN-BC code outperforms a linear-based concatenated scheme. Second, we show in some regimes, the lightweight architecture far exceeds the RNN-based code, but in especially unreliable conditions, the RNN-based code dominates.  The results show the promise of deep-learning aided broadcast codes in unreliable channels, and future research directions are discussed.", "title_embedding_index": 2257, "title_abs_embedding_index": 2282}, {"title": "Quality Measures for Dynamic Graph Generative Models", "link_suffix": "/forum?id=8bjspmAMBk", "link": "https://openreview.net/forum?id=8bjspmAMBk", "pdf_link": "https://openreview.net/pdf?id=8bjspmAMBk", "keywords": "graph generative networks, dynamic graphs, evaluation metrics", "abstract": "Deep generative models have recently achieved significant success in modeling graph data, including dynamic graphs, where topology and features evolve over time. However, unlike in vision and language domains, evaluating generative models for dynamic graphs is challenging due to the difficulty of visualizing their output, making quantitative metrics essential. In this work, we develop a new quality metric specifically for evaluating generative models of dynamic graphs. Current metrics for dynamic graphs typically involve discretizing the continuous-evolution of graphs into static snapshots and then applying conventional graph similarity measures. This approach has several limitations: (a) it models temporally related events as i.i.d. samples, failing to capture the non-uniform evolution of dynamic graphs; (b) it lacks a unified measure that is sensitive to both features and topology; (c) it fails to provide a scalar metric, requiring multiple metrics without clear superiority; and (d) it requires explicitly instantiating each static snapshot, leading to impractical runtime demands that hinder evaluation at scale. We propose a novel metric based on the Johnson-Lindenstrauss lemma, applying random projections directly to dynamic graph data. This results in an expressive, scalar, and application-agnostic measure of dynamic graph similarity that overcomes the limitations of traditional methods. We also provide a comprehensive empirical evaluation of metrics for continuous-time dynamic graphs, demonstrating the effectiveness of our approach compared to existing methods.", "title_embedding_index": 2258, "title_abs_embedding_index": 2283}, {"title": "Backoff Decoding: A Language Model Inference Acceleration Framework with a Tunable Efficiency-Performance Tradeoff", "link_suffix": "/forum?id=Yz7ts36V7A", "link": "https://openreview.net/forum?id=Yz7ts36V7A", "pdf_link": "https://openreview.net/pdf?id=Yz7ts36V7A", "keywords": "language modeling, inference acceleration, decoding strategies", "abstract": "In current transformer-based language models, all tokens in a sequence are generated by identical forward passes and thereby incur the same inference cost. However, tokens vary widely in their importance to the overall generation and their difficulty for models to generate correctly, making this equal allocation of inference resources suboptimal. We introduce backoff decoding, a framework for efficient language model inference that dynamically allocates token generations between two (or more) models of different sizes, according to an arbitrary decision function. By modifying how this decision function allocates generations between the differently sized models, users can tune their generation along an efficiency-performance tradeoff to suit the needs of their application. Backoff decoding can be used on any set of models with the same tokenizer and does not require any training or finetuning of the models themselves. As a demonstration of our framework, we show that backoff decoding with a large and a small model can significantly reduce inference cost while sacrificing virtually no performance compared to the standalone large model. We then show that inference costs can be reduced even further, achieving inference accelerations of up to 5-6x in exchange for small reductions in model performance, demonstrating an efficiency-performance tunability not found in other inference acceleration techniques.", "title_embedding_index": 2259, "title_abs_embedding_index": 2284}, {"title": "SemCLIP: Aligning vision-language encoder models to semantic spaces for stability in retrieval", "link_suffix": "/forum?id=xrazpGhJ10", "link": "https://openreview.net/forum?id=xrazpGhJ10", "pdf_link": "https://openreview.net/pdf?id=xrazpGhJ10", "keywords": "Semantic-preserving queries, Vision-language encoder models, Stability of retrieval, joint embeddings", "abstract": "Vision-language models (VLM) bring image and textual representations close together in a joint embedding space to tackle many tasks ranging from image captioning to text-to-image retrieval. For such models to be reliably used in cloud vector stores, it is important to have a stable association between images and text such that synonymous queries bring up the same images or have a high degree of overlap. Current textual representations based on transformer models used to build the VLMs cannot adequately capture linguistic similarities to ensure such stability. In this paper we develop a database of linguists-curated similarity list of words derived from Wordnet, and train a semantics preserving textual embedding. We then train an alignment transformation to map existing VLM (CLIP) embeddings to bring synonymous embeddings closer while also preserving image-text similarities. The alignment transform is learned from textual embeddings alone thus avoiding large-scale retraining of VLMs from image-text pairs.   This simple method outperforms other methods of creating image-joint text embeddings including even those by fine-tuning the encoders using the same synonyms lists. Results of analysis and comparison  on multiple benchmark datasets  is indicating both stable and improved quality of retrieval. The dataset of similarity lists and the semantics-preserve textual embedding itself can be employed in a variety of ways for other downstream tasks and will be made available for other researchers.", "title_embedding_index": 2260, "title_abs_embedding_index": 2285}, {"title": "MaD-Scientist: AI-based Scientist solving Convection-Diffusion-Reaction Equations Using Massive PINN-Based Prior Data", "link_suffix": "/forum?id=VRlihVklCL", "link": "https://openreview.net/forum?id=VRlihVklCL", "pdf_link": "https://openreview.net/pdf?id=VRlihVklCL", "keywords": "in-context learning, scientific foundation model, zero-shot, PINN-prior", "abstract": "Large language models (LLMs), like ChatGPT, have shown that even trained with noisy prior data,  they can generalize effectively to new tasks through in-context learning (ICL) and pre-training techniques.\nMotivated by this, we explore whether a similar approach can be applied to scientific foundation models (SFMs). Our methodology is structured as follows: (i) we collect low-cost physics-informed neural network (PINN)-based approximated prior data in the form of solutions to partial differential equations (PDEs) constructed through an arbitrary linear combination of mathematical dictionaries; (ii) we utilize Transformer architectures with self and cross-attention mechanisms to predict PDE solutions without knowledge of the governing equations in a zero-shot setting; (iii) we provide experimental evidence on the one-dimensional convection-diffusion-reaction equation, which demonstrate that pre-training remains robust even with approximated prior data, with only marginal impacts on test accuracy. Notably, this finding opens the path to pre-training SFMs with realistic, low-cost data instead of (or in conjunction with) numerical high-cost data. These results support the conjecture that SFMs can improve in a manner similar to LLMs, where fully cleaning the vast set of sentences crawled from the Internet is nearly impossible.", "title_embedding_index": 2261, "title_abs_embedding_index": 2286}, {"title": "Network-based Active Inference for Adaptive and Cost-efficient Real-World Applications: A Benchmark Study of a Valve-turning Task Against Deep Reinforcement Learning", "link_suffix": "/forum?id=Hm7RYDspQP", "link": "https://openreview.net/forum?id=Hm7RYDspQP", "pdf_link": "https://openreview.net/pdf?id=Hm7RYDspQP", "keywords": "Active Inference (AIF), Deep Reinforcement Learning (DRL), Free Energy Principle (FEP), Robotics, Trajectory generation, Random dynamical systems, Random attractor dynamics, Non-Equilibrium Steady State (NESS), Adaptive control, Industrial automation, Computational efficiency, Cost-efficient solutions", "abstract": "This paper introduces Network-based Active Inference (NetAIF), a novel approach that integrates Active Inference (AIF) principles with network dynamics to enable adaptive, cost-efficient real-world applications. In benchmark tests against Deep Reinforcement Learning (DRL), NetAIF outperforms DRL in both computational efficiency and task performance. Leveraging random attractor dynamics, NetAIF generates real-time trajectories, allowing robots to adapt to complex, dynamic environments without the need for extensive pre-training. We demonstrate NetAIF's superiority in industrial valve manipulation, achieving over 99% accuracy in goal position and orientation in untrained dynamic environments, with a 45,000-fold reduction in computational costs. NetAIF is approximately 100,000 times more efficient in iteration count than DRL, making it a highly robust and efficient solution for industrial applications.", "title_embedding_index": 2262, "title_abs_embedding_index": 2287}, {"title": "Self-Normalized Resets for Plasticity in Continual Learning", "link_suffix": "/forum?id=G82uQztzxl", "link": "https://openreview.net/forum?id=G82uQztzxl", "pdf_link": "https://openreview.net/pdf?id=G82uQztzxl", "keywords": "Continual Learning, Plasticity, Lifelong Learning, Neuron Resets", "abstract": "Plasticity Loss is an increasingly important phenomenon that refers to the empirical observation that as a neural network is continually trained on a sequence of changing tasks, its ability to adapt to a new task diminishes over time. We introduce Self-Normalized Resets (SNR), a simple adaptive algorithm that mitigates plasticity loss by resetting a neuron\u2019s weights when evidence suggests its firing rate has effectively dropped to zero. Across a battery of continual learning problems and network architectures, we demonstrate that SNR consistently attains superior performance compared to its competitor algorithms. We also demonstrate that SNR is robust to its sole hyperparameter, its rejection percentile threshold, while competitor algorithms show significant sensitivity. SNR\u2019s threshold-based reset mechanism is motivated by a simple hypothesis test we derive. Seen through the lens of this hypothesis test, competing reset proposals yield suboptimal error rates in correctly detecting inactive neurons, potentially explaining our experimental observations. We also conduct a theoretical investigation of the optimization landscape for the problem of learning a single ReLU. We show that even when initialized adversarially, an idealized version of SNR learns the target ReLU, while regularization based approaches can fail to learn.", "title_embedding_index": 2263, "title_abs_embedding_index": 2288}, {"title": "Homomorphism Counts as Structural Encodings for Graph Learning", "link_suffix": "/forum?id=qFw2RFJS5g", "link": "https://openreview.net/forum?id=qFw2RFJS5g", "pdf_link": "https://openreview.net/pdf?id=qFw2RFJS5g", "keywords": "graph transformers, structural encodings, homomorphism counts, expressivity", "abstract": "Graph Transformers are popular neural networks that extend the well-known Transformer architecture to the graph domain. These architectures operate by applying self-attention on graph nodes and incorporating graph structure through the use of positional encodings (e.g., Laplacian positional encoding) or structural encodings (e.g., random-walk structural encoding). The quality of such encodings is critical, since they provide the necessary \\emph{graph inductive biases} to condition the model on graph structure. In this work, we propose \\emph{motif structural encoding} (MoSE) as a flexible and powerful structural encoding framework based on counting graph homomorphisms. Theoretically, we compare the expressive power of MoSE to random-walk structural encoding and relate both encodings to the expressive power of standard message passing neural networks. Empirically, we observe that MoSE outperforms other well-known positional and structural encodings across a range of architectures, and it achieves state-of-the-art performance on widely studied molecular property prediction datasets.", "title_embedding_index": 2264, "title_abs_embedding_index": 2289}, {"title": "Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking", "link_suffix": "/forum?id=msEr27EejF", "link": "https://openreview.net/forum?id=msEr27EejF", "pdf_link": "https://openreview.net/pdf?id=msEr27EejF", "keywords": "reward hacking, reward gaming, overoptimization, occupancy measures", "abstract": "Because it is difficult to precisely specify complex objectives, reinforcement learning policies are often optimized using flawed proxy rewards that seem to capture the true objective. However, optimizing proxy rewards frequently leads to reward hacking: the optimized reward function ceases to be a good proxy and the resulting policy performs poorly with respect to the unspecified true reward. Principled solutions to reward hacking have been impeded by the lack of a good definition for the problem. We introduce a definition of reward hacking based on correlation between proxy and true rewards for states and actions seen by a \"base policy\" that breaks down under optimization. We show that this definition captures reward hacking behavior across several realistic settings, including in reinforcement learning from human feedback (RLHF). We then show theoretically that regularization to the base policy can effectively prevent reward hacking. Our theory suggests regularizing $\\chi^2$ divergence between the policies' occupancy measures, rather than the current practice in RLHF of using a KL penalty between action distributions. We intuitively show why this type of regularization is superior, and demonstrate that it better mitigates reward hacking in practice across four realistic settings, including RLHF.", "title_embedding_index": 2265, "title_abs_embedding_index": 2290}, {"title": "Understanding Data Poisoning Attacks for RAG: Insights and Algorithms", "link_suffix": "/forum?id=2aL6gcFX7q", "link": "https://openreview.net/forum?id=2aL6gcFX7q", "pdf_link": "https://openreview.net/pdf?id=2aL6gcFX7q", "keywords": "Safety; Retrieval", "abstract": "Large Language Models (LLMs) have achieved success across various domains but also exhibit problematic issues, such as hallucinations. Retrieval-Augmented Generation (RAG) effectively alleviates these problems by incorporating external information to improve the factual accuracy of LLM-generated content. However, recent studies reveal that RAG systems are vulnerable to adversarial poisoning attacks, where attackers manipulate retrieval systems by poisoning the data corpus used for retrieval. These attacks raise serious safety concerns, as they can easily bypass existing defenses. In this work, we address these safety issues by first providing insights into the factors contributing to successful attacks. In particular, we show that more effective poisoning attacks tend to occur along directions where the clean data distribution exhibits small variances. Based on these insights, we propose two strategies. First, we introduce a new defense, named DRS (Directional Relative Shifts), which examines shifts along those directions where effective attacks are likely to occur. Second, we develop a new attack algorithm to generate more stealthy poisoning data (i.e., less detectable) by regularizing the poisoning data\u2019s DRS. We conducted extensive experiments across multiple application scenarios, including RAG Agent and dense passage retrieval for Q&A, to demonstrate the effectiveness of our proposed methods.", "title_embedding_index": 2266, "title_abs_embedding_index": 2291}, {"title": "ScaLES: Scalable Latent Exploration Score for Pre-Trained Generative Networks", "link_suffix": "/forum?id=5MNJKgaj54", "link": "https://openreview.net/forum?id=5MNJKgaj54", "pdf_link": "https://openreview.net/pdf?id=5MNJKgaj54", "keywords": "VAE, Latent Space Optimization, OOD", "abstract": "We develop Scalable Latent Exploration Score (ScaLES) to mitigate over-exploration in Latent Space Optimization (LSO), a popular method for solving black-box discrete optimization problems. \nLSO utilizes continuous optimization within the latent space of a Variational Autoencoder (VAE) and is known to be susceptible to over-exploration, which manifests in unrealistic solutions that reduce its practicality. \nScaLES is an exact and theoretically motivated method leveraging the trained decoder's approximation of the data distribution. ScaLES can be employed with any VAE decoder--including pretrained ones--without additional training, architectural changes, access to the training data or hyperparameters. Our evaluation across five LSO benchmark tasks and twenty-two VAE models demonstrates that ScaLES always enhances the quality of the solutions while maintaining high objective values, leading to improvements over existing solutions in most cases.\nWe believe that new avenues to LSO will be opened by ScaLES' ability to identify out of distribution areas, differentiability, and computational tractability.", "title_embedding_index": 2267, "title_abs_embedding_index": 2292}, {"title": "CoPS: Empowering LLM Agents with Provable Cross-Task Experience Sharing", "link_suffix": "/forum?id=9W6Z9IeLzc", "link": "https://openreview.net/forum?id=9W6Z9IeLzc", "pdf_link": "https://openreview.net/pdf?id=9W6Z9IeLzc", "keywords": "Agent, LLM", "abstract": "Sequential reasoning in agent systems has been significantly advanced by large language models (LLMs), yet existing approaches face limitations. Reflection-driven reasoning relies solely on knowledge in pretrained models, limiting performance in novel scenarios, while experience-assisted reasoning often depends on external experiences and lacks clear principles for selecting representative experiences. We address these limitations by proposing CoPS (Cross-Task Experience Sharing), a generalizable algorithm that enhances sequential reasoning by cross-task experience sharing and selection. In detail, CoPS leverages agents' experiences on previous tasks, selecting distribution-matched experiences via a provable pessimism-based strategy to maximize utility while minimizing risks from distribution shifts. Extensive experimental results on benchmarks like Alfworld, Webshop, and HotPotQA demonstrate that CoPS consistently outperforms state-of-the-art baselines, with superior sample efficiency suitable for resource-constrained scenarios. Theoretically, we show that the performance of our algorithm depends on both the quality of the pretrained LLM and the matching between the agent's task-dependent trial distribution and that generated by the LLM. Our work bridges the gap between existing sequential reasoning paradigms and validates the effectiveness of leveraging cross-task experiences, shedding light on the potential to improve agents' generalization and adaptability across diverse tasks. Our codes are released atthis link.", "title_embedding_index": 2268, "title_abs_embedding_index": 2293}, {"title": "Real2Code: Reconstruct Articulated Objects via Code Generation", "link_suffix": "/forum?id=CAssIgPN4I", "link": "https://openreview.net/forum?id=CAssIgPN4I", "pdf_link": "https://openreview.net/pdf?id=CAssIgPN4I", "keywords": "articulated objects, code generation LLMs, foundation models", "abstract": "We present Real2Code, a novel approach to reconstructing articulated objects via code generation. Given visual observations of an object, we first reconstruct its part geometry using image segmentation and shape completion. We represent these object parts with oriented bounding boxes, from which a fine-tuned large language model (LLM) predicts joint articulation as code. By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects in unstructured environments. Experimental results demonstrate that Real2Code significantly outperforms the previous state-of-the-art in terms of reconstruction accuracy, and is the first approach to extrapolate beyond objects' structural complexity in the training set, as we show for objects with up to 10 articulated parts. When incorporated with a stereo reconstruction model, Real2Code moreover generalizes to real-world objects, given only a handful of multi-view RGB images and without the need for depth or camera information.", "title_embedding_index": 2269, "title_abs_embedding_index": 2294}, {"title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks", "link_suffix": "/forum?id=Q5VlpYRxGF", "link": "https://openreview.net/forum?id=Q5VlpYRxGF", "pdf_link": "https://openreview.net/pdf?id=Q5VlpYRxGF", "keywords": "KV cache compression, LLM, Gaussian Kernel weighted merging", "abstract": "How to efficiently serve Large Language Models (LLMs) has become a pressing issue because of their huge computational cost in their autoregressive generation process. To mitigate computational costs, LLMs often employ the KV Cache technique to improve the generation speed. While improving the computational efficiency, the storage requirements of the KV cache are substantial, particularly in long-context scenarios, leading to significant memory consumption. Existing KV cache eviction methods often degrade the performance of LLMs in long-context scenarios due to the information loss introduced by eviction. In this paper, we propose a novel KV cache merging approach, called KVMerger, to achieve adaptive KV cache compression for long-context tasks without significant performance degradation under constrained memory budgets. Our approach is inspired by the intriguing observation that key states exhibit high similarity at the token level within a single sequence. To facilitate merging, we develop an effective yet straightforward merging set identification algorithm to identify suitable KV states for merging. Our merging set identification algorithm stimulates the second observation that KV cache sparsity, from similarity perspective, is independent of the dataset and remains persistent at the model level. Subsequently, we propose a Gaussian kernel weighted merging algorithm to selectively merge all states within each merging set. We conduct extensive experiments to demonstrate the effectiveness of KVMerger for long-context tasks under constrained memory budgets, applying it to models including Llama2-7B/13B-chat and Mistral-7B-instruct across various tasks. We also compare our method with other KV cache compression algorithms, including H2O and CaM, showing that our method achieves superior performance across tasks with different KV cache budgets.", "title_embedding_index": 2270, "title_abs_embedding_index": 2295}, {"title": "Reliability-Aware Preference Learning for LLM Reward Models", "link_suffix": "/forum?id=xRDYDI6Rc9", "link": "https://openreview.net/forum?id=xRDYDI6Rc9", "pdf_link": "https://openreview.net/pdf?id=xRDYDI6Rc9", "keywords": "preference learning, RLHF, human models, scalable oversight", "abstract": "Reward functions learned from human feedback serve as the training objective for RLHF, the current state-of-the-art approach for aligning large language models to our values. However, in practice, these reward models fail to robustly capture our desiderata, often attributing more value to features such as output length or agreement with the user and less value to important features like factual correctness. A major reason is that human annotators provide feedback that is an unreliable reflection of their true preferences because of knowledge gaps, limited resources, cognitive biases, or other factors. We focus on making preference learning robust to unreliable feedback by explicitly modeling the knowledge and judgment of annotators. In particular, we estimate reliablity scores for each provided pairwise comparison and incoporate them into the implicit human model used in RLHF, DPO, and other alignment techniques, a technique we call Reliability Aware Preference Learning (RAPL). To test our approach, we introduce the Length Incentivized Evaluations dataset as a setting in which annotators are particularly likely to provide unreliable feedback. Then, we curate the Testing Reasoning and Understanding Errors dataset for training models to predict reliability scores. We find that traditional preference learning on the LIE dataset and other commonly used RLHF datasets leads to models that place far more weight on output length than accuracy. In contrast, RAPL results in models that better capture the true values of annotators.", "title_embedding_index": 2271, "title_abs_embedding_index": 2296}, {"title": "An Asynchronous Bundle Method for Distributed Learning Problems", "link_suffix": "/forum?id=Kwo20MWWCb", "link": "https://openreview.net/forum?id=Kwo20MWWCb", "pdf_link": "https://openreview.net/pdf?id=Kwo20MWWCb", "keywords": "Distributed optimization; asynchronous optimization; model-based optimization", "abstract": "We propose a novel asynchronous bundle method to solve distributed learning problems. Compared to existing asynchronous methods, our algorithm computes the next iterate based on a more accurate approximation of the objective function and does not require any prior  information about the maximal information delay in the system. This makes the proposed method fast and easy to tune. We prove that the algorithm converges in both deterministic and stochastic (mini-batch) settings, and quantify how the convergence times depend on the level of asynchrony. The practical advantages of our method are illustrated through numerical experiments on classification problems of varying complexities and scales.", "title_embedding_index": 2272, "title_abs_embedding_index": 2297}, {"title": "On Linear Representations and Pretraining Data Frequency in Language Models", "link_suffix": "/forum?id=EDoD3DgivF", "link": "https://openreview.net/forum?id=EDoD3DgivF", "pdf_link": "https://openreview.net/pdf?id=EDoD3DgivF", "keywords": "pretraining data, pretraining, linear, linear features, interpretability, linear representations, corpus frequency", "abstract": "Pretraining data has a direct impact on the behaviors and quality of language models (LMs), but we only understand the most basic principles of this relationship. While most work focuses on pretraining data's effect on downstream task behavior, we investigate its relationship to LM representations. Previous work has discovered that, in language models, some concepts are encoded as ``linear representations'', but what factors cause these representations to form (or not)? We study the connection between differences in pretraining data frequency and differences in trained models' linear representations of factual recall relations. We find evidence that the two are linked, with the formation of linear representations strongly connected to pretraining term frequencies. First, we establish that the presence of linear representations for subject-relation-object (s-r-o) fact triplets is highly correlated with both subject-object co-occurrence frequency and in-context learning accuracy. This is the case across all phases of pretraining, i.e., it is not affected by the model's underlying capability. In OLMo 7B and GPT-J (6B), we discover that a linear representation consistently (but not exclusively) forms when the subjects and objects within a relation co-occur at least 1-2k times, regardless of when these occurrences happen during pretraining. In the OLMo 1B model, consistent linearity only occurs after 4.4k occurrences, suggesting a connection to scale. Finally, we train a regression model on measurements of linear representation quality that can predict how often a term was seen in pretraining. We show such model achieves low error even for a different model and pretraining dataset, providing a new unsupervised method for exploring possible data sources of closed-source models. We conclude that the presence or absence of linear representations in LMs contains signal about their pretraining corpora that may provide new avenues for controlling and improving model behavior. We release our code to support future work.", "title_embedding_index": 2273, "title_abs_embedding_index": 2298}, {"title": "Differentiation Through Black-Box Quadratic Programming Solvers", "link_suffix": "/forum?id=S5wIXxlvfw", "link": "https://openreview.net/forum?id=S5wIXxlvfw", "pdf_link": "https://openreview.net/pdf?id=S5wIXxlvfw", "keywords": "optimization, differentiable optimization, quadratic programming", "abstract": "In recent years, many deep learning approaches have incorporated layers that solve optimization problems (e.g., linear, quadratic, and semidefinite programs). Integrating these optimization problems as differentiable layers requires computing the derivatives of the optimization problem's solution with respect to its objective and constraints. This has so far prevented the use of state-of-the-art black-box numerical solvers within neural networks, as they lack a differentiable interface. To address this issue for one of the most common convex optimization problems - quadratic programming (QP) - we introducedQP, a modular framework that enables plug-and-play differentiation for any QP solver, allowing seamless integration into neural networks and bi-level optimization tasks. Our solution is based on the core theoretical insight that knowledge of the active constraint set at the QP optimum allows forexplicitdifferentiation. This insight reveals a unique relationship between the computation of the solution and its derivative, enabling efficient differentiation of any solver, that only requires the primal solution. Our implementation, which will be made publicly available, interfaces with an existing framework that supports over 15 state-of-the-art QP solvers, providing each with a fully differentiable backbone for immediate use as a differentiable layer in learning setups. To demonstrate the scalability and effectiveness of dQP, we evaluate it on a large benchmark dataset of QPs with varying structures. We compare dQP with existing differentiable QP methods, demonstrating its advantages across a range of problems, from challenging small and dense problems to large-scale sparse ones, including a novel bi-level geometry optimization problem.", "title_embedding_index": 2274, "title_abs_embedding_index": 2299}]