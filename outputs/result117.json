[
    {
        "title": "INSTRUCTION-FOLLOWING LLMS FOR TIME SERIES PREDICTION: A TWO-STAGE MULTIMODAL APPROACH",
        "link_suffix": "/forum?id=01wMplF8TL",
        "link": "https://openreview.net/forum?id=01wMplF8TL",
        "pdf_link": "https://openreview.net/pdf?id=01wMplF8TL",
        "keywords": "Large Language Models, Time-series Prediction, Multi-modal, Instruction-following",
        "abstract": "We introduce Text-Informed Time Series Prediction (TITSP), an innovative multimodal framework that integrates textual knowledge with temporal dynamics using Large Language Models (LLMs). TITSP employs a two-stage process that bridges numerical data with rich contextual information for enhanced forecasting accuracy and interpretability.In the first stage, we present AutoPrompter, which captures temporal dependencies from time series data and aligns them with semantically meaningful text embeddings.In the second stage, these aligned embeddings are refined by incorporating task-specific textual instructions through LLM. We evaluate TITSP on several multimodal time series prediction tasks, demonstrating substantial improvements over state-of-the-art baselines. Quantitative results reveal significant gains in predictive performance, while qualitative analyses show that textual context enhances interpretability and actionable insights. Our findings indicate that integrating multimodal inputs not only improves prediction accuracy but also fosters more intuitive, user-centered forecasting"
    },
    {
        "title": "Gradient-based Jailbreak Images for Multimodal Fusion Models",
        "link_suffix": "/forum?id=wNg0LibmQt",
        "link": "https://openreview.net/forum?id=wNg0LibmQt",
        "pdf_link": "https://openreview.net/pdf?id=wNg0LibmQt",
        "keywords": "jailbreak, adversarial examples, multimodal, language models",
        "abstract": "Augmenting language models with image inputs may enable more effective jailbreak attacks through continuous optimization, unlike text inputs that require discrete optimization. However, newmultimodal fusion modelstokenize all input modalities using non-differentiable functions, which hinders straightforward attacks. In this work, we introduce the notion of atokenizer shortcutthat approximates tokenization with a continuous function and enables continuous optimization. We use tokenizer shortcuts to create the first end-to-end gradient image attacks against multimodal fusion models. We evaluate our attacks on Chameleon models and obtain jailbreak images that elicit harmful information for 72.5% of prompts. Jailbreak images outperform text jailbreaks optimized with the same objective and require 3x lower compute budget to optimize 50x more input tokens. Finally, we find that representation engineering defenses, like Circuit Breakers, trained only on text attacks can effectively transfer to adversarial image inputs."
    },
    {
        "title": "Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for Fine-Tuning Large Language Models",
        "link_suffix": "/forum?id=vqJZb9SX1T",
        "link": "https://openreview.net/forum?id=vqJZb9SX1T",
        "pdf_link": "https://openreview.net/pdf?id=vqJZb9SX1T",
        "keywords": "zeroth-order optimization, large language models",
        "abstract": "Fine-tuning is powerful for adapting large language models to downstream tasks, but it often results in huge memory usages. \nA promising approach to mitigate this is using Zeroth-Order (ZO) optimization, which estimates gradients to replace First-Order (FO) gradient calculations, albeit with longer training time due to its stochastic nature. \nBy revisiting the Memory-efficient ZO (MeZO) optimizer, we discover that the full-parameter perturbation and updating processes consume over 50% of its overall fine-tuning time cost. \nBased on these observations, we introduce a novel layer-wise sparse computation and memory efficient ZO optimizer, named LeZOLeZO treats layers as fundamental units for sparsification and dynamically perturbs different parameter subsets in each step to achieve full-parameter fine-tuning. \nLeZO incorporates layer-wise parameter sparsity in the process of simultaneous perturbation stochastic approximation (SPSA) and ZO stochastic gradient descent (ZO-SGD). \nIt achieves accelerated computation during perturbation and updating processes without additional memory overhead.\nWe conduct extensive experiments with the OPT model family on the SuperGLUE benchmark and two generative tasks. \nThe experiments show that LeZO accelerates training without compromising the performance of ZO optimization.\nSpecifically, it achieves over $3 \\times$ speedup compared to MeZO on the SST-2, BoolQ, and Copa tasks."
    },
    {
        "title": "Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words",
        "link_suffix": "/forum?id=HpUs2EXjOl",
        "link": "https://openreview.net/forum?id=HpUs2EXjOl",
        "pdf_link": "https://openreview.net/pdf?id=HpUs2EXjOl",
        "keywords": "Sparse Autoencoder, Mechanistic Interpretability, Polysemantic Representation.",
        "abstract": "Sparse Autoencoders (SAEs) have gained a lot of attention as a promising tool for improving interpretability in large language models (LLMs). \nHowever, traditional evaluation metrics like Mean Squared Error and $\\text{L}_{0}$ sparsity ignore the evaluation of the representational power of SAEs --- whether they can acquire monosemantic features without sacrifices.\nIn this paper, we propose a suite of evaluations for SAEs to quantify the simple and monosemantic features by taking into account polysemous words.\nOur findings reveal that SAEs developed to improve the MSE-$\\text{L}_0$ Pareto frontier do not necessarily enhance the extraction of monosemantic features, supporting the need for our semantics-focused evaluation.\nThrough evaluations of SAEs across different layers of LLMs and Transformer components (Residual, MLP, Attention),  we find that deeper layers and the Attention mechanism contribute to distinguishing polysemous words.\nOur results offer new insights into the validity of the existing goals of SAEs and contribute to the development of more practical SAEs."
    },
    {
        "title": "Tool-Planner: Task Planning with Clusters across Multiple Tools",
        "link_suffix": "/forum?id=dRz3cizftU",
        "link": "https://openreview.net/forum?id=dRz3cizftU",
        "pdf_link": "https://openreview.net/pdf?id=dRz3cizftU",
        "keywords": "Tool Learning, Task Planning, LLM agent",
        "abstract": "Large language models (LLMs) have demonstrated exceptional reasoning capabilities, enabling them to solve various complex problems. Recently, this ability has been applied to the paradigm of tool learning. Tool learning involves providing examples of tool usage and their corresponding functions, allowing LLMs to formulate plans and demonstrate the process of invoking and executing each tool. LLMs can address tasks that they cannot complete independently, thereby enhancing their potential across different tasks. However, this approach faces two key challenges. First, redundant error correction leads to unstable planning and long execution time. Additionally, designing a correct plan among multiple tools is also a challenge in tool learning. To address these issues, we propose Tool-Planner, a task-processing framework based on toolkits. Tool-Planner groups tools based on the API functions with the same function into a toolkit and allows LLMs to implement planning across the various toolkits. When a tool error occurs, the language model can reselect and adjust tools based on the toolkit. Experiments show that our approach demonstrates a high pass and win rate across different datasets and optimizes the planning scheme for tool learning in models such as GPT-4 and Claude 3, showcasing the potential of our method."
    },
    {
        "title": "Locating Information in Large Language Models via Random Matrix Theory",
        "link_suffix": "/forum?id=MmWkNmeDNE",
        "link": "https://openreview.net/forum?id=MmWkNmeDNE",
        "pdf_link": "https://openreview.net/pdf?id=MmWkNmeDNE",
        "keywords": "Random matrix theory, RMT, Llama 3, Spectra, singular value decomposition",
        "abstract": "As large language models (LLMs) become central to AI applications, gaining a deeper understanding of their inner workings is increasingly important.   In this work, we analyze the weight matrices of pretrained transformer models -- specifically BERT and Llama -- using random matrix theory (RMT) as a zero-information hypothesis. While randomly initialized weights perfectly agree with RMT predictions, deviations emerge after training, allowing us to locate learned structures within the models. We identify layer-type specific behaviors that are consistent across all blocks and architectures considered. By pinpointing regions that deviate from RMT predictions, we highlight areas of feature learning and confirm this through comparisons with the activation covariance matrices of the corresponding layers.  Our method provides a diagnostic tool for identifying relevant regions in transformer weights using only the trained matrices.  Additionally, we address the ongoing debate regarding the significance of small singular values in the context of finetuning and alignment in LLMs.  Our findings reveal that, after finetuning, small singular values play a crucial role in the models' capabilities, suggesting that removing them in an already aligned transformer can be detrimental, as it may compromise model alignment."
    },
    {
        "title": "StepTool: A Step-grained Reinforcement Learning Framework for Tool Learning in LLMs",
        "link_suffix": "/forum?id=PNHjoWcQje",
        "link": "https://openreview.net/forum?id=PNHjoWcQje",
        "pdf_link": "https://openreview.net/pdf?id=PNHjoWcQje",
        "keywords": "Tool Learning, Large Language Models, Reinforcement Learning",
        "abstract": "Despite having powerful reasoning and inference capabilities, Large Language Models (LLMs) still need external tools to acquire real-time information or domain-specific expertise to solve complex tasks, which is referred to as tool learning. Existing tool learning methods primarily rely on tuning with expert trajectories, focusing on token-sequence learning from a linguistic perspective. However, there are several challenges: 1) imitating static trajectories limits their ability to generalize to new tasks. 2) even expert trajectories can be suboptimal, and better solution paths may exist. In this work, we introduce StepTool, a novel step-grained reinforcement learning framework to improve tool learning in LLMs. It consists of two components: Step-grained Reward Shaping, which assigns rewards at each tool interaction based on tool invocation success and its contribution to the task, and Step-grained Optimization, which uses policy gradient methods to optimize the model in a multi-step manner. Experimental results demonstrate that StepTool significantly outperforms existing methods in multi-step, tool-based tasks, providing a robust solution for complex task environments."
    },
    {
        "title": "Effective LLM Knowledge Learning Requires Rethinking Generalization",
        "link_suffix": "/forum?id=sNycNM577m",
        "link": "https://openreview.net/forum?id=sNycNM577m",
        "pdf_link": "https://openreview.net/pdf?id=sNycNM577m",
        "keywords": "knowledge learning, generalization, large language models, knowledge acquisition",
        "abstract": "Large language models (LLMs) are trained on a substantial amount of documents that contain extensive world knowledge. However, it is still not well-understood how knowledge is acquired via autoregressive pre-training and extracted via question-answering. This lack of understanding greatly hinders effective knowledge learning, especially for continued pre-training on up-to-date information, as this evolving information often does not have diverse repetitions like foundational knowledge. In this paper, we focus on understanding and improving LLM knowledge learning. We found and verified that knowledge learning for LLMs can be deemed as an implicit supervised task hidden in the autoregressive pre-training objective. Our findings suggest that knowledge learning for LLMs would benefit from methods designed to improve generalization ability for supervised tasks. Based on our analysis, we propose to diversify training documents\u2019 formats as data augmentation to grow in-distribution samples. This data augmentation method does not present the risk of altering the facts embedded in documents as text paraphrasing. We also introduce sharpness-aware minimization as an effective optimization algorithm to better improve generalization. Moreover, we adapt our method to instruction tuning for generalization to various phrasings of questions. Extensive experiment results validate our findings and demonstrate our methods\u2019 effectiveness in improving knowledge learning in both the continued pre-training and instruction tuning stages. This paper offers new perspectives and insights to interpret and design effective strategies for LLM knowledge learning."
    },
    {
        "title": "Direct Imitation Learning: RLHF Secretly Performs Imitation Learning",
        "link_suffix": "/forum?id=2QdsjiNXgj",
        "link": "https://openreview.net/forum?id=2QdsjiNXgj",
        "pdf_link": "https://openreview.net/pdf?id=2QdsjiNXgj",
        "keywords": "Alignment",
        "abstract": "This work studies the alignment of large language models with preference data. We address this problem from a novel imitation learning (IL) perspective. We establish a close connection between alignment and imitation learning, which shows that existing alignment objectives implicitly align model and preference data distributions. Built upon this connection, we develop a principled method DIL to\ndirectly optimize the imitation learning objective. DIL derives a surrogate objective for imitation learning with direct density ratio estimates, allowing effective use of preference data. DIL eliminates the need for complex adversarial training required by current IL methods, and optimizes the IL objective through simple density ratio estimation losses, achieving lightweight and efficient fine-tuning for large language\nmodels. This paper provides a unified imitation learning perspective on alignment, encompassing existing algorithms as special cases while naturally introducing new variants. Bridging IL and RLHF, DIL opens up new opportunities to improve alignment by leveraging tools from imitation learning. Extensive experiments demonstrate that DIL consistently and significantly outperforms off-the-shelf methods on\nvarious challenging benchmarks, including Open LLM Leadboard and AlpacaEval 2.0. Code for DIL is available athttps://github.com/Code-DIL/DIL."
    },
    {
        "title": "Bridging Lottery Ticket and Grokking: Understanding Grokking from Inner Structure of Networks",
        "link_suffix": "/forum?id=8iH8YHrGTh",
        "link": "https://openreview.net/forum?id=8iH8YHrGTh",
        "pdf_link": "https://openreview.net/pdf?id=8iH8YHrGTh",
        "keywords": "Grokking, Lottery ticket, Generalization, Representation",
        "abstract": "Grokking is the intriguing phenomenon of delayed generalization: networks initially memorize training data with perfect accuracy but poor generalization, then transition to a generalizing solution with continued training. While reasons for this delayed generalization, such as weight norms and sparsity, have been discussed, the influence of network structure, particularly the role of subnetworks, still needs to be explored.\nIn this work, we link the grokking phenomenon to the lottery ticket hypothesis to investigate the impact of inner network structures. \nWe demonstrate that using lottery tickets obtained at the generalizing phase (`grokking tickets') significantly reduces delayed generalization on various tasks, including multiple modular arithmetic, polynomial regression, sparse parity, and MNIST. For example, lottery tickets accelerate the grokking (transition from memorization to generalization) up to \\emph{1/65} compared to dense networks in modular addition.\nThrough a series of controlled experiments, our findings reveal that neither small weight norms nor sparsity alone account for the reduction of delayed generalization; instead, the presence of a good subnetwork structure is crucial. Analyzing the transition from memorization to generalization, we observe that rapid changes in subnetwork structures, measured by the Jaccard distance, strongly correlate with improvements in test accuracy. We further show that pruning techniques can accelerate the grokking process, transforming a memorizing network into a generalizing one without updating the weights. By demonstrating that good subnetworks are key to achieving generalization and that pruning can expedite this process, we provide new insights into the mechanisms underlying neural network generalization."
    },
    {
        "title": "Autoencoders for Anomaly Detection are Unreliable",
        "link_suffix": "/forum?id=X8XQOLjLX6",
        "link": "https://openreview.net/forum?id=X8XQOLjLX6",
        "pdf_link": "https://openreview.net/pdf?id=X8XQOLjLX6",
        "keywords": "Anomaly Detection, Autoencoder, Reliability, AI Safety, Adversarial Attacks",
        "abstract": "Autoencoders are frequently used for anomaly detection, both in the unsupervised and semi-supervised settings. They rely on the assumption that when trained using the reconstruction loss, they will be able to reconstruct normal data more accurately than anomalous data. Some recent works have posited that this assumption may not always hold, but little has been done to study the validity of the assumption in theory. In this work we prove that this assumption indeed does not hold, and show that anomalies, lying far away from normal data, can be perfectly reconstructed in practice. We extend the understanding of autoencoders for anomaly detection by showing how they can perfectly reconstruct out of bounds, or interpolate undesirably, and note how this can be dangerous in safety critical applications. We connect theory to practice by showing that the proven behavior in linear autoencoders also occurs when applying non-linear autoencoders on both tabular data and real-world image data, the two primary application areas of autoencoders for anomaly detection."
    },
    {
        "title": "GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost",
        "link_suffix": "/forum?id=FoF5RaA3ug",
        "link": "https://openreview.net/forum?id=FoF5RaA3ug",
        "pdf_link": "https://openreview.net/pdf?id=FoF5RaA3ug",
        "keywords": "Dataset Distillation, Soft Label",
        "abstract": "Recent advancements in dataset distillation have demonstrated the significant benefits of employing soft labels generated by pre-trained teacher models.\n    In this paper, we introduce a novel perspective by emphasizing the full utilization of labels.\n    We first conduct a comprehensive comparison of various loss functions for soft label utilization in dataset distillation, revealing that the model trained on the synthetic dataset exhibits high sensitivity to the choice of loss function for soft label utilization.\n    This finding highlights the necessity of a universal loss function for training models on synthetic datasets.\n    Building on these insights, we introduce an extremely simple yet surprisingly effective plug-and-play approach, GIFT, which encompasses soft label refinement and a cosine similarity-based loss function to efficiently leverage full label information. \n    Extensive experiments indicate that GIFT consistently enhances state-of-the-art dataset distillation methods across various dataset scales without incurring additional computational costs.\n    Importantly, GIFT significantly enhances cross-optimizer generalization, an area previously overlooked.\n    For instance, on ImageNet-1K with IPC = 10, GIFT enhances the state-of-the-art method RDED by 30.8% in cross-optimizer generalization."
    },
    {
        "title": "In Search of Forgotten Domain Generalization",
        "link_suffix": "/forum?id=Fk3eod9aaD",
        "link": "https://openreview.net/forum?id=Fk3eod9aaD",
        "pdf_link": "https://openreview.net/pdf?id=Fk3eod9aaD",
        "keywords": "Out-of-Distribution Robustness, OOD generalization, Out-of-Domain Robustness, Evaluation",
        "abstract": "Out-of-Domain (OOD) generalization is the ability of a model trained on one or more domains to generalize to unseen domains. In the ImageNet era of computer vision, evaluation sets for measuring a model's OOD performance were designed to be strictly OOD with respect to style. However, the emergence of foundation models and expansive web-scale datasets has obfuscated this evaluation process, as datasets cover a broad range of domains and risk test domain contamination. In search of the forgotten domain generalization, we create large-scale datasets subsampled from LAION---LAION-Natural and LAION-Rendition---that are strictly OOD to corresponding ImageNet and DomainNet test sets in terms of style. Training CLIP models on these datasets reveals that a significant portion of their performance is explained by in-domain examples. This indicates that the OOD generalization challenges from the ImageNet era still prevail and that training on web-scale data merely creates the illusion of OOD generalization. Furthermore, through a systematic exploration of combining natural and rendition datasets in varying proportions, we identify optimal mixing ratios for model generalization across these domains. Our datasets and results re-enable meaningful assessment of OOD robustness at scale---a crucial prerequisite for improving model robustness."
    },
    {
        "title": "Decoding Generalization from Memorization in Deep Neural Networks",
        "link_suffix": "/forum?id=z4bfNsrum4",
        "link": "https://openreview.net/forum?id=z4bfNsrum4",
        "pdf_link": "https://openreview.net/pdf?id=z4bfNsrum4",
        "keywords": "Generalization, Memorization",
        "abstract": "Overparameterized Deep Neural Networks that generalize well have been key to the dramatic success of Deep Learning in recent years. The reasons for their remarkable ability to generalize are not well understood yet. It has also been known that deep networks possess the ability to memorize training data, as evidenced by perfect or high training accuracies on models trained with corrupted data that have class labels shuffled to varying degrees. Concomitantly, such models are known to generalize poorly, i.e. they suffer from poor test accuracies, due to which it is thought that the act of memorizing substantially degrades the ability to generalize. It has, however, been unclear why the poor generalization that accompanies such memorization, comes about. One possibility is that in the process of training with corrupted data, the layers of the network irretrievably re-organize their representations in a manner that makes generalization difficult. The other possibility is that the network retains significant ability to generalize, but the trained network somehow \u201cchooses\u201d to readout in a manner that is detrimental to generalization. Here, we provide evidence for the latter possibility by demonstrating, empirically, that such models possess information in their representations for substantially improved generalization, even in the face of memorization. Furthermore, such generalization abilities can be easily decoded from the internals of the trained model, and we build a technique to do so from the outputs of specific layers of the network. In particular, we show the following: (1) For models trained using standard methods & datasets with corrupted training data, while the model has poor test accuracy, we can build a simple classifier with dramatically better test accuracy that uses only the model's hidden layer outputs obtained for the (corrupted) training set. (2) For the aforementioned models, if the true training class labels are known post hoc, i.e. after the model is trained, we can build a simple classifier, with significantly better generalization performance than in (1). This is true, in many cases, even for models where training class labels are shuffled with equal probability. This demonstrates that the layers of the network maintain representations in a manner that is amenable to straightforward generalization to a degree not previously recognized. (3) On the other hand, we asked if a model trained on the true training labels similarly retained the capability to memorize easily.  Adapting our technique to this setting, we find that in a few cases, we can extract a high degree of memorization. The same classifier sometimes exhibits high test accuracy (on the true test labels), which further supports the idea that generalization can co-exist with memorization. Together, these results suggest a more nuanced view of the interplay of generalization with memorization in Deep Learning and suggest the need for further experiments and theory to better understand this phenomenon."
    },
    {
        "title": "Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks",
        "link_suffix": "/forum?id=rnJxelIZrq",
        "link": "https://openreview.net/forum?id=rnJxelIZrq",
        "pdf_link": "https://openreview.net/pdf?id=rnJxelIZrq",
        "keywords": "Jailbreak Attack, LLMs, LLM Security, AI Security",
        "abstract": "Recent jailbreak attempts on Large Language Models (LLMs) have shifted from algorithm-focused to human-like social engineering attacks, with persuasion-based techniques emerging as a particularly effective subset. These attacks evolve rapidly, demonstrate high creativity, and boast superior attack success rates. To combat such threats, we propose a promising approach to enhancing LLM safety by leveraging the underlying geometry of input prompt token embeddings using hypergraphs. This approach allows us to model the differences in information flow between benign and malicious LLM prompts.In our approach, each LLM prompt is represented as a metric hypergraph, forming a compact metric space. We then construct a higher-order metric space over these compact metric hypergraphs using the Gromov-Hausdorff distance as a generalized metric. Within this space of metric hypergraph spaces, our safety filter learns to classify between harmful and benign prompts. Our study presents theoretical guarantees on the classifier's generalization error for novel and unseen LLM input prompts. Extensive empirical evaluations demonstrate that our method significantly outperforms both existing state-of-the-art generic defense mechanisms and naive baselines. Notably, our approach also achieves comparable performance to specialized defenses against algorithm-focused attacks."
    },
    {
        "title": "Diffusion Models for 4D Novel View Synthesis",
        "link_suffix": "/forum?id=d2UrCGtntF",
        "link": "https://openreview.net/forum?id=d2UrCGtntF",
        "pdf_link": "https://openreview.net/pdf?id=d2UrCGtntF",
        "keywords": "generative, models, diffusion, novel, view, synthesis, 3d, 4d, scenes",
        "abstract": "We present 4DiM, a cascaded diffusion model for 4D novel view synthesis (NVS), supporting generation with arbitrary camera trajectories and timestamps, in natural scenes, conditioned on one or more images. With a novel architecture and sampling procedure, we enable training on a mixture of 3D (with camera pose), 4D (pose+time) and video (time but no pose) data, which greatly improves generalization to unseen images and camera pose trajectories over prior works which generally operate in limited domains (e.g., object centric).\n4DiM is the first-ever NVS method with intuitive metric-scale camera pose control enabled by our novel calibration pipeline for structure-from-motion-posed data. Experiments demonstrate that 4DiM outperforms prior 3D NVS models both in terms of \nimage fidelity and pose alignment, while also enabling the generation of scene dynamics. 4DiM provides a general framework for a variety of tasks including single-image-to-3D, two-image-to-video (interpolation and extrapolation), and pose-conditioned video-to-video translation, which we illustrate qualitatively on a variety of scenes.\nSeehttps://anonymous-4d-diffusion.github.iofor video samples."
    },
    {
        "title": "How big does your neural network have to be?: A Scaling Law Study in Multi-Spectral Remote Sensing",
        "link_suffix": "/forum?id=py3RTHNT6J",
        "link": "https://openreview.net/forum?id=py3RTHNT6J",
        "pdf_link": "https://openreview.net/pdf?id=py3RTHNT6J",
        "keywords": "machine learning, remote sensing",
        "abstract": "Remote sensing imagery from systems such as Sentinel provides full coverage of the Earth's surface at around 10 meter resolution. The remote sensing community has transitioned to extensive use of deep learning models based on their high performance on benchmarks such as the ISPRS Vaihingen. Convolutional models such as UNet and ResNet variations are commonly employed for remote sensing but typically only accept three channels due to their development for RGB imagery, while Sentinel satellite systems have more than 10. Recently, a number of transformer architectures have also been proposed for remote sensing, but they typically have not been extensively benchmarked and have only been employed on rather small datasets. Meanwhile, it is becoming possible to obtain dense spatial land-use labels for entire first-level administrative divisions of some countries. Scaling law observations indicate that substantially larger, multi-spectral transformer models may provide a huge leap in the performance of remote sensing models in these settings. In this work, we develop a family of multi-spectral transformer models, which we evaluate across orders of magnitude differences in model parameters to evaluate their performance and scaling effectiveness on a densely labeled imagery dataset. We develop a novel multi-spectral attention strategy and demonstrate its effectiveness through ablations. We further show in this setting that models many orders of magnitude larger than conventional architectures such as UNet lead to substantial improvements in accuracy: a UNet++ model with 23M parameters results in less than 65% accuracy, while a multi-spectral transformer with 655M parameters yields an accuracy of over 95% on the Biological Valuation Map of Flanders.\nA link to open source code will be provided in the camera ready document."
    },
    {
        "title": "Decomposed Learning and Grokking",
        "link_suffix": "/forum?id=7Cx05z4pUc",
        "link": "https://openreview.net/forum?id=7Cx05z4pUc",
        "pdf_link": "https://openreview.net/pdf?id=7Cx05z4pUc",
        "keywords": "grokking, optimisation, linear algebra, SVD, compression",
        "abstract": "Grokking is a delayed transition from memorisation to generalisation in neural networks. It poses challenges for efficient learning, particularly in structured tasks and small-data regimes. This paper explores grokking in modular arithmetic, explicitly focusing on modular division with a modulus of 97. We introduce a novel learning method called Decomposed Learning, which leverages Singular Value Decomposition (SVD) to modify the weight matrices of neural networks. Decomposed learning reduces or avoids grokking by changing the representation of the weight matrix, $A$, into the product of three matrices $U$, $\\Sigma$ and $V^T$,  promoting the discovery of compact, generalisable representations early in the learning process. Through empirical evaluations on the modular division task, we show that Decomposed Learning significantly reduces the effect of grokking and, in some cases, eliminates it. Moreover, Decomposed Learning can reduce the parameters required for practical training, enhancing model efficiency and generalisation. These results suggest that our SVD-based method provides a practical and scalable solution for mitigating grokking, with implications for broader transformer-based learning tasks."
    },
    {
        "title": "Distributed Constrained Optimal  Consensus  Under a Directed Graph",
        "link_suffix": "/forum?id=CLVMAUDeJz",
        "link": "https://openreview.net/forum?id=CLVMAUDeJz",
        "pdf_link": "https://openreview.net/pdf?id=CLVMAUDeJz",
        "keywords": "Constrained optimal consensus, multi-agent systems, set constraints, general constraints",
        "abstract": "In  this paper, the distributed  constrained optimal consensus problem of multi-agent systems under a directed graph  is investigated. We propose two projection-based distributed constrained optimal consensus algorithms: one addressing set constraints and the other tailored for general constraints. Only the relative state is exchanged among agents in these two algorithms. In the stability analysis of case with set constraints, we transform the distributed  optimization problem  into a constrained leaderless consensus problem by adopting a sliding mode approach. Building on this foundational transformation, we further develop a projection-based  distributed constrained optimal consensus algorithm to  address  general constraints. It is shown that the proposed algorithm achieves an ergodic convergence rate of  $O(\\frac{1}{k})$  with respect to the first-order optimality residuals. Numerical simulations are conducted to validate the effectiveness of our theoretical results."
    },
    {
        "title": "AN INFORMATION THEORETIC EVALUATION METRIC FOR STRONG UNLEARNING",
        "link_suffix": "/forum?id=NGF1wDDBMm",
        "link": "https://openreview.net/forum?id=NGF1wDDBMm",
        "pdf_link": "https://openreview.net/pdf?id=NGF1wDDBMm",
        "keywords": "Strong unlearning, information-theoretic metrics, Information Difference Index, residual information, evaluation metric",
        "abstract": "Machine unlearning (MU) aims to remove the influence of specific data from trained models, addressing privacy concerns and ensuring compliance with regulations such as the \"right to be forgotten.\"\nEvaluating strong unlearning, where the unlearned model is indistinguishable from one retrained without the forgetting data, remains a significant challenge in deep neural networks (DNNs).\nCommon black-box metrics, such as variants of membership inference attacks and accuracy comparisons, primarily assess model outputs but often fail to capture residual information in intermediate layers.\nTo bridge this gap, we introduce the Information Difference Index (IDI), a novel white-box metric inspired by information theory.\nIDI quantifies retained information in intermediate features by measuring mutual information between those features and the labels to be forgotten, offering a more comprehensive assessment of unlearning efficacy.\nOur experiments demonstrate that IDI effectively measures the degree of unlearning across various datasets and architectures,\nproviding a reliable tool for evaluating strong unlearning in DNNs."
    },
    {
        "title": "SPECTRUM: Empowering Online Handwriting Verification via Temporal-Frequency Multimodal Representation Learning",
        "link_suffix": "/forum?id=ori83fBg71",
        "link": "https://openreview.net/forum?id=ori83fBg71",
        "pdf_link": "https://openreview.net/pdf?id=ori83fBg71",
        "keywords": "Online handwriting verification; Multimodal representation learning; Temporal and frequency learning; Handwritten biometrics",
        "abstract": "Tapping into the uncharted multimodal representation learning in online handwriting verification (OHV), we propose SPECTRUM, a temporal-frequency synergistic model tailored to enhance handwriting representations. SPECTRUM comprises three core components: (1) a multi-scale interactor that interweaves fine-grained temporal and frequency features across multiple scales through complementary domain interaction; (2) a self-gated fusion module, dynamically integrating global temporal and frequency features via self-driven balancing. Collectively, these two components achieve micro-to-macro multimodal integration; (3) a multimodal distance-based verifier that fully harnesses temporal and frequency representations, sharpening genuine-forged discrimination beyond conventional temporal-only approaches. Extensive experiments demonstrate SPECTRUM's pronounced outperformance over existing OHV methods. Furthermore, we reveal that incorporating multiple handwritten biometrics fundamentally improves the discriminatory power of individual writing features. These findings not only validate the efficacy of multimodal learning in OHV but also encourage broader multimodal research across both feature and biometric domains, potentially opening new avenues for future explorations. Code will be publicly available."
    },
    {
        "title": "Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation",
        "link_suffix": "/forum?id=Glm7Kj47nN",
        "link": "https://openreview.net/forum?id=Glm7Kj47nN",
        "pdf_link": "https://openreview.net/pdf?id=Glm7Kj47nN",
        "keywords": "Text-to-3D; Geometry Image; Non-watertight mesh; Data efficiency",
        "abstract": "Generating high-quality 3D objects from textual descriptions remains a challenging problem due to high computational costs, the scarcity of 3D data, and the complexity of 3D representations. We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to efficiently represent 3D shapes using 2D images, thereby avoiding the need for complex 3D-aware architectures. By integrating a Collaborative Control mechanism, we exploit the rich 2D priors of existing Text-to-Image models, such as Stable Diffusion, to achieve strong generalization despite limited 3D training data. This allows us to use only high-quality training data while retaining compatibility with guidance techniques such as IPAdapter. GIMDiffusion enables the generation of 3D assets at speeds comparable to current Text-to-Image models, without being restricted to manifold meshes during either training or inference. We simultaneously generate a UV unwrapping for the objects, consisting of semantically meaningful parts as well as internal structures, enhancing both usability and versatility."
    },
    {
        "title": "TDR-HGN:Residual-enhanced heterogeneous graph networks for topology-driven feature completion",
        "link_suffix": "/forum?id=kp8T7G9hIh",
        "link": "https://openreview.net/forum?id=kp8T7G9hIh",
        "pdf_link": "https://openreview.net/pdf?id=kp8T7G9hIh",
        "keywords": "Heterogeneous graph networks, feature completion, topological features, residual networks, meta-path",
        "abstract": "Heterogeneous graphs are composed of multiple types of edges and nodes. The existing heterogeneous graph neural network can be understood as a node feature smoothing process guided by the graph structure, which can accurately simulate complex relationships in the real world. However, due to real-world privacy and data scarcity, some node features are inevitably missing. Furthermore, as model depth increases and multiple types of meta-paths are aggregated, node embeddings tend to be consistent, leading to semantic confusion and overfitting problems. To improve the quality of node embeddings, we propose topology-driven residual boosting network (TDR-HGN). It introduces one-hot encoding and node type encoding to generate initial features, uses topological structure features to guide feature completion, combines residual networks to deal with semantic confusion and over-fitting problems, and builds neighbor-based high-order graph networks through meta-paths to achieve feature enhancement. We conduct extensive experiments on three heterogeneous graph datasets, and the results show that TDR-HGN can significantly improve the performance compared to other methods."
    },
    {
        "title": "Truth-value judgment in language models: belief directions are context sensitive",
        "link_suffix": "/forum?id=guyICBe4p1",
        "link": "https://openreview.net/forum?id=guyICBe4p1",
        "pdf_link": "https://openreview.net/pdf?id=guyICBe4p1",
        "keywords": "interpretability, truth directions, LLM beliefs, large language model, llm",
        "abstract": "Recent work has demonstrated that the latent spaces of large language models\n(LLMs) contain directions predictive of the truth of sentences. Multiple methods recover such directions and build probes that are described as uncovering\na model\u2019s \u201cknowledge\u201d or \u201cbeliefs\u201d. We investigate this phenomenon, looking\nclosely at the impact of context on the probes. Our experiments establish where\nin the LLM the probe\u2019s predictions are (most) sensitive to the presence of related\nsentences, and how to best characterize this kind of sensitivity. We do so by\nmeasuring different types of consistency errors that occur after probing an LLM\nwhose inputs consist of hypotheses preceded by (negated) supporting and contradicting sentences. We also perform a causal intervention experiment, investigating whether moving the representation of a premise along these belief directions\ninfluences the position of an entailed or contradicted sentence along that same direction. We find that the probes we test are generally context sensitive, but that\ncontexts which should not affect the truth often still impact the probe outputs.\nOur experiments show that the type of errors depend on the layer, the model, and\nthe kind of data. Finally, our results suggest that belief directions are (one of the)\ncausal mediators in the inference process that incorporates in-context information."
    },
    {
        "title": "Persistent Pre-training Poisoning of LLMs",
        "link_suffix": "/forum?id=eiqrnVaeIw",
        "link": "https://openreview.net/forum?id=eiqrnVaeIw",
        "pdf_link": "https://openreview.net/pdf?id=eiqrnVaeIw",
        "keywords": "poisoning, pretraining, large language models, security",
        "abstract": "Large language models are pre-trained on uncurated text datasets consisting of trillions of tokens scraped from the Web.\nPrior work has shown that: (1) web-scraped pre-training datasets can be practically poisoned by malicious actors; and (2) adversaries can compromise language models after poisoning fine-tuning datasets.\nOur work evaluates for the first time whether language models can also be \\emph{compromised during pre-training}, with a focus on the persistence of pre-training attacks after models are fine-tuned as helpful and harmless chatbots (i.e., after SFT and DPO).\nWe pre-train a series of LLMs from scratch to measure the impact of a potential poisoning adversary under four different attack objectives (denial-of-service, belief manipulation, jailbreaking, and prompt stealing), and across a wide range of model sizes (from 600M to 7B).\nOur main result is that poisoning only 0.1% of a model's pre-training dataset is sufficient for three out of four attacks to measurably persist through post-training. Moreover, simple attacks like denial-of-service persist through post-training with a poisoning rate of only 0.001%."
    }
]