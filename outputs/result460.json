[{"title": "MindSimulator: Exploring Brain Concept Localization via Synthetic fMRI", "link_suffix": "/forum?id=vgt2rSf6al", "link": "https://openreview.net/forum?id=vgt2rSf6al", "pdf_link": "https://openreview.net/pdf?id=vgt2rSf6al", "keywords": "Neuroscience, fMRI encoding, Generative model, fMRI generation, fMRI functional localizer, Concept-selective voxel", "abstract": "Concept-selective regions within the human cerebral cortex exhibit significant activation in response to specific visual stimuli associated with particular concepts. Precisely localizing these regions stands as a crucial long-term goal in neuroscience to grasp essential brain functions and mechanisms. Conventional experiment-driven approaches hinge on manually constructed visual stimulus collections and corresponding brain activity recordings, constraining the support and coverage of concept localization. Additionally, these stimuli often consist of concept objects in unnatural contexts and are potentially biased by subjective preferences, thus prompting concerns about the validity and generalizability of the identified regions. To address these limitations, we propose a data-driven exploration approach. By synthesizing extensive brain activity recordings, we statistically localize various concept-selective regions. Our proposed MindSimulator leverages advanced generative technologies to learn the probability distribution of brain activity conditioned on concept-oriented visual stimuli. This enables the creation of simulated brain recordings that reflect real neural response patterns. Using the synthetic recordings, we successfully localize several well-studied concept-selective regions and validate them against empirical findings, achieving promising prediction accuracy. The feasibility opens avenues for exploring novel concept-selective regions and provides prior hypotheses for future neuroscience research.", "title_embedding_index": 22950, "title_abs_embedding_index": 22975}, {"title": "ImDy: Human Inverse Dynamics from Imitated Observations", "link_suffix": "/forum?id=br8YB7KMug", "link": "https://openreview.net/forum?id=br8YB7KMug", "pdf_link": "https://openreview.net/pdf?id=br8YB7KMug", "keywords": "Motion Understanding, Inverse Dynamics, Biomechanics", "abstract": "Inverse dynamics (ID), which aims at reproducing the driven torques from human kinematic observations, has been a critical tool for gait analysis. However, it is hindered from wider application to general motion due to its limited scalability. Conventional optimization-based ID requires expensive laboratory setups, restricting its availability. To alleviate this problem, we propose to exploit the recently progressive human motion imitation algorithms to learn human inverse dynamics in a data-driven manner. The key insight is that the human ID knowledge is implicitly possessed by motion imitators, though not directly applicable. In light of this, we devise an efficient data collection pipeline with state-of-the-art motion imitation algorithms and physics simulators, resulting in a large-scale human inverse dynamics benchmark as Imitated Dynamics (ImDy). ImDy contains over 150 hours of motion with joint torque and full-body ground reaction force data. With ImDy, we train a data-driven human inverse dynamics solver ImDyS(olver) in a fully supervised manner, which conducts ID and ground reaction force estimation simultaneously. Experiments on ImDy and real-world data demonstrate the impressive competency of ImDyS in human inverse dynamics and ground reaction force estimation. Moreover, the potential of ImDy(-S) as a fundamental motion analysis tool is exhibited with downstream applications. Our data and code would be made publicly available.", "title_embedding_index": 22951, "title_abs_embedding_index": 22976}, {"title": "Parameter-Efficient Fine-Tuning   via Circular Convolution", "link_suffix": "/forum?id=IZjBfdVRB0", "link": "https://openreview.net/forum?id=IZjBfdVRB0", "pdf_link": "https://openreview.net/pdf?id=IZjBfdVRB0", "keywords": "transfer learning, circular convolution, adaptation, efficiency", "abstract": "Low-Rank Adaptation (LoRA) has gained popularity for fine-tuning large foundation models, leveraging low-rank matrices $\\mathbf{A}$ and $\\mathbf{B}$ to represent weight changes (i.e., $\\Delta \\mathbf{W} = \\mathbf{B} \\mathbf{A}$). This method reduces trainable parameters and mitigates heavy memory consumption associated with full delta matrices by sequentially multiplying $\\mathbf{A}$ and $\\mathbf{B}$ with the activation. Despite its success, the intrinsic low-rank characteristic may limit its performance. Although several variants have been proposed to address this issue, they often overlook the crucial computational and memory efficiency brought by LoRA. In this paper, we propose Circular Convolution Adaptation (C$^3$A), which not only achieves high-rank adaptation with enhanced performance but also excels in both computational power and memory utilization. Extensive experiments demonstrate that C$^3$A consistently outperforms LoRA and its variants across various fine-tuning tasks.", "title_embedding_index": 22952, "title_abs_embedding_index": 22977}, {"title": "Goal-conditioned Reinforcement Learning with Subgoals Generated from Relabeling", "link_suffix": "/forum?id=m7lBCyROPP", "link": "https://openreview.net/forum?id=m7lBCyROPP", "pdf_link": "https://openreview.net/pdf?id=m7lBCyROPP", "keywords": "Goal-Conditioned Reinforcement Learning, Hindsight Experience Replay, Subgoal-based Approach", "abstract": "In goal-conditioned reinforcement learning (RL), the primary objective is to develop a goal-conditioned policy capable of reaching diverse desired goals, a process often hindered by sparse reward signals. To address the challenges associated with sparse rewards, existing approaches frequently employ hindsight relabeling, substituting original goals with achieved goals. However, these methods exhibit a tendency to prioritize the optimization of closer achieved goals during training, leading to the loss of potentially valuable information from the trajectory and low sample efficiency. Our key insight is that these achieved goals, generated from the same hindsight relabeling, can serve as effective subgoals to facilitate the learning of policies that reach possible long-horizon desired goals within the same trajectory. Leveraging this perspective, we propose a novel framework called Goal-Conditioned reinforcement learning with Q-BC (i.e, behavior cloning (BC)-regularized Q) and Subgoals (GCQS) for goal-conditioned RL. GCQS is a innovative goal-conditioned actor-critic framework that systematically exploits more trajectory information to improve policy learning and sample efficiency. Specifically, GCQS initially optimizes a Q-BC objective to facilitate learning policies that reach achieved goals effectively. Subsequently, these achieved goals are redefined as subgoals, which serve to enhance the goal-conditioned policies, thereby predicting better actions to reach the desired goals. Experimental results in simulated robotics environments demonstrate that GCQS significantly enhances sample efficiency and overall performance compared to existing goal-conditioned methods.", "title_embedding_index": 22953, "title_abs_embedding_index": 22978}, {"title": "CALoR: Towards Comprehensive Model Inversion Defense", "link_suffix": "/forum?id=ysZvK6b60c", "link": "https://openreview.net/forum?id=ysZvK6b60c", "pdf_link": "https://openreview.net/pdf?id=ysZvK6b60c", "keywords": "Privacy Leakage, Model Inversion, Defense", "abstract": "Model Inversion Attacks (MIAs) aim at recovering privacy-sensitive training data from the knowledge encoded in the released machine learning models. Recent advances in the MIA field have significantly enhanced the attack performance under multiple scenarios, posing serious privacy risks of Deep Neural Networks (DNNs). However, the development of defense strategies against MIAs is relatively backward to resist the latest MIAs and existing defenses fail to achieve further trade-off between model utility and model robustness. In this paper, we provide an in-depth analysis from the perspective of intrinsic vulnerabilities of MIAs, comprehensively uncovering the weaknesses inherent in the basic pipeline, which are partially investigated in the previous defenses. Building upon these new insights, we propose a robust defense mechanism, integratingConfidenceAdaptationandLow-Rank compression(CALoR). Our method includes a novel robustness-enhanced classification loss specially-designed for model inversion defenses and reveals the extraordinary effectiveness of compressing the classification header. With CALoR, we can mislead the optimization objective, reduce the leaked information and impede the backpropagation of MIAs, thus mitigating the risk of privacy leakage. Extensive experimental results demonstrate that our method achieves state-of-the-art (SOTA) defense performance against MIAs and exhibits superior generalization to existing defenses across various scenarios.", "title_embedding_index": 22954, "title_abs_embedding_index": 22979}, {"title": "A Super-Aligned Driving Generalist Is Your Cockpit", "link_suffix": "/forum?id=1FiMrJxPAM", "link": "https://openreview.net/forum?id=1FiMrJxPAM", "pdf_link": "https://openreview.net/pdf?id=1FiMrJxPAM", "keywords": "Driving Cockpit; Super alined; Driving Generalist", "abstract": "The intelligent driving cockpit, an important part of intelligent driving, needs to match different users' comfort, interaction, and safety needs. This paper aims to build a \\textbf{s}uper-\\textbf{a}ligned and \\textbf{ge}neralist \\textbf{dr}iving agent, \\textbf{sage deer}. Sage Deer achieves two highlights: (1) Super alignment: It achieves different reactions according to different people's preferences and biases. (2) Generalist: It can understand the user's physiological indicators, facial emotions, hand movements, body movements, driving scenarios, and behavioral decisions. (3) Multimodal: He can understand RGB, NIR, and depth video to build more robust perception, understanding, and reasoning. To achieve the above requirements, we design retrieval-enhanced multimodal frameworks. We collected multiple data sets and built a large-scale benchmark. This benchmark measures the sage deer's perceptual decision-making ability and the super alignment's accuracy.", "title_embedding_index": 22955, "title_abs_embedding_index": 22980}, {"title": "PIORF: Physics-Informed Ollivier-Ricci Flow for Long\u2013Range Interactions in Mesh Graph Neural Networks", "link_suffix": "/forum?id=qkBBHixPow", "link": "https://openreview.net/forum?id=qkBBHixPow", "pdf_link": "https://openreview.net/pdf?id=qkBBHixPow", "keywords": "graph neural network, fluid dynamics, simulation, mesh, physics, over-squashing, rewiring", "abstract": "Recently, data-driven simulators based on graph neural networks have gained attention in modeling physical systems on unstructured meshes. However, they struggle with long-range dependencies in fluid flows, particularly in refined mesh regions. This challenge, known as the 'over-squashing' problem, hinders information propagation. While existing graph rewiring methods address this issue to some extent, they only consider graph topology, overlooking the underlying physical phenomena. We propose Physics-Informed Ollivier--Ricci Flow (PIORF), a novel rewiring method that combines physical correlations with graph topology. PIORF uses Ollivier--Ricci curvature (ORC) to identify bottleneck regions and connects these areas with nodes in high-velocity gradient nodes, enabling long-range interactions and mitigating over-squashing. Our approach is computationally efficient in rewiring edges and can scale to larger simulations. Experimental results on 3 fluid dynamics benchmark datasets show that PIORF consistently outperforms baseline models and existing rewiring methods, achieving up to 26.2% improvement.", "title_embedding_index": 22956, "title_abs_embedding_index": 22981}, {"title": "MEDIC: Zero-shot Music Editing with Disentangled Inversion Control", "link_suffix": "/forum?id=3f8556SIEn", "link": "https://openreview.net/forum?id=3f8556SIEn", "pdf_link": "https://openreview.net/pdf?id=3f8556SIEn", "keywords": "Zero-shot Music Editing, Inversion Techniques, Attention Control", "abstract": "Text-guided diffusion models make a paradigm shift in audio generation, facilitating the adaptability of source audio to conform to specific textual prompts. Recent works introduce inversion techniques, like DDIM inversion, to zero-shot editing, exploiting pretrained diffusion models for audio modification. Nonetheless, our investigation exposes that DDIM inversion suffers from an accumulation of errors across each diffusion step, undermining its efficacy. Moreover,  existing editing methods fail to achieve effective complex non-rigid music editing while maintaining essential content preservation and high editing fidelity. To counteract these issues, we introduce the Disentangled Inversion technique to disentangle the diffusion process into triple branches, rectifying the deviated path of the source branch caused by DDIM inversion.  In addition, we propose the Harmonized Attention Control framework, which unifies the mutual self-attention control and cross-attention control with an intermediate Harmonic Branch to progressively achieve the desired harmonic and melodic information in the target music. Collectively, these innovations comprise the Disentangled Inversion Control (DIC)  framework, enabling accurate music editing while safeguarding content integrity. To benchmark audio editing efficacy, we introduce ZoME-Bench, a comprehensive music editing benchmark hosting 1,100 samples spread across ten distinct editing categories. This facilitates both zero-shot and instruction-based music editing tasks. Our method achieves unparalleled performance in edit fidelity and essential content preservation, outperforming contemporary state-of-the-art inversion techniques. Audio samples are available athttps://MEDIC-Zero.github.io. Both code and benchmark will be released.", "title_embedding_index": 22957, "title_abs_embedding_index": 22982}, {"title": "ControlAR: Controllable Image Generation with Autoregressive Models", "link_suffix": "/forum?id=BWuBDdXVnH", "link": "https://openreview.net/forum?id=BWuBDdXVnH", "pdf_link": "https://openreview.net/pdf?id=BWuBDdXVnH", "keywords": "controllable image generation, autoregressive models, autoregressive image generation, diffusion models, image generation", "abstract": "Autoregressive (AR) models have reformulated image generation as next-token prediction, demonstrating remarkable potential and emerging as strong competitors to diffusion models. However, control-to-image generation, akin to ControlNet, remains largely unexplored within AR models. Although a natural approach, inspired by advancements in Large Language Models, is to tokenize control images into tokens and prefill them into the autoregressive model before decoding image tokens, it still falls short in generation quality compared to ControlNet and suffers from inefficiency. To this end, we introduce ControlAR, an efficient and effective framework for integrating spatial controls into autoregressive image generation models. Firstly, we explore control encoding for AR models and propose a lightweight control encoder to transform spatial inputs (e.g., canny edges or depth maps) into control tokens. Then ControlAR exploits the conditional decoding method to generate the next image token conditioned on the per-token fusion between control and image tokens, similar to positional encodings. Compared to prefilling tokens, using conditional decoding significantly strengthens the control capability of AR models but also maintains the model efficiency. Furthermore, the proposed ControlAR surprisingly empowers AR models with arbitrary-resolution image generation via conditional decoding and specific controls. Extensive experiments can demonstrate the controllability of the proposed ControlAR for the autoregressive control-to-image generation across diverse inputs, including edges, depths, and segmentation masks. Furthermore, both quantitative and qualitative results indicate that ControlAR surpasses previous state-of-the-art\ncontrollable diffusion models, e.g., ControlNet++.", "title_embedding_index": 22958, "title_abs_embedding_index": 22983}, {"title": "EVF-SAM: Early Vision-Language Fusion For Text-Prompted Segment Anything Model", "link_suffix": "/forum?id=OYrqvAVXiQ", "link": "https://openreview.net/forum?id=OYrqvAVXiQ", "pdf_link": "https://openreview.net/pdf?id=OYrqvAVXiQ", "keywords": "referring image segmentation, vision-language models, multimodal models, segment anything", "abstract": "Segment Anything Model (SAM) has attracted widespread attention for its superior interactive segmentation capabilities with visual prompts while lacking further exploration of text prompts. In this paper, we empirically investigate what text prompt encoders (e.g., CLIP or LLM) are good for adapting SAM for referring expression segmentation and introduce the Early Vision-language Fusion-based SAM (EVF-SAM). EVF-SAM is a simple yet effective referring segmentation method which exploits multimodal prompts (i.e., image and text) and comprises a pre-trained vision-language model to generate referring prompts and a SAM for segmentation. Surprisingly, we observe that: (1) multimodal prompts and (2) vision-language models with early fusion (e.g., BEIT-3) are beneficial for prompting SAM for accurate referring segmentation. Our experiments show that the proposed EVF-SAM based on BEIT-3 can obtain state-of-the-art performance on RefCOCO/+/g for referring expression segmentation and demonstrate the superiority of prompting SAM with early vision-language fusion. In addition, the proposed EVF-SAM with 1.32B parameters achieves remarkably higher performance while reducing nearly 82% of parameters compared to previous SAM methods based on large multimodal models.", "title_embedding_index": 22959, "title_abs_embedding_index": 22984}, {"title": "LucidFusion: Generating 3D Gaussians with Arbitrary Unposed Images", "link_suffix": "/forum?id=XLcu8vHRpZ", "link": "https://openreview.net/forum?id=XLcu8vHRpZ", "pdf_link": "https://openreview.net/pdf?id=XLcu8vHRpZ", "keywords": "3D vision, 3D Gaussian Splatting", "abstract": "Recent large reconstruction models have made notable progress in generating high-quality 3D objects from single images. However, these methods often struggle with controllability, as they lack information from multiple views, leading to incomplete or inconsistent 3D reconstructions. To address this limitation, we introduce LucidFusion, a flexible end-to-end feed-forward framework that leverages the Relative Coordinate Map (RCM).  Unlike traditional methods linking images to 3D world thorough pose, LucidFusion utilizes RCM to align geometric features coherently across different views, making it highly adaptable for 3D generation from arbitrary, unposed images. Furthermore, LucidFusion seamlessly integrates with the original single-image-to-3D pipeline, producing detailed 3D Gaussians at a resolution of $512 \\times 512$, making it well-suited for a wide range of applications.", "title_embedding_index": 22960, "title_abs_embedding_index": 22985}, {"title": "Distribution Corrected Estimation via Adversarial Density Weighted Regression", "link_suffix": "/forum?id=SUL5L9rIyF", "link": "https://openreview.net/forum?id=SUL5L9rIyF", "pdf_link": "https://openreview.net/pdf?id=SUL5L9rIyF", "keywords": "Imitation Learning, Deep Reinforcement Learning", "abstract": "We propose a novel one-step supervised imitation learning (IL) framework called Adversarial Density Regression (ADR). This IL framework aims to correct the policy learned on unknown-quality to match the expert distribution by utilizing demonstrations, without relying on the Bellman operator. Specifically, ADR addresses several limitations in previous IL algorithms: First, most IL algorithms are based on the Bellman operator, which inevitably suffer from cumulative offsets from sub-optimal rewards during multi-step update processes. Additionally, off-policy training frameworks suffer from Out-of-Distribution (OOD) state-actions. Second, while conservative terms help solve the OOD issue, balancing the conservative term is difficult. To address these limitations, we fully integrate a refined one-step Distribution Corrected Estimation (DICE)-type supervised framework named ADR. Theoretically, we demonstrate that this adaptation can effectively correct the distribution of policies trained on unknown-quality datasets to align with the expert policy's distribution. Moreover, the difference between the empirical and the optimal value function is proportional to the lower bound of ADR's objective, indicating that minimizing ADR's objective is akin to approaching the optimal value. Experimentally, we validated the performance of ADR by conducting extensive evaluations. Specifically, ADR outperforms all of the selected IL algorithms on tasks from the Gym-Mujoco domain. Meanwhile, it achieves an 89.5% improvement over IQL when utilizing ground truth rewards on tasks from the Adroit and Kitchen domains.", "title_embedding_index": 22961, "title_abs_embedding_index": 22986}, {"title": "Linear Multistep Solver Distillation for Fast Sampling of Diffusion Models", "link_suffix": "/forum?id=vkOFOUDLTn", "link": "https://openreview.net/forum?id=vkOFOUDLTn", "pdf_link": "https://openreview.net/pdf?id=vkOFOUDLTn", "keywords": "Diffusion Probabilistic Model, Diffusion Sampler, Solver Schedule", "abstract": "Sampling from diffusion models can be seen as solving the corresponding \n   probability flow ordinary differential equation (ODE). \n   The solving process requires a significant number of function \n   evaluations (NFE), making it time-consuming. \n   Recently, several solver search frameworks have attempted to find \n   better-performing model-specific solvers. However, predicting the impact of \n   intermediate solving strategies on final sample quality remains challenging, \n   rendering the search process inefficient.\n   In this paper, we propose a novel method for designing \n   solving strategies. We first introduce a unified prediction formula \n   for linear multistep solvers. Subsequently, we present a solver distillation \n   framework, which enables a student solver to mimic the sampling trajectory \n   generated by a teacher solver with more steps. We utilize the mean Euclidean \n   distance between the student and teacher sampling trajectories as a metric, \n   facilitating rapid adjustment and optimization of intermediate solving strategies.\n   The design space of our framework encompasses multiple aspects, \n   including prediction coefficients, time step schedules, and time scaling \n   factors. \n   Our framework has the ability to complete a solver search \n   for Stable-Diffusion in less than 10 total GPU hours.\n   Compared to previous reinforcement learning-based \n   search frameworks, \n   our approach achieves over a 10$\\times$ increase in search efficiency. \n   With just 5 NFE, we achieve FID scores of 3.23 on CIFAR10, 7.16 on ImageNet-64, \n   5.44 on LSUN-Bedroom, and 15.69 on MS-COCO, resulting in a 2$\\times$ sampling acceleration ratio \n   compared to handcrafted solvers.", "title_embedding_index": 22962, "title_abs_embedding_index": 22987}, {"title": "A lightweight Transformer guided by features from multiple receptive fields for few-shot fine-grained image classification", "link_suffix": "/forum?id=7ftRWFUVLu", "link": "https://openreview.net/forum?id=7ftRWFUVLu", "pdf_link": "https://openreview.net/pdf?id=7ftRWFUVLu", "keywords": "Convolutional Neural Networks (CNNs), Vision Transformers (ViTs), Few-shot Learning, End-to-end training", "abstract": "Convolutional neural networks (CNNs) and vision Transformers (ViTs) play key roles in few-shot fine-grained image classification (FSFGIC). One of the main challenges of FSFGIC is how to consistently learn high-quality feature representations from different very limited fine-grained datasets. CNNs struggle with long-range dependencies due to their inherent localized receptive fields, and ViTs might impair high-frequency information, e.g., local texture information. Furthermore, ViTs require a large number of training samples to infer feature properties such as translation invariance, locality, and the hierarchy of visual data, while FSFGIC's training samples are extremely limited. To address the problems mentioned, a new lightweight Transformer guided by features from multiple receptive fields (LT-FMRF) is proposed which has considered how to manage long-range dependencies and how to extract local features with multiple scales, global features, and fused features from input images for increasing inter-class differences and consistently obtaining high-quality feature representations from different types of limited training datasets. Furthermore, the proposed LT-FMRF can be easily embedded into a given few-shot episodic training mechanism for end-to-end training from scratch. Experimental results conducted on five widely used FSFGIC datasets consistently show significant improvements over twenty state-of-the-art end-to-end training-based methods.", "title_embedding_index": 22963, "title_abs_embedding_index": 22988}, {"title": "Less is More: Extreme Gradient Boost Rank-1 Adaption for Efficient Finetuning of LLMs", "link_suffix": "/forum?id=WkhlUyJcOJ", "link": "https://openreview.net/forum?id=WkhlUyJcOJ", "pdf_link": "https://openreview.net/pdf?id=WkhlUyJcOJ", "keywords": "LLM, PEFT, LoRA", "abstract": "Fine-tuning Large Language Models (LLMs) has become a crucial technique for adapting pre-trained models to downstream tasks. However, the enormous size of LLMs poses significant challenges in terms of computational complexity and resource requirements. Low-Rank Adaptation (LoRA) has emerged as a promising solution. However, there exists a gap between the practical performance of low-rank adaptations and its theoretical optimum. In this work, we propose eXtreme Gradient Boosting LoRA (XGBLoRA), a novel framework that bridges this gap by leveraging the power of ensemble learning. Inspired by gradient boosting, XGBLoRA iteratively learns and merges a sequence of LoRA adaptations to refine model predictions. It achieves better performance than the standard LoRA, while enjoying the computational efficiency of rank-1 adaptations. We provide theoretical analysis to show the convergence and optimality of our approach, and conduct extensive experiments on a range of natural language processing tasks. The results demonstrate that XGBLoRA consistently outperforms standard LoRA and achieves performance comparable to full fine-tuning with significantly fewer trainable parameters. This work advances parameter-efficient fine-tuning for LLMs, and offers a promising solution for adapting LLMs to downstream tasks while optimizing performance and efficiency.", "title_embedding_index": 22964, "title_abs_embedding_index": 22989}, {"title": "AnyExpress: One Adapter Enabling Highly Flexible Audio-Driven Portrait Animation", "link_suffix": "/forum?id=sOmojPmnlL", "link": "https://openreview.net/forum?id=sOmojPmnlL", "pdf_link": "https://openreview.net/pdf?id=sOmojPmnlL", "keywords": "Diffusion-based Portrait Animation, Audio-Driven Talking Face, Plug-and-Play Adapter, Flexible Video Generation", "abstract": "Portrait animation, particularly audio-driven portrait animation, requires flexibility in facial expressions, head movement, and dynamic contexts. However, existing diffusion-based methods rely heavily on the design of ReferenceNet, leading to increased training complexity and incompatibility with other custom base models or adapters, also limiting face position, view changes, and animated context generation. To address these challenges, we proposeAnyExpress, a lightweight, modular framework that eliminates the need for ReferenceNet, reducing the number of trainable parameters by7times. By training one plug-and-playaudio-motion adapter, it allows freeform, expressive audio-driven portrait animation with any face pose and any animated context, while supporting text-driven modifications. In the context of character generation, there are two primary methods to control the desired character attributes. First, if a specific ID needs to be assigned, this can be achieved through ID controls (e.g., IP-Adapter-Face). Alternatively, the character\u2019s attributes can be controlled through textual descriptions. Through comprehensive qualitative and quantitative analyses,AnyExpressdemonstrates unprecedented freedom in generating videos with dynamic background, lower training demand, and seamless integration with evolving custom models and control adapters, providing a flexible solution for diverse generation needs. The demo is available athttps://anyexpress-alpha.github.io/Any, and we will release our code, encouraging further improvement.", "title_embedding_index": 22965, "title_abs_embedding_index": 22990}, {"title": "ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination", "link_suffix": "/forum?id=vQFw9ryKyK", "link": "https://openreview.net/forum?id=vQFw9ryKyK", "pdf_link": "https://openreview.net/pdf?id=vQFw9ryKyK", "keywords": "Robotics, Visual Navigation, Vision-Language Model, Scene Imagination", "abstract": "Visual navigation is an essential skill for home-assistance robots, providing the object-searching ability to accomplish long-horizon daily tasks. Many recent approaches use Large Language Models (LLMs) for commonsense inference to improve exploration efficiency. However, the planning process of LLMs is limited within texts and it is difficult to represent the spatial occupancy and geometry layout only by texts. Both are important for making rational navigation decisions. In this work, we seek to unleash the spatial perception and planning ability of Vision-Language Models (VLMs), and explore whether the VLM, with only on-board camera captured RGB/RGB-D stream inputs, can efficiently finish the visual navigation tasks in a mapless manner. We achieve this by developing the imagination-powered navigation framework ImagineNav, which imagines the future observation images at valuable robot views and translates the complex navigation planning process into a rather simple best-view image selection problem for VLM. To generate appropriate candidate robot views for imagination, we introduce the Where2Imagine module, which is distilled to align with human navigation habits. Finally, to reach the VLM preferred views, an off-the-shelf point-goal navigation policy is utilized. Empirical experiments on the challenging open-vocabulary object navigation benchmarks demonstrates the superiority of our proposed system.", "title_embedding_index": 22966, "title_abs_embedding_index": 22991}, {"title": "Towards the Effect of Large Language Models on Out-Of-Distribution Challenge in Text-Attributed Graphs", "link_suffix": "/forum?id=FGIBKpOj8m", "link": "https://openreview.net/forum?id=FGIBKpOj8m", "pdf_link": "https://openreview.net/pdf?id=FGIBKpOj8m", "keywords": "Out-Of-Distribution, Large Language Models, Text-Attributed-Graphs", "abstract": "Text-Attributed Graphs (TAGs),  where each node is associated with text attributes, are ubiquitous and have been widely applied in the real world. The Out-Of-Distribution (OOD) issue, i.e., the training data and the test data not from the same distribution, is quite common in learning on real-world TAGs, posing significant challenges to the effectiveness of graph learning models. Recently, Large Language Models (LLMs) have shown extraordinary capability in processing text data, and have demonstrated tremendous potential in handling TAGs. However, there is no benchmark work that  systematically and comprehensively investigates the effect of these LLM-based methods on alleviating the OOD issue on TAGs.  To bridge this gap, we first develop OOD-TAG, a comprehensive OOD benchmark dataset in TAGs which consists of diverse distributions. Meanwhile, we conduct a systematic and comprehensive investigation on OOD-TAG with different LLM pipelines for graphs. In addition, we provide original observations and novel insights based on the empirical study, which can suggest promising directions for the research of LLMs in addressing the OOD challenges on TAGs. Our code and dataset are available inhttps://anonymous.4open.science/r/GraphOOD-benchmark-5FCF/.", "title_embedding_index": 22967, "title_abs_embedding_index": 22992}, {"title": "MaskCLIP++: A Mask-Based CLIP Fine-tuning Framework for Open-Vocabulary Image Segmentation", "link_suffix": "/forum?id=UsgFE3cxuP", "link": "https://openreview.net/forum?id=UsgFE3cxuP", "pdf_link": "https://openreview.net/pdf?id=UsgFE3cxuP", "keywords": "Open Vocabulary, Image Segmentation, Vision Language Model", "abstract": "Open-vocabulary image segmentation has been advanced through the synergy between mask generators and vision-language models like Contrastive Language-Image Pre-training (CLIP).\nPrevious approaches focus on generating masks while aligning mask features with text embeddings during training.\nHowever, in this paper, we observed that relying on low-quality masks can weaken the alignment of vision and language in regional representations.\nTo address this limitation, we propose MaskCLIP++, a fine-tuning framework designed to transfer CLIP's image-level recognition to local regions via ground truth masks and labels, removing the need for a specific mask generator.\nTo enhance this process, we propose to use a consistency alignment module during fine-tuning, based on the model's local image-text similarity. We empirically verify that this could significantly reduce the overfitting issues, which are commonly observed in prior methods.\nCompared to previous state-of-the-art mask-based open vocabulary segmentation methods, \nwe achieved performance improvements of +1.7, +2.3, +2.1, +3.1, and +0.3 mIoU on the A-847, PC-459, A-150, PC-59, and PAS-20 datasets, respectively, with lower training costs.\nOur code will be made publicly available.", "title_embedding_index": 22968, "title_abs_embedding_index": 22993}, {"title": "Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A", "link_suffix": "/forum?id=aIIYzzGKZp", "link": "https://openreview.net/forum?id=aIIYzzGKZp", "pdf_link": "https://openreview.net/pdf?id=aIIYzzGKZp", "keywords": "large language models, uncertainty quantification, calibration, question-answering", "abstract": "We study 14 large language models (LLMs) fine-tuned for chat and find that their maximum softmax probabilities (MSPs) are consistently miscalibrated on multiple-choice Q&A. However, those MSPs might still encode useful uncertainty information. Specifically, we hypothesized that wrong answers would be associated with smaller MSPs compared to correct answers. Via rigororous statistical testing, we show that this hypothesis holds for models which perform well on the underlying Q&A task. We also find a strong direction correlation between Q&A accuracy and MSP correctness prediction, while finding no correlation between Q&A accuracy and calibration error. This suggests that within the current fine-tuning paradigm, we can expect correctness prediction but not calibration to improve as LLM capabilities progress. To demonstrate the utility of correctness prediction, we show that when models have the option to abstain, performance can be improved by selectively abstaining based on the MSP of the initial model response, using only a small amount of labeled data to choose the MSP threshold.", "title_embedding_index": 22969, "title_abs_embedding_index": 22994}, {"title": "LEARN TO LEARN CONSISTENTLY", "link_suffix": "/forum?id=GcFX8rZNSX", "link": "https://openreview.net/forum?id=GcFX8rZNSX", "pdf_link": "https://openreview.net/pdf?id=GcFX8rZNSX", "keywords": "meta learning, few-shot learning, meta self-distillation, consistency of learned knowledge", "abstract": "In the few-shot learning problem, a model trained on a disjoint meta-train dataset\nis required to address novel tasks with limited novel examples. A key challenge in\nfew-shot learning is the model\u2019s propensity to learn biased shortcut features(e.g.,\nbackground, noise, shape, color), which are sufficient to distinguish the few ex-\namples during fast adaptation but lead to poor generalization. In our work, we\nobserved when the model learns with higher consistency, the model tends to be\nless influenced by shortcut features, resulting in better generalization. Based on\nthe observation, we propose a simple yet effective meta-learning method named\nMeta Self-Distillation. By maximizing the consistency of the learned knowledge\nduring the meta-train phase, the model initialized by our method shows better\ngeneralization in the meta-test phase. Extensive experiments demonstrate that our\nmethod improves the model\u2019s generalization across various few-shot classification\nscenarios and enhances the model\u2019s ability to learn consistently.", "title_embedding_index": 22970, "title_abs_embedding_index": 22995}, {"title": "Avoiding Catastrophe in Online Learning by Asking for Help", "link_suffix": "/forum?id=uuEQsqb0GH", "link": "https://openreview.net/forum?id=uuEQsqb0GH", "pdf_link": "https://openreview.net/pdf?id=uuEQsqb0GH", "keywords": "online learning, AI safety, asking for help, irreversibility", "abstract": "Most learning algorithms with formal regret guarantees assume that no mistake is irreparable and essentially rely on trying all possible behaviors. This approach is problematic when some mistakes arecatastrophic, i.e., irreparable. We propose an online learning problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff in each round represents the chance of avoiding catastrophe that round and try to maximize the product of payoffs (the overall chance of avoiding catastrophe) while allowing a limited number of queries to a mentor. We first show that in general, any algorithm either constantly queries the mentor or is nearly guaranteed to cause catastrophe. However, in settings where the mentor policy class is learnable in the standard online model, we provide an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows. Conceptually, if a policy class is learnable in the absence of catastrophic risk, it is learnable in the presence of catastrophic risk if the agent can ask for help.", "title_embedding_index": 22971, "title_abs_embedding_index": 22996}, {"title": "Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving", "link_suffix": "/forum?id=Vv76fCYffN", "link": "https://openreview.net/forum?id=Vv76fCYffN", "pdf_link": "https://openreview.net/pdf?id=Vv76fCYffN", "keywords": "End-to-End, Autonomous Driving, Sparse Scene Representation", "abstract": "End-to-End Autonomous Driving (E2EAD) methods typically rely on supervised perception tasks to extract explicit scene information (e.g., objects, maps). This reliance necessitates expensive annotations and constrains deployment and data scalability in real-time applications. In this paper, we introduce SSR, a novel framework that utilizes only 16 navigation-guided tokens as Sparse Scene Representation, efficiently extracting crucial scene information for E2EAD. Our method eliminates the need for supervised sub-tasks, allowing computational resources to concentrate on essential elements directly related to navigation intent. We further introduce a temporal enhancement module that employs a Bird's-Eye View (BEV) world model, aligning predicted future scenes with actual future scenes through self-supervision. SSR achieves state-of-the-art planning performance on the nuScenes dataset, demonstrating a 27.2% relative reduction in L2 error and a 51.6% decrease in collision rate to the leading E2EAD method, UniAD. Moreover, SSR offers a 10.9\u00d7 faster inference speed and 13\u00d7 faster training time. This framework represents a significant leap in real-time autonomous driving systems and paves the way for future scalable deployment. Code will be released.", "title_embedding_index": 22972, "title_abs_embedding_index": 22997}, {"title": "Propensity-driven Uncertainty Learning for Sample Exploration in Source-Free Active Domain Adaptation", "link_suffix": "/forum?id=WtGpBjnMq8", "link": "https://openreview.net/forum?id=WtGpBjnMq8", "pdf_link": "https://openreview.net/pdf?id=WtGpBjnMq8", "keywords": "Active learning, Source-free domain adaptation", "abstract": "Source-free active domain adaptation (SFADA) addresses the challenge of adapting a pre-trained model to new domains without access to source data while minimizing the need for target domain annotations. This scenario is particularly relevant in real-world applications where data privacy, storage limitations, or labeling costs are significant concerns. Key challenges in SFADA include selecting the most informative samples from the target domain for labeling, effectively leveraging both labeled and unlabeled target data, and adapting the model without relying on source domain information. Additionally, existing methods often struggle with noisy or outlier samples and may require impractical progressive labeling during training. To effectively select more informative samples without frequently requesting human annotations, we propose the Propensity-driven Uncertainty Learning (ProULearn) framework. ProULearn utilizes a novel homogeneity propensity estimation mechanism combined with correlation index calculation to evaluate feature-level relationships. This approach enables the identification of representative and challenging samples while avoiding noisy outliers. Additionally, we develop a central correlation loss to refine pseudo-labels and create compact class distributions during adaptation. In this way, ProULearn effectively bridges the domain gap and maximizes adaptation performance. The principles of informative sample selection underlying ProULearn have broad implications beyond SFADA, offering benefits across various deep learning tasks where identifying key data points or features is crucial. Extensive experiments on four benchmark datasets demonstrate that ProULearn consistently outperforms state-of-the-art methods in domain adaptation scenarios.", "title_embedding_index": 22973, "title_abs_embedding_index": 22998}, {"title": "DAG-NAS: Explainable Neural Architecture Search\\for Reinforcement Learning via Scalar-level DAG Modeling", "link_suffix": "/forum?id=rws9sRnBEf", "link": "https://openreview.net/forum?id=rws9sRnBEf", "pdf_link": "https://openreview.net/pdf?id=rws9sRnBEf", "keywords": "automated machine learning, neural architecture search, directed acyclic graphs, reinforcement learning", "abstract": "We present an explainable and effective Neural Architecture Search (NAS) framework for Reinforcement Learning (RL). We model a feed-forward neural network as a Directed Acyclic Graph (DAG) that consists of scalar-level operations and their interconnections. We train the model for RL tasks using a differentiable search method, followed by pruning the search outcomes. This process results in a compact neural architecture that achieves high performance and enhances explainability by emphasizing crucial information for solving the RL problem. This process results in a compact and efficient neural architecture that enhances explainability by emphasizing crucial information for solving the RL problem. We apply our NAS framework to the Actor-Critic PPO algorithm, targeting both actor and critic networks. We evaluate its performance across various RL tasks. Extensive experiments demonstrate that our architectures achieve comparable performance with significantly fewer parameters while also enhancing explainability by highlighting key features.", "title_embedding_index": 22974, "title_abs_embedding_index": 22999}]