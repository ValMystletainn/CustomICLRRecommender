[{"title": "Zebra: In-Context and Generative Pretraining for Solving Parametric PDEs", "link_suffix": "/forum?id=TyycdsNeeg", "link": "https://openreview.net/forum?id=TyycdsNeeg", "pdf_link": "https://openreview.net/pdf?id=TyycdsNeeg", "keywords": "PDE, Adaptation, In-Context, Transformer, Vector-Quantization", "abstract": "Solving time-dependent parametric partial differential equations (PDEs) is challenging, as models must adapt to variations in parameters such as coefficients, forcing terms, and boundary conditions. Data-driven neural solvers either train on data sampled from the PDE parameters distribution in the hope that the model generalizes to new instances or rely on gradient-based adaptation and meta-learning to implicitly encode the dynamics from observations. This often comes with increased inference complexity.\nInspired by the in-context learning capabilities of large language models (LLMs), we introduce Zebra, a novel generative auto-regressive transformer designed to solve parametric PDEs without requiring gradient adaptation at inference. By leveraging in-context information during both pre-training and inference, Zebra dynamically adapts to new tasks by conditioning on input sequences that incorporate context trajectories or preceding states. This approach enables Zebra to flexibly handle arbitrarily sized context inputs and supports uncertainty quantification through the sampling of multiple solution trajectories.\nWe evaluate Zebra across a variety of challenging PDE scenarios, demonstrating its adaptability, robustness, and superior performance compared to existing approaches.", "title_embedding_index": 4300, "title_abs_embedding_index": 4325}, {"title": "Liquid Dino: A Multi-Task Neural Network towards Autonomous Driving", "link_suffix": "/forum?id=0qfIhtel8N", "link": "https://openreview.net/forum?id=0qfIhtel8N", "pdf_link": "https://openreview.net/pdf?id=0qfIhtel8N", "keywords": "Autonomous Driving, Multi-task Learning, Advanced Driver-Assistance Systems (ADAS), Deep Learning", "abstract": "In the realm of advanced driver-assistance systems (ADAS) and autonomous driving, the accurate classification of driver emotions, behaviors and contextual environments is critical for enhancing vehicle safety and user experience. This study investigates the performance of various neural network architectures across four distinct classification tasks: Emotion Recognition, Driver Behavior Recognition, Scene-Centric Context Recognition and Vehicle-Based Context Recognition, all of which incorporate visual information captured through cameras. By utilizing camera-based data, we aim to evaluate how different neural architectures handle visual inputs in these diverse contexts, thereby exploring the robustness and generalization of each model to different real-world scenarios. We compare the performance of several state-of-the-art models and introduce a novel contribution that significantly improve classification accuracies in all areas. Our results demonstrate that the proposed Liquid Dino architecture achieves an overall average accuracy of 83.79%, outperforming other models in recognizing driver emotions, behaviors and contextual scenarios. These enhancements underscore the potential of our proposed methods in contributing to the development of more reliable and responsive ADAS.In the realm of advanced driver-assistance systems (ADAS) and autonomous driving, the accurate classification of driver emotions, behaviors and contextual environments is critical for enhancing vehicle safety and user experience. This study investigates the performance of various neural network architectures across four distinct classification tasks: Emotion Recognition, Driver Behavior Recognition, Scene-Centric Context Recognition and Vehicle-Based Context Recognition, all of which incorporate visual information captured through cameras. By utilizing camera-based data, we aim to evaluate how different neural architectures handle visual inputs in these diverse contexts, thereby exploring the robustness and generalization of each model to different real-world scenarios. We compare the performance of several state-of-the-art models and introduce a novel contribution that significantly improve classification accuracies in all areas. Our results demonstrate that the proposed Liquid Dino architecture achieves an overall average accuracy of 83.79%, outperforming other models in recognizing driver emotions, behaviors and contextual scenarios. These enhancements underscore the potential of our proposed methods in contributing to the development of more reliable and responsive ADAS.", "title_embedding_index": 4301, "title_abs_embedding_index": 4326}, {"title": "An Empirical Study on Enhancing LLMs' Alignment Capabilities through Restyled In-Context Learning Demonstration Examples", "link_suffix": "/forum?id=IHqlU2J5ia", "link": "https://openreview.net/forum?id=IHqlU2J5ia", "pdf_link": "https://openreview.net/pdf?id=IHqlU2J5ia", "keywords": "alignment, in-context learning, safety", "abstract": "Alignment tuning is crucial for ensuring large language models (LLMs) behave safely, ethically, and align with human values. It bridges the gap between raw model capabilities and nuanced task requirements, such as helpfulness and user safety. Current alignment approaches, like instruction-following through supervised fine-tuning (SFT) and preference optimization (PO), require high-quality data and significant resources. This paper proposes a low-cost, tuning-free method using in-context learning (ICL) to enhance LLM alignment.Leveraging the autoregressive nature of LLMs, we observed that aligned models adjust the probability distribution of early polarity tokens during decoding, influencing their response trajectory. Among polarity tokens, malicious tokens induce LLMs to positively respond to toxic queries, whereas benign tokens encourage constructive output. Based on this, we designed heuristic rules to select ICL demonstration examples that effectively influence polarity token distributions.We packaged these examples as prompts to trigger few-shot learning, improving LLM alignment. Furthermore, the style and content of ICL demonstrations critically impact few-shot learning. Rewriting examples in a unified, structured style improved LLM accuracy and helpfulness, while specific content encouraged refusal of malicious prompts, enhancing safety.Our experiments show that rewritten examples boost alignment, safety, and reasoning across various tasks. Compared to the best baseline approach, with an average score of 5.00 as the maximum, our method achieves a maximum 0.15 increase on the Alpaca-eval task (from 4.44 \u2192 4.59), a 0.10 enhancement on the just-eval-instruct benchmark (from 4.50 \u2192 4.60), and a maximum improvement of 0.08 (from 3.53 \u2192 3.61) on the MT-Bench dataset. These findings underscore the need for deeper analysis and theoretical understanding of alignment for advancing future LLM research.", "title_embedding_index": 4302, "title_abs_embedding_index": 4327}, {"title": "SOAP: Improving and Stabilizing Shampoo using Adam", "link_suffix": "/forum?id=IDxZhXrpNf", "link": "https://openreview.net/forum?id=IDxZhXrpNf", "pdf_link": "https://openreview.net/pdf?id=IDxZhXrpNf", "keywords": "Shampoo, Adam, Second Order Optimizer", "abstract": "There is growing evidence of the effectiveness of Shampoo, a higher-order preconditioning method, over Adam in deep learning optimization tasks. However, Shampoo's drawbacks include additional hyperparameters and computational overhead when compared to Adam, which only updates running averages of first- and second-moment quantities. This work establishes a formal connection between Shampoo (implemented with the 1/2 power) and Adafactor --- a memory-efficient approximation of Adam --- showing that Shampoo is equivalent to running Adafactor in the eigenbasis of Shampoo's preconditioner. This insight leads to the design of a simpler and computationally efficient algorithm:ShampoOwithAdam in thePreconditioner's eigenbasis (SOAP).\nWith regards to improving Shampoo's computational efficiency, the most straightforward approach would be to simply compute Shampoo's eigendecomposition less frequently. Unfortunately, as our empirical results show, this leads to performance degradation that worsens with this frequency. SOAP mitigates this degradation by continually updating the running average of the second moment, just as Adam does, but in the current (slowly changing) coordinate basis. Furthermore, since SOAP is equivalent to running Adam in a rotated space, it introduces only one additional hyperparameter (the preconditioning frequency) compared to Adam. We empirically evaluate SOAP on language model pre-training with 360m and 660m sized models. In the large batch regime, SOAP reduces the number of iterations by over 40% and wall clock time by over 35% compared to AdamW, with approximately 20% improvements in both metrics compared to Shampoo. An implementation of SOAP is available athttps://anonymous.4open.science/status/SOAP-F93B.", "title_embedding_index": 4303, "title_abs_embedding_index": 4328}, {"title": "Active Task Disambiguation with LLMs", "link_suffix": "/forum?id=JAMxRSXLFz", "link": "https://openreview.net/forum?id=JAMxRSXLFz", "pdf_link": "https://openreview.net/pdf?id=JAMxRSXLFz", "keywords": "Task Ambiguity, Bayesian Experimental Design, Large Language Models, Active Learning", "abstract": "Despite the impressive performance of large language models (LLMs) across various benchmarks, their ability to address ambiguously specified problems\u2014frequent in real-world interactions\u2014remains underexplored. To address this gap, we introduce a formal definition of task ambiguity and frame the problem of task disambiguation through the lens of Bayesian Experimental Design. By posing clarifying questions, LLM agents can acquire additional task specifications, progressively narrowing the space of viable solutions and reducing the risk of generating unsatisfactory outputs. Yet, generating effective clarifying questions requires LLM agents to engage in a form of meta-cognitive reasoning, an ability LLMs may presently lack. Our proposed approach of active task disambiguation enables LLM agents to generate targeted questions maximizing the information gain. Effectively, this approach shifts the load from implicit to explicit reasoning about the space of viable solutions. Empirical results demonstrate that this form of question selection leads to more effective task disambiguation in comparison to approaches relying on reasoning solely within the space of questions.", "title_embedding_index": 4304, "title_abs_embedding_index": 4329}, {"title": "Online Continual Graph Learning", "link_suffix": "/forum?id=4sJJixGIZX", "link": "https://openreview.net/forum?id=4sJJixGIZX", "pdf_link": "https://openreview.net/pdf?id=4sJJixGIZX", "keywords": "continual learning, online learning, graph neural network", "abstract": "The aim of Continual Learning (CL) is to learn new tasks incrementally while avoiding catastrophic forgetting. Online Continual Learning (OCL) specifically focuses on learning efficiently from a continuous stream of data with shifting distribution. While recent studies explore Continual Learning on graphs exploiting Graph Neural Networks (GNNs), only few of them focus on a streaming setting.  Many real-world graphs evolve over time and timely (online) predictions could be required. However, current approaches are not well aligned with the standard OCL literature, partly due to the lack of a clear definition of online continual learning on graphs. In this work, we propose a general formulation for online continual learning on graphs, emphasizing the efficiency of batch processing while accounting for graph topology, providing a grounded setting to analyze different methods. We present a set of benchmark datasets for online continual graph learning, together with the results of several methods in CL literature, adapted to our setting. Additionally, we address the challenge of GNN memory usage, as considering multiple hops of neighborhood aggregation can require access to the entire growing graph, resulting in prohibitive costs for the setting. We thus propose solutions to maintain bounded complexity for efficient online learning.", "title_embedding_index": 4305, "title_abs_embedding_index": 4330}, {"title": "Instruction Contrastive Tuning for Zero-shot Composed Image Retrieval", "link_suffix": "/forum?id=OZdr2mV5EI", "link": "https://openreview.net/forum?id=OZdr2mV5EI", "pdf_link": "https://openreview.net/pdf?id=OZdr2mV5EI", "keywords": "Multimodal Learning, Composed Image Retrieval, Large Vision Language Models", "abstract": "Composed Image Retrieval (CIR) requires retrieving a target image based on a composed query consisting of an image and accompanying text that modifies or instructs changes to the visual reference. This task is particularly challenging as it demands the model effectively follow modification instructions for accurate retrieval. Additionally, data acquisition difficulties hinder training models for specific tasks. To address these challenges, recent approaches explore Zero-Shot CIR (ZS-CIR), mainly leveraging CLIP-based models with tailored projections to compose images and textual modifications. However, these base models are not trained on instruction-aware data, limiting their ability to effectively combine visual and textual cues. In this paper, we propose a novel embedding method utilizing an instruction-tuned Multimodal Large Language Model (MLLM) to generate unified embeddings that seamlessly integrate images and modification instructions. Instruction-tuned MLLMs inherently align vision and text while exhibiting strong instruction-following capabilities, though they are primarily used in text generation. We introduce a two-stage training strategy to efficiently transform the MLLM\u2019s text generation capabilities into embedding extraction, and further refining its ability to follow modification instructions in CIR. Our model demonstrates significant advancements in ZS-CIR, outperforming state-of-the-art baselines across four public datasets: FashionIQ, CIRR, GeneCIS, and CIRCO. Our model highlights the potential of instruction-tuned MLLMs in capturing nuanced instruction comprehension and advancing CIR systems.", "title_embedding_index": 4306, "title_abs_embedding_index": 4331}, {"title": "Causal Graphical Models for Vision-Language Compositional Understanding", "link_suffix": "/forum?id=haJHr4UsQX", "link": "https://openreview.net/forum?id=haJHr4UsQX", "pdf_link": "https://openreview.net/pdf?id=haJHr4UsQX", "keywords": "Compositionality, Vision-Language Models, Causal Learning", "abstract": "Recent work has empirically shown that Vision-Language Models (VLMs) struggle to fully understand the compositional properties of the human language, usually modeling an image caption as a ``bag of words''. As a result, they perform poorly on compositional tasks, which require a deeper understanding of the different entities of a sentence (subject, verb, etc.) jointly with their mutual relationships in order to be solved. In this paper, we model  the dependency relations among textual and visual tokens using a Causal Graphical Model (CGM), built using a dependency parser, and we train a decoder conditioned by the VLM visual encoder. \nDifferently from standard autoregressive or parallel predictions,\nour decoder's generative process is partially-ordered following the CGM structure. This structure encourages the decoder to learn only the main causal dependencies in a sentence discarding spurious correlations.\nUsing extensive experiments on five compositional benchmarks, we show that our method significantly outperforms all the state-of-the-art compositional approaches, \nusually by a large margin, and it also improves over  methods trained  using much larger datasets.\nThe code is anonymously available and it will be publicly released after the paper acceptance.", "title_embedding_index": 4307, "title_abs_embedding_index": 4332}, {"title": "Sparse MoE as a New Retriever: Addressing Missing Modality Problem in Incomplete Multimodal Data", "link_suffix": "/forum?id=j9DbobO0mY", "link": "https://openreview.net/forum?id=j9DbobO0mY", "pdf_link": "https://openreview.net/pdf?id=j9DbobO0mY", "keywords": "Missing Multimodal Data, Sparse MoE", "abstract": "In multimodal machine learning, effectively addressing the missing modality scenario is crucial for improving performance in downstream tasks such as in medical contexts where data may be incomplete. Although some attempts have been made to effectively retrieve embeddings for missing modalities, two main bottlenecks remain: the consideration of both intra- and inter-modal context, and the cost of embedding selection with the lack of specialized knowledge in embedding candidates. In response, we propose MoE-Retriever, a novel framework inspired by the design principles of Sparse Mixture of Experts (SMoE). First, MoE-Retriever samples the relevant data from modality combinations, using a so-called supporting group to construct intra-modal inputs while incorporating inter-modal inputs. These inputs are then processed by Multi-Head Attention, after which the SMoE Router automatically selects the most relevant expert, i.e., the embedding candidate to be retrieved. Comprehensive experiments on both medical and general multimodal datasets demonstrate the robustness and generalizability of MoE-Retriever, marking a significant step forward in embedding retrieval methods for incomplete multimodal data.", "title_embedding_index": 4308, "title_abs_embedding_index": 4333}, {"title": "Mixture of Attentions For Speculative Decoding", "link_suffix": "/forum?id=Rz0kozh3LE", "link": "https://openreview.net/forum?id=Rz0kozh3LE", "pdf_link": "https://openreview.net/pdf?id=Rz0kozh3LE", "keywords": "large language models, speculative decoding, EAGLE", "abstract": "The growth in the number of parameters of Large Language Models (LLMs) has led to a significant surge in computational requirements, making them challenging and costly to deploy.\nSpeculative decoding (SD) leverages smaller models to efficiently propose future tokens, which are then verified by the LLM in parallel.\nSmall models that utilise activations from the LLM currently achieve the fastest decoding speeds.\nHowever, we identify several limitations of SD models including the lack of on-policyness during training and partial observability. \nTo address these shortcomings, we propose a more grounded architecture for small models by introducing a Mixture of Attentions for SD.\nOur novel architecture can be applied in two scenarios: a conventional single device deployment and a novel client-server deployment where the small model is hosted on a consumer device and the LLM on a server.\nIn a single-device scenario, we demonstrate state-of-the-art speedups improving EAGLE-2 by 9.5% and its acceptance length by 25%.\nIn a client-server setting, our experiments demonstrate: 1) state-of-the-art latencies with minimal calls to the server for different network conditions, and 2) in the event of a complete disconnection, our approach can maintain higher accuracy compared to other SD methods and demonstrates advantages over API calls to LLMs, which would otherwise be unable to continue the generation process.", "title_embedding_index": 4309, "title_abs_embedding_index": 4334}, {"title": "The Relevancy Metric: Understanding the Impact of Training Data", "link_suffix": "/forum?id=NfMmo2eQhG", "link": "https://openreview.net/forum?id=NfMmo2eQhG", "pdf_link": "https://openreview.net/pdf?id=NfMmo2eQhG", "keywords": "Train-Test Relationship, Influence functions, Memorization, Learning dynamics, Dataset properties", "abstract": "Deep learning models are central to many critical decision-making processes, making it imperative to gain deeper insights into their behavior to improve performance, transparency, interpretability, and fairness. \nA key challenge is understanding how training data shapes model predictions on unseen test data. \nIn this paper, we introduce a novel metric, $\\textbf{\\textit{Relevancy}}$, which quantifies the impact of individual training samples on inference predictions. \nOur proposed metric is calculated by observing the learning dynamics of the model during training, and it is computationally efficient and applicable across a wide range of tasks. \nWe demonstrate that it is between $80\\times$ and $100,000\\times$ more efficient than existing metrics for capturing the train-test relationship. \nUsing $\\textit{relevancy}$, we enable the identification of coresets \u2014 compact datasets that represent the essence of the training distribution. \nQuantitative evaluations show that coresets selected using our metric outperform state-of-the-art methods by up to $5.2$% on CIFAR-100. \nAdditionally, we qualitatively demonstrate how $\\textit{relevancy}$ can be extended to assess various training data properties, such as identifying mislabeled samples in widely used datasets like ImageNet, CIFAR-100, and Fashion-MNIST.\nThese examples illustrate just a few of the many potential uses of $\\textit{relevancy}$, highlighting its versatility in promoting more interpretable, efficient, and fair deep learning systems across diverse tasks.", "title_embedding_index": 4310, "title_abs_embedding_index": 4335}, {"title": "Exploring How LLMs Capture and Represent Domain-Specific Knowledge", "link_suffix": "/forum?id=9tMzqRaEL3", "link": "https://openreview.net/forum?id=9tMzqRaEL3", "pdf_link": "https://openreview.net/pdf?id=9tMzqRaEL3", "keywords": "Large Language Models, domain-trajectories, hidden states, prefill-phase, model selection.", "abstract": "We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains.  We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks.", "title_embedding_index": 4311, "title_abs_embedding_index": 4336}, {"title": "VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration", "link_suffix": "/forum?id=HMrcv7Q4Ub", "link": "https://openreview.net/forum?id=HMrcv7Q4Ub", "pdf_link": "https://openreview.net/pdf?id=HMrcv7Q4Ub", "keywords": "KV Cache Compression, Vision-Language Models, Inference Acceleration, Sparsity, Modality", "abstract": "Vision-Language Models (VLMs) have demonstrated impressive performance across a versatile set of tasks. A key challenge in accelerating VLMs is storing and accessing the large Key-Value (KV) cache that encodes long visual contexts, such as images or videos. While existing KV cache compression methods are effective for Large Language Models (LLMs), directly migrating them to VLMs yields suboptimal accuracy and speedup. To bridge the gap, we propose VL-Cache, a novel KV cache compression recipe tailored for accelerating VLM inference. In this paper, we first investigate the unique sparsity pattern of VLM attention by distinguishing visual and text tokens in prefill and decoding phases. Based on these observations, we introduce a layer-adaptive sparsity-aware cache budget allocation method that effectively distributes the limited cache budget across different layers, further reducing KV cache size without compromising accuracy. Additionally, we develop a modality-aware token scoring policy to better evaluate the token importance. Empirical results on multiple benchmark datasets demonstrate that retaining only 10% of KV cache achieves accuracy comparable to that with full cache. In a speed benchmark, our method accelerates end-to-end latency of generating 100 tokens by up to 2.33x and speeds up decoding by up to 7.08x, while reducing the memory footprint of KV cache in GPU by 90%.", "title_embedding_index": 4312, "title_abs_embedding_index": 4337}, {"title": "Data-Efficient Training by Evolved Sampling", "link_suffix": "/forum?id=2CflgSMLoK", "link": "https://openreview.net/forum?id=2CflgSMLoK", "pdf_link": "https://openreview.net/pdf?id=2CflgSMLoK", "keywords": "learning efficiency, evolved sampling, data selection, loss dynamics", "abstract": "Data selection is designed to accelerate learning with preserved performance. To achieve this, a fundamental thought is to identify informative data samples with significant contributions to the training. In this work, we proposeEvolved Sampling(ES), a simple yet effective framework fordynamicsampling performed along the training process. This method conductsbatchlevel data selection based ondifferencesof historical and current losses, significantly reducing the back propagation time with modest additional overheads while maintaining the model performance. Due to its conciseness, ES is readily extensible to incorporatesetlevel data selection for further training accelerations. As a plug-and-play framework, ES consistently achieves lossless training accelerations across various models (ResNet, ViT, ALBERT), datasets (CIFAR, ImageNet, GLUE), and optimizers (SGD, Adam), saving up to 40% wall-clock time. Particularly, the improvement is more significant under thenoisy supervisionsetting. When there are severe corruptions in labels, ES can obtain accuracy improvements of approximately 20% relative to the standard batched sampling. Our results motivate further investigations on the data efficiency aspect of modern large-scale machine learning.", "title_embedding_index": 4313, "title_abs_embedding_index": 4338}, {"title": "Restructuring Vector Quantization with the Rotation Trick", "link_suffix": "/forum?id=GMwRl2e9Y1", "link": "https://openreview.net/forum?id=GMwRl2e9Y1", "pdf_link": "https://openreview.net/pdf?id=GMwRl2e9Y1", "keywords": "Vector Quantization, VQ-VAE", "abstract": "Vector Quantized Variational AutoEncoders (VQ-VAEs) are designed to compress a continuous input to a discrete latent space and reconstruct it with minimal distortion. \nThey operate by maintaining a set of vectors---often referred to as the codebook---and quantizing each encoder output to the nearest vector in the codebook. \nHowever, as vector quantization is non-differentiable, the gradient to the encoder flowsaroundthe vector quantization layer rather thanthroughit in a straight-through approximation.\nThis approximation may be undesirable as all information from the vector quantization operation is lost. \nIn this work, we propose a way to propagate gradients through the vector quantization layer of VQ-VAEs. \nWe smoothly transform each encoder output into its corresponding codebook vector via a rotation and rescaling linear transformation that is treated as a constant during backpropagation. \nAs a result, the relative magnitude and angle between encoder output and codebook vector becomes encoded into the gradient as it propagates through the vector quantization layer and back to the encoder.\nAcross 11 different VQ-VAE training paradigms, we find this restructuring improves reconstruction metrics, codebook utilization, and quantization error.", "title_embedding_index": 4314, "title_abs_embedding_index": 4339}, {"title": "Formal Theorem Proving by Rewarding LLMs to Decompose Proofs Hierarchically", "link_suffix": "/forum?id=D23JcXiUwf", "link": "https://openreview.net/forum?id=D23JcXiUwf", "pdf_link": "https://openreview.net/pdf?id=D23JcXiUwf", "keywords": "formal theorem proving, large language models, reinforcement learning", "abstract": "Mathematical theorem proving is an important testbed for large language models\u2019 deep and abstract reasoning capability. This paper focuses on improving LLMs\u2019 ability to write proofs in formal languages that permit automated proof verification/ evaluation. Most previous results provide human-written lemmas to the theorem prover, which is an arguably oversimplified setting that does not sufficiently test the provers' planning and decomposition capabilities. Instead, we work in a more natural setup where the lemmas that are directly relevant to the theorem are not given to the theorem prover at test time. We design an RL-based training algorithm that encourages the model to decompose a theorem into lemmas, prove the lemmas, and then prove the theorem by using the lemmas. Our reward mechanism is inspired by how mathematicians train themselves: even if a theorem is too challenging to be proved by the current model, a reward is still given to the model for any correct and novel lemmas that are proposed and proved in this process. During training, our model proves 37.7% lemmas that are not in the training dataset. When tested on a set of holdout theorems, our model improves the pass rate from 40.8% to 45.5% compared with the supervised fine-tuned model.", "title_embedding_index": 4315, "title_abs_embedding_index": 4340}, {"title": "YOLO-MARL: You Only LLM Once for Multi-agent Reinforcement Learning", "link_suffix": "/forum?id=SOXxa4pPGY", "link": "https://openreview.net/forum?id=SOXxa4pPGY", "pdf_link": "https://openreview.net/pdf?id=SOXxa4pPGY", "keywords": "Multi-agent reinforcement learning, Large Language Models, reinforcement learning", "abstract": "Advancements in deep multi-agent reinforcement learning (MARL) have positioned it as a promising approach for decision-making in cooperative games. However, it still remains challenging for MARL agents to learn cooperative strategies for some game environments. Recently, large language models (LLMs) have demonstrated emergent reasoning capabilities, making them promising candidates for enhancing coordination among the agents. However, due to the model size of LLMs, it can be expensive to frequently infer LLMs for actions that agents can take. In this work, we propose You Only LLM Once for MARL (YOLO-MARL), a novel framework that leverages the high-level task planning capabilities of LLMs to improve the policy learning process of multi-agents in cooperative games. Notably, for each game environment, YOLO-MARL only requires one time interaction with LLMs in the proposed strategy generation, state interpretation and planning function generation modules,  before the MARL policy training process. This avoids the ongoing costs and computational time associated with frequent LLMs API calls during training. Moreover, the trained decentralized normal-sized neural network-based policies operate independently of the LLM. We evaluate our method across three different environments and demonstrate that YOLO-MARL outperforms traditional MARL algorithms.", "title_embedding_index": 4316, "title_abs_embedding_index": 4341}, {"title": "FactTest: Factuality Testing in Large Language Models with Statistical Guarantees", "link_suffix": "/forum?id=BVCGTsgpOS", "link": "https://openreview.net/forum?id=BVCGTsgpOS", "pdf_link": "https://openreview.net/pdf?id=BVCGTsgpOS", "keywords": "Large Language Models, Factuality, Uncertainty Quantification, Hallucination Detection", "abstract": "The propensity of Large Language Models (LLMs) to generate hallucinations and non-factual content undermines their reliability in high-stakes domains, where rigorous control over Type I errors (the conditional probability of incorrectly classifying hallucinations as truthful content) is essential. Despite its importance, formal verification of LLM factuality with such guarantees remains largely unexplored.\nIn this paper, we introduce FactTest, a novel framework that statistically assesses whether an LLM can confidently provide correct answers to given questions with high-probability correctness guarantees. We formulate factuality testing as hypothesis testing problem to enforce an upper bound of Type I errors at user-specified significance levels. Notably, we prove that our framework also ensures strong Type II error control under mild conditions and can be extended to maintain its effectiveness when covariate shifts exist. Our approach is distribution-free and works for any number of human-annotated samples. It is model-agnostic and applies to any black-box or white-box LM. Extensive experiments on question-answering (QA) and multiple-choice benchmarks demonstrate that FactTest effectively detects hallucinations and improves the model's ability to abstain from answering unknown questions, leading to an over 40% accuracy improvement.", "title_embedding_index": 4317, "title_abs_embedding_index": 4342}, {"title": "Sub-Domain Aware Granular Segmentation via Fine Tuning Network", "link_suffix": "/forum?id=bnpeTgh29E", "link": "https://openreview.net/forum?id=bnpeTgh29E", "pdf_link": "https://openreview.net/pdf?id=bnpeTgh29E", "keywords": "domain adaptation", "abstract": "Recent advances in deep learning (DL) have led to improved vision-based algorithms. DL-based semantic segmentation, in particular, has enabled precise predictions using Convolutional Neural Networks (CNNs). State-of-the-art CNN-based networks have achieved high accuracy on various datasets in multiple fields, such as building, scene, and object segmentation. However, subdomain shifts between training and test sets within a single domain can cause degraded accuracy in fine-grained segmentation. To counter this, this paper introduces a novel Sub-Domain Adaptation (SDA) framework for fine-grained and granular segmentation, which divides one single domain into multiple sub-domains and optimizes the baseline-network for each sub-domain. The baseline-network is further fine-tuned by recognizing the domain of the input in run-time, leading to more accurate predictions. Benchmarks of scene parsing, autonomous driving, and aerial imagery demonstrate the superior performance of SDA for granular segmentation.", "title_embedding_index": 4318, "title_abs_embedding_index": 4343}, {"title": "Diffusion Guided Adversarial State Perturbations in Reinforcement Learning", "link_suffix": "/forum?id=DoB8DmrsSS", "link": "https://openreview.net/forum?id=DoB8DmrsSS", "pdf_link": "https://openreview.net/pdf?id=DoB8DmrsSS", "keywords": "Reinforcement Learning, Adversarial Example, Diffusion Model", "abstract": "Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. Upon closer investigation, however, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$-norm constrained attacks, which can barely alter the semantics of the input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel diffusion-based state perturbation attack to go beyond this limitation. Specifically, we train a history-conditioned diffusion model, enhanced with policy guidance and realism detection to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, and significantly lowers the agent's cumulative reward in various Atari games by more than 50%. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies for safety-critical domains.", "title_embedding_index": 4319, "title_abs_embedding_index": 4344}, {"title": "Connectome Mapping: Shape-Memory Network via Interpretation of Contextual Semantic Information", "link_suffix": "/forum?id=PZYr22zFyE", "link": "https://openreview.net/forum?id=PZYr22zFyE", "pdf_link": "https://openreview.net/pdf?id=PZYr22zFyE", "keywords": "neural representation", "abstract": "Contextual semantic information plays a pivotal role in the brain's visual interpretation of the surrounding environment. When processing visual information, electrical signals within synapses facilitate the dynamic activation and deactivation of synaptic connections, guided by the contextual semantic information associated with different objects. In the realm of Artificial Intelligence (AI), neural networks have emerged as powerful tools to emulate complex signaling systems, enabling tasks such as classification and segmentation by understanding visual information. However, conventional neural networks have limitations in simulating the conditional activation and deactivation of synapses, collectively known as the connectome, a comprehensive map of neural connections in the brain. Additionally, the pixel-wise inference mechanism of conventional neural networks failed to account for the explicit utilization of contextual semantic information in the prediction process. To overcome these limitations, we developed a novel neural network, dubbed the Shape Memory Network (SMN), which excels in two key areas: (1) faithfully emulating the intricate mechanism of the brain's connectome, and (2) explicitly incorporating contextual semantic information during the inference process. The SMN memorizes the structure suitable for contextual semantic information and leverages this structure at the inference phase. The structural transformation emulates the conditional activation and deactivation of synaptic connections within the connectome. Rigorous experimentation carried out across a range of semantic segmentation benchmarks demonstrated the outstanding performance of the SMN, highlighting its superiority and effectiveness. Furthermore, our pioneering network on connectome emulation reveals the immense potential of the SMN for next-generation neural networks.", "title_embedding_index": 4320, "title_abs_embedding_index": 4345}, {"title": "MMMU-Pro: A More Robust  Multi-discipline Multimodal Understanding Benchmark", "link_suffix": "/forum?id=2jTdHYuguF", "link": "https://openreview.net/forum?id=2jTdHYuguF", "pdf_link": "https://openreview.net/pdf?id=2jTdHYuguF", "keywords": "Evaluation, Multimodal Understanding, Multimodal LLMs", "abstract": "This paper introduces MMMU-Pro, a robust version of the Massive Multi-discipline Multimodal Understanding and Reasoning (MMMU) benchmark. MMMU-Pro rigorously assesses multimodal models' true understanding and reasoning capabilities through a three-step process based on MMMU: (1) filtering out questions answerable by text-only models, (2) augmenting candidate options, and (3) introducing a vision-only input setting where questions are embedded within images. This setting challenges AI to truly \"see\" and \"read\" simultaneously, testing \\textit{a core human cognitive skill of seamlessly integrating visual and textual information}. Results show that model performance is substantially lower on MMMU-Pro than on MMMU, ranging from 16.8% to 26.9% across models. \nWe explore the impact of OCR prompts and Chain of Thought (CoT) reasoning, finding that OCR prompts have minimal effect while CoT generally improves performance. MMMU-Pro provides a more rigorous evaluation tool, closely mimicking real-world scenarios and offering valuable directions for future multimodal research.", "title_embedding_index": 4321, "title_abs_embedding_index": 4346}, {"title": "Surprising Effectiveness of pretraining Ternary  Language Model at Scale", "link_suffix": "/forum?id=TJo6aQb7mK", "link": "https://openreview.net/forum?id=TJo6aQb7mK", "pdf_link": "https://openreview.net/pdf?id=TJo6aQb7mK", "keywords": "Large Language Models, low-bit language models, quantization-aware training, pretraining of large language models, and scaling laws", "abstract": "Rapid advancements in GPU computational power has outpaced memory capacity and bandwidth growth, creating bottlenecks in Large Language Model (LLM) inference. Post-training quantization is the leading method for addressing memory-related bottlenecks in LLM inference, but it suffers from significant performance degradation below 4-bit precision. This paper addresses these challenges by investigating the pretraining of low-bitwidth models specifically Ternary Language Models (TriLMs) as an alternative to traditional floating-point models (FloatLMs) and their post-training quantized versions (QuantLMs). We present Spectra LLM suite, the first open suite of LLMs spanning multiple bit-widths, including FloatLMs, QuantLMs, and TriLMs, ranging from 99M to 3.9B parameters trained on 300B tokens. Our comprehensive evaluation demonstrates that TriLMs offer superior scaling behavior in terms of model size (in bits). Surprisingly, at scales exceeding one billion parameters, TriLMs consistently outperform their QuantLM and FloatLM counterparts for a given bit size across various benchmarks. Notably, the 3.9B parameter TriLM matches the performance of the FloatLM 3.9B across all benchmarks, despite having fewer bits than FloatLM 830M. Overall, this research provides valuable insights into the feasibility and scalability of low-bitwidth language models, paving the way for the development of more efficient LLMs.", "title_embedding_index": 4322, "title_abs_embedding_index": 4347}, {"title": "Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism", "link_suffix": "/forum?id=1YlfHUVq7q", "link": "https://openreview.net/forum?id=1YlfHUVq7q", "pdf_link": "https://openreview.net/pdf?id=1YlfHUVq7q", "keywords": "Error Broadcasting, Biologically Plausible Neural Networks, Backpropagation Alternative, Direct Feedback Alignment", "abstract": "We introduce the Error Broadcast and Decorrelation (EBD) algorithm, a novel learning framework that addresses the credit assignment problem in neural networks by directly broadcasting output error to individual layers. The EBD algorithm leverages the orthogonality property of the optimal minimum mean square error (MMSE) estimator, which states that estimation errors are orthogonal to any nonlinear function of the input, specifically the activations of each layer. By defining layerwise loss functions that penalize correlations between these activations and output errors, the EBD method offers a principled and efficient approach to error broadcasting. This direct error transmission eliminates the need for weight transport inherent in backpropagation. Additionally, the optimization framework of the EBD algorithm naturally leads to the emergence of the experimentally observed three-factor learning rule. We further demonstrate how EBD can be integrated with other biologically plausible learning frameworks, transforming time-contrastive approaches into single-phase, non-contrastive forms, thereby enhancing biological plausibility and performance. Numerical experiments demonstrate that EBD achieves performance comparable to or better than state-of-the-art methods on benchmark datasets. Our findings suggest that EBD offers a promising, principled direction for both artificial and natural learning paradigms, providing a biologically plausible and flexible alternative for neural network training  with inherent simplicity and adaptability that could benefit future developments in neural network technologies.", "title_embedding_index": 4323, "title_abs_embedding_index": 4348}, {"title": "Performant, Memory Efficient and Scalable Multi-Agent Reinforcement Learning", "link_suffix": "/forum?id=ZLOpfkf5aj", "link": "https://openreview.net/forum?id=ZLOpfkf5aj", "pdf_link": "https://openreview.net/pdf?id=ZLOpfkf5aj", "keywords": "Multi-agent reinforcement learning, Reinforcement Learning, Decision Making, Multi-agent systems", "abstract": "As the field of multi-agent reinforcement learning (MARL) progresses towards larger and more complex environments, achieving strong performance while maintaining memory efficiency and scalability to many agents becomes increasingly important. Although recent research has led to several advanced algorithms, to date, none fully address all of these key properties simultaneously. In this work, we introduce Sable, a novel and theoretically sound algorithm that adapts the retention mechanism from Retentive Networks to MARL. Sable's retention-based sequence modelling architecture allows for computationally efficient scaling to a large number of agents, as well as maintaining a long temporal context, making it well-suited for large-scale partially observable environments. Through extensive evaluations across six diverse environments, we demonstrate how Sable is able to significantly outperform existing state-of-the-art methods in the majority of tasks (34 out of 45, roughly 75%). Furthermore, Sable demonstrates stable performance as we scale the number of agents, handling environments with more than a thousand agents while exhibiting a linear increase in memory usage. Finally, we conduct ablation studies to isolate the source of Sable's performance gains and confirm its efficient computational memory usage. Our results highlight Sable's performance and efficiency, positioning it as a leading approach to MARL at scale.", "title_embedding_index": 4324, "title_abs_embedding_index": 4349}]