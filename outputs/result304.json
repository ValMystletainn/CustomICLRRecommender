[{"title": "AG-SLAM: Active Gaussian Splatting SLAM", "link_suffix": "/forum?id=Lwf5WeiyA9", "link": "https://openreview.net/forum?id=Lwf5WeiyA9", "pdf_link": "https://openreview.net/pdf?id=Lwf5WeiyA9", "keywords": "Active Learning, SLAM, View Synthesis, 3D Gaussian Splatting", "abstract": "We present AG-SLAM, the first active SLAM system utilizing 3D Gaussian Splatting (3DGS) for online scene reconstruction. In recent years, radiance field scene representations, including 3DGS have been widely used in SLAM and exploration, but actively planning trajectories for robotic exploration is still unvisited. In particular, many exploration methods assume precise localization and thus do not mitigate the significant risk of constructing a trajectory, which is difficult for a SLAM system to operate on. This can cause camera tracking failure and lead to failures in real-world robotic applications. Our method leverages Fisher Information to balance the dual objectives of maximizing the information gain for the environment while minimizing the cost of localization errors. Experiments conducted on the Gibson and Habitat-Matterport 3D datasets demonstrate state-of-the-art results of the proposed method.", "title_embedding_index": 15150, "title_abs_embedding_index": 15175}, {"title": "Ensemble and Mixture-of-Experts DeepONets For Operator Learning", "link_suffix": "/forum?id=BvMuyqPvk1", "link": "https://openreview.net/forum?id=BvMuyqPvk1", "pdf_link": "https://openreview.net/pdf?id=BvMuyqPvk1", "keywords": "scientific machine learning, basis enrichment, DeepONet, neural operators, operator learning, sparse methods", "abstract": "We present a novel deep operator network (DeepONet) architecture for operator learning, the ensemble DeepONet, that allows for enriching the trunk network of a single DeepONet with multiple distinct trunk networks. This trunk enrichment allows for greater expressivity and generalization capabilities over a range of operator learning problems. We also present a spatial mixture-of-experts (MoE) DeepONet trunk network architecture that utilizes a partition-of-unity (PoU) approximation to promote spatial locality and model sparsity in the operator learning problem. We first prove that both the ensemble and PoU-MoE DeepONets are universal approximators. We then demonstrate that ensemble DeepONets containing a trunk ensemble of a standard trunk, the PoU-MoE trunk, and/or a proper orthogonal decomposition (POD) trunk can achieve 2-4x lower relative $\\ell_2$ errors than standard DeepONets and POD-DeepONets on both standard and challenging new operator learning problems involving partial differential equations (PDEs) in two and three dimensions. Our new PoU-MoE formulation provides a natural way to incorporate spatial locality and model sparsity into any neural network architecture, while our new ensemble DeepONet provides a powerful and general framework for incorporating basis enrichment in scientific machine learning architectures for operator learning.", "title_embedding_index": 15151, "title_abs_embedding_index": 15176}, {"title": "Efficient Fatigue Modeling: Applying Operator Networks for Stress Intensity Factor Prediction and Analysis", "link_suffix": "/forum?id=Di3VLZHZdj", "link": "https://openreview.net/forum?id=Di3VLZHZdj", "pdf_link": "https://openreview.net/pdf?id=Di3VLZHZdj", "keywords": "Stress Intensity Factors, Scientific ML, Operator Network, Crack Growth Simulation, Fatigue Modeling, Solid Mechanics, AI for Science", "abstract": "Fatigue modeling is essential for material-related applications, including design, engineering, manufacturing, and maintenance. Central to fatigue modeling is the computation and analysis of stress intensity factors (SIFs), which model the crack-driving force and are influenced by factors such as geometry, load, crack shape, and crack size. Traditional methods are based on finite element analysis, which is computationally expensive. A common engineering practice is manually constructing handbook (surrogate) solutions, though these are limited when dealing with complex scenarios, such as intricate geometries. In this work, we reformulate SIF computation as an operator learning problem, leveraging recent advancements in data-driven operator networks to enable efficient and accurate predictions. Our results show that, when trained on a relatively small finite element dataset, operator networks --- such as Deep Operator Networks (DeepONet) and Fourier Neural Operators (FNO) --- achieve less than 5% relative error, significantly outperforming popular handbook solutions. We further demonstrate how these predictions can be integrated into crack growth simulations and used to calculate the probability of failure in small aircraft applications.", "title_embedding_index": 15152, "title_abs_embedding_index": 15177}, {"title": "When Large Models Meet Generalized Linear Models: Hierarchy Statistical Network for Secure Federated Learning", "link_suffix": "/forum?id=NwQfwm3tHf", "link": "https://openreview.net/forum?id=NwQfwm3tHf", "pdf_link": "https://openreview.net/pdf?id=NwQfwm3tHf", "keywords": "Federated Learning, Large Pre-trained Models, Generalized Linear Models, Security in Federated Learning, Poisoning Attacks, Deviance Residuals", "abstract": "Large pre-trained models perform well on many Federated Learning (FL) tasks. Recent studies have revealed that fine-tuning only the final layer of large pre-trained models can reduce computational and communication costs while maintaining high performance. We can model the final layer, which typically performs a linear transformation, as a Generalized Linear Model (GLM). GLMs offer advantages in statistical modeling, especially for anomaly detection. Leveraging these advantages, GLM-based methods can be utilized to enhance the security of the fine-tuning process for large pre-trained models. However, integrating GLMs with large pre-trained models in FL presents challenges. GLMs rely on linear decision boundaries and struggle with the complex feature representation spaces from pre-trained models. To address this, we introduce the Hierarchy Statistical Network (HStat-Net). HStat-Net refines the spaces to make them more discriminative, allowing GLMs to work effectively in FL. Based on HStat-Net, we further develop FedRACE to detect poisoning attacks using deviance residuals from GLMs. We also provide a theorem to support FedRACE\u2019s detection. Extensive experiments conducted on CIFAR-100, Food-101, and Tiny ImageNet demonstrate that FedRACE significantly outperforms existing state-of-the-art defense algorithms.", "title_embedding_index": 15153, "title_abs_embedding_index": 15178}, {"title": "Hybrid MILP to efficiently and accuratly  solve hard DNN verification instances", "link_suffix": "/forum?id=mUFdrdQJds", "link": "https://openreview.net/forum?id=mUFdrdQJds", "pdf_link": "https://openreview.net/pdf?id=mUFdrdQJds", "keywords": "Safety; Neural Netowrks; Verification; robustness; MILP", "abstract": "Deep neural networks have demonstrated remarkable capabilities, achieving human-like or even superior performance across a wide range of tasks. However, their robustness is often compromised by their susceptibility to input perturbations. This vulnerability has catalyzed the verification community to develop various methodologies, each presenting a unique balance between completeness and computational efficiency. $\\alpha,\\beta$-CROWN has won the last 4 VNNcomp(etitions), as the DNN verifier with the best \ntrade-off between accuracy vs computational time. VNNcomp however is focusing on relatively easy verification instances (network, inputs (images)), with few {\\em unstable nodes}. In this paper, we consider harder verification instances. On such instances, $\\alpha,\\beta$-CROWN displays a large gap ($20-58$%) between instances that can be verified, and instances with an explicit attack. Enabling much larger time-outs for $\\alpha,\\beta$-CROWN only improves verification rate by few percents, leaving a large gap of undecided instances while already taking a considerable amount of time. Resorting to other techniques, such as complete verifiers, does not fare better even with very large time-outs: They would theoretically be able to close the gap, but with an untractable runtime on all but small {\\em hard} instances.In this paper, we study the MILP encoding of ReLU-DNNs, and provide new insights in the LP relaxation. This allows us to carefully craft a {\\em partial MILP} solution which selects automatically few neurons encoded as integer variables, the rest using the LP relaxation. Compared with previous attempts, we can reduce the number of integer variables by around 4 times while maintaining the same level of accuracy. Implemented in {\\em Hybrid MILP}, calling first $\\alpha,\\beta$-CROWN with a short time-out to solve easier instances, and then partial MILP for those for which $\\alpha,\\beta$-CROWN fails, produces a very accurate yet efficient verifier, reducing tremendously the gap of undecided instances ($8-15$%), while keeping a reasonable runtime ( $46s-417s$ on average per instance ).", "title_embedding_index": 15154, "title_abs_embedding_index": 15179}, {"title": "Satisficing Exploration in Bandit Optimization", "link_suffix": "/forum?id=5WPQIVgWCg", "link": "https://openreview.net/forum?id=5WPQIVgWCg", "pdf_link": "https://openreview.net/pdf?id=5WPQIVgWCg", "keywords": "Online learning, Bandits, Satisficing, Regret", "abstract": "Motivated by the concept of satisficing in decision-making, we consider the problem of satisficing exploration in bandit optimization. In this setting, the learner aims at finding a satisficing arm whose mean reward exceeds a certain threshold. The performance is measured by satisficing regret, which is the cumulative deficit of the chosen arm's mean reward compared to the threshold. We propose SELECT, a general algorithmic template for Satisficing Exploration via LowEr Confidence bound Testing, that attains constant satisficing regret for a wide variety of bandit optimization problems in the realizable case (i.e., whenever a satisficing arm exists). Specifically, given a class of bandit optimization problems and a corresponding learning oracle with sub-linear (standard) regret upper bound, SELECT iteratively makes use of the oracle to identify a potential satisficing arm. Then, it collects data samples from this arm, and continuously compares the lower confidence bound of the identified arm's mean reward against the threshold value to determine if it is a satisficing arm. As a complement, SELECT also enjoys the same (standard) regret guarantee as the oracle in the non-realizable case. Finally, we conduct numerical experiments to validate the performance of SELECT for several popular bandit optimization settings.", "title_embedding_index": 15155, "title_abs_embedding_index": 15180}, {"title": "VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control", "link_suffix": "/forum?id=0n4bS0R5MM", "link": "https://openreview.net/forum?id=0n4bS0R5MM", "pdf_link": "https://openreview.net/pdf?id=0n4bS0R5MM", "keywords": "video generation, 3d, diffusion", "abstract": "Modern text-to-video synthesis models demonstrate coherent, photorealistic generation of complex videos from a text description. However, most existing models lack fine-grained control over camera movement, which is critical for downstream applications related to content creation, visual effects, and 3D vision. Recently, new methods demonstrate the ability to generate videos with controllable camera poses---these techniques leverage pre-trained U-Net-based diffusion models that explicitly disentangle spatial and temporal generation. Still, no existing approach enables camera control for new, transformer-based video diffusion models that process spatial and temporal information jointly. Here, we propose to tame video transformers for 3D camera control using a ControlNet-like conditioning mechanism that incorporates spatiotemporal camera embeddings based on Plucker coordinates. The approach demonstrates state-of-the-art performance for controllable video generation after fine-tuning on the RealEstate10K dataset. To the best of our knowledge, our work is the first to enable camera control for transformer-based video diffusion models.", "title_embedding_index": 15156, "title_abs_embedding_index": 15181}, {"title": "Kitten: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities", "link_suffix": "/forum?id=MY1pJ1RDMv", "link": "https://openreview.net/forum?id=MY1pJ1RDMv", "pdf_link": "https://openreview.net/pdf?id=MY1pJ1RDMv", "keywords": "Text-to-image generation", "abstract": "Recent advancements in text-to-image generation have significantly enhanced the quality of synthesized images. Despite this progress, evaluations predominantly focus on aesthetic appeal or alignment with text prompts. Consequently, there is limited understanding of whether these models can accurately represent a wide variety of realistic visual entities \u2014 a task requiring real-world knowledge. To address this gap, we propose a benchmark focused on evaluating Knowledge-InTensive image generaTion on real-world ENtities (i.e., KITTEN). Using KITTEN, we conduct a systematic study on the fidelity of entities in text-to-image generation models, focusing on their ability to generate a wide range of real-world visual entities, such as landmark buildings, aircraft, plants, and animals. We evaluate the latest text-to-image models and retrieval-augmented customization models using both automatic metrics and carefully-designed human evaluations, with an emphasis on the fidelity of entities in the generated images. Our findings reveal that even the most advanced text-to-image models often fail to generate entities with accurate visual details. Although retrieval-augmented models can enhance the fidelity of entity by incorporating reference images during testing, they often over-rely on these references and struggle to produce novel configurations of the entity as requested in creative text prompts.", "title_embedding_index": 15157, "title_abs_embedding_index": 15182}, {"title": "SeaS: Few-shot Industrial Anomaly Image Generation with Separation and Sharing Fine-tuning", "link_suffix": "/forum?id=VzZTHukfCB", "link": "https://openreview.net/forum?id=VzZTHukfCB", "pdf_link": "https://openreview.net/pdf?id=VzZTHukfCB", "keywords": "Industrial Anomaly Image Generation, Industrial Anomaly Segmentation", "abstract": "Current segmentation methods typically require many training images and precise masks, while insufficient anomaly images hinder their application in industrial scenarios. To address such an issue, we explore producing diverse anomalies and accurate pixel-wise annotations. By observing the real production lines, we find that anomalies vary randomly in shape and appearance, whereas products hold globally consistent patterns with slight local variations. Such a characteristic inspires us to develop a Separation and Sharing Fine-tuning (SeaS) approach using only a few abnormal and some normal images.\nFirstly, we propose the Unbalanced Abnormal (UA) Text Prompt tailored to industrial anomaly generation, consisting of one product token and several anomaly tokens. Then, for anomaly images, we propose a Decoupled Anomaly Alignment (DA) loss to bind the attributes of the anomalies to different anomaly tokens. Re-blending such attributes may produce never-seen anomalies, achieving a high diversity of anomalies. For normal images, we propose a Normal-image Alignment (NA) loss to learn the products' key features that are used to synthesize products with both global consistency and local variations. The two training processes are separated but conducted on a shared U-Net. Finally, SeaS produces high-fidelity annotations for the generated anomalies by  fusing discriminative features of U-Net and high-resolution VAE features. The extensive evaluations on the challenging MVTec AD and MVTec 3D AD dataset (RGB images) demonstrate the effectiveness of our approach. For anomaly image generation, on MVTec AD dataset, we achieve 1.88 on IS and 0.34 on IC-LPIPS, while on the MVTec 3D AD dataset, we obtain 1.95 on IS and 0.30 on IC-LPIPS.  For the downstream task, by using our generated anomaly image-mask pairs, three common segmentation methods achieve an average 11.17% improvement on IoU on MVTec AD dataset, and a 15.49% enhancement in IoU on the MVTec 3D AD dataset. The source code will be released publicly available.", "title_embedding_index": 15158, "title_abs_embedding_index": 15183}, {"title": "Structure Language Models for Protein Conformation Generation", "link_suffix": "/forum?id=OzUNDnpQyd", "link": "https://openreview.net/forum?id=OzUNDnpQyd", "pdf_link": "https://openreview.net/pdf?id=OzUNDnpQyd", "keywords": "protein, conformation generation, conformation sampling, generative models, language models, diffusion models", "abstract": "Proteins adopt multiple structural conformations to perform their diverse biological functions, and understanding these conformations is crucial for advancing drug discovery. Traditional physics-based simulation methods often struggle with sampling equilibrium conformations and are computationally expensive. Recently, deep generative models have shown promise in generating protein conformations as a more efficient alternative. However, these methods predominantly rely on the diffusion process within a 3D geometric space, which typically centers around the vicinity of metastable states and is often inefficient in terms of runtime. In this paper, we introduce Structure Language Modeling (SLM) as a novel framework for efficient protein conformation generation. Specifically, the protein structures are first encoded into a compact latent space using a discrete variational auto-encoder, followed by conditional language modeling that effectively captures sequence-specific conformation distributions.  This enables a more efficient and interpretable exploration of diverse ensemble modes compared to existing methods. Based on this general framework, we instantiate SLM with various popular LM architectures as well as proposing the ESMDiff, a novel BERT-like structure language model fine-tuned from ESM3 with masked diffusion. We verify our approach in various scenarios, including the equilibrium dynamics of BPTI, conformational change pairs, and intrinsically disordered proteins. SLM provides a highly efficient solution, offering a 20-100x speedup than existing methods in generating diverse conformations, shedding light on promising avenues for future research.", "title_embedding_index": 15159, "title_abs_embedding_index": 15184}, {"title": "RACCooN: A Versatile Instructional Video Editing Framework with Auto-Generated Narratives", "link_suffix": "/forum?id=qnGir4dyu9", "link": "https://openreview.net/forum?id=qnGir4dyu9", "pdf_link": "https://openreview.net/pdf?id=qnGir4dyu9", "keywords": "Video Inpainting, Video Editing, Video Caption, Multimodal Large Language Model", "abstract": "Recent video generative models primarily rely on carefully written text prompts for specific tasks, like inpainting or style editing. They require labor-intensive textual descriptions for input videos, hindering their flexibility to adapt personal/raw videos to user specifications. This paper proposes RACCooN, a versatile and user-friendly video-to-paragraph-to-video generative framework that supports multiple video editing capabilities, such as removal, addition, and modification, through a unified pipeline. RACCooN consists of two principal stages: Video-to-Paragraph (V2P) and Paragraph-to-Video (P2V). In the V2P stage, we automatically describe video scenes in well-structured natural language, capturing both the holistic context and focused object details. Subsequently, in the P2V stage, users can optionally refine these descriptions to guide the video diffusion model, enabling various modifications to the input video, such as removing, changing subjects, and/or adding new objects. The proposed approach stands out from other methods through several significant contributions: (1) RACCooN suggests a multi-granular spatiotemporal pooling strategy to generate well-structured video descriptions, capturing both the broad context and object details without requiring complex human annotations, simplifying precise video content editing based on text for users. (2) Our video generative model incorporates auto-generated narratives or instructions to enhance the quality and accuracy of the generated content. (3) RACCooN also plans to imagine new objects in a given video, so users simply prompt the model to receive a detailed video editing plan for complex video editing. The proposed framework demonstrates impressive versatile capabilities in video-to-paragraph generation (up to 9.4% absolute improvement in human evaluations against the baseline), video content editing (relative 49.7% in FVD), and can be incorporated into other SoTA video generative models for further enhancement.", "title_embedding_index": 15160, "title_abs_embedding_index": 15185}, {"title": "SafeDPO: A Simple Approach to Direct Preference Optimization with Enhanced Safety", "link_suffix": "/forum?id=MoJSnVZ59d", "link": "https://openreview.net/forum?id=MoJSnVZ59d", "pdf_link": "https://openreview.net/pdf?id=MoJSnVZ59d", "keywords": "Safety Alignment, LLM Fine-tuning, Preferences, Large Language Models, AI Safety", "abstract": "As large language models (LLMs) continue to advance and find applications across a growing number of fields, ensuring the safety of LLMs has become increasingly critical. To address safety concerns, recent studies have proposed integrating safety constraints into reinforcement learning from human feedback (RLHF). However, these approaches tend to be complex and often unstable, as they encompass complicated procedures in RLHF along with additional procedures required by the safety constraints. Inspired by direct preference optimization (DPO), we introduce a new algorithm called \\textit{SafeDPO}, which is designed to implicitly optimize the safety alignment objective within a single stage of policy learning. The resulting algorithm can be implemented by introducing only one additional hyperparameter, which aims to further enhance safety, along with minor modifications to the DPO implementation. Consequently, SafeDPO successfully eliminates the necessity of fitting a reward and a cost model, as well as sampling from the language model during fine-tuning, while still enhancing the safety of LLMs. Finally, we demonstrate that SafeDPO achieves competitive performance compared to the current state-of-the-art safety alignment algorithm, both in terms of aligning with human preferences and improving safety.", "title_embedding_index": 15161, "title_abs_embedding_index": 15186}, {"title": "Using Contrastive Learning with Generative Similarity to Learn Spaces that Capture Human Inductive Biases", "link_suffix": "/forum?id=OOt5RMI0JC", "link": "https://openreview.net/forum?id=OOt5RMI0JC", "pdf_link": "https://openreview.net/pdf?id=OOt5RMI0JC", "keywords": "contrastive learning, cognitive science, generative models, similarity, Bayesian inference, human intelligence, representations", "abstract": "Humans rely on strong inductive biases to learn from few examples and abstract useful information from sensory data. Instilling such biases in machine learning models has been shown to improve their performance on various benchmarks including few-shot learning, robustness, and alignment. However, finding effective training procedures to achieve that goal can be challenging as psychologically-rich training data such as human similarity judgments are expensive to scale, and Bayesian models of human inductive biases are often intractable for complex, realistic domains. Here, we address this challenge by introducing a Bayesian notion of generative similarity whereby two datapoints are considered similar if they are likely to have been sampled from the same distribution. This measure can be applied to complex generative processes, including probabilistic programs. We show that generative similarity can be used to define a contrastive learning objective even when its exact form is intractable, enabling learning of spatial embeddings that express specific inductive biases. We demonstrate the utility of our approach by showing that it can be used to capture human inductive biases for geometric shapes, distinguish different abstract drawing styles that are parameterized by probabilistic programs, and capture abstract high-level categories that enable generalization.", "title_embedding_index": 15162, "title_abs_embedding_index": 15187}, {"title": "Three Mechanisms of Feature Learning in a Linear Network", "link_suffix": "/forum?id=Wh4SE2S7Mo", "link": "https://openreview.net/forum?id=Wh4SE2S7Mo", "pdf_link": "https://openreview.net/pdf?id=Wh4SE2S7Mo", "keywords": "solvable model, feature learning, neural tangent kernel", "abstract": "Understanding the dynamics of neural networks in different width regimes is crucial for improving their training and performance. We present an exact solution for the learning dynamics of a one-hidden-layer linear network, with one-dimensional data, across any finite width, uniquely exhibiting both kernel and feature learning phases. This study marks a technical advancement by enabling the analysis of the training trajectory from any initialization and a detailed phase diagram under varying common hyperparameters such as width, layer-wise learning rates, and scales of output and initialization. We identify three novel prototype mechanisms specific to the feature learning regime: (1) learning by alignment, (2) learning by disalignment, and (3) learning by rescaling, which contrast starkly with the dynamics observed in the kernel regime. Our theoretical findings are substantiated with empirical evidence showing that these mechanisms also manifest in deep nonlinear networks handling real-world tasks, enhancing our understanding of neural network training dynamics and guiding the design of more effective learning strategies.", "title_embedding_index": 15163, "title_abs_embedding_index": 15188}, {"title": "Memory Proxy Maps for Visual Navigation", "link_suffix": "/forum?id=Zf7EFQt04n", "link": "https://openreview.net/forum?id=Zf7EFQt04n", "pdf_link": "https://openreview.net/pdf?id=Zf7EFQt04n", "keywords": "visual navigation, representation learning, hierarchical networks", "abstract": "Visual navigation takes inspiration from humans, who navigate in previously unseen environments using vision without detailed environment maps. Inspired by this, we introduce a novel no-RL, no-graph, no-odometry approach to visual navigation using feudal learning to build a three tiered agent. Key to our approach is a memory proxy map (MPM), an intermediate representation of the environment\nlearned in a self-supervised manner by the high-level manager agent that serves as a simplified memory, approximating what the agent has seen. We demonstrate that recording observations in this learned latent space is an effective and efficient memory proxy that can remove the need for graphs and odometry in visual navigation tasks. For the mid-level manager agent, we develop a waypoint network\n(WayNet) that outputs intermediate subgoals, or waypoints, imitating human waypoint selection during local navigation. For the low-level worker agent, we learn a classifier over a discrete action space that avoids local obstacles and moves the agent towards the WayNet waypoint. The resulting feudal navigation network offers a novel approach with no RL, no graph, no odometry, and no metric map; all while achieving SOTA results on the image goal navigation task.", "title_embedding_index": 15164, "title_abs_embedding_index": 15189}, {"title": "Matryoshka Multimodal Models", "link_suffix": "/forum?id=Uhj5OxAz7I", "link": "https://openreview.net/forum?id=Uhj5OxAz7I", "pdf_link": "https://openreview.net/pdf?id=Uhj5OxAz7I", "keywords": "Multimodal Model, matryoshka", "abstract": "Large Multimodal Models (LMMs) such as LLaVA have shown strong performance in visual-linguistic reasoning. These models first embed images into a fixed large number of visual tokens and then feed them into a Large Language Model (LLM). However, this design causes an excessive number of tokens for dense visual scenarios such as high-resolution images and videos, leading to great inefficiency. While token pruning/merging methods do exist, they produce a single length output for each image and do not afford flexibility in trading off information density v.s. efficiency. Inspired by the concept of Matryoshka Dolls, we propose \n: Matryoshka Multimodal Models, which learns to represent visual content as nested sets of visual tokens that capture information across multiple coarse-to-fine granularities. Our approach offers several unique benefits for LMMs: (1) One can explicitly control the visual granularity per test instance during inference, e.g. , adjusting the number of tokens used to represent an image based on the anticipated complexity or simplicity of the content; (2) \n provides a framework for analyzing the granularity needed for existing datasets, where we find that COCO-style benchmarks only need around \n9 visual tokens to obtain accuracy similar to that of using all 576 tokens; (3) Our approach provides a foundation to explore the best trade-off between performance and visual token length at sample level, where our investigation reveals that a large gap exists between the oracle upper bound and current fixed-scale representations.", "title_embedding_index": 15165, "title_abs_embedding_index": 15190}, {"title": "Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion", "link_suffix": "/forum?id=3UaOlzDEt2", "link": "https://openreview.net/forum?id=3UaOlzDEt2", "pdf_link": "https://openreview.net/pdf?id=3UaOlzDEt2", "keywords": "Video-Language Reasoning, Video Question Answering, Multimodal Fusion, Parameter-Efficient Fine-tuning", "abstract": "Despite impressive advancements in recent multimodal reasoning approaches, they are still limited in flexibility and efficiency, as these models typically process only a few fixed modality inputs and require updates to numerous parameters. This paper tackles these critical challenges and proposes CREMA, a generalizable, highly efficient, and modular modality-fusion framework that can incorporate many new modalities to enhance video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio, thermal heatmap, and touch map) from given videos without extra human annotation by leveraging sensors or existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a novel progressive multimodal fusion design supported by a lightweight fusion module and modality-sequential training strategy. It helps compress information across various assisting modalities, maintaining computational efficiency in the LLM while improving performance. We validate our method on 7 video-language reasoning tasks assisted by diverse modalities, including conventional VideoQA and Video-Audio/3D/Touch/Thermal QA, and achieve better/equivalent performance against strong multimodal LLMs, including OneLLM, BLIP-2, and SeViLA while reducing over 90% trainable parameters. We provide extensive analyses of CREMA, including the impact of each modality on reasoning domains, the design of the fusion module, and example visualizations.", "title_embedding_index": 15166, "title_abs_embedding_index": 15191}, {"title": "Unconstrained Robust Online Convex Optimization", "link_suffix": "/forum?id=z7JBs8UOLI", "link": "https://openreview.net/forum?id=z7JBs8UOLI", "pdf_link": "https://openreview.net/pdf?id=z7JBs8UOLI", "keywords": "online learning, online convex optimization, adversarial corruption, comparator adaptive, parameter-free, unconstrained domain", "abstract": "This paper addresses online learning with ''corrupted'' feedback. Our learner is provided with potentially corrupted gradients $\\tilde g_t$ instead of the ''true'' gradients $g_t$. We make no assumptions about how the corruptions arise: they could be the result of outliers, mislabeled data, or even malicious interference. We focus on the difficult  ''unconstrained'' setting in which our algorithm must maintain low regret with respect to any comparison point $||u|| \\in \\mathbb{R}^d$. Perhaps surprisingly, the unconstrained setting is significantly more challenging as existing algorithms suffer extremely high regret  even with very tiny amounts of corruption (which is not true in the case of a bounded domain). Our algorithms guarantee regret $ ||u||G (\\sqrt{T} + k) $ when Lipschitz constant $G \\ge \\max_t ||g_t||$ is known, where $k$ is a measure of the total amount of corruption. When $G$ is unknown and incur an extra additive penalty of $(||u||^2+G^2) k$.", "title_embedding_index": 15167, "title_abs_embedding_index": 15192}, {"title": "Stream-level flow matching from a Bayesian decision theoretic perspective", "link_suffix": "/forum?id=Jb1XGe4ioQ", "link": "https://openreview.net/forum?id=Jb1XGe4ioQ", "pdf_link": "https://openreview.net/pdf?id=Jb1XGe4ioQ", "keywords": "generative model, flow matching, Bayesian decision theory, Gaussian process", "abstract": "Flow matching (FM) is a family of training algorithms for fitting continuous normalizing flows (CNFs). A standard approach to FM, called conditional flow matching (CFM), exploits the fact that the marginal vector field of a CNF can be learned by fitting least-square regression to the so-called conditional vector field specified given one or both ends of the flow path. We show that viewing CFM training from a Bayesian decision theoretic perspective on parameter estimation opens the door to generalizations of CFM algorithms. We propose one such extension by introducing a CFM algorithm based on defining conditional probability paths given what we refer to as \"streams'', instances of latent stochastic paths that connect pairs of noise and observed data. Further, we advocates the modeling of these latent streams using Gaussian processes (GPs). The unique distributional properties of GPs, and in particular the fact that the velocities of a GP is still a GP, allows drawing samples from the resulting stream-augmented conditional probability path without simulating the actual streams, and hence the \"simulation-free\" nature of CFM training is preserved. We show that this generalization of the CFM can substantially reduce the variance in the estimated marginal vector field at a moderate computational cost, thereby improving the quality of the generated samples under common metrics. Additionally, we show that adopting the GP on the streams allows for flexibly linking multiple related training data points (e.g., time series) and incorporating additional prior information. We empirically validate our claim through both simulations and applications to two hand-written image datasets.", "title_embedding_index": 15168, "title_abs_embedding_index": 15193}, {"title": "Diffusion-based Prompt Generation for Lifelong Continual Adaptation", "link_suffix": "/forum?id=p2oFwfwebT", "link": "https://openreview.net/forum?id=p2oFwfwebT", "pdf_link": "https://openreview.net/pdf?id=p2oFwfwebT", "keywords": "Domain shift, Prompt generation, Lifelong continual adaptation, Diffusion model, Foundation model", "abstract": "Continual Test-time Adaptation (TTA) addresses sequential out-of-distribution scenarios with unlabeled data but overlooks long-term and recurring in-distribution aspects of the real world. Therefore, we introduce Lifelong Continual Adaptation, which enables models to efficiently retrieve domain-specific knowledge when encountering in-distribution data streams with sequential and recurring domains. We found that optimization-based Continual TTA methods underperform on the proposed problem due to two major pitfalls: updating the model's parameters is expensive and impractical for resource-constrained devices, and these methods exhibit instability when adapting to long-term recurring domains. To address these challenges, we propose a diffusion-based prompt generation method (DiffPrompt). Specifically, instead of continually optimizing the foundation model, we generate domain-specific prompts for it to adapt. We use a conditional diffusion model to learn a prompt-space distribution for various domains. During testing, the diffusion model generates prompts for the current domain based on the incoming batch of data, facilitating the continual adaptation of the foundation model. Our experiments demonstrate that DiffPrompt enables stable and efficient deployment in practical scenarios involving sequential and recurring domains.", "title_embedding_index": 15169, "title_abs_embedding_index": 15194}, {"title": "Mind the Gap: a Spectral Analysis of Rank Collapse and Signal Propagation in Transformers", "link_suffix": "/forum?id=X6xzYP2cMk", "link": "https://openreview.net/forum?id=X6xzYP2cMk", "pdf_link": "https://openreview.net/pdf?id=X6xzYP2cMk", "keywords": "transformers, attention mechanism, spectral analysis, initialisation, random matrix theory, signal propagation", "abstract": "Attention layers are the core component of transformers, the current state-of-the-art neural network architecture. However, \\softmaxx-based attention puts transformers' trainability at risk. Even \\textit{at initialisation}, the propagation of signals and gradients through the random network can be pathological, resulting in known issues such as (i) vanishing/exploding gradients and (ii) \\textit{rank collapse}, i.e. when all tokens converge to a single representation \\textit{with depth}. This paper examines signal propagation in \\textit{attention-only} transformers from a random matrix perspective, illuminating the origin of such issues, as well as unveiling a new phenomenon---(iii) rank collapse \\textit{in width}. Modelling \\softmaxx-based attention at initialisation with Random Markov matrices, our theoretical analysis reveals that a \\textit{spectral gap} between the two largest singular values of the attention matrix causes (iii), which, in turn, exacerbates (i) and (ii). Building on this insight, we propose a novel, yet simple, practical solution to resolve rank collapse in width by removing the spectral gap. Moreover, we validate our findings and discuss the training benefits of the proposed fix through experiments that also motivate a revision of some of the default parameter scaling. Our attention model accurately describes the standard key-query attention in a single-layer transformer, making this work a significant first step towards a better understanding of the initialisation dynamics in the multi-layer case.", "title_embedding_index": 15170, "title_abs_embedding_index": 15195}, {"title": "Flexible Heteroscedastic Count Regression with Deep Double Poisson Networks", "link_suffix": "/forum?id=j8SwCtP2RG", "link": "https://openreview.net/forum?id=j8SwCtP2RG", "pdf_link": "https://openreview.net/pdf?id=j8SwCtP2RG", "keywords": "Predictive uncertainty, Heteroscedastic regression", "abstract": "Neural networks that can produce accurate, input-conditional uncertainty representations are critical for real-world applications. Recent progress on heteroscedastic $\\textit{continuous}$ regression has shown great promise for calibrated uncertainty quantification on complex tasks, like image regression. However, when these methods are applied to $\\textit{discrete}$ regression tasks, such as crowd counting, ratings prediction, or inventory estimation, they tend to produce predictive distributions with numerous pathologies. Moreover, discrete models based on the Generalized Linear Model (GLM) framework either cannot process complex input or are not fully heterosedastic. To address these issues we propose the Deep Double Poisson Network (DDPN). In contrast to networks trained to minimize Gaussian negative log likelihood (NLL), discrete network parameterizations (i.e., Poisson, Negative binomial), and GLMs, DDPN can produce discrete predictive distributions of arbitrary flexibility. Additionally, we propose a technique to tune the prioritization of mean fit and probabilistic calibration during training. We show DDPN 1) vastly outperforms existing discrete models; 2) meets or exceeds the accuracy and flexibility of networks trained with Gaussian NLL; 3) produces proper predictive distributions over discrete counts; and 4) exhibits superior out-of-distribution detection. DDPN can easily be applied to a variety of count regression datasets including tabular, image, point cloud, and text data.", "title_embedding_index": 15171, "title_abs_embedding_index": 15196}, {"title": "Gradient-based inference of abstract task representations for generalization in neural networks", "link_suffix": "/forum?id=7MYu2xO4pp", "link": "https://openreview.net/forum?id=7MYu2xO4pp", "pdf_link": "https://openreview.net/pdf?id=7MYu2xO4pp", "keywords": "Cognitive science, cognitive control, cognitive abstractions, task representations, context-dependent models, variational expectation-maximization", "abstract": "Humans and many animals show remarkably adaptive behavior and can respond differently to the same input depending on their internal goals. The brain not only represents the intermediate abstractions needed to perform a computation but also actively maintains a representation of the computation itself (task abstraction). Such separation of the computation and its abstraction is associated with faster learning, flexible decision-making, and broad generalization capacity. We investigate if such benefits might extend to neural networks trained with task abstractions. For such benefits to emerge, one needs a task inference mechanism that possesses two crucial abilities: First, the ability to infer abstract task representations when no longer explicitly provided (task inference), and second, manipulate task representations to adapt to novel problems (task recomposition). To tackle this, we cast task inference as an optimization problem from a variational inference perspective and ground our approach in an expectation-maximization framework. We show that gradients backpropagated through a neural network to a task representation layer are an efficient heuristic to infer current task demands, a process we refer to as gradient-based inference (GBI). Further iterative optimization of the task representation layer allows for recomposing abstractions to adapt to novel situations. Using a toy example, a novel image classifier, and a language model, we demonstrate that GBI provides higher learning efficiency and generalization to novel tasks and limits forgetting. Moreover, we show that GBI has unique advantages such as preserving information for uncertainty estimation and detecting out-of-distribution samples.", "title_embedding_index": 15172, "title_abs_embedding_index": 15197}, {"title": "GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering", "link_suffix": "/forum?id=hJIEtJlvhL", "link": "https://openreview.net/forum?id=hJIEtJlvhL", "pdf_link": "https://openreview.net/pdf?id=hJIEtJlvhL", "keywords": "Inverse rendering, Gaussian Splatting, Relighting", "abstract": "We present GI-GS, a novel inverse rendering framework that leverages 3D Gaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novel view synthesis and relighting. In inverse rendering, accurately modeling the shading processes of objects is essential for achieving high-fidelity results. Therefore, it is critical to incorporate global illumination to account for indirect lighting that reaches an object after multiple bounces across the scene. Previous 3DGS-based methods have attempted to model indirect lighting by characterizing indirect illumination as learnable lighting volumes or additional attributes of each Gaussian, while using baked occlusion to represent shadow effects. These methods, however, fail to accurately model the complex physical interactions between light and objects, making it impossible to construct realistic indirect illumination during relighting. To address this limitation, we propose to calculate indirect lighting using efficient path tracing with deferred shading. In our framework, we first render a G-buffer to capture the detailed geometry and material properties of the scene. Then, we perform physically-based rendering (PBR) only for direct lighting. With the G-buffer and previous rendering results, the indirect lighting can be calculated through a lightweight path tracing. Our method effectively models indirect lighting under any given lighting conditions, thereby achieving better novel view synthesis and relighting. Quantitative and qualitative results show that our GI-GS outperforms existing baselines in both rendering quality and efficiency. Project page:https://gi-gs.github.io/GI-GS-iclr2025/.", "title_embedding_index": 15173, "title_abs_embedding_index": 15198}, {"title": "TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data", "link_suffix": "/forum?id=pZz0nOroGv", "link": "https://openreview.net/forum?id=pZz0nOroGv", "pdf_link": "https://openreview.net/pdf?id=pZz0nOroGv", "keywords": "vision-language model, large multimodal model, satellite imagery, earth observation, change detection", "abstract": "Large vision and language assistants have enabled new capabilities for interpreting natural images. These approaches have recently been adapted to earth observation data, but they are only able to handle single image inputs, limiting their use for many real-world tasks. In this work, we develop a new vision and language assistant called TEOChat that can engage in conversations about temporal sequences of earth observation data. To train TEOChat, we curate an instruction-following dataset composed of many single image and temporal tasks including building change and damage assessment, semantic change detection, and temporal scene classification. We show that TEOChat can perform a wide variety of spatial and temporal reasoning tasks, substantially outperforming previous vision and language assistants, and even achieving comparable or better performance than specialist models trained to perform these specific tasks. Furthermore, TEOChat achieves impressive zero-shot performance on a change detection and change question answering dataset, outperforms GPT-4o and Gemini 1.5 Pro on multiple temporal tasks, and exhibits stronger single image capabilities than a comparable single EO image instruction-following model. We publicly release our data, models, and code.", "title_embedding_index": 15174, "title_abs_embedding_index": 15199}]