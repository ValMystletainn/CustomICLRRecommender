[
    {
        "title": "KL DIVERGENCE OPTIMIZATION WITH ENTROPY- RATIO ESTIMATION FOR STOCHASTIC GFLOWNETS",
        "link_suffix": "/forum?id=Uj0h13lVrR",
        "link": "https://openreview.net/forum?id=Uj0h13lVrR",
        "pdf_link": "https://openreview.net/pdf?id=Uj0h13lVrR",
        "keywords": "Stochastic environments, MDP, GFlowNets, KL divergence, Molecule Generation",
        "abstract": "This paper introduces a novel approach for optimizing Generative Flow Networks (GFlowNets) in stochastic environments by incorporating KL divergence objectives with entropy-ratio estimation. We leverage the relationship between high and low entropy states, as defined in entropy-regularized Markov Decision Processes (MDPs), to dynamically adjust exploration and exploitation. Detailed proofs and analysis demonstrate the efficacy of this methodology in enhancing mode discovery, state coverage, and policy robustness in complex environments."
    },
    {
        "title": "Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces",
        "link_suffix": "/forum?id=AP0ndQloqR",
        "link": "https://openreview.net/forum?id=AP0ndQloqR",
        "pdf_link": "https://openreview.net/pdf?id=AP0ndQloqR",
        "keywords": "reinforcement learning, deep learning, geometry",
        "abstract": "Advances in reinforcement learning (RL) have led to its successful application in complex tasks with continuous state and action spaces. Despite these advances in practice, most theoretical work pertains to finite state and action spaces. We propose building a theoretical understanding of continuous state and action spaces by employing a geometric lens to understand the locally attained set of states. The set of all parametrised policies learnt through a semi-gradient based approach induce a set of attainable states in RL. We show that training dynamics of a two layer neural policy induce a low dimensional manifold of attainable states embedded in the high-dimensional nominal state space trained using an actor-critic algorithm. We prove that, under certain conditions, the dimensionality of this manifold is of the order of the dimensionality of the action space. This is the first result of its kind, linking the geometry of the state space to the dimensionality of the action space. We empirically corroborate this upper bound for four MuJoCo environments and also demonstrate the results in a toy environment with varying dimensionality. We also show the applicability of this theoretical result by introducing a local manifold learning layer to the policy and value function networks to improve the performance in control environments with very high degrees of freedom by changing one layer of the neural network to learn sparse representations."
    },
    {
        "title": "Conveyor: Efficient Tool-aware LLM Serving with Tool Partial Execution",
        "link_suffix": "/forum?id=A0VvDN4arV",
        "link": "https://openreview.net/forum?id=A0VvDN4arV",
        "pdf_link": "https://openreview.net/pdf?id=A0VvDN4arV",
        "keywords": "Large language models, External Tools",
        "abstract": "The complexity of large language model (LLM) serving workloads has substantially increased due to the integration with external tool invocations, such as ChatGPT plugins. In this paper, we identify a new opportunity for efficient LLM serving for requests that trigger tools: tool partial execution alongside LLM decoding. To this end, we design Conveyor, an efficient LLM serving system optimized for handling requests involving external tools. We introduce a novel interface for tool developers to expose partial execution opportunities to the LLM serving system and a request scheduler that facilitates partial tool execution. Our results demonstrate that tool partial execution can reduce request completion latency by up to 38.8%."
    },
    {
        "title": "Dopamine transients in the ventral striatum provide evidence for average-reward reinforcement learning",
        "link_suffix": "/forum?id=qoGdpin3om",
        "link": "https://openreview.net/forum?id=qoGdpin3om",
        "pdf_link": "https://openreview.net/pdf?id=qoGdpin3om",
        "keywords": "Average-reward reinforcement learning, Dopamine, Reward prediction error, Ventral striatum",
        "abstract": "Agents in real environments need to organize their behavior over a wide range of time scales. This might be achieved by reinforcement learning (RL) algorithms employing a spectrum of discount factors. Neural evidence for this idea includes recordings of dopamine (DA) release transients, which appear to reflect shorter time horizons in dorsal striatum and much longer horizons in ventral striatum (VS). However, this also presents a challenge, because with very long time horizons all states have similar, large values, impeding learning. Prior theoretical work has therefore proposed algorithms, including average-reward RL, that segregate out the large shared component of value. Here we compare temporal-difference reward prediction errors derived from recurrent neural network models (RNNs) to rat VS DA transients measured in three behavioral tasks. We show that using average-reward RL to train RNNs can provide an improved match to VS DA, compared to using discounting alone. We further find that the activity dynamics in RNNs trained with average-reward RL readily encodes key decision variables such as recent reward history, in a task-specific manner. The functional alignment between DA dynamics and average-reward RL may offer new insights into neural mechanisms of learning and decision-making."
    },
    {
        "title": "DLGrapher: Dual Latent Diffusion for Attributed Graph Generation",
        "link_suffix": "/forum?id=Oy9l6UDWIN",
        "link": "https://openreview.net/forum?id=Oy9l6UDWIN",
        "pdf_link": "https://openreview.net/pdf?id=Oy9l6UDWIN",
        "keywords": "graph generation, diffusion model, attributed graph",
        "abstract": "Graphs for applications like social data and financial transactions are particularly complex, with large node counts and high-dimensional features. State-of-the-art diffusion graph synthesizers model the node structure via discrete diffusion and are, unfortunately, limited to small-scale graphs with few to no features. In contrast, continuous diffusion models capture rich node features well, but have issues faithfully modelling connectivity. In this paper, we design DLGrapher, a  dual latent diffusion framework for jointly synthesizing large graph structures and high-dimension node features. DLGrapher models node features and structure as a joint latent representation. Structure-wise, we design a reversible coarsening scheme to merge pairs of similar neighboring nodes and their respective edges after encoding node features through a structure-aware variational autoencoder. To capture the dependencies between node features and the graph structure, DLGrapher trains a single diffusion over a dual denoising objective, one for the continuous node representations and another for the discrete edge connectivity. We extensively evaluate DLGrapher's performance on three complex social graph datasets against baselines combining tabular and graph synthesizers. Our solution fares 12.9x better at statistically capturing feature-structure interaction and 25.2% better at downstream tasks thanks to the dual diffusion on average and the latent compressed representation increases throughput by 2.5X. Furthermore, we maintain competitive synthesis quality for simple-featured molecular graphs and structure-only synthetic graphs while drastically reducing computation in the latter case."
    },
    {
        "title": "Detecting Adversarial Examples",
        "link_suffix": "/forum?id=KAWlH5pfQu",
        "link": "https://openreview.net/forum?id=KAWlH5pfQu",
        "pdf_link": "https://openreview.net/pdf?id=KAWlH5pfQu",
        "keywords": "adversarial machine learning, security, robustness",
        "abstract": "Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial examples. While numerous successful adversarial attacks have been proposed, defenses against these attacks remain relatively understudied. Existing defense approaches either focus on negating the effects of perturbations caused by the attacks to restore the DNNs' original predictions or use a secondary model to detect adversarial examples. However, these methods often become ineffective due to the continuous advancements in attack techniques. We propose a novel universal and lightweight method to detect adversarial examples by analyzing the layer outputs of DNNs. Through theoretical justification and extensive experiments, we demonstrate that our detection method is highly effective, compatible with any DNN architecture, and applicable across different domains, such as image, video, and audio."
    },
    {
        "title": "Towards Efficient Mixture of Experts: A Holistic Study of Compression Techniques",
        "link_suffix": "/forum?id=qh1goDZ0ZQ",
        "link": "https://openreview.net/forum?id=qh1goDZ0ZQ",
        "pdf_link": "https://openreview.net/pdf?id=qh1goDZ0ZQ",
        "keywords": "Mixture of Experts, Model Compression",
        "abstract": "Scaling large language models has driven remarkable advancements across various\ndomains, yet the continual increase in model size presents significant challenges\nfor real-world deployment. The Mixture of Experts (MoE) architecture offers a\npromising solution by dynamically selecting and activating only a subset of experts\nduring inference, thus substantially reducing computational costs while preserving\nhigh performance. Despite these benefits, MoE introduces new inefficiencies, such\nas excessive parameters and communication overhead. In this work, we present\na holistic study on compression techniques of Mixture of Experts to enhance\nboth efficiency and scalability. While recent efforts have focused on reducing the\nnumber of experts, these approaches still suffer from considerable communication\nand computational costs. To address this, we propose more aggressive strategies,\nsuch as Layer Drop, which removes entire MoE layers, and Block Drop, which\neliminates transformer blocks. Surprisingly, these aggressive structure pruning\ntechniques not only preserve model performance but also substantially improve\nefficiency. Additionally, beyond Expert Trimming, we also introduce Expert\nSlimming, which compresses individual experts to further boost performance and\ncan be seamlessly integrated with Expert Trimming. Extensive experimental results\ndemonstrate the effectiveness of our proposed methods \u2014 Layer Drop and Block\nDrop \u2014 along with the comprehensive recipe that integrates Expert Slimming and\nExpert Trimming, achieving a 6.05\u00d7 speedup with 77.1% reduced memory usage\nwhile maintaining over 92% of performance on Mixtral-8\u00d77B. Our code will be\nmade publicly available upon acceptance."
    },
    {
        "title": "Link Prediction on Textual Edge Graphs",
        "link_suffix": "/forum?id=lYDiuQ7vJA",
        "link": "https://openreview.net/forum?id=lYDiuQ7vJA",
        "pdf_link": "https://openreview.net/pdf?id=lYDiuQ7vJA",
        "keywords": "Link Prediction, Large Language Model, Graph Neural Network",
        "abstract": "Textual-edge Graphs (TEGs), characterized by rich text annotations on edges, are increasingly significant in network science due to their ability to capture rich contextual information among entities. Existing works have proposed various edge-aware graph neural networks (GNNs) or let language models directly make predictions. However, they often fail to fully capture the contextualized semantics on edges and graph topology, respectively. This inadequacy is particularly evident in link prediction tasks that require a comprehensive understanding of graph topology and semantics between nodes. In this paper, we present a novel framework - \\textsc{Link2Doc}, designed especially for link prediction on TEGs. Specifically, we propose to summarize neighborhood information between node pairs as a human-written document to preserve both semantic and topology information. We also present a specialized GNN framework to process the multi-scaled interaction between target nodes in a stratified manner. Finally, a self-supervised learning model is utilized to enhance the GNN's text-understanding ability from language models. Empirical evaluations, including link prediction, edge classification, parameter analysis, runtime comparison, and ablation studies, on five real-world datasets demonstrate that \\textsc{Link2Doc} achieves generally better performance against existing edge-aware GNNs and language models in link predictions."
    },
    {
        "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning",
        "link_suffix": "/forum?id=FJFVmeXusW",
        "link": "https://openreview.net/forum?id=FJFVmeXusW",
        "pdf_link": "https://openreview.net/pdf?id=FJFVmeXusW",
        "keywords": "Key-Value cache, Contextual reasoning, Efficiency inference, Large-Lauguage Model",
        "abstract": "Key-Value (KV) caching is a common technique to enhance the computational efficiency of Large Language Models (LLMs), but its memory overhead grows rapidly with input length. Prior work has shown that not all tokens are equally important for text generation, proposing layer-level KV cache compression to selectively retain key information. Recognizing the distinct roles of attention heads in generation, we propose HeadKV, a head-level KV cache compression method, and HeadKV-R2, which leverages a novel contextual reasoning ability estimation for compression. Our approach operates at the level of individual heads, estimating their importance for contextual QA tasks that require both retrieval and reasoning capabilities. Extensive experiments across diverse benchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct, Mistral-7B-Instruct), and long-context abilities tests demonstrate that our head-level KV cache compression significantly outperforms strong baselines, particularly in low-resource settings (KV size = 64 & 128). Notably, our method retains just 1.5% of the KV cache while achieving 97% of the performance of the full KV cache on the contextual question answering benchmark."
    },
    {
        "title": "Preble: Efficient Distributed Prompt Scheduling for LLM Serving",
        "link_suffix": "/forum?id=meKEKDhdnx",
        "link": "https://openreview.net/forum?id=meKEKDhdnx",
        "pdf_link": "https://openreview.net/pdf?id=meKEKDhdnx",
        "keywords": "LLM prefix caching, LLM serving, Distributed systems for ML",
        "abstract": "Prompts to large language models (LLMs) have evolved beyond simple user questions.\nFor LLMs to solve complex problems, today\u2019s practices are to include domain-specific\ninstructions, illustration of tool usages, and/or long context such as textbook chapters in\nprompts. As such, many parts of prompts are repetitive across requests. Recent works\npropose to cache and reuse KV state of prompts. However, they are all confined to a single-\nGPU optimization, while production LLM serving systems are distributed by nature.This paper proposes Preble, the first distributed LLM serving platform that targets and op-\ntimizes for prompt sharing. We designed a distributed scheduling system that co-optimizes\nKV state reuse and computation load-balancing with a new scheduling algorithm and a\nhierarchical scheduling mechanism. Our evaluation of Preble with real workloads and re-\nquest arrival patterns on two open-source LLMs shows that Preble outperforms the SOTA\nserving systems by 1.5\u00d7 to 14.5\u00d7 on average latency and 2\u00d7 to 10\u00d7 on p99 latency."
    },
    {
        "title": "Can foundation models actively gather information in interactive environments to test hypotheses?",
        "link_suffix": "/forum?id=F1Xb2sYR4H",
        "link": "https://openreview.net/forum?id=F1Xb2sYR4H",
        "pdf_link": "https://openreview.net/pdf?id=F1Xb2sYR4H",
        "keywords": "exploration, hypothesis testing, reinforcement learning, embodied environments, foundation models, large language models, visual language models",
        "abstract": "While problem solving is a standard evaluation task for foundation models, a crucial component of problem solving---actively and strategically gathering information to test hypotheses---has not been closely investigated. To assess the information gathering abilities of foundation models in interactive environments, we introduce a framework in which a model must determine the factors influencing a hidden reward function by iteratively reasoning about its previously gathered information and proposing its next exploratory action to maximize information gain at each step. We implement this framework in both a text-based environment, which offers a tightly controlled setting and enables high-throughput parameter sweeps, and in an embodied 3D environment, which requires addressing complexities of multi-modal interaction more relevant to real-world applications. We further investigate whether approaches such as self-correction and increased inference time improve information gathering efficiency. In a relatively simple task that requires identifying a single rewarding feature, we find that Gemini's information gathering capability is close to optimal. However, when the model must identify a conjunction of rewarding features, performance is suboptimal. The hit in performance is due partly to the model translating task description to a policy and partly to the model's effectiveness in using its in-context memory. Performance is comparable in both text and 3D embodied environments, although imperfect visual object recognition reduces its accuracy in drawing conclusions from gathered information in the 3D embodied case. For single-feature-based rewards, we find that smaller models curiously perform better; for conjunction-based rewards, incorporating self correction into the model improves performance."
    },
    {
        "title": "GenoAgent: A Baseline method for LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians",
        "link_suffix": "/forum?id=v7aeTmfGOu",
        "link": "https://openreview.net/forum?id=v7aeTmfGOu",
        "pdf_link": "https://openreview.net/pdf?id=v7aeTmfGOu",
        "keywords": "Multi-agent, Bioinformatics",
        "abstract": "Recent advancements in machine learning have significantly improved the identification of disease-associated genes from gene expression datasets. However, these processes often require extensive expertise and manual effort, limiting their scalability. Large Language Model (LLM)-based agents have shown promise in automating these tasks due to their increasing problem-solving abilities. To leverage the potential of agentic system, we introduce GenoAgent, a team of LLM-based agents designed with context-aware planning, iterative correction, and domain expert consultation to collaboratively explore gene datasets. GenoAgent provides generalized approach for addressing a wide range of gene identification problems, in a completely automated analysis pipeline that follows the standard of computational genomics. Our experiments with GenoAgent demonstrate the potential of LLM-based approaches in genomics data analysis, while error analysis highlights the challenges and areas for future improvement. We also propose GenoTEX, a benchmark dataset for automatic exploration of gene expression data, and also a promising resource for evaluating and enhancing AI-driven methods for genomics data analysis."
    },
    {
        "title": "Sufficient Context: A New Lens on Retrieval Augmented Generation Systems",
        "link_suffix": "/forum?id=Jjr2Odj8DJ",
        "link": "https://openreview.net/forum?id=Jjr2Odj8DJ",
        "pdf_link": "https://openreview.net/pdf?id=Jjr2Odj8DJ",
        "keywords": "retrieved augmented generation, RAG, LLMs, selective generation, sufficient context, factuality",
        "abstract": "Augmenting LLMs with context leads to improved performance across many applications. Despite much research on Retrieval Augmented Generation (RAG) systems, an open question is whether errors arise because LLMs fail to utilize the context from retrieval or the context itself is insufficient to answer the query. To shed light on this, we develop a new notion of sufficient context, along with a way to classify instances that have enough information to answer the query. We then use sufficient context to analyze several models and datasets. By stratifying errors based on context sufficiency, we find that proprietary LLMs (Gemini, GPT, Claude) excel at answering queries when the context is sufficient, but often output incorrect answers instead of abstaining when the context is not. On the other hand, open-source LLMs (Llama, Mistral, Gemma) hallucinate or abstain often, even with sufficient context. We further categorize cases when the context is useful, and improves accuracy, even though it does not fully answer the query and the model errs without the context. Building on our findings, we explore ways to reduce hallucinations in RAG systems, including a new selective generation method that leverages sufficient context information for guided abstention. Our method improves the fraction of correct answers among times where the model responds by 2--10% for Gemini, GPT, and Gemma."
    },
    {
        "title": "Efficient Continuous Video Flow Model for Video Prediction",
        "link_suffix": "/forum?id=6rydymz1Qg",
        "link": "https://openreview.net/forum?id=6rydymz1Qg",
        "pdf_link": "https://openreview.net/pdf?id=6rydymz1Qg",
        "keywords": "Video Diffusion model, video prediction model",
        "abstract": "Multi-step prediction models, such as diffusion and rectified flow models, have emerged as state-of-the-art solutions for generation tasks. However, these models exhibit higher latency in sampling new frames compared to single-step methods. This latency issue becomes a significant bottleneck when adapting such methods for video prediction tasks, given that a typical 60-second video comprises approximately 1.5K frames. In this paper, we propose a novel approach to modeling the multi-step process, aimed at alleviating latency constraints and facilitating the adaptation of such processes for video prediction tasks. Our approach not only reduces the number of sample steps required to predict the next frame but also minimizes computational demands by reducing the model size to one-third of the original size. We evaluate our method on standard video prediction datasets, including KTH, BAIR action robot, Human3.6M and UCF101, demonstrating its efficacy in achieving state-of-the-art performance on these benchmarks."
    },
    {
        "title": "Retraining with Predicted Hard Labels Provably Increases Model Accuracy",
        "link_suffix": "/forum?id=GCfllRcbvt",
        "link": "https://openreview.net/forum?id=GCfllRcbvt",
        "pdf_link": "https://openreview.net/pdf?id=GCfllRcbvt",
        "keywords": "Retraining; Label Noise; Classification; Model Accuracy; Label DP; Hard Label",
        "abstract": "The performance of a model trained withnoisy labelsis often improved by simplyretrainingthe model with its own predictedhardlabels (i.e., $1$/$0$ labels). Yet, a detailed theoretical characterization of this phenomenon is lacking. In this paper, we theoretically analyze retraining in a linearly separable setting with randomly corrupted labels given to us and prove that retraining can improve the population accuracy obtained by initially training with the given (noisy) labels. To the best of our knowledge, this is the first such theoretical result. Retraining finds application in improving training with local label differential privacy (DP) which involves training with noisy labels. We empirically show that retraining selectively on the samples for which the predicted label matches the given label significantly improves label DP training atno extra privacy cost; we call thisconsensus-based retraining. As an example, when training ResNet-18 on CIFAR-100 with $\\epsilon=3$ label DP, we obtain $6.4$% improvement in accuracy with consensus-based retraining."
    },
    {
        "title": "SAMBLE: Learning Shape-Specific Sampling Strategies for Point Cloud Shapes with Sparse Attention Map and Adaptive Bin Partitioning",
        "link_suffix": "/forum?id=LppenBe0fr",
        "link": "https://openreview.net/forum?id=LppenBe0fr",
        "pdf_link": "https://openreview.net/pdf?id=LppenBe0fr",
        "keywords": "3D Shapes, Point Cloud Sampling, Sparse Attention Map, Bin Partitioning",
        "abstract": "Point cloud sampling plays a pivotal role in facilitating efficient analysis of large-scale point clouds. Recently, learning-to-sample methods have garnered growing interest from the community, particularly for their ability to be jointly trained with downstream tasks. However, previous learning-based sampling methods either lead to unrecognizable sampling patterns by generating a new point cloud or biased sampled results by focusing excessively on shape details. Moreover, they all fail to take the natural point distribution variations over different shapes into consideration and learn a similar sampling strategy for all point clouds. In this paper, we propose a Sparse Attention Map and Bin-based Learning method (termed SAMBLE) to learn shape-specific sampling strategies for point cloud shapes, striking a superior balance between the overall shape outline and intricate local details for the sampling process. In particular, we first propose sparse attention map by integrating both local and global information. Based on this, multiple point-wise sampling score computation methods are proposed and explored by leveraging heatmaps as a guiding tool. Subsequently, we introduce a binning strategy that partitions points within each point cloud based on these scores. Finally, additional learnable tokens are introduced during the attention computation phase to acquire sampling weights for each bin, thereby enabling the development of shape-specific sampling strategies for an optimized sampling process. Extensive experiments demonstrate that our method adeptly strikes a refined balance between sampling edge points for local details and preserving uniformity in the global shape, leading to superior performance across common point cloud downstream tasks and even in scenarios involving few-point cloud sampling."
    },
    {
        "title": "What Matters in Transformers? Not All Attention is Needed",
        "link_suffix": "/forum?id=YLTWwEjkdx",
        "link": "https://openreview.net/forum?id=YLTWwEjkdx",
        "pdf_link": "https://openreview.net/pdf?id=YLTWwEjkdx",
        "keywords": "Transformer, Attention, Model Compression, Efficiency",
        "abstract": "While scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks, it also introduces redundant archi-\ntectures, posing efficiency challenges for real-world deployment. Despite some\nrecognition of redundancy in LLMs, the variability of redundancy across different\narchitectures in transformers, such as MLP and Attention layers, is under-explored.\nIn this work, we investigate redundancy across different modules within Trans-\nformers, including Blocks, MLP, and Attention layers, using a similarity-based\nmetric. Surprisingly, despite the critical role of attention layers in distinguishing\ntransformers from other architectures, we found that a large portion of these layers\nexhibit excessively high similarity and can be pruned without degrading perfor-\nmance. For instance, Llama-2-70B achieved a 48.4% speedup with only a 2.4%\nperformance drop by pruning half of the attention layers. Furthermore, by tracing\nmodel checkpoints throughout the training process, we observed that attention\nlayer redundancy is inherent and consistent across training stages. Additionally,\nwe further propose a method that jointly drops Attention and MLP layers, allowing\nus to more aggressively drop additional layers. For instance, when dropping 31\nlayers (Attention + MLP), Llama-2-13B still retains 90% of the performance on the\nMMLU task. Our work provides valuable insights for future network architecture\ndesign. The code will be released upon acceptance."
    },
    {
        "title": "Enhancing Accuracy and Parameter Efficiency of Neural Representations for Network Parameterization",
        "link_suffix": "/forum?id=0mo2yqOS6Z",
        "link": "https://openreview.net/forum?id=0mo2yqOS6Z",
        "pdf_link": "https://openreview.net/pdf?id=0mo2yqOS6Z",
        "keywords": "Implicit Neural Representations, Parameter Generation, Network Prediction, Distillation",
        "abstract": "In this work, we investigate the fundamental trade-off regarding accuracy and parameter efficiency in neural network weight parameterization using predictor networks. We present a surprising finding where the predicted model not only matches but also surpasses the original model's performance through the reconstruction objective (MSE loss) alone. Remarkably this improvement can be compound incrementally over multiple rounds of reconstruction. Moreover, we extensively explore the underlying factors for improving weight reconstruction under parameter-efficiency constraints and propose a novel training scheme that decouples the reconstruction objective from auxiliary objectives such as knowledge distillation that leads to significant improvements compared to state-of-the-art approaches. Finally, these results pave the way for more practical scenarios, where one needs to achieve improvements in both model accuracy and predictor network parameter-efficiency simultaneously."
    },
    {
        "title": "L-MSA: Layer-wise Fine-tuning using the Method of Successive Approximations",
        "link_suffix": "/forum?id=xi3sDtf8A0",
        "link": "https://openreview.net/forum?id=xi3sDtf8A0",
        "pdf_link": "https://openreview.net/pdf?id=xi3sDtf8A0",
        "keywords": "layer-wise finetuning, parameter-efficient fine-tuning, method of successive approximations",
        "abstract": "With the emergence of large-scale models, the machine learning community has witnessed remarkable advancements. However, the substantial memory consumption associated with these models has emerged as a significant obstacle to large-scale training. To mitigate this challenge, an increasing emphasis has been placed on parameter-efficient fine-tuning methodologies, which adapt pre-trained models by fine-tuning only a subset of parameters.  We observe that in various scenarios, fine-tuning different layers could lead to varying performance outcomes, and selectively fine-tuning certain layers has the potential to yield favorable performance results. Drawing upon this insight, we propose L-MSA, a novel layer-wise fine-tuning approach that integrates two key components: a metric for layer selection and an algorithm for optimizing the fine-tuning of the selected layers. By leveraging the principles of the Method of Successive Approximations, our method enhances model performance by targeting specific layers based on their unique characteristics and fine-tuning them efficiently. We also provide a theoretical analysis within deep linear networks, establishing a strong foundation for our layer selection criterion. Empirical evaluations across various datasets demonstrate that L-MSA identifies layers that yield superior training outcomes and fine-tunes them efficiently, consistently outperforming existing layer-wise fine-tuning methods."
    },
    {
        "title": "Smoothness Bridges Sparsity and Stability in MoEs",
        "link_suffix": "/forum?id=hAyw43h0MH",
        "link": "https://openreview.net/forum?id=hAyw43h0MH",
        "pdf_link": "https://openreview.net/pdf?id=hAyw43h0MH",
        "keywords": "Mixture-of-Experts (MoE), Model Sparsity, Training Stability",
        "abstract": "Mixture of experts architectures have recently emerged as an effective approach for scaling model capacity while managing computational costs by leveraging expert sparsity, where only a subset of experts is activated during inference. Despite their computational efficiency, MoE models face challenges in training stability compared to their dense counterparts, largely due to the introduction of expert sparsity. While several methods have been proposed to mitigate this instability, the underlying relationship between expert sparsity and training stability remains unclear. In this work, we develop a theoretical framework that demonstrates an inverse correlation between training stability and expert sparsity, with gradient smoothness serving as the bridge. We derive an upper bound on training stability, formalizing for the first time the sparsity-stability trade-off in MoE models. Our findings show that activating more experts enhances gradient smoothness and improves training stability but at the cost of reduced sparsity. We validate our theory through extensive experiments on various architectures and datasets, and propose a novel MoE structure that addresses stability without sacrificing sparsity. This design introduces independent router heads and a soft top-$K$ selection via sampling without replacement, which smooths the gradient landscape while maintaining expert sparsity. Further analysis confirms the promise of this structure in striking the optimal balance between sparsity and stability, offering a new direction for optimizing MoE architectures in large-scale models."
    },
    {
        "title": "Balancing Label Quantity and Quality for Scalable Elicitation",
        "link_suffix": "/forum?id=y8TjnkdWNA",
        "link": "https://openreview.net/forum?id=y8TjnkdWNA",
        "pdf_link": "https://openreview.net/pdf?id=y8TjnkdWNA",
        "keywords": "Scalable oversight, Alignment, Safety, Few-shot learning, Eliciting latent knowledge, Weak-to-strong generalization",
        "abstract": "Scalable oversight studies methods of training and evaluating AI systems in domains where human judgement is unreliable or expensive, such as scientific research and software engineering in complex codebases. Recent work in this area by Burns et al. (2023) suggests that Language Models (LMs) pretrained on internet-scale corpora exhibit an inductive bias toward producing correct answers, even when finetuned on error-prone labels produced by a smaller language model. This suggests that massive pretraining combined with finetuning on imperfect human labels may be a solid baseline method for scalable oversight. In the real world, however, label quality is not fixed: practitioners face a quantity-quality tradeoff when generating finetuning data. In this paper, we explore the microeconomics of the quantity-quality tradeoff on binary NLP classification tasks used in Burns et al. (2023).  We find that there are three regimes of eliciting classification knowledge from pretrained models using supervised finetuning: quantity-dominant, quality-dominant, and a mixed regime involving the use of low- and high-quality data together to attain higher accuracy at a lower cost than using either alone. We explore sample-efficient elicitation methods that make use of two datasets of differing qualities, and establish a Pareto frontier of scalable elicitation methods that optimally trade off labeling cost and classifier performance."
    },
    {
        "title": "SELFIES-TED : A Robust Transformer Model for Molecular Representation using SELFIES",
        "link_suffix": "/forum?id=uPj9oBH80V",
        "link": "https://openreview.net/forum?id=uPj9oBH80V",
        "pdf_link": "https://openreview.net/pdf?id=uPj9oBH80V",
        "keywords": "molecular representation, property prediction, molecular generation",
        "abstract": "Large-scale molecular representation methods have revolutionized applications in material science, such as drug discovery, chemical modeling, and material design. With the rise of transformers, models now learn representations directly from molecular structures. In this paper, we introduce SELFIES-TED, a transformer-based model designed for molecular representation using SELFIES, a more robust, unambiguous method for encoding molecules compared to traditional SMILES strings. By leveraging the robustness of SELFIES and the power of the transformer encoder-decoder architecture, SELFIES-TED effectively captures the intricate relationships between molecular structures and their properties. Our model demonstrates improved performance on molecular property prediction tasks across various benchmarks, showcasing its generalizability and robustness. Additionally, we explore the latent space of SELFIES-TED, revealing valuable insights that enhance its capabilities in molecule generation tasks, opening new avenues for innovation in molecular design."
    },
    {
        "title": "Score-Based Neural Processes",
        "link_suffix": "/forum?id=rZzcaduYU1",
        "link": "https://openreview.net/forum?id=rZzcaduYU1",
        "pdf_link": "https://openreview.net/pdf?id=rZzcaduYU1",
        "keywords": "Neural Processes, Diffusion Models, Generative Models, Stochastic Processes",
        "abstract": "Neural processes (NP) are a flexible class of models that generate stochastic\nprocesses by operating on finite-dimensional marginal distributions. NPs are\ndesigned to maintain exchangeability and marginal consistency, which are necessary\nto define a valid stochastic process. However, NP variants can come with drawbacks\nsuch as limited expressivity, uncorrelated samples, and consistency sacrifices. To address the issues of previous NPs, we introduce score-based neural processes, \\emph{scoreNP}, which incorporate a score-based generative model within the neural process paradigm. This score-based approach enhances expressivity, allowing the model to capture complex non-Gaussian distributions of functions, generate correlated samples, and maintain marginal consistency. Previously, no NP variant has been able to maintain conditional consistency. We show that using \\emph{guidance} methods from conditional diffusion sampling, \\emph{scoreNP} is the first NP is able to satisfy conditional consistency. Empirically, \\emph{scoreNP} performs well qualitatively and quantitatively well across a range of unconditional and conditional functional generation tasks."
    },
    {
        "title": "Exploratory Preference Optimization: Provably Sample-Efficient Exploration in RLHF with General Function Approximation",
        "link_suffix": "/forum?id=QYigQ6gXNw",
        "link": "https://openreview.net/forum?id=QYigQ6gXNw",
        "pdf_link": "https://openreview.net/pdf?id=QYigQ6gXNw",
        "keywords": "Learning theory, Reinforcement learning theory, Sample-efficient reinforcement learning",
        "abstract": "This paper investigates a basic question in reinforcement learning from human feedback (RLHF) from a theoretical perspective: how to efficiently explore in an online manner under preference feedback and general function approximation. We take the initial step towards a theoretical understanding of this problem by proposing a novel algorithm,Exploratory Preference Optimization(XPO). This algorithm is elegantly simple---requiring only a one-line modification to (online) Direct Preference Optimization  (DPO; Rafailov et al., 2023)---yet provides the strongest known provable guarantees. XPO augments the DPO objective with a novel and principledexploration bonus, enabling the algorithm to strategically explore beyond the support of the initial model and preference feedback data. We prove that XPO is provably sample-efficient and converges to a near-optimal policy under natural exploration conditions, regardless of the initial model's coverage. Our analysis builds on the observation that DPO implicitly performs a form ofBellman error minimization. It synthesizes previously disparate techniques from language modeling and theoretical reinforcement learning in a serendipitous fashion through the lens ofKL-regularized Markov decision processes."
    },
    {
        "title": "Fine-Grained Machine-Generated Text Detection",
        "link_suffix": "/forum?id=FHsaa6lZMp",
        "link": "https://openreview.net/forum?id=FHsaa6lZMp",
        "pdf_link": "https://openreview.net/pdf?id=FHsaa6lZMp",
        "keywords": "Machine-Generated Text Detection, Fine-grained Classification, Mixture of Experts",
        "abstract": "Machine-Generated Text (MGT) detection identifies whether a given text is human-written or machine-generated. However, this can result in detectors that would flag paraphrased or translated text as machine-generated. Fine-grained classification that separates the different types of machine text is valuable in real-world applications, as different types of MGT convey distinct implications. For example, machine-generated articles are more likely to contain misinformation, whereas paraphrased and translated texts may improve understanding of human-written text. Despite this benefit, existing studies consider this a binary classification task, either overlooking machine-paraphrased and machine-translated text entirely or simply grouping all machine-processed text into one category.  To address this shortcoming, this paper provides an in-depth study of fine-grained MGT detection, categorizing input text into four classes: human-written, machine-generated, machine-paraphrased, and machine-translated. A key challenge is the performance drop on out-of-domain texts due to the variability in text generators, especially for translated or paraphrased text. We introduce a RoBERTa-based Mixture of Detectors (RoBERTa-MoD), which leverages multiple domain-optimized detectors for more robust and generalized performance. We offer theoretical proof that our method outperforms a single detector, and experimental findings demonstrate a 5--9% improvement in mean Average Precision (mAP) over prior work on six diverse datasets: GoodNews, VisualNews, WikiText, Essay, WP, and Reuters. Our code and data will be publicly released upon acceptance."
    }
]