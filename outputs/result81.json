[
    {
        "title": "Model Zoos for Benchmarking Phase Transitions in Neural Networks",
        "link_suffix": "/forum?id=JlkqReTftJ",
        "link": "https://openreview.net/forum?id=JlkqReTftJ",
        "pdf_link": "https://openreview.net/pdf?id=JlkqReTftJ",
        "keywords": "Phase Transition, Model Zoo, Population, Evaluation",
        "abstract": "Understanding the complex dynamics of neural network training remains a central challenge in deep learning research.\nWork rooted in statistical physics has identified phases and phase transitions in neural network (NN) models, where models within the same phase exhibit similar characteristics but qualitatively differ across phases. A prominent example is the double-descent phenomenon. \nRecognizing these transitions is essential for building a deeper understanding of model behavior and the underlying mechanics.\nSo far, these phases are typically studied in isolation or in specific applications. \nIn this paper, we show that phase transitions are a widespread phenomenon.\nHowever, identifying phase transitions across different methods requires populations that cover different phases.\nFor that reason, we introduce Phase Transition Model Zoos, a structured collection of neural networks trained on diverse datasets and architectures. These model zoos are carefully designed to help researchers systematically identify and study phase transitions in their methods. \nWe demonstrate the relevance of phase transitions across multiple applications, including fine-tuning, transfer learning, out-of-distribution generalization, pruning, ensembling, and weight averaging. The diversity of applications underscores the universal nature of phase transitions and their impact on different tasks.\nBy providing the first structured dataset specifically designed to capture phase transitions in NNs, we offer a valuable tool for the community to systematically evaluate machine learning methods and improve their understanding of phase behavior across a wide range of applications and architectures."
    },
    {
        "title": "Mimetic Initialization Helps State Space Models Learn to Recall",
        "link_suffix": "/forum?id=iVy7aRMb0K",
        "link": "https://openreview.net/forum?id=iVy7aRMb0K",
        "pdf_link": "https://openreview.net/pdf?id=iVy7aRMb0K",
        "keywords": "Mamba, SSM, sequence models, language models, initialization, linear attention",
        "abstract": "Recent work has shown that state space models such as Mamba are significantly worse than Transformers on recall-based tasks due to the fact that their state size is constant with respect to their input sequence length. But in practice, state space models have fairly large state sizes, and we conjecture that they should be able to perform much better at these tasks than previously reported. We investigate whether their poor copying and recall performance could be due in part to training difficulties rather than fundamental capacity constraints. Based on observations of their \"attention'' maps, we propose a structured initialization technique that allows state space layers to more readily mimic attention. Across a variety of architecture settings, our initialization makes it substantially easier for Mamba to learn to copy and do associative recall from scratch."
    },
    {
        "title": "Generalization and Distributed Learning of GFlowNets",
        "link_suffix": "/forum?id=PJNhZoCjLh",
        "link": "https://openreview.net/forum?id=PJNhZoCjLh",
        "pdf_link": "https://openreview.net/pdf?id=PJNhZoCjLh",
        "keywords": "GFlowNets",
        "abstract": "Conventional wisdom attributes the success of Generative Flow Networks (GFlowNets) to their ability to exploit the compositional structure of the sample space for learning generalizable flow functions (Bengio et al., 2021). Despite the abundance of empirical evidence, formalizing this belief with verifiable non-vacuous statistical guarantees has remained elusive. We address this issue with the first data-dependent generalization bounds for GFlowNets. We also elucidate the negative impact of the state space size on the generalization performance of these models via Azuma-Hoeffding-type oracle PAC-Bayesian inequalities. We leverage our theoretical insights to design a novel distributed learning algorithm for GFlowNets, which we callSubgraph Asynchronous Learning(SAL). In a nutshell, SAL utilizes a divide-and-conquer strategy: multiple GFlowNets are trained in parallel on smaller subnetworks of the flow network, and then aggregated with an additional GFlowNet that allocates appropriate flow to each subnetwork.  Our experiments with synthetic and real-world problems demonstrate the benefits of SAL over centralized training in terms of mode coverage and distribution matching."
    },
    {
        "title": "MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions",
        "link_suffix": "/forum?id=WWXjMYZxfH",
        "link": "https://openreview.net/forum?id=WWXjMYZxfH",
        "pdf_link": "https://openreview.net/pdf?id=WWXjMYZxfH",
        "keywords": "Human Alignment, Large Language Models, Reinforcement Learning",
        "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated effectiveness in aligning large language models (LLMs) with human preferences. However, token-level RLHF suffers from the credit assignment problem over long sequences, where delayed rewards make it challenging for the model to discern which actions contributed to successful outcomes. This hinders learning efficiency and slows convergence. In this paper, we propose MA-RLHF, a simple yet effective RLHF framework that incorporates macro actions-- sequences of tokens or higher-level language constructs--into the learning process. By operating at this higher level of abstraction, our approach reduces the temporal distance between actions and rewards, facilitating faster and more accurate credit assignment. This results in more stable policy gradient estimates and enhances learning efficiency within each episode, all without increasing computational complexity during training or inference. We validate our approach through extensive experiments across various model sizes and tasks, including text summarization, dialogue generation, question answering, and program synthesis. Our method achieves substantial performance improvements over standard RLHF, with performance gains of up to 30% in text summarization and code generation, 18% in dialogue, and 8% in question answering tasks. Notably, our approach reaches parity with vanilla RLHF 1.7x to 2x faster in terms of training time and continues to outperform it with further training. We will release our code, data, and models to inspire future research."
    },
    {
        "title": "High Probability Contextual Bandits for Optimal Dosage Selection",
        "link_suffix": "/forum?id=z0B7A6Dh1H",
        "link": "https://openreview.net/forum?id=z0B7A6Dh1H",
        "pdf_link": "https://openreview.net/pdf?id=z0B7A6Dh1H",
        "keywords": "Linear Bandits, Dosage Selection, Contextual Bandits",
        "abstract": "Multi-Armed Bandit ($\\textit{MAB}$) formulations are commonly used to model the problem of $\\textit{Optimal Dose-Finding}$.\nHowever, in many practical applications, it is necessary to receive data about the patient’s current state and then administer a drug dosage adapted to that state. \nTo overcome this issue, we adopt a linear contextual bandit formulation with stage-wise constraints.\nAt each round, the learner selects a dosage and receives both a reward signal and a cost signal.\nThe learner’s goal is to maximize the drug's efficacy—captured as the expected cumulative reward—while ensuring that the toxicity, reflected by the cost signal, remains below a known threshold.\nSatisfying the cost signal constraint only in expectation can be dangerous, as it may lead to over-dosage complications in certain cases.\nTo address this issue, we introduce a novel model that controls the realization of the cost signal with high probability, in contrast to previous works where control was only applied to the expected cost signal.\nOur algorithm follows the $\\textit{UCB}$ approach, for which we establish a regret bound over \n$T$ rounds and run numerical experiments.\nWe further generalize our results to $\\textit{non-linear}$ functions and provide a regret bound in terms of the $\\textit{eluder dimension}$, a measure of function class complexity."
    },
    {
        "title": "DiscQuant: A Quantization Method for Neural Networks Inspired by Discrepancy Theory",
        "link_suffix": "/forum?id=vJmpg0exYA",
        "link": "https://openreview.net/forum?id=vJmpg0exYA",
        "pdf_link": "https://openreview.net/pdf?id=vJmpg0exYA",
        "keywords": "Quantization, Discrepancy Theory, LLMs, Weights Only Quantization",
        "abstract": "Quantizing the weights of a neural network has two steps: (1) Finding a good low bit-complexity representation for weights (which we call the quantization grid) and (2) Rounding the original weights to values in the quantization grid. In this paper, we study the problem of rounding optimally given any quantization grid. The simplest and most commonly used way to round is Round-to-Nearest (RTN). By rounding in a data-dependent way instead, one can improve the quality of the quantized model significantly.We study the rounding problem from the lens of \\emph{discrepancy theory}, which studies how well we can round a continuous solution to a discrete solution without affecting solution quality too much. We prove that given $m=poly(1/\\epsilon)$ samples from the data distribution, we can round all but $O(m)$ model weights such that the expected approximation error of the quantized model on the true data distribution is $\\le \\epsilon$ as long as the space of gradients of the original model is approximately low rank (which we empirically validate).Our proof, which is algorithmic, inspired a  simple and practical rounding algorithm called \\emph{DiscQuant}. In our experiments, we demonstrate that DiscQuant significantly improves over the prior state-of-the-art rounding method called GPTQ and the baseline RTN over a range of benchmarks on Phi3mini-3.8B and Llama3.1-8B. For example, rounding Phi3mini-3.8B to a fixed quantization grid with 3.25 bits per parameter using DiscQuant gets 64% accuracy on the GSM8k dataset, whereas GPTQ achieves 54% and RTN achieves 31% (the original model achieves 84%)."
    },
    {
        "title": "Bonsai: Gradient-free Graph Distillation for Node Classification",
        "link_suffix": "/forum?id=5x88lQ2MsH",
        "link": "https://openreview.net/forum?id=5x88lQ2MsH",
        "pdf_link": "https://openreview.net/pdf?id=5x88lQ2MsH",
        "keywords": "Graph Neural Networks, Machine Learning, Data Distillation, Graph Distillation, Dataset Distillation, Sustainable AI",
        "abstract": "Graph distillation has emerged as a promising avenue to enable scalable training of GNNs by compressing the training dataset while preserving essential graph characteristics. Our study uncovers significant shortcomings in current graph distillation techniques. First, the majority of the algorithms paradoxically require training on the full dataset to perform distillation. Second, due to their gradient-emulating approach, these methods require fresh distillation for any change in hyperparameters or GNN architecture, limiting their flexibility and reusability. Finally, they fail to achieve substantial size reduction due to synthesizing fully-connected, edge-weighted graphs. To address these challenges, we present Bonsai, a novel graph distillation method empowered by the observation thatcomputation treesform the fundamental processing units of message-passing GNNs. Bonsai distills datasets by encoding a careful selection ofexemplartrees that maximize the representation of all computation trees in the training set. This unique approach imparts Bonsai as the first linear-time, model-agnostic graph distillation algorithm for node classification that outperforms existing baselines across $6$ real-world datasets on accuracy, while being $22$ times faster on average. Bonsai is grounded in rigorous mathematical guarantees on the adopted approximation strategies making it robust to GNN architectures, datasets, and parameters."
    },
    {
        "title": "Informative Data Selection for Thorax Disease Classification",
        "link_suffix": "/forum?id=VbkGysQ0Rl",
        "link": "https://openreview.net/forum?id=VbkGysQ0Rl",
        "pdf_link": "https://openreview.net/pdf?id=VbkGysQ0Rl",
        "keywords": "Informative Data Selection, Generative Data Augmentation, Thorax Disease Classification",
        "abstract": "Although Deep Neural Networks (DNNs) such as Vision Transformers (ViTs) have demonstrated superior performance in medical imaging tasks, the training of DNNs usually requires large amounts of high-quality labeled training data, which is usually difficult or even impractical to collect in the medical domain. To address this issue, Generative Data Augmentation (GDA) has been employed to improve the performance of DNNs trained on augmented training data comprising both original training data in the standard benchmark datasets and synthetic training data generated by generative models such as Diffusion Models (DMs). However, the synthetic data generated by GDA universally suffer from noise, and such synthetic data can severely hurt the performance of classifiers trained on the augmented training data. Existing works, such as data selection and data re-weighting methods aiming to mitigate this issue, usually depend on a given clean metadata or external classifier.\nIn this work, we propose a principled sample re-weighting method, Informative Data Selection (IDS), based on an established information theoretic measure, the Information Bottleneck (IB), to improve the performance of DNNs trained for thorax disease classification with GDA. Extensive experiments demonstrate that IDS successfully assigns higher weights to more informative synthetic images and significantly outperforms existing data selection and data re-weighting methods in GDA for thorax disease classification.\nThe code of IDS is available at \\url{https://anonymous.4open.science/r/IDS-20D1}."
    },
    {
        "title": "TVBench: Redesigning Video-Language Evaluation",
        "link_suffix": "/forum?id=DrNN5qx66Z",
        "link": "https://openreview.net/forum?id=DrNN5qx66Z",
        "pdf_link": "https://openreview.net/pdf?id=DrNN5qx66Z",
        "keywords": "Video-Language evaluation, Video-Language benchmark",
        "abstract": "Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating these video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than visual reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that most recent state-of-the-art video-language models perform similarly to random performance on TVBench, with only Gemini-Pro and Tarsier surpassing this baseline."
    },
    {
        "title": "Explainable Sequential Optimization",
        "link_suffix": "/forum?id=Ut4XAYg0NF",
        "link": "https://openreview.net/forum?id=Ut4XAYg0NF",
        "pdf_link": "https://openreview.net/pdf?id=Ut4XAYg0NF",
        "keywords": "Explainability, Sequential Optimization",
        "abstract": "We propose formulating stochastic model predictive control into a coalition game to use Shapley values for feature attribution. Such analysis is crucial for transparency and achieving optimal outcomes in high-stake applications such as portfolio optimization and autonomous driving. We categorize Shapley values estimation methods into three families: those based on weighted linear regression, sampling permutations, and multilinear extension.  We survey, benchmark, and provide valuable insight into these methods, previously not attempted in this context. Our experiments show that halved Owen sampling from multilinear extension and KernelShap-Paired from weighted linear regression, both utilizing antithetic sampling, perform best."
    },
    {
        "title": "Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs",
        "link_suffix": "/forum?id=urQi0TgXFY",
        "link": "https://openreview.net/forum?id=urQi0TgXFY",
        "pdf_link": "https://openreview.net/pdf?id=urQi0TgXFY",
        "keywords": "Large Language Models, Steganography, Collusion, Reinforcement Learning, In-Context Learning, Multi-agent Systems",
        "abstract": "The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render collusion practically undetectable. This underscores the need for evaluation frameworks to monitor and mitigate steganographic collusion capabilities. We address a crucial gap in the literature by demonstrating, for the first time, that robust steganographic collusion in LLMs can arise indirectly from optimization pressure. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography. Importantly, we find that emergent steganographic collusion can be robust to both passive steganalytic oversight of model outputs and active mitigation through communication paraphrasing. We contribute a novel model evaluation framework and discuss limitations and future work. Our findings imply that effective risk mitigation from steganographic collusion post-deployment requires innovation in passive and active oversight techniques."
    },
    {
        "title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks",
        "link_suffix": "/forum?id=CN2bmVVpOh",
        "link": "https://openreview.net/forum?id=CN2bmVVpOh",
        "pdf_link": "https://openreview.net/pdf?id=CN2bmVVpOh",
        "keywords": "transformers; neural networks; working memory; computational neuroscience; gating; computational cognitive science; mechanistic interpretability",
        "abstract": "The Transformer neural network architecture has seen success on a wide variety of tasks that appear to require executive function - the ability to represent, coordinate, and manage multiple subtasks. In cognitive neuroscience, executive function is thought to rely on sophisticated frontostriatal mechanisms for selective gating, which enable role-addressable updating-- and later readout-- of information to and from distinct \"addresses\" of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so resemble the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer when trained on a task from computational cognitive neuroscience explicitly designed to place demands on working memory gating. We find that the self-attention mechanism within the Transformer develops input and output gating mechanisms, particularly when task demands require them. These gating mechanisms mirror those  incorporated into earlier biologically-inspired architectures and mimic those in human studies.  When learned effectively,  these gating strategies support enhanced generalization  and increase the models' effective capacity to store and access multiple items in memory. Despite not having memory limits, we also find that storing and accessing multiple items requires an efficient gating policy, resembling the constraints found in frontostriatal models.These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain."
    },
    {
        "title": "Building Luganda Machine Translation models for the   Medical Domain",
        "link_suffix": "/forum?id=g7DHM6MRE4",
        "link": "https://openreview.net/forum?id=g7DHM6MRE4",
        "pdf_link": "https://openreview.net/pdf?id=g7DHM6MRE4",
        "keywords": "machine translation, luganda, Low-resourced languages, medical machine translation",
        "abstract": "Globalization and migration have highlighted the critical need for effective cross-language communication, particularly in healthcare. In Uganda, a multilingual nation where Luganda is widely spoken, language barriers in predominantly English-speaking medical settings often lead to misunderstandings, misdiagnoses, and compromised patient care. This study aims to mitigate these issues by developing an  machine translation model built for medical communication, specifically targeting translations from English to Luganda within the context of malaria diagnosis and community engagement. Utilizing recent advancements in Artificial Intelligence and unsupervised learning, this research involves curating a parallel medical corpus, training a transformer-based model with domain-specific adapters, and rigorously evaluating the model's accuracy and cultural sensitivity.The results demonstrate that the MarianMT-Adapter LoRa model, when combined with active learning, achieved a significant improvement in translation quality, evidenced by a BLEU score increase to 56. This model effectively reduced translation errors and preserved the contextual integrity of medical texts. The findings are anticipated to enhance healthcare communication, reduce disparities, and improve access to medical knowledge for Luganda-speaking communities, providing a blueprint for similar efforts in other multilingual environments."
    },
    {
        "title": "Routoo: Learning to Route to Large Language Models Effectively",
        "link_suffix": "/forum?id=RQ9fQLEajC",
        "link": "https://openreview.net/forum?id=RQ9fQLEajC",
        "pdf_link": "https://openreview.net/pdf?id=RQ9fQLEajC",
        "keywords": "LLM, efficiency, knowledge integration, generation",
        "abstract": "LLMs with superior response quality—particularly larger or closed-source models—often come with higher inference costs, making their deployment inefficient and costly. Meanwhile, developing foundational LLMs from scratch is becoming increasingly resource-intensive and impractical for many applications. To address the challenge of balancing quality and cost, we introduce Routoo, an architecture designed to optimize the selection of LLMs for specific prompts based on performance, cost, and efficiency. Routoo provides controllability over the trade-off between inference cost and quality, enabling significant reductions in inference costs for a given quality requirement.\nRoutoo comprises two key components: a performance predictor and cost-aware selector. The performance predictor is a lightweight LLM that estimates the expected performance of various underlying LLMs on a given prompt without executing them. The cost-aware selector module then selects the most suitable model based on these predictions and constraints such as cost and latency, significantly reducing inference costs for the same quality. \nWe evaluated Routoo using the MMLU benchmark across 57 domains employing open-source models. Our results show that Routoo matches the performance of the Mixtral 8x7b model while reducing inference costs by one-third. Additionally, by allowing increased costs, Routoo surpasses Mixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of 75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's performance at half the cost and exceeds it with a 25% cost reduction. \nThese outcomes highlight Routoo's potential to significantly reduce inference costs without compromising quality, and even to establish new state-of-the-art results by leveraging the collective capabilities of multiple LLMs."
    },
    {
        "title": "A Comprehensive Framework for Benchmarking Algorithms Across Hyperparameter Spaces",
        "link_suffix": "/forum?id=FaL6aTuXod",
        "link": "https://openreview.net/forum?id=FaL6aTuXod",
        "pdf_link": "https://openreview.net/pdf?id=FaL6aTuXod",
        "keywords": "Benchmarking, Hyperparameter optimization",
        "abstract": "We introduce a framework for benchmarking algorithms with varying hyperparameters from multiple perspectives. The dependency of algorithms' performance on hyperparameters complicates fair comparisons and often leads to inconsistent empirical studies. Our framework addresses this challenge by proposing two key criteria: \\textit{Performance-HPO} trajectory and \\textit{Reliability-HPO}.\nThe Performance-HPO trajectory tracks how an algorithm’s performance changes with different hyperparameter optimization (HPO) budget allocations, leveraging a variety of off-the-shelf hyperparameter optimizers. This enables users to identify the most suitable algorithm for their specific needs. The Reliability-HPO criterion evaluates the expected value of an algorithm's success rate across hyperparameters, estimated using Monte Carlo simulations in log-space.\nWe demonstrate our framework by benchmarking widely-used convex optimizers. Our experiments, conducted with {\\footnotesize\\texttt{CVXPY}} across various problem types, settings, and dimensionalities, reveal that the {\\footnotesize\\texttt{SCS}} solver exhibits the highest Performance-HPO, while {\\footnotesize\\texttt{ECOS}} and {\\footnotesize\\texttt{MOSEK}} demonstrate superior Reliability-HPO."
    },
    {
        "title": "Logically Consistent Language Models via Neuro-Symbolic Integration",
        "link_suffix": "/forum?id=7PGluppo4k",
        "link": "https://openreview.net/forum?id=7PGluppo4k",
        "pdf_link": "https://openreview.net/pdf?id=7PGluppo4k",
        "keywords": "probabilistic reasoning, logical consistency, LLMs, neuro-symbolic, semantic loss",
        "abstract": "Large language models (LLMs) are a promising venue for natural language understanding and generation tasks. However, current LLMs are far from reliable: they are prone to generate non-factual information and, more crucially, to contradict themselves when prompted to reason about relations between real entities of the world. These problems are currently addressed with large scale fine-tuning or by delegating consistent reasoning to external tools. In this work, we strive for a middle ground and leverage a training objective based on a principled neuro-symbolic loss that teaches a LLM to be consistent with external knowledge in the form of a set of facts and rules. Fine-tuning with such a loss on a limited set of facts enables our LLMs to be more logically consistent than previous baselines for a given constraint. Our approach also allows to easily combine multiple logical constraints at once in a principled way, delivering LLMs that are more consistent w.r.t. all the selected rules. Moreover, our method allows LLMs to extrapolate to unseen but semantically similar factual knowledge, represented in unseen datasets, more systematically."
    },
    {
        "title": "Multigraph Message Passing with Bi-Directional Multi-Edge Aggregations",
        "link_suffix": "/forum?id=uYAG9Gla5u",
        "link": "https://openreview.net/forum?id=uYAG9Gla5u",
        "pdf_link": "https://openreview.net/pdf?id=uYAG9Gla5u",
        "keywords": "graph neural networks, multigraph, message passing, financial fraud detection",
        "abstract": "Graph Neural Networks (GNNs) have seen significant advances in recent years, yet their application to multigraphs, where parallel edges exist between the same pair of nodes, remains under-explored. Standard GNNs, designed for simple graphs, compute node representations by combining all connected edges at once, without distinguishing between edges from different neighbors. There are some GNN architectures proposed specifically for multigraph tasks, yet these architectures perform only node-level aggregation in their message-passing layers, which limits their expressive power. Furthermore, these approaches either lack permutation equivariance when a natural edge ordering is absent, or fail to preserve the topological structure of the multigraph. To address all these shortcomings, we propose MEGA-GNN, a unified framework for message passing on multigraphs that can effectively perform diverse graph learning tasks. Our approach introduces a two-stage aggregation process in the message passing layers: first, parallel edges are aggregated, followed by a node-level aggregation that operates on aggregated messages from distinct neighbors. We show that MEGA-GNN supports permutation equivariance and invariance properties. We also show that MEGA-GNN is universal when the edges are consistently ordered. Experiments on synthetic and real-world financial transaction datasets demonstrate that MEGA-GNN either significantly outperforms or is on par with the accuracy of state-of-the-art solutions."
    },
    {
        "title": "Enhanced Long LoRA Inspired Perceiver Architectures for Auto-Regressive Language Modeling",
        "link_suffix": "/forum?id=VjHOGqHC4I",
        "link": "https://openreview.net/forum?id=VjHOGqHC4I",
        "pdf_link": "https://openreview.net/pdf?id=VjHOGqHC4I",
        "keywords": "Large Language Models, Perceiver, Efficient Machine Learning",
        "abstract": "The Transformer architecture has revolutionized the Natural Language Processing field and is the backbone of Large Language Models (LLMs). The Transformer uses the attention mechanism that computes the pair-wise similarity between its input tokens to produce latent vectors that are able to understand the semantic meaning of the input text. One of the challenges in the Transformer architecture is the quadratic complexity of the attention mechanism that prohibits the efficient processing of long sequence lengths. While many recent research works have attempted to provide a reduction from $O(n^2)$ time complexity of attention to semi-linear complexity, it remains an unsolved problem in the sense of maintaining a high performance when such complexity is reduced. One of the important works in this respect is the Perceiver class of architectures that have demonstrated excellent performance while reducing the computation complexity. In this paper, we use the PerceiverAR that was proposed for Auto-Regressive modeling as a baseline, and provide three different architectural enhancements to it with varying computation overhead tradeoffs. Inspired by the recently proposed efficient attention computation approach of Long-LoRA, we then present an equally efficient Perceiver-based architecture (termed as Long LoRA Pereceiver - LLP) that can be used as the base architecture in LLMs instead of just a fine-tuning add-on. Our results on different benchmarks indicate much improved performance over the baseline PerceiverAR model, with the LLP showing impressive improvements compared to recent Transformer based models."
    },
    {
        "title": "When SNN meets ANN: Error-Free ANN-to-SNN Conversion for Extreme Edge Efficiency",
        "link_suffix": "/forum?id=GTzP2GC7NR",
        "link": "https://openreview.net/forum?id=GTzP2GC7NR",
        "pdf_link": "https://openreview.net/pdf?id=GTzP2GC7NR",
        "keywords": "SNN, ANN-to-SNN conversion, IF model, ImageNet, spiking activity",
        "abstract": "Spiking Neural Networks (SNN) are now demonstrating comparable accuracy to convolutional neural networks (CNN), thanks to advanced ANN-to-SNN conversion techniques, all while delivering remarkable energy and latency efficiency when deployed on neuromorphic hardware. However, these conversion techniques incur a large number of time steps, and consequently, high spiking activity. In this paper, we propose a novel ANN-to-SNN conversion framework, that incurs an exponentially lower number of time steps compared to that required in the existing conversion approaches. Our framework modifies the standard integrate-and-fire (IF) neuron model used in SNNs with no change in computational complexity and shifts the bias term of each batch normalization (BN) layer in the trained ANN. To reduce spiking activity, we propose training the source ANN with a fine-grained $\\ell_1$ regularizer with surrogate gradients that encourages high spike sparsity in the converted SNN. Our proposed framework thus yields lossless SNNs with low latency, low compute energy, thanks to the low time steps and high spike sparsity, and high test accuracy, for example, 75.12% with only 4 time steps on the ImageNet dataset. Codes will be made available."
    },
    {
        "title": "Stateful Dynamics for Training of Binary Activation Recurrent Networks",
        "link_suffix": "/forum?id=A6K4aqReoF",
        "link": "https://openreview.net/forum?id=A6K4aqReoF",
        "pdf_link": "https://openreview.net/pdf?id=A6K4aqReoF",
        "keywords": "recurrent network, quantization, spiking neural network, dynamical systems",
        "abstract": "In recent years, there has been an increased interest in heavily quantized neural networks, which utilize a mixture of numerical precision for various layers in the network. Training such networks requires modifications or alternatives to standard backpropagation, typically in the form of surrogate gradient descent. While multiple methods exist, they have primarily been utilized in feedforward networks, neglecting application to domains which require recurrent layers. Here, we show that surrogate gradient methods are unstable when applied to standard binary activation layers, and fail to converge when applied to temporal datasets.We then incorporate the pre-activation integrative state from spiking neural network approaches into these training methods and show that the incorporation of this state can enable binary activation networks to reach similar performance as floating-point networks. We show that other aspects of spiking networks, namely explicit reset mechanisms and leakage terms, do not contribute additional performance. These results show how incorporating local dynamic states can allow training of recurrent binary networks to maintain low-resolution communication."
    },
    {
        "title": "Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability",
        "link_suffix": "/forum?id=INqLJwqUmc",
        "link": "https://openreview.net/forum?id=INqLJwqUmc",
        "pdf_link": "https://openreview.net/pdf?id=INqLJwqUmc",
        "keywords": "Interpretability, CLIP",
        "abstract": "The task of identifying multimodal image-text representations has recently received increasing attention, particularly with models like CLIP (Contrastive Language-Image Pretraining), which excel at learning complex associations between images and text. Despite these advancements, ensuring the interpretability of such models remains crucial for their safe application in real-world scenarios, such as healthcare. Numerous interpretability methods have been developed for unimodal tasks; however, these approaches often fail to transfer effectively to multimodal tasks due to fundamental differences in representation. Bottleneck methods, widely recognized in information theory, have been applied to enhance CLIP’s interpretability, but they often suffer from strong assumptions or inherent randomness. To address these limitations, we introduce the Narrowing Information Bottleneck  Theory, a novel approach that re-engineers the bottleneck method from the ground up. This theory is designed to satisfy modern attribution axioms, offering a robust solution for improving the interpretability of multimodal models. In our experiments, compared to state-of-the-art methods, our approach improves image interpretability by an average of 9%, text interpretability by an average of 58.83%, and increases processing speed by 63.95%. Our code is publicly available at:https://anonymous.4open.science/r/NIB-DBCD/."
    },
    {
        "title": "Massively Multi-Agents Reveal That Large Language Models Can Understand Value",
        "link_suffix": "/forum?id=obYDlJN0oU",
        "link": "https://openreview.net/forum?id=obYDlJN0oU",
        "pdf_link": "https://openreview.net/pdf?id=obYDlJN0oU",
        "keywords": "computational finance, stock prediction, large language models, economics",
        "abstract": "Large Language Models (LLMs) have been trained over a large corpus of data, allowing them to learn internal representations of how humans would respond in different scenarios. This makes them well-suited for doing stock prediction, as stock prices are typically determined by human market participants reasoning on information that impacts the value of the stock. On the other hand, stock\nvaluation also requires the ability to understand numerical values, which LLMs are not known to be good at. In this work, we seek to study this limitation and use LLMs for the task of stock valuation. We introduce an agent-based simulation method MMARP (Massively Multi-Agents Role Playing), which simulates the responses for a large number of LLM agents over a range of numerical price values. Through experimental results, we find that while LLMs do not have a precise understanding of numerical stock price values individually, it is possible to surface reasonable stock price forecasts by tapping on their collective understanding of human behavior. By analyzing the interaction between the responses of LLM agents, we find that MMARP is able to obtain better forecasts than direct prompting of both general and financial LLMs, and deep-learning models for stock prediction."
    },
    {
        "title": "Efficient Source-Free Time-Series Adaptation via Parameter Subspace Disentanglement",
        "link_suffix": "/forum?id=Q5Sawm0nqo",
        "link": "https://openreview.net/forum?id=Q5Sawm0nqo",
        "pdf_link": "https://openreview.net/pdf?id=Q5Sawm0nqo",
        "keywords": "Time Series, Source-Free Domain Adaptation, Efficiency",
        "abstract": "In this paper, we propose a framework for efficient Source-Free Domain Adaptation (SFDA) in the context of time-series, focusing on enhancing both parameter efficiency and data-sample utilization. Our approach introduces an improved paradigm for source-model preparation and target-side adaptation, aiming to enhance training efficiency during target adaptation. Specifically, we reparameterize the source model's weights in a Tucker-style decomposed manner, factorizing the model into a compact form during the source model preparation phase. During target-side adaptation, only a subset of these decomposed factors is fine-tuned, leading to significant improvements in training efficiency. We demonstrate using PAC Bayesian analysis that this selective fine-tuning strategy implicitly regularizes the adaptation process by constraining the model's learning capacity. Furthermore, this re-parameterization reduces the overall model size and enhances inference efficiency, making the approach particularly well suited for resource-constrained devices. Additionally, we demonstrate that our framework is compatible with various SFDA methods and achieves significant computational efficiency, reducing the number of fine-tuned parameters and inference overhead in terms of MACs by over 90% while maintaining model performance."
    },
    {
        "title": "Online Reinforcement Learning in Non-Stationary Context-Driven Environments",
        "link_suffix": "/forum?id=l6QnSQizmN",
        "link": "https://openreview.net/forum?id=l6QnSQizmN",
        "pdf_link": "https://openreview.net/pdf?id=l6QnSQizmN",
        "keywords": "catastrophic forgetting, reinforcement learning, context-driven MDP, online learning, non-stationary",
        "abstract": "We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to ``catastrophic forgetting'' (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice), employ brittle regularization heuristics or use off-policy methods that suffer from instability and poor performance.We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it outperforms a variety of baselines in the non-stationary setting, while achieving results on-par with a ``prescient'' agent trained offline across all context traces."
    },
    {
        "title": "CaLMFlow: Volterra Flow Matching using Causal Language Models",
        "link_suffix": "/forum?id=Xw86qj6FV5",
        "link": "https://openreview.net/forum?id=Xw86qj6FV5",
        "pdf_link": "https://openreview.net/pdf?id=Xw86qj6FV5",
        "keywords": "Flow matching, operator learning, large language models, single-cell transcriptomics, causal language models, integral equations",
        "abstract": "We introduce CaLMFlow (Causal Language Models for Flow Matching), a novel framework that casts flow matching as a Volterra integral equation (VIE), leveraging the power of large language models (LLMs) for continuous data generation. CaLMFlow enables the direct application of LLMs to learn complex flows by formulating flow matching as a sequence modeling task, bridging discrete language modeling and continuous generative modeling. Our method implements tokenization across space and time, thereby solving a VIE over these domains. This approach enables efficient handling of high-dimensional data and outperforms ODE solver-dependent methods like conditional flow matching (CFM). We demonstrate CaLMFlow's effectiveness on synthetic and real-world data, including single-cell perturbation response prediction, showcasing its ability to incorporate textual context and generalize to unseen conditions. Our results highlight LLM-driven flow matching as a promising paradigm in generative modeling, offering improved scalability, flexibility, and context-awareness."
    }
]