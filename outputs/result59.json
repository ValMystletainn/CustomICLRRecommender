[{"title": "Make Haste Slowly: A Theory of Emergent Structured Mixed Selectivity in Feature Learning ReLU Networks", "link_suffix": "/forum?id=27SSnLl85x", "link": "https://openreview.net/forum?id=27SSnLl85x", "pdf_link": "https://openreview.net/pdf?id=27SSnLl85x", "keywords": "Gated Deep Linear Networks, Feature Learning Dynamics, Structured Mixed Selectivity, ReLU Networks", "abstract": "In spite of finite dimension ReLU neural networks being a consistent factor behind recent deep learning successes, a theory of feature learning in these models remains elusive. Currently, insightful theories still rely on assumptions including the linearity of the network computations, unstructured input data and architectural constraints such as infinite width or a single hidden layer. To begin to address this gap we establish an equivalence between ReLU networks and Gated Deep Linear Networks, and use their greater tractability to derive dynamics of learning. We then consider multiple variants of a core task reminiscent of multi-task learning or contextual control which requires both feature learning and nonlinearity. We make explicit that, for these tasks, the ReLU networks possess an inductive bias towards latent representations which arenotstrictly modular or disentangled but are still highly structured and reusable between contexts. This effect is amplified with the addition of more contexts and hidden layers. Thus, we take a step towards a theory of feature learning in finite ReLU networks and shed light on how structured mixed-selective latent representations can emerge due to a bias for node-reuse and learning speed.", "title_embedding_index": 2900, "title_abs_embedding_index": 2925}, {"title": "Linear Relational Decoding of Morphological Relations in Language Models", "link_suffix": "/forum?id=L6xxFCafqY", "link": "https://openreview.net/forum?id=L6xxFCafqY", "pdf_link": "https://openreview.net/pdf?id=L6xxFCafqY", "keywords": "LMs, Model Interpretability, Representation Theory, Morphology, Linear Representation Hypothesis, Analogical Reasoning", "abstract": "The recent success of transformer language models owes much to their conversational fluency and productivity in linguistic and morphological aspects. An affine Taylor approximation has been found to be a good approximation for transformer computations over certain factual and encyclopedic relations. We show that the truly linear approximation $W\\textbf{s}$, where $\\textbf{s}$ is a middle layer representation of the base form and $W$ is a local model derivative, is necessary and sufficient to approximate morphological derivations. This approach achieves above 80% faithfulness across most morphological tasks in the Bigger Analogy Test Set. We argue that morphological forms in transformer models are likely to be encoded by linear transformations, with implications for how entities are represented in language models.", "title_embedding_index": 2901, "title_abs_embedding_index": 2926}, {"title": "PREDICTING THE BEHAVIOR OF AI AGENTS USING TRANSFER OPERATORS", "link_suffix": "/forum?id=BegT6Y00Rm", "link": "https://openreview.net/forum?id=BegT6Y00Rm", "pdf_link": "https://openreview.net/pdf?id=BegT6Y00Rm", "keywords": "stochastic differential equations, markov process, operator theory", "abstract": "Predicting the behavior of AI-driven agents is particularly challenging without a preexisting model. In our paper, we address this by treating AI agents as stochastic nonlinear dynamical systems and adopting a probabilistic perspective to predict their statistical behavior using the Fokker-Planck equation. We formulate the approximation of the density transfer operator as an entropy minimization problem, which can be solved by leveraging the Markovian property and decomposing its spectrum. Our data-driven methodology simultaneously approximates the Markov operator to perform prediction of the evolution of the agents and also predicts the terminal probability density of AI agents, such as robotic systems and generative models. We demonstrate the effectiveness of our prediction model through extensive experiments on practical systems driven by AI algorithms.", "title_embedding_index": 2902, "title_abs_embedding_index": 2927}, {"title": "Eliciting Human Preferences with Language Models", "link_suffix": "/forum?id=LvDwwAgMEW", "link": "https://openreview.net/forum?id=LvDwwAgMEW", "pdf_link": "https://openreview.net/pdf?id=LvDwwAgMEW", "keywords": "question asking, preference elicitation, language models, evaluation, human studies", "abstract": "Language models (LMs) can be directed to perform user- and context-dependent\ntasks by using labeled examples or natural language prompts.\nBut selecting examples or writing prompts can be challenging---especially in tasks that require users to precisely articulate nebulous preferences or reason about complex edge cases. For such tasks, we introduceGenerative Active Task Elicitation (GATE), a method for usingLMs themselvesto guide the task specification process. GATE is a learning framework in which models elicit and infer human preferences through free-form, language-based interaction with users.\nWe identify prototypical challenges that users face when specifying preferences, and design three preference modeling tasks to study these challenges:\ncontent recommendation, moral reasoning, and email validation.\nIn preregistered experiments, we show that LMs that learn to perform these tasks using GATE (by interactively querying users with open-ended questions) obtain preference specifications that are more informative than user-written prompts or examples. GATE matches existing task specification methods in the moral reasoning task, and significantly outperforms them in the content recommendation and email validation tasks. Users additionally report that interactive task elicitation requires less effort than prompting or example labeling and surfaces considerations that they did not anticipate on their own. Our findings suggest that LM-driven elicitation can be a powerful tool for aligning models to complex human preferences and values.", "title_embedding_index": 2903, "title_abs_embedding_index": 2928}, {"title": "SymDiff: Equivariant Diffusion via Stochastic Symmetrisation", "link_suffix": "/forum?id=i1NNCrRxdM", "link": "https://openreview.net/forum?id=i1NNCrRxdM", "pdf_link": "https://openreview.net/pdf?id=i1NNCrRxdM", "keywords": "Equivariance, Diffusion Models, Symmetrisation, Molecular Generation, Markov Categories", "abstract": "We propose SYMDIFF, a novel method for constructing equivariant diffusion\nmodels using the recently introduced framework of stochastic symmetrisation.\nSYMDIFF resembles a learned data augmentation that is deployed at sampling\ntime, and is lightweight, computationally efficient, and easy to implement on\ntop of arbitrary off-the-shelf models. Notably, in contrast to previous work,\nSYMDIFF typically does not require any neural network components that are\nintrinsically equivariant, avoiding the need for complex parameterizations and the\nuse of higher-order geometric features. Instead, our method can leverage highly\nscalable modern architectures as drop-in replacements for these more constrained\nalternatives. We show that this additional flexibility yields significant empirical\nbenefit on E(3)-equivariant molecular generation. To the best of our knowledge,\nthis is the first application of symmetrisation to generative modelling, suggesting\nits potential in this domain more generally.", "title_embedding_index": 2904, "title_abs_embedding_index": 2929}, {"title": "GUD: Generation with Unified Diffusion", "link_suffix": "/forum?id=zn0eqMtsrw", "link": "https://openreview.net/forum?id=zn0eqMtsrw", "pdf_link": "https://openreview.net/pdf?id=zn0eqMtsrw", "keywords": "diffusion models, renormalization group, autoregressive models, wavelet decomposition, denoising score matching", "abstract": "Diffusion generative models transform noise into data by inverting a process that progressively adds noise to data samples. Inspired by concepts from the renormalization group in physics, which analyzes systems across different scales, we revisit diffusion models by exploring three key design aspects: 1) the choice of representation in which the diffusion process operates (e.g. pixel-, PCA-, Fourier-, or wavelet-basis), 2) the prior distribution that data is transformed into during diffusion (e.g. Gaussian with covariance $\\Sigma$), and 3) the scheduling of noise levels applied separately to different parts of the data, captured by a component-wise noise schedule. \n Incorporating the flexibility in these choices, we develop a unified framework for diffusion generative models with greatly enhanced design freedom. In particular, we introduce soft-conditioning models that smoothly interpolate between standard diffusion models and autoregressive models (in any basis), conceptually bridging these two approaches. \nOur framework opens up a wide design space which may lead to more efficient training and data generation, and paves the way to novel architectures integrating different generative approaches and generation tasks.", "title_embedding_index": 2905, "title_abs_embedding_index": 2930}, {"title": "On the Role of Depth and Looping for In-Context Learning with Task Diversity", "link_suffix": "/forum?id=lYongcxaNz", "link": "https://openreview.net/forum?id=lYongcxaNz", "pdf_link": "https://openreview.net/pdf?id=lYongcxaNz", "keywords": "transformers, attention, looped transformers, task diversity, in-context learning, in-context linear regression, out-of-distribution generalization", "abstract": "The intriguing in-context learning (ICL) abilities of \\emph{deep Transformer models} have lately garnered significant attention. By studying in-context linear regression on unimodal Gaussian data, recent empirical and theoretical works have argued that ICL emerges from Transformers' abilities to simulate learning algorithms like gradient descent. However, these works fail to capture the remarkable ability of Transformers to learn \\emph{multiple tasks} in context.\nTo this end, we study in-context learning for linear regression with diverse tasks, characterized by data covariance matrices with condition numbers ranging from $[1, \\kappa]$, and highlight the importance of depth in this setting. More specifically, (1) (1) theoretical lower bounds of $\\log(\\kappa)$ (or $\\sqrt{\\kappa}$) linear attention layers in the unrestricted (or restricted) attention and (2) we show that the class of {\\em multilayer Transformers} can indeed solve such tasks with a number of layers that matches the lower bounds. Furthermore, we show that this expressivity of multilayer Transformer comes at the price of robustness; in particular, multilayer Transformers are not robust to even distributional shifts as small as $O(e^{-L})$ in Wasserstein distance, where $L$ is the depth of the network. We then demonstrate that Looped Transformers ---a special class of multilayer Transformers with weight-sharing--- not only exhibit similar expressive power but are also provably robust under mild assumptions. Besides out-of-distribution generalization, we also show that Looped transformers are the only models that exhibit a monotonic behavior of loss with respect to depth (or number of loops).", "title_embedding_index": 2906, "title_abs_embedding_index": 2931}, {"title": "AdaWM: Adaptive World Model based Planning for Autonomous Driving", "link_suffix": "/forum?id=NEu8wgPctU", "link": "https://openreview.net/forum?id=NEu8wgPctU", "pdf_link": "https://openreview.net/pdf?id=NEu8wgPctU", "keywords": "World Model, Autonomous Driving, Reinforcement Learning", "abstract": "World model based reinforcement learning (RL) has emerged as a promising approach for autonomous driving, which learns a latent dynamics model and uses it to train a   planning policy. To speed up the learning process, the pretrain-finetune paradigm is often used, where online RL is initialized by a pretrained model and a policy learned offline. However, naively performing such initialization in RL may result in dramatic performance degradation during the online interactions in the new task. To tackle this challenge, we first analyze the  performance degradation and identify two primary root causes therein: the mismatch of the planning policy and the mismatch of the dynamics model,  due to distribution shift. We further analyze the effects of these factors  on performance degradation during finetuning, and our findings reveal that the choice of finetuning strategies plays a pivotal role in mitigating these effects. We then introduce AdaWM, an Adaptive World Model based planning method, featuring two key steps: (a) mismatch identification, which quantifies the mismatches and informs the finetuning strategy, and (b) alignment-driven finetuning, which selectively updates either the policy or the model as needed  using efficient low-rank updates. Extensive experiments  on the challenging CARLA driving tasks demonstrate that AdaWM significantly improves the finetuning process, resulting in more robust and efficient performance in autonomous driving systems.", "title_embedding_index": 2907, "title_abs_embedding_index": 2932}, {"title": "GraphRCG: Self-Conditioned Graph Generation", "link_suffix": "/forum?id=Mw42TqZ0o5", "link": "https://openreview.net/forum?id=Mw42TqZ0o5", "pdf_link": "https://openreview.net/pdf?id=Mw42TqZ0o5", "keywords": "Graph Generation, Diffusion Models", "abstract": "Graph generation aims to create new graphs that closely align with a target graph distribution. Existing works often implicitly capture this distribution by aligning the output of a generator with each training sample. As such, the overview of the entire distribution is not explicitly captured and used for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance for the generation process, thereby facilitating the generation of graphs that more accurately reflect the learned distributions. We conduct extensive experiments on generic and molecular graph datasets. Our framework demonstrates superior performance over existing state-of-the-art graph generation methods in terms of graph quality and fidelity to training data.", "title_embedding_index": 2908, "title_abs_embedding_index": 2933}, {"title": "A Meta-Learning Approach to Bayesian Causal Discovery", "link_suffix": "/forum?id=eeJz7eDWKO", "link": "https://openreview.net/forum?id=eeJz7eDWKO", "pdf_link": "https://openreview.net/pdf?id=eeJz7eDWKO", "keywords": "neural processes, bayesian causal discovery, transformers", "abstract": "Discovering a unique causal structure is difficult due to both inherent identifiability issues, and the consequences of finite data.\nAs such, uncertainty over causal structures, such as those obtained from a Bayesian posterior, are often necessary for downstream tasks.\nFinding an accurate approximation to this posterior is challenging, due to the large number of possible causal graphs, as well as the difficulty in the subproblem of finding posteriors over the functional relationships of the causal edges.\nRecent works have used Bayesian meta learning to view the problem of posterior estimation as a supervised learning task.\nYet, these methods are limited as they cannot reliably sample from the posterior over causal structures and fail to encode key properties of the posterior, such as correlation between edges and permutation equivariance with respect to nodes.\nTo address these limitations, we propose a Bayesian meta learning model that allows for sampling causal structures from the posterior and encodes these key properties.\nWe compare our meta-Bayesian causal discovery against existing Bayesian causal discovery methods, demonstrating the advantages of directly learning a posterior over causal structure.", "title_embedding_index": 2909, "title_abs_embedding_index": 2934}, {"title": "Inference Scaling for Long-Context Retrieval Augmented Generation", "link_suffix": "/forum?id=FSjIrOm1vz", "link": "https://openreview.net/forum?id=FSjIrOm1vz", "pdf_link": "https://openreview.net/pdf?id=FSjIrOm1vz", "keywords": "inference scaling, long-context LLM, retrieval augmented generation", "abstract": "The scaling of inference computation has unlocked the potential of long-contextlarge language models (LLMs) across diverse settings. For knowledge-intensivetasks, the increased compute is often allocated to incorporate more external knowl-edge.  However, without effectively utilizing such knowledge, solely expandingcontext does not always enhance performance. In this work, we investigate infer-ence scaling for retrieval augmented generation (RAG), exploring strategies beyondsimply increasing the quantity of knowledge. We focus on two inference scalingstrategies: in-context learning and iterative prompting. These strategies provideadditional flexibility to scale test-time computation (e.g., by increasing retrieveddocuments or generation steps), thereby enhancing LLMs\u2019 ability to effectivelyacquire and utilize contextual information. We address two key questions: (1) Howdoes RAG performance benefit from thescaling of inference computationwhenoptimally configured? (2) Can we predict the optimaltest-time compute allocationfor a given budget by modeling the relationship between RAG performance andinference parameters? Our observations reveal that increasing inference computa-tion leads to nearly linear gains in RAG performance when optimally allocated, arelationship we describe as theinference scaling laws for RAG. Building on this,we further develop thecomputation allocation modelto estimate RAG performanceacross different inference configurations.  The model predicts optimal inferenceparameters under various computation constraints, which align closely with theexperimental results. By applying these optimal configurations, we demonstratethat scaling inference compute on long-context LLMs achieves up to 58.9% gainson benchmark datasets compared to standard RAG.", "title_embedding_index": 2910, "title_abs_embedding_index": 2935}, {"title": "The Geometry of Tokens in Internal Representations of Large Language Models", "link_suffix": "/forum?id=an3jH2qD2r", "link": "https://openreview.net/forum?id=an3jH2qD2r", "pdf_link": "https://openreview.net/pdf?id=an3jH2qD2r", "keywords": "Intrinsic Dimension, Neighborhood Overlap, Internal Representations, Large Language Models", "abstract": "Understanding the inner workings and decision-making processes of large language models (LLMs) requires thorough investigation and interpretation of these models. Toward this goal, previous studies have utilized metrics such as intrinsic dimension and neighborhood overlap to probe the geometry of internal representations, where prompts are summarized as a single point in representation space. We expand single points to point clouds by investigating how models geometrically distribute tokens in their internal representations. We measure the intrinsic dimension, neighborhood overlap, and cosine similarity on these point clouds for a large number of prompts. To validate our approach, we compare these metrics to a dataset in which tokens are shuffled, where the model is forced to operate out of distribution. Our analysis consistently distinguishes the structured and shuffled cases across all three metrics\u2014intrinsic dimension, neighborhood overlap, and cosine similarity- showing explicitly how the model operates in the two cases.", "title_embedding_index": 2911, "title_abs_embedding_index": 2936}, {"title": "Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding", "link_suffix": "/forum?id=Tv36j85SqR", "link": "https://openreview.net/forum?id=Tv36j85SqR", "pdf_link": "https://openreview.net/pdf?id=Tv36j85SqR", "keywords": "Neural compression, vector quantization, lattice quantization, nonlinear transform coding", "abstract": "Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal on a few specific sources, we show that it can be highly sub-optimal on synthetic sources whose intrinsic dimensionality is greater than one. With integer rounding in the latent space, the quantization regions induced by neural transformations, remain square-like and fail to match those of optimal vector quantization. We demonstrate that this phenomenon is due to the choice of scalar quantization in the latent space, and not the transform design. By employing lattice quantization instead, we propose  Lattice Transform Coding (LTC) and show that it approximately recovers optimal vector quantization at reasonable complexity. On real-world sources, LTC improves upon standard neural compressors. LTC also provides a framework that can integrate structurally (near) optimal information-theoretic designs into lossy compression; examples include block coding, which yields coding gain over optimal one-shot coding and approaches the asymptotically-achievable rate-distortion function, as well as nested lattice quantization for low complexity fixed-rate coding.", "title_embedding_index": 2912, "title_abs_embedding_index": 2937}, {"title": "Is Your Video Language Model a Reliable Judge?", "link_suffix": "/forum?id=m8yby1JfbU", "link": "https://openreview.net/forum?id=m8yby1JfbU", "pdf_link": "https://openreview.net/pdf?id=m8yby1JfbU", "keywords": "Video Language Models, Model evaluation, Collective thought, Reliability", "abstract": "Evaluating Video Language Models (VLMs) is crucial for advancing their capabilities in understanding complex video content. Traditional evaluation methods often rely on single models, which can be biased due to their training data and architecture or susceptible to hallucinations, thereby undermining their reliability. A straightforward approach is to apply the principle of collective intelligence, aggregating judgments from multiple VLMs to enhance evaluation reliability. This study investigates the efficacy of collective thought approaches in VLM evaluation, particularly when the pool of judges includes both reliable and unreliable models. Contrary to expectations, our findings reveal that incorporating collective judgments from a mix of reliable and unreliable VLMs does not necessarily enhance the accuracy of the final evaluation outcomes. The inclusion of less reliable models introduces noise and biases that can overshadow the potential benefits of aggregation, leading to evaluations that may be less accurate than those from individual reliable models. These findings emphasize the limitations of collective intelligence approaches in VLM evaluation and highlight the need for more advanced methods that can accurately account for the reliability of individual models. Our research contributes to the development of more robust evaluation frameworks for VLMs, promoting accuracy and fairness in evaluation of VLMs.", "title_embedding_index": 2913, "title_abs_embedding_index": 2938}, {"title": "Boosting Multiple Views for pretrained-based Continual Learning", "link_suffix": "/forum?id=AZR4R3lw7y", "link": "https://openreview.net/forum?id=AZR4R3lw7y", "pdf_link": "https://openreview.net/pdf?id=AZR4R3lw7y", "keywords": "continual learning, ViT-pretrained continual learning", "abstract": "Recent research has shown that Random Projection (RP) can effectively improve\nthe performance of pre-trained models in Continual learning (CL). The authors\nhypothesized that using RP to map features onto a higher-dimensional space can\nmake them more linearly separable. In this work, we theoretically analyze the\nrole of RP and present its benefits for improving the model\u2019s generalization ability\nin each task and facilitating CL overall. Additionally, we take this result to the\nnext level by proposing a Multi-View Random Projection scheme for a stronger\nensemble classifier. In particular, we train a set of linear experts, among which\ndiversity is encouraged based on the principle of AdaBoost, which was initially very\nchallenging to apply to CL. Moreover, we employ a task-based adaptive backbone\nwith distinct prompts dedicated to each task for better representation learning. To\nproperly select these task-specific components and mitigate potential feature shifts\ncaused by misprediction, we introduce a simple yet effective technique called the\nself-improvement process. Experimentally, our method consistently outperforms\nstate-of-the-art baselines across a wide range of datasets.", "title_embedding_index": 2914, "title_abs_embedding_index": 2939}, {"title": "Explain Like I'm Five: Using LLMs to Improve PDE Surrogate Models with Text", "link_suffix": "/forum?id=D3iJmVAmT7", "link": "https://openreview.net/forum?id=D3iJmVAmT7", "pdf_link": "https://openreview.net/pdf?id=D3iJmVAmT7", "keywords": "Multimodal, Partial Differential Equation, Neural Operator", "abstract": "Solving Partial Differential Equations (PDEs) is ubiquitous in science and engineering. Computational complexity and difficulty in writing numerical solvers has motivated the development of machine learning techniques to generate solutions quickly. Many existing methods are purely data driven, relying solely on numerical solution fields, rather than known system information such as boundary conditions and governing equations. However, the recent rise in popularity of Large Language Models (LLMs) has enabled easy integration of text in multimodal machine learning models. In this work, we use pretrained LLMs to integrate various amounts known system information into PDE learning. Our multimodal approach significantly outperforms our baseline model, FactFormer, in both next-step prediction and autoregressive rollout performance on the 2D Heat, Burgers, Navier-Stokes, and Shallow Water equations. Further analysis shows that pretrained LLMs provide highly structured latent space that is consistent with the amount of system information provided through text.", "title_embedding_index": 2915, "title_abs_embedding_index": 2940}, {"title": "Towards Effective Discrimination Testing for Generative AI", "link_suffix": "/forum?id=JrfWj5Ae1j", "link": "https://openreview.net/forum?id=JrfWj5Ae1j", "pdf_link": "https://openreview.net/pdf?id=JrfWj5Ae1j", "keywords": "Generative AI, LLMs, fairness, discrimination, policy, diffusion", "abstract": "Generative AI (GenAI) models present new challenges in testing for, and regulating against, discriminatory behavior. In this paper, we argue that GenAI fairness research still has not met these challenges: there is a dearth of reliable bias assessment methods for GenAI systems that speak to regulatory goals. This leads to ineffective regulation that can allow deployment of reportedly fair, yet actually discriminatory GenAI systems. Towards remedying this problem, we connect the legal and technical literature around GenAI bias evaluation and identify areas of misalignment. Through four case studies, we demonstrate how this misalignment between fairness testing techniques and regulatory goals can result in discriminatory outcomes in real-world deployments, especially in adaptive or complex environments. We offer practical recommendations for improving discrimination testing to better align with regulatory goals and enhance the reliability of fairness assessments in future deployments.", "title_embedding_index": 2916, "title_abs_embedding_index": 2941}, {"title": "Evaluating Information Gathering Abilities of Large Language Models with QuestBench", "link_suffix": "/forum?id=BwGeIhGPgn", "link": "https://openreview.net/forum?id=BwGeIhGPgn", "pdf_link": "https://openreview.net/pdf?id=BwGeIhGPgn", "keywords": "information gathering, question asking, language model, evaluation, benchmarks", "abstract": "Human queries and instructions to large language models (LLMs) often containincompleteorunderspecifiedinformation.\nIn these circumstances, the ability to acquire the missing information by asking clarifying questions is crucial; in particular, doing so in a way to obtainminimally sufficientpiece of information.\nTo assess whether LLMs possess this ability, we construct QuestBench, a benchmark of underspecified tasks that can be resolved by asking at most a single question.\nWe frame underspecified tasks as a constraint satisfaction problems with missing variable assignments, where the exact model response cannot be determined unless certain variables\u2019 values are acquired. This framework allows us to more precisely focus on tasks where uncertainty arises due to missing information, in contrast to tasks where it arises due to semantic ambiguity. The benchmarks include (1) Logic-Q: Logical reasoning tasks where one proposition is missing, (2)Planning-Q: PDDL planning problems where the initial state is underspecified, and (3) GSM-Q: Grade school math problems where one variable assignment is missing. We evaluate Gemini and GPT-4o models and find that they achieve 20 \u201330% accuracy in both zero-shot and few-shot settings. Furthermore, when evaluating GPT-4-o1 on a subset of our data, we find that it is only 41 \u2013 44% accurate, despite utilizing state-of-the-art inference-time reasoning techniques. Overall, our results show that there is significant room for improvement on information gathering tasks.  We conduct preliminary analysis to study factors that may correlate with reasoning mechanisms LLMs may use to tackle QuestBench.", "title_embedding_index": 2917, "title_abs_embedding_index": 2942}, {"title": "Adaptively Private Next-Token Prediction of Large Language Models", "link_suffix": "/forum?id=fGSEWgRHNZ", "link": "https://openreview.net/forum?id=fGSEWgRHNZ", "pdf_link": "https://openreview.net/pdf?id=fGSEWgRHNZ", "keywords": "Differential Privacy, Language Models", "abstract": "As Large Language Models (LLMs) proliferate, developing privacy safeguards for these models is crucial. One popular safeguard involves training LLMs in a differentially private manner. However, such solutions are shown to be computationally expensive and detrimental to the utility of these models. Since LLMs are deployed on the cloud and thus only accessible via an API, a Machine Learning as a Service (MLaaS) provider can protect its downstream data by privatizing the predictions during the decoding process. However, the practicality of such solutions still largely lags behind DP training methods. One recent promising approach, Private Mixing of Ensemble Distributions (PMixED), avoids additive noise by sampling from the output distributions of private LLMs mixed with the output distribution of a public model. Yet, PMixED must satisfy a fixed privacy level for a given number of queries, which is difficult for an analyst to estimate before inference and, hence, does not scale. To this end, we relax the requirements to a more practical setting by introducing Adaptive PMixED ($\\texttt{AdaPMixED}$), a private decoding framework based on PMixED that is adaptive to the private and public output distributions evaluated on a given input query. In this setting, we introduce a noisy screening mechanism that filters out queries with potentially expensive privacy loss, and a data-dependent analysis that exploits the divergence of the private and public output distributions in its privacy loss calculation. Our experimental evaluations demonstrate that our mechanism and analysis $\\textit{can reduce the privacy loss by $16\\times$}$ while preserving the utility over the original PMixED. Furthermore, performing 100K predictions with $\\texttt{AdaPMixED}$ still achieves strong utility and a reasonable data-dependent privacy loss of $\\epsilon=5.25$.", "title_embedding_index": 2918, "title_abs_embedding_index": 2943}, {"title": "IAUNet: Instance-Aware U-Net", "link_suffix": "/forum?id=HeK3c9YIxG", "link": "https://openreview.net/forum?id=HeK3c9YIxG", "pdf_link": "https://openreview.net/pdf?id=HeK3c9YIxG", "keywords": "Medical and Biological Vision, Cell Microscopy, Instance Segmentation, Deep Learning", "abstract": "Instance segmentation is critical in biomedical imaging for accurately distinguishing individual objects, such as cells, which often overlap and vary in size. Recent query-based methods\u2014where object-specific queries guide segmentation\u2014have shown strong performance in this task. While U-Net has been a go-to architecture in medical image segmentation, it was neither specifically designed for instance segmentation nor explored in the context of query-based approaches. In this work, we present IAUNet, a novel architecture that brings instance awareness to U-Net with query-based mechanisms to achieve superior pixel-to-instance clustering. The key design includes lightweight Instance Activation (IA) layers, which generate guided object queries by highlighting semantically important regions. Additionally, we propose a Parallel Dual-Path Transformer decoder that refines object-specific features across multiple scales, allowing us to assign multiple queries from different scale levels to a specific object. Finally, we introduce the 2025 Revvity Full Cell Segmentation Dataset, comprising hundreds of manually labeled cells from brightfield images. This dataset is unique in capturing the complex morphology of overlapping cell cytoplasm with an unprecedented level of detail, making it a valuable resource and benchmark for advancing instance segmentation in biomedical imaging. Experiments on multiple public datasets and our own show that IAUNet outperforms most state-of-the-art fully convolutional, transformer-based, and query-based models, setting a strong baseline for medical image instance segmentation tasks.", "title_embedding_index": 2919, "title_abs_embedding_index": 2944}, {"title": "Compact Multimodal Context Represenations Using Visual Tokens", "link_suffix": "/forum?id=pCx6DYN43D", "link": "https://openreview.net/forum?id=pCx6DYN43D", "pdf_link": "https://openreview.net/pdf?id=pCx6DYN43D", "keywords": "Vision and Language Models, Compact Textual Information Encoding in Visual Space", "abstract": "The rapid progress in Multimodal Large Language Models (MLLMs) has significantly advanced their ability to process and understand complex visual and textual information. However, the integration of multiple images and extensive textual contexts remains a challenge due to the inherent limitation of the models' capacity to handle long input sequences efficiently. In this paper, we introduce SEEKER, a multimodal large language model designed to tackle this issue. SEEKER aims to optimize the compact encoding of long text by compressing the text sequence into the visual pixel space via images, enabling the model to handle long text within a fixed token-length budget efficiently. Our empirical experiments on six long-context multimodal tasks demonstrate that SEEKER can leverage fewer image tokens to convey the same amount of textual information compared with the OCR-based approach, and is more efficient in understanding long-form multimodal input and generating long-form textual output, outperforming all existing proprietary and open-source MLLMs by large margins.", "title_embedding_index": 2920, "title_abs_embedding_index": 2945}, {"title": "ChipVQA: Benchmarking Visual Language Models for Chip Design", "link_suffix": "/forum?id=xao3fIJC6M", "link": "https://openreview.net/forum?id=xao3fIJC6M", "pdf_link": "https://openreview.net/pdf?id=xao3fIJC6M", "keywords": "Multimodal LLM; Chip Design and Manufacturing; VQA", "abstract": "Large-language models (LLMs) have exhibited great potential to as-\nsist chip designs and analysis. Recent research and efforts are mainly\nfocusing on text-based tasks including general QA, debugging, design\ntool scripting, and so on. However, chip design and implementa-\ntion workflow usually require a visual understanding of diagrams,\nflow charts, graphs, schematics, waveforms, etc, which demands\nthe development of multi-modality foundation models. In this paper, we propose ChipVQA, a benchmark designed to evaluate the\ncapability of visual language models for chip design. ChipVQA includes 142 carefully designed and collected VQA questions covering five chip design disciplines: Digital Design, Analog Design, Architecture, Physical Design and Semiconductor Manufacturing. Un-\nlike existing VQA benchmarks, ChipVQA questions are carefully\ndesigned by chip design experts and require in-depth domain knowledge and reasoning to solve. We conduct comprehensive evaluations\non both open-source and proprietary multi-modal models that are\ngreatly challenged by the benchmark suit. ChipVQA examples are available athttps://anonymous.4open.science/r/chipvqa-2079/.", "title_embedding_index": 2921, "title_abs_embedding_index": 2946}, {"title": "Towards Sampling Data Structures for Tensor Products", "link_suffix": "/forum?id=CNPLXcMcSP", "link": "https://openreview.net/forum?id=CNPLXcMcSP", "pdf_link": "https://openreview.net/pdf?id=CNPLXcMcSP", "keywords": "sampling, data structures, tensor products", "abstract": "This paper studies the computational challenges of attention-based models in artificial intelligence by introducing innovative sampling methods to accelerate attention computation in large language models (LLM). Inspired by the recent progress of LLM in real-life applications, we introduces a streaming sampler question for attention setting. Our approach significantly reduces the computational burden of traditional attention mechanisms while maintaining or enhancing model performance. We demonstrate these methods' effectiveness from theoretical perspective, including space, update time. Additionally, our framework exhibits scalability and broad applicability across various model architectures and domains.", "title_embedding_index": 2922, "title_abs_embedding_index": 2947}, {"title": "Persistent Similarity in Internal Representations of Large Language Models", "link_suffix": "/forum?id=OqEsj4S240", "link": "https://openreview.net/forum?id=OqEsj4S240", "pdf_link": "https://openreview.net/pdf?id=OqEsj4S240", "keywords": "Topological Data Analysis, Persistent Homology, Large Language Models, Internal Representations, Similarity, Layer Pruning", "abstract": "Understanding the decision-making processes of large language models (LLMs) is critical given their widespread applications. Towards this goal, describing the topological and geometrical properties of internal representations has recently provided valuable insights. For a more comprehensive characterization of these inherently complex spaces, we present a novel framework based on zigzag persistence, a method in topological data analysis (TDA) well-suited for describing data undergoing dynamic transformations across layers. Within this framework, we introduce persistence similarity, a new metric that quantifies the persistence and transformation of topological features such as $p$-cycles throughout the model layers. Unlike traditional similarity measures, our approach captures the entire evolutionary trajectory of these features, providing deeper insights into the internal workings of LLMs. As a practical application, we leverage persistence similarity to identify and prune redundant layers, demonstrating comparable performance to state-of-the-art methods across several benchmark datasets. Additionally, our analysis reveals consistent topological behaviors across various models and hyperparameter settings, suggesting a universal structure in LLM internal representations.", "title_embedding_index": 2923, "title_abs_embedding_index": 2948}, {"title": "Scaling Test-Time Compute Optimally Can be More Effective than Scaling LLM Parameters", "link_suffix": "/forum?id=4FWAwZtd2n", "link": "https://openreview.net/forum?id=4FWAwZtd2n", "pdf_link": "https://openreview.net/pdf?id=4FWAwZtd2n", "keywords": "test-time compute, LLMs, scaling, language models", "abstract": "Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we scale up inference-time computation in LLMs, with a focus on answering: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how to tradeoff inference-time and pre-training compute. Little research has attempted to understand the scaling behaviors of test-time inference methods, with current work largely providing negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a ``compute-optimal'' scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute  can be used to outperform a 14x larger model.", "title_embedding_index": 2924, "title_abs_embedding_index": 2949}]