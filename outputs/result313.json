[{"title": "MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance", "link_suffix": "/forum?id=PJqP0wyQek", "link": "https://openreview.net/forum?id=PJqP0wyQek", "pdf_link": "https://openreview.net/pdf?id=PJqP0wyQek", "keywords": "Image Personalization, Multiple Subjects, Diffusion Models", "abstract": "Recent advancements in text-to-image generation models have dramatically enhanced the generation of photorealistic images from textual prompts, leading to an increased interest in personalized text-to-image applications, particularly in multi-subject scenarios. However, these advances are hindered by two main challenges: firstly, the need to accurately maintain the details of each referenced subject in accordance with the textual descriptions; and secondly, the difficulty in achieving a cohesive representation of multiple subjects in a single image without introducing inconsistencies. To address these concerns, our research introduces the MS-Diffusion framework for layout-guided zero-shot image personalization with multi-subjects. This innovative approach integrates grounding tokens with the feature resampler to maintain detail fidelity among subjects. With the layout guidance, MS-Diffusion further improves the cross-attention to adapt to the multi-subject inputs, ensuring that each subject condition acts on specific areas. The proposed multi-subject cross-attention orchestrates harmonious inter-subject compositions while preserving the control of texts. Comprehensive quantitative and qualitative experiments affirm that this method surpasses existing models in both image and text fidelity, promoting the development of personalized text-to-image generation.", "title_embedding_index": 15600, "title_abs_embedding_index": 15625}, {"title": "Improving Neuron-level Interpretability with White-box Language Models", "link_suffix": "/forum?id=6X7HaOEpZS", "link": "https://openreview.net/forum?id=6X7HaOEpZS", "pdf_link": "https://openreview.net/pdf?id=6X7HaOEpZS", "keywords": "Language model interpretation, neuron-level interpretation, white-box language models, deep learning architectures", "abstract": "Neurons in auto-regressive language models like GPT-2 can be interpreted by analyzing their activation patterns. Recent studies have shown that techniques such as dictionary learning, a form of post-hoc sparse coding, enhance this neuron-level interpretability.\nIn our research, we are driven by the goal to fundamentally improve neural network interpretability by embedding sparse coding directly within the model architecture, rather than applying it as an afterthought. In our study, we introduce a white-box transformer-like architecture named Coding RAte TransformEr (CRATE), explicitly engineered to capture sparse, low-dimensional structures within data distributions. \nOur comprehensive experiments showcase significant improvements (up to 106% relative improvement) in neuron-level interpretability across a variety of evaluation metrics. Detailed investigations confirm that this enhanced interpretability is steady across different layers irrespective of the model size, underlining CRATE's robust performance in enhancing neural network interpretability. Further analysis shows that CRATE's increased interpretability comes from its enhanced ability to consistently and distinctively activate on relevant tokens. These findings point towards a promising direction for creating white-box foundation models that excel in neuron-level interpretation.", "title_embedding_index": 15601, "title_abs_embedding_index": 15626}, {"title": "Test-Time Backdoor Attacks on Multimodal Large Language Models", "link_suffix": "/forum?id=9Orm76dUuT", "link": "https://openreview.net/forum?id=9Orm76dUuT", "pdf_link": "https://openreview.net/pdf?id=9Orm76dUuT", "keywords": "Multimodal Large Language Models, Test-Time Backdoor Attacks", "abstract": "Backdoor attacks typically set up a backdoor by contaminating training data or modifying parameters before the model is deployed, such that a predetermined trigger can activate harmful effects during the test phase. Can we, however, carry out test-time backdoor attacksafterdeploying the model? In this work, we presentAnyDoor, a test-time backdoor attack against multimodal large language models (MLLMs), without accessing training data or modifying parameters. In AnyDoor, the burden ofsetting upbackdoors is assigned to the visual modality (better capacity but worse timeliness), while the textual modality is responsible foractivatingthe backdoors (better timeliness but worse capacity). This decomposition takes advantage of the characteristics of different modalities, making attacking timing more controllable compared to directly applying adversarial attacks. We empirically validate the effectiveness of AnyDoor against popular MLLMs such as LLaVA-1.5, MiniGPT-4, InstructBLIP, and BLIP-2, and conduct extensive ablation studies. Notably, AnyDoor can dynamically change its backdoor trigger prompts and/or harmful effects, posing a new challenge for developing backdoor defenses.", "title_embedding_index": 15602, "title_abs_embedding_index": 15627}, {"title": "Generalization Bounds and Model Complexity for Kolmogorov\u2013Arnold Networks", "link_suffix": "/forum?id=q5zMyAUhGx", "link": "https://openreview.net/forum?id=q5zMyAUhGx", "pdf_link": "https://openreview.net/pdf?id=q5zMyAUhGx", "keywords": "Deep neural networks, Empirical process, Generalization bounds, Kolmogorov-Arnold representation theorem, Model complexity", "abstract": "Kolmogorov\u2013Arnold Network (KAN) is a network structure recently proposed in Liu et al. (2024) that offers improved interpretability and a more parsimonious design in many science-oriented tasks compared to multi-layer perceptrons. This work provides a rigorous theoretical analysis of KAN by establishing generalization bounds for KAN equipped with activation functions that are either represented by linear combinations of basis functions or lying in a low-rank Reproducing Kernel Hilbert Space (RKHS). In the first case, the generalization bound accommodates various choices of basis functions in forming the activation functions in each layer of KAN and is adapted to different operator norms at each layer. For a particular choice of operator norms, the bound scales with the $l_1$ norm of the coefficient matrices and the Lipschitz constants for the activation functions, and it has no dependence on combinatorial parameters (e.g., number of nodes) outside of logarithmic factors. Moreover, our result does not require the boundedness assumption on the loss function and, hence, is applicable to a general class of regression-type loss functions. In the low-rank case, the generalization bound scales polynomially with the underlying ranks as well as the Lipschitz constants of the activation functions in each layer. These bounds are empirically investigated for KANs trained with stochastic gradient descent on simulated and real data sets. The numerical results demonstrate the practical relevance of these bounds.", "title_embedding_index": 15603, "title_abs_embedding_index": 15628}, {"title": "Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification", "link_suffix": "/forum?id=Jwtpbhheoy", "link": "https://openreview.net/forum?id=Jwtpbhheoy", "pdf_link": "https://openreview.net/pdf?id=Jwtpbhheoy", "keywords": "in-context learning, Transformer, uncertainty quantification, Bayes optimal, supervised learning", "abstract": "Predicting simple function classes has been widely used as a testbed for developing theory and understanding of the trained Transformer's in-context learning (ICL) ability. In this paper, we revisit the training of Transformers on linear regression tasks, and different from the existing literature, we consider a bi-objective prediction task of predicting both the conditional expectation $\\mathbb{E}[Y|X]$ and the conditional variance Var$(Y|X)$. This additional uncertainty quantification objective provides a handle to (i) better design out-of-distribution experiments to distinguish ICL from in-weight learning (IWL) and (ii) make a better separation between the algorithms with and without using the prior information of the training distribution. Theoretically, we show that the trained Transformer reaches near Bayes optimum, suggesting the usage of the information of the training distribution. Our method can be extended to other cases. Specifically, with the Transformer's context window $S$, we prove a new generalization bound of $\\tilde{\\mathcal{O}}(\\sqrt{\\min{S, T}/(n T)})$ on $n$ tasks with sequences of length $T$, providing sharper analysis compared to previous results of $\\tilde{\\mathcal{O}}(\\sqrt{1/n})$. Empirically, we illustrate that while the trained Transformer behaves as the Bayes-optimal solution as a natural consequence of supervised training in distribution, it does not necessarily perform a Bayesian inference when facing task shifts, in contrast to the \\textit{equivalence} between these two proposed in many existing literature. We also demonstrate the trained Transformer's ICL ability over covariates shift and prompt-length shift and interpret them as a generalization over a meta distribution.", "title_embedding_index": 15604, "title_abs_embedding_index": 15629}, {"title": "FAVEN: Fast Audio-Visual Embodied Navigation in 3D Environments", "link_suffix": "/forum?id=48nAxwEyQ0", "link": "https://openreview.net/forum?id=48nAxwEyQ0", "pdf_link": "https://openreview.net/pdf?id=48nAxwEyQ0", "keywords": "audio-visual learning, audio-visual navigation", "abstract": "Achieving fast audio-visual embodied navigation in 3D environments is still a challenging problem. Existing methods typically rely on separate audio and visual data processing merged in late stages, leading to suboptimal path planning and increased time to locate targets. In this paper, we introduce FavEN, a novel transformer and mamba architecture that combines audio and visual data into $\\textit{early fusion}$ tokens. These tokens are passed through the entire network from the initial layer on and cross-attend to both data modalities. The effect of our early fusion approach is that the network can correlate information from the two data modalities from the get-go, which vastly improves its downstream navigation performance. We demonstrate this empirically through experimental results on the Replica and Matterport3D benchmarks. Furthermore, for the first time, we demonstrate the effectiveness of early fusion in improving the path search speed of audio-visual embodied navigation systems in real-world settings. Across various benchmarks, in comparison to previous approaches, FavEN reduces the search time by 93.6% and improves the SPL metrics by 10.4 and 6.5 on heard and unheard sounds.", "title_embedding_index": 15605, "title_abs_embedding_index": 15630}, {"title": "ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids", "link_suffix": "/forum?id=0ctvBgKFgc", "link": "https://openreview.net/forum?id=0ctvBgKFgc", "pdf_link": "https://openreview.net/pdf?id=0ctvBgKFgc", "keywords": "protein design, diffusion model, controllable generation, drug discovery, proteins, biology", "abstract": "We develop ProtComposer to generate protein structures conditioned on spatial protein layouts that are specified via a set of 3D ellipsoids capturing substructure shapes and semantics. At inference time, we condition on ellipsoids that are hand-constructed, extracted from existing proteins, or from a statistical model, with each option unlocking new capabilities. Hand-specifying ellipsoids enables users to control the location, size, orientation, secondary structure, and approximate shape of protein substructures. Conditioning on ellipsoids of existing proteins enables redesigning their substructure's connectivity or editing substructure properties. By conditioning on novel and diverse ellipsoid layouts from a simple statistical model, we improve protein generation with expanded Pareto frontiers between designability, novelty, and diversity. Further, this enables sampling designable proteins with a helix-fraction that matches natural proteins, unlike existing generative models that commonly oversample conceptually simple helix bundles.", "title_embedding_index": 15606, "title_abs_embedding_index": 15631}, {"title": "Finding Second-order Stationary Points for Generalized-Smooth Nonconvex Minimax Optimization via Gradient-based Algorithm", "link_suffix": "/forum?id=feZcpZbbqL", "link": "https://openreview.net/forum?id=feZcpZbbqL", "pdf_link": "https://openreview.net/pdf?id=feZcpZbbqL", "keywords": "Minimax Optimization, Nonconvex Optimization, Generalized Smoothness, Second-order Stationary Point", "abstract": "Nonconvex minimax problems have received intense interest in many machine learning applications such as generative adversarial network, robust optimization and adversarial \nRecently, a variety of minimax optimization algorithms based on Lipschitz smoothness \nfor finding first-order or second-order stationary points have been proposed.\nHowever, the standard Lipschitz continuous gradient or Hessian assumption could fail to hold even in some classic minimax problems,\nrendering conventional minimax optimization algorithms fail to converge in practice.\nTo address this challenge, we demonstrate a new gradient-based method for nonconvex-strongly-concave minimax optimization \nunder a generalized smoothness assumption.\nMotivated by the important application of escaping saddle points, we propose a generalized Hessian smoothness condition, \nunder which our gradient-based method can achieve the complexity of $\\mathcal{O}(\\epsilon^{-1.75}\\log n)$ \nto find a second-order stationary point with only gradient calls involved, \nwhich improves the state-of-the-art complexity results for the nonconvex minimax optimization \neven under standard Lipschitz smoothness condition.\nTo the best of our knowledge, this is the first work to show convergence \nfor finding second-order stationary points on nonconvex minimax optimization with generalized smoothness.\nThe experimental results on the application of domain adaptation confirm the superiority of our algorithm compared with existing methods.", "title_embedding_index": 15607, "title_abs_embedding_index": 15632}, {"title": "Certifiably Robust RAG against Retrieval Corruption Attacks", "link_suffix": "/forum?id=cU6ZdN87p3", "link": "https://openreview.net/forum?id=cU6ZdN87p3", "pdf_link": "https://openreview.net/pdf?id=cU6ZdN87p3", "keywords": "Retrieval-augemented Generation, Retrieval Corruption Attack, Certifiable Robustness", "abstract": "Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks. The key insight of RobustRAG is an isolate-then-aggregate strategy: we isolate passages into disjoint groups, generate LLM responses based on the concatenated passages from each isolated group, and then securely aggregate these responses for a robust output. To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, even when an adaptive attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages. We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability.", "title_embedding_index": 15608, "title_abs_embedding_index": 15633}, {"title": "Backdooring Vision-Language Models with Out-Of-Distribution Data", "link_suffix": "/forum?id=tZozeR3VV7", "link": "https://openreview.net/forum?id=tZozeR3VV7", "pdf_link": "https://openreview.net/pdf?id=tZozeR3VV7", "keywords": "VLMs, Backdoor Attack, image-to-text generation", "abstract": "The emergence of Vision-Language Models (VLMs) represents a significant advancement in integrating computer vision with Large Language Models (LLMs) to generate detailed text descriptions from visual inputs. Despite their growing importance, the security of VLMs, particularly against backdoor attacks, is under explored. Moreover, prior works often assume attackers have access to the original training data, which is often unrealistic. In this paper, we address a more practical and challenging scenario where attackers must rely solely on Out-Of-Distribution (OOD) data. We introduce VLOOD (Backdoor Vision-Language Models using Out-of-Distribution Data), a novel approach with two key contributions: (1) demonstrating backdoor attacks on VLMs in complex image-to-text tasks while minimizing degradation of the original semantics under poisoned inputs, and (2) proposing innovative techniques for backdoor injection without requiring any access to the original training data. Our evaluation on image captioning and visual question answering (VQA) tasks confirms the effectiveness of VLOOD, revealing a critical security vulnerability in VLMs and laying the foundation for future research on securing multimodal models against sophisticated threats.", "title_embedding_index": 15609, "title_abs_embedding_index": 15634}, {"title": "Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark", "link_suffix": "/forum?id=sMwYn2lZjO", "link": "https://openreview.net/forum?id=sMwYn2lZjO", "pdf_link": "https://openreview.net/pdf?id=sMwYn2lZjO", "keywords": "Sparse Mixture-of-Experts, Efficiency, Compression, Quantization", "abstract": "Large Language Models (LLMs) have become foundational in the realm of natural language processing, demonstrating performance improvements as model sizes increase. The Mixture-of-Experts (MoE) approach offers a promising way to scale LLMs more efficiently by using fewer computational FLOPs through sparse activation. However, it suffers from significant memory overheads, necessitating model compression techniques. Post-training quantization, a popular method for model compression, proves less effective when directly applied to MoE models due to MoE's overlooked inherent sparsity. This paper explores several MoE structure-aware quantization heuristics, ranging from coarse to fine granularity, from MoE block to individual linear weight. Our investigations reveal critical principles: different MoE structures (i.e., blocks, experts, linear layers) require varying numbers of weight bits for effective and efficient quantization. Conclusions are supported by extensive benchmarking across two representative MoE models and six tasks. We further introduce novel enhancements to more accurately identify the most critical weights in MoE quantization that necessitate higher bit allocations, including the linear weight outlier scorer and MoE block scorer. Additionally, subsequent experiments validate our findings in the context of both weight and activation quantization. Our code for reproducing all our experiments is provided as supplemental material.", "title_embedding_index": 15610, "title_abs_embedding_index": 15635}, {"title": "Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings", "link_suffix": "/forum?id=cWEfRkYj46", "link": "https://openreview.net/forum?id=cWEfRkYj46", "pdf_link": "https://openreview.net/pdf?id=cWEfRkYj46", "keywords": "brain-computer interfaces; speech decoding; tonal language; Homogeneity-Heterogeneity Disentanglement", "abstract": "Recent advancements in brain-computer interfaces (BCIs) and deep learning have made decoding lexical tones from intracranial recordings possible, providing the potential to restore the communication ability of speech-impaired tonal language speakers. However, data heterogeneity induced by both physiological and instrumental factors poses a significant challenge for unified invasive brain tone decoding. Particularly, the existing heterogeneous decoding paradigm (training subject-specific models with individual data) suffers from the intrinsic limitation that fails to learn generalized neural representations and leverages data across subjects. To this end, we introduce Homogeneity-Heterogeneity Disentangled Learning for Neural Representations (H2DiLR), a framework that disentangles and learns the homogeneity and heterogeneity from intracranial recordings of multiple subjects. To verify the effectiveness of H2DiLR, we collected stereoelectroencephalography (sEEG) from multiple participants reading Mandarin materials containing 407 syllables (covering nearly all Mandarin characters). Extensive experiments demonstrate that H2DiLR, as a unified decoding paradigm, outperforms the naive heterogeneous decoding paradigm by a large margin. We also empirically show that H2DiLR indeed captures homogeneity and heterogeneity during neural representation learning.", "title_embedding_index": 15611, "title_abs_embedding_index": 15636}, {"title": "Model aggregation: minimizing empirical variance outperforms minimizing empirical error", "link_suffix": "/forum?id=grM2Yv49cI", "link": "https://openreview.net/forum?id=grM2Yv49cI", "pdf_link": "https://openreview.net/pdf?id=grM2Yv49cI", "keywords": "Model aggregation; Ensemble Learning; Scientific Machine Learning; Gaussian Processes; Neural operators", "abstract": "Whether deterministic or stochastic, models can be viewed as functions designed to approximate a specific quantity of interest. \nWe introduce a data-driven framework that integrates predictions from various models, enhancing overall accuracy by leveraging the individual strengths of each. This non-intrusive, model-agnostic approach treats the contributing models as black boxes and accommodates outputs from diverse methodologies, including machine learning algorithms and traditional numerical solvers.\nWe advocate for a point-wise linear aggregation process and propose two methods for optimizing this aggregate: Minimal Error Aggregation (MEA), which minimizes the prediction error, and Minimal Variance Aggregation (MVA), which focuses on reducing variance. \n While MEA is inherently more accurate when correlations between models and the target quantity are perfectly known, Minimal Empirical Variance Aggregation (MEVA), an empirical version of MVA, consistently outperforms Minimal Empirical Error Aggregation (MEEA), the empirical counterpart of MEA, when these correlations must be estimated from data. The key difference is that MEVA constructs an aggregate by estimating model errors, while MEEA treats the models as features for direct interpolation of the quantity of interest. This makes MEEA more susceptible to overfitting and poor generalization, where the aggregate may underperform individual models during testing. \n We demonstrate the versatility and effectiveness of our framework across various applications, including data science and partial differential equations, illustrating its ability to significantly enhance both robustness and accuracy.", "title_embedding_index": 15612, "title_abs_embedding_index": 15637}, {"title": "Towards Continuous Reuse of Graph Models via Holistic Memory Diversification", "link_suffix": "/forum?id=Pbz4i7B0B4", "link": "https://openreview.net/forum?id=Pbz4i7B0B4", "pdf_link": "https://openreview.net/pdf?id=Pbz4i7B0B4", "keywords": "Incremental learning, Model Reuse, Transfer learning", "abstract": "This paper addresses the challenge of incremental learning in growing graphs with increasingly complex tasks. The goal is to continually train a graph model to handle new tasks while retaining its inference ability on previous tasks. Existing methods usually neglect the importance of memory diversity, limiting in effectively selecting high-quality memory from previous tasks and remembering broad previous knowledge within the scarce memory on graphs. To address that, we introduce a novel holistic Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, which first introduces a buffer selection strategy that considers both intra-class and inter-class diversities, employing an efficient greedy algorithm for sampling representative training nodes from graphs into memory buffers after learning each new task. Then, to adequately rememorize the knowledge preserved in the memory buffer when learning new tasks, we propose a diversified memory generation replay method. This method first utilizes a variational layer to generate the distribution of buffer node embeddings and sample synthesized ones for replaying. Furthermore, an adversarial variational embedding learning method and a reconstruction-based decoder are proposed to maintain the integrity and consolidate the generalization of the synthesized node embeddings, respectively. Finally, we evaluate our model on node classification tasks involving increasing class numbers. Extensive experimental results on publicly accessible datasets demonstrate the superiority of DMSG over state-of-the-art methods.", "title_embedding_index": 15613, "title_abs_embedding_index": 15638}, {"title": "Multi-modal brain encoding models for multi-modal stimuli", "link_suffix": "/forum?id=0dELcFHig2", "link": "https://openreview.net/forum?id=0dELcFHig2", "pdf_link": "https://openreview.net/pdf?id=0dELcFHig2", "keywords": "brain encoding, fMRI, multi-modal models, multi-modal stimuli, Transformers, videos, speech, language", "abstract": "Despite participants engaging in unimodal stimuli, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information across modalities through a hierarchy of early sensory regions to higher cognition (language regions). We investigate this question by using multiple unimodal and two types of multi-modal models\u2014cross-modal and jointly pretrained\u2014to determine which type of models is more relevant to fMRI brain activity when participants are engaged in watching movies (videos with audio). We observe that both types of multi-modal models show improved alignment in several language and visual regions. This study also helps in identifying which brain regions process unimodal versus multi-modal information. We further investigate the contribution of each modality to multi-modal alignment by carefully removing unimodal features one by one from multi-modal representations, and find that there is additional information beyond the unimodal embeddings that is processed in the visual and language regions. Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities. These findings serve as strong motivation for the neuro-science community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain.", "title_embedding_index": 15614, "title_abs_embedding_index": 15639}, {"title": "Forecasting Whole-Brain Neural Activity from Volumetric Video", "link_suffix": "/forum?id=4UXIGATUTj", "link": "https://openreview.net/forum?id=4UXIGATUTj", "pdf_link": "https://openreview.net/pdf?id=4UXIGATUTj", "keywords": "neuroscience, forecasting, video, lightsheet microscopy, zebrafish, calcium imaging, neuron activity", "abstract": "Large-scale neuronal activity recordings with fluorescent calcium indicators are increasingly common, yielding high-resolution 2D or 3D videos. Traditional analysis pipelines reduce this data to 1D traces by segmenting regions of interest, leading to inevitable information loss. Inspired by the success of deep learning on minimally processed data in other domains, we investigate the potential of forecasting neuronal activity directly from volumetric videos: we design a model to handle the high resolution and large receptive fields necessary for capturing spatio-temporal dependencies in volumetric whole-brain recordings. We explore effects of pre-training and perform extensive model selection, analyzing spatio-temporal trade-offs for generating accurate forecasts. Our model outperforms trace-based forecasting approaches on ZAPBench, a recently proposed benchmark on whole-brain activity prediction in zebrafish, demonstrating the advantages of preserving the spatial structure of neuronal activity.", "title_embedding_index": 15615, "title_abs_embedding_index": 15640}, {"title": "Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning", "link_suffix": "/forum?id=PITFO1ddeh", "link": "https://openreview.net/forum?id=PITFO1ddeh", "pdf_link": "https://openreview.net/pdf?id=PITFO1ddeh", "keywords": "Knowledge Editing, Representation Fine-tuning, Large Language Models", "abstract": "Large language models (LLMs) have achieved remarkable performance on vari-\nous natural language tasks. However, they are trained on static corpora and their\nknowledge can become outdated quickly in the fast-changing world. This moti-\nvates the development of knowledge editing methods designed to update certain\nknowledge in LLMs without changing unrelated others. To make selective edits,\nprevious efforts often sought to update a small amount of parameters in some spe-\ncific layer(s) of a LLM. Nonetheless, in challenging scenarios, they still fall short\nin making successful edits while preserving knowledge irrelevant to the updates\nsimultaneously, resulting in a notable editing-locality trade-off. In this work, we\nquestion if the trade-offs are caused by the fact that parameter-based updates have\na global effect, i.e., edited parameters affect all inputs indiscriminately. In light of\nthis, we explore the feasibility of representation fine-tuning, which applied some\nlinear update to a few representations in a learned subspace, for knowledge edit-\ning. While being effective to enhance an LLM\u2019s general ability as demonstrated in\nthe previous work, we theoretically show that this linear update imposes a tension\nin editing-locality trade-off. Subsequently, BaFT is proposed to break the linear-\nity. BaFT computes a weight for each basis that spans a dimension of the subspace\nbased on the input representation. This input-dependent weighting mechanism al-\nlows BaFT to manage different types of knowledge in an adaptive way, thereby\nachieving a better editing-locality trade-off. Experiments on three LLMs with five\nediting benchmarks in diverse scenarios show the superiority of our method.", "title_embedding_index": 15616, "title_abs_embedding_index": 15641}, {"title": "R3HF: Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback", "link_suffix": "/forum?id=9LAqIWi3QG", "link": "https://openreview.net/forum?id=9LAqIWi3QG", "pdf_link": "https://openreview.net/pdf?id=9LAqIWi3QG", "keywords": "RLHF", "abstract": "Reinforcement learning from human feedback (RLHF) provides a paradigm for aligning large language models (LLMs) with human preferences. This involves the initial training of a reward model based on pairwise human feedback. The reward model is subsequently utilized in reinforcement learning to assess the scores of each generated sentence as a whole, further guiding the optimization of LLMs. However, current approaches have a significant shortcoming: They allocate a single, sparse, and delayed reward to an entire sequence of output. This may overlook some significant individual contributions of each token towards the desired outcome. To overcome this limitation, our paper proposes a novel reward redistribution method called R3HF, which facilitates a more fine-grained, token-level reward allocation. Specifically, our method treats the reward prediction task of the reward model as a regression problem. As a result, the redistributed rewards are computed by evaluating the specific contribution of each token to the reward model's output. This detailed approach improves the model's understanding of language nuances, leading to more precise enhancements in its performance. Our method is crafted to integrate seamlessly with most current techniques while incurring minimal computational costs. Through comprehensive experiments across diverse datasets and tasks, we have verified the effectiveness and superiority of our approach.", "title_embedding_index": 15617, "title_abs_embedding_index": 15642}, {"title": "Factor Graph Optimization of Error-Correcting Codes for Belief Propagation Decoding", "link_suffix": "/forum?id=XCP0MOMLPo", "link": "https://openreview.net/forum?id=XCP0MOMLPo", "pdf_link": "https://openreview.net/pdf?id=XCP0MOMLPo", "keywords": "ECC, Binary Programming, Belief Propagation", "abstract": "The design of optimal linear block codes capable of being efficiently decoded is of major concern, especially for short block lengths. \nAs near capacity-approaching codes, Low-Density Parity-Check (LDPC) codes possess several advantages over\nother families of codes, the most notable being its efficient decoding via Belief Propagation.\n While many LDPC code design methods exist, the development of efficient sparse codes that meet the constraints of modern short code lengths and accommodate new channel models remains a challenge.\nIn this work, we propose for the first time a gradient-based data-driven approach for the design of sparse codes. We develop locally optimal codes with respect to Belief Propagation decoding via the learning of the Factor graph under channel noise simulations. \nThis is performed via a novel complete graph tensor representation of the Belief Propagation algorithm, optimized over finite fields via backpropagation and coupled with an efficient line-search method. \nThe proposed approach is shown to outperform the decoding performance of existing popular codes by orders of magnitude and demonstrates the power of data-driven approaches for code design.", "title_embedding_index": 15618, "title_abs_embedding_index": 15643}, {"title": "Contextual Document Embeddings", "link_suffix": "/forum?id=Wqsk3FbD6D", "link": "https://openreview.net/forum?id=Wqsk3FbD6D", "pdf_link": "https://openreview.net/pdf?id=Wqsk3FbD6D", "keywords": "text, embeddings, retrieval, context, contrastive", "abstract": "Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes.  Our method can be applied to improve performance on any contrastive learning dataset and any biencoder.", "title_embedding_index": 15619, "title_abs_embedding_index": 15644}, {"title": "Do LLMs have Consistent Values?", "link_suffix": "/forum?id=8zxGruuzr9", "link": "https://openreview.net/forum?id=8zxGruuzr9", "pdf_link": "https://openreview.net/pdf?id=8zxGruuzr9", "keywords": "LLM, values", "abstract": "Values are a basic driving force underlying human behavior. Large Language Models (LLM) technology is constantly improving towards human-like dialogue. However, little research has been done to study the values exhibited in text generated by LLMs. Here we study this question by turning to the rich literature on value structure in psychology. We ask whether LLMs exhibit the same value structure that has been demonstrated in humans, including the ranking of values, and correlation between values. We show that the results of this analysis strongly depend on how the LLM is prompted, and that under a particular prompting strategy (referred to as ``Value Anchoring'') the agreement with human data is quite compelling. Our results serve both to improve our understanding of values in LLMs, as well as introduce novel methods for assessing consistency in LLM responses.", "title_embedding_index": 15620, "title_abs_embedding_index": 15645}, {"title": "Efficient Adaptive Federated Optimization", "link_suffix": "/forum?id=AbJWZp4THG", "link": "https://openreview.net/forum?id=AbJWZp4THG", "pdf_link": "https://openreview.net/pdf?id=AbJWZp4THG", "keywords": "Adaptivity, Optimization, Federated Learning", "abstract": "Adaptive optimization plays a pivotal role in federated learning, where simultaneous server and client-side adaptivity have been shown to be essential for maximizing its performance. However, the scalability of jointly adaptive systems is often constrained by limited resources in communication and memory. In this paper, we introduce a class of efficient adaptive algorithms, named $FedAda^2$, designed specifically for large-scale, cross-device federated environments. $FedAda^2$ optimizes communication efficiency by avoiding the transfer of preconditioners between the server and clients, while simultaneously utilizing memory-efficient adaptive optimizers on the client-side to reduce extra on-device memory cost. Theoretically, we demonstrate that $FedAda^2$ achieves the same convergence rates for general, non-convex objectives as its more resource-intensive counterparts that directly integrate joint adaptivity. Empirically, we showcase the benefits of joint adaptivity and the effectiveness of $FedAda^2$ on both image and text datasets.", "title_embedding_index": 15621, "title_abs_embedding_index": 15646}, {"title": "ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish", "link_suffix": "/forum?id=oCHsDpyawq", "link": "https://openreview.net/forum?id=oCHsDpyawq", "pdf_link": "https://openreview.net/pdf?id=oCHsDpyawq", "keywords": "neuroscience, zebrafish, forecasting, benchmark, timeseries, lightsheet microscopy, calcium imaging", "abstract": "Data-driven benchmarks have led to significant progress in key scientific modeling domains including weather and structural biology. Here, we introduce the Zebrafish Activity Prediction Benchmark (ZAPBench) to measure progress on the problem of predicting cellular-resolution neural activity throughout an entire vertebrate brain. The benchmark is based on a novel dataset containing 4d light-sheet microscopy recordings of over 70,000 neurons in a larval zebrafish brain, along with motion stabilized and voxel-level cell segmentations of these data that facilitate development of a variety of forecasting methods. Initial results from a selection of time series and volumetric video modeling approaches achieve better performance than naive baseline methods, but also show room for further improvement. The specific brain used in the activity recording is also undergoing synaptic-level anatomical mapping, which will enable future integration of detailed structural information into forecasting methods.", "title_embedding_index": 15622, "title_abs_embedding_index": 15647}, {"title": "Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation", "link_suffix": "/forum?id=EbWf36quzd", "link": "https://openreview.net/forum?id=EbWf36quzd", "pdf_link": "https://openreview.net/pdf?id=EbWf36quzd", "keywords": "Generative Models, Text-to-Image Generation, Diffusion Models, Flow Matching", "abstract": "Sora unveils the potential of scaling Diffusion Transformer (DiT) for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this paper, we introduce the Lumina-T2X family -- a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a simple and scalable generative framework that can be adapted to various modalities, e.g., transforming noise into images, videos, multi-view 3D objects, or audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as |[nextline]| and |[nextframe]| tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. Advanced techniques like RoPE, KQ-Norm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT (PixArt-alpha), indicating that increasing the number of parameters significantly accelerates convergence of generative models without compromising visual quality. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. All code and checkpoints of Lumina-T2X are released to further foster creativity, transparency, and diversity in the generative AI community.", "title_embedding_index": 15623, "title_abs_embedding_index": 15648}, {"title": "Q-Adapt: Adapting LMM for  Visual Quality Perceiver with Progressive Instruction Tuning", "link_suffix": "/forum?id=KUf2iyin77", "link": "https://openreview.net/forum?id=KUf2iyin77", "pdf_link": "https://openreview.net/pdf?id=KUf2iyin77", "keywords": "Explainable Image Quality Assessment, LMM", "abstract": "The rapid advancement of Large Multi-modal Foundation Models (LMM) has paved the way for the possible Explainable Image Quality Assessment (EIQA) with instruction tuning from two perspectives: overall quality explanation, and attribute-wise perception answering. However, existing works usually overlooked the conflicts between these two types of perception explanations during joint instruction tuning, leading to insufficient perception understanding. To mitigate this, we propose a new paradigm for perception-oriented instruction tuning, i.e., Q-Adapt, which aims to eliminate the conflicts and achieve the synergy between these two EIQA tasks when adapting LMM, resulting in enhanced multi-faceted explanations of IQA. Particularly, we propose a progressive instruction tuning strategy by dividing the adaption process of LMM for EIQA into two stages, where the first stage empowers the LMM with universal perception knowledge tailored for two tasks using an efficient transfer learning strategy, i.e., LoRA, and the second stage introduces the instruction-adaptive visual prompt tuning to dynamically adapt visual features for the different instructions from two tasks. In this way, our proposed Q-Adapt can achieve a lightweight visual quality perceiver, demonstrating comparable performance and, in some instances, superior results across perceptual-related benchmarks and commonly-used IQA databases.", "title_embedding_index": 15624, "title_abs_embedding_index": 15649}]