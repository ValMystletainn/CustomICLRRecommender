[{"title": "Deep Bayesian Filter for Bayes-Faithful Data Assimilation", "link_suffix": "/forum?id=WQy61tS53c", "link": "https://openreview.net/forum?id=WQy61tS53c", "pdf_link": "https://openreview.net/pdf?id=WQy61tS53c", "keywords": "Data Assimilation, Variational Inference, State-Space Models, Koopman Operator", "abstract": "State estimation for nonlinear state space models (SSMs) is a challenging task. Existing assimilation methodologies predominantly assume Gaussian posteriors on physical space, where true posteriors become inevitably non-Gaussian. We propose Deep Bayesian Filtering (DBF) for data assimilation on nonlinear SSMs. DBF constructs new latent variables $h_t$ in addition to the original physical variables $z_t$ and assimilates observations $o_t$. By (i) constraining the state transition on the new latent space to be linear and (ii) learning a Gaussian inverse observation operator $r(h_t|o_t)$, posteriors remain Gaussian. Notably, the structured design of test distributions enables an analytical formula for the recursive computation, eliminating the accumulation of Monte Carlo sampling errors across time steps. DBF trains the Gaussian inverse observation operators $r(h_t|o_t)$ and other latent SSM parameters (e.g., dynamics matrix) by maximizing the evidence lower bound. Experiments demonstrate that DBF outperforms model-based approaches and latent assimilation methods in tasks where the true posterior distribution on physical space is significantly non-Gaussian.", "title_embedding_index": 17750, "title_abs_embedding_index": 17775}, {"title": "Auxiliary-Loss-Free Load Balancing Strategy for Mixture-of-Experts", "link_suffix": "/forum?id=y1iU5czYpE", "link": "https://openreview.net/forum?id=y1iU5czYpE", "pdf_link": "https://openreview.net/pdf?id=y1iU5czYpE", "keywords": "mixture of experts, load balancing, auxiliary-loss-free", "abstract": "For Mixture-of-Experts (MoE) models, an unbalanced expert load will lead to routing collapse or increased computational overhead. Existing methods commonly employ an auxiliary loss to encourage load balance, but a large auxiliary loss will introduce non-negligible interference gradients into training and thus impair the model performance. In order to control load balance while not producing undesired gradients during training, we proposeLoss-Free Balancing, a new load balancing strategy that operates without auxiliary losses. To be specific, before the top-K routing decision, Loss-Free Balancing will first apply an expert-wise bias to the routing scores of each expert. By dynamically updating the bias of each expert according to its recent load, Loss-Free Balancing can consistently maintain a balanced distribution of expert load. In addition, since Loss-Free Balancing does not produce any interference gradients, it also elevates the upper bound of model performance gained from MoE training. We validate the performance of Loss-Free Balancing on MoE models with up to 3B parameters trained on up to 200B tokens. Experimental results show that Loss-Free Balancing achieves both better performance and better load balance compared with traditional auxiliary-loss-controlled load balancing strategies.", "title_embedding_index": 17751, "title_abs_embedding_index": 17776}, {"title": "Moirai-MoE: Empowering Time Series Foundation Models with Sparse Mixture of Experts", "link_suffix": "/forum?id=HbbnlrmsAH", "link": "https://openreview.net/forum?id=HbbnlrmsAH", "pdf_link": "https://openreview.net/pdf?id=HbbnlrmsAH", "keywords": "Time Series Foundation Models, Mixture of Experts", "abstract": "Time series foundation models have demonstrated impressive performance as zero-shot forecasters, i.e. tackling a wide variety of downstream forecasting tasks without explicit task-specific training. However, achieving effectively unified training on time series remains an open challenge. Existing approaches introduce some level of model specialization to account for the highly heterogeneous nature of time series data. For instance, Moirai pursues unified training by employing multiple input/output projection layers, each tailored to handle time series at a specific frequency. Similarly, TimesFM maintains a frequency embedding dictionary for this purpose. We identify two major drawbacks to this human-imposed frequency-level model specialization: (1) Frequency is not a reliable indicator of the underlying patterns in time series. For example, time series with different frequencies can display similar patterns, while those with the same frequency may exhibit varied patterns. (2) Non-stationarity is an inherent property of real-world time series, leading to varied distributions even within a short context window of a single time series. Frequency-level specialization is too coarse-grained to capture this level of diversity. To address these limitations, this paper introduces Moirai-MoE, using a single input/output projection layer while delegating the modeling of diverse time series patterns to the sparse mixture of experts (MoE) within Transformers. With these designs, Moirai-MoE reduces reliance on human-defined heuristics and enables automatic token-level specialization. Extensive experiments on 39 datasets demonstrate the superiority of Moirai-MoE over existing foundation models in both in-distribution and zero-shot scenarios. Furthermore, this study conducts comprehensive model analyses to explore the inner workings of time series MoE foundation models and provides valuable insights for future research.", "title_embedding_index": 17752, "title_abs_embedding_index": 17777}, {"title": "Semantic-Centric Alignment for Zero-shot Panoptic and Semantic Segmentation", "link_suffix": "/forum?id=Xd2Qxf5RYI", "link": "https://openreview.net/forum?id=Xd2Qxf5RYI", "pdf_link": "https://openreview.net/pdf?id=Xd2Qxf5RYI", "keywords": "zero-shot segmentation, zero-shot learning, feature alignment, semantic segmentation, panoptic segmentation, semantic-centric", "abstract": "Zero-shot segmentation has achieved great success by generating features from semantic embeddings to adapt the model to unseen classes. These semantic-generated features are typically aligned with the visual distribution of seen classes to improve generalization on extracted image features. However, this vision-centric alignment may easily overfit seen classes due to the lack of visual data for unseen classes. To address this issue, we propose a semantic-centric alignment method that aligns the generated features with the well-structured semantic distribution across all classes. First, we align the vision backbone features with CLIP tokens through Vision-to-CLIP alignment. This approach leverages CLIP\u2019s visual-language matching capabilities to produce semantic-aligned backbone features. Then, we generate synthetic features from semantic embeddings for unseen classes, supervised by semantic-aligned visual features and CLIP semantic tokens for improving visual diversity while maintaining semantic consistency. Finally, we finetune the class projector through the semantic-aligned joint features to further adapt the model for unseen classes. Our semantic-centric alignment effectively enhances the model\u2019s zero-shot generalization by constructing a unified and well-structured semantic-aligned feature space. Our method achieves SOTA performance in both zero-shot panoptic and semantic segmentation, and can directly segment unseen classes without fine-tuning.", "title_embedding_index": 17753, "title_abs_embedding_index": 17778}, {"title": "Can We Predict Performance of Large Models across Vision-Language Tasks?", "link_suffix": "/forum?id=72nCh5JtLQ", "link": "https://openreview.net/forum?id=72nCh5JtLQ", "pdf_link": "https://openreview.net/pdf?id=72nCh5JtLQ", "keywords": "Large Vision-Language Models (LVLMs), Benchmarking, Probabilistic Matrix Factorization (PMF), Markov Chain Monte Carlo (MCMC), Active Evaluation", "abstract": "Evaluating large vision-language models (LVLMs) is very expensive, due to the high computational costs and the wide variety of tasks. The good news is that if we already have some observed scores, we may be able to infer unknown ones. In this study, we propose a new framework for predicting unknown performance scores based on observed ones from other LVLMs or tasks. We first formulate the performance prediction as a matrix completion task. Specifically, we construct a sparse performance matrix $\\boldsymbol{R}$, where each entry $R_{mn}$ represents the performance score of the $m$-th model on the $n$-th dataset. By applying probabilistic matrix factorization (PMF) with Markov chain Monte Carlo (MCMC), we can complete the performance matrix, that is, predict unknown scores. Additionally, we estimate the uncertainty of performance prediction based on MCMC. Practitioners can evaluate their models on untested tasks with higher uncertainty first, quickly reducing errors in performance prediction. We further introduce several improvements to enhance PMF for scenarios with sparse observed performance scores. In experiments, we systematically evaluate 108 LVLMs on 176 datasets from 36 benchmarks, constructing training and testing sets for validating our framework. Our experiments demonstrate the accuracy of PMF in predicting unknown scores, the reliability of uncertainty estimates in ordering evaluations, and the effectiveness of our enhancements for handling sparse data.", "title_embedding_index": 17754, "title_abs_embedding_index": 17779}, {"title": "Balancing Token Efficiency and Structural Accuracy in LLMs Image Generation by Combining VQ-VAE and Diffusion Tokenizers", "link_suffix": "/forum?id=IqGVIU4rvM", "link": "https://openreview.net/forum?id=IqGVIU4rvM", "pdf_link": "https://openreview.net/pdf?id=IqGVIU4rvM", "keywords": "Visual Tokenizer, VQ-VAE, Diffusion", "abstract": "We proposes a novel visual tokenizer by combining high-level semantic tokens and low-level pixel tokens to represent images, aiming to address the challenges of image-to-sequence conversion for Large Language Models (LLMs). Existing visual tokenizers, such as VQ-VAE and diffusion-based models, either struggle with token explosion as image resolution increases or fail to capture detailed structural information. Our method introduces a dual-token system: high-level semantic tokens capture the main content of the image, while low-level pixel tokens preserve structural details. By integrating these tokens in a hybrid architecture, we leverage a VQ-VAE branch to generate low-resolution guidance and a diffusion process to reconstruct high-resolution images with both semantic coherence and structural accuracy. This approach significantly reduces the number of required tokens and enhances image reconstruction quality, offering an efficient solution for tasks like image generation and understanding based on LLMs.", "title_embedding_index": 17755, "title_abs_embedding_index": 17780}, {"title": "Wasn\u2019t Me: Enabling Users to Falsify Deepfake Attacks", "link_suffix": "/forum?id=TEckHPheJg", "link": "https://openreview.net/forum?id=TEckHPheJg", "pdf_link": "https://openreview.net/pdf?id=TEckHPheJg", "keywords": "deepfake detection, deepfake verification", "abstract": "The rise of deepfake technology has made everyone vulnerable to false claims based on manipulated media. While many existing deepfake detection methods aim to identify fake media, they often struggle with deepfakes created by new generative models not seen during training. In this paper, we propose VeriFake, a method that enables users to verify that media claiming to show them are false. VeriFake is based on two key assumptions: (i) generative models struggle to exactly depict a specific identity, and (ii) they often fail to perfectly synchronize generated lip movements with speech. By combining these assumptions with powerful modern representation encoders, VeriFake achieves highly effective results, even against previously unseen deepfakes. Through extensive experiments, we demonstrate that VeriFake significantly outperforms general-purpose deepfake detection techniques despite being simple to implement and not relying on any fake data for pretraining.", "title_embedding_index": 17756, "title_abs_embedding_index": 17781}, {"title": "Rethinking Graph Super-Resolution: Dual Frameworks for Topological Fidelity", "link_suffix": "/forum?id=rdhB4ZvQR0", "link": "https://openreview.net/forum?id=rdhB4ZvQR0", "pdf_link": "https://openreview.net/pdf?id=rdhB4ZvQR0", "keywords": "Deep Graph Learning, Graph Super-resolution, Network Neuroscience", "abstract": "Graph super-resolution is an underexplored yet highly relevant research direction that circumvents the need for costly and time-consuming data collection, preparation, and storage. This makes it especially desirable for resource-constrained fields such as the medical domain. Existing work on graph super-resolution leverages graph neural networks (GNNs) and achieves impressive results. However, we note two major limitations in the current model design: (1) It violates the underlying graph structure when increasing the number of nodes, and (2) it relies heavily on node representation learning, which has limited capacity to accurately model edges. To address these limitations, we propose two novel frameworks: (1) Bi-SR, which performs structure-aware node super-resolution, and (2) DEFEND, which focuses on edge representation learning for enhanced edge modeling. We supplement our work with rigorous theoretical analysis and conduct extensive experiments on simulated and real-world datasets covering diverse graph topologies and low-to-high resolution relationships. The results demonstrate substantial improvements across all experiments, highlighting the potential of both frameworks for graph super-resolution tasks.", "title_embedding_index": 17757, "title_abs_embedding_index": 17782}, {"title": "Robust Consensus Anchor Learning for Efficient Multi-view Subspace Clustering", "link_suffix": "/forum?id=rDb9oY6Ww7", "link": "https://openreview.net/forum?id=rDb9oY6Ww7", "pdf_link": "https://openreview.net/pdf?id=rDb9oY6Ww7", "keywords": "Multi-view clustering, consensus anchor learning, effectiveness and efficiency", "abstract": "As a leading unsupervised classification algorithm in artificial intelligence, multi-view subspace clustering segments unlabeled data from different subspaces. Recent works based on the anchor have been proposed to decrease the computation complexity for the datasets with large scales in multi-view clustering. The major differences among these methods lie on the objective functions they define. Despite considerable success, these works pay few attention to guaranting the robustness of learned consensus anchors via effective manner for efficient multi-view clustering and investigating the specific local distribution of cluster in the affine subspace. Besides, the robust consensus anchors as well as the common cluster structure shared by different views are not able to be simultaneously learned. In this paper, we propose Robust Consensus anchors learning for efficient multi-view Subspace Clustering (RCSC). We first show that if the data are sufficiently sampled from independent subspaces, and the objective function meets some conditions, the achieved anchor graph has the block-diagonal structure. As a special case, we provide a model based on Frobenius norm, non-negative and affine constraints in consensus anchors learning, which guarantees the robustness of learned consensus anchors for efficient multi-view clustering and investigates the specific local distribution of cluster in the affine subspace. While it is simple, we theoretically give the geometric analysis regarding the formulated RCSC. The union of these three constraints is able to restrict how each data point is described in the affine subspace with specific local distribution of cluster for guaranting the robustness of learned consensus anchors. RCSC takes full advantages of correlation among consensus anchors, which encourages the grouping effect and groups highly correlated consensus anchors together with the guidance of view-specific projection. The anchor graph construction, partition and robust anchor learning are jointly integrated into a unified framework. It ensures the mutual enhancement for these procedures and helps lead to more discriminative consensus anchors as well as the cluster indicator. We then adopt an alternative optimization strategy for solving the formulated problem. Experiments performed on eight multi-view datasets confirm the superiority of RCSC based on the effectiveness and efficiency.", "title_embedding_index": 17758, "title_abs_embedding_index": 17783}, {"title": "Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity", "link_suffix": "/forum?id=BpfsxFqhGa", "link": "https://openreview.net/forum?id=BpfsxFqhGa", "pdf_link": "https://openreview.net/pdf?id=BpfsxFqhGa", "keywords": "Video reconstruction, Brain-computer Interface (BCI).", "abstract": "Reconstructing human dynamic vision from brain activity is a challenging task with great scientific significance.  Although prior video reconstruction methods have made substantial progress, they still suffer from several limitations, including: (1) difficulty in simultaneously reconciling semantic (e.g. categorical descriptions), structure (e.g. size and color), and consistent motion information (e.g. order of frames); (2) low temporal resolution of fMRI, which poses a challenge in decoding multiple frames of video dynamics from a single fMRI frame; (3) reliance on video generation models, which introduces ambiguity regarding whether the dynamics observed in the reconstructed videos are genuinely derived from fMRI data or are hallucinations from generative model. To overcome these limitations,  we propose a two-stage model named Mind-Animator. During the fMRI-to-feature stage, we decouple semantic, structure, and motion features from fMRI. Specifically, we employ fMRI-vision-language tri-modal contrastive learning to decode semantic feature from fMRI and design a sparse causal attention mechanism for decoding multi-frame video motion features through a next-frame-prediction task. In the feature-to-video stage, these features are integrated into videos using an inflated Stable Diffusion, effectively eliminating external video data interference.  Extensive experiments on multiple video-fMRI datasets demonstrate that our model achieves state-of-the-art performance. Comprehensive visualization analyses further elucidate the interpretability of our model from a neurobiological perspective.  Project page:https://mind-animator-design.github.io/.", "title_embedding_index": 17759, "title_abs_embedding_index": 17784}, {"title": "Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models", "link_suffix": "/forum?id=hUdLs6TqZL", "link": "https://openreview.net/forum?id=hUdLs6TqZL", "pdf_link": "https://openreview.net/pdf?id=hUdLs6TqZL", "keywords": "Text-to-image diffusion model, Continual Learning, Personalization", "abstract": "Personalized text-to-image diffusion models have grown popular for their ability to efficiently acquire a new concept from user-defined text descriptions and a few images. However, in the real world, a user may wish to personalize a model on multiple concepts but one at a time, with no access to the data from previous concepts due to storage/privacy concerns. When faced with this continual learning (CL) setup, most personalization methods fail to find a balance between acquiring new concepts and retaining previous ones -- a challenge thatcontinual personalization(CP) aims to solve. \nInspired by the successful CL methods that rely on class-specific information for regularization, we resort to the  inherent class-conditioned density estimates, also known asdiffusion classifier(DC) scores, for CP of text-to-image diffusion models. \nNamely, we propose using DC scores for regularizing the parameter-space and function-space of text-to-image diffusion models.\nUsing several diverse evaluation setups, datasets, and  metrics, we show that our proposed regularization-based CP methods outperform the state-of-the-art C-LoRA, and other baselines. Finally, by operating in the replay-free CL setup and on low-rank adapters, our method incurs zero storage and parameter overhead, respectively, over the state-of-the-art.", "title_embedding_index": 17760, "title_abs_embedding_index": 17785}, {"title": "Training Language Models to Self-Correct via Reinforcement Learning", "link_suffix": "/forum?id=CjwERcAU7w", "link": "https://openreview.net/forum?id=CjwERcAU7w", "pdf_link": "https://openreview.net/pdf?id=CjwERcAU7w", "keywords": "language models, reinforcement learning", "abstract": "Self-correction is a highly desirable capability of large language models (LLMs), yet it has consistently been found to be largely ineffective in modern LLMs.  Current methods for training self-correction typically depend on either multiple models, a more advanced model, or additional forms of supervision. To address these shortcomings, we develop a multi-turn online reinforcement learning (RL) approach, SCoRe, that significantly improves an LLM's self-correction ability using entirely self-generated data. To build SCoRe, we first show that variants of supervised fine-tuning (SFT) on offline model-generated correction traces are insufficient for instilling self-correction behavior. In particular, we observe that training via SFT either suffers from a distribution mismatch between the training data and the model's own responses or implicitly prefers only a certain mode of correction behavior that is often not effective at test time. SCoRe addresses these challenges by training under the model's own distribution of self-generated correction traces and using appropriate regularization to steer the learning process into learning a self-correction strategy that is effective at test time as opposed to simply fitting high-reward responses for a given prompt. This regularization prescribes running a first phase of RL on a base model to generate a policy initialization that is less susceptible to collapse  and then using a reward bonus to amplify self-correction during training.  When applied to Gemini 1.0 Pro and 1.5 Flash models, we find that SCoRe achieves state-of-the-art self-correction performance, improving the base models' self-correction by 15.6% and 9.1% respectively on the MATH and HumanEval benchmarks.", "title_embedding_index": 17761, "title_abs_embedding_index": 17786}, {"title": "Optimal Memorization Capacity of Transformers", "link_suffix": "/forum?id=UGVYezlLcZ", "link": "https://openreview.net/forum?id=UGVYezlLcZ", "pdf_link": "https://openreview.net/pdf?id=UGVYezlLcZ", "keywords": "Transformer, Self-Attention, Memorization, Contextual Mapping, Permutation Equivariance", "abstract": "Recent research in the field of machine learning has increasingly focused on the memorization capacity of Transformers, but how efficient they are is not yet well understood.\nWe demonstrate that Transformers can memorize labels with $\\tilde{O}(\\sqrt{N})$ parameters in a next-token prediction setting for $N$ input sequences of length $n$, which is proved to be optimal up to logarithmic factors.\nThis indicates that Transformers can efficiently perform memorization with little influence from the input length $n$ owing to the benefit of parameter sharing.\nWe also analyze the memorization capacity in the sequence-to-sequence setting, and find that $\\tilde{O}(\\sqrt{nN})$ parameters are not only sufficient, but also necessary at least for Transformers with hardmax.\nThese results suggest that while self-attention mechanisms can efficiently identify input sequences, the feed-forward network becomes a bottleneck when associating a label to each token.", "title_embedding_index": 17762, "title_abs_embedding_index": 17787}, {"title": "Playing the Fool: Jailbreaking Large Language Models with Out-of-Distribution Strategies", "link_suffix": "/forum?id=rgiIZ3pcZY", "link": "https://openreview.net/forum?id=rgiIZ3pcZY", "pdf_link": "https://openreview.net/pdf?id=rgiIZ3pcZY", "keywords": "Large Language Models, Multimodal Large Language Models, Safety, Jailbreak Attacks", "abstract": "Despite the remarkable versatility of Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to generalize across both language and vision tasks, LLMs and MLLMs have shown vulnerability to jailbreaking, generating textual outputs that undermine safety, ethical, and bias standards when exposed to harmful or sensitive inputs. With the recent advancement of safety-alignment via preference-tuning from human feedback, LLMs and MLLMs have been equipped with safety guardrails to yield safe, ethical, and fair responses with regard to harmful inputs. However, despite the significance of safety-alignment, research on the vulnerabilities remains largely underexplored. In this paper, we investigate the vulnerability of the safety-alignment, examining its ability to consistently provide safety guarantees for out-of-distribution(OOD)-ifying harmful inputs that may fall outside the aligned data distribution. Our key observation is that OOD-ifying the vanilla harmful inputs highly increases the uncertainty of the model to discern the malicious intent within the input, leading to a higher chance of being jailbroken. Exploiting this vulnerability, we propose JOOD, a new Jailbreak strategy via generating OOD-ifying inputs beyond the safety-alignment with diverse visual and textual transformation techniques. Specifically, even simple mixing-based techniques such as image mixup prove highly effective in OOD-ifying the harmful inputs by increasing the uncertainty of the model, thereby facilitating the bypass of the safety-alignment. Experimental results across diverse jailbreak scenarios demonstrate that JOOD effectively jailbreaks recent proprietary LLMs and MLLMs such as GPT-4 and GPT-4V with high attack success rate, which previous attack approaches have consistently struggled to jailbreak.", "title_embedding_index": 17763, "title_abs_embedding_index": 17788}, {"title": "Query-Aware Learnable Graph Pooling Tokens as Prompt for Large Language Models", "link_suffix": "/forum?id=2mg5FvBz0J", "link": "https://openreview.net/forum?id=2mg5FvBz0J", "pdf_link": "https://openreview.net/pdf?id=2mg5FvBz0J", "keywords": "Graph Neural Network, Large Language Model, Continuous Prompting, Sf", "abstract": "Graph-structured data plays a vital role in numerous domains, such as social networks, citation networks, commonsense reasoning graphs and knowledge graphs. While graph neural networks have been employed for graph processing, recent advancements have explored integrating large language models for graph-based tasks. In this paper, we propose a novel approach named Learnable Graph Pooling Token (LGPT), which addresses the limitations of the scalability issues in node-level projection and information loss in graph-level projection. LGPT enables flexible and efficient graph representation by introducing learnable parameters that act as tokens in large language models, balancing fine-grained and global graph information. Additionally, we investigate an Early Query Fusion technique, which fuses query context before constructing the graph representation, leading to more effective graph embeddings. Our method achieves a 4.13% performance improvement on the GraphQA benchmark without training the large language model, demonstrating significant gains in handling complex textual-attributed graph data.", "title_embedding_index": 17764, "title_abs_embedding_index": 17789}, {"title": "Edge-aware Image Smoothing with Relative Wavelet Domain Representation", "link_suffix": "/forum?id=0UO1mH3Iwv", "link": "https://openreview.net/forum?id=0UO1mH3Iwv", "pdf_link": "https://openreview.net/pdf?id=0UO1mH3Iwv", "keywords": "Image smoothing, Wavelet transformation, Relative wavelet domain representation, Edge-preserving, Non-convex optimization", "abstract": "Image smoothing is a fundamental technique in image processing, designed to eliminate perturbations and textures while preserving dominant structures. It plays a pivotal role in numerous high-level computer vision tasks. More recently, both traditional and deep learning-based smoothing methods have been developed. However, existing algorithms frequently encounter issues such as gradient reversals and halo artifacts. Furthermore, the smoothing strength of deep learning-based models, once trained, cannot be adjusted for adapting different complexity levels of textures.  These limitations stem from the inability of previous approaches to achieve an optimal balance between smoothing intensity and edge preservation. Consequently, image smoothing while maintaining edge integrity remains a significant challenge. To address these challenges, we propose a novel edge-aware smoothing model that leverages a relative wavelet domain representation. Specifically, by employing wavelet transformation, we introduce a new measure, termed Relative Wavelet Domain Representation (RWDR), which effectively distinguishes between textures and structures. Additionally, we present an innovative edge-aware scale map that is incorporated into the adaptive bilateral filter, facilitating mutual guidance in the smoothing process. This paper provides complete theoretical derivations for solving the proposed non-convex optimization model. Extensive experiments substantiate that our method has a competitive superiority with previous algorithms in edge-preserving and artifact removal. Visual and numerical comparisons further validate the effectiveness and efficiency of our approach in several applications of image smoothing.", "title_embedding_index": 17765, "title_abs_embedding_index": 17790}, {"title": "Decompose the model: mechanistic interpretability in image models with generalized integrated gradients (GIG)", "link_suffix": "/forum?id=avlfmW32qO", "link": "https://openreview.net/forum?id=avlfmW32qO", "pdf_link": "https://openreview.net/pdf?id=avlfmW32qO", "keywords": "explainable ai, interpretability, mechanistic interpretability", "abstract": "In the field of eXplainable AI (XAI) in language models, the progression from local explanations of individual decisions to global explanations with high-level concepts has laid the groundwork for mechanistic interpretability, which aims to decode the exact operations.\nHowever, this paradigm has not been adequately explored in image models, where existing methods have primarily focused on class-specific interpretations.\nThis paper introduces a novel approach to systematically trace the entire pathway from input through all intermediate layers to the final output within the whole dataset.\nWe utilize Pointwise Feature Vectors (PFVs) and instance-specific Effective Receptive Fields (iERFs) to decompose model embeddings into interpretable Concept Vectors.\nThen, we calculate the relevance between concept vectors with our Generalized Integrated Gradients (GIG), enabling a comprehensive, dataset-wide analysis of model behavior.\nWe validate our method of concept extraction and concept attribution in both qualitative and quantitative evaluations.\nOur approach advances the understanding of semantic significance within image models, offering a holistic view of their operational mechanics.https://iclr2025gig.netlify.app/graph_visualization.html", "title_embedding_index": 17766, "title_abs_embedding_index": 17791}, {"title": "Action Sequence Augmentation for Action Anticipation", "link_suffix": "/forum?id=f3CdjpPkSq", "link": "https://openreview.net/forum?id=f3CdjpPkSq", "pdf_link": "https://openreview.net/pdf?id=f3CdjpPkSq", "keywords": "Sequence Augmentation, Action Anticipation, Context-free Grammar", "abstract": "Action anticipation models require an understanding of temporal action patterns and dependencies to predict future actions from previous events. The key challenges arise from the vast number of possible action sequences, given the flexibility in action ordering and the interleaving of multiple goals. Since only a subset of such action sequences are present in action anticipation datasets, there is an inherent ordering bias in them. Another challenge is the presence of noisy input to the models due to erroneous action recognition or other upstream tasks. This paper addresses these challenges by introducing a novel data augmentation strategy that separately augments observed action sequences and next actions. To address biased action ordering, we introduce a grammar induction algorithm that derives a powerful context-free grammar from action sequence data. We also develop an efficient parser to generate plausible next-action candidates beyond the ground truth. For noisy input, we enhance model robustness by randomly deleting or replacing actions in observed sequences. Our experiments on the 50Salads, EGTEA Gaze+, and Epic-Kitchens-100 datasets demonstrate significant performance improvements over existing state-of-the-art methods.", "title_embedding_index": 17767, "title_abs_embedding_index": 17792}, {"title": "Genshin: General Shield for Natural Language Processing with Large Language Models", "link_suffix": "/forum?id=9XprjIqkBI", "link": "https://openreview.net/forum?id=9XprjIqkBI", "pdf_link": "https://openreview.net/pdf?id=9XprjIqkBI", "keywords": "large language model, texual attack, interpretable machine learning", "abstract": "Large language models (LLMs) like ChatGPT, Gemini, or LLaMA have been trending recently, demonstrating considerable advancement and generalizability power in countless domains. However, LLMs create an even bigger black box exacerbating opacity, with interpretability limited to few approaches. The uncertainty and opacity embedded in LLMs' nature restrict their application in high-stakes domains like financial fraud, phishing, etc. Current approaches mainly rely on traditional textual classification with posterior interpretable algorithms, suffering from attackers who may create versatile adversarial samples to break the system's defense, forcing users to make trade-offs between efficiency and robustness. To address this issue, we propose a novel cascading framework called Genshin (General Shield for Natural Language Processing with Large Language Models), utilizing LLMs as defensive one-time plug-ins. Unlike most applications of LLMs that try to transform text into something new or structural, Genshin uses LLMs to recover text to its original state. Genshin aims to combine the generalizability of the LLM, the discrimination of the median model, and the interpretability of the simple model. Our experiments on the task of sentimental analysis and spam detection have shown fatal flaws of the current median models and exhilarating results on LLMs' recovery ability, demonstrating that Genshin is both effective and efficient. In our ablation study, we unearth several intriguing observations. Utilizing the LLM defender, a tool derived from the 4th paradigm, we have reproduced BERT's 15% optimal mask rate results in the 3rd paradigm of NLP. Additionally, when employing the LLM as a potential adversarial tool, attackers are capable of executing effective attacks that are nearly semantically lossless. We conduct detailed case analyses using the SHAP interpreter, which could yield insights for systemic enhancements. Lastly, we provide discussions on the architecture of Genshin, underscoring the necessity of each component and outlining the current limitations.", "title_embedding_index": 17768, "title_abs_embedding_index": 17793}, {"title": "MindGrapher: Dynamic-Aware fMRI-to-Video Reconstruction", "link_suffix": "/forum?id=JfKF7Pdigi", "link": "https://openreview.net/forum?id=JfKF7Pdigi", "pdf_link": "https://openreview.net/pdf?id=JfKF7Pdigi", "keywords": "fMRI; fMRI-to-Video; Dynamic-Aware; Video Reconstruction from Brain Activities;", "abstract": "Existing methods for fMRI-to-video reconstruction typically focus on accurately reconstructing visual content ($i.e.$, appearance), neglecting dynamic event information. However, as highlighted in cognitive neurology, these key dynamic events significantly influence brain signal changes during video perception. In this article, we introduce Mindgrapher, a two-stream framework designed to address this gap by enhancing the reconstruction of dynamic-aware videos from fMRI data. Mindgrapher comprises $i)$ a visual content reconstruction stream, that improves the accuracy of the reconstructed visual content from sparsely distributed fMRI data through a temporal dynamics enrichment approach and multi-moment multimodal contrastive learning; $ii)$ a dynamics injection stream, that firstly crafts dynamic-aware fMRI embeddings and then integrates them into the reconstruction process via a fine-grained approach, thereby producing videos that effectively perceive dynamic events. Moreover, to address the lack of suitable metrics for evaluating dynamic event information, we introduce a new evaluation metric named dynamic content fidelity (DCF), which measures how accurately dynamic events within the video are reconstructed. Upon evaluation with a publicly available fMRI dataset, Mindgrapher outperforms the state-of-the-arts on all metrics, $i.e.$, semantic classification accuracy, structural similarity index, and DCF. The reconstructed video results are available on our web page. Code shall be released.", "title_embedding_index": 17769, "title_abs_embedding_index": 17794}, {"title": "TIEM: Enhancing Explanation of Video Prediction via Temporal Dynamics-Focused Dual Perturbation", "link_suffix": "/forum?id=TEjXRrhqtJ", "link": "https://openreview.net/forum?id=TEjXRrhqtJ", "pdf_link": "https://openreview.net/pdf?id=TEjXRrhqtJ", "keywords": "XAI, visual explanation, extremal mask, dual perturbation, video prediction", "abstract": "Explaining video data predictions is challenging due to the complex spatio-temporal information in videos. In particular, the existing perturbation-based methods for video interpretation often fail to consider different temporal contexts, making them ineffective for dynamic videos where the important regions change rapidly or appear ephemerally across frames. To address this, we propose a novel video interpretation method, time importance score-aware extremal perturbation masks (TIEM), that enhances explainability by focusing on temporal dynamics in videos. TIEM exploits a dual perturbation process: first, it evaluates temporal importance across frames via temporal perturbation and then generates spatio-temporal extremal perturbation masks using the temporal importance explicitly. Our experimental results demonstrate that TIEM resolves the key challenges of the existing methods, providing more precise explanations across the time domain in synthetic white-box models and black-box models for real-world videos.", "title_embedding_index": 17770, "title_abs_embedding_index": 17795}, {"title": "FoundTS: Comprehensive and Unified Benchmarking of Foundation Models for Time Series Forecasting", "link_suffix": "/forum?id=B4OaA0aJ4Z", "link": "https://openreview.net/forum?id=B4OaA0aJ4Z", "pdf_link": "https://openreview.net/pdf?id=B4OaA0aJ4Z", "keywords": "Time Series Forecasting, Foundation Model, Benchmark", "abstract": "Time Series Forecasting (TSF) is key functionality in numerous fields, including in finance, weather services, and energy management. While TSF methods are emerging these days, many of them require domain-specific data collection and model training and struggle with poor generalization performance on new domains. Foundation models aim to overcome this limitation. Pre-trained on large-scale language or time series data, they exhibit promising inferencing capabilities in new or unseen data. This has spurred a surge in new TSF foundation models. We propose a new benchmark, $\\texttt{FoundTS}$, to enable thorough and fair evaluation and comparison of such models. $\\texttt{FoundTS}$ covers a variety of TSF foundation models, including those based on large language models and those pretrained on time series. Next, $\\texttt{FoundTS}$ supports different forecasting strategies, including zero-shot, few-shot, and full-shot, thereby facilitating more thorough evaluations. Finally, $\\texttt{FoundTS}$ offers a pipeline that standardizes evaluation processes such as dataset splitting, loading, normalization, and few-shot sampling, thereby facilitating fair evaluations. Building on this, we report on an extensive evaluation of TSF foundation models on a broad range of datasets from diverse domains and with different statistical characteristics. Specifically, we identify pros and cons and inherent limitations of existing foundation models, and we identify directions for future model design. We make our code and datasets available athttps://anonymous.4open.science/r/FoundTS-C2B0.", "title_embedding_index": 17771, "title_abs_embedding_index": 17796}, {"title": "MILCA: Multiple Instance Learning using Counting and Attention", "link_suffix": "/forum?id=YCdag94iZs", "link": "https://openreview.net/forum?id=YCdag94iZs", "pdf_link": "https://openreview.net/pdf?id=YCdag94iZs", "keywords": "Multiple Instance Learning, counting, attention", "abstract": "In Multiple Instance Learning (MIL), a bag is comprised of instances and the label is prescribed to the whole bag, with no information on the labels of each instance.  The leading approaches for MIL are Embedded Space (ES) solutions, where the full bag is embedded into a vector space. We show that features are often associated with a class, and a simple counting/summing algorithm of such features leads to similar or better accuracy than current advanced machine-learning-based solutions.  This can be improved in some cases by weighting these selected features using a fully connected network to predict the coefficient of each feature. However, a simple relative contribution of each feature, where the sum of the coefficients is normalized to 1,  fails to count the feature. Thus instead, we replace the softmax by a projection of the coefficients to [-1,1] or [0,1] but do not limit their sum. This allows the model to count features. The resulting algorithm - MILCA (Multiple Instance Learning using Counting and Attention) is applied to multiple previous and new real-world MIL tasks, as well as a task of recovering the host disease history from sequenced T Cell Receptor Repertoires. In the majority of cases,  MILCA is significantly better and way more efficient than currently used MIL algorithms, with a 3 % higher accuracy than current SOTA on average. The code for MILCA is available at: github.com/submissionanonymous6/MILCA", "title_embedding_index": 17772, "title_abs_embedding_index": 17797}, {"title": "Optimal Brain Apoptosis", "link_suffix": "/forum?id=88rjm6AXoC", "link": "https://openreview.net/forum?id=88rjm6AXoC", "pdf_link": "https://openreview.net/pdf?id=88rjm6AXoC", "keywords": "Network Pruning, Efficient Maching Learning, Hessian Matrix", "abstract": "The increasing complexity and parameter count of Convolutional Neural Networks (CNNs) and Transformers pose challenges in terms of computational efficiency and resource demands. Pruning has been identified as an effective strategy to address these challenges by removing redundant elements such as neurons, channels, or connections, thereby enhancing computational efficiency without heavily compromising performance. This paper builds on the foundational work of Optimal Brain Damage (OBD) by advancing the methodology of parameter importance estimation using the Hessian matrix. Unlike previous approaches that rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel pruning method that calculates the Hessian-vector product value directly for each parameter. By decomposing the Hessian matrix across network layers and identifying conditions under which inter-layer Hessian submatrices are non-zero, we propose a highly efficient technique for computing the second-order Taylor expansion of parameters. This approach allows for a more precise pruning process, particularly in the context of CNNs and Transformers, as validated in our experiments including VGG19, ResNet32, ResNet50, and ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code will be available soon.", "title_embedding_index": 17773, "title_abs_embedding_index": 17798}, {"title": "SlotSAM: Bootstrap Segmentation Foundation Model under Real-world Shifts via Object-Centric Learning", "link_suffix": "/forum?id=Pq2yEKXOl7", "link": "https://openreview.net/forum?id=Pq2yEKXOl7", "pdf_link": "https://openreview.net/pdf?id=Pq2yEKXOl7", "keywords": "segmentation foundation model, distribution shift, object-centric learning, weakly supervised", "abstract": "Foundation models have made incredible strides in achieving zero-shot or few-shot generalization, leveraging prompt engineering to mimic the problem-solving approach of human intelligence. However, when it comes to some foundation models like Segment Anything, there is still a challenge in performing well under real-world shifts. One of the real-world shifts is the distribution shift, the out-of-distribution data, such as camouflaged and medical images. Another is inconsistent prompting strategies during fine-tuning and testing, leading to decreased performance. We draw inspiration from human intelligence, particularly the process by which individuals decompose scenes into components in unfamiliar environments to determine the positions or boundaries of each component. To this end, we introduce SlotSAM, a method that reconstructs features from the encoder in a self-supervised manner to create object-centric representations. These representations are then integrated into the foundation model, bolstering its object-level perceptual capabilities while reducing the impact of distribution-related variables. The beauty of SlotSAM lies in its simplicity and adaptability to various tasks, making it a versatile solution that significantly enhances the generalization abilities of foundation models. Through limited parameter fine-tuning in a bootstrap manner, our approach paves the way for improved generalization in novel environments.", "title_embedding_index": 17774, "title_abs_embedding_index": 17799}]