[{"title": "From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients", "link_suffix": "/forum?id=OjP6LUrw1O", "link": "https://openreview.net/forum?id=OjP6LUrw1O", "pdf_link": "https://openreview.net/pdf?id=OjP6LUrw1O", "keywords": "Large language models, LLM finetuning, Memory-efficient training, Optimization, Low Rank Compression", "abstract": "Modern Large Language Models (LLMs) are composed of matrices with billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Being significantly large, such matrices can often be expressed in low-rank format with potential to relax resource requirements. Unlike prior works which focus on developing novel matrix decomposition algorithms, in this work we first study the emergence of low-rank structures across matrices within different layers of LLMs and establish a consequential relationship between the gradient dynamics and emerging low-rank expressiveness of matrices. Our findings reveal that different layers exhibit varying levels of converged low-rank structure, necessitating a non-uniform rank reduction across them to minimize performance drop due to compression. In view of that, we present Weight Low-Rank Projection (WeLore) that unifies weight compression and memory-efficient fine-tuning as ONE, in a data-agnostic and one-shot way. WeLore capitalizes the heavy-tail distribution of singular values to identify a suitable rank reduction ratio for matrices within LLMs. Going beyond only as a compression technique, WeLore categorizes weight matrices into Low-rank Components (LRCs) and Non-Low-rank Components (N-LRCs) based on their ability to express themselves as low-rank. Our gradient perspective and extensive experiments illustrate that LRCs tend to have better finetuning capabilities and can closely mimic (sometimes outperform) the training loss trajectory and performance of full-finetuning with notable memory and compute footprint reduction. For example, finetuning a 50% compressed LLaMa-2 7B model using only a fraction of parameters in LRCs (WeLore) can outperform its full finetuning with ~3x better throughput and ~0.6x GPU requirement.", "title_embedding_index": 5050, "title_abs_embedding_index": 5075}, {"title": "Tighter Performance Theory of FedExProx", "link_suffix": "/forum?id=q2VK1Z8XFo", "link": "https://openreview.net/forum?id=q2VK1Z8XFo", "pdf_link": "https://openreview.net/pdf?id=q2VK1Z8XFo", "keywords": "optimization, federated learning, proximal methods", "abstract": "We revisit FedExProx -- a recently proposed distributed optimization method designed to enhance convergence properties of parallel proximal algorithms via extrapolation. In the process, we uncover a surprising flaw: its known theoretical guarantees on quadratic optimization tasks are no better than those offered by the vanilla Gradient Descent (GD) method. Motivated by this observation, we develop a novel analysis framework, establishing a tighter linear convergence rate for non-strongly convex quadratic problems. By incorporating both computation and communication costs, we demonstrate that FedExProx can indeed provably outperform GD, in stark contrast to the original analysis. Furthermore, we consider partial participation scenarios and analyze two adaptive extrapolation strategies-based on gradient diversity and Polyak stepsizes --- again significantly outperforming previous results. Moving beyond quadratics, we extend the applicability of our analysis to general functions satisfying the Polyak-\u0141ojasiewicz condition, outperforming the previous strongly convex analysis while operating under weaker assumptions. Backed by empirical results, our findings point to a new and stronger potential of FedExProx, paving the way for further exploration of the benefits of extrapolation in federated learning.", "title_embedding_index": 5051, "title_abs_embedding_index": 5076}, {"title": "Decision-making with speculative opponent model-aided value function factorization", "link_suffix": "/forum?id=yZdPpKTO9R", "link": "https://openreview.net/forum?id=yZdPpKTO9R", "pdf_link": "https://openreview.net/pdf?id=yZdPpKTO9R", "keywords": "Decision making, Cooperative multi-agent reinforcement learning;", "abstract": "In many real-world scenarios, teams of agents must coordinate their actions while competing against opponents. Traditional multi-agent reinforcement learning (MARL) approaches often treat opponents as part of the environment, causing controlled agents to overlook the impact of their adversaries. Opponent modeling can enhance an agent\u2019s decision-making by constructing predictive models of other agents. However, existing approaches typically rely on centralized learning with access to opponent data, and the process of extracting decentralized policies becomes impractical with larger teams. To address this issue, we propose the Distributional Speculative Opponent-aided mixing framework (DSOMIX), a novel value-based speculative opponent modeling algorithm that relies solely on local information\u2014namely the agent's own observations, actions, and rewards. DSOMIX uses speculative beliefs to predict the behaviors of unseen opponents, enabling agents to make decisions based on local observations. Additionally, it incorporates distributional value decomposition models to capture a more granular representation of the agent's return distribution, improving the training process for the speculative opponent models. We formally derive a value-based theorem that underpins the training process. Extensive experiments across four challenging MARL benchmarks, including MPE and Pommerman, demonstrate that DSOMIX outperforms state-of-the-art methods, achieving superior performance and faster convergence.", "title_embedding_index": 5052, "title_abs_embedding_index": 5077}, {"title": "Can Transformers Do Enumerative Geometry?", "link_suffix": "/forum?id=4X9RpKH4Ls", "link": "https://openreview.net/forum?id=4X9RpKH4Ls", "pdf_link": "https://openreview.net/pdf?id=4X9RpKH4Ls", "keywords": "AI for Mathematics, Algebraic Geometry, Theorem Discovery, Transformers, Recursive functions, Interpretability Analysis and world model.", "abstract": "We introduce a Transformer-based approach to computational enumerative geometry, specifically targeting the computation of $\\psi$-class intersection numbers on the moduli space of curves. Traditional methods for calculating these numbers suffer from factorial computational complexity, making them impractical to use. By reformulating the problem as a continuous optimization task, we compute intersection numbers across a wide value range from $10^{-45}$ to $10^{45}$. To capture the recursive and hierarchical nature inherent in the intersection numbers, we propose the Dynamic Range Activator (DRA), a new activation function that enhances the Transformer's ability to model recursive patterns and handle severe heteroscedasticity. Given precision requirements for computing $\\psi$-class intersections, we quantify the uncertainty of the predictions using Conformal Prediction with a dynamic sliding window adaptive to the partitions of equivalent number of marked points. Beyond simply computing intersection numbers, we explore the enumerative \"world-model\" of Transformers. Our interpretability analysis reveals that the network is implicitly modeling the Virasoro constraints in a purely data-driven manner. Moreover, through abductive hypothesis testing, probing, and causal inference, we uncover evidence of an emergent internal representation of the the large-genus asymptotic of $\\psi$-class intersection numbers. These findings suggest that the network internalizes the parameters of the asymptotic closed-form formula linearly, while capturing the polynomiality phenomenon of $\\psi$-class intersection numbers in a nonlinear manner.", "title_embedding_index": 5053, "title_abs_embedding_index": 5078}, {"title": "Intrinsic Explanation of Random Subspace Method for Enhanced Security Applications", "link_suffix": "/forum?id=9YRUmPV7Jy", "link": "https://openreview.net/forum?id=9YRUmPV7Jy", "pdf_link": "https://openreview.net/pdf?id=9YRUmPV7Jy", "keywords": "Certified Defense, Feature Attribution", "abstract": "Random subspace method has wide security applications such as providing certified defenses against adversarial and backdoor attacks, and building robustly aligned LLM against jailbreaking attacks. However, the explanation of random subspace method lacks sufficient exploration. Existing state-of-the-art feature attribution methods such as Shapley value and LIME are computationally impractical and lacks security guarantee when applied to random subspace method. In this work, we propose EnsembleSHAP, an intrinsically faithful and secure feature attribution for random subspace method that reuses its computational byproducts. Specifically, our feature attribution method is 1) computationally efficient, 2) maintains essential properties of effective feature attribution (such as local accuracy), and 3) offers guaranteed protection against attacks on feature attribution methods. We perform comprehensive evaluations for our explanation's effectiveness when faced with different empirical attacks. Our experimental results demonstrates that our explanation not only faithfully reports the most important features, but also certifiably detects the harmful features embedded in the input sample.", "title_embedding_index": 5054, "title_abs_embedding_index": 5079}, {"title": "Error Slice Discovery via Manifold Compactness", "link_suffix": "/forum?id=upALuXjdxc", "link": "https://openreview.net/forum?id=upALuXjdxc", "pdf_link": "https://openreview.net/pdf?id=upALuXjdxc", "keywords": "Error Slice Discovery, Manifold Compactness, Model Evaluation", "abstract": "Despite the great performance of deep learning models in many areas, they still make mistakes and underperform on certain subsets of data, i.e. error slices. Given a trained model, it is important to identify its semantically coherent error slices that are easy to interpret, which is referred to as the error slice discovery problem. However, there is no proper metric of slice coherence without relying on extra information like predefined slice labels. The current evaluation of slice coherence requires access to predefined slices formulated by metadata like attributes or subclasses. Its validity heavily relies on the quality and abundance of metadata, where some possible patterns could be ignored. Besides, current algorithms cannot directly incorporate the constraint of coherence into their optimization objective due to the absence of an explicit coherence metric, which could potentially hinder their effectiveness. In this paper, we propose manifold compactness, a coherence metric without reliance on extra information by incorporating the data geometry property into its design, and experiments on typical datasets empirically validate the rationality of the metric. Then we develop Manifold Compactness based error Slice Discovery (MCSD), a novel algorithm that directly treats risk and coherence as the optimization objective, and is flexible to be applied to models of various tasks. Extensive experiments on the current benchmark and case studies on other typical datasets demonstrate the effectiveness of our algorithm.", "title_embedding_index": 5055, "title_abs_embedding_index": 5080}, {"title": "Online Convex Optimization with Prediction Through Accelerated Gradient Descent", "link_suffix": "/forum?id=Rdb0HxGJa3", "link": "https://openreview.net/forum?id=Rdb0HxGJa3", "pdf_link": "https://openreview.net/pdf?id=Rdb0HxGJa3", "keywords": "online optimization, accelerated gradient descent", "abstract": "We study online convex optimization with predictions, where, at each time step $t$, predictions about the next $k$ steps are available, and with coupled costs over time steps, where the cost function at time step $t$ depends on the decisions made between time $t-a$ and time $t+b$ for some nonnegative integers $a,b$.We provide a general recipe to run synchronous update in an asynchronous fashion that respects the sequential revelation of information. Combined with existing convergence results for convex optimization using inexact first-order oracle, we show that acceleration is possible in this framework, where the dynamic regret can be reduced by a factor of $(1-O(\\sqrt{\\kappa}))^{\\frac{k}{a+b}}$ through accelerated gradient descent, at a cost of an additive error term that depends on the prediction accuracy. This generalizes and improves the $(1-\\kappa/4)^k$ factor obtained by Li & Li (2020) for $a+b = 1$. Our algorithm also has smaller dependency on longer-term prediction error. Moreover, our algorithm is the first gradient based algorithm which, when the strong-convexity assumption is relaxed, constructs a solution whose regret decays at the rate of $O(1/k^2)$, at a cost of an additive error term that depends on the prediction accuracy.", "title_embedding_index": 5056, "title_abs_embedding_index": 5081}, {"title": "Interpretability-driven active feature acquisition in learning systems", "link_suffix": "/forum?id=kUWZX0Atch", "link": "https://openreview.net/forum?id=kUWZX0Atch", "pdf_link": "https://openreview.net/pdf?id=kUWZX0Atch", "keywords": "Active feature acquisition, Model interpretability", "abstract": "In real-world applications like medicine, machine learning models must often work with a limited number of features due to the high cost and time required to acquire all relevant data. While several static feature selection methods exist, they are suboptimal due to their inability to adapt to varying feature importance across different instances. A more flexible approach is active feature acquisition (AFA), which dynamically selects features based on their relevance for each individual case. Here, we introduce an AFA framework that leverages Shapley Additive explanations (SHAP) to generate instance-specific feature importance rankings. By reframing the AFA problem as a feature prediction task, we propose a policy network based on a decision transformer architecture, trained to predict the next most informative feature based on SHAP values. This method allows us to sequentially acquire features in order of their predictive significance, resulting in more efficient feature selection and acquisition. Extensive experiments across multiple datasets show that our approach achieves superior performance compared to current state-of-the-art AFA techniques, both in terms of predictive accuracy and feature acquisition efficiency. These results demonstrate the potential of SHAP-based AFA for applications where feature acquisition cost is a critical consideration, such as in disease diagnosis.", "title_embedding_index": 5057, "title_abs_embedding_index": 5082}, {"title": "AlphaIntegrator: Transformer Action Search for Symbolic Integration Proofs", "link_suffix": "/forum?id=lJdgUUcLaA", "link": "https://openreview.net/forum?id=lJdgUUcLaA", "pdf_link": "https://openreview.net/pdf?id=lJdgUUcLaA", "keywords": "symbolic, math, integration, neurosymbolic, transformer, proof, search, symbolic engine", "abstract": "We present the first correct-by-construction learning-based system for step-by-step mathematical integration. The key idea is to learn a policy, represented by a GPT transformer model, which guides the search for the right mathematical integration rule, to be carried out by a symbolic solver. Concretely, we introduce a symbolic engine with axiomatically correct actions on mathematical expressions, as well as the first dataset for step-by-step integration. Our GPT-style transformer model, trained on this synthetic data, demonstrates strong generalization by surpassing its own data generator in accuracy and efficiency, using 50% fewer search steps. Our experimental results with SoTA LLMs also demonstrate that the standard approach of fine-tuning LLMs on a set of question-answer pairs is insufficient for solving this mathematical task. This motivates the importance of discovering creative methods for combining LLMs with symbolic reasoning engines, of which our work is an instance.", "title_embedding_index": 5058, "title_abs_embedding_index": 5083}, {"title": "Large Language Models Assume People are More Rational than We Really are", "link_suffix": "/forum?id=dAeET8gxqg", "link": "https://openreview.net/forum?id=dAeET8gxqg", "pdf_link": "https://openreview.net/pdf?id=dAeET8gxqg", "keywords": "Large Language Models, Rationality, Cognitive Models, Psychology", "abstract": "In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate --- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice --- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.", "title_embedding_index": 5059, "title_abs_embedding_index": 5084}, {"title": "Learned Data Transformation: A Data-centric Plugin for Enhancing Time Series Forecasting", "link_suffix": "/forum?id=6hJ3khuJY4", "link": "https://openreview.net/forum?id=6hJ3khuJY4", "pdf_link": "https://openreview.net/pdf?id=6hJ3khuJY4", "keywords": "time series, data-centric, data transformation, forecasting, generalization, deep learning", "abstract": "Data-centric approaches in Time Series Forecasting (TSF) often involve heuristic-based operations on data. This paper proposes to find a general end-to-end data transformation that serves as a plugin to enhance any arbitrary TSF model's performance. To achieve this, we propose the Proximal Transformation Network (PTN), which learns effective transformations while maintaining proximity to the raw data to ensure fidelity. When orthogonally integrated with popular TSF models, our method helps achieve state-of-the-art performance on seven real-world datasets. Additionally, we show that the proximal transformation process can be interpreted in terms of predictability and distribution alignment among channels, highlighting the potential of data-centric methods for future research. Our code is available athttps://anonymous.4open.science/r/PTN-2FC6/.", "title_embedding_index": 5060, "title_abs_embedding_index": 5085}, {"title": "Small-to-Large Generalization: Training Data Influences Models Consistently Across Scale", "link_suffix": "/forum?id=79ZkWgY2FI", "link": "https://openreview.net/forum?id=79ZkWgY2FI", "pdf_link": "https://openreview.net/pdf?id=79ZkWgY2FI", "keywords": "data attribution", "abstract": "Choice of training data distribution greatly affects model behavior. Yet, in\nlarge-scale settings, precisely characterizinghowchanges in training\ndata influence predictions is often difficult due to model training costs.\nCurrent practice is to instead extrapolate from scaled down,\ninexpensive-to-train proxy models. However, changes in data do not influence\nsmaller and larger models identically. Therefore, understanding how choice of\ndata affects large-scale models raises the question: how does training data\ninfluence model behavior across compute scale? We find that the answer is\nnuanced. Small- and large-scale language model predictions generallydohighly correlate across choice of training data---often, even when small-model\npredictions are at the level of random guessing. However, therealsoexist\ndownstream datasets where these predictions correlate much less. Equipped with these\nfindings, we characterize how proxy scale affects performance in two downstream\nproxy model applications: data attribution and dataset selection.", "title_embedding_index": 5061, "title_abs_embedding_index": 5086}, {"title": "LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models", "link_suffix": "/forum?id=LSp4KBhAom", "link": "https://openreview.net/forum?id=LSp4KBhAom", "pdf_link": "https://openreview.net/pdf?id=LSp4KBhAom", "keywords": "3D foundation model, model specialization, robust optimization, low rank adaptation, self-supervised learning", "abstract": "Emerging 3D geometric foundation models, such as DUSt3R, offer a promising approach for in-the-wild 3D vision tasks.\nHowever, due to the high-dimensional nature of the problem space and scarcity of high-quality 3D data,\nthese pre-trained models still struggle to generalize to many challenging circumstances,\nsuch as limited view overlap or low lighting.\nTo address this, we propose LoRA3D, an efficient self-calibration pipeline tospecializethe pre-trained models to target scenes using their own multi-view predictions.\nTaking sparse RGB images as input, we leverage robust optimization techniques to refine multi-view predictions and align them into a global coordinate frame.\nIn particular, we incorporate prediction confidence into the geometric optimization process, \nautomatically re-weighting the confidence to better reflect point estimation accuracy. \nWe use the calibrated confidence to generate high-quality pseudo labels for the calibrating views and fine-tune the models using low-rank adaptation (LoRA) on the pseudo-labeled data.\nOur method does not require any external priors or manual labels. It completes the self-calibration process on asingle standard GPU within just 5 minutes.\nEach low-rank adapter requires only18MBof storage. \nWe evaluated our method onmore than 160 scenessampled from the Replica, TUM and Waymo Open datasets,\nachieving up to88% performance improvementon 3D reconstruction, multi-view pose estimation and novel-view rendering.", "title_embedding_index": 5062, "title_abs_embedding_index": 5087}, {"title": "Spatial-aware decision-making with ring attractors in Reinforcement Learning systems", "link_suffix": "/forum?id=E5ulvtj86q", "link": "https://openreview.net/forum?id=E5ulvtj86q", "pdf_link": "https://openreview.net/pdf?id=E5ulvtj86q", "keywords": "Reinforcement Learning, Computational Neuroscience, Deep Learning, Ring Attractors, Spatial Awareness, Bioinspired", "abstract": "This paper explores the integration of ring attractors, a mathematical model inspired by neural circuit dynamics, into the reinforcement learning (RL) action selection process. Ring attractors, as specialized brain-inspired structures that encode spatial information and uncertainty, offer a biologically plausible mechanism to improve learning speed and predictive performance. They do so by explicitly encoding the action space, facilitating the organization of neural activity, and enabling the distribution of spatial representations across the neural network in the context of deep RL. The application of ring attractors in the RL action selection process involves mapping actions to specific locations on the ring and decoding the selected action based on neural activity. We investigate the application of ring attractors by both building them as exogenous models and integrating them as part of a Deep Learning policy algorithm. Our results show a significant improvement in state-of-the-art models for the Atari 100k benchmark. Notably, our integrated approach improves the performance of state-of-the-art models by half, representing a 53% increase over selected baselines.", "title_embedding_index": 5063, "title_abs_embedding_index": 5088}, {"title": "Instance-Aware Graph Prompt Learning", "link_suffix": "/forum?id=VBeLiRkZMP", "link": "https://openreview.net/forum?id=VBeLiRkZMP", "pdf_link": "https://openreview.net/pdf?id=VBeLiRkZMP", "keywords": "Graph Neural Networks, Prompt Learning, Pre-training", "abstract": "Graph neural networks stand as the predominant technique for graph representation learning owing to their strong expressive power, yet the performance highly depends on the availability of high-quality labels in an end-to-end manner. Thus the pretraining and fine-tuning paradigm has been proposed to mitigate the label cost issue. Subsequently, the gap between the pretext tasks and downstream tasks has spurred the development of graph prompt learning which inserts a set of graph prompts into the original graph data with minimal parameters while preserving competitive performance. However, the current exploratory works are still limited since they all concentrate on learning fixed task-specific prompts which may not generalize well across the diverse instances that the task comprises. To tackle this challenge, we introduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aiming to generate distinct prompts tailored to different input instances. The process involves generating intermediate prompts for each instance using a lightweight architecture, quantizing these prompts through trainable codebook vectors, and employing the exponential moving average technique to ensure stable training. Extensive experiments conducted on multiple datasets and settings showcase the superior performance of IA-GPL compared to state-of-the-art baselines.", "title_embedding_index": 5064, "title_abs_embedding_index": 5089}, {"title": "Decentralized Transformers with Centralized Aggregation are Sample-Efficient Multi-Agent World Models", "link_suffix": "/forum?id=4E0lCxBD0U", "link": "https://openreview.net/forum?id=4E0lCxBD0U", "pdf_link": "https://openreview.net/pdf?id=4E0lCxBD0U", "keywords": "multi-agent reinforcement learning, world models, learning in imagination", "abstract": "Learning a world model for model-free Reinforcement Learning (RL) agents can significantly improve the sample efficiency by learning policies in imagination. However, building a world model for Multi-Agent RL (MARL) can be particularly challenging due to the scalability issue in a centralized architecture arising from a large number of agents, and also the non-stationarity issue in a decentralized architecture stemming from the inter-dependency among agents. To address both challenges, we propose a novel world model for MARL that learns decentralized local dynamics for scalability, combined with a centralized representation aggregation from all agents. We cast the dynamics learning as an auto-regressive sequence modeling problem over discrete tokens by leveraging the expressive Transformer architecture, in order to model complex local dynamics across different agents and provide accurate and consistent long-term imaginations. As the first pioneering Transformer-based world model for multi-agent systems, we introduce a Perceiver Transformer as an effective solution to enable centralized representation aggregation within this context. Main results on Starcraft Multi-Agent Challenge (SMAC) and additional results on MAMujoco show that it outperforms strong model-free approaches and existing model-based methods in both sample efficiency and overall performance.", "title_embedding_index": 5065, "title_abs_embedding_index": 5090}, {"title": "GAQAT: Gradient-Adaptive Quantization-Aware Training for Domain Generalization", "link_suffix": "/forum?id=ZCPtUkkkUU", "link": "https://openreview.net/forum?id=ZCPtUkkkUU", "pdf_link": "https://openreview.net/pdf?id=ZCPtUkkkUU", "keywords": "Model Quantization; Domain Generation; Sharpness-Aware Minimization", "abstract": "Research on loss surface geometry, such as Sharpness-Aware Minimization (SAM), shows that flatter minima improve generalization. Recent studies further reveal that flatter minima can also reduce the domain generalization (DG) gap. However, existing flatness-based DG techniques predominantly operate within a full-precision training process, which is impractical for deployment on resource-constrained edge devices that typically rely on lower bit-width representations (e.g., 4 bits, 3 bits). Consequently, low-precision quantization-aware training is critical for optimizing these techniques in real-world applications.\nIn this paper, we observe a significant degradation in performance when applying state-of-the-art DG-SAM methods to quantized models, suggesting that current approaches fail to preserve generalizability during the low-precision training process. To address this limitation, we propose a novel Gradient-Adaptive Quantization-Aware Training (GAQAT) framework for DG. \nOur approach begins by identifying the scale-gradient conflict problem in low-precision quantization, where the task loss and smoothness loss induce conflicting gradients for the scaling factors of quantizers, with certain layers exhibiting opposing gradient directions. This conflict renders the optimization of quantized weights highly unstable. To mitigate this, we further introduce a mechanism to quantify gradient inconsistencies and selectively freeze the gradients of scaling factors, thereby stabilizing the training process and enhancing out-of-domain generalization.\nExtensive experiments validate the effectiveness of the proposed GAQAT framework. On PACS, both 3-bit and 4-bit exceed the baseline by nearly 4.5%, and on DomainNet, 4-bit achieves performance close to full precision.", "title_embedding_index": 5066, "title_abs_embedding_index": 5091}, {"title": "Enhancing Logits Distillation with Plug&Play Kendall's\u03c4Ranking Loss", "link_suffix": "/forum?id=BMqBvRPDhX", "link": "https://openreview.net/forum?id=BMqBvRPDhX", "pdf_link": "https://openreview.net/pdf?id=BMqBvRPDhX", "keywords": "Knowledge Distillation, Kendall's tau Coefficient, Ranking Loss", "abstract": "Knowledge distillation typically employs the Kullback-Leibler (KL) divergence to constrain the output of the student model to precisely match the soft labels provided by the teacher model. However, the optimization process of KL divergence is challenging for the student and prone to suboptimal points. Also, we demonstrate that the gradients provided by KL divergence depend on channel scale and thus tend to overlook low-probability channels. The mismatch in low-probability channels also results in the neglect of inter-class relationship information, making it difficult for the student to further enhance performance. To address this issue, we propose an auxiliary ranking loss based on Kendall\u2019s $\\tau$ Coefficient, which can be plug-and-play in any logit-based distillation method, providing inter-class relationship information and balancing the attention to low-probability channels. We show that the proposed ranking loss is less affected by channel scale, and its optimization objective is consistent with that of KL divergence. Extensive experiments on CIFAR-100, ImageNet, and COCO datasets, as well as various CNN and ViT teacher-student architecture combinations, demonstrate that the proposed ranking loss can be plug-and-play on various baselines and enhance their performance.", "title_embedding_index": 5067, "title_abs_embedding_index": 5092}, {"title": "Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation", "link_suffix": "/forum?id=BkwCrIsTbR", "link": "https://openreview.net/forum?id=BkwCrIsTbR", "pdf_link": "https://openreview.net/pdf?id=BkwCrIsTbR", "keywords": "Large Language Models, Long Context, Instruction-Tuning Data", "abstract": "Large Language Models (LLMs) struggle with long-context reasoning, not only due to the quadratic scaling of computational complexity with sequence length but also because of the scarcity and expense of annotating long-context data. There has been barely any open-source work that systematically ablates long-context data, nor is there any openly available instruction tuning dataset with contexts surpassing 100K tokens. To bridge this gap, we introduce a novel post-training synthetic data generation strategy designed to efficiently extend the context window of LLMs while preserving their general task performance. Our approach scalably extends to arbitrarily long context lengths, unconstrained by the length of available real-world data, which effectively addresses the scarcity of raw long-context data. \nThrough a step-by-step rotary position embedding (RoPE) scaling training strategy, we demonstrate that our model, with a context length of up to 1M tokens, performs well on the RULER benchmark and InfiniteBench and maintains robust performance on general language tasks.", "title_embedding_index": 5068, "title_abs_embedding_index": 5093}, {"title": "Astral: training physics-informed neural networks with error majorants", "link_suffix": "/forum?id=btaxn8Xce6", "link": "https://openreview.net/forum?id=btaxn8Xce6", "pdf_link": "https://openreview.net/pdf?id=btaxn8Xce6", "keywords": "a posteriori error analysis, functional error estimate, PiNN, physics-informed neural network, scientific computing, uncertainty quantification, PDE", "abstract": "The primal approach to physics-informed learning is a residual minimization. We argue that residual is, at best, an indirect measure of the error of approximate solution and propose to train with error majorant instead. Since error majorant provides a direct upper bound on error, one can reliably estimate how close PiNN is to the exact solution and stop the optimization process when the desired accuracy is reached. We call loss function associated with error majorantAstral: neurAl a poSTerioRi functionAlLoss. To compare Astral and residual loss functions, we illustrate how error majorants can be derived for various PDEs and conduct experiments with diffusion equations (including anisotropic and in the L-shaped domain), convection-diffusion equation, temporal discretization of Maxwell's equation, magnetostatics and nonlinear elastoplasticity problems. The results indicate that Astral loss is competitive to the residual loss, typically leading to faster convergence and lower error (e.g., for Maxwell's equations, we observe an order of magnitude better relative error and training time). The main benefit of using Astral loss comes from its ability to estimate error, which is impossible with other loss functions. Our experiments indicate that the error estimate obtained with Astral loss is usually tight enough, e.g., for a highly anisotropic equation, on average, Astral overestimates error by a factor of $1.5$, and for convection-diffusion by a factor of $1.7$. We further demonstrate that Astral loss is better correlated with error than residual and is a more reliable predictor (in a statistical sense) of the error value. Moreover, unlike residual, the error indicator obtained from Astral loss has a superb spatial correlation with error. Backed with the empirical and theoretical results, we argue that one can productively use Astral loss to perform reliable error analysis and approximate PDE solutions with accuracy similar to standard residual-based techniques.", "title_embedding_index": 5069, "title_abs_embedding_index": 5094}, {"title": "A Dual-Fusion Cognitive Diagnosis Framework for Open Student Learning Environments", "link_suffix": "/forum?id=iucVyVC8jQ", "link": "https://openreview.net/forum?id=iucVyVC8jQ", "pdf_link": "https://openreview.net/pdf?id=iucVyVC8jQ", "keywords": "Cognitive Diagnosis, Open Student Learning Environments, Inductive Learning, Intelligent Education", "abstract": "Cognitive diagnosis model (CDM) is a fundamental and upstream component in intelligent education. It aims to infer students' mastery levels based on historical response logs. However, existing CDMs usually follow the ID-based embedding paradigm, which could often diminish the effectiveness of CDMs in open student learning environments. This is mainly because they can hardly directly infer new students' mastery levels or utilize new exercises or knowledge without retraining. Textual semantic information, due to its unified feature space and easy accessibility, can help alleviate this issue. Unfortunately, directly incorporating semantic information may not benefit CDMs, since it does not capture response-relevant features and thus discards the individual characteristics of each student. To this end, this paper proposes a dual-fusion cognitive diagnosis framework (DFCD) to address the challenge of aligning two different modalities, i.e., textual semantic features and response-relevant features. Specifically, in DFCD, we first propose the exercise-refiner and concept-refiner to make the exercises and knowledge concepts more coherent and reasonable via large language models. Then, DFCD encodes the refined features using text embedding models to obtain the semantic information. For response-related features, we propose a novel response matrix to fully incorporate the information within the response logs. Finally, DFCD designs a dual-fusion module to merge the two modal features. The ultimate representations possess the capability of inference in open student learning environments and can be also plugged in existing CDMs. Extensive experiments across real-world datasets show that DFCD achieves superior performance by integrating different modalities and strong adaptability in open student learning environments.", "title_embedding_index": 5070, "title_abs_embedding_index": 5095}, {"title": "Label Informativeness-based Minority Oversampling in Graphs (LIMO)", "link_suffix": "/forum?id=xEDB5sSIK0", "link": "https://openreview.net/forum?id=xEDB5sSIK0", "pdf_link": "https://openreview.net/pdf?id=xEDB5sSIK0", "keywords": "class imbalance, graph neural networks, mutual information, label informativeness", "abstract": "Class imbalance is a pervasive issue in many real-world datasets, particularly in graph-structured data, where certain classes are significantly underrepresented. This imbalance can severely impact the performance of Graph Neural Networks (GNNs), leading to biased learning or over-fitting. The existing oversampling techniques often overlook the intrinsic properties of graphs, such as Label Informativeness (LI), which measures the amount of information a neighbor's label provides about a node's label. To address this, we propose Label Informativeness-based Minority Oversampling (LIMO), a novel algorithm that strategically oversamples minority class nodes by augmenting edges to maximize LI. This technique generates a balanced, synthetic graph that enhances GNN performance without significantly increasing data volume. Our theoretical analysis shows that the effectiveness of GNNs is directly proportional to label informativeness, with mutual information as a mediator. Additionally, we provide insights into how variations in the number of inter-class edges influence the LI by analyzing its derivative. Experimental results on various homophilous and heterophilous benchmark datasets demonstrate the effectiveness of LIMO in improving the performance on node classification for different imbalance ratios, with particularly significant improvements observed in heterophilous graph datasets. Our code is available at \\url{https://anonymous.4open.science/r/limo-1A36/}", "title_embedding_index": 5071, "title_abs_embedding_index": 5096}, {"title": "Continual Memorization of Factoids in Large Language Models", "link_suffix": "/forum?id=2gW8lTRh9m", "link": "https://openreview.net/forum?id=2gW8lTRh9m", "pdf_link": "https://openreview.net/pdf?id=2gW8lTRh9m", "keywords": "Continual Learning, Large Language Model, Memorization", "abstract": "Large language models (LLMs) can absorb a massive amount of knowledge through pretraining, but pretraining is inefficient for acquiring long-tailed or specialized facts. Therefore, fine-tuning on specialized or new knowledge that reflects changes in the world has become popular, though it risks disrupting the model\u2019s original capabilities. We study this fragility in the context of continual memorization, where the model is trained on a small set of long-tail factoids (subject-relation-object associations) and must retain these factoids after multiple stages of subsequent training on other datasets. Continual memorization focuses on the specific challenge of retaining long-tail factoids, whereas general continual learning aims to maintain the LLM\u2019s capabilities across a wide range of generic tasks (e.g., reasoning, commonsense knowledge). Through extensive experiments, we show that LLMs suffer from forgetting across a wide range of subsequent tasks, and simple replay techniques do not fully prevent forgetting, especially when the factoid datasets are trained in the later stages. We posit that there are two ways to alleviate forgetting: 1) protect the memorization process as the model learns the factoids, or 2) reduce interference from training in later stages. With this insight, we develop an effective mitigation strategy: REMIX (Random and Generic Data Mixing). REMIX prevents forgetting by mixing generic data sampled from pretraining corpora or even randomly generated word sequences during each stage, despite being unrelated to the memorized factoids in the first stage. REMIX can recover performance from severe forgetting, often outperforming replay-based methods that have access to the factoids from the first stage. We then analyze how REMIX alters the learning process and find that successful forgetting prevention is associated with a pattern: the model stores factoids in earlier layers than usual and diversifies the set of layers that store these factoids. The efficacy of REMIX invites further investigation into the underlying dynamics of memorization and forgetting, opening exciting possibilities for future research.", "title_embedding_index": 5072, "title_abs_embedding_index": 5097}, {"title": "Inverse Reinforcement Learning with Switching Rewards and History Dependency for Characterizing Animal Behaviors", "link_suffix": "/forum?id=v7a4KET0Md", "link": "https://openreview.net/forum?id=v7a4KET0Md", "pdf_link": "https://openreview.net/pdf?id=v7a4KET0Md", "keywords": "neuroscience, decision-making, inverse reinforcement learning", "abstract": "Traditional approaches to studying decision-making in neuroscience focus on simplified behavioral tasks where animals perform repetitive, stereotyped actions to receive explicit rewards. While informative, these methods constrain our understanding of decision-making to short timescale behaviors driven by explicit goals. In natural environments, animals exhibit more complex, long-term behaviors driven by intrinsic motivations that are often unobservable. Recent works in time-varying inverse reinforcement learning (IRL) aim to capture shifting motivations in long-term, freely moving behaviors. However, a crucial challenge remains: animals make decisions based on their history, not just their current state. To address this, we introduce SWIRL (SWItching IRL), a novel framework that extends traditional IRL by incorporating time-varying, history-dependent reward functions. SWIRL models long behavioral sequences as transitions between short-term decision-making processes, each governed by a unique reward function. SWIRL incorporates biologically plausible history dependency to capture how past decisions and environmental contexts shape behavior, offering a more accurate description of animal decision-making. We apply SWIRL to simulated and real-world animal behavior datasets and show that it outperforms models lacking history dependency, both quantitatively and qualitatively. This work presents the first IRL model to incorporate history-dependent policies and rewards to advance our understanding of complex, naturalistic decision-making in animals.", "title_embedding_index": 5073, "title_abs_embedding_index": 5098}, {"title": "Weighted Point Cloud Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric", "link_suffix": "/forum?id=uSz2K30RRd", "link": "https://openreview.net/forum?id=uSz2K30RRd", "pdf_link": "https://openreview.net/pdf?id=uSz2K30RRd", "keywords": "contrastive learning, multimodal representation learning, theoretical analysis, InfoNCE, pointwise mutual information", "abstract": "In typical multimodal contrastive learning, such as CLIP, encoders produce onepoint in the latent representation space for each input. However, one-point representation has difficulty in capturing the relationship and the similarity structure of a huge amount of instances in the real world. For richer classes of the similarity, we propose the use of weighted point clouds, namely, sets of pairs of weight and vector, as representations of instances. In this work, we theoretically show the benefit of our proposed method through a new understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE. We clarify that the optimal similarity\nthat minimizes symmetric InfoNCE is the pointwise mutual information, and show an upper bound of excess risk on downstream classification tasks of representations that achieve the optimal similarity. In addition, we show that our proposed similarity based on weighted point clouds consistently achieves the optimal similarity. To verify the effectiveness of our proposed method, we demonstrate pretraining of text-image representation models and classification tasks on common benchmarks.", "title_embedding_index": 5074, "title_abs_embedding_index": 5099}]