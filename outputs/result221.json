[{"title": "Interpretable Table Question Answering via Plans of Atomic Table Transformations", "link_suffix": "/forum?id=mDV36U4d6u", "link": "https://openreview.net/forum?id=mDV36U4d6u", "pdf_link": "https://openreview.net/pdf?id=mDV36U4d6u", "keywords": "Table QA; Interpretability; XAI; LLM-as-a-Judge", "abstract": "Interpretability for Table Question Answering (Table QA) is critical, particularly in high-stakes domains like finance or healthcare.\nWhile recent Large Language Models (LLMs) have improved the accuracy of Table QA models, their explanations for how answers are derived may not be transparent, hindering user ability to trust, explain, and debug predicted answers, especially on complex queries.\nWe introduce Plan-of-SQLs (POS), a novel method specifically crafted to enhance interpretability by decomposing a query into simpler sub-queries that are sequentially translated into SQL commands to generate the final answer.\nUnlike existing approaches, \nPOS offers full transparency in Table QA by ensuring that every transformation of the table is traceable, allowing users to follow the reasoning process step-by-step.\nVia subjective and objective evaluations, we show that POS explanations significantly improve interpretability, enabling both human and LLM judges to predict model responses with 93.00% and 85.25% accuracy, respectively.\nPOS explanations also consistently rank highest in clarity, coherence, and helpfulness compared to state-of-the-art Table QA methods such as Chain-of-Table and DATER.\nFurthermore, POS demonstrates high accuracy on Table QA benchmarks (78.31% on TabFact and 54.80% on WikiTQ with GPT3.5), outperforming methods that rely solely on LLMs or programs for table transformations, while remaining competitive with hybrid approaches that often trade off interpretability for accuracy.", "title_embedding_index": 11000, "title_abs_embedding_index": 11025}, {"title": "Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models", "link_suffix": "/forum?id=BCyAlMoyx5", "link": "https://openreview.net/forum?id=BCyAlMoyx5", "pdf_link": "https://openreview.net/pdf?id=BCyAlMoyx5", "keywords": "Large Language Models, Multilingual, Crosslingual Knowledge Barrier", "abstract": "Large language models (LLMs) are typically multilingual due to pretraining on diverse multilingual corpora. But can these models relate corresponding concepts across languages, i.e., be crosslingual? This study evaluates six state-of-the-art LLMs on inherently crosslingual tasks. We observe that while these models show promising surface-level crosslingual abilities on machine translation and embedding space analyses, they struggle with deeper crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in both general (MMLU benchmark) and domain-specific (Harry Potter quiz) contexts.  Since simple inference-time mitigation methods seem to offer only limited improvement, we  propose fine-tuning of LLMs on mixed-language data, which effectively reduces these gaps, even when using out-of-domain datasets like WikiText. Our findings suggest the need for explicit optimization to unlock the full crosslingual potential of LLMs.", "title_embedding_index": 11001, "title_abs_embedding_index": 11026}, {"title": "F2M-Reg: Unsupervised RGB-D Registration with Frame-to-Model Optimization", "link_suffix": "/forum?id=5G9PrHERql", "link": "https://openreview.net/forum?id=5G9PrHERql", "pdf_link": "https://openreview.net/pdf?id=5G9PrHERql", "keywords": "RGB-D registation, unsupervised learning, frame-to-model optimization", "abstract": "This paper focuses on training a robust RGB-D registration model without ground-truth pose supervision. Existing methods usually adopt a pairwise training strategy based on differentiable rendering, which enforces the photometric and the geometric consistency between the two registered frames as supervision. However, this frame-to-frame framework suffers from poor multi-view consistency due to factors such as lighting changes, geometry occlusion and reflective materials. In this paper, we present F2M-Reg, a novel frame-to-model optimization framework for unsupervised RGB-D registration. Instead of frame-to-frame consistency, we leverage the neural implicit field as a global model of the scene and use the consistency between the input and the rerendered frames for pose optimization. This design can significantly improve the robustness in scenarios with poor multi-view consistency and provides better learning signal for the registration model. Furthermore, to bootstrap the neural field optimization, we create a synthetic dataset, Sim-RGBD, through a photo-realistic simulator to warm up the registration model. By first training the registration model on Sim-RGBD and later unsupervisedly fine-tuning on real data, our framework enables distilling the capability of feature extraction and registration from simulation to reality. Our method outperforms the state-of-the-art counterparts on two popular indoor RGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper reproduction.", "title_embedding_index": 11002, "title_abs_embedding_index": 11027}, {"title": "AVID: Adapting Video Diffusion Models to World Models", "link_suffix": "/forum?id=15ASUbzg0N", "link": "https://openreview.net/forum?id=15ASUbzg0N", "pdf_link": "https://openreview.net/pdf?id=15ASUbzg0N", "keywords": "world models, video diffusion, black box adaptation, controllable video generation", "abstract": "Large-scale generative models have achieved remarkable success in a number of domains. However, for sequential decision-making problems, such as robotics, action-labelled data is often scarce and therefore scaling-up foundation models for decision-making remains a challenge. A potential solution lies in leveraging widely-available unlabelled videos to train world models that simulate the consequences of actions. If the world model is accurate, it can be used to optimize decision-making in downstream tasks. Image-to-video diffusion models are already capable of generating highly realistic synthetic videos. However, these models are not action-conditioned, and the most powerful models are closed source which means they cannot be finetuned. In this work, we propose to adapt pretrained video diffusion models to action-conditioned world models, without access to the parameters of the pretrained model. Our approach, AVID, trains an adapter on a small domain-specific dataset of action-labelled videos. AVID uses a learnt mask to modify the intermediate outputs of the pretrained model and generate accurate action-conditioned videos. We evaluate AVID on video game and real-world robotics data, and show that it outperforms existing baselines for diffusion model adaptation. Our results demonstrate that if utilized correctly, pretrained video models have the potential to be powerful tools for embodied AI.", "title_embedding_index": 11003, "title_abs_embedding_index": 11028}, {"title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "link_suffix": "/forum?id=ZRDhBwKs7l", "link": "https://openreview.net/forum?id=ZRDhBwKs7l", "pdf_link": "https://openreview.net/pdf?id=ZRDhBwKs7l", "keywords": "Concept erasing, Generative model, Diffusion model", "abstract": "Remarkable progress in text-to-image diffusion models has brought a major concern about potentially generating images on inappropriate or trademarked concepts. Concept erasing has been investigated with the goals of deleting target concepts in diffusion models while preserving other concepts with minimal distortion. To achieve these goals, recent concept erasing methods usually fine-tune the cross-attention layers of diffusion models. In this work, we first show that merely updating the cross-attention layers in diffusion models, which is mathematically equivalent to adding linear modules to weights, may not be able to preserve diverse remaining concepts. Then, we propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding nonlinear Residual Attention Gates (ResAGs) that selectively erase (or cut) target concepts while safeguarding remaining concepts from broad distributions by employing an attention anchoring loss to prevent the forgetting. Moreover, we adversarially train CPE with ResAG and learnable text embeddings in an iterative manner to maximize erasing performance and enhance robustness against adversarial attacks. Extensive experiments on the erasure of celebrities, artistic styles, and explicit contents demonstrated that the proposed CPE outperforms prior arts by keeping diverse remaining concepts while deleting the target concepts with robustness against attack prompts.", "title_embedding_index": 11004, "title_abs_embedding_index": 11029}, {"title": "Certified\u21132Attribution Robustness via Uniformly Smoothed Attributions", "link_suffix": "/forum?id=ZYuiuxB7H4", "link": "https://openreview.net/forum?id=ZYuiuxB7H4", "pdf_link": "https://openreview.net/pdf?id=ZYuiuxB7H4", "keywords": "Explainability, Robustness", "abstract": "Model attribution is a popular tool to explain the rationales behind model predictions. However, recent work suggests that the attributions are vulnerable to minute perturbations, which can be added to input samples to fool the attributions while maintaining the prediction outputs. Although empirical studies have shown positive performance via adversarial training, an effective certified defense method is eminently needed to understand the robustness of attributions. In this work, we propose to use uniform smoothing technique that augments the vanilla attributions by noises uniformly sampled from a certain space. It is proved that, for all perturbations within the attack region, the cosine similarity between uniformly smoothed attribution of perturbed sample and the unperturbed sample is guaranteed to be lower bounded. We also derive alternative formulations of the certification that is equivalent to the original one and provides the maximum size of perturbation or the minimum smoothing radius such that the attribution can not be perturbed. We evaluate the proposed method on three datasets and show that the proposed method can effectively protect the attributions from attacks, regardless of the architecture of networks, training schemes and the size of the datasets.", "title_embedding_index": 11005, "title_abs_embedding_index": 11030}, {"title": "A GREAT Architecture for Edge-Based Graph Problems Like TSP", "link_suffix": "/forum?id=iWCfiDxLIY", "link": "https://openreview.net/forum?id=iWCfiDxLIY", "pdf_link": "https://openreview.net/pdf?id=iWCfiDxLIY", "keywords": "Traveling Salesman Problem, Asymmetric Traveling Salesman Problem, Graph Learning, Graph Neural Networks, Routing Problems", "abstract": "In the last years, many neural network-based approaches have been proposed to tackle combinatorial optimization problems such as routing problems.\nMany of these approaches are based on graph neural networks (GNNs) or related transformers, operating on the Euclidean coordinates representing the routing problems. \nHowever, GNNs are inherently not well suited to operate on dense graphs, such as in routing problems. Furthermore, models operating on Euclidean coordinates cannot be applied to non-Euclidean versions of routing problems that are often found in real-world settings.\nTo overcome these limitations, we propose a novel GNN-related edge-based neural model called Graph Edge Attention Network (GREAT).\nWe evaluate the performance of GREAT in the edge-classification task to predict optimal edges in the Traveling Salesman Problem (TSP). We can use such a trained GREAT model to produce sparse TSP graph instances, keeping only the edges GREAT finds promising. Compared to other, non-learning-based methods to sparsify TSP graphs, GREAT can produce very sparse graphs while keeping most of the optimal edges. Furthermore, we build a reinforcement learning-based GREAT framework which we apply to Euclidean and non-Euclidean asymmetric TSP. This framework achieves state-of-the-art results.", "title_embedding_index": 11006, "title_abs_embedding_index": 11031}, {"title": "Single-Step Diffusion Model-Based Generative Model Inversion Attacks", "link_suffix": "/forum?id=TvhEoz1nim", "link": "https://openreview.net/forum?id=TvhEoz1nim", "pdf_link": "https://openreview.net/pdf?id=TvhEoz1nim", "keywords": "Diffusion models, model inversion attacks", "abstract": "Generative model inversion attacks (MIAs) have garnered increasing attention for their ability to reconstruct synthetic samples that closely resemble private training data, exposing significant privacy risks in machine learning models. The success of generative MIAs is primarily attributed to image priors learned by generative adversarial networks (GANs) on public auxiliary data, which help constrain the optimization space during the inversion process. However, GAN-based generative MIAs still face limitations, particularly regarding the instability during model inversion optimization and the fidelity of reconstructed samples, indicating substantial room for improvement. In this paper, we address these challenges by exploring generative MIAs based on diffusion models, which offer superior generative performance compared to GANs. Specifically, we replace the GAN generator in existing generative MIAs with a single-step generator distilled from pretrained diffusion models, constraining the search space to the manifold of the generator during the inversion process. In addition, we leverage generative model inversion techniques to investigate privacy leakage issues in widely used large-scale multimodal models, particularly CLIP, highlighting the inherent privacy risks in these models. Our extensive experiments demonstrate that single-step diffusion models-based MIAs significantly outperform their GAN-based counterparts, achieving substantial improvements in traditional metrics and greatly enhancing the visual fidelity of reconstructed samples. This research uncovers vulnerabilities in CLIP models and opens new research directions in generative MIAs.", "title_embedding_index": 11007, "title_abs_embedding_index": 11032}, {"title": "FeedSign: Full-parameter Federated Fine-tuning of Large Models with Extremely Low Communication Overhead of One Bit", "link_suffix": "/forum?id=DJRd4IQHGQ", "link": "https://openreview.net/forum?id=DJRd4IQHGQ", "pdf_link": "https://openreview.net/pdf?id=DJRd4IQHGQ", "keywords": "Large Model Fine-tuning, Federated Learning, Heterogeneity Resilience, Byzantine Resilience", "abstract": "Federated fine-tuning (FFT) aims to fine-tune a pre-trained model with private data from distributed clients by exchanging models rather than data under the orchestration of a parameter server (PS). However, as large models are acing in almost every machine learning task, the communication overhead and memory demand are surging accordingly, hindering the practical deployment on consumer devices. To overcome the bottleneck forged by the growing communication overhead of federated learning and lower the high memory demand of large model fine-tuning, we propose FeedSign, an FFT algorithm where a client uploads its update model and downloads the global model of any size using exactly $1$ bit per step, while the memory demand is squeezed to the amount needed for inference. This is realized by utilizing zeroth-order (ZO) optimizers on large models and shared pseudo-random number generators (PRNG) across devices to split the gradient estimate from the clients to 1) a direction corresponding to a designated random seed and 2) a binary vote from the client indicating whether the seed-corresponding direction grants a local loss descent, which is the only information the clients should convey to the PS. We conduct theoretical analysis on FeedSign and show that it converges at an exponential rate $\\mathcal{O}(e^{-t})$, where $t$ is the number of elapsed steps, the same rate as in first-order (FO) methods can attain in big $\\mathcal{O}$ notation. Moreover, it is also found that FeedSign enjoys good robustness against data heterogeneity and Byzantine attacks. We conduct extensive experiments on models across different structures and sizes (11M to 13B) and found that the proposed method performs better or closely, depending on scenarios, compared to its ZO and FO counterparts albeit an orders-of-magnitude lower communication overhead. We also discuss some interesting advantages as byproducts guaranteed by the minimalistic design of FeedSign.", "title_embedding_index": 11008, "title_abs_embedding_index": 11033}, {"title": "SingNet: Towards a Large-Scale, Diverse, and In-the-Wild Singing Voice Dataset", "link_suffix": "/forum?id=X6ffdf6nh3", "link": "https://openreview.net/forum?id=X6ffdf6nh3", "pdf_link": "https://openreview.net/pdf?id=X6ffdf6nh3", "keywords": "Singing Voice Dataset, Singing Voice Corpus, Self-supervised Learning, Automatic Lyric Transcription, Neural Vocoder, Singing Voice Conversion", "abstract": "The lack of a publicly-available large-scale and diverse dataset has long been a significant bottleneck for singing voice applications like Singing Voice Synthesis (SVS) and Singing Voice Conversion (SVC). To tackle this problem, we present SingNet, an extensive, diverse, and in-the-wild singing voice dataset. Specifically, we propose a data processing pipeline to extract ready-to-use training data from sample packs and songs on the internet, forming 3000 hours of singing voices in various languages and styles. Furthermore, to facilitate the use and demonstrate the effectiveness of SingNet, we pre-train and open-source various state-of-the-art (SOTA) models on Wav2vec2, BigVGAN, and NSF-HiFiGAN based on our collected singing voice data. We also conduct benchmark experiments on Automatic Lyric Transcription (ALT), Neural Vocoder, and Singing Voice Conversion (SVC). Audio demos are available at:https://singnet-dataset.github.io/.", "title_embedding_index": 11009, "title_abs_embedding_index": 11034}, {"title": "Pyramid Vector Quantization for LLMs", "link_suffix": "/forum?id=ZBlfjXubgG", "link": "https://openreview.net/forum?id=ZBlfjXubgG", "pdf_link": "https://openreview.net/pdf?id=ZBlfjXubgG", "keywords": "quantization large language model llm pyramid vector quantization pvq", "abstract": "Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering \\textit{Pyramid Vector Quantization} (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to utilise Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieve state-of-the-art quantization performance in terms of bits per weight on various large language models.", "title_embedding_index": 11010, "title_abs_embedding_index": 11035}, {"title": "ConDS: Context Distribution Shift for Robust In-Context Learning", "link_suffix": "/forum?id=92GUJzTRXs", "link": "https://openreview.net/forum?id=92GUJzTRXs", "pdf_link": "https://openreview.net/pdf?id=92GUJzTRXs", "keywords": "In-context learning, Distribution shift, Robustness", "abstract": "In-context Learning (ICL) is a popular approach to filling Large Language Models (LLMs) with the context without fine-tuning. ICL works by feeding the test input along with the context information selected from the candidate dataset as examples of explaining the target task and getting the answer. In real-world applications, noisy samples are easily to be included in the datasets, so it is unavoidable that the candidate set might contain noise caused by human or measurement errors. The effectiveness of ICL is highly dependent on the quality of the selected ICL samples. Thus the noise in the candidate set can severely mislead the query answer and degrade the ICL performance. However, the noise ICL problem is largely overlooked. To tackle this challenge, in this paper, we propose Context Distribution Shift (ConDS), which iteratively revises the distribution of the candidate dataset so that the retrieved ICL samples are emphasized to improve the robustness of ICL. Specifically, we first identify the informative samples based on the retriever ranking score and the feedback from the LLMs, and then augment the identified informative samples. A subsampling strategy is also adopted to emphasize the importance of informative samples and decrease the size of noisy samples. Thus, ICL's reliability can be improved by reducing the catastrophic impact of noisy samples on almost all test queries to a small percentage. Our ConDS can be easily combined with existing off-the-shelf and fine-tuned retrievers. An analysis is also provided to reveal the relationship between ConDS and retrievers. Experimental results show that ConDS outperforms baselines on various tasks under the influence of noise by a large margin of 8.12%.", "title_embedding_index": 11011, "title_abs_embedding_index": 11036}, {"title": "Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting", "link_suffix": "/forum?id=56vHbnk35S", "link": "https://openreview.net/forum?id=56vHbnk35S", "pdf_link": "https://openreview.net/pdf?id=56vHbnk35S", "keywords": "3D Gaussian Splatting, VR, 3D Reconstruction, NeRF, Large-Scale Scene Reconstruction, Graph", "abstract": "This paper investigates an open research challenge of reconstructing high-quality, large-scale 3D open scenes from images. It is observed existing methods have various limitations, such as requiring precise camera poses for input and dense viewpoints for supervision. \nTo perform effective and efficient 3D scene reconstruction, we propose a novel graph-guided 3D scene reconstruction framework, GraphGS. Specifically, given a set of images captured by RGB cameras on a scene, we first design a spatial prior-based scene structure estimation method. This is then used to create a camera graph that includes information about the camera topology. Further, we propose to apply the graph-guided multi-view consistency constraint and adaptive sampling strategy to the 3D Gaussian Splatting optimization process. This greatly alleviates the issue of Gaussian points overfitting to specific sparse viewpoints and expedites the 3D reconstruction process. We demonstrate GraphGS achieves high-fidelity 3D reconstruction from images, which presents state-of-the-art performance through quantitative and qualitative evaluation across multiple datasets.", "title_embedding_index": 11012, "title_abs_embedding_index": 11037}, {"title": "VideoEval: Comprehensive Benchmark Suite for Low-Cost Evaluation of Video Foundation Model", "link_suffix": "/forum?id=wMRFTQwp1d", "link": "https://openreview.net/forum?id=wMRFTQwp1d", "pdf_link": "https://openreview.net/pdf?id=wMRFTQwp1d", "keywords": "Video Understanding, Video Foundation Model, Benchmark", "abstract": "With the accumulation of high-quality data and advancements in visual pretraining paradigms, recent Video Foundation Models (VFMs) have made significant progress, demonstrating remarkable performance on popular video understanding benchmarks. However, conventional benchmarks (e.g. Kinetics) and evaluation protocols are limited by their relatively poor diversity, high evaluation costs, and saturated performance metrics. In this work, we introduce a comprehensive benchmark suite to address these issues, namelyVideoEval. We establish theVideoTaskAdaptionBenchmark (VidTAB) and theVideoEmbeddingBenchmark (VidEB) from two perspectives: evaluating the task adaptability of VFMs under few-shot conditions and assessing their feature embedding's direct applicability to downstream tasks. With VideoEval, we conduct a large-scale study of 20 popular open-source vision foundation models. Our study reveals some insightful findings, 1) overall, current VFMs exhibit weak generalization across diverse tasks, 2) increasing video data, whether labeled or in video-text pairs, does not necessarily improve task performance, 3) the effectiveness of some pre-training paradigms may not be fully validated in previous benchmarks, and 4) combining different pre-training paradigms can help develop models with better generalization capabilities. We believe this study serves as a important complement to the current evaluation methods for VFMs and offers valuable insights for future research directions.", "title_embedding_index": 11013, "title_abs_embedding_index": 11038}, {"title": "ODE-based Smoothing Neural Network for Reinforcement Learning Tasks", "link_suffix": "/forum?id=S5Yo6w3n3f", "link": "https://openreview.net/forum?id=S5Yo6w3n3f", "pdf_link": "https://openreview.net/pdf?id=S5Yo6w3n3f", "keywords": "Reinforcement Learning, Smooth Control, Low-pass Filter, Neural ODE", "abstract": "The smoothness of control actions is a significant challenge faced by deep reinforcement learning (RL) techniques in solving optimal control problems. Existing RL-trained policies tend to produce non-smooth actions due to high-frequency input noise and unconstrained Lipschitz constants in neural networks. This article presents a Smooth ODE (SmODE) network capable of simultaneously addressing both causes of unsmooth control actions, thereby enhancing policy performance and robustness under noise condition. We first design a smooth ODE neuron with first-order low-pass filtering expression, which can dynamically filter out high frequency noises of hidden state by a learnable state-based system time constant. Additionally, we construct a state-based mapping function, $g$, and theoretically demonstrate its capacity to control the ODE neuron's Lipschitz constant. Then, based on the above neuronal structure design, we further advanced the SmODE network serving as RL policy approximators. This network is compatible with most existing RL algorithms, offering improved adaptability compared to prior approaches. Various experiments show that our SmODE network demonstrates superior anti-interference capabilities and smoother action outputs than the multi-layer perception and smooth network architectures like LipsNet.", "title_embedding_index": 11014, "title_abs_embedding_index": 11039}, {"title": "Residual Kernel Policy Network: Enhancing Stability and Robustness in RKHS-Based Reinforcement Learning", "link_suffix": "/forum?id=2vgcDW2blS", "link": "https://openreview.net/forum?id=2vgcDW2blS", "pdf_link": "https://openreview.net/pdf?id=2vgcDW2blS", "keywords": "policy learning, reproducing kernel Hilbert space, representation learning, variance reduction", "abstract": "Achieving optimal performance in reinforcement learning requires robust policies supported by training processes that ensure both sample efficiency and stability. Modeling the policy in reproducing kernel Hilbert space (RKHS) enables efficient exploration of local optimal solutions. However, the stability of existing RKHS-based methods is hindered by significant variance in gradients, while the robustness of the learned policies is often compromised due to the sensitivity of hyperparameters. In this work, we conduct a comprehensive analysis of the significant instability in RKHS policies and reveal that the variance of the policy gradient increases substantially when a wide-bandwidth kernel is employed. To address these challenges, we propose a novel RKHS policy learning method integrated with representation learning to dynamically process observations in complex environments, enhancing the robustness of RKHS policies. Furthermore, inspired by the advantage functions, we introduce a residual layer that further stabilizes the training process by significantly reducing gradient variance in RKHS. Our novel algorithm, the Residual Kernel Policy Network (ResKPN), demonstrates state-of-the-art performance, achieving a 30% improvement in episodic rewards across complex environments.", "title_embedding_index": 11015, "title_abs_embedding_index": 11040}, {"title": "Receptor-Specific Diffusion Model: Towards Generating Protein-Protein Structures with Customized Perturbing and Sampling", "link_suffix": "/forum?id=3zWvZv9xFh", "link": "https://openreview.net/forum?id=3zWvZv9xFh", "pdf_link": "https://openreview.net/pdf?id=3zWvZv9xFh", "keywords": "protein structure prediction, diffusion model, graph neural network", "abstract": "Recent advancements in deep generative models have signi\ufb01cantly facilitated protein-ligand structure design, which is crucial in protein engineering. However, recent generative approaches based on diffusion models in this field usually start sampling from a unified distribution, failing to capture the intricate biochemical differences between receptors. This may limits their capacity to generate reliable ligands for the corresponding receptors. Moreover, the current sampling process incurs a heavy computational burden and inefficiency, which further escalates the training demands on the model. To this end, we introduce a novel diffusion model with customized perturbing and sampling for the ligand design targeting the specific receptor, named as Receptor-Specific Diffusion Model (RSDM). In particular, the receptor-specific information is used to tailor fine-grained sampling distributions via changing the noise for customized perturbing. Meantime, we refine the sampling process using a predefined schedule to perform stepwise denoising and gradually decrease the influence of the receptor's guidence in the ligand generation for customized sampling. The experimental reaults indicate that RSDM is highly competitive with state-of-the-art learning-based models, including recent models like ElliDock and DiffDock-PP. Additionally, RSDM stands out for its faster inference speed compared with all baseline methods, highlighting its potential for generating dependable protein-ligand.", "title_embedding_index": 11016, "title_abs_embedding_index": 11041}, {"title": "Disentangling Latent Shifts of In-Context Learning Through Self-Training", "link_suffix": "/forum?id=pdf6MbXnAS", "link": "https://openreview.net/forum?id=pdf6MbXnAS", "pdf_link": "https://openreview.net/pdf?id=pdf6MbXnAS", "keywords": "large language models, in-context learning, self-training, latent shifts", "abstract": "In-context learning (ICL) has become essential in natural language processing, particularly with autoregressive large language models capable of learning from demonstrations provided within the prompt. However, ICL faces challenges with stability and long contexts, especially as the number of demonstrations grows, leading to poor generalization and inefficient inference. To address these issues, we introduce STICL (Self-Training ICL), an approach that disentangles the latent shifts of demonstrations from the latent shift of the query through self-training. STICL employs a teacher model to generate pseudo-labels and trains a student model using these labels, encoded in an adapter module. The student model exhibits weak-to-strong generalization, progressively refining its predictions over time. Our empirical results show that STICL improves generalization and stability, consistently outperforming traditional ICL methods and other disentangling strategies across both in-domain and out-of-domain data.", "title_embedding_index": 11017, "title_abs_embedding_index": 11042}, {"title": "Speech Robust Bench: A Robustness Benchmark For Speech Recognition", "link_suffix": "/forum?id=D0LuQNZfEl", "link": "https://openreview.net/forum?id=D0LuQNZfEl", "pdf_link": "https://openreview.net/pdf?id=D0LuQNZfEl", "keywords": "robustness, automatic speech recognition, benchmark, adversarial", "abstract": "As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose  Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 114 input perturbations which simulate an heterogeneous range of corruptions that ASR models may encounter when deployed in the wild. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as the use of discrete representations, or self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females. Our results revealed noticeable disparities in the model's robustness across subgroups. We believe that SRB will significantly facilitate future research towards robust ASR models, by making it easier to conduct comprehensive and comparable robustness evaluations.", "title_embedding_index": 11018, "title_abs_embedding_index": 11043}, {"title": "Phase retrieval: Global convergence of gradient descent with optimal sample complexity", "link_suffix": "/forum?id=T0MLnjav7P", "link": "https://openreview.net/forum?id=T0MLnjav7P", "pdf_link": "https://openreview.net/pdf?id=T0MLnjav7P", "keywords": "Phase Retrieval, Gradient Descent, Global Convergence, Benign Landscape, Sample Complexity, Random Initialization", "abstract": "This paper addresses the phase retrieval problem, which aims to recover a signal vector $x^{\\natural}$ from $m$ measurements $y_i=|\\langle a_i,x^{\\natural}\\rangle|^2$, $i=1,\\ldots,m$. A standard approach is to solve a nonconvex least squares problem using gradient descent with random initialization, which is known to work efficiently given a sufficient number of measurements. However, whether $O(n)$ measurements suffice for gradient descent to recover the ground truth efficiently has remained an open question. Prior work has established that $O(n{\\rm poly}(\\log n))$ measurements are sufficient. In this paper, we resolve this open problem by proving that $m=O(n)$ Gaussian random measurements are sufficient to guarantee, with high probability, that the objective function has a benign global landscape. This sample complexity is optimal because at least $\\Omega(n)$ measurements are required for exact recovery. The landscape result allows us to further show that gradient descent with a constant step size converges to the ground truth from almost any initial point.", "title_embedding_index": 11019, "title_abs_embedding_index": 11044}, {"title": "Versatile Motion-Language Models for Multi-turn Interactive Agents", "link_suffix": "/forum?id=chiDvQc1F6", "link": "https://openreview.net/forum?id=chiDvQc1F6", "pdf_link": "https://openreview.net/pdf?id=chiDvQc1F6", "keywords": "Motion Synthesis, Human Motion Modeling, Interactive Motion, Motion Language Model", "abstract": "Recent advancements in large language models (LLMs) have greatly enhanced their ability to generate natural and contextually relevant text, making AI interactions more human-like. However, generating and understanding interactive human-like motion, where two individuals engage in coordinated movements, remains a challenge due to the complexity of modeling these coordinated interactions. Furthermore, a versatile model is required to handle diverse interactive scenarios, such as chat systems that follow user instructions or adapt to their assigned role while adjusting interaction dynamics. To tackle this problem, we introduce VIM, short for the Versatile Interactive Motion language model, which integrates both language and motion modalities to effectively understand, generate, and control interactive motions in multi-turn conversational contexts. To address the scarcity of multi-turn interactive motion data, we introduce a synthetic dataset called INTER-MT2; where we utilize pre-trained models to create diverse instructional datasets with interactive motion. Our approach first trains a motion tokenizer that encodes interactive motions into residual discrete tokens. In the pre-training stage, the model learns to align motion and text representations with these discrete tokens. During the instruction fine-tuning stage, VIM adapts to multi-turn conversations using INTER-MT2. We evaluate the versatility of our method across motion-related tasks\u2014motion-to-text, text-to-motion, reaction generation, motion editing, and reasoning about motion sequences. The results highlight VIM\u2019s versatility and effectiveness in handling complex interactive motion synthesis.", "title_embedding_index": 11020, "title_abs_embedding_index": 11045}, {"title": "Do We Really Need Parameter-Isolation to Protect Task Knowledge?", "link_suffix": "/forum?id=tVNZj27pb3", "link": "https://openreview.net/forum?id=tVNZj27pb3", "pdf_link": "https://openreview.net/pdf?id=tVNZj27pb3", "keywords": "continual learning", "abstract": "Due to the dynamic nature of tasks, how deep networks can transition from a static structure, trained on previous tasks, to a dynamic structure that adapts to continuously changing data inputs has garnered significant attention. This involves learning new task knowledge while avoiding catastrophic forgetting of previously acquired knowledge. Continual learning is a learning approach aimed at addressing the problem of catastrophic forgetting, primarily by constraining or isolating parameter changes to protect the knowledge of prior tasks. However, while existing methods offer good protection for old task knowledge, they often diminish the ability to learn new task knowledge. Given the sparsity of activation channels in a deep network, we introduce a novel misaligned fusion method within the context of continual learning. This approach allows for the adaptive allocation of available pathways to protect crucial knowledge from previous tasks, replacing traditional isolation techniques. Furthermore, when new tasks are introduced, the network can undergo full parameter training, enabling a more comprehensive learning of new tasks. This work conducts comparative tests of our method against other approaches using deep network architectures of various sizes and popular benchmark datasets. The performance demonstrates the effectiveness and superiority of our method.", "title_embedding_index": 11021, "title_abs_embedding_index": 11046}, {"title": "Hierarchical overlapping clustering: cost function, algorithm and scalability", "link_suffix": "/forum?id=oHSXRy29tj", "link": "https://openreview.net/forum?id=oHSXRy29tj", "pdf_link": "https://openreview.net/pdf?id=oHSXRy29tj", "keywords": "hierarchical overlapping clustering, cost function, approximation algorithm", "abstract": "Overlap and hierarchy are two prevalent phenomena in clustering, and usually coexist in a single system. There are several studies on each of them separately, but it is unclear how to characterize and evaluate the hybrid structures yet. To address this issue, we initiate the study of hierarchical overlapping clustering by introducing a new cost function for it. We show the rationality of our cost function via several intuitive properties, and develop an approximation algorithm that achieves a provably constant approximation factor for its dual version. Our algorithm is a recursive process of overlapping bipartition based on local search, which makes a speed-up version of it extremely scalable. Our experiments demonstrate that the speed-up algorithm has good performances in both effectiveness and scalability on synthetic and real datasets.", "title_embedding_index": 11022, "title_abs_embedding_index": 11047}, {"title": "The Implicit Bias of Stochastic AdaGrad-Norm on Separable Data", "link_suffix": "/forum?id=nE1l0vpQDP", "link": "https://openreview.net/forum?id=nE1l0vpQDP", "pdf_link": "https://openreview.net/pdf?id=nE1l0vpQDP", "keywords": "stochastic AdaGrad-Norm, implicit bias, convergence results, stochastic optimization", "abstract": "This work explores stochastic adaptive gradient descent, i.e., stochastic AdaGrad-Norm, when applied to linearly separable datasets. For the stochastic AdaGrad-Norm method equipped with a wide range of sampling noise, we demonstrate its almost surely convergence result to the $\\mathcal{L}^{2}$ max-margin solution. This means that stochastic AdaGrad-Norm has an implicit bias that yields good generalization, even without regularization terms. We show that the convergence rate of the classification direction is $o({1}/{\\ln^{(1-\\epsilon)/{2}}n})$. Our approach takes a novel stance by explicitly characterizing the $\\mathcal{L}^{2}$ max-margin direction. By doing so, we overcome the challenge that arises from the dependency between the stepsize and the gradient and also address the limitations in the previous AdaGrad-Norm analyses.", "title_embedding_index": 11023, "title_abs_embedding_index": 11048}, {"title": "DeepCircuitX: Repository-Level RTL Dataset for Code Understanding, Generation, and Multimodal Analysis", "link_suffix": "/forum?id=TkXisc47la", "link": "https://openreview.net/forum?id=TkXisc47la", "pdf_link": "https://openreview.net/pdf?id=TkXisc47la", "keywords": "Multi-modal dataset of EDA, AI for EDA, LLM for RTL, Circuit Learning", "abstract": "This paper introduces DeepCircuitX, a comprehensive multimodal dataset designed to advance RTL code understanding, generation, and completion tasks in hardware design automation. Unlike existing datasets, which focus either on file-level RTL code or downstream netlist and layout data, DeepCircuitX spans repository, file, module, and block-level RTL code, providing a more holistic resource for training and evaluating large language models (LLMs). The dataset is enriched with Chain of Thought (CoT) annotations that offer detailed functionality and structure descriptions at multiple levels, enhancing its utility for RTL code understanding, generation, and completion.In addition to RTL data, DeepCircuitX includes synthesized netlists and power-performance-area (PPA) metrics, allowing for early-stage design exploration and PPA prediction directly from RTL code. We establish comprehensive benchmarks for RTL code understanding, generation, and completion using open-source models such as CodeLlama, CodeT5+, and CodeGen, demonstrating substantial improvements in task performance. Furthermore, we introduce and evaluate models for PPA prediction, setting new benchmarks for RTL-to-PPA analysis. We conduct human evaluations and reviews to confirm the high quality and functionality of the generated RTL code and annotations. Our experimental results show that DeepCircuitX significantly improves model performance across multiple benchmarks, underscoring its value as a critical resource for advancing RTL code tasks in hardware design automation.", "title_embedding_index": 11024, "title_abs_embedding_index": 11049}]