[
    {
        "title": "Reconstruction-Guided Policy: Enhancing Decision-Making through Agent-Wise State Consistency",
        "link_suffix": "/forum?id=Y8L5RB4GWb",
        "link": "https://openreview.net/forum?id=Y8L5RB4GWb",
        "pdf_link": "https://openreview.net/pdf?id=Y8L5RB4GWb",
        "keywords": "multi-agent reinforcement learning, partial observability, cooperation, centralized training distributed execution, global state",
        "abstract": "An important challenge in multi-agent reinforcement learning is partial observability, where agents cannot access the global state of the environment during execution and can only receive observations within their field of view. To address this issue, previous works typically use the dimensional-wise state, which is obtained by applying MLP or dimensional-based attention on the global state, for decision-making during training and relying on a reconstructed dimensional-wise state during execution. However, dimensional-wise states tend to divert agent attention to specific features, neglecting potential dependencies between agents, making it difficult to make optimal decisions. Moreover, the inconsistency between the states used in training and execution further increases additional errors. To resolve these issues, we propose a method called Reconstruction-Guided Policy (RGP) to reconstruct the agent-wise state, which represents the information of inter-agent relationships, as input for decision-making during both training and execution. This not only preserves the potential dependencies between agents but also ensures consistency between the states used in training and execution. We conducted extensive experiments on both discrete and continuous action environments to evaluate RGP, and the results demonstrates its superior effectiveness. Our code is public inhttps://anonymous.4open.science/r/RGP-9F79"
    },
    {
        "title": "Efficient and Trustworthy Causal Discovery with Latent Variables and Complex Relations",
        "link_suffix": "/forum?id=BZYIEw4mcY",
        "link": "https://openreview.net/forum?id=BZYIEw4mcY",
        "pdf_link": "https://openreview.net/pdf?id=BZYIEw4mcY",
        "keywords": "causal discovery, latent variables, complex causal relations",
        "abstract": "Most traditional causal discovery methods assume that all task-relevant variables are observed, an assumption often violated in practice. Although some recent works allow the presence of latent variables, they typically assume the absence of certain special causal relations to ensure a degree of simplicity, which might also be invalid in real-world scenarios. This paper tackles a challenging and important setting where latent and observed variables are interconnected through complex causal relations. Under an assumption ensuring that latent variables leave adequate footprints in observed variables, we develop a series of novel theoretical results, leading to an efficient causal discovery algorithm which is the first one capable of handling the setting with both latent variables and complex relations within polynomial time. Our algorithm first sequentially identifies latent variables from leaves to roots and then sequentially infers causal relations from roots to leaves. Moreover, we prove trustworthiness of our algorithm, meaning that when the assumption is invalid, it can raise an error rather than draw an incorrect causal conclusion, thus preventing potential damage to downstream tasks. We demonstrate the efficacy of our algorithm through experiments. Our work significantly enhances efficiency and reliability of causal discovery in complex systems."
    },
    {
        "title": "ToMiE: Towards Modular Growth in Enhanced SMPL Skeleton for 3D Human Gaussians with Animatable Garments",
        "link_suffix": "/forum?id=vtUbXd5Cyg",
        "link": "https://openreview.net/forum?id=vtUbXd5Cyg",
        "pdf_link": "https://openreview.net/pdf?id=vtUbXd5Cyg",
        "keywords": "Human Gaussians, Adaptive Growth, Animatable Garments",
        "abstract": "In this paper, we highlight a critical yet often overlooked factor in most 3D human tasks, namely modeling humans with complex garments. \nIt is known that the parameterized formulation of SMPL is able to fit human skin; while complex garments, e.g., hand-held objects and loose-fitting garments, are difficult to get modeled within the unified framework, since their movements are usually decoupled with the human body.\nTo enhance the capability of SMPL skeleton in response to this situation, we propose a modular growth strategy that enables the joint tree of the skeleton to expand adaptively. Specifically, our method, called ToMiE, consists of parent joints localization and external joints optimization. For parent joints localization, we employ a gradient-based approach guided by both LBS blending weights and motion kernels. Once the external joints are obtained, we proceed to optimize their transformations in SE(3) across different frames, enabling rendering and explicit animation. ToMiE manages to outperform other methods across various cases with garments, not only in rendering quality but also by offering free animation of grown joints, thereby enhancing the expressive ability of SMPL skeleton for a broader range of applications."
    },
    {
        "title": "DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale",
        "link_suffix": "/forum?id=b10lRabU9W",
        "link": "https://openreview.net/forum?id=b10lRabU9W",
        "pdf_link": "https://openreview.net/pdf?id=b10lRabU9W",
        "keywords": "circuit representation learning, graph transformer",
        "abstract": "Circuit representation learning has become pivotal in electronic design automation, enabling critical tasks such as testability analysis, logic reasoning, power estimation, and SAT solving. However, existing models face significant challenges in scaling to large circuits due to limitations like over-squashing in graph neural networks and the quadratic complexity of transformer-based models. To address these issues, we introduce \\textbf{DeepGate4}, a scalable and efficient graph transformer specifically designed for large-scale circuits. DeepGate4 incorporates several key innovations: (1) a partitioning method and update strategy tailored for circuit graphs, reducing memory complexity to sub-linear levels; (2) a GAT-based sparse transformer optimized for inference by leveraging the sparse nature of circuits; and (3) global and local structural encodings for circuits, along with a loss balancer that dynamically adjusts the weights of multitask losses to stabilize training. Our extensive experiments on the ITC99 and EPFL benchmarks show that DeepGate4 significantly surpasses state-of-the-art methods, achieving 15.5% and 31.1% performance improvements over the next-best models. Furthermore, the Fused-DeepGate4 variant reduces runtime by 35.1% and memory usage by 46.8%, making it highly efficient for large-scale circuit analysis. These results demonstrate the potential of DeepGate4 to handle complex EDA tasks while offering superior scalability and efficiency."
    },
    {
        "title": "OLMoE: Open Mixture-of-Experts Language Models",
        "link_suffix": "/forum?id=xXTkbTBmqq",
        "link": "https://openreview.net/forum?id=xXTkbTBmqq",
        "pdf_link": "https://openreview.net/pdf?id=xXTkbTBmqq",
        "keywords": "large language models, mixture-of-experts, open-source",
        "abstract": "We introduce OLMoE, a fully open, state-of-the-art language model leveraging sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but uses only 1B per input token. We pretrain it on 5 trillion tokens and further adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available models with similar active parameters, even surpassing larger ones like Llama2-13B-Chat and DeepSeekMoE-16B. We present novel findings on MoE training, define and analyze new routing properties showing high specialization in our model, and open-source all our work: model weights, training data, code, and logs."
    },
    {
        "title": "TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics",
        "link_suffix": "/forum?id=8e2LirwiJT",
        "link": "https://openreview.net/forum?id=8e2LirwiJT",
        "pdf_link": "https://openreview.net/pdf?id=8e2LirwiJT",
        "keywords": "datasets and benchmarks, temporal graph learning",
        "abstract": "Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. \nHowever, these datasets often feature excessive repeated edges and lack complex sequential dynamics, a key characteristic inherent in many real-world applications such as recommender systems and ``Who-To-Follow'' on social networks. This oversight has led existing methods to inadvertently downplay the importance of learning sequential dynamics, focusing primarily on predicting repeated edges.In this study, we demonstrate that existing methods, such as GraphMixer and DyGFormer, are inherently incapable of learning simple sequential dynamics, such as ``a user who has followed OpenAI and Anthropic is more likely to follow AI at Meta next.'' Motivated by this issue, we introduce the \\underline{T}emporal \\underline{G}raph \\underline{B}enchmark with \\underline{Seq}uential Dynamics (TGB-Seq), a new benchmark carefully curated to minimize repeated edges, challenging models to learn sequential dynamics and generalize to unseen edges. TGB-Seq comprises large real-world datasets spanning diverse domains, including e-commerce interactions, movie ratings, business reviews, social networks, citation networks and web link networks. Benchmarking experiments reveal that current methods usually suffer significant performance degradation and incur substantial training costs on TGB-Seq, posing new challenges and opportunities for future research. The datasets and benchmarking code are available athttps://anonymous.4open.science/r/TGB-Seq-3F23."
    },
    {
        "title": "Rethinking the Expressiveness of GNNs: A Computational Model Perspective",
        "link_suffix": "/forum?id=7ZaSRZVsbb",
        "link": "https://openreview.net/forum?id=7ZaSRZVsbb",
        "pdf_link": "https://openreview.net/pdf?id=7ZaSRZVsbb",
        "keywords": "Graph Neural Networks, Expressive Power, Computational Model, Weisfeiler-Lehman Test",
        "abstract": "Graph Neural Networks (GNNs) are extensively employed in graph machine learning, with considerable research focusing on their expressiveness. Current studies often assess GNN expressiveness by comparing them to the Weisfeiler-Lehman (WL) tests or classical graph algorithms. However, we identify three key issues in existing analyses: (1) some studies use preprocessing to enhance expressiveness but overlook its computational costs; (2) some claim the anonymous WL test's limited power while enhancing expressiveness using non-anonymous features, creating a mismatch; and (3) some characterize message-passing GNNs (MPGNNs) with the CONGEST model but make unrealistic assumptions about computational resources, allowing $\\textsf{NP-Complete}$ problems to be solved in $O(m)$ depth. We contend that a well-defined computational model is urgently needed to serve as the foundation for discussions on GNN expressiveness. To address these issues, we introduce the Resource-Limited CONGEST (RL-CONGEST) model, incorporating optional preprocessing and postprocessing to form a framework for analyzing GNN expressiveness. Our framework sheds light on computational aspects, including the computational hardness of hash functions in the WL test and the role of virtual nodes in reducing network capacity. Additionally, we suggest that high-order GNNs correspond to first-order model-checking problems, offering new insights into their expressiveness."
    },
    {
        "title": "LoRA Unleashed: Effortlessly Advancing from Low to Arbitrary Rank",
        "link_suffix": "/forum?id=HGxGCjqnDd",
        "link": "https://openreview.net/forum?id=HGxGCjqnDd",
        "pdf_link": "https://openreview.net/pdf?id=HGxGCjqnDd",
        "keywords": "low-rank adaptation, parameter-efficient fine-tuning, sparse learning, large language models",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a prominent technique for fine-tuning large foundation models, facilitating a reduction in trainable parameters through the utilization of low-rank matrices to represent weight changes $\\mathbf{A}$ and $\\mathbf{B}$ (\\textit{i.e.,} $\\Delta \\mathbf{W} = \\mathbf{B} \\mathbf{A}$). Although LoRA has demonstrated considerable success, its expressiveness is inherently limited by the constrained capacity of its low-rank structure. To ameliorate this limitation, we introduce \\underline{Fo}urier-based Flexible \\underline{R}ank \\underline{A}daptation (FoRA), which harnesses the robust expressiveness of the Fourier basis to re-parameterize $\\mathbf{A}$ and $\\mathbf{B}$ from a sparse spectral subspace. Utilizing FoRA, adaptation matrices can overcome conventional rank limitations, achieving up to a 15x reduction in the parameter budget. We illustrate that FoRA achieves an optimal balance of efficiency and performance across various tasks, including natural language understanding,  mathematical reasoning, commonsense reasoning, and image classification. Our codes are available athttps://anonymous.4open.science/r/FoRA-0E9C."
    },
    {
        "title": "ContraDiff: Planning Towards High Return States via Contrastive Learning",
        "link_suffix": "/forum?id=XMOaOigOQo",
        "link": "https://openreview.net/forum?id=XMOaOigOQo",
        "pdf_link": "https://openreview.net/pdf?id=XMOaOigOQo",
        "keywords": "Offline Reinforcement Learning, Decision Making, Diffusion Models, Machine Learning",
        "abstract": "The performance of offline reinforcement learning (RL) is sensitive to the proportion of high-return trajectories in the offline dataset. However, in many simulation environments and real-world scenarios, there are large ratios of low-return trajectories rather than high-return trajectories, which makes learning an efficient policy challenging. In this paper, we propose a method called Contrastive Diffuser (ContraDiff) to make full use of low-return trajectories and improve the performance of offline RL algorithms. Specifically, ContraDiff groups the states of trajectories in the offline dataset into high-return states and low-return states and treats them as positive and negative samples correspondingly. Then, it designs a contrastive mechanism to pull the planned trajectory of an agent toward high-return states and push them away from low-return states. Through the contrast mechanism, trajectories with low returns can serve as negative examples for policy learning, guiding the agent to avoid areas associated with low returns and achieve better performance. Through the contrast mechanism, trajectories with low returns provide a ``counteracting force'' guides the agent to avoid areas associated with low returns and achieve better performance.\nExperiments on 27 sub-optimal datasets demonstrate the effectiveness of our proposed method. Our code is publicly available at \\url{https://anonymous.4open.science/r/ContraDiff}."
    },
    {
        "title": "LUDVIG: Learning-free Uplifting of 2D Visual Features to Gaussian Splatting Scenes",
        "link_suffix": "/forum?id=GqBO71SPjL",
        "link": "https://openreview.net/forum?id=GqBO71SPjL",
        "pdf_link": "https://openreview.net/pdf?id=GqBO71SPjL",
        "keywords": "3D vision, vision foundation models, graph diffusion",
        "abstract": "We address the task of uplifting visual features or semantic masks from 2D vision models to 3D scenes represented by Gaussian Splatting. Whereas common approaches rely on iterative optimization-based procedures, we show that a simple yet effective aggregation technique yields excellent results. Applied to semantic masks from Segment Anything (SAM), our uplifting approach leads to segmentation quality comparable to the state of the art. We then extend this method to generic DINOv2 features, integrating 3D scene geometry through graph diffusion, and achieve competitive segmentation results despite DINOv2 not being trained on millions of annotated masks like SAM."
    },
    {
        "title": "FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine Learning Force Fields",
        "link_suffix": "/forum?id=sfi2j1Ot6j",
        "link": "https://openreview.net/forum?id=sfi2j1Ot6j",
        "pdf_link": "https://openreview.net/pdf?id=sfi2j1Ot6j",
        "keywords": "Machine Learning Force Fields, Equivariant Graph Neural Network, Clebsch-Gordan Transform",
        "abstract": "Machine Learning Force Fields (MLFFs) are of great importance for chemistry, physics, materials science, and many other related fields. The Clebsch\u2013Gordan transform (CG transform) effectively encodes many-body interactions and is thus an important building block for many models of MLFFs. However, the permutation-equivariance requirement of MLFFs limits the design space of CG transform, that is, intensive CG transform has to be conducted for each neighboring edge and the operations should be performed in the same manner for all edges. Freeing up the design space can greatly improve the model's expressiveness while simultaneously decreasing computational demands.\nTo reach this goal, we utilize a mathematical proposition, invariance transitivity, to show that implementing the CG transform layer on the permutation-invariant abstract edges allows complete freedom in the design of the layer without compromising the overall permutation equivariance. Developing on this free design space, we further propose group CG transform with sparse path, abstract edges shuffling, and attention enhancer to form a powerful and efficient CG transform layer. Our method, known asFreeCG, achieves state-of-the-art (SOTA) results in force prediction for MD17, rMD17, MD22, and is well extended to property prediction in QM9 datasets with several improvements greater than 15% and the maximum beyond 20%. The extensive real-world applications showcase high practicality. FreeCG introduces a novel paradigm for carrying out efficient and expressive CG transform in future geometric network designs. To demonstrate this, the recent SOTA, QuinNet, is also enhanced under our paradigm. Code and checkpoints will be publicly available."
    },
    {
        "title": "Diffusion2: Dynamic 3D Content Generation via Score Composition of Video and Multi-view Diffusion Models",
        "link_suffix": "/forum?id=fectsEG2GU",
        "link": "https://openreview.net/forum?id=fectsEG2GU",
        "pdf_link": "https://openreview.net/pdf?id=fectsEG2GU",
        "keywords": "4D generation, diffusion model, generative model, video diffusion",
        "abstract": "Recent advancements in 3D generation are predominantly propelled by improvements in 3D-aware image diffusion models. These models are pretrained on Internet-scale image data and fine-tuned on massive 3D data, offering the capability of producing highly consistent multi-view images. However, due to the scarcity of synchronized multi-view video data, it remains challenging to adapt this paradigm to 4D generation directly. Despite that, the available video and 3D data are adequate for training video and multi-view diffusion models separately that can provide satisfactory dynamic and geometric priors respectively. To take advantage of both, this paper presents Diffusion$^2$, a novel framework for dynamic 3D content creation that reconciles the knowledge about geometric consistency and temporal smoothness from these models to directly sample dense multi-view multi-frame images which can be employed to optimize continuous 4D representation. Specifically, we design a simple yet effective denoising strategy via score composition of pretrained video and multi-view diffusion models based on the probability structure of the target image array. To alleviate the potential conflicts between two heterogeneous scores, we further introduce variance-reducing sampling via interpolated steps, facilitating smooth and stable generation. Owing to the high parallelism of the proposed image generation process and the efficiency of the modern 4D reconstruction pipeline, our framework can generate 4D content within few minutes. Notably, our method circumvents the reliance on expensive and hard-to-scale 4D data, thereby having the potential to benefit from the scaling of the foundation video and multi-view diffusion models. Extensive experiments demonstrate the efficacy of our proposed framework in generating highly seamless and consistent 4D assets under various types of conditions."
    },
    {
        "title": "Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention",
        "link_suffix": "/forum?id=Bjq4W7P2Us",
        "link": "https://openreview.net/forum?id=Bjq4W7P2Us",
        "pdf_link": "https://openreview.net/pdf?id=Bjq4W7P2Us",
        "keywords": "Large Vision-Language Models, Hallucination, Interpretability",
        "abstract": "Large Vision-Language Models (LVLMs) exhibit impressive capabilities in complex visual tasks but are prone to hallucination, especially in open-ended generation tasks. This paper explores why LVLMs tend to hallucinate and how to mitigate it. First, we conduct causal mediation analysis through counterfactual edits on specific modules in LVLMs. Our results disclose that Multi-Head Attention (MHA) modules contribute more to the probability of generating hallucination words than multi-layer perceptron modules. We then identify specific heads that are responsible for hallucination, referred to as hallucination heads. Second, we examine the behavior of hallucination heads. We find that they are concentrated in the middle and deeper layers, displaying a strong attention bias toward text tokens. Further, we show that the attention patterns of certain hallucination heads exhibit greater similarity to the base language model and change slowly during the instruction tuning process. Finally, we propose two simple yet effective methods to mitigate hallucination: one is training-free and can be applied directly during decoding, while the other involves fine-tuning. Both methods are targeted for hallucination heads to reduce their reliance on text tokens. Notably, our methods achieve up to 1.7x reduction in hallucination rate for the LLaVA-v1.5-7B model in COCO captioning task, outperforming existing baselines. Overall, our findings suggest that hallucinations in LVLMs are likely to stem from certain modules, and targeted interventions can effectively mitigate these issues."
    },
    {
        "title": "Dual-level Affinity Induced Embedding-free Multi-view Clustering with Joint-alignment",
        "link_suffix": "/forum?id=58T7xcTxJD",
        "link": "https://openreview.net/forum?id=58T7xcTxJD",
        "pdf_link": "https://openreview.net/pdf?id=58T7xcTxJD",
        "keywords": "Mulit-view Clustering, Large-scale Clustering, Anchor Clustering",
        "abstract": "Despite remarkable progress, there still exist several limitations in current multi-view clustering (MVC) techniques. Specially, they generally focus only on the affinity relationship between anchors and samples, while overlooking that between anchors. Moreover, due to the lack of data labels, the cluster order is inconsistent across views and accordingly anchors encounter misalignment issue,  which will confuse the graph structure and disorganize cluster representation. Even worse, it typically brings variance during forming embedding, degenerating the stability of clustering results.   In response to these concerns, in the paper we propose a MVC approach named DLA-EF-JA. Concretely, we explicitly exploit the geometric properties between anchors via  self-expression learning skill, and utilize topology learning strategy to feed captured anchor-anchor features into anchor-sample graph  so as to explore the manifold structure hidden within samples  more adequately.  To reduce the misalignment risk, we introduce a permutation mechanism for each view to jointly rearrange anchors according to respective view  characteristics. Besides not involving selecting the baseline view, it also can coordinate with anchors in the unified framework and thereby facilitate the learning of anchors.  Further, rather than forming embedding and then performing spectral  partitioning, based on the criterion that samples and clusters should be hard assignment, we manage to construct the cluster labels directly from original samples using the binary strategy,  not only preserving the data diversity but avoiding variance. Experiments on multiple publicly available datasets confirm the effectiveness of our DLA-EF-JA."
    },
    {
        "title": "Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities",
        "link_suffix": "/forum?id=84pDoCD4lH",
        "link": "https://openreview.net/forum?id=84pDoCD4lH",
        "pdf_link": "https://openreview.net/pdf?id=84pDoCD4lH",
        "keywords": "vision-language models, spatial reasoning, multimodal reasoning",
        "abstract": "Spatial expressions in situated communication can be ambiguous, as their meanings vary depending on the frames of reference (FoR) adopted by speakers and listeners. While spatial language understanding and reasoning by vision-language models (VLMs) have gained increasing attention, potential ambiguities in these models are still under-explored. To address this issue, we present the COnsistent Multilingual Frame Of Reference Test (COMFORT), an evaluation protocol to systematically assess the spatial reasoning capabilities of VLMs. We evaluate nine state-of-the-art VLMs using COMFORT. Despite showing some alignment with English conventions in resolving ambiguities, our experiments reveal significant shortcomings of VLMs: notably, the models (1) exhibit poor robustness and consistency, (2) lack the flexibility to accommodate multiple FoRs, and (3) fail to adhere to language-specific or culture-specific conventions in cross-lingual tests, as English tends to dominate other languages. With a growing effort to align vision-language models with human cognitive intuitions, we call for more attention to the ambiguous nature and cross-cultural diversity of spatial reasoning."
    },
    {
        "title": "ETC: Towards Training-Efficient Video Synthesis with Exploiting Temporal Capabilities of Spatial Attention",
        "link_suffix": "/forum?id=0lVQBMhsPG",
        "link": "https://openreview.net/forum?id=0lVQBMhsPG",
        "pdf_link": "https://openreview.net/pdf?id=0lVQBMhsPG",
        "keywords": "Efficient Video Generation, Video Diffusion Model",
        "abstract": "Recently, synthesizing video from the text, i.e, Text-to-Video (T2V), has demonstrated remarkable progress by transferring the pre-trained Text-to-Image (T2I) diffusion models to the video domain, whose core is to add new temporal layers for capturing temporal information. However, these additional layers inevitably incur extra computational overhead, as they need to be trained from scratch on large-scale video datasets. Instead of retraining these costly layers, we conjecture whether temporal information can be learned from the original T2I model with only Spatial Attention. To this end, our theoretical and experimental explorations reveal that Spatial Attention has a strong potential for temporal modeling and greatly promotes training efficiency. Inspired by it, we propose ETC, a new T2V framework that achieves high fidelity and high efficiency in terms of training and inference. Specifically, to adapt the video to the spatial attention of T2I, we first design a novel temporal-to-spatial transfer strategy to organize entire video frames into a spatial grid. Then, we devise a simple yet effective Spatial-Temporal Mixed Embedding, to distinguish the inter-frame and intra-frame features. Benefiting from the above strategy that actually reduces the model's dependence on the text-video pairing dataset, we present a data-efficient strategy, Triple-Data (caption-image, label-image, and caption-video pairs) fusion that can achieve better performance with a small amount of video data for training. Extensive experiments show the superiority of our method over the four strong SOTA methods in terms of quality and efficiency, particularly improving FVD by 49% on average with only 1% training dataset."
    },
    {
        "title": "FAST: Federated Average with Snapshot Unleashes Arbitrary Client Participation",
        "link_suffix": "/forum?id=JUGLP5L8F3",
        "link": "https://openreview.net/forum?id=JUGLP5L8F3",
        "pdf_link": "https://openreview.net/pdf?id=JUGLP5L8F3",
        "keywords": "Federated Learning, Optimization",
        "abstract": "Federated Learning (FL) provides a flexible distributed platform where numerous clients with high degrees of heterogeneity in data and system can collaborate to learn a model jointly. Previous research has shown that Federated Learning is effective in handling diverse data, but often assumes idealized conditions. Specifically, client participation is often simplified in these studies, while real-world factors make it difficult to predict or design individual client participation. This complexity often diverges from the ideal client participation assumption, rendering an unknown pattern of client participation, referred to asarbitrary client participation. Hence, it is an important open problem to explore the impact of client participation and find a lightweight mechanism to enable arbitrary client participation in FL. In this paper, we first empirically investigate the influence of client participation on FL, revealing that FL algorithms are significantly impacted by arbitrary client participation. Afterward, to alleviate the influence, we propose a lightweight solution, Federated Average with Snapshot (FAST), to unleash the almost arbitrary client participation for FL. It can seamlessly integrate with other classic FL algorithms. Specifically, FAST enforces the clients to take a snapshot once in a while and facilitates arbitrary client participation for the majority of the training process. We show the convergence rates of FAST in non-convex and strongly-convex cases, which could match the rates with those in ideal client participation. Furthermore, we empirically introduce an adaptive strategy for dynamically configuring the snapshot frequency, tailored to accommodate diverse FL systems. Our extensive numerical results demonstrate that our FAST algorithm attains significant improvements under the conditions of arbitrary client participation and highly heterogeneous data."
    },
    {
        "title": "Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive learning",
        "link_suffix": "/forum?id=vFanHFE4Qv",
        "link": "https://openreview.net/forum?id=vFanHFE4Qv",
        "pdf_link": "https://openreview.net/pdf?id=vFanHFE4Qv",
        "keywords": "representation learning, biology, neuroscience, contrastive learning",
        "abstract": "Unlocking the secrets of neuronal activity\u2014what information is conveyed by neural activities\u2014is both a grand challenge and a holy grail of neuroscience. Inspired by Plato's cave allegory and Platonic Representation Hypothesis (PRH), each neuron, according to its intrinsic properties, maps different stimuli into distinct dynamic activities. Significant strides have been made in modeling time-varying dynamics, while decoupling the intrinsic properties of individual neurons from the varying signals remains a nascent field. These intrinsic properties, such as molecular profiles and morphologies, are crucial for comprehensive neuron characterization but are typically elusive due to the complexity and expense of biotechnological extraction methods. PRH posits that representations of different activity segments of the same neuron converge, while segments from inherently dissimilar neurons diverge, thereby allowing the intrinsic properties of each neuron to be represented. In this paper, we introduce NeurPIR, a novel self-supervised multi-segment contrastive learning approach to achieve the objective of PRH, obtaining a Platonic Intrinsic Representation for each neuron from population dynamics. To validate the efficacy of the representations, we firstly simulated neuron population dynamics data using the Izhikevich model and successfully confirmed that NeurPIR captures the preset hyperparameters of each neuron. Furthermore, we applied NeurPIR to two real neuron dynamics datasets, including spatial transcriptomics-derived neuron type annotations and the brain area where each neuron is located. The representations learned by NeurPIR are not only able to predict neuron type and location but also and exhibit robustness on out of domain data (Unseen Animals). This underscores the potential of our approach in advancing neuroscientific research."
    },
    {
        "title": "Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction",
        "link_suffix": "/forum?id=NuHYh4YKNe",
        "link": "https://openreview.net/forum?id=NuHYh4YKNe",
        "pdf_link": "https://openreview.net/pdf?id=NuHYh4YKNe",
        "keywords": "Generative Models, Novel View Synthesis, Camera Pose Estimation",
        "abstract": "Spatial intelligence is the ability of a machine to perceive, reason, and act in three dimensions within space and time.\nRecent advancements in large-scale auto-regressive models have demonstrated remarkable capabilities across various reasoning tasks. However, these models often struggle with fundamental aspects of spatial reasoning, particularly in answering questions like \"Where am I?\" and \"What will I see?\". While some attempts have been done, existing approaches typically treat them as separate tasks, failing to capture their interconnected nature. In this paper, we present GST, a novel auto-regressive framework that jointly addresses spatial localization and view prediction. Our model simultaneously estimates the camera pose from a single image and predicts the view from a new camera pose, effectively bridging the gap between spatial awareness and visual prediction. The proposed innovative camera tokenization method enables the model to learn the joint distribution of 2D projections and their corresponding spatial perspectives in an auto-regressive manner. This unified training paradigm demonstrates that joint optimization of pose estimation and novel view synthesis leads to improved performance in both tasks, for the first time, highlighting the inherent relationship between spatial awareness and visual prediction."
    },
    {
        "title": "Ultra-Low Accumulation Precision Inference with Block Floating Point Arithmetic",
        "link_suffix": "/forum?id=Dzamphz35c",
        "link": "https://openreview.net/forum?id=Dzamphz35c",
        "pdf_link": "https://openreview.net/pdf?id=Dzamphz35c",
        "keywords": "accumulation precision; block floating-point quantization; MAC; deep learning",
        "abstract": "Block Floating Point (BFP) quantization offers a hardware-efficient numerical range trade-off. Previous studies have quantized weights and activations to an extremely low precision using the BFP arithmetic. However, as the precision of weights and activations is reduced, we have identified that accumulation becomes a hardware bottleneck in the BFP MAC. Nevertheless, existing attempts to decrease the precision of accumulation in matrix multiplication have generally preserved model performance through training with a pre-selected, fixed accumulation precision. Nonetheless, selecting an unduly low precision leads to notable performance degradation, and these studies lack an effective approach to establish the lower precision limit, potentially incurring considerable training costs. Hence, we propose a statistical method to analyze the impact of reduced accumulation precision on the inference of deep learning applications. Due to the presence of fixed-point accumulation and floating-point accumulation in BFP matrix multiplication, we have formulated a set of equations to relate the data range of fixed-point multiply-accumulate operations and the effects of floating-point swamping to the parameters of BFP quantization, the length of accumulation, model weights, and the minimum number of bits required for accumulation, thereby determining the appropriate accumulation precision. Applied to MMLU Llama2-7B, SQuAD-v1.1 BERT-Large and BERT-Base and CIFAR-10 ResNet-50, our precision settings yield performance close to the FP32 baseline. Meanwhile, further precision reduction degrades performance, indicating our approach\u2019s proximity to precision limits. Guided by our equations, the hardware exhibited a 13.7%-28.7% enhancement in area and power efficiency over high-precision accumulation under identical quantization configuration, and it demonstrated a $10.3\\times$ area reduction and an $11.0\\times$ power reduction compared to traditional BFP16 implementations."
    },
    {
        "title": "The Critic as an Explorer: Lightweight and Provably Efficient Exploration for Deep Reinforcement Learning",
        "link_suffix": "/forum?id=Z7FLmWFUFo",
        "link": "https://openreview.net/forum?id=Z7FLmWFUFo",
        "pdf_link": "https://openreview.net/pdf?id=Z7FLmWFUFo",
        "keywords": "Reinforcement learning, exploration, embedding",
        "abstract": "Exploration remains a critical challenge in reinforcement learning (RL), with many existing methods either lacking theoretical guarantees or being computationally impractical for real-world applications. We introduce Litee, a lightweight algorithm that repurposes the value network in standard deep RL algorithms to effectively drive exploration without introducing additional parameters. Litee utilizes linear multi-armed bandit (MAB) techniques, enabling efficient exploration with provable sub-linear regret bounds while preserving the core structure of existing RL algorithms. Litee is simple to implement, requiring only around 10 lines of code. It also substantially reduces computational overhead compared to previous theoretically grounded methods, lowering the complexity from O(n^3) to O(d^3), where n is the number of network parameters and d is the size of the embedding in the value network. Furthermore, we propose Litee+, an extension that adds a small auxiliary network to better handle sparse reward environments, with only a minor increase in parameter count (less than 1%) and additional 10 lines of code. Experiments on the MiniHack suite and MuJoCo demonstrate that Litee and Litee+ empirically outperform state-of-the-art baselines, effectively bridging the gap between theoretical rigor and practical efficiency in RL exploration."
    },
    {
        "title": "Do Vision Models Develop Human-Like Progressive Difficulty Understanding?",
        "link_suffix": "/forum?id=frbfEqZX5R",
        "link": "https://openreview.net/forum?id=frbfEqZX5R",
        "pdf_link": "https://openreview.net/pdf?id=frbfEqZX5R",
        "keywords": "Understanding Image Classification; Adaptive Testing; Synthetic Benchmarks",
        "abstract": "When a human undertakes a test, their responses likely follow a pattern: if they answered an easy question (2x3) incorrectly, they would likely answer a more difficult one (2x3x4) incorrectly; and if they answered a difficult question correctly, they would likely answer the easy one correctly. Anything else hints at memorization. Do current visual recognition models exhibit a similarly structured learning capacity? In this work, we consider the task of image classification and study if those models' responses follow that pattern. Since real images aren't labeled with difficulty, we first create a dataset of 100 categories, 10 attributes, and 3 difficulty levels using recent generative models: for each category (e.g., dog) and attribute (e.g., occlusion), we generate images of increasing difficulty (e.g., a dog without occlusion, a dog only partly visible). We find that most of the models do in fact behave similarly to the aforementioned pattern around 80-90% of the time. Using this property, we then present a new way to evaluate those models' image recognition ability. Instead of testing the model on every possible test image, we create an adaptive test akin to GRE, in which the model's performance on the current round of images determines the test images in the next round. This allows the model to skip over questions too easy/hard for itself, and helps us get its overall performance in fewer steps."
    },
    {
        "title": "NPF-kCT: Ak-center clustering solver with neural process filter for continuous POMDP-based object search",
        "link_suffix": "/forum?id=WYsNjw02DE",
        "link": "https://openreview.net/forum?id=WYsNjw02DE",
        "pdf_link": "https://openreview.net/pdf?id=WYsNjw02DE",
        "keywords": "robotics, planning under uncertainty, continuous Partially Observable Markov Decision Process, Neural Process",
        "abstract": "Efficiently searching for target objects in intricate environments poses a significant challenge for mobile robots, given factors like perception errors, limited field of view (FOV), and visual occlusion. To alleviate this difficulty, we formulate the object-search task as a high-dimensional Partially Observable Markov Decision Process (POMDP) with hybrid (continuous and discrete) action spaces in a 3D region. We propose a novel sampling-based online POMDP solver named Neural Process Filtered $k$-Center Clustering Tree (NPF-$k$CT). The optimal action is selected using Monte Carlo Tree Search (MCTS) in conjunction with a neural process network to filter out ineffective primitive actions (i.e., basic robot operations), alongside $k$-center clustering hypersphere discretization to efficiently refine high-dimensional continuous sub-action spaces. Adhering to the hierarchical optimistic optimization (HOO) concept, we leverage an upper-confidence bound (UCB) on the action value function within the hypersphere with estimated diameters to guide the MCTS expansion. We thoroughly tested our approach in Gazebo simulations across a range of target-finding scenarios using two robot simulators: Fetch and Stretch.  A comparative analysis with baseline methods  shows our approach achieves a higher success rate and faster target detection without increasing computational demands. We also validated our method on a real robot in an office environment. Project page:https://sites.google.com/view/npfkct."
    },
    {
        "title": "Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization",
        "link_suffix": "/forum?id=omrLHFzC37",
        "link": "https://openreview.net/forum?id=omrLHFzC37",
        "pdf_link": "https://openreview.net/pdf?id=omrLHFzC37",
        "keywords": "Federated Learning, Zeroth-order Optimization",
        "abstract": "Federated Learning (FL) offers a promising framework for collaborative and privacy-preserving machine learning across distributed data sources. However, the substantial communication costs associated with FL pose a significant challenge to its efficiency. Specifically, in each communication round, the communication costs scale linearly with the model's dimension, which presents a formidable obstacle, especially in large model scenarios. Despite various communication-efficient strategies, the intrinsic dimension-dependent communication cost remains a major bottleneck for current FL implementations. In this paper, we introduce a novel dimension-free communication strategy for FL, leveraging zeroth-order optimization techniques. We propose a new algorithm, DeComFL, which facilitates the transmission of only a constant number of scalar values between clients and the server in each communication round no matter in both uplink and downlink, thereby reducing the communication cost from $\\mathcal{O}(d)$ to $\\mathcal{O}(1)$, where $d$ is the dimension of the model parameters. Theoretically, in non-convex functions, we prove that our algorithm achieves state-of-the-art rates, which show a linear speedup of the number of clients and local steps under standard assumptions and dimension-free rate for low effective rank scenarios. Empirical evaluations through classic deep learning training and large language model fine-tuning substantiate significant reductions in communication overhead compared to traditional FL approaches. By DeComFL, we can achieve around 1MB level of total communication cost between the server and a client until convergence."
    },
    {
        "title": "FlexGen: Flexible Multi-View Generation from Text and Image Inputs",
        "link_suffix": "/forum?id=90z4EDqcmu",
        "link": "https://openreview.net/forum?id=90z4EDqcmu",
        "pdf_link": "https://openreview.net/pdf?id=90z4EDqcmu",
        "keywords": "Multi-view Generation; AI-based 3D modeling",
        "abstract": "In this work, we introduce FlexGen, a flexible framework designed to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both. FlexGen tackles the challenges of controllable multi-view synthesis through additional conditioning on 3D-aware text annotations.  We utilize the strong reasoning capabilities of GPT-4V to generate 3D-aware text annotations. By analyzing four orthogonal views of an object arranged as tiled multi-view images, GPT-4V can produce text annotations that include 3D-aware information with spatial relationship. \nBy integrating the control signal with proposed adaptive dual-control module, our model can generate multi-view images that correspond to the specified text.\nFlexGen supports multiple controllable capabilities, allowing users to modify text prompts to generate reasonable and corresponding unseen parts. Additionally, users can influence attributes such as appearance and material properties, including metallic and roughness.\nExtensive experiments demonstrate that our approach offers enhanced multiple controllability, marking a significant advancement over existing multi-view diffusion models. This work has substantial implications for fields requiring rapid and flexible 3D content creation, including game development, animation, and virtual reality."
    }
]