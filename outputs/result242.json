[{"title": "The Mutual Information Matrix in Hyperbolic Embedding and a Generalization Error Bound", "link_suffix": "/forum?id=4wpqmhh05N", "link": "https://openreview.net/forum?id=4wpqmhh05N", "pdf_link": "https://openreview.net/pdf?id=4wpqmhh05N", "keywords": "Hyperbolic embedding, Mutual information, Generalization error bounds", "abstract": "Representation learning is a crucial task of deep learning, which aims to project texts and other symbolic inputs into mathematical embedding. Traditional representation learning encodes symbolic data into an Euclidean space. However, the high dimensionality of the Euclidean space used for embedding words presents considerable computational and storage challenges. Hyperbolic space has emerged as a promising alternative for word embedding, which demonstrates strong representation and generalization capacities, particularly for latent hierarchies of language data. In this paper, we analyze the Skip-Gram Negative-sampling representation learning method in hyperbolic spaces, and explore the potential relationship between the mutual information and hyperbolic embedding. Furthermore, we establish generalization error bounds for hyperbolic embedding. These bounds demonstrate the dimensional parsimony of hyperbolic space and its relationship between the generalization error and the sample size. Finally, we conduct two experiments on the Wordnet dataset and the THUNews dataset, whose results further validate our theoretical properties.", "title_embedding_index": 12050, "title_abs_embedding_index": 12075}, {"title": "Calibrated Decision-Making through Large Language Model-Assisted Retrieval", "link_suffix": "/forum?id=nNQmZGjEVe", "link": "https://openreview.net/forum?id=nNQmZGjEVe", "pdf_link": "https://openreview.net/pdf?id=nNQmZGjEVe", "keywords": "calibration, RAG, LLMs", "abstract": "Recently, large language models (LLMs) have been increasingly used to support various decision-making tasks, assisting humans in making informed decisions. However, when LLMs confidently provide incorrect information, it can lead humans to make suboptimal decisions. To prevent LLMs from generating incorrect information on topics they are unsure of and to improve the accuracy of generated content, prior works have proposed Retrieval Augmented Generation (RAG), where external documents are referenced to generate responses. However, traditional RAG methods focus only on retrieving documents most relevant to the input query, without specifically aiming to ensure that the human user's decisions are well-calibrated. To address this limitation, we propose a novel retrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG), which ensures that decisions informed by the retrieved documents are well-calibrated. Then we empirically validate that CalibRAG improves calibration performance as well as accuracy, compared to other baselines across various datasets.", "title_embedding_index": 12051, "title_abs_embedding_index": 12076}, {"title": "Neural Superposition Networks", "link_suffix": "/forum?id=htOl3M7II8", "link": "https://openreview.net/forum?id=htOl3M7II8", "pdf_link": "https://openreview.net/pdf?id=htOl3M7II8", "keywords": "Scientific Machine Learning, Physics-Informed Neural Networks, Differential Equations", "abstract": "Machine learning models can be biased towards the solutions of given differential equations in two principal ways: through regularisation, or through architecture design. Recent research has successfully constrained neural network architectures to satisfy divergence-free fields and Laplace's equation in two dimensions. This work reinterprets these architectures as linear superpositions of general formulated solutions. The notion of superposition is then exploited to develop novel architectures which satisfy both these and novel differential equations. In addition to new architectures for Laplace's equation and divergence-free fields, we propose novel constraints apt for the heat equation, and even some nonlinear differential equations including Burgers' equation. Benchmarks of superposition-based approaches against previously published architectures and physics-informed regularisation approaches are presented. We find that embedding differential equation constraints directly into neural network architectures can lead to improved performance and hope our results motivate further development of neural networks architectures developed to adhere specifically to given differential constraints.", "title_embedding_index": 12052, "title_abs_embedding_index": 12077}, {"title": "Learning on One Mode: Addressing Multi-Modality in Offline Reinforcement Learning", "link_suffix": "/forum?id=upkxzurnLC", "link": "https://openreview.net/forum?id=upkxzurnLC", "pdf_link": "https://openreview.net/pdf?id=upkxzurnLC", "keywords": "Offline reinforcement learning, weighted imitation learning, multi-modality.", "abstract": "Offline reinforcement learning (RL) seeks to learn optimal policies from static datasets without interacting with the environment. A common challenge is handling multi-modal action distributions, where multiple behaviours are represented in the data. Existing methods often assume unimodal behaviour policies, leading to suboptimal performance when this assumption is violated. We propose Weighted Imitation Learning on One Mode (LOM), a novel approach that focuses on learning from a single, promising mode of the behaviour policy. By using a Gaussian mixture model to identify modes and selecting the best mode based on expected returns, LOM avoids the pitfalls of averaging over conflicting actions. Theoretically, we show that LOM improves performance while maintaining simplicity in policy learning. Empirically, LOM outperforms existing methods on standard D4RL benchmarks and demonstrates its effectiveness in complex, multi-modal scenarios.", "title_embedding_index": 12053, "title_abs_embedding_index": 12078}, {"title": "CityBench: Evaluating the Capabilities of Large Language Models for Urban Tasks", "link_suffix": "/forum?id=oIWN7eMhTb", "link": "https://openreview.net/forum?id=oIWN7eMhTb", "pdf_link": "https://openreview.net/pdf?id=oIWN7eMhTb", "keywords": "large language model, urban science, world model, benchmark, multi-modal", "abstract": "Recently, large language models (LLMs) with extensive general knowledge and powerful reasoning abilities have seen rapid development and widespread application. A systematic and reliable evaluation of LLMs or visual language model (VLMs) is a crucial step in applying and developing them for various fields. There have been some early explorations about the usability of LLMs for limited urban tasks, but a systematic and scalable evaluation benchmark is still lacking. The challenge in constructing a systematic evaluation benchmark for urban research lies in the diversity of urban data, the complexity of application scenarios and the highly dynamic nature of the urban environment. In this paper, we design CityBench, an interactive simulator based evaluation platform, as the first systematic benchmark for evaluating the capabilities of LLMs for diverse tasks in urban research. First, we build CityData to integrate the diverse urban data and CitySim to simulate fine-grained urban dynamics. Based on CityData and CitySim, we design 8 representative urban tasks in 2 categories of perception-understanding and decision-making as the CityBench. With extensive results from 30 well-known LLMs and VLMs in 13 cities around the world, we find that advanced LLMs and VLMs can achieve competitive performance in diverse urban tasks requiring commonsense and semantic understanding abilities, e.g., understanding the human dynamics and semantic inference of urban images. Meanwhile, they fail to solve the challenging urban tasks requiring professional knowledge and high-level reasoning abilities, e.g., geospatial prediction and traffic control task. These observations provide valuable perspectives for utilizing and developing LLMs in the future. The dataset, benchmark and source codes are openly accessible to the research community viahttps://github.com/CityBench24/CityBench.", "title_embedding_index": 12054, "title_abs_embedding_index": 12079}, {"title": "Towards Continual Domain Adaptation of Vision-language Models", "link_suffix": "/forum?id=Q4s7nFoowt", "link": "https://openreview.net/forum?id=Q4s7nFoowt", "pdf_link": "https://openreview.net/pdf?id=Q4s7nFoowt", "keywords": "continual learning, domain adaptation, vision-language models", "abstract": "Large-scale vision-language models have achieved remarkable performance on various downstream tasks. {Nevertheless, how to efficiently adapt vision-language models to new data distributions without re-training, \\ie,  domain incremental learning (DIL) of vision-language models, is still under-explored. Existing DIL methods for single modality are either not applicable to multi-modal settings or need exemplar buffers to store previous samples to avoid catastrophic forgetting, which is not memory-efficient.} To address these limitations, we propose an exemplar-free paradigm to improve DIL of vision-language models based on prompt-tuning. We theoretically analyze and decompose the problem into two optimization objectives. Guided by the theoretical insights, we propose a novel framework named {M}ultimodal {C}ontinual {D}omain {A}daptation (MCDA), which incorporates two strategies: Multimodal Domain Alignment (MDA) and Maximum Softmax Gating (MSG). MDA enhances cross-domain performance by aligning visual and language representation spaces, while MSG improves the accuracy of domain identification by gating through Softmax probability. Extensivev experimental results demonstrate that our method outperforms current state-of-the-art approaches.", "title_embedding_index": 12055, "title_abs_embedding_index": 12080}, {"title": "Evaluating and Improving Large Language Models on Graph Computation", "link_suffix": "/forum?id=Y1r9yCMzeA", "link": "https://openreview.net/forum?id=Y1r9yCMzeA", "pdf_link": "https://openreview.net/pdf?id=Y1r9yCMzeA", "keywords": "Large Language Model Evaluation, Graph Algorithms", "abstract": "The ``arms race'' of Large Language Models (LLMs) demands new benchmarks to examine their progresses. In this paper, we introduce GraphArena, a benchmarking tool designed to evaluate LLMs on real-world graph computational problems. It offers a suite of four polynomial-time tasks (e.g., Shortest Distance) and six NP-complete challenges (e.g., Travelling Salesman Problem). GraphArena features a rigorous evaluation framework that classifies LLM outputs as correct, suboptimal (feasible but not optimal), hallucinatory (properly formatted but infeasible), or missing. Evaluation of over 10 LLMs reveals that even top-performing LLMs struggle with larger, more complex graph problems and exhibit hallucination issues. We further explore four potential solutions to address this issue and improve LLMs on this benchmark, including chain-of-thought prompting, instruction tuning, code writing, and scaling test-time compute, each demonstrating unique strengths and limitations.", "title_embedding_index": 12056, "title_abs_embedding_index": 12081}, {"title": "GigaSpeech 2: An Evolving, Large-Scale and Multi-domain ASR Corpus for Low-Resource Languages with Automated Crawling, Transcription and Refinement", "link_suffix": "/forum?id=SbK9GtQlVg", "link": "https://openreview.net/forum?id=SbK9GtQlVg", "pdf_link": "https://openreview.net/pdf?id=SbK9GtQlVg", "keywords": "multilingual, low-resource language, large-scale speech dataset, speech recognition, noisy student training", "abstract": "The evolution of speech technology has been spurred by the rapid increase in dataset sizes. Traditional speech models generally depend on a large amount of labeled training data, which is scarce for low-resource languages. This paper presents GigaSpeech 2, a large-scale, multi-domain, multilingual speech recognition corpus. It is designed for low-resource languages and does not rely on paired speech and text data. GigaSpeech 2 comprises about 30,000 hours of automatically transcribed speech, including Thai, Indonesian, and Vietnamese, gathered from unlabeled YouTube videos. We also introduce an automated pipeline for data crawling, transcription, and label refinement. Specifically, this pipeline uses Whisper for initial transcription and TorchAudio for forced alignment, combined with multi-dimensional filtering for data quality assurance. A modified Noisy Student Training is developed to further refine flawed pseudo labels iteratively, thus enhancing model performance. Experimental results on our manually transcribed evaluation set and two public test sets from Common Voice and FLEURS confirm our corpus's high quality and broad applicability. Notably, ASR models trained on GigaSpeech 2 can reduce the word error rate for Thai, Indonesian, and Vietnamese on our challenging and realistic YouTube test set by 25% to 40% compared to the Whisper large-v3 model, with merely 10% model parameters. Furthermore, our ASR models trained on GigaSpeech 2 yield superior performance compared to commercial services. We believe that our newly introduced corpus and pipeline will open a new avenue for low-resource speech recognition and significantly facilitate research in this area.", "title_embedding_index": 12057, "title_abs_embedding_index": 12082}, {"title": "Learning Label Distribution with Subtasks", "link_suffix": "/forum?id=GMu1yJST86", "link": "https://openreview.net/forum?id=GMu1yJST86", "pdf_link": "https://openreview.net/pdf?id=GMu1yJST86", "keywords": "label distribution learning, subtask, label polysemy", "abstract": "Label distribution learning (LDL) is a novel learning paradigm that emulates label polysemy by assigning label distributions over the label space. However, recent LDL work seems to exhibit a notable contradiction: 1) some existing LDL methods employ auxiliary tasks to enhance performance, which narrows their focus to specific domains, thereby lacking generalization capability; 2) conversely, LDL methods without auxiliary tasks rely on losses tailored solely to label distributions of the primary task, lacking additional supervised information to guide the learning process. In this paper, we propose $\\mathcal{S}$-LDL, a novel and minimalist solution that partitions the label distribution of the primary task into subtask label distributions, i.e., a form of pseudo-supervised information, to reconcile the above contradiction. $\\mathcal{S}$-LDL encompasses two key aspects: 1) an algorithm capable of generating subtasks without any extra knowledge, with subtasks deemed valid and reconstructable via our analysis; and 2) a plug-and-play framework seamlessly compatible with existing LDL methods, and even adaptable to derivative tasks of LDL. Experiments demonstrate that $\\mathcal{S}$-LDL is effective and efficient. To the best of our knowledge, this represents the first endeavor to address LDL via subtasks. The code will soon be available on GitHub to facilitate reproducible research.", "title_embedding_index": 12058, "title_abs_embedding_index": 12083}, {"title": "GAMformer: In-Context Learning for Generalized Additive Models", "link_suffix": "/forum?id=7heZQqlY5t", "link": "https://openreview.net/forum?id=7heZQqlY5t", "pdf_link": "https://openreview.net/pdf?id=7heZQqlY5t", "keywords": "interpretable machine learning, in-context learning, synthetic data, generalized additive models, gams, glassbox machine learning", "abstract": "Generalized Additive Models (GAMs) are widely recognized for their ability to create fully interpretable machine learning models for tabular data. Traditionally, training GAMs involves iterative learning algorithms, such as splines, boosted trees, or neural networks, which refine the additive components through repeated error reduction. In this paper, we introduce \\textit{GAMformer}, the first method to leverage in-context learning to estimate shape functions of a GAM in a single forward pass, representing a significant departure from the conventional iterative approaches to GAM fitting. Building on previous research applying in-context learning to tabular data, we exclusively use complex, synthetic data to train GAMformer, yet find it extrapolates well to real-world data. Our experiments show that GAMformer performs on par with other leading GAMs across various classification benchmarks while generating highly interpretable shape functions.", "title_embedding_index": 12059, "title_abs_embedding_index": 12084}, {"title": "OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation Learning", "link_suffix": "/forum?id=SM1guXel3E", "link": "https://openreview.net/forum?id=SM1guXel3E", "pdf_link": "https://openreview.net/pdf?id=SM1guXel3E", "keywords": "Data Augmentation, Image Classification, Mixup, Vision Transformer, Benchmark", "abstract": "Mixup augmentation has emerged as a widely used technique for improving the generalization ability of deep neural networks (DNNs). However, the lack of standardized implementations and benchmarks has impeded recent progress, resulting in poor reproducibility, unfair comparisons, and conflicting insights. In this paper, we introduce OpenMixup, the first mixup augmentation codebase and benchmark for visual representation learning. Specifically, we train 18 representative mixup baselines from scratch and rigorously evaluate them across 11 image datasets of varying scales and granularity, ranging from fine-grained scenarios to complex non-iconic scenes. We also open-source our modular codebase including a collection of popular vision backbones, optimization strategies, and analysis toolkits, which not only supports the benchmarking but enables broader mixup applications beyond classification, such as self-supervised learning and regression tasks. Through experiments and empirical analysis, we gain observations and insights on mixup performance-efficiency trade-offs, generalization, and optimization behaviors, and thereby identify preferred choices for different needs. To the best of our knowledge, OpenMixup has facilitated several recent studies. We believe this work can further advance reproducible mixup augmentation research and thereby lay a solid ground for future progress in the community. The source code will be publicly available.", "title_embedding_index": 12060, "title_abs_embedding_index": 12085}, {"title": "Agnostic Sharpness-Aware Minimization", "link_suffix": "/forum?id=0aTIvSJ83I", "link": "https://openreview.net/forum?id=0aTIvSJ83I", "pdf_link": "https://openreview.net/pdf?id=0aTIvSJ83I", "keywords": "sharpness-aware, agnostic model, optimizer, MAML, SAM", "abstract": "Sharpness-aware minimization (SAM) has been instrumental in improving deep neural network training by minimizing both the training loss and the sharpness of the loss landscape, leading the model into flatter minima that are associated with better generalization properties. In another aspect, Model-Agnostic Meta-Learning (MAML) is a framework designed to improve the adaptability of models. MAML optimizes a set of meta-models that are specifically tailored for quick adaptation to multiple tasks with minimal fine-tuning steps and can generalize well with limited data. In this work, we explore the connection between SAM and MAML in enhancing model generalization. We introduce Agnostic-SAM, a novel approach that combines the principles of both SAM and MAML. Agnostic-SAM adapts the core idea of SAM by optimizing the model toward wider local minima using training data, while concurrently maintaining low loss values on validation data. By doing so, it seeks flatter minima that are not only robust to small perturbations but also less vulnerable to data distributional shift problems. Our experimental results demonstrate that Agnostic-SAM significantly improves generalization over baselines across a range of datasets and under challenging conditions such as noisy labels or data limitation.", "title_embedding_index": 12061, "title_abs_embedding_index": 12086}, {"title": "Synthesizing Post-Training Data for LLMs through Multi-Agent Simulation", "link_suffix": "/forum?id=o83aL1nZJd", "link": "https://openreview.net/forum?id=o83aL1nZJd", "pdf_link": "https://openreview.net/pdf?id=o83aL1nZJd", "keywords": "large language models, llm alignment, multi-agent simulation, llm society", "abstract": "Post-training is essential for enabling large language models (LLMs) to follow human instructions. \nInspired by the recent success of using LLMs to simulate human society, we leverage multi-agent simulation to automatically generate diverse text-based scenarios, capturing a wide range of real-world human needs. \nWe introduce MATRIX, a multi-agent simulator that creates realistic and scalable scenarios. \nLeveraging these outputs, we introduce a novel scenario-driven instruction generator MATRIX-Gen for controllable and highly realistic data synthesis. Extensive experiments demonstrate that our framework effectively generates both general and domain-specific data. Notably, on AlpacaEval 2 and Arena-Hard benchmarks, Llama-3-8B-Base, post-trained on datasets synthesized by MATRIX-Gen with just 20K instruction-response pairs, outperforms Meta's Llama-3-8B-Instruct model, which was trained on over 10M pairs.", "title_embedding_index": 12062, "title_abs_embedding_index": 12087}, {"title": "Knowledge Enhanced Image Captioning for Fashion Products", "link_suffix": "/forum?id=ZVOGMy8Sd8", "link": "https://openreview.net/forum?id=ZVOGMy8Sd8", "pdf_link": "https://openreview.net/pdf?id=ZVOGMy8Sd8", "keywords": "Image captioning, Knowledge base, Visual language, Fashion description", "abstract": "The field of image captioning has witnessed a surge in attention, particularly in the context of e-commerce, where the exploration of automated fashion description has gained significant momentum. This growing interest can be attributed to the increasing influence of visual language and its impact on effective communication within the fashion industry.\nHowever, generating detailed and accurate natural language descriptions for fashion items remains a topic of intense discussion. This paper introduces an innovative approach that specifically addresses this challenge by proposing a method tailored to the requirements of the e-commerce domain. Our approach integrates a knowledge base into the widely adopted end-to-end architecture, thereby enhancing the availability of comprehensive data about fashion items.\nWe design a mode mapping network that facilitates the fusion of attribute features extracted from the knowledge base with image features. \nAdditionally, we introduce a filter strategy to enhance the quality of the generated descriptions by selecting the best result among the candidate sentences generated through beam search using a language model.\nThrough extensive experimentation and evaluation, our proposed method demonstrates superior performance in the task of fashion description, surpassing the performance of state-of-the-art approaches in this domain.", "title_embedding_index": 12063, "title_abs_embedding_index": 12088}, {"title": "SELU: Self-Learning Embodied MLLMs in Unknown Environments", "link_suffix": "/forum?id=VSfvQxPPB0", "link": "https://openreview.net/forum?id=VSfvQxPPB0", "pdf_link": "https://openreview.net/pdf?id=VSfvQxPPB0", "keywords": "embodied MLLM, self-learning, actor-critic", "abstract": "Recently, multimodal large language models (MLLMs) have demonstrated strong visual understanding and decision-making capabilities, enabling the exploration of autonomously improving MLLMs in unknown environments. However, external feedback like human or environmental feedback is not always available. To address this challenge, existing methods primarily focus on enhancing the decision-making capabilities of MLLMs through voting and scoring mechanisms, while little effort has been paid to improving the environmental comprehension of MLLMs in unknown environments. To fully unleash the self-learning potential of MLLMs, we propose a novel actor-critic self-learning paradigm, dubbed SELU, inspired by the actor-critic paradigm in reinforcement learning. The critic employs self-asking and hindsight relabeling to extract knowledge from interaction trajectories collected by the actor, thereby augmenting its environmental comprehension. Simultaneously, the actor is improved by the self-feedback provided by the critic, enhancing its decision-making. We evaluate our method in the AI2-THOR and VirtualHome environments, and SELU achieves critic improvements of approximately 28% and 30%, and actor improvements of about 20% and 24% via self-learning.", "title_embedding_index": 12064, "title_abs_embedding_index": 12089}, {"title": "Entropy-Based Uncertainty Modeling for Trajectory Prediction in Autonomous Driving", "link_suffix": "/forum?id=RflvsSxM0u", "link": "https://openreview.net/forum?id=RflvsSxM0u", "pdf_link": "https://openreview.net/pdf?id=RflvsSxM0u", "keywords": "Autonomous Driving, Trajectory Prediction, Uncertainty Quantification", "abstract": "In autonomous driving, accurate motion prediction is essential for safe and efficient motion planning. To ensure safety, planners must rely on reliable uncertainties in the future behavior of surrounding agents, yet this aspect has received limited attention. This paper addresses the problem of uncertainty modeling in trajectory prediction. We adopt a holistic approach that focuses on uncertainty quantification, decomposition, and the influence of model composition. Our method is based on a theoretically-grounded information-theoretic approach to measure uncertainty, allowing us to decompose total uncertainty into its aleatoric and epistemic components. We conduct extensive experiments on the nuScenes dataset to assess how different model architectures and configurations affect uncertainty quantification and model robustness. Our analysis thoroughly explores the uncertainty quantification capabilities of several state-of-the-art prediction models, examining the relationship between uncertainty and prediction error in both in- and out-of-distribution scenarios, as well as robustness in out-of-distribution.", "title_embedding_index": 12065, "title_abs_embedding_index": 12090}, {"title": "Uncovering Self-Emergent Similarity in Deep Vision Networks: A Systematic Framework", "link_suffix": "/forum?id=VyxlbbK8WV", "link": "https://openreview.net/forum?id=VyxlbbK8WV", "pdf_link": "https://openreview.net/pdf?id=VyxlbbK8WV", "keywords": "Similarity, Deep Vision Networks, Explainable Artificial Intelligence, Evaluation Metrics, Framework", "abstract": "Similarity is one of the most central constructs in psychology and neuroscience. Human perception of similarity evolves over time, revealing the correlated nature of real-world objects, and is crucial for the optimal use of our finite cognitive resources. At the same time, there is still a lack of a clear understanding about the emergence of similarity perception in deep vision models and to what extent it aligns with the human semantic perception. In this paper, we introduce Deep Similarity Inspector (DSI) - a systematic framework to inspect and visualize how deep vision networks develop their similarity perception during training. A set of experiments conducted with this framework provides evidence that both Convolutional Neural Networks' (CNNs) and Vision Transformers' (ViTs) develop a rich similarity perception during learning. The development has 3 phases (initial similarity surge, refinement, stabilization). Both CNNs and ViTs, in addition to gradual mistakes elimination, make improvements in the quality of mistakes being made (we call it the mistakes refinement phenomenon). Moreover, clear differences are found in the development dynamics between CNNs and ViTs, further highlighting the differences in their operation.", "title_embedding_index": 12066, "title_abs_embedding_index": 12091}, {"title": "Zero-shot Mixed Precision Quantization via Joint Optimization of Data Generation and Bit Allocation", "link_suffix": "/forum?id=OCHSgafZ1Y", "link": "https://openreview.net/forum?id=OCHSgafZ1Y", "pdf_link": "https://openreview.net/pdf?id=OCHSgafZ1Y", "keywords": "Zero-shot Quantization, Post-training quantization, mixed-precision quantization", "abstract": "Mixed-precision quantization (MPQ) aims to identify optimal bit-widths for layers to quantize a model.\nOn the other hand,\nzero-shot quantization (ZSQ) aims to learn a quantized model from a pre-trained full-precision model in a data-free manner, which is commonly done by generating a synthetic calibration set used for quantizing the full-precision model. While it is intuitive that there exists inherent correlation between the quality of the generated calibration dataset\nand the bit allocation to the model's layers, \nall existing frameworks treat them as separate problems. This paper proposes a novel method that jointly optimizes both the calibration set and the bit-width of each layer in the context of zero-shot quantization. Specifically, we first propose a novel data optimization approach that take into consideration the Gram-Gradient matrix constructed from the gradient vectors of calibration samples. We then propose a novel  scalable quadratic optimization-based approach to identify the model's bit-widths. These proposals will then be combined into a single framework to jointly optimize both the calibration data and the bit allocation to the model's layers.\nExperimental results on the ImageNet dataset demonstrate the proposed method's superiority compared to current state-of-the-art techniques in ZSQ.", "title_embedding_index": 12067, "title_abs_embedding_index": 12092}, {"title": "Continuous Ensemble Weather Forecasting with Diffusion models", "link_suffix": "/forum?id=ePEZvQNFDW", "link": "https://openreview.net/forum?id=ePEZvQNFDW", "pdf_link": "https://openreview.net/pdf?id=ePEZvQNFDW", "keywords": "weather forecasting, diffusion, ensemble forecasting", "abstract": "Weather forecasting has seen a shift in methods from numerical simulations to data-driven systems. While initial research in the area focused on deterministic forecasting, recent works have used diffusion models to produce skillful ensemble forecasts. These models are trained on a single forecasting step and rolled out autoregressively. However, they are computationally expensive and accumulate errors for high temporal resolution due to the many rollout steps. We address these limitations with Continuous Ensemble Forecasting, a novel and flexible method for sampling ensemble forecasts in diffusion models. The method can generate temporally consistent ensemble trajectories completely in parallel, with no autoregressive steps. Continuous Ensemble Forecasting can also be combined with autoregressive rollouts to yield forecasts at an arbitrary fine temporal resolution without sacrificing accuracy. We demonstrate that the method achieves competitive results for global weather forecasting with good probabilistic properties.", "title_embedding_index": 12068, "title_abs_embedding_index": 12093}, {"title": "VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning", "link_suffix": "/forum?id=jf7C7EGw21", "link": "https://openreview.net/forum?id=jf7C7EGw21", "pdf_link": "https://openreview.net/pdf?id=jf7C7EGw21", "keywords": "Robotics; Manipulation; Vision and tactile; Multi-modal pretraining; Reinforcement learning", "abstract": "Vision and touch are the most commonly used senses in human manipulation. While leveraging human manipulation videos for robotic task pretraining has shown promise in prior works, it is limited to image and language modalities and deployment to simple parallel grippers. In this paper, aiming to address the limitations, we collect a vision-tactile dataset by humans manipulating 10 daily tasks and 182 objects. In contrast with the existing datasets, our dataset is the first visual-tactile dataset for complex robotic manipulation skill learning. Also, we introduce a novel benchmark, featuring six complex dexterous manipulation tasks and a reinforcement learning-based vision-tactile skill learning framework. 17 non-pretraining and pretraining methods within the framework are designed and compared to investigate the effectiveness of different modalities and pertaining strategies. Key findings based on our benchmark results and analyses experiments include: 1) Despite the tactile modality used in our experiments being binary and sparse, including it directly in the policy training boosts the success rate by about 20% and joint pretraining it with vision gains a further 20%. 2) Joint pretraining visual-tactile modalities exhibits strong adaptability in unknown tasks and achieves robust performance among all tasks. 3) Using binary tactile signals with vision is robust to viewpoint setting, tactile noise, and the binarization threshold, which facilitates to the visual-tactile policy to be deployed in reality.", "title_embedding_index": 12069, "title_abs_embedding_index": 12094}, {"title": "ReFeR: Improving Evaluation and Reasoning through Hierarchy of Models", "link_suffix": "/forum?id=GDd5H92egZ", "link": "https://openreview.net/forum?id=GDd5H92egZ", "pdf_link": "https://openreview.net/pdf?id=GDd5H92egZ", "keywords": "Multi-agent framework, reasoning, evaluation, multimodal, hierarchy, LLMs, VLMs", "abstract": "Assessing the quality of outputs generated by generative models, such as large language models and vision language models, presents notable challenges. Traditional methods for evaluation typically rely on either human assessments, which are resource-intensive, or automatic metrics that often show a low correlation with human judgment. Another common approach is to use deep learning systems, which not only consume a substantial amount of compute and time but also require extensive training data. In this study, we introduce a tuning-free framework called ReFeR, designed to evaluate generative outputs, including both text and images, by leveraging a 2-level hierarchy of LLMs and VLMs themselves.  We rigorously evaluate our framework, ReFeR, across four diverse evaluation tasks. The framework not only improves the accuracy of these evaluations, surpassing previous benchmarks but also generates constructive feedback. Interestingly, the framework is also applicable to reasoning tasks. Experiments on four reasoning tasks demonstrate superior collective reasoning abilities of the framework. We present two variants of the framework: ReFeR-Turbo, optimized for accelerated performance, and ReFeR-Lite, offering a more cost-effective solution. ReFeR-Lite is $\\sim7.7\\times$ more efficient while being comparably accurate to ReFeR-Turbo.", "title_embedding_index": 12070, "title_abs_embedding_index": 12095}, {"title": "Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization", "link_suffix": "/forum?id=WZ0s2smcKP", "link": "https://openreview.net/forum?id=WZ0s2smcKP", "pdf_link": "https://openreview.net/pdf?id=WZ0s2smcKP", "keywords": "Interpretability, natural language processing, feature selection", "abstract": "Extracting a small subset of crucial rationales from the full input is a key problem in explainable natural language processing research.  The most widely used fundamental criterion for rationale extraction is the maximum mutual information (MMI) criterion. In this paper, we first demonstrate that MMI suffers from diminishing marginal returns. Once part of the rationale has been identified, finding the remaining portions contributes only marginally to increasing the mutual information, making it difficult to use MMI to locate the rest. In contrast to MMI that aims to reproduce the prediction, we seek to identify the parts of the input that the network can actually utilize.  This is achieved by comparing how different rationale candidates match the non-zero rank components of the weight matrix. The weight matrix of a neural network is typically low-rank. If an input is fully utilized by the network, it generally occupies the non-zero rank subspaces of the weight matrix, resulting in a representation with a high norm. Conversely, if an input primarily occupies the zero-rank subspaces of the weight matrix, its representation norm will approach zero, behaving like noise that the network cannot effectively utilize.  Building on this, we propose using the norms of rationale candidates as an alternative objective to MMI. Through experiments on four text classification datasets and one graph classification dataset using three network architectures (GRUs, BERT, and GCN), we show that our method outperforms MMI and its improved variants in identifying better rationales. We also compare our method with a representative LLM (llama-3.1-8b-instruct) and find that our simple method gets comparable results to it and can sometimes even outperform it. Our work represents a pioneering attempt to abandon MMI in the XAI field.", "title_embedding_index": 12071, "title_abs_embedding_index": 12096}, {"title": "Transformer Training Instability of Softmax and Lipschitz-Kernel Attentions", "link_suffix": "/forum?id=q541p2YLt2", "link": "https://openreview.net/forum?id=q541p2YLt2", "pdf_link": "https://openreview.net/pdf?id=q541p2YLt2", "keywords": "Transformer, linear attention, self-attention, optimization, training stability, entropy collapse", "abstract": "Transformers have been making significant progress across various domains, and recently, with scaling up of models like LLMs, they have achieved even greater success. Recent findings have shown that the softmax function in the self-attention used to re-weight the attention logits into probability vectors causes \\emph{attention entropy collapse}, where the attention is concentrated on a single token, and it leads to unstable training. In this work, we first demonstrate that the (non-Lipschitz) softmax-based attention leads to the attention entropy collapse but the \\emph{Lipschitz-kernel}-based attention does not. We show that the Lipschitzness of the attention plays an important role in keeping the attention entropy stable regardless of the variance of the attention logits. Moreover, we argue that the underlying reason why the attention entropy collapse leads to the training instability is that as the attention probabilities become more concentrated, it causes the attention matrix to gradually increase, leading to gradient exploding.", "title_embedding_index": 12072, "title_abs_embedding_index": 12097}, {"title": "DebugAgent: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging", "link_suffix": "/forum?id=l30moNjSY9", "link": "https://openreview.net/forum?id=l30moNjSY9", "pdf_link": "https://openreview.net/pdf?id=l30moNjSY9", "keywords": "slice discovery, interpretable error analysis, multimodal, robustness, model debug", "abstract": "Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this paper, we introduce DebugAgent, an automated framework for error slice discovery and model repair. DebugAgent first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process. It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration. Additionally, DebugAgent extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches. Extensive experiments across multiple domains \u2014 including image classification, pose estimation, and object detection \u2014 show that DebugAgent not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities.", "title_embedding_index": 12073, "title_abs_embedding_index": 12098}, {"title": "Counterfactual Causal Inference in Natural Language with Large Language Models", "link_suffix": "/forum?id=aya06N6R4W", "link": "https://openreview.net/forum?id=aya06N6R4W", "pdf_link": "https://openreview.net/pdf?id=aya06N6R4W", "keywords": "Causal structure discovery, Counterfactual inference, End-to-end, Large Language Models", "abstract": "Causal structure discovery methods are commonly applied to structured data where the causal variables are known and where statistical testing can be used to assess the causal relationships. By contrast, recovering a causal structure from unstructured natural language data such as news articles contains numerous challenges due to the absence of known variables or counterfactual data to estimate the causal links. Large Language Models (LLMs) have shown promising results in this direction but also exhibit limitations. This work investigates LLM's abilities to build causal graphs from text documents and perform counterfactual causal inference. We propose an end-to-end causal structure discovery and causal inference method from natural language: we first use an LLM to extract the instantiated causal variables from text data and build a causal graph. We merge causal graphs from multiple data sources to represent the most exhaustive set of causes possible. We then conduct counterfactual inference on the estimated graph. The causal graph conditioning allows reduction of LLM biases and better represents the causal estimands. We use our method to show that the limitations in the counterfactual causal reasoning abilities come from prediction errors and propose directions to mitigate them. We demonstrate the applicability of our method on real-world news articles.", "title_embedding_index": 12074, "title_abs_embedding_index": 12099}]