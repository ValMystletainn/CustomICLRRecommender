[
    {
        "title": "DDIL: Improved Diffusion Distillation with Imitation Learning",
        "link_suffix": "/forum?id=hGKuATIGhr",
        "link": "https://openreview.net/forum?id=hGKuATIGhr",
        "pdf_link": "https://openreview.net/pdf?id=hGKuATIGhr",
        "keywords": "Diffusion Models, Distillation, Imitation Learning",
        "abstract": "Diffusion models excel at generative modeling (e.g., text-to-image) but sampling requires multiple denoising network passes, limiting practicality. Diffusion distillation methods have shown promise by reducing the number of passes at the expense of quality of the generated samples but suffer from lack of diversity, quality, etc. . In this work we identify co-variate shift as one of reason for poor performance of multi-step distilled models from compounding error at inference time. To address co-variate shift, we formulate diffusion distillation within imitation learningDDILframework and enhance training distribution for distilling diffusion models on both data distribution (forward diffusion) and student induced distributions (backward diffusion). Training on data distribution helps to diversify the generations bypreserving marginal data distributionand training on student distribution addresses compounding error bycorrecting covariate shift. In addition, we adopt reflected diffusion formulation for distillation and demonstrate improved performance, stable training across different distillation methods. We show that DDIL and reflected diffusion formulation consistency improves on baseline algorithms of progressive distillation(PD), Latent consistency models(LCM)and Distribution Matching Distillation(DMD2)"
    },
    {
        "title": "ClinicalBench: Can LLMs Beat Traditional ML Models in Clinical Prediction?",
        "link_suffix": "/forum?id=o9SuQXZvNA",
        "link": "https://openreview.net/forum?id=o9SuQXZvNA",
        "pdf_link": "https://openreview.net/pdf?id=o9SuQXZvNA",
        "keywords": "LLMs, Clinical Prediction, Benchmark",
        "abstract": "Large Language Models (LLMs) hold great promise to revolutionize current clinical systems for their superior capacities on medical text processing tasks and medical licensing exams. Meanwhile, traditional ML models such as SVM and XGBoost have still been mainly adopted in clinical prediction tasks. An emerging question is Can LLMs beat traditional ML models in clinical prediction? Thus, we build a new benchmark ClinicalBench to comprehensively study the clinical predictive modeling capacities of both general-purpose and medical LLMs, and compare them with traditional ML models. ClinicalBench embraces three common clinical prediction tasks, two databases, 14 general-purpose LLMs, 8 medical LLMs, and 11 traditional ML models. Through extensive empirical investigation, we discover that both general-purpose and medical LLMs, even with different model scales, diverse prompting or fine-tuning strategies, still cannot beat traditional ML models in clinical prediction yet,  shedding light on their surprising but critical deficiency in clinical reasoning. We call for caution when practitioners adopt LLMs in clinical applications. ClinicalBench can be utilized to bridge the gap between LLMs' development for healthcare and real-world clinical practice. Code is here."
    },
    {
        "title": "Distributional Associations vs In-Context Reasoning: A Study of Feed-forward and Attention Layers",
        "link_suffix": "/forum?id=WCVMqRHWW5",
        "link": "https://openreview.net/forum?id=WCVMqRHWW5",
        "pdf_link": "https://openreview.net/pdf?id=WCVMqRHWW5",
        "keywords": "reasoning, in-context learning, associative memory, transformers, distribution shift",
        "abstract": "Large language models have been successful at tasks involving basic forms of in-context reasoning, such as generating coherent language, as well as storing vast amounts of knowledge. At the core of the Transformer architecture behind such models are feed-forward and attention layers, which are often associated to knowledge and reasoning, respectively. In this paper, we study this distinction empirically and theoretically in a controlled synthetic setting where certain next-token predictions involve both distributional and in-context information. We find that feed-forward layers tend to learn simple distributional associations such as bigrams, while attention layers focus on in-context reasoning. Our theoretical analysis identifies gradient noise as a key factor behind this discrepancy. Finally, we illustrate how similar disparities emerge in pre-trained models through ablations on the Pythia model family on simple reasoning tasks."
    },
    {
        "title": "DIPPER: Direct Preference Optimization for Primitive-Enabled Hierarchical Reinforcement Learning",
        "link_suffix": "/forum?id=mJKhn7Ey4y",
        "link": "https://openreview.net/forum?id=mJKhn7Ey4y",
        "pdf_link": "https://openreview.net/pdf?id=mJKhn7Ey4y",
        "keywords": "hierarchical reinforcement learning, preference learning",
        "abstract": "Hierarchical reinforcement learning (HRL) is an elegant framework for learning efficient control policies to perform complex robotic tasks, especially in sparse reward settings. However, concurrently learning policies at multiple hierarchical levels often suffers from training instability due to non-stationary behavior of lower-level primitives. In this work, we introduce DIPPER, an efficient hierarchical framework that leverages Direct Preference Optimization (DPO) to mitigate non-stationarity at the higher level, while using reinforcement learning to train the corresponding primitives at the lower level. We observe that directly applying DPO to the higher level in HRL is ineffective and leads to infeasible subgoal generation issues. To address this, we develop a novel, principled framework based on lower-level primitive regularization of upper-level policy learning. We provide a theoretical justification for the proposed framework utilizing bi-level optimization. The application of DPO also necessitates the development of a novel reference policy formulation for feasible subgoal generation. To validate our approach, we conduct extensive experimental analyses on a variety of challenging, sparse-reward robotic navigation and manipulation tasks. Our results demonstrate that DIPPER shows impressive performance and demonstrates an improvement of up to 40% over the baselines in complex sparse robotic control tasks."
    },
    {
        "title": "Model predictive control is almost optimal for restless bandits",
        "link_suffix": "/forum?id=EUAxxrxOM8",
        "link": "https://openreview.net/forum?id=EUAxxrxOM8",
        "pdf_link": "https://openreview.net/pdf?id=EUAxxrxOM8",
        "keywords": "Restless Multi-Armed Bandits, Markov Decision Processes, Constrained Optimization, Stochastic Control",
        "abstract": "We consider the discrete time infinite horizon average reward restless markovian bandit (RMAB) problem. We propose a model predictive control based non-stationary policy with a rolling computational horizon $\\tau$. At each time-slot, this policy solves a $\\tau$ horizon linear program whose first control value is kept as a control for the RMAB. Our solution requires minimal assumptions and quantifies the loss in optimality in terms of $\\tau$ and the number of arms, $N$. We show that its sub-optimality gap is $O(1/\\sqrt{N})$ in general, and $\\exp(-\\Omega{N})$ under a local-stability condition. Our proof is based on a framework from dynamic control known as dissipativity. Not only is our solution easy to implement but performs very well in practice when compared to the state of the art. Further, both our solution and our proof methodology can easily be generalized to more general constrained MDP settings and should thus, be of great interest to the burgeoning RMAB community."
    },
    {
        "title": "Proxy Denoising for Source-Free Domain Adaptation",
        "link_suffix": "/forum?id=FIj9IEPCKr",
        "link": "https://openreview.net/forum?id=FIj9IEPCKr",
        "pdf_link": "https://openreview.net/pdf?id=FIj9IEPCKr",
        "keywords": "Domain adaptation, source-free, multimodal proxy space, proxy confidence theory",
        "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain with no access to the source data. Inspired by the success of large Vision-Language (ViL) models in many applications, the latest research has validated ViL's benefit for SFDA by using their predictions as pseudo supervision. However, we observe that ViL's supervision could be noisy and inaccurate at an unknown rate, potentially introducing additional negative effects during adaption. To address this thus-far ignored challenge, we introduce a novel Proxy Denoising (ProDe) approach. The key idea is to leverage the ViL model as a proxy to facilitate the adaptation process towards the latent domain-invariant space. Concretely, we design a proxy denoising mechanism to correct ViL's predictions. This is grounded on a proxy confidence theory that models the dynamic effect of proxy's divergence against the domain-invariant space during adaptation. To capitalize the corrected proxy, we further derive a mutual knowledge distilling regularization. Extensive experiments show that ProDe significantly outperforms the current state-of-the-art alternatives under both conventional closed-set setting and the more challenging open-set, partial-set and generalized SFDA settings. Our code will be released."
    },
    {
        "title": "Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models",
        "link_suffix": "/forum?id=tc90LV0yRL",
        "link": "https://openreview.net/forum?id=tc90LV0yRL",
        "pdf_link": "https://openreview.net/pdf?id=tc90LV0yRL",
        "keywords": "Language Model Agents, Benchmark, Cybersecurity, Risk",
        "abstract": "Language Model (LM) agents for cybersecurity that are capable of autonomously identifying vulnerabilities and executing exploits have the potential to cause real-world impact. Policymakers, model providers, and other researchers in the AI and cybersecurity communities are interested in quantifying the capabilities of such agents to help mitigate cyberrisk and investigate opportunities for penetration testing. Toward that end, we introduce Cybench, a framework for specifying cybersecurity tasks and evaluating agents on those tasks.  We include 40 professional-level Capture the Flag (CTF) tasks from 4 distinct CTF competitions, chosen to be recent, meaningful, and spanning a wide range of difficulties. Each task includes its own description, starter files, and is initialized in an environment where an agent can execute bash commands and observe outputs. Since many tasks are beyond the capabilities of existing LM agents, we introduce subtasks, which break down a task into intermediary steps for a more detailed evaluation; we add subtasks to these 40 tasks. To evaluate agent capabilities, we construct a cybersecurity agent and evaluate 8 models: GPT-4o, OpenAI o1-preview, Claude 3 Opus, Claude 3.5 Sonnet, Mixtral 8x22b Instruct, Gemini 1.5 Pro, Llama 3 70B Chat, and Llama 3.1 405B Instruct. Without subtask guidance, agents solve only the easiest complete tasks that took human teams up to 11 minutes to solve. Claude 3.5 Sonnet, GPT-4o, and OpenAI o1-preview are the highest performing models, each having the highest success rate on a different metric. Finally, subtasks provide more signal for measuring performance compared to unguided runs, with models achieving a higher success rate on complete tasks with subtask guidance than without subtask guidance. Anonymized code and data are available athttps://drive.google.com/file/d/1kp3H0pw1WMAH-Qyyn9WA0ZKmEa7Cr4D4andhttps://drive.google.com/file/d/1BcTQ02BBR0m5LYTiK-tQmIK17_TxijIy."
    },
    {
        "title": "Selective State-Space Modeling of Correlation Maps for Semantic Correspondence",
        "link_suffix": "/forum?id=GmE8ovvXaJ",
        "link": "https://openreview.net/forum?id=GmE8ovvXaJ",
        "pdf_link": "https://openreview.net/pdf?id=GmE8ovvXaJ",
        "keywords": "semantic correspondence, state-space models, correlation aggregation, Mamba, feature aggregation, Similarity-aware Selective Scan",
        "abstract": "Establishing semantic correspondences between images is a fundamental yet challenging task in computer vision. Traditional feature-metric methods enhance visual features but may miss complex inter-image relationships, while recent correlation-metric approaches attempt to model these relationships but are hindered by high computational costs due to processing 4D correlation maps. We introduce MambaMatcher, a novel method that overcomes these limitations by efficiently modeling high-dimensional correlations using selective state-space models (SSMs), treating multi-level correlation scores as states. By implementing a similarity-aware selective scan mechanism adapted from Mamba\u2019s linear-complexity algorithm, MambaMatcher refines the 4D correlation tensor effectively without compromising feature map resolution or receptive field. Experiments on standard semantic correspondence benchmarks demonstrate that MambaMatcher achieves state-of-the-art performance without relying on large input images or computationally expensive diffusion-based feature extractors, effectively capturing rich inter-image correlations while maintaining computational efficiency."
    },
    {
        "title": "Last Iterate Convergence of Incremental Methods as a Model of Forgetting",
        "link_suffix": "/forum?id=mSGcDhQPwm",
        "link": "https://openreview.net/forum?id=mSGcDhQPwm",
        "pdf_link": "https://openreview.net/pdf?id=mSGcDhQPwm",
        "keywords": "incremental methods, last-iterate convergence, proximal methods, continual learning",
        "abstract": "Incremental gradient and incremental proximal methods are a fundamental class of optimization algorithms used for solving finite sum problems, broadly studied in the literature. Yet, without strong convexity, their convergence guarantees have primarily been established for the ergodic (average) iterate. We establish the first nonasymptotic convergence guarantees for the last iterate of both incremental gradient and incremental proximal methods, in general convex smooth (for both) and convex Lipschitz (for the proximal variants) settings. Our oracle complexity bounds for the last iterate nearly match (i.e., match up to a square-root-log or a log factor) the best known oracle complexity bounds for the average iterate, for both classes of methods. We further obtain generalizations of our results to weighted averaging of the iterates with increasing weights and for randomly permuted ordering of updates. We study last iterate convergence of the incremental proximal method as a mathematical abstraction of forgetting in continual learning and prove a lower bound that certifies that a large amount of regularization is crucial to mitigating catastrophic forgetting---one of the key considerations in continual learning. Our results generalize last iterate guarantees for incremental methods compared to state of the art, as such results were previously known only for overparameterized linear models, which correspond to convex quadratic problems with infinitely many solutions."
    },
    {
        "title": "PEAR: Primitive Enabled Adaptive Relabeling for Boosting Hierarchical Reinforcement Learning",
        "link_suffix": "/forum?id=0nJEgNpb4l",
        "link": "https://openreview.net/forum?id=0nJEgNpb4l",
        "pdf_link": "https://openreview.net/pdf?id=0nJEgNpb4l",
        "keywords": "Hierarchical reinforcement learning, Learning from demonstrations",
        "abstract": "Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to bound the sub-optimality of our approach and derive a joint optimization framework using RL and IL. Since PEAR utilizes only a handful of expert demonstrations and considers minimal limiting assumptions on the task structure, it can be easily integrated with typical off-policy \\RL algorithms to produce a practical HRL approach. We perform extensive experiments on challenging environments and show that PEAR is able to outperform various hierarchical and non-hierarchical baselines and achieve upto 80% success rates in complex sparse robotic control tasks where other baselines typically fail to show significant progress. We also perform ablations to thoroughly analyze the importance of our various design choices. Finally, we perform real world robotic experiments on complex tasks and demonstrate that PEAR consistently outperforms the baselines."
    },
    {
        "title": "Image and Video Tokenization with Binary Spherical Quantization",
        "link_suffix": "/forum?id=yGnsH3gQ6U",
        "link": "https://openreview.net/forum?id=yGnsH3gQ6U",
        "pdf_link": "https://openreview.net/pdf?id=yGnsH3gQ6U",
        "keywords": "quantization, visual compression, visual generation",
        "abstract": "We propose a new transformer-based image and video tokenizer with Binary Spherical Quantization (BSQ). BSQ projects the high-dimensional visual embedding to a lower-dimensional hypersphere and then applies binary quantization. BSQ is (1) parameter-efficient without an explicit codebook, (2) scalable to arbitrary token dimensions, and (3) compact: compressing visual data by up to 100\u00d7\nwith minimal distortion. Our tokenizer uses a transformer encoder and decoder with simple block-wise causal masking to support variable-length videos as input. The resulting BSQ-ViT achieves state-of-the-art visual reconstruction quality on image and video reconstruction benchmarks with 2.4\u00d7 throughput compared to the best prior methods. Furthermore, by learning an autoregressive prior for adap-\ntive arithmetic coding, BSQ-ViT achieves comparable visual compression results with commonly used compression standards, e.g. JPEG2000/WebP for images and H.264/H.265 for videos. BSQ-ViT also enables masked language models to achieve competitive image synthesis quality to GAN and diffusion approaches."
    },
    {
        "title": "Enhancing Dataset Distillation with Concurrent Learning: Addressing Negative Correlations and Catastrophic Forgetting in Trajectory Matching",
        "link_suffix": "/forum?id=dWEDLUmddV",
        "link": "https://openreview.net/forum?id=dWEDLUmddV",
        "pdf_link": "https://openreview.net/pdf?id=dWEDLUmddV",
        "keywords": "Dataset Distillation; Efficient Machine Learning; Data-centric AI",
        "abstract": "Dataset distillation generates a small synthetic dataset on which a model is trained to achieve performance comparable to that obtained on a complete dataset. Current state-of-the-art methods primarily focus on Trajectory Matching (TM), which optimizes the synthetic dataset by matching its training trajectory with that from the real dataset. Due to convergence issues and numerical stability, it is impractical to match the entire trajectory in one go; typically, a segment is sampled for matching at each iteration. However, previous TM-based methods overlook the potential interactions between matching different segments, particularly the presence of negative correlations. To study this problem, we conduct a quantitative analysis of the correlation between matching different segments and discover varying degrees of negative correlation depending on the image per class (IPC). Such negative correlation could lead to an increase in accumulated trajectory error and transform trajectory matching into a continual learning paradigm, potentially causing catastrophic forgetting. To tackle this issue, we propose a concurrent learning-based trajectory matching that simultaneously matches multiple segments. Extensive experiments demonstrate that our method consistently surpasses previous TM-based methods on CIFAR-10, CIFAR-100, Tiny ImageNet, and ImageNet-1K."
    },
    {
        "title": "Long-Term Fairness in Reinforcement Learning with Bisimulation Metrics",
        "link_suffix": "/forum?id=M9SAhECerP",
        "link": "https://openreview.net/forum?id=M9SAhECerP",
        "pdf_link": "https://openreview.net/pdf?id=M9SAhECerP",
        "keywords": "fairness, reinforcement learning, bisimulation",
        "abstract": "Ensuring long-term fairness is crucial when developing automated decision making systems, specifically in dynamic and sequential environments. By maximizing their reward without consideration of fairness, AI agents can introduce disparities in their treatment of groups or individuals. In this paper, we establish the connection between bisimulation metrics and group fairness in reinforcement learning. We propose a novel approach that leverages bisimulation metrics to learn reward functions and observation dynamics, ensuring that learners treat groups fairly while reflecting the original problem. We demonstrate the effectiveness of our method in addressing disparities in sequential decision making problems through empirical evaluation on a standard fairness benchmark consisting of lending and college admission scenarios."
    },
    {
        "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
        "link_suffix": "/forum?id=iWSl5Zyjjw",
        "link": "https://openreview.net/forum?id=iWSl5Zyjjw",
        "pdf_link": "https://openreview.net/pdf?id=iWSl5Zyjjw",
        "keywords": "Mamba, Long context, Context extension, Long-range language modeling",
        "abstract": "Long-range sequence processing poses a significant challenge for Transformers due to their quadratic complexity in input length. A promising alternative is Mamba, which demonstrates high performance and achieves Transformer-level capabilities while requiring substantially fewer computational resources. In this paper we explore the length-generalization capabilities of Mamba, which we find to be relatively limited. Through a series of visualizations and analyses we identify that the limitations arise from a restricted effective receptive field, dictated by the sequence length used during training. To address this constraint, we introduce DeciMamba, a context-extension method specifically designed for Mamba. This mechanism, built on top of a hidden filtering mechanism embedded within the S6 layer, enables the trained model to extrapolate well even without additional training. Empirical experiments over real-world long-range NLP tasks show that DeciMamba can extrapolate to context lengths that are significantly longer than the ones seen during training, while enjoying faster inference. We will release our code and models."
    },
    {
        "title": "The Low-Rank Bottleneck in Attention",
        "link_suffix": "/forum?id=y9Xp9NozPR",
        "link": "https://openreview.net/forum?id=y9Xp9NozPR",
        "pdf_link": "https://openreview.net/pdf?id=y9Xp9NozPR",
        "keywords": "Learning theory, Expressive capacity, Expressive power, Transformer, Attention",
        "abstract": "Attention-based mechanisms are widely used in machine learning, most prominently in transformers. However, hyperparameters such as the rank of the attention matrices and the number of attention heads are scaled nearly the same way in all realizations of this architecture, without theoretical justification. In this paper, we prove that the rank can have a dramatic effect on the representational capacity of attention. This effect persists even when the number of heads and the parameter count are very large. Specifically, we present a simple and natural target function based on nearest neighbor search that can be represented using a single full-rank attention head for any context length, but that cannot be approximated by low-rank attention unless the number of heads is exponential in the embedding dimension, even for short context lengths. Moreover, we show that, for short context lengths, adding depth allows the target to be approximated by low-rank attention. For long contexts, we conjecture that full-rank attention is necessary. Finally, we present experiments with standard multilayer transformers that validate our theoretical findings."
    },
    {
        "title": "Evolving Multi-Scale Normalization for Time Series Forecasting under Distribution Shifts",
        "link_suffix": "/forum?id=eQDdfqacoR",
        "link": "https://openreview.net/forum?id=eQDdfqacoR",
        "pdf_link": "https://openreview.net/pdf?id=eQDdfqacoR",
        "keywords": "Time series forecasting, Distribution shifts, Normalization, Online learning, Multi-scale modeling",
        "abstract": "Complex distribution shifts are the main obstacle to achieving accurate long-term time series forecasting. Several efforts have been conducted to capture the distribution characteristics and propose adaptive normalization techniques to alleviate the influence of distribution shifts. However, these methods neglect intricate distribution dynamics that are observed from various scales and the evolving functions of both distribution dynamics and normalized mapping relationships. To this end, we propose a novel model-agnostic Evolving Multi-Scale Normalization (EvoMSN) framework to tackle the distribution shift problem. Flexible normalization and denormalization are proposed based on the multi-scale statistics prediction module and adaptive ensembling. An evolving optimization strategy is designed to update the forecasting model and statistics prediction module collaboratively to track the shifting distributions. We evaluate the effectiveness of EvoMSN in improving the performance of five mainstream forecasting methods on benchmark datasets and also show its superiority compared to existing advanced normalization and online learning approaches."
    },
    {
        "title": "On Provable Length and Compositional Generalization",
        "link_suffix": "/forum?id=Hxm0hOxph2",
        "link": "https://openreview.net/forum?id=Hxm0hOxph2",
        "pdf_link": "https://openreview.net/pdf?id=Hxm0hOxph2",
        "keywords": "sequence-to-sequence models, length generalization, compositional generalization, out-of-distribution generalization",
        "abstract": "Out-of-distribution generalization capabilities of sequence-to-sequence models can be studied from the lens of two crucial forms of generalization: length generalization --  the ability to generalize to longer sequences than ones seen during training, and compositional generalization: the ability to generalize to token combinations not seen during training. In this work, we provide first provable guarantees on length and compositional generalization for common sequence-to-sequence models -- deep sets, transformers, state space models, and recurrent neural nets -- trained to minimize the prediction error. Taking a first principles perspective, we study the realizable case, i.e., the labeling function is realizable on the architecture.  We show that simple limited capacity versions of these different architectures achieve  both length and compositional generalization. Across different architectures, we also find that a linear relationship between the learned representation and the representation in the labeling function is necessary for length and compositional generalization."
    },
    {
        "title": "Why Fine-Tuning Struggles with Forgetting in Machine Unlearning? Theoretical Insights and a Remedial Approach",
        "link_suffix": "/forum?id=CGfWyU28Pd",
        "link": "https://openreview.net/forum?id=CGfWyU28Pd",
        "pdf_link": "https://openreview.net/pdf?id=CGfWyU28Pd",
        "keywords": "Machine Unlearning, Fine-Tuning, Learning Theory",
        "abstract": "Machine Unlearning has emerged as a significant area of research, focusing on 'removing' specific subsets of data from a trained model. Fine-tuning (FT) methods have become one of the fundamental approaches for approximating unlearning, as they effectively retain model performance. However, it is consistently observed that naive FT methods struggle to forget the targeted data. In this paper, we present the first theoretical analysis of FT methods for machine unlearning within a linear regression framework, providing a deeper exploration of this phenomenon. We investigate two scenarios with distinct features and overlapping features. Our findings reveal that FT models can achieve zero remaining loss yet fail to forget the forgetting data, unlike golden models (trained from scratch without the forgetting data). This analysis reveals that naive FT methods struggle with forgetting because the pretrained model retains information about the forgetting data, and the fine-tuning process has no impact on this retained information. To address this issue, we first propose a theoretical approach to mitigate the retention of forgetting data in the pretrained model. Our analysis shows that removing the forgetting data's influence allows FT models to match the performance of the golden model. Building on this insight, we introduce a discriminative regularization term to practically reduce the unlearning loss gap between the fine-tuned model and the golden model. Our experiments on both synthetic and real-world datasets validate these theoretical insights and demonstrate the effectiveness of the proposed regularization method."
    },
    {
        "title": "Approximately Aligned Decoding",
        "link_suffix": "/forum?id=9WbNpRuFuS",
        "link": "https://openreview.net/forum?id=9WbNpRuFuS",
        "pdf_link": "https://openreview.net/pdf?id=9WbNpRuFuS",
        "keywords": "Constrained Decoding, Large Language Models",
        "abstract": "It is common to reject undesired outputs of Large Language Models (LLMs); however, current methods to do so require an excessive amount of computation, or severely distort the distribution of outputs.\nWe present a method to balance the distortion of the output distribution with computational efficiency, allowing for the generation of long sequences of text with difficult-to-satisfy constraints, with less amplification of low probability outputs compared to existing methods.\nWe show through a series of experiments that the task-specific performance of our method is comparable to methods that do not distort the output distribution, while being much more computationally efficient."
    },
    {
        "title": "Energy-Weighted Flow Matching for Offline Reinforcement Learning",
        "link_suffix": "/forum?id=HA0oLUvuGI",
        "link": "https://openreview.net/forum?id=HA0oLUvuGI",
        "pdf_link": "https://openreview.net/pdf?id=HA0oLUvuGI",
        "keywords": "Flow Matching Models, Diffusion Models, Energy-guidance Generative Models, Offline Reinforcement Learning",
        "abstract": "This paper investigates energy guidance in generative modeling, where the target distribution is defined as $q(\\mathbf x) \\propto p(\\mathbf x)\\exp(-\\beta \\mathcal E(\\mathbf x))$, with $p(\\mathbf x)$ being the data distribution and $\\mathcal E(\\mathbf x)$ as the energy function. To comply with energy guidance, existing methods often require auxiliary procedures to learn intermediate guidance during the diffusion process. To overcome this limitation, we explore energy-guided flow matching, a generalized form of the diffusion process. We introduce energy-weighted flow matching (EFM), a method that directly learns the energy-guided flow without the need for auxiliary models. Theoretical analysis shows that energy-weighted flow matching accurately captures the guided flow. Additionally, we extend this methodology to energy-weighted diffusion models and apply it to offline reinforcement learning (RL) by proposing the Q-weighted Iterative Policy Optimization (QIPO). Empirically, we demonstrate that the proposed QIPO algorithm improves performance in offline RL tasks. Notably, our algorithm is the first energy-guided diffusion model that operates independently of auxiliary models and the first exact energy-guided flow matching model in the literature."
    },
    {
        "title": "LD-SDM: Language-Driven Hierarchical Species Distribution Modeling",
        "link_suffix": "/forum?id=nlwMlQ1RPW",
        "link": "https://openreview.net/forum?id=nlwMlQ1RPW",
        "pdf_link": "https://openreview.net/pdf?id=nlwMlQ1RPW",
        "keywords": "Species Distribution Modeling, Language-Driven, Spherical Harmonics",
        "abstract": "We focus on species distribution modeling using global-scale presence-only data, leveraging geographical and environmental features to map species ranges, as in previous studies. However, we innovate by integrating taxonomic classification into our approach. Specifically, we propose using a large language model to extract a latent representation of the taxonomic classification from a textual prompt. This allows us to map the range of any taxonomic rank, including unseen species, without additional supervision. We also present a new proximity-aware evaluation metric, suitable for evaluating species distribution models, which addresses critical shortcomings of traditional metrics. We evaluated our model for species range prediction, zero-shot prediction, and geo-feature regression and found that it outperforms several state-of-the-art models. We will share code, data, and model checkpoints after acceptance."
    },
    {
        "title": "Not all parameters are equal: a Hessian informed differential learning rate for deep learning",
        "link_suffix": "/forum?id=BjaHYhr7VS",
        "link": "https://openreview.net/forum?id=BjaHYhr7VS",
        "pdf_link": "https://openreview.net/pdf?id=BjaHYhr7VS",
        "keywords": "Differential learning rate, Newton's method, parameter-efficient training",
        "abstract": "Differential learning rate (DLR), a technique that applies different learning rates (instead of a single one) to different model parameters, has been widely used in deep learning and achieved empirical success via its various forms. For example, parameter-efficient training (PET) applies zero learning rates to most parameters so as to significantly saves the computational cost; adaptive optimizers such as Adam apply the coordinate-wise learning rate to accelerate the convergence.At the core, DLR leverages the observation that different parameters can have different loss curvature, which is hard to characterize in general. We propose the Hessian-informed differential learning rate (Hi-DLR), an efficient approach that captures the loss curvature of parameters for any model and optimizer adaptively. Given a proper grouping of parameters, we empirically demonstrate that Hi-DLR can improve the convergence by dynamically determining the learning rates during the training. Furthermore, we can quantify the influence of different parameters and freeze the less-contributing parameters, which leads to a new PET that automatically adapts to various tasks and models."
    },
    {
        "title": "X-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs",
        "link_suffix": "/forum?id=c1Ng0f8ivn",
        "link": "https://openreview.net/forum?id=c1Ng0f8ivn",
        "pdf_link": "https://openreview.net/pdf?id=c1Ng0f8ivn",
        "keywords": "contrastive learning, vision language model, simclr, clip",
        "abstract": "Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss\u2014an objective matching related samples\u2014underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity graph to indicate how samples should relate in the embedding space.  This view reveals a shortcoming in contrastive learning: the similarity graph is binary, as only one sample is the related positive sample. Crucially, similarities \\textit{across} samples are ignored. \nBased on this observation, we revise the standard contrastive loss to explicitly encode how a sample relates to others. We experiment with this new objective, called $\\mathbb{X}$-Sample Contrastive, to train vision models based on similarities in class or text caption descriptions.\nOur study spans three scales: ImageNet-1k with 1 million, CC3M with 3 million, and CC12M with 12 million samples. The representations learned via our objective outperform both contrastive self-supervised and vision-language models trained on the same data across a range of tasks. When training on CC12M, we outperform CLIP by $0.6%$ on both ImageNet and ImageNet Real. Our objective appears to work particularly well in lower-data regimes, with gains over CLIP of $17.2%$ on ImageNet and $18.0%$ on ImageNet Real when training with CC3M. Finally, our objective encourages the model to learn representations that separate objects from their attributes and backgrounds, with gains of $3.3$-$5.6$% over CLIP on ImageNet9. The proposed method takes a step towards developing richer learning objectives for understanding sample relations in foundation models."
    },
    {
        "title": "Instruction Following without Instruction Tuning",
        "link_suffix": "/forum?id=vyHFTsOUWu",
        "link": "https://openreview.net/forum?id=vyHFTsOUWu",
        "pdf_link": "https://openreview.net/pdf?id=vyHFTsOUWu",
        "keywords": "instruction tuning, instruction following, ablation, rule-based",
        "abstract": "Instruction tuning commonly means finetuning a language model on instruction-\nresponse pairs. We discover two forms of adaptation (tuning) that are deficient\ncompared to instruction tuning, yet still yield instruction following; we call this\nimplicit instruction tuning. We first find that instruction-response pairs are not necessary: training solely on responses, without any corresponding instructions, yields\ninstruction following. This suggests pretrained models have an instruction-response\nmapping which is revealed by teaching the model the desired distribution of re-\nsponses. However, we then find it\u2019s not necessary to teach the desired distribution\nof responses: instruction-response training on narrow-domain data like poetry still\nleads to broad instruction-following behavior like recipe generation. In particular,\nwhen instructions are very different from those in the narrow finetuning domain,\nmodels\u2019 responses do not adhere to the style of the finetuning domain. To begin\nto explain implicit instruction tuning, we hypothesize that very simple changes to\na language model\u2019s distribution yield instruction following. We support this by\nhand-writing a rule-based language model which yields instruction following in a\nproduct-of-experts with a pretrained model. The rules are to slowly increase the\nprobability of ending the sequence, penalize repetition, and uniformly change 15\nwords\u2019 probabilities. In summary, adaptations made without being designed to\nyield instruction following can do so implicitly."
    },
    {
        "title": "Metamizer: A Versatile Neural Optimizer for Fast and Accurate Physics Simulations",
        "link_suffix": "/forum?id=60TXv9Xif5",
        "link": "https://openreview.net/forum?id=60TXv9Xif5",
        "pdf_link": "https://openreview.net/pdf?id=60TXv9Xif5",
        "keywords": "Physics-based Deep Learning, Physics Simulations, Meta-Learning",
        "abstract": "Efficient physics simulations are essential for numerous applications, ranging from realistic cloth animations or smoke effects in video games, to analyzing pollutant dispersion in environmental sciences, to calculating vehicle drag coefficients in engineering applications. Unfortunately, analytical solutions to the underlying physical equations are rarely available, and numerical solutions require high computational resources. Latest developments in the field of physics-based Deep Learning have led to promising efficiency improvements but still suffer from limited generalization capabilities and low accuracy compared to numerical solvers.In this work, we introduceMetamizer, a novel neural optimizer that iteratively solves a wide range of physical systems with high accuracy by minimizing a physics-based loss function. To this end, our approach leverages a scale-invariant architecture that enhances gradient descent updates to accelerate convergence. Since the neural network itself acts as an optimizer, training this neural optimizer falls into the category of meta-optimization approaches.We demonstrate that Metamizer achieves unprecedented accuracy for deep learning based approaches - sometimes approaching machine precision - across multiple PDEs after training on the Laplace, advection-diffusion and incompressible Navier-Stokes equation as well as on cloth simulations. Remarkably, the model also generalizes to PDEs that were not covered during training such as the Poisson, wave and Burgers equation.Our results suggest that Metamizer could have a profound impact on future numerical solvers, paving the way for fast and accurate neural physics simulations without the need for retraining."
    }
]