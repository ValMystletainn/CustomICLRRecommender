[{"title": "Allostatic Control of Persistent States in Spiking Neural Networks for Perception and Computation", "link_suffix": "/forum?id=zbIS2r0t0F", "link": "https://openreview.net/forum?id=zbIS2r0t0F", "pdf_link": "https://openreview.net/pdf?id=zbIS2r0t0F", "keywords": "Allostatic, Dynamic, Attractors", "abstract": "We introduce a novel model for updating perceptual beliefs about the environment\nby extending the concept of Allostasis to the control of internal representations.\nAllostasis is a fundamental regulatory mechanism observed in animal physiology\nthat orchestrates responses to maintain a dynamic equilibrium in bodily needs and\ninternal states. In this paper, we focus on an application in numerical cognition,\nwhere a bump of activity in an attractor network is used as a spatial-numerical\nrepresentation. While existing neural networks can maintain persistent states, to\ndate, there is no unified framework for dynamically controlling spatial changes in\nneuronal activity in response to enviromental changes. To address this, we couple\na well-known allostatic microcircuit, the Hammel model, with a ring attractor, re-\nsulting in a Spiking Neural Network architecture that can modulate the location of\nthe bump as a function of some reference input. This localised activity in turn is\nused as a perceptual belief in a simulated subitization task \u2013 a quick enumeration\nprocess without counting. We provide a general procedure to fine-tune the model\nand demonstrate the successful control of the bump location. We also study the\nresponse time in the model with respect to changes in parameters and compare\nit with biological data. Finally, we analyze the dynamics of the network to un-\nderstand the selectivity and specificity of different neurons to different categories\npresent in the input. The results of this paper, particularly the mechanism for mov-\ning persistent states, are not limited to numerical cognition but can be applied to a\nwide range of tasks involving similar representations.", "title_embedding_index": 3600, "title_abs_embedding_index": 3625}, {"title": "Measuring similarity between embedding spaces using induced neighborhood graphs", "link_suffix": "/forum?id=BYwdia04ZA", "link": "https://openreview.net/forum?id=BYwdia04ZA", "pdf_link": "https://openreview.net/pdf?id=BYwdia04ZA", "keywords": "Embedding Space Geometry, Paired Representation Similarity, Graph-Based Embedding Comparison", "abstract": "Deep Learning techniques have excelled at generating embedding spaces that capture semantic similarities between items. Often these representations are paired, enabling experiments with analogies (pairs within the same domain) and cross-modality (pairs across domains). These experiments are based on specific assumptions about the geometry of embedding spaces, which allow finding paired items by extrapolating the positional relationships between embedding pairs in the training dataset, allowing for tasks such as finding new analogies, and multimodal zero-shot classification. In this work, we propose a metric to evaluate the similarity between paired item representations. Our proposal is built from the structural similarity between the nearest-neighbors induced graphs of each representation, and can be configured to compare spaces based on different distance metrics and on different neighborhood sizes. We demonstrate that our proposal can be used to identify similar structures at different scales, which is hard to achieve with kernel methods such as Centered Kernel Alignment (CKA). We further illustrate our method with two case studies: an analogy task using GloVe embeddings, and zero-shot classification in the CIFAR-100 dataset using CLIP embeddings. Our results show that accuracy in both analogy and zero-shot classification tasks correlates with the embedding similarity. These findings can help explain performance differences in these tasks, and may lead to improved design of paired-embedding models in the future.", "title_embedding_index": 3601, "title_abs_embedding_index": 3626}, {"title": "Support is All You Need for Certified VAE Training", "link_suffix": "/forum?id=oZkqkkvdND", "link": "https://openreview.net/forum?id=oZkqkkvdND", "pdf_link": "https://openreview.net/pdf?id=oZkqkkvdND", "keywords": "Certified Training, Trustworthy Machine Learning, Variational Autoencoder, Wireless", "abstract": "Variational Autoencoders (VAEs) have become increasingly popular and deployed in safety-critical applications. In such applications, we want to give certified probabilistic guarantees on performance under adversarial attacks. We propose a novel method, CIVET, for certified training of VAEs. CIVET depends on the key insight that we can bound worst-case VAE error by bounding the error on carefully chosen support sets at the latent layer. We show this point mathematically and present a novel training algorithm utilizing this insight. We show in an extensive evaluation across different datasets (in both the wireless and vision application areas), architectures, and perturbation magnitudes that our method outperforms SOTA methods achieving good standard performance with strong robustness guarantees.", "title_embedding_index": 3602, "title_abs_embedding_index": 3627}, {"title": "OntoFAR: Hierarchical Multi-Ontology Fusion Better Augments EHR Representation", "link_suffix": "/forum?id=y1UHa9sl2w", "link": "https://openreview.net/forum?id=y1UHa9sl2w", "pdf_link": "https://openreview.net/pdf?id=y1UHa9sl2w", "keywords": "Health Informatics, EHR, Diagnosis Prediction, Healthcare Representation", "abstract": "Medical ontology graphs, which typically organize and relate comprehensive medical concepts in a hierarchical structure, are able to map a rich set of external knowledge onto the specific medical codes observed in electronic health records (EHRs). Through the connectivity in ontologies, healthcare predictive models can utilize the ancestor, descendant, or sibling information to add supplementary contexts on medical codes, thereby augmenting expressiveness of EHR representations. However, existing approaches are limited by the heterogeneous isolation of different ontology systems (e.g., conditions vs. drugs), that different types of ontology concepts have to be learned individually, and only the homogeneous ontology relationships can be exploited. This limitation restricts the existing methods from fully leveraging the cross-ontology relationships which could substantially enhance healthcare representations. \nIn this paper, we propose OntoFAR, a framework that fuse multiple ontology graphs, utilizing the collaboration across ontologies to enhance medical concept representation. Our method jointly represents medical concepts cross multiple ontology structures by performing message passing in two dimensions: (1) vertical propagation over levels of ontology hierarchy, and (2) horizontal propagation over co-occurring concepts in EHR visits. Additionally, OntoFAR leverages the large language models (LLMs) pre-trained on massive open world information to understand each target concept with its ontology relationships, providing enhanced embedding initialization for concepts. Through extensive experimental studies on two public datasets, MIMIC-III and MIMIC-IV, we validate the superior performance of OntoFAR over the state-of-the-art baselines. Beyond accuracy, our model also exhibits the add-on compatibility to boost existing healthcare prediction models, and demonstrate a good robustness in scenarios with limited data availability. The implementation code is available athttps://anonymous.4open.science/r/OntoFAR-35D4", "title_embedding_index": 3603, "title_abs_embedding_index": 3628}, {"title": "KDA: A Knowledge-Distilled Attacker for Scalable LLM Red Teaming", "link_suffix": "/forum?id=UWuTZYPSxJ", "link": "https://openreview.net/forum?id=UWuTZYPSxJ", "pdf_link": "https://openreview.net/pdf?id=UWuTZYPSxJ", "keywords": "Jailbreak attack;  Large Language Models; Adversarial Attack; Red Teaming", "abstract": "Jailbreak attacks exploit specific prompts to bypass LLM safeguards and generate harmful or inappropriate content. Recently, numerous approaches have emerged for generating jailbreak attacks across diverse malicious scenarios. However, these methods often suffer from critical limitations such as the reliance on handcrafted prompts, the necessity for white-box access to target LLMs, the generation of monotonous prompts, or the dependence on expensive queries to commercial LLMs. Moreover, these methods typically require considerable time to generate jailbreak attacks. In this paper, we propose a Knowledge-Distilled Attacker (KDA) that leverages existing realistic and semantically meaningful prompts to learn a model that efficiently produces successful attacks. Specifically, we finetune an open-source LLM on a diverse set of attack prompts, enabling our framework to automatically generate black-box, coherent, and diverse attack prompts independent of commercial LLMs. Our KDA achieves a 100% success rate on multiple state-of-the-art LLMs while only requiring less than 10 seconds per attack generation. Further, using KDA, we introduce the RedTeam-10k dataset, a large-scale dataset of 10,000 harmful attack prompts inducing malicious LLM behavior spanning 12 categories such as bias, hate, and illegal activities. This dataset is 20x larger than any existing attack prompt dataset, positioning KDA as a powerful tool for large-scale adversarial testing.", "title_embedding_index": 3604, "title_abs_embedding_index": 3629}, {"title": "CONGO: Compressive Online Gradient Optimization", "link_suffix": "/forum?id=4BFzTrIjPN", "link": "https://openreview.net/forum?id=4BFzTrIjPN", "pdf_link": "https://openreview.net/pdf?id=4BFzTrIjPN", "keywords": "online convex optimization, compressive sensing, regret analysis", "abstract": "We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's gradient even when the only information available is a limited number of function samples. Our motivation stems from the optimization of large-scale queueing networks that process time-sensitive jobs. Here, a job must be processed by potentially many queues in sequence to produce an output, and the service time at any queue is a function of the resources allocated to that queue. Since resources are costly, the end-to-end latency for jobs must be balanced with the overall cost of the resources used. While the number of queues is substantial, the latency function primarily reacts to resource changes in only a few, rendering the gradient sparse. We tackle this problem by introducing the Compressive Online Gradient Optimization framework which allows compressive sensing methods previously applied to stochastic optimization to achieve regret bounds with an optimal dependence on the time horizon without the full problem dimension appearing in the bound. For specific algorithms, we reduce the samples required per gradient estimate to scale with the gradient's sparsity factor rather than its full dimensionality. Numerical simulations and real-world microservices benchmarks demonstrate CONGO's superiority over gradient descent approaches that do not account for sparsity.", "title_embedding_index": 3605, "title_abs_embedding_index": 3630}, {"title": "RL, but don't do anything I wouldn't do", "link_suffix": "/forum?id=o2uHg0Skil", "link": "https://openreview.net/forum?id=o2uHg0Skil", "pdf_link": "https://openreview.net/pdf?id=o2uHg0Skil", "keywords": "AI safety, Superalignment, Algorithmic information theory, Kolmogorov complexity, Reinforcement learning, Large language models", "abstract": "In reinforcement learning, if the agent's reward differs from the designers' true utility, even only rarely, the state distribution resulting from the agent's policy can be very bad, in theory and in practice. When RL policies would devolve into undesired behavior, a common countermeasure is KL regularization to a trusted policy (\"Don't do anything I wouldn't do\"). All current cutting-edge language models are RL agents that are KL-regularized to a \"base policy\" that is purely predictive. Unfortunately, we demonstrate that when this base policy is a Bayesian predictive model of a trusted policy, the KL constraint is no longer reliable for controlling the behavior of an advanced RL agent. We demonstrate this theoretically using algorithmic information theory, and while systems today are too weak to exhibit this theorized failure precisely, we RL-finetune a language model and find evidence that our formal results are plausibly relevant in practice. We also propose a theoretical alternative that avoids this problem by replacing the \"Don't do anything I wouldn't do\" principle with \"Don't do anything I mightn't do\".", "title_embedding_index": 3606, "title_abs_embedding_index": 3631}, {"title": "Scalable Extraction of Training Data from Aligned, Production Language Models", "link_suffix": "/forum?id=vjel3nWP2a", "link": "https://openreview.net/forum?id=vjel3nWP2a", "pdf_link": "https://openreview.net/pdf?id=vjel3nWP2a", "keywords": "privacy, language models, data extraction, security", "abstract": "We show thatalignment---a standard process that tunes LLMs to follow instructions in a harmless manner---seems to prevent existing data extraction attacks. We develop two novel attacks that undo a model's alignment and recover thousands of training examples from the popular proprietary model, OpenAI's ChatGPT.  Our most potent attack causes ChatGPT to emit training data in over 23% of conversations, and enables targeted reconstruction of chosen training documents, including those containing copyrighted or harmful content. Our work highlights the limitations of existing safeguards to prevent training-data leakage in LLMs.", "title_embedding_index": 3607, "title_abs_embedding_index": 3632}, {"title": "Listening to the Wise Few: Select-and-Copy Attention Heads for Multiple-Choice QA", "link_suffix": "/forum?id=A0W7VCSQev", "link": "https://openreview.net/forum?id=A0W7VCSQev", "pdf_link": "https://openreview.net/pdf?id=A0W7VCSQev", "keywords": "large language models (LLMs), attention mechanisms, model interpretability, zero-shot learning", "abstract": "Multiple-choice question answering (MCQA) tasks are fundamental benchmarks for evaluating large language models (LLMs), yet models, especially smaller ones, often struggle with these tasks. In this paper, we uncover that intermediate attention heads within LLMs hold valuable insights for improving MCQA performance. We introduce a novel method that leverages query-key interactions in specific \"select-and-copy\" attention heads to effectively select correct answers. Building on this mechanism, we propose two option scoring methods: the QK-score and the attention score, based on the query-key representations from these heads. Our approach demonstrates consistent performance improvements across popular MCQA datasets, yielding up to a 16% increase in accuracy for LLaMA-2-7B and up to 10% for larger models on established benchmarks, with especially notable gains in zero-shot scenarios. In controlled setting of a simple synthetic dataset where the correct answer is explicitly known, accuracy improves by up to 60%. Furthermore, our method often shows better stability to option perturbations compared to existing baselines. By analyzing the decision process in these select-and-copy heads, we contribute to a deeper understanding of how LLMs process MCQA tasks, offering insights that can enhance the development of more reliable and interpretable models.", "title_embedding_index": 3608, "title_abs_embedding_index": 3633}, {"title": "Measuring Non-Adversarial Reproduction of Training Data in Large Language Models", "link_suffix": "/forum?id=590yfqz1LE", "link": "https://openreview.net/forum?id=590yfqz1LE", "pdf_link": "https://openreview.net/pdf?id=590yfqz1LE", "keywords": "large language models, memorization, data extraction, originality, privacy", "abstract": "Large language models frequently memorize parts of their training data.\nThis behavior led to a large body of research on data extraction attacks,\nwhere adversaries coerce a model to output memorized examples.\nHowever, most LLM users are not malicious;\nthey only want an LLM to perform some desired task.\nIn this work, we investigate non-adversarial reproduction,\nwhere the outputs of a large language model overlap with existing public text\nwhen responding to natural and benign prompts.\nFor a variety of innocuous prompt categories\n(e.g., writing a letter or a tutorial),\nwe show that up to 15% of the text output by\npopular conversational language models overlaps with moderate snippets (40\u201360 characters) of the Internet.\nFor the same tasks, we find that human-written text\nhas far less overlap with existing Internet data.\nWe further study whether prompting strategies can close this reproduction gap\nbetween models and humans.\nHowever, while appropriate prompting can reduce non-adversarial reproduction on average,\nwe find that mitigating worst-case reproduction of training data\nrequires stronger defenses\u2014even for benign interactions.", "title_embedding_index": 3609, "title_abs_embedding_index": 3634}, {"title": "TULiP: Test-time Uncertainty Estimation via Linearization and Weight Perturbation", "link_suffix": "/forum?id=E4kuNZWost", "link": "https://openreview.net/forum?id=E4kuNZWost", "pdf_link": "https://openreview.net/pdf?id=E4kuNZWost", "keywords": "Out-of-distribution detection, Uncertainty Quantification, Lazy Training, Neural Tangent Kernel", "abstract": "A reliable uncertainty estimation method is the foundation of many modern out-of-distribution (OOD) detectors, which are critical for safe deployments of deep learning models in the open world. In this work, we propose TULiP, a novel, theoretically-driven, post-hoc uncertainty estimator for OOD detection. Our method considers a hypothetical perturbation applied to the network prior to convergence. Based on linearized training dynamics, we bound the effect of such perturbation, resulting in an uncertainty score computable by perturbing model parameters. Compared to existing methods, our approach has a more versatile backbone that can be extended to other problem settings. We visualize our bound on synthetic regression and classification datasets. Furthermore, we demonstrate the effectiveness of TULiP using large-scale OOD detection benchmarks for image classification. Our method exhibits state-of-the-art performance, particularly for near-distribution samples.", "title_embedding_index": 3610, "title_abs_embedding_index": 3635}, {"title": "Task Characteristic and Contrastive Contexts for Improving Generalization in Offline Meta-Reinforcement Learning", "link_suffix": "/forum?id=5GauLpaNGC", "link": "https://openreview.net/forum?id=5GauLpaNGC", "pdf_link": "https://openreview.net/pdf?id=5GauLpaNGC", "keywords": "Reinforcement Learning, Meta-Reinforcement Learning", "abstract": "Context-based offline meta-reinforcement learning (meta-RL) methods typically extract contexts summarizing task information from historical trajectories to achieve adaptation to unseen target tasks. Nevertheless, previous methods may lack generalization and suffer from ineffective adaptation. Our key insight to counteract this issue is that they fail to capture both task characteristic and task contrastive information when generating contexts. In this work, we propose a framework called task characteristic and contrastive contexts for offline meta-RL (TCMRL), which consists of a task characteristic extractor and a task contrastive loss. More specifically, the task characteristic extractor aims at identifying transitions within a trajectory, that are characteristic of a task, when generating contexts. Meanwhile, the task contrastive loss favors the learning of task information that distinguishes tasks from one another by considering interrelations among transitions of trajectory subsequences. Contexts that include both task characteristic and task contrastive information provide a comprehensive understanding of the tasks themselves and implicit relationships among tasks. Experiments in meta-environments show the superiority of TCMRL over previous offline meta-RL methods in generating more generalizable contexts, and achieving efficient and effective adaptation to unseen target tasks.", "title_embedding_index": 3611, "title_abs_embedding_index": 3636}, {"title": "Meta-Referential Games to Learn Compositional Learning Behaviours", "link_suffix": "/forum?id=BhBVAC5i2T", "link": "https://openreview.net/forum?id=BhBVAC5i2T", "pdf_link": "https://openreview.net/pdf?id=BhBVAC5i2T", "keywords": "referential game, language grounding, compositionality, systematicity, few-shot learning, meta-learning, reinforcement learning, language emergence, symbolic behaviours, benchmark", "abstract": "Human beings use compositionality to generalise from past to novel experiences, assuming that past experiences can be decomposed into fundamental atomic components that can be recombined in novel ways. \nWe frame this as the ability to learn to generalise compositionally, and refer to behaviours making use of this ability as compositional learning behaviours (CLBs). \nLearning CLBs requires the resolution of a binding problem (BP). \nWhile it is another feat of intelligence that human beings perform with ease, it is not the case for artificial agents. \nThus, in order to build artificial agents able to collaborate with human beings, we develop a novel benchmark to investigate agents\u2019 abilities to exhibit CLBs by solving a domain-agnostic version of the BP. \nTaking inspiration from the Emergent Communication, we propose a meta-learning extension of referential games, entitled Meta-Referential Games, to support our benchmark, the Symbolic Behaviour Benchmark (S2B). \nBaseline results and error analysis show that the S2B is a compelling challenge that we hope will spur the research community to develop more capable artificial agents.", "title_embedding_index": 3612, "title_abs_embedding_index": 3637}, {"title": "Information Structure in Large Language Models", "link_suffix": "/forum?id=VB8xHF1Rdl", "link": "https://openreview.net/forum?id=VB8xHF1Rdl", "pdf_link": "https://openreview.net/pdf?id=VB8xHF1Rdl", "keywords": "Large Language Models, Interpretability, Information Theory, Language", "abstract": "Despite the widespread use of large language models, we still lack unified notation for thinking about and describing their representational spaces. This limits our ability to understand how they work. Ideally we would understand how their representations are structured, how that structure emerges over training, and what kinds of structures are desirable. Unfortunately we as humans tend not to have strong intuitions about high-dimensional vector spaces. Here we propose an information theoretic approach to quantifying structure in deep-learning models. We introduce a novel method for estimating the entropy of vector spaces, and use it to quantify the amount of information in the model we can explain with a set of labels. This can show when regularities emerge in representation space with respect to token, bigram, and trigram information in the input. As these models are learning from human language data, we formalise this in terms of 3 linguistically derived quantities: regularity, variation, and disentanglement. These show how larger models become proportionally more disentangled. We also are able to predict downstream task performance on GLUE benchmarks based on representational structure at the end of pre-training but before fine tuning.", "title_embedding_index": 3613, "title_abs_embedding_index": 3638}, {"title": "PREDICT: Preference Reasoning by Evaluating Decomposed preferences Inferred from Candidate Trajectories", "link_suffix": "/forum?id=pk4YjZeevI", "link": "https://openreview.net/forum?id=pk4YjZeevI", "pdf_link": "https://openreview.net/pdf?id=pk4YjZeevI", "keywords": "personalization, preference learning, LLM personalization, personalization benchmark", "abstract": "Accommodating human preferences is essential for creating AI agents that deliver personalized and effective interactions. Recent work has shown the potential for LLMs to infer preferences from user interactions, but they often produce broad and generic preferences, failing to capture the unique and individualized nature of human preferences. This paper introduces PREDICT, a method designed to enhance the precision and adaptability of inferring preferences. PREDICT incorporates three key elements: (1) iterative refinement of inferred preferences, (2) decomposition of preferences into constituent components, and (3) validation of preferences across multiple trajectories. We evaluate PREDICT on two distinct environments: a gridworld setting and a new text-domain environment (PLUME). PREDICT more accurately infers nuanced human preferences improving over existing baselines by 66.2% (gridworld environment) and 41.0% (PLUME).", "title_embedding_index": 3614, "title_abs_embedding_index": 3639}, {"title": "InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation", "link_suffix": "/forum?id=ZGqd0cbBvm", "link": "https://openreview.net/forum?id=ZGqd0cbBvm", "pdf_link": "https://openreview.net/pdf?id=ZGqd0cbBvm", "keywords": "Automated Data Analysis, Data Analytics Benchmark, LLM agents, Code Generation, LLM Evaluation", "abstract": "Data analytics is essential for extracting valuable insights from data that can assist organizations in making effective decisions. We introduce InsightBench, a benchmark dataset with three key features. First, it consists of 100 datasets representing diverse business use cases such as finance and incident management, each accompanied by a carefully curated set of insights planted in the datasets. Second, unlike existing benchmarks focusing on answering single queries, InsightBench evaluates agents based on their ability to perform end-to-end data analytics, including formulating questions, interpreting answers, and generating a summary of insights and actionable steps. Third, we conducted comprehensive quality assurance to ensure that each dataset in the benchmark had clear goals and included relevant and meaningful questions and analysis. Furthermore, we implement a two-way evaluation mechanism using LLaMA-3 as an effective, open-source evaluator to assess agents\u2019 ability to extract insights. We also propose AgentPoirot, our baseline data analysis agent capable of performing end-to-end data analytics. Our evaluation on InsightBench shows that AgentPoirot outperforms existing approaches (such as Pandas Agent) that focus on resolving single queries. We also compare the performance of open- and closed-source LLMs and various evaluation strategies. Overall, this benchmark serves as a testbed to motivate further development in comprehensive automated data analytics", "title_embedding_index": 3615, "title_abs_embedding_index": 3640}, {"title": "Optimizing Posterior Samples for Bayesian Optimization via Rootfinding", "link_suffix": "/forum?id=I6UbnkUveF", "link": "https://openreview.net/forum?id=I6UbnkUveF", "pdf_link": "https://openreview.net/pdf?id=I6UbnkUveF", "keywords": "Bayesian optimization, global optimization, acquisition function, Thompson sampling", "abstract": "Bayesian optimization devolves the global optimization of a costly objective function to the global optimization of a sequence of acquisition functions. This inner-loop optimization can be catastrophically difficult if it involves posterior samples, especially in higher dimensions. We introduce an efficient global optimization strategy for posterior samples based on global rootfinding. It provides gradient-based optimizers with judiciously selected starting points, designed to combine exploitation and exploration. The algorithm scales practically linearly to high dimensions. For posterior sample-based acquisition functions such as Gaussian process Thompson sampling (GP-TS) and variants of entropy search, we demonstrate remarkable improvement in both inner- and outer-loop optimization, surprisingly outperforming alternatives like EI and GP-UCB in most cases. We also propose a sample average formulation of GP-TS, which has a parameter to explicitly control exploitation and can be computed at the cost of one posterior sample.", "title_embedding_index": 3616, "title_abs_embedding_index": 3641}, {"title": "Poisoning with A Pill: Circumventing Detection in Federated Learning", "link_suffix": "/forum?id=TbJJjwtBKX", "link": "https://openreview.net/forum?id=TbJJjwtBKX", "pdf_link": "https://openreview.net/pdf?id=TbJJjwtBKX", "keywords": "Federated Learning, Byzantine Attack, Model Poisoning", "abstract": "Federated learning (FL) protects data privacy by enabling distributed model training without direct access to client data. However, its distributed nature makes it vulnerable to model and data poisoning attacks. While numerous defenses filter malicious clients using statistical metrics, they overlook the role of model redundancy, where not all parameters contribute equally to the model/attack performance. Current attacks manipulate all model parameters uniformly, making them more detectable, while defenses focus on the overall statistics of client updates, leaving gaps for more sophisticated attacks. We propose an attack-agnostic augmentation method to enhance the stealthiness and effectiveness of existing poisoning attacks in FL, exposing flaws in current defenses and highlighting the need for fine-grained FL security. Our three-stage methodology\u2014$\\textit{pill construction}$, $\\textit{pill poisoning}$, and $\\textit{pill injection}$\u2014injects poison into a compact subnet (i.e., pill) of the global model during the iterative FL training. Experimental results show that FL poisoning attacks enhanced by our method can bypass 8 state-of-the-art (SOTA) defenses, gaining an up to 7x error rate increase, as well as on average a more than 2x error rate increase on both IID and non-IID data, in both cross-silo and cross-device FL systems.", "title_embedding_index": 3617, "title_abs_embedding_index": 3642}, {"title": "Self-learning Compositional Representations for Zero-shot Chinese Character Recognition", "link_suffix": "/forum?id=s2jLQDqVUE", "link": "https://openreview.net/forum?id=s2jLQDqVUE", "pdf_link": "https://openreview.net/pdf?id=s2jLQDqVUE", "keywords": "Chinese Character Recognition, Object-centric Representations", "abstract": "Chinese character recognition has been a longstanding research topic and remains essential in visual tasks like ancient manuscript recognition. Chinese character recognition faces numerous challenges, particularly the issue of zero-shot characters. Existing Chinese zero-shot character recognition methods primarily focus on the radical or stroke decomposition. However, radical-based methods still struggle to solve zero-shot radicals, while stroke-based ones are hard to perceive fine-grained information. Besides, previous methods can hardly generalize to characters of other languages. In this paper, we propose a novel Self-learning Compositional Representation method for zero-shot Chinese Character Recognition (SCR-CCR). SCR-CCR learns compositional components automatically from the data, which are not aligned with human-defined radical or stroke decomposition methods. SCR-CCR follows the pretraining-inference paradigm. First, we train a Character Slot Attention (ChSA) via pure feature reconstruction loss to parse appropriate components from character images. Then we recognize zero-shot characters without finetuning or retraining in the inference stage by comparing components between input and example images. To evaluate the proposed method, we conduct experiments of zero-shot character recognition. The experiments illustrate that SCR-CCR outperforms previous methods in most cases of character and radical zero-shot settings. In particular, visualization experiments indicate that the components learned by SCR-CCR reflect the structure of characters in an interpretable way, and can be used to recognize Japanese and Korean characters.", "title_embedding_index": 3618, "title_abs_embedding_index": 3643}, {"title": "Meta-Dynamical State Space Models for Integrative Neural Data Analysis", "link_suffix": "/forum?id=SRpq5OBpED", "link": "https://openreview.net/forum?id=SRpq5OBpED", "pdf_link": "https://openreview.net/pdf?id=SRpq5OBpED", "keywords": "neural dynamics, state-space model, meta learning", "abstract": "Learning shared structure across environments facilitates rapid learning and adaptive behavior in neural systems. This has been widely demonstrated and applied in machine learning to train models that are capable of generalizing to novel settings. However, there has been limited work exploiting the shared structure in neural activity during similar tasks for learning latent dynamics from neural recordings.\nExisting approaches are designed to infer dynamics from a single dataset and cannot be readily adapted to account for statistical heterogeneities across recordings. In this work, we hypothesize that similar tasks admit a corresponding family of\nrelated solutions and propose a novel approach for meta-learning this solution space from task-related neural activity of trained animals. Specifically, we capture the variabilities across recordings on a low-dimensional manifold which concisely parametrizes this family of dynamics, thereby facilitating rapid learning of latent dynamics given new recordings. We demonstrate the efficacy of our approach on\nfew-shot reconstruction and forecasting of synthetic dynamical systems, and neural recordings from the motor cortex during different arm reaching tasks.", "title_embedding_index": 3619, "title_abs_embedding_index": 3644}, {"title": "Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout", "link_suffix": "/forum?id=3ep9ZYMZS3", "link": "https://openreview.net/forum?id=3ep9ZYMZS3", "pdf_link": "https://openreview.net/pdf?id=3ep9ZYMZS3", "keywords": "deep learning, knowledge guided machine learning, scientific machine learning, computational fluid dynamics, reinforcement learning", "abstract": "Modeling the evolution of physical systems is critical to many applications in science and engineering. As the evolution of these systems is predominantly governed by partial differential equations (PDEs), there are a number of sophisticated computational simulations which resolve these systems with high accuracy. However, as these simulations incur high computational costs, they are infeasible to be employed for large-scale analysis. A popular alternative to simulators are neural network surrogates which are trained in a data-driven manner and are much more computationally efficient. However, these surrogate models suffer from high rollout error when used autoregressively, especially when confronted with training data paucity (i.e., a small number of trajectories to learn from). Existing work proposes to improve surrogate rollout error by either including physical loss terms directly in the optimization of the model or incorporating computational simulators as `differentiable layers' in the neural network. Both of these approaches have their challenges, with physical loss functions suffering from slow convergence for stiff PDEs and simulator layers requiring gradients which are not always available, especially in legacy simulators. We propose the Hybrid PDE Predictor with RL (HyPER) model: a model-agnostic, RL based, cost-aware model which combines a neural surrogate, RL decision model, and a physics simulator (with or without gradients) to reduce surrogate rollout error significantly. In addition to reducing rollout error by 60%-90% we show that HyPER learns an intelligent policy that is adaptable to changing physical conditions and resistant to noise corruption.", "title_embedding_index": 3620, "title_abs_embedding_index": 3645}, {"title": "Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence", "link_suffix": "/forum?id=iflKXk8oeg", "link": "https://openreview.net/forum?id=iflKXk8oeg", "pdf_link": "https://openreview.net/pdf?id=iflKXk8oeg", "keywords": "embedding methods, interchangeable tokens, extendable vocabulary, transformer models, linear temporal logic, formal reasoning, token generalization, alpha-equivalence, inductive bias, language models, symbolic representation, neural networks", "abstract": "We propose a novel approach for learning interchangeable tokens in language models to obtain an extendable vocabulary that can generalize to new tokens. Our method is designed to address alpha-equivalence, the principle that renaming bound variables in a syntactic expression preserves semantics. This property arises in many formal languages such as temporal logics, in which all proposition symbols represent the same concept but are distinguishable from each other. To handle such tokens, we develop a dual-part embedding approach. The first part is shared across all interchangeable tokens, thereby enforcing that they represent the same core concept. The second part is randomly generated for each token, which enables distinguishability. We evaluate our method in a Transformer encoder-decoder model on two tasks: solving linear temporal logic formulae and copying with extendable vocabulary. Our method demonstrates promising generalization capabilities in addition to introducing a favorable inductive bias for alpha-equivalence.", "title_embedding_index": 3621, "title_abs_embedding_index": 3646}, {"title": "CURATe: Benchmarking Personalised Alignment of Conversational AI Assistants", "link_suffix": "/forum?id=ZJCSlcEjEn", "link": "https://openreview.net/forum?id=ZJCSlcEjEn", "pdf_link": "https://openreview.net/pdf?id=ZJCSlcEjEn", "keywords": "LLM, safety and alignment, agentic AI, personalised alignment, context-sensitive, recommender systems, benchmark, multi-turn evaluation, dialogue assistants", "abstract": "We introduce a multi-turn benchmark for evaluating personalised alignment in LLM-based AI assistants, focusing on their ability to handle user-provided safety-critical contexts. Our assessment of six leading models across five scenarios (each with 337 use cases) reveals systematic inconsistencies in maintaining user-specific consideration, with even top-rated \"harmless\" models making recommendations which should be recognised as obviously harmful to the user given the context provided. Key failure modes include improper weighing of conflicting preferences, sycophancy (prioritising user preferences above safety), a lack of attentiveness to critical user information in the context window, and inconsistent application of user-specific knowledge. We find that prompting LLMs to consider safety-critical context significantly improves performance, unlike a generic 'harmless and helpful' reminder. Based on these findings, we propose research directions for embedding self-reflection capabilities, online user modelling, and dynamic risk assessment in AI assistants. Our work emphasises the need for nuanced, context-aware approaches to alignment in systems designed for persistent human interaction, aiding the development of safe and considerate AI assistants.", "title_embedding_index": 3622, "title_abs_embedding_index": 3647}, {"title": "Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance", "link_suffix": "/forum?id=eFoj2egr7G", "link": "https://openreview.net/forum?id=eFoj2egr7G", "pdf_link": "https://openreview.net/pdf?id=eFoj2egr7G", "keywords": "Large Vision-Language Models, Object Hallucination, Multi-modal LLMs", "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs for post-generation correction. In response to these limitations, we proposeMitigating hallucinAtion via image-gRounded guIdaNcE(MARINE), a framework that is bothtraining-freeandAPI-free. MARINE effectively and efficiently reduces object hallucinations during inference by introducing image-grounded guidance to LVLMs. This is achieved by leveraging open-source vision models to extract object-level information, thereby enhancing the precision of LVLM-generated content. Our framework's flexibility further allows for the integration of multiple vision models, enabling more reliable and robust object-level guidance. Through comprehensive evaluations across $5$ popular LVLMs with diverse evaluation metrics and benchmarks, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it reduces hallucinations consistently in GPT-4V-assisted evaluation while maintaining the detailedness of LVLMs' generations.", "title_embedding_index": 3623, "title_abs_embedding_index": 3648}, {"title": "Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians", "link_suffix": "/forum?id=1durmugh3I", "link": "https://openreview.net/forum?id=1durmugh3I", "pdf_link": "https://openreview.net/pdf?id=1durmugh3I", "keywords": "machine learning force fields, graph neural networks, knowledge distillation", "abstract": "Foundation models trained on large datasets have transformed machine learning by leveraging general-purpose representations to solve many downstream tasks. A similar paradigm is arising in Machine Learning Force Fields (MLFFs), a powerful tool in computational chemistry for a variety of atomistic modeling tasks. Recent MLFF foundation models are enabled by a combination of increasing ab-initio data availability and larger model sizes. Although MLFF foundation models have begun to close the accuracy gap relative to first-principles methods, there is still a strong need for faster inference speed. Additionally, while model development is increasingly focused on general-purpose models which transfer across chemical space, practitioners typically only study a small subset of systems at a given time. This underscores the need for fast, specialized MLFFs relevant to specific downstream applications. In this work, we introduce a method to transfer general-purpose representations from MLFF foundation models to smaller, faster MLFFs specialized to specific regions of chemical space. We formulate our approach as a knowledge distillation procedure, where the smaller \"student\" MLFF is trained to match the Hessians of the energy predictions of the \"teacher\" foundation model. By selectively subsampling rows of the Hessian corresponding to individual atomic coordinates, we significantly reduce the number of required backward passes. This ensures that distillation incurs a small computational cost relative to training the original foundation model. We demonstrate our approach across multiple recent foundation models, large-scale datasets, and chemical subsets. Our results demonstrate that our specialized MLFFs can be up to 20 $\\times$ faster than the original foundation model, while retaining, and in some cases exceeding, its performance. More broadly, our work suggests a new paradigm for MLFF development, in which foundation models are released along with smaller, specialized simulation \"engines\" for common chemical subsets.", "title_embedding_index": 3624, "title_abs_embedding_index": 3649}]