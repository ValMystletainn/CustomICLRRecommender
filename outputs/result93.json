[{"title": "Learning Reliable Rules by Re-generating Deep Features", "link_suffix": "/forum?id=ELQ8X02IEp", "link": "https://openreview.net/forum?id=ELQ8X02IEp", "pdf_link": "https://openreview.net/pdf?id=ELQ8X02IEp", "keywords": "Interpretable ML, Neuro-symbolic AI, SATNet, Logical Reasoning", "abstract": "Improving the interpretability and reliability of deep learning models is essential for advancing machine learning applications, though it remains a significant challenge. One promising approach is the integration of logical reasoning into deep learning systems. Previous works have demonstrated that SATNet, a differentiable MaxSAT solver, can learn interpretable and reliable rules from input-output examples in puzzle domains. In this work, we proposeVisual SATNet(Vi-SATNet), an extended version of SATNet capable of learning logical reasoning rules in more general and complex domains, such as the feature space of real-life images. We find that, given a pre-trained deep convolutional neural network (CNN) architecture, a Vi-SATNet layer can be integrated and trained efficiently to learn a set of reasoning rules on the deep features, guiding the classifier\u2019s decision. Vi-SATNets are trained to perform feature re-generation tasks for a given image dataset, where the re-generated features maintain high accuracy when used for image classification, proving their quality. In our experiment on the Imagenette dataset with a pre-trained VGG19 model, masking out 10% to 80% of the features results in classification accuracy ranging from 98.50% to 93.92% with Vi-SATNet re-generation, compared to 97.07% to 9.83% without re-generation. Furthermore, we introduce a visualization method to illustrate the rules learned by Vi-SATNets, thereby enhancing the interpretability of the pre-trained CNN model.", "title_embedding_index": 4600, "title_abs_embedding_index": 4625}, {"title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution", "link_suffix": "/forum?id=9kJperA2a4", "link": "https://openreview.net/forum?id=9kJperA2a4", "pdf_link": "https://openreview.net/pdf?id=9kJperA2a4", "keywords": "Large Language Model, Context Attribution, Interpretability", "abstract": "The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a 300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response $30\\times$ faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.", "title_embedding_index": 4601, "title_abs_embedding_index": 4626}, {"title": "From Counseling Transcript to Mind Map: Leveraging LLMs for Effective Summarization in Mental Health Counseling", "link_suffix": "/forum?id=zPxlHOLxmh", "link": "https://openreview.net/forum?id=zPxlHOLxmh", "pdf_link": "https://openreview.net/pdf?id=zPxlHOLxmh", "keywords": "Large Language Models, Visual-based Summarization, Mental Health Counseling", "abstract": "The increasing number of patients with mental health illness has heightened the cognitive load on therapists, making it challenging for them to provide personalized care that each patient requires. Summarizing counseling sessions can aid mental health practitioners in recalling key details. However, most existing research on summarization focuses primarily on text-based summaries which often require significant cognitive effort to read and interpret. Visual-based summary such as mind maps is proven to help enhance cognitive understanding by giving a quick overview of topics and content. Nevertheless, due to the complex nature of counseling which involves substantial qualitative data, generating visual-based summaries using traditional AI models can be challenging. With the recent advancements in Large Language Models (LLMs), these models have demonstrated the capability to perform tasks based on instructions and generate outputs in various formats. In this study, we develop a web-based summarization tool that serves as a pipeline in performing summarization of counseling transcripts into visual-based mind map summaries using LLMs. We conducted a human evaluation to validate the effectiveness of the generated visual-based summary based on criteria of accuracy, completeness, conciseness and coherence. Our findings show that our web-based summarization tool can effectively extract key points from counseling transcripts and present them in visual-based mind maps, demonstrating its potential in enhancing insights for therapists, ultimately simplifying the process of documenting counseling sessions.", "title_embedding_index": 4602, "title_abs_embedding_index": 4627}, {"title": "Provably Safeguarding a Classifier from OOD and Adversarial Samples: an Extreme Value Theory Approach", "link_suffix": "/forum?id=kwCHcaeHrf", "link": "https://openreview.net/forum?id=kwCHcaeHrf", "pdf_link": "https://openreview.net/pdf?id=kwCHcaeHrf", "keywords": "OOD detection, Adversarial detection, Extreme Value Theory", "abstract": "This paper introduces a novel method, Sample-efficient Probabilistic Detection using Extreme Value Theory (SPADE),  which transforms a classifier into an abstaining classifier, offering provable protection against out-of-distribution and adversarial samples. The approach is based on a Generalized Extreme Value (GEV) model of the training distribution in the classifier's latent space, enabling the formal characterization of OOD samples. Interestingly, under mild assumptions, the GEV model also allows for a formal characterization of adversarial samples. The abstaining classifier, which rejects samples based on their assessment by the GEV model, provably avoids OOD and adversarial samples. The empirical validation of the approach, conducted on various neural architectures (ResNet, VGG, and Vision Transformer) and tested on medium and large-sized datasets (CIFAR-10, CIFAR-100, and ImageNet), demonstrates its frugality, stability, and efficiency compared to the state of the art.", "title_embedding_index": 4603, "title_abs_embedding_index": 4628}, {"title": "Research Town: Simulator of Research Community", "link_suffix": "/forum?id=IwhvaDrL39", "link": "https://openreview.net/forum?id=IwhvaDrL39", "pdf_link": "https://openreview.net/pdf?id=IwhvaDrL39", "keywords": "multi-agent simulation; automatic research; large language model", "abstract": "Collaboration is central to modern research, with most studies today being the result of teamwork rather than individual efforts. This raises an intriguing question: Can multiple large language models (LLMs) collaborate like human researcher community to generate high-quality research proposals? To explore this, we introduce ResearchTown, a multi-agent research simulator that can automatically pair LLM agents for tasks such as literature review, idea discussion, and peer review. Each academic paper builds on previous research, citing earlier works as its foundation. In ResearchTown, we take advantage of these connections by selecting a paper and providing its related works to LLM agents with relevant expertise. These agents then engage in a collaborative brainstorming process, similar to human researchers, with the goal of generating novel high-quality proposals. This setup allows us to benchmark the research capabilities of LLM agents by measuring the similarity between their generated ideas and the original paper using automatic similarity scores, combined with human evaluations to assess their quality and coherence. Through a comprehensive simulation of research community activities, we discover three key insights: (1) LLMs can generate research proposals that partially align with human-written proposals, demonstrating their ability to match the quality and structure of academic work. (2) As we scale the number of samples during decoding, LLMs produce a wider range of novel and valuable ideas, often surpassing the scope of existing human research. (3) ResearchTown can foster interdisciplinary collaboration and lead to novel cross-domain research ideas that are rarely observed in real-world settings.", "title_embedding_index": 4604, "title_abs_embedding_index": 4629}, {"title": "MIMOSA: Multimodal Concept-based representations", "link_suffix": "/forum?id=uffmkDtlR2", "link": "https://openreview.net/forum?id=uffmkDtlR2", "pdf_link": "https://openreview.net/pdf?id=uffmkDtlR2", "keywords": "Concept-based model, Multimodal, Explainability", "abstract": "In recent years, deep learning-based architectures have significantly improved multimodal representation. However, interpretability remains challenging with traditional attention and gradient-based methods, offering limited insights into decision-making processes. Concept-based explainability provides intrinsic model interpretability by mapping raw data to higher-level abstractions, yet it has only been applied to unimodal data. \nWe present MIMOSA (MultIMOdal concept-based repreSentAtion), a unified multimodal model that integrates concept-based interpretability. Our research shows that exploiting a joint multimodal conceptual representation achieves comparable accuracy with multimodal black-box models, surpassing approaches based on unimodal concepts. This unified representation also prevents misclassification of concepts between modalities and improves concept interventions. Through a concept decoder, MIMOSA can extract concept visualizations for each modality. \nExperimental results obtained from three distinct multimodal datasets substantiate the efficacy of our approach, showcasing enhanced interpretability in multimodal models.", "title_embedding_index": 4605, "title_abs_embedding_index": 4630}, {"title": "Learning Chaos In A Linear Way", "link_suffix": "/forum?id=Llh6CinTiy", "link": "https://openreview.net/forum?id=Llh6CinTiy", "pdf_link": "https://openreview.net/pdf?id=Llh6CinTiy", "keywords": "Dynamical systems, operator learning, chaos, physics-informed learning", "abstract": "Learning long-term behaviors in chaotic dynamical systems, such as turbulent flows and climate modelling, is challenging due to their inherent instability and unpredictability. These systems exhibit positive Lyapunov exponents, which significantly hinder accurate long-term forecasting. As a result, understanding long-term statistical behavior is far more valuable than focusing on short-term accuracy. While autoregressive deep sequence models have been applied to capture long-term behavior, they often lead to exponentially increasing errors in learned dynamics. To address this, we shift the focus from simple prediction errors to preserving an invariant measure in dissipative chaotic systems. These systems have attractors, where trajectories settle, and the invariant measure is the probability distribution on attractors that remains unchanged under dynamics. Existing methods generate long trajectories of dissipative chaotic systems by aligning invariant measures, but it is not always possible to obtain invariant measures for arbitrary datasets. We propose the Poincar\u00e9 Flow Neural Network (PFNN), a novel operator learning framework designed to capture behaviors of chaotic systems without any explicit knowledge of the invariant measure. PFNN employs an auto-encoder to map the chaotic system to a finite-dimensional feature space, effectively linearizing the chaotic evolution. It then learns the linear evolution operators to match the physical dynamics by addressing two critical properties in dissipative chaotic systems: (1) contraction, the system\u2019s convergence toward its attractors, and (2) measure invariance, trajectories on the attractors following a probability distribution invariant to the dynamics. \nOur experiments on a variety of chaotic systems, including Lorenz 96, Kuramoto-Sivashinsky equation and Navier\u2013Stokes equation, demonstrate that PFNN has more accurate predictions and physical statistics compared to competitive baselines including the Fourier Neural Operator and the Markov Neural Operator.", "title_embedding_index": 4606, "title_abs_embedding_index": 4631}, {"title": "AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies", "link_suffix": "/forum?id=jCPak79Kev", "link": "https://openreview.net/forum?id=jCPak79Kev", "pdf_link": "https://openreview.net/pdf?id=jCPak79Kev", "keywords": "Circuit Generation, Application of Generative Models, Electronic Design Automation", "abstract": "The massive and large-scale design of foundational semiconductor integrated circuits (ICs) is crucial to sustaining the advancement of many emerging and future technologies, such as generative AI, 5G/6G, and quantum computing.\nExcitingly, recent studies have shown the great capabilities of foundational models in expediting the design of digital ICs.\nYet, applying generative AI techniques to accelerate the design of analog ICs remains a significant challenge due to critical domain-specific issues, such as the lack of a comprehensive dataset and effective representation methods for analog circuits.\nThis paper proposes, $\\textbf{AnalogGenie}$, a $\\underline{\\textbf{Gen}}$erat$\\underline{\\textbf{i}}$ve  $\\underline{\\textbf{e}}$ngine for automatic design/discovery of $\\underline{\\textbf{Analog}}$ circuit topologies--the most challenging and creative task in the conventional manual design flow of analog ICs.\nAnalogGenie addresses two key gaps in the field: building a foundational comprehensive dataset of analog circuit topology and developing a scalable sequence-based graph representation universal to analog circuits.\nExperimental results show the remarkable generation performance of AnalogGenie in broadening the variety of analog ICs, increasing the number of devices within a single design, and discovering unseen circuit topologies far beyond any prior arts.\nOur work paves the way to transform the longstanding time-consuming manual design flow of analog ICs to an automatic and massive manner powered by generative AI.", "title_embedding_index": 4607, "title_abs_embedding_index": 4632}, {"title": "Data Taggants: Dataset Ownership Verification Via Harmless Targeted Data Poisoning", "link_suffix": "/forum?id=6ldD8Y4gBQ", "link": "https://openreview.net/forum?id=6ldD8Y4gBQ", "pdf_link": "https://openreview.net/pdf?id=6ldD8Y4gBQ", "keywords": "dataset watermarking, dataset ownership verification, data poisoning, backdoor attack", "abstract": "Dataset ownership verification, the process of determining if a dataset is used in a model's training data, is necessary for detecting unauthorized data usage and data contamination.\nExisting approaches, such as backdoor watermarking, rely on inducing a detectable behavior into the trained model on a part of the data distribution.\nHowever, these approaches have limitations, as they can be harmful to the model's performances or require unpractical access to the model's internals.\nMost importantly, previous approaches lack guarantee against false positives.This paper introducesdata taggants, a novel non-backdoor dataset ownership verification technique.\nOur method uses pairs of out-of-distribution samples and random labels as secretkeys, and leverages clean-label targeted data poisoning to subtly alter a dataset, so that models trained on it respond to the key samples with the corresponding key labels.\nThe keys are built as to allow for statistical certificates with black-box access only to the model.We validate our approach through comprehensive and realistic experiments on ImageNet1k using ViT and ResNet models with state-of-the-art training recipes.\nOur findings demonstrate that data taggants can reliably make models trained on the protected dataset detectable with high confidence, without compromising validation accuracy, and demonstrates superiority over backdoor watermarking.\nMoreover, our method shows to be stealthy and robust against various defense mechanisms.", "title_embedding_index": 4608, "title_abs_embedding_index": 4633}, {"title": "FedADM: Adaptive Federated Learning via Dissimilarity Measure", "link_suffix": "/forum?id=IsHWcsk4Fz", "link": "https://openreview.net/forum?id=IsHWcsk4Fz", "pdf_link": "https://openreview.net/pdf?id=IsHWcsk4Fz", "keywords": "Distributed Optimization, Federated Learning, Local Updates, Aggregation", "abstract": "In federated learning, there are two critical challenges: 1) the data on distributed learners is heterogeneous; and 2) communication resources within the network are limited. In this work, we propose a framework, Federated Adaptive Dissimilarity Measure (FedADM), which can be regarded as an adaptively enhanced version of the Federated Proximal (FedProx) algorithm. This adaptiveness is primarily manifested in two aspects: (i) how it adaptively adjusts the proximity between the local models on different learners and the global model; and (ii) how it adaptively aggregates local model parameters. Building on the FedProx model, FedADM incorporates the concept of the Lagrangian multiplier to control the proximal coefficients of different learners, using \u201c\\textit{parameter dissimilarity}\" to address data heterogeneity. It explicitly captures the essence of using \u201c\\textit{loss dissimilarity}\" to adaptively adjust the aggregation frequency on distributed learners, thereby reducing communication overhead. Theoretically, we provide the performance upper bounds and convergence analysis of our proposed FedADM. Experiment results demonstrate that FedADM allows for higher accuracy and lower communication overhead compared to the baselines across a suite of realistic datasets.", "title_embedding_index": 4609, "title_abs_embedding_index": 4634}, {"title": "DEPT: Decoupled Embeddings for Pre-training Language Models", "link_suffix": "/forum?id=vf5aUZT0Fz", "link": "https://openreview.net/forum?id=vf5aUZT0Fz", "pdf_link": "https://openreview.net/pdf?id=vf5aUZT0Fz", "keywords": "Decentralized Training, Federated Learning, Multi-domain Training, Multilingual Training", "abstract": "Language Model pre-training benefits from a broader data mixture to enhance performance across domains and languages. However, training on such heterogeneous text corpora is complex, requiring extensive and cost-intensive efforts. Since these data sources vary in lexical, syntactic, and semantic aspects, they cause negative interference or the ``curse of multilinguality''. We propose a novel pre-training framework to alleviate this curse. Our method, DEPT, decouples the embedding layers from the transformer body while simultaneously training the latter in multiple contexts. DEPT enables the model to train without being bound to a shared global vocabulary. DEPT: (1) can train robustly and effectively under significant data heterogeneity, (2) reduces the parameter count of the token embeddings by up to 80% and the communication costs by 675x for billion-scale models (3) enhances model generalization and plasticity in adapting to new languages and domains, and (4) allows training with custom optimized vocabulary per data source. We prove DEPT's potential by performing the first vocabulary-agnostic federated multilingual pre-training of a 1.3  billion-parameter model across high and low-resource languages, reducing its parameter count by 409 million.", "title_embedding_index": 4610, "title_abs_embedding_index": 4635}, {"title": "BDetCLIP: Multimodal Prompting Contrastive Test-Time Backdoor Detection", "link_suffix": "/forum?id=Uqxf2YH9LZ", "link": "https://openreview.net/forum?id=Uqxf2YH9LZ", "pdf_link": "https://openreview.net/pdf?id=Uqxf2YH9LZ", "keywords": "Multimodal contrastive learning, test-time backdoor detection, contrastive prompting", "abstract": "Multimodal contrastive learning methods (e.g., CLIP) have shown impressive zero-shot classification performance due to their strong ability to joint representation learning for visual and textual modalities. However, recent research revealed that multimodal contrastive learning on poisoned pre-training data with a small proportion of maliciously backdoored data can induce backdoored CLIP that could be attacked by inserted triggers in downstream tasks with a high success rate. To defend against backdoor attacks on CLIP, existing defense methods focus on either the pre-training stage or the fine-tuning stage, which would unfortunately cause high computational costs due to numerous parameter updates and are not applicable in the black-box setting. In this paper, we provide the first attempt at a computationally efficient backdoor detection method to defend against backdoored CLIP in the inference stage. We empirically find that the visual representations of backdoored images are insensitive to both benign and malignant changes in class description texts. Motivated by this observation, we propose BDetCLIP, a novel test-time backdoor detection method based on contrastive prompting. Specifically, we first prompt the language model (e.g., GPT-4) to produce class-related description texts (benign) and class-perturbed random texts (malignant) by specially designed instructions. Then, the distribution difference in cosine similarity between images and the two types of class description texts can be used as the criterion to detect backdoor samples. Extensive experiments validate that our proposed BDetCLIP is superior to state-of-the-art backdoor detection methods, in terms of both effectiveness and efficiency.", "title_embedding_index": 4611, "title_abs_embedding_index": 4636}, {"title": "STDM: Spatio-Temporal Diffusion Models for Time Series Analysis", "link_suffix": "/forum?id=2orBSi7pvi", "link": "https://openreview.net/forum?id=2orBSi7pvi", "pdf_link": "https://openreview.net/pdf?id=2orBSi7pvi", "keywords": "Diffusion Models, Time Series Analysis, Anomaly Detection, Forecasting", "abstract": "Denoising diffusion models have emerged as a formidable method, consistently surpassing previous state-of-the-art benchmarks. However, a notable challenge in time series-related tasks like anomaly detection and forecasting is the conditioning for models to reconstruct inputs accurately or generate samples based on past time steps rather than producing entirely new samples. To address this, we introduce a novel technique that enhances the sampling capabilities of denoising diffusion models for time series analysis, namely Spatio-Temporal Diffusion Models (STDM). While recent methods fall short of mapping contextual neighborhood dependencies directly into the sampling of a noisy sample, we focus on guiding the forward process of the diffusion model. The degeneration of a sample is based on the idea that values of neighboring time steps are highly correlated. We benefit from this assumption by presenting a diffusion step-dependent convolutional kernel to capture spatial relations and a combined, correlated noise to degenerate the input. Our method can be integrated seamlessly into various existing time series diffusion models. We compare the results of anomaly detection and forecasting when using the traditional and our novel forward process. In our experiments on synthetic and real-world datasets, we show that an adaption of the forward process can be beneficial, as our approach outperforms diffusion models with the ordinary forward process in task-specific metrics, underscoring the efficacy of our strategy in enhancing time series analysis through advanced diffusion techniques.", "title_embedding_index": 4612, "title_abs_embedding_index": 4637}, {"title": "Do Contemporary CATE Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark", "link_suffix": "/forum?id=Q2bJ2qgcP1", "link": "https://openreview.net/forum?id=Q2bJ2qgcP1", "pdf_link": "https://openreview.net/pdf?id=Q2bJ2qgcP1", "keywords": "causal inference", "abstract": "We present unexpected findings from a large-scale benchmark study evaluating Conditional Average Treatment Effect (CATE) estimation algorithms. By running 16 modern CATE models across 43,200 datasets, we find that: (a) 62% of CATE estimates have a higher Mean Squared Error (MSE) than a trivial zero-effect predictor, rendering them ineffective; (b) in datasets with at least one useful CATE estimate, \n80% still have higher MSE than a constant-effect model; and (c) Orthogonality-based models outperform other models only 30% of the time, despite widespread optimism about their performance.  These findings expose significant limitations in current CATE models and suggest ample opportunities for further research.Our findings stem from a novel application of \\textit{observational sampling}, originally developed to evaluate Average Treatment Effect (ATE) estimates from observational methods with experiment data. To adapt observational sampling for CATE evaluation, we introduce a statistical parameter, $Q$, equal to MSE minus a constant and preserves the ranking of models by their MSE. We then derive a family of sample statistics, collectively called $\\hat{Q}$, that can be computed from real-world data. We prove that $\\hat{Q}$ is a consistent estimator of $Q$ under mild technical conditions. When used in observational sampling, $\\hat{Q}$ is unbiased and asymptotically selects the model with the smallest MSE. To ensure the benchmark reflects real-world heterogeneity, we handpick datasets where outcomes come from field rather than simulation. By combining the new observational sampling method, new statistics, and real-world datasets, the benchmark provides a unique perspective on CATE estimator performance and uncover gaps in capturing real-world heterogeneity.", "title_embedding_index": 4613, "title_abs_embedding_index": 4638}, {"title": "Symbolic Music Generation with Fine-grained Interactive Textural Guidance", "link_suffix": "/forum?id=Qt5sBi0u7I", "link": "https://openreview.net/forum?id=Qt5sBi0u7I", "pdf_link": "https://openreview.net/pdf?id=Qt5sBi0u7I", "keywords": "Symbolic Music Generation; Guided Diffusion Models", "abstract": "The problem of symbolic music generation presents unique challenges due to the combination of limited data availability and the need for high precision in note pitch. To overcome these difficulties, we introduce Fine-grained Textural Guidance (FTG) within diffusion models to correct errors in the learned distributions. By incorporating FTG, the diffusion models improve the accuracy of music generation, which makes them well-suited for advanced tasks such as progressive music generation, improvisation and interactive music creation. We provide theoretical characterizations for both the challenges in symbolic music generation and the effect of the FTG approach. We provide numerical experiments and a demo page for interactive music generation with user input to showcase the effectiveness of our approach.", "title_embedding_index": 4614, "title_abs_embedding_index": 4639}, {"title": "Improving real-world sequence design with a simple meta-heuristic for detecting distribution shift", "link_suffix": "/forum?id=B1nfjxZI6z", "link": "https://openreview.net/forum?id=B1nfjxZI6z", "pdf_link": "https://openreview.net/pdf?id=B1nfjxZI6z", "keywords": "protein engineering, sequence design, model-based optimization", "abstract": "Biological sequence design is one of the most impactful areas where model-based optimization is applied. A common scenario involves using a fixed training set to train predictive models, with the goal of designing new sequences that outperform those present in the training data. This by definition results in a distribution shift, where the model is applied to samples that are substantially different from those in the training set (or otherwise they wouldn\u2019t have a chance of being much better). While most MBO methods offer some balancing heuristic to control for false positives, finding the right balance of pushing the design distribution while maintaining model accuracy requires deep knowledge of the algorithm and artful application, limiting successful adoption by practitioners. To tackle this issue, we propose a straightforward meta-algorithm for design practitioners that detects distribution shifts when using any MBO. By doing a real-world sequence design experiment, we show that (1) Real world distribution shift is far more severe than observed in simulated settings, where most MBO algorithms are benchmarked (2) Our approach successfully reduces the adverse effects of distribution shift. We believe this method can significantly improve design quality for sequence design tasks and potentially other domain applications where offline optimization faces harsh distribution shifts.", "title_embedding_index": 4615, "title_abs_embedding_index": 4640}, {"title": "LLM Detectors Still Fall Short of Real World: Case of LLM-Generated Short News-Like Posts", "link_suffix": "/forum?id=HsB1sQvXML", "link": "https://openreview.net/forum?id=HsB1sQvXML", "pdf_link": "https://openreview.net/pdf?id=HsB1sQvXML", "keywords": "LLM Detection, misinformation, benchmarking", "abstract": "With the emergence of widely available powerful LLMs, disinformation generated by large Language Models (LLMs) has become a major concern. Historically, LLM detectors have been touted as a solution, but their effectiveness in the real world is still to be proven. In this paper, we focus on an important setting in information operations\u2014short news-like posts generated by moderately sophisticated attackers.We demonstrate that existing LLM detectors, whether zero-shot or purpose-trained, are not ready for real-world use in that setting. All tested zero-shot detectors perform inconsistently with prior benchmarks and are highly vulnerable to sampling temperature increase, a trivial attack absent from recent benchmarks. A purpose-trained detector generalizing across LLMs and unseen attacks can be developed, but it fails to generalize to new human-written texts.We argue that the former indicates domain-specific benchmarking is needed, while the latter suggests a trade-off between the adversarial evasion resilience and overfitting to the reference human text, with both needing evaluation in benchmarks and currently absent. We believe this suggests a re-consideration of current LLM detector benchmarking approaches and provides a dynamically extensible benchmark to allow it (https://anonymous.4open.science/r/text_llm_detector-3E07).", "title_embedding_index": 4616, "title_abs_embedding_index": 4641}, {"title": "Harnessing Webpage UIs for Text-Rich Visual Understanding", "link_suffix": "/forum?id=IIsTO4P3Ag", "link": "https://openreview.net/forum?id=IIsTO4P3Ag", "pdf_link": "https://openreview.net/pdf?id=IIsTO4P3Ag", "keywords": "Multimodal, Instruction-tuning, Large Language Model", "abstract": "Text-rich visual understanding\u2014the ability to interpret both textual content and visual elements within a scene\u2014is crucial for multimodal large language models (MLLMs) to effectively interact with structured environments. We propose leveraging webpage UIs as a naturally structured and diverse data source to enhance MLLMs\u2019 capabilities in this area. Existing approaches, such as rule-based extraction, multimodal model captioning, and rigid HTML parsing, are hindered by issues like noise, hallucinations, and limited generalization. To overcome these challenges, we introduce MultiUI, a dataset of 7.3 million samples spanning various UI types and tasks, structured using enhanced accessibility trees and task taxonomies. By scaling multimodal instructions from web UIs through LLMs, our dataset enhances generalization beyond web domains, significantly improving performance in document understanding, GUI comprehension, grounding, and advanced agent tasks. This demonstrates the potential of structured web data to elevate MLLMs\u2019 proficiency in processing text-rich visual environments and generalizing across domains.", "title_embedding_index": 4617, "title_abs_embedding_index": 4642}, {"title": "Fast unsupervised ground metric learning with tree-Wasserstein distance", "link_suffix": "/forum?id=FBhKUXK7od", "link": "https://openreview.net/forum?id=FBhKUXK7od", "pdf_link": "https://openreview.net/pdf?id=FBhKUXK7od", "keywords": "unsupervised learning, optimal transport, distance-based learning, clustering, trees, wasserstein distance", "abstract": "The performance of unsupervised methods such as clustering depends on the choice of distance metric between features, or ground metric. Commonly, ground metrics are decided with heuristics or learned via supervised algorithms. However, since many interesting datasets are unlabelled, unsupervised ground metric learning approaches have recently been introduced. One promising option employs Wasserstein singular vectors (WSV), which emerge when computing optimal transport distances between features and samples simultaneously. While WSV is effective, it is computationally expensive ($\\mathcal{O}(n^5)$ complexity). In this work, we propose to augment the WSV method by embedding samples and features on trees, on which we compute the tree-Wasserstein distance (TWD). We demonstrate theoretically and in practice that the algorithm converges to a better approximation of the full WSV approach than the best known alternative, entropy regularisation or Sinkhorn singular vectors, but with faster (cubic) computational efficiency. In addition, we prove that the initial tree structure can be chosen flexibly, since tree geometry does not constrain the richness of the approximation up to the number of edge weights. This proof suggests a fast, recursive algorithm for computing the tree parameter basis set, which we find crucial to realising the efficiency gains at scale. Finally, we employ the tree-WSV algorithm to several single-cell RNA sequencing genomics datasets, demonstrating its scalability and utility for unsupervised cell-type clustering problems. These results poise unsupervised ground metric learning with TWD as a low-rank approximation of WSV with the potential for widespread low-compute application.", "title_embedding_index": 4618, "title_abs_embedding_index": 4643}, {"title": "Homomorphism Expressivity of Spectral Invariant Graph Neural Networks", "link_suffix": "/forum?id=rdv6yeMFpn", "link": "https://openreview.net/forum?id=rdv6yeMFpn", "pdf_link": "https://openreview.net/pdf?id=rdv6yeMFpn", "keywords": "Graph Neural Network, Expressive Power, Spectral Invariant, Graph Homomorphism, Weisfeiler-Lehman", "abstract": "Graph spectra are an important class of structural features on graphs that have shown promising results in enhancing Graph Neural Networks (GNNs). Despite their widespread practical use, the theoretical understanding of the power of spectral invariants --- particularly their contribution to GNNs --- remains incomplete. In this paper, we address this fundamental question through the lens of homomorphism expressivity, providing a comprehensive and quantitative analysis of the expressive power of spectral invariants. Specifically, we prove that spectral invariant GNNs can homomorphism-count exactly a class of specific tree-like graphs which we refer to as \\emph{parallel trees}. We highlight the significance of this result in various contexts, including establishing a quantitative expressiveness hierarchy across different architectural variants, offering insights into the impact of GNN depth, and understanding the subgraph counting capabilities of spectral invariant GNNs. In particular, our results significantly extend \\citet{arvind2024hierarchy} and settle their open questions. Finally, we generalize our analysis to higher-order GNNs and answer an open question raised by \\citet{zhang2024expressive}.", "title_embedding_index": 4619, "title_abs_embedding_index": 4644}, {"title": "Prover-Verifier Games improve legibility of LLM outputs", "link_suffix": "/forum?id=j4s6V1dl8m", "link": "https://openreview.net/forum?id=j4s6V1dl8m", "pdf_link": "https://openreview.net/pdf?id=j4s6V1dl8m", "keywords": "Prover-Verifier Games, Large Language Models, AI alignment, Human Evaluation, Scalable Oversight", "abstract": "One way to increase confidence in the outputs of Large Language Models (LLMs) is to support them with reasoning that is clear and easy to check \u2014 a property we call legibility. We study legibility in the context of solving grade-school math problems and show that optimizing chain-of-thought solutions only for answer correctness can make them less legible. To mitigate the loss in legibility, we propose a training algorithm inspired by Prover-Verifier Game from Anil et al. (2021). Our algorithm iteratively trains small verifiers to predict solution correctness, \u201chelpful\u201d provers to produce correct solutions that the verifier accepts, and \u201csneaky\u201d provers to produce incorrect solutions that fool the verifier. We find that the helpful prover\u2019s accuracy and the verifier\u2019s robustness to adversarial attacks increase over the course of training. Furthermore, we show that legibility training transfers to time-constrained humans tasked with verifying solution correctness. Over course of LLM training human accuracy increases when checking the helpful prover\u2019s solutions, and decreases when checking the sneaky prover\u2019s solutions. Hence, training for checkability by small verifiers is a plausible technique for increasing output legibility. Our results suggest legibility training against small verifiers as a practical avenue for increasing legibility of large LLMs to humans, and thus could help with alignment of superhuman models.", "title_embedding_index": 4620, "title_abs_embedding_index": 4645}, {"title": "World Model on Million-Length Video And Language With Blockwise RingAttention", "link_suffix": "/forum?id=HN8V0flwJF", "link": "https://openreview.net/forum?id=HN8V0flwJF", "pdf_link": "https://openreview.net/pdf?id=HN8V0flwJF", "keywords": "long context, language model", "abstract": "Enabling long-context understanding remains a key challenge in scaling existing sequence models -- a crucial component in developing generally intelligent models that can process and operate over long temporal horizons that potentially consist of millions of tokens. In this paper, we seek to make strides towards addressing these challenges by providing a comprehensive exploration into the full development process to produce 1M context language models and video-language models, setting new benchmarks in language retrieval and new capabilities in long video understanding. Furthermore, we provide details in our long context data curation process, progressive context extension from 4K to 1M tokens, and an efficient Fused Blockwise RingAttention implementation to scalably train on long sequences. As a benefit to the community, we additionally fully open-source a family of 7B parameter models capable of processing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM, LWM-Chat) of over 1M tokens.", "title_embedding_index": 4621, "title_abs_embedding_index": 4646}, {"title": "TICKing All the Boxes: Generated Checklists Improve LLM Evaluation and Generation", "link_suffix": "/forum?id=1dUdNzLJRF", "link": "https://openreview.net/forum?id=1dUdNzLJRF", "pdf_link": "https://openreview.net/pdf?id=1dUdNzLJRF", "keywords": "large language models, evaluation, instruction following, self-critique", "abstract": "Given the widespread adoption and usage of Large Language Models (LLMs), it is crucial to have flexible and interpretable evaluations of their instruction-following ability. Furthermore, as human annotation is slow and costly, LLMs are increasingly used to make these judgments, at the expense of reliability and interpretability. In this work, we propose TICK (Targeted Instruct-evaluation with ChecKlists), a fully automated, interpretable evaluation protocol that structures evaluations with LLM-generated, instruction-specific checklists. We first show that, given an instruction, LLMs can reliably produce high-quality, tailored evaluation checklists that decompose the instruction into a series of YES/NO questions. Each question asks whether a candidate response meets a specific requirement of the instruction. We demonstrate that using TICK leads to a significant increase (46.4% $\\to$ 52.2%) in the frequency of exact agreements between LLM judgements and human preferences, as compared to having an LLM directly score an output. We then show that \\textbf{STICK} (Self-TICK) can be used to improve generation quality across multiple benchmarks via self-refinement and best-of-N selection. STICK self-refinement on LiveBench reasoning tasks leads to an absolute gain of $+$7.8%, whilst best-of-N selection with STICK attains $+$6.3% absolute improvement on the real-world instruction dataset, WildBench. In light of this, structured, multi-faceted self-improvement is shown to be a promising way to further advance LLM capabilities. Finally, by providing LLM-generated checklists to human evaluators tasked with directly scoring LLM responses to WildBench instructions, we notably increase inter-annotator agreement (0.194 $\\to$ 0.256).", "title_embedding_index": 4622, "title_abs_embedding_index": 4647}, {"title": "Quamba: A Post-Training Quantization Recipe for Selective State Space Models", "link_suffix": "/forum?id=mnna9LUg7P", "link": "https://openreview.net/forum?id=mnna9LUg7P", "pdf_link": "https://openreview.net/pdf?id=mnna9LUg7P", "keywords": "State Space Models, Model quantization", "abstract": "State Space Models (SSMs) have emerged as an appealing alternative to Transformers for large language models, achieving state-of-the-art accuracy with constant memory complexity which allows for holding longer context lengths than attention-based networks. The superior computational efficiency of SSMs in long sequence modeling positions them favorably over Transformers in many scenarios. However, improving the efficiency of SSMs on request-intensive cloud-serving and resource-limited edge applications is still a formidable task. SSM quantization is a possible solution to this problem, making SSMs more suitable for wide deployment, while still maintaining their accuracy. Quantization is a common technique to reduce the model size and to utilize the low bit-width acceleration features on modern computing units, yet existing quantization techniques are poorly suited for SSMs. Most notably, SSMs have highly sensitive feature maps within the selective scan mechanism (i.e., linear recurrence) and massive outliers in the output activations which are not present in the output of token-mixing in the self-attention modules. To address this issue, we propose a static 8-bit per-tensor SSM quantization method which suppresses the maximum values of the input activations to the selective SSM for finer quantization precision and quantizes the output activations in an outlier-free space with Hadamard transform. Our 8-bit weight-activation quantized Mamba 2.8B SSM benefits from hardware acceleration and achieves a 1.72 $\\times$ lower generation latency on an Nvidia Orin Nano 8G, with only a 0.9% drop in average accuracy on zero-shot tasks. When quantizing Jamba, a 52B parameter SSM-style language model, we observe only a $1%$  drop in accuracy, demonstrating that our SSM quantization method is both effective and scalable for large language models, which require appropriate compression techniques for deployment. The experiments demonstrate the effectiveness and practical applicability of our approach for deploying SSM-based models of all sizes on both cloud and edge platforms.", "title_embedding_index": 4623, "title_abs_embedding_index": 4648}, {"title": "Impact of Prompt on Latent Representations in LLMs", "link_suffix": "/forum?id=10kBEqYKKN", "link": "https://openreview.net/forum?id=10kBEqYKKN", "pdf_link": "https://openreview.net/pdf?id=10kBEqYKKN", "keywords": "Explainability, Representation analysis, LLM, prompting, zero-shot", "abstract": "The effectiveness of zero-shot learning frameworks, particularly in Large Language Models (LLMs), has lately shown tremendous improvement. Nonetheless, zero-shot performance critically depends on the prompt quality. Scientific literature has been prolific in proposing methods to select, create, and evaluate prompts from a language or performance perspective, changing their phrasing or creating them following heuristics rules. While these approaches are intuitive, they are insufficient in unveiling the internal mechanisms of Large Language Models. In this work,  we propose exploring the impact of prompts on the latent representations of auto-regressive transformer models considering a zero-shot setting. We focus on the geometrical properties of prompts' inner representation at different stages of the model. Experiments conducted give insights into how prompt characteristics influence the structure and distribution of vector representations in generative models. We focus on binary classification tasks on which prompting methods have shown robust performance and show that prompt formulation has indeed an influence on latent representation. However, their impact is dependent on the model family. Using clustering methods, we show that even though prompts are similar in natural language, surprisingly, their representations can differ. This is highly model-dependent, demonstrating the need for more precise analysis.", "title_embedding_index": 4624, "title_abs_embedding_index": 4649}]