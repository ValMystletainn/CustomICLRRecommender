[{"title": "Towards a Knowledge guided Multimodal Foundation Model for Spatio-Temporal Remote Sensing Applications", "link_suffix": "/forum?id=y8qBBbAdEv", "link": "https://openreview.net/forum?id=y8qBBbAdEv", "pdf_link": "https://openreview.net/pdf?id=y8qBBbAdEv", "keywords": "Foundation model, Spatiotemporal modelling, Remote Sensing, Knowledge guided", "abstract": "In recent years, there has been an increased interest in foundation models for geoscience due to the vast amount of Earth observing satellite imagery. Existing remote sensing foundation models make use of the various sources of spectral imagery to create large models pretrained on the task of masked reconstruction. In this paper, we present a foundation model framework, where the pretraining task captures the causal relationship between multiple modalities. Our framework leverages the knowledge guided principles that the spectral imagery captures the impact of the physical drivers on the environmental system, and that the relationship between them is governed by the characteristics of the system. Specifically, our method, called MultiModal Variable Step Forecasting (MM-VSF), uses forecasting of satellite imagery as a pretraining task and is able to capture the causal relationship between spectral imagery and weather. In our evaluation we show that the forecasting of satellite imagery using weather can be used as an effective pretraining task for foundation models. We further show the effectiveness of the embeddings produced by MM-VSF on the downstream tasks of pixel wise crop mapping and missing image prediction of spectral imagery, when compared with embeddings created by models trained in alternative pretraining settings including the traditional single modality input masked reconstruction.", "title_embedding_index": 1500, "title_abs_embedding_index": 1525}, {"title": "Scale-Aware Contrastive Reverse Distillation for Unsupervised Anomaly Detection", "link_suffix": "/forum?id=HNOo4UNPBF", "link": "https://openreview.net/forum?id=HNOo4UNPBF", "pdf_link": "https://openreview.net/pdf?id=HNOo4UNPBF", "keywords": "unsupervised anomaly detection, medical images, contrastive reverse distillation, student-teacher", "abstract": "Unsupervised anomaly detection using deep learning has garnered significant research attention due to its broad applicability, particularly in medical imaging where labeled anomalous data are scarce. While earlier approaches leverage generative models like autoencoders and generative adversarial networks (GANs), they often fall short due to overgeneralization. Recent methods explore various strategies, including memory banks, normalizing flows, self-supervised learning, and knowledge distillation, to enhance discrimination. Among these, knowledge distillation, particularly reverse distillation, has shown promise. Following this paradigm, we propose a novel scale-aware contrastive reverse distillation model that addresses two key limitations of existing reverse distillation methods: insufficient feature discriminability and inability to handle anomaly scale variations. Specifically, we introduce a contrastive student-teacher learning approach to derive more discriminative representations by generating and exploring out-of-normal distributions. Further, we design a scale adaptation mechanism to softly weight contrastive distillation losses at different scales to account for the scale variation issue. Extensive experiments on benchmark datasets demonstrate state-of-the-art performance, validating the efficacy of the proposed method. The code will be made publicly available.", "title_embedding_index": 1501, "title_abs_embedding_index": 1526}, {"title": "Local-Global Shortest Path Algorithms on Random Graphs, Enhanced with GNNs", "link_suffix": "/forum?id=YAf7MSsdTu", "link": "https://openreview.net/forum?id=YAf7MSsdTu", "pdf_link": "https://openreview.net/pdf?id=YAf7MSsdTu", "keywords": "shortest path, random graphs, graph neural networks, Erd\u0151s-R\u00e9nyi", "abstract": "Graph neural networks (GNNs) using local message passing were recently shown to inherit the intrinsic limitations of local algorithms in solving combinatorial graph optimization problems such as finding shortest distances (Loukas, 2020). To address this issue, Awasthi et al. (2022) proposed architectures based on Bourgain\u2019s (1985) seminal work on Hilbert space embeddings. These architectures enhance local message passing in GNNs with a single global computation, yielding a local-global algorithm. This paper focuses on the average-case analysis of more general local-global algorithms for finding shortest distances (of which GNN+ is a particular case). Our primary contribution is a theoretical analysis of these algorithms on Erd\u0151s-R\u00e9nyi (ER) random graphs. We prove that, on random graphs, these algorithms have lower distortion of shortest distances for most pairs of vertices w.h.p. while requiring a lower embedding dimension. Inspired by Awasthi et al. (2022), and to automate local computations and improve computational efficiency in practical scenarios, we further propose a modification to these algorithms that incorporates GNNs in the local computation phase. Empirical results on ER graphs and benchmark graph datasets demonstrate the enhanced performance of the GNN-augmented algorithm over the traditional approach.", "title_embedding_index": 1502, "title_abs_embedding_index": 1527}, {"title": "Scalable Influence and Fact Tracing for Large Language Model Pretraining", "link_suffix": "/forum?id=gLa96FlWwn", "link": "https://openreview.net/forum?id=gLa96FlWwn", "pdf_link": "https://openreview.net/pdf?id=gLa96FlWwn", "keywords": "training data attribution, LLM pretraining, influence functions, factual knowledge", "abstract": "Training data attribution (TDA) methods aim to attribute model outputs back to specific training examples, and the application of these methods to large language model (LLM) outputs could significantly advance model transparency and data curation. However, it has been challenging to date to apply these methods to the full scale of LLM pretraining. In this paper, we introduce a gradient-based method that works effectively at scale, allowing us to retrieve influential examples for an 8B-parameter language model from a pretraining corpus of over 160B tokens with no need for subsampling or pre-filtering. Our method combines several techniques, including optimizer state correction, a task-specific Hessian approximation, and normalized encodings, which we find to be critical for performance at scale. Our method performs best at identifying examples thatinfluencemodel predictions, but classical, model-agnostic retrieval methods such as BM25 still perform better at finding passages which explicitly contain relevant facts. These results demonstrate a misalignment between factualattributionand causalinfluence. With increasing model size and training tokens, we find that influence more closely aligns with attribution. Finally, we examine different types of examples identified as influential by our method, finding that while many directly entail a particular fact, others support the same output by reinforcing priors on relation types, common entities, and names.", "title_embedding_index": 1503, "title_abs_embedding_index": 1528}, {"title": "AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption", "link_suffix": "/forum?id=m73tETvFkX", "link": "https://openreview.net/forum?id=m73tETvFkX", "pdf_link": "https://openreview.net/pdf?id=m73tETvFkX", "keywords": "Adversarial Example, Adversarial Attack, Inpainting, Image Protection", "abstract": "The outstanding capability of diffusion models in generating high-quality images poses significant threats when misused by adversaries. In particular, we assume malicious adversaries exploiting diffusion models for inpainting tasks, such as replacing a specific region with a celebrity. While existing methods for protecting images from manipulation in diffusion-based generative models have primarily focused on image-to-image and text-to-image tasks, the challenge of preventing unauthorized inpainting has been rarely addressed, often resulting in suboptimal protection performance. To mitigate inpainting abuses, we propose ADVPAINT, a novel defensive framework that generates adversarial perturbations that effectively disrupt the adversary\u2019s inpainting tasks. ADVPAINT targets the self- and cross-attention blocks in a target diffusion inpainting model to distract semantic understanding and prompt interactions during image generation. ADVPAINT also employs a two-stage perturbation strategy, dividing the perturbation region based on an enlarged bounding box around the object, enhancing robustness across diverse masks of varying shapes and sizes. Our experimental results demonstrate that ADVPAINT\u2019s perturbations are highly effective in disrupting the adversary\u2019s inpainting tasks, outperforming existing methods; ADVPAINT attains over a 100-point increase in FID and substantial decreases in precision.", "title_embedding_index": 1504, "title_abs_embedding_index": 1529}, {"title": "Temporal-Aware Test-Time Training via Self-Distillation for One-Shot Image-to-Video Segmentation", "link_suffix": "/forum?id=BhECSDSkAE", "link": "https://openreview.net/forum?id=BhECSDSkAE", "pdf_link": "https://openreview.net/pdf?id=BhECSDSkAE", "keywords": "medical video analysis, one-shot video object segmentation, test-time training, self-distillation", "abstract": "This paper introduces a novel task and approach for one-shot medical video object segmentation using static image datasets. We address the critical challenge of limited annotated video data in medical imaging by proposing a framework that leverages readily available labeled static images to segment objects in medical videos with minimal annotation---specifically, a ground truth mask for only the first frame. Our method comprises training a one-shot segmentation model exclusively on images, followed by adapting it to medical videos through a test-time training strategy. This strategy incorporates a memory mechanism to utilize spatiotemporal context and employs self-distillation to maintain generalization capabilities. To facilitate research in this domain, we present OS-I2V-Seg, a comprehensive dataset comprising 28 categories in images and 4 categories in videos, totaling 68,416 image/frame-mask pairs. Extensive experiments demonstrate the efficacy of our approach in this extremely low-data regime for video object segmentation, establishing baseline performance on OS-I2V-Seg. The code and data will be made publicly available.", "title_embedding_index": 1505, "title_abs_embedding_index": 1530}, {"title": "Mastering Task Arithmetic:\u03c4Jp as a Key Indicator for Weight Disentanglement", "link_suffix": "/forum?id=1VwWi6zbxs", "link": "https://openreview.net/forum?id=1VwWi6zbxs", "pdf_link": "https://openreview.net/pdf?id=1VwWi6zbxs", "keywords": "task arithmetic, model editing, task vector", "abstract": "Model-editing techniques using task arithmetic have rapidly gained attention. Through task arithmetic, simply through arithmetic operations on the weights of pre-trained and fine-tuned models create desired models, such as multi-task models, models with specific tasks unsolvable, or domain-transferred models. However, task arithmetic faces challenges, such as low reproducibility and the high cost associated with adjusting coefficients in the arithmetic operations on model parameters, which have limited its practical success. In this paper, we present three key contributions in the context of task addition and task negation within task arithmetic. First, we propose a new metric called $\\tau$Jp which is based on the product of the task vector ($\\tau$) and the Jacobian of the pre-trained model with respect to its weights. We show that $\\tau$Jp has a causal relationship with the interference that occurs from arithmetic operations. Second, we show that introducing regularization to minimize $\\tau$Jp significantly mitigates interference between task inferences, which leads to eliminating coefficient tuning and better accuracy on each task. Third, in the context of incremental learning, we confirmed that our $\\tau$Jp regularization demonstrates more robust performance in environments where future tasks to be learned are not accessible, validating the scalability of the approach. Finally, we demonstrate that the $\\tau$Jp regularizer further reinforces the performance of task arithmetic by leveraging publicly available fine-tuned models, offering practical benefits for real-world applications.", "title_embedding_index": 1506, "title_abs_embedding_index": 1531}, {"title": "Generating CAD Code with Vision-Language Models for 3D Designs", "link_suffix": "/forum?id=BLWaTeucYX", "link": "https://openreview.net/forum?id=BLWaTeucYX", "pdf_link": "https://openreview.net/pdf?id=BLWaTeucYX", "keywords": "Code Generation, Self-refinement", "abstract": "Generative AI has transformed the fields of Design and Manufacturing by providing\nefficient and automated methods for generating and modifying 3D objects. One\napproach involves using Large Language Models (LLMs) to generate Computer-\nAided Design (CAD) scripting code, which can then be executed to render a 3D\nobject; however, the resulting 3D object may not meet the specified requirements.\nTesting the correctness of CAD generated code is challenging due to the complexity\nand structure of 3D objects (e.g., shapes, surfaces, and dimensions) that are not\nfeasible in code. In this paper, we introduce CADCodeVerify, a novel approach to\niteratively verify and improve 3D objects generated from CAD code. Our approach\nworks by producing ameliorative feedback by prompting a Vision-Language Model\n(VLM) to generate and answer a set of validation questions to verify the generated\nobject and prompt the VLM to correct deviations. To evaluate CADCodeVerify, we\nintroduce, CADPrompt, the first benchmark for CAD code generation, consisting of\n200 natural language prompts paired with expert-annotated scripting code for 3D\nobjects to benchmark progress. Our findings show that CADCodeVerify improves\nVLM performance by providing visual feedback, enhancing the structure of the 3D\nobjects, and increasing the success rate of the compiled program. When applied to\nGPT-4, CADCodeVerify achieved a 7.30% reduction in Point Cloud distance and a\n5.0% improvement in success rate compared to prior work.", "title_embedding_index": 1507, "title_abs_embedding_index": 1532}, {"title": "A Formal Framework for Understanding Length Generalization in Transformers", "link_suffix": "/forum?id=U49N5V51rU", "link": "https://openreview.net/forum?id=U49N5V51rU", "pdf_link": "https://openreview.net/pdf?id=U49N5V51rU", "keywords": "transformers, theory, length generalization, expressivity, analysis, algorithmic reasoning, systematic generalization, formal languages", "abstract": "A major challenge for transformers is generalizing to sequences longer than those observed during training. While previous works have empirically shown that transformers can either succeed or fail at length generalization depending on the task, theoretical understanding of this phenomenon remains limited. In this work, we introduce a rigorous theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings. In particular, we characterize those functions that are identifiable in the limit from sufficiently long inputs with absolute positional encodings under an idealized inference scheme using a norm-based regularizer. This enables us to prove the possibility of length generalization for a rich family of problems. We experimentally validate the theory as a predictor of success and failure of length generalization across a range of algorithmic and formal language tasks. Our theory not only explains a broad set of empirical observations but also opens the way to provably predicting length generalization capabilities in transformers.", "title_embedding_index": 1508, "title_abs_embedding_index": 1533}, {"title": "Instruction Following is not all you need: Rethinking LLM Generation's Evaluation", "link_suffix": "/forum?id=RuY1r1PDdQ", "link": "https://openreview.net/forum?id=RuY1r1PDdQ", "pdf_link": "https://openreview.net/pdf?id=RuY1r1PDdQ", "keywords": "LLM, evaluation, hallucination, query, decomposition, NLP, Machine Learning, Deep Learning, Benchmark", "abstract": "Current evaluation over large language model (LLM) generation is mostly focus-\ning on instruction following, which misses a critical aspect: even if a response is a\ninstruct-following generation does not guarantee its factual accuracy. This type of\nfollowing instruction but factually wrong hallucination phenomenon, as we called\nIntent Hallucination problem, remains under-explored for current LLM evalua-\ntion. To this end, we introduce FAITHQA, a novel benchmark for intent hallu-\ncination that contains 18,068 problems, covering both query-only and retrieval-\naugmented generation (RAG) setups with varying topics and difficulty. Further,\nwe propose that LLM\u2019s intent hallucination problem can manifest in two granu-\nlated ways: minor fabrication, where the response introduces sentence-level fac-\ntually incorrect information or major fabrication, where the paragraph level of the\nresponse is entirely factually inaccurate or fabricated. We further evaluate vari-\nous state-of-the-art LLMs on the proposed FAITHQA benchmark. Our analysis\non the results demonstrates that models exhibit varying degrees of omission and\nmisinterpretation, which leading to intent hallucination phenomenon. To facili-\ntate future research, we further introduce an automatic LLM evaluation method\nINTENT DECOMPOSE that (1) breaks the query into constraints, each assigned a\ndifferent importance label and (2) calculates an importance-weighted score based\non how well the response addresses the constraints. Our analysis shows that IN-\nTENT DECOMPOSE significantly outperforms the baseline.", "title_embedding_index": 1509, "title_abs_embedding_index": 1534}, {"title": "Testing the Limits of Jailbreaking with the Purple Problem", "link_suffix": "/forum?id=FD9sPyS8ve", "link": "https://openreview.net/forum?id=FD9sPyS8ve", "pdf_link": "https://openreview.net/pdf?id=FD9sPyS8ve", "keywords": "Jailbreaking, Adversarial Robustness, Security, Adaptive Attacks", "abstract": "The rise of ''jailbreak'' attacks on language models has led to a flurry of defenses aimed at preventing undesirable responses. Nonetheless, most benchmarks remain to be solved, not to mention real-world safety problems. We critically examine the two stages of the defense pipeline: (i) defining what constitutes unsafe outputs, and (ii) enforcing the definition via methods such as fine-tuning or input preprocessing. To understand whether we fail because of definition or enforcement, we consider a simple and well-specified definition of unsafe outputs---outputs that contain the word ''purple''. Surprisingly, all existing fine-tuning and input defenses fail to enforce this definition under adaptive attacks and increasing compute, casting doubt on whether enforcement algorithms can be robust for more complicated definitions. We hope that this definition serves as a testbed to evaluate enforcement algorithms and prevent a false sense of security.", "title_embedding_index": 1510, "title_abs_embedding_index": 1535}, {"title": "Measuring Language Model Uncertainty With Internal Concepts", "link_suffix": "/forum?id=tZk3LnvVtK", "link": "https://openreview.net/forum?id=tZk3LnvVtK", "pdf_link": "https://openreview.net/pdf?id=tZk3LnvVtK", "keywords": "uncertainty, correctness, large language models, concepts", "abstract": "We study the problem of evaluating the predictive uncertainty of large language models (LLMs). \nWe assign an uncertainty measure to the correctness of an LLM using a form of entropy that applies to semantic objects (concepts).\nUnlike prior works, the notion of meaning used to define concepts is derived from the LLM, rather than from\nan external model. \nOur notion of conceptual equivalence draws from ideas in Formal Concept Analysis (FCA) and lattice/order theory, and can be used to estimate correctness in closed- and open-ended scenarios.\nOur method has a relative improvement of up to 4.8% on average across five benchmarks and up to 10.9% on mixtures of closed- and open-ended questions.", "title_embedding_index": 1511, "title_abs_embedding_index": 1536}, {"title": "PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS", "link_suffix": "/forum?id=xzSUdw6s76", "link": "https://openreview.net/forum?id=xzSUdw6s76", "pdf_link": "https://openreview.net/pdf?id=xzSUdw6s76", "keywords": "Mobile Platforms, Large Language Models, Quantization, Benchmark", "abstract": "Deploying large language models (LLMs) locally on mobile devices is advantageous in scenarios where transmitting data to remote cloud servers is either undesirable due to privacy concerns or impractical due to network connection. Recent advancements have facilitated the local deployment of LLMs. However, local deployment also presents challenges, particularly in balancing quality (generative performance), latency, and throughput within the hardware constraints of mobile devices. In this paper, we introduce our lightweight, all-in-one automated benchmarking framework that allows users to evaluate LLMs on mobile devices. We provide a comprehensive benchmark of various popular LLMs with different quantization configurations (both weights and activations) across multiple mobile platforms with varying hardware capabilities. Unlike traditional benchmarks that assess full-scale models on high-end GPU clusters, we focus on evaluating resource efficiency (memory and power consumption) and harmful output for compressed models on mobile devices. Our key observations include: i) differences in energy efficiency and throughput across mobile platforms; ii) the impact of quantization on memory usage, GPU execution time, and power consumption; and iii) accuracy and performance degradation of quantized models compared to their non-quantized counterparts; and iv) the frequency of hallucinations and toxic content generated by compressed LLMs on\nmobile devices.", "title_embedding_index": 1512, "title_abs_embedding_index": 1537}, {"title": "On Stochastic Contextual Bandits with Knapsacks in Small Budget Regime", "link_suffix": "/forum?id=FCMpUOZkxi", "link": "https://openreview.net/forum?id=FCMpUOZkxi", "pdf_link": "https://openreview.net/pdf?id=FCMpUOZkxi", "keywords": "Contextual bandits with knapsacks, small budget", "abstract": "This paper studies stochastic contextual bandits with knapsack constraints (CBwK), where a learner observes a context, takes an action, receives a reward, and incurs a vector of costs at every round. The learner aims to maximize the cumulative rewards across $T$ rounds under the knapsack constraints with an initial budget of $B$. We study CBwK in the small budget regime where the budget $B = \\Omega(\\sqrt{T})$\nand propose an Adaptive and Universal Primal--Dual algorithm (AUPD) that achieves strong regret performance: \ni) AUPD achieves $\\tilde{O}((1 + \\frac{\\nu^*}{\\delta b})\\sqrt{T})$ regret under the strict feasibility assumption without any prior information, matching the best-known bounds;\nii) AUPD achieves $\\tilde{O}(\\sqrt{T}+ \\frac{\\nu^*}{\\sqrt{b}}T^{\\frac{3}{4}})$ regret without strict feasibility assumption, \nwhich, to the best of our knowledge, is the first result in the literature. Here, the parameter $\\nu^*$ represents the optimal average reward; $b=B/T$ is the average budget and $\\delta b$ is the feasibility/safety margin.\nWe establish these strong results through the adaptive budget-aware design, which effectively balances reward maximization and budget consumption. We provide a new perspective on analyzing budget consumption using the Lyapunov drift method, along with a refined analysis of its cumulative variance. Our theory is further supported by experiments conducted on a large-scale dataset.", "title_embedding_index": 1513, "title_abs_embedding_index": 1538}, {"title": "Fisher Contrastive Learning: A Robust Solution to the Feature Suppression Effect", "link_suffix": "/forum?id=Tl6hStJNYX", "link": "https://openreview.net/forum?id=Tl6hStJNYX", "pdf_link": "https://openreview.net/pdf?id=Tl6hStJNYX", "keywords": "self-supervised contrastive learning, sufficient dimension reduction, Fisher discriminant analysis", "abstract": "Self-supervised contrastive learning (SSCL) is a rapidly advancing approach for learning data representations. However, a significant challenge in this paradigm is the feature suppression effect, where useful features for downstream tasks are suppressed due to dominant or easy-to-learn features overshadowing others crucial for downstream performance, ultimately degrading the performance of SSCL models. While prior research has acknowledged the feature suppression effect, solutions with theoretical guarantees to mitigate this issue are still lacking. In this work, we address the feature suppression problem by proposing a novel method, Fisher Contrastive Learning, which unbiasedly and exhaustively estimates the central sufficient dimension reduction function class in SSCL settings. In addition, FCL empirically maintains the embedding dimensionality by maximizing the discriminative power of each linear classifier learned through Fisher Contrastive Learning. We demonstrate that using our proposed method, the class-relevant features are not suppressed by strong or easy-to-learn features on datasets known for strong feature suppression effects. In addition, the embedding dimensionality is not preserved in practice. Furthermore, we show that Fisher Contrastive Learning consistently outperforms existing benchmark methods on standard image benchmarks, illustrating its practical advantages.", "title_embedding_index": 1514, "title_abs_embedding_index": 1539}, {"title": "Unsupervised Federated Learning for Privacy Preserving in Face Recognition System", "link_suffix": "/forum?id=XH3OiIhtvf", "link": "https://openreview.net/forum?id=XH3OiIhtvf", "pdf_link": "https://openreview.net/pdf?id=XH3OiIhtvf", "keywords": "Unsupervised Federated Learning for Face Recognition in Decentralized Environments", "abstract": "Recent advancements in face recognition involve training on a single computer, often containing sensitive personal information, raising privacy concerns. To address this, attention turns to federated learning for unsupervised face recognition, leveraging decentralized edge devices. Each device independently undergoes model training, transmitting results to a secure aggregator. We utilize GANs to diversify data without the need for transmission, thereby preserving privacy throughout the entire process. The aggregator integrates these diverse models into a single global model, which is then transmitted back to the edge devices for continued improvement. Experiments on CelebA datasets demonstrate that federated learning not only preserves privacy but also maintains high levels of performance.", "title_embedding_index": 1515, "title_abs_embedding_index": 1540}, {"title": "World-Model based Hierarchical Planning with Semantic Communications for Autonomous Driving", "link_suffix": "/forum?id=HyS9pkHNTN", "link": "https://openreview.net/forum?id=HyS9pkHNTN", "pdf_link": "https://openreview.net/pdf?id=HyS9pkHNTN", "keywords": "World Model, Hierarchical Planning, Reinforcement Learning, Autonomous Driving, Communications", "abstract": "World-model (WM) is a highly promising approach for training AI agents. However, in complex learning systems such as autonomous driving, AI agents interact with others in a dynamic environment and face significant challenges such as partial observability and non-stationarity. Inspired by how humans naturally solve complex tasks hierarchically and how drivers share their intentions by using turn signals, we introduce HANSOME, a WM-based hierarchical planning with semantic communications framework. In HANSOME, semantic information, particularly text and compressed visual data, is generated and shared to improve two-level planning. HANSOME incorporates two important designs: 1) A hierarchical planning strategy, where the higher-level policy generates intentions with text semantics, and a semantic alignment technique ensures the lower-level policy determines specific controls to achieve these intentions. 2) A cross-modal encoder-decoder to fuse and utilize the shared semantic information to enhance planning through multi-modal understanding. A key advantage of HANSOME is that the generated intentions not only enhance the lower-level policy but also can be shared and understood by humans or other AVs to improve their planning. Furthermore, we devise AdaSMO, an entropy-controlled adaptive scalarization method, to tackle the multi-objective optimization problem in hierarchical policy learning. Extensive experiments show that HANSOME outperforms state-of-the-art WM-based methods in challenging driving tasks, enhancing overall traffic safety and efficiency.", "title_embedding_index": 1516, "title_abs_embedding_index": 1541}, {"title": "From Few to Many: Enhancing Many-Shot In-Context Learning with Optimized Example Selection and Expansion", "link_suffix": "/forum?id=JBXO05r4AV", "link": "https://openreview.net/forum?id=JBXO05r4AV", "pdf_link": "https://openreview.net/pdf?id=JBXO05r4AV", "keywords": "many-shot, in-context learning, large language models", "abstract": "Recent advances in long-context large language models (LLMs) have led to the emerging paradigm of many-shot in-context learning (ICL), where it is observed that scaling many more demonstrating examples beyond the conventional few-shot setup in the context can lead to performance benefits. However, despite its promise, it is unclear what aspects dominate the benefits and whether simply scaling to more examples is the most effective way of improving many-shot ICL. In this work, we first provide an analysis on the factors driving many-shot ICL, and we find that 1) many-shot performance can still be attributed to often a few disproportionately influential examples and 2) identifying such influential examples (\"optimize\") and using them as demonstrations to regenerate new examples (\"generate\") can lead to further improvements. Inspired by the findings, we propose BRIDGE, an algorithm that alternates between the \\textit{optimize} step with Bayesian optimization to discover the influential sets of examples and the \\textit{generate} step to reuse this set to expand the reasoning paths of the examples back to the many-shot regime automatically. On two state-of-the-art long-context Gemini models of different sizes, we show \\ours led to significant improvements across a diverse set of tasks including symbolic reasoning, numerical reasoning and code generation.", "title_embedding_index": 1517, "title_abs_embedding_index": 1542}, {"title": "Can Transformers Perform PCA ?", "link_suffix": "/forum?id=mjDNVksC5G", "link": "https://openreview.net/forum?id=mjDNVksC5G", "pdf_link": "https://openreview.net/pdf?id=mjDNVksC5G", "keywords": "Principle Component Analysis, Transformers, Machine Learning Theory", "abstract": "Transformers demonstrate significant advantage as the building block of Large Language Models. Recent efforts are devoted to understanding the learning capacities of transformers at a fundamental level. This work attempts to understand the intrinsic capacity of transformers in performing dimension reduction from complex data. Theoretically, our results rigorously show that transformers can perform Principle Component Analysis (PCA) similar to the Power Method, given a supervised pre-training phase. Moreover, we show the generalization error of transformers decays by $n^{-1/5}$ in $L_2$. Empirically, our extensive experiments on the simulated and real world high dimensional datasets justify that a pre-trained transformer can successfully perform PCA by simultaneously estimating the first $k$ eigenvectors and eigenvalues. These findings demonstrate that transformers can efficiently extract low dimensional patterns from high dimensional data, shedding light on the potential benefits of using pre-trained LLM to perform inference on high dimensional data.", "title_embedding_index": 1518, "title_abs_embedding_index": 1543}, {"title": "Pseudo-Probability Unlearning: Towards Efficient and Privacy-Preserving Machine Unlearning", "link_suffix": "/forum?id=Xagys9QD3T", "link": "https://openreview.net/forum?id=Xagys9QD3T", "pdf_link": "https://openreview.net/pdf?id=Xagys9QD3T", "keywords": "machine unlearning, deep learning", "abstract": "Machine unlearning\u2014enabling a trained model to forget specific data\u2014is crucial for addressing biased data and adhering to privacy regulations like the General Data Protection Regulation (GDPR)'s ``right to be forgotten.\" Recent works have paid little attention to privacy concerns, leaving the data intended for forgetting vulnerable to membership inference attacks. Moreover, they often come with high computational overhead. In this work, we propose Pseudo-Probability Unlearning (PPU), a novel method that enables models to forget data efficiently and in a privacy-preserving manner. Our method replaces the final-layer output probabilities of the neural network with pseudo-probabilities for the data to be forgotten. These pseudo-probabilities follow either a uniform distribution or align with the model\u2019s overall distribution, enhancing privacy and reducing risk of membership inference attacks. Our optimization strategy further refines the predictive probability distributions and updates the model's weights accordingly, ensuring effective forgetting with minimal impact on the model's overall performance. Through comprehensive experiments on multiple benchmarks, our method achieves over 20% improvements in forgetting error compared to the state-of-the-art. Additionally, our method enhances privacy by preventing the forgotten set from being inferred to around random guesses.", "title_embedding_index": 1519, "title_abs_embedding_index": 1544}, {"title": "Jigsaw Puzzles: Splitting Harmful Questions to Jailbreak Large Language Models", "link_suffix": "/forum?id=ov678VcvlO", "link": "https://openreview.net/forum?id=ov678VcvlO", "pdf_link": "https://openreview.net/pdf?id=ov678VcvlO", "keywords": "Jailbreak, Red Teaming, LLMs, Safety, Multi-turn Interaction", "abstract": "Large language models (LLMs) have exhibited outstanding performance in engaging with humans and addressing complex questions by leveraging their vast implicit knowledge and robust reasoning capabilities. However, such models are vulnerable to jailbreak attacks, leading to the generation of harmful responses. Despite recent research on single-turn jailbreak strategies to facilitate the development of defence mechanisms, the challenge of revealing vulnerabilities under multi-turn setting remains relatively under-explored. In this work, we propose Jigsaw Puzzles (JSP), a straightforward yet effective multi-turn jailbreak strategy against the advanced LLMs. JSP splits questions into harmless fractions as the input of each turn, and requests LLMs to reconstruct and respond to questions under multi-turn interaction. Our experimental results demonstrate that the proposed JSP jailbreak bypasses original safeguards against explicitly harmful content, achieving an average attack success rate of 93.76% on 189 harmful queries across 5 advanced LLMs (Gemini-1.5-Pro, Llama-3.1-70B, GPT-4, GPT-4o, GPT-4o-mini). Moreover, JSP achieves a state-of-the-art attack success rate of 92% on GPT-4 on the harmful query benchmark, and exhibits strong resistant to defence strategies. Warning: this paper contains offensive examples.", "title_embedding_index": 1520, "title_abs_embedding_index": 1545}, {"title": "Feynman: Knowledge-Infused Diagramming Agent for Scaling Visual Reasoning Data", "link_suffix": "/forum?id=jNmsuEE4Gf", "link": "https://openreview.net/forum?id=jNmsuEE4Gf", "pdf_link": "https://openreview.net/pdf?id=jNmsuEE4Gf", "keywords": "Vision-Language Dataset, Synthetic Data, Visual Reasoning Benchmark", "abstract": "Visual reasoning is an essential ability of state-of-the-art multi-modal AI systems. Improving these systems requires high-quality vision-language data at scale. Despite the abundance of internet image and text data, knowledge-rich and well-aligned image-text pairs are rare. In this paper, we present a scalable data generation pipeline built with our diagramming agent,Feynman. To create diagrams, Feynman first enumerates domain-specific knowledge components (\"ideas\") and performs code planning based on the ideas. Given the plan, Feynman translates ideas into simple declarative programs and iterates to receives feedback and visually refine diagrams. Finally, the declarative programs are rendered by the Penrose diagramming system. The optimization-based rendering of Penrose preserves the visual semantics while injecting fresh randomness into the layout, thereby producing diagrams with visual consistency and diversity. As a result, Feynman can author diagrams along with grounded captions with very little cost and time. Using Feynman, we synthesized a dataset with more than 100$k$ well-aligned diagram-caption pairs. We also curate a visual-language benchmark,Diagramma, from freshly generated data.", "title_embedding_index": 1521, "title_abs_embedding_index": 1546}, {"title": "Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees", "link_suffix": "/forum?id=ed7zI29lRF", "link": "https://openreview.net/forum?id=ed7zI29lRF", "pdf_link": "https://openreview.net/pdf?id=ed7zI29lRF", "keywords": "pac learning, distribution shift, distribution testing, testable learning, neural networks, kernel methods", "abstract": "We give the first provably efficient algorithms for learning neural networks with respect to distribution shift. We work in the Testable Learning with Distribution Shift  framework (TDS learning) of Klivans et al. (2024), where the learner receives labeled examples from a training distribution and unlabeled examples from a test distribution and must either output a hypothesis with low test error or reject if distribution shift is detected.  No assumptions are made on the test distribution.All prior work in TDS learning focuses on classification, while here we must handle the setting of nonconvex regression. Our results apply to real-valued networks with arbitrary Lipschitz activations and work whenever the training distribution has strictly sub-exponential tails. For training distributions that are bounded and hypercontractive, we give a fully polynomial-time algorithm for TDS learning one hidden-layer networks with sigmoid activations. We achieve this by importing classical kernel methods into the TDS framework using data-dependent feature maps and a type of kernel matrix that couples samples from both train and test distributions.", "title_embedding_index": 1522, "title_abs_embedding_index": 1547}, {"title": "MuJoCo Manipulus: A Robot Learning Benchmark for Generalizable Tool Manipulation", "link_suffix": "/forum?id=b9Ne5lHJ8Y", "link": "https://openreview.net/forum?id=b9Ne5lHJ8Y", "pdf_link": "https://openreview.net/pdf?id=b9Ne5lHJ8Y", "keywords": "robotics, deep reinforcement learning, benchmark", "abstract": "We propose MuJoCo Manipulus, a novel open-source benchmark powered by the MuJoCo physics simulation engine, designed to accelerate advances in robot learning for manipulation. Our benchmark includes a set of tasks for tool manipulation---a domain where the field currently lacks a unified benchmark. Different research groups rely on custom-designed tasks or closed-source setups, limiting cross-comparability and hindering significant progress in this field. To that end, our benchmark provides a diverse set of challenging tool manipulation tasks, including variants of Pouring, Stacking, Scooping, Gathering, Hammering, and Scraping. The benchmark supports both state-based and vision-based observation spaces, is fully integrated with the Gymnasium API, and seamlessly connects with widely used Deep Reinforcement Learning libraries, ensuring easy adoption by the community. We conduct extensive reinforcement learning experiments on our benchmark, and our results demonstrate that there is substantial progress to be made for training tool manipulation policies. Our codebase and additional videos of the learned policies can be found on our anonymous project website: mujoco-manipulus.github.io.", "title_embedding_index": 1523, "title_abs_embedding_index": 1548}, {"title": "Forgetting Transformer: Softmax Attention with a Forget Gate", "link_suffix": "/forum?id=q2Lnyegkr8", "link": "https://openreview.net/forum?id=q2Lnyegkr8", "pdf_link": "https://openreview.net/pdf?id=q2Lnyegkr8", "keywords": "sequence model, long-context sequence modeling, Transformer, softmax attention, linear attention, RNN, language model", "abstract": "An essential component of modern recurrent sequence models is the forget gate. While Transformers do not have an explicit recurrent form, we show that a forget gate can be naturally incorporated into Transformers by down-weighting the unnormalized attention scores in a data-dependent way. We name the resulting model the Forgetting Transformer.  We show that the Forgetting Transformer significantly outperforms the standard Transformer on long-context language modeling and downstream tasks. Moreover, the Forgetting Transformer does not require any position embeddings and generalizes beyond the training context length. Several analyses, including the needle-in-the-haystack experiment, show that the Forgetting Transformer also retains the standard Transformer's superior long-context capabilities over recurrent sequence models such as Mamba-2, HGRN2, and DeltaNet.", "title_embedding_index": 1524, "title_abs_embedding_index": 1549}]