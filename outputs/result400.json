[{"title": "MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents", "link_suffix": "/forum?id=FGSgsefE0Y", "link": "https://openreview.net/forum?id=FGSgsefE0Y", "pdf_link": "https://openreview.net/pdf?id=FGSgsefE0Y", "keywords": "Multimodal Role-Playing Agents, Large Multimodal Models", "abstract": "Recently, Role-Playing Agents (RPAs) have garnered increasing attention for their potential to deliver emotional value and facilitate sociological research.\nHowever, existing studies are primarily confined to the textual modality, unable to simulate humans' multimodal perceptual capabilities.\nTo bridge this gap, we introduce the concept of Multimodal Role-Playing Agents (MRPAs), and propose a comprehensive framework, MMRole, for their development and evaluation, which comprises a personalized multimodal dataset and a robust evaluation method.\nSpecifically, we construct a large-scale, high-quality dataset, MMRole-Data, consisting of 85 characters, 11K images, and 14K single or multi-turn dialogues.\nAdditionally, we present a robust evaluation method, MMRole-Eval, encompassing eight metrics across three dimensions,\nwhere a reward model is trained to score MRPAs with the constructed ground-truth data for comparison.\nMoreover, we develop the first specialized MRPA, MMRole-Agent.\nExtensive evaluation results demonstrate the improved performance of MMRole-Agent and highlight the primary challenges in developing MRPAs, emphasizing the need for enhanced multimodal understanding and role-playing consistency.\nThe data, code, and models will all be available.", "title_embedding_index": 19950, "title_abs_embedding_index": 19975}, {"title": "I-Max: Maximize the Resolution Potential of Pre-trained Rectified Flow Transformers with Projected Flow", "link_suffix": "/forum?id=w2BELPYbU0", "link": "https://openreview.net/forum?id=w2BELPYbU0", "pdf_link": "https://openreview.net/pdf?id=w2BELPYbU0", "keywords": "Diffusion Model, Generative Model, Image Generation, High-resolution", "abstract": "Rectified Flow Transformers (RFTs) offer superior training and inference efficiency, making them likely the most viable direction for scaling up diffusion models. However, progress in generation resolution has been relatively slow due to data quality and training costs. Tuning-free resolution extrapolation presents an alternative, but current methods often reduce generative stability, limiting practical application. In this paper, we review existing resolution extrapolation methods and introduce the I-Max framework to maximize the resolution potential of Text-to-Image RFTs. I-Max features: (i) a novel Projected Flow strategy for stable extrapolation and (ii) an advanced inference toolkit for generalizing model knowledge to higher resolutions. Experiments with Lumina-Next-2K and Flux.1-dev demonstrate I-Max's ability to enhance stability in resolution extrapolation and show that it can bring image detail emergence and artifact correction, confirming the practical value of tuning-free resolution extrapolation.", "title_embedding_index": 19951, "title_abs_embedding_index": 19976}, {"title": "Measuring Bias of Web-filtered Text Datasets and Bias Propagation Through Training", "link_suffix": "/forum?id=FDhAngvHuf", "link": "https://openreview.net/forum?id=FDhAngvHuf", "pdf_link": "https://openreview.net/pdf?id=FDhAngvHuf", "keywords": "LLMs, text datasets, classification, bias, rewrite, propagation", "abstract": "In this paper, we investigate biases in pretraining datasets for large language models (LLMs) through dataset classification experiments. Building on prior work demonstrating the existence of biases in popular computer vision datasets, we analyze popular open-source pretraining text datasets derived from CommonCrawl including C4, RefinedWeb, DolmaCC, RedPajama-V2, FineWeb, DCLM-Baseline, and others. Despite those datasets being obtained with similar filtering and deduplication steps, LLMs can classify surprisingly well which dataset a single text sequence belongs to, significantly better than a human can. This indicates that popular pretraining datasets have their own unique biases or fingerprints. Those biases remain even when the text is rewritten with LLMs. We also demonstrate that these biases propagate through training: Random sequences generated by models trained on those datasets can be classified well by a classifier trained on the original datasets.", "title_embedding_index": 19952, "title_abs_embedding_index": 19977}, {"title": "AutoLoRA: AutoGuidance Meets Low-Rank Adaptation for Diffusion Models", "link_suffix": "/forum?id=afgqQYxTyR", "link": "https://openreview.net/forum?id=afgqQYxTyR", "pdf_link": "https://openreview.net/pdf?id=afgqQYxTyR", "keywords": "diffusion models, guidance, generative model, LoRA", "abstract": "Low-rank adaptation (LoRA) is a fine-tuning technique that can be applied to conditional generative diffusion models. LoRA utilizes a small number of context examples to adapt the model to a specific domain, character, style, or concept. However, due to the limited data utilized during training, the fine-tuned model performance is often characterized by strong context bias and a low degree of variability in the generated images. To solve this issue, we introduce AutoLoRA, a novel guidance technique for diffusion models fine-tuned with the LoRA approach. Inspired by other guidance techniques, AutoLoRA searches for a trade-off between consistency in the domain represented by LoRA weights and sample diversity from the base conditional diffusion model. Moreover, we show that incorporating classifier-free guidance for both LoRA fine-tuned and base models leads to generating samples with higher diversity and better quality. The experimental results for several fine-tuned LoRA domains show superiority over existing guidance techniques on selected metrics.", "title_embedding_index": 19953, "title_abs_embedding_index": 19978}, {"title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Data", "link_suffix": "/forum?id=CEE9cAQJ10", "link": "https://openreview.net/forum?id=CEE9cAQJ10", "pdf_link": "https://openreview.net/pdf?id=CEE9cAQJ10", "keywords": "Large Language Models, Mathematical Reasoning, Data Synthesis", "abstract": "Synthesizing high-quality data for continual training has been proven to be effective for boosting the performance of Large Language Models (LLMs). However, previous approaches struggle to easily scale up synthetic data and incur high costs in creating high-quality data. In this paper, we propose the Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable framework for high-quality data synthesis. Inspired by knowledge graphs, we extracted knowledge points from seed data and constructed a knowledge points relationships graph to explore their interconnections. By exploring the implicit relationships among knowledge, our method achieves $\\times$255 data expansion. Furthermore, GSDP led by open-source models, achieves synthesis quality comparable to GPT-4-0613 while maintaining $\\times$100 lower costs. To tackle the most challenging mathematical reasoning task, we present the GSDP-MATH dataset comprising over 1.91 million pairs of math problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating the effectiveness of our method. The dataset and models trained in this paper will be available.", "title_embedding_index": 19954, "title_abs_embedding_index": 19979}, {"title": "UTSD: Unified Time Series Diffusion Model", "link_suffix": "/forum?id=aCz7TiKjwJ", "link": "https://openreview.net/forum?id=aCz7TiKjwJ", "pdf_link": "https://openreview.net/pdf?id=aCz7TiKjwJ", "keywords": "time series anlysis, foundation model, cross-doman, diffusition model", "abstract": "Transformer-based architectures have achieved unprecedented success in time series analysis. However, facing the challenge of across-domain modeling, existing studies utilize statistical prior as prompt engineering fails under the huge distribution shift among various domains. In this paper, a Unified Time Series Diffusion (UTSD) model is established for the first time to model the multi-domain probability distribution, utilizing the powerful probability distribution modeling ability of Diffusion. Unlike the autoregressive models that capture the conditional probabilities of the prediction horizon to the historical sequence, we use a diffusion denoising process to model the mixture distribution of the cross-domain data and generate the prediction sequence for the target domain directly utilizing conditional sampling. The proposed UTSD contains three pivotal designs: (1) The condition network captures the multi-scale fluctuation patterns from the observation sequence, which are utilized as context representations to guide the denoising network to generate the prediction sequence; (2) Adaptor-based fine-tuning strategy, the multi-domain universal representation learned in the pretraining stage is utilized for downstream tasks in target domains; (3) The diffusion and denoising process on the actual sequence space, combined with the improved classifier free guidance as the conditional generation strategy, greatly improves the stability and accuracy of the downstream task. We conduct extensive experiments on mainstream benchmarks, and the pre-trained UTSD outperforms existing foundation models on all data domains, exhibiting superior zero-shot generalization ability. After training from scratch, UTSD achieves comparable performance against domain-specific proprietary models. In particular, UTSD shows stable and reliable time series generation, and the empirical results validate the potential of UTSD as a time series foundational model. The source codes of UTSD are publicly available onhttps://anonymous.4open.science/r/UTSD-1BFF.", "title_embedding_index": 19955, "title_abs_embedding_index": 19980}, {"title": "PFAvatar: Avatar Reconstruction from Multiple In-the-wild Images", "link_suffix": "/forum?id=KaV6jGCxvs", "link": "https://openreview.net/forum?id=KaV6jGCxvs", "pdf_link": "https://openreview.net/pdf?id=KaV6jGCxvs", "keywords": "Avatar Reconstruction, Text-to-Image Diffusion Model, Image-based Modeling, Text-guided 3D Generation, Digital Human;", "abstract": "In this work, we present \\textit{PFAvatar}, a new approach to avatar reconstruction and editing from multiple in-the-wild images with varying poses, unknown camera conditions, cropped views, and occlusions. Traditional methods often rely on full-body images captured with controlled avatar pose, camera settings, lighting, and background, while struggling to reconstruct under in-the-wild settings.To address this issue, we fuse the varying pose priors of avatars in in-the-wild images, thereby enabling precise control over avatar generation.Specifically, we first inject avatar features (pose, appearance) from input images using a Vision-Language Model (VLM) and ControlNet. Subsequently, we employ a pose-conditioned 3D-Consistent Score Distillation Sampling (3D-SDS), which enables reconstructing a high-quality 3D avatar. Additionally, we propose a Condition Prior Preservation Loss (CPPL) to mitigate the issues of language and control drift caused by fine-tuning VLM and ControlNet with few-shot data. Through comprehensive experiments and evaluation, we demonstrate the effectiveness of our method for reconstructing avatars from in-the-wild images, supporting further applications like avatar editing.", "title_embedding_index": 19956, "title_abs_embedding_index": 19981}, {"title": "Learning Towards Emergence: Paving the Way to Induce Emergence by Inhibiting Monosemantic Neurons on Pre-trained Models", "link_suffix": "/forum?id=exfy4e7OJq", "link": "https://openreview.net/forum?id=exfy4e7OJq", "pdf_link": "https://openreview.net/pdf?id=exfy4e7OJq", "keywords": "Deep Learning, Emergent Abilities, Monosemanticity, Large Language Model", "abstract": "Emergence, the phenomenon of a rapid performance increase once the model scale reaches a threshold, has achieved widespread attention recently. The literature has observed that monosemantic neurons in neural networks gradually diminish as the model scale increases. Subsequently,Learning From Emergenceis proposed to actively inhibit monosemantic neurons in relatively small neural networks (e.g., BERT and Swin-Transformer) for promoting model performance with fine-tuning. However, to ultimately achieve emergence, it is demanding to support the monosemantic neuron inhibition in the pretraining phase of large-scale models. Thus, this work further pushes the boundary of this research direction to beLearning Towards Emergence (L2E)and enables the training and validating of the impact of inhibiting monosemantic neurons on larger pre-trained neural networks (e.g., Pythia-70M, 410M, and 2.8B). More specifically, to bridge the gap in current research, we first conduct experiments on models of various scales (up to 6.9B) to validate the monosemantic ideas. Then, we present a novel method L2E to address the inefficient monosemantic neuron retrieval and ineffective monosemantic neuron inhibition when existing methods are applied in the pretraining phase of large-scale models. It employs an adjustable thresholding technique for efficient neuron retrieval, incorporates a False Killing Rate metric to assess inhibition effects, and proposes a regularization-style inhibition approach, which addresses the limitations of previous approaches in both efficiency and effectiveness.", "title_embedding_index": 19957, "title_abs_embedding_index": 19982}, {"title": "Tradiffusion++\uff1aHierarchical Guidance for Fine-Grained Trajectory-Based  Image Generation", "link_suffix": "/forum?id=xTsvE8gOPT", "link": "https://openreview.net/forum?id=xTsvE8gOPT", "pdf_link": "https://openreview.net/pdf?id=xTsvE8gOPT", "keywords": "Diffusion models; Trajectory control; TraDiffusion++; Training-free methods; Controllable generation; Stable Diffusion (SD); Fine-Grained Control", "abstract": "Currently, many training-free methods based on diffusion models allow controllable generation. These methods, such as TraDiffusion, introduce control through additional trajectory input. While they are more user-friendly than traditional methods, they offer only coarse control over the Stable Diffusion (SD) model. We observe that SD focuses more on layout control at lower resolutions of cross-attention and shape control at higher ones. Based on this, we propose TraDiffusion++, which introduces a Hierarchical Guidance Mechanism (HGM) for finer-grained control in generation. HGM includes three key components: Control Loss (CL), Suppress Loss (SL), and Fix Loss (FL). CL aligns the layout with the trajectory across layers. SL suppresses objects outside the trajectory at lower resolutions. FL refines regions not fully controlled by the trajectory using attention feedback at middle and high resolutions. The combination of CL and SL ensures effective layout control. The interaction between CL and FL improves shape generation. We build a dataset with simple and complex trajectories. Experiments show that TraDiffusion++ achieves stable layout control and fine-grained object generation. This also reveals new insights into SD\u2019s control mechanisms.", "title_embedding_index": 19958, "title_abs_embedding_index": 19983}, {"title": "VidEgoThink: Assessing Egocentric Video Understanding Capabilities for Embodied AI", "link_suffix": "/forum?id=Z5nqeTH24j", "link": "https://openreview.net/forum?id=Z5nqeTH24j", "pdf_link": "https://openreview.net/pdf?id=Z5nqeTH24j", "keywords": "Multi-modal Large Language Models, Egocentric Video Understanding, Embodied AI, Benchmark", "abstract": "Recent advancements in Multi-modal Large Language Models (MLLMs) have opened new avenues for applications in Embodied AI.\nBuilding on previous work, EgoThink, we introduce VidEgoThink, a comprehensive benchmark for evaluating egocentric video understanding capabilities. To bridge the gap between MLLMs and low-level control in Embodied AI, we design four key interrelated tasks: video question-answering, hierarchy planning, visual grounding and reward modeling. To minimize manual annotation costs, we develop an automatic data generation pipeline based on the Ego4D dataset, leveraging the prior knowledge and multimodal capabilities of GPT-4o. Three human annotators then filter the generated data to ensure diversity and quality, resulting in the VidEgoThink benchmark. We conduct extensive experiments with three types of models: API-based MLLMs, open-source image-based MLLMs, and open-source video-based MLLMs. Experimental results indicate that all MLLMs, including GPT-4o, perform poorly across all tasks related to egocentric video understanding. These findings suggest that foundation models still require significant advancements to be effectively applied to first-person scenarios in Embodied AI. In conclusion, VidEgoThink reflects a research trend towards employing MLLMs for egocentric vision, akin to human capabilities, enabling active observation and interaction in the complex real-world environments.", "title_embedding_index": 19959, "title_abs_embedding_index": 19984}, {"title": "S7: Selective and Simplified State Space Layers for Sequence Modeling", "link_suffix": "/forum?id=4wtcXV0kbi", "link": "https://openreview.net/forum?id=4wtcXV0kbi", "pdf_link": "https://openreview.net/pdf?id=4wtcXV0kbi", "keywords": "state space models, neural network architectures, deep learning architectures, sequence modeling, event-based vision, event cameras, neural odes", "abstract": "A central challenge in sequence modeling is efficiently handling tasks with extended contexts. While recent state-space models (SSMs) have made significant progress in this area, they often lack input-dependent filtering or require substantial increases in model complexity to handle input variability. We address this gap by introducing S7, a simplified yet powerful SSM that can handle input dependence while incorporating stable reparameterization and specific design choices to dynamically adjust state transitions based on input content, maintaining efficiency and performance. We prove that this reparameterization ensures stability in long-sequence modeling by keeping state transitions well-behaved over time. Additionally, it controls the gradient norm, enabling efficient training and preventing issues like exploding or vanishing gradients. S7 significantly outperforms baselines across various sequence modeling tasks, including neuromorphic event-based datasets, Long Range Arena benchmarks, and various physical and biological time series. Overall, S7 offers a more straightforward approach to sequence modeling without relying on complex, domain-specific inductive biases, achieving significant improvements across key benchmarks.", "title_embedding_index": 19960, "title_abs_embedding_index": 19985}, {"title": "Smart Placement Enhanced Vision: Enhancing 3D-Detection With Learned 3D Placement", "link_suffix": "/forum?id=UzgMX1rwGc", "link": "https://openreview.net/forum?id=UzgMX1rwGc", "pdf_link": "https://openreview.net/pdf?id=UzgMX1rwGc", "keywords": "Synthetic data; 3D object detection; Data augmentation", "abstract": "The diversity and scale of annotated real-world 3D datasets limit the performance of monocular 3D detectors. Although data augmentation holds potential, creating realistic, scene-aware augmentations for outdoor environments presents a significant challenge.\nExisting augmentation methods majorly focus on realistic object appearance by advancing the rendering quality. However, we show that object placement is equally important for downstream 3D detection performance. The main challenge, however, for realistic placement, is to automatically identify the plausible physical properties (location, scale, and orientation) for placing objects in real-world scenes. To this end, we propose Smart-Placement, a novel 3D scene-aware augmentation method for generating diverse and realistic augmentations. In particular, given a background scene, we train a placement network to learn a distribution over plausible 3D bounding boxes. Subsequently, we render realistic cars from 3D assets and place them according to the locations sampled from the learned distribution. Through extensive empirical evaluation on standard benchmark datasets - KITTI and NuScenes, we show that our proposed augmentation method significantly boosts the performance of several existing monocular 3D detectors, setting a new state-of-the-art benchmark, while being highly data efficient.", "title_embedding_index": 19961, "title_abs_embedding_index": 19986}, {"title": "PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation", "link_suffix": "/forum?id=996aKQIom0", "link": "https://openreview.net/forum?id=996aKQIom0", "pdf_link": "https://openreview.net/pdf?id=996aKQIom0", "keywords": "LLM, language models, role-play, benchmark, language model evaluation, role-playing benchmark, multi-turn conversations, user emulation, automated assessment, character consistency, entertainment value, language fluency, multi-model evaluation, dynamic test generation", "abstract": "We introduce a novel benchmark for evaluating the role-playing capabilities of language models. Our approach leverages language models themselves to emulate users in dynamic, multi-turn conversations and to assess the resulting dialogues. The framework consists of three main components: a player model assuming a specific character role, an interrogator model simulating user behavior, and a judge model evaluating conversation quality. We conducted experiments comparing automated evaluations with human annotations to validate our approach, demonstrating strong correlations across multiple criteria. This work provides a foundation for a robust and dynamic evaluation of model capabilities in interactive scenarios.", "title_embedding_index": 19962, "title_abs_embedding_index": 19987}, {"title": "REMEDY: Recipe Merging Dynamics in Large Vision-Language Models", "link_suffix": "/forum?id=iX7eHHE5Tx", "link": "https://openreview.net/forum?id=iX7eHHE5Tx", "pdf_link": "https://openreview.net/pdf?id=iX7eHHE5Tx", "keywords": "Multi-Modal Large Language Models, Zero-shot Generalization, Model Merging", "abstract": "Model merging has emerged as a powerful technique for combining task-specific vision models into a unified and multi-functional model. Previous methods represented by task arithmetic, have demonstrated effectiveness and scalability in this domain. When large vision-language models (LVLMs) arise with model size scaling up, this design becomes challenging to fuse different instruction-tuned LVLMs for generalization enhancement. The large scale and multi-modal nature of LVLMs present unique obstacles, including constructing reusable and modular components to accommodate the multi-component architecture of LVLMs and the requirement for dynamic fusion based on multi-modal input tokens. To address these challenges, we propose the \\textbf{RE}cipe \\textbf{ME}rging \\textbf{DY}namics (REMEDY) method, a scalable and flexible paradigm for model merging in LVLMs. We first define reusable modules termed \\textit{recipes} including the projector and shallow LLM layers, enhancing visual-language understanding. Then, we introduce a modality-aware allocator dynamically generates weights in a one-shot manner based on input relevance to existing recipes, enabling efficient cross-modal knowledge integration. REMEDY thus offers an adaptive solution for LVLMs to tackle both seen (i.e., multi-task learning) and unseen (i.e., zero-shot generalization) tasks. Experimental results demonstrate that our method consistently improves performance on both seen and unseen tasks, underscoring the effectiveness of REMEDY in diverse multi-modal scenarios.", "title_embedding_index": 19963, "title_abs_embedding_index": 19988}, {"title": "TIPS: Text-Image Pretraining with Spatial awareness", "link_suffix": "/forum?id=DaA0wAcTY7", "link": "https://openreview.net/forum?id=DaA0wAcTY7", "pdf_link": "https://openreview.net/pdf?id=DaA0wAcTY7", "keywords": "image representations, image-text, vision-language, dense understanding, computer vision", "abstract": "While image-text representation learning has become very popular in recent years, existing models tend to lack spatial awareness and have limited direct applicability for dense understanding tasks. For this reason, self-supervised pretraining is still the go-to method for many dense vision applications (e.g. depth estimation, semantic segmentation), despite the lack of explicit supervisory signals. In this paper, we close this gap between image-text and self-supervised learning, by proposing a novel general-purpose image-text model, which can be effectively used off-the-shelf for dense and global vision tasks. Our method, which we refer to as Text-Image Pretraining with Spatial awareness (TIPS), leverages two simple and effective insights. First, on textual supervision: we reveal that replacing noisy web image captions by synthetically generated textual descriptions boosts dense understanding performance significantly, due to a much richer signal for learning spatially aware representations. We propose an adapted training method that combines noisy and synthetic captions, resulting in improvements across both dense and global understanding tasks. Second, on the learning technique: we propose to combine contrastive image-text learning with self-supervised masked image modeling, to encourage spatial coherence, unlocking substantial enhancements for downstream applications. Building on these two ideas, we scale our model using the transformer architecture, trained on a curated set of public images. Our experiments are conducted on 8 tasks involving 16 datasets in total, demonstrating strong off-the-shelf performance on both dense and global understanding, for several image-only and image-text tasks.", "title_embedding_index": 19964, "title_abs_embedding_index": 19989}, {"title": "3D Object Manipulation in a Single Image Using Generative Models", "link_suffix": "/forum?id=xxzukMsYs9", "link": "https://openreview.net/forum?id=xxzukMsYs9", "pdf_link": "https://openreview.net/pdf?id=xxzukMsYs9", "keywords": "3d object manipulation, diffusion models, image editing, image animation", "abstract": "Object manipulation in images aims to not only edit the object presentation but also gift objects with motion. Previous methods encountered challenges in concurrently handling static editing and dynamic motion applications, while also struggling to achieve realism in object appearance and scene lighting. In this work, we introduce OMG3D, a novel framework that integrates the precise geometric control with the generative power of diffusion models, thus achieving significant enhancements in visual performance. Our framework converts 2D objects into 3D, enabling user-directed modifications and lifelike motions at the geometric level. To address texture realism, we propose CustomRefiner, a texture refinement module that pretrain a customized diffusion model to align the style and perspectives of coarse renderings with the original image. Additionally, we introduce IllumiCombiner, an lighting processing module that estimates and adjusts background lighting to match human visual perception, resulting in more realistic illumination. Extensive experiments demonstrate the outstanding visual performance of our approach in both static and dynamic scenarios. Remarkably, all these steps can be done using one NVIDIA 3090. The code and project page will be released upon acceptance of the paper.", "title_embedding_index": 19965, "title_abs_embedding_index": 19990}, {"title": "From Promise to Practice: Realizing High-performance Decentralized Training", "link_suffix": "/forum?id=lo3nlFHOft", "link": "https://openreview.net/forum?id=lo3nlFHOft", "pdf_link": "https://openreview.net/pdf?id=lo3nlFHOft", "keywords": "distributed training, data parallelism, decentralized optimization", "abstract": "Decentralized training of deep neural networks has attracted significant attention for its theoretically superior scalability over synchronous data-parallel methods like All-Reduce. However, realizing this potential in multi-node training is challenging due to the complex design space that involves communication topologies, computation patterns, and optimization algorithms. This paper identifies three key factors that can lead to speedups over All-Reduce training and constructs a runtime model to determine when, how, and to what degree decentralization can yield shorter per-iteration runtimes. Furthermore, to support the decentralized training of transformer-based models, we study a decentralized Adam algorithm that allows for overlapping communications and computations, prove its convergence, and propose an accumulation technique to mitigate the high variance caused by small local batch sizes. We deploy the proposed approach in clusters with up to 64 GPUs and demonstrate its practicality and advantages in both runtime and generalization performance under a fixed iteration budget.", "title_embedding_index": 19966, "title_abs_embedding_index": 19991}, {"title": "A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes", "link_suffix": "/forum?id=yIRtu2FJvY", "link": "https://openreview.net/forum?id=yIRtu2FJvY", "pdf_link": "https://openreview.net/pdf?id=yIRtu2FJvY", "keywords": "variant effect prediction, variational auto-encoder, transformer, deep learning", "abstract": "Variant effect predictors (VEPs) are designed to predict the impact of protein variants on cellular function, traditionally using data from multiple sequence alignments (MSAs). This assumes that natural variants are fit, a premise challenged by pharmacogenomics, where some pharmacogenes have low evolutionary pressure. In this context, deep mutational scanning (DMS) datasets are of particular interest since they provide quantitative fitness scores for variants. In this work, we propose a transformer-based matrix variational auto-encoder architecture and evaluate its performances on $33$ DMS datasets corresponding to $26$ drug target and absorption-distribution-metabolism-excretion (ADME) proteins available in the ProteinGym benchmark. Our model trained on MSAs (matVAE-MSA) outperforms a model similar to the widely used VEPs in pharmacogenomics, and sets a new benchmark for $2$ out of $26$ proteins. We compare matVAE-MSA with matENC-DMS, a model with similar capacity, but trained and evaluated on DMS data in a 5-fold cross-validation framework. matENC-DMS outperforms both the best available model for $15$ out of $33$ DMS datasets and matVAE-MSA for all ADME, and certain drug target proteins. Our results shed new light on the role of evolutionary pressure for the validity of the premise of VEP design. In turn motivating the development of DMS datasets to improve VEPs on pharmacogene-related proteins.", "title_embedding_index": 19967, "title_abs_embedding_index": 19992}, {"title": "Diffusion Compose: Compositional Depth Aware Scene Editing in Diffusion Models", "link_suffix": "/forum?id=GVABHyvrRU", "link": "https://openreview.net/forum?id=GVABHyvrRU", "pdf_link": "https://openreview.net/pdf?id=GVABHyvrRU", "keywords": "Image editing, 3D-aware scene control, Text-to-Image Diffusion Models", "abstract": "We introduce Diffusion Compose, a zero-shot approach for depth-aware scene editing using Text-to-Image diffusion models. While existing methods for 3D-aware editing focus on object-centric control, they do not support compositional depth-aware edits, such as placing objects at specific depths or combining multiple scenes realistically. We address this by incorporating depth-based multiplane scene representation in diffusion models. These planes, placed at fixed depths, can be individually edited or composed to enable 3D-aware scene modifications. However, direct manipulation of multiplane representation of diffusion latents often leads to identity loss or unrealistic blending. To overcome this, we propose a novel multiplane feature guidance technique that gradually aligns source latents with the target edit at each denoising step. We validate Diffusion Compose on two challenging tasks: a) scene composition, blending scenes with consistent depth order and scene illumination, and b) depth-aware object insertion, inserting novel objects at specified depths in a scene while preserving occlusions and scene structure and illumination. Extensive experiments demonstrate that Diffusion Compose significantly outperforms task-specific baselines for object placement and harmonization. A user study further confirms that it produces realistic, identity-preserving, and accurate depth-aware scene edits.", "title_embedding_index": 19968, "title_abs_embedding_index": 19993}, {"title": "mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models", "link_suffix": "/forum?id=pr37sbuhVa", "link": "https://openreview.net/forum?id=pr37sbuhVa", "pdf_link": "https://openreview.net/pdf?id=pr37sbuhVa", "keywords": "multimodal large language model, long sequence, efficient multimodal understanding", "abstract": "Multi-modal Large Language Models have demonstrated remarkable capabilities in executing instructions for a variety of single-image tasks. Despite this progress, significant challenges remain in modeling long image sequences. In this work, we introduce the versatile multi-modal large language model, mPLUG-Owl3, which enhances the capability for long image-sequence understanding in scenarios that incorporate retrieved image-text knowledge, multimodal in-context examples, and lengthy videos. Specifically, we propose novel hyper attention blocks to efficiently integrate vision and language into a common language-guided semantic space, thereby facilitating the processing of extended multi-image scenarios. We conduct evaluations on 21 benchmarks that cover single/multi-image, and short/long video understanding. mPLUG-Owl3 achieves competitive performance with the state-of-the-art methods while reducing inference time and memory usage by 87.8% and 48.5% in average. Moreover, we propose a Distractor Resistance evaluation to assess the ability of models to maintain focus amidst distractions. mPLUG-Owl3 also demonstrates outstanding performance in distractor resistance on ultra-long visual sequence inputs. We hope that mPLUG-Owl3 can contribute to the development of more efficient and powerful multimodal large language models.", "title_embedding_index": 19969, "title_abs_embedding_index": 19994}, {"title": "Long-Sequence Recommendation Models Need Decoupled Embeddings", "link_suffix": "/forum?id=jkpGIxSsUD", "link": "https://openreview.net/forum?id=jkpGIxSsUD", "pdf_link": "https://openreview.net/pdf?id=jkpGIxSsUD", "keywords": "Recommender System, User Interest Modeling", "abstract": "Lifelong user behavior sequences, comprising up to tens of thousands of history behaviors, are crucial for capturing user interests and predicting user responses in modern recommendation systems. \nA two-stage paradigm is typically adopted to handle these long sequences: a few relevant behaviors are first searched from the original long sequences via an attention mechanism in the first stage and then aggregated with the target item to construct a discriminative representation for prediction in the second stage. \nIn this work, we identify and characterize, for the first time, a neglected deficiency in existing long-sequence recommendation models: a single set of embeddings struggles with learning both attention and representation, leading to interference between these two processes. \nInitial attempts to address this issue using linear projections---a technique borrowed from language processing---proved ineffective, shedding light on the unique challenges of recommendation models. \nTo overcome this, we propose the Decoupled Attention and Representation Embeddings (DARE) model, where two distinct embedding tables are initialized and learned separately to fully decouple attention and representation. \nExtensive experiments and analysis demonstrate that DARE provides more accurate search of correlated behaviors and outperforms baselines with AUC gains up to 9\u2030 on public datasets and notable online system improvements. \nFurthermore, decoupling embedding spaces allows us to reduce the attention embedding dimension and accelerate the search procedure by 50% without significant performance impact, enabling more efficient, high-performance online serving.", "title_embedding_index": 19970, "title_abs_embedding_index": 19995}, {"title": "The Renaissance of Classic Feature Aggregations for Visual Place Recognition in the Era of Foundation Models", "link_suffix": "/forum?id=NtDXCDvkXJ", "link": "https://openreview.net/forum?id=NtDXCDvkXJ", "pdf_link": "https://openreview.net/pdf?id=NtDXCDvkXJ", "keywords": "Visual Place Recognition, Feature Representation, Supervised Learning", "abstract": "Visual Place Recognition (VPR) addresses the retrieval problem in large-scale geographic image databases through feature representations. Recent approaches have leveraged visual foundation models and have proposed novel feature aggregations. However, these methods have failed to grasp the core concepts of foundational models, such as leveraging extensive training sets, and have also neglected the potential of classical feature aggregations, such as GeM and NetVLAD, for low-dimensional representations. Building on these insights, we revive classic aggregation methods and create more fundamental VPR models, abbreviated SuperPlace. First, we introduce a supervised label alignment method that combines grid partitioning and local feature matching. This allows models to be trained on diverse VPR datasets within a unified framework, similar to the design principles of foundation models. Second, we introduce G$^2$M, a compact feature aggregation with two GeMs, in which one GeM learns the principal components of feature maps along the channel direction and calibrates the other GeM's output. Third, we propose the secondary fine-tuning (FT$^2$) strategy for NetVLAD-Linear (NVL). NetVLAD first learns feature vectors in a high-dimensional space and then compresses them into a low-dimensional space using a single linear layer. G$^2$M excels in large-scale applications requiring rapid response and low latency, while NVL-FT$^2$ is optimized for scenarios demanding high precision across a broad range of conditions. Extensive experiments (12 test sets, 14 previous methods, and 11 tables) highlight our contributions and demonstrate the superiority of SuperPlace. Specifically, SuperPlace-G$^2$M achieves state-of-the-art results with only one-tenth of the feature dimensions compared to recent methods. Moreover, SuperPlace-NVL-FT$^2$ holds the top rank on the MSLS challenge leaderboard. We have submitted a ranking screenshot, the source code, and the original experimental records in the supplementary materials.", "title_embedding_index": 19971, "title_abs_embedding_index": 19996}, {"title": "Token-Aware Inference-Time Intervention for Large Language Model Alignment", "link_suffix": "/forum?id=af2ztLTFqe", "link": "https://openreview.net/forum?id=af2ztLTFqe", "pdf_link": "https://openreview.net/pdf?id=af2ztLTFqe", "keywords": "LLM Alignment, Inference-Time Intervention, Mutual Information, Graph Network, Misalignment Estimation, Uncertainty Quantification", "abstract": "Effectively mitigating the misalignment of large language models (LLMs) is crucial for ensuring secure AI applications. Inference-Time Intervention (ITI) technique, which applies interventions to internal representations along the probed alignment direction during inference, offers substantial alignment enhancements with minimal cost. However, previous ITI methods adopt coarse sentence-level analysis which neglects the misalignment discrepancy among varied tokens, resulting in deviant alignment direction and inflexible intervention strength.\nIn this work, we propose a Token-Aware Inference-Time Intervention (TA-ITI) approach to fully utilize token-level alignment information, therefore realizing superior post-intervention performance. TA-ITI primarily consists of Mutual Information-Guided Token-level Graph Aggregation (MIG) and Misalignment-aware Adaptive Token-level Intervention (MAI). MIG develops a MI-guided graph to exploit the tokens' informative interaction for representation enrichment, thus improving alignment probing and facilitating subsequent intervention.\nMAI comprehensively perceives the token-level misalignment degree from token representation and prediction to guide the adaptive adjustment of intervention strength, thereby enhancing final alignment performance. Extensive experiments on three alignment capabilities demonstrate the efficacy of TA-ITI, notably surpassing baseline by 25.8% on the primary metric of truthfulness.", "title_embedding_index": 19972, "title_abs_embedding_index": 19997}, {"title": "PFML: Self-Supervised Learning of Time-Series Data Without Representation Collapse", "link_suffix": "/forum?id=r2nwBwodth", "link": "https://openreview.net/forum?id=r2nwBwodth", "pdf_link": "https://openreview.net/pdf?id=r2nwBwodth", "keywords": "self-supervised learning, statistical functionals, representation collapse, masking, time-series data", "abstract": "Self-supervised learning (SSL) is a data-driven learning approach that utilizes the innate structure of the data to guide the learning process. In contrast to supervised learning, which depends on external labels, SSL utilizes the inherent characteristics of the data to produce its own supervisory signal. However, one frequent issue with SSL methods is representation collapse, where the model outputs a constant input-invariant feature representation. This issue hinders the potential application of SSL methods to new data modalities, as trying to avoid representation collapse wastes researchers' time and effort. This paper introduces a novel SSL algorithm for time-series data called Prediction of Functionals from Masked Latents (PFML). Instead of predicting masked input signals or their latent representations directly, PFML operates by predicting statistical functionals of the input signal corresponding to masked embeddings, given a sequence of unmasked embeddings. The algorithm is designed to avoid representation collapse, rendering it straightforwardly applicable to different time-series data domains, such as novel sensor modalities in clinical data. We demonstrate the effectiveness of PFML through complex, real-life classification tasks across three different data modalities: infant posture and movement classification from multi-sensor inertial measurement unit data, emotion recognition from speech data, and sleep stage classification from EEG data. The results show that PFML is superior to a conceptually similar pre-existing SSL method and competitive against the current state-of-the-art SSL method, while also being conceptually simpler and without suffering from representation collapse.", "title_embedding_index": 19973, "title_abs_embedding_index": 19998}, {"title": "Towards Scalable Semantic Representation for Recommendation", "link_suffix": "/forum?id=medKq3cONT", "link": "https://openreview.net/forum?id=medKq3cONT", "pdf_link": "https://openreview.net/pdf?id=medKq3cONT", "keywords": "LLM, Recommendation System", "abstract": "With recent advances in large language models (LLMs), there has been emerging numbers of research in developing Semantic IDs based on LLMs to enhance the performance of recommendation systems. \nHowever, the dimension of these embeddings needs to match that of the ID embedding in recommendation, which is usually much smaller than the original length.\nSuch dimension compression results in inevitable losses in discriminability and dimension robustness of the LLM embeddings, which motivates us to scale up the semantic representation. \nIn this paper, we propose Mixture-of-Codes, which first constructs multiple independent codebooks for LLM representation in the indexing stage, and then utilizes the Semantic Representation along with a fusion module for the downstream recommendation stage. \nExtensive analysis and experiments demonstrate that our method achieves superior discriminability and dimension robustness scalability, leading to the best scale-up performance in recommendations.", "title_embedding_index": 19974, "title_abs_embedding_index": 19999}]