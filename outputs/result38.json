[
    {
        "title": "What's the Move? Hybrid Imitation Learning via Salient Points",
        "link_suffix": "/forum?id=r0pLGGcuY6",
        "link": "https://openreview.net/forum?id=r0pLGGcuY6",
        "pdf_link": "https://openreview.net/pdf?id=r0pLGGcuY6",
        "keywords": "Imitation Learning, Robot Learning, Robot Manipulation, Robotics",
        "abstract": "While imitation learning (IL) offers a promising framework for teaching robots various behaviors, learning complex tasks remains challenging. Existing IL policies struggle to generalize effectively across visual and spatial variations even for simple tasks. In this work, we introduceSPHINX:SalientPoint-basedHybridImitatioNand eXecution, a flexible IL policy that leverages multimodal observations (point clouds and wrist images), along with a hybrid action space of low-frequency, sparse waypoints and high-frequency, dense end effector movements. Given 3D point cloud observations, SPHINX learns to infer task-relevant points within a point cloud, orsalient points, which support spatial generalization by focusing on semantically meaningful features. These salient points serve as anchor points to predict waypoints for long-range movement, such as reaching target poses in free-space. Once near a salient point, SPHINX learns to switch to predicting dense end-effector movements given close-up wrist images for precise phases of a task. By exploiting the strengths of different input modalities and action representations for different manipulation phases, SPHINX tackles complex tasks in a sample-efficient, generalizable manner. Our method achieves86.7%success across 4 real-world and 2 simulated tasks, outperforming the next best state-of-the-art IL baseline by41.1%on average across440real world trials. SPHINX additionally generalizes to novel viewpoints, visual distractors, spatial arrangements, and execution speeds with a1.7xspeedup over the most competitive baseline. Our website (http://sphinx-il.github.io) provides open-sourced code for data collection, training, and evaluation, along with supplementary videos."
    },
    {
        "title": "RMAAT: A Bio-Inspired Approach for Efficient Long-Context Sequence Processing in Transformers",
        "link_suffix": "/forum?id=ikSrEv8FId",
        "link": "https://openreview.net/forum?id=ikSrEv8FId",
        "pdf_link": "https://openreview.net/pdf?id=ikSrEv8FId",
        "keywords": "Astrocyte, Neuromorphic Computing, Bio Inspired Learning, Neuroscience-Algorithm-Application Codesign",
        "abstract": "Astrocytes, an essential component of the brain's neural circuitry, demonstrate learning capabilities through bioplausible mechanisms such as presynaptic plasticity and hebbian plasticity. However, their integration into computational models remains underexplored. This paper advances astromorphic computing techniques to emulate transformer self-attention mechanisms, leveraging astrocytic nonlinearity and memory retention to improve long-range dependency processing in machine learning and natural language processing (NLP) tasks. Existing transformer models have difficulty handling lengthy contexts with thousands of tokens, even with substantial computational resources.  We propose Recurrent Memory Augmented Astromorphic Transformers (RMAAT), integrating astrocytic memory and recurrent processing into self-attention, enabling longer context handling without quadratic complexity growth. Our bioplausible model has been found to outperform traditional transformers in experimental tests conducted on the Long Range Arena benchmark and IMDB dataset. Specifically, our model achieves a significant reduction in memory utilization and computational latency. This paves the way for biologically inspired AI models by illustrating how astrocytic characteristics may enhance the performance and efficiency of computational models."
    },
    {
        "title": "LLM as GNN: Graph Vocabulary Learning for Graph Foundation Model",
        "link_suffix": "/forum?id=1FY1apsMxc",
        "link": "https://openreview.net/forum?id=1FY1apsMxc",
        "pdf_link": "https://openreview.net/pdf?id=1FY1apsMxc",
        "keywords": "large language model, foundation model, graph neural networks",
        "abstract": "Graphs typically exhibit distinctive structure and domain-specific knowledge, motivating the development of a Graph Foundation Model (GFM) capable of generalizing across various graphs and tasks. While recent efforts have focused on combining the strengths of Large Language Models (LLMs) and Graph Neural Networks (GNNs), they often struggle to maximize mutual benefit due to the decoupled architectures. Moreover, existing methods assign out-of-vocabulary (OOV) tokens to nodes, which are incompatible with the natural language vocabulary for task-oriented prompt generation, hindering knowledge transfer in GFM. In this paper, we introduce PromptGFM, a versatile GFM grounded in graph vocabulary learning, comprising two key components: (1) Graph Understanding Module, which explicitly replicates the finest GNN workflow in the language space using LLMs, enabling seamless GNN-LLM integration and elegant graph-text alignment; (2) Graph Inference Module, where we establish a novel language-based graph vocabulary to ensure expressiveness, transferability, and scalability. This vocabulary enables the generation of readable instructions for LLM inference, resolving modality incompatibility and facilitating positive transfer. Extensive experiments demonstrate the superiority of PromptGFM in node classification and link prediction, along with its strong transferability across different datasets and tasks. The code is available at \\url{https://anonymous.4open.science/r/PromptGFM}."
    },
    {
        "title": "PEARL: Towards Permutation-Resilient LLMs",
        "link_suffix": "/forum?id=txoJvjfI9w",
        "link": "https://openreview.net/forum?id=txoJvjfI9w",
        "pdf_link": "https://openreview.net/pdf?id=txoJvjfI9w",
        "keywords": "LLM, Distributionally Robust Optimization, OT",
        "abstract": "The in-context learning (ICL) ability of large language models (LLMs) enables them to undertake challenging tasks using provided demonstrations. However, it is prone to instability: different orderings of demonstrations can significantly influence predictions, revealing LLMs’ limitations in processing combinatorial inputs. This paper shows that this vulnerability can be exploited to design a natural and completely imperceptible attack that achieves nearly 80% success rates on the SOTA open-source model, LLaMA, by simply permuting the demonstrations. In light of this, how to overcome the ordering sensitivity problem is an important issue for improving the performance of LLMs. However, current mitigation methods focus on post-processing and fail to enhance models’ inherent robustness to the vast space of possible input permutations. To overcome this issue, we propose a novel Permutation-resilient learning framework (PEARL) based on distributional robust optimization (DRO), which optimizes model performance against the worst case among all possible permutations. Specifically, PEARL consists of a hard permutation mining network (P-Net) and the LLM. The P-Net identifies the most challenging permutations by formulating the task as an optimal transport problem, which is solved using an entropy-constrained Sinkhorn algorithm. Through minimax optimization, the P-Net progressively generates harder samples to enhance the LLM’s worst-case performance. Experiments with synthetic data and instruction\ntuning tasks demonstrate that the proposed PEARL framework effectively mitigates\npermutation attacks and improves overall performance."
    },
    {
        "title": "GC4NC: A Benchmark Framework for Graph Condensation on Node Classification with New Insights",
        "link_suffix": "/forum?id=gjC91PwBZy",
        "link": "https://openreview.net/forum?id=gjC91PwBZy",
        "pdf_link": "https://openreview.net/pdf?id=gjC91PwBZy",
        "keywords": "Graph condensation, Dataset distillation, Dataset condensation, Graph neural network",
        "abstract": "Graph condensation (GC) is an emerging technique designed to learn a significantly smaller graph that retains the essential information of the original graph. This condensed graph has shown promise in accelerating graph neural networks while preserving performance comparable to those achieved with the original, larger graphs. Additionally, this technique facilitates downstream applications like neural architecture search and deepens our understanding of redundancies in large graphs. Despite the rapid development of GC methods, particularly for node classification, a unified evaluation framework is still lacking to systematically compare different GC methods or clarify key design choices for improving their effectiveness. To bridge these gaps, we introduceGC4NC, a comprehensive framework for evaluating diverse GC methods on node classification across multiple dimensions including performance, efficiency, privacy preservation, denoising ability, NAS effectiveness, and transferability. Our systematic evaluation offers novel insights into how condensed graphs behave and the critical design choices that drive their success. These findings pave the way for future advancements in GC methods, enhancing both performance and expanding their real-world applications. The code is available athttps://anonymous.4open.science/r/GC4NC-1620."
    },
    {
        "title": "Competence-Based Analysis of Language Models",
        "link_suffix": "/forum?id=InWaCoIMMN",
        "link": "https://openreview.net/forum?id=InWaCoIMMN",
        "pdf_link": "https://openreview.net/pdf?id=InWaCoIMMN",
        "keywords": "interpretability, probing, causality, interventions, counterfactuals, mechanistic interpretability, language models, linguistic theory, lexical relations",
        "abstract": "Despite the recent successes of large language models (LLMs), little is known regarding the representations of linguistic structure they learn during pretraining, which can lead to unexpected behaviors in response to prompt variation or distribution shift. To better understand these models and behaviors, we introduce a general model analysis framework to study LLMs with respect to their representation and use of human-interpretable linguistic properties. Our framework, CALM (Competence-based Analysis of Language Models), is designed to investigate LLM competence in the context of specific tasks by intervening on models’ internal representations of different linguistic properties using causal probing, and measuring models’ alignment under these interventions with a given ground-truth causal model of the task. We also develop a new approach for performing causal probing interventions using gradient-based adversarial attacks, which can target a broader range of properties and representations than prior techniques. Finally, we carry out a case study of CALM using these interventions to analyze and compare LLM competence across a variety of lexical inference tasks, showing that CALM can be used to explain and predict behaviors across these tasks."
    },
    {
        "title": "Adaptive Causal Experimental Design: Amortizing Sequential Bayesian Experimental Design for Causal Models",
        "link_suffix": "/forum?id=JPnq9wSuSG",
        "link": "https://openreview.net/forum?id=JPnq9wSuSG",
        "pdf_link": "https://openreview.net/pdf?id=JPnq9wSuSG",
        "keywords": "Causal Discovery; Causal Reasoning; Bayesian Experimental Design; Amortized Variational Inference; Expected Information Gain; Mutual Information Lower Bounds; Adaptive Experiments",
        "abstract": "Interventions are essential for causal discovery and causal reasoning. Acquiring interventional data, however, is often costly, especially in real-world systems.\nA careful experimental design can therefore bring substantial savings. \nIn the sequential experimental design setting, \nmost existing approaches seek the best \ninterventions in a greedy (myopic) manner that does not account for the synergy from the yet-to-come future experiments. We propose Adaptive Causal Experimental Design (ACED),\na novel Bayesian sequential design framework for learning a design policy capable of generating non-myopic interventions that incorporate the effect on future experiments.\nIn particular, ACED maximizes the Expected Information Gain (EIG) on flexible choices of causal quantities of interest (e.g., causal structure, specific causal effects) directly, bypassing the need for computing intermediate posteriors in the experimental sequence.\nLeveraging a variational lower bound estimator for the EIG, ACED trains an amortized policy network that can be executed rapidly during deployment. \nWe present numerical results demonstrating ACED's effectiveness on synthetic datasets with both linear and nonlinear structural causal models, as well as on in-silico single-cell gene expression datasets."
    },
    {
        "title": "Language Models as Feature Extractors for Accurate Continual Learning",
        "link_suffix": "/forum?id=ClixrtIHUJ",
        "link": "https://openreview.net/forum?id=ClixrtIHUJ",
        "pdf_link": "https://openreview.net/pdf?id=ClixrtIHUJ",
        "keywords": "class incremental learning, continual learning with LMs, distance-based methods",
        "abstract": "This paper addresses the challenges of class incremental learning (CIL) within the broader context of continual learning. In CIL, a system learns a sequence of tasks or classes incrementally. The resulting classifier can categorize test samples into any learned class thus far without relying on task-specific information during testing. CIL presents two significant challenges: catastrophic forgetting (CF) and inter-task class separation (ICS). ICS occurs because the system lacks data from previous tasks when learning new ones, making it harder to establish decision boundaries between classes, reducing accuracy. This paper proposes a novel method to overcome both CF and ICS. The basic classifier is based on the statistical technique Mahalanobis distance (MD), which measures the distance of a data point to a normal distribution. In the proposed approach, each class is represented by a normal distribution with the mean and covariance derived from the features of its training data, which are extracted from a language model (LM). To reduce storage, all classes share a common covariance matrix. Two additional techniques are also proposed to enhance the accuracy: (1) using a kernel function to expand the feature space, and (2) incorporating an ensemble mechanism. Our experiments show that the proposed method achieves accuracy comparable to the upper bound accuracy of joint fine-tuning, which, to our knowledge, has not been achieved before."
    },
    {
        "title": "Action Sequence Planner: An Alternative For Offline Reinforcement Learning",
        "link_suffix": "/forum?id=MtjPIDWyWK",
        "link": "https://openreview.net/forum?id=MtjPIDWyWK",
        "pdf_link": "https://openreview.net/pdf?id=MtjPIDWyWK",
        "keywords": "Offline reinforcement learning, policy gradient, long horizon",
        "abstract": "Offline reinforcement learning methods, which typically train agents that make decisions step by step, are known to suffer from instability due to bootstrapping and function approximation, especially when applied to tasks requiring long-horizon planning. To alleviate these issues, in this paper, we propose a novel policy gradient approach by planning an action sequence in a high-dimensional space.This design implicitly models temporal dependencies, excelling in long-horizon and horizon-critical tasks. Furthermore, we discover that replacing maximum likelihood with cross-entropy loss in policy gradient methods significantly stabilizes training gradients, leading to substantial performance improvements in long-horizon tasks. The proposed neural network-based solution features a simple architecture that not only facilitates ease of training and convergence but also demonstrates high efficiency and effective performance. Extensive experimental results reveal that our method exhibits strong performance across a variety of tasks."
    },
    {
        "title": "Which Network is Trojaned? Increasing Trojan Evasiveness for Model-Level Detectors",
        "link_suffix": "/forum?id=31J6aWPnlR",
        "link": "https://openreview.net/forum?id=31J6aWPnlR",
        "pdf_link": "https://openreview.net/pdf?id=31J6aWPnlR",
        "keywords": "trojan detection, neural trojans, trojans, hidden functionality, monitoring, security, ML safety",
        "abstract": "Trojan attacks can pose serious risks by injecting deep neural networks with hidden, adversarial functionality. Recent methods for detecting whether a model is trojaned appear highly successful. However, a concerning and relatively unexplored possibility is that trojaned networks could be made harder to detect. To better understand the scope of this risk, we develop a general method for making trojans more evasive based on several novel techniques and observations. In experiments, we find that our evasive trojans reduce the efficacy of a wide range of detectors across numerous evaluation settings while maintaining high attack success rates. Surprisingly, we also find that our evasive trojans are substantially harder to reverse-engineer despite not being explicitly designed with this attribute in mind. These findings underscore the importance of developing more robust monitoring mechanisms for hidden functionality and clarifying the offense-defense balance of trojan detection."
    },
    {
        "title": "LLM Spark: Critical Thinking Evaluation of Large Language Models",
        "link_suffix": "/forum?id=0sJ8TqOLGS",
        "link": "https://openreview.net/forum?id=0sJ8TqOLGS",
        "pdf_link": "https://openreview.net/pdf?id=0sJ8TqOLGS",
        "keywords": "critical thinking, llm, problem-solving, benchmarks",
        "abstract": "Large language models (LLMs) excel in complex tasks but often struggle with\ninconsistencies in problem framing, a critical skill for real-world scenarios. This\npaper introduces SPARK, a novel evaluation framework grounded in the Hierar-\nchical Three-Space Theory, to assess LLMs’ ability to identify missing informa-\ntion and challenge flawed problem setups. We create benchmarks by introducing\ninconsistencies and misleading cues in diverse question-answering datasets, cov-\nering mathematics, science, and reading comprehension. Our experiments with\nstate-of-the-art LLMs reveal their limitations in critical thinking, particularly in\nrecognizing inconsistencies. We also explore mitigation strategies such as modi-\nfied prompting and targeted fine-tuning. Furthermore, we conduct comprehensive\nexperiments to investigate how model and problem properties influence critical\nthinking capabilities in LLMs."
    },
    {
        "title": "Minimax Optimal Regret Bound for Reinforcement Learning with Trajectory Feedback",
        "link_suffix": "/forum?id=en3NwykrHW",
        "link": "https://openreview.net/forum?id=en3NwykrHW",
        "pdf_link": "https://openreview.net/pdf?id=en3NwykrHW",
        "keywords": "Reinforcement learning theory, regret analysis, trajectory feedback",
        "abstract": "We study the reinforcement learning (RL) problem with trajectory feedback. The trajectory feedback based reinforcement learning problem, where the learner can only observe the accumulative noised reward along the trajectory, is particularly suitable for the practical scenarios where the agent suffers extensively from querying the reward in each single step. For a finite-horizon Markov Decision Process (MDP) with $S$ states, $A$ actions and a horizon length of $H$, we develop an algorithm that enjoys an optimal regret of $\\tilde{O}\\left(\\sqrt{SAH^3K}\\right)$ in $K$ episodes for sufficiently large $K$. To achieve this, our technical contributions are two-fold: (1) we incorporate reinforcement learning with linear bandits problem to construct a tighter confidence region for the reward function; (2) we construct a reference transition model to better guide the exploration process."
    },
    {
        "title": "When can isotropy help adapt LLMs' next word prediction to numerical domains?",
        "link_suffix": "/forum?id=L39yPOGCma",
        "link": "https://openreview.net/forum?id=L39yPOGCma",
        "pdf_link": "https://openreview.net/pdf?id=L39yPOGCma",
        "keywords": "Contextual embedding space, clusters, isotropy, language model representation, numeric downstream task",
        "abstract": "Recent studies have shown that vector representations of embeddings learned by pre-trained large language models (LLMs) are effective in various downstream tasks in numerical domains.  Despite their significant benefits, the tendency of LLMs to hallucinate in such domains can have severe consequences in applications like finance, energy, retail, climate science,  wireless networks, synthetic tabular generation, among others. To guarantee prediction reliability and accuracy in numerical domains, it is necessary to have performance guarantees through explainability. However, there is little theoretical understanding of when pre-trained language models help solve numeric downstream tasks. This paper seeks to bridge this gap by understanding when the next-word prediction capability of LLMs can be adapted to numerical domains through the lens of isotropy. Specifically, we first provide a general numeric data generation process that captures the core characteristics of numeric data across various numerical domains. Then, we consider a log-linear model for LLMs in which numeric data can be predicted from its context through a network with softmax as its last layer. We demonstrate that, in order to achieve state-of-the-art performance in numerical domains, the hidden representations of the LLM embeddings must possess a structure that accounts for the shift-invariance of the softmax function. We show how the isotropic property of LLM embeddings preserves the underlying structure of representations, thereby resolving the shift-invariance problem problem of softmax function. In other words, isotropy allows numeric downstream tasks to effectively leverage pre-trained representations, thus providing performance guarantees in the numerical domain. Experiments show that different characteristics of numeric data could have different impacts on isotropy."
    },
    {
        "title": "Grounded Video Caption Generation",
        "link_suffix": "/forum?id=xYzOkOGD96",
        "link": "https://openreview.net/forum?id=xYzOkOGD96",
        "pdf_link": "https://openreview.net/pdf?id=xYzOkOGD96",
        "keywords": "vision-language models, VLM, LLM, video grounding, automatic annotation, pseudo-labeling",
        "abstract": "We propose a new task, dataset and model for grounded video caption generation. This task unifies captioning and object grounding in video, where the objects in the caption are grounded in the video via temporally consistent bounding boxes.  We introduce the following contributions. First, we present a task definition and a manually annotated test dataset for this task, referred to as GROunded Video Caption Generation (GROC). Second, we introduce a large-scale automatic annotation method leveraging an existing model for grounded still image captioning together with an LLM for summarising frame-level captions into temporally consistent captions in video. \nFurthermore, we prompt the LLM to track by language – classifying noun phrases from the frame-level captions into noun phrases of the video-level generated caption. We apply this approach to videos from the HowTo100M dataset, which results in a new large-scale training dataset, called HowToGround, with automatically annotated captions and spatio-temporally consistent bounding boxes with coherent natural language labels. Third, we introduce a new grounded video caption generation model, called VideoGLaMM, and train the model on the new automatically annotated HowToGround dataset. Finally, results of our VideoGLaMM model set the state of the art for the new task of grounded video caption generation. We perform extensive ablations and demonstrate the importance of key technical contributions of our model."
    },
    {
        "title": "TAO-Amodal: A Benchmark for Tracking Any Object Amodally",
        "link_suffix": "/forum?id=cfuZKjGDW7",
        "link": "https://openreview.net/forum?id=cfuZKjGDW7",
        "pdf_link": "https://openreview.net/pdf?id=cfuZKjGDW7",
        "keywords": "Amodal perception, Large-scale evaluation benchmark, Multi-object tracking.",
        "abstract": "Amodal perception, the ability to comprehend complete object structures from partial visibility, is a fundamental skill, even for infants. Its significance extends to applications like autonomous driving, where a clear understanding of heavily occluded objects is essential. However, modern detection and tracking algorithms often overlook this critical capability, perhaps due to the prevalence of \\textit{modal} annotations in most benchmarks. To address the scarcity of amodal benchmarks, we introduce TAO-Amodal, featuring 833 diverse categories in thousands of video sequences. Our dataset includes \\textit{amodal} and modal bounding boxes for visible and partially or fully occluded objects, including those that are partially out of the camera frame. We investigate the current lay of the land in both amodal tracking and detection by benchmarking state-of-the-art modal trackers and amodal segmentation methods. We find that existing methods, even when adapted for amodal tracking, struggle to detect and track objects under heavy occlusion. To mitigate this, we explore simple finetuning schemes that can increase the amodal tracking and detection metrics of occluded objects by 2.1% and 3.3%."
    },
    {
        "title": "Learning Time-shared Hidden Heterogeneity for Counterfactual Outcome Forecast",
        "link_suffix": "/forum?id=FLR1K8h5Eq",
        "link": "https://openreview.net/forum?id=FLR1K8h5Eq",
        "pdf_link": "https://openreview.net/pdf?id=FLR1K8h5Eq",
        "keywords": "Hidden Heterogeneity; Counterfactual Outcome Forecast; Time series",
        "abstract": "Forecasting counterfactual outcome in the longitudinal setting can be critical for many time-related applications. To solve this problem, the previous works propose to apply different sequence models including long short-term memory (LSTM) networks and transformers to model the relationship between the observed histories, treatments and outcomes, and apply various approaches to remove treatment selection bias. However, these methods neglect the hidden heterogeneity of outcome generation among samples induced by hidden factors which can bring hurdles to counterfactual outcome forecast. To alleviate this problem, we capture the hidden heterogeneity by recovering the hidden factors and incorporate it into the outcome prediction process. Specifically, we propose a Time-shared Heterogeneity Learning from Time Series (THLTS) method which infers the shared part of hidden factors characterizing the heterogeneity across time steps with the architecture of variational encoders (VAE). This method can be a flexible component and combined with arbitrary counterfactual outcome forecast method. Experimental results on (semi-)synthetic datasets demonstrate that combined with our method, the mainstream models can improve their performance."
    },
    {
        "title": "SWE-bench Multimodal: Do Autonomous Programming Systems Generalize to New Software Domains?",
        "link_suffix": "/forum?id=riTiq3i21b",
        "link": "https://openreview.net/forum?id=riTiq3i21b",
        "pdf_link": "https://openreview.net/pdf?id=riTiq3i21b",
        "keywords": "Language models, Natural language processing, Software engineering",
        "abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains\n(e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent’s flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task\ninstances compared to 6% for the next best system."
    },
    {
        "title": "Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems",
        "link_suffix": "/forum?id=Y4aWwRh25b",
        "link": "https://openreview.net/forum?id=Y4aWwRh25b",
        "pdf_link": "https://openreview.net/pdf?id=Y4aWwRh25b",
        "keywords": "Retrieval-Augmented Generation, Security, Privacy",
        "abstract": "Retrieval-Augmented Generation (RAG) improves pre-trained models by incorporating external knowledge at test time to enable customized adaptation. \nWe study the risk of datastore leakage in Retrieval-In-Context RAG Language Models (LMs). We show that an adversary can exploit LMs' instruction-following capabilities to easily extract text data verbatim from the datastore of RAG systems built with instruction-tuned LMs via prompt injection. \nThe vulnerability exists for a wide range of modern LMs that span Llama2, Mistral/Mixtral, Vicuna, SOLAR, WizardLM, Qwen1.5, and Platypus2, and the exploitability exacerbates as the model size scales up. \nWe also study multiple effects of RAG setup on the extractability of data, indicating that following unexpected instructions to regurgitate data can be an outcome of failure in effectively utilizing contexts for modern LMs, and further show that such vulnerability can be greatly mitigated by position bias elimination strategies. \nExtending our study to production RAG models GPTs, we design an attack that can cause datastore leakage with a 100% success rate on 25 randomly selected customized GPTs with at most 2 queries, and we extract text data verbatim at a rate of 41% from a book of 77,000 words and 3% from a corpus of 1,569,000 words by prompting the GPTs with only 100 queries generated by themselves."
    },
    {
        "title": "Distributionally Robust Surface Reconstruction from Sparse Point Clouds",
        "link_suffix": "/forum?id=lT7Wq8qEvT",
        "link": "https://openreview.net/forum?id=lT7Wq8qEvT",
        "pdf_link": "https://openreview.net/pdf?id=lT7Wq8qEvT",
        "keywords": "implicit neural representations, 3D reconstruction from unoriented point could, Distributionally Robust Optimization",
        "abstract": "Implicit Neural Representations have been widely established as a powerful framework for capturing complex data modalities, including a wide range from 3D shapes to images and audio. Within the realm of 3D shape representation, Neural Signed Distance Functions (SDF) have demonstrated remarkable potential in faithfully encoding intricate shape geometry. Nevertheless, the process of learning SDFs from sparse 3D point clouds without actual supervision continues to be a highly challenging task. Unlike recent approaches that rely on smoothness priors, our method, rooted in a distributionally robust optimization framework, incorporates a regularization term that leverages samples from the uncertainty regions of the model to improve the learned SDFs. Thanks to tractable dual formulations, we show that this framework enables a stable and efficient optimization of Implicit Neural Representations in the absence of ground truth supervision.  Through extensive experiments and evaluations, we illustrate the efficacy of our DRO inspired learning framework, highlighting its capacity to improve SDF learning with respect to baselines and the state-of-the-art using synthetic and real data."
    },
    {
        "title": "Measuring the Reliability of Causal Probing Methods: Tradeoffs, Limitations, and the Plight of Nullifying Interventions",
        "link_suffix": "/forum?id=Ku1tUKnAnC",
        "link": "https://openreview.net/forum?id=Ku1tUKnAnC",
        "pdf_link": "https://openreview.net/pdf?id=Ku1tUKnAnC",
        "keywords": "interpretability, probing, causal probing, interventions, mechanistic interpretability, language models",
        "abstract": "Causal probing aims to analyze large language models (or other foundation models) by examining how modifying their representation of various latent properties using interventions derived from probing classifiers impacts their outputs. Recent works have cast doubt on the theoretical basis of several leading causal probing intervention methods, but it has been unclear how to systematically evaluate the effectiveness of probing interventions in practice. To address this, we formally define and quantify two key causal probing desiderata: completeness (how thoroughly the representation of the target property has been transformed) and selectivity (how little other properties have been impacted). We introduce an empirical analysis framework to measure and evaluate completeness and selectivity, allowing us to make the first direct comparisons of the reliability of different families of causal probing methods (e.g., linear vs. nonlinear or counterfactual vs. nullifying interventions). Our experimental analysis shows that: (1) there is an inherent tradeoff between completeness and selectivity, (2) no leading probing method is able to consistently satisfy both criteria at once, and (3) across the board, nullifying interventions are far less complete than counterfactual interventions, which suggests that nullifying methods may not be an effective approach to causal probing."
    },
    {
        "title": "Fast and Accurate Language Model Decoding via Parallel Token Processing",
        "link_suffix": "/forum?id=IRsxTYPqhQ",
        "link": "https://openreview.net/forum?id=IRsxTYPqhQ",
        "pdf_link": "https://openreview.net/pdf?id=IRsxTYPqhQ",
        "keywords": "Autoregressive model, efficient decoding, parallel token processing",
        "abstract": "Autoregressive decoding suffers from an inherent efficiency bottleneck due to its sequential token generation process, where each token must be generated before the next can be processed. This sequential dependency significantly limits the ability to fully exploit the parallel processing power of modern hardware. While speculative decoding and layer skipping offer promising speedups, both approaches come with drawbacks. Speculative decoding relies on a secondary small ''drafter'' model, which not only increases memory overhead but may also be unavailable in many cases---the drafter must share the same tokenizer and vocabulary as the main model for compatibility between generated and verified tokens. Layer skipping, on the other hand, can cause discrepancies in the generated output compared to standard autoregressive decoding, as skipped layers do not compute the key-value (KV) cache that plays a crucial role in predicting future tokens.\nIn this work, we introduce a fast and accurate decoding method, ParaDecode, which accelerates autoregressive decoding while ensuring output parity, without the need for auxiliary models or changes to original model parameters. Our approach is driven by the observation that many tokens---particularly simple or highly-predictable ones---can be accurately predicted using intermediate layer representations, without requiring computation through the entire model. Once the model reaches a certain confidence, further layers are unlikely to significantly alter the prediction. ParaDecode generates tokens at an intermediate layer when confidence is sufficiently high. \nThis allows the next token computation to commence immediately, in parallel with the completion of the KV cache computation for the early-predicted token in its remaining layers. This parallelism, implemented using batched matrix operations, enables simultaneous processing of multiple tokens across different layers, thereby maximizing hardware utilization and reducing overall decoding latency. To ensure output consistency, a final verification step is applied to guarantee that the early-predicted tokens match the results of standard autoregressive decoding. Experiments across diverse generation tasks, including text summarization, code generation, and mathematical reasoning, demonstrate that ParaDecode consistently achieves superior decoding throughput compared to baselines with up to 1.53$\\times$ speedup, while guaranteeing output parity with standard autoregressive decoding."
    },
    {
        "title": "JANET: Joint Adaptive predictioN-region Estimation for Time-series",
        "link_suffix": "/forum?id=p8sr9kfUbQ",
        "link": "https://openreview.net/forum?id=p8sr9kfUbQ",
        "pdf_link": "https://openreview.net/pdf?id=p8sr9kfUbQ",
        "keywords": "Joint Prediction Region, Conformal Prediction, Uncertainty in Time Series Forecasting",
        "abstract": "Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial.  We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs.  Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data."
    },
    {
        "title": "Entailment Progressions: A Robust Approach to Evaluating Reasoning Within Larger Discourse",
        "link_suffix": "/forum?id=Nrm1yJ577U",
        "link": "https://openreview.net/forum?id=Nrm1yJ577U",
        "pdf_link": "https://openreview.net/pdf?id=Nrm1yJ577U",
        "keywords": "Natural language inference, discourse structure, text detection",
        "abstract": "Textual entailment, or the ability to deduce whether a proposed hypothesis is logically supported by a given premise, has historically been applied to the evaluation of language modelling efficiency in tasks like question answering and text summarization. However, we hypothesize that these zero-shot entailment evaluations can be extended to the task of evaluating discourse within larger textual narratives.\nIn this paper, we propose a simple but effective method that sequentially evaluates\nchanges in textual entailment between sentences within a larger text, in an approach we denote as “Entailment Progressions”. These entailment progressions aim to capture the inference relations between sentences as an underlying component capable of distinguishing texts generated from various models and procedures. Our results suggest that entailment progressions can be used to effectively distinguish between machine-generated and human-authored texts across multiple established benchmark corpora and our own EP4MGT dataset. Additionally, our method displays robustness in performance when evaluated on paraphrased texts a technique that has historically affected the performance of well-established metrics when distinguishing between machine generated and human authored texts."
    },
    {
        "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life",
        "link_suffix": "/forum?id=PGhiPGBf47",
        "link": "https://openreview.net/forum?id=PGhiPGBf47",
        "pdf_link": "https://openreview.net/pdf?id=PGhiPGBf47",
        "keywords": "language model, moral dilemma, model alignment, machine ethics",
        "abstract": "As we increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of the users. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma includes two possible actions and with each action, the affected parties and human values invoked. Based on these dilemmas, we consolidated a set of human values across everyday topics e.g., interpersonal relationships, workplace, and environmental issues. We evaluated LLMs on these dilemmas to determine what action they will take and the values represented by these actions. Then, we analyzed these values through the lens of five popular theories inspired by sociology, psychology and philosophy. These theories are: World Value Survey, Moral Foundation Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik Wheel of Emotion. We find that LLMs are most aligned with the self-expression over survival values in terms of World Value Survey, care over loyalty in Moral Foundation Theory. Interestingly, we find large preferences differences in models for some core values such as truthfulness e.g., Mixtral-8x7B model tends to neglect it by 9.7% while GPT-4-turbo model tends to select it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their released principles reflect their actual value prioritization when facing nuanced moral reasoning in daily-life settings. We find that end users cannot effectively steer such prioritization using system prompts."
    },
    {
        "title": "Rare-Mark-Aware Next Event Prediction In Marked Event Streams",
        "link_suffix": "/forum?id=7IP7dvswE5",
        "link": "https://openreview.net/forum?id=7IP7dvswE5",
        "pdf_link": "https://openreview.net/pdf?id=7IP7dvswE5",
        "keywords": "Marked Temporal Point Process",
        "abstract": "In marked event streams, Marked Temporal Point Process (MTPP) is central to predicting when and what mark the next event will occur based on the history. In various real-world applications, the mark distribution is significantly imbalanced, i.e., some marks are frequent, and others are rare. We unveil that such imbalance can cause the rare mark missing issue when predicting the next event – frequent marks are dominant, and rare marks often have no chance. However, rare marks can be essential in some applications (e.g., the occurrence of a 7-magnitude earthquake), and missing such rare marks in the next event prediction is risky. To address this issue, we tackle a novel Rare-mark-aware Next Event Prediction problem (RM-NEP), answering two questions for each mark m: “what is the probability that the mark of the next event is m?, and if m, when will the next event happen?”. Solving RM-NEP gives rare marks equal opportunity as frequent marks in the next event prediction. This guarantees that rare marks are always included in the predicted results. Moreover, RM-NEP allows arbitrary number of rare marks samples for time prediction without interference from frequent marks, ensuring the time prediction is accurate. To solve RM-NEP effectively, we first unify the improper integration of two different functions into one and then develop a novel Integral-free Neural Marked Temporal Point Process (IFNMTPP) to approximate the target integral directly. Extensive experiments on real-world and synthetic datasets demonstrate the superior performance of our solution for RM-NEP against various baselines."
    }
]