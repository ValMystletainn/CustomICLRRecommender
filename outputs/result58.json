[{"title": "Over 100x Speedup in Relational Deep Learning via Static GNNs and Tabular Distillation", "link_suffix": "/forum?id=c93my9VkqO", "link": "https://openreview.net/forum?id=c93my9VkqO", "pdf_link": "https://openreview.net/pdf?id=c93my9VkqO", "keywords": "Relational Databases, Relational Deep Learning, GNNs, Training Acceleration, Real-time Inference, Distillation", "abstract": "Relational databases, organized into tables connected by primary-foreign key relationships, are widely used in industry. Companies leverage this data to build highly accurate, feature-engineered tabular models\u2014often using boosted decision trees\u2014to predict key metrics such as customer transactions and product revenues. However, these models need frequent retraining as new data is introduced, which is both expensive and time-consuming. Despite this, by being the result of extensive engineering effort, they remain difficult to outperform using generalist methods, like Temporal Graph Neural Networks (TGNNs) trained over the same relational data. Rather than attempting to replace tabular models with generalist approaches, we propose to combine the strengths of tabular models and static Graph Neural Networks (GNNs). GNNs offer better speed and scalability than TGNNs, and, as we argue, the primary strength of graph representation learning for these tasks does not lie in modeling temporal dynamics\u2014something highly- engineered tabular models excel at\u2014but in capturing complex relationships within the database, which are hard to featurize. Our approach integrates all predictive embeddings of all tabular models developed for various tasks into a single static GNN framework. Experimental results on the RelBench benchmark show that our approach achieves a performance improvement of up to 33% and an inference speedup of up to 1050x, making it highly suitable for real-time inference.", "title_embedding_index": 2850, "title_abs_embedding_index": 2875}, {"title": "A primal-dual algorithm for variational image reconstruction with learned convex regularizers", "link_suffix": "/forum?id=R9W6fFlr8W", "link": "https://openreview.net/forum?id=R9W6fFlr8W", "pdf_link": "https://openreview.net/pdf?id=R9W6fFlr8W", "keywords": "variational problem+learned convex regularizer+convex optimization+primal-dual algorithm", "abstract": "We address the optimization problem in a data-driven variational reconstruction framework, where the regularizer is parameterized by an input-convex neural network (ICNN). While gradient-based methods are commonly used to solve such problems, they struggle to effectively handle non-smoothness which often leads to slow convergence. Moreover, the nested structure of the neural network complicates the application of standard non-smooth optimization techniques, such as proximal algorithms. To overcome these challenges, we reformulate the problem and eliminate the network's nested structure. By relating this reformulation to epigraphical projections of the activation functions, we transform the problem into a convex optimization problem that can be efficiently solved using a primal-dual algorithm. We also prove that this reformulation is equivalent to the original variational problem. Through experiments on several imaging tasks, we demonstrate that the proposed approach outperforms subgradient methods in terms of both speed and stability.", "title_embedding_index": 2851, "title_abs_embedding_index": 2876}, {"title": "Extendable and Iterative Structure Learning for Bayesian Networks", "link_suffix": "/forum?id=3n6DYH3cIP", "link": "https://openreview.net/forum?id=3n6DYH3cIP", "pdf_link": "https://openreview.net/pdf?id=3n6DYH3cIP", "keywords": "structure learning, Bayesian networks, iterative", "abstract": "Learning the structure of Bayesian networks is a fundamental yet computationally intensive task, especially as the number of variables grows. Traditional algorithms require retraining from scratch when new variables are introduced, making them impractical for dynamic or large-scale applications. In this paper, we propose an extendable structure learning strategy that efficiently incorporates a new variable $Y$ into an existing Bayesian network graph $\\mathcal{G}$ over variables $\\mathcal{X}$, resulting in an updated P-map graph $\\bar{\\mathcal{G}}$ on $\\bar{\\mathcal{X}} = \\mathcal{X} \\cup {Y}$. By leveraging the information encoded in $\\mathcal{G}$, our method significantly reduces computational overhead compared to learning $\\bar{\\mathcal{G}}$ from scratch. Empirical evaluations demonstrate runtime reductions of up to 1300x without compromising accuracy. Building on this approach, we introduce a novel iterative paradigm for structure learning over $\\mathcal{X}$. Starting with a small subset $\\mathcal{U} \\subset \\mathcal{X}$, we iteratively add the remaining variables using our extendable algorithms to construct a P-map graph over the full set. This method offers runtime advantages comparable to common algorithms like PC while maintaining similar accuracy. Our contributions provide a scalable solution for Bayesian network structure learning, enabling efficient model updates in real-time and high-dimensional settings.", "title_embedding_index": 2852, "title_abs_embedding_index": 2877}, {"title": "Learning Multiple Semantic Views For Self-explaining Physiological Signal Stratification", "link_suffix": "/forum?id=1d8Egv45of", "link": "https://openreview.net/forum?id=1d8Egv45of", "pdf_link": "https://openreview.net/pdf?id=1d8Egv45of", "keywords": "Explainable artificial intelligence (XAI), Interpretable machine learning, Interpretability, Deep learning, Time Series Analysis, Segmentation, End-to-end, Self-explaining models, Physiological signals, Photoplethysmogram (PPG), Electrocardiogram (ECG), Obstructive sleep apnea (OSA), Atrial fibrillation (AF), Heart rate variability (HRV), Blood pressure (BP)", "abstract": "Explainable artificial intelligence (XAI) offers enhanced transparency by revealing key features, relationships, and patterns within the input data that drive model decisions. In healthcare and clinical applications, where physiological signals serve as inputs to the models for decision making, such transparency is critical for ensuring reliability, identifying biases, and uncovering new insights. However, despite the potential to reveal clinically-relevant information used for inference, generalized solutions for explainability have remained limited in this domain. In this work, we propose a generalized self-explaining multi-view deep learning architecture, that generates task-relevant human-interpretable representations during model inference, for stratifying health information from physiological signals. Specifically, the proposed network architecture employs a mask network to produce multiple mask-modulated versions of the signal, referred to as \u201csemantic views\u201d, highlighting distinct regions of the signal that may be relevant to clinically significant information. These views offer complementary perspectives to enhance interpretability and feature extraction. A shared embedding network is used to extract task-related features from each semantic view, which are used to produce the model's output. Through supervised training with labels, each semantic view is updated based on the saliency information between the semantic view and the model's output, toward fitting the model's output to the task labels. Validated on 4 different clinically-relevant classification and regression tasks taking electrocardiogram (ECG) or photoplethysmogram (PPG) as input, the proposed multi-view architecture displays universal usability, achieving comparable or superior performance across all tasks, when compared to state-of-the-art methods designed for each task. Unlike current state-of-the-art models, which lack task-agnostic human interpretability, our model uniquely provides interpretable outputs. As it is shown, the semantic views generated by the proposed model highlight task-specific characteristic regions in the input signal, aligning closely with the domain knowledge of human experts for each task. Overall, the proposed method offers new directions for interpretable machine learning and data-driven analysis of physiological signals, envisioning self-explaining models for clinical applications.", "title_embedding_index": 2853, "title_abs_embedding_index": 2878}, {"title": "LoRTA: Low Rank Tensor Adaptation of Large Language Models", "link_suffix": "/forum?id=ayPfZIkovt", "link": "https://openreview.net/forum?id=ayPfZIkovt", "pdf_link": "https://openreview.net/pdf?id=ayPfZIkovt", "keywords": "PEFT, LLM, Fine-tuning, Efficiency, Low-rank, Tensors", "abstract": "Low Rank Adaptation (LoRA) is a popular Parameter Efficient Fine Tuning (PEFT) method that effectively adapts large pre-trained models for downstream tasks. LoRA parameterizes model updates using low-rank matrices at each layer,  significantly reducing the number of trainable parameters and, consequently, resource requirements during fine-tuning. However, the lower bound on the number of trainable parameters remains high due to the use of the low-rank matrix model. In this paper, we address this limitation by proposing a novel approach that employs a low rank tensor parametrization for model updates. The proposed low rank tensor model can significantly reduce the number of trainable parameters, while also allowing for finer-grained control over adapter size. Our experiments on Natural Language Understanding, Instruction Tuning, Preference Optimization and Protein Folding benchmarks demonstrate that our method is both efficient and effective for fine-tuning Large Language Models, achieving a substantial reduction in the number of parameters while maintaining comparable performance.", "title_embedding_index": 2854, "title_abs_embedding_index": 2879}, {"title": "ViTally Consistent: Scaling Biological Representation Learning for Cell Microscopy", "link_suffix": "/forum?id=niywLsa54R", "link": "https://openreview.net/forum?id=niywLsa54R", "pdf_link": "https://openreview.net/pdf?id=niywLsa54R", "keywords": "MAE, microscopy, transformers, SSL, linear probing, biology, high-content screening, foundation models", "abstract": "Large-scale cell microscopy screens are used in drug discovery and molecular biology research to study the effects of millions of chemical and genetic perturbations on cells. To use these images in downstream analysis, we need models that can map each image into a feature space that represents diverse biological phenotypes consistently, in the sense that perturbations with similar biological effects have similar representations.\nIn this work, we present the largest foundation model for cell microscopy data to date, a new 1.9 billion-parameter ViT-G/8 MAE trained on over 8 billion microscopy image crops. Compared to a previous published ViT-L/8 MAE, our new model achieves a 60% improvement in linear separability of genetic perturbations and obtains the best overall performance on whole-genome biological relationship recall and replicate consistency benchmarks.\nBeyond scaling, we developed two key methods that improve performance: (1) training on a curated and diverse dataset; and, (2) using biologically motivated linear probing tasks to search across each transformer block for the best candidate representation of whole-genome screens. We find that many self-supervised vision transformers, pretrained on either natural or microscopy images, yield significantly more biologically meaningful representations of microscopy images in their intermediate blocks than in their typically used final blocks. More broadly, our approach and results provide insights toward a general strategy for successfully building foundation models for large-scale biological data.", "title_embedding_index": 2855, "title_abs_embedding_index": 2880}, {"title": "SMORE-DRL: Scalable Multi-Objective Robust and Efficient Deep Reinforcement Learning for Molecular Optimization", "link_suffix": "/forum?id=ZujxvJS0uI", "link": "https://openreview.net/forum?id=ZujxvJS0uI", "pdf_link": "https://openreview.net/pdf?id=ZujxvJS0uI", "keywords": "Drug Design, Molecular Optimization, Deep Reinforcement Learning, Multi-Objective Optimization, Transformer, Scalable", "abstract": "The adoption of machine learning techniques within the domain of drug design provides an opportunity of systematic and efficient exploration of the vast chemical search space. In recent years, advancements in this domain have been achieved through the application of deep reinforcement learning (DRL) frameworks. However, the scalability and performance of existing methodologies are constrained by prolonged training periods and inefficient sample data utilization. Furthermore, generalization capabilities of these models have not been fully investigated. To overcome these limitations, we take a multi-objective optimization perspective and introduce SMORE-DRL, a fragment and transformer-based multi-objective DRL architecture for the optimization of molecules across multiple pharmacological properties, including binding affinity to a cancer protein target. Our approach involves pretraining a transformer-encoder model on molecules encoded by a novel hybrid fragment-SMILES representation method. Fine-tuning is performed through a novel gradient-alignment-based DRL, where lead molecules are optimized by selecting and replacing their fragments with alternatives from a fragment dictionary, ultimately resulting in more desirable drug candidates. Our findings indicate that SMORE-DRL is superior to current DRL models for lead optimization in terms of quality, efficiency, scalability, and robustness. Furthermore SMORE-DRL demonstrates the capability of generalizing its optimization process to lead molecules that are not present during the pretraining or fine-tuning phases.", "title_embedding_index": 2856, "title_abs_embedding_index": 2881}, {"title": "Zero-Shot Generalization of GNNs over Distinct Attribute Domains", "link_suffix": "/forum?id=5btqauRdz0", "link": "https://openreview.net/forum?id=5btqauRdz0", "pdf_link": "https://openreview.net/pdf?id=5btqauRdz0", "keywords": "GNN, zero-shot, graph foundation models", "abstract": "Inductive GNNs are able to generalize across graphs with the same set of node attributes. However, zero-shot generalization across attributed graphs with disparate node attribute domains remains a fundamental challenge in graph machine learning. Existing methods are unable to effectively make use of node attributes when transferring to unseen attribute domains, frequently performing no better than models that ignore attributes entirely. This limitation stems from the fact that models trained on one set of attributes (e.g., biographical data in social networks) fail to capture relational dependencies that extend to new attributes in unseen test graphs (e.g., TV and movies preferences). Here, we introduce STAGE, a method that learns representations ofstatistical dependenciesbetween attributes rather than the attribute values themselves, which can then be applied to completely unseen test-time attributes, generalizing by identifying analogous dependencies between features in test. STAGE leverages the theoretical link between maximal invariants and measures of statistical dependencies, enabling it to provably generalize to unseen feature domains for a family of domain shifts. Our empirical results show that when STAGE is pretrained on multiple graph datasets with unrelated feature spaces (distinct feature types and dimensions) and evaluated zero-shot on graphs with yet new feature types and dimensions, it achieves a relative improvement in Hits@1 between 40% to 103% for link prediction, and an 11% improvement in node classification against state-of-the-art baselines.", "title_embedding_index": 2857, "title_abs_embedding_index": 2882}, {"title": "Improving Graph Neural Networks by Learning Continuous Edge Directions", "link_suffix": "/forum?id=iAmR7FfMmq", "link": "https://openreview.net/forum?id=iAmR7FfMmq", "pdf_link": "https://openreview.net/pdf?id=iAmR7FfMmq", "keywords": "Graph Neural Networks, Directed Graphs, Graph Laplacian, Continuous Edge Directions, Graph Ensemble Data", "abstract": "Graph Neural Networks (GNNs) traditionally employ a message-passing mechanism that resembles diffusion over undirected graphs, which often leads to homogenization of node features and reduced discriminative power in tasks such as node classification. Our key insight for addressing this limitation is to assign fuzzy edge directions---that can vary continuously from node $i$ pointing to node $j$ to vice versa---to the edges of a graph so that features can preferentially flow in one direction between nodes to enable long-range information transmission across the graph. We also introduce a novel complex-valued Laplacian for directed graphs with fuzzy edges where the real and imaginary parts represent information flow in opposite directions. Using this Laplacian, we propose a general framework, called Continuous Edge Direction (CoED) GNN, for learning on graphs with fuzzy edges and prove its expressivity limits using a generalization of the Weisfeiler-Leman (WL) graph isomorphism test for directed graphs with fuzzy edges. Our architecture aggregates neighbor features scaled by the learned edge directions and processes the aggregated messages from in-neighbors and out-neighbors separately alongside the self-features of the nodes. Because continuous edge directions are differentiable, we can learn both the edge directions and the GNN weights end-to-end via gradient-based optimization. CoED GNN is particularly well-suited for graph ensemble data where the graph structure remains fixed but multiple realizations of node features are available, such as in gene regulatory networks, web connectivity graphs, and power grids. We demonstrate through extensive experiments on both synthetic and real datasets that learning continuous edge directions significantly improves performance both for undirected and directed graphs compared with existing methods.", "title_embedding_index": 2858, "title_abs_embedding_index": 2883}, {"title": "Auditingf-Differential Privacy in One Run", "link_suffix": "/forum?id=0QZcoGdmtJ", "link": "https://openreview.net/forum?id=0QZcoGdmtJ", "pdf_link": "https://openreview.net/pdf?id=0QZcoGdmtJ", "keywords": "Differential privacy, Auditing privacy", "abstract": "Privacy-preserving machine learning requires carefully designed and rigorously analyzed algorithms. However, such designs and analyses are often susceptible to errors or imperfections, leading to mechanisms that may not offer the expected level of privacy due to mathematical inaccuracies or implementation flaws. Conversely, some mechanisms might provide stronger privacy guarantees than can be proven through a loose analysis. Empirical privacy auditing has emerged as a means to address this gap. Existing auditing mechanisms, however, are either inefficient\u2014requiring multiple runs of machine learning algorithms\u2014or suboptimal in calculating the empirical privacy of these algorithms. In this work, we present a tight and efficient auditing procedure and analysis that can effectively assess the privacy of mechanisms. Our approach requires only a single run of the mechanism and achieves tight empirical privacy by leveraging the $f$-DP curve, which provides a more accurate measure of privacy than the traditional $\\epsilon,\\delta$ parameters. Experiments demonstrate that our auditing algorithm delivers tighter empirical privacy guarantees.", "title_embedding_index": 2859, "title_abs_embedding_index": 2884}, {"title": "improve weakly supervised visual grounding by learning where to focus on", "link_suffix": "/forum?id=BwQUo5RVun", "link": "https://openreview.net/forum?id=BwQUo5RVun", "pdf_link": "https://openreview.net/pdf?id=BwQUo5RVun", "keywords": "weakly supervised learning, visual grounding, grad-cam, vision and language", "abstract": "Visual grounding is a crucial task for connecting visual and language descriptions by identifying target objects based on language entities. However, fully supervised methods require extensive annotations, which can be challenging and time-consuming to obtain. Weakly supervised visual grounding, which only relies on image-sentence association without object-level annotations, offers a promising solution. Previous approaches have mainly focused on finding the relationship between detected candidates, without considering improving object localization. In this work, we propose a novel method that leverages Grad-CAM to help the model identify precise objects. Specifically, we introduce a CAM encoder that exploits Grad-CAM information and a new loss function, attention mining loss, to guide the Grad-CAM feature to focus on the entire object. We also use an architecture which combines CNN and transformer, and a multi-modality fusion module to aggregate visual features, language features and CAM features. Our proposed approach achieves state-of-the-art results on several datasets, demonstrating its effectiveness in different scenes. Ablation studies further confirm the benefits of our architecture.", "title_embedding_index": 2860, "title_abs_embedding_index": 2885}, {"title": "Advancing Energy Efficiency in On-Device Streaming Speech Recognition", "link_suffix": "/forum?id=A6QotWIQim", "link": "https://openreview.net/forum?id=A6QotWIQim", "pdf_link": "https://openreview.net/pdf?id=A6QotWIQim", "keywords": "speech recognition, speech and audio", "abstract": "Power consumption plays a crucial role in on-device streaming speech recognition, significantly influencing the user experience. This study explores how the configuration of weight parameters in speech recognition models affects their overall energy efficiency. We found that the influence of these parameters on power consumption varies depending on factors such as invocation frequency and memory allocation. Leveraging these insights, we propose design principles that enhance on-device speech recognition models by reducing power consumption with minimal impact on accuracy. Our approach, which adjusts model components based on their specific energy sensitivities, achieves up to 47% lower energy usage while preserving comparable model accuracy and improving real-time performance compared to leading methods.", "title_embedding_index": 2861, "title_abs_embedding_index": 2886}, {"title": "Learning Chaotic Dynamics with Embedded Dissipativity", "link_suffix": "/forum?id=XqDM97DtMf", "link": "https://openreview.net/forum?id=XqDM97DtMf", "pdf_link": "https://openreview.net/pdf?id=XqDM97DtMf", "keywords": "Dynamical systems, learning for dynamics, chaotic dynamics", "abstract": "Chaotic dynamics, commonly seen in weather systems and fluid turbulence, are characterized by their sensitivity to initial conditions, which makes accurate prediction challenging. Despite its sensitivity to initial perturbations, many chaotic systems observe dissipative behaviors and ergodicity. Therefore, recently various approaches have been proposed to develop data-driven models preserving invariant statistics over long horizons. Although these methods have shown empirical success in reducing instances of unbounded trajectory generation, many of the models are still prone to generating unbounded trajectories, leading to invalid statistics evaluation. In this paper, we propose a novel neural network architecture that simultaneously learns a dissipative dynamics emulator that guarantees to generate bounded trajectories and an energy-like function that governs the dissipative behavior. More specifically, by leveraging control-theoretic ideas, we derive algebraic conditions based on the learned energy-like function that ensure asymptotic convergence to an invariant level set. Using these algebraic conditions, our proposed model enforces dissipativity through a ReLU projection layer, which provides formal trajectory boundedness guarantees. Furthermore, the invariant level set provides an outer estimate for the strange attractor, which is known to be very difficult to characterize due to its complex geometry. We demonstrate the capability of our model in producing bounded long-horizon trajectory forecasts that preserve invariant statistics and characterizing the attractor, for chaotic dynamical systems including Lorenz 96 and a truncated Kuramoto-Sivashinsky equation.", "title_embedding_index": 2862, "title_abs_embedding_index": 2887}, {"title": "Symmetrization of Loss Functions for Robust Training of Neural Networks in the Presence of Noisy Labels", "link_suffix": "/forum?id=i8BaiywFYx", "link": "https://openreview.net/forum?id=i8BaiywFYx", "pdf_link": "https://openreview.net/pdf?id=i8BaiywFYx", "keywords": "Noisy labels, Symmetric loss functions, Multi-class loss decomposition, Unhinged loss function", "abstract": "Labeling a training set is not only often expensive but also susceptible to errors. Consequently, the development of robust loss functions to handle label noise has emerged as a problem of great importance. The symmetry condition provides theoretical guarantees for robustness to such noise. In this work, we investigate a symmetrization method that arises from the unique decomposition of any multi-class loss function into a sum of a symmetric loss function and a class-insensitive term. Notably, the special case of symmetrizing the cross-entropy loss leads to a multi-class extension of the unhinged loss function. This loss function is linear, but unlike in the binary case, it must have specific coefficients in order to satisfy the symmetry condition. Under appropriate assumptions, we demonstrate that this multi-class unhinged loss function is the unique convex multi-class symmetric loss function. It holds a significant role among multi-class symmetric loss functions since the linear approximation of any symmetric loss function around points with equal components must be equivalent to the multi-class unhinged. Furthermore, we introduce SGCE and \u03b1-MAE, two novel loss functions that smoothly transition between the multi-class unhinged loss and the Mean Absolute Error (MAE). Our experiments demonstrate superior performance over previous state-of-the-art robust loss functions on standard benchmarks, highlighting the effectiveness of our approach in handling label noise.", "title_embedding_index": 2863, "title_abs_embedding_index": 2888}, {"title": "Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation", "link_suffix": "/forum?id=8DuJ5FK2fa", "link": "https://openreview.net/forum?id=8DuJ5FK2fa", "pdf_link": "https://openreview.net/pdf?id=8DuJ5FK2fa", "keywords": "Spurious Correlation, Group Robustness, Zero Group Annotation, Distribution Shift, Out-of-Distribution Generalization", "abstract": "Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to enhance robustness to spurious correlation, but they sometimes depend on group annotations for training. Additionally, a common limitation in previous research is the reliance on group-annotated validation datasets for model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are not available. To enhance model robustness with minimal group annotation assumptions, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from an ERM-trained model to construct a balanced dataset of high-loss and low-loss samples, mitigating group imbalance in data. This significantly enhances robustness to group shifts when equipped with a simple post-training last layer retraining. By using environment inference methods to create diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in validation data. In this context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.", "title_embedding_index": 2864, "title_abs_embedding_index": 2889}, {"title": "Informed Exploration via Generative Modeling", "link_suffix": "/forum?id=JNhU9NeOFr", "link": "https://openreview.net/forum?id=JNhU9NeOFr", "pdf_link": "https://openreview.net/pdf?id=JNhU9NeOFr", "keywords": "bandit algorithms, Thompson sampling, bayesian inference, generative models", "abstract": "Conventionally trained neural networks excel at prediction but often struggle to model uncertainty in their own predictions. We explore this challenge in a meta-learning bandit decision-making problem for news recommendations; this setting require decision-making algorithms to incorporate pretrained language models to process text data for the best performance. We present a scalable approach to Bayesian uncertainty quantification by posing it as a problem of autoregressive generative modeling of future rewards. First, we use historical data on previously released news articles to pre-train a generative model to predict sequences of future potential rewards. At inference time, our algorithm makes decisions based on limited previous rewards and autoregressively generated future rewards. Far from a heuristic, we synthesize insights from the literature to show our method is a novel implementation of Thompson (posterior) sampling, a prominent bandit algorithm. We prove our pretraining loss directly controls online decision-making performance, and we demonstrate our framework on a news recommendation task where we integrate end-to-end fine-tuning of a pretrained language model to process news article headline text to improve performance.", "title_embedding_index": 2865, "title_abs_embedding_index": 2890}, {"title": "Inverse Flow and Consistency Models", "link_suffix": "/forum?id=YSJNKWOjKV", "link": "https://openreview.net/forum?id=YSJNKWOjKV", "pdf_link": "https://openreview.net/pdf?id=YSJNKWOjKV", "keywords": "diffusion models; flow matching; consistency models; denoising", "abstract": "Inverse generation problems, such as denoising without ground truth observations, is a critical challenge in many scientific inquiries and real-world applications. While recent advances in generative models like diffusion models, conditional flow matching, and consistency models achieved impressive results by casting generation as denoising problems, they cannot be directly used for inverse generation without access to clean data. Here we introduce Inverse Flow (IF), a novel framework that enables using these generative models for inverse generation problems including denoising without ground truth. Inverse Flow can be flexibly applied to nearly any continuous noise distribution and allows complex dependencies. We propose two algorithms for learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency Model (ICM). Notably, to derive the computationally efficient, simulation-free inverse consistency model objective, we generalized consistency training to any forward diffusion processes or conditional flows, which have applications beyond denoising. We demonstrate the effectiveness of IF on synthetic and real datasets, outperforming prior approaches while enabling noise distributions that previous methods cannot support. Finally, we showcase applications of our techniques to fluorescence microscopy and single-cell genomics data, highlighting IF's utility in scientific problems. This work opens up the use of powerful generative models for denoising.", "title_embedding_index": 2866, "title_abs_embedding_index": 2891}, {"title": "Fast constrained sampling in pre-trained diffusion models", "link_suffix": "/forum?id=c3i8uRSE9h", "link": "https://openreview.net/forum?id=c3i8uRSE9h", "pdf_link": "https://openreview.net/pdf?id=c3i8uRSE9h", "keywords": "Diffusion models, conditional sampling, generative models", "abstract": "Diffusion models have dominated the field of large, generative image models, with the prime examples of Stable Diffusion and DALL-E 3 being widely adopted. These models have been trained to perform text-conditioned generation on vast numbers of image-caption pairs and as a byproduct, have acquired general knowledge about natural image statistics. However, when confronted with the task of constrained sampling, e.g. generating the right half of an image conditioned on the known left half, applying these models is a delicate and slow process, with previously proposed algorithms relying on expensive iterative operations that are usually orders of magnitude slower than text-based inference. This is counter-intuitive, as image-conditioned generation should rely less on the difficult-to-learn semantic knowledge that links captions and imagery, and should instead be achievable by lower-level correlations among image pixels. In practice, inverse models are trained or tuned separately for each inverse problem, e.g. by providing parts of images during training as an additional condition, to allow their application in realistic settings. However, we argue that this is not necessary and propose an algorithm for fast-constrained sampling in large pre-trained diffusion models (Stable Diffusion) that requires no expensive backpropagation operations through the model and produces results comparable even to the state-of-the-art \\emph{tuned} models. Our method is based on a novel optimization perspective to sampling under constraints and employs a numerical approximation to the expensive gradients, previously computed using backpropagation, incurring significant speed-ups.", "title_embedding_index": 2867, "title_abs_embedding_index": 2892}, {"title": "Accelerating Task Generalisation with Multi-Level Hierarchical Options", "link_suffix": "/forum?id=KfeRfxTemB", "link": "https://openreview.net/forum?id=KfeRfxTemB", "pdf_link": "https://openreview.net/pdf?id=KfeRfxTemB", "keywords": "Reinforcement Learning, Generalisation, Hierarchical Reinforcement Learning", "abstract": "Creating reinforcement learning agents that generalise effectively to new tasks is a key challenge in AI research. This paper introduces Fracture Cluster Options (FraCOs), a multi-level hierarchical reinforcement learning method that achieves state-of-the-art performance on difficult generalisation tasks. FraCOs identifies patterns in agent behaviour and forms options based on the expected future usefulness of those patterns, enabling rapid adaptation to new tasks. In tabular settings, FraCOs demonstrates effective transfer and improves performance as it grows in hierarchical depth. We evaluate FraCOs against state-of-the-art deep reinforcement learning algorithms in several complex procedurally generated environments. Our results show that FraCOs achieves higher in-distribution and out-of-distribution performance than competitors.", "title_embedding_index": 2868, "title_abs_embedding_index": 2893}, {"title": "SiReRAG: Indexing Similar and Related Information for Multihop Reasoning", "link_suffix": "/forum?id=yp95goUAT1", "link": "https://openreview.net/forum?id=yp95goUAT1", "pdf_link": "https://openreview.net/pdf?id=yp95goUAT1", "keywords": "Retrieval-augmented generation (RAG), RAG indexing, Multi-hop question answering", "abstract": "Indexing is an important step towards strong performance in retrieval-augmented generation (RAG) systems. However, existing methods organize data based on either semantic similarity or related information, but not both. As our analysis reveals, modeling only one perspective leads to suboptimal performance on complex tasks requiring multi-hop reasoning. In this paper, we propose SiReRAG, a novel RAG indexing approach that explicitly considers both similar and related information. On the similarity side, we follow existing work and explore some variances to construct a similarity tree based on recursive summarization. On the relatedness side, SiReRAG extracts propositions and entities from texts, groups propositions via shared entities, and generates recursive summaries to construct a relatedness tree. We index and flatten both similarity and relatedness trees into a unified retrieval pool, demonstrating that SiReRAG consistently outperforms state-of-the-art indexing methods on three multi-hop datasets, with an average 4.6% improvement in F1 scores. SiReRAG also enhances existing embedding and reranking methods, with an average improvement of 7.8% and 4% in F1 scores.", "title_embedding_index": 2869, "title_abs_embedding_index": 2894}, {"title": "Training on test proteins improves fitness, structure, and function prediction", "link_suffix": "/forum?id=qT0IWGqo1j", "link": "https://openreview.net/forum?id=qT0IWGqo1j", "pdf_link": "https://openreview.net/pdf?id=qT0IWGqo1j", "keywords": "proteins, generalization, self-supervised learning, test-time training", "abstract": "Data scarcity and distribution shifts often hinder the ability of machine learning models to generalize when applied to proteins and other biological data. Self-supervised pre-training on large datasets is a common method to enhance generalization. However, striving to perform well on all possible proteins can limit a model\u2019s capacity to excel on any specific one. To address this limitation, we propose an orthogonal approach to generalization. Building on the prevalence of self-supervised pre-training, we introduce a method for self-supervised fine-tuning at test time, allowing models to adapt to the test protein of interest on the fly and without requiring any additional data. We study our test-time training (TTT) method through the lens of perplexity minimization and show that it consistently enhances generalization across different models, their scales, and datasets. Notably, our method leads to new state-of-the-art results on the standard benchmark for protein fitness prediction, improves protein structure prediction for challenging targets, and enhances the accuracy of protein function predictions.", "title_embedding_index": 2870, "title_abs_embedding_index": 2895}, {"title": "LLM-Cite: Cheap Fact Verification with Attribution via URL Generation", "link_suffix": "/forum?id=qb2QRoE4W3", "link": "https://openreview.net/forum?id=qb2QRoE4W3", "pdf_link": "https://openreview.net/pdf?id=qb2QRoE4W3", "keywords": "Fact Verification, Attribution, Citation, Factuality", "abstract": "Hallucinations are one of the main issues with Large Language Models (LLMs). This has led to increased interest in automated ways to verify the factuality of LLMs' responses. Existing methods either rely on: (a) search over a knowledge base (KB), which is costly especially if the KB must be updated frequently to keep up with fresh content, (b) LLM's parametric knowledge to fact-check claims, which is cheaper but does not give attribution and is limited to verifying claims related to knowledge acquired during pretraining. In this work, we present LLM-Cite, a cheap and easy to implement method that does not rely on any external search system while still providing attribution and the ability to verify fresh claims. Our key insight is to leverage an LLM to directly generate potential citation URLs for a given claim, and then use entailment checks to verify the claim against content of the URLs (which are fetched on-the-fly). We benchmark LLM-Cite on three datasets containing fresh and non-fresh claims generated by humans and models. We show that LLM-Cite performs comparable or better than existing methods on all categories of claims --- importantly, without sacrificing attribution, or requiring costly external search --- overall LLM-Cite is more than 45x cheaper than a Google Search based approach.", "title_embedding_index": 2871, "title_abs_embedding_index": 2896}, {"title": "Learning equivariant tensor functions with applications to sparse vector recovery", "link_suffix": "/forum?id=kyVzYpDxHg", "link": "https://openreview.net/forum?id=kyVzYpDxHg", "pdf_link": "https://openreview.net/pdf?id=kyVzYpDxHg", "keywords": "equivariant machine learning, tensors, sparse vector recovery", "abstract": "This work characterizes equivariant polynomial functions from tuples of tensor inputs to tensor outputs. Loosely motivated by physics, we focus on equivariant functions with respect to the diagonal action of the orthogonal group on tensors. We show how to extend this characterization to other linear algebraic groups, including the Lorentz and symplectic groups.Our goal behind these characterizations is to define equivariant machine learning models. In particular, we focus on the sparse vector estimation problem. This problem has been broadly studied in the theoretical computer science literature, and explicit spectral methods, derived by techniques from sum-of-squares, can be shown to recover sparse vectors under certain assumptions. Our numerical results show that the proposed equivariant machine learning models can learn spectral methods that outperform the best theoretically known spectral methods in some regimes. The experiments also suggest that learned spectral methods can solve the problem in settings that have yet to be theoretically analyzed.This is an example of a promising direction in which theory can inform machine learning models and machine learning models can inform theory.", "title_embedding_index": 2872, "title_abs_embedding_index": 2897}, {"title": "AutoCoder: Enhancing Code Large Language Model with AIEV-INSTRUCT", "link_suffix": "/forum?id=cDdeTXOnAK", "link": "https://openreview.net/forum?id=cDdeTXOnAK", "pdf_link": "https://openreview.net/pdf?id=cDdeTXOnAK", "keywords": "LLM, Code generation, data annotation, Agents interaction", "abstract": "We introduce AutoCoder, an open-source Large Language Model to surpass GPT-4 Turbo and GPT-4o in pass@1 on the Human Eval benchmark test (90.9% vs. 90.2). In addition, AutoCoder offers a more versatile code interpreter compared to GPT-4 Turbo and GPT-4o. It's code interpreter can install external packages instead of limiting to built-in packages. AutoCoder's training data is a multi-turn dialogue dataset created by a system combining agent interaction and external code execution verification, a method we term AIEV-Instruct (Agent-Interaction Execution-Verified). Compared to previous large-scale code dataset annotation methods, AIEV-Instruct reduces dependence on proprietary large models and provides more accurate code annotation data.", "title_embedding_index": 2873, "title_abs_embedding_index": 2898}, {"title": "MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents", "link_suffix": "/forum?id=BfQNrKJMXq", "link": "https://openreview.net/forum?id=BfQNrKJMXq", "pdf_link": "https://openreview.net/pdf?id=BfQNrKJMXq", "keywords": "LLM, Agent, Benchmark", "abstract": "Large Language Model (LLM)-based mobile agents are increasingly popular due to their capability to interact directly with mobile phone Graphic User Interfaces (GUIs) and their potential to autonomously manage daily tasks. Despite their promising prospects in both academic and industrial sectors, little research has focused on benchmarking the performance of existing mobile agents, due to the inexhaustible states of apps and the vague definition of feasible action sequences. To address this challenge, we propose an efficient and user-friendly benchmark, MobileAgentBench, designed to alleviate the burden of extensive manual testing. We initially define 100 tasks across 10 open-source apps, categorized by multiple levels of difficulty. Subsequently, we evaluate several existing mobile agents, including AppAgent and MobileAgent, to thoroughly and systematically compare their performance. All materials will be accessible on our project webpage, contributing to the advancement of both academic and industrial fields.", "title_embedding_index": 2874, "title_abs_embedding_index": 2899}]