[
    {
        "title": "Peacock: Multi-Objective Optimization for Deep Neural Network Calibration",
        "link_suffix": "/forum?id=nUOmJ4Qop5",
        "link": "https://openreview.net/forum?id=nUOmJ4Qop5",
        "pdf_link": "https://openreview.net/pdf?id=nUOmJ4Qop5",
        "keywords": "Deep Neural Network Calibration, Uncertainty Calibration, Robustness, Safety, Out-of-Distribution",
        "abstract": "The rapid adoption of deep neural networks underscores an urgent need for models to be safe, trustworthy and well-calibrated. Despite recent advancements in network calibration, the optimal combination of techniques remains relatively unexplored. By framing the task as a multi-objective optimization problem, we demonstrate that combining state-of-the-art methods can further boost calibration performance. We feature a total of seven state-of-the-art calibration algorithms and provide both theoretical and empirical motivation for their equal and weighted importance unification. We conduct experiments on both in and out-of-distribution computer vision and natural language benchmarks, investigating the speeds and contributions of different components."
    },
    {
        "title": "MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?",
        "link_suffix": "/forum?id=k5VHHgsRbi",
        "link": "https://openreview.net/forum?id=k5VHHgsRbi",
        "pdf_link": "https://openreview.net/pdf?id=k5VHHgsRbi",
        "keywords": "multimodal Large Language Models, benchmark, high-resolution images, real-world scenarios",
        "abstract": "Comprehensive evaluation of Multimodal Large Language Models (MLLMs) has recently garnered widespread attention in the research community. However, we observe that existing benchmarks present several common barriers that make it difficult to measure the significant challenges that models face in the real world, including: 1) small data scale leads to a large performance variance; 2) reliance on model-based annotations results in restricted data quality; 3) insufficient task difficulty, especially caused by the limited image resolution. To tackle these issues, we introduce MME-RealWorld. Specifically, we collect more than $300$ K images from public datasets and the Internet, filtering $13,366$ high-quality images for annotation. This involves the efforts of professional $25$ annotators and $7$ experts in MLLMs, contributing to $29,429$ question-answer pairs that cover $43$ subtasks across $5$ real-world scenarios, extremely challenging even for humans. As far as we know,MME-RealWorld is the largest manually annotated benchmark to date, featuring the highest resolution and a targeted focus on real-world applications. We further conduct a thorough evaluation involving $29$ prominent MLLMs, such as GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. Our results show that even the most advanced models struggle with our benchmarks, where none of them reach 60% accuracy. The challenges of perceiving high-resolution images and understanding complex real-world scenarios remain urgent issues to be addressed. The data and evaluation code are released in our Project Page."
    },
    {
        "title": "State Space Model Meets Transformer: A New Paradigm for 3D Object Detection",
        "link_suffix": "/forum?id=Tisu1L0Jwt",
        "link": "https://openreview.net/forum?id=Tisu1L0Jwt",
        "pdf_link": "https://openreview.net/pdf?id=Tisu1L0Jwt",
        "keywords": "Point Cloud; 3D Object Detection; State Space Model",
        "abstract": "DETR-based methods, which use multi-layer transformer decoders to refine object queries iteratively, have shown promising performance in 3D indoor object detection. However, the scene point features in the transformer decoder remain fixed, leading to minimal contributions from later decoder layers, thereby limiting performance improvement. Recently, State Space Models (SSM) have shown efficient context modeling ability with linear complexity through iterative interactions between system states and inputs. Inspired by SSMs, we propose a new 3D object DEtection paradigm with an interactive STate space model (DEST). In the interactive SSM, we design a novel state-dependent SSM parameterization method that enables system states to effectively serve as queries in 3D indoor detection tasks. In addition, we introduce four key designs tailored to the characteristics of point cloud and SSM: The serialization and bidirectional scanning strategies enable bidirectional feature interaction among scene points within the SSM. The inter-state attention mechanism models the relationships between state points, while the gated feed-forward network enhances inter-channel correlations. To the best of our knowledge, this is the first method to model queries as system states and scene points as system inputs, which can simultaneous update scene point features and query features with linear complexity. Extensive experiments on two challenging datasets demonstrate the effectiveness of our DEST-based method. Our method imporoves the GroupFree baseline in terms of $\\text{AP}_{50}$ on ScanNet V2 (+5.3) and SUN RGB-D (+3.2) datasets. Based on the VDETR baseline, Our method sets a new state-of-the-art on the ScanNetV2 and SUN RGB-D datasets."
    },
    {
        "title": "Mambular: A Sequential Model for Tabular Deep Learning",
        "link_suffix": "/forum?id=wElgE9qBb5",
        "link": "https://openreview.net/forum?id=wElgE9qBb5",
        "pdf_link": "https://openreview.net/pdf?id=wElgE9qBb5",
        "keywords": "Tabular Deep Learning, Mamba, Sequential Models, SSM, Recurrent Neural Networks",
        "abstract": "The analysis of tabular data has traditionally been dominated by gradient-boosted decision trees (GBDTs), known for their proficiency with mixed categorical and numerical features. However, recent deep learning innovations are challenging this dominance. We introduce Mambular, an adaptation of the Mamba architecture optimized for tabular data. We extensively benchmark Mambular against state-of-the-art models, including neural networks and tree-based methods, and demonstrate its competitive performance across diverse datasets.\nAdditionally, we explore various adaptations of Mambular to understand its effectiveness for tabular data. We investigate different pooling strategies, feature interaction mechanisms, and bi-directional processing. Our analysis shows that interpreting features as a sequence and passing them through Mamba layers results in surprisingly performant models.  The results highlight Mambular\u2019s potential as a versatile and powerful architecture for tabular data analysis, expanding the scope of deep learning applications in this domain.\n The source code is available at:https://anonymous.4open.science/r/mamba-tabular-485F/"
    },
    {
        "title": "What to align in multimodal contrastive learning?",
        "link_suffix": "/forum?id=Pe3AxLq6Wf",
        "link": "https://openreview.net/forum?id=Pe3AxLq6Wf",
        "pdf_link": "https://openreview.net/pdf?id=Pe3AxLq6Wf",
        "keywords": "Multimodal representation learning, Self-supervised learning, Contrastive learning",
        "abstract": "Humans perceive the world through multisensory integration, blending the information of different modalities to adapt their behavior.\nContrastive learning offers an appealing solution for multimodal self-supervised learning. Indeed, by considering each modality as a different view of the same entity, it learns to align features of different modalities in a shared representation space. However, this approach is intrinsically limited as it only learns shared or redundant information between modalities, while multimodal interactions can arise in other ways. In this work, we introduce CoMM, a Contrastive Multimodal learning strategy that enables the communication between modalities in a single multimodal space. Instead of imposing cross- or intra- modality constraints, we propose to align multimodal representations by maximizing the mutual information between augmented versions of these multimodal features. Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation, allowing us to estimate multimodal interactions beyond redundancy. We test CoMM both in a controlled and in a series of real-world settings: in the former, we demonstrate that CoMM effectively captures redundant, unique and synergistic information between modalities. In the latter, CoMM learns complex multimodal interactions and achieves state-of-the-art results on seven multimodal tasks."
    },
    {
        "title": "Vision Search Assistant: Empower Vision-Language Models as Multimodal Search Engines",
        "link_suffix": "/forum?id=PC5WxcMRs8",
        "link": "https://openreview.net/forum?id=PC5WxcMRs8",
        "pdf_link": "https://openreview.net/pdf?id=PC5WxcMRs8",
        "keywords": "Vision Search Assistant, VLM-Agent Collaboration, Multimodal Search Engine",
        "abstract": "Search engines enable the retrieval of unknown information with texts. However, traditional methods fall short when it comes to understanding unfamiliar visual content, such as identifying an object that the model has never seen before. This challenge is particularly pronounced for large vision-language models (VLMs): if the model has not been exposed to the object depicted in an image, it struggles to generate reliable answers to the user's question regarding that image. Moreover, as new objects and events continuously emerge, frequently updating VLMs is impractical due to heavy computational burdens. To address this limitation, we propose Vision Search Assistant, a novel framework that facilitates collaboration between VLMs and web agents. This approach leverages VLMs' visual understanding capabilities and web agents' real-time information access to perform open-world Retrieval-Augmented Generation via the web. By integrating visual and textual representations through this collaboration, the model can provide informed responses even when the image is novel to the system. Extensive experiments conducted on both open-set and closed-set QA benchmarks demonstrate that the Vision Search Assistant significantly outperforms the other models and can be widely applied to existing VLMs."
    },
    {
        "title": "Conformal Prediction for Deep Classifier via Truncating",
        "link_suffix": "/forum?id=uUkpYafkVl",
        "link": "https://openreview.net/forum?id=uUkpYafkVl",
        "pdf_link": "https://openreview.net/pdf?id=uUkpYafkVl",
        "keywords": "Conformal Prediction, Uncertainty Quantification",
        "abstract": "Conformal Prediction is a distribution-free statistical framework that outputs a set of possible labels to capture the predictive uncertainty. In this work, we show that existing conformal prediction methods may generate inefficient sets arising from the inclusion of redundant labels. To mitigate this issue, we propose a novel conformal prediction algorithm, $\\textit{Post-Calibration Truncated Conformal Prediction}$ (PoT-CP), which limits the size of the prediction sets generated by existing conformal prediction methods through a maximum rank cutoff. Specifically, PoT-CP determines this cutoff by minimizing a truncation rank that preserves the marginal coverage of the calibration dataset. The key idea is to eliminate classes with high predictive uncertainty in the prediction sets, allowing PoT-CP to further shorten the prediction sets. Theoretically, we provide the asymptotic validity of marginal coverage for PoT-CP and demonstrate the asymptotic conditional coverage equivalence between PoT-CP and the standard conformal prediction algorithm. Extensive experiments demonstrate that PoT-CP can effectively reduce prediction set sizes while maintaining the stable conditional coverage of various conformal prediction algorithms across different classification tasks."
    },
    {
        "title": "AlignIQL: Policy Alignment in Implicit Q-Learning through Constrained Optimization",
        "link_suffix": "/forum?id=3Xfa63ggsq",
        "link": "https://openreview.net/forum?id=3Xfa63ggsq",
        "pdf_link": "https://openreview.net/pdf?id=3Xfa63ggsq",
        "keywords": "Offline reinforcement learning, optimization, Implict Q learning, diffusion model",
        "abstract": "Implicit Q-learning (IQL) serves as a strong baseline for offline RL, which never needs to evaluate actions outside of the dataset through quantile regression. However, it is unclear how to recover the implicit policy from the learned implicit Q-function and whether IQL can utilize weighted regression for policy extraction. IDQL reinterprets IQL as an actor-critic method and gets weights of implicit policy, however, this weight only holds for the optimal value function under certain critic loss functions. In this work, we introduce a different way to solve the $\\textit{implicit policy-finding problem}$ (IPF) by formulating this problem as an optimization problem. Based on this optimization problem, we further propose two practical algorithms AlignIQL and AlignIQL-hard, which inherit the advantages of decoupling actor from critic in IQL and provide insights into why IQL can use weighted regression for policy extraction. Compared with IQL and IDQL, we find that our method keeps the simplicity of IQL and solves the implicit policy-finding problem.  Experimental results on D4RL datasets show that our method achieves competitive or superior results compared with other SOTA offline RL methods. Especially in complex sparse reward tasks like AntMaze and Adroit, our method outperforms IQL and IDQL by a significant margin."
    },
    {
        "title": "GVFi: Learning 3D Gaussian Velocity Fields from Dynamic Videos",
        "link_suffix": "/forum?id=0Zot73kfLB",
        "link": "https://openreview.net/forum?id=0Zot73kfLB",
        "pdf_link": "https://openreview.net/pdf?id=0Zot73kfLB",
        "keywords": "Dynamic Reconstruction, Physics, Motion Extrapolation",
        "abstract": "In this paper, we aim to model 3D scene geometry, appearance, and physical information just from dynamic multi-view videos in the absence of any human labels. By leveraging physics-informed losses as soft constraints or integrating simple physics models into neural networks, existing works often fail to learn complex motion physics, or doing so requires additional labels such as object types or masks. In this paper, we propose a general framework named GVFi to model the motion physics of complex dynamic 3D scenes. The key novelty of our approach is that, by formulating each 3D point as a rigid particle with size and orientation in space, we choose to directly learn a translation rotation dynamics system for each particle, explicitly estimating a complete set of physical parameters to govern the particle's motion over time. Extensive experiments on three existing dynamic datasets and a newly created challenging dataset demonstrate the extraordinary performance of our method over baselines in the task of future frame extrapolation. A nice property of our framework is that multiple objects or parts can be easily segmented just by clustering the learned physical parameters."
    },
    {
        "title": "Unified Universality Theorem for Deep and Shallow Joint-Group-Equivariant Machines",
        "link_suffix": "/forum?id=NxLWeK4P3q",
        "link": "https://openreview.net/forum?id=NxLWeK4P3q",
        "pdf_link": "https://openreview.net/pdf?id=NxLWeK4P3q",
        "keywords": "group theory, irreducible representation, universality, fully-connected network, joint-group-equivariance, ridgelet transform",
        "abstract": "We present a constructive universal approximation theorem for learning machines equipped with joint-group-equivariant feature maps, based on the group representation theory. ``Constructive'' here indicates that the distribution of parameters is given in a closed-form expression known as the ridgelet transform. Joint-group-equivariance encompasses a broad class of feature maps that generalize classical group-equivariance. Notably, this class includes fully-connected networks, which arenotgroup-equivariantbutare joint-group-equivariant. Moreover, our main theorem also unifies the universal approximation theorems for both shallow and deep networks. While the universality of shallow networks has been investigated in a unified manner by the ridgelet transform, the universality of deep networks has been investigated in a case-by-case manner."
    },
    {
        "title": "Knowledge-localized Unlearning for Faithful Forgetting in Language Models",
        "link_suffix": "/forum?id=AcR5Mngp1p",
        "link": "https://openreview.net/forum?id=AcR5Mngp1p",
        "pdf_link": "https://openreview.net/pdf?id=AcR5Mngp1p",
        "keywords": "Unlearning, Knowledge-localization, Faithful Unlearning, Superficial Unlearning",
        "abstract": "Large language models are exposed to privacy risks since they are trained on large text corpus, which may include sensitive or private information. Therefore, existing studies have attempted to unlearn undesirable knowledge exposed without permission from a language model. However, they are limited in that they have overlooked the complex and interconnected nature of knowledge, where related knowledge must be carefully examined. Specifically, they have failed to evaluate whether an unlearning method faithfully erases interconnected knowledge that should be removed, retaining knowledge that appears relevant but exists in a completely different context. To resolve this problem, we first define a new concept called superficial unlearning, which refers to the phenomenon where an unlearning method either fails to erase the interconnected knowledge it should remove or unintentionally erases irrelevant knowledge. Based on the definition, we introduce a new benchmark, FaithUnBench, to analyze and evaluate the faithfulness of unlearning in real-world knowledge QA settings. Furthermore, we propose a novel unlearning method, KLUE, which identifies and updates only knowledge-related neurons to achieve faithful unlearning. KLUE categorizes knowledge neurons using an explainability method and updates only those neurons using selected unforgotten samples.  Experimental results demonstrate that widely-used unlearning methods fail to ensure faithful unlearning, while our method shows significant effectiveness in real-world QA settings."
    },
    {
        "title": "PPLLaVA: Varied Video Sequence Understanding With Prompt Guidance",
        "link_suffix": "/forum?id=qUZY7ymDPr",
        "link": "https://openreview.net/forum?id=qUZY7ymDPr",
        "pdf_link": "https://openreview.net/pdf?id=qUZY7ymDPr",
        "keywords": "Video LLM, Prompt-guided Pooling, PPLLaVA",
        "abstract": "The past year has witnessed the significant advancement of video-based large language models. However, the challenge of developing a unified model for both short and long video understanding remains unresolved. Most existing video LLMs cannot handle hour-long videos, while methods custom for long videos tend to be ineffective for shorter videos and images. In this paper, we identify the key issue as the redundant content in videos. To address this, we propose a novel pooling strategy that simultaneously achieves token compression and instruction-aware visual feature aggregation. Our model is termed Prompt-guided Pooling LLaVA, or PPLLaVA for short. Specifically, PPLLaVA consists of three core components: the CLIP-based visual-prompt alignment that extracts visual information relevant to the user's instructions, the prompt-guided pooling that compresses the visual sequence to arbitrary scales using convolution-style pooling, and the clip context extension designed for lengthy prompt common in visual dialogue. Moreover, our codebase also integrates the most advanced video Direct Preference Optimization (DPO) and visual interleave training. Extensive experiments have validated the performance of our model. With superior throughput, PPLLaVA achieves better results on image benchmarks as a video LLM, while achieving state-of-the-art performance across various video benchmarks, excelling in tasks ranging from caption generation to multiple-choice questions, and handling video lengths from seconds to hours. The codes are promised to be made public."
    },
    {
        "title": "Smoothing the Shift: Towards Stable Test-time Adaptation under Complex Multimodal Noises",
        "link_suffix": "/forum?id=rObkvzJxTG",
        "link": "https://openreview.net/forum?id=rObkvzJxTG",
        "pdf_link": "https://openreview.net/pdf?id=rObkvzJxTG",
        "keywords": "Test-time Adaptation, Multimodal Noises, Domain Adaptation",
        "abstract": "Test-time adaptation (TTA) aims to tackle distribution shifts using unlabeled test data without access to the source data. In the context of multimodal data, there are more complex noise patterns than unimodal data such as simultaneous corruptions for multiple modalities and missing modalities. Besides, in real-world applications, corruptions from different distribution shifts are always mixed. Existing TTA methods always fail in such multimodal scenario because the abrupt distribution shifts will destroy the prior knowledge from the source model, thus leading to performance degradation.\nTo address this challenging problem, we propose two novel strategies: sample identification with interquartile rangeSmoothing andunimodal assistance andMutualinformation sharing (SuMi). SuMi smooths the adaptation process by interquartile range which avoids the abrupt distribution shifts. Then, SuMi fully utilizes the unimodal features to select low-entropy samples with rich multimodal information for optimization. Furthermore, mutual information sharing is introduced to align the information, reduce the discrepancies and enhance the information utilization across different modalities. Extensive experiments show the effectiveness and superiority over existing methods under the complex noise patterns in multimodal data. Code will be available."
    },
    {
        "title": "Improving Sequence Level Distillation through Hidden State Matching",
        "link_suffix": "/forum?id=IcVSKhVpKu",
        "link": "https://openreview.net/forum?id=IcVSKhVpKu",
        "pdf_link": "https://openreview.net/pdf?id=IcVSKhVpKu",
        "keywords": "Knowledge Distillation, Centered Kernel Alignment, BART, mBART, T5",
        "abstract": "Hidden State Matching is a prominent technique in the knowledge distillation of language models. Most existing methods follow DistilBERT in using a cosine loss to encourage similarity between the student and the teacher's hidden states. However, the cosine loss restricts the architecture and dimensionality of the student, thereby severely limiting the compression ratio. We present a different technique using Centered Kernel Alignment (CKA) to match hidden states of different dimensionality, allowing for smaller students and higher compression ratios. We show the efficacy of our method using encoder--decoder (BART, mBART & T5) as well as encoder-only (BERT) architectures across a range of tasks from classification to summarization and translation. Our technique is competitive with the current state-of-the-art distillation methods at comparable compression rates. It can scale to students smaller than the current methods, is no slower in training and inference, and is considerably more flexible."
    },
    {
        "title": "PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers in LLMs",
        "link_suffix": "/forum?id=vw0NurJ7UX",
        "link": "https://openreview.net/forum?id=vw0NurJ7UX",
        "pdf_link": "https://openreview.net/pdf?id=vw0NurJ7UX",
        "keywords": "Large language model; Token-wise outliers; Static quantization;",
        "abstract": "Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60\u00d7 to 2.81\u00d7 faster than FP16 models and exceeds QuaRot models by 1.2\u00d7 to 1.3\u00d7."
    },
    {
        "title": "Long-Context Linear System Identification",
        "link_suffix": "/forum?id=2TuUXtLGhT",
        "link": "https://openreview.net/forum?id=2TuUXtLGhT",
        "pdf_link": "https://openreview.net/pdf?id=2TuUXtLGhT",
        "keywords": "autoregressive, linear, statistics, low rank, mispecification",
        "abstract": "This paper addresses the problem of long-context linear system identification, where the state $x_t$ of the system at time $t$ depends linearly on previous states $x_s$ over a fixed context window of length $p$. We establish a sample complexity bound that matches thei.i.d.parametric rate, up to logarithmic factors for a broad class of systems, extending previous work that considered only first-order dependencies. Our findings reveal a ``learning-without-mixing'' phenomenon, indicating that learning long-context linear autoregressive models is not hindered by slow mixing properties potentially associated with extended context windows. Additionally, we extend these results to(i)shared low-rank feature representations, where rank-regularized estimators improve rates with respect to dimensionality, and(ii)misspecified context lengths in strictly stable systems, where shorter contexts offer statistical advantages."
    },
    {
        "title": "Diagonalizing Affinity Matrix to Identify Clustering Structure",
        "link_suffix": "/forum?id=81qyvxW9pe",
        "link": "https://openreview.net/forum?id=81qyvxW9pe",
        "pdf_link": "https://openreview.net/pdf?id=81qyvxW9pe",
        "keywords": "Block diagonal, clustering analysis, affinity matrix",
        "abstract": "Affinity matrix-based clustering constitutes an eminent approach within the domain of data mining. Nevertheless, prior research overlooked the opportunity to directly exploit the block-diagonal structure of the affinity matrix for the purpose of identifying cluster formations. In this paper, we propose an affinity matrix-based clustering strategy, termed as DAM, which employs a traversal algorithm to discern high-density clusters within the graph weighted by the affinity matrix, thereby establishing a traversal sequence. This sequence is subsequently utilized to permute the affinity matrix, thereby revealing its intrinsic block-diagonal structure. Moreover, we introduce an innovative split-and-refine algorithm that autonomously detects all diagonal blocks within the permuted matrix, ensuring theoretical optimality in the presence of well-separated clusters. Extensive evaluations on six real-world benchmark image clustering datasets demonstrate the superiority of our method over contemporary state-of-the-art clustering techniques."
    },
    {
        "title": "Controllable Unlearning for Image-to-Image Generative Models via\u03f5-Constrained Optimization",
        "link_suffix": "/forum?id=9OJflnNu6C",
        "link": "https://openreview.net/forum?id=9OJflnNu6C",
        "pdf_link": "https://openreview.net/pdf?id=9OJflnNu6C",
        "keywords": "Machine unlearning, Generative model, Controllable",
        "abstract": "While generative models have made significant advancements in recent years, they also raise concerns such as privacy breaches and biases. Machine unlearning has emerged as a viable solution, aiming to remove specific training data, e.g., containing private information and bias, from models. In this paper, we study the machine unlearning problem in Image-to-Image (I2I) generative models. Previous studies mainly treat it as a single objective optimization problem, offering a solitary solution, thereby neglecting the varied user expectations towards the trade-off between complete unlearning and model utility. To address this issue, we propose a controllable unlearning framework that uses a control coefficient $\\epsilon$ to control the trade-off. We reformulate the I2I generative model unlearning problem into a $\\epsilon$-constrained optimization problem and solve it with a gradient-based method to find optimal solutions for unlearning boundaries. These boundaries define the valid range for the control coefficient. Within this range, every yielded solution is theoretically guaranteed with Pareto optimality. We also analyze the convergence rate of our framework under various control functions. Extensive experiments on two benchmark datasets across three mainstream I2I models demonstrate the effectiveness of our controllable unlearning framework."
    },
    {
        "title": "Random Is All You Need: Random Noise Injection on Feature Statistics for Generalizable Deep Image Denoising",
        "link_suffix": "/forum?id=z8PcUSKXXN",
        "link": "https://openreview.net/forum?id=z8PcUSKXXN",
        "pdf_link": "https://openreview.net/pdf?id=z8PcUSKXXN",
        "keywords": "Image Denoising, Low-Level Vision, Generalization Problem",
        "abstract": "Recent advancements in generalizable deep image denoising have catalyzed the development of robust noise-handling models. The current state-of-the-art, Masked Training (MT), constructs a masked swinir model which is trained exclusively on Gaussian noise ($\\sigma$=15) but can achieve commendable denoising performance across various noise types (i.e.speckle noise, poisson noise). However, this method, while focusing on content reconstruction, often produces over-smoothed images and poses challenges in mask ratio optimization, complicating its integration with other methodologies. In response, this paper introduces RNINet, a novel architecture built on a streamlined encoder-decoder framework to enhance both efficiency and overall performance. Initially, we train a pure RNINet (only simple encoder-decoder) on individual noise types, observing that feature statistics such as mean and variance shift in response to different noise conditions. Leveraging these insights, we incorporate a noise injection block that injects random noise into feature statistics within our framework, significantly improving generalization across unseen noise types. Our framework not only simplifies the architectural complexity found in MT but also delivers superior performance. Comprehensive experimental evaluations demonstrate that our method outperforms MT in various unseen noise conditions in terms of denoising effectiveness and computational efficiency (lower MACs and GPU memory usage), achieving up to 10 times faster inference speeds and underscoring it's capability for large scale deployments."
    },
    {
        "title": "Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence",
        "link_suffix": "/forum?id=Q95MaWfF4e",
        "link": "https://openreview.net/forum?id=Q95MaWfF4e",
        "pdf_link": "https://openreview.net/pdf?id=Q95MaWfF4e",
        "keywords": "Explainable AI, Concept-based Explanations, Concept Activation Vectors",
        "abstract": "With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space.\nCommonly, CAVs are computed by leveraging linear classifiers optimizing theseparabilityof latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction.\nThis discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability.\nTo address this, we introducepattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions.\nWe evaluate various CAV methods in terms of their alignment with the true concept direction and their impact on CAV applications, including concept sensitivity testing and model correction for shortcut behavior caused by data artifacts. \nWe demonstrate the benefits of pattern-based CAVs using the Pediatric Bone Age, ISIC2019, and FunnyBirds datasets with VGG, ResNet, ReXNet, EfficientNet, and Vision Transformer as model architectures."
    },
    {
        "title": "MaskInversion: Localized Embeddings via Optimization of Explainability Maps",
        "link_suffix": "/forum?id=DhlbK7tAjz",
        "link": "https://openreview.net/forum?id=DhlbK7tAjz",
        "pdf_link": "https://openreview.net/pdf?id=DhlbK7tAjz",
        "keywords": "localized embedding, fondation models, test-time optimization",
        "abstract": "Vision-language foundation models such as CLIP have achieved tremendous results in global vision-language alignment, but still show some limitations in creating representations for specific image regions. \nTo address this problem, we propose MaskInversion, a method that leverages the feature representations of pre-trained foundation models, such as CLIP, to generate a context-aware embedding for a query image region specified by a mask at test time.\nMaskInversion starts with initializing an embedding token and compares its explainability map, derived from the pretrained model, to the query mask.\nThe embedding token is then subsequently refined to approximate the query region by minimizing the discrepancy between its explainability map and the query mask. During this process, only the embedding vector is updated, while the underlying foundation model is kept frozen\nallowing to use MaskInversion with any pre-trained model. \nAs deriving the explainability map involves computing its gradient, which can be expensive, we propose a gradient decomposition strategy that simplifies this computation.\nThe learned region representation can be used for a broad range of tasks, including open-vocabulary class retrieval, referring expression comprehension, as well as for localized captioning and image generation. We evaluate the proposed method on all those tasks on several datasets such as PascalVOC, MSCOCO, RefCOCO, and OpenImagesV7 and show its capabilities compared to other SOTA approaches."
    },
    {
        "title": "Robust Probabilistic Unsupervised Segmentation with Uncertainty Modeling",
        "link_suffix": "/forum?id=Rf4NnqHNSz",
        "link": "https://openreview.net/forum?id=Rf4NnqHNSz",
        "pdf_link": "https://openreview.net/pdf?id=Rf4NnqHNSz",
        "keywords": "Unsupervised Image Segmentation, Probabilistic Unsupervised Segmentation",
        "abstract": "Unsupervised semantic segmentation aims to assign a semantic label to each pixel in an image, identifying the object or scene class without any supervision. However, the task becomes particularly difficult due to factors like unclear or overlapping boundaries, intricate object textures, and the presence of multiple objects within the same region. Traditional unsupervised models often suffer from class misalignment and poor spatial coherence, leading to fragmented and imprecise segmentation, often employing postprocessing with Conditional Random Fields (CRFs) to improve their results. Additionally, deterministic models lack the ability to capture prediction uncertainty, making their outputs particularly prone to errors in ambiguous regions. To address these issues, we propose a probabilistic unsupervised semantic segmentation framework that enhances the robustness and accuracy of segmentation by refining predictions through uncertainty modeling and spatial smoothing techniques. We also introduce a novel loss function that encourages the model to focus on learning similarities within pixels by leveraging feature information from pre-trained vision transformer backbones.\nWe also provide theoretical analyses of our proposed loss function, highlighting its favorable properties in relation to the optimization of our models. Our method demonstrates superior accuracy and calibration, outperforming various baselines across multiple unsupervised semantic segmentation benchmarks including COCO, Potsdam, and Cityscapes. In conclusion, our framework offers a foundation for more reliable, uncertainty-aware segmentation models, advancing research in unsupervised semantic segmentation."
    },
    {
        "title": "LLaVA-MoD: Making LLaVA Tiny via MoE-Knowledge Distillation",
        "link_suffix": "/forum?id=uWtLOy35WD",
        "link": "https://openreview.net/forum?id=uWtLOy35WD",
        "pdf_link": "https://openreview.net/pdf?id=uWtLOy35WD",
        "keywords": "MLLM, MoE, Distillation",
        "abstract": "We introduce LLaVA-MoD, a novel framework designed to enable the efficient training of small-scale Multimodal Language Models ($s$-MLLM) distilling knowledge from large-scale MLLM ($l$-MLLM). Our approach tackles two fundamental challenges in MLLM distillation. First, we optimize the network structure of $s$-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the language model, striking a balance between computational efficiency and model expressiveness. Second, we propose a progressive knowledge transfer strategy for comprehensive knowledge transfer. This strategy begins with mimic distillation, where we minimize the Kullback-Leibler (KL) divergence between output distributions to enable $s$-MLLM to emulate $s$-MLLM's understanding. Following this, we introduce preference distillation via Preference Optimization (PO), where the key lies in treating $l$-MLLM as the reference model. During this phase, the $s$-MLLM's ability to discriminate between superior and inferior examples is significantly enhanced beyond $l$-MLLM, leading to a better $s$-MLLM that surpasses $l$-MLLM, particularly in hallucination benchmarks.\nExtensive experiments demonstrate that LLaVA-MoD surpasses existing works across various benchmarks while maintaining a minimal activated parameters and low computational costs. Remarkably, LLaVA-MoD-2B surpasses Qwen-VL-Chat-7B with an average gain of 8.8%, using merely $0.3%$ of the training data and 23% trainable parameters. The results underscore LLaVA-MoD's ability to effectively distill comprehensive knowledge from its teacher model, paving the way for developing efficient MLLMs."
    },
    {
        "title": "ShuffleNorm: A Better Normalization for Semi-supervised Learning",
        "link_suffix": "/forum?id=qI1gmHbs0Z",
        "link": "https://openreview.net/forum?id=qI1gmHbs0Z",
        "pdf_link": "https://openreview.net/pdf?id=qI1gmHbs0Z",
        "keywords": "Semi-supervised Learning; Normalization",
        "abstract": "We identify critical challenges with normalisation layers commonly used in fully supervised learning when applied to semi-supervised settings. Specifically, batch normalisation (BN) can experience severe performance degradation when labelled and unlabelled data have mismatched label distributions, due to biased statistical estimation. This results in unstable gradients, hindering the model's ability to converge effectively. While group/layer normalisation (GN/LN) avoids these issues, it lacks the stochastic regularisation provided by BN, leading to weaker generalisation. Poor generalisation, in turn, produces low-quality pseudo-labels, exacerbating confirmation bias. To address these limitations, we propose novel normalisation techniques termed Shuffle Layer normalisation and Shuffle Group normalisation (SLN/SGN) that introduce controllable randomness into LN/GN without increasing model parameters, thus making semi-supervised learning more robust and effective. Through experiments across diverse datasets, including image, text, and audio modalities, we demonstrate that SLN/SGN significantly enhances the performance of state-of-the-art semi-supervised learning algorithms."
    },
    {
        "title": "LeGrad: An Explainability Method for Vision Transformers via Feature Formation Sensitivity",
        "link_suffix": "/forum?id=El4Cs8Su3r",
        "link": "https://openreview.net/forum?id=El4Cs8Su3r",
        "pdf_link": "https://openreview.net/pdf?id=El4Cs8Su3r",
        "keywords": "segmentation;gradient-based method",
        "abstract": "Vision Transformers (ViTs) have become a standard architecture in computer vision. However, because of their modeling of long-range dependencies through self-attention mechanisms, the explainability of these models remains a challenge.\nTo address this, we propose LeGrad, an explainability method specifically designed for ViTs. \nLeGrad computes the gradient with respect to the attention maps of single ViT layers, considering the gradient itself as the explainability signal.\nWe aggregate the signal over all layers, combining the activations of the last as well as intermediate tokens to produce the merged explainability map.\nThis makes LeGrad a conceptually simple and an easy-to-implement method to enhance the transparency of ViTs. \nWe evaluate LeGrad in various setups, including segmentation, perturbation, and open-vocabulary settings, showcasing its improved spatial fidelity as well as its versatility compared to other SotA explainability methods."
    }
]