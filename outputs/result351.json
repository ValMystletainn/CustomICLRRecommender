[
    {
        "title": "Hallucinating LLM Could Be Creative",
        "link_suffix": "/forum?id=W48CPXEpXR",
        "link": "https://openreview.net/forum?id=W48CPXEpXR",
        "pdf_link": "https://openreview.net/pdf?id=W48CPXEpXR",
        "keywords": "LLM, Hallucination, Creativity in AI",
        "abstract": "Large Language Models (LLMs), such as GPT-4o, frequently produce hallucinations\u2014factually incorrect or nonsensical outputs generally regarded as undesirable. This study, however, explores the notion of \u201cgood\u201d hallucinations that may contribute to creativity and innovation. We propose metrics to assess hallucination quality, focusing on correctness, consistency, and reasoning diversity, which are evaluated using sample responses and semantic clustering. Our experiments explore different prompting techniques and hyperparameter configurations to provide comprehensive results based on these metrics. Furthermore, we investigate the distinction between process and outcome supervision, using multiple reasoning paths to enhance both creativity and accuracy. Preliminary results indicate that LLMs can generate creative hallucinations with minimal factual inaccuracies. This research provides a refined perspective on hallucinations in LLMs and suggests strategies to harness their creative potential, improving the reliability and flexibility of AI systems."
    },
    {
        "title": "Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive Fine-tuning",
        "link_suffix": "/forum?id=XnDyddPcBT",
        "link": "https://openreview.net/forum?id=XnDyddPcBT",
        "pdf_link": "https://openreview.net/pdf?id=XnDyddPcBT",
        "keywords": "neural ODEs, transformer, adaptive finetune",
        "abstract": "Recent advancements in large language models (LLMs) based on transformer architectures have sparked significant interest in understanding their inner workings. In this paper, we introduce a novel approach to modeling transformer architectures using highly flexible non-autonomous neural ordinary differential equations (ODEs). Our proposed model fully parameterizes all the weights of attention and feed-forward blocks through neural networks, with weights articulated as functions of a continuous layer index. We examine the model's dynamics through spectral analysis, uncovering an increase in eigenvalue magnitude offering a practical insights against weight-sharing assumption in existing theoretical studies. We also introduce the use of the Lyapunov exponent to examine token-level sensitivity, improving model interpretability. Our neural ODE transformer performs similarly to GPT across various configurations and datasets, while offering flexible fine-tuning capabilities under different architectures."
    },
    {
        "title": "Reinitializing weights vs hidden units for maintaining plasticity in neural networks",
        "link_suffix": "/forum?id=ffuHn3Q6Hc",
        "link": "https://openreview.net/forum?id=ffuHn3Q6Hc",
        "pdf_link": "https://openreview.net/pdf?id=ffuHn3Q6Hc",
        "keywords": "Continual Learning, Supervised Learning, Deep Learning, Loss of Plasticity",
        "abstract": "Loss of plasticity is a phenomenon where a neural network loses its ability to learn when trained for an extended time on non-stationary data.\nIt is a crucial problem to overcome when designing systems that learn continually.\nAn effective technique for preventing loss of plasticity is reinitializing parts of the network.\nIn this paper, we compare two different reinitialization schemes: reinitializing units vs reinitializing weights.\nWe propose a new algorithm named \\textit{selective weight reinitialization} for reinitializing the least useful weights in the network. \nWe compare our algorithm to continual backpropagation, a previously proposed algorithm that reinitializes units.\nThrough our experiments in continual supervised learning problems, we identify two settings when reinitializing weights is more effective at maintaining plasticity than reinitializing units: (1) when the network has a small number of units and (2) when the network includes layer normalization.\nConversely, reinitializing weights and units are equally effective at maintaining plasticity when the network is of sufficient size and does not include layer normalization. \nWe found that reinitializing weights maintains plasticity in a wider variety of settings than reinitializing units."
    },
    {
        "title": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws",
        "link_suffix": "/forum?id=gjC3QvVh1U",
        "link": "https://openreview.net/forum?id=gjC3QvVh1U",
        "pdf_link": "https://openreview.net/pdf?id=gjC3QvVh1U",
        "keywords": "Scaling Laws, Reinforcement Learning, Zipf's Law",
        "abstract": "Neural scaling laws are observed in a range of domains, to date with no clear understanding of why they occur. For language scaling, a recent theory suggests that loss power laws arise from Zipf's law, a power law observed in natural language. If Zipf-distributed task quanta  are learned in descending order of frequency, language scaling laws emerge. In this paper we connect power-law scaling in AlphaZero, a reinforcement learning algorithm, to the theory of language-model scaling. We find that states in training and inference data scale with Zipf's law, which arises from the tree structure of the environment, and examine the correlation between scaling-law  and Zipf's-law exponents. In agreement with language-scaling theory, we find that agents optimize state loss in descending order of frequency, even though this order scales inversely with modelling complexity. We also find that inverse scaling, the failure of models to improve with size, is correlated with unusual Zipf curves where end-game states are among the most frequent states. We show evidence that larger models shift their focus to these less-important states, sacrificing their understanding of important early-game states."
    },
    {
        "title": "FMP-AE: A HYBRID APPROACH TO TIME SERIES ANOMALY DETECTION",
        "link_suffix": "/forum?id=fErm1seIom",
        "link": "https://openreview.net/forum?id=fErm1seIom",
        "pdf_link": "https://openreview.net/pdf?id=fErm1seIom",
        "keywords": "Anomaly detection; Matrix Profile; Deep learning;",
        "abstract": "Unsupervised anomaly detection in time series presents significant challenges, particularly due to the lack of clear criteria and the prevalence of highly imbalanced data. Traditional statistical and machine learning methods often struggle with low recall rates and computational inefficiency. While deep learning techniques offer the advantage of automatic feature extraction, they are also affected by the issue of data imbalance. This paper introduces an integrated time series anomaly detection model, Feature map Matrix Profile with an AutoEncoder (FMP-AE), which combines matrix profile structures with deep learning techniques. The model leverages a one-dimensional convolutional neural network (1D-CNN) to extract features and compute the matrix profile. Then a novel Matrix Profile loss function is defined and integrated with the Autoencoder's reconstruction loss for model training and anomaly detection. Experimental results on the UCR250 benchmark datasets highlight the model's impressive performance, showing notable success across various metrics, including accuracy, precision, recall, F1-score, and AUC. These findings indicate that the hybrid FMP-AE model, significantly improves accuracy, robustness, and computational efficiency in anomaly detection tasks."
    },
    {
        "title": "Spiking Transformer-CNN for Event-based Object Detection",
        "link_suffix": "/forum?id=zweyouirw7",
        "link": "https://openreview.net/forum?id=zweyouirw7",
        "pdf_link": "https://openreview.net/pdf?id=zweyouirw7",
        "keywords": "Event data, Object detection, Spike neural networks, Low power consumption, Transformer-CNN",
        "abstract": "Spiking Neural Networks (SNNs) enable energy-efficient computation through event-driven computing and multiplication-free inference, making them well-suited for processing sparse events. Recently, deep Spiking Convolutional Neural Networks (CNNs) have shown energy efficiency advantages on event-based object detection. However, spiking CNNs have been limited to local and single-scale features, making it challenging for them to achieve better detection accuracy. To address this challenge, we propose a hierarchical Spiking Transformer-CNN (i.e., Spike-TransCNN) architecture, which is the first attempt to leverage the global information extraction capabilities of Spiking Transformers and the local information capture abilities of Spiking CNNs for event-based object detection. Technically, we first propose using the Spiking Transformer to extract global features and employ a multi-scale local feature extraction CNN module to complement the Spiking Transformers in local feature extraction. Then, we design intra-stage and inter-stage feature fusion modules to integrate global and multi-scale local features within the network architecture. Experimental results demonstrate that our Spike-TransCNN significantly outperforms existing SNN-based object detectors on the Gen1 dataset, achieving higher detection accuracy (mAP 0.336 vs. 0.321) with lower energy consumption (5.49 mJ vs. 7.26 mJ). Our code can be available in the supplementary materials."
    },
    {
        "title": "Communication-efficient Algorithms Under Generalized Smoothness Assumptions",
        "link_suffix": "/forum?id=Z4s2oe3Oiq",
        "link": "https://openreview.net/forum?id=Z4s2oe3Oiq",
        "pdf_link": "https://openreview.net/pdf?id=Z4s2oe3Oiq",
        "keywords": "Error Feedback; Distributed Learning; Generalized Smoothness; Nonconvex Optimization",
        "abstract": "We provide the first proof of convergence for normalized error feedback algorithms across a wide range of machine learning problems. \nDespite their popularity and efficiency in training deep neural networks, traditional analyses of error feedback algorithms rely on the smoothness assumption that does not capture the properties of objective functions in these  problems. \nRather, these problems have recently been shown to satisfy generalized smoothness assumptions, and the theoretical understanding of error feedback algorithms under these assumptions remains largely unexplored. \nMoreover, to the best of our knowledge, all existing analyses under generalized smoothness either i) focus on single-node settings or ii) make unrealistically strong assumptions for distributed settings, such as requiring data heterogeneity, and almost surely bounded stochastic gradient noise variance. \nIn this paper, we propose distributed error feedback algorithms that utilize normalization to achieve the $\\mathcal{O}(1/\\sqrt{K})$ convergence rate for nonconvex problems under generalized smoothness. Our analyses apply for distributed settings without data heterogeneity conditions, and enable stepsize tuning that is independent of problem parameters. \nAdditionally, we provide strong convergence guarantees of normalized error feedback algorithms for stochastic settings. \nFinally, we show that normalized EF21, due to its larger allowable stepsizes, outperforms EF21 on various tasks, including the minimization of polynomial functions, logistic regression, and ResNet-20 training."
    },
    {
        "title": "A Temporally Correlated Latent Exploration for Reinforcement Learning",
        "link_suffix": "/forum?id=uGka5qOsop",
        "link": "https://openreview.net/forum?id=uGka5qOsop",
        "pdf_link": "https://openreview.net/pdf?id=uGka5qOsop",
        "keywords": "Colored Noise, Intrinsic Reward, Latent State Representation, Reinforcement Learning, Temporal Correlation",
        "abstract": "Efficient exploration remains one of the longstanding problems of deep reinforcement learning. Instead of depending solely on extrinsic rewards from the environments, existing methods use intrinsic rewards to enhance exploration. However, we demonstrate that these methods are vulnerable to Noisy TV and stochasticity. To tackle this problem, we propose Temporally Correlated Latent Exploration (TeCLE), which is a novel intrinsic reward formulation that employs an action-conditioned latent space and temporal correlation. The action-conditioned latent space models the probability distribution of states, thereby avoiding the assignment of excessive intrinsic rewards to unpredictable states and effectively addressing both problems. Whereas previous works inject temporal correlation for action selection, the proposed method injects it for intrinsic reward computation. We find that the injected temporal correlation determines the exploratory behaviors of agents. Various experiments show that the environment where the agent performs well depends on the amount of temporal correlation. To the best of our knowledge, the proposed TeCLE is the first approach to consider the action-conditioned latent space and temporal correlation for curiosity-driven exploration. We prove that the proposed TeCLE can be robust to the Noisy TV and stochasticity in benchmark environments, including Minigrid and Stochastic Atari."
    },
    {
        "title": "Harnessing Shallow Features in Pre-Trained Models for Out-of-Distribution Detection",
        "link_suffix": "/forum?id=UTnq6hJJYa",
        "link": "https://openreview.net/forum?id=UTnq6hJJYa",
        "pdf_link": "https://openreview.net/pdf?id=UTnq6hJJYa",
        "keywords": "out-of-distribution detection, long-tail learning",
        "abstract": "Recognizing out-of-distribution (OOD) samples is essential for deploying robust machine learning systems in the open-world environments. Conventional OOD detection approaches rely on feature representations from the final layer of neuron networks, often neglecting the rich information encapsulated in shallow layers. Leveraging the strengths of transformer-based architectures, we introduce an attention-based fusion module, which dynamically assigns importance weights to representations learned by each Transformer layer and detects OOD samples using the Mahalanobis distance. Compared to existing approaches, our method enables a lightweight fine-tuning of pre-trained models, and retains all feature representations that are beneficial to the OOD detection. We also thoroughly study various parameter-efficient fine-tuning strategies. Our experiments show the benefit of using shallow features, and demonstrate the influence of different Transformer layers. We fine-tune pre-trained models in both class-balanced and long-tailed in-distribution classification tasks, and show that our method achieves state-of-the-art OOD detection performance averaged across nine OOD datasets. The source code is provided in the supplementary material."
    },
    {
        "title": "A PATCH LEVEL PERSPECTIVE OF PROMPT TUNING",
        "link_suffix": "/forum?id=dGMJ93qpfq",
        "link": "https://openreview.net/forum?id=dGMJ93qpfq",
        "pdf_link": "https://openreview.net/pdf?id=dGMJ93qpfq",
        "keywords": "Vision Language models, Efficient Adaptation, Prompt Tuning, Few-shot learning",
        "abstract": "Prompt tuning is an efficient way to adapt large foundation models, such as CLIP, by introducing learnable prompts with the input data tokens, offering a practical alternative to full model finetuning. However, when prompts are trained on base/target tasks, they often overfit, leading to reduced performance on novel, unseen tasks. To address this limitation, various techniques leverage global image semantics to improve accuracy on unseen tasks while maintaining performance on base tasks. However, they often overlook the rich fine-grained local information that could be crucial for capturing finer semantics and improving generalisation. In this work, we propose a modular approach to prompt tuning that leverages local semantics by incorporating patch-level information, representing the first integration of such semantics in this context. Specifically, we integrate patch-level information across vision, text, and predictions through three consistency mechanisms: 1) Patch-based consistency loss that aligns patches from the prompted input image with those from the same image processed by a frozen model, while also enforcing inter-view consistency by applying the loss across different views, capturing fine-grained regional dependencies and improving vision representation quality, 2) Text prompt consistency loss, where view-specific text prompts are tailored and regularised to maintain coherence across views, and 3) Vision features for each view, enriched with patch-level information, are used to generate predictions based on view-tailored text features. These predictions are then regularised across views, complementing the earlier consistency mechanisms and contributing to a cohesive overall framework. Our approach outperforms existing methods across multiple benchmarks, including base-to-novel generalisation, domain generalisation, and cross-dataset evaluation. These results underscore the potential of integrating fine-grained details for more robust and adaptable prompts, marking a step forward in foundation model tuning."
    },
    {
        "title": "Tilted Losses in Training Quantum Neural Networks",
        "link_suffix": "/forum?id=erowpbZcPi",
        "link": "https://openreview.net/forum?id=erowpbZcPi",
        "pdf_link": "https://openreview.net/pdf?id=erowpbZcPi",
        "keywords": "Quantum machine learning, exponential tilting, empirical risk minimization",
        "abstract": "Empirical risk minimization is a fundamental paradigm in the optimization process of machine learning (ML) models. Several techniques extend this idea by introducing parameters which further regularize this strategy in training these models. One of these paradigms is the so-called tilted empirical risk minimization (TERM), which uses a tilted hyperparameter to penalize the presence of outliers, which represent data samples that differ significantly from the rest of the dataset. Quantum machine learning (QML) models have been studied and benchmarked across various criteria stemming from classical ML, including their training via the parameter-shift rule. Therefore, it is natural to extend the concept of TERM in training QML models, namely the type of models known as quantum neural networks (QNNs). In this work, we examine the impact of a tilted loss function in training a class of QNNs, specifically for binary classification tasks involving two different datasets with induced class imbalance. In the first dataset, the Iris dataset, we show that varying the value of the tilted hyperparameter modifies the decision boundary leading to reduced importance of outliers and better training accuracy --- highlighting the importance of using tilted risk minimization. Additionally, in a synthetic dataset we validate that the training accuracy can be improved using the tilted parameter. Analytically, we extend the parameter-shift training method to accommodate weighted inputs by introducing the tilted hyperparameter for training QNNs. These results highlight the significance of incorporating regularization techniques from ML models into QML models."
    },
    {
        "title": "Steering 3D Molecule Generation in Data-Sparse Regions via Distributional Physical Priors",
        "link_suffix": "/forum?id=an3kPpce6b",
        "link": "https://openreview.net/forum?id=an3kPpce6b",
        "pdf_link": "https://openreview.net/pdf?id=an3kPpce6b",
        "keywords": "Molecule Generation, Diffusion Model, Out of distribution",
        "abstract": "Can we train a 3D molecule generator using data from dense regions to generate samples in sparse regions? This challenge can be framed as an out-of-distribution (OOD) generation problem. Existing works on OOD generation primarily focus on property shifts. However, the distribution shifts may come from structural variations in molecules, such as certain types of scaffolds, dubbed as physical priors. This work introduces a novel and principled diffusion-based generative framework, termedGODD, which enables training a generator on data-abundant distributions to generalize to data-scarce distributions under structure shifts. Specifically, we propose utilizing a designated equivariant asymmetric autoencoder to capture distributional physical priors. The asymmetric module allows generalization to unseen, out-of-distribution structural variations. As these captured physical priors represent distinct distributions, they can steer the generation of samples that are not in dense regions. We demonstrate that with these encoded structural-grained distributional physical priors,GODDdoes not need to train with any molecules from the sparse regions. We conduct extensive experiments across various out-of-distribution molecule generation tasks using benchmark datasets. Compared to alternative baselines, our approach shows a significant improvement of up to 65.6% in success rate, defined based on molecular validity, uniqueness, and novelty. Additionally, we show that our generative framework, steered by physical priors, can be readily adapted to canonical fragment-based drug design tasks, exhibiting promising performance."
    },
    {
        "title": "BiQAP: Neural Bi-level Optimization-based Framework for Solving Quadratic Assignment Problems",
        "link_suffix": "/forum?id=vYBzgwkwZb",
        "link": "https://openreview.net/forum?id=vYBzgwkwZb",
        "pdf_link": "https://openreview.net/pdf?id=vYBzgwkwZb",
        "keywords": "Quadratic Assignment Problems, Entropic Regularization, Differential Gromov-Wasserstein Solver, Unsupervised Learning",
        "abstract": "Quadratic Assignment Problem (QAP) has attracted lasting attention for its wide applications and computational challenge. Despite the rich literature in machine learning for QAP, most works often address the problem in the setting of image matching, whereby deep networks could play a vital role in extracting useful features for the subsequent matching. While its power on pure numerical QAP instances is limited in node embedding, often with a vanilla graph neural network. This paper tries to tap the potential of deep nets for QAP, specifically by modifying the input instance which is orthogonal to previous efforts. Specifically, we develop a bi-level  unsupervised framework, where the inner optimization involves trying to solve the modified instance with entropic regularization that can be solved iteratively using the Sinkhorn algorithm without affecting backpropagation by truncating gradients during training. The outer minimization deals with the quadratic objective function of the original QAP. In particular, seeing the intractable scale of the most general form i.e. Lawler's QAP and the practical utility of the more efficient Koopmans-Beckmann QAP (KBQAP) form for solving other graph and combinatorial problems like TSP and graph edit distance, we embody our network on the KBQAP, and show its strong performance on various benchmarks in our experiments. Source code will be made publicly available."
    },
    {
        "title": "Large Language Models based Graph Convolution for Text-Attributed Networks",
        "link_suffix": "/forum?id=x5FfUvsLIE",
        "link": "https://openreview.net/forum?id=x5FfUvsLIE",
        "pdf_link": "https://openreview.net/pdf?id=x5FfUvsLIE",
        "keywords": "Text attributed graphs, Long-context model",
        "abstract": "Text-attributed graph (TAG) tasks involve analyzing both structural information and textual attributes. Existing methods employ text embeddings as node features, and leverage structural information by employing Graph Neural Networks (GNNs) to aggregate features from neighbors. These approaches demand substantial computational resources and rely on two cascaded stages, limiting scalability in large-scale scenarios and making them vulnerable to the influence of irrelevant neighboring nodes. The advancement of language models (LMs) presents new avenues for tackling this task without GNNs, leveraging their ability to process text attributes of both the target node and its important neighbors. Instead of using graph convolution modules, LMs can assign weights to these tokens based on relevance, enabling token-level weighted summarization. However, it is nontrivial to directly employ LMs for TAG tasks because assessing the importance of neighbor nodes involves both semantic and structural considerations. Additionally, the large search space presents efficiency issues for computing importance scores in a scalable manner.\nTo this end, we propose a novel semantic knowledge and Structural Enrichment framework, namely SKETCH, to adapt LMs for TAG tasks by retrieving both structural and text-related content. Specifically, we propose a retrieval model that identifies neighboring nodes exhibiting similarity to the target node across two dimensions: structural similarity and text similarity. To enable efficient retrieval, we introduce a hash-based common neighbor estimation algorithm for structural similarity and a nearest-neighbor recalling algorithm for embedding similarity. These two similarity measures are then aggregated using a weighted rank aggregation mechanism. The text attributes of both the retrieved nodes and the target node provide effective descriptions of the target node and are used as input for the LM predictor. Extensive experiments demonstrate that SKETCH can outperform other baselines on three datasets with fewer resources."
    },
    {
        "title": "BetterBodies: Reinforcement Learning guided Diffusion for Antibody Sequence Design",
        "link_suffix": "/forum?id=eluQFbNeuH",
        "link": "https://openreview.net/forum?id=eluQFbNeuH",
        "pdf_link": "https://openreview.net/pdf?id=eluQFbNeuH",
        "keywords": "Reinforcement Learning, Generative Models, Diffusion Models, Antibody Design",
        "abstract": "Antibodies offer great potential for the treatment of various diseases. However, the discovery of therapeutic antibodies through traditional wet lab methods is expensive and time-consuming. The use of generative models in designing antibodies therefore holds great promise, as it can reduce the time and resources required. Recently, the class of diffusion models has gained considerable traction for their ability to synthesize diverse and high-quality samples. In their basic form, however, they lack mechanisms to optimize for specific properties, such as binding affinity to an antigen. In contrast, the class of offline Reinforcement Learning (RL) methods has demonstrated strong performance in navigating large search spaces, including scenarios where frequent real-world interaction, such as interaction with a wet lab, is impractical. Our novel method, BetterBodies, which combines Variational Autoencoders (VAEs) with offline RL guided latent diffusion, can generate novel sets of antibody CDRH3 sequences from different data distributions. Furthermore, we reflect biophysical properties in the VAE latent space using a contrastive loss and add a novel Q-function based filtering to enhance the affinity of generated sequences. Using the Absolut! simulator, we demonstrate that BetterBodies generates sequences with improved binding affinity to the SARS-CoV spike receptor-binding domain and matches or outperforms the state-of-the-art method Generative Flow Network (GFlowNet). In conclusion, our method has the potential for great implications in real-world biological sequence design, where the generation of novel high-affinity binders is a cost-intensive endeavor."
    },
    {
        "title": "PSformer: Parameter-efficient Transformer with Segment Attention for Time Series Forecasting",
        "link_suffix": "/forum?id=PQzZrRNynC",
        "link": "https://openreview.net/forum?id=PQzZrRNynC",
        "pdf_link": "https://openreview.net/pdf?id=PQzZrRNynC",
        "keywords": "Time Series Forecasting, Transformer, Parameter Sharing",
        "abstract": "Time series forecasting remains a critical challenge across various domains, often complicated by high-dimensional data and long-term dependencies. This paper presents a novel transformer architecture for time series forecasting, incorporating two key innovations: parameter sharing (PS) and Spatial-Temporal Segment Attention (SegAtt). We also define the time series segment as the concatenation of sequence patches from the same positions across different variables. The proposed model, PSformer, reduces the number of training parameters through the parameter sharing mechanism, thereby improving model efficiency and scalability. The introduction of SegAtt could enhance the capability of capturing local spatio-temporal dependencies by computing attention over the segments, and improve global representation by integrating information across segments. The combination of parameter sharing and SegAtt significantly improves the forecasting performance. Extensive experiments on benchmark datasets demonstrate that PSformer outperforms popular baselines and other transformer-based approaches in terms of accuracy and scalability, establishing itself as an accurate and scalable tool for time series forecasting."
    },
    {
        "title": "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts",
        "link_suffix": "/forum?id=e1wDDFmlVu",
        "link": "https://openreview.net/forum?id=e1wDDFmlVu",
        "pdf_link": "https://openreview.net/pdf?id=e1wDDFmlVu",
        "keywords": "time series, foundation model, forecasting",
        "abstract": "Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility."
    },
    {
        "title": "MaskTwins: Dual-form Complementary Masking for Domain-Adaptive Image Segmentation",
        "link_suffix": "/forum?id=wJGXiHQwpZ",
        "link": "https://openreview.net/forum?id=wJGXiHQwpZ",
        "pdf_link": "https://openreview.net/pdf?id=wJGXiHQwpZ",
        "keywords": "domain adaptation, unsupervised learning, masked image modeling, semantic segmentation, complementary masking",
        "abstract": "Recent works have correlated Masked Image Modeling (MIM) with consistency regularization in unsupervised domain adaptation. However, they merely treat masking as a special form of deformation on the input images and neglect the theoretical analysis, which leads to a superficial understanding of masked reconstruction and insufficient exploitation of its potential in enhancing feature extraction and representation learning. In this paper, we reframe masked reconstruction as a sparse signal reconstruction problem and theoretically prove that the dual form of complementary masks possesses superior capabilities in extracting domain-agnostic image features. Based on this compelling insight, we propose MaskTwins, a simple yet effective learning strategy that integrates masked reconstruction directly into the main training pipeline. MaskTwins uncovers intrinsic structural patterns that persist across disparate domains by enforcing consistency between predictions of images masked in complementary ways, enabling domain generalization in an end-to-end manner. Extensive experiments verify the superiority of MaskTwins over baseline methods in natural and biological image segmentation. These results demonstrate the significant advantages of MaskTwins in extracting domain-invariant features without the need for separate pre-training, offering a new paradigm for domain-adaptive segmentation."
    },
    {
        "title": "Typography Leads Semantic Diversifying: Amplifying Adversarial Transferability across Multimodal Large Language Models",
        "link_suffix": "/forum?id=vF4RhEPGtb",
        "link": "https://openreview.net/forum?id=vF4RhEPGtb",
        "pdf_link": "https://openreview.net/pdf?id=vF4RhEPGtb",
        "keywords": "Adversarial Transferability; Multimodal Large Language Models; Data Augmentation",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) achieve remarkable performance in numerous zero-shot tasks due to their outstanding cross-modal interaction and comprehension abilities. However, MLLMs are found to still be vulnerable to human-imperceptible adversarial examples. In the exploration of security vulnerabilities in real-world scenarios, transferability, which can achieve cross-model impact, is considered the greatest threat posed by adversarial examples. However, there is currently no systematic research on the threat of cross-MLLMs adversarial transferability. Therefore, this paper as the first step to provide a comprehensive evaluation of the transferability of adversarial examples generated by various MLLMs. Furthermore, leveraging two key factors that influence transferability performance: 1) The strength of information diversity involved in the adversarial generation process; 2) Editing across vision-language modality information. We propose a boosting method called Typography Augment Transferability Method (TATM) to investigate the adversarial transferability performance across MLLMs further. Through extensive experimental validation, our TATM demonstrates exceptional performance in real-world applications of \"Harmful Word Insertion\" and \"Important Information Protection.\""
    },
    {
        "title": "Diss-l-ECT: Dissecting Graph Data with local Euler Characteristic Transforms",
        "link_suffix": "/forum?id=3cnXu5iIP5",
        "link": "https://openreview.net/forum?id=3cnXu5iIP5",
        "pdf_link": "https://openreview.net/pdf?id=3cnXu5iIP5",
        "keywords": "topology, geometry, topological data analysis, graph learning, node classification, spatial alignment, interpretable graph learning",
        "abstract": "The Euler Characteristic Transform (ECT) is an efficiently-computable\n    geometrical-topological invariant that characterizes the global shape of data. \n    In this paper, we introduce the Local Euler Characteristic Transform (l-ECT), a novel extension of the ECT particularly designed to enhance expressivity and interpretability in graph representation learning.\n    Unlike traditional Graph Neural Networks (GNNs), which may lose critical local details through aggregation, the l-ECT provides a lossless representation of local neighborhoods.\n    This approach addresses key limitations in GNNs by preserving nuanced local structures while maintaining global interpretability.\n    Moreover, we construct a rotation-invariant metric based on l-ECTs for spatial alignment of data spaces.\n    Our method exhibits superior performance than standard GNNs on a variety of node classification tasks, particularly in graphs with high heterophily."
    },
    {
        "title": "Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings",
        "link_suffix": "/forum?id=GrDne4055L",
        "link": "https://openreview.net/forum?id=GrDne4055L",
        "pdf_link": "https://openreview.net/pdf?id=GrDne4055L",
        "keywords": "Out-of-Distribution Detection, Adversarial Robustness, Stability Theorem, Neural Ordinary Differential Equations, Lyapunov Theorem",
        "abstract": "Despite significant advancements in out-of-distribution (OOD) detection, existing methods still struggle to maintain robustness against adversarial attacks, compromising their reliability in critical real-world applications. Previous studies have attempted to address this challenge by exposing detectors to auxiliary OOD datasets alongside adversarial training. However, the increased data complexity inherent in adversarial training, and the myriad of ways that OOD samples can arise during testing, often prevent these approaches from establishing robust decision boundaries. To address these limitations, we propose AROS, a novel approach leveraging neural ordinary differential equations (NODEs) with Lyapunov stability theorem in order to obtain robust embeddings for OOD detection.\nBy incorporating a tailored loss function, we apply Lyapunov stability theory to ensure that both in-distribution (ID) and OOD data converge to stable equilibrium points within the dynamical system. This approach encourages any perturbed input to return to its stable equilibrium, thereby enhancing the model\u2019s robustness against adversarial perturbations. To not use additional data, we generate fake OOD embeddings by sampling from low-likelihood regions of the ID data feature space, approximating the boundaries where OOD data are likely to reside. To then further enhance robustness, we propose the use of an orthogonal binary layer following the stable feature space, which maximizes the separation between the equilibrium points of ID and OOD samples. We validate our method through extensive experiments across several benchmarks, demonstrating superior performance, particularly under adversarial attacks. Notably, our approach improves robust detection performance from 37.8% to 80.1% on CIFAR-10 vs. CIFAR-100 and from 29.0% to 67.0% on CIFAR-100 vs. CIFAR-10."
    },
    {
        "title": "Shape as Line Segments: Accurate and Flexible Implicit Surface Representation",
        "link_suffix": "/forum?id=RavSZTIe2s",
        "link": "https://openreview.net/forum?id=RavSZTIe2s",
        "pdf_link": "https://openreview.net/pdf?id=RavSZTIe2s",
        "keywords": "Implicit representation, 3D geometric, surface reconstruction",
        "abstract": "Distance field-based implicit representations like signed/unsigned distance fields have recently gained prominence in geometry modeling and analysis. However, these distance fields are reliant on the closest distance of points to the surface, introducing inaccuracies when interpolating along cube edges during surface extraction. Additionally, their gradients are ill-defined at certain locations, causing distortions in the extracted surfaces. To address this limitation, we propose Shape as Line Segments (SALS), an accurate and efficient implicit geometry representation based on attributed line segments, which can handle arbitrary structures. Unlike previous approaches, SALS leverages a differentiable Line Segment Field to implicitly capture the spatial relationship between line segments and the surface. Each line segment is associated with two key attributes, intersection flag and ratio, from which we propose edge-based dual contouring to extract a surface. We further implement SALS with a neural network, producing a new neural implicit presentation. Additionally, based on SALS, we design a novel learning-based pipeline for reconstructing surfaces from 3D point clouds. We conduct extensive experiments, showcasing the significant advantages of our methods over state-of-the-art methods. We have included the source code in the Supplemental Material."
    },
    {
        "title": "MoS: Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards",
        "link_suffix": "/forum?id=1uLW9eYNJB",
        "link": "https://openreview.net/forum?id=1uLW9eYNJB",
        "pdf_link": "https://openreview.net/pdf?id=1uLW9eYNJB",
        "keywords": "LoRA, parameter efficiency, parameter sharing, instruction tuning, NLP",
        "abstract": "The rapid scaling of large language models necessitates more lightweight finetuning methods to reduce the explosive GPU memory overhead when numerous customized models are served simultaneously.\nTargeting more parameter-efficient low-rank adaptation (LoRA), parameter sharing presents a promising solution. Empirically, our research into high-level sharing principles highlights the indispensable role of differentiation in reversing the detrimental effects of pure sharing.\nGuided by this finding, we propose Mixture of Shards (MoS), incorporating both inter-layer and intra-layer sharing schemes, and integrating four nearly cost-free differentiation strategies, namely subset selection, pair dissociation, vector sharding, and shard privatization. Briefly, it selects a designated number of shards from global pools with a Mixture-of-Experts (MoE)-like routing mechanism before sequentially concatenating them to low-rank matrices.\nHence, it retains all the advantages of LoRA while offering enhanced parameter efficiency, and effectively circumvents the drawbacks of peer parameter-sharing methods.\nOur empirical experiments demonstrate approximately $8\\times$ parameter savings in a standard LoRA setting. The ablation study confirms the significance of each component.\nOur insights into parameter sharing and MoS method may illuminate future developments of more parameter-efficient finetuning methods."
    },
    {
        "title": "Stabilized Neural Dynamics for Behavioral Decoding via Hierarchical Domain Adaptation",
        "link_suffix": "/forum?id=LNp7KW33Cg",
        "link": "https://openreview.net/forum?id=LNp7KW33Cg",
        "pdf_link": "https://openreview.net/pdf?id=LNp7KW33Cg",
        "keywords": "Unsupervised Domain Adaptation, Brain-Computer Interface, Neural Dynamics, Lyapunov Theory",
        "abstract": "Brain-Computer Interfaces (BCI) have demonstrated significant potential in neural rehabilitation. However, the variability of non-stationary neural signals often leads to instabilities of behavioral decoding, posing critical obstacles to chronic applications. Domain adaptation technique offers a promising solution. Nonetheless, the existing direct adaptation within latent spaces could result in feature deviations. Therefore, developing a stable and efficient alignment framework is crucial for neural decoders.\nIn this work, we find that dynamical latent features can be extracted from neural dynamics utilizing causal architectures. \nWe also demonstrate that the process of self-consistent alignment can generate more stable latent features.\nBased on these insights, we propose a novel hierarchical domain adaptation (HDA) method for the alignment of dynamical latent features.\nUsing Lyapunov theory, we further analytically validate the stability of dynamical features, which experimentally exhibit significant enhancements across various datasets.\nOur HDA approach effectively addresses the challenge of non-stationary neural signals, thereby potentially improving the reliability of BCIs."
    },
    {
        "title": "InteractiveCOT: Aligning Dynamic Chain-of-Thought Planning for Embodied Decision-Making",
        "link_suffix": "/forum?id=Y4iaDU4yMi",
        "link": "https://openreview.net/forum?id=Y4iaDU4yMi",
        "pdf_link": "https://openreview.net/pdf?id=Y4iaDU4yMi",
        "keywords": "Embodied Agent, Multi-modal Large Model, Chain-of-Thought, Planning, Reinforcement Learning, Preference Learning",
        "abstract": "Vision-Language Models (VLMs) are increasingly being employed as the decision-making \"brains\" of embodied agents. Effectively harnessing their powerful generalization capabilities in dynamic, context-specific tasks remains a significant challenge. Chain-of-Thought (CoT) prompting is often utilized for complex task execution, but existing methods either rely on static strategies that fail to adapt to changing environments or fine-tune on offline datasets, which are insufficient for optimizing agent decision-making through interaction.\nIn this paper, we propose a novel approach that focuses on optimizing the CoT reasoning process rather than just the final action tokens. By aligning the CoT process through preference-based reinforcement learning, specifically Direct Preference Optimization (DPO), we enhance the agent's ability to make accurate decisions in dynamic environments while mitigating model degradation during fine-tuning. Our method models the environment as a Markov decision process, requiring the agent to reflect on the current state in real time to generate adaptive plans and actions.\nBy prioritizing the optimization of the CoT process over the final actions, we enhance the agent's reasoning adaptability while effectively mitigating model degradation during fine-tuning.\nExperiments in the ALFWorld environment demonstrate an average success rate of \\textbf 26.67%, which is a 6% improvement over RL4VLM, and show that our method effectively mitigates model degradation post fine-tuning. These results highlight the potential of integrating preference-based reinforcement learning techniques with CoT processes to enhance the decision-making capabilities of vision-language models in embodied agents."
    }
]