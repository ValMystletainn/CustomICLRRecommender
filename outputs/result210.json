[
    {
        "title": "Continual Learning: Less Forgetting, More OOD Generalization via Adaptive Contrastive Replay",
        "link_suffix": "/forum?id=YGflij9S6x",
        "link": "https://openreview.net/forum?id=YGflij9S6x",
        "pdf_link": "https://openreview.net/pdf?id=YGflij9S6x",
        "keywords": "Continual learning, OOD generalization, contrastive learning",
        "abstract": "Machine learning models often suffer from catastrophic forgetting of previously learned knowledge when learning new classes. Various methods have been proposed to mitigate this issue. However, rehearsal-based learning, which retains samples from previous classes, typically achieves good performance but tends to memorize specific instances, struggling with Out-of-Distribution (OOD) generalization. This often leads to high forgetting rates and poor generalization. Surprisingly, the OOD generalization capabilities of these methods have been largely unexplored. In this paper, we highlight this issue and propose a simple yet effective strategy inspired by contrastive learning and data-centric principles to address it.\nWe introduce Adaptive Contrastive Replay (ACR), a method that employs dual optimization to simultaneously train both the encoder and the classifier. ACR adaptively populates the replay buffer with misclassified samples while ensuring a balanced representation of classes and tasks. By refining the decision boundary in this way, ACR achieves a balance between stability and plasticity. Our method significantly outperforms previous approaches in terms of OOD generalization, achieving an improvement of 13.41% on Split CIFAR-100, 9.91% on Split Mini-ImageNet, and 5.98% on Split Tiny-ImageNet."
    },
    {
        "title": "Patch-Based Diffusion Models Beat Whole-Image Models for Mismatched Distribution Inverse Problems",
        "link_suffix": "/forum?id=7SFTZwNUQA",
        "link": "https://openreview.net/forum?id=7SFTZwNUQA",
        "pdf_link": "https://openreview.net/pdf?id=7SFTZwNUQA",
        "keywords": "reconstruction, computed tomography, deblurring, superresolution",
        "abstract": "Diffusion models have achieved excellent success in solving inverse problems\ndue to their ability to learn strong image priors,\nbut existing approaches require a large training dataset of images\nthat should come from the same distribution as the test dataset.\nWhen the training and test distributions are mismatched,\nartifacts and hallucinations can occur in reconstructed images due to the incorrect priors.\nIn this work, we systematically study out of distribution (OOD) problems where a known training distribution is first provided.\nWe first study the setting where only a single measurement obtained from the unknown test distribution is available.\nNext we study the setting where a very small sample of data belonging to the test distribution\nis available, and our goal is still to reconstruct an image from a measurement that came from the test distribution.\nIn both settings, we use a patch-based diffusion prior\nthat learns the image distribution solely from patches.\nFurthermore, in the first setting, we include a self-supervised loss\nthat helps the network output maintain consistency with the measurement.\nExtensive experiments show that in both settings,\nthe patch-based method can obtain high quality image reconstructions \nthat can outperform whole-image models\nand can compete with methods that have access to large in-distribution training datasets.\nFurthermore, we show how whole-image models are prone to memorization and overfitting,\nleading to artifacts in the reconstructions, while a patch-based model can resolve these issues."
    },
    {
        "title": "Time, Space and Streaming Efficient Algorithm for Heavy Attentions",
        "link_suffix": "/forum?id=90DC0IvlSs",
        "link": "https://openreview.net/forum?id=90DC0IvlSs",
        "pdf_link": "https://openreview.net/pdf?id=90DC0IvlSs",
        "keywords": "transformers, attention, randomized linear algebra, leverage scores, Lewis weights",
        "abstract": "A central problem related to transformers can be stated as follows: given two $n \\times d$ matrices $Q$ and $K$, and a non-negative function $f$, define the matrix $A$ as follows: (1) apply the function $f$ to each entry of the $n \\times n$ matrix $Q K^T$, and then (2) normalize each of the row sums of $A$ to be equal to $1$. The matrix $A$ can be computed in $O(n^2 d)$ time assuming $f$ can be applied to a number in constant time, but the quadratic dependence on $n$ is prohibitive in applications where it corresponds to long context lengths. For a large class of functions $f$, we show how to find all the \"large attention scores\", i.e., entries of $A$ which are at least a positive value $\\varepsilon$, in time with linear dependence on $n$ (i.e., $n \\cdot \\textrm{poly}(d/\\varepsilon)$) for a positive parameter $\\varepsilon > 0$. Our class of functions include all functions $f$ of the form $f(x) = |x|^p$, as explored recently in transformer models. Using recently developed tools from randomized numerical linear algebra, we prove that for any $K$, there is a \"universal set\" $U \\subset [n]$ of size independent of $n$, such that for any $Q$ and any row $i$, the large attention scores $A_{i,j}$ in row $i$ of $A$ all have $j \\in U$. We also find $U$ in $n \\cdot \\textrm{poly}(d/\\varepsilon)$ time. Notably, we \n(1) make no assumptions on the data, (2) our workspace does not grow with $n$, and (3) our algorithms can be computed in streaming and parallel settings. We empirically show the benefits of our scheme for vision transformers, showing how to train new models that use our universal set while training as well, showing that our model is able to consistently select \"important keys'\" during training."
    },
    {
        "title": "Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model",
        "link_suffix": "/forum?id=ny8T8OuNHe",
        "link": "https://openreview.net/forum?id=ny8T8OuNHe",
        "pdf_link": "https://openreview.net/pdf?id=ny8T8OuNHe",
        "keywords": "Adapter, Diffusion, ControlNet, Text-to-video Generation, Image-to-video Generation, Text-to-image Generation",
        "abstract": "ControlNets are widely used for adding spatial control to text-to-image diffusion models. However, when it comes to controllable video generation, ControlNets cannot be directly integrated into new backbones due to feature space mismatches, and training ControlNets for new backbones can be a significant burden for many users. Furthermore, applying ControlNets independently to different frames can not effectively maintain object temporal consistency. To address these challenges, we introduce Ctrl-Adapter, an efficient and versatile framework that adds diverse controls to any image/video diffusion models through the adaptation of pretrained ControlNets. Ctrl-Adapter offers strong and diverse capabilities, including image and video control, sparse-frame video control, fine-grained patch-level multi-condition control, zero-shot adaptation to unseen conditions, and supports a variety of downstream tasks beyond spatial control, including video editing, video style transfer, and text-guided motion control. With six diverse U-Net/DiT-based image/video diffusion models (SDXL, PixArt-\u03b1, I2VGen-XL, SVD, Latte, Hotshot-XL), Ctrl-Adapter matches the performance of pretrained ControlNets on COCO and achieves the state-of-the-art on DAVIS 2017 with significantly lower computation (< 10 GPU hours). We provide video examples inhttps://ctrladapterexamples.github.ioand code in the supplementary material."
    },
    {
        "title": "ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning",
        "link_suffix": "/forum?id=aKRADWBJ1I",
        "link": "https://openreview.net/forum?id=aKRADWBJ1I",
        "pdf_link": "https://openreview.net/pdf?id=aKRADWBJ1I",
        "keywords": "Safe Exploration, Constrained Markov Decision Processes, Safe Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) is ubiquitous in the development of modern AI systems. However, state-of-the-art RL agents require extensive, and potentially\nunsafe, interactions with their environments to learn effectively. These limitations\nconfine RL agents to simulated environments, hindering their ability to learn\ndirectly in real-world settings. In this work, we present ActSafe, a novel\nmodel-based RL algorithm for safe and efficient exploration. ActSafe learns\na well-calibrated probabilistic model of the system and plans optimistically\nw.r.t. the epistemic uncertainty about the unknown dynamics, while enforcing\npessimism w.r.t. the safety constraints. Under regularity assumptions on the\nconstraints and dynamics, we show that ActSafe guarantees safety during\nlearning while also obtaining a near-optimal policy in finite time. In addition, we\npropose a practical variant of ActSafe that builds on latest model-based RL advancements and enables safe exploration even in high-dimensional settings such\nas visual control. We empirically show that ActSafe obtains state-of-the-art\nperformance in difficult exploration tasks on standard safe deep RL benchmarks\nwhile ensuring safety during learning."
    },
    {
        "title": "Filtered Semantic Search via Vector Arithmetic",
        "link_suffix": "/forum?id=anN4a8h4od",
        "link": "https://openreview.net/forum?id=anN4a8h4od",
        "pdf_link": "https://openreview.net/pdf?id=anN4a8h4od",
        "keywords": "passage retrieval, dense retrieval, feature representation",
        "abstract": "How can we retrieve search results that are both semantically relevant and satisfy certain filter criteria? Modern day semantic search engines are increasingly reliant on vector-based search, yet the ability to restrict vector search to a fixed set of filter criteria remains an interesting problem with no known satisfactory solution. In this note, we leverage the rich emergent structure of vector embeddings of pre-trained search transformers to offer a simple solution. Our method involves learning, for each filter, a vector direction in the space of vector embeddings, and adding it to the query vector at run-time to perform a search constrained by that filter criteria. Our technique is broadly applicable to any finite set of semantically meaningful filters, compute-efficient in that it does not require modifying or rebuilding an existing $k$-NN index over document vector embeddings, lightweight in that it adds negligible latency, and widely compatible in that it can be utilized with any transformer model and $k$-NN algorithm. We also establish, subject to mild assumptions, an upper bound on the probability that our method errantly retrieves irrelevant results, and reveal new empirical insights about the geometry of transformer embeddings. In experiments, we find that our method, on average, yields more than a 21% boost over the baseline (measured in terms of nDCG@10) across three different transformer models and datasets."
    },
    {
        "title": "Diffusion Minimization and Sheaf Neural Networks for Recommender Systems",
        "link_suffix": "/forum?id=VSVQljJU5N",
        "link": "https://openreview.net/forum?id=VSVQljJU5N",
        "pdf_link": "https://openreview.net/pdf?id=VSVQljJU5N",
        "keywords": "Graph Neural Networks, Sheaves, Oversmoothing",
        "abstract": "Graph Neural Networks (GNN) are well-known for successful applications in recommender systems. Despite recent advances in GNN development, various authors report that in certain cases GNN suffer from so-called oversmoothing problems. Sheaf Neural Networks (SNN) is one of the ways to address the issue of oversmoothing. In the present work we propose a novel approach for training SNN together with user and item embeddings. In that approach parameters of the sheaf are inferred via minimization of the classical BPR loss and sheaf diffusion on graphs subjected to orthogonality and consistency constraints. Performance of the novel technique is evaluated on synthetic test cases and standard benchmarks for recommendations."
    },
    {
        "title": "Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs",
        "link_suffix": "/forum?id=hrOlBgHsMI",
        "link": "https://openreview.net/forum?id=hrOlBgHsMI",
        "pdf_link": "https://openreview.net/pdf?id=hrOlBgHsMI",
        "keywords": "Learning rate schedules, Large language models (LLMs), AdamW optimizer, Weight decay, Compute-optimal training",
        "abstract": "LLMs are commonly trained with a learning rate (LR) warmup, followed by cosine decay to 10% of the maximum (10x decay). In a large-scale empirical study, we show that under an optimal max LR, a simple linear decay-to-zero (D2Z) schedule consistently outperforms other schedules when training at compute-optimal dataset sizes. Benefits increase further with more training tokens; e.g., a 617M-parameter model trained for 80 tokens-per-parameter (TPP) using D2Z achieves lower loss than when trained for 200 TPP using 10x decay, corresponding to an astonishing 60% FLOPs savings. This implies models like Llama2-7B, trained for 286 TPP with 10x decay, were severely under-decayed. We demonstrate the benefits of D2Z across a range of model sizes, batch sizes, and other training configurations. We explain the success of linear D2Z via a novel interpretation of AdamW as a convex combination of weight updates, with coefficients governed by the LR schedule. This interpretation demonstrates how linear D2Z balances the demands of early training (moving away quickly from initial conditions) and late training (smoothing over more updates to mitigate gradient noise)."
    },
    {
        "title": "Revisiting the Variational Information Bottleneck",
        "link_suffix": "/forum?id=w10KdRwcMk",
        "link": "https://openreview.net/forum?id=w10KdRwcMk",
        "pdf_link": "https://openreview.net/pdf?id=w10KdRwcMk",
        "keywords": "information bottleneck, information theory, representation learning, adversarial attacks, regularization, supervised learning",
        "abstract": "The Information Bottleneck (IB) framework offers a theoretically optimal approach to data modeling, though it is often intractable. Recent efforts have optimized supervised deep neural networks (DNNs) using a variational upper bound on the IB objective, leading to enhanced robustness against adversarial attacks. In these studies, supervision assumes a dual role: sometimes as a random variable with the empirical distribution of the data, and at other times as a random variable distributed according to the classification performed. This work proposes an extension to the framework, and consequently to the derivation of the bound, that resolves this duality. Applying the resulting bound as an objective for supervised DNNs induces substantial empirical improvements."
    },
    {
        "title": "HierT2S: Enhancing Part-Level Text-to-Shape Generation via Hierarchical Structure Modeling",
        "link_suffix": "/forum?id=GQ2Ks23bJ6",
        "link": "https://openreview.net/forum?id=GQ2Ks23bJ6",
        "pdf_link": "https://openreview.net/pdf?id=GQ2Ks23bJ6",
        "keywords": "3D shape generation, text-to-shape",
        "abstract": "Text-driven 3D shape generation still faces key challenges, especially in achieving high levels of control over the generated outputs. Paticularly, existing text-to-shape methods ignore the explicit modeling of hierarchical structures in the text and 3D shapes, which makes it hard for using long text descriptions with multiple prompts to guide the coherent part-level 3D shape generation. In this work, we introduce HierT2S, a framework that integrates a hierarchical tree representation with a conditional diffusion model, to enhance the generation of 3D shapes with coherent structures induced by the hierarchical and structured text representations. The key idea is to first segment the input text into several clusters and construct a hierarchical tree representation, with each node representing a parent entity or the fine-level part components. Then, we process the lower-level clusters of the tree with a relation graph module which uses self-attention mechanism to aggregate the relationships of the clusters, and generate a new sequence containing the processed text features. Finally, the text features are embedded into the 3D feature space and used for learning the 3D shape generation by a conditional diffusion model, where the sparsely implicit parsed hierarchical tree graph further enhances the structural details of the generated 3D shapes, leading to results that are close to structure-aware generation. We conducted comprehensive experiments on the existing text-to-shape pairing dataset Text2Shape, and the results demonstrate that our model significantly outperforms current state-of-the-art methods. Moreover, our method can enable progressive part-level 3D shape manipulation and modification guided by the partially modified text prompt."
    },
    {
        "title": "Generalized Anomaly Detection with Knowledge Exposure:The Dual Effects of Augmentation",
        "link_suffix": "/forum?id=MbtUctg3KW",
        "link": "https://openreview.net/forum?id=MbtUctg3KW",
        "pdf_link": "https://openreview.net/pdf?id=MbtUctg3KW",
        "keywords": "OOD gneralziation, OOD detection, Anomaly detection, One class classification, Outlier Expousre",
        "abstract": "Anomaly detection involves identifying samples that deviate from the training data. While previous methods have demonstrated significant performance, our experiments reveal that their generalization ability declines substantially when faced with slight shifts in the test data. This limitation stems from an underlying assumption: these methods generally expect the distribution of normal test samples to closely resemble that of the training set, while anomalies are presumed to be far from this distribution. However, in real-world scenarios, test samples often experience varying degrees of distributional shift while retaining their semantic consistency. The ability to generalize successfully to semantically preserved transformations while accurately detecting normal samples whose semantic meaning has changed as anomalies is critical for a model's trustworthiness and reliability. For instance, while a rotation may alter the semantic meaning of a car in the context of anomaly detection, it typically preserves the meaning of an apple. Yet, current methods, particularly those based on contrastive learning, are likely to detect both as anomalies. This complexity underscores the need for dynamic learning procedures grounded in a deeper understanding of outliers. To address this, we propose a novel approach called Knowledge Exposure (KE), which incorporates external knowledge to interpret concept dynamics and distinguish between transformations that induce semantic shifts. Our approach improves generalization by leveraging insights from a pre-trained CLIP model to assess the significance of anomalies for each concept. Evaluations on datasets such as CIFAR-10, CIFAR-100, SVHN demonstrate superior performance compared to previous methods, validating the effectiveness of our approach."
    },
    {
        "title": "RetriBooru: Leakage-Free Retrieval of Conditions from Reference Images for Subject-Driven Generation",
        "link_suffix": "/forum?id=IjVCcykKdr",
        "link": "https://openreview.net/forum?id=IjVCcykKdr",
        "pdf_link": "https://openreview.net/pdf?id=IjVCcykKdr",
        "keywords": "Dataset, Subject-driven generation, Evaluation Metrics, Diffusion Models",
        "abstract": "Diffusion-based methods have demonstrated remarkable capabilities in generating a diverse array of high-quality images, sparking interests for styled avatars, virtual try-on, and more. Previous methods use the same reference image as the target. An overlooked aspect is the leakage of the target's spatial information, style, etc. from the reference, harming the generated diversity and causing shortcuts. However, this approach continues as widely available datasets usually consist of single images not grouped by identities, and it is expensive to recollect large-scale same-identity data. Moreover, existing metrics adopt decoupled evaluation on text alignment and identity preservation, which fail at distinguishing between balanced outputs and those that over-fit to one aspect. \nIn this paper, we propose a multi-level, same-identity dataset RetriBooru, which groups anime characters by both face and cloth identities. RetriBooru enables adopting reference images of the same character and outfits as the target, while keeping flexible gestures and actions. We benchmark previous methods on our dataset, and demonstrate the effectiveness of training with a reference image different from target (but same identity). We introduce a new concept composition task, where the conditioning encoder learns to retrieve different concepts from several reference images, and modify a baseline network RetriNet for the new task. Finally, we introduce a novel class of metrics named Similarity Weighted Diversity (SWD), to measure the overlooked diversity and better evaluate the alignment between similarity and diversity."
    },
    {
        "title": "Revisiting Quantum Algorithms for Linear Regressions: Quadratic Speedups without Data-Dependent Parameters",
        "link_suffix": "/forum?id=hyrRupfS0o",
        "link": "https://openreview.net/forum?id=hyrRupfS0o",
        "pdf_link": "https://openreview.net/pdf?id=hyrRupfS0o",
        "keywords": "Linear regression, quantum algorithms",
        "abstract": "Linear regression is one of the most fundamental linear algebra problems. Given a dense matrix $A \\in \\mathbb{R}^{n \\times d}$ and a vector $b$, the goal is to find $x'$ such that $|| Ax' - b ||_2^2 \\leq (1+\\epsilon) \\min_{x} || A x - b ||_2^2$. The best classical algorithm takes $O(nd) + \\mathrm{poly}(d/\\epsilon)$ time [Clarkson and Woodruff STOC 2013, Nelson and Nguyen FOCS 2013]. On the other hand, quantum linear regression algorithms can achieve exponential quantum speedups, as shown in [Wang \\emph{Phys. Rev. A 96}, 012335, Kerenidis and Prakash ITCS 2017, Chakraborty, Gily{'e}n and Jeffery ICALP 2019]. However, the running times of these algorithms depend on some quantum linear algebra-related parameters, such as $\\kappa(A)$, the condition number of $A$. In this work, we develop a quantum algorithm that runs in $\\widetilde{O}(\\epsilon^{-1}\\sqrt{n}d^{1.5}) + \\mathrm{poly}(d/\\epsilon)$ time and outputs a classical solution. It provides a quadratic quantum speedup in $n$ over the classical lower bound without any dependence on data-dependent parameters. In addition, we also show our result can be generalized to multiple regression and ridge linear regression."
    },
    {
        "title": "Differentiable Integer Linear Programming",
        "link_suffix": "/forum?id=FPfCUJTsCn",
        "link": "https://openreview.net/forum?id=FPfCUJTsCn",
        "pdf_link": "https://openreview.net/pdf?id=FPfCUJTsCn",
        "keywords": "Integer Linear Programming, Learning to Optimize",
        "abstract": "Machine learning (ML) techniques have shown great potential in generating high-quality solutions for integer linear programs (ILPs).\nHowever, existing methods typically rely on asupervised learningparadigm, leading to (1)expensive training costdue to repeated invocations of traditional solvers to generate training labels, and (2)plausible yet infeasible solutionsdue to the misalignment between the training objective (minimizing prediction loss) and the inference objective (generating high-quality solutions).\nTo tackle this challenge, we proposeDiffILO(DifferentiableIntegerLinear ProgrammingOptimization), anunsupervised learning paradigm for learning to solve ILPs.\nSpecifically, through a novel probabilistic modeling, DiffILO reformulates ILPs---discrete and constrained optimization problems---into continuous, differentiable (almost everywhere), and unconstrained optimization problems.\nThis reformulation enables DiffILO to simultaneously solve ILPs and train the model via straightforward gradient descent, providing two major advantages.\nFirst, it significantly reduces the training cost, as the training process does not need the aid of traditional solvers at all.\nSecond, it facilitates the generation of feasible and high-quality solutions, as the modellearns to solve ILPsin an end-to-end manner, thus aligning the training and inference objectives.\nExperiments on commonly used ILP datasets demonstrate that DiffILO not only achieves an average training speedup of $13.2$ times compared to supervised methods, but also outperforms them by generating heuristic solutions with significantly higher feasibility ratios and much better solution qualities."
    },
    {
        "title": "How many tokens is an image worth?",
        "link_suffix": "/forum?id=mb2ryuZ3wz",
        "link": "https://openreview.net/forum?id=mb2ryuZ3wz",
        "pdf_link": "https://openreview.net/pdf?id=mb2ryuZ3wz",
        "keywords": "Representation Learning, Image Tokenization, Compression",
        "abstract": "Current visual representation learning systems typically assign fixed-length representations to images, regardless of their information content. This contrasts with human visual intelligence\u2014and even large language models\u2014which allocate varying representational capacities based on information entropy, context, and familiarity. Inspired by this, we propose an approach to learning variable-length token representations for 2D images. Our encoder-decoder architecture recursively processes 2D image tokens, distilling them into 1D latent tokens over multiple iterations of recurrent rollouts. Each iteration refines the 2D tokens, updates the existing 1D latent tokens, and adaptively increases representational capacity by adding new tokens. This enables the compression of images into a variable number of tokens, ranging from 32 to 256. We validate our tokenizer using reconstruction loss and FID metrics, demonstrating that token counts align with image entropy, familiarity, and downstream task requirements. This recurrent processing, with increasing representational capacity in each iteration, shows signs of token specialization, revealing the potential for object and part discovery."
    },
    {
        "title": "On Rademacher Complexity-based Generalization Bounds for Deep Learning",
        "link_suffix": "/forum?id=Y7lc4aZ4iP",
        "link": "https://openreview.net/forum?id=Y7lc4aZ4iP",
        "pdf_link": "https://openreview.net/pdf?id=Y7lc4aZ4iP",
        "keywords": "Deep Learning, CNN, Generalisation Errors",
        "abstract": "We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between vector spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not explicitly depend on the network length for CNNs with some common types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh."
    },
    {
        "title": "IndianRoad: A Video Dataset of Diverse Atomic Visual Elements in Dense and Unpredictable Environments",
        "link_suffix": "/forum?id=8gCgXG40Wn",
        "link": "https://openreview.net/forum?id=8gCgXG40Wn",
        "pdf_link": "https://openreview.net/pdf?id=8gCgXG40Wn",
        "keywords": "Dataset, Vulnerable Road Users, Dense and Unpredictable Environment, Video Understanding, Behaviour Understanding",
        "abstract": "Most existing traffic video datasets including Waymo are structured, focusing predominantly on Western traffic, which hinders global applicability. Specifically, most Asian scenarios are far more complex, involving numerous objects with distinct motions and behaviors. Addressing this gap, we present a new dataset, IndianRoad, designed for evaluating perception methods with high representation of Vulnerable Road Users (VRUs: e.g. pedestrians, animals, motorbikes, and bicycles) in complex and unpredictable environments. IndianRoad is a manually annotated dataset encompassing 16 diverse actor categories (spanning animals, humans, vehicles, etc.) and 16 action types (complex and rare cases like cut-ins, zigzag movement, U-turn, etc.), which require high reasoning ability. IndianRoad densely annotates over 13 million bounding boxes (bboxes) actors with identification, and more than 1.6 million boxes are annotated with both actor identification and action/behavior details. The videos within IndianRoad are collected based on a broad spectrum of factors, such as weather conditions, the time of day, road scenarios, and traffic density. IndianRoad can benchmark video tasks like Tracking, Detection, Spatiotemporal Action Localization, Language-Visual Moment retrieval, and Multi-label Video Action Recognition. Given the critical importance of accurately identifying VRUs to prevent accidents and ensure road safety, in IndianRoad, vulnerable road users constitute 41.13% of instances, compared to 23.71% in Waymo. IndianRoad provides an invaluable resource for the development of more sensitive and accurate visual perception algorithms in the complex real world.\n  Our experiments show that existing methods suffer degradation in performance when evaluated on IndianRoad, highlighting its benefit for future video recognition research."
    },
    {
        "title": "ELCC: the Emergent Language Corpus Collection",
        "link_suffix": "/forum?id=X8Mhumi52G",
        "link": "https://openreview.net/forum?id=X8Mhumi52G",
        "pdf_link": "https://openreview.net/pdf?id=X8Mhumi52G",
        "keywords": "emergent language, emergent communication, dataset, resource",
        "abstract": "We introduce the Emergent Language Corpus Collection (ELCC): a collection of corpora generated from open source implementations of emergent communication systems across the literature. These systems include a variety of signalling game environments as well as more complex tasks environments like a social deduction game and embodied navigation. Each corpus is annotated with metadata describing the characteristics of the source system as well as a suite of analyses of the corpus (e.g., size, entropy, average message length, performance as transfer learning data). Currently, research studying emergent languages requires directly running different systems which takes time away from actual analyses of such languages, makes studies which compare diverse emergent languages rare, and presents a barrier to entry for researchers without a background in deep learning. The availability of a substantial collection of well-documented emergent language corpora, then, will enable research which can analyze a wider variety of emergent languages; this more effectively uncovers general principles in emergent communication rather than artifacts of particular environments. We provide some quantitative and qualitative analyses with ELCC to demonstrate potential use cases of the resource in this vein."
    },
    {
        "title": "Can Large Language Models Help Experimental Design for Causal Discovery?",
        "link_suffix": "/forum?id=aUeQPyRMeJ",
        "link": "https://openreview.net/forum?id=aUeQPyRMeJ",
        "pdf_link": "https://openreview.net/pdf?id=aUeQPyRMeJ",
        "keywords": "Large Language Model, Experimental Design, Causal Discovery",
        "abstract": "Designing proper experiments and intervening targets is a longstanding problem in scientific or causal discovery. It is fundamentally impossible to identify the underlying causal structure merely based on the observational data. Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive or time-consuming to obtain sufficient interventional data to facilitate causal discovery. Previous approaches usually leverage uncertainty or gradient signals to determine the intervention targets, and may suffer from the suboptimality. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in LLM. Specifically, we present Large Language Model Guided Intervention Targeting (LeGIT), a robust framework that effectively incorporates LLMs to assist with the intervention targeting in causal discovery. Surprisingly, across 4 different scales of realistic benchmarks, LeGIT significantly outperforms previous approaches. LeGIT opens up a new frontier for using LLMs in experimental design."
    },
    {
        "title": "Accelerated Diffusion using Closed-form Discriminator Guidance",
        "link_suffix": "/forum?id=UK0jrVGCg2",
        "link": "https://openreview.net/forum?id=UK0jrVGCg2",
        "pdf_link": "https://openreview.net/pdf?id=UK0jrVGCg2",
        "keywords": "diffusion models, GANs, time-step-shifted sampling, discriminator guidance",
        "abstract": "Diffusion models are a state-of-the-art generative modeling framework that transform noise to images via Langevin sampling, guided by the score, which is the gradient of the logarithm of the data distribution. Recent works have shown empirically that the generation quality can be improved when guided by classifier network, which is typically the discriminator trained in a generative adversarial network (GAN) setting. In this paper, we propose a theoretical framework to analyze the effect of the GAN discriminator on Langevin-based sampling, and show that in IPM GANs, the optimal generator matches {\\it score-like} functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, we show that IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the kernel associated with the constraint. The proposed approach serves to unify score-based training and optimization of IPM-GANs. Based on these insights, we demonstrate that closed-form discriminator guidance, using a kernel-based implementation, results in  improvements (in terms of CLIP-FID and KID metrics) when applied atop baseline diffusion models. We demonstrate these results by applying closed-form discriminator guidance to denoising diffusion implicit model (DDIM) and latent diffusion model (LDM) settings on the FFHQ and CelebA-HQ datasets. We also demonstrate improvements to accelerated time-step-shifted diffusion, when coupled with a wavelet-based noise estimator for latent-space image generation."
    },
    {
        "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
        "link_suffix": "/forum?id=j1tSLYKwg8",
        "link": "https://openreview.net/forum?id=j1tSLYKwg8",
        "pdf_link": "https://openreview.net/pdf?id=j1tSLYKwg8",
        "keywords": "diffusion models; diffusion language models; text diffusion models; discrete diffusion models",
        "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (with 127M, 355M, and 7B parameters) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions."
    },
    {
        "title": "Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models",
        "link_suffix": "/forum?id=xiDJaTim3P",
        "link": "https://openreview.net/forum?id=xiDJaTim3P",
        "pdf_link": "https://openreview.net/pdf?id=xiDJaTim3P",
        "keywords": "Federated learning, prompt learning, vision-language model, mixture of experts",
        "abstract": "Prompt learning for pre-trained Vision-Language Models (VLMs) like CLIP has demonstrated potent applicability across diverse downstream tasks. This lightweight approach has quickly gained traction from federated learning (FL) researchers who seek to efficiently adapt VLMs to heterogeneous scenarios. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating clients are generally only allowed to download a single globally aggregated model from the server. While justifiable for training full-sized models under federated settings, in this work, we argue that this paradigm is ill-suited for lightweight prompts. By facilitating the clients to download multiple pre-aggregated prompts as fixed non-local experts, we propose Personalized Federated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning process through the lens of Mixture of Experts (MoE). pFedMoAP implements a local attention-based gating network that learns to generate enhanced text features for better alignment with local image data on the client, benefiting from both local and downloaded non-local adaptive prompt experts. The non-local experts are sparsely selected from a server-maintained pool, fostering collaborative learning across clients. To evaluate the proposed algorithm, we conduct extensive experiments across 9 datasets under various heterogeneous federated settings. The results show that pFedMoAP consistently outperforms the state-of-the-art alternatives, underscoring its efficacy in personalizing prompt learning for CLIP within the federated learning paradigm."
    },
    {
        "title": "BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models",
        "link_suffix": "/forum?id=fAAaT826Vv",
        "link": "https://openreview.net/forum?id=fAAaT826Vv",
        "pdf_link": "https://openreview.net/pdf?id=fAAaT826Vv",
        "keywords": "Large language models, Reasoning, Planning, Trustworthiness, Interpretability, Probability Estimation, Bayesian Methods",
        "abstract": "Predictive models often need to work with incomplete information in real-world tasks. Consequently, they must provide reliable probability or confidence estimation, especially in large-scale decision making and planning tasks. Current large language models (LLM) are insufficient for such accurate estimations, but they can generate relevant factors that may affect the probabilities, produce coarse-grained probabilities when the information is more complete, and help determine which factors are relevant to specific downstream contexts. In this paper, we make use of these capabilities of LLMs to provide a significantly more accurate probabilistic estimation. We propose BIRD, a novel probabilistic inference framework that aligns a Bayesian network with LLM abductions and then estimates more accurate probabilities in a deduction step. We show BIRD provides reliable probability estimations that are 30% better than those provided directly by LLM baselines. These estimates can further contribute to better and more trustworthy decision-making."
    },
    {
        "title": "FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"",
        "link_suffix": "/forum?id=UeVx6L59fg",
        "link": "https://openreview.net/forum?id=UeVx6L59fg",
        "pdf_link": "https://openreview.net/pdf?id=UeVx6L59fg",
        "keywords": "Large Language Models, Contextual LLM, Faithfulness, Hallucination, Benchmark and Evaluation",
        "abstract": "Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination\u2014where models generate responses misaligned with the provided context\u2014remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness."
    },
    {
        "title": "GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs",
        "link_suffix": "/forum?id=Pd7IOswRUZ",
        "link": "https://openreview.net/forum?id=Pd7IOswRUZ",
        "pdf_link": "https://openreview.net/pdf?id=Pd7IOswRUZ",
        "keywords": "abstract visual reasoning; contrastive learning; variational autoencoders; out of distribution generalization",
        "abstract": "Raven\u2019s Progressive Matrices (RPMs) is an established benchmark to examine\nthe ability to perform high-level abstract visual reasoning (AVR). Despite the current success of algorithms that solve this task, humans can generalize beyond a given puzzle and create new puzzles given a set of rules, whereas machines remain locked in solving a fixed puzzle from a curated choice list. We propose Generative Visual Puzzles (GenVP), a framework to model the entire RPM generation process, a substantially more challenging task. Our model\u2019s capability spans from generating multiple solutions for one specific problem prompt to creating complete new puzzles out of the desired set of rules. Experiments on five different datasets indicate that GenVP achieves state-of-the-art (SOTA) performance both in puzzle-solving accuracy and out-of-distribution (OOD) generalization in 22 out\nof 24 OOD scenarios. Further, compared to SOTA generative approaches, which struggle to solve RPMs when the feasible solution space increases, GenVP efficiently generalizes to these challenging scenarios. Moreover, our model demonstrates the ability to produce a wide range of complete RPMs given a set of abstract rules by effectively capturing the relationships between abstract rules and visual object properties."
    }
]