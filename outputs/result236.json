[
    {
        "title": "Efficient Discovery of Pareto Front for Multi-Objective Reinforcement Learning",
        "link_suffix": "/forum?id=fDGPIuCdGi",
        "link": "https://openreview.net/forum?id=fDGPIuCdGi",
        "pdf_link": "https://openreview.net/pdf?id=fDGPIuCdGi",
        "keywords": "multi-objective reinforcement learning, constrained reinforcement learning",
        "abstract": "Multi-objective reinforcement learning (MORL) excels at handling rapidly changing preferences in tasks that involve multiple criteria, even for unseen preferences. However, previous dominating MORL methods typically generate a fixed policy set or preference-conditioned policy through multiple training iterations exclusively for sampled preference vectors, and cannot ensure the efficient discovery of the Pareto front. Furthermore, integrating preferences into the input of policy or value functions presents scalability challenges, in particular as the dimension of the state and preference space grow, which can complicate the learning process and hinder the algorithm's performance on more complex tasks. To address these issues, we propose a two-stage Pareto front discovery algorithm called Constrained MORL (C-MORL), which serves as a seamless bridge between constrained policy optimization and MORL. Concretely, a set of policies is trained in parallel in the initialization stage, with each optimized towards its individual preference over the multiple objectives. Then, to fill the remaining vacancies in the Pareto front, the constrained optimization steps are employed to maximize one objective while constraining the other objectives to exceed a predefined threshold. Empirically, compared to recent advancements in MORL methods, our algorithm achieves more consistent and superior performances in terms of hypervolume, expected utility, and sparsity on both discrete and continuous control tasks, especially with numerous objectives (up to nine objectives in our experiments)."
    },
    {
        "title": "MoDiff: a Protein Diffusion Model for Multi-motif Scaffolding Problem",
        "link_suffix": "/forum?id=jXy5B0auu0",
        "link": "https://openreview.net/forum?id=jXy5B0auu0",
        "pdf_link": "https://openreview.net/pdf?id=jXy5B0auu0",
        "keywords": "Motif Scaffolding, Drug Design, Protein Modeling, Protein Diffusion Model",
        "abstract": "The motif scaffolding problem, crucial in drug and enzyme design, involves creating a scaffold to support one or more specified motifs. Existing methods employ the paradigm of inpainting, thereby limiting their applicability to scenarios where only a single motif is present or the positions between multiple motifs are known. However, in many significant scenarios, the positions between motifs are unknown, leaving the multi-motif scaffolding problem open. To tackle this challenge, we introduce a protein diffusion model called MoDiff. During the diffusion process, MoDiff implicitly assigns motifs to the protein backbone, thereby achieving the automatic design of relative positions among motifs. Our experiments demonstrate that MoDiff can: 1) solve the multi-motif scaffolding problem even when the positions between motifs are unknown, and 2) generate diverse scaffolds based on multiple given motifs. This indicates that MoDiff is a potential general solution to the multi-motif scaffolding problem."
    },
    {
        "title": "Offline-to-online Reinforcement Learning for Image-based Grasping with Scarce Demonstrations",
        "link_suffix": "/forum?id=nYEw2KHVxl",
        "link": "https://openreview.net/forum?id=nYEw2KHVxl",
        "pdf_link": "https://openreview.net/pdf?id=nYEw2KHVxl",
        "keywords": "Reinforcement learning, learning from demonstrations, image-based grasping",
        "abstract": "Offline-to-online reinforcement learning (O2O RL) aims to obtain a continually improving policy as it interacts with the environment, while ensuring the initial behaviour is satisficing.\n   This satisficing behaviour is necessary for robotic manipulation where random exploration can be costly due to catastrophic failures and time.\n   O2O RL is especially compelling when we can only obtain a scarce amount of (potentially suboptimal) demonstrations\u2014a scenario where behavioural cloning (BC) is known to suffer from distribution shift.\n   Previous works have outlined the challenges in applying O2O RL algorithms under the image-based environments.\n   In this work, we propose a novel O2O RL algorithm that can learn in a real-life image-based robotic vacuum grasping task with a small number of demonstrations where BC fails majority of the time.\n   The proposed algorithm replaces the target network in off-policy actor-critic algorithms with a regularization technique inspired by neural tangent kernel.\n   We demonstrate that the proposed algorithm can reach above 90% success rate in under two hours of interaction time, with only 50 human demonstrations, while BC and two commonly-used RL algorithms fail to achieve similar performance."
    },
    {
        "title": "Can Editing LLMs Inject Harm?",
        "link_suffix": "/forum?id=mlCRJnETWz",
        "link": "https://openreview.net/forum?id=mlCRJnETWz",
        "pdf_link": "https://openreview.net/pdf?id=mlCRJnETWz",
        "keywords": "Knowledge Editing, LLM safety, Harm Injection",
        "abstract": "Knowledge editing has been increasingly adopted to correct the false or outdated knowledge in Large Language Models (LLMs). Meanwhile, one critical but under-explored question is: can knowledge editing be used to inject harm into LLMs? In this paper, we propose to reformulate knowledge editing as a new type of safety threat for LLMs, namely Editing Attack, and conduct a systematic investigation with a newly constructed dataset EditAttack. Specifically, we focus on two typical safety risks of Editing Attack including Misinformation Injection and Bias Injection. For the risk of misinformation injection, we first categorize it into commonsense misinformation injection and long-tail misinformation injection. Then, we find that editing attacks can inject both types of misinformation into LLMs, and the effectiveness is particularly high for commonsense misinformation injection. For the risk of bias injection, we discover that not only can biased sentences be injected into LLMs with high effectiveness, but also one single biased sentence injection can cause a bias increase in general outputs of LLMs, which are even highly irrelevant to the injected sentence, indicating a catastrophic impact on the overall fairness of LLMs. Then, we further illustrate the high stealthiness of editing attacks, measured by their impact on the general knowledge and reasoning capacities of LLMs, and show the hardness of defending editing attacks with empirical evidence. Our discoveries demonstrate the emerging misuse risks of knowledge editing techniques on compromising the safety alignment of LLMs and the feasibility of disseminating misinformation or bias with LLMs as new channels. The code and dataset are available here."
    },
    {
        "title": "CAMEx: Curvature-aware Merging of Experts",
        "link_suffix": "/forum?id=nT2u0M0nf8",
        "link": "https://openreview.net/forum?id=nT2u0M0nf8",
        "pdf_link": "https://openreview.net/pdf?id=nT2u0M0nf8",
        "keywords": "Sparse Mixture-of-Experts, efficiency, expert merging",
        "abstract": "Existing methods for merging experts during model training and fine-tuning predominantly rely on Euclidean geometry, which assumes a flat parameter space. This assumption can limit the model's generalization ability, especially during the pre-training phase, where the parameter manifold might exhibit more complex curvature. Curvature-aware merging methods typically require additional information and computational resources to approximate the Fisher Information Matrix, adding memory overhead. In this paper, we introduce CAMEx (Curvature-Aware Merging of Experts), a novel expert merging protocol that incorporates natural gradients to account for the non-Euclidean curvature of the parameter manifold. By leveraging natural gradients, CAMEx adapts more effectively to the structure of the parameter space, improving alignment between model updates and the manifold's geometry. This approach enhances both pre-training and fine-tuning, resulting in better optimization trajectories and improved generalization without the substantial memory overhead typically associated with curvature-aware methods. Our contributions are threefold: (1) CAMEx significantly outperforms traditional Euclidean-based expert merging techniques across various natural language processing tasks, leading to enhanced performance during pre-training and fine-tuning; (2) we introduce a dynamic merging architecture that optimizes resource utilization, achieving high performance while reducing computational costs, facilitating efficient scaling of large language models; and (3) we provide both theoretical and empirical evidence to demonstrate the efficiency of our proposed method."
    },
    {
        "title": "ML4MILP: A Benchmark Dataset for Machine Learning-based Mixed-Integer Linear Programming",
        "link_suffix": "/forum?id=ueeqGvQozB",
        "link": "https://openreview.net/forum?id=ueeqGvQozB",
        "pdf_link": "https://openreview.net/pdf?id=ueeqGvQozB",
        "keywords": "Mixed Integer Linear Programming, Machine Learning, Benchmark Dataset",
        "abstract": "Machine learning (ML)-based approaches for solving mixed integer linear programming (MILP) problems have shown significant potential and are growing in sophistication. Despite this advancement, progress in this field is often hindered by the mixed and unsorted nature of current benchmark datasets, which typically lack carefully categorized collections of homogeneous instances.\nTo bridge this gap, we propose ML4MILP, the premier open-source benchmark dataset specifically designed for evaluating ML-based optimization algorithms in the MILP domain. Based on the proposed structure and embedding similarity metrics, we used a novel classification algorithm to carefully categorize the collected and generated instances, resulting in a benchmark dataset encompassing 100,000 instances across more than 70 heterogeneous classes.\nWe demonstrate the utility of ML4MILP through extensive benchmarking against a comprehensive suite of algorithms in the baseline library, consisting of traditional exact solvers and heuristic algorithms, as well as ML-based approaches. Our ML4MILP is open-source and accessible at:https://anonymous.4open.science/r/ML4MILP-6BE0."
    },
    {
        "title": "Efficient Online Pruning and Abstraction for Imperfect Information Extensive-Form Games",
        "link_suffix": "/forum?id=MTcgsz1SHr",
        "link": "https://openreview.net/forum?id=MTcgsz1SHr",
        "pdf_link": "https://openreview.net/pdf?id=MTcgsz1SHr",
        "keywords": "Game Theory, Imperfect Information Games, Counterfactual Regret Minimization, Poker, Machine Learning",
        "abstract": "Efficiently computing approximate equilibrium strategies in large-scale Imperfect Information Extensive-Form Games (IIEFGs) poses significant challenges due to the vast size of the game trees. Pruning and abstraction methods effectively reduce the size of the game tree and enhance computational efficiency. However, seamlessly integrating pruning techniques with variants of Counterfactual Regret Minimization (CFR), a leading method for solving IIEFGs, remains a complex task. Furthermore, existing information abstraction methods often involve high computational costs and may require months of offline pre-computation, limiting their practical applicability. In this paper, we introduce Expected-Value Pruning and Abstraction (EVPA), an online approach that improves efficiency by leveraging expected value estimation within information sets. EVPA consists of three core components: expected value estimation of information sets, expected value-based pruning, and information abstraction for subgames. It estimates the expected value of information sets using Nash equilibrium strategies, employing these estimations for both pruning and abstraction. By integrating Minimax pruning with CFR, EVPA streamlines decision-making by permanently eliminating sub-optimal actions from the game tree prior to CFR application. Additionally, EVPA features an advanced information abstraction mechanism that merges information sets based on both current and future expected values in the subgame, achieving efficient information abstraction within just $1$ second. Experiments on HUNL demonstrate that EVPA outperforms DeepStack's replication and Slumbot, with win-rates of $202\\pm 31$ and $96\\pm 43$ mbb/h, respectively. Remarkably, EVPA requires only $1$%-$2$% of the solving time to reach an approximate Nash equilibrium compared to DeepStack's replication."
    },
    {
        "title": "KAAN: Kolmogorov-Arnold Activation Network --- a Flexible Activation Enhanced KAN",
        "link_suffix": "/forum?id=3VOKrLao5g",
        "link": "https://openreview.net/forum?id=3VOKrLao5g",
        "pdf_link": "https://openreview.net/pdf?id=3VOKrLao5g",
        "keywords": "Kolmogorov-Arnold representation Theorem, Kolmogorov-Arnold Network, Multi-Layer Perceptrons",
        "abstract": "Kolmogorov-Arnold Networks (KAN) have led to a significant breakthrough in the foundational structures of machine learning by applying the Kolmogorov-Arnold representation theorem. Through this approach, the target conditional distribution is expressed as the summation of multiple continuous univariate B-spline functions. However, KAN faces the challenges of being unintuitive and inflexible. To address this issue, we analyze the structural configurations of Multi-Layer Perceptrons (MLPs) and KANs, finding that MLP can be represented in a form conforming to Kolmogorov-Arnold representation Theorem (KAT). Therefore, we propose MLP style KAN framework Kolmogorov-Arnold Activation Network (KAAN), which is more intuitive, flexible and transferable. To verify the flexibility and transferability of our approach, we extend it to Convolutional Neural Network (CNN). Also, we demonstrate that parameter sharing is beneficial not only for efficiency but also for effectiveness. KAAN shows better representation capacity than MLP on several benchmarks. Furthermore, our experiment results lead us to conclude that this method is feasible for integrating modern network approaches such as CNNs."
    },
    {
        "title": "metabench - A Sparse Benchmark of Reasoning and Knowledge in Large Language Models",
        "link_suffix": "/forum?id=4T33izzFpK",
        "link": "https://openreview.net/forum?id=4T33izzFpK",
        "pdf_link": "https://openreview.net/pdf?id=4T33izzFpK",
        "keywords": "llm, benchmarking, item response theory, factor analysis, information",
        "abstract": "Large Language Models (LLMs) vary in their abilities on a range of tasks. Initiatives such as the $\\texttt{Open LLM Leaderboard}$ aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly).\n   However, high correlations within and between benchmark scores suggest that (1) there exists a small set of common underlying abilities that these benchmarks measure, and (2) items tap into redundant information and the benchmarks may thus be considerably compressed.\n   We use data from $n > 5000$ LLMs to identify the most informative items of six benchmarks, $\\texttt{ARC}, \\texttt{GSM8K}, \\texttt{HellaSwag}, \\texttt{MMLU}, \\texttt{TruthfulQA}$ and $\\texttt{WinoGrande}$ (with $d=28,632$ items in total). From them we distill a sparse benchmark, \\texttt{metabench}, that has less than $3%$ of the original size of all six benchmarks combined. This new sparse benchmark goes beyond point scores by yielding estimators of the underlying benchmark-specific abilities.\n   We show that these estimators (1) can be used to reconstruct each original \\textit{individual} benchmark score with, on average, $1.24%$ root mean square error (RMSE), (2) reconstruct the original \\textit{total} score with $0.58%$ RMSE, and (3) have a single underlying common factor whose Spearman correlation with the total score is $r = 0.94$."
    },
    {
        "title": "Backdoor Attacks for LLMs with Weak-To-Strong Knowledge Distillation",
        "link_suffix": "/forum?id=29LC48aY3U",
        "link": "https://openreview.net/forum?id=29LC48aY3U",
        "pdf_link": "https://openreview.net/pdf?id=29LC48aY3U",
        "keywords": "Backdoor Attacks, Large Language Models, Knowledge Distillation",
        "abstract": "Despite being widely applied due to their exceptional capabilities, Large Language Models (LLMs) have been proven to be vulnerable to backdoor attacks. These attacks introduce targeted vulnerabilities into LLMs by poisoning training samples and full-parameter fine-tuning. However, this kind of backdoor attack is limited since they require significant computational resources, especially as the size of LLMs increases. Besides, parameter-efficient fine-tuning (PEFT) offers an alternative but the restricted parameter updating may impede the alignment of triggers with target labels. In this study, we first verify that backdoor attacks with PEFT may encounter challenges in achieving feasible performance. To address these issues and improve the effectiveness of backdoor attacks with PEFT, we propose a novel backdoor attack algorithm from weak to strong based on feature alignment-enhanced knowledge distillation (W2SAttack). Specifically, we poison small-scale language models through full-parameter fine-tuning to serve as the teacher model. The teacher model then covertly transfers the backdoor to the large-scale student model through feature alignment-enhanced knowledge distillation, which employs PEFT. Theoretical analysis reveals that W2SAttack has the potential to augment the effectiveness of backdoor attacks. We demonstrate the superior performance of W2SAttack on classification tasks across four language models, four backdoor attack algorithms, and two different architectures of teacher models. Experimental results indicate success rates close to 100% for backdoor attacks targeting PEFT."
    },
    {
        "title": "Conformal Prediction for Dose-Response Models with Continuous Treatments",
        "link_suffix": "/forum?id=AKAz88zYLB",
        "link": "https://openreview.net/forum?id=AKAz88zYLB",
        "pdf_link": "https://openreview.net/pdf?id=AKAz88zYLB",
        "keywords": "conformal prediction, dose-response models, uncertainty quantification, continuous treatment, covariate shift, causal inference",
        "abstract": "Understanding the dose-response relation between a continuous treatment and the outcome for an individual can greatly drive decision-making, particularly in areas like personalized drug dosing and personalized healthcare interventions. Point estimates are often insufficient in these high-risk environments, highlighting the need for uncertainty quantification to support informed decisions. Conformal prediction, a distribution-free and model-agnostic method for uncertainty quantification, has seen limited application in continuous treatments or dose-response models. To address this gap, we propose a novel methodology that frames the causal dose-response problem as a covariate shift, leveraging weighted conformal prediction. By incorporating propensity estimation, conformal predictive systems, and likelihood ratios, we present a practical solution for generating prediction intervals for dose-response models. Additionally, our method approximates local coverage for every treatment value by applying kernel functions as weights in weighted conformal prediction. Finally, we use a new synthetic benchmark dataset to demonstrate the significance of covariate shift assumptions in achieving robust prediction intervals for dose-response models."
    },
    {
        "title": "HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?",
        "link_suffix": "/forum?id=1yJ3IDpb1D",
        "link": "https://openreview.net/forum?id=1yJ3IDpb1D",
        "pdf_link": "https://openreview.net/pdf?id=1yJ3IDpb1D",
        "keywords": "Event Sequences, Marked Temporal Point Processes, Long Horizon Forecasting, Evaluation Metric, Benchmark",
        "abstract": "Accurately forecasting multiple future events within a given time horizon is crucial for applications in finance, retail, social networks, and healthcare. Event timing and labels are typically modeled using Marked Temporal Point Processes (MTPP), with evaluations often focused on next-event prediction quality. While some studies have extended evaluations to a fixed number of future events, we demonstrate that this approach leads to inaccuracies in handling false positives and false negatives. To address these issues, we propose a novel evaluation method inspired by object detection techniques from computer vision. Specifically, we introduce Temporal mean Average Precision (T-mAP), a temporal variant of mAP, which overcomes the limitations of existing long-horizon evaluation metrics. Our extensive experiments demonstrate that models with strong next-event prediction accuracy can yield poor long-horizon forecasts, and vice versa, indicating that specialized methods are needed for each task. To support further research, we release HoTPP, the first benchmark specifically designed for evaluating long-horizon MTPP predictions. HoTPP includes large-scale datasets with up to 43 million events and provides optimized procedures for both autoregressive and parallel inference, paving the way for future advancements in the field."
    },
    {
        "title": "CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression",
        "link_suffix": "/forum?id=m3KuuE2ozw",
        "link": "https://openreview.net/forum?id=m3KuuE2ozw",
        "pdf_link": "https://openreview.net/pdf?id=m3KuuE2ozw",
        "keywords": "3D Gaussian Splatting, Rate-Distortion Optimization, Context Models, Grid Representations",
        "abstract": "3D Gaussian Splatting (3DGS) has recently emerged as a promising 3D representation. Much research has been focused on reducing its storage requirements and memory footprint. However, the needs to compress and transmit the 3DGS representation to the remote side are overlooked. This new application calls for rate-distortion-optimized 3DGS compression. How to quantize and entropy encode sparse Gaussian primitives in the 3D space remains largely unexplored. Few early attempts resort to the hyperprior framework from learned image compression. But, they fail to utilize fully the inter and intra correlation inherent in Gaussian primitives. Built on ScaffoldGS, this work, termed CAT-3DGS, introduces a context-adaptive triplane approach to their rate-distortion-optimized coding. It features multi-scale triplanes, oriented according to the principal axes of Gaussian primitives in the 3D space, to capture their inter correlation (i.e. spatial correlation) for spatial autoregressive coding in the projected 2D planes. With these triplanes serving as the hyperprior, we further perform channel-wise autoregressive coding to leverage the intra correlation within each individual Gaussian primitive. Our CAT-3DGS incorporates a view frequency-aware masking mechanism. It actively skips from coding those Gaussian primitives that potentially have little impact on the rendering quality. When trained end-to-end to strike a good rate-distortion trade-off, our CAT-3DGS achieves the state-of-the-art compression performance on the commonly used real-world datasets."
    },
    {
        "title": "On the Benefits of Attribute-Driven Graph Domain Adaptation",
        "link_suffix": "/forum?id=t2TUw5nJsW",
        "link": "https://openreview.net/forum?id=t2TUw5nJsW",
        "pdf_link": "https://openreview.net/pdf?id=t2TUw5nJsW",
        "keywords": "Deep Learning and representational learning",
        "abstract": "Graph Domain Adaptation (GDA) addresses a pressing challenge in cross-network learning, particularly pertinent due to the absence of labeled data in real-world graph datasets. Recent studies attempted to learn domain invariant representations by eliminating structural shifts between graphs. In this work, we show that existing methodologies have overlooked the significance of the graph node attribute, a pivotal factor for graph domain alignment. \nSpecifically, we first reveal the impact of node attributes for GDA by theoretically proving that in addition to the graph structural divergence between the domains, the node attribute discrepancy also plays a critical role in GDA. Moreover, we also empirically show that the attribute shift is more substantial than the topology shift, which further underscore the importance of node attribute alignment in GDA. Inspired by this finding, a novel cross-channel module is developed to fuse and align both views between the source and target graphs for GDA. Experimental results on a variety of benchmark verify the effectiveness of our method."
    },
    {
        "title": "Adaptive dense reward:Understanding the Gap Between Action and Reward Space in Alignment",
        "link_suffix": "/forum?id=M31bgolPnl",
        "link": "https://openreview.net/forum?id=M31bgolPnl",
        "pdf_link": "https://openreview.net/pdf?id=M31bgolPnl",
        "keywords": "Large Language Model; Reinforcement Learning from Human Feedback (RLHF); Adaptive Message-wise RLHF",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven highly effective in aligning Large Language Models (LLMs) with human preferences. However, the original RLHF typically optimizes under an overall reward, which can lead to a suboptimal learning process. This limitation stems from RLHF's lack of awareness regarding which specific tokens should be reinforced or suppressed. Moreover, conflicts in supervision can arise, for instance, when a chosen response includes erroneous tokens, while a rejected response contains accurate elements. To rectify these shortcomings, increasing dense reward methods, such as step-wise and token-wise RLHF, have been proposed. However, these existing methods are limited to specific tasks (like mathematics).In this paper, we propose the \"Adaptive Message-wise RLHF\" method, which robustly applies to various tasks. By defining pivot tokens as key indicators, our approach adaptively identifies essential information and converts sample-level supervision into fine-grained, subsequence-level supervision. This aligns the density of rewards and action spaces more closely with the information density of the input.Experiments demonstrate that our method can be integrated into various training methods, significantly mitigating hallucinations and catastrophic forgetting problems, while outperforming other methods on multiple evaluation metrics. Our method improves the success rate on adversarial samples by 10% compared to the sample-wise approach, and achieves a 1.3% improvement on evaluation benchmarks such as MMLU, GSM8K, and HumanEval et al."
    },
    {
        "title": "LongHalQA: Long-Context Hallucination Evaluation for MultiModal Large Language Models",
        "link_suffix": "/forum?id=514rdneWOX",
        "link": "https://openreview.net/forum?id=514rdneWOX",
        "pdf_link": "https://openreview.net/pdf?id=514rdneWOX",
        "keywords": "hallucination benchmark, multimodal large language model",
        "abstract": "Hallucination, a phenomenon where multimodal large language models(MLLMs) tend to generate textual responses that are plausible but unaligned with the image, has become one major hurdle in various MLLM-related applications. Several benchmarks have been created to gauge the hallucination levels of MLLMs, by either raising discriminative questions about the existence of objects or introducing LLM evaluators to score the generated text from MLLMs. However, the discriminative data largely involve simple questions that are not aligned with real-world text, while the generative data involve LLM evaluators that are computationally intensive and unstable due to their inherent randomness. We propose LongHalQA, an LLM-free hallucination benchmark that comprises 6K long and complex hallucination text. LongHalQA is featured by GPT4V-generated hallucinatory data that are well aligned with real-world scenarios, including object/image descriptions and multi-round conversations with 14/130 words and 189 words, respectively, on average. It introduces two new tasks, hallucination discrimination and hallucination completion, unifying both discriminative and generative evaluations in a single multiple-choice-question form and leading to more reliable and efficient evaluations without the need for LLM evaluators. Further, we propose an advanced pipeline that greatly facilitates the construction of future hallucination benchmarks with long and complex questions and descriptions. Extensive experiments over multiple recent MLLMs reveal various new challenges when they are handling hallucinations with long and complex textual data."
    },
    {
        "title": "EgoSim: Egocentric Exploration in Virtual Worlds with Multi-modal Conditioning",
        "link_suffix": "/forum?id=zAyS5aRKV8",
        "link": "https://openreview.net/forum?id=zAyS5aRKV8",
        "pdf_link": "https://openreview.net/pdf?id=zAyS5aRKV8",
        "keywords": "Controllable video generation, Egocentric video prediction, World model",
        "abstract": "Recent advancements in video diffusion models have established a strong foundation for developing world models with practical applications. The next challenge lies in exploring how an agent can leverage these foundation models to understand, interact with, and plan within observed environments. This requires adding more controllability to the model, transforming it into a versatile game engine capable of dynamic manipulation and control. To address this, we investigated three key conditioning factors: camera, context frame, and text, identifying limitations in current model designs. Specifically, the fusion of camera embeddings with video features leads to camera control being influenced by those features. Additionally, while textual information compensates for necessary spatiotemporal structures, it often intrudes into already observed parts of the scene. To tackle these issues, we designed the Spacetime Epipolar Attention Layer, which ensures that egomotion generated by the model strictly aligns with the camera\u2019s movement through rigid constraints. Moreover, we propose the CI2V-adapter, which uses camera information to better determine whether to prioritize textual or visual embeddings, thereby alleviating the issue of textual intrusion into observed areas. Through extensive experiments, we demonstrate that our new model EgoSim achieves excellent results on both the RealEstate and newly repurposed Epic-Field datasets. For more results, please refer tohttps://egosim.github.io/EgoSim/."
    },
    {
        "title": "FUSION IS ALL YOU NEED : FACE FUSION FOR CUSTOMIZED IDENTITY-PRESERVING IMAGE SYNTHESIS",
        "link_suffix": "/forum?id=kpPjV2krAa",
        "link": "https://openreview.net/forum?id=kpPjV2krAa",
        "pdf_link": "https://openreview.net/pdf?id=kpPjV2krAa",
        "keywords": "Image Synthesis, Image Customization, ID Preservation, Diffusion",
        "abstract": "Text-to-image (T2I) models have significantly advanced the development of artificial intelligence, enabling the generation of high-quality images in diverse contexts based on specific text prompts. However, existing T2I-based methods often struggle to accurately reproduce the appearance of individuals from a reference image and to create novel representations of those individuals in various settings. To address this, we leverage the pre-trained UNet from Stable Diffusion to incorporate the target face image directly into the generation process. Our approach diverges from prior methods that depend on fixed encoders or static face embeddings, which often fail to bridge encoding gaps. Instead, we capitalize on UNet\u2019s sophisticated encoding capabilities to process reference images across multiple scales. By innovatively altering the cross-attention layers of the UNet, we effectively fuse individual identities into the generative process. This strategic integration of facial features across various scales not only enhances the robustness and consistency of the generated images but also facilitates efficient multi-reference and multi-identity generation. Our method sets a new benchmark in identity-preserving image generation, delivering state-of-the-art results in similarity metrics while maintaining prompt alignment."
    },
    {
        "title": "LocDiffusion: Identifying Locations on Earth by Diffusing in the Hilbert Space",
        "link_suffix": "/forum?id=fQSZMrjW8X",
        "link": "https://openreview.net/forum?id=fQSZMrjW8X",
        "pdf_link": "https://openreview.net/pdf?id=fQSZMrjW8X",
        "keywords": "Image geo-localization, location encoding, diffusion",
        "abstract": "Image geolocalization is a fundamental yet challenging task, aiming at inferring the geolocation on Earth where an image is taken. Existing methods approach it either via grid-based classification or via image retrieval. The geolocalization accuracy of these methods is constrained by the choice of geographic grid cell sizes or the spatial distributions of the retrieval image/geolocation gallery, and\ntheir performance significantly suffers when the spatial distribution of test images does not align with such choices. To address these limitations, we propose to leverage diffusion models to achieve image geolocalization with arbitrary resolutions. To avoid the problematic manifold reprojection step in diffusion, we developed a novel spherical positional encoding-decoding framework, which encodes points on a spherical surface (e.g., geolocations on Earth) into a Hilbert space of Spherical Harmonics coefficients and decodes points (geolocations) by mode-seeking. We call this type of position encoding Spherical Harmonics Dirac Delta (SHDD) Representation. We also propose a novel SirenNet-based architecture called CS-UNet to learn the conditional backward process in the latent SHDD space by minimizing a latent KL-divergence loss. We train a conditional latent diffusion model called LocDiffusion that generates geolocations under the guidance of images \u2013 to the best of our knowledge, the first generative model to address the image geolocalization problem. We evaluate our LocDiffusion model against SOTA image geolocalization baselines. LocDiffusion achieves competitive geolocalization performance and demonstrates significantly stronger generalizability to unseen geolocations."
    },
    {
        "title": "Diffusion models for Gaussian distributions: Exact solutions and Wasserstein errors",
        "link_suffix": "/forum?id=Wi74fYCX2f",
        "link": "https://openreview.net/forum?id=Wi74fYCX2f",
        "pdf_link": "https://openreview.net/pdf?id=Wi74fYCX2f",
        "keywords": "Diffusion models, image generation, differential equations, discretization schemes",
        "abstract": "Diffusion or score-based models recently showed high performance in image generation.\nThey rely on a forward and a backward stochastic differential equations (SDE). The sampling of a data distribution is achieved by solving numerically the backward SDE or its associated flow ODE.\nStudying the convergence of these models necessitates to control four different types of error: the initialization error, the truncation error, the discretization and the score approximation.\nIn this paper, we study theoretically the behavior of diffusion models and their numerical implementation when the data distribution is Gaussian.\nIn this restricted framework where the score function is a linear operator, we derive the analytical solutions of the backward SDE and the probability flow ODE.\nWe prove that these solutions and their discretizations are all Gaussian processes, which allows us to compute exact Wasserstein errors induced by each error type for any sampling scheme.\nMonitoring convergence directly in the data space instead of relying on Inception features, our experiments show that the recommended numerical schemes from the diffusion models literature are also the best sampling schemes for Gaussian distributions."
    },
    {
        "title": "LLM-guided spatio-temporal disease progression modelling",
        "link_suffix": "/forum?id=UTP6bdFlkB",
        "link": "https://openreview.net/forum?id=UTP6bdFlkB",
        "pdf_link": "https://openreview.net/pdf?id=UTP6bdFlkB",
        "keywords": "disease progression modelling, LLM, spatio temporal modelling",
        "abstract": "Understanding the interactions between biomarkers across brain regions during disease progression is essential for unraveling the mechanisms underlying neurodegenerative disease.  For example, in Alzheimer's Disease (AD) and other neurodegenerative conditions, existing models describe how variables interact with each other spatiotemporally within a dynamical system driven by an underlying causal substrate often based on brain connectivity. However, such methods typically grossly oversimplify the complex relationship between brain connectivity and brain pathology appearance and propagation. In this work, we propose a novel LLM guided spatio-temporal framework for disease progression, to model long-term disease progression on brain graphs based on discovery of structure from longitudinal population-level patient data. The new approach replaces the static brain connectome that drives most models of AD pathology propagation with a more comprehensive graph of local independencies jointly informed by human knowledge encoded in the LLM and observed patient data sets. Crucially, we formulate the method to work with typical medical data sets (cross-sectional or short-term longitudinal designs) in which the exact timepoint along a hidden underlying trajectory of change is unknown for each data point. Specifically, our method leverages a mixture of different LLMs as expert guides to constrain graph estimation. Furthermore, we account for the unknown temporal location of observations by embedding the proposed method within a dual-optimization framework, which simultaneously estimates a disease progression modelling (trajectory of change of biomarkers common to a population) and executes structure discovery (learning a graph capturing spatiotemporal influence among biomarkers). Experiments evaluate our method's performance in estimating regional tau-pathology propagation over the brain. Results demonstrate superiority over existing approaches using standard underpinning directed graphs estimated and fixed a-priori. Our method achieves higher prediction accuracy, faster and more stable convergence, and improves interpretability of the fitted model. The LLM produces additional factors that drive the disease which go beyond the factors contained in the prompt."
    },
    {
        "title": "TimeCAT: Hierarchical Context-Aware Transformer with Dynamic Grouping for Time Series Forecasting",
        "link_suffix": "/forum?id=0ziGSo4uWp",
        "link": "https://openreview.net/forum?id=0ziGSo4uWp",
        "pdf_link": "https://openreview.net/pdf?id=0ziGSo4uWp",
        "keywords": "Time Series, Context-Aware, Transformer",
        "abstract": "Transformer-based models have achieved significant success in time series forecasting by modeling global dependencies through self-attention mechanisms. However, these models often rely on fixed patch settings with locality constraints, tokenizing time series into spatially connected sub-series. This approach can hinder the capture of semantic relationships and lead to computational inefficiencies, especially when dealing with long sequences with complex temporal dependencies. \nIn this work, we introduce \\textbf{TimeCAT}\u2014a \\underline{Time} series \\underline{C}ontext-\\underline{A}ware \\underline{T}ransformer that dynamically groups input sequences into semantically coherent groups, enabling efficient modeling of both local and global dependencies. By appending group and global tokens, TimeCAT facilitates fine-grained information exchange through a novel \\emph{Context-Aware Mixing Block}, which utilizes self-attention and MLP mixing operations. This hierarchical approach efficiently models long sequences by processing inputs in structured contexts, reducing computational overhead without sacrificing accuracy.\nExperiments on several challenging real-world datasets demonstrate that TimeCAT achieves consistent state-of-the-art performance, significantly improving forecasting accuracy and computational efficiency over existing methods. This advancement enhances the Transformer family with improved performance, generalization ability, and better utilization of sequence information."
    },
    {
        "title": "Capability Localization: Capabilities Can be Localized rather than Individual Knowledge",
        "link_suffix": "/forum?id=f6r1mYwM1g",
        "link": "https://openreview.net/forum?id=f6r1mYwM1g",
        "pdf_link": "https://openreview.net/pdf?id=f6r1mYwM1g",
        "keywords": "Capability Localization; Knowledge Localization",
        "abstract": "Large scale language models have achieved superior performance in tasks related to natural language processing, however, it is still unclear how model parameters affect performance improvement. Previous studies assumed that individual knowledge is stored in local parameters, and the storage form of individual knowledge is dispersed parameters, parameter layers, or parameter chains, which are not unified. We found through fidelity and reliability evaluation experiments that individual knowledge cannot be localized. Afterwards, we constructed a dataset for decoupling experiments and discovered the potential for localizing data commonalities. To further reveal this phenomenon, this paper proposes a Commonality Neuron Localization (CNL) method, which successfully locates commonality neurons and achieves a neuron overlap rate of 96.42% on the GSM8K dataset. Finally, we have demonstrated through cross data experiments that commonality neurons are a collection of capability neurons that possess the capability to enhance performance."
    },
    {
        "title": "Learning Representations of Instruments for Partial Identification of Treatment Effects",
        "link_suffix": "/forum?id=Q5CLpqbrFM",
        "link": "https://openreview.net/forum?id=Q5CLpqbrFM",
        "pdf_link": "https://openreview.net/pdf?id=Q5CLpqbrFM",
        "keywords": "causal inference, partial identification, instrumental variables, treatment effect, representation learning",
        "abstract": "Reliable estimation of treatment effects from observational data is important in many disciplines, such as medicine. However, estimation is challenging when unconfoundedness as a standard assumption in the causal inference literature is violated. In this work, we leverage arbitrary (potentially high-dimensional) instruments to estimate bounds on the conditional average treatment effect (CATE). Our contributions are three-fold: (1) We propose a novel approach for partial identification through a mapping of instruments to a discrete representation space so that we yield valid bounds on the CATE. This is crucial for reliable decision-making in real-world applications. (2) We derive a two-step method that learns tight bounds using a tailored neural partitioning of the latent instrument space. As a result, we avoid instability issues due to numerical approximations or adversarial training. Furthermore, our procedure aims to reduce the estimation variance in finite-sample settings to yield more reliable estimates. (3) We show theoretically that our procedure obtains valid bounds while reducing estimation variance and we perform experiments to demonstrate the effectiveness across various settings. Overall, our procedure offers a novel path for practitioners to make use of potentially high-dimensional instruments (e.g., as in Mendelian randomization)."
    },
    {
        "title": "Segment Anything with Multiple Modalities",
        "link_suffix": "/forum?id=RBL3Gm5ygj",
        "link": "https://openreview.net/forum?id=RBL3Gm5ygj",
        "pdf_link": "https://openreview.net/pdf?id=RBL3Gm5ygj",
        "keywords": "segment anything, multimodal fusion, foundation model, parameter-efficient tuning, domain adaptation, unsupervised learning, weakly supervised learning",
        "abstract": "Robust and accurate segmentation of scenes has become one core functionality in various visual recognition and navigation tasks. This has inspired the recent development of Segment Anything Model (SAM), a foundation model for general mask segmentation. However, SAM is largely tailored for single-modal RGB images, limiting its applicability to multi-modal data captured with widely-adopted sensor suites, such as LiDAR plus RGB, depth plus RGB, thermal plus RGB, etc. We develop MM-SAM, an extension and expansion of SAM that supports cross-modal and multi-modal processing for robust and enhanced segmentation with different sensor suites. MM-SAM features two key designs, namely, unsupervised cross-modal transfer and weakly-supervised multi-modal fusion, enabling label-free and parameter-efficient adaptation toward various sensor modalities. It addresses three main challenges: 1) adaptation toward diverse non-RGB sensors for single-modal processing, 2) synergistic processing of multi-modal data via sensor fusion, and 3) mask-free training for different downstream tasks. Notably, we demonstrate that the output latent space of SAM's RGB image encoder can function as a highly abstract, shareable embedding space compatible with segmentation across different sensor modalities. Extensive experiments show that MM-SAM consistently outperforms SAM by large margins, demonstrating its effectiveness and robustness across various sensors and data modalities. Code will be released."
    }
]