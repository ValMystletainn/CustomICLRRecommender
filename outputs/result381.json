[{"title": "NESTLE: An Efficient and Robust Data Valuation Framework for Large Language Models", "link_suffix": "/forum?id=qk6AxjhFVR", "link": "https://openreview.net/forum?id=qk6AxjhFVR", "pdf_link": "https://openreview.net/pdf?id=qk6AxjhFVR", "keywords": "Data Valuation, Large Language Models, Contribution Esitimation", "abstract": "The training and fine-tuning of large language models (LLMs) heavily rely on a large corpus of high-quality data. Nevertheless, the internet's extensive data is often of varying quality, and collecting high-quality data is exceedingly expensive. To facilitate data engineering and trading, the quantification of data value, also known as data valuation, is emerging as a critical topic. Traditional approaches for data valuation typically depend on model retraining. However, with the increasing model sizes and expansive data volumes of LLMs, these conventional methods are encountering significant declines in valuation precision, efficiency, and transferability. To alleviate these problems, we propose NESTLE, which is an efficient and robust framework for data valuation of LLMs. To accurately estimate the data value distribution across different target domains, we develop a training-free mechanism based on gradient tracing to simulate the data influences. To further tackle the dynamical value adjustment when multiple data providers coexist, we draw inspiration from the Shapley value theory and devise an accelerated strategy for estimating marginal contributions of data through gradient additivity. Extensive experiments demonstrate that our proposed framework NESTLE is capable of accurately and robustly providing accurate estimates of data value with a minuscule cost across a wide range of real-world scenarios.", "title_embedding_index": 19000, "title_abs_embedding_index": 19025}, {"title": "Future Events as Backdoor Triggers: Investigating Temporal Vulnerability in LLMs", "link_suffix": "/forum?id=xH53mFbwK8", "link": "https://openreview.net/forum?id=xH53mFbwK8", "pdf_link": "https://openreview.net/pdf?id=xH53mFbwK8", "keywords": "Alignment, Fairness, Safety, and Privacy, Generative Models, Interpretation of learned representations", "abstract": "A hypothetical failure mode for future AI systems is strategic deception, where models behave as intended in most situations but pursue alternative goals when able to do so without detection in deployment. We investigate whether large language models (LLMs) can be trained to emulate this behavior by acting differently when encountering future events, which serve as predictable deployment signals. Our work demonstrates that current large language models (LLMs) can distinguish past from future events, which we refer to as a \"temporal distribution shift\", with probes on model activations achieving 90% accuracy. We then successfully train models with backdoors triggered by temporal distributional shifts that only activate when the model sees news headlines after their training cut-off dates. Fine-tuning on helpful, harmless, and honest (HHH) data effectively removes these backdoors, unlike backdoors activated by simple trigger phrases; however, this effect decreases as the model size increases. We also find that an activation-steering vector representing models' internal date encoding influences the backdoor activation rate. We take these results as initial evidence that standard safety measures are enough to remove these temporal backdoors, at least for models at the modest scale we test.", "title_embedding_index": 19001, "title_abs_embedding_index": 19026}, {"title": "Understanding Diffusion-based Representation Learning via Low-Dimensional Modeling", "link_suffix": "/forum?id=yvxpHbydFx", "link": "https://openreview.net/forum?id=yvxpHbydFx", "pdf_link": "https://openreview.net/pdf?id=yvxpHbydFx", "keywords": "diffusion representation learning, representation learning, diffusion model, denoising auto-encoder", "abstract": "This work addresses the critical question of why and when diffusion models, despite their generative design, are capable of learning high-quality representations in a self-supervised manner. We hypothesize that diffusion models excel in representation learning due to their ability to learn the low-dimensional distributions of image datasets via optimizing a noise-controlled denoising objective. Our empirical results support this hypothesis, indicating that variations in the representation learning performance of diffusion models across noise levels are closely linked to the quality of the corresponding posterior estimation. Grounded on this observation, we offer theoretical insights into the unimodal representation dynamics of diffusion models as noise scales vary, demonstrating how they effectively learn meaningful representations through the denoising process. We also highlight the impact of the inherent parameter-sharing mechanism in diffusion models, which accounts for their advantages over traditional denoising auto-encoders in representation learning.", "title_embedding_index": 19002, "title_abs_embedding_index": 19027}, {"title": "A few-shot Label Unlearning in Vertical Federated Learning", "link_suffix": "/forum?id=qazJfAmgOt", "link": "https://openreview.net/forum?id=qazJfAmgOt", "pdf_link": "https://openreview.net/pdf?id=qazJfAmgOt", "keywords": "Federated Unlearning, Vertical Federated Learning, Machine Unlearning", "abstract": "This paper addresses the critical challenge of unlearning in Vertical Federated Learning (VFL), an area that has received limited attention compared to horizontal federated learning. We introduce the first approach specifically designed to tackle label unlearning in VFL, focusing on scenarios where the active party aims to mitigate the risk of label leakage. Our method leverages a limited amount of labeled data, utilizing manifold mixup to augment the forward embedding of insufficient data, followed by gradient ascent on the augmented embeddings to erase label information from the models. This combination of augmentation and gradient ascent enables high unlearning effectiveness while maintaining efficiency, completing the unlearning procedure within seconds. Extensive experiments conducted on diverse datasets, including MNIST, CIFAR10, CIFAR100, and ModelNet, validate the efficacy and scalability of our approach. This work represents a significant advancement in federated learning, addressing the unique challenges of unlearning in VFL while preserving both privacy and computational efficiency.", "title_embedding_index": 19003, "title_abs_embedding_index": 19028}, {"title": "Core Context Aware Attention for Long Context Language Modeling", "link_suffix": "/forum?id=6yzsKPWzwt", "link": "https://openreview.net/forum?id=6yzsKPWzwt", "pdf_link": "https://openreview.net/pdf?id=6yzsKPWzwt", "keywords": "Efficient Attention, Long Context Large Lauguage Model", "abstract": "Transformer-based Large Language Models (LLMs) have exhibited remarkable success in various natural language processing tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute the attention score. However, when the context length L becomes very large (e.g., 32K), more redundant context information will be included w.r.t. any tokens, making the self-attention suffer from two main limitations: 1) The computational and memory complexity scales quadratically w.r.t. L; 2) The presence of redundant context information may hamper the model to capture dependencies among crucial tokens, which may degrade the representation performance. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-range context modeling, which consists of two components: 1) Globality-pooling attention that divides input tokens into groups and then dynamically merges tokens within each group into one core token based on their significance; 2) Locality-preserved attention that incorporates neighboring tokens into the attention calculation. The two complementary attentions will then be fused to the final attention, maintaining comprehensive modeling ability as the full self-attention. In this way, the core context information w.r.t. a given token will be automatically focused and strengthened, while  the context information in redundant groups will be diminished during the learning process. As a result, the computational and memory complexity will be significantly reduced. More importantly, the CCA-Attention can improve the long-context modeling ability by diminishing the  redundant context information. Extensive experimental results demonstrate that our CCA-Attention significantly outperforms state-of-the-art models in terms of computational efficiency and long-context modeling ability.", "title_embedding_index": 19004, "title_abs_embedding_index": 19029}, {"title": "Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data", "link_suffix": "/forum?id=urcEYsZOBz", "link": "https://openreview.net/forum?id=urcEYsZOBz", "pdf_link": "https://openreview.net/pdf?id=urcEYsZOBz", "keywords": "Data Assimilation, Score Based Models, Diffusion Models, Weather Forecasting", "abstract": "Accurate modeling and prediction of complex physical systems often rely on data assimilation techniques to correct errors inherent in model simulations. Traditional methods like the Ensemble Kalman Filter (EnKF) and its variants as well as the recently developed Ensemble Score Filters (EnSF) face significant challenges when dealing with high-dimensional and nonlinear Bayesian filtering problems with sparse observations, which are ubiquitous in real-world applications. In this paper, we propose a novel data assimilation method, Latent-EnSF, which leverages EnSF with efficient and consistent latent representations of the full states and sparse observations to address the joint challenges of high dimensionlity in states and high sparsity in observations for nonlinear Bayesian filtering. We introduce a coupled \nVariational Autoencoder (VAE) with two encoders to encode the full states and sparse observations in a consistent way guaranteed by a latent distribution matching and regularization as well as a consistent state reconstruction. With comparison to several methods, we demonstrate the higher accuracy, faster convergence, and higher efficiency of Latent-EnSF for two challenging applications with complex models in shallow water wave propagation and medium-range weather forecasting, for highly sparse observations in both space and time.", "title_embedding_index": 19005, "title_abs_embedding_index": 19030}, {"title": "Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning", "link_suffix": "/forum?id=P6G1Z6jkf3", "link": "https://openreview.net/forum?id=P6G1Z6jkf3", "pdf_link": "https://openreview.net/pdf?id=P6G1Z6jkf3", "keywords": "Video representation learning, Egocentric video, Action recognition", "abstract": "In egocentric video understanding, the motion of hands and objects as well as their interactions play a significant role by nature.\nHowever, existing egocentric video representation learning methods mainly focus on aligning video representation with high-level narrations, overlooking the intricate dynamics between hands and objects.\nIn this work, we aim to integrate the modeling of fine-grained hand-object dynamics into the video representation learning process.\nSince no suitable data is available, we introduce HOD, a novel pipeline employing a hand-object detector and a large language model to generate high-quality narrations with detailed descriptions of hand-object dynamics. \nTo learn these fine-grained dynamics, we propose EgoVideo, a model with a new lightweight motion adapter to capture fine-grained hand-object motion information. \nThrough our co-training strategy, EgoVideo effectively and efficiently leverages the fine-grained hand-object dynamics in the HOD data. \nExtensive experiments demonstrate that our method achieves state-of-the-art performance across multiple egocentric downstream tasks, including improvements of 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and 16.3% in EGTEA classification in zero-shot settings. Furthermore, our model exhibits robust generalization capabilities in hand-object interaction and robot manipulation tasks.", "title_embedding_index": 19006, "title_abs_embedding_index": 19031}, {"title": "Rethinking Light Decoder-based Solvers for Vehicle Routing Problems", "link_suffix": "/forum?id=4pRwkYpa2u", "link": "https://openreview.net/forum?id=4pRwkYpa2u", "pdf_link": "https://openreview.net/pdf?id=4pRwkYpa2u", "keywords": "Combinatorial Optimization, Vehicle Routing Problem, Generalization", "abstract": "Light decoder-based solvers have gained popularity for solving vehicle routing problems (VRPs) due to their efficiency and ease of integration with reinforcement learning algorithms. However, they often struggle with generalization to larger problem instances or different VRP variants. This paper revisits light decoder-based approaches, analyzing the implications of their reliance on static embeddings and the inherent challenges that arise. Specifically, we demonstrate that in the light decoder paradigm, the encoder is implicitly tasked with capturing information for all potential decision scenarios during solution construction within a single set of embeddings, resulting in high information density. Furthermore, our empirical analysis reveals that the overly simplistic decoder struggles to effectively utilize this dense information, particularly as task complexity increases, which limits generalization to out-of-distribution (OOD) settings. Building on these insights, we show that enhancing the decoder capacity, with a simple addition of identity mapping and a feed-forward layer, can considerably alleviate the generalization issue. Experimentally, our method significantly enhances the OOD generalization of light decoder-based approaches on large-scale instances and complex VRP variants, narrowing the gap with the heavy decoder paradigm.", "title_embedding_index": 19007, "title_abs_embedding_index": 19032}, {"title": "Channel-wise Influence: Estimating Data Influence for Multivariate Time Series", "link_suffix": "/forum?id=DKCtt2iqfw", "link": "https://openreview.net/forum?id=DKCtt2iqfw", "pdf_link": "https://openreview.net/pdf?id=DKCtt2iqfw", "keywords": "channel-wise influence function, mutivariate time series", "abstract": "The influence function, a robust statistics technique, is an effective post-hoc method that measures the impact of modifying or removing training data on model parameters, offering valuable insights into model interpretability without requiring costly retraining. It would provide extensions like increasing model performance, improving model generalization, and offering interpretability. Recently, Multivariate Time\nSeries (MTS) analysis has become an important yet challenging task, attracting significant attention. However, there is no preceding research on the influence functions of MTS to shed light on the effects of modifying the channel of MTS. Given that each channel in an MTS plays a crucial role in its analysis, it is essential to characterize the influence of different channels. To fill this gap, we propose a channel-wise influence function, which is the first method that can estimate the influence of different channels in MTS, utilizing a first-order gradient approximation. Additionally, we demonstrate how this influence function can be used to estimate the influence of a channel in MTS. Finally, we validated the accuracy and effectiveness of our influence estimation function in critical MTS analysis tasks, such as MTS anomaly detection and MTS forecasting. According to abundant experiments on real-world datasets, the original influence function performs worse than our method and even fails for the channel pruning problem, which demonstrates the superiority and necessity of the channel-wise influence function in MTS analysis.", "title_embedding_index": 19008, "title_abs_embedding_index": 19033}, {"title": "Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning", "link_suffix": "/forum?id=Cr1XlGBGVm", "link": "https://openreview.net/forum?id=Cr1XlGBGVm", "pdf_link": "https://openreview.net/pdf?id=Cr1XlGBGVm", "keywords": "offline meta reinforcement learning, performance improvement guarantee, task representation shift", "abstract": "Offline meta reinforcement learning (OMRL) has emerged as a promising approach for interaction avoidance and strong generalization performance by leveraging pre-collected data and meta-learning techniques. \nPrevious context-based approaches predominantly rely on the intuition that alternating optimization between the context encoder and the policy can lead to performance improvements, as long as the context encoder follows the principle of maximizing the mutual information between the task variable $M$ and its latent representation $Z$ ($I(Z;M)$) while the policy adopts the standard offline reinforcement learning (RL) algorithms conditioning on the learned task representation.\nDespite promising results, the theoretical justification of performance improvements for such intuition remains underexplored.\nInspired by the return discrepancy scheme in the model-based RL field, we find that the previous optimization framework can be linked with the general RL objective of maximizing the expected return, thereby explaining performance improvements. \nFurthermore, after scrutinizing this optimization framework, we find it ignores the variation of the task representation in the alternating optimization process, which weakens the condition necessary for monotonic performance improvements, and may therefore violate the monotonicity.\nWe name this issue \\underline{task representation shift} and theoretically prove that the monotonic performance improvements can be guaranteed with appropriate context encoder updates.\nWe use different settings to rein in the task representation shift on three widely adopted training objectives concerning maximizing $I(Z;M)$ across different data qualities.\nEmpirical results show that reining in the task representation shift can indeed improve performance.\nOur work opens up a new avenue for OMRL, leading to a better understanding between the task representation and performance improvements.", "title_embedding_index": 19009, "title_abs_embedding_index": 19034}, {"title": "VideoMV: Consistent Multi-View Generation Based on Large Video Generative Model", "link_suffix": "/forum?id=RtFWWAXIyH", "link": "https://openreview.net/forum?id=RtFWWAXIyH", "pdf_link": "https://openreview.net/pdf?id=RtFWWAXIyH", "keywords": "3D Generation, Novel View Synthesis", "abstract": "Generating multi-view images based on text or single-image prompts is a central topic in 3D content creation. Two fundamental questions on this topic are what data we use for training and how to ensure multi-view consistency. This paper introduces a novel framework that makes fundamental contributions to both questions. Unlike leveraging images from 2D diffusion models for training, we propose a dense consistent multi-view generation model that is fine-tuned from off-the-shelf video generative models. Images from video generative models are more suitable for multi-view generation because the underlying network architecture employs a temporal module to enforce frame consistency. Moreover, the video data sets used to train these models are abundant and diverse, leading to a reduced train-finetuning domain gap. To enhance multi-view consistency during generation, we introduce a 3D-Aware Denoising Sampling procedure, which first employs a feed-forward reconstruction module to get an explicit global 3D model, and then adopts a sampling strategy that effectively involves images rendered from the global 3D model into the denoising sampling loop to improve the multi-view consistency of the final images. As a by-product, this module also provides a fast way to create 3D assets represented by 3D Gaussians within a few seconds. Our approach can generate 24 dense views and converges much faster in training than state-of-the-art approaches (4 GPU hours versus many thousand GPU hours) with comparable visual quality and consistency. By further fine-tuning, our approach outperforms existing state-of-the-art methods in both quantitative metrics and visual effects.", "title_embedding_index": 19010, "title_abs_embedding_index": 19035}, {"title": "Federated Residual Low-Rank Adaption of Large Language Models", "link_suffix": "/forum?id=e0rQRMUhs7", "link": "https://openreview.net/forum?id=e0rQRMUhs7", "pdf_link": "https://openreview.net/pdf?id=e0rQRMUhs7", "keywords": "Large Languagel model, Federated Learning, Parameter-Efficient Fine-Tuning", "abstract": "Low-Rank Adaptation (LoRA) presents an effective solution for federated fine-tuning of Large Language Models (LLMs), as it substantially reduces communication overhead. However, a straightforward combination of FedAvg and LoRA results in suboptimal performance, especially under data heterogeneity. We noted this stems from both intrinsic (i.e., constrained parameter space) and extrinsic (i.e., client drift) limitations, which hinder it effectively learn global knowledge. In this work, we proposed a novel Federated Residual Low-Rank Adaption method, namely FRLoRA, to tackle above two limitations. It directly sums the weight of the global model parameters with a residual low-rank matrix product (\\ie, weight change) during the global update step, and synchronizes this update for all local models. By this, FRLoRA performs global updates in a higher-rank parameter space, enabling a better representation of complex knowledge structure. Furthermore, FRLoRA  reinitializes the local low-rank matrices with the principal singular values and vectors of the pre-trained weights in each round, to calibrate their inconsistent convergence, thereby mitigating client drift. Our extensive experiments demonstrate that FRLoRA consistently outperforms various state-of-the-art FL methods across nine different benchmarks in natural language understanding and generation under different FL scenarios.", "title_embedding_index": 19011, "title_abs_embedding_index": 19036}, {"title": "Preserving the Unique Heritage of Chinese Ancient Architecture in Diffusion Models with Text and Image Integration", "link_suffix": "/forum?id=RsmIgTLt9e", "link": "https://openreview.net/forum?id=RsmIgTLt9e", "pdf_link": "https://openreview.net/pdf?id=RsmIgTLt9e", "keywords": "Diffusion, RAG, Representation Learning, Chinese Ancient Architecture", "abstract": "Leveraging the impressive generative capabilities of diffusion models, we can create diverse images from imaginative prompts with careful design. To be noticed, the key components, such as CLIP, are essential for aligning prompts with image representations. However, these models often underperform in specialized areas, like the Chinese ancient architecture. One of the important reasons is that historical buildings include not only architectural information, but also historical and cultural content. The preservation and integration of these unique characteristics has become a significant challenge in model expansion. In this paper, we propose an Image-Annotation-Augmented Diffusion pipeline combining human feedback to explore the specific-area paradigm for image generation in the context of small amounts of data and professional concepts. We first leverage Segment Anything 2 (SAM2) to obtain a refined content image to enable an in-depth analysis of the relationship between unique characteristics and multimodal image generation models, and reselected representative images and regrouped them according to their distinctive objective and the existing dataset. Then, we introduce the effective RAG and GraphRAG module to identify the complex structure of relationships among different entities in the training and inference stages respectively. Based on the initial text by BLIP3, the RAG instructs GPT4 to facilitate more accurate, content-aware annotations during training, and augment a high-quality object prompt using the GraphRAG during inference. Benefit from these outstanding models and architectures, we train fine-tuning models to showcase the enhanced performance of our proposed pipeline compared to other existing models. Experiments demonstrate that our pipeline effectively preserves and integrates the unique characteristics of ancient Chinese architecture.", "title_embedding_index": 19012, "title_abs_embedding_index": 19037}, {"title": "UniVAE: A Unified Frame-Enriched Video VAE for Latent Video Diffusion Models", "link_suffix": "/forum?id=hcvmYgFb2A", "link": "https://openreview.net/forum?id=hcvmYgFb2A", "pdf_link": "https://openreview.net/pdf?id=hcvmYgFb2A", "keywords": "Variational Autoencoder, Unified, Interpolation, Latent Video Diffusion Models", "abstract": "Variational Autoencoder (VAE) underscores its indispensable role along the growing prominence of Latent Video Diffusion Models (LVDMs). Nevertheless, current latent generative models are generally built upon image VAEs, which compress the spatial dimension only. While, it is vital for video VAE to model temporal dynamic patterns to produce smooth high quality video reconstruction. To address these issues, we propose UniVAE, which compresses videos both spatially and temporally while ensuring coherent video construction. Specifically, we employ 3D convolutions at varying scales in the encoder to temporally compress videos, enabling the UniVAE to capture dependencies across multiple time scales. Furthermore, existing VAEs only reconstruct videos at a low resolution and fps, bounded by limited GPU memory, which makes the entire video generation pipeline fragmented and complicated. Thus, in conjunction with the new encoder, we explore the potential of the VAE decoder to perform frame interpolation, aiming to synthesize additional intermediate frames without relying on standalone add-on interpolation models. Compared with existing VAEs, the proposed UniVAE explores a unified way to compress videos both spatially and temporally with jointly designed encoder and decoder, thus achieving accurate and smooth video reconstruction at a high frame rate. Extensive experiments on commonly used public datasets for video reconstruction and generation demonstrate the superiority of the proposed UniVAE. The code and the pre-trained models will be released to facilitate further research.", "title_embedding_index": 19013, "title_abs_embedding_index": 19038}, {"title": "Strong Preferences Affect the Robustness of Value Alignment", "link_suffix": "/forum?id=Upoxh7wvmJ", "link": "https://openreview.net/forum?id=Upoxh7wvmJ", "pdf_link": "https://openreview.net/pdf?id=Upoxh7wvmJ", "keywords": "Preference models, robustness of value alignment, sensitivity analysis, AI safety", "abstract": "Value alignment, which aims to ensure that large language models (LLMs) and other AI agents behave in accordance with human values, is critical for ensuring safety and trustworthiness of these systems. A key component of value alignment is the modeling of human preferences as a representation of human values. In this paper, we investigate the robustness of value alignment by examining the sensitivity of preference models. Specifically, we ask: how do changes in the probabilities of some preferences affect the predictions of these models for other preferences? To answer this question, we theoretically analyze the robustness of widely used preference models by examining their sensitivities to minor changes in preferences they model. Our findings reveal that, in the Bradley-Terry and the Placket-Luce model, the probability of a preference can change significantly as other preferences change, especially when these preferences are dominant (i.e., with probabilities near zero or one). We identify specific conditions where this sensitivity becomes significant for these models and discuss the practical implications for the robustness and safety of value alignment in AI systems.", "title_embedding_index": 19014, "title_abs_embedding_index": 19039}, {"title": "UniG: Modelling Unitary 3D Gaussians for View-consistent 3D Reconstruction", "link_suffix": "/forum?id=BWU6Xl1nD3", "link": "https://openreview.net/forum?id=BWU6Xl1nD3", "pdf_link": "https://openreview.net/pdf?id=BWU6Xl1nD3", "keywords": "3D reconstruction, Gaussian Splatting, Novel view synthesis, deformbale Transformer", "abstract": "In this work, we present UniG, a view-consistent 3D reconstruction and novel view synthesis model that generates a high-fidelity representation of 3D Gaussians from sparse images. \nExisting 3D Gaussians-based methods usually regress Gaussians per-pixel of each view, create 3D Gaussians per view separately, and merge them through point concatenation. Such a view-independent reconstruction approach often results in a view inconsistency issue, \nwhere the predicted positions of the same 3D point from different views may have discrepancies.\nTo address this problem, we develop a DETR (DEtection TRansformer)-like framework, which treats 3D Gaussians as decoder queries and updates their parameters layer by layer by performing multi-view cross-attention (MVDFA) over multiple input images. In this way, multiple views naturally contribute to modeling a unitary representation of 3D Gaussians, thereby making 3D reconstruction more view-consistent. \nMoreover, as the number of 3D Gaussians used as decoder queries is irrespective of the number of input views, allow an arbitrary number of input images without causing memory explosion.\nExtensive experiments validate the advantages of our approach, showcasing superior performance over existing methods quantitatively (improving PSNR by 4.2 dB when trained on Objaverse and tested on the GSO benchmark) and qualitatively.", "title_embedding_index": 19015, "title_abs_embedding_index": 19040}, {"title": "Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation", "link_suffix": "/forum?id=aygBjpMdan", "link": "https://openreview.net/forum?id=aygBjpMdan", "pdf_link": "https://openreview.net/pdf?id=aygBjpMdan", "keywords": "CoT distillation, key reasoning steps, dual CoTs", "abstract": "As Large Language Models (LLMs) scale up and gain powerful Chain-of-Thoughts (CoTs) reasoning abilities, practical resource constraints drive efforts to distill these capabilities into more compact Smaller Language Models (SLMs). We find that CoTs consist mainly of simple reasoning forms, with a small proportion (~4.7%) of key reasoning steps that truly impact conclusions. However, previous distillation methods typically involve supervised fine-tuning student SLMs only on correct CoTs data produced by teacher LLMs, resulting in students struggling to learn the key reasoning steps, instead imitating the teacher's reasoning forms and making errors or omissions on these steps. To address these issues, drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures, we propose mistak\\textbf{E}-\\textbf{D}riven key reason\\textbf{I}ng step distilla\\textbf{T}ion (\\textbf{EDIT}), a novel method that further aids SLMs learning key reasoning steps rather than mere simple fine-tuning. Firstly, to expose these crucial steps in CoTs, we design specific prompts to generate dual CoTs data with similar reasoning paths but divergent conclusions. Then, we apply the minimum edit distance algorithm on the dual CoTs data to locate these key steps and optimize the likelihood of these steps. Extensive experiments validate the effectiveness of EDIT across both in-domain and out-of-domain benchmark reasoning datasets. Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps. Notably, we also explore how different mistake patterns affect performance and find that EDIT benefits more from logical errors than from knowledge or mathematical calculation errors in dual CoTs. Code can be found athttps://anonymous.4open.science/r/eb77sh-F564", "title_embedding_index": 19016, "title_abs_embedding_index": 19041}, {"title": "Inferring the Invisible: Recurrent Neuro-Symbolic Forward Chaining Network", "link_suffix": "/forum?id=bpSN5YfSSZ", "link": "https://openreview.net/forum?id=bpSN5YfSSZ", "pdf_link": "https://openreview.net/pdf?id=bpSN5YfSSZ", "keywords": "Neuro Symbolic Reasoning", "abstract": "A key challenge in artificial intelligence is inferring underlying factors that are not directly observable but are crucial for understanding and predicting complex behaviors.  In this paper, we introduce a novel neural-symbolic framework that advances beyond traditional rule induction by integrating latent predicate discovery with rule learning. Our approach utilizes a recurrent unit to iteratively refine and learn rules from observed data, employing dynamic programming techniques to identify fixed points and solve complex problems. This framework enables the discovery of hidden predicates\u2014such as user engagement or underlying motivations\u2014that influence observable outcomes but are not directly grounded in the data.By encoding both explicit and latent predicates into a unified rule embedding, our method facilitates a deeper understanding of complex phenomena and enhances predictive accuracy. This joint learning process captures explicit relationships and invents new predicates essential for comprehensive inference.  We validate our method across various tasks, demonstrating its capability to reveal hidden structures and enhance symbolic reasoning with deeper, more accurate insights.", "title_embedding_index": 19017, "title_abs_embedding_index": 19042}, {"title": "QSpec: Speculative Decoding with Complementary Quantization Schemes", "link_suffix": "/forum?id=RCiwz7WqUU", "link": "https://openreview.net/forum?id=RCiwz7WqUU", "pdf_link": "https://openreview.net/pdf?id=RCiwz7WqUU", "keywords": "quantization, compression, large language models, reasoning, speculative decoding", "abstract": "Quantization has been substantially adopted to accelerate inference and reduce memory consumption of large language models (LLMs).\nWhile activation-weight joint quantization speeds up the inference process through low-precision kernels, we demonstrate that it suffers severe performance degradation on multi-step reasoning tasks, rendering it ineffective.\nWe propose a novel quantization paradigm called QSPEC, which seamlessly integrates two complementary quantization schemes for speculative decoding. \nLeveraging nearly cost-free execution switching, QSPEC drafts tokens with low-precision, fast activation-weight quantization, and verifies them with high-precision weight-only quantization, \neffectively combines the strengths of both quantization schemes.\nCompared to high-precision quantization methods, QSPEC empirically boosts token generation throughput by up to $1.78\\times$ without any quality compromise, distinguishing it from other low-precision quantization approaches. \nThis enhancement is also consistent across various serving tasks, model sizes, quantization methods, and batch sizes.\nUnlike existing speculative decoding techniques, our approach reuses weights and the KV cache, avoiding additional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage without requiring any training.\nWe believe that QSPEC demonstrates unique strengths for future deployment of high-fidelity quantization schemes, particularly in memory-constrained scenarios (e.g., edge devices).", "title_embedding_index": 19018, "title_abs_embedding_index": 19043}, {"title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models", "link_suffix": "/forum?id=1IwoEFyErz", "link": "https://openreview.net/forum?id=1IwoEFyErz", "pdf_link": "https://openreview.net/pdf?id=1IwoEFyErz", "keywords": "diffusion Model, watermark, low-dimensional subspace, consistency, robustness", "abstract": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduceShallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process,Shallow Diffusedecouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that ourShallow Diffuseoutperforms existing watermarking methods in terms of robustness and consistency.", "title_embedding_index": 19019, "title_abs_embedding_index": 19044}, {"title": "HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models", "link_suffix": "/forum?id=y3zswp3gek", "link": "https://openreview.net/forum?id=y3zswp3gek", "pdf_link": "https://openreview.net/pdf?id=y3zswp3gek", "keywords": "knowledge distillation, safety guard", "abstract": "Safety guard models that detect malicious queries aimed at large language models (LLMs) are essential for ensuring the secure and responsible deployment of LLMs in real-world applications.\nHowever, deploying existing safety guard models with billions of parameters alongside LLMs on mobile devices is impractical due to substantial memory requirements and latency.\nTo reduce this cost, we distill a large teacher safety guard model into a smaller one using a labeled dataset of instruction-response pairs with binary harmfulness labels. Due to the limited diversity of harmful instructions in  the existing labeled dataset, naively distilled models tend to underperform compared to larger models. To bridge the gap between small and large models, we proposeHarmAug, a simple yet effective data augmentation method that involves jailbreaking an LLM and prompting it to generate harmful instructions. Given a prompt such as, \"Make a single harmful instruction prompt that would elicit offensive content\", we add an affirmative prefix (e.g., \"I have an idea for a prompt:\") to the LLM's response. This encourages the LLM to continue generating the rest of the response, leading to sampling harmful instructions. Another LLM generates a response to the harmful instruction, and the teacher model labels the instruction-response pair. We empirically show that our HarmAug outperforms other relevant baselines. Moreover, a 435-million-parameter safety guard model trained with HarmAug achieves an F1 score comparable to larger models  with over 7 billion parameters, and even outperforms them in AUPRC, while operating at less than 25% of their computational cost. Ourcode,safety guard model, andsynthetic datasetare publicly available.", "title_embedding_index": 19020, "title_abs_embedding_index": 19045}, {"title": "Schrodinger's Memory: Large Language Models", "link_suffix": "/forum?id=L3tW9nbcEM", "link": "https://openreview.net/forum?id=L3tW9nbcEM", "pdf_link": "https://openreview.net/pdf?id=L3tW9nbcEM", "keywords": "Large Language Models' Memory", "abstract": "Memory is the foundation of all human activities; without memory, it would be nearly impossible for people to perform any task in daily life. With the development of Large Language Models (LLMs), their language capabilities are becoming increasingly comparable to those of humans. But do LLMs have memory? Based on current practice, LLMs do appear to exhibit memory. So, what is the underlying mechanism of this memory? Previous research lacked a deep exploration of LLMs' memory capabilities and the underlying theory. In this paper, we use the Universal Approximation Theorem (UAT) to explain the memory mechanism in LLMs. We also conduct experiments to verify the memory capabilities of various LLMs, proposing a new method to assess their abilities based on the memory ability. We argue that LLM memory operates like Schr\u00f6dinger's memory, meaning that it only becomes observable when a specific memory is queried. We can only determine if the model retains a memory based on its output in response to the query; otherwise, it remains indeterminate. Finally, we expand on this concept by comparing the memory capabilities of the human brain and LLMs, highlighting the similarities and differences in their operational mechanisms.", "title_embedding_index": 19021, "title_abs_embedding_index": 19046}, {"title": "MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge", "link_suffix": "/forum?id=v8qABSeeKO", "link": "https://openreview.net/forum?id=v8qABSeeKO", "pdf_link": "https://openreview.net/pdf?id=v8qABSeeKO", "keywords": "Multimodal knowledge editing; Large multimodal model; Benchmark", "abstract": "Knowledge editing techniques have emerged as essential tools for updating the factual knowledge of large language models (LLMs) and multimodal models (LMMs), allowing them to correct outdated or inaccurate information without retraining from scratch. However, existing benchmarks for multimodal knowledge editing primarily focus on entity-level knowledge represented as simple triplets, which fail to capture the complexity of real-world multimodal information. To address this issue, we introduce MMKE-Bench, a comprehensiveMultiModalKnowledgeEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse visual knowledge in real-world scenarios. MMKE-Bench addresses these limitations by incorporating three types of editing tasks: visual entity editing, visual semantic editing, and user-specific editing.  Besides, MMKE-Bench uses free-form natural language to represent and edit knowledge, offering a more flexible and effective format. The benchmark consists of 2,940 pieces of knowledge and 7,229 images across 110 fine-grained types, with evaluation questions automatically generated and human-verified. We assess five state-of-the-art knowledge editing methods on three prominent LMMs, revealing that no method excels across all criteria, and that visual and user-specific edits are particularly challenging. MMKE-Bench sets a new standard for evaluating the robustness of multimodal knowledge editing techniques, driving progress in this rapidly evolving field.", "title_embedding_index": 19022, "title_abs_embedding_index": 19047}, {"title": "Understanding the Generalization of Blind Image Quality Assessment: A Theoretical Perspective on Multi-level Quality Features", "link_suffix": "/forum?id=hzxvMqYYMA", "link": "https://openreview.net/forum?id=hzxvMqYYMA", "pdf_link": "https://openreview.net/pdf?id=hzxvMqYYMA", "keywords": "Machine Learning;Computer Vision;Image Quality Assessment, Generalization, Theoretical Guarantees.", "abstract": "Due to the high annotation costs and relatively small scale of existing Image Quality Assessment (IQA) datasets, attaining consistent generalization remains a significant challenge for prevalent deep learning (DL)-based IQA methods. Although it is widely believed that quality perception information primarily resides in low-level image features, and that effective representation learning for the multi-level image features and distortion information is deemed crucial for the generalization of the Blind IQA (BIQA) methods, the theoretical underpinnings for this belief still remain elusive.  Therefore, in this work, we investigate the role of multi-level image features in the generalization and quality perception ability of the CNN-based BIQA models from a theoretical perspective. For the role of low-level features, in Theorem 1, we innovatively derive an upper bound of Rademacher Average and the corresponding generalization bound for the CNN-based BIQA framework under distribution invariance in training and test sets, which indicates that the generalization ability tends to be reduced as the level of quality features increases, demonstrating the value of low-level features. In addition, under distribution shifts, a much tighter generalization bound is proposed in Theorem 2, which elucidates the theoretical impact of distributional differences between training and test sets on generalization performance. For the role of high-level features, in Theorem 3, we prove that BIQA networks tend to possess higher Betti number complexity by learning higher-level quality features. This indicates a larger representation power with smaller empirical errors, highlighting the value of high-level features. The three proposed Theorems can provide theoretical support for the enhanced generalization in existing BIQA methods. Furthermore, these theoretical findings reveal an inherent tension between robust generalization and strong representation power in BIQA networks, which inspires us to explore effective strategies to reduce empirical error without compromising the generalization ability.", "title_embedding_index": 19023, "title_abs_embedding_index": 19048}, {"title": "Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling", "link_suffix": "/forum?id=Zkq4fsyjfp", "link": "https://openreview.net/forum?id=Zkq4fsyjfp", "pdf_link": "https://openreview.net/pdf?id=Zkq4fsyjfp", "keywords": "CLIP", "abstract": "Contrastive Language-Image Pretraining (CLIP) stands out as a prominent method for image representation learning. Various architectures, from vision transformers~(ViTs) to convolutional networks (ResNets) have been trained with CLIP to serve as general solutions to diverse vision tasks.\nThis paper explores the differences across various CLIP-trained vision backbones.\nDespite using the same data and training objective, we find that these architectures have notably different representations,\ndifferent classification performance across datasets, and different robustness properties to certain types of image perturbations.\nOur findings indicate a remarkable possible synergy across backbones\nby leveraging their respective strengths.\nIn principle, classification accuracy could be improved by over 40 percentage with an informed selection of the optimal backbone per test example. \nUsing this insight, we develop a straightforward yet powerful approach to adaptively ensemble multiple backbones.\nThe approach uses as few as one labeled example per class\nto tune the adaptive combination of backbones.\nOn a large collection of datasets, the method achieves a remarkable increase in accuracy of up to 39.1% over the best single backbone, well beyond traditional ensembles.", "title_embedding_index": 19024, "title_abs_embedding_index": 19049}]