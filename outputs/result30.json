[
    {
        "title": "Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning",
        "link_suffix": "/forum?id=GBIUbwW9D8",
        "link": "https://openreview.net/forum?id=GBIUbwW9D8",
        "pdf_link": "https://openreview.net/pdf?id=GBIUbwW9D8",
        "keywords": "AI agent, tree search, self-improvement",
        "abstract": "Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly.\nR-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning."
    },
    {
        "title": "Learning Generalizable and Well-Shaped Reward Functions from Too Few Demonstrations",
        "link_suffix": "/forum?id=qk8JMpwWPh",
        "link": "https://openreview.net/forum?id=qk8JMpwWPh",
        "pdf_link": "https://openreview.net/pdf?id=qk8JMpwWPh",
        "keywords": "Inverse Reinforcement Learning, Imitation Learning, Learning from Few Demonstrations",
        "abstract": "Inverse reinforcement learning (IRL) is an important problem that aims to learn a reward function and policy directly from demonstrations, which can often be easier to provide than a well-shaped reward function. However, many real-world tasks include natural variations (i.e., a cleaning robot in a house with different furniture configurations), making it costly to provide demonstrations of every possible scenario. We tackle the problem of few-shot IRL with multi-task data where the goal is for an agent to learn from a few demonstrations, not sufficient to fully specify the task, by utilizing an offline multi-task demonstration dataset. Prior work utilizes meta-learning or imitation learning which additionally requires reward labels, a multi-task training environment, or cannot improve with online interactions. We propose Multitask Discriminator Proximity-guided IRL (MPIRL), an IRL method that learns a generalizable and well-shaped reward function by learning a multi-task generative adversarial discriminator with an auxiliary proximity-to-expert reward. We demonstrate the effectiveness of our method on multiple navigation and manipulation tasks."
    },
    {
        "title": "Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling-Downscaling Structure",
        "link_suffix": "/forum?id=MdBt0ttZrZ",
        "link": "https://openreview.net/forum?id=MdBt0ttZrZ",
        "pdf_link": "https://openreview.net/pdf?id=MdBt0ttZrZ",
        "keywords": "Super-Resoltuion, Laplacian Pyramid-based detail loss, Repeated Upscaling-Downscaling Process, Supervised learning",
        "abstract": "With advances in artificial intelligence, image processing has also gained significant interest. Image super-resolution, in particular, is a vital technology closely related to real-life applications, as it enhances the quality of existing images. Since enhancing details is important in the super-resolution task, it is often necessary to activate pixels that appear only at high frequencies, distinct from low frequencies. \nIn this paper, we propose a method that generates a detail image separately from the super-resolution image. This approach introduces a loss function designed to enhance detail, allowing the model to generate an upscaled image and a detail image independently, with control over each component. Consequently, the model can focus more effectively on high-frequency data, resulting in an improved super-resolution image. Our loss function utilizes detail images based on the Laplacian Pyramid, which is widely used in image reconstruction. The multi-level property of the Laplacian Pyramid is well-suited for applying upscaling and downscaling repeatedly.\nOur experiments demonstrate that a structure applying the repetition of upscaling and downscaling integrates effectively with our detail loss control. The results show that this structure efficiently extracts diverse information, enabling the generation of improved super-resolution images from multiple low-resolution features. We conduct two types of experiments. First, we construct a simple CNN-based model incorporating the Laplacian Pyramid-based detail control and a repeated upscaling and downscaling structure. This model achieves a state-of-the-art PSNR value of 38.48 dB, surpassing all currently available CNN-based models and even some attention-based models without additional special techniques. Second, we apply our methods to existing attention-based models on a small scale. In all the experiments, attention-based models using our detail loss show improvements compared to the original models. These experiments demonstrate that our detail control loss effectively enhances performance, regardless of the model's structure in the super-resolution task."
    },
    {
        "title": "Using Reinforcement Learning to Investigate Neural Dynamics During Motor Learning",
        "link_suffix": "/forum?id=d8hURACo0P",
        "link": "https://openreview.net/forum?id=d8hURACo0P",
        "pdf_link": "https://openreview.net/pdf?id=d8hURACo0P",
        "keywords": "Computational Neuroscience, Motor Learning, RNN modeling, Reinforcement LearningRecent work characterized shifts in preparatory activity of the motor cortex during motor learning.  The specific geometry of the shifts during learning, washout, and relearning blocks was hypothesized to implement the acquisition, retention, and retrieval of motor memories.  %This leads to the question: what learning algorithms lead to the emergence of these phenomena when monkeys perform a curl field (CF) adaptation task?  We sought to develop a framework to train recurrent neural network (RNN) models that could be used to study these motor learning phenomena. We built an environment for a curl field (CF) motor learning task and trained RNNs with reinforcement learning (RL) with novel regularization terms to perform behaviorally realistic reaching trajectories over the course of learning.  Our choice of RL rather than supervised learning was motivated by the idea that motor adaptation to a novel environment is a process of reoptimization.  We find these models, despite lack of supervision, reproduce many behavioral findings from human and monkey CF adaptation experiments.  Relearning is faster than initial learning, indicating formation of motor memories.  Optimal reaches under a CF are not straight, but rather curved, which is optimal and has been observed in humans and macaques.  These models also captured key neurophysiological findings. We found that the model’s preparatory activity shifted uniformly, independently of the distance to the CF trained target.  %We also found the washout shift was consistently approximately orthogonal to the learning shift.  Finally, we found that the washout shift becomes more orthogonal to the learning shift when the RNNs are pretrained to learn CF dynamics.  We argue the increased fit to neurophysiological recordings is driven by more generalizable circuitry in the pretrained model.  This suggests that some aspects of the neural geometry underlying motor memory may be influenced by priors learned over experience in the motor cortex circuitry.  Together, this work provides a modeling framework for exploring algorithms that support motor memory, acquisition, retention, and retrieval during motor learning. %These results may inform additional future theoretical exploration of the algorithms underlying motor memory acquisition, retention, and retrieval.",
        "abstract": "Recent work characterized shifts in preparatory activity of the motor cortex during motor learning. \nThe specific geometry of the shifts during learning, washout, and relearning blocks was hypothesized to implement the acquisition, retention, and retrieval of motor memories. \nWe sought train recurrent neural network (RNN) models that could be used to study these motor learning phenomena.\nWe built an environment for a curl field (CF) motor learning task and trained RNNs with reinforcement learning (RL) with novel regularization terms to perform behaviorally realistic reaching trajectories over the course of learning. \nOur choice of RL rather than supervised learning was motivated by the idea that motor adaptation to a novel environment, in the absence of demonstrations, is a process of reoptimization. \nWe find these models, despite lack of supervision, reproduce many behavioral findings from human and monkey CF adaptation experiments. \nRelearning is faster than initial learning, indicating formation of motor memories. \nOptimal reaches under a CF are not straight, but rather curved, which is optimal and has been observed in humans and macaques. \nThese models also captured key neurophysiological findings. \nWe found that the model’s preparatory activity existed in a force-predictive subspace that remained stable across learning, washout, and relearning. \nAdditionally, preparatory activity shifted uniformly, independently of the distance to the CF trained target. \nFinally, we found that the washout shift became more orthogonal to the learning shift, and hence more brain-like, when the RNNs are pretrained to have prior experience with CF dynamics. \nWe argue the increased fit to neurophysiological recordings is driven by more generalizable and structured dynamical motifs in the model with prior experience from pretraining. \nThis suggests that the near-orthogonality of learning-washout neural geometry underlying motor memory may be influenced by structured dynamical motifs in the motor cortex circuitry developed from prior experience. \nTogether, our work takes a step towards elucidating the factors that support motor memory, acquisition, retention, and retrieval during motor learning."
    },
    {
        "title": "Can Multimodal Foundation Models Perform Visual Temporal Reasoning?",
        "link_suffix": "/forum?id=fCi4o83Mfs",
        "link": "https://openreview.net/forum?id=fCi4o83Mfs",
        "pdf_link": "https://openreview.net/pdf?id=fCi4o83Mfs",
        "keywords": "visual temporal reasoning, video understanding, benchmark, vision-language benchmark, video-language models, evaluation",
        "abstract": "State-of-the-art Multimodal Foundation Models (MFMs) typically interpret videos as sequences of individual frames, and achieve relatively high accuracy on existing temporal reasoning video benchmarks. However, are these models truly comprehending the videos as continuous sequences, or just isolated single frames? To our surprise, many questions posed by existing temporal reasoning video benchmarks can be accurately answered using a single, a few, or even out-of-order frames. This suggests that the visual temporal reasoning capabilities of current models are overestimated. To address this limitation, we introduce TVBench, a novel benchmark crafted to rigorously assess MFMs' temporal reasoning capabilities in video understanding. Unlike prior benchmarks, our benchmark strictly demands reasoning about the transitions across all frames by focusing on dynamic, continuous processes that unfold over time. TVBench comprises 1,484 carefully curated, human-annotated questions spanning six tasks (i.e. action count, direction, rotation, shape&trend, velocity&frequency, and visual cues), applied to 1,417 videos that encompass human-centric, real-world, and simulated scenarios. We present a comprehensive evaluation of 23 MFMs, including both open-source and proprietary models. Our benchmark exposes a more significant performance gap between human-level and MFM-enabled temporal reasoning video understanding than was previously known. This performance gap highlights the substantial challenges posed by the benchmark. Furthermore, we provide a thorough analysis across different subsets of evaluation results, highlighting key research challenges and potential directions for future MFMs. We believe TVBench will serve as a crucial testbed for evaluating the next-generation MFMs and as a call to the community to develop AI agents capable of comprehending the human world dynamics through the video modality."
    },
    {
        "title": "Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers",
        "link_suffix": "/forum?id=yzloNYH3QN",
        "link": "https://openreview.net/forum?id=yzloNYH3QN",
        "pdf_link": "https://openreview.net/pdf?id=yzloNYH3QN",
        "keywords": "Large Language Model, Information Retrieval",
        "abstract": "Information retrieval (IR) systems have played a vital role in modern digital life and have cemented their continued usefulness in this new era of generative AI via retrieval-augmented generation. With strong language processing capabilities and remarkable versatility, large language models (LLMs) have become popular choices for zero-shot re-ranking in IR systems. So far, LLM-based re-ranking methods rely on strong generative capabilities, which restricts their use to either specialized or powerful proprietary models. Given these restrictions, we ask: is autoregressive generation necessary and optimal for LLMs to perform re-ranking? We hypothesize that there are abundant signals relevant to re-ranking within LLMs that might not be used to their full potential via generation. To more  directly leverage such signals, we propose in-context re-ranking (ICR), a novel method  that leverages the change in attention pattern caused by the search query for accurate and efficient re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration method using a content-free query. Due to the absence of generation, ICR only requires two ($O(1)$) forward passes to re-rank $N$ documents, making it substantially more efficient than generative re-ranking methods that require at least $O(N)$ forward passes. Our novel design also enables ICR to be applied to any LLM without specialized training while guaranteeing a well-formed ranking. Extensive experiments with two popular open-weight LLMs on standard single-hop and multi-hop information retrieval benchmarks show that ICR outperforms RankGPT while cutting the latency by more than 60% in practice. Through detailed analyses, we show that ICR's performance is specially strong on tasks that require more complex re-ranking signals, such as handling contextualization and contradiction between the query and passages, as well as information integration across multiple passages. Our findings call for further exploration on novel ways of utilizing open-weight LLMs beyond text generation."
    },
    {
        "title": "Edge Matters: A Predict-and-Search Framework for MILP based on Sinkhorn-Nomalized Edge Attention Networks and Adaptive Regret-Greedy Search",
        "link_suffix": "/forum?id=9p2YMVs1Tl",
        "link": "https://openreview.net/forum?id=9p2YMVs1Tl",
        "pdf_link": "https://openreview.net/pdf?id=9p2YMVs1Tl",
        "keywords": "MILP; EGAT; Sinkhorn; Adaptive Trust Region",
        "abstract": "Predict-and-search is increasingly becoming the predominant framework for solving Mixed-Integer Linear Programming (MILP) problems through the application of ML algorithms. Traditionally, MILP problems are represented as bipartite graphs, wherein nodes and edges encapsulate critical information pertaining to the objectives and constraints. However, existing ML approaches have primarily concentrated on extracting features from nodes while largely ignoring those associated with edges. To bridge this gap, we propose a novel framework named \\model{} which leverages a graph neural network SKEGAT that integrates both node and edge features. Furthermore, we design an adaptive Regret-Greedy algorithm to break the barriers of the problem scale and hand-crafted tuning. Experiments across a variety of combinatorial optimization problems show that \\model{} surpasses current SOTA algorithms, delivering notable enhancements in both solution accuracy and computational efficiency."
    },
    {
        "title": "A Systemic Review of Static Memory Analysis",
        "link_suffix": "/forum?id=Mphd6Sf6z4",
        "link": "https://openreview.net/forum?id=Mphd6Sf6z4",
        "pdf_link": "https://openreview.net/pdf?id=Mphd6Sf6z4",
        "keywords": "C++, Java, pattern matching, Python, SharpChecker, static memory analysis, symbolic execution",
        "abstract": "This review aims to evaluate and compare various static analysis tools across multiple programming languages for memory management. The tools and techniques under scrutiny include pattern matching, symbolic execution, CppCheck, SharpChecker, FindBugs, CheckStyle, and Pylint. When examining the methods, pattern-matching, and symbolic execution, we identified implementations using pattern-matching and symbolic execution for each programming language. We focus on understanding the full scope of their capabilities and effectiveness in managing internal and external memory  components such as RAM, SRAM, PROM, Cache, Optical Drive, etc. While static analysis tools do not directly analyze physical memory components, they are crucial in enhancing memory behavior. By detecting and addressing memory-related issues early in the development process, these tools contribute significantly to the overall quality of software systems. This review will thoroughly examine the strengths and weaknesses of each static analysis tool, aiding in selecting the most suitable tool or combination of tools for effective memory management across diverse programming environments."
    },
    {
        "title": "Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct",
        "link_suffix": "/forum?id=wWnsoLhHwt",
        "link": "https://openreview.net/forum?id=wWnsoLhHwt",
        "pdf_link": "https://openreview.net/pdf?id=wWnsoLhHwt",
        "keywords": "LLM, Interpretability, AI, Activation Steering, Representation Engineering, Control",
        "abstract": "It has been reported that LLMs can recognize their own writing. As this has potential implications for AI safety, yet is relatively understudied, we investigate the phenomenon, seeking to establish whether it robustly occurs at the behavioral level, how the observed behavior is achieved, and whether it can be controlled. First, we find that the RLHF’d Llama3-8b–Instruct chat model - but not the base Llama3-8b model - can reliably distinguish its own outputs from those of humans, and present evidence that the chat model is likely using its experience with its own outputs, acquired during post-training, to succeed at the writing recognition task. Second, we identify a vector in the residual stream of the model that is differentially activated when the model makes a correct self-written-text recognition judgment, show that the vector activates in response to information relevant to self-authorship, present evidence that the vector is related to the concept of “self” in the model, and demonstrate that the vector is causally related to the model’s ability to perceive and assert self-authorship. Finally, we show that the vector can be used to control both the model’s behavior and its perception, steering the model to claim or disclaim authorship by applying the vector to the model’s output as it generates it, and steering the model to believe or disbelieve it wrote arbitrary texts by applying the vector to them as the model reads them."
    },
    {
        "title": "Towards Learning High-Precision Least Squares Algorithms with Sequence Models",
        "link_suffix": "/forum?id=snocoXIQXz",
        "link": "https://openreview.net/forum?id=snocoXIQXz",
        "pdf_link": "https://openreview.net/pdf?id=snocoXIQXz",
        "keywords": "high precision, least squares, algorithm learning, Transformers, gated convolutions, linear regression, in-context learning",
        "abstract": "This paper investigates whether sequence models can learn to perform numerical algorithms, e.g. gradient descent, on the fundamental problem of least squares. Our goal is to inherit two properties of standard algorithms from numerical analysis: (1) machine precision, i.e. we want to obtain solutions that are accurate to near floating point error, and (2) numerical generality, i.e. we want them to apply broadly across problem instances. We find that prior approaches using Transformers fail to meet these criteria, and identify limitations present in existing architectures and training procedures. First, we show that softmax Transformers struggle to perform high-precision multiplications, which prevents them from precisely learning numerical algorithms. Second, we identify an alternate class of architectures, comprised entirely of polynomials, that can efficiently represent high-precision gradient descent iterates. Finally, we investigate precision bottlenecks during training and address them via a high-precision training recipe that reduces stochastic gradient noise. Our recipe enables us to train two polynomial architectures, gated convolutions and linear attention, to perform gradient descent iterates on least squares problems. For the first time, we demonstrate the ability to train to near machine precision. Applied iteratively, our models obtain $100,000\\times$ lower MSE than standard Transformers trained end-to-end and they incur a $10,000\\times$ smaller generalization gap on out-of-distribution problems. We make progress towards end-to-end learning of numerical algorithms for least squares."
    },
    {
        "title": "Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction",
        "link_suffix": "/forum?id=pQsllTesiE",
        "link": "https://openreview.net/forum?id=pQsllTesiE",
        "pdf_link": "https://openreview.net/pdf?id=pQsllTesiE",
        "keywords": "Sequential Decision-Making, Monte Carlo Tree Search, Temporal Abstraction, Planning, Model-based Reinforcement Learning, Offline Reinforcement Learning",
        "abstract": "Sequential decision-making in high-dimensional continuous action spaces, particularly in stochastic environments, faces significant computational challenges. We explore this challenge in the traditional offline RL setting, where an agent must learn how to make decisions based on data collected through a stochastic behavior policy. We present \\textit{Latent Macro Action Planner} (L-MAP), which addresses this challenge by learning a set of temporally extended macro-actions through a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs a (separate) learned prior model that acts as a latent transition model and allows efficient sampling of plausible actions. During planning, our approach accounts for stochasticity in both the environment and the behavior policy by using Monte Carlo tree search (MCTS). In offline RL settings, including stochastic continuous control tasks, L-MAP efficiently searches over discrete latent actions to yield high expected returns.\nEmpirical results demonstrate that L-MAP maintains low decision latency despite increased action dimensionality. Notably, across tasks ranging from continuous control with inherently stochastic dynamics to high-dimensional robotic hand manipulation, L-MAP significantly outperforms existing model-based methods and performs on par with strong model-free actor-critic baselines, highlighting the effectiveness of the proposed approach in planning in complex and stochastic environments with high-dimensional action spaces."
    },
    {
        "title": "ALLoRA: Adaptive Learning Rate Mitigates LoRA Fatal Flaws",
        "link_suffix": "/forum?id=7X65yoKl3Y",
        "link": "https://openreview.net/forum?id=7X65yoKl3Y",
        "pdf_link": "https://openreview.net/pdf?id=7X65yoKl3Y",
        "keywords": "Large Language Models, low rank adaption, finetuning, dropout",
        "abstract": "Low-Rank Adaptation (LoRA) is the bread and butter of Large Language Model (LLM) finetuning. LoRA learns an additive low-rank perturbation of a pretrained matrix parameter to align the model to a new task or dataset. We identify three core limitations to LoRA for finetuning--with only a limited amount of training steps. First, it employs Dropout as a means to prevent overfitting. We prove that Dropout is only suitable for long training episodes but fails to reliably regularize training for short training episodes, e.g., finetuning. Second, LoRA’s parameters initialization is at $0$ makes the optimization landscape poorly conditioned during the first steps of training. That poor conditioning combined with the need to move away from $0$ lead to slow training dynamics. Third, the scaling factor that multiply each LoRA additive perturbation create ``short-sighted'' interactions between the LoRA modules of different layers. Motivated by principled analysis of those limitations, we find an elegant solution: a Dropout-free, scaling-free, LoRA with Adaptive Learning rate--coined ALLoRA. By scaling the per sample and per parameter gradients with a coefficient inversely proportional to parameters’ $\\ell_2$ norm, ALLoRA alleviates those three limitations. As a by-product, ALLoRA removes two hyper-parameters from LoRA: the scaling factor and the dropout rate. Empirical results show that ALLoRA admits better accuracy than LoRA on various settings, including against recent LoRA variants such as Weight-Decomposed Low-Rank Adaptation (DoRA). Ablation studies show our solution is the optimal in a family of weight-dependent / output-dependent approaches."
    },
    {
        "title": "Learning Efficient Positional Encodings with Graph Neural Networks",
        "link_suffix": "/forum?id=AWg2tkbydO",
        "link": "https://openreview.net/forum?id=AWg2tkbydO",
        "pdf_link": "https://openreview.net/pdf?id=AWg2tkbydO",
        "keywords": "positional encodings, graph neural networks, graph transformers",
        "abstract": "Positional encodings (PEs) are essential for effective graph representation learning because they provide position awareness in inherently position-agnostic transformer architectures and increase the expressive capacity of Graph Neural Networks (GNNs). However, designing powerful and efficient PEs for graphs poses significant challenges due to the absence of canonical node ordering and the scale of the graph. Here, we investigate PEs for graphs based on four key criteria: stability, expressive power, scalability, and genericness. We find that existing eigenvector-based PE methods often fall short of jointly satisfying these criteria. To address this gap, we introduce PEARL, a novel framework of learnable PEs for graphs. Our primary insight is that message-passing GNNs function as nonlinear mappings of eigenvectors, enabling the design of GNN architectures for generating powerful and efficient PEs. A crucial challenge lies in initializing node attributes in a manner that is both expressive and permutation equivariant. We tackle this by initializing GNNs with random node inputs or basis vectors, thereby unlocking the expressive power of message-passing operations, while employing statistical pooling functions to maintain permutation equivariance. Our analysis demonstrates that PEARL approximates equivariant functions of eigenvectors with linear complexity, while rigorously establishing its stability and high expressive power. Experimental evaluations show that \nPEARL outperforms lightweight versions of eigenvector-based PEs and achieves comparable performance to full eigenvector-based PEs, but with one or two orders of magnitude lower complexity."
    },
    {
        "title": "Enhancing Cooperative Problem-Solving in Sparse-Reward Systems via Co-evolutionary Curriculum Learning",
        "link_suffix": "/forum?id=J9pNS44qcT",
        "link": "https://openreview.net/forum?id=J9pNS44qcT",
        "pdf_link": "https://openreview.net/pdf?id=J9pNS44qcT",
        "keywords": "Reinforcement Learning, Task Learning",
        "abstract": "Sparse reward environments consistently challenge reinforcement learning, as agents often need to finish tasks before receiving any feedback, leading to limited incentive signals. This issue becomes even more pronounced in multi-agent systems (MAS), where a single reward must be distributed among multiple agents over time, frequently resulting in suboptimal or inconsistent learning outcomes. To tackle this challenge, we introduce a novel approach called Collaborative Multi-dimensional Course Learning (CCL) for multi-agent cooperation scenarios. CCL features three key innovations: (1) It establishes an adaptive curriculum framework tailored for MAS, refining intermediate tasks to individual agents to ensure balanced strategy development. (2) A novel variant evolution algorithm creates more detailed intermediate tasks. (3) Co-evolution between agents and their environment is modeled to enhance training stability under sparse reward conditions. In evaluations across five tasks within multi-particle environments (MPE) and Hide and Seek (Hns), CCL demonstrated superior performance, surpassing existing benchmarks and excelling in sparse reward settings."
    },
    {
        "title": "Inductive Linguistic Reasoning with Large Language Models",
        "link_suffix": "/forum?id=8XQ1hLbwmU",
        "link": "https://openreview.net/forum?id=8XQ1hLbwmU",
        "pdf_link": "https://openreview.net/pdf?id=8XQ1hLbwmU",
        "keywords": "language models, linguistic reasoning, prompting, analogical reasoning, linguistics puzzles",
        "abstract": "Evaluating large language models (LLMs) on their linguistic reasoning capabilities is an important task to understand the gaps in their skills that may surface during large-scale adoption. In this work, we investigate the abilities of such models to perform abstract multilingual reasoning through the lens of linguistic puzzles on extremely low-resource languages. As these translation tasks involve inductive and deductive reasoning from reference instances, we examine whether diverse auxiliary demonstrations can be automatically induced from seed exemplars, through analogical prompting. We employ a two-stage procedure, first generating analogical exemplars with a language model, and then applying them in-context along with provided target language exemplars. We explore various combinations of language models as analogical generators and reasoning agents, testing different model sizes and specialized multilingual LLMs. Our results on the modeLing dataset show that analogical prompting is effective in eliciting models' knowledge of language grammar similarities, boosting the performance of GPT-4o by as much as 8.1% and Llama-3.1-405B by 5.9% over chain-of-thought approaches. These gains are realized with self-generated analogical demonstrations as well as those generated by weaker multilingual models. We also report several findings about interesting phenomena which drive linguistic reasoning performance, suggesting that such puzzles are a valuable benchmark for new reasoning methods."
    },
    {
        "title": "Deployment Efficient Reward-Free Exploration with Linear Function Approximation",
        "link_suffix": "/forum?id=NvaZn3uwzJ",
        "link": "https://openreview.net/forum?id=NvaZn3uwzJ",
        "pdf_link": "https://openreview.net/pdf?id=NvaZn3uwzJ",
        "keywords": "Linear MDP, Deployment Complexity, Sample Complexity",
        "abstract": "We study deployment efficient reward-free exploration with linear function approximation, where the goal is to explore a linear Markov Decision Process (MDP) without revealing the reward function, while minimizing the number of exploration policies used during the algorithm. We design a new reinforcement learning (RL) algorithm whose sample complexity is polynomial in the feature dimension and horizon length, while achieving nearly optimal deployment efficiency for linear MDPs under the reward-free exploration setting. More specifically, our algorithm explores a linear MDP in a reward-free manner, while using at most $H$ exploration policies during its execution where $H$ is the horizon length. Compared to previous algorithms with similar deployment efficiency guarantees, the sample complexity of our algorithm does not depend on the reachability coefficient or the explorability coefficient of the underlying MDP, which can be arbitrarily small for certain MDPs. Our result addresses an open problem proposed in prior work. To achieve such a result, we show how to truncate state-action pairs of the underlying linear MDP in a data-dependent manner, and devise efficient offline policy evaluation and offline policy optimization algorithms in the truncated linear MDP. We further show how to implement reward-free exploration mechanisms in the linear function approximation setting by carefully combines these offline RL algorithms without sacrificing the deployment efficiency."
    },
    {
        "title": "How Transformers Solve Propositional Logic Problems: A Mechanistic Analysis",
        "link_suffix": "/forum?id=eks3dGnocX",
        "link": "https://openreview.net/forum?id=eks3dGnocX",
        "pdf_link": "https://openreview.net/pdf?id=eks3dGnocX",
        "keywords": "Mechanistic Interpretability, Language Models, Transformers, Logical Reasoning, Learned Representations",
        "abstract": "Large language models (LLMs) have shown amazing performance on tasks that require planning and reasoning. Motivated by this, we investigate the internal mechanisms that underpin a network's ability to perform complex logical reasoning. We first construct a synthetic propositional logic problem that serves as a concrete test-bed for network training and evaluation. Crucially, this problem demands nontrivial planning to solve, but we can train a small transformer to achieve perfect accuracy. Building on our set-up, we then pursue an understanding of precisely how a three-layer transformer, trained from scratch, solves this problem. We are able to identify certain \"planning\" and \"reasoning\" circuits in the network that necessitate cooperation between the attention blocks to implement the desired logic. To expand our findings, we then study a larger model, Mistral 7B. Using activation patching, we characterize internal components that are critical in solving our logic problem. Overall, our work systemically uncovers novel aspects of small and large transformers, and continues the study of how they plan and reason."
    },
    {
        "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF",
        "link_suffix": "/forum?id=5ZkuWAbxzT",
        "link": "https://openreview.net/forum?id=5ZkuWAbxzT",
        "pdf_link": "https://openreview.net/pdf?id=5ZkuWAbxzT",
        "keywords": "Reinforcement learning, KL regularization",
        "abstract": "Reverse-Kullback-Leiblerregularization has emerged to be a predominant technique used to enhance policy optimization in reinforcement learning (RL) and reinforcement learning from human feedback (RLHF), which forces the learned policy to stay close to a reference policy. While the effectiveness and necessity of KL-regularization has been empirically demonstrated in various practical scenarios, current theoretical analysis of KL-regularized RLHF still obtain the same $\\mathcal{O}(1 / \\epsilon^2)$ sample complexity as problems without KL-regularization. To understand the fundamental distinction between policy learning objectives with KL-regularization and ones without KL-regularization, we are the first to theoretically demonstrate the power of KL-regularization by providing a sharp analysis for KL-regularized contextual bandits and RLHF, revealing an $\\mathcal{O}(1 / \\epsilon)$ sample complexity when $\\epsilon$ is sufficiently small.We further explore the role of data coverage in contextual bandits and RLHF. While the coverage assumption is commonly employed in offline RLHF to link the samples from the reference policy to the optimal policy, often at the cost of a multiplicative dependence on the coverage coefficient, its impact on the sample complexity of online RLHF remains unclear. Previous theoretical analyses of online RLHF typically require explicit exploration and additional structural assumptions on the reward function class. In contrast, we show that with sufficient coverage from the reference policy, a simple two-stage mixed sampling strategy can achieve a sample complexity with only an additive dependence on the coverage coefficient. Our results provide a comprehensive understanding of the roles of KL-regularization and data coverage in RLHF, shedding light on the design of more efficient RLHF algorithms."
    },
    {
        "title": "Understanding Gradient Descent through the Training Jacobian",
        "link_suffix": "/forum?id=kkVTeMvC9D",
        "link": "https://openreview.net/forum?id=kkVTeMvC9D",
        "pdf_link": "https://openreview.net/pdf?id=kkVTeMvC9D",
        "keywords": "sgd, interpretability, generalization",
        "abstract": "We examine the geometry of neural network training using the Jacobian of trained network parameters with respect to their initial values. Our analysis reveals low-dimensional structure in the training process which is dependent on the input data but largely independent of the labels. We find that the singular value spectrum of the Jacobian matrix consists of three distinctive regions: a \"chaotic\" region of values orders of magnitude greater than one, a large \"bulk\" region of values extremely close to one, and a \"stable\" region of values less than one. Along each bulk direction, the left and right singular vectors are nearly identical, indicating that perturbations to the initialization are carried through training almost unchanged. These perturbations have virtually no effect on the network's output in-distribution, yet do have an effect far out-of-distribution. While the Jacobian applies only locally around a single initialization, we find substantial overlap in bulk subspaces for different random seeds."
    },
    {
        "title": "Conditional density estimation for video prediction with score-based models",
        "link_suffix": "/forum?id=mHkbi3XM58",
        "link": "https://openreview.net/forum?id=mHkbi3XM58",
        "pdf_link": "https://openreview.net/pdf?id=mHkbi3XM58",
        "keywords": "temporal prediction, diffusion models, denoising, conditioning, occlusion boundaries, cue combination",
        "abstract": "Temporal prediction is inherently uncertain, and representing the ambiguity in natural image sequences is a challenging high-dimensional probabilistic inference problem. For natural scenes, the curse of dimensionality renders explicit density estimation, let alone integration, statistically and computationally intractable. We describe an implicit regression-based framework for learning and sampling the conditional density of the next frame in a video given the past few observed frames. We show that sequence-to-image deep networks trained on a simple resilience-to-noise objective function extract adaptive representations for temporal prediction. Synthetic experiments demonstrate that this score-based framework handles occlusion boundaries:  unlike classical methods that average bifurcating temporal trajectories, it chooses among likely trajectories, selecting more probable options with higher frequency. Furthermore, local linear analysis of networks trained on natural image sequences reveals that the representation automatically weights predictive evidence by reliability, which is a hallmark of probabilistic inference."
    },
    {
        "title": "Systematic Review of Large Language Models: Applications, Limitations, Practical Usages and Future Directions",
        "link_suffix": "/forum?id=8QTpYC4smR",
        "link": "https://openreview.net/forum?id=8QTpYC4smR",
        "pdf_link": "https://openreview.net/pdf?id=8QTpYC4smR",
        "keywords": "Large Language Models, Systematic Review",
        "abstract": "Large Language Models have revolutionized natural language processing with their remarkable ability to understand and generate human-like text. This review explores the various applications of large language models, highlighting their versatility across different domains. The paper begins with an introduction to LLMs, followed by an overview of their types and a detailed literature review. We then examine their limitations before delving into specific applications such as text generation, translation, summarization, and more. Finally, we discuss future directions for research and development, concluding with a summary of key findings and the potential impact of large language models on various industries."
    },
    {
        "title": "On the Cost-Effectiveness of Partially-Annotating Methods for Multi-Label Learning",
        "link_suffix": "/forum?id=xNf8sOtFbx",
        "link": "https://openreview.net/forum?id=xNf8sOtFbx",
        "pdf_link": "https://openreview.net/pdf?id=xNf8sOtFbx",
        "keywords": "Partially-annotating, multi-label learning",
        "abstract": "Precisely annotating instances with multiple labels is costly and has emerged as a significant bottleneck in the real-world multi-label learning tasks. To deal with this problem, the most straightforward strategy is partially-annotating, which aims to reduce the cost by annotating only a subset of labels. Existing works mainly includes label-level partially-annotating (LPA), where each instance is assigned a subset of positive labels, and instance-level partially-annotating (IPA), where all positive labels are assigned to an instance, but only a subset of instances are annotated. However, these methods tend to focus on improving model performance under each type of partial annotation, often neglecting a fundamental question: \\textit{which method is the most cost-effective?} In this paper, we empirically evaluate which partially-annotating method achieves better model performance at the same annotation cost. To make a fair comparison, we manually annotated images in the MS-COCO dataset using two partially-annotating methods and recorded their averaging annotation time per image. This allows us to train models on two types of partial annotations with the same annotation cost and to compare their performance. Empirical results show that even when the number of examples annotated with IPA is only one-fifth that of LPA, models trained on IPA annotations significantly outperform those trained on LPA annotations, yielding that IPA is significantly more cost-effective than LPA. To explain the superiority of IPA, our causal reasoning framework shows that compared to LPA, IPA preserves complete co-occurrence relationships, enabling the model to capture correlative patterns, which is useful for improving model performance."
    },
    {
        "title": "SAFE: Spiking Neural Network-based Audio Fidelity Evaluation",
        "link_suffix": "/forum?id=QWDZE2mYIe",
        "link": "https://openreview.net/forum?id=QWDZE2mYIe",
        "pdf_link": "https://openreview.net/pdf?id=QWDZE2mYIe",
        "keywords": "Fake Audio Detection, Spiking Neural Networks (SNNs), Partial Fake Audio",
        "abstract": "Recent advancements in generative AI have enabled the creation of highly realistic synthetic audio, posing significant challenges in voice authentication, media verification, and fraud detection. While deep learning models are frequently used for fake audio detection, they often struggle to generalize to unseen and complex manipulations, particularly partial fake audio, where real and synthetic segments are seamlessly combined. This paper explores the use of Spiking Neural Networks (SNNs) for fake and partial fake audio detection, an area that has not yet been investigated. SNNs, known for their energy-efficient computation and ability to process temporal data, offer a promising alternative to traditional Artificial Neural Networks (ANNs). We propose an SNN-based approach for fake audio detection and comprehensively evaluate its performance through a series of experiments, including hyperparameter tuning, cross-dataset generalization and partial fake audio detection. \nOur results show that SNNs achieve accuracy comparable to state-of-the-art ANN models with fewer number of parameters. Although, SNNs did not offer significant improvements in generalization capabilities, they provided advantages such as reduced model sizes and computational efficiency, making them more suitable for resource-constrained and real-time voice authentication applications.\nThis study lays the groundwork for further exploration of SNNs in audio spoofing countermeasures, providing a foundation for future advancements in security-critical voice applications."
    },
    {
        "title": "All-in-One: Prompt-Driven Mixture of Hallucination-Aware Experts for Universal Anomaly Detection Across Multi-Modal Multi-Organ Medical Images",
        "link_suffix": "/forum?id=TjTe2sEOMP",
        "link": "https://openreview.net/forum?id=TjTe2sEOMP",
        "pdf_link": "https://openreview.net/pdf?id=TjTe2sEOMP",
        "keywords": "universal anomaly detection, medical images, MoE, hallucinatory anomalies",
        "abstract": "Unsupervised anomaly detection in medical images facilitates practical clinical adoption by identifying abnormalities without relying on scarce and costly annotated data. However, prior works have predominantly focused on specialized models for individual organs and modalities, impeding knowledge transfer and scalable deployment. In this paper, we investigate a task of universal anomaly detection guided by natural language prompts. We propose a prompt-driven mixture of experts framework that detects anomalies across multiple organs and modalities within a single network. Specifically, our method comprises encoders for vision and text, a routing network, and a mixture of hallucination-minimized expert decoders. An image and a prompt describing the organ and modality are fed to the encoders. The routing network then selects specialized yet collaborative expert decoders to analyze the image. We observe that anomaly detection models often erroneously identify normal image regions as anomalous, a phenomenon we term ``hallucinatory anomaly''. To address this issue, we design hallucination-aware experts that produce improved anomaly maps by jointly learning reconstruction and minimizing these false positives. For comprehensive evaluation, we curate a diverse dataset of 12,153 images spanning 5 modalities and 4 organs. Extensive experiments demonstrate state-of-the-art anomaly detection performance in this universal setting. Moreover, the natural language conditioning enables interpretability and user interaction. The code and data will be made publicly available."
    },
    {
        "title": "Dynamic Noise Preference Optimization for LLM Self-Improvement via Synthetic Data",
        "link_suffix": "/forum?id=QdiMWcwU5w",
        "link": "https://openreview.net/forum?id=QdiMWcwU5w",
        "pdf_link": "https://openreview.net/pdf?id=QdiMWcwU5w",
        "keywords": "Preference Optimization, Alignment of LLMs, Self-improvement, Synthetic Data, Noise",
        "abstract": "Although LLMs have achieved significant success, their reliance on large volumes of human-annotated data has limited their potential for further scaling. In this situation, utilizing self-generated synthetic data has become crucial for fine-tuning LLMs without extensive human annotation. However, current methods often fail to ensure consistent improvements across iterations, with performance stagnating after only minimal updates. To overcome these challenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO employs a dynamic sample labeling mechanism to construct preference pairs for training and introduces controlled, trainable noise into the preference optimization process. Our approach effectively prevents stagnation and enables continuous improvement. In experiments with Zephyr-7B, DNPO consistently outperforms existing methods, showing an average performance boost of 2.6% across multiple benchmarks. \nAdditionally, DNPO shows a significant improvement in model-generated data quality, with a 29.4% win-loss rate gap compared to the baseline in GPT-4 evaluations. This highlights its effectiveness in enhancing model performance through iterative refinement."
    }
]