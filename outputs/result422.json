[{"title": "On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning", "link_suffix": "/forum?id=s15HrqCqbr", "link": "https://openreview.net/forum?id=s15HrqCqbr", "pdf_link": "https://openreview.net/pdf?id=s15HrqCqbr", "keywords": "Discriminative Probabilistic Modeling; Self-Supervised Representation Learning; Multiple Importance Sampling", "abstract": "We study the discriminative probabilistic modeling problem on a continuous domain for (multimodal) self-supervised representation learning. To address the challenge of computing the integral in the partition function for each anchor data, we leverage the multiple importance sampling (MIS) technique for robust Monte Carlo integration, which can recover InfoNCE-based contrastive loss as a special case. Within this probabilistic modeling framework,  we conduct generalization error analysis to reveal the limitation of current InfoNCE-based contrastive loss for self-supervised representation learning and derive insights for developing better approaches by reducing the error of Monte Carlo integration. To this end, we propose a novel non-parametric method for approximating the sum of conditional densities required by MIS through convex optimization, yielding a new contrastive objective for self-supervised representation learning. Moreover, we design an efficient algorithm for solving the proposed objective. We empirically compare our algorithm to representative baselines on the contrastive image-language pretraining task. Experimental results on the CC3M and CC12M datasets demonstrate the superior overall performance of our algorithm.", "title_embedding_index": 21050, "title_abs_embedding_index": 21075}, {"title": "What Makes a Good Diffusion Planner for Decision Making?", "link_suffix": "/forum?id=7BQkXXM8Fy", "link": "https://openreview.net/forum?id=7BQkXXM8Fy", "pdf_link": "https://openreview.net/pdf?id=7BQkXXM8Fy", "keywords": "Diffusion Models, Offline Reinforcement Learning, Decision Making, Planning", "abstract": "Diffusion models have recently shown significant potential in solving decision-making problems, particularly in generating behavior plans -- also known as diffusion planning. While numerous studies have demonstrated the impressive performance of diffusion planning, the mechanisms behind the key components of a good diffusion planner remain unclear and the design choices are highly inconsistent in existing studies. In this work, we address this issue through systematic empirical experiments on diffusion planning in an offline reinforcement learning (RL) setting, providing practical insights into the essential components of diffusion planning. We trained and evaluated over 6,000 diffusion models, identifying the critical components such as guided sampling, network architecture, action generation and planning strategy. We revealed that some design choices opposite to the common practice in previous work in diffusion planning actually lead to better performance, e.g., unconditional sampling with selection can be better than guided sampling and Transformer outperforms U-Net as denoising network. Based on these insights, we suggest a simple yet strong diffusion planning baseline that achieves state-of-the-art results on standard offline RL benchmarks.", "title_embedding_index": 21051, "title_abs_embedding_index": 21076}, {"title": "Law of Vision Representation in MLLMs", "link_suffix": "/forum?id=SZm3hxmksx", "link": "https://openreview.net/forum?id=SZm3hxmksx", "pdf_link": "https://openreview.net/pdf?id=SZm3hxmksx", "keywords": "Multimodality Large Language Models; Computer Vision; Vision Representation", "abstract": "We present the \"Law of Vision Representation\" in multimodal large language models (MLLMs). It reveals a strong correlation between the combination of cross-modal alignment, correspondence in vision representation, and MLLM performance. We quantify the two factors using the cross-modal Alignment and Correspondence score (AC score). Through extensive experiments involving thirteen different vision representation settings and evaluations across eight benchmarks, we find that the AC score is linearly correlated to model performance. By leveraging this relationship, we are able to identify and train the optimal vision representation only, which does not require finetuning the language model every time, resulting in a 99.7% reduction in computational cost.", "title_embedding_index": 21052, "title_abs_embedding_index": 21077}, {"title": "LoRA Recycle: Towards Fine-Tuning-Free Visual Foundation Model via Double-Efficient Data-Free Meta-Learning", "link_suffix": "/forum?id=vErsELb7Qg", "link": "https://openreview.net/forum?id=vErsELb7Qg", "pdf_link": "https://openreview.net/pdf?id=vErsELb7Qg", "keywords": "data-free meta-learning, few-shot classification, synthetic data", "abstract": "Large Language Models (LLMs) such as ChatGPT can efficiently adapt to few-shot tasks without fine-tuning, making them ideal for data-limited applications requiring real-time responses. However, this adaptability has not yet been replicated in current Visual Foundation Models (VFMs), which require explicit fine-tuning with sufficient tuning data. Low-Rank Adaptation (LoRA), an effective fine-tuning approach, adapts VFMs to specific tasks by updating extra lightweight modules. Thanks to its modularity, users can upload locally tuned LoRAs to public repositories without exposing private training data. In this paper, we explore the potential of reusing diverse pre-tuned LoRAs without accessing their private training data, to improve the few-shot adaptability of VFMs without requiring further fine-tuning. To achieve this, we propose a data-free meta-learning framework named LoRA Recycle, which distills a meta-LoRA from diverse pre-tuned LoRAs using synthetic data generated via LoRA Inversion. The VFM, once equipped with the meta-LoRA, is empowered to solve new few-shot tasks in a single forward pass without further fine-tuning, akin to the in-context learning of LLMs. To further enhance efficiency, we propose a double-efficient mechanism that uses only the foreground patches and prunes background patches in the synthetic data, significantly accelerating the meta-training process while maintaining or even improving performance. Comprehensive experiments across eight datasets within both in- and cross-domain scenarios verify the superiority of our framework.", "title_embedding_index": 21053, "title_abs_embedding_index": 21078}, {"title": "VFDiff: SE(3)-Equivariant Vector Field Guided Diffusion Model for Target-Aware Molecule Generation in 3D", "link_suffix": "/forum?id=5YLsnsjgeC", "link": "https://openreview.net/forum?id=5YLsnsjgeC", "pdf_link": "https://openreview.net/pdf?id=5YLsnsjgeC", "keywords": "Diffusion Model, Molecule Generation, Structure-Based Drug Design", "abstract": "Structure-based drug design (SBDD) is a key challenge in drug discovery that aims to generate small molecules capable of binding tightly to specific protein pockets. However, current diffusion models have focused on the complementarity of ligand molecules and protein pockets in physical space while ignoring the docking energy requirements,  resulting in only generating suboptimal docking postures. In this paper, we present VFDiff, a novel SE(3)-equivariant diffusion model for 3D molecular generation, guided by vector fields derived from protein-ligand binding energy. In contrast to current diffusion models, VFDiff incorporates energy-based guidance in both forward and reverse processes to ensure ligand molecules are spatially complementary and energetically matched to their target pockets. Our approach includes three fundamental mechanisms: energy-planning, which adjusts diffusion trajectories based on energy gradients; force-guiding, which refines molecular generation; and position-tuning, which improves sampling accuracy. Extensive experiments on the CrossDocked2020 dataset demonstrate that VFDiff outperforms state-of-the-art methods, achieving superior binding binding affinity with an impressive Avg. Vina Score of up to -7.37, while maintaining competitive molecular properties, and diversity. This work introduces a new framework for generating target-specific molecules with improved structural and functional fidelity, offering a significant advancement in SBDD.", "title_embedding_index": 21054, "title_abs_embedding_index": 21079}, {"title": "ThermalGaussian: Thermal 3D Gaussian Splatting", "link_suffix": "/forum?id=ybFRoGxZjs", "link": "https://openreview.net/forum?id=ybFRoGxZjs", "pdf_link": "https://openreview.net/pdf?id=ybFRoGxZjs", "keywords": "3D reconstruction; Thermal fild reconstruction; 3D Computer Vision; Machine learning approaches;", "abstract": "Thermography is especially valuable for the military and other users of surveillance cameras. Some recent methods based on Neural Radiance Fields (NeRF) are proposed to reconstruct the thermal scenes in 3D from a set of thermal and RGB images. However, unlike NeRF, 3D Gaussian splatting (3DGS) prevails due to its rapid training and real-time rendering. In this work, we propose ThermalGaussian, the first thermal 3DGS approach capable of rendering high-quality images in RGB and thermal modalities. We first calibrate the RGB camera and the thermal camera to ensure that both modalities are accurately aligned. Subsequently, we use the registered images to learn the multimodal 3D Gaussians. To prevent the overfitting of any single modality, we introduce several multimodal regularization constraints. We also develop smoothing constraints tailored to the physical characteristics of the thermal modality.\nBesides, we contribute a real-world dataset named RGBT-Scenes, captured by a hand-hold thermal-infrared camera, facilitating future research on thermal scene reconstruction. We conduct comprehensive experiments to show that ThermalGaussian achieves photorealistic rendering of thermal images and improves the rendering quality of RGB images. With the proposed multimodal regularization constraints, we also reduced the model's storage cost by 90%. The code and dataset will be released.", "title_embedding_index": 21055, "title_abs_embedding_index": 21080}, {"title": "3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion", "link_suffix": "/forum?id=CGbfokGFP7", "link": "https://openreview.net/forum?id=CGbfokGFP7", "pdf_link": "https://openreview.net/pdf?id=CGbfokGFP7", "keywords": "3D Generation, Diffusion Model, Image-to-3D, Text-to-3D, PBR Asset, 3D Representation, Primitives", "abstract": "The increasing demand for high-quality 3D assets across various industries necessitates efficient and automated 3D content creation.\nDespite recent advancements in 3D generative models, existing methods still face challenges with optimization speed, geometric fidelity, and the lack of assets for physically based rendering (PBR). In this paper, we introduce 3DTopia-XL, a scalable native 3D generative model designed to overcome these limitations. 3DTopia-XL leverages a novel primitive-based 3D representation, PrimX, which encodes detailed shape, albedo, and material field into a compact tensorial format, facilitating the modeling of high-resolution geometry with PBR assets. On top of the novel representation, we propose a generative framework based on Diffusion Transformer (DiT), which comprises 1) Primitive Patch Compression, 2) and Latent Primitive Diffusion. 3DTopia-XL learns to generate high-quality 3D assets from textual or visual inputs. We conduct extensive qualitative and quantitative experiments to demonstrate that 3DTopia-XL significantly outperforms existing methods in generating high-quality 3D assets with fine-grained textures and materials, efficiently bridging the quality gap between generative models and real-world applications.", "title_embedding_index": 21056, "title_abs_embedding_index": 21081}, {"title": "Forecasting chaotic systems with zero-shot learning", "link_suffix": "/forum?id=TqYjhJrp9m", "link": "https://openreview.net/forum?id=TqYjhJrp9m", "pdf_link": "https://openreview.net/pdf?id=TqYjhJrp9m", "keywords": "chaos, nonlinear dynamics, forecasting, physics, scientific machine learning", "abstract": "Time-series forecasting is a challenging task that traditionally requires specialized models custom-trained for the specific task at hand. Recently, inspired by the success of large language models, foundation models pre-trained on vast amounts of time-series data from diverse domains have emerged as a promising candidate for general-purpose time-series forecasting. The defining characteristic of these foundation models is their ability to perform zero-shot learning, that is, forecasting a new system from limited context data without explicit re-training or fine-tuning. Here, we evaluate whether the zero-shot learning paradigm extends to the challenging task of forecasting chaotic systems. Across $135$ distinct chaotic dynamical systems and $10^8$ timepoints, we find that foundation models produce competitive forecasts compared to custom-trained models (including NBEATS, TiDE, etc.), particularly when training data is limited. Interestingly, even after point forecasts fail, foundation models preserve the geometric and statistical properties of the chaotic attractors, demonstrating a surprisingly strong ability to capture the long-term behavior of chaotic dynamical systems. Our results highlight the promises and pitfalls of foundation models in making zero-shot forecasts of chaotic systems.", "title_embedding_index": 21057, "title_abs_embedding_index": 21082}, {"title": "GBIR: A Novel Gaussian Iterative Method for Medical Image Reconstruction", "link_suffix": "/forum?id=AkCWbxntll", "link": "https://openreview.net/forum?id=AkCWbxntll", "pdf_link": "https://openreview.net/pdf?id=AkCWbxntll", "keywords": "Medical Image Reconstruction, Computed Tomography, Nuclear Magnetic Resonance Imaging", "abstract": "Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial diagnostic tools, but undersampling techniques like Sparse-View CT (SV-CT) and Compressed-Sensing MRI (CS-MRI), aimed at reducing patient exposure and scan time, make image reconstruction more challenging. While deep learning-based reconstruction (DLR) methods have made significant strides, they face limitations in adapting to varying scan geometries and handling diverse patient data, hindering widespread clinical use.\n  In this paper, we propose a novel Gaussian-Based Iterative Reconstruction (GBIR) framework that uses learnable Gaussians representations for personalized medical image reconstruction, addressing the shortcomings of DLR methods. GBIR optimizes case-specific parameters in an end-to-end fashion, enabling better generalization and flexibility under sparse measurements. Additionally, we introduce theMulti-Organ Medical ImageREconstruction (MORE) dataset, comprising over 70,000 CT and MRI slices across multiple body parts and conditions.\n  Our experiments show that GBIR outperforms state-of-the-art methods in both accuracy and speed, offering a robust solution for personalized medical image reconstruction.", "title_embedding_index": 21058, "title_abs_embedding_index": 21083}, {"title": "Optimizing Knowledge Distillation in Transformers: Enabling Power of Multi-Head Attention without Alignment Barriers", "link_suffix": "/forum?id=QDkPAV9Fa5", "link": "https://openreview.net/forum?id=QDkPAV9Fa5", "pdf_link": "https://openreview.net/pdf?id=QDkPAV9Fa5", "keywords": "Knowledge distillation; Multi-head Attention; Transformers; Generative models;", "abstract": "Knowledge distillation has been proven effective for compressing transformer architectures by transferring knowledge from teacher to student models. Logits-based methods of knowledge distillation cannot fully capture the intermediate representations and features within the teacher model, which may result in the student model not fully learning all the knowledge from the teacher model. Thus, previous work focuses on transferring knowledge through intermediate features or attention maps. However, leveraging multi-head attention maps in transformers for knowledge distillation presents challenges due to head misalignment and suboptimal feature alignment, often requiring projectors to align features or special modifications to the model architecture. To address above limitations, we propose the Squeezing-Heads Distillation (SHD) method. This method reduces the number of attention maps to any desired number through linear approximation, without requiring additional projectors or parameters. This facilitates better alignment and knowledge transfer between models with different numbers of heads, enhancing both flexibility and efficiency. Experimental results demonstrate significant improvements in both language and vision generative models, validating the effectiveness of our method.", "title_embedding_index": 21059, "title_abs_embedding_index": 21084}, {"title": "Closing the Gap between Neural Networks for Approximate and Rigorous Logical Reasoning", "link_suffix": "/forum?id=4ndvumlZak", "link": "https://openreview.net/forum?id=4ndvumlZak", "pdf_link": "https://openreview.net/pdf?id=4ndvumlZak", "keywords": "neural reasoning, syllogistic reasoning, Euler diagram, composition tables, rigorous reasoning", "abstract": "Despite the historical successes of neural networks, the rigour of logical reasoning is still beyond their reach. Though this is consistent with the dual-process model of the mind, which separates the model for rigorous reasoning from that for approximate associative thinking, a systematic explanation is still missing in the literature. We review syllogistic reasoning and its irreplaceable role in logic and human rationality, show existing neural networks cannot reach the rigour of syllogistic reasoning, and propose features that neural networks for rigorous reasoning should and should not have. (1) They should not use combination tables: We reduce syllogistic relations into part-whole relations, and translate the criterion of rigorous syllogistic reasoning into a deterministic process of Euler diagram construction in vector space. Then, we survey recent neural architectures (Siamese Masked Autoencoder) for reasoning part-whole relations in object completion and degrade the task into reconstructing Euler diagrams for syllogistic reasoning. We dissect Euler Net (EN), the Siamese (Masked) Autoencoder for syllogistic reasoning, and report three experiments, showing that EN, utilising a pre-designed combination table, cannot reach 100% accuracy for testing data without restriction. As Transformer's Key-Query-Value structure is a combination table, we conclude that LLMs and Foundation Models built upon Transformers cannot reach the rigour of syllogistic reasoning. (2) They should use non-vector embedding as computational building blocks: Transformer's oversmoothing prevents any neural architecture built upon them from reaching the rigour of syllogistic reasoning. We prove, however, that in the setting of part-whole relations, if neural networks use non-vector embedding as computational building blocks, they will not oversmooth. This work suggests a new way to close the gap between neural networks for approximate and rigorous logical reasoning.", "title_embedding_index": 21060, "title_abs_embedding_index": 21085}, {"title": "Investigating the Effectiveness of HyperTuning via Gisting", "link_suffix": "/forum?id=EjCrfVFZTx", "link": "https://openreview.net/forum?id=EjCrfVFZTx", "pdf_link": "https://openreview.net/pdf?id=EjCrfVFZTx", "keywords": "hypernetworks, llm, parameter-efficient fine-tuning, prefix tuning", "abstract": "Gisting (Mu et al., 2023) is a simple method for training models to compress information into fewer token representations using a modified attention mask, and can serve as an economical approach to training Transformer-based hypernetworks. We introduce HyperLlama, a set of Gisting-based hypernetworks built on Llama-2 models that generates task-specific soft prefixes based on few-shot inputs. In experiments across P3, Super-NaturalInstructions and Symbol Tuning datasets, we show that HyperLlama models can effectively compress information from few-shot examples into soft prefixes. However, they still underperform multi-task fine-tuned language models with full attention over few-shot in-context examples. We also show that HyperLlama-generated soft prefixes can serve as better initializations for further prefix tuning. Overall, Gisting-based hypernetworks are economical and easy to implement, but have mixed empirical performance.", "title_embedding_index": 21061, "title_abs_embedding_index": 21086}, {"title": "RelationBooth: Towards Relation-Aware Customized Object Generation", "link_suffix": "/forum?id=LcoOwM5y7r", "link": "https://openreview.net/forum?id=LcoOwM5y7r", "pdf_link": "https://openreview.net/pdf?id=LcoOwM5y7r", "keywords": "relation generation; customize image generation;", "abstract": "Customized image generation is crucial for delivering personalized content based on user-provided image prompts, aligning large-scale text-to-image diffusion models with individual needs. However, existing models often overlook the relationships between customized objects in generated images. Instead, this work addresses that gap by focusing on relation-aware customized image generation, which aims to preserve the identities from image prompts while maintaining the predicate relations described in text prompts. Specifically, we introduce RelationBooth, a framework that disentangles identity and relation learning through a well-curated dataset. Our training data consists of relation-specific images, independent object images containing identity information, and text prompts to guide relation generation. Then, we propose two key modules to tackle the two main challenges\u2014generating accurate and natural relations, especially when significant pose adjustments are required, and avoiding object confusion in cases of overlap. First, we introduce a keypoint matching loss that effectively guides the model in adjusting object poses closely tied to their relationships. Second, we incorporate local features from the image prompts to better distinguish between objects, preventing confusion in overlapping cases. Extensive results on three benchmarks demonstrate the superiority of RelationBooth in generating precise relations while preserving object identities across a diverse set of objects and relations. The source code and trained models will be made available to the public.", "title_embedding_index": 21062, "title_abs_embedding_index": 21087}, {"title": "MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models", "link_suffix": "/forum?id=f7WBRSuf9l", "link": "https://openreview.net/forum?id=f7WBRSuf9l", "pdf_link": "https://openreview.net/pdf?id=f7WBRSuf9l", "keywords": "Large Vision Language Models", "abstract": "Visual preference alignment involves training Large Vision-Language Models (LVLMs) to predict human preferences between visual inputs. This is typically achieved by using labeled datasets of chosen/rejected pairs and employing optimization algorithms like direct preference optimization (DPO).\nExisting visual alignment methods, primarily designed for single-image scenarios, struggle to effectively handle the complexity of multi-image tasks due to the scarcity of diverse training data and the high cost of annotating chosen/rejected pairs.\nWe present Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a visual preference alignment approach that effectively handles multi-image inputs.\nMIA-DPO mitigates the scarcity of diverse multi-image training data by extending single-image data with unrelated images arranged in grid collages or pic-in-pic formats, significantly reducing the costs associated with multi-image data annotations.\nOur observation reveals that attention values of LVLMs vary considerably across different images. We use attention values to identify and filter out rejected responses the model may have mistakenly focused on.\nOur attention-aware selection for constructing the chosen/rejected pairs without relying on (i) human annotation, (ii) extra data, and (iii) external models or APIs.\nMIA-DPO is compatible with various architectures and outperforms existing methods on five multi-image benchmarks, achieving an average performance boost of 3.0% on LLaVA-v1.5 and 4.3% on the recent InternLM-XC2.5.\nMoreover, MIA-DPO has a minimal effect on the model's ability to understand single images.", "title_embedding_index": 21063, "title_abs_embedding_index": 21088}, {"title": "RAR: Retrieving And Ranking Augmented MLLMs for Visual Recognition", "link_suffix": "/forum?id=ayg1PztmXP", "link": "https://openreview.net/forum?id=ayg1PztmXP", "pdf_link": "https://openreview.net/pdf?id=ayg1PztmXP", "keywords": "Multimodal Large Language Models", "abstract": "CLlP (Contrastive Language lmage Pre-training) uses contrastive learning from noise image-text pairs to excel at recognizing a wide array of candidates, yet its focus on broad associations hinders the precision in distinguishing subtle differences among fine-grained items.Conversely, Multimodal Large Language Models (MLLMs) excel at classifying fine-grained categories, thanks to their substantial knowledge from pre-training on web-level corpora. However, the performance of MLLMs declines with an increase in category numbers, primarily due to growing complexity and constraints of limited context window size.To synergize the strengths of both approaches and enhance the few-shot/zero-shot recognition abilities for datasets characterized by extensive and fine-grained vocabularies, this paper introduces RAR, a Retrieving And Ranking augmented method for MLLMs. We initially establish a multi-modal retriever based on CLIP to create and store explicit memory for different categories beyond the immediate context window. During inference, RAR retrieves the top-k similar results from the memory and uses MLLMs to rank and make the final predictions. Our proposed approach not only addresses the inherent limitations in fine-grained recognition but also preserves the model's comprehensive knowledge base,significantly boosting accuracy across a range of vision-language recognition tasks. Notably, our approach demonstrates a significant improvement in performance on 5 fine-grained visual recognition benchmarks, 11 few-shot image recognition datasets, and the 2 object detection datasets under the zero-shot recognition setting.", "title_embedding_index": 21064, "title_abs_embedding_index": 21089}, {"title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated", "link_suffix": "/forum?id=cit3SNnZ6Q", "link": "https://openreview.net/forum?id=cit3SNnZ6Q", "pdf_link": "https://openreview.net/pdf?id=cit3SNnZ6Q", "keywords": "Activation sparsity, efficiency, large language models", "abstract": "We introduce, Q-Sparse, a simple yet effective approach to training sparsely-activated large language models (LLMs). Q-Sparse enables full sparsity of activations in LLMs which can bring significant efficiency gains in inference. This is achieved by applying top-K sparsification to the activations and the straight-through-estimator to the training. We also introduce Block Q-Sparse for batch training and inference. The key results from this work are, (1) Q-Sparse can achieve results comparable to those of baseline LLMs while being much more efficient at inference time; (2) We present an inference-optimal scaling law for sparsely-activated LLMs; (3) Q-Sparse is effective in different settings, including training-from-scratch, continue-training of off-the-shelf LLMs, and finetuning; (4) Q-Sparse works for both full-precision and 1-bit LLMs (e.g., BitNet b1.58). Particularly, the synergy of BitNet b1.58 and Q-Sparse (can be equipped with MoE) provides the cornerstone and a clear path to revolutionize the efficiency, including cost and energy consumption, of future LLMs.", "title_embedding_index": 21065, "title_abs_embedding_index": 21090}, {"title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants", "link_suffix": "/forum?id=8w22WLy2R8", "link": "https://openreview.net/forum?id=8w22WLy2R8", "pdf_link": "https://openreview.net/pdf?id=8w22WLy2R8", "keywords": "LLM-based agent, memory, evaluation, personal assistant", "abstract": "LLM-based agents have been widely applied as personal assistants, capable of memorizing information from user messages and responding to personal queries. However, there still lacks an objective and automatic evaluation on their memory capability, largely due to the challenges in constructing reliable questions and answers (QAs) according to user messages. In this paper, we propose MemSim, a Bayesian simulator designed to automatically construct reliable QAs from generated user messages, simultaneously keeping their diversity and scalability. Specifically, we introduce the Bayesian Relation Network (BRNet) and a causal generation mechanism to mitigate the impact of LLM hallucinations on factual information, facilitating the automatic creation of an evaluation dataset. Based on MemSim, we generate a dataset in the daily-life scenario, named MemDaily, and conduct extensive experiments to assess the effectiveness of our approach. We also provide a benchmark for evaluating different memory mechanisms in LLM-based agents with the MemDaily dataset.", "title_embedding_index": 21066, "title_abs_embedding_index": 21091}, {"title": "Causal Information Prioritization for Efficient Reinforcement Learning", "link_suffix": "/forum?id=nDj45w5wam", "link": "https://openreview.net/forum?id=nDj45w5wam", "pdf_link": "https://openreview.net/pdf?id=nDj45w5wam", "keywords": "causality, reinforcement learning, empowerment, sample efficiency", "abstract": "Current Reinforcement Learning (RL) methods often suffer from sample-inefficiency, resulting from blind exploration strategies that neglect causal relationships among states, actions, and rewards. Although recent causal approaches aim to address this problem, they lack grounded modeling of reward-guided causal understanding of states and actions for goal-orientation, thus impairing learning efficiency. To tackle this issue, we propose a novel method named Causal Information Prioritization (CIP) that improves sample efficiency by leveraging factored MDPs to infer causal relationships between different dimensions of states and actions with respect to rewards, enabling the prioritization of causal information. Specifically, CIP identifies and leverages causal relationships between states and rewards to execute counterfactual data augmentation to prioritize high-impact state features under the causal understanding of the environments. Moreover, CIP integrates a causality-aware empowerment learning objective, which significantly enhances the agent's execution of reward-guided actions for more efficient exploration in complex environments. \nTo fully assess the effectiveness of CIP, we conduct extensive experiments across $39$ tasks in $5$ diverse continuous control environments, encompassing both locomotion and manipulation skills learning with pixel-based and sparse reward settings. Experimental results demonstrate that CIP consistently outperforms existing RL methods across a wide range of scenarios.", "title_embedding_index": 21067, "title_abs_embedding_index": 21092}, {"title": "Image Generation with Channel-wise Quantization", "link_suffix": "/forum?id=YlWvQSBCgl", "link": "https://openreview.net/forum?id=YlWvQSBCgl", "pdf_link": "https://openreview.net/pdf?id=YlWvQSBCgl", "keywords": "generative models, image generation, visual tokenization", "abstract": "We present a novel image generation model with channel-wise quantization. Our method quantizes image feature along channel into discrete codes. Then based on the learned codes, our approach adopts masked-prediction paradigm for image generation. Compared with widely used spatial tokenizers, our channel-wise tokenizer has an efficient modeling for image structure and strong representational capacity. Besides, the codebook usage of our tokenizer can reach 100% under different codebook size. Using the channel-wise tokenizer, our generation framework achieves competitive performances on various benchmarks of image generation. In particular, on ImageNet 256x256 benchmark, our method significantly improve baseline by improving Frechet inception distance (FID) to 2.21 with 634M. Furthermore, we also validate the effectiveness of our proposed method on text-to-image generation.", "title_embedding_index": 21068, "title_abs_embedding_index": 21093}, {"title": "Towards Empowerment Gain through Causal Structure Learning in Model-Based RL", "link_suffix": "/forum?id=vgXI1Ws0ma", "link": "https://openreview.net/forum?id=vgXI1Ws0ma", "pdf_link": "https://openreview.net/pdf?id=vgXI1Ws0ma", "keywords": "Causal RL, MBRL, Empowerment, Intrinsic Motivation", "abstract": "In Model-Based Reinforcement Learning (MBRL), incorporating causal structures into dynamics models provides agents with a structured understanding of the environments, enabling efficient decision. \nEmpowerment as an intrinsic motivation enhances the ability of agents to actively control their environments by maximizing the mutual information between future states and actions. \nWe posit that empowerment coupled with causal understanding can improve controllability, while enhanced empowerment gain can further facilitate causal reasoning in MBRL. \nTo improve learning efficiency and controllability, we propose a novel framework, Empowerment through Causal Learning (ECL), where an agent with the awareness of causal dynamics models achieves empowerment-driven exploration and optimizes its causal structure for task learning. \nSpecifically, ECL operates by first training a causal dynamics model of the environment based on collected data. We then maximize empowerment under the causal structure for exploration, simultaneously using data gathered through exploration to update causal dynamics model to be more controllable than dense dynamics model without causal structure. In downstream task learning, an intrinsic curiosity reward is included to balance the causality, mitigating overfitting. \nImportantly, ECL is method-agnostic and is capable of integrating various causal discovery methods. \nWe evaluate ECL combined with $3$ causal discovery methods across $6$ environments including pixel-based tasks, demonstrating its superior performance compared to other causal MBRL methods, in terms of causal discovery, sample efficiency, and asymptotic performance.", "title_embedding_index": 21069, "title_abs_embedding_index": 21094}, {"title": "Progressive Visual Relationship Inference", "link_suffix": "/forum?id=V73W8MXnNW", "link": "https://openreview.net/forum?id=V73W8MXnNW", "pdf_link": "https://openreview.net/pdf?id=V73W8MXnNW", "keywords": "scene graph generation, visual relationship detection, visual scene understanding", "abstract": "As an important component of visual scene, visual relationship has received extensive attention in recent years.\nMost existing works directly utilize the rough visual appearance to represent visual relationships.\nAlthough they have been made tremendous progress, the study of visual relationship may be still far from perfect.\nThis common idea may have three problems.The similarity of space aggravates the ambiguity of predicate representation.The differences between many visual relationships are subtle.It lacks interpretability.\nTo address these problems, we propose a novel method - Progressive Visual Relationship Inference(\\PVRI) - which considers both rough visual appearance and fine-grained visual cues to gradually infer visual relationships.\nIt includes the following three steps.Known Cues Collection:\nfirstly, we utilize Large Language Model(LLM) to collect the cues that may help infer visual relationships;Unknown Cues Extraction:\nsecondly, we design UCE strategy to extract the cues that are not defined by the text.Progressive Inference:\nthirdly, we utilize the obtained cues to infer visual relationships.\nWe demonstrate the effectiveness and efficiency of our method for the Visual Genome, Open Image V6 datasets.", "title_embedding_index": 21070, "title_abs_embedding_index": 21095}, {"title": "Toward Generalizing Visual Brain Decoding to Unseen Subjects", "link_suffix": "/forum?id=At9JmGF3xy", "link": "https://openreview.net/forum?id=At9JmGF3xy", "pdf_link": "https://openreview.net/pdf?id=At9JmGF3xy", "keywords": "Visual brain decoding, fMRI - image retrieval, Generalizing to unseen subjects", "abstract": "Visual brain decoding aims to decode visual information from human brain activities. Despite the great progress, one critical limitation of current brain decoding research lies in the lack of generalization capability to unseen subjects. Prior works typically focus on decoding brain activity of individuals based on the observation that different subjects exhibit different brain activities, while it remains unclear whether brain decoding can be generalized to unseen subjects. This study is designed to answer this question. We first consolidate an image-fMRI dataset consisting of stimulus-image and fMRI-response pairs, involving 177 subjects in the movie-viewing task of the Human Connectome Project (HCP). This dataset allows us to investigate the brain decoding performance with the increase of participants. We then present a learning paradigm that applies uniform processing across all subjects, instead of employing different network heads or tokenizers for individuals as in previous methods, which can accommodate a large number of subjects to explore the generalization capability across different subjects. We conduct a series of experiments and find the following: First, the network exhibits clear generalization capabilities with the increase of training subjects. Second, the generalization capability is common to popular network architectures (MLP, CNN and Transformer). Third, the generalization performance is affected by the similarity between subjects. Our findings reveal the inherent similarities in brain activities across individuals. With the emerging of larger and more comprehensive datasets, it is possible to train a brain decoding foundation model in the future.", "title_embedding_index": 21071, "title_abs_embedding_index": 21096}, {"title": "CONCORD: Concept-informed Diffusion for Dataset Distillation", "link_suffix": "/forum?id=CaexTRYaN6", "link": "https://openreview.net/forum?id=CaexTRYaN6", "pdf_link": "https://openreview.net/pdf?id=CaexTRYaN6", "keywords": "dataset distillation, diffusion model", "abstract": "Dataset distillation has witnessed significant progress in synthesizing small-scale datasets that encapsulate rich information from large-scale original ones. Particularly, methods based on generative priors show promising performance, while maintaining computational efficiency and cross-architecture generalization. However, the generation process lacks explicit controllability for each sample. Previous distillation methods primarily match the real distribution from the perspective of the entire dataset, whereas overlooking conceptual completeness at the instance level. This oversight can result in missing or incorrectly represented object details and compromised dataset quality. To this end, we propose to incorporate the conceptual understanding of large language models (LLMs) to perform a CONCept-infORmed Diffusion process for dataset distillation, in short as CONCORD. Specifically, distinguishable and fine-grained concepts are retrieved based on category labels to explicitly inform the denoising process and refine essential object details. By integrating these concepts, the proposed method significantly enhances both the controllability and interpretability of the distilled image generation, without replying on pre-trained classifiers. We demonstrate the efficacy of CONCORD by achieving state-of-the-art performance on ImageNet-1K and its subsets. It further advances the practical application of dataset distillation methods. The code implementation is attached in the supplementary material.", "title_embedding_index": 21072, "title_abs_embedding_index": 21097}, {"title": "UNLEARNING IS BETTER THAN UNSEEN: UNLEARNING SCORE-BASED GENERATIVE MODEL", "link_suffix": "/forum?id=88wyP257x4", "link": "https://openreview.net/forum?id=88wyP257x4", "pdf_link": "https://openreview.net/pdf?id=88wyP257x4", "keywords": "Machine Unlearning\uff0cScore-based generative model", "abstract": "Diffusion generative models, including Score-Based Generative Models (SGM) and Denoising Diffusion Probabilistic Models (DDPM), have demonstrated remarkable performance across various domains in recent years. However, concerns regarding privacy and potential misuse of AI-generated content have become increasingly prominent. While generative unlearning methods have been investigated on DDPM models, research on unlearning SGM is still largely missing. Furthermore, the current 'gold standard' of machine unlearning---retraining a model from scratch after removing the undesirable data, does not perform well in SGM and its downstream tasks, such as image inpainting and reconstruction. To fill this gap, we propose the first Score-based Generative Unlearning (SGU) for SGM, which surpasses the previous 'gold standard' of unlearning.SGU introduces a new score adjustment strategy that deviates the learned score from the original undesirable data score during the continuous-time stochastic differential equation process. Extensive experimental results demonstrate that SGU significantly reduces the likelihood of generating undesirable content while preserving high quality for normal image generation. Albeit designed for SGM, SGU is a general and flexible unlearning framework that is compatible with diverse diffusion architectures (SGM and DDPM) and training strategies (re-training and fine-tuning), and enables zero-shot transfer of the unlearning generative model to downstream tasks, including image inpainting and reconstruction. The code will be shared upon acceptance.", "title_embedding_index": 21073, "title_abs_embedding_index": 21098}, {"title": "Low-Dimension-to-High-Dimension Generalization and Its Implications for Length Generalization", "link_suffix": "/forum?id=jdpELUL0T6", "link": "https://openreview.net/forum?id=jdpELUL0T6", "pdf_link": "https://openreview.net/pdf?id=jdpELUL0T6", "keywords": "Length Generalization, Position Embedding", "abstract": "Low-Dimension-to-High-Dimension (LDHD) generalization is a special case of Out-of-Distribution (OOD) generalization, where the training data are restricted to a low-dimensional subspace of the high-dimensional testing space. Assuming that each instance is generated from a latent variable and the dimension of the latent variable reflects the problem scale, the inherent scaling challenge in length generalization can be captured by the LDHD generalization in the latent space. We theoretically demonstrate that LDHD generalization is generally unattainable without exploiting prior knowledge to provide appropriate inductive bias. Specifically, we explore LDHD generalization in Boolean functions. We verify that different architectures trained with (S)GD converge to \\emph{min-degree interpolators w.r.t. different independent sets}. LDHD generalization is achievable if and only if the target function coincides with this inductive bias. Applying the insights from LDHD generalization to length generalization, we explain the effectiveness of CoT as changing the structure latent space to enable better LDHD generalization. We also propose a principle for position embedding design to handle both the inherent LDHD generalization and the nuisances such as the data format. Following the principle, we propose a novel position embedding called RPE-Square that remedies the RPE for dealing with the data format nuisance.", "title_embedding_index": 21074, "title_abs_embedding_index": 21099}]