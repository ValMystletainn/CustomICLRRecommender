[
    {
        "title": "Imputation for prediction: beware of diminishing returns.",
        "link_suffix": "/forum?id=D1Y2XFgsPI",
        "link": "https://openreview.net/forum?id=D1Y2XFgsPI",
        "pdf_link": "https://openreview.net/pdf?id=D1Y2XFgsPI",
        "keywords": "imputation, missing",
        "abstract": "Missing values are prevalent across various fields, posing challenges for training and deploying predictive models. In this context, imputation is a common practice, driven by the hope that accurate imputations will enhance predictions. However, recent theoretical and empirical studies indicate that simple constant imputation can be consistent and competitive. This empirical study aims at clarifyingifandwheninvesting in advanced imputation methods yields significantly better predictions. Relating imputation and predictive accuracies across combinations of imputation and predictive models on 19 datasets, we show that imputation accuracy matters less i) when using expressive models, ii) when incorporating missingness indicators as complementary inputs, iii) matters much more for generated linear outcomes than for real-data outcomes. Interestingly, we also show that the use of the missingness indicator is beneficial to the prediction performance, even in MCAR scenarios. Overall, on real-data with powerful models, imputation quality has only a minor effect on prediction performance. Thus, investing in better imputations for improved predictions often offers limited benefits."
    },
    {
        "title": "Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers",
        "link_suffix": "/forum?id=B6HtEFoJiG",
        "link": "https://openreview.net/forum?id=B6HtEFoJiG",
        "pdf_link": "https://openreview.net/pdf?id=B6HtEFoJiG",
        "keywords": "Scientific computing, PDEs, Linear systems, Iterative solvers, Graph neural networks",
        "abstract": "Large linear systems are ubiquitous in modern computational science and engineering. The main recipe for solving them is the use of Krylov subspace iterative methods with well-designed preconditioners. Deep learning models can be used as nonlinear preconditioners during the iteration of linear solvers such as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners with predefined sparsity patterns. Recently, GNNs have been shown to be a promising tool for designing preconditioners to reduce the overall computational cost of iterative methods by constructing them more efficiently than with classical linear algebra techniques. However, preconditioners designed with these approaches cannot outperform those designed with classical methods in terms of the number of iterations in CG. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN to obtain preconditioners that reduce the condition number of the system more significantly. Numerical experiments show that our approach outperforms both classical and neural network-based methods for an important class of parametric partial differential equations. We also provide a heuristic justification for the loss function used and show that preconditioners obtained by learning with this loss function reduce the condition number in a more desirable way for CG."
    },
    {
        "title": "big.LITTLE Vision Transformer for Efficient Visual Recognition",
        "link_suffix": "/forum?id=pjNjlJN7up",
        "link": "https://openreview.net/forum?id=pjNjlJN7up",
        "pdf_link": "https://openreview.net/pdf?id=pjNjlJN7up",
        "keywords": "Efficient Vision Transformer",
        "abstract": "In this paper, we introduce the big.LITTLE Vision Transformer, an innovative architecture aimed at achieving efficient visual recognition. This dual-transformer system is composed of two distinct blocks: the big performance block, characterized by its high capacity and substantial computational demands, and the LITTLE efficiency block, designed for speed with lower capacity. The key innovation of our approach lies in its dynamic inference mechanism. When processing an image, our system determines the importance of each token and allocates them accordingly: essential tokens are processed by the high-performance big model, while less critical tokens are handled by the more efficient little model. This selective processing significantly reduces computational load without sacrificing the overall performance of the model, as it ensures that detailed analysis is reserved for the most important information. To validate the effectiveness of our big.LITTLE Vision Transformer, we conducted comprehensive experiments on image classification and segment anything task. Our results demonstrate that the big.LITTLE architecture not only maintains high accuracy but also achieves substantial computational savings. Specifically, our approach enables the efficient handling of large-scale visual recognition tasks by dynamically balancing the trade-offs between performance and efficiency. The success of our method underscores the potential of hybrid models in optimizing both computation and performance in visual recognition tasks, paving the way for more practical and scalable deployment of advanced neural networks in real-world applications."
    },
    {
        "title": "Efficient Controlled Language Generation with Low-Rank Autoregressive Reward Models",
        "link_suffix": "/forum?id=CPhqrV5Ehg",
        "link": "https://openreview.net/forum?id=CPhqrV5Ehg",
        "pdf_link": "https://openreview.net/pdf?id=CPhqrV5Ehg",
        "keywords": "Controlled text generation, LLM, Natural Language Processing, Reward modelling, Efficiency",
        "abstract": "Language models trained on large amounts of data are known to produce inappropriate content in some cases and require careful tuning to be used in the real world. We revisit the reward augmented decoding (RAD) approach to control the generation from a language model using the scores from a task-specific reward model. We investigate the training objective of RAD, and reformulate it as a task of learning a reward matrix. We show that RAD is designed to support high flexibility when representing the reward matrices, which leads to a higher computational costs during decoding. However, we demonstrate that RAD does not use its full flexibility. Motivated by this, we propose a simpler but more efficient low-rank parametrization of the reward model enabling fast and effective guided decoding. For the detoxification and sentiment control tasks, we show that our low-rank reward model performs on par with the more flexible RAD parametrization, while requiring only a single reward model call per generated token."
    },
    {
        "title": "CR-CTC: Consistency regularization on CTC for improved speech recognition",
        "link_suffix": "/forum?id=CIs9x2ZRgh",
        "link": "https://openreview.net/forum?id=CIs9x2ZRgh",
        "pdf_link": "https://openreview.net/pdf?id=CIs9x2ZRgh",
        "keywords": "Consistency regularization, CTC, speech recognition",
        "abstract": "Connectionist Temporal Classification (CTC) is a widely used method for automatic speech recognition (ASR), renowned for its simplicity and computational efficiency. However, it often falls short in recognition performance compared to transducer or systems combining CTC and attention-based encoder-decoder (CTC/AED). In this work, we propose the Consistency-Regularized CTC (CR-CTC), which enforces consistency between two CTC distributions obtained from different augmented views of the input speech mel-spectrogram. We provide in-depth insights into its essential behaviors from three perspectives: 1) it conducts self-distillation between random pairs of sub-models that process different augmented views; 2) it learns contextual representation through masked prediction for positions within time-masked regions, especially when we increase the amount of time masking; 3) it suppresses the extremely peaky CTC distributions, thereby reducing overfitting and improving the generalization ability. Extensive experiments on LibriSpeech, Aishell-1, and GigaSpeech datasets demonstrate the effectiveness of our CR-CTC, which achieves performance comparable to, or even slightly better than, that of transducer and CTC/AED."
    },
    {
        "title": "ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains",
        "link_suffix": "/forum?id=whaO3482bs",
        "link": "https://openreview.net/forum?id=whaO3482bs",
        "pdf_link": "https://openreview.net/pdf?id=whaO3482bs",
        "keywords": "Temporal knowledge, Knowledge update",
        "abstract": "Large language models (LLMs) have brought significant changes to many aspects of our lives.\nHowever, assessing and ensuring their chronological knowledge remains challenging.\nExisting approaches fall short in addressing the accumulative nature of knowledge, often relying on a single time stamp. \nTo overcome this, we introduce ChroKnowBench, a benchmark dataset designed to evaluate chronologically accumulated knowledge across three key aspects: multiple domains, time dependency, temporal state.\nOur benchmark distinguishes between knowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge that remain constant (e.g., mathematical truths, commonsense facts). \nBuilding on this benchmark, we present ChroKnowledge (Chronological Categorization of Knowledge), a novel sampling-based framework for evaluating and updating LLMs' non-parametric chronological knowledge.\nOur evaluation led to the following observations: \n(1) The ability of eliciting temporal knowledge varies depending on the data format that model was trained on.\n(2) LLMs partially recall knowledge or show a cut-off at temporal boundaries rather than recalling all aspects of knowledge correctly.\nThus, we apply our ChroKnowPrompt, an in-depth prompting to elicit chronological knowledge by traversing step-by-step through the surrounding time spans.\nWe observe that our framework successfully updates the overall knowledge across the entire timeline in both the biomedical domain (+11.9%) and the general domain (+2.8%), highlighting its positive effect in refining temporal knowledge. \nThis non-parametric approach also enables knowledge updates not only in open-source models but also in proprietary LLMs, ensuring comprehensive applicability across model types.\nWe perform a comprehensive analysis based on temporal characteristics of ChroKnowPrompt and validate the potential of various models to elicit intrinsic temporal knowledge through our method."
    },
    {
        "title": "NF-MKV Net: A Constraint-Preserving Neural Network Approach to Solving Mean-Field Games Equilibrium",
        "link_suffix": "/forum?id=jXrXTuvA3L",
        "link": "https://openreview.net/forum?id=jXrXTuvA3L",
        "pdf_link": "https://openreview.net/pdf?id=jXrXTuvA3L",
        "keywords": "Mean-Field Games, Normalizing Flow, McKean-Vlasov type Forward-Backward Stochastic Differential Equations, Mathematical Constraints Neural Network",
        "abstract": "Neural network-based methods for solving Mean-Field Games (MFGs) equilibrium have gained significant attention due to their effectiveness in high-dimensional settings. However, many algorithms face the problem that the density distribution evolution does not satisfy the mathematical constraints in the solution. This paper explores the neural network solution of MFGs equilibrium from the perspective of stochastic process, while coupling process-regularized Normalizing Flow (NF) frameworks and state-policy-connected time series neural networks to solve the McKean-Vlasov type Forward-Backward Stochastic Differential Equations (MKV FBSDEs) fixed point problems, which is equivalent of the MFGs equilibrium. First, we convert the MFGs equilibrium to MKV FBSDEs which introduce the density distribution into equations coefficients within a probabilistic framework, and construct neural networks to approximate value functions and gradients based on these equations. Second, we employ NF architectures\u2014generative neural network models\u2014and by imposing loss constraints on each density transfer function, the algorithm ensures compliance with volumetric-invariance and time-continuity. Additionally, this paper provides theoretical proofs of the algorithm's validity and demonstrates its applicability across various scenarios, showcasing its effective compared to existing approaches."
    },
    {
        "title": "Equivariant Denoisers Cannot Copy Graphs: Align Your Graph Diffusion Models",
        "link_suffix": "/forum?id=onIro14tHv",
        "link": "https://openreview.net/forum?id=onIro14tHv",
        "pdf_link": "https://openreview.net/pdf?id=onIro14tHv",
        "keywords": "Graph Diffusion, Discrete Diffusion, Equivariance, Symmetries",
        "abstract": "Graph diffusion models, while dominant in graph generative modeling, remain relatively underexplored for graph-to-graph translation tasks like chemical reaction prediction. We show that standard permutation equivariant denoisers cause severe limitations such tasks, a problem that we pinpoint to their inability for breaking symmetries present in the noisy inputs. We then propose to \\emph{align} the input and target graphs in order to break the input symmetries, while retaining permutation equivariance in the non-matching portions of the graph. We choose retrosynthesis as an application domain, and  show how alignment takes the performance of a discrete diffusion model from a mere 5% to a SOTA-matching 54.7% top-1 accuracy."
    },
    {
        "title": "Controlling Information Leakage in Concept Bottleneck Models with Trees",
        "link_suffix": "/forum?id=BipgUWZWNi",
        "link": "https://openreview.net/forum?id=BipgUWZWNi",
        "pdf_link": "https://openreview.net/pdf?id=BipgUWZWNi",
        "keywords": "interpretable models, concept bottleneck model, information leakage, decision tree",
        "abstract": "As AI models grow larger, the demand for accountability and interpretability has become increasingly critical for understanding their decision-making processes. Concept Bottleneck Models (CBMs) have gained attention for enhancing interpretability by mapping inputs to intermediate concepts before making final predictions. However, CBMs often suffer from information leakage, where additional input data, not captured by the concepts, is used to improve task performance, complicating the interpretation of downstream predictions. In this paper, we introduce a novel approach for training both joint and sequential CBMs that allows us to identify and control leakage using decision trees. Our method quantifies leakage by comparing the decision paths of hard CBMs with their soft, leaky counterparts. Specifically, we show that soft leaky CBMs extend the decision paths of hard CBMs, particularly in cases where concept information is incomplete. Using this insight, we develop a technique to better inspect and manage leakage, isolating the subsets of data  most affected by this. Through synthetic and real-world experiments, we demonstrate that controlling leakage in this way not only improves task accuracy but also yields more informative and transparent explanations."
    },
    {
        "title": "AnyView: Few Shot Personalized View Transfer",
        "link_suffix": "/forum?id=GuQeZWbaGr",
        "link": "https://openreview.net/forum?id=GuQeZWbaGr",
        "pdf_link": "https://openreview.net/pdf?id=GuQeZWbaGr",
        "keywords": "View Transfer, Diffusion, Generative",
        "abstract": "Fine-tuning generative models for concept driven personalization have witnessed tremendous growth ever since the arrival of methods like DreamBooth, Textual Inversion etc. Particularly, such techniques have been thoroughly explored for style-driven generation. Recently, diffusion models have also demonstrated impressive capabilities in view synthesis tasks, setting the foundation for exploring view-driven generation approaches. Motivated by these advancements, we investigate the capacity of a pretrained stable diffusion model to grasp ``what constitutes a view\" without relying on explicit 3D priors. Specifically, we base our method on a personalized text to image model, Dreambooth, given its strong ability to adapt to specific novel objects with a few shots. Our research reveals two interesting findings. First, we observe that Dreambooth can learn the high level concept of a view, compared to arguably more complex strategies which involve fine-tuning diffusions on large amounts of multi-view data. Second, we establish that the concept of a view can be disentangled and transferred to a novel object irrespective of the original object\u2019s identity from which the views are learnt. Motivated by this, we introduce a learning strategy, AnyView, which inherits a specific view through only one image sample of a single scene, and transfers the knowledge to a novel object, learnt from a few shots, using low rank adapters. Through extensive experiments we demonstrate that our method, albeit simple, is efficient in generating reliable view samples for in the wild images. Code and models will be released."
    },
    {
        "title": "Optimizing Q-Learning Using Expectile Regression: A Dual Approach to Handle In-Sample and Out-of-Sample Data",
        "link_suffix": "/forum?id=C9BA0T3xhq",
        "link": "https://openreview.net/forum?id=C9BA0T3xhq",
        "pdf_link": "https://openreview.net/pdf?id=C9BA0T3xhq",
        "keywords": "reinforcement learning",
        "abstract": "Offline Reinforcement Learning (RL) presents unique challenges, primarily due to the constraint of learning from a static dataset without additional environmental interaction. Traditional methods often face limitations in effectively exploiting the available data, particularly when navigating the exploration-exploitation trade-off inherent in RL. This paper introduces a novel algorithm inspired by Implicit Q-Learning, designed to extend the utility of the Bellman update to actions not explicitly present in the dataset. Our approach, termed Extended Implicit Q-Learning (EIQL), strategically incorporates actions beyond the dataset constraints by allowing selection actions with maximum Q. By doing so, it leverages the maximization capability of the Bellman update, while simultaneously mitigating error extrapolation risks. We demonstrate the efficacy of EIQL through a series of experiments that show its improved performance over traditional offline RL algorithms, particularly in environments characterized by sparse rewards or those containing suboptimal and incomplete trajectories. Our results suggest that EIQL enhances the potential of offline RL by utilizing a broader action spectrum."
    },
    {
        "title": "Influence-Guided Diffusion for Dataset Distillation",
        "link_suffix": "/forum?id=0whx8MhysK",
        "link": "https://openreview.net/forum?id=0whx8MhysK",
        "pdf_link": "https://openreview.net/pdf?id=0whx8MhysK",
        "keywords": "Dataset Distillation, Dataset Condensation, Diffusion Model, Guided Diffusion Generation",
        "abstract": "Dataset distillation aims to streamline the training process by creating a compact yet effective dataset for a much larger original dataset. However, existing methods often struggle with distilling large, high-resolution datasets due to prohibitive resource costs and limited performance, primarily stemming from sample-wise optimizations in the pixel space. Motivated by the remarkable capabilities of diffusion generative models in learning target dataset distributions and controllably sampling high-quality data tailored to user needs, we propose framing dataset distillation as a controlled diffusion generation task aimed at generating data specifically tailored for effective training purposes. By establishing a correlation between the overarching objective of dataset distillation and the trajectory influence function, we introduce the Influence-Guided Diffusion (IGD) sampling framework to generate training-effective data without the need to retrain diffusion models. An efficient guided function is designed by leveraging the trajectory influence function as an indicator to steer diffusions to produce data with influence promotion and diversity enhancement. Extensive experiments show that the training performance of distilled datasets generated by diffusions can be significantly improved by integrating with our IGD method and achieving state-of-the-art performance in distilling ImageNet datasets. Particularly, an exceptional result is achieved on the ImageNet-1K, reaching 60.3% at IPC=50."
    },
    {
        "title": "Why DP \"LOCAL\" SGD \u2013 Faster Convergence in Less Composition with Clipping Bias Reduction",
        "link_suffix": "/forum?id=u1rlO94Bnr",
        "link": "https://openreview.net/forum?id=u1rlO94Bnr",
        "pdf_link": "https://openreview.net/pdf?id=u1rlO94Bnr",
        "keywords": "Differential Privacy, Local Stochastic Gradient Descent, Clipping Bias",
        "abstract": "We argue to apply Differentially-Private Local Stochastic Gradient Descent (DP-LSGD), a generalization of regular DP-SGD with per-sample local iterations, to systematically improve privacy-preserving machine learning. We prove and show the following facts in this paper: a). DP-LSGD with local iterations can produce more concentrated per-sample updates and therefore enables a more efficient exploitation of the clipping budget with a better utility-privacy tradeoff; b). given the same $T$ privacy composition or per-sample update aggregation, with properly-selected local iterations, DP-LSGD can converge faster in $O(1/T)$ to a small neighborhood of (local) optimum compared to $O(1/\\sqrt{T})$ in regular DP-SGD, i.e., DP-LSGD produces the same accuracy while consumes less of the privacy budget. From an empirical side, thorough experiments are provided to support our developed theory and we show DP-LSGD produces the best-known performance in various practical deep learning tasks: for example with an $(\\epsilon=4,\\delta=10^{-5})$-DP guarantee, we successfully train ResNet20 from scratch with test accuracy $74.1%, 86.5%$ and $91.7%$ on CIFAR10, SVHN and EMNIST, respectively. Our code is released in an anonymous GitHub link."
    },
    {
        "title": "ELICIT: LLM Augmentation Via External In-context Capability",
        "link_suffix": "/forum?id=CI4sCBMXjP",
        "link": "https://openreview.net/forum?id=CI4sCBMXjP",
        "pdf_link": "https://openreview.net/pdf?id=CI4sCBMXjP",
        "keywords": "modular",
        "abstract": "Enhancing the adaptive capabilities of large language models is a critical pursuit in both research and application.\nTraditional fine-tuning methods require substantial data, computational resources, and specific capabilities, while in-context learning is limited by the need for appropriate demonstrations and efficient token usage.\n    Inspired by the expression of in-context learned capabilities through task vectors and the concept of modular capability or knowledge, we propose ELICIT, a framework consisting of two modules designed to effectively store and reuse task vectors to enhance the diverse adaptive capabilities of models without additional training or inference tokens.\n    Our comprehensive experiments and analysis demonstrate that our pipeline is highly transferable across different input formats, tasks, and model architectures.\n    Externally storing and reusing vectors that represent in-context learned capabilities not only shows the potential to extract modular capabilities but also significantly enhances the performance, versatility, adaptability, and scalability of large language models, paving the way for more efficient and effective use of these models in a wide range of applications."
    },
    {
        "title": "SADE: a Scene-text Autoregressive Diffusion Engine for Character Sequence Recognition",
        "link_suffix": "/forum?id=TBw53TdDgb",
        "link": "https://openreview.net/forum?id=TBw53TdDgb",
        "pdf_link": "https://openreview.net/pdf?id=TBw53TdDgb",
        "keywords": "optical character recognition, diffusion models, autoregressive image generation",
        "abstract": "We consider the problem of training an optical character recognition (OCR) model to read short alphanumeric scene-text sequences, such as number plates or vehicle type labels, in scenarios where labelled training images are limited in quantity and sequence variety. OCR models may under-perform in these scenarios, so we explore whether a diffusion model can be trained on the small set of labelled images, to generate synthetic images with similar background statistics but new character sequences. We find that a diffusion model struggles to generate characters in positions of the sequence where they did not appear during training. We address this problem by introducing SADE: a scene-text autoregressive diffusion engine that generates multiple image parts one by one, conditioned on previously generated image parts for visual coherency. This approach reduces the effective number of possible positions for a character, and increases the diffusion model's ability to generate characters in novel positions of the full sequence. Our results indicate that SADE can indeed lead to substantial improvements in OCR accuracy in data-scare scenarios, particularly on sequences with characters at positions not encountered in the original training data."
    },
    {
        "title": "Diffusion-Based Offline RL for Improved Decision-Making in Augmented Single ARC Task",
        "link_suffix": "/forum?id=VR9RS7tZGG",
        "link": "https://openreview.net/forum?id=VR9RS7tZGG",
        "pdf_link": "https://openreview.net/pdf?id=VR9RS7tZGG",
        "keywords": "System-2 Reasoning, Reasoning, Abstraction, Diffusion model, offline rl",
        "abstract": "Effective long-term strategies enable AI systems to navigate complex environments by making sequential decisions over extended horizons. Similarly, reinforcement learning (RL) agents optimize decisions across sequences to maximize rewards, even without immediate feedback. To verify that Latent Diffusion-Constrained Q-learning (LDCQ), a prominent diffusion-based offline RL method, demonstrates strong reasoning abilities in multi-step decision-making, we aimed to evaluate its performance on the Abstraction and Reasoning Corpus (ARC). However, applying offline RL methodologies to enhance strategic reasoning in AI for solving tasks in ARC is challenging due to the lack of sufficient experience data in the ARC training set. To address this limitation, we introduce an augmented offline RL dataset for ARC, called Synthesized Offline Learning Data for Abstraction and Reasoning (SOLAR), along with the SOLAR-Generator, which generates diverse trajectory data based on predefined rules. SOLAR enables the application of offline RL methods by offering sufficient experience data. We synthesized SOLAR for a simple task and used it to train an agent with the LDCQ method. Our experiments demonstrate the effectiveness of the offline RL approach on a simple ARC task, showing the agent's ability to make multi-step sequential decisions and correctly identify answer states. These results highlight the potential of the offline RL approach to enhance AI's strategic reasoning capabilities."
    },
    {
        "title": "RecFlow: An Industrial Full Flow Recommendation Dataset",
        "link_suffix": "/forum?id=vVHc8bGRns",
        "link": "https://openreview.net/forum?id=vVHc8bGRns",
        "pdf_link": "https://openreview.net/pdf?id=vVHc8bGRns",
        "keywords": "recommendation system, recommendation dataset",
        "abstract": "Industrial recommendation systems (RS) rely on the multi-stage pipeline to balance effectiveness and efficiency when delivering items from a vast corpus to users. Existing RS benchmark datasets primarily focus on the exposure space, where novel RS algorithms are trained and evaluated. However, when these algorithms transition to real-world industrial RS, they face a critical challenge: handling unexposed items\u2014a significantly larger space than the exposed one. This discrepancy profoundly impacts their practical performance. Additionally, these algorithms often overlook the intricate interplay between multiple RS stages, resulting in suboptimal overall system performance. To address this issue, we introduce RecFlow\u2014an industrial full-flow recommendation dataset designed to bridge the gap between offline RS benchmarks and the real online environment. Unlike existing datasets, RecFlow includes samples not only from the exposure space but also unexposed items filtered at each stage of the RS funnel. Our dataset comprises 38M interactions from 42K users across nearly 9M items with additional 1.9B stage samples collected from 9.3M online requests over 37 days and spanning 6 stages. Leveraging the RecFlow dataset, we conduct courageous exploration experiments, showcasing its potential in designing new algorithms to enhance effectiveness by incorporating stage-specific samples. Some of these algorithms have already been deployed online, consistently yielding significant gains. We propose RecFlow as the first comprehensive benchmark dataset for the RS community, supporting research on designing algorithms at any stage, study of selection bias, debiased algorithms, multi-stage consistency and optimality, multi-task recommendation, and user behavior modeling. The RecFlow dataset, along with the corresponding source code, is publicly available at \\textcolor{red}{\\url{https://github.com/RecFlow-ICLR/RecFlow}}. The dataset is licensed under CC-BY-NC-SA-4.0 International License."
    },
    {
        "title": "Zero-Shot Offline Imitation Learning via Optimal Transport",
        "link_suffix": "/forum?id=vDecbmWf6w",
        "link": "https://openreview.net/forum?id=vDecbmWf6w",
        "pdf_link": "https://openreview.net/pdf?id=vDecbmWf6w",
        "keywords": "Imitation Learning, Deep Reinforcement Learning, Optimal Transport",
        "abstract": "Zero-shot imitation learning algorithms hold the promise of reproducing unseen behavior from as little as a single demonstration at test time.\nExisting practical approaches view the expert demonstration as a sequence of goals, enabling imitation with a high-level goal selector, and a low-level goal-conditioned policy. \nHowever, this framework can suffer from myopic behavior: the agent's immediate actions towards achieving individual goals may undermine long-term objectives.\nWe introduce a novel method that mitigates this issue by directly optimizing the occupancy matching objective that is intrinsic to imitation learning. \nWe propose to lift a goal-conditioned value function to a distance between occupancies, which are in turn approximated via a learned world model.\nThe resulting method can learn from offline, suboptimal data, and is capable of non-myopic, zero-shot imitation, as we demonstrate in complex, continuous benchmarks."
    },
    {
        "title": "Secure Diffusion Model Unlocked: Efficient Inference via Score Distillation",
        "link_suffix": "/forum?id=fkNsgI1nye",
        "link": "https://openreview.net/forum?id=fkNsgI1nye",
        "pdf_link": "https://openreview.net/pdf?id=fkNsgI1nye",
        "keywords": "private inference, diffusion model",
        "abstract": "As services based on diffusion models expand across various domains, preserving the privacy of client data becomes more critical. Fully homomorphic encryption and secure multi-party computation have been employed for privacy-preserving inference, but these methods are computationally expensive and primarily work for linear computations, making them challenging to apply to large diffusion models. While homomorphic encryption has been recently applied to diffusion models, it falls short of fully safeguarding privacy, as inputs used in the $\\epsilon$ prediction are not encrypted. In this paper, we propose a novel framework for private inference for both inputs and outputs. To ensure robust approximations, we introduce several techniques for handling non-linear operations. Additionally, to reduce latency, we curtail the number of denoising steps while minimizing performance degradation of conditional generation through score distillation from the unconditional generation of the original model with full denoising steps. Experimental results show that our model produces high-quality images comparable to the original, and the proposed score distillation significantly enhances performance, compensating for fewer steps and approximation errors."
    },
    {
        "title": "Beyond Markov Assumption: Improving Sample Efficiency in MDPs by Historical Augmentation",
        "link_suffix": "/forum?id=v9GwGQoOG5",
        "link": "https://openreview.net/forum?id=v9GwGQoOG5",
        "pdf_link": "https://openreview.net/pdf?id=v9GwGQoOG5",
        "keywords": "Deep reinforcement learning, Sample efficiency, State representation, Historical augmentation, Markov decision processes",
        "abstract": "Under the Markov assumption of Markov Decision Processes (MDPs), an optimal stationary policy does not need to consider history and is no worse than any non-stationary or history-dependent policy. Therefore, existing Deep Reinforcement Learning (DRL) algorithms usually model sequential decision-making as an MDP and then try to optimize a stationary policy by single-step state transitions. However, such optimization is often faced with sample inefficiency when the causal relationships of state transitions are complex. To address the above problem, this paper investigates if augmenting the states with their historical information can simplify the complex causal relationships in MDPs and thus improve the sample efficiency of DRL. First, we demonstrate that a complex causal relationship of single-step state transitions may be inferred by a simple causal function of the historically augmented states. Then, we propose a convolutional neural network architecture to learn the representation of the current state and its historical trajectory. The main idea of this representation learning is to compress the high-dimensional historical trajectories into a low-dimensional space. In this way, we can extract the simple causal relationships from historical information and avoid the overfitting caused by high-dimensional data. Finally, we formulate Historical Augmentation Aided Actor-Critic (HA3C) algorithm by adding the learned representations to the actor-critic method. The experiment on standard MDP tasks demonstrates that HA3C outperforms current state-of-the-art methods in terms of both sample efficiency and performance."
    },
    {
        "title": "Object-Aware Audio-Visual Sound Generation",
        "link_suffix": "/forum?id=hsnt2TKvLU",
        "link": "https://openreview.net/forum?id=hsnt2TKvLU",
        "pdf_link": "https://openreview.net/pdf?id=hsnt2TKvLU",
        "keywords": "Sound Generation, Audio-Visual Learning, Multimodal Learning",
        "abstract": "Generating accurate sounds for complex audio-visual scenes is challenging, especially when multiple objects and sound sources are present. In this paper, we introduce an object-aware sound generation model that aligns generated sounds with visual objects in a scene. By grounding sound generation in object-centric representations, our model learns to associate specific visual objects with their corresponding sounds. We fine-tune a conditional latent diffusion model with dot-product attention to improve sound-object alignment. At test time, users can compositionally generate sounds by selecting objects via segmentation masks. We theoretically validate our test-time object-grounding ability, ensuring that even subtle sounds can be represented. Quantitative and qualitative evaluations show that our model outperforms baselines, achieving better alignment between objects and their associated sounds."
    },
    {
        "title": "Offline Hierarchical Reinforcement Learning via Inverse Optimization",
        "link_suffix": "/forum?id=dTPz4rEDok",
        "link": "https://openreview.net/forum?id=dTPz4rEDok",
        "pdf_link": "https://openreview.net/pdf?id=dTPz4rEDok",
        "keywords": "Offline Reinforcement Learning, Hierarchical Reinforcement Learning",
        "abstract": "Hierarchical policies enable strong performance in many sequential decision-making problems, such as those with high-dimensional action spaces, those requiring long-horizon planning, and settings with sparse rewards. \nHowever, learning hierarchical policies from static offline datasets presents a significant challenge.\nCrucially, actions taken by higher-level policies may not be directly observable within hierarchical controllers, and the offline dataset might have been generated using a different policy structure, hindering the use of standard offline learning algorithms.\nIn this work, we propose $\\textit{OHIO}$: a framework for offline reinforcement learning (RL) of hierarchical policies. \nOur framework leverages knowledge of the policy structure to solve the $\\textit{inverse problem}$, recovering the unobservable high-level actions that likely generated the observed data under our hierarchical policy.\nThis approach constructs a dataset suitable for off-the-shelf offline training.\nWe demonstrate our framework on robotic and network optimization problems and show that it substantially outperforms end-to-end RL methods and improves robustness. \nWe investigate a variety of instantiations of our framework, both in direct deployment of policies trained offline and when online fine-tuning is performed."
    },
    {
        "title": "Why Barlow Twins Work: The Critical Role of Normalization and Its Link to Sample Contrastive Learning",
        "link_suffix": "/forum?id=ZINaxJyoQr",
        "link": "https://openreview.net/forum?id=ZINaxJyoQr",
        "pdf_link": "https://openreview.net/pdf?id=ZINaxJyoQr",
        "keywords": "self-supervised Learning, pretaining, cross-correlation matrix, covariance matrix, diagonalization",
        "abstract": "Barlow Twins is a feature-contrastive self-supervised learning framework built on the principle of redundancy reduction. The idea is to train a network by maximizing the correlation between corresponding features and minimizing the correlation between non-corresponding features in distorted views of the same image, through this facilitating effective pretraining of a backbone network for a subsequent classification head. This is achieved by diagonalizing the cross-correlation matrix of the network\u2019s representations and scaling it towards the identity matrix. We show that the cross-correlation matrix of distorted images is inherently symmetric, independent of the backbone network's weights, which leads to two key insights: (i) the cross-correlation matrix can always be diagonalized using a linear transformation (layer), and (ii) the core idea of maximizing correlations between corresponding features while minimizing them for non-corresponding features alone is insufficient for effective backbone network pretraining. Nevertheless, Barlow Twins provide highly effective pretraining. We show that this is due to the normalization of the cross-correlation matrix in the Barlow Twins cost function. This normalization leads to minima of the cost function which are equivalent to the minima of sample contrastive approaches to enforce invariance."
    },
    {
        "title": "Improving Uncertainty Quantification in Large Language Models via Semantic Embeddings",
        "link_suffix": "/forum?id=N4mb3MBV6J",
        "link": "https://openreview.net/forum?id=N4mb3MBV6J",
        "pdf_link": "https://openreview.net/pdf?id=N4mb3MBV6J",
        "keywords": "uncertainty estimation, large language models, natural language generation, variational inference",
        "abstract": "Accurately quantifying uncertainty in large language models (LLMs) is crucial for their reliable deployment, especially in high-stakes applications. Current state-of-the-art methods for measuring semantic uncertainty in LLMs rely on strict bidirectional entailment criteria between multiple generated responses and also depend on sequence likelihoods. While effective, these approaches often overestimate uncertainty due to their sensitivity to minor wording differences, additional correct information, and non-important words in the sequence. We propose a novel approach that leverages semantic embeddings to achieve smoother and more robust estimation of semantic uncertainty in LLMs. By capturing semantic similarities without depending on sequence likelihoods, our method inherently reduces any biases introduced by irrelevant words in the answers. Furthermore, we introduce an amortised version of our approach by explicitly modelling semantics as latent variables in a joint probabilistic model. This allows for uncertainty estimation in the embedding space with a single forward pass, significantly reducing computational overhead compared to existing multi-pass methods. Experiments across multiple question-answering datasets and frontier LLMs demonstrate that our embedding-based methods provide more accurate and nuanced uncertainty quantification than traditional approaches."
    },
    {
        "title": "Evaluating Single-Cell Foundation Models for Cell Retrieval",
        "link_suffix": "/forum?id=iOltCu4TPS",
        "link": "https://openreview.net/forum?id=iOltCu4TPS",
        "pdf_link": "https://openreview.net/pdf?id=iOltCu4TPS",
        "keywords": "Single-cell Foundation Models, Cell Retrieval, Benchmarking",
        "abstract": "Efficiently and accurately searching large-scale single-cell RNA-seq databases has been a long standing computational challenge. There is an increasing number of single-cell retrieval methods, particularly those based on single-cell foundation models, proposed in the literature. However, this field lacks a comprehensive benchmark among these methods. This gap exists due to the lack of standard evaluation metrics and comprehensive benchmark datasets. Addressing these challenges, we propose a comprehensive evaluation benchmark to assess the capabilities of 12 existing single-cell retrieval methods from three classes: non-machine learning method, VAE-based methods and single-cell foundation model (scFM) based methods. We propose a series of label-dependent and label-free evaluation metrics to assess the performance of single-cell retrieval methods. Through benchmarking across diverse settings (cross-platform, cross-species and cross-omics), our notable findings include: top scFMs such as UCE, scFoundation and SCimilarity show substantial overall advantage compared with other methods; traditional non-machine learning method perform well in cell retrieval thus should not be neglected; common cells retrieved by top methods share distinct gene expression patterns; label-free metrics have consistent evaluation outcome compared with label-based methods thus can be employed in a broader scenario. Our rigorous and comprehensive evaluation identifies the challenges and limitations of current single-cell retrieval methods and serves as foundation for further development of single-cell retrieval methods."
    }
]