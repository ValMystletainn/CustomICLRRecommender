[
    {
        "title": "Decoupled representation and policy acquisition for continual reinforcement learning",
        "link_suffix": "/forum?id=Q1Hr9dVfDS",
        "link": "https://openreview.net/forum?id=Q1Hr9dVfDS",
        "pdf_link": "https://openreview.net/pdf?id=Q1Hr9dVfDS",
        "keywords": "continual learning, reinforcement learning, q-learning, replay",
        "abstract": "This contribution proposes adiabatic reinforcement learning (ARL), a new method for continual reinforcement learning (CRL). \nIn CRL, we assume a non-stationary environment partitioned into \\textit{tasks}. To avoid catastrophic forgetting (CF), RL requires the use\nof large replay buffers, which leads to very slow learning and high memory requirements.\nTo remedy this, we propose adiabatic reinforcement learning (ARL), a wake-sleep method that performs slow learning of internal representations from high-error transitions during sleep phases. Wake phases are used for the fast learning of policies, i.e., mappings from representations to actions, \nand to collect new high-error transitions. \nRepresentation learning is performed by \\textit{adiabatic replay} (AR), a recent CL technique we adapted to the RL setting. AR uses selective, internal replay of samples\nthat are likely to be affected by forgetting. Since this process is conditioned on incoming samples only, its has constant time-complexity w.r.t. tasks. Other benefits include \nfast adaptation to new tasks, and a very low memory footprint due to the complete absence of replay buffers."
    },
    {
        "title": "ReHub: Linear Complexity Graph Transformers with Adaptive Hub-Spoke Reassignment",
        "link_suffix": "/forum?id=kRaWc3Hk0q",
        "link": "https://openreview.net/forum?id=kRaWc3Hk0q",
        "pdf_link": "https://openreview.net/pdf?id=kRaWc3Hk0q",
        "keywords": "Learning on graphs, long-range",
        "abstract": "We present ReHub, a novel graph transformer architecture that achieves linear complexity through an efficient reassignment technique between nodes and virtual nodes. Graph transformers have become increasingly important in graph learning for their ability to utilize long-range node communication explicitly, addressing limitations such as oversmoothing and oversquashing found in message-passing graph networks. However, their dense attention mechanism scales quadratically with the number of nodes, limiting their applicability to large-scale graphs. ReHub draws inspiration from the airline industry's hub-and-spoke model, where flights are  assigned to optimize operational efficiency. In our approach, graph nodes (spokes) are dynamically reassigned to a fixed number of virtual nodes (hubs) at each model layer. Recent work, Neural Atoms (Li et al., 2024), has demonstrated impressive and consistent improvements over GNN baselines by utilizing such virtual nodes; their findings suggest that the number of hubs strongly influences performance. However, increasing the number of hubs typically raises complexity, requiring a trade-off to maintain linear complexity. Our key insight is that each node only needs to interact with a small subset of hubs to achieve linear complexity, even when the total number of hubs is large. To leverage all hubs without incurring additional computational costs, we propose a simple yet effective adaptive reassignment technique based on hub-hub similarity scores, eliminating the need for expensive node-hub computations. Our experiments on long-range graph benchmarks indicate a consistent improvement in results over the base method, Neural Atoms, while maintaining a linear complexity instead of $O(n^{3/2})$. Remarkably, our sparse model achieves performance on par with its non-sparse counterpart. Furthermore, ReHub outperforms competitive baselines and consistently ranks among the top performers across various benchmarks."
    },
    {
        "title": "Federated Adapter on Foundation Models:  An Out-Of-Distribution Approach",
        "link_suffix": "/forum?id=LcpdPCkZwI",
        "link": "https://openreview.net/forum?id=LcpdPCkZwI",
        "pdf_link": "https://openreview.net/pdf?id=LcpdPCkZwI",
        "keywords": "Federated Learning, Foundation Models",
        "abstract": "As foundation models gain increasing attention from both academic and industrial communities, Federated Foundation Models (FedFM) have emerged as a privacy-preserving approach for collaboratively fine-tuning models in federated learning (FL) frameworks using distributed datasets across multiple clients. A key challenge for FedFM, given the versatile nature of foundation models, is addressing out-of-distribution (OOD) generalization, where unseen tasks or clients may exhibit distribution shifts leading to suboptimal performance. \nAlthough numerous studies have explored OOD generalization in conventional FL, these methods are inadequate for FedFM due to the challenges posed by large parameter scales and increased data heterogeneity, where large parameter scales would result in high computational and communication costs while increased data heterogeneity like cross-domain would lead to suboptimal performance of the aggregated global model on individual client distributions. To bridge this gap, we propose a new method, called FedOA, to enhance the OOD generalization of FedFM under these conditions.\nSpecifically, our method employs adapter-based parameter-efficient fine-tuning methods for efficient learning, and introduces an additional personalized model with a feature distance-based regularization to ensure distribution alignment and provide OOD generalization guarantees for each client. Theoretically, we demonstrate that the conventional aggregated global model in FedFM inherently retains OOD generalization capabilities, and our proposed method enhances the personalized model's OOD generalization through regularization informed by the global model, with proven convergence under general non-convex settings.\nEmpirically, the effectiveness of the proposed method is validated on benchmark datasets across various NLP tasks."
    },
    {
        "title": "Instance-aware Generalized Multi-task Visual Grounding",
        "link_suffix": "/forum?id=66jlxeAU4G",
        "link": "https://openreview.net/forum?id=66jlxeAU4G",
        "pdf_link": "https://openreview.net/pdf?id=66jlxeAU4G",
        "keywords": "Visual Grounding, Referring Expression Comprehension, Referring Image Segmentation, Multi-Modality",
        "abstract": "The recently proposed Generalized Referring Expression Segmentation (GRES) and Comprehension (GREC) tasks extend the traditional RES/REC paradigm by incorporating multi-target and non-target scenarios. However, the existing approaches focus on these tasks individually, leaving the unified generalized multi-task visual grounding unexplored. Moreover, current GRES methods are limited to global segmentation, lacking fine-grained instance-level awareness. To address these gaps, this paper introduces a novel $\\textbf{I}$nstance-aware $\\textbf{G}$eneralized multi-task $\\textbf{V}$isual $\\textbf{G}$rounding ($\\textbf{IGVG}$) framework. IGVG is the first to integrate GREC and GRES, establishing a consistent correspondence between detection and segmentation via query guidance. Additionally, IGVG introduces instance-level awareness, enabling precise and fine-grained instance recognition. Furthermore, we present a Point-guided Instance-aware Perception Head (PIPH), which employs attention-based query generation to identify coarse reference points. These points guide the correspondence between queries, objects, and instances, enhancing the directivity and interpretability of the queries.\nExperimental results on the gRefCOCO (GREC/GRES), Ref-ZOM, and R-RefCOCO/+/g benchmarks demonstrate that IGVG outperforms state-of-the-art methods."
    },
    {
        "title": "DPD-LoRA: Dynamic Prompt-Driven Low-Rank Adaptation for Improved Generalization",
        "link_suffix": "/forum?id=Dci14asFPV",
        "link": "https://openreview.net/forum?id=Dci14asFPV",
        "pdf_link": "https://openreview.net/pdf?id=Dci14asFPV",
        "keywords": "Vision-Language Models, PEFT, Prompt Learning",
        "abstract": "Fine-tuning large models presents technical challenges such as catastrophic forgetting and parameter inefficiency. Low-rank Adaptation (LoRA) and Propmt Learning can help address some of these challenges by providing more compact and flexible representations. However, Low-rank approximation is susceptible to outliers and relies on the assumption of a global low-rank structure, which can be suboptimal. Additionally, Prompt learning can overfit to specific downstream tasks, reducing its effectiveness when adapting to new tasks. In this paper, we introduce $\\textbf{Dynamic Prompt-Driven Low-Rank Adaptation (DPD-LoRA)}$, a novel framework that seamlessly integrates task-specific guidance using hierarchical prompt tokens and parameter-efficient adaptation. Unlike traditional methods, task-aware prompts in the DPD-LoRA dynamically influences low-rank updates in the model's parameters, thus enabling robust adaptation and generalization across diverse tasks and mitigating the forgetting issues. We further improve the learning capabilities of the model by breaking down the standard LoRA into multiple low-rank sub-matrices, without adding additional parameters. Further, we use an adaptive loss function to guarantee alignment with the distribution of the pre-trained model. Specifically, we introduce a self-regulated mechanism to improve stability, and a soft-gated selection mechanism to decide when to activate adaptation modules to improve performance on unseen categories. Extensive experiments on 11 benchmark datasets demonstrate that DPD-LoRA significantly outperforms state-of-the-art methods in both accuracy and generalization, offering a comprehensive solution to the challenges of fine-tuning large-scale models."
    },
    {
        "title": "Self-Supervised Feature Re-Representation via Lennard-Jones Potential Loss",
        "link_suffix": "/forum?id=BgvAzuCfHc",
        "link": "https://openreview.net/forum?id=BgvAzuCfHc",
        "pdf_link": "https://openreview.net/pdf?id=BgvAzuCfHc",
        "keywords": "Physics-Inspired Optimization, Pluggable Self-Supervised Loss, Lennard-Jones Potential",
        "abstract": "The Lennard-Jones potential, initially developed to model molecular interactions, is characterized by a repulsive force at short distances to prevent over-clustering and an attractive force at longer distances to maintain balanced proximity, resembling the equilibrium-seeking behavior of particles in natural systems.  This offers a potential pathway for more orderly entropy reduction in higher-order features.\nThis paper introduces a self-supervised approach for feature re-representation, utilizing a Lennard-Jones potential loss to constrain the gradient directions between positive and negative features in computer vision tasks.  Unlike supervised learning directly driven by downstream tasks or contrastive learning with multi-label data pairs and multi-feature extractors, the proposed loss term integrates with existing task-specific losses by directly constraining gradient directions, thereby enhancing the feature learning process.\nExtensive theoretical analysis and experimental results demonstrate that, across various domains, datasets, network architectures, and tasks, models incorporating the Lennard-Jones potential loss significantly outperform baseline models without this auxiliary loss in both accuracy and robustness.  This approach highlights the potential of physics-inspired loss functions to improve deep learning optimization."
    },
    {
        "title": "VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling",
        "link_suffix": "/forum?id=6cGKi7FqJS",
        "link": "https://openreview.net/forum?id=6cGKi7FqJS",
        "pdf_link": "https://openreview.net/pdf?id=6cGKi7FqJS",
        "keywords": "Video-to-Music Generation, Transformer",
        "abstract": "In this work, we systematically study music generation conditioned solely on the video. First, we present a large-scale dataset by collecting 360K video-music pairs, including various genres such as movie trailers, advertisements, and documentaries. Furthermore, we propose VidMuse, a simple framework for generating music aligned with video inputs. VidMuse stands out by producing high-fidelity music that is both acoustically and semantically aligned with the video. By incorporating local and global visual cues, VidMuse enables the creation of coherent music tracks that consistently match the video content through Long-Short-Term modeling. Through extensive experiments, VidMuse outperforms existing models in terms of audio quality, diversity, and audio-visual alignment."
    },
    {
        "title": "SynFER: Towards Boosting Facial Expression Recognition with Synthesis Data",
        "link_suffix": "/forum?id=H3u2vA0bdi",
        "link": "https://openreview.net/forum?id=H3u2vA0bdi",
        "pdf_link": "https://openreview.net/pdf?id=H3u2vA0bdi",
        "keywords": "Facial Experssion Recognition, Synthesis Data",
        "abstract": "Facial expression datasets remain limited in scale due to privacy concerns, the subjectivity of annotations, and the labor-intensive nature of data collection. This limitation poses a significant challenge for developing modern deep learning-based facial expression analysis models, particularly foundation models, that rely on large-scale data for optimal performance. To tackle the overarching and complex challenge, we introduce SynFER (Synthesis of Facial Expressions with Refined Control), a novel framework for synthesizing facial expression image data based on high-level textual descriptions as well as more fine-grained and precise control through facial action units.\nTo ensure the quality and reliability of the synthetic data, we propose a semantic guidance technique to steer the generation process and a pseudo-label generator to help rectify the facial expression labels for the synthetic images.\nTo demonstrate the generation fidelity and the effectiveness of the synthetic data from SynFER, we conduct extensive experiments on representation learning using both synthetic data and real-world data. Experiment results validate the efficacy of the proposed approach and the synthetic data. \nNotably, our approach achieves a 67.23% classification accuracy on AffectNet when training solely with synthetic data equivalent to the AffectNet training set size, which increases to 69.84% when scaling up to five times the original size.\nOur code will be made publicly available."
    },
    {
        "title": "The Curse of Multi-Modalities: Evaluating Hallucinations of Large Multimodal Models across Language, Visual, and Audio",
        "link_suffix": "/forum?id=VeSsiD0DP9",
        "link": "https://openreview.net/forum?id=VeSsiD0DP9",
        "pdf_link": "https://openreview.net/pdf?id=VeSsiD0DP9",
        "keywords": "Mutimodal, Large Multimodal Models, Hallucinations, Vision-Language, Audio-Language, Vision-Audio-Language",
        "abstract": "Recent advancements in large multimodal models (LMMs) have significantly enhanced performance across diverse tasks, with ongoing efforts to further integrate additional modalities such as video and audio. However, most existing LMMs remain vulnerable to hallucinations, the discrepancy between the factual multimodal input and the generated textual output, which has limited their applicability in various real-world scenarios. This paper presents the first systematic investigation of hallucinations in LMMs involving the three most common modalities: language, visual, and audio. Our study reveals two key contributors to hallucinations: overreliance on unimodal priors and spurious inter-modality correlations. To address these challenges, we introduce the benchmark \\textit{The Curse of Multi-Modalities} (\\textbf{CMM}), which comprehensively evaluates hallucinations in LMMs, providing a detailed analysis of their underlying issues. Our findings highlight key vulnerabilities, including imbalances in modality integration and biases from training data, underscoring the need for balanced cross-modal learning and enhanced hallucination mitigation strategies. Based on our observations and findings, we suggest potential research directions that could enhance the reliability of LMMs. We will make our code and data publicly available."
    },
    {
        "title": "Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment",
        "link_suffix": "/forum?id=D9CRb1KZQc",
        "link": "https://openreview.net/forum?id=D9CRb1KZQc",
        "pdf_link": "https://openreview.net/pdf?id=D9CRb1KZQc",
        "keywords": "diffusion model; inpainting; generative artifacts; image editing; image synthesis; artifacts refinement",
        "abstract": "Personalized image generation has emerged from the recent advancements in generative models. However, these generated personalized images often suffer from localized artifacts such as incorrect logos, reducing fidelity and fine-grained identity details of the generated results. Furthermore, there is little prior work tackling this problem. To help improve these identity details in the personalized image generation, we introduce a new task: reference-guided artifacts refinement. We present Refine-by-Align, a first-of-its-kind model that employs a diffusion-based framework to address this challenge. Our model consists of two stages: Alignment Stage and Refinement Stage, which share weights of a unified neural network model. Given a generated image, a masked artifact region, and a reference image, the alignment stage identifies and extracts the corresponding regional features in the reference, which are then  used by the refinement stage to fix the artifacts. Our model-agnostic pipeline requires no test-time tuning or optimization. It automatically enhances image fidelity and reference identity in the generated image, generalizing well to existing models on various tasks including but not limited to customization, generative compositing, view synthesis, and virtual try-on. Extensive experiments and comparisons demonstrate that our pipeline greatly pushes the boundary of fine details in the image synthesis models."
    },
    {
        "title": "FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling",
        "link_suffix": "/forum?id=TsBDfe8Ra5",
        "link": "https://openreview.net/forum?id=TsBDfe8Ra5",
        "pdf_link": "https://openreview.net/pdf?id=TsBDfe8Ra5",
        "keywords": "generative models, diffusion models, training-free",
        "abstract": "While image generation with diffusion models has achieved a great success, generating images of higher resolution than the training size remains a challenging task due to the high computational cost. Current methods typically perform the entire sampling process at full resolution and process all frequency components simultaneously, contradicting with the inherent coarse-to-fine nature of latent diffusion models and wasting computations on processing premature high-frequency details at early diffusion stages. To address this issue, we introduce an efficient $\\textbf{Fre}$quency-aware $\\textbf{Ca}$scaded $\\textbf{S}$ampling framework, $\\textbf{FreCaS}$ in short, for higher-resolution image generation. FreCaS decomposes the sampling process into cascaded stages with gradually increased resolutions, progressively expanding frequency bands and refining the corresponding details. We propose an innovative frequency-aware classifier-free guidance (FA-CFG) strategy to assign different guidance strengths for different frequency components, directing the diffusion model to add new details in the expanded frequency domain of each stage. Additionally, we fuse the cross-attention maps of previous and current stages to avoid synthesizing unfaithful layouts. Experiments demonstrate that FreCaS significantly outperforms state-of-the-art methods in image quality and generation speed. In particular, FreCaS is about 2.86$\\times$ and 6.07$\\times$ faster than ScaleCrafter and DemoFusion in generating a 2048$\\times$2048 image using a pretrained SDXL model and achieves an $\\text{FID}_b$ improvement of 11.6 and 3.7, respectively. FreCaS can be easily extended to more complex models such as SD3."
    },
    {
        "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
        "link_suffix": "/forum?id=1Iuw1jcIrf",
        "link": "https://openreview.net/forum?id=1Iuw1jcIrf",
        "pdf_link": "https://openreview.net/pdf?id=1Iuw1jcIrf",
        "keywords": "large language model, mathematical reasoning, continued pretraining",
        "abstract": "Code has been shown to be effective in enhancing the mathematical reasoning abilities of large language models due to its precision and accuracy. Previous works involving continued mathematical pretraining  often include code that utilizes math-related packages, which are primarily designed for fields such as engineering, machine learning, signal processing, or module testing, rather than being directly focused on mathematical reasoning. In this paper, we introduce a novel method for generating mathematical code accompanied with corresponding reasoning steps for continued pretraining. Our approach begins with the construction of a high-quality mathematical continued pretraining dataset by incorporating math-related web data, code using mathematical packages, math textbooks, and synthetic data. Next, we construct reasoning steps by extracting LaTeX expressions, the conditions needed for the expressions, and the results of the expressions from the previously collected dataset. Based on this extracted information, we generate corresponding code to accurately capture the mathematical reasoning process. Appending the generated code to each reasoning step results in data consisting of paired natural language reasoning steps and their corresponding code. Combining this data with the original dataset results in a 19.2B-token high-performing mathematical pretraining corpus, which we name MathCode-Pile. Training several popular base models with this corpus significantly improves their mathematical abilities, leading to the creation of the MathCoder2 family of models. All of our data processing and training code is open-sourced, ensuring full transparency and easy reproducibility of the entire data collection and training pipeline."
    },
    {
        "title": "Data Extrapolation for Text-to-image Generation on Small Datasets",
        "link_suffix": "/forum?id=TJHB4ySVZM",
        "link": "https://openreview.net/forum?id=TJHB4ySVZM",
        "pdf_link": "https://openreview.net/pdf?id=TJHB4ySVZM",
        "keywords": "Diffuison, Text-to-image\uff0cData augmentation",
        "abstract": "Text-to-image generation requires large amount of training data to synthesizing high-quality images. For augmenting training data, previous methods rely on data interpolations like cropping, flipping, and mixing up, which fail to introduce new information and yield only marginal improvements. In this paper, we propose a new data augmentation method for text-to-image generation using linear extrapolation. Specifically,  we apply linear extrapolation only on text feature, and new image data are retrieved from the internet by search engines. For the reliability of new text-image pairs, we design two outlier detectors to purify retrieved images. Based on extrapolation, we construct training samples dozens of times larger than the original dataset, resulting in a significant improvement in text-to-image performance. Moreover, we propose a NULL-guidance to refine score estimation, and apply recurrent affine transformation to fuse text information.  Our model achieves FID scores of 7.91, 9.52 and 5.00 on the CUB, Oxford and COCO datasets. The code and data will be available on GitHub."
    },
    {
        "title": "FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction and Rendering",
        "link_suffix": "/forum?id=kNSem64csJ",
        "link": "https://openreview.net/forum?id=kNSem64csJ",
        "pdf_link": "https://openreview.net/pdf?id=kNSem64csJ",
        "keywords": "3D gaussian splatting, flexible rendering, autonomous driving, simulation",
        "abstract": "Driving scene reconstruction and rendering have advanced significantly using the 3D Gaussian Splatting. However, most prior research has focused on the rendering quality along a pre-recorded vehicle path and struggles to generalize to out-of-path viewpoints, which is caused by the lack of high-quality supervision in those out-of-path views.  To address this issue, we introduce an Inverse View Warping technique to create compact and high-quality images as supervision for the reconstruction of the out-of-path views, enabling high-quality rendering results for those views. For accurate and robust inverse view warping, a depth bootstrap strategy is proposed to obtain on-the-fly dense depth maps during the optimization process, overcoming the sparsity and incompleteness of LiDAR depth data. Our method achieves superior in-path and out-of-path reconstruction and rendering performance on the widely adopted Waymo Open dataset. In addition, a simulator-based benchmark is proposed to obtain the out-of-path ground truth and quantitatively evaluate the performance of out-of-path rendering, where our method outperforms previous methods by a significant margin."
    },
    {
        "title": "SwitchLoss: A Novel Optimization Scheme for Imbalanced Regression",
        "link_suffix": "/forum?id=8FJ6MOiP91",
        "link": "https://openreview.net/forum?id=8FJ6MOiP91",
        "pdf_link": "https://openreview.net/pdf?id=8FJ6MOiP91",
        "keywords": "SwitchLoss, Cost-sensitive Methods, Imbalanced Regression, High-dimensional Regression",
        "abstract": "In the realm of machine learning, conventional techniques like neural networks often encounter challenges when dealing with imbalanced data. Unfortunately, imbalanced data is a common occurrence in real-world datasets, where collection methods may fail to capture sufficient data within specific target variable ranges. Additionally, certain tasks inherently involve imbalanced data, where the occurrences of normal events significantly outweigh those of edge cases. While the problem of imbalanced data has been extensively studied in the context of classification, only a limited number of methods have been proposed for regression tasks. Furthermore, the existing methods often yield suboptimal performance when applied to high-dimensional data, and the domain of imbalanced high-dimensional regression remains relatively unexplored. In response to the identified challenge, this paper presents SwitchLoss, a novel optimization scheme for neural networks, and SwitchLossR, a variant with a restricted search space. Diverging from conventional approaches, SwitchLoss and SwitchLossR integrate variable loss functions into the traditional training process. Our assessment of these methods spans 15 regression datasets across diverse imbalanced domains, 5 synthetic high-dimensional imbalanced datasets, and two imbalanced age estimation image datasets. Findings from our investigation demonstrate that the combined utilization of SwitchLoss and SwitchLossR not only leads to a notable reduction in validation error, but also surpasses prevailing state-of-the-art techniques dedicated to imbalanced regression."
    },
    {
        "title": "Fast Tensor-Based Multi-View Clustering with Anchor Probability Transition Matrix",
        "link_suffix": "/forum?id=2IUO0Iq5Bq",
        "link": "https://openreview.net/forum?id=2IUO0Iq5Bq",
        "pdf_link": "https://openreview.net/pdf?id=2IUO0Iq5Bq",
        "keywords": "Multi-view clustering, Fast clustering",
        "abstract": "Multi-view clustering effectively integrates information from multiple data representations, yet current methods face key challenges. They often lack interpretability, obscuring how clusters are formed, and fail to fully leverage the complementary information across views, limiting clustering quality. Additionally, large-scale data introduces high computational demands, with traditional methods requiring extensive post-processing and manual tuning.To address these issues, we propose a novel multi-view clustering approach based on probability transition matrices. By selecting anchor points and constructing bipartite similarity graphs, we can capture the relationships between data points and anchors in different views and reduce computational complexity. Through probability matrices, we efficiently transfer cluster labels from anchors to samples, generating membership matrices without the need for post-processing. We further assemble these membership matrices into a tensor and apply a Schatten (p)-norm constraint to exploit complementary information across views, ensuring consistency and robustness. To prevent trivial solutions and ensure well-defined clusters, we incorporate nuclear norm-based regularization. Extensive experiments on various datasets confirm the effectiveness and efficiency of our method."
    },
    {
        "title": "Consistency Calibration: Improving Uncertainty Calibration via Consistency among Perturbed Neighbors",
        "link_suffix": "/forum?id=ivXe7J6U0k",
        "link": "https://openreview.net/forum?id=ivXe7J6U0k",
        "pdf_link": "https://openreview.net/pdf?id=ivXe7J6U0k",
        "keywords": "Uncertainty Estimation, Confidence Calibration",
        "abstract": "Calibration is crucial in deep learning applications, especially in fields like healthcare and autonomous driving, where accurate confidence estimates are vital for decision-making. However, deep neural networks often suffer from miscalibration, with reliability diagrams and Expected Calibration Error (ECE) being the only standard perspective for evaluating calibration performance. In this paper, we introduce the concept of consistency as an alternative perspective on model calibration, inspired by uncertainty estimation literature in large language models (LLMs). We highlight its advantages over the traditional reliability-based view. Building on this concept, we propose a post-hoc calibration method called Consistency Calibration (CC), which adjusts confidence based on the model's consistency across perturbed inputs. CC is particularly effective in locally uncertainty estimation, as it requires no additional data samples or label information, instead generating input perturbations directly from the source data. Moreover, we show that performing perturbations at the logit level significantly improves computational efficiency. We validate the effectiveness of CC through extensive comparisons with various post-hoc and training-time calibration methods, demonstrating state-of-the-art performance on standard datasets such as CIFAR-10, CIFAR-100, and ImageNet, as well as on long-tailed datasets like ImageNet-LT."
    },
    {
        "title": "Point-Calibrated Spectral Neural Operators",
        "link_suffix": "/forum?id=HbqYdvL1mB",
        "link": "https://openreview.net/forum?id=HbqYdvL1mB",
        "pdf_link": "https://openreview.net/pdf?id=HbqYdvL1mB",
        "keywords": "Operator Learning, PDE",
        "abstract": "Two typical neural models have been extensively studied for operator learning, learning in spatial space via attention mechanism or learning in spectral space via spectral analysis technique such as Fourier Transform. Spatial learning enables point-level flexibility but lacks global continuity constraint, while spectral learning enforces spectral continuity prior but lacks point-wise adaptivity. This work innovatively combines the continuity prior and the point-level flexibility, with the introduced Point-Calibrated Spectral Transform. It achieves this by calibrating the preset spectral eigenfunctions with the predicted point-wise frequency preference via neural gate mechanism. Beyond this, we introduce Point-Calibrated Spectral Neural Operators, which learn operator mappings by approximating functions with the point-level adaptive spectral basis, thereby not only preserving the benefits of spectral prior but also boasting the superior adaptability comparable to the attention mechanis. Comprehensive experiments demonstrate its consistent performance enhancement in extensive PDE solving scenarios."
    },
    {
        "title": "VideoAlchemy: Open-set Personalization in Video Generation",
        "link_suffix": "/forum?id=popKM1zAYa",
        "link": "https://openreview.net/forum?id=popKM1zAYa",
        "pdf_link": "https://openreview.net/pdf?id=popKM1zAYa",
        "keywords": "generative models, video generation, content personalization, content customization",
        "abstract": "Video personalization methods allow us to synthesize videos with specific concepts such as people, pets, and places. However, existing methods often focus on limited domains, require time-consuming optimization per subject, or support only a single subject. We present $VideoAlchemy~-$ a video model equipped with built-in multi-subject, open-set personalization capabilities for both foreground objects and backgrounds, eliminating the need for time-consuming test-time optimization. Our model is built on a new Diffusion Transformer module that fuses each reference image conditioning and its corresponding subject-level text prompt with cross-attention layers. Developing such a large model presents two main challenges: $dataset$ and $evaluation$. First, as paired datasets of reference images and videos are extremely hard to collect, we opt to sample video frames as reference images and synthesize entire videos. This approach, however, introduces data biases issue, where models can easily denoise training videos but fail to generalize to new contexts during inference. To mitigate these issue, we carefully design a new automatic data construction pipeline with extensive image augmentation and sampling techniques. Second, evaluating open-set video personalization is a challenge in itself. To address this, we introduce a new personalization benchmark with evaluation protocols focusing on accurate subject fidelity assessment and accommodating different types of personalization conditioning. Finally, our extensive experiments show that our method significantly outperforms existing personalization methods, regarding quantitative and qualitative evaluations."
    },
    {
        "title": "Transformers Learn Low Sensitivity Functions: Investigations and Implications",
        "link_suffix": "/forum?id=4ikjWBs3tE",
        "link": "https://openreview.net/forum?id=4ikjWBs3tE",
        "pdf_link": "https://openreview.net/pdf?id=4ikjWBs3tE",
        "keywords": "transformers, sensitivity, grokking",
        "abstract": "Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of their inductive biases and how those biases differ from other neural network architectures remains elusive. In this work, we identify the sensitivity of the model to token-wise random perturbations in the input as a unified metric which explains the inductive bias of transformers across different data modalities and distinguishes them from other architectures. We show that transformers have lower sensitivity than MLPs,  CNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show that this low-sensitivity bias has important implications: i) lower sensitivity correlates with improved robustness; it can also be used as an efficient intervention to further improve the robustness of transformers; ii) it corresponds to flatter minima in the loss landscape; and iii) it can serve as a progress measure for grokking. We support these findings with theoretical results showing (weak) spectral bias of transformers in the NTK regime, and improved robustness due to the lower sensitivity."
    },
    {
        "title": "TLDR: Token-Level Detective Reward Model for Large Vision Language Models",
        "link_suffix": "/forum?id=Zy2XgaGpDw",
        "link": "https://openreview.net/forum?id=Zy2XgaGpDw",
        "pdf_link": "https://openreview.net/pdf?id=Zy2XgaGpDw",
        "keywords": "vision language model, multimodal, reward model",
        "abstract": "Although reward models have been successful in improving multimodal large language models, the reward models themselves remain brutal and contain minimal information. Notably, existing reward models only mimic human annotations by assigning only one feedback to any text, no matter how long the text is. In the realm of multimodal language models, where models are required to process both images and texts, a naive reward model may learn implicit biases toward texts and become less grounded in images. In this paper, we propose aToken-LevelDetectiveReward Model (TLDR) to provide fine-grained annotations to each text token. We first introduce a perturbation-based model to generate synthetic hard negatives for training TLDR models. Then we show the rich usefulness of TLDR models both in assisting off-the-shelf models to self-correct their generations, and in serving as a hallucination evaluation tool. Finally, we show that TLDR models can significantly speed up human annotation to acquire a broader range of high-quality vision language data."
    },
    {
        "title": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference",
        "link_suffix": "/forum?id=OfjIlbelrT",
        "link": "https://openreview.net/forum?id=OfjIlbelrT",
        "pdf_link": "https://openreview.net/pdf?id=OfjIlbelrT",
        "keywords": "Large Language Models (LLMs), LLM inference, Long-context LLMs, Sparse Attention Mechanism",
        "abstract": "Large language models (LLMs) encounter computational challenges during long-sequence inference, especially in the attention pre-filling phase, where the complexity grows quadratically with the prompt length. Previous efforts to mitigate these challenges have relied on fixed sparse attention patterns or identifying sparse attention patterns based on limited cases. However, these methods lacked the flexibility to efficiently adapt to varying input demands. In this paper, we introduce FlexPrefill, a Flexible sparse Pre-filling mechanism that dynamically adjusts sparse attention patterns and computational budget in real-time to meet the specific requirements of each input and attention head. The flexibility of our method is demonstrated through two key innovations: 1) Query-Aware Sparse Pattern Determination: By measuring Jensen-Shannon divergence, this component adaptively switches between query-specific diverse attention patterns and predefined attention patterns. 2) Cumulative-Attention Based Index Selection: This component dynamically selects query-key indexes to be computed based on different attention patterns, ensuring the sum of attention scores meets a predefined threshold.\nFlexPrefill adaptively optimizes the sparse pattern and sparse ratio of each attention head based on the prompt, enhancing efficiency in long-sequence inference tasks. Experimental results show significant improvements in both speed and accuracy over prior methods, providing a more flexible and efficient solution for LLM inference."
    },
    {
        "title": "Training-Free Activation Sparsity in Large Language Models",
        "link_suffix": "/forum?id=dGVZwyq5tV",
        "link": "https://openreview.net/forum?id=dGVZwyq5tV",
        "pdf_link": "https://openreview.net/pdf?id=dGVZwyq5tV",
        "keywords": "Large Language Models, Activation Sparsity, Efficiency",
        "abstract": "Activation sparsity can enable practical inference speedups in large language models (LLMs) by reducing the compute and memory-movement required for  matrix multiplications during the forward pass. \nHowever, existing methods face limitations that inhibit widespread adoption. Some approaches are tailored towards older models with ReLU-based sparsity, while others require extensive continued pre-training on up to hundreds of billions of tokens. \nThis paper describes TEAL (Training-FreeActivation Sparsity inLLMs), a simple training-free method that applies magnitude-based activation sparsity to hidden states throughout the entire model. TEAL achieves 40-50% model-wide sparsity with minimal performance degradation across Llama-2, Llama-3, and Mistral families, with sizes varying from 7B to 70B. We improve existing sparse kernels and demonstrate wall-clock decoding speed-ups of up to 1.53\u00d7 and 1.8\u00d7 at 40% and 50% model-wide sparsity. TEAL is compatible with weight quantization, enabling further efficiency gains."
    },
    {
        "title": "Beautifying Diffusion Models: Learning Context-Aware Filters for Robust Dense Prediction on Test-Time Corrupted Images",
        "link_suffix": "/forum?id=dnp63LgTgc",
        "link": "https://openreview.net/forum?id=dnp63LgTgc",
        "pdf_link": "https://openreview.net/pdf?id=dnp63LgTgc",
        "keywords": "Diffusion, Test-time adaptation, Dense Prediction, Frequency-Aware Modeling",
        "abstract": "Diffusion models have enabled input-based domain adaptation to unseen test-time corruption for the classification problem.\nNevertheless, while dense prediction tasks share similar robustness issues with image-level classification, previous input adaptation work may fail to preserve the semantic information necessary for robust pixel-level prediction. \nTo address the issue, we propose a novel diffusion-driven strategy that translates the corrupted inputs to the source domain (\\ie, the training data domain), while also preserving the semantic information (\\ie high-frequency shape information and low-frequency color information).\nWe first studied how to leverage frequency filtering to guide the diffusion generation process and analyze the influence of different filters.\nFrom our experiments, we observed that utilizing both high and low spatial frequency information during diffusion driven denoising can substantially improve the adaptation performance of dense classification.\nThis observation motivates us to develop a novel framework, \\ie a predictive frequency filtering-driven diffusion (FDD) adaptation, where we predict the filters from the corrupted test-time inputs and use them to guide the diffusion process.\nWe design a Y-like frequency prediction network to predict context-aware low-pass and high-pass filters.\nTo train this network, we propose a novel data augmentation method, FrequencyMix, to generate pairs of clean and corrupted images.\nWe validate our method via extensive experiments on two semantic segmentation datasets and two depth estimation datasets.\nAgainst a broad range of common corruptions, we demonstrate that our method is competitive with state of the art work."
    },
    {
        "title": "PRIME: Protect Your Videos From Malicious Editing",
        "link_suffix": "/forum?id=YMvRZCA8Zo",
        "link": "https://openreview.net/forum?id=YMvRZCA8Zo",
        "pdf_link": "https://openreview.net/pdf?id=YMvRZCA8Zo",
        "keywords": "Diffusion Model, Video Protection, Video Editing",
        "abstract": "Over the years, video generation has experienced significant advancement. A variety of open-source models emerge, making it surprisingly easy to manipulate and edit videos with just a few simple prompts. While these cutting-edge technologies have gained huge popularity, they have also given rise to concerns regarding the privacy and portrait rights of individuals: malicious users can exploit these tools for deceptive or illegal purposes. Existing works on protecting images against generative models cannot be directly grafted to video protection, due to their efficiency and effectiveness limitations. Motivated by this, we introduce PRIME, a new methodology dedicated to the protection of videos from unauthorized editing via generative models. Our key idea is to craft highly transferable and robust perturbations, which can be efficiently added to the protected videos to disrupt their editing feasibility. We perform comprehensive evaluations using both objective metrics and human studies. The results indicate that PRIME only needs 8.3% GPU hours of existing state-of-the-art methods while achieving better protection results."
    }
]