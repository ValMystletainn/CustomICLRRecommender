[
    {
        "title": "Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data",
        "link_suffix": "/forum?id=qeXcMutEZY",
        "link": "https://openreview.net/forum?id=qeXcMutEZY",
        "pdf_link": "https://openreview.net/pdf?id=qeXcMutEZY",
        "keywords": "inverse problems, diffusion, ambient diffusion, mri",
        "abstract": "We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Firstly, we extend the Ambient Diffusion framework to enable training directly from measurements corrupted in the Fourier domain. Subsequently, we train diffusion models for MRI with access only to Fourier subsampled multi-coil measurements at acceleration factors R$=2, 4, 6, 8$. Secondly, we propose $\\textit{Ambient Diffusion Posterior Sampling}$ (A-DPS), a reconstruction algorithm that leverages generative models pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling on measurements from a different forward process (e.g. image blurring). For MRI reconstruction in high acceleration regimes, we observe that A-DPS models trained on subsampled data are better suited to solving inverse problems than models trained on fully sampled data. We also test the efficacy of A-DPS on natural image datasets (CelebA, FFHQ, and AFHQ) and show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance."
    },
    {
        "title": "Test-Time Fairness and Robustness in Large Language Models",
        "link_suffix": "/forum?id=FEDnzAhIT4",
        "link": "https://openreview.net/forum?id=FEDnzAhIT4",
        "pdf_link": "https://openreview.net/pdf?id=FEDnzAhIT4",
        "keywords": "large language models, trustworthiness, fairness, robustness, causality",
        "abstract": "Frontier Large Language Models (LLMs) can be socially discriminatory or sensitive to spurious features of their inputs. Because only well-resourced corporations can train frontier LLMs, we need robust test-time strategies to control such biases. Existing solutions, which instruct the LLM to be fair or robust, rely on the model\u2019s implicit understanding of bias. Causality provides a rich formalism through which we can be explicit about our debiasing requirements. Yet, as we show, a naive application of the standard causal debiasing strategy, counterfactual data augmentation, fails under standard assumptions to debias predictions at an individual level at test time. To address this, we develop a stratified notion of debiasing called stratified invariance, which can capture a range of debiasing requirements from population level to individual level through an additional measurement that stratifies the predictions. We present a complete observational test for stratified invariance. Finally, we introduce a data augmentation strategy that guarantees stratified invariance at test time under suitable assumptions, together with a prompting strategy that encourages stratified invariance in LLMs. We show that our prompting strategy, unlike implicit instructions, consistently reduces the bias of frontier LLMs across a suite of synthetic and real-world benchmarks without requiring additional data, finetuning or pre-training."
    },
    {
        "title": "Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning",
        "link_suffix": "/forum?id=yMHe9SRvxk",
        "link": "https://openreview.net/forum?id=yMHe9SRvxk",
        "pdf_link": "https://openreview.net/pdf?id=yMHe9SRvxk",
        "keywords": "Online RLHF, Diffusion Model Finetuning",
        "abstract": "Controllable generation through Stable Diffusion (SD) fine-tuning aims to improve fidelity, safety, and alignment with human guidance. Existing reinforcement learning from human feedback methods usually rely on predefined heuristic reward functions or pretrained reward models built on large-scale datasets, limiting their applicability to scenarios where collecting such data is costly or difficult. To effectively and efficiently utilize human feedback, we develop a framework, HERO, which leverages online human feedback collected on the fly during model learning. Specifically, HERO features two key mechanisms: (1) Feedback-Aligned Representation Learning, an online training method that captures human feedback and provides informative learning signals for fine-tuning, and (2) Feedback-Guided Image Generation, which involves generating images from SD's refined initialization samples, enabling faster convergence towards the evaluator's intent. We demonstrate that HERO is 4x more efficient in online feedback for body part anomaly correction compared to the best existing method. Additionally, experiments show that HERO can effectively handle tasks like reasoning, counting, personalization, and reducing NSFW content with only 0.5K online feedback."
    },
    {
        "title": "Pixelated Instructions: Can Multimodal Large Language Models Follow Printed Instructions in Images?",
        "link_suffix": "/forum?id=DiRJUdmZoK",
        "link": "https://openreview.net/forum?id=DiRJUdmZoK",
        "pdf_link": "https://openreview.net/pdf?id=DiRJUdmZoK",
        "keywords": "multimodal large langauage models, instruction following",
        "abstract": "Recent multimodal large language models (MLLMs) have shown promising instruction following capabilities on vision-language tasks. In this work, we introduce VISUAL MODALITY INSTRUCTION (VIM), and investigate how well multimodal models can understand textual instructions provided in pixels, despite not being explicitly trained on such data during pretraining or fine-tuning. We adapt VIM to eight benchmarks, including OKVQA, MM-Vet, MathVista, MMMU, and probe diverse MLLMs in both the text-modality instruction (TEM) setting and VIM setting. Notably, we observe a significant performance disparity between the original TEM and VIM settings for open-source MLLMs, indicating that open-source MLLMs face greater challenges when text instruction is presented solely in image form. To address this issue, we train V-MLLM, a generalizable model that is capable to conduct robust instruction following in both text-modality and visual-modality instructions."
    },
    {
        "title": "Interpretable Language Modeling via Induction-head Ngram Models",
        "link_suffix": "/forum?id=Zq8wylMZ8A",
        "link": "https://openreview.net/forum?id=Zq8wylMZ8A",
        "pdf_link": "https://openreview.net/pdf?id=Zq8wylMZ8A",
        "keywords": "interpretability, ngram, language modeling, fmri, neuroscience",
        "abstract": "Recent large language models (LLMs) have excelled across a wide range of tasks, but their use in high-stakes and compute-limited settings has intensified the demand for interpretability and efficiency. We address this need by proposing Induction-head ngram models (Induction-Gram), a method that builds an efficient, interpretable LM by bolstering modern ngram models with a hand-engineered ``induction head''. This induction head uses a custom neural similarity metric to efficiently search the model's input context for potential next-word completions. This process enables Induction-Gram to provide ngram-level grounding for each generated token. Moreover, experiments show that this simple method significantly improves next-word prediction over baseline interpretable models (up to 26%p) and can be used to speed up LLM inference for large models through speculative decoding. We further study Induction-Gram in a natural-language neuroscience setting, where the goal is to predict the next fMRI response in a sequence. It again provides a significant improvement over interpretable models (20% relative increase in the correlation of predicted fMRI responses), potentially enabling deeper scientific investigation of language selectivity in the brain."
    },
    {
        "title": "Rational Decision-Making Agent with Learning Internal Utility Judgment",
        "link_suffix": "/forum?id=GEBkyKZOc4",
        "link": "https://openreview.net/forum?id=GEBkyKZOc4",
        "pdf_link": "https://openreview.net/pdf?id=GEBkyKZOc4",
        "keywords": "Decision Making, Autonomous Agent, Large Lanugage Model, Elo Rating",
        "abstract": "With remarkable advancements, large language models (LLMs) have attracted significant efforts to develop LLM-based agents capable of executing intricate multi-step decision-making tasks. Existing approaches predominantly build upon the external performance measure to guide the decision-making process but the reliance on the external performance measure as prior is problematic in real-world scenarios, where such prior may be unavailable, flawed, or even erroneous. For genuine autonomous decision-making for LLM-based agents, it is imperative to develop rationality from their posterior experiences to judge the utility of each decision independently. In this work, we propose RaDAgent (Rational Decision-Making Agent), which fosters the development of its rationality through an iterative framework involving Experience Exploration and Utility Learning. Within this framework, Elo-based Utility Learning is devised to assign Elo scores to individual decision steps to judge their utilities via pairwise comparisons. Consequently, these Elo scores guide the decision-making process to derive optimal outcomes. Experimental results on the Game of 24, WebShop, ToolBench and RestBench datasets demonstrate RaDAgent\u2019s superiority over baselines, achieving about 7.8% improvement on average. Besides, RaDAgent also can reduce costs (ChatGPT API calls), highlighting its effectiveness and efficiency."
    },
    {
        "title": "Pareto Prompt Optimization",
        "link_suffix": "/forum?id=HGCk5aaSvE",
        "link": "https://openreview.net/forum?id=HGCk5aaSvE",
        "pdf_link": "https://openreview.net/pdf?id=HGCk5aaSvE",
        "keywords": "Large Language Model, Prompt Optimization, Multiobjective Optimization, Reinforcement Learning, DPO",
        "abstract": "Natural language prompt optimization, or prompt engineering, has emerged as a powerful technique to unlock the potential of Large Language Models (LLMs) for various tasks. While existing methods primarily focus on maximizing a single task-specific performance metric for LLM outputs, real-world applications often require considering trade-offs between multiple objectives. In this work, we address this limitation by proposing an effective technique for multi-objective prompt optimization for LLMs. Specifically, we proposeParetoPrompt, a reinforcement learning~(RL) method that leverages dominance relationships between prompts to derive a policy model for prompts optimization using preference-based loss functions. By leveraging multi-objective dominance relationships, ParetoPrompt enables efficient exploration of the entire Pareto front without the need for a predefined scalarization of multiple objectives. Our experimental results show that ParetoPrompt consistently outperforms existing algorithms that use specific objective values. ParetoPrompt also yields robust performances when the objective metrics differ between training and testing."
    },
    {
        "title": "Transformers versus LSTMs for electronic trading",
        "link_suffix": "/forum?id=2L1OxhQCwS",
        "link": "https://openreview.net/forum?id=2L1OxhQCwS",
        "pdf_link": "https://openreview.net/pdf?id=2L1OxhQCwS",
        "keywords": "transformer, LSTM, electronic trading",
        "abstract": "The rapid advancement of artificial intelligence has seen widespread application of long short-term memory (LSTM), a type of recurrent neural network (RNN), in time series forecasting. Despite the success of Transformers in natural language processing (NLP), which prompted interest in their efficacy for time series prediction, their application in financial time series forecasting is less explored compared to the dominant LSTM models. This study investigates whether Transformer-based models can outperform LSTMs in financial time series forecasting. It involves a comparative analysis of various LSTM-based and Transformer-based models on multiple financial prediction tasks using high-frequency limit order book data. A novel LSTM-based model named DLSTM is introduced alongside a newly designed Transformer-based model tailored for financial predictions. The findings indicate that Transformer-based models exhibit only a marginal advantage in predicting absolute price sequences, whereas LSTM-based models demonstrate superior and more consistent performance in predicting differential sequences such as price differences and movements."
    },
    {
        "title": "BigDocs: An Open and Permissively-Licensed Dataset for Training Multimodal Models on Document and Code Tasks",
        "link_suffix": "/forum?id=b1ivBPLb1n",
        "link": "https://openreview.net/forum?id=b1ivBPLb1n",
        "pdf_link": "https://openreview.net/pdf?id=b1ivBPLb1n",
        "keywords": "datasets, vision language models, multimodal, large language models",
        "abstract": "Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to relevant training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure that our data is high quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench,, a benchmark suite with 10 novel tasks where we carefully create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench, improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations revealed that participants preferred the outputs from models trained with BigDocs over those from GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning."
    },
    {
        "title": "AutoScale: Combining Multi-Task Optimization with Linear Scalarization",
        "link_suffix": "/forum?id=VvxuD3cdJx",
        "link": "https://openreview.net/forum?id=VvxuD3cdJx",
        "pdf_link": "https://openreview.net/pdf?id=VvxuD3cdJx",
        "keywords": "Multi-task Learning, Autonomous Driving",
        "abstract": "Multi-task learning is favored due to its efficiency and potential transfer learning achieved by sharing networks across tasks. While a series of multi-task optimization algorithms (MTOs) have been proposed to solve MTL optimization challenges and enhance performance, recent research claims that simple linear scalarization, which sums per-task loss with a carefully searched weight set, is sufficient, casting doubt on the added value of more complex MTO algorithms. In this paper, we provide a novel perspective that linear scalarization and MTOs are closely related and can be combined to yield high performance and efficiency. We show, for the first time, that a well-performing linear scalarization exhibits specific characteristics of certain optimization metrics proposed by MTOs, such as high task gradient magnitude similarity and low condition number, via an extensive empirical study. We then propose AutoScale, an efficient pipeline that leverages these influential metrics to guide the search for optimal linear scalarization weights. AutoScale shows superior performance than prior MTOs and performs close to the searched weight performance consistently across different datasets."
    },
    {
        "title": "Linear Mode Connectivity in Differentiable Tree Ensembles",
        "link_suffix": "/forum?id=UqYNPyotxL",
        "link": "https://openreview.net/forum?id=UqYNPyotxL",
        "pdf_link": "https://openreview.net/pdf?id=UqYNPyotxL",
        "keywords": "Linear Mode Connectivity, Soft Tree",
        "abstract": "Linear Mode Connectivity (LMC) refers to the phenomenon that performance remains consistent for linearly interpolated models in the parameter space. For independently optimized model pairs from different random initializations, achieving LMC is considered crucial for understanding the stable success of the non-convex optimization in modern machine learning models and for facilitating practical parameter-based operations such as model merging. While LMC has been achieved for neural networks by considering the permutation invariance of neurons in each hidden layer, its attainment for other models remains an open question. In this paper, we first achieve LMC for soft tree ensembles, which are tree-based differentiable models extensively used in practice. We show the necessity of incorporating two invariances: subtree flip invariance and splitting order invariance, which do not exist in neural networks but are inherent to tree architectures, in addition to permutation invariance of trees. Moreover, we demonstrate that it is even possible to exclude such additional invariances while keeping LMC by designing decision list-based tree architectures, where such invariances do not exist by definition. Our findings indicate the significance of accounting for architecture-specific invariances in achieving LMC."
    },
    {
        "title": "3D-Properties: Identifying Challenges in DPO and Charting a Path Forward",
        "link_suffix": "/forum?id=9Hxdixed7p",
        "link": "https://openreview.net/forum?id=9Hxdixed7p",
        "pdf_link": "https://openreview.net/pdf?id=9Hxdixed7p",
        "keywords": "LLM, DPO, RLHF",
        "abstract": "Aligning large language models (LLMs) with human preferences has recently garnered significant attention, with Proximal Policy Optimization (PPO) being a canonical yet computationally expensive method, and Direct Preference Optimization (DPO) offering a simpler and more efficient alternative. While prior studies have explored the trade-offs between PPO and DPO, DPO remains underutilized in state-of-the-art production-level LLMs, suggesting potential limitations. In this work, we revisit DPO with a comprehensive analysis of its theoretical foundations and empirical performance, aiming to chart a path forward and bridge this gap. We identify three critical properties\u2014termed the \\textbf{3D}-properties\u2014that arise from DPO\u2019s learning process: \\textbf{D}rastic drop in the likelihood of rejected responses, \\textbf{D}egradation into response suppression, and \\textbf{D}ispersion effect on unseen responses. We show that these phenomena stem from the inherent features of DPO's optimization objective, where the interaction between the gradients of chosen and rejected responses causes instability. These findings are supported by experiments on both a carefully constructed toy model and practical LLM tasks, including mathematical problem-solving and instruction following. Our work offers new insights, connecting these observations to related research while providing a theoretical explanation for the underlying mechanisms. To address the challenges posed by the \\textbf{3D}-properties, we propose straightforward regularization techniques that enhance training stability and final performance. Additionally, we investigate how the distribution of paired preference data affects DPO\u2019s efficacy, contributing to a broader understanding of how alignment models handle out-of-domain (OOD) data. We believe our findings will help guide future research toward closing the gap between reward-model-free preference learning and reward-model-based approaches."
    },
    {
        "title": "Learning Universal Features for Generalizable Image Forgery Localization",
        "link_suffix": "/forum?id=OKzvovmUbh",
        "link": "https://openreview.net/forum?id=OKzvovmUbh",
        "pdf_link": "https://openreview.net/pdf?id=OKzvovmUbh",
        "keywords": "image forgery detection, inpainting detection, forgery localization, image forgery dataset",
        "abstract": "In recent years, advanced image editing and generation methods have rapidly evolved, making detecting and locating forged image content increasingly challenging. Most existing image forgery detection methods rely on identifying the edited traces left in the image. However, because the traces of different forgeries are distinct, these methods can identify familiar forgeries included in the training data but struggle to handle unseen ones.\nIn response, we present an approach for Generalizable Image Forgery Localization (GIFL). Once trained, our model can detect both seen and unseen forgeries, providing a more practical and efficient solution to counter false information in the era of generative AI. \nOur method focuses on learning general features from the pristine content rather than traces of specific forgeries, which are relatively consistent across different types of forgeries and therefore can be used as universal features to locate unseen forgeries. Additionally, as existing image forgery datasets are still dominated by traditional hand-crafted forgeries, we construct a new dataset consisting of images edited by various popular deep generative image editing methods to further encourage research in detecting images manipulated by deep generative models. Extensive experimental results show that the proposed approach outperforms state-of-the-art methods in the detection of unseen forgeries and also demonstrates competitive results for seen forgeries."
    },
    {
        "title": "MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation",
        "link_suffix": "/forum?id=1v7SRWsYve",
        "link": "https://openreview.net/forum?id=1v7SRWsYve",
        "pdf_link": "https://openreview.net/pdf?id=1v7SRWsYve",
        "keywords": "model merging, transfer learning, multitask learning, task arithmetic, multi-objective optimization",
        "abstract": "Model merging has emerged as an effective approach to combine multiple single-task models into a multitask model. This process typically involves computing a weighted average of the model parameters without any additional training. Existing model-merging methods focus on enhancing average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during the merging process. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel and low-compute algorithm, \\textbf{Model Merging with Amortized Pareto Front (MAP)}. MAP efficiently identifies a Pareto set of scaling coefficients for merging multiple models, reflecting the trade-offs involved. It amortizes the substantial computational cost of evaluations needed to estimate the Pareto front by using quadratic approximation surrogate models derived from a pre-selected set of scaling coefficients. Experimental results on vision and natural language processing tasks demonstrate that MAP can accurately identify the Pareto front, providing practitioners with flexible solutions to balance competing task objectives. We also introduce Bayesian MAP for scenarios with a relatively low number of tasks and Nested MAP for situations with a high number of tasks, further reducing the computational cost of evaluation."
    },
    {
        "title": "Long-tailed Adversarial Training with Self-Distillation",
        "link_suffix": "/forum?id=vM94dZiqx4",
        "link": "https://openreview.net/forum?id=vM94dZiqx4",
        "pdf_link": "https://openreview.net/pdf?id=vM94dZiqx4",
        "keywords": "Adversarial Robustness, Adversarial Training, Long-Tail Distribution Learning",
        "abstract": "Adversarial training significantly enhances adversarial robustness, yet superior performance is predominantly achieved on balanced datasets.\n Addressing adversarial robustness in the context of unbalanced or long-tailed distributions is considerably more challenging, mainly due to the scarcity of tail data instances. \n Previous research on adversarial robustness within long-tailed distributions has primarily focused on combining traditional long-tailed natural training with existing adversarial robustness methods.\n In this study, we provide an in-depth analysis for the challenge that adversarial training struggles to achieve high performance on tail classes in long-tailed distributions.\n Furthermore, we propose a simple yet effective solution to advance adversarial robustness on long-tailed distributions through a novel self-distillation technique.\n Specifically, this approach leverages a balanced self-teacher model, which is trained using a balanced dataset sampled from the original long-tailed dataset.\nOur extensive experiments demonstrate state-of-the-art performance in both clean and robust accuracy for long-tailed adversarial robustness, with significant improvements in tail class performance on various datasets.\nWe improve the accuracy against PGD attacks for tail classes by 20.3, 7.1, and 3.8 percentage points on CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, while achieving the highest robust accuracy."
    },
    {
        "title": "MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection",
        "link_suffix": "/forum?id=Y6aHdDNQYD",
        "link": "https://openreview.net/forum?id=Y6aHdDNQYD",
        "pdf_link": "https://openreview.net/pdf?id=Y6aHdDNQYD",
        "keywords": "Test-Time Adaptation, 3D Object Detection",
        "abstract": "LiDAR-based 3D object detection is crucial for various applications but often experiences performance degradation in real-world deployments due to domain shifts. While most studies focus on cross-dataset shifts, such as changes in environments and object geometries, practical corruptions from sensor variations and weather conditions remain underexplored. In this work, we propose a novel online test-time adaptation framework for 3D detectors that effectively tackles these shifts, including a challenging $\\textit{cross-corruption}$ scenario where cross-dataset shifts and corruptions co-occur. By leveraging long-term knowledge from previous test batches, our approach mitigates catastrophic forgetting and adapts effectively to diverse shifts. Specifically, we propose a Model Synergy (MOS) strategy that dynamically selects historical checkpoints with diverse knowledge and assembles them to best accommodate the current test batch. This assembly is directed by our proposed Synergy Weights (SW), which perform a weighted averaging of the selected checkpoints, minimizing redundancy in the composite model. The SWs are computed by evaluating the similarity of predicted bounding boxes on the test data and the independence of features between checkpoint pairs in the model bank. To maintain an efficient and informative model bank, we discard checkpoints with the lowest average SW scores, replacing them with newly updated models. Our method was rigorously tested against existing test-time adaptation strategies across three datasets and eight types of corruptions, demonstrating superior adaptability to dynamic scenes and conditions. Notably, it achieved a 67.3% improvement in a challenging cross-corruption scenario, offering a more comprehensive benchmark for adaptation. The source code is provided in the supplementary materials."
    },
    {
        "title": "Pok\u00e9LLMon: A Grounding and Reasoning Benchmark for Large Language Models in Pok\u00e9mon Battles",
        "link_suffix": "/forum?id=nGBJpY4rIJ",
        "link": "https://openreview.net/forum?id=nGBJpY4rIJ",
        "pdf_link": "https://openreview.net/pdf?id=nGBJpY4rIJ",
        "keywords": "Large language models, Pok\u00e9mon Battles, Reasoning, Grounding, Interactive environment, Benchmark",
        "abstract": "Developing grounding techniques for LLMs poses two requirements for interactive environments, i.e., (i) the presence of rich knowledge beyond the scope of existing LLMs and (ii) the complexity of tasks that require strategic reasoning. Existing environments fail to meet both requirements due to their simplicity or reliance on commonsense knowledge already encoded in LLMs for interaction. In this paper, we present Pok\u00e9LLMon, a new benchmark enriched with fictional game knowledge and characterized by the intense, dynamic, and adversarial gameplay of Pok\u00e9mon battles, setting new challenges for the development of grounding and reasoning techniques in interactive environments. Empirical evaluations demonstrate that existing LLMs lack game knowledge and struggle in Pok\u00e9mon battles. We investigate grounding techniques that leverage game knowledge and self-play experience, and provide a thorough analysis of reasoning methods from a new perspective of action consistency. Additionally, we introduce higher-level reasoning challenges when playing against human players. The implementation of our benchmark is anonymously released at:https://anonymous.4open.science/r/PokeLLMon."
    },
    {
        "title": "Assessing the Interpretability of Programmatic Policies using Large Language Models",
        "link_suffix": "/forum?id=tvWD9YueN4",
        "link": "https://openreview.net/forum?id=tvWD9YueN4",
        "pdf_link": "https://openreview.net/pdf?id=tvWD9YueN4",
        "keywords": "Programmatic Policies, Interpretability, Program Synthesis",
        "abstract": "Programmatic representations of policies for solving sequential decision-making problems often carry the promise of interpretability. However, previous work on programmatic policies has only presented anecdotal evidence of policy interpretability. The lack of systematic evaluations of policy interpretability can be attributed to user studies being time-consuming and costly. In this paper, we introduce the LLM-based INTerpretability (LINT) score, a simple and cost-effective metric that uses large-language models (LLMs) to assess the interpretability of programmatic policies. To compute the LINT score of a policy, an LLM generates a natural language description of the policy's behavior. This description is then passed to a second LLM, which attempts to reconstruct the policy from the natural language description. The LINT score measures the behavioral similarity between the original and reconstructed policies. We hypothesized that the LINT score of programmatic policies correlates with their actual interpretability, and evaluated this hypothesis in the domains of MicroRTS and Karel the Robot. Our evaluation relied on a technique from the static obfuscation literature and a user study, where people with various levels of programming proficiency evaluated the interpretability of the programmatic policies. The results of our experiments support our hypothesis. Specifically, the LINT score decreases as the level of obfuscation of the policies increases. The user study showed that LINT can correctly distinguish the ``degree of interpretability'' of programmatic policies generated by the existing algorithms. Our results suggest that LINT can be a helpful tool for advancing the research on interpretability of programmatic policies."
    },
    {
        "title": "JudgeBench: A Benchmark for Evaluating LLM-Based Judges",
        "link_suffix": "/forum?id=G0dksFayVq",
        "link": "https://openreview.net/forum?id=G0dksFayVq",
        "pdf_link": "https://openreview.net/pdf?id=G0dksFayVq",
        "keywords": "benchmark, llm-based judges, llm-as-a-judge, reward models",
        "abstract": "LLM-based judges have emerged as a scalable alternative to human evaluation and are increasingly used to assess, compare, and improve models. However, the reliability of LLM-based judges themselves is rarely scrutinized. As LLMs become more advanced, their responses grow more sophisticated, requiring stronger judges to evaluate them. Existing benchmarks primarily focus on a judge\u2019s alignment with human preferences, but often fail to account for more challenging tasks where crowdsourced human preference is a poor indicator of factual and logical correctness. To address this, we propose a novel evaluation framework to objectively evaluate LLM-based judges. Based on this framework, we propose JudgeBench, a benchmark for evaluating LLM-based judges on challenging response pairs spanning knowledge, reasoning, math, and coding. JudgeBench leverages a novel pipeline for converting existing difficult datasets into challenging response pairs with preference labels reflecting objective correctness. Our comprehensive evaluation on a collection of prompted judges, fine-tuned judges, multi-agent judges, and reward models shows that JudgeBench poses a significantly greater challenge than previous benchmarks, with many strong models (e.g. GPT-4o) performing just slightly better than random guessing. Overall, JudgeBench offers a reliable platform for assessing increasingly advanced LLM-based judges. Data and code are available at \\url{https://anonymous.4open.science/r/JudgeBench-ICLR2025/}."
    },
    {
        "title": "Synthetic continued pretraining",
        "link_suffix": "/forum?id=07yvxWDSla",
        "link": "https://openreview.net/forum?id=07yvxWDSla",
        "pdf_link": "https://openreview.net/pdf?id=07yvxWDSla",
        "keywords": "large language model, synthetic data, continued pretraining",
        "abstract": "Pretraining on large-scale, unstructured internet text enables language models to acquire a significant amount of world knowledge.\nHowever, this knowledge acquisition is data-inefficient---to learn a fact, models must be trained on hundreds to thousands of diverse representations of it.\nThis poses a challenge when adapting a pretrained model to a small corpus of domain-specific documents, where each fact may appear rarely or only once.\nWe propose to bridge this gap with synthetic continued pretraining: using the small domain-specific corpus to synthesize a large corpus more amenable to learning, and then performing continued pretraining on the synthesized corpus.\nWe instantiate this proposal with EntiGraph, a synthetic data augmentation algorithm that extracts salient entities from the source corpus and then generates diverse text by drawing connections between those entities.\nSynthetic continued pretraining with EntiGraph enables a language model to answer questions and follow generic instructions related to the source documents without access to them.\nIf the source documents are instead available at inference time, we show that the knowledge acquired through our approach compounds with retrieval-augmented generation.\nTo better understand these results, we build a simple mathematical model of EntiGraph, and show how synthetic data augmentation can \"rearrange\" knowledge to enable more data-efficient learning."
    },
    {
        "title": "Tensor Attention Training: Provably Efficient Learning of Higher-order Transformers",
        "link_suffix": "/forum?id=HtS2NUcqtD",
        "link": "https://openreview.net/forum?id=HtS2NUcqtD",
        "pdf_link": "https://openreview.net/pdf?id=HtS2NUcqtD",
        "keywords": "High Order Attention, Efficient Gradient Computation, Tensor Operation",
        "abstract": "Tensor Attention, a multi-view attention that is able to capture high-order correlations among multiple modalities, can overcome the representational limitations of classical matrix attention. However, the $O(n^3)$ time complexity of tensor attention poses a significant obstacle to its utilization in transformers, where $n$ is the input sequence length. In this work, we prove that the backward gradient of tensor attention training can be computed in almost linear time $n^{1+o(1)}$, the same complexity as its forward computation under the bounded entries assumption. We provide a closed-form solution for the gradient and propose a fast computation method utilizing polynomial approximation methods and tensor algebraic techniques. Furthermore, we prove the necessity and tightness of our assumption through hardness analysis, showing that slightly weakening it renders the gradient problem unsolvable in truly subcubic time. Our theoretical results establish the feasibility of efficient higher-order transformer training and may facilitate practical applications of tensor attention architectures."
    },
    {
        "title": "Hierarchical Graph Learners for Cardinality Estimation",
        "link_suffix": "/forum?id=0zmHFyZwkA",
        "link": "https://openreview.net/forum?id=0zmHFyZwkA",
        "pdf_link": "https://openreview.net/pdf?id=0zmHFyZwkA",
        "keywords": "Cardinality Estimation, Many small models, Graph Hash, Group-by-template, Fast Learning",
        "abstract": "Cardinality estimation -- the task of estimating the number of records that a database query will return -- is core to performance optimization in  modern database systems. Traditional optimizers used in commercial systems use heuristics that can lead to large errors. Recently, neural network based models have been proposed that outperform the traditional optimizers. These neural network based estimators perform well if they are trained with large amounts of query samples. In this work, we observe that data warehouse workloads contain highly repetitive queries, and propose a hierarchy of localized on-line models to target these repetitive queries. At the core, these models use an extension of Merkle-Trees to hash query plans which are directed acyclic graphs. The hash values can divisively partition a large set of graphs into many sets, each containing few (whole) graphs. We learn an online model for each partition of the hierarchy. No upfront training is needed; on-line models learn as the queries are executed. When a new query comes, we check the partitions it is hashed to and if no such local model was sufficiently confident along the hierarchy, we fall-back onto a default model at the root.  Our experimental results show that not only our hierarchical on-line models perform better than the traditional optimizers, they also outperform neural models, with robust errors rates at the tail."
    },
    {
        "title": "Combating Dual Noise Effect in Spatial-temporal Forecasting via Information Bottleneck Principle",
        "link_suffix": "/forum?id=Mg1stYVYTl",
        "link": "https://openreview.net/forum?id=Mg1stYVYTl",
        "pdf_link": "https://openreview.net/pdf?id=Mg1stYVYTl",
        "keywords": "Robust spatial-temporal forecasting, Multi-Layer Perceptron, Information bottleneck",
        "abstract": "Spatial-temporal forecasting plays a pivotal role in urban planning and computing. Although Spatial-Temporal Graph Neural Networks (STGNNs) excel in modeling spatial-temporal dynamics, they often suffer from relatively poor computational efficiency. Recently, Multi-Layer Perceptrons (MLPs) have gained popularity in spatial-temporal forecasting for their simplified architecture and better efficiency. However, existing MLP-based models can be susceptible to noise interference, especially when the noise can affect both input and target sequences in spatial-temporal forecasting on noisy data. To alleviate this impact, we proposeRobust Spatial-Temporal Information Bottleneck (RSTIB)principle. The RSTIB extends previous Information Bottleneck (IB) approaches by lifting the specific Markov assumption without impairing the IB nature. Then, by explicitly minimizing the irrelevant noisy information, the representation learning guided by RSTIB can be more robust against noise interference. Furthermore, the instantiation, RSTIB-MLP, can be seamlessly implemented with MLPs, thereby achieving efficient and robust spatial-temporal modeling. Moreover, a training regime is designed to handle the dynamic nature of spatial-temporal relationships by incorporating a knowledge distillation module to alleviate feature collapse and enhance model robustness under noisy conditions. Our extensive experimental results on six intrinsically noisy benchmark datasets from various domains show that the RSTIB-MLP runs much faster than state-of-the-art STGNNs and delivers superior forecasting accuracy across noisy environments, substantiating its robustness and efficiency."
    },
    {
        "title": "A Solver-Aided Hierarchical Language For LLM-Driven CAD Design",
        "link_suffix": "/forum?id=2qJXhflNbR",
        "link": "https://openreview.net/forum?id=2qJXhflNbR",
        "pdf_link": "https://openreview.net/pdf?id=2qJXhflNbR",
        "keywords": "Computer-Aided Design, Parametric Modeling, Machine Learning, Large Language Models, Programming Languages",
        "abstract": "Large language models (LLMs) have been enormously successful in solving a wide variety of structured and unstructured generative tasks, but they struggle to generate procedural geometry in Computer Aided Design (CAD). These difficulties arise from an inability to do spatial reasoning and the necessity to guide a model through complex, long range planning required for generating complex geometry. We enable generative CAD Design with LLMs through the introduction of a solver-aided, hierarchical domain specific language (DSL) called AIDL, which offloads the spatial reasoning requirements to a geometric constraint solver. Additionally, we show that in the few-shot regime, AIDL outperforms even a language with in-training data (OpenSCAD), both in terms of generating visual results closer to the prompt and creating objects that are easier to post-process and reason about."
    },
    {
        "title": "EM-GANSim: Real-time and Accurate EM Simulation Using Conditional GANs for 3D Indoor Scenes",
        "link_suffix": "/forum?id=29JDZxRgPZ",
        "link": "https://openreview.net/forum?id=29JDZxRgPZ",
        "pdf_link": "https://openreview.net/pdf?id=29JDZxRgPZ",
        "keywords": "Generative Adversarial Networks (GAN), Electromagnetic Propagation, Real-time Simulation, 3D Indoor Environments",
        "abstract": "We present a novel machine-learning (ML) approach  (EM-GANSim) for real-time electromagnetic (EM) propagation that is used for wireless communication simulation in 3D indoor environments. Our approach uses a modified conditional Generative Adversarial Network (GAN) that incorporates encoded geometry and transmitter location while adhering to the electromagnetic propagation theory. The overall physically-inspired learning is able to predict the power distribution in 3D scenes, which is represented using heatmaps.  Our overall accuracy is comparable to ray tracing-based EM simulation, as evidenced by lower mean squared error values. Furthermore, our GAN-based method drastically reduces the computation time, achieving a 5X speedup on complex benchmarks. In practice, it can compute the signal strength in a few milliseconds on any location in 3D indoor environments. We also present a large dataset of 3D models and EM ray tracing-simulated heatmaps. To the best of our knowledge, EM-GANSim is the first real-time algorithm for EM simulation in complex 3D indoor environments. We plan to release the code and the dataset."
    }
]