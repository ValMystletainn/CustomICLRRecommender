[{"title": "LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models", "link_suffix": "/forum?id=1eI236MqEA", "link": "https://openreview.net/forum?id=1eI236MqEA", "pdf_link": "https://openreview.net/pdf?id=1eI236MqEA", "keywords": "Multi-Concept Customization, LoRA Integration, Training-Free", "abstract": "Customization generation techniques have significantly advanced the synthesis of specific concepts across varied contexts. Multi-concept customization emerges as the challenging task within this domain. Existing approaches often rely on training a fusion matrix of multiple Low-Rank Adaptations (LoRAs) to merge various concepts into a single image. However, we identify this straightforward method faces two major challenges: 1) concept confusion, where the model struggles to preserve distinct individual characteristics, and 2) concept vanishing, where the model fails to generate the intended subjects. To address these issues, we introduce LoRA-Composer, a training-free framework designed for seamlessly integrating multiple LoRAs, thereby enhancing the harmony among different concepts within generated images.\nLoRA-Composer addresses concept vanishing through concept injection constraints, enhancing concept visibility via an expanded cross-attention mechanism. To combat concept confusion, concept isolation constraints are introduced, refining the self-attention computation. Furthermore, latent re-initialization is proposed to effectively stimulate concept-specific latent within designated regions. Our extensive testing showcases a notable enhancement in LoRA-Composer's performance compared to standard baselines, especially when eliminating the image-based conditions like canny edge or pose estimations.", "title_embedding_index": 13400, "title_abs_embedding_index": 13425}, {"title": "Stagewise Development in Transformers and the Geometry of the Loss Landscape", "link_suffix": "/forum?id=xEZiEhjTeq", "link": "https://openreview.net/forum?id=xEZiEhjTeq", "pdf_link": "https://openreview.net/pdf?id=xEZiEhjTeq", "keywords": "Science of deep learning, loss landscape geometry, training dynamics, singular learning theory", "abstract": "We show that internal structure emerges in discrete developmental stages during transformer training, for language modeling and in-context linear regression. We introduce a method for detecting boundaries between these stages by probing the degeneracy of the geometry of the population loss. The measure of degeneracy we use is the Local Learning Coefficient (LLC), which is derived from singular learning theory. We establish the validity of the stages revealed by the LLC using a range of behavioral and structural metrics. In a majority of the stages the loss decreases and the geometry becomes less degenerate, which is consistent with an emerging literature on stagewise learning and saddle-to-saddle dynamics in neural network training. However, we also discover several stages in which the geometry becomes more degenerate, and in one example link this to a phenomenon that we call layer normalization collapse. These findings provide new insights into the intricate process of transformer training and underscore the importance of loss landscape geometry in understanding model development.", "title_embedding_index": 13401, "title_abs_embedding_index": 13426}, {"title": "Sync4D: Video Guided Controllable Dynamics for Physics-Based 4D Generation", "link_suffix": "/forum?id=O0RIrM5iqX", "link": "https://openreview.net/forum?id=O0RIrM5iqX", "pdf_link": "https://openreview.net/pdf?id=O0RIrM5iqX", "keywords": "4D generation", "abstract": "In this work, we introduce a novel approach for creating controllable dynamics in 3D-generated Gaussians using casually captured reference videos. Our method transfers the motion of objects from reference videos to a variety of generated 3D Gaussians across different categories, ensuring precise and customizable motion transfer. We achieve this by employing blend skinning-based non-parametric shape reconstruction to extract the shape and motion of reference objects. This process involves segmenting the reference objects into motion-related parts based on skinning weights and establishing shape correspondences with generated target shapes. To address shape and temporal inconsistencies prevalent in existing methods, we integrate physical simulation, driving the target shapes with matched motion. This integration is optimized through a displacement loss to ensure reliable and genuine dynamics. Our approach supports diverse reference inputs, including humans, quadrupeds, and articulated objects, and can generate dynamics of arbitrary length, providing enhanced fidelity and applicability. Unlike methods heavily reliant on diffusion video generation models, our technique offers specific and high-quality motion transfer, maintaining both shape integrity and temporal consistency.", "title_embedding_index": 13402, "title_abs_embedding_index": 13427}, {"title": "Playing Language Game with LLMs Leads to Jailbreaking", "link_suffix": "/forum?id=BeOEmnmyFu", "link": "https://openreview.net/forum?id=BeOEmnmyFu", "pdf_link": "https://openreview.net/pdf?id=BeOEmnmyFu", "keywords": "large language model, jailbreaking attack, language game", "abstract": "The advent of large language models (LLMs) has spurred the development of numerous jailbreak techniques aimed at circumventing their security defenses against malicious attacks. An effective jailbreak approach is to identify a domain where safety generalization fails, a phenomenon known as mismatched generalization. In this paper, we introduce two novel jailbreak methods based on mismatched generalization: natural language games and custom language games, both of which effectively bypass the safety mechanisms of LLMs, with various kinds and different variants, making them hard to defend and leading to high attack rates. Natural language games involve the use of synthetic linguistic constructs and the actions intertwined with these constructs, such as the Ubbi Dubbi language. Building on this phenomenon, we propose the custom language games method: by engaging with LLMs using a variety of custom rules, we successfully execute jailbreak attacks across multiple LLM platforms. Extensive experiments demonstrate the effectiveness of our methods, achieving success rates of 93% on GPT-4o, 89% on GPT-4o-mini and 83% on Claude-3.5-Sonnet. Furthermore, to investigate the generalizability of safety alignments, we fine-tuned Llama-3.1-70B with the custom language games to achieve safety alignment within our datasets and found that when interacting through other language games, the fine-tuned models still failed to identify harmful content. This finding indicates that the safety alignment knowledge embedded in LLMs fails to generalize across different linguistic formats, thus opening new avenues for future research in this area.\nOur code is available athttps://anonymous.4open.science/r/encode_jailbreaking_anonymous-B4C4.", "title_embedding_index": 13403, "title_abs_embedding_index": 13428}, {"title": "Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation", "link_suffix": "/forum?id=1PDz4Ny1N2", "link": "https://openreview.net/forum?id=1PDz4Ny1N2", "pdf_link": "https://openreview.net/pdf?id=1PDz4Ny1N2", "keywords": "Jensen Gap, Recommender Systems, Max-min Fairness", "abstract": "Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform. However, our theoretical analysis indicates that integrating MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity. Such nonlinearity property introduces the Jensen gap between the model's convergence point and the optimal point if mini-batch sampling is applied. Both theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, the Jensen gap will widen accordingly. Some methods using heuristic re-weighting or debiasing strategies have the potential to bridge the Jensen gap. However, they either lack theoretical guarantees or suffer from heavy computational costs. To overcome these limitations, we first theoretically demonstrate that the MMF-constrained objective can be essentially reformulated as a group-weighted optimization objective. Then we present an efficient and effective algorithm named FairDual, which utilizes a dual optimization technique to minimize Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution and the Jensen gap can be well bounded under a mini-batch sampling strategy with random shuffle. Extensive experiments conducted using three large-scale RS backbone models on two publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness.", "title_embedding_index": 13404, "title_abs_embedding_index": 13429}, {"title": "Improved Convergence Rate for Diffusion Probabilistic Models", "link_suffix": "/forum?id=SOd07Qxkw4", "link": "https://openreview.net/forum?id=SOd07Qxkw4", "pdf_link": "https://openreview.net/pdf?id=SOd07Qxkw4", "keywords": "score-based generative model, diffusion model, probability flow ODE, randomized learning rate", "abstract": "Score-based diffusion models have achieved remarkable empirical performance in the field of machine learning and artificial intelligence for their ability to generate high-quality new data instances from complex distributions. Improving our understanding of diffusion models, including mainly convergence analysis for such models, has attracted a lot of interests. Despite a lot of theoretical attempts, there still exists significant gap between theory and practice. Towards to close this gap, we establish an iteration complexity at the order of $d^{1/3}\\varepsilon^{-2/3}$, which is better than $d^{5/12}\\varepsilon^{-1}$, the best known complexity achieved before our work. This convergence analysis is based on a randomized midpoint method, which is first proposed for log-concave sampling \\citep{Shen2019TheRandomized}, and then extended to diffusion models by \\citet{Gupta2024Faster}. Our theory accommodates $\\varepsilon$-accurate score estimates, and does not require log-concavity on the target distribution. Moreover, the algorithm can also be parallelized to run in only $O(\\log^2(d/\\varepsilon))$ parallel rounds in a similar way to prior works.", "title_embedding_index": 13405, "title_abs_embedding_index": 13430}, {"title": "Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models", "link_suffix": "/forum?id=r6XqXoRT6N", "link": "https://openreview.net/forum?id=r6XqXoRT6N", "pdf_link": "https://openreview.net/pdf?id=r6XqXoRT6N", "keywords": "multimodal hallucination, diffusion-based model, knowledge graph, large language model", "abstract": "The rapid advancement of Text-to-Image(T2I) generative models has enabled the synthesis of high-quality images guided by textual descriptions. Despite this significant progress, these models are often susceptible in generating contents that contradict the input text, which poses a challenge to their reliability and practical deployment. To address this problem, we introduce a novel diffusion-based framework to significantly enhance the alignment of generated images with their corresponding descriptions, addressing the inconsistency between visual output and textual input. Our framework is built upon a comprehensive analysis of inconsistency phenomena, categorizing them based on their manifestation in the image. Leveraging a state-of-the-art large language module, we first extract objects and construct a knowledge graph to predict the locations of these objects in potentially generated images. We then integrate a state-of-the-art controllable image generation model with a visual text generation module to generate an image that is consistent with the original prompt, guided by the predicted object locations. Through extensive experiments on an advanced multimodal hallucination benchmark, we demonstrate the efficacy of our approach in accurately generating the images without the inconsistency with the original prompt.", "title_embedding_index": 13406, "title_abs_embedding_index": 13431}, {"title": "MindDETR: Beyond Semantics, Exploring Positional Cues from Brain Activity", "link_suffix": "/forum?id=kMi8jCKxYr", "link": "https://openreview.net/forum?id=kMi8jCKxYr", "pdf_link": "https://openreview.net/pdf?id=kMi8jCKxYr", "keywords": "brain decoding, object detection", "abstract": "Decoding visual stimuli from brain recordings offers a unique opportunity to understand how the brain represents the world and seeks to interpret the connection between computer vision models and our visual system. Recent efforts mainly adopt diffusion models to reconstruct images from brain signals. However, while these methods generally capture correct semantic information, they often struggle with precise object localization. Additionally, the commonly used proxy task, image reconstruction from brain signals, mainly measures semantic consistency, to some extent neglecting positional information of the decoded signals. In this work, to encourage more accurate brain signal decoding, we propose to use object detection as the proxy task, aiming at decoding both the semantic and positional cues from brain recordings. Based on this task, we propose MindDETR, a brain recording-based object detection model with the DETR pipeline. After aligning feature representations with a pretrained image-based DETR model, our model demonstrates that accurately brain decoding at both semantic and positional levels is feasible, and our detection-based approach achieves significantly superior results than existing reconstruction-based approaches. This result suggests the effectiveness of applying object detection as a proxy task for brain signal decoding.  Our code will be publicly available.", "title_embedding_index": 13407, "title_abs_embedding_index": 13432}, {"title": "Impact of Dataset Properties on Membership Inference Vulnerability of Deep Transfer Learning", "link_suffix": "/forum?id=qqZijHRcA5", "link": "https://openreview.net/forum?id=qqZijHRcA5", "pdf_link": "https://openreview.net/pdf?id=qqZijHRcA5", "keywords": "Membership Inference Attack, Transfer Learning, Few-shot Learning, Image Classification", "abstract": "We analyse the relationship between privacy vulnerability and dataset properties, such as examples per class and number of classes, when applying two state-of-the-art membership inference attacks (MIAs) to fine-tuned neural networks. We derive per-example MIA vulnerability in terms of score distributions and statistics computed from shadow models. We introduce a simplified model of membership inference and prove that in this model, the logarithm of the difference of true and false positive rates depends linearly on the logarithm of the number of examples per class. We complement the theoretical analysis with empirical analysis by systematically testing the practical privacy vulnerability of fine-tuning large image classification models and obtain the previously derived power law dependence between the number of examples per class in the data and the MIA vulnerability, as measured by true positive rate of the attack at a low false positive rate. Finally, we fit a parametric model of the previously derived form to predict true positive rate based on dataset properties and observe good fit for MIA vulnerability on unseen fine-tuning scenarios.", "title_embedding_index": 13408, "title_abs_embedding_index": 13433}, {"title": "GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model", "link_suffix": "/forum?id=RmmrHEH6Nx", "link": "https://openreview.net/forum?id=RmmrHEH6Nx", "pdf_link": "https://openreview.net/pdf?id=RmmrHEH6Nx", "keywords": "Efficient Visual State Space Models, Parameter-Efficient Vision Backbones, Stable Visual State Space Models", "abstract": "Recent advancements in state-space models (SSMs) have showcased effective performance in modeling long-range dependencies with subquadratic complexity. However, pure SSM-based models still face challenges related to stability and achieving optimal performance on computer vision tasks. Our paper addresses the challenges of scaling SSM-based models for computer vision, particularly the instability and inefficiency of large model sizes. To address this, we introduce a Modulated Group Mamba layer which divides the input channels into four groups and applies our proposed SSM-based efficient Visual Single Selective Scanning (VSSS) block independently to each group, with each VSSS block scanning in one of the four spatial directions. The Modulated Group Mamba layer also wraps the four VSSS blocks into a channel modulation operator to improve cross-channel communication. Furthermore, we introduce a distillation-based training objective to stabilize the training of large models, leading to consistent performance gains. Our comprehensive experiments demonstrate the merits of the proposed contributions, leading to superior performance over existing methods for image classification on ImageNet-1K, object detection, instance segmentation on MS-COCO, and semantic segmentation on ADE20K. Our tiny variant with 23M parameters achieves state-of-the-art performance with a classification top-1 accuracy of 83.3% on ImageNet-1K, while being 26% efficient in terms of parameters, compared to the best existing Mamba design of same model size. Our code and models will be publicly released.", "title_embedding_index": 13409, "title_abs_embedding_index": 13434}, {"title": "Improving Resistance to Noisy Label Fitting by Reweighting Gradient in SAM", "link_suffix": "/forum?id=x3lE88YkUl", "link": "https://openreview.net/forum?id=x3lE88YkUl", "pdf_link": "https://openreview.net/pdf?id=x3lE88YkUl", "keywords": "label noise, sharpness-aware minimization, optimization", "abstract": "Noisy labels pose a substantial challenge in machine learning, often resulting in overfitting and poor generalization. Sharpness-Aware Minimization (SAM), as demonstrated in Foret et al. (2021), improves generalization over traditional Stochastic Gradient Descent (SGD) in classification tasks with noisy labels by implicitly slowing noisy learning. While SAM\u2019s ability to generalize in noisy environments has been studied in several simplified settings, its full potential in more realistic training settings remains underexplored. In this work, we analyze SAM\u2019s behavior at each iteration, identifying specific components of the gradient vector that contribute significantly to its robustness against noisy labels. Based on these insights, we propose SANER (Sharpness-Aware Noise-Explicit Reweighting), an effective variant that enhances SAM\u2019s ability to manage noisy fitting rate. Our experiments on CIFAR-10, CIFAR-100, and Mini-WebVision demonstrate that SANER consistently outperforms SAM, achieving up to an 8% increase on CIFAR-100 with 50% label noise.", "title_embedding_index": 13410, "title_abs_embedding_index": 13435}, {"title": "KokerNet: Koopman Kernel Network for Time Series Forecasting", "link_suffix": "/forum?id=w7vn6ah0Qg", "link": "https://openreview.net/forum?id=w7vn6ah0Qg", "pdf_link": "https://openreview.net/pdf?id=w7vn6ah0Qg", "keywords": "Spectral kernel, Koopman operator, Time series", "abstract": "The Koopman operator has gained increasing attention in time series forecasting due to its ability to simplify the complex evolution of dynamic systems. However, most existing Koopman-based methods suffer from significant computational costs in constructing measurement functions and struggle to address the challenge posed by the variation in data distribution. Additionally, these approaches tend to empirically decompose time series or distributions into combinations of components, lacking interpretability. To tackle these issues, we propose a novel approach,Koopmankernelnetwork (KokerNet), for time series forecasting. On one hand, we construct a measurement function space using the spectral kernel method, which enables us to perform Koopman operator learning in a low-dimensional feature space, efficiently reducing computational costs. On the other hand, an index is designed to characterize the stationarity of data in both time and frequency domains. This index can interpretably guide us to decompose the time series into stationary and non-stationary components. The global and local Koopman operators are then learned within the constructed measurement function space to predict the future behavior of the stationary and non-stationary components, respectively. Particularly, to address the challenge posed by the variation in distribution, we incorporate a distribution module for the non-stationary component, ensuring that the model can make aligned distribution predictions. Extensive experiments across multiple benchmarks illustrate the superiority of our proposed KokerNet, consistently outperforming the state-of-the-art models.", "title_embedding_index": 13411, "title_abs_embedding_index": 13436}, {"title": "Improved Sample Access for Quantum-Inspired Algorithms", "link_suffix": "/forum?id=aj87NEVSiO", "link": "https://openreview.net/forum?id=aj87NEVSiO", "pdf_link": "https://openreview.net/pdf?id=aj87NEVSiO", "keywords": "Quantum-inspired machine learning, importance sampling, Parametrized Quantum Circuit, Direct Fidelity Estimation", "abstract": "Quantum-inspired classical algorithms has received much attention due to its exponential speedup compared to existing algorithms, under certain data storage assumptions. The improvements are noticeable in fundamental linear algebra tasks. In this work, we analyze two major subroutines and discuss their possible improvements by generalizing the data structure. We demonstrate that this generalization proves beneficial for dequantized quantum classifiers. Motivated by this, we also tighten the upper bound on the number of required measurements for direct fidelity estimation. We expect our findings to suggest optimal implementations for various potential quantum-inspired machine learning algorithms that involve extremely high-dimensional operations, which has potential for many applications.", "title_embedding_index": 13412, "title_abs_embedding_index": 13437}, {"title": "Class-Relational Label Smoothing for Lifelong Visual Place Recognition", "link_suffix": "/forum?id=ZS1lCBLljq", "link": "https://openreview.net/forum?id=ZS1lCBLljq", "pdf_link": "https://openreview.net/pdf?id=ZS1lCBLljq", "keywords": "Lifelong place recognition, visual geo-localization", "abstract": "Visual Place Recognition (VPR) is a task of estimating the location of a query image, predominantly executed through image retrieval using learned global descriptors from a reference database of geo-tagged images. While recent approaches have aimed to improve the scalability of VPR training by leveraging classification loss as a proxy task, this leads to a task gap between classification and retrieval - classification discretizes the feature space into distinct class regions, often overlooking visual differences between classes. This gap makes VPR systems particularly vulnerable to extreme visual changes such as lifelong variations. To remedy these problems, we propose a novel Class-Relational Label Smoothing (CRLS) that transforms one-hot labels into soft labels by considering visual information of inter-class relations. We further enhance this method by dynamically adjusting the influence of CRLS based on the stability of class weights, which is quantified by their magnitudes. Importantly, our findings suggest that the magnitude of class weights serves as an indicator of class stability, which is also supported by derivative analysis. We demonstrate that our method outperforms state-of-the-art methods on the most extensive 17 benchmarks, effectively bridging the task gap between classification and retrieval in visual place recognition. Codes and trained weights will be made publicly available.", "title_embedding_index": 13413, "title_abs_embedding_index": 13438}, {"title": "From Context to Concept: Concept Encoding in In-Context Learning", "link_suffix": "/forum?id=0ULf242ApE", "link": "https://openreview.net/forum?id=0ULf242ApE", "pdf_link": "https://openreview.net/pdf?id=0ULf242ApE", "keywords": "mechanistic interpretability, in-context learning, large language models", "abstract": "Humans distill complex experiences into fundamental abstractions, enabling rapid learning and adaptation. Similarly, autoregressive transformers exhibit adaptive learning through in-context learning (ICL). This raises the question of how abstractions play a role in ICL and are represented within the models. In this paper, we investigate how this mechanism emerges in transformers during training by studying their representations. By observing the training dynamics of a small transformer on synthetic ICL tasks, we show the coupled emergence of concept encoding and concept decoding. As the model learns to encode different latent concepts (e.g., \"Finding the first noun in a sentence.\") into distinct, separable representations, it conditionally builds decoding algorithms and improve its ICL performance. Based on this mutual dependency, we hypothesize that the model's ability to discern between the latent concepts is predictive of downstream ICL performance. We empirically characterize that a pretrained Llama-3.1-8B also exhibits concept encoding abilities. Moreover, with mechanistic interventions and controlled finetuning, we reveal that the accuracy of concept inference is causally related to ICL performance. Our empirical insights shed light into better understanding the success and failure modes of large language models via their representations.", "title_embedding_index": 13414, "title_abs_embedding_index": 13439}, {"title": "Evidence-Enhanced Triplet Generation Framework for Hallucination Alleviation in Generative Question Answering", "link_suffix": "/forum?id=1t1YSuBv3T", "link": "https://openreview.net/forum?id=1t1YSuBv3T", "pdf_link": "https://openreview.net/pdf?id=1t1YSuBv3T", "keywords": "Evidence-Enhanced, Hallucination Alleviation, Generative Question Answering", "abstract": "To\naddress the hallucination in generative question answering (GQA) where the answer can not be derived from the document, we propose a novel evidence-enhanced triplet generation framework,\nEATQA, encouraging the model to\npredict all the combinations of \u27e8Question, Evidence, Answer\u27e9 triplet\nby flipping the source pair and the target label\nto understand their logical relationships, i.e.,\npredict Answer(A), Question(Q), and Evidence(E) given a QE, EA, and QA\npairs, respectively. Furthermore, we bridge the distribution gap to distill the knowledge from evidence in inference stage. Our framework ensures the model to learn the logical relation between query, evidence and answer, which simultaneously improves the evidence generation and query answering. In this paper, we apply EATQA to LLama and it outperforms other LLMs-based methods and hallucination mitigation approaches on two challenging GQA benchmarks. Further analysis shows that our method not only keeps prior knowledge within LLM, but also mitigates hallucination and generates faithful answers.", "title_embedding_index": 13415, "title_abs_embedding_index": 13440}, {"title": "In-Context Transfer Learning: Demonstration Synthesis by Transferring Similar Tasks", "link_suffix": "/forum?id=ptTt8mhS7n", "link": "https://openreview.net/forum?id=ptTt8mhS7n", "pdf_link": "https://openreview.net/pdf?id=ptTt8mhS7n", "keywords": "In-Context Learning, Transfer Learning, Demonstration Synthesis", "abstract": "In-context learning (ICL) is an effective approach to help large language models (LLMs) adapt to various tasks by providing demonstrations of the target task. Considering the high cost of labeling demonstrations, many methods propose synthesizing demonstrations from scratch using LLMs. However, the quality of the demonstrations synthesized from scratch is limited by the capabilities and knowledge of LLMs. To address this, inspired by transfer learning, we propose In-Context Transfer Learning (ICTL), which synthesizes target task demonstrations by transferring labeled demonstrations from similar source tasks. ICTL consists of two steps: source sampling and target transfer. First, we define an optimization objective, which minimizes transfer error to sample source demonstrations similar to the target task.  Then, we employ LLMs to transfer the sampled source demonstrations to match the definition and format of the target task. Experiments on Super-NI show that ICTL outperforms synthesis from scratch by 2.0% on average, demonstrating the effectiveness of our method.", "title_embedding_index": 13416, "title_abs_embedding_index": 13441}, {"title": "Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models", "link_suffix": "/forum?id=6embY8aclt", "link": "https://openreview.net/forum?id=6embY8aclt", "pdf_link": "https://openreview.net/pdf?id=6embY8aclt", "keywords": "large language models, knowledge graphs, reasoning", "abstract": "Large language models (LLMs) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs (KGs) have been utilized to enhance LLM reasoning through their structured knowledge. However, existing KG-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing KGs at scale. In this work, we introduce graph-constrained reasoning (GCR), a novel framework that bridges structured knowledge in KGs with unstructured reasoning in LLMs. To eliminate hallucinations, GCR ensures faithful KG-grounded reasoning by integrating KG structure into the LLM decoding process through KG-Trie, a trie-based index that encodes KG reasoning paths. KG-Trie constrains the decoding process, allowing LLMs to directly reason on graphs and generate faithful reasoning paths grounded in KGs. Additionally, GCR leverages a lightweight KG-specialized LLM for graph-constrained reasoning alongside a powerful general LLM for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several KGQA benchmarks demonstrate that GCR achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen KGs without additional training.", "title_embedding_index": 13417, "title_abs_embedding_index": 13442}, {"title": "Breaking Free: Hacking Diffusion Models for Generating Adversarial Examples and Bypassing Safety Guardrails", "link_suffix": "/forum?id=6qeCyvlJUJ", "link": "https://openreview.net/forum?id=6qeCyvlJUJ", "pdf_link": "https://openreview.net/pdf?id=6qeCyvlJUJ", "keywords": "Conditioned-Image Synthesis, Natural Adversarial Examples, CMA Evolutionary Strategy Optimization", "abstract": "Deep neural networks can be exploited using natural adversarial samples, which do not impact human perception. Current approaches often rely on synthetically altering the distribution of adversarial samples compared to the training distribution. In contrast, we propose EvoSeed, a novel evolutionary strategy-based algorithmic framework that uses auxiliary Conditional Diffusion and Classifier models to generate photo-realistic natural adversarial samples. We employ CMA-ES to optimize the initial seed vector search, which, when processed by the Conditional Diffusion Model, results in the natural adversarial sample misclassified by the Classifier Model. Experiments show that generated adversarial images are of high image quality, raising concerns about generating harmful content bypassing safety classifiers. We also show that beyond generating adversarial images, EvoSeed can also be used as a red-teaming tool to understand classification systems' misclassification. Our research opens new avenues for understanding the limitations of current safety mechanisms and the risk of plausible attacks against classifier systems using image generation.", "title_embedding_index": 13418, "title_abs_embedding_index": 13443}, {"title": "Enabling Fine-Tuning of Direct Feedback Alignment via Feedback-Weight Matching", "link_suffix": "/forum?id=Fx4fFBjn6j", "link": "https://openreview.net/forum?id=Fx4fFBjn6j", "pdf_link": "https://openreview.net/pdf?id=Fx4fFBjn6j", "keywords": "direct feedback alignment, deep learning, fine tuning", "abstract": "In this paper, we introduce feedback-weight matching, a new method that facilitates reliable fine-tuning of fully connected neural networks using Direct Feedback Alignment (DFA). Although DFA has demonstrated potential by enabling efficient and parallel updates of weight parameters through direct propagation of the network's output error, its usage has been primarily restricted to training networks from scratch. We provide the first analysis showing that existing standard DFA struggles to fine-tune networks pre-trained via back-propagation. Through an analysis of weight alignment (WA) and gradient alignment (GA), we show that the proposed feedback-weight matching enhances DFA's ability and stability in fine-tuning pre-trained networks, providing insights into DFA's behavior and characteristics when applied to fine-tuning. In addition, we find that feedback-weight matching, when combined with weight decay, not only mitigates over-fitting but also further reduces the network output error, leading to improved learning performance during DFA-based fine-tuning. Our experimental results show that, for the first time, feedback-weight matching enables reliable and superior fine-tuning across various fine-tuning tasks compared to existing standard DFA, e.g., achieving 7.97% accuracy improvement on image classification tasks (i.e., 82.67% vs. 74.70%) and 0.66 higher correlation score on NLP tasks (i.e., 0.76 vs. 0.10). The code implementation is available at an anonymous GitHub repository.", "title_embedding_index": 13419, "title_abs_embedding_index": 13444}, {"title": "From Noise to Factors: Diffusion-based Unsupervised Sequential Disentanglement", "link_suffix": "/forum?id=YFKH1vO0W2", "link": "https://openreview.net/forum?id=YFKH1vO0W2", "pdf_link": "https://openreview.net/pdf?id=YFKH1vO0W2", "keywords": "Sequential Disentanglement, Deep Learning, Generative Models", "abstract": "Unsupervised representation learning, in particular, sequential disentanglement, where the goal is to learn disentangled static and dynamic factors of variation, remains a significant challenge due to the absence of labels. Existing models, based on variational autoencoders and generative adversarial networks, achieved success in certain domains, but they often struggle with disentangling sequences, especially when dealing with real-world complexity and variability. Further, there is no real-world evaluation protocol for assessing the effectiveness of sequential disentanglement models. Recently, diffusion autoencoders have emerged as a new promising generative model, offering semantically rich representations by gradual noise-to-data transformations. Despite their advantages, these models face limitations: they are non-sequential, fail to disentangle the latent space effectively, and are computationally intensive, making them difficult to scale to sequences. In this work, we introduce our diffusion sequential disentanglement autoencoder (DiffSDA), a novel approach effective on real-world visual data and accompanied by a new and challenging evaluation protocol. DiffSDA is based on a new probabilistic modeling and is implemented using latent diffusion models and efficient samplers, facilitating processing of high-resolution videos. We test our approach on several real-world datasets and metrics, and we demonstrate its effectiveness in comparison to recent state-of-the-art sequential disentanglement methods.", "title_embedding_index": 13420, "title_abs_embedding_index": 13445}, {"title": "A Competitive-Cooperative Actor-critic Framework for Reinforcement Learning", "link_suffix": "/forum?id=ywHOnGOLb1", "link": "https://openreview.net/forum?id=ywHOnGOLb1", "pdf_link": "https://openreview.net/pdf?id=ywHOnGOLb1", "keywords": "Deep reinforcement learning; Double-actor framework; Competition and Cooperation", "abstract": "In the field of Deep reinforcement learning (DRL), enhancing exploration capabilities and improving the accuracy of Q-value estimation remain two major challenges.\nRecently, double-actor DRL methods have emerged as a promising class of DRL approaches, achieving substantial advancements in both exploration and Q-value estimation. However, existing double-actor DRL methods feature actors that operate independently in exploring the environment, lacking mutual learning and collaboration, which leads to suboptimal policies. To address this challenge, this work proposes a generic solution that can be seamlessly integrated into existing double-actor DRL methods by promoting mutual learning among the actors to develop improved policies. Specifically, we calculate the difference in actions output by the actors and minimize this difference as a loss during training to facilitate mutual imitation among the actors. Simultaneously, we also minimize the differences in Q-values output by the various critics as part of the loss, thereby avoiding significant discrepancies in value estimation for the imitated actions. We present two specific implementations of our method and extend these implementations beyond double-actor DRL methods to other DRL approaches to encourage broader adoption. Experimental results demonstrate that our method effectively enhances four state-of-the-art (SOTA) double-actor DRL methods and five other types of SOTA DRL methods across four MuJoCo tasks, as measured by return.", "title_embedding_index": 13421, "title_abs_embedding_index": 13446}, {"title": "DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation", "link_suffix": "/forum?id=L8e7tBf4pP", "link": "https://openreview.net/forum?id=L8e7tBf4pP", "pdf_link": "https://openreview.net/pdf?id=L8e7tBf4pP", "keywords": "robustness, distribution shift, robust fine-tuning, weight interpolation, model merging", "abstract": "Adapting a pre-trained foundation model on downstream tasks should ensure robustness against distribution shifts without the need to retrain the whole model. Although existing weight interpolation methods are simple yet effective, we argue their static nature limits downstream performance while achieving efficiency. In this work, we propose DaWin, a training-free dynamic weight interpolation method that leverages the entropy of individual models over each unlabeled test sample to assess model expertise, and compute per-sample interpolation coefficients dynamically. Unlike previous works that typically rely on additional training to learn such coefficients, our approach requires no training. Then, we propose a mixture modeling approach that greatly reduces inference overhead raised by dynamic interpolation. We validate DaWin on the large-scale visual recognition benchmarks, spanning 14 tasks across robust fine-tuning -- ImageNet and derived five distribution shift benchmarks -- and multi-task learning with eight classification tasks. Results demonstrate that DaWin achieves significant performance gain in considered settings, with minimal computational overhead. We further discuss DaWin's analytic behavior to explain its empirical success. Code is available athttps://anonymous.4open.science/r/dawin-237C.", "title_embedding_index": 13422, "title_abs_embedding_index": 13447}, {"title": "Competitive Fair Scheduling with Predictions", "link_suffix": "/forum?id=jBYQAtzp5Z", "link": "https://openreview.net/forum?id=jBYQAtzp5Z", "pdf_link": "https://openreview.net/pdf?id=jBYQAtzp5Z", "keywords": "Learning-augmented Algorithms, Scheduling, Competitive analysis, Fairness, Predictions", "abstract": "We consider online non-clairvoyant scheduling to minimize the max-stretch under the learning-augmented framework, where the scheduler has access to job size predictions. We present a family of algorithms:Relaxed-Greedy (RG)with an $O(\\eta^3 \\cdot \\sqrt{P})$ competitive ratio, where $\\eta$ denotes the prediction error for job sizes and $P$ is the maximum job size ratio;Adaptive Relaxed-Greedywith an $O(\\lambda^{0.5} \\cdot \\eta^{2.5} \\cdot \\sqrt{P})$ competitive ratio, where $\\lambda$ denotes the prediction error for the minimum job size;Predictive Relaxed-Greedywith an $O(\\lambda^{0.5} \\cdot \\varphi^{0.5} \\cdot \\eta \\cdot \\max \\{ \\eta, \\varphi \\} \\cdot \\sqrt{P})$ competitive ratio, where $\\varphi$ denotes the prediction error for the maximum job size. We also present${RG}^x$, an algorithm that represents a trade-off between consistency and smoothness, with an $O(\\eta^{2+2x} \\cdot P^{1-x})$ competitive ratio. We introduce a general method using resource augmentation to bound robustness, resulting inRR-augmentedRG, which achieves a $(1 + \\epsilon)$-speed $O(\\min \\{ \\eta^3 \\sqrt{P}, \\frac{n}{\\epsilon} \\})$ competitive ratio. Finally, we conduct simulations on synthetic and real-world datasets to evaluate the practical performance of these algorithms.", "title_embedding_index": 13423, "title_abs_embedding_index": 13448}, {"title": "Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts", "link_suffix": "/forum?id=421D67DY3i", "link": "https://openreview.net/forum?id=421D67DY3i", "pdf_link": "https://openreview.net/pdf?id=421D67DY3i", "keywords": "clustering of bandits, linear bandits, online learning", "abstract": "The contextual multi-armed bandit (MAB) problem is crucial in sequential decision-making. A line of research, known as online clustering of bandits, extends contextual MAB by grouping similar users into clusters, utilizing shared features to improve learning efficiency. However, existing algorithms, which rely on the upper confidence bound (UCB) strategy, struggle to gather adequate statistical information to accurately identify unknown user clusters. As a result, their theoretical analyses require several strong assumptions about the \"diversity\" of contexts generated by the environment, leading to impractical settings, complicated analyses, and poor practical performance. Removing these assumptions has been a long-standing open problem in the clustering of bandits literature. In this work, we provide two partial solutions. First, we introduce an additional exploration phase to accelerate the identification of clusters. We integrate this general strategy into both graph-based and set-based algorithms and propose two new algorithms, UniCLUB and UniSCLUB. Remarkably, our algorithms require substantially weaker assumptions and simpler theoretical analyses while achieving superior cumulative regret compared to previous studies. Second, inspired by the smoothed analysis framework, we propose a more practical setting that eliminates the requirement for i.i.d. context generation used in previous studies, thus enhancing the performance of existing algorithms for online clustering of bandits. Extensive evaluations on both synthetic and real-world datasets demonstrate that our proposed algorithms outperform existing approaches.", "title_embedding_index": 13424, "title_abs_embedding_index": 13449}]