[
    {
        "title": "UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation",
        "link_suffix": "/forum?id=yj9lLwMjnE",
        "link": "https://openreview.net/forum?id=yj9lLwMjnE",
        "pdf_link": "https://openreview.net/pdf?id=yj9lLwMjnE",
        "keywords": "speech foundation model, generative pre-training, self-supervised learning, speech generation, speech tokenization",
        "abstract": "Pre-training and representation learning have been playing an increasingly important role in modern speech processing.  Nevertheless, different applications have been relying on different foundation models, since predominant pre-training techniques are either designed for discriminative tasks or generative tasks. In this work, we make the first attempt at building a unified pre-training framework for both types of tasks in speech. We show that with the appropriate design choices for pre-training, one can jointly learn a representation encoder and generative audio decoder that can be applied to both types of tasks. We propose UniWav, an encoder-decoder framework designed to unify pre-training representation learning and generative tasks. On speech recognition, text-to-speech, and speech tokenization, \\proposed{} achieves comparable performance to different existing foundation models, each trained on a specific task. Our findings suggest that a single general-purpose foundation model for speech can be built to replace different foundation models, reducing the overhead and cost of pre-training."
    },
    {
        "title": "When do GFlowNets learn the right distribution?",
        "link_suffix": "/forum?id=9GsgCUJtic",
        "link": "https://openreview.net/forum?id=9GsgCUJtic",
        "pdf_link": "https://openreview.net/pdf?id=9GsgCUJtic",
        "keywords": "GFlowNets",
        "abstract": "Generative Flow Networks (GFlowNets) are an emerging class of sampling methods for distributions over discrete and compositional objects, e.g., graphs. In spite of their remarkable success in problems such as drug discovery and phylogenetic inference, the question of when and whether GFlowNets learn to sample from the target distribution remains underexplored. To tackle this issue, we first assess the extent to which a violation of the detailed balance of the underlying flow network might hamper the correctness of GFlowNet's sampling distribution. In particular, we demonstrate that the impact of an imbalanced edge on the model's accuracy is influenced by the total amount of flow passing through it and, as a consequence, is unevenly distributed across the network. We also argue that, depending on the parameterization, imbalance may be inevitable. In this regard, we consider the problem of sampling from distributions over graphs with GFlowNets parameterized by graph neural networks (GNNs) and show that the representation limits of GNNs delineate which distributions these GFlowNets can approximate. Lastly, we address these limitations by proposing a theoretically sound and computationally tractable metric for assessing GFlowNets, experimentally showing it is a better proxy for correctness than popular evaluation protocols."
    },
    {
        "title": "Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation",
        "link_suffix": "/forum?id=NDLmZZWATc",
        "link": "https://openreview.net/forum?id=NDLmZZWATc",
        "pdf_link": "https://openreview.net/pdf?id=NDLmZZWATc",
        "keywords": "Prompt learning, Vision-language models, Large language models, Few-shot image recognition",
        "abstract": "Recent advances in pre-trained Vision Language Models (VLMs) have shown promising potential through \\textit{prompt learning} in effectively adapting to downstream tasks without requiring additional annotated paired datasets.\nTo supplement text information in VLMs dependently trained on correlation with vision data, new approaches leveraging Large Language Models (LLM) in prompts have been proposed, enhancing robustness to unseen and diverse data.\nExisting methods query LLM for text-based responses (i.e., \\textit{descriptions}) to utilize in prompts, but this approach has limitations: high variability and low reliability.\nIn this work, we propose \\textbf{De}scription-free \\textbf{Mul}ti-prompt Learning(\\textbf{DeMul}) for image recognition task, a novel method that eliminates the process of extracting descriptions and instead directly distills knowledge from LLM into prompts.\nBy adopting a description-free approach, prompts can encapsulate richer semantics and still be defined as continuous vectors to optimize, thereby eliminating the need for discrete pre-defined templates.\nAdditionally, in a multi-prompt setting, we have empirically shown the potential of using prompt weighting to reflect the importance of different prompts during training.\nExperimental results demonstrate that our approach achieves superior performance across 11 recognition datasets."
    },
    {
        "title": "Interfering with Interference: Blind Shuffling and Superposition for Better Multi-Model Compression",
        "link_suffix": "/forum?id=4wuvmJRAU4",
        "link": "https://openreview.net/forum?id=4wuvmJRAU4",
        "pdf_link": "https://openreview.net/pdf?id=4wuvmJRAU4",
        "keywords": "Task Arithmetic, Superposition, Model Merging, Multi-model Compression, Model Serving",
        "abstract": "We present two complementary random mechanisms to significantly reduce interference when eliminating cross-model redundancy for efficient multi-model serving:Layer ShufflingandTask Vector Superposition. They work together to increase the orthogonality among interfering task vectors, forcing them into self-destruction without requiring any post-training learning or optimization.Layer Shufflingrandomly reorders layers of each individual models to reduce the alignment between interfering task vectors. WhileTask Vector Superpositionleverages random orthogonal transformations to decorrelate task vectors further. Together, these techniques drastically minimize interference, yielding improved performance across multiple tasks with effectively zero incremental memory cost when incorporating new models. Their data and model-independent nature also allows for seamless on-the-fly addition or removal of models, without requiring any re-computation, making them highly practical for real-world deployment scenarios."
    },
    {
        "title": "One-Hot Multi-Level LIF Spiking Neural Networks for Enhanced Accuracy-Latency Tradeoff",
        "link_suffix": "/forum?id=rEnPEIwXrB",
        "link": "https://openreview.net/forum?id=rEnPEIwXrB",
        "pdf_link": "https://openreview.net/pdf?id=rEnPEIwXrB",
        "keywords": "spiking neural networks, leaky integrate-and-fire, energy-efficient, low latency",
        "abstract": "Spiking neural networks (SNNs) hold significant promise as energy-efficient alternatives to conventional artificial neural networks (ANNs). However, SNNs require computations across multiple timesteps, resulting in increased latency, heightened energy consumption, and additional memory access overhead. Techniques to reduce SNN latency down to a unit timestep have emerged to realize true superior energy efficiency over ANNs. Nonetheless, this latency reduction often comes at the expense of noticeable accuracy degradation. Therefore, achieving an optimal balance in the tradeoff between accuracy and energy consumption by adjusting the latency of multiple timesteps remains a significant challenge. In this paper, we introduce a new dimension to the accuracy-energy tradeoff space using a novel one-hot multi-level leaky integrate-and-fire (M-LIF) neuron model. The proposed M-LIF model represents the inputs and outputs of hidden layers as a set of one-hot binary-weighted spike lanes to find better tradeoff points while still being able to model conventional SNNs. For image classification on static datasets, we demonstrate M-LIF SNNs outperform iso-architecture conventional LIF SNNs in terms of accuracy ($2$% higher than VGG16 SNN on ImageNet) while still being energy-efficient ($20\\times$ lower energy than VGG16 ANN on ImageNet). For dynamic vision datasets, we demonstrate the ability of M-LIF SNNs to reduce latency by $3\\times$ compared to conventional LIF SNNs while limiting accuracy degradation ($<1$%)."
    },
    {
        "title": "Shapley Is Not All You Need: Sobol's Total Indices for Feature Selection and Performance Loss Estimation",
        "link_suffix": "/forum?id=9ILaEDrwWY",
        "link": "https://openreview.net/forum?id=9ILaEDrwWY",
        "pdf_link": "https://openreview.net/pdf?id=9ILaEDrwWY",
        "keywords": "Feature selection, Shapley values, Sobol indices, global sensitivity analysis, machine learning",
        "abstract": "The selection of pertinent features constitutes a pivotal step in developing interpretable machine learning models, particularly when handling high-dimensional data, where the combinatorial interactions among features must be considered. The Shapley value, a concept originating from cooperative game theory, has gained recognition as a method for quantifying feature importance. However, the Shapley value often fails to precisely reflect the variance reduction that occurs when a feature is removed from the model. As the number of features increases, these challenges are further exacerbated by the high computational complexity of computing the exact Shapley value. Additionally, the common approximation techniques used to calculate the Shapley value are not model-agnostic.\nTo address these gaps, we propose utilizing Sobol's total indices, a variance-based sensitivity analysis technique, as a more efficient and robust alternative to Shapley values. In this paper, we present both theoretical and empirical studies comparing these two methods. Sobol's total indices provide several key advantages. It captures both main effects and interactions, offering a more accurate importance measure than Shapley values. Its computation scales linearly with the number of features, making it suitable for high-dimensional problems. Additionally, it is derived from the data itself, ensuring complete model-agnosticism. \nExperiments on synthetic and real-world datasets demonstrate that feature selection using Sobol's total indices achieves better predictive performance than Shapley-based selection while requiring significantly less computational time. Our findings suggest that Sobol's total indices are a promising alternative to Shapley values, offering greater computational efficiency, comprehensiveness in accounting for interactions, and robustness in estimating variance. This represents a favorable substitute, particularly for high-dimensional feature selection."
    },
    {
        "title": "Multiple Heads are Better than One: Mixture of Modality Knowledge Experts for Entity Representation Learning",
        "link_suffix": "/forum?id=ue1Tt3h1VC",
        "link": "https://openreview.net/forum?id=ue1Tt3h1VC",
        "pdf_link": "https://openreview.net/pdf?id=ue1Tt3h1VC",
        "keywords": "Multi-modal Information Fusion, Knowledge Graph, Multi-modal Entity Representation, Mixture-of-Experts",
        "abstract": "Learning high-quality multi-modal entity representations is an important goal of multi-modal knowledge graph (MMKG) representation learning, which can enhance reasoning tasks within the MMKGs, such as MMKG completion (MMKGC). The main challenge is to collaboratively model the structural information concealed in massive triples and the multi-modal features of the entities. Existing methods focus on crafting elegant entity-wise multi-modal fusion strategies, yet they overlook the utilization of multi-perspective features concealed within the modalities under diverse relational contexts. To address this issue, we introduce a novel framework with Mixture of Modality Knowledge experts (MoMoK for short) to learn adaptive multi-modal entity representations for better MMKGC. We design relation-guided modality knowledge experts to acquire relation-aware modality embeddings and integrate the predictions from multi-modalities to achieve joint decisions. Additionally, we disentangle the experts by minimizing their mutual information. Experiments on four public MMKG benchmarks demonstrate the outstanding performance of MoMoK under complex scenarios. Our code and data are available athttps://anonymous.4open.science/r/MoMoK-8532/."
    },
    {
        "title": "GIFT-Eval: A Benchmark for General Time Series Forecasting Model Evaluation",
        "link_suffix": "/forum?id=9EBSEkFSje",
        "link": "https://openreview.net/forum?id=9EBSEkFSje",
        "pdf_link": "https://openreview.net/pdf?id=9EBSEkFSje",
        "keywords": "benchmark, time series forecasting, foundation models, forecasting, univariate forecasting, multivariate forecasting, pretraining data, deep learning, statistical models, foundation models, dataset",
        "abstract": "Time series foundation models excel in zero-shot forecasting, handling diverse tasks without explicit training. However, the advancement of these models has been hindered by the lack of comprehensive benchmarks. To address this gap, we introduce theGeneral TIme SeriesForecasTing ModelEvaluation,GIFT-EVAL, a pioneering benchmark aimed at promoting evaluation across diverse datasets. GIFT-EVAL encompasses 28 datasets over 144,000 time series and 177 million data points, spanning seven domains, 10 frequencies, multivariate inputs, and prediction lengths ranging from short to long-term forecasts. To facilitate the effective pretraining and evaluation of foundation models, we also provide a non-leaking pretraining dataset containing approximately 230 billion data points. Additionally, we provide a comprehensive analysis of 17 baselines, which includes statistical models, deep learning models, and foundation models. We discuss each model in the context of various benchmark characteristics and offer a qualitative analysis that spans both deep learning and foundation models. We believe the insights from this analysis, along with access to this new standard zero-shot time series forecasting benchmark, will guide future developments in time series foundation models."
    },
    {
        "title": "Contrastive Learning from Synthetic Audio Doppelg\u00e4ngers",
        "link_suffix": "/forum?id=XRtyVELwr6",
        "link": "https://openreview.net/forum?id=XRtyVELwr6",
        "pdf_link": "https://openreview.net/pdf?id=XRtyVELwr6",
        "keywords": "synthetic data, audio, contrastive learning, representation learning",
        "abstract": "Learning robust audio representations currently demands extensive datasets of real-world sound recordings. By applying artificial transformations to these recordings, models can learn to recognize similarities despite subtle variations through techniques like contrastive learning. However, these transformations are only approximations of the true diversity found in real-world sounds, which are generated by complex interactions of physical processes, from vocal cord vibrations to the resonance of musical instruments. We propose a solution to both the data scale and transformation limitations, leveraging synthetic audio. By randomly perturbing the parameters of a sound synthesizer, we generate audio doppelg\u00e4ngers\u2014synthetic positive pairs with causally manipulated variations in timbre, pitch, and temporal envelopes. These variations, difficult to achieve through augmentations of existing audio, provide a rich source of contrastive information. Despite the shift to randomly generated synthetic data, our method produces strong representations, outperforming real data on several standard audio classification tasks. Notably, our approach is lightweight, requires no data storage, and has only a single hyperparameter, which we extensively analyze. We offer this method as a complement to existing strategies for contrastive learning in audio, using synthesized sounds to reduce the data burden on practitioners."
    },
    {
        "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs",
        "link_suffix": "/forum?id=pwIGnH2LHJ",
        "link": "https://openreview.net/forum?id=pwIGnH2LHJ",
        "pdf_link": "https://openreview.net/pdf?id=pwIGnH2LHJ",
        "keywords": "LLM, benchmarks, code generation",
        "abstract": "Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve \u201ccheating\u201d as the solutions were directly provided in the issue report\nor the comments. We refer to as \u2018solution leakage\u2019 problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 drops from 12.47% to 3.97%. We also observed that the same data qualify issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM\u2019s knowledge cutoff dates, posing potential data leakage issues.The critical problem in the current versions of SWE-bench dataset motivated us to refine it to build a more rigorous evaluation dataset SWE-Bench+. We created SWE-bench+ by collecting GitHub issues that were created after the training cutoff dates of the LLMs to prevent the potential data leakage problem. We also ensure that the issues collected do not contain solutions in their reports or comments. After carefully analyzing the passed instances from the SWE-Agent + GPT-4 model with the new dataset, SWE-Bench+, we observed a decline in the pass rate, dropping from 3.97% (as seen on the refined SWE-Bench) to a resolution rate of 0.55%. We further evaluated SWE-RAG + GPT-4, SWE-RAG + GPT-3.5, and AutoCodeRover + GPT-4o models on the new dataset to verify our findings, where the resolution rates of the models drop significantly, which are 0.73%, 0.55%, and 3.83%, respectively."
    },
    {
        "title": "MetaMetrics: Calibrating Metrics for Generation Tasks Using Human Preferences",
        "link_suffix": "/forum?id=slO3xTt4CG",
        "link": "https://openreview.net/forum?id=slO3xTt4CG",
        "pdf_link": "https://openreview.net/pdf?id=slO3xTt4CG",
        "keywords": "metrics, human preferences, calibrating, generation",
        "abstract": "Understanding the quality of a performance evaluation metric is crucial for ensuring that model outputs align with human preferences. However, it remains unclear how well each metric captures the diverse aspects of these preferences, as metrics often excel in one particular area but not across all dimensions. To address this, it is essential to systematically calibrate metrics to specific aspects of human preference, catering to the unique characteristics of each aspect. We introduce MetaMetrics, a calibrated meta-metric designed to evaluate generation tasks across different modalities in a supervised manner. MetaMetrics optimizes the combination of existing metrics to enhance their alignment with human preferences. Our metric demonstrates flexibility and effectiveness in both language and vision downstream tasks, showing significant benefits across various multilingual and multi-domain scenarios. MetaMetrics aligns closely with human preferences and is highly extendable and easily integrable into any application. This makes MetaMetrics a powerful tool for improving the evaluation of generation tasks, ensuring that metrics are more representative of human judgment across diverse contexts."
    },
    {
        "title": "TELEPORTATION WITH NULL SPACE GRADIENT PROJECTION FOR OPTIMIZATION ACCELERATION",
        "link_suffix": "/forum?id=IcNzKiB8CP",
        "link": "https://openreview.net/forum?id=IcNzKiB8CP",
        "pdf_link": "https://openreview.net/pdf?id=IcNzKiB8CP",
        "keywords": "Optimization, Teleportation, Gradient Projection",
        "abstract": "Optimization techniques have become increasingly critical due to the ever-growing model complexity and data scale. In particular, teleportation has emerged as a promising approach, which accelerates convergence of gradient descent-based methods by navigating within the loss invariant level set to identify parameters with advantageous geometric properties. Existing teleportation algorithms have primarily demonstrated their effectiveness in optimizing Multi-Layer Perceptrons (MLPs), but their extension to more advanced architectures, such as Convolutional Neural Networks (CNNs) and Transformers, remains challenging. Moreover, they often impose significant computational demands, limiting their applicability to complex architectures. To this end, we introduce an algorithm that projects the gradient of the teleportation objective function onto the input null space, effectively preserving the teleportation within the loss invariant level set and reducing computational cost. Our approach is readily generalizable from MLPs to CNNs, transformers, and potentially other advanced architectures. We validate the effectiveness of our algorithm across various benchmark datasets and optimizers, demonstrating its broad applicability."
    },
    {
        "title": "Boosting Document Layout Analysis with Graphic Multi-modal Data Fusion and Spatial Geometric Transformation",
        "link_suffix": "/forum?id=kmbU3EdLtS",
        "link": "https://openreview.net/forum?id=kmbU3EdLtS",
        "pdf_link": "https://openreview.net/pdf?id=kmbU3EdLtS",
        "keywords": "Multi-modal Data Fusion, Document Intelligence",
        "abstract": "Document layout analysis is essential for Document Intelligence, playing a pivotal role in automated understanding and processing of document content. Most existing approaches within this domain are predicated on computer vision techniques that concentrate on image modality, despite documents containing both rich visual and textual information. While recent advances in multi-modal approaches begin to incorporate word embeddings to enhance recognition capabilities, they also incur a substantial computational burden.  Moreover,  the diversity of document structures demands models with great robustness, especially during the document editing process.  In this paper, we introduce pluggable and efficient data pre-processing strategies to boost the layout analysis performance. Firstly, we discover that element categories depend on relative relationships and propose a Graphical Multi-modal Data Fusion technique, which constructs a graph to establish connections between disparate textual segments. Secondly, in terms of structural diversity of documents, we devise a Spatial Geometric Transformation strategy to improve model robustness against layout alterations. Our methods operate during the pre-processing phase, which facilitates straightforward integration with existing models to achieve significant accuracy increase with negligible extra computations. Experimental results show that our strategies illustrate State-Of-The-Art performance across multiple document layout analysis datasets. We will make the code publicly available shortly."
    },
    {
        "title": "Robust Federated Learning Frameworks Guarding Against Data Flipping Threats for Autonomous Vehicles",
        "link_suffix": "/forum?id=oA5GmyvMUY",
        "link": "https://openreview.net/forum?id=oA5GmyvMUY",
        "pdf_link": "https://openreview.net/pdf?id=oA5GmyvMUY",
        "keywords": "Federated Learning, Data Poisoning, Adversarial Attack",
        "abstract": "Federated Learning (FL) has become an established technique to facilitate privacy-preserving collaborative training across a multitude of clients. The ability to achieve collaborative learning from multiple parties containing an extensive volume of data while providing the essence of data privacy made it an attractive solution to address numerous challenges in sensitive data-driven fields such as autonomous vehicles (AVs). However, its decentralized nature exposes it to security threats, such as evasion and data poisoning attacks, where malicious participants can compromise training data. This paper addresses the challenge of defending federated learning systems against data poisoning attacks specifically focusing on data-flipping techniques in AVs by proposing a novel defense mechanism that combines anomaly detection with robust aggregation techniques. Our approach employs statistical outlier detection and model-based consistency checks to filter out compromised updates before they affect the global model. Experiments on benchmark datasets show that our method significantly enhances robustness by preventing nearly 15% of accuracy drop for our global model when confronted with a malicious participant and reduction the the attack success rate even when dealing with 20% of poisoning level. These findings provide a comprehensive solution to strengthen FL systems against adversarial threats."
    },
    {
        "title": "Latent Matrix Completion Model",
        "link_suffix": "/forum?id=pppyig2kYe",
        "link": "https://openreview.net/forum?id=pppyig2kYe",
        "pdf_link": "https://openreview.net/pdf?id=pppyig2kYe",
        "keywords": "Clustering, Union of Subspace, matrix completion, Image reconstruction",
        "abstract": "Large amounts of missing data are becoming increasingly ubiquitous in modern high-dimensional datasets. High-rank matrix completion (HRMC) uses the powerful union of subspace (UoS) model to handle these vast amounts of missing data. However, existing HRMC methods often fail when dealing with real data that does not follow the UoS model exactly. Here we propose a new approach: instead of finding a UoS that fits the observed data directly, we will find a UoS in a latent space that can fit a non-linear embedding of the original data. Embeddings of this sort are typically attained with deep architectures. However, the abundance of missing data impedes the training process, as the coordinates of the observed samples rarely overlap. We overcome this difficulty with a novel pseudo-completion layer (in charge of estimating the missing values) followed by an auto-encoder (in charge of finding the embedding) coupled with a self-expressive layer (that clusters data according to a UoS in the latent space). Our design reduces the exponential memory requirements typically induced by uneven patterns of missing data. We describe our architecture, model, loss functions, and training strategy. Our experiments on several real datasets show that our method consistently outperforms the state-of-the-art accuracy by more than a staggering 40%."
    },
    {
        "title": "RAGC: Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models",
        "link_suffix": "/forum?id=3XTw909oXt",
        "link": "https://openreview.net/forum?id=3XTw909oXt",
        "pdf_link": "https://openreview.net/pdf?id=3XTw909oXt",
        "keywords": "Copyright Protection, Ownership Verification, Retrieval-augmented Generation",
        "abstract": "Large language models (LLMs) are increasingly integrated into real-world applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with up-to-date and domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve backdoor or poisoning attacks, which introduce harmful behaviors (\\eg, generating incorrect outputs for verification), thereby compromising the LLM's reliability. To address these challenges, we propose \\name{} for harmless copyright protection of knowledge bases. Instead of manipulating the final output, \\name{} implants distinct verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our approach involves three main stages: (1) \\textbf{Generating CoTs}: For each verification question, we generate two CoTs, including a target CoT for building watermark behaviors; (2) \\textbf{Optimizing Watermark Phrases and Target CoTs}: We optimize them to minimize retrieval errors under the black-box setting of suspicious LLM, ensuring that the watermarked verification queries activate the target CoTs without being activated in non-watermarked ones; (3) \\textbf{Ownership Verification}: We exploit a pairwise Wilcoxon test to statistically verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \\name{} effectively protects knowledge bases against unauthorized usage while preserving the integrity and performance of the RAG."
    },
    {
        "title": "Chain-of-Jailbreak Attack for Image Generation Models via Editing Step by Step",
        "link_suffix": "/forum?id=V7PYbRzD0h",
        "link": "https://openreview.net/forum?id=V7PYbRzD0h",
        "pdf_link": "https://openreview.net/pdf?id=V7PYbRzD0h",
        "keywords": "Jailbreak Attack, Text-to-Image Models, Safety",
        "abstract": "Text-based image generation models, such as Stable Diffusion and DALL-E 3, hold significant potential in content creation and publishing workflows, making them the focus in recent years.\nDespite their remarkable capability to generate diverse and vivid images, considerable efforts are being made to prevent the generation of harmful content, such as abusive, violent, or pornographic material.\nTo assess the safety of existing models, we introduce a novel jailbreaking method called Chain-of-Jailbreak (CoJ) attack, which compromises image generation models through a step-by-step editing process.\nSpecifically, for malicious queries that cannot bypass the safeguards with a single prompt, we intentionally decompose the query into multiple sub-queries. The image generation models are then prompted to generate and iteratively edit images based on these sub-queries.\nTo evaluate the effectiveness of our CoJ attack method, we constructed a comprehensive dataset, CoJ-Bench, encompassing nine safety scenarios, three types of editing operations, and three editing elements.\nExperiments on four widely-used image generation services provided by GPT-4V, GPT-4o, Gemini 1.5 and Gemini 1.5 Pro, demonstrate that our CoJ attack method can successfully bypass the safeguards of models for over 60% cases, which significantly outperforms other jailbreaking methods (i.e., 14%).\nFurther, to enhance these models' safety against our CoJ attack method, we also propose an effective prompting-based method, Think Twice Prompting, that can successfully defend over 95% of CoJ attack.\nWe will release our dataset and code to facilitate the AI safety research."
    },
    {
        "title": "Confidence Elicitation: A New Attack Vector for Large Language Models",
        "link_suffix": "/forum?id=aTYexOYlLb",
        "link": "https://openreview.net/forum?id=aTYexOYlLb",
        "pdf_link": "https://openreview.net/pdf?id=aTYexOYlLb",
        "keywords": "adversarial attack, adversarial robustness, confidence elicitation.",
        "abstract": "A fundamental issue in deep learning has been adversarial robustness. As these systems have scaled, such issues have persisted. Currently, large language models (LLMs) with billions of parameters suffer from adversarial attacks just like their earlier, smaller counterparts. However, the threat models have changed. Previously, having gray-box access, where input embeddings or output logits/probabilities were visible to the user, might have been reasonable. However, with the introduction of closed-source models, no information about the model is available apart from the generated output. This means that current black-box attacks can only utilize the final prediction to detect if an attack is successful. In this work, we investigate and demonstrate the potential of attack guidance, akin to using output probabilities, while having only black-box access in a classification setting. This is achieved through the ability to elicit confidence from the model. We empirically show that the elicited confidence is calibrated and not hallucinated for current LLMs. By minimizing the elicited confidence, we can therefore increase the likelihood of misclassification. Our new proposed paradigm demonstrates promising state-of-the-art results on three datasets across two models (LLaMA 3 and Mistral V0.3) when comparing our technique to existing hard-label black-box attack methods that introduce word-level substitutions. The code is publicly available athttps://shorturl.at/s9DIr."
    },
    {
        "title": "LM4LV: A Frozen Large Language Model for Low-level Vision Tasks",
        "link_suffix": "/forum?id=ZAx5DxAucB",
        "link": "https://openreview.net/forum?id=ZAx5DxAucB",
        "pdf_link": "https://openreview.net/pdf?id=ZAx5DxAucB",
        "keywords": "Low-level vision, Large Language Model, Self-supervised Learning",
        "abstract": "The success of large language models (LLMs) has fostered a new research trend of multi-modality large language models (MLLMs), which changes the paradigm of various fields in computer vision. Though MLLMs have shown promising results in numerous vision-language tasks such as VQA and text-to-image, no work has demonstrated how low-level vision tasks can benefit from MLLMs. We find that most current MLLMs are blind to low-level features due to their design of vision modules, and thus are inherently incapable of solving low-level vision tasks. In this work, we purposeLM4LV, a framework that enables a FROZEN LLM to solve a range of low-level vision tasks without any multi-modal data or prior. This showcases the LLM's strong potential in low-level vision and bridges the gap between MLLMs and low-level vision tasks. We hope that this work can inspire new perspectives on LLMs and a deeper understanding of their mechanisms."
    },
    {
        "title": "On the Effectiveness of Discrete Representations in Sparse Mixture of Experts",
        "link_suffix": "/forum?id=RVPZJpmyGU",
        "link": "https://openreview.net/forum?id=RVPZJpmyGU",
        "pdf_link": "https://openreview.net/pdf?id=RVPZJpmyGU",
        "keywords": "Sparse mixture of experts, discrete representations, vector quantization, large language model",
        "abstract": "Sparse mixture of experts (SMoE) is an effective solution for scaling up model capacity without increasing the computational costs. A crucial component of SMoE is the router, responsible for directing the input to relevant experts; however, it also presents a major weakness, leading to routing inconsistencies and representation collapse issues. Instead of fixing the router like previous works, we propose an alternative that assigns experts to input via indirection, which employs the discrete representation of input that points to the expert. The discrete representations are learnt via vector quantization, resulting in a new architecture dubbed VectorQuantized Mixture of Experts (VQMoE). We provide theoretical support and empirical evidence demonstrating the VQMoE\u2019s ability to overcome the challenges present in traditional routers. Through extensive evaluations on both large language models and vision tasks for pre-training and fine-tuning, we show that VQMoE achieves a 28% improvement in robustness compared to other SMoE routing methods, while maintaining strong performance in fine-tuning tasks."
    },
    {
        "title": "AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models",
        "link_suffix": "/forum?id=jTEKTdI3K9",
        "link": "https://openreview.net/forum?id=jTEKTdI3K9",
        "pdf_link": "https://openreview.net/pdf?id=jTEKTdI3K9",
        "keywords": "Multi-modal Large Language Models, Hallucination in Large Language Models, Audio-visual Learning",
        "abstract": "Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world. In recognition of this fact, audio-visual LLMs have recently emerged. Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations, underscoring the need for reliable benchmarks. To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. Our benchmark includes tests for assessing hallucinations, as well as the cross-modal matching and reasoning abilities of these models. Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by cross-interactions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations."
    },
    {
        "title": "METHODS OF IMPROVING LLM TRAINING STABILITY",
        "link_suffix": "/forum?id=RL6R5ryuL5",
        "link": "https://openreview.net/forum?id=RL6R5ryuL5",
        "pdf_link": "https://openreview.net/pdf?id=RL6R5ryuL5",
        "keywords": "LLM, training stability",
        "abstract": "Training stability of large language models (LLMs) is an important research topic. Reproducing training instabilities can be costly, so we use a small language model with 830M parameters and experiment with higher learning rates to force models to diverge, as in Wortsman et al. (2024). One of the sources of training instability is the growth of logits in attention layers Dehghani et al. (2023). We extend the focus of the previous work [Dehghani et al. (2023),Wortsman et al. (2024)] and look not only at the magnitude of the logits but at all outputs of linear layers in the Transformer block. We observe that with a high learning rate the L2 norm of all linear layer outputs grow with each training step and the model diverges. Specifically we observe that QKV, Proj and FC2 layers have the largest growth of the output magnitude. This prompts us to explore several options: 1) apply layer normalization not only after QK layers (as it is done in [Dehghani et al. (2023), Wortsman et al. (2024)]) but after Proj and FC2 layers too; 2) apply layer normalization after the QKV layer (and remove pre normalization). 3) apply QK layer normalization together with softmax capping. We show that with the last two methods we can increase learning rate by 1.5x (without model divergence) in comparison to an approach based on QK layer normalization only Dehghani et al. (2023). Also we observe significant perplexity improvements for all three methods in comparison to the baseline model."
    },
    {
        "title": "Medical Vision Generalist: Unifying Medical Imaging Tasks in Context",
        "link_suffix": "/forum?id=EtJWnTnqku",
        "link": "https://openreview.net/forum?id=EtJWnTnqku",
        "pdf_link": "https://openreview.net/pdf?id=EtJWnTnqku",
        "keywords": "Medical Image Analysis, Generalist Models",
        "abstract": "This study presents Medical Vision Generalist (MVG), the first foundation model capable of handling various medical imaging tasks---such as cross-modal synthesis, image segmentation, denoising, and inpainting---within a unified image-to-image generation framework. Specifically, MVG employs an in-context generation strategy that standardizes the handling of inputs and outputs as images. By treating these tasks as an image generation process conditioned on prompt image-label pairs and input images, this approach enables a flexible unification of various tasks, even those spanning different modalities and datasets. To capitalize on both local and global context, we design a hybrid method combining masked image modeling with autoregressive training for conditional image generation. This hybrid approach yields the most robust performance across all involved medical imaging tasks. To rigorously evaluate MVG's capabilities, we curated the first comprehensive generalist medical vision benchmark, comprising 13 datasets and spanning four imaging modalities (CT, MRI, X-ray, and micro-ultrasound). Our results consistently etablish MVG's superior performance, outperforming existing vision generalists, such as Painter and LVM. Furthermore, MVG exhibits strong scalability, with its performance demonstrably improving when trained on a more diverse set of tasks, and can be effectively adapted to unseen datasets with only minimal task-specific samples. The code and the benchmark will be publicly available."
    },
    {
        "title": "Looking Inward: Language Models Can Learn About Themselves by Introspection",
        "link_suffix": "/forum?id=eb5pkwIB5i",
        "link": "https://openreview.net/forum?id=eb5pkwIB5i",
        "pdf_link": "https://openreview.net/pdf?id=eb5pkwIB5i",
        "keywords": "Introspection, Large Language Models, Model awareness, Self-simulation, Generalization, Capability Evaluations, AI safety",
        "abstract": "Humans acquire knowledge by observing the external world, but also by introspection. Introspection gives a person privileged access to their current state of mind (e.g. thoughts and feelings) that are not accessible to external observers. Can LLMs introspect? If they can, this would show that LLMs can acquire knowledge not contained in or inferable from training data.\nWe investigate a form of introspection in which LLMs predict properties of their own behavior in hypothetical situations. If a model M1 can introspect, it should outperform a different model M2 in predicting M1's behavior---even if M2 is trained on M1's ground-truth behavior.\nThe idea is that M1 has privileged access to its own behavioral tendencies, and this enables it to predict itself better than M2 (even if M2 is generally stronger).\nIn experiments with GPT-4, GPT-4o, and Llama-3 models, we find that the model M1 outperforms M2 in predicting itself, providing evidence for introspection. Further experiments and ablations provide additional evidence.\nOur results show that LLMs can offer reliable self-information independent of external data in certain domains. By demonstrating this, we pave the way for further work on introspection in more practical domains, which would have significant implications for model transparency and explainability."
    },
    {
        "title": "A New 3D Image Block Ranking Method Using Axial, Coronal and Sagittal Image Patch Rankings for Explainable Medical Imaging",
        "link_suffix": "/forum?id=ilGdLPy3mA",
        "link": "https://openreview.net/forum?id=ilGdLPy3mA",
        "pdf_link": "https://openreview.net/pdf?id=ilGdLPy3mA",
        "keywords": "convolutional neural networks, feature selection, gradcam, medical imaging, disease diagnosis, image classification",
        "abstract": "Although a 3D Convolutional Neural Network (CNN) has been applied to medical\nimaging in recent years, understanding the relationships among input 2D image\npatches, input 3D image blocks, extracted feature maps, and final diagnosis remains\na significant challenge, particularly in explainable medical imaging. To\nhelp address this challenge, firstly, we created a new Grad-CAM-based method\nusing feature selection to produce explainable heatmaps with a small number of\nhighlighted image patches corresponding to top-ranked features. Secondly, we\ndesigned a new 2D image patch ranking algorithm that leverages the newly defined\nfeature matrices and relevant statistical data from numerous heatmaps to\nreliably rank axial patches, coronal patches, and sagittal patches. Thirdly, we created\na novel 3D image block ranking algorithm to generate a \u201cBlock Ranking Map\n(BRM)\u201d by using the axial patch ranking scores, coronal patch ranking scores, and\nsagittal patch ranking scores. Lastly, we developed a hybrid 3D image block ranking\nalgorithm to generate a reliable hybrid BRM by using different block ranking\nscores generated by the 3D image block ranking algorithm using different top\nfeature sets. A preprocessed ADNI (Alzheimer\u2019s Disease (AD) Neuroimaging\nInitiative) dataset with 982 64\u00d764\u00d764 brain images with 4, 096 4\u00d74\u00d74 blocks\nfor three-class (cognitively normal class, mild cognitive impairment class, and AD\nclass) 3D image classification is used to extract 19640 axial images, 19640 coronal\nimages, and 19640 sagittal images for simulations. The associations between\nbrain areas and AD are generated efficiently by using information from ChatGPT\nand relevant publications. Simulation results show that the hybrid 3D image block\nranking algorithm is able to effectively identify 10 top-ranked blocks (i.e., 0.24%\nof all 4, 096 blocks) that are associated with the 16 brain areas that are in the six\nmost important brain regions associated with AD. A doctor may conveniently use\nthe hybrid BRM with axial, coronal, and sagittal views to better understand the\nrelationship between the top-ranked blocks and medical diagnosis so that he/she\ncan efficiently and effectively make a rational and explainable medical diagnosis."
    }
]