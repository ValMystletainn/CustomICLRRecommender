[{"title": "Adaptive Inference: Theoretical Limits and Opportunities for Efficient AI", "link_suffix": "/forum?id=hJDTuVQcQp", "link": "https://openreview.net/forum?id=hJDTuVQcQp", "pdf_link": "https://openreview.net/pdf?id=hJDTuVQcQp", "keywords": "Adaptive Inference, Efficient ML, Dynamic Neural Networks, Dynamic Routing, Computer Vision, Natural Language processing", "abstract": "With the commercial deployment of increasingly larger and more complex neural networks at the cloud and the edge in recent years, inference has become too costly in terms of compute workload worldwide. Adaptive inference methods, which dynamically adjust a neural network's size or structure during inference, offer a means to enhance efficiency of neural networks beyond what static network compression and optimization methods can fundamentally achieve.This paper introduces the first theoretical framework for quantifying the efficiency and performance gain opportunity size of adaptive inference algorithms. We provide new approximate and exact bounds for the achievable efficiency and performance gains, supported by empirical evidence demonstrating the potential for 10-100x efficiency improvements in both Computer Vision and Natural Language Processing tasks without incurring any performance penalties. Additionally, we offer insights on improving achievable efficiency gains through the optimal selection and design of adaptive inference state spaces.", "title_embedding_index": 9700, "title_abs_embedding_index": 9725}, {"title": "Differentiable Polygon Modeling for Object Instance Segmentation", "link_suffix": "/forum?id=HCOOQUcWiF", "link": "https://openreview.net/forum?id=HCOOQUcWiF", "pdf_link": "https://openreview.net/pdf?id=HCOOQUcWiF", "keywords": "Differentiable Polygon Modeling, Object Instance Segmentation, PolygonAlign", "abstract": "Differentiable polygon (boundary-/contour-based) modeling for object instance segmentation remains an open problem in computer vision and deep learning. It also has been under-explored in the deep learning era, compared with its counterpart, bit-mask (region-based) modeling. In this paper, we present a method of differentiable polygon-based instance segmentation. As commonly done in the prior art, we assume a fixed topology, i.e., the number of vertices, $K$ is predefined and fixed (e.g., $K=250$) in learning and inference. We address two modeling problems: i) The alignment between a predicted $K$-vertex polygon and a target ground-truth $L$-vertex polygon in learning, where $L$ varies significantly. We present PolygonAlign similar in spirit to RoIAlign used in bit-mask-based instance segmentation, which enables using a simple $\\ell_2$ norm as the vertex prediction loss function in learning. ii) The parameterization of a $K$-vertex polygon. We present a variant of the active contour model,  which consists of a learnable contour initialization module and an one-step vertex-aware refinement/updating module. The initialization is learned via an affine transformation decoupled vertex regression method. A polygon is parameterized by a translation vector, a rotation transformation matrix, and the vertex displacement vectors. In experiments, the proposed method is tested on the MS-COCO 2017 benchmark using the Sparse R-CNN framework. It obtains state-of-the-art performance compared with the prior art of polygon modeling methods. We also show the empirical upper-bound performance of the proposed method is much higher than all existing instance segmentation methods, which encourages further research on differentiable polygon modeling.", "title_embedding_index": 9701, "title_abs_embedding_index": 9726}, {"title": "Guided Stream of Search: Learning to Better Search with Language Models via Optimal Path Guidance", "link_suffix": "/forum?id=1YXkDXIqVw", "link": "https://openreview.net/forum?id=1YXkDXIqVw", "pdf_link": "https://openreview.net/pdf?id=1YXkDXIqVw", "keywords": "planning with language models, supervised fine-tuning with self-generated data, reinforcement learning fine-tuning", "abstract": "While language models have demonstrated impressive capabilities across a range of tasks, they still struggle with tasks that require complex planning and reasoning. Recent studies have proposed training language models on search processes rather than optimal solutions, resulting in better generalization performance even though search processes are noisy and even suboptimal. However, these studies overlook the value of optimal solutions, which can serve as step-by-step landmarks to guide more effective search. In this work, we explore how to leverage optimal solutions to enhance the search and planning abilities of language models. To this end, we propose guided stream of search (GSoS), which seamlessly incorporates optimal solutions into the self-generation process in a progressive manner, producing high-quality search trajectories. These trajectories are then distilled into the pre-trained model via supervised fine-tuning. Our approach significantly enhances the search and planning abilities of language models on Countdown, a simple yet challenging mathematical reasoning task. Notably, combining our method with RL fine-tuning yields further improvements, whereas previous supervised fine-tuning methods do not benefit from RL. Furthermore, our approach exhibits greater effectiveness than leveraging optimal solutions in the form of subgoal rewards.", "title_embedding_index": 9702, "title_abs_embedding_index": 9727}, {"title": "Proxy-FDA: Proxy-based Feature Distribution Alignment for Fine-tuning Vision Foundation Models without Forgetting", "link_suffix": "/forum?id=eIFHoPsIkw", "link": "https://openreview.net/forum?id=eIFHoPsIkw", "pdf_link": "https://openreview.net/pdf?id=eIFHoPsIkw", "keywords": "Proxy-FDA, robust fine-tuning, concept forgetting, vision foundation model", "abstract": "Vision foundation models pre-trained on massive data encode rich representations of real-world concepts, which can be adapted to downstream tasks by fine-tuning. However, fine-tuning foundation models on one task often leads to the issue of concept forgetting on other tasks, and this issue is exacerbated by the typically limited data for fine-tuning. Recent methods of robust fine-tuning aim to mitigate forgetting of prior knowledge without affecting the fine-tuning performance. Knowledge is often preserved by matching the original and fine-tuned model weights or feature pairs. However, such point-wise matching can be too strong, without explicit awareness of the feature neighborhood structures that encode rich knowledge as well. We propose a novel regularization method Proxy-FDA that explicitly preserves the structural knowledge in feature space. Proxy-FDA performs Feature Distribution Alignment (using nearest neighbor graphs) between the pre-trained and fine-tuned feature spaces, and the alignment is further improved by informative proxies that are generated dynamically to increase data diversity. We show in end-to-end fine-tuning experiments that Proxy-FDA significantly reduces concept forgetting, and we find a strong correlation between forgetting and a distributional distance metric (in comparison to L2 distance). We further demonstrate Proxy-FDA's utility in both few-shot (based on prompt tuning) and continual fine-tuning settings, where we achieve consistent gains over the corresponding baselines.", "title_embedding_index": 9703, "title_abs_embedding_index": 9728}, {"title": "COMiX: Compositional explanations using prototypes", "link_suffix": "/forum?id=HXwrppoSPc", "link": "https://openreview.net/forum?id=HXwrppoSPc", "pdf_link": "https://openreview.net/pdf?id=HXwrppoSPc", "keywords": "explanation-by-design, class-defining-features", "abstract": "Aligning machine representations with human understanding is key to improving interpretability of machine learning (ML) models. \nWhen classifying a new image, humans often explain their decisions by decomposing the image into concepts and pointing to corresponding regions in familiar images.\nCurrent ML explanation techniques typically either trace decision-making processes to reference prototypes, generate attribution maps highlighting feature importance, or incorporate intermediate bottlenecks designed to align with human-interpretable concepts.\nThe proposed method, named COMiX, classifies an image by decomposing it into regions based on learned concepts and tracing each region to corresponding ones in images from the training dataset, assuring that explanations fully represent the actual decision-making process. We dissect the test image into selected internal representations of a neural network to derive prototypical parts (primitives) and match them with the corresponding primitives derived from the training data. \nIn a series of qualitative and quantitative experiments, we theoretically prove and demonstrate that our method, in contrast to \\textit{post hoc} analysis, provides fidelity of explanations and shows that the efficiency is competitive with other inherently interpretable architectures. Notably, it shows substantial improvements in fidelity and sparsity metrics, including $48.82%$ improvement in the C-insertion score on the ImageNet dataset over the best state-of-the-art baseline.", "title_embedding_index": 9704, "title_abs_embedding_index": 9729}, {"title": "MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models", "link_suffix": "/forum?id=H9UnNgdq0g", "link": "https://openreview.net/forum?id=H9UnNgdq0g", "pdf_link": "https://openreview.net/pdf?id=H9UnNgdq0g", "keywords": "Medical foundation models, Benchmarking, Vision encoding, Radiology", "abstract": "Multimodal Large Language Models (MLLMs) have tremendous potential to improve the accuracy, availability, and cost-effectiveness of healthcare by providing automated solutions or serving as aids to medical professionals. Despite promising first steps in developing medical MLLMs in the past few years, their capabilities and limitations are not well understood. Recently, many benchmark datasets have been proposed that test the general medical knowledge of such models across a variety of medical areas. However, the systematic failure modes and vulnerabilities of such models are severely underexplored with most medical benchmarks failing to expose the shortcomings of existing models in this safety-critical domain. In this paper, we introduce MediConfusion, a challenging medical Visual Question Answering (VQA) benchmark dataset, that probes the failure modes of medical MLLMs from a vision perspective. We reveal that state-of-the-art models are easily confused by image pairs that are otherwise visually dissimilar and clearly distinct for medical experts. Strikingly, all available models (open-source or proprietary) achieve performance below random guessing on MediConfusion, raising serious concerns about the reliability of existing medical MLLMs for healthcare deployment. We also extract common patterns of model failure that may help the design of a new generation of more trustworthy and reliable MLLMs in healthcare.", "title_embedding_index": 9705, "title_abs_embedding_index": 9730}, {"title": "Revisiting and Expanding Targeted Universal Adversarial Perturbations", "link_suffix": "/forum?id=eDduYIUgHk", "link": "https://openreview.net/forum?id=eDduYIUgHk", "pdf_link": "https://openreview.net/pdf?id=eDduYIUgHk", "keywords": "Universal Adversarial Perturbations, Ordered Top-K AttacKs, AllAttacK", "abstract": "Universal adversarial perturbations (UAPs) have deepened the vulnerability concern of Deep Neural Networks (DNNs) after the initial intriguing discovery of vanilla single-model-single-image adversarial attacks. However, the landscape of UAPs has not been thoroughly investigated. In this paper, we revisit and expand UAPs for white-box targeted attacks along three axes simultaneously: the model-axis, the data-axis, and the target-axis. For the target-axis, we adopt the most aggressive ordered top-$K$ attack protocol ($K\\geq 1$) to expand the traditional top-$1$ attack setting in the prior art of learning UAPs. Our proposed method is thus dubbed as AllAttacK. \nIn implementation, our AllAttacK is built on two state-of-the-art single-model-single-image ordered top-$K$ attack methods, the KL divergence based adversarial distillation method and the more recently proposed quadratic programming based method. We propose a simple yet effective joint  mini-data-batch and mini-model-batch optimization strategy in learning UAPs for a large number of models (e.g., up to 18 disparate DNNs) and a large number of images (e.g., 1000 images).  We test our AllAttacK on the ImageNet-1k classification task using an ensemble of disparate models such as Convolutional Neural Networks and their adversarially-robustified versions, Vision Transformers, CLIP vision encoders, and MLP-Mixers. Our learned AllAttacK perturbations are doubly transferable across training and testing models, and across training and testing images, and they also show intriguing yet sensible looking.", "title_embedding_index": 9706, "title_abs_embedding_index": 9731}, {"title": "Learning and aligning single-neuron invariance manifolds in visual cortex", "link_suffix": "/forum?id=kbjJ9ZOakb", "link": "https://openreview.net/forum?id=kbjJ9ZOakb", "pdf_link": "https://openreview.net/pdf?id=kbjJ9ZOakb", "keywords": "neural invariances, invariance manifold, MEI, implicit neural representations, contrastive learning, invariance alignment, clustering, visual cortex, macaque V1, primary visual cortex", "abstract": "Understanding how sensory neurons exhibit selectivity to certain features and invariance to others is central to uncovering the computational principles underlying robustness and generalization in visual perception. Existing methods for characterizing selectivity and invariance mainly identify single or finite discrete sets of stimuli. Since these are only isolated measurements from an underlying continuous manifold, characterizing invariance properties accurately and comparing them across neurons with varying receptive field size, position, and orientation, becomes challenging. Consequently, a systematic analysis of invariance types at the population level remains under-explored. We introduce a novel method to accurately identify and align invariance manifolds of visual sensory neurons, overcoming these challenges. Our approach first learns the continuous  invariance manifold of stimuli that maximally excite a neuron modeled by a response-predicting deep neural network. It then learns an affine transformation on the pixel coordinates such that the same manifold activates another neuron as strongly as possible, effectively aligning their invariance manifolds spatially. This alignment provides a principled way to quantify and compare neuronal invariances irrespective of receptive field differences. Using simulated neurons, we demonstrate that our method accurately learns and aligns known invariance manifolds, robustly identifying functional clusters. When applied to macaque V1 neurons, it reveals functional clusters of neurons, including simple and complex cells. Overall, our method enables systematic, quantitative exploration of the neural invariance landscape, to gain new insights into the functional properties of visual sensory neurons.", "title_embedding_index": 9707, "title_abs_embedding_index": 9732}, {"title": "Latent Trajectory: A New Framework for Actor-Critic Reinforcement Learning with Uncertainty Quantification", "link_suffix": "/forum?id=ve5Omkxc13", "link": "https://openreview.net/forum?id=ve5Omkxc13", "pdf_link": "https://openreview.net/pdf?id=ve5Omkxc13", "keywords": "Reinforcement learning, Stochastic gradient MCMC, Bayesian sampling, Uncertainty quantification", "abstract": "Uncertainty quantification for deep neural networks is crucial for building reliable modern AI models. This challenge is particularly pronounced in deep reinforcement learning, where agents continuously learn from their interactions with stochastic environments, and the uncertainty of the value function is a key concern for ensuring reliable and robust RL applications. The complexity increases in actor-critic methods, as the training process alternates between optimizing the actor and critic networks, whose optimization nature makes the uncertainty of the value function hard to be quantified. \nTo address this issue, we introduce a novel approach to RL training that conceptualizes transition trajectories as latent variables. Building on this framework, we propose an adaptive Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) algorithm for training deep actor-critic models. This new training method allows for the implicit integration of latent transition trajectories, resulting in a trajectory-independent training process. We provide theoretical guarantees for the convergence of our algorithm and offer empirical evidence showing improvements in both performance and robustness of the deep actor-critic model under our Latent Trajectory Framework (LTF). Furthermore, this framework enables accurate uncertainty quantification for the value function of the RL system, paving the way for more reliable and robust RL applications.", "title_embedding_index": 9708, "title_abs_embedding_index": 9733}, {"title": "Time-Accurate Speech Rich Transcription with Non-Fluencies", "link_suffix": "/forum?id=gHPUXP51L0", "link": "https://openreview.net/forum?id=gHPUXP51L0", "pdf_link": "https://openreview.net/pdf?id=gHPUXP51L0", "keywords": "Non-fluency, Dysfluency, Alignment, speech transcription, clinical", "abstract": "Speech is a hierarchical collection of text, prosody, emotions, dysfluencies, etc. Automatic transcription of speech that goes beyond text (words) is an underexplored problem.\nWe focus on transcribing speech along with non-fluencies (dysfluencies). The current state-of-the-art pipeline \\citep{lian2024ssdmscalablespeechdysfluency} suffers from complex architecture design, training complexity, and significant shortcomings in the local sequence aligner, and it does not explore in-context learning capacity. In this work, we propose SSDM 2.0, which tackles those shortcomings via four main contributions:\n(1) We propose a novel \\textit{neural articulatory flow} to derive more scalable speech representations.\n(2) We developed a \\textit{full-stack connectionist subsequence aligner} that captures all types of dysfluencies.\n(3) We introduced a mispronunciation prompt pipeline and consistency learning module into LLM to leverage dysfluency \\textit{in-context pronunciation learning} abilities.\n(4) We curated Libri-Dys \\citep{lian2024ssdmscalablespeechdysfluency} and open-sourced the current largest-scale co-dysfluency corpus, \\textit{Libri-Co-Dys}, for future research endeavors.\nOverall, SSDM 2.0 outperforms SSDM and all other dysfluency transcription models by a large margin. See our project demo page at \\url{https://srnf2.github.io/}.", "title_embedding_index": 9709, "title_abs_embedding_index": 9734}, {"title": "Improving Generalization of Meta Reinforcement Learning via Explanation", "link_suffix": "/forum?id=vTLLyVCsrD", "link": "https://openreview.net/forum?id=vTLLyVCsrD", "pdf_link": "https://openreview.net/pdf?id=vTLLyVCsrD", "keywords": "explainable meta reinforcement learning; meta reinforcement learning generalization", "abstract": "Meta reinforcement learning learns a meta-prior (e.g., meta-policy) from a set of training tasks, such that the learned meta-prior can efficiently adapt to all the tasks in a task distribution. However, it has been observed in literature that the learned meta-prior usually has imbalanced generalization, i.e., it adapts well to some tasks but adapts poorly to some other tasks. This paper aims to explain why certain tasks are poorly adapted and, more importantly, use this explanation to improve generalization. Our methodology has two parts. The first part identifies ``critical\" training tasks that are most important to achieve good performance on those poorly-adapted tasks. An explanation of the poor generalization is that the meta-prior does not pay enough attention to the critical training tasks. To improve generalization, the second part formulates a bi-level optimization problem where the upper level learns how to augment the critical training tasks such that the meta-prior can pay more attention to the critical tasks, and the lower level computes the meta-prior distribution corresponding to the current augmentation. We propose an algorithm to solve the bi-level optimization problem and theoretically guarantee that (1) the algorithm converges at the rate of $O(1/\\sqrt{K})$, (2) the learned augmentation makes the meta-prior focus more on the critical training tasks, and (3) the generalization improves after the task augmentation. We use two real-world experiments and three MuJoCo experiments to show that our algorithm improves the generalization and outperforms state-of-the-art baselines.", "title_embedding_index": 9710, "title_abs_embedding_index": 9735}, {"title": "Mutual Effort for Efficiency: A Similarity-based Token Pruning for Vision Transformers in Self-Supervised Learning", "link_suffix": "/forum?id=GTcEe5fayC", "link": "https://openreview.net/forum?id=GTcEe5fayC", "pdf_link": "https://openreview.net/pdf?id=GTcEe5fayC", "keywords": "Efficient Self-Supervised Learning, Vision Transformer, Token Pruning", "abstract": "Self-supervised learning (SSL) offers a compelling solution to the challenge of extensive labeled data requirements in traditional supervised learning.\nWith the proven success of Vision Transformers (ViTs) in supervised tasks, there is increasing interest in adapting them for SSL frameworks. However, the high computational demands of SSL pose substantial challenges, particularly on resource-limited platforms like edge devices, despite its ability to achieve high accuracy without labeled data.\nRecent studies in supervised learning have shown that token pruning can reduce training costs by removing less informative tokens without compromising accuracy. However, SSL\u2019s dual-branch encoders make traditional single-branch pruning strategies less effective, as they fail to account for the critical cross-branch similarity information, leading to reduced accuracy in SSL.\nTo this end, we introduce SimPrune, a novel token pruning strategy designed for ViTs in SSL. SimPrune leverages cross-branch similarity information to efficiently prune tokens, retaining essential semantic information across dual branches. Additionally, we incorporate a difficulty-aware pruning strategy to further enhance SimPrune's effectiveness.\nExperimental results show that our proposed approach effectively reduces training computation while maintaining accuracy. Specifically, our approach offers 24% savings in training costs compared to SSL baseline, without sacrificing accuracy.", "title_embedding_index": 9711, "title_abs_embedding_index": 9736}, {"title": "Towards counterfactual fairness thorough auxiliary variables", "link_suffix": "/forum?id=GpUv1FvZi1", "link": "https://openreview.net/forum?id=GpUv1FvZi1", "pdf_link": "https://openreview.net/pdf?id=GpUv1FvZi1", "keywords": "Counterfactual, Fairness, Auxiliary variables", "abstract": "The challenge of balancing fairness and predictive accuracy in machine learning models, especially when sensitive attributes such as race, gender, or age are considered, has motivated substantial research in recent years. Counterfactual fairness ensures that predictions remain consistent across counterfactual variations of sensitive attributes, which is a crucial concept in addressing societal biases. \nHowever, existing counterfactual fairness approaches usually overlook intrinsic information about sensitive features, limiting their ability to achieve fairness while simultaneously maintaining performance. To tackle this challenge, we introduce EXOgenous Causal reasoning (EXOC), a novel causal reasoning framework motivated by exogenous variables. It leverages auxiliary variables to uncover intrinsic properties that give rise to sensitive attributes. Our framework explicitly defines an auxiliary node and a control node that contribute to counterfactual fairness and control the information flow within the model. Our evaluation, conducted on synthetic and real-world datasets, validates EXOC's superiority, showing that it outperforms state-of-the-art approaches in achieving counterfactual fairness without sacrificing accuracy.", "title_embedding_index": 9712, "title_abs_embedding_index": 9737}, {"title": "Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "link_suffix": "/forum?id=3VD92FuNCd", "link": "https://openreview.net/forum?id=3VD92FuNCd", "pdf_link": "https://openreview.net/pdf?id=3VD92FuNCd", "keywords": "Large Language Models, Interpretability, AI Safety", "abstract": "Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. \nHowever, a way to quantitatively and systematically analyze its effect on individual outputs is still lacking.\nIn this work, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. \nWe introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component.\nEmpirically, we find that one can steer model behavior and performance by up- or down-scaling the fine-tuning component during the forward pass.\nMotivated by this finding and our theoretical analysis, we define the Tuning Contribution ($\\mathrm{TuCo}$) in terms of the ratio of the magnitudes fine-tuning component and the pre-training component.\nWe find that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces the Tuning Contribution, and that $\\mathrm{TuCo}$ is consistently lower on prompts where the attacks succeed compared to ones where they don't. \nThis suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of these attacks.\nIn summary, $\\mathrm{TuCo}$ enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.", "title_embedding_index": 9713, "title_abs_embedding_index": 9738}, {"title": "Multimodal Instruction Tuning with Hybrid State Space Models", "link_suffix": "/forum?id=cagNCwQEEN", "link": "https://openreview.net/forum?id=cagNCwQEEN", "pdf_link": "https://openreview.net/pdf?id=cagNCwQEEN", "keywords": "multimodal, hybrid SSM, Jamba", "abstract": "Handling lengthy context is crucial for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs) in applications such as processing high-resolution images or high frame rate videos. The rise in image resolution and frame rate substantially increases computational demands due to the increased number of input tokens. This challenge is further exacerbated by the quadratic complexity with respect to sequence length of the self-attention mechanism. Most prior works either pre-train models with long contexts, overlooking the efficiency problem, or attempt to reduce the context length via downsampling (e.g., identify the key image patches or frames) to decrease the context length, which may result in information loss. To circumvent this issue while keeping the remarkable effectiveness of MLLMs, we propose a novel approach using a hybrid transformer-MAMBA model to efficiently handle long contexts in multimodal applications. Our multimodal model can effectively process long context input exceeding 100k tokens, outperforming existing models across various benchmarks. Remarkably, our model enhances inference efficiency for high-resolution images and high-frame-rate videos by about 4 times compared to current models, with efficiency gains increasing as image resolution or video frames rise. Furthermore, our model is the first to be trained on low-resolution images or low-frame-rate videos while being capable of inference on high-resolution images and high-frame-rate videos, offering flexibility for inference in diverse scenarios.", "title_embedding_index": 9714, "title_abs_embedding_index": 9739}, {"title": "Synergistic Weak-Strong Collaboration by Aligning Preferences", "link_suffix": "/forum?id=3iJ7eSj2rE", "link": "https://openreview.net/forum?id=3iJ7eSj2rE", "pdf_link": "https://openreview.net/pdf?id=3iJ7eSj2rE", "keywords": "Weak-Strong Model Collaboration, Preferences Tuning, Large Language Model", "abstract": "Current Large Language Models (LLMs) demonstrate exceptional general reasoning and problem-solving abilities but often struggle with specialized tasks or domains requiring proprietary information due to their generalized training and size constraints. Fine-tuning large models for every specific domain is impractical because of inaccessibility to black-box model parameters and high computational costs. We explore a solution to this challenge: can a collaborative framework between a specialized weak model and a general strong model effectively extend LLMs' capabilities to niche but critical tasks? We propose a dynamic interaction where the weak model, tailored to specific domains, generates detailed initial drafts and background information, while the strong model refines and enhances these drafts using its advanced reasoning skills. To optimize this collaboration, we introduce a feedback loop by fine-tuning the weak model based on the strong model's preferences, fostering an adaptive and synergistic relationship. We validate our framework through experiments on three datasets. We find that the collaboration significantly outperforms each model alone by leveraging complementary strengths. Moreover, fine-tuning the weak model with strong model's preference further enhances overall performance.\nOur collaborative approach achieves an average F1 score improvement of 3.24% over the weak model alone and 12.17% over the strong model alone across all benchmarks.", "title_embedding_index": 9715, "title_abs_embedding_index": 9740}, {"title": "Regularity explains emergence", "link_suffix": "/forum?id=4mni4W1ZXy", "link": "https://openreview.net/forum?id=4mni4W1ZXy", "pdf_link": "https://openreview.net/pdf?id=4mni4W1ZXy", "keywords": "large language model, emergence ability, approximation, scaling law, regularity", "abstract": "We investigate the mechanisms behind emergence in large language models from the viewpoint of the regularity of the optimal response function $f^*$ on the space of prompt tokens. Based on theoretical justification, we provide an interpretation that the derivatives of $f^*$ are in general unbounded and the model gives up reasoning in regions where the derivatives are large. In such regions, instead of predicting $f^*$, the model predicts a smoothified version obtained via an averaging operator. The threshold on the norm of derivatives for regions that are given up increases together with the number of parameters $N$, causing emergence. The relation between regularity and emergence is supported by experiments on arithmetic tasks such as multiplication and summation and other tasks. Our interpretation also shed light on why fine-tuning and Chain-of-Thought can significantly improves LLM performance.", "title_embedding_index": 9716, "title_abs_embedding_index": 9741}, {"title": "Augmenting Offline Reinforcement Learning with State-only Interactions", "link_suffix": "/forum?id=r27Nwu0t86", "link": "https://openreview.net/forum?id=r27Nwu0t86", "pdf_link": "https://openreview.net/pdf?id=r27Nwu0t86", "keywords": "Offline Reinforcement Learning, Generative Model, Data Augmentation, Trajectory Stitching", "abstract": "Batch offline data have been shown considerably beneficial for reinforcement learning. Their benefit is further amplified by upsampling with generative models. In this paper, we consider a novel opportunity where interaction with environment is feasible, but only restricted to observations, i.e.no rewardfeedback is available. This setting is realistic, because simulators or even real cyber-physical systems are often accessible, while in contrast reward is often difficult or expensive to obtain, similar to imitation learning settings. As a result, the learner must make best sense of the offline data to synthesize the most sample-efficient scheme of querying the transition of observation. Our method first leverages online interactions to generate high-return trajectories via conditional diffusion models. They are then blended with the original offline trajectories through a stitching algorithm, and the resulting augmented data is applied to downstream reinforcement learner. Superior empirical performance is demonstrated over state-of-the-art data augmentation methods that are extended to utilize observation-only interactions.", "title_embedding_index": 9717, "title_abs_embedding_index": 9742}, {"title": "Spatio-Temporal Dependency-Aware Neuron Optimization for Spiking Neural Networks", "link_suffix": "/forum?id=eN4g4cjFX1", "link": "https://openreview.net/forum?id=eN4g4cjFX1", "pdf_link": "https://openreview.net/pdf?id=eN4g4cjFX1", "keywords": "Neuromorphic computation, spiking neural networks, temporal dependency", "abstract": "As a biologically inspired computing paradigm, Spiking Neural Networks (SNNs) process information through discrete spike sequences, mimicking the brain's temporal dynamics and energy efficiency. The combination of backpropagation through time (BPTT) and direct input encoding (i.e., feeding decimal data directly into the network) has emerged as the mainstream training approach for SNNs. However, this combination introduces varying temporal dependency requirements across the network\u2019s spatial dimension. These differences are often neglected in existing studies, which typically apply uniform temporal dependency configurations throughout the network. Consequently, this could result in missing key gradients or introducing redundant ones in the temporal dimension, ultimately affecting the network's performance. To address this gap, we propose a novel Spatio-Temporal Dependency-Aware Neuron Optimization (ST-DANO) method for SNNs, which consists of two key components: neuron design and neuron search. Specifically, to overcome the limitations of traditional Leaky Integrate-and-Fire (LIF) neurons in adapting to varying temporal dependencies, we designed two variants, Long-LIF and Short-LIF, which improve the neuron's ability to capture long-term and short-term dependencies, respectively, by dynamic modulation of membrane potential thresholds and time constants. After validating our neuron designs through ablation studies, we developed a layer-wise neuron search strategy that automatically selects the optimal neuron type for each layer to ensure optimal temporal dependency configurations across the network. Extensive experiments on static and neuromorphic datasets demonstrate that ST-DANO can effectively adapt to temporal dependency differences across the spatial dimension in SNNs under various time-step configurations. The resulting architectures surpass state-of-the-art performance, achieving a remarkable 83.90% accuracy on the DVS-CIFAR-10 dataset\u2014a more than 5% improvement over the baseline.", "title_embedding_index": 9718, "title_abs_embedding_index": 9743}, {"title": "Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement", "link_suffix": "/forum?id=V0GlKhMLFl", "link": "https://openreview.net/forum?id=V0GlKhMLFl", "pdf_link": "https://openreview.net/pdf?id=V0GlKhMLFl", "keywords": "diversity, data selection, training efficiency, iterative refinement", "abstract": "Finetuning large language models on instruction data is an important step in enriching the knowledge learned during pre-training and improving instruction-following capabilities. \nAs the number of instruction datasets continues to grow, selecting the right data to achieve optimal results becomes increasingly important.\nIn this work, we ask a prominent question: How can we determine the optimal subset of data for effective training?\nWhile much of the existing research primarily emphasizes local criteria, such as instance quality, for subset selection, we argue that a global approach focused on data diversity is more critical.\nOur approach utilizes $k$-means clustering to ensure that the selected subset effectively represents the full dataset.\nWe propose an iterative refinement method inspired by active learning techniques to resample instances from clusters, with the importance and sampling weight of each cluster being reassessed in every training iteration.\nThis method allows us to reduce the effect of outliers and automatically filter out clusters containing low-quality data.\nThrough extensive evaluation across natural language reasoning, general world knowledge, code and math reasoning tasks, and by fine-tuning models from various families, we observe consistent improvements, achieving a 7% increase over the random selection and a 3.8% improvement over state-of-the-art sampling methods.\nOur work highlights the significance of diversity-first sampling when finetuning LLMs to enhance performance across a broad array of evaluation tasks. Our code is submitted as supplementary materials.", "title_embedding_index": 9719, "title_abs_embedding_index": 9744}, {"title": "Monty Hall and Optimized Conformal Prediction to Improve Decision-Making with LLMs", "link_suffix": "/forum?id=9poxbngJzR", "link": "https://openreview.net/forum?id=9poxbngJzR", "pdf_link": "https://openreview.net/pdf?id=9poxbngJzR", "keywords": "Large Language Models, Foundation Models, Uncertainty Quantification, Conformal Prediction, Multiple Choice Question Answering, Tool Usage Learning, Prompt Engineering, Monty Hall", "abstract": "Large language models (LLMs) are empowering decision-making in open-world agents in several applications, including tool or API usage and answering multiple choice questions (MCQs). However, they often make overconfident, incorrect predictions, which can be risky in high-stakes settings like healthcare and finance. To mitigate these risks, recent works have used conformal prediction (CP), a model-agnostic framework for distribution-free uncertainty quantification. CP transforms a \\emph{score function} into prediction sets that contain the true answer with high probability. While CP provides this coverage guarantee for arbitrary scores, the score quality significantly impacts prediction set sizes. Prior works have relied on LLM logits or other heuristic scores, lacking quality guarantees. We address this limitation by introducing CP-OPT, an optimization framework to learn scores that minimize set sizes while maintaining coverage. Furthermore, inspired by the Monty Hall problem, we extend CP's utility beyond uncertainty quantification to improve accuracy. We propose a method called \\emph{conformal revision of questions} (CROQ) to revise the problem by narrowing down the available choices to those in the prediction set. The coverage guarantee of CP ensures that the correct choice is in the revised question prompt with high probability, while the smaller number of choices increases the LLM's chances of answering it correctly. Experiments on the MMLU,  ToolAlpaca, and TruthfulQA datasets with Llama-3 and Phi-3 models show that optimized CP scores reduce set sizes while maintaining coverage guarantee, and CROQ shows significant improvement in accuracy over the standard inference procedure.", "title_embedding_index": 9720, "title_abs_embedding_index": 9745}, {"title": "Theoretical Convergence Analysis for Hilbert Space MCMC with Score-based Priors for Nonlinear Bayesian Inverse Problems", "link_suffix": "/forum?id=VpOwviiYxf", "link": "https://openreview.net/forum?id=VpOwviiYxf", "pdf_link": "https://openreview.net/pdf?id=VpOwviiYxf", "keywords": "theory paper, theoretical convergence analysis, nonlinear inverse problems, bayesian inference, hilbert space, Langevin MCMC, score-based generative models", "abstract": "In recent years, several works have explored the use of score-based generative models as expressive priors in Markov chain Monte Carlo (MCMC) algorithms for provable posterior sampling, even in the challenging case of nonlinear Bayesian inverse problems. However, these approaches have been mostly limited to finite-dimensional approximations, while the original problems are  typically defined in function spaces of infinite dimension. It is well known that  algorithms designed for finite-dimensional settings can encounter theoretical and practical issues when applied to infinite-dimensional objects, such as an inconsistent behavior across different discretizations. In this work, we address this limitation by leveraging the recently developed framework for score-based generative models in Hilbert spaces to learn an infinite-dimensional score, which we use as a prior in a function-space Langevin-type MCMC algorithm, providing theoretical guarantees for convergence in the context of nonlinear Bayesian inverse problems. Crucially, we prove that controlling the approximation error of the score is not only essential for ensuring convergence but also that modifying the standard score-based Langevin MCMC through the selection of an appropriate preconditioner is necessary. Our analysis shows how the control over the score approximation error influences the design of the preconditioner---an aspect unique to the infinite-dimensional setting.", "title_embedding_index": 9721, "title_abs_embedding_index": 9746}, {"title": "Towards Interpretable, Sequential Multiple Instance Learning: An Application to Clinical Imaging", "link_suffix": "/forum?id=lo9HMoGNwQ", "link": "https://openreview.net/forum?id=lo9HMoGNwQ", "pdf_link": "https://openreview.net/pdf?id=lo9HMoGNwQ", "keywords": "Multiple Instance Learning, Sequential Learning, Binary Classification, Medical Imaging, Uncertainty, Interpretability", "abstract": "This work introduces the Sequential Multiple Instance Learning (SMIL) framework, addressing the challenge of interpreting sequential, variable-length sequences of medical images with a single diagnostic label. Diverging from traditional MIL approaches that treat image sequences as unordered sets, SMIL systematically integrates the sequential nature of clinical imaging. We develop a bidirectional Transformer architecture, BiSMIL, that optimizes for both early and final prediction accuracies through a novel training procedure to balance diagnostic accuracy with operational efficiency. We evaluate BiSMIL on three medical image datasets to demonstrate that it simultaneously achieves state-of-the-art final accuracy and superior performance in early prediction accuracy, requiring 30-50% fewer images for a similar level of performance compared to existing models. Additionally, we introduce SMILU, an interpretable uncertainty metric that outperforms traditional metrics in identifying challenging instances.", "title_embedding_index": 9722, "title_abs_embedding_index": 9747}, {"title": "Improving Gaussian Splatting with Localized Points Management", "link_suffix": "/forum?id=FbbusgKmSW", "link": "https://openreview.net/forum?id=FbbusgKmSW", "pdf_link": "https://openreview.net/pdf?id=FbbusgKmSW", "keywords": "3D Reconstruction; 3D Gaussian Splatting; Novel View Synthesis", "abstract": "Point management is critical for optimizing 3D Gaussian Splatting models, as point initiation (e.g., via structure from motion) is often distributionally inappropriate. Typically, Adaptive Density Control (ADC) algorithm is adopted, leveraging view-averaged gradient magnitude thresholding for point densification, opacity thresholding for pruning, and regular all-points opacity reset. We reveal that this strategy is limited in tackling intricate/special image regions (e.g., transparent) due to inability of identifying all 3D zones requiring point densification, and lacking an appropriate mechanism to handle ill-conditioned points with negative impacts (occlusion due to false high opacity). To address these limitations, we propose a  Localized Point Management (LPM) strategy, capable of identifying those error-contributing zones in greatest need for both point addition and geometry calibration. Zone identification is achieved by leveraging the underlying multiview geometry constraints, subject to image rendering errors. We apply point densification in the identified zones and then reset the opacity of the points in front of these regions, creating a new opportunity to correct poorly conditioned points. Serving as a versatile plugin, LPM can be seamlessly integrated into existing static 3D and dynamic 4D Gaussian Splatting models. Experimental evaluations validate the efficacy of our LPM in boosting a variety of existing 3D/4D models both quantitatively and qualitatively.  Notably, LPM improves both static 3DGS and dynamic SpaceTimeGS to achieve state-of-the-art rendering quality while retaining real-time speeds, excelling on challenging datasets such as Tanks & Temples and the Neural 3D Video dataset.", "title_embedding_index": 9723, "title_abs_embedding_index": 9748}, {"title": "Feedback Schr\u00f6dinger Bridge Matching", "link_suffix": "/forum?id=k3tbMMW8rH", "link": "https://openreview.net/forum?id=k3tbMMW8rH", "pdf_link": "https://openreview.net/pdf?id=k3tbMMW8rH", "keywords": "Diffusion models, Schr\u00f6dinger bridge, Distribution matching, Semi-Supervised Learning", "abstract": "Recent advancements in diffusion bridges for distribution transport problems have heavily relied on matching frameworks, yet existing methods often face a trade-off between scalability and access to optimal pairings during training. \nFully unsupervised methods make minimal assumptions but incur high computational costs, limiting their practicality. On the other hand, imposing full supervision of the matching process with optimal pairings improves scalability, however, it can be infeasible in most applications.\nTo strike a balance between scalability and minimal supervision, we introduce Feedback Schr\u00f6dinger Bridge Matching (FSBM), a novel semi-supervised matching framework that incorporates a small portion ($<8%$ of the entire dataset) of pre-aligned pairs as state feedback to guide the transport map of non-coupled samples, thereby significantly improving efficiency. This is achieved by formulating a static Entropic Optimal Transport (EOT) problem with an additional term capturing the semi-supervised guidance. The generalized EOT objective is then recast into a dynamic formulation to leverage the scalability of matching frameworks. Extensive experiments demonstrate that FSBM accelerates training and enhances generalization by leveraging coupled pairs' guidance, opening new avenues for training matching frameworks with partially aligned datasets.", "title_embedding_index": 9724, "title_abs_embedding_index": 9749}]