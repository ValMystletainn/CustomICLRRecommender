[
    {
        "title": "Forward-Backward Feature Transfer for Industrial Anomaly Detection and Segmentation",
        "link_suffix": "/forum?id=bwVV0rHwrb",
        "link": "https://openreview.net/forum?id=bwVV0rHwrb",
        "pdf_link": "https://openreview.net/pdf?id=bwVV0rHwrb",
        "keywords": "anomaly, detection, segmentation, localization",
        "abstract": "Motivated by efficiency requirements, most industrial anomaly detection and segmentation (IADS) methods process low-resolution images, e.g., $224\\times 224$ pixels, obtained by downsampling the original input images.\nIn this setting, downsampling is typically applied also to the provided ground-truth defect masks.\nYet, as numerous industrial applications demand the identification of both large and small defects, this downsampling procedure may fail to reflect the actual performance achievable by current methods.\nIn this work, we propose a fast approach based on a novel Teacher-Student paradigm. \nThis paradigm relies on two shallow Student MLPs that learn to transfer patch features across the layers of a frozen Teacher Vision Transformer. \nOur framework can spot anomalies from high-resolution images faster than other methods, even when they process low-resolution images, achieving state-of-the-art overall performance on MVTec AD and segmentation results on VisA.\nWe also propose novel evaluation metrics that capture robustness regarding defect size, i.e., the ability of a method to preserve good localization from large anomalies to tiny ones, focusing on segmentation performance as a function of anomaly size. \nEvaluating our method with these metrics reveals its stable performance in detecting anomalies of any size."
    },
    {
        "title": "Orca: Enhancing Role-Playing Abilities of Large Language Models by Integrating Personality Traits",
        "link_suffix": "/forum?id=HPuLU6q7xq",
        "link": "https://openreview.net/forum?id=HPuLU6q7xq",
        "pdf_link": "https://openreview.net/pdf?id=HPuLU6q7xq",
        "keywords": "Large Language Models, Role-Playing, BigFive, Personality Traits",
        "abstract": "Large language models have catalyzed the development of personalized dialogue systems, leading to the emergence of numerous role-playing conversational agents. While previous research has predominantly focused on enhancing the model's capability to follow instructions by designing character profiles, it has neglected the psychological factors that drive human conversations. In this paper, we propose Orca, a framework for data processing and training custom LLM characters by integrating personality traits. Orca comprises four stages: (1) Personality traits inference, leveraging LLMs to infer users' BigFive personality trait reports and scores. (2) Data Augmentation, simulating users' profiles, background stories, and psychological activities. (3) Dataset construction, employing personality-conditioned instruction prompting (PCIP) to stimulate LLMs. (4) Modeling and Training, utilizing personality-conditioned instruction tuning (PTIT and PSIT) to enhance existing open-source LLMs using the generated data. We introduce OrcaBench, the first benchmark for evaluating the quality of LLM-generated content on social platforms across multiple scales. Our experiments demonstrate that our proposed model achieves superior performance on this benchmark, highlighting its excellence and effectiveness in perceiving personality traits, thereby significantly improving role-playing abilities."
    },
    {
        "title": "On the Identifiability of Nonlinear Representation Learning with General Noise",
        "link_suffix": "/forum?id=7oT1X8xjIk",
        "link": "https://openreview.net/forum?id=7oT1X8xjIk",
        "pdf_link": "https://openreview.net/pdf?id=7oT1X8xjIk",
        "keywords": "Latent Variable Models, Identifiability, Noise",
        "abstract": "Noise is pervasive in real-world data, posing significant challenges to reliably uncovering latent generative processes. While evolution may have enabled the brain to solve such problems over millions of years, machine learning faces this task in just a few years. Most prior identifiability theories, even under restrictive assumptions like linear generating functions, are limited to handling only additive noise and fail to address nonparametric noise. In contrast, we study the problem of provably learning nonlinear representations in the presence of nonparametric noise. Specifically, we show that, under certain structural conditions between latent and observed variables, latent factors can be identified up to element-wise transformations, even when both the generative processes and noise are nonlinear and lack specific parametric forms. We further present extensions of the general framework, demonstrating trade-offs between different assumptions and the identifiability of latent variables in the presence of both noise and distortions. Moreover, we prove that the underlying directed acyclic graph can be recovered even with nonlinear measurement errors, offering independent insights into structure learning. Our theoretical results are validated on both synthetic and real-world datasets."
    },
    {
        "title": "Automatic Dataset Construction (ADC): Sample Collection, Data Curation, and Beyond",
        "link_suffix": "/forum?id=GcJE0HPy4X",
        "link": "https://openreview.net/forum?id=GcJE0HPy4X",
        "pdf_link": "https://openreview.net/pdf?id=GcJE0HPy4X",
        "keywords": "Dataset Construction, LLM, Label Noise, Class-Imbalance",
        "abstract": "Large-scale data collection is essential for developing personalized training data, mitigating the shortage of training data, and fine-tuning specialized models. However, creating high-quality datasets quickly and accurately remains a challenge due to annotation errors, the substantial time and costs associated with human labor. To address these issues, we propose Automatic Dataset Construction (ADC), an innovative methodology that automates dataset creation with negligible cost and high efficiency. Taking the image classification task as a starting point, ADC leverages LLMs for the detailed class design and code generation to collect relevant samples via search engines, significantly reducing the need for manual annotation and speeding up the data generation process. Despite these advantages, ADC also encounters real-world challenges such as label errors (label noise) and imbalanced data distributions (label bias). We provide open-source software that incorporates existing methods for label error detection, robust learning under noisy and biased data, ensuring a higher-quality training data and more robust model training procedure. Furthermore, we design three benchmark datasets focused on label noise detection, label noise learning, and class-imbalanced learning. These datasets are vital because there are few existing datasets specifically for label noise detection, despite its importance. Finally, we evaluate the performance of existing popular methods on these datasets, thereby facilitating further research in the field."
    },
    {
        "title": "CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks",
        "link_suffix": "/forum?id=lbe3BiDCQr",
        "link": "https://openreview.net/forum?id=lbe3BiDCQr",
        "pdf_link": "https://openreview.net/pdf?id=lbe3BiDCQr",
        "keywords": "LLMs, verification, self-check",
        "abstract": "Large Language Models (LLMs) are revolutionizing various domains, yet verifying their answers remains a significant challenge, especially for intricate open-ended tasks such as consolidation, summarization, and extraction of knowledge. In this work, we propose CheckEmbed: an accurate, scalable, and simple LLM verification approach. CheckEmbed is driven by a straightforward yet powerful idea: in order to compare LLM solutions to one another or to the ground-truth, compare their corresponding answer-level embeddings obtained with a model such as GPT Text Embedding Large. This reduces a complex textual answer to a single embedding, facilitating straightforward, fast, and meaningful verification. We develop a comprehensive verification pipeline implementing the CheckEmbed methodology. The CheckEmbed pipeline also comes with metrics for assessing the truthfulness of the LLM answers, such as embedding\nheatmaps and their summaries. We show how to use these metrics for deploying practical engines that decide whether an LLM answer is satisfactory or not. We apply the pipeline to real-world document analysis tasks, including term extraction and document summarization, showcasing significant improvements in accuracy, cost-effectiveness, and runtime performance compared to existing token-, sentence-, and fact-level schemes such as BERTScore or SelfCheckGPT."
    },
    {
        "title": "Efficient Model Editing with Task-Localized Sparse Fine-tuning",
        "link_suffix": "/forum?id=TDyE2iuvyc",
        "link": "https://openreview.net/forum?id=TDyE2iuvyc",
        "pdf_link": "https://openreview.net/pdf?id=TDyE2iuvyc",
        "keywords": "task arithmetic, parameter-efficient fine-tuning",
        "abstract": "Pre-trained models are stepping stones for modern machine learning systems, but how to efficiently extract, reuse, and steer their knowledge for new tasks is an area of research with still several open questions. Existing Task Arithmetic solutions are strongly tied to model linearization which leads to computational bottlenecks during training and inference, and potentially neglect essential task dependencies. In this work, we focus on the fine-tuning stage that defines task vectors and propose TaLoS, a new strategy based on sparse fine-tuning that strategically updates only parameters expected to provide functional task localization. This efficiently yields weight-disentangled models without the need for explicit linearization. We present a thorough experimental analysis showing how our approach significantly improves in training and inference efficiency while outperforming state of the art approaches in task addition and negation performance. Our work offers a principled solution to pre-trained model editing and and paves the way to more cost-effective and scalable machine learning systems for real-world applications."
    },
    {
        "title": "Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos",
        "link_suffix": "/forum?id=FZv3kPHTtB",
        "link": "https://openreview.net/forum?id=FZv3kPHTtB",
        "pdf_link": "https://openreview.net/pdf?id=FZv3kPHTtB",
        "keywords": "vision language model, video question answering, video captioning, multi-shot videos",
        "abstract": "A short clip of video may contain progression of multiple events and an interesting story line. A human need to capture both the event in every shot and associate them together to understand the story behind it. In this work, we present a new multi-shot video understanding benchmark \\dataset with detailed shot-level captions, comprehensive video summaries and question-answering pairs. To facilitate better semantic understanding of videos, we provide captions for both visual signals and human narrations. We design several distinct tasks including single-shot video captioning, multi-shot video summarization, and multi-shot video question answering. Preliminary experiments show some challenges to generate a long and comprehensive video summary for multi-shot videos. Nevertheless, the generated imperfect summaries can already achieve competitive performance on existing video understanding tasks such as video question-answering, promoting an under-explored setting of video understanding with detailed summaries."
    },
    {
        "title": "Towards Generalisable Time Series Understanding Across Domains",
        "link_suffix": "/forum?id=39n570rxyO",
        "link": "https://openreview.net/forum?id=39n570rxyO",
        "pdf_link": "https://openreview.net/pdf?id=39n570rxyO",
        "keywords": "Time Series Analysis, Multi-Domain, Self-Supervised Learning",
        "abstract": "In natural language processing and computer vision, self-supervised pre-training on large datasets unlocks foundational model capabilities across domains and tasks. However, this potential has not yet been realised in time series analysis, where existing methods disregard the heterogeneous nature of time series characteristics. Time series are prevalent in many domains, including medicine, engineering, natural sciences, and finance, but their characteristics vary significantly in terms of variate count, inter-variate relationships, temporal dynamics, and sampling frequency. This inherent heterogeneity across domains prevents effective pre-training on large time series corpora. To address this issue, we introduce OTiS, an open model for general time series analysis, that has been specifically designed to handle multi-domain heterogeneity. We propose a novel pre-training paradigm including a tokeniser with learnable domain-specific signatures, a dual masking strategy to capture temporal causality, and a normalised cross-correlation loss to model long-range dependencies. Our model is pre-trained on a large corpus of 640,187 samples and 11 billion time points spanning 8 distinct domains, enabling it to analyse time series from any (unseen) domain. In comprehensive experiments across 15 diverse applications - including classification, regression, and forecasting - OTiS showcases its ability to accurately capture domain-specific data characteristics and demonstrates its competitiveness against state-of-the-art baselines. Our code and pre-trained weights are publicly available at \\url{https://github.com/OTiS-official/OTiS}."
    },
    {
        "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering",
        "link_suffix": "/forum?id=ETMIPPtJp9",
        "link": "https://openreview.net/forum?id=ETMIPPtJp9",
        "pdf_link": "https://openreview.net/pdf?id=ETMIPPtJp9",
        "keywords": "Large Language Models, Knowledge Graph Question Answering, Retrieval-Augmented Generation",
        "abstract": "Large language models are often challenged by generating erroneous or `hallucinated' responses, especially in complex reasoning tasks.\nTo mitigate this, we propose a retrieval augmented reasoning method, FiDeLiS, which enhances knowledge graph question answering by anchoring responses to structured, verifiable reasoning paths. FiDeLiS uses a keyword-enhanced retrieval mechanism that fetches relevant entities and relations from a vector-based index of KGs to ensure high-recall retrieval. Once these entities and relations are retrieved, our method constructs candidate reasoning paths which are then refined using a stepwise beam search. This ensures that all the paths we create can be confidently linked back to KGs, ensuring they are accurate and reliable.\nA distinctive feature of our approach is its blend of natural language planning with beam search to optimize the selection of reasoning paths. Moreover, we redesign the way reasoning paths are scored by transforming this process into a deductive reasoning task, allowing the LLM to assess the validity of the paths through deductive reasoning rather than traditional logit-based scoring. This helps avoid misleading reasoning chains and reduces unnecessary computational demand. Extensive experiments demonstrate that our method, even as a training-free method which has lower computational costs and superior generality, outperforms established strong baselines across three datasets. The code of this paper will be released athttps://anonymous.4open.science/r/FiDELIS-E7FC."
    },
    {
        "title": "HGM\u00b3: Hierarchical Generative Masked Motion Modeling with Hard Token Mining",
        "link_suffix": "/forum?id=IEul1M5pyk",
        "link": "https://openreview.net/forum?id=IEul1M5pyk",
        "pdf_link": "https://openreview.net/pdf?id=IEul1M5pyk",
        "keywords": "text-to-motion generation, generative masked model, hard token mining, hierarchical semantic graph",
        "abstract": "Text-to-motion generation has significant potential in a wide range of applications including animation, robotics, and AR/VR. While recent works on masked motion models are promising, the task remains challenging due to the inherent ambiguity in text and the complexity of human motion dynamics. To overcome the issues, we propose a novel text-to-motion generation framework that integrates two key components: Hard Token Mining (HTM) and a Hierarchical Generative Masked Motion Model (HGM\u00b3). Our HTM identifies and masks challenging regions in motion sequences and directs the model to focus on hard-to-learn components for efficacy. Concurrently, the hierarchical model uses a semantic graph to represent sentences at different granularity, allowing the model to learn contextually feasible motions. By leveraging a shared-weight masked motion model, it reconstructs the same sequence under different conditioning levels and facilitates comprehensive learning of complex motion patterns. During inference, the model progressively generates motions by incrementally building up coarse-to-fine details. Extensive experiments on benchmark datasets, including HumanML3D and KIT-ML, demonstrate that our method outperforms existing methods in both qualitative and quantitative measures for generating context-aware motions."
    },
    {
        "title": "LRVS-Fashion: Extending Visual Search with Referring Instructions",
        "link_suffix": "/forum?id=nD5tbHBfut",
        "link": "https://openreview.net/forum?id=nD5tbHBfut",
        "pdf_link": "https://openreview.net/pdf?id=nD5tbHBfut",
        "keywords": "Visual Search, Image Embedding, Retrieval, Dataset",
        "abstract": "This paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LRVS-Fashion, consisting of 272k fashion products with 842k images extracted from fashion catalogs, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors."
    },
    {
        "title": "Improving Large Language Model Planning with Action Sequence Similarity",
        "link_suffix": "/forum?id=tpGkEgxMJT",
        "link": "https://openreview.net/forum?id=tpGkEgxMJT",
        "pdf_link": "https://openreview.net/pdf?id=tpGkEgxMJT",
        "keywords": "planning with LLM, in-context learning (ICL), action sequence",
        "abstract": "Planning is essential for artificial intelligence systems to look ahead and proactively determine a course of actions to reach objectives in the virtual and real world. Recent work on large language models (LLMs) sheds light on their planning capability in various tasks. However, it remains unclear what signals in the context influence the model performance. In this work, we explore how to improve the model planning capability through in-context learning (ICL), specifically, what signals can help select the exemplars. Through extensive experiments, we observe that commonly used problem similarity may result in false positives with drastically different plans, which can mislead the model. In response, we propose to sample and filter exemplars leveraging plan side action sequence similarity (AS). We propose GRASE-DC: a two-stage pipeline that first re-samples high AS exemplars and then curates the selected exemplars with dynamic clustering on AS to achieve a balance of relevance and diversity.  Our experimental result confirms that GRASE-DC achieves significant performance improvement on various planning tasks (up to ~11-40 point absolute accuracy improvement with 27.3% fewer exemplars needed on average). With GRASE-DC* + VAL, where we iteratively apply GRASE-DC with a validator, we are able to even boost the performance by 18.9% more.\nExtensive analysis validates the consistent performance improvement of GRASE-DC with various backbone LLMs and on both classical planning and natural language planning benchmarks. GRASE-DC can further boost the planning accuracy by ~24 absolute points on harder problems using simpler problems as exemplars over a random baseline. This demonstrates its ability to generalize to out-of-distribution problems."
    },
    {
        "title": "Provable Length Generalization in Sequence Prediction via Spectral Filtering",
        "link_suffix": "/forum?id=e1ETy9XW0T",
        "link": "https://openreview.net/forum?id=e1ETy9XW0T",
        "pdf_link": "https://openreview.net/pdf?id=e1ETy9XW0T",
        "keywords": "online learning, state space models, linear dynamical systems, sequence prediction, LLMs",
        "abstract": "We consider the problem of length generalization in sequence prediction. We define a new metric of performance in this setting -- the Unfair-Regret -- which measures regret against a benchmark predictor with longer context length than available to the learner. We continue by studying this concept from the lens of the spectral filtering algorithm. We give a gradient-based learning algorithm that provably length generalizes for linear dynamical systems. We conclude with proof-of-concept experiments demonstrating the validity of our theory."
    },
    {
        "title": "Tackling Data Corruption in Offline Reinforcement Learning via Sequence Modeling",
        "link_suffix": "/forum?id=phAlw3JPms",
        "link": "https://openreview.net/forum?id=phAlw3JPms",
        "pdf_link": "https://openreview.net/pdf?id=phAlw3JPms",
        "keywords": "Offline Reinforcement Learning, Data Corruption, Robust Reinforcement Learning",
        "abstract": "Learning policy from offline datasets through offline reinforcement learning (RL) holds promise for scaling data-driven decision-making while avoiding unsafe and costly online interactions. However, real-world data collected from sensors or humans often contains noise and errors, posing a significant challenge for existing offline RL methods, particularly when the real-world data is limited. Our study reveals that prior research focusing on adapting predominant offline RL methods based on temporal difference learning still falls short under data corruption when the dataset is limited. In contrast, we discover that vanilla sequence modeling methods, such as Decision Transformer, exhibit robustness against data corruption, even without specialized modifications. To unlock the full potential of sequence modeling, we proposeRobustDecisionTransformer (RDT) by incorporating three simple yet effective robust techniques: embedding dropout to improve the model's robustness against erroneous inputs, Gaussian weighted learning to mitigate the effects of corrupted labels, and iterative data correction to eliminate corrupted data from the source. \nExtensive experiments on MoJoCo, Kitchen, and Adroit tasks demonstrate RDT's superior performance under various data corruption scenarios compared to prior methods. Furthermore, RDT exhibits remarkable robustness in a more challenging setting that combines training-time data corruption with test-time observation perturbations. These results highlight the potential of sequence modeling for learning from noisy or corrupted offline datasets, thereby promoting the reliable application of offline RL in real-world scenarios."
    },
    {
        "title": "VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?",
        "link_suffix": "/forum?id=vbmSSIhKAM",
        "link": "https://openreview.net/forum?id=vbmSSIhKAM",
        "pdf_link": "https://openreview.net/pdf?id=vbmSSIhKAM",
        "keywords": "spoken dialogue system, paralinguistic information, benchmark",
        "abstract": "With the rapid advancement of large models, voice assistants are gradually acquiring the ability to engage in open-ended daily conversations with humans. However, current spoken dialogue systems often overlook multi-modal information in audio beyond text, such as speech rate, volume, emphasis, and background sounds. Relying solely on Automatic Speech Recognition (ASR) can lead to the loss of valuable auditory cues, thereby weakening the system\u2019s ability to generate contextually appropriate responses. To address this limitation, we propose \\textbf{VoxDialogue}, a comprehensive benchmark for evaluating the ability of spoken dialogue systems to understand multi-modal information beyond text. Specifically, we have identified 12 attributes highly correlated with acoustic information beyond words and have meticulously designed corresponding spoken dialogue test sets for each attribute, encompassing a total of 4.5K multi-turn spoken dialogue samples. Finally, we evaluated several existing spoken dialogue models, analyzing their performance on the 12 attribute subsets of VoxDialogue. Experiments have shown that in spoken dialogue scenarios, many acoustic cues cannot be conveyed through textual information and must be directly interpreted from the audio input. In contrast, while direct spoken dialogue systems excel at processing acoustic signals, they still face limitations in handling complex dialogue tasks due to their restricted context understanding capabilities. All data and code will be open source at \\url{https://voxdialogue.github.io/}."
    },
    {
        "title": "Retrieval-Augmented Language Model for Knowledge-aware Protein Encoding",
        "link_suffix": "/forum?id=0MVWOHwHDb",
        "link": "https://openreview.net/forum?id=0MVWOHwHDb",
        "pdf_link": "https://openreview.net/pdf?id=0MVWOHwHDb",
        "keywords": "Knowledge Graphs; Protein Science; Representation Learning",
        "abstract": "Protein language models often struggle to capture the biological functions encoded within protein sequences due to their lack of factual knowledge (e.g., gene descriptions of proteins). Existing solutions leverage protein knowledge graphs (PKGs), using knowledge as auxiliary encoding objectives. However, none of them explored the direct injection of correlated knowledge into protein language models, and task-oriented knowledge integration during fine-tuning, making them suffer from insufficient knowledge exploitation and catastrophic forgetting of pre-trained knowledge. The root cause is that they fail to align PKGs with downstream tasks, forcing their knowledge modeling to adapt to the knowledge-isolated nature of these tasks. To tackle these limitations, we propose a novel knowledge retriever that can accurately predict gene descriptions for new proteins in downstream tasks and thus align them with PKGs. On this basis, we propose Knowledge-aware retrieval-augmented protein language model (Kara), achieving the first unified and direct integration of PKGs and protein language models. Using the knowledge retriever, both the pre-training and fine-tuning stages can incorporate knowledge through a unified modeling process, where contextualized virtual tokens enable token-level integration of high-order knowledge. Moreover, structure-based regularization is introduced to inject function similarity into protein representations, and unify the pre-training and fine-tuning optimization objectives. Experimental results show that Kara consistently outperforms existing knowledge-enhanced models in 6 representative tasks, achieving on average 5.1% improvements."
    },
    {
        "title": "Diffusion Transportation Cost for Domain Adaptation",
        "link_suffix": "/forum?id=TvwsOrl865",
        "link": "https://openreview.net/forum?id=TvwsOrl865",
        "pdf_link": "https://openreview.net/pdf?id=TvwsOrl865",
        "keywords": "Optimal Transport, Domain Adaptation, Diffusion geometry, Manifold learning, Kernel methods, Riemannian manifolds.",
        "abstract": "In recent years, there has been considerable interest in leveraging the Optimal Transport (OT) problem for domain adaptation, a strategy shown to be highly effective. \nHowever, a less explored aspect is the choice of the transportation cost function, as most existing methods rely on the pairwise squared Euclidean distances for the transportation cost, potentially overlooking important intra-domain geometries.\nThis paper presents Diffusion-OT, a new transport cost for the OT problem, designed specifically for domain adaptation. By utilizing concepts and tools from the field of manifold learning, specifically diffusion geometry, we derive an operator that accounts for the intra-domain relationships, thereby extending beyond the conventional inter-domain distances.\nThis operator, which quantifies the probability of transporting between source and target samples, forms the basis for our transportation cost. \nWe provide proof that the proposed operator is in fact a diffusion operator, demonstrating that the cost function is defined by an anisotropic diffusion process between the domains.\nIn addition, to enhance performance, we integrate source labels into the operator, thereby guiding the anisotropic diffusion according to the classes.\nWe showcase the effectiveness of Diffusion-OT through comprehensive experiments, demonstrating its superior performance compared to recent methods across various benchmarks and datasets."
    },
    {
        "title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
        "link_suffix": "/forum?id=I1MKOjNVup",
        "link": "https://openreview.net/forum?id=I1MKOjNVup",
        "pdf_link": "https://openreview.net/pdf?id=I1MKOjNVup",
        "keywords": "benchmark, biomedical agent, knowledge graph, literature",
        "abstract": "Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, researchers typically rely on direct Question-Answering (QA) to the LLM itself or through biomedical experiments. How to benchmark biomedical agents precisely from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from scientists\u2019 crucial ability to understand the literature and introduce BioKGBench. In contrast to traditional evaluation benchmarks that focus solely on factual QA, where the LLMs are known to have hallucination issues, we first disentangle \u201cUnderstanding Literature\u201d into two atomic abilities: i) \u201cUnderstanding\u201d the unstructured text from research papers by performing scientific claim verification, and ii) interacting with structured Knowledge-Graphs for Question-Answering (KGQA) as a form of \u201cLiterature\u201d grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify factual errors in existing large-scale knowledge graphs. We collect over two thousand data points for the two atomic tasks and 225 high-quality annotated samples for the agent task. Surprisingly, we find that state-of-the-art general and biomedical agents have either failed or performed inferiorly on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors, which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach."
    },
    {
        "title": "State Combinatorial Generalization In Decision Making With Conditional Diffusion Models",
        "link_suffix": "/forum?id=PH7ja3T0vN",
        "link": "https://openreview.net/forum?id=PH7ja3T0vN",
        "pdf_link": "https://openreview.net/pdf?id=PH7ja3T0vN",
        "keywords": "RL generalization, decision making, combinatorial generalization, diffusion model",
        "abstract": "Many real-world decision-making problems are combinatorial in nature, where states (e.g., surrounding traffic of a self-driving car) can be seen as a combination of basic elements (e.g., pedestrians, trees, and other cars). Due to combinatorial complexity, observing all combinations of basic elements in the training set is infeasible, which leads to an essential yet understudied problem of $\\textit{zero-shot generalization to states that are unseen combinations of previously seen elements.}$ In this work, we first formalize this problem and then demonstrate how existing value-based reinforcement learning (RL) algorithms struggle due to unreliable value predictions in unseen states. We argue that this problem cannot be addressed with exploration alone, but requires more expressive and generalizable models. We demonstrate that behavior cloning with a conditioned diffusion model trained on expert trajectory generalizes better to states formed by new combinations of seen elements than traditional RL methods. Through experiments in maze, driving, and multiagent environments, we show that conditioned diffusion models outperform traditional RL techniques and highlight the broad applicability of our problem formulation."
    },
    {
        "title": "PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation",
        "link_suffix": "/forum?id=n7qGCmluZr",
        "link": "https://openreview.net/forum?id=n7qGCmluZr",
        "pdf_link": "https://openreview.net/pdf?id=n7qGCmluZr",
        "keywords": "Generative Models, Model Evaluation, Sample-based Metrics, Hypothesis Testing",
        "abstract": "We propose a likelihood-free method for comparing two distributions given samples from each, with the goal of assessing the quality of generative models. The proposed approach, PQMass, provides a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models. PQMass divides the sample space into non-overlapping regions and applies chi-squared tests to the number of data samples that fall within each region, giving a $p$-value that measures the probability that the bin counts derived from two sets of samples are drawn from the same multinomial distribution. PQMass does not depend on assumptions regarding the density of the true distribution, nor does it rely on training or fitting any auxiliary models. We evaluate PQMass on data of various modalities and dimensions, demonstrating its effectiveness in assessing the quality, novelty, and diversity of generated samples. We further show that PQMass scales well to moderately high-dimensional data and thus obviates the need for feature extraction in practical applications."
    },
    {
        "title": "OpenHands: An Open Platform for AI Software Developers as Generalist Agents",
        "link_suffix": "/forum?id=OJd3ayDDoF",
        "link": "https://openreview.net/forum?id=OJd3ayDDoF",
        "pdf_link": "https://openreview.net/pdf?id=OJd3ayDDoF",
        "keywords": "AI agents, evaluation, infrastructure, benchmark",
        "abstract": "Software is one of the most powerful tools that we humans have at our disposal; it allows a skilled programmer to interact with the world in complex and profound ways. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and effect change in their surrounding environments. In this paper, we introduce OpenHands, a platform for the development of powerful and flexible AI agents that interact with the world in similar ways to a human developer: by writing code, interacting with a command line, and browsing the web. We describe how the platform allows for the implementation of new agents, utilization of various LLMs, safe interaction with sandboxed environments for code execution, and incorporation of evaluation benchmarks. Based on our currently incorporated benchmarks, we perform an evaluation of agents over 13 challenging tasks, including software engineering (e.g., SWE-Bench) and web browsing (e.g., WebArena), amongst others. Released under the permissive MIT license, OpenHands is a community project spanning academia and industry with more than 2K contributions from over 186 contributors in less than six months of development, and will improve going forward."
    },
    {
        "title": "miniCTX: Neural Theorem Proving with (Long-)Contexts",
        "link_suffix": "/forum?id=KIgaAqEFHW",
        "link": "https://openreview.net/forum?id=KIgaAqEFHW",
        "pdf_link": "https://openreview.net/pdf?id=KIgaAqEFHW",
        "keywords": "Neural theorem proving, Formal mathematics, Benchmark dataset",
        "abstract": "Real-world formal theorem proving often depends on a wealth of context, including definitions, lemmas, comments, file structure, and other information. We introduce $\\texttt{miniCTX}$, which tests a model's ability to prove formal mathematical theorems that depend on new context that is not seen during training. $\\texttt{miniCTX}$ contains theorems sourced from real Lean projects and textbooks, each associated with a context that can span tens of thousands of tokens. Models are tasked with proving a theorem given access to code from the theorem's repository, which contains context that is needed for the proof. As a baseline for $\\texttt{miniCTX}$, we tested fine-tuning and prompting methods that condition theorem proving on preceding context. Both approaches substantially outperform traditional methods that rely solely on state information. We found that this ability to use context is not captured by previous benchmarks such as $\\texttt{miniF2F}$. Alongside $\\texttt{miniCTX}$, we offer $\\texttt{ntp-toolkit}$ for automatically extracting and annotating theorem proving data, making it easy to add new projects into $\\texttt{miniCTX}$ to ensure that contexts are not seen during training. $\\texttt{miniCTX}$ offers a challenging and realistic evaluation of neural theorem provers."
    },
    {
        "title": "Exploring Group and Symmetry Principles in Large Language Models",
        "link_suffix": "/forum?id=Em6GkQfLKM",
        "link": "https://openreview.net/forum?id=Em6GkQfLKM",
        "pdf_link": "https://openreview.net/pdf?id=Em6GkQfLKM",
        "keywords": "probing, robustness, LLM",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across a wide range of applications; however, assessing their reasoning capabilities remains a significant challenge. In this paper, we introduce a framework grounded in group and symmetry principles, which have played a crucial role in fields such as physics and mathematics, and offer another way to evaluate their capabilities. While the proposed framework is general, to showcase the benefits of employing these properties, we focus on arithmetic reasoning and investigate the performance of these models on four group properties: closure, identity, inverse, and associativity. Our findings reveal that LLMs studied in this work struggle to preserve group properties across different test regimes. In the closure test, we observe biases towards specific outputs and an abrupt degradation in their performance from $100%$ to $0%$ after a specific sequence length. They also perform poorly in the identity test, which represents adding irrelevant information in the context, and show sensitivity when subjected to inverse test, which examines the robustness of the model with respect to negation. In addition, we demonstrate that breaking down problems into smaller steps helps LLMs in the associativity test that we have conducted. To support these tests we have developed a synthetic dataset which will be released."
    },
    {
        "title": "A Defense of One-Step Learning: Examining Single-Batch Distillations",
        "link_suffix": "/forum?id=CCoa6XgO8F",
        "link": "https://openreview.net/forum?id=CCoa6XgO8F",
        "pdf_link": "https://openreview.net/pdf?id=CCoa6XgO8F",
        "keywords": "distillation, interpretability, explainability, compression, cost surface, loss landscape",
        "abstract": "Dataset distillation produces a compressed synthetic dataset that approximates a large dataset or other learning task. A model can be trained on a distillation in a single gradient descent step. Conventional wisdom suggests that single-step learning is not generalizable and should yield poor performance; yet, distillation defies these expectations with good approximations of full direct-task training for a large distribution of models. In order to understand how distilled datasets can perform one-shot learning, we examine the distilled data instances and the cost surfaces produced by the distilled datasets. We demonstrate that the distilled dataset not only mimics features of the true dataset but also produces cost surfaces such that one-step training leads models from the initialization space into local minima of the true task's cost surface. This shows how one-step learning's counter-intuitive success is not only reasonable but also the expected outcome of dataset distillation."
    },
    {
        "title": "Conformal confidence sets for biomedical image segmentation",
        "link_suffix": "/forum?id=2P4p4RxUxT",
        "link": "https://openreview.net/forum?id=2P4p4RxUxT",
        "pdf_link": "https://openreview.net/pdf?id=2P4p4RxUxT",
        "keywords": "Deep learning, neural networks, uncertainty quantification, confidence sets",
        "abstract": "We develop confidence sets which provide spatial uncertainty guarantees for the output of a black-box machine learning model designed for image segmentation. To do so we adapt conformal inference to the imaging setting, obtaining thresholds on a calibration dataset based on the distribution of the maximum of the transformed logit scores within and outside of the ground truth masks. We prove that these confidence sets, when applied to new predictions of the model, are guaranteed to contain the true unknown segmented mask with desired probability. We show that learning appropriate score transformations on a learning dataset before performing calibration is crucial for optimizing performance. We illustrate and validate our approach on a polpys tumor dataset. To do so we obtain the logit scores from a deep neural network trained for polpys segmentation and show that using distance transformed scores to obtain outer confidence sets and the original scores for inner confidence sets enables tight bounds on tumor location whilst controlling the false coverage rate."
    }
]