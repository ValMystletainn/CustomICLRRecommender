[
    {
        "title": "Dataset Distillation for Domain Generalization",
        "link_suffix": "/forum?id=ENVwvyiJXY",
        "link": "https://openreview.net/forum?id=ENVwvyiJXY",
        "pdf_link": "https://openreview.net/pdf?id=ENVwvyiJXY",
        "keywords": "Dataset Distillation, Domain Generalization, Style Transfer, Self-Supervised Learning",
        "abstract": "Dataset Distillation (DD) has been applied to various downstream tasks and recently scaled to ImageNet-1k, highlighting its potential for practical applications. However, in real-world scenarios, robustness to unseen domains is essential, and the robustness of models trained on synthetic datasets remains uncertain. To address this, we propose a novel task, Dataset Distillation for Domain Generalization (DD for DG), and evaluate the unseen domain generalization of models trained on synthetic datasets distilled by state-of-the-art DD methods using the DomainBed benchmark. Additionally, we introduce a new method for this task, which interprets DD through the lens of image style transfer, achieving superior performance in unseen domain generalization compared to baseline approaches."
    },
    {
        "title": "ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities",
        "link_suffix": "/forum?id=cPD2hU35x3",
        "link": "https://openreview.net/forum?id=cPD2hU35x3",
        "pdf_link": "https://openreview.net/pdf?id=cPD2hU35x3",
        "keywords": "Long Context LLM, Retrieval-augmented generation",
        "abstract": "In this work, we introduce ChatQA 2, an Llama 3.0-based model with a 128K context window, designed to bridge the gap between open-source LLMs and leading proprietary models (e.g., GPT-4-Turbo) in long-context understanding and retrieval-augmented generation (RAG) capabilities. These two capabilities are essential for LLMs to process large volumes of information that cannot fit into a single prompt and are complementary to each other, depending on the downstream tasks and computational budgets. We present a detailed continued training recipe to extend the context window of Llama3-70B-base from 8K to 128K tokens, along with a three-stage instruction tuning process to enhance the model's instruction-following, RAG performance, and long-context understanding capabilities. Our results demonstrate that the Llama3-ChatQA-2-70B model outperforms most existing state-of-the-art models, including GPT-4-Turbo-2024-04-09, Qwen2-72B-Instruct, and Llama3.1-70B-Instruct, on ultra-long tasks beyond 100K tokens, as well as on the RAG benchmark using only a 4K context window, showing the strong long context capability across varying sequence lengths. We further provide extensive comparisons between direct long-context and RAG solutions using the same state-of-the-art long-context LLMs. Interestingly, we find that the performance of strong long-context LLMs using RAG improves when retrieving a larger number of chunks. With a large set of top-k chunks, RAG consistently outperforms direct long-context solution using the same state-of-the-art long-context models (e.g., Llama3-ChatQA-2-70B and Qwen2-72B-Instruct) on both 32K benchmarks and real-world 128K tasks. To advance research in this field, we open-sourced the model weights, training data, and the evaluation setup for the for the community."
    },
    {
        "title": "VLMaterial: Procedural Material Generation with Large Vision-Language Models",
        "link_suffix": "/forum?id=wHebuIb6IH",
        "link": "https://openreview.net/forum?id=wHebuIb6IH",
        "pdf_link": "https://openreview.net/pdf?id=wHebuIb6IH",
        "keywords": "generative model, procedural material, appearance modeling",
        "abstract": "Procedural materials, represented as functional node graphs, are ubiquitous in computer graphics for photorealistic material appearance design. They allow users to perform intuitive and precise editing to achieve desired visual appearances. However, creating a procedural material given an input image requires professional knowledge and significant effort. In this work, we leverage the ability to convert procedural materials into standard Python programs and fine-tune a large pre-trained vision-language model (VLM) to generate such programs from input images. To enable effective fine-tuning, we also contribute an open-source procedural material dataset and propose to perform program-level augmentation by prompting another VLM. Through extensive evaluation, we show that our method outperforms previous methods on both synthetic and real-world examples."
    },
    {
        "title": "Rethinking Copyright Infringements In the Era Of Text-to-Image Generative Models",
        "link_suffix": "/forum?id=0OTVNEm9N4",
        "link": "https://openreview.net/forum?id=0OTVNEm9N4",
        "pdf_link": "https://openreview.net/pdf?id=0OTVNEm9N4",
        "keywords": "evaluating copying, copyright, generative ai, text-to-image, ai art, law, interpretability, social impact",
        "abstract": "The advent of text-to-image generative models has led artists to worry that their individual styles may be copied, creating a pressing need to reconsider the lack of protection for artistic styles under copyright law. This requires answering challenging questions, like what defines style and what constitutes style infringment. In this work, we build on prior legal scholarship to develop an automatic and interpretable framework to \\emph{quantitatively} assess style infringement. Our methods hinge on a simple logical argument: if an artist's works can consistently be recognized as their own, then they have a unique style. Based on this argument, we introduce ArtSavant, a practical (i.e., efficient and easy to understand) tool to (i) determine the unique style of an artist by comparing it to a reference corpus of works from hundreds of artists, and (ii) recognize if the identified style reappears in generated images. We then apply ArtSavant in an empirical study to quantify the prevalence of artistic style copying across 3 popular text-to-image generative models, finding that under simple prompting, $20%$ of $372$ prolific artists studied appear to have their styles be at risk of copying by today's generative models. Our findings show that prior legal arguments can be operationalized in quantitative ways, towards more nuanced examination of the issue of artistic style infringements."
    },
    {
        "title": "Can Large Vision-Language Models Correct Grounding Errors By Themselves?",
        "link_suffix": "/forum?id=fO1xnmW8T6",
        "link": "https://openreview.net/forum?id=fO1xnmW8T6",
        "pdf_link": "https://openreview.net/pdf?id=fO1xnmW8T6",
        "keywords": "vision-language models, self-correction, feedback",
        "abstract": "Enhancing semantic grounding abilities in Vision-Language Models (VLMs) often involves collecting domain-specific training data, refining the network architectures, or modifying the training recipes. In this work, we venture into an orthogonal direction and explore semantic grounding in VLMs through self-correction, without requiring in-domain data, fine-tuning, or modifications to the network architectures. Despite the concerns raised in the self-correction of LLMs, we find that if prompted and framed properly, VLMs can correct their own semantic grounding mistakes even without the access to the oracle feedback. We also show an identified self-correction framework in an iterative setting which consistently improves performance across all models investigated. Overall, iterative self-correction consistently improves VLM performance by up to 8.4 accuracy points across all models investigated; yet, after several rounds of feedback, strong models like GPT-4V and GPT-4o still exhibit significant error rates, indicating promising directions for further research."
    },
    {
        "title": "Skill-based Safe Reinforcement Learning with Risk Planning",
        "link_suffix": "/forum?id=KkALFpRWSV",
        "link": "https://openreview.net/forum?id=KkALFpRWSV",
        "pdf_link": "https://openreview.net/pdf?id=KkALFpRWSV",
        "keywords": "Safe Reinforcement Learning, Skill-based, Risk Planning",
        "abstract": "Safe Reinforcement Learning (Safe RL) aims to ensure safety when an RL agent conducts learning by interacting with real-world environments where improper actions can induce high costs or lead to severe consequences. In this paper, we propose a novel Safe Skill Planning (SSkP) approach to enhance effective safe RL by exploiting auxiliary offline demonstration data. SSkP involves a two-stage process. First, we employ PU learning to learn a skill risk predictor from the offline demonstration data. Then, based on the learned skill risk predictor, we develop a novel risk planning process to enhance online safe RL and learn a risk-averse safe policy efficiently through interactions with the online RL environment, while simultaneously adapting the skill risk predictor to the environment. We conduct experiments in several benchmark robotic simulation environments. The experimental results demonstrate that the proposed approach consistently outperforms previous state-of-the-art safe RL methods."
    },
    {
        "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions",
        "link_suffix": "/forum?id=QKBu1BOAwd",
        "link": "https://openreview.net/forum?id=QKBu1BOAwd",
        "pdf_link": "https://openreview.net/pdf?id=QKBu1BOAwd",
        "keywords": "Large Language Model, Tool Learning, Learning from Experience",
        "abstract": "Tool learning enables Large Language Models (LLMs) to interact with external environments by invoking tools, serving as an effective strategy to mitigate the limitations inherent in their pre-training data. In this process, tool documentation plays a crucial role by providing usage instructions for LLMs, thereby facilitating effective tool utilization. This paper concentrates on the critical challenge of bridging the comprehension gap between LLMs and external tools due to the inadequacies and inaccuracies inherent in existing human-centric tool documentation. We propose a novel framework, DRAFT, aimed at Dynamically Refining tool documentation through the Analysis of Feedback and Trails emanating from LLMs' interactions with external tools. This methodology pivots on an innovative trial-and-error approach, consisting of three distinct learning phases: experience gathering, learning from experience, and documentation rewriting, to iteratively enhance the tool documentation. This process is further optimized by implementing a diversity-promoting exploration strategy to ensure explorative diversity and a tool-adaptive termination mechanism to prevent overfitting while enhancing efficiency. \nExtensive experiments on multiple datasets demonstrate that DRAFT's iterative, feedback-based refinement significantly ameliorates documentation quality, fostering a deeper comprehension and more effective utilization of tools by LLMs. Notably, our analysis reveals that the tool documentation refined via our approach demonstrates robust cross-model generalization capabilities. Our code is available athttps://anonymous.4open.science/r/DRAFT-10B3."
    },
    {
        "title": "Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs",
        "link_suffix": "/forum?id=bU1JOvdXXK",
        "link": "https://openreview.net/forum?id=bU1JOvdXXK",
        "pdf_link": "https://openreview.net/pdf?id=bU1JOvdXXK",
        "keywords": "Benchmark, Large Vision language Model, Perception Ability",
        "abstract": "Currently many benchmarks have been proposed to evaluate the perception ability of the Large Vision-Language Models (LVLMs).\nHowever, most benchmarks conduct questions by selecting images from existing datasets, resulting in the potential data leakage.Besides, these benchmarks merely focus on evaluating LVLMs on the realistic style images and clean scenarios, leaving the multi-stylized images and noisy scenarios unexplored. \nIn response to these challenges, we propose a dynamic and scalable benchmark named Dysca for evaluating LVLMs by leveraging synthesis images. \nSpecifically, we leverage Stable Diffusion and design a rule-based method to dynamically generate novel images, questions and the corresponding answers. \nWe consider 51 kinds of image styles and evaluate the perception capability in 20 subtasks.\nMoreover, we conduct evaluations under 4 scenarios (i.e., Clean, Corruption, Print Attacking and Adversarial Attacking) and 3 question types (i.e., Multi-choices, True-or-false and Free-form). Thanks to the generative paradigm, Dysca serves as a scalable benchmark for easily adding new subtasks and scenarios.\nA total of 24 advanced open-source LVLMs and 2 close-source LVLMs are evaluated on Dysca, revealing the drawbacks of current LVLMs. \nThe benchmark is released in anonymous github page \\url{https://github.com/Benchmark-Dysca/Dysca}."
    },
    {
        "title": "Overfitting: An Unexpected Asset in AI\u2010Generated Image Detection",
        "link_suffix": "/forum?id=F1OdjlfCLS",
        "link": "https://openreview.net/forum?id=F1OdjlfCLS",
        "pdf_link": "https://openreview.net/pdf?id=F1OdjlfCLS",
        "keywords": "Overfitting, AI-generated image detection, Generative models",
        "abstract": "AI-generated images have become highly realistic, raising concerns about potential misuse for malicious purposes. In this work, we propose a novel approach, DetGO, to detect generated images by overfitting the distribution of natural images. Our critical insight is that a model overfitting to one distribution (natural images) will fail to generalize to another (AI\u2010generated images). Inspired by the sharpness\u2010aware minimization, where the objective function is designed in a $\\min$-$\\max$ scheme to find flattening minima for better generalization, DetGO instead seeks to overfit the natural image distribution in a $\\max$-$\\min$ manner. This requires finding a solution with a minimal loss near the current solution and then maximizing the loss at this solution, leading to sharp minima. To address the divergence issue caused by the outer maximization, we introduce an anchor model that fits the natural image distribution. In particular, we learn an overfitting model that produces the same outputs as the anchor model while exhibiting abrupt loss behavior for small perturbations. Consequently, we can effectively determine whether an input image is AI-generated by calculating the output differences between these two models. Extensive experiments across multiple benchmarks demonstrate the effectiveness of our proposed method."
    },
    {
        "title": "PAFT: A Parallel Training Paradigm for Effective LLM Fine-Tuning",
        "link_suffix": "/forum?id=KHTkRhq2aB",
        "link": "https://openreview.net/forum?id=KHTkRhq2aB",
        "pdf_link": "https://openreview.net/pdf?id=KHTkRhq2aB",
        "keywords": "LLM, fine-tuning, alignment, SFT",
        "abstract": "Large language models (LLMs) have shown remarkable abilities in diverse natural language processing (NLP) tasks. The LLMs generally undergo supervised fine-tuning (SFT) followed by preference alignment to be usable in downstream applications. However, this sequential training pipeline leads to alignment tax that degrades the LLM performance.This paper introduces PAFT, a new PArallel training paradigm for effective LLM Fine-Tuning, which independently performs SFT and preference alignment (e.g., DPO and ORPO, etc.) with the same pre-trained model on respective datasets. The model produced by SFT and the model from preference alignment are then merged into a final model by parameter fusing for use in downstream applications. This work reveals important findings that preference alignment like DPO naturally results in a sparse model while SFT leads to a natural dense model which needs to be sparsified for effective model merging. This paper introduces an effective interference resolution which reduces the redundancy by sparsifying the delta parameters. The LLM resulted from the new training paradigm achieved Rank #1 on the HuggingFace Open LLM Leaderboard. Comprehensive evaluation shows the effectiveness of the parallel training paradigm."
    },
    {
        "title": "ClinicalLab: Aligning Agents for Multi-Departmental Clinical Diagnostics in the Real World",
        "link_suffix": "/forum?id=YAMlVKRLnc",
        "link": "https://openreview.net/forum?id=YAMlVKRLnc",
        "pdf_link": "https://openreview.net/pdf?id=YAMlVKRLnc",
        "keywords": "Large language models, Medical Benchmark, Medical Agents, Medical Metrics",
        "abstract": "Large language models (LLMs) have achieved significant performance progress in various natural language processing applications. However, LLMs still struggle to meet the strict requirements for accuracy and reliability in the medical field and face many challenges in clinical applications. Existing clinical diagnostic evaluation benchmarks for evaluating medical agents powered by LLMs have severe limitations. Firstly, most existing medical evaluation benchmarks face the risk of data leakage or contamination. Secondly, existing benchmarks often neglect the characteristics of multiple departments and specialization in modern medical practice. Thirdly, existing evaluation methods are limited to multiple-choice questions, which do not align with the real-world diagnostic scenarios. Lastly, existing evaluation methods lack comprehensive evaluations of end-to-end real clinical scenarios. These limitations in benchmarks in turn obstruct advancements of LLMs and agents for medicine. To address these limitations, we introduceClinicalLab, a comprehensive clinical diagnosis agent alignment suite. ClinicalLab includesClinicalBench, an end-to-end multi-departmental clinical diagnostic evaluation benchmark for evaluating medical agents and LLMs. ClinicalBench is based on real cases that cover 24 departments and 150 diseases. We ensure that ClinicalBench does not have data leakage. ClinicalLab also includes four novel metrics (ClinicalMetrics) for evaluating the effectiveness of LLMs in clinical diagnostic tasks. We evaluate 17 general and medical-domain LLMs and find that their performance varies significantly across different departments. Based on these findings, in ClinicalLab, we proposeClinicalAgent, an end-to-end clinical agent that aligns with real-world clinical diagnostic practices. We systematically investigate the performance and applicable scenarios of variants of ClinicalAgent on ClinicalBench. Our findings demonstrate the importance of aligning with modern medical practices in designing medical agents."
    },
    {
        "title": "FlexBCQ: Flexible Binary-coding Quantization for Large Language Models",
        "link_suffix": "/forum?id=dZ3cI69BE8",
        "link": "https://openreview.net/forum?id=dZ3cI69BE8",
        "pdf_link": "https://openreview.net/pdf?id=dZ3cI69BE8",
        "keywords": "LLM, Quantization, Binary-coding Quantization (BCQ), Uniform Quantization (UQ)",
        "abstract": "How can we compress large language models without compromising accuracy?\nQuantization, which reduces the number of bits for representing weights, is an essential technique to utilize large language models (LLMs) in real-world applications.\nSpecifically, binary-coding quantization (BCQ) is a promising approach since it has extensive representation space, which encompasses the representation space of uniform quantization (UQ), and fast inference speed.\nHowever, because of the lack of accurate optimization techniques, BCQ shows inferior performance compared to UQ algorithms, failing to leverage their powerful expressive power.\nIn this paper, we propose FlexBCQ (Flexible Binary-coding Quantization), an accurate optimization algorithm for BCQ.\nWe leverage the sophisticated optimization techniques of UQ by decomposing the quantization process of BCQ into the composition of a UQ and an inner BCQ.\nAs a result, we take advantage of both the sophisticated optimizing techniques of UQ, specifically the flexible mapping technique, and the powerful expressive capability of BCQ.\nThrough extensive experiments, we find that FlexBCQ provides 3.24%p higher accuracy than existing UQ and BCQ algorithms on MMLU 5-shot benchmark when quantizing a Llama-3 70B model into 3 bits."
    },
    {
        "title": "Enhancing Large Language Models' Situated Faithfulness to External Contexts",
        "link_suffix": "/forum?id=K2jOacHUlO",
        "link": "https://openreview.net/forum?id=K2jOacHUlO",
        "pdf_link": "https://openreview.net/pdf?id=K2jOacHUlO",
        "keywords": "Large Language Model, Knowledge Conflict, Retrieval Augmented Generation, Confidence Estimation, Reasoning",
        "abstract": "Large Language Models (LLMs) are often augmented with external information as contexts, but this external information can sometimes be inaccurate or even intentionally misleading. We argue that robust LLMs should demonstrate situated faithfulness, dynamically calibrating their trust in external information based on their confidence in the internal knowledge and the external context. To benchmark this capability, we evaluate LLMs across several QA datasets, including a newly created dataset featuring in-the-wild incorrect contexts sourced from Reddit posts. We show that when provided with both correct and incorrect contexts, both open-source and proprietary models tend to overly rely on external information, regardless of its factual accuracy. To enhance situated faithfulness, we propose two approaches: Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR). SCR enables models to self-access the confidence of external information relative to their own internal knowledge to produce the most accurate answer. RCR, in contrast, extracts explicit confidence signals from the LLM and determines the final answer using predefined rules. \nOur results show that for LLMs with strong reasoning capabilities, such as GPT-4o and GPT-4o mini, SCR outperforms RCR, achieving improvements of up to 24.2% over a direct input augmentation baseline. Conversely, for a smaller model like Llama-3-8B, RCR outperforms SCR. Fine-tuning SCR with our proposed Confidence Reasoning Direct Preference Optimization (CR-DPO) method improves performance on both seen and unseen datasets, yielding an average improvement of 8.9% on Llama-3-8B. In addition to quantitative results, we offer insights into the relative strengths of SCR and RCR. Our findings highlight promising avenues for improving situated faithfulness in LLMs."
    },
    {
        "title": "Heavy-Tailed Diffusion Models",
        "link_suffix": "/forum?id=tozlOEN4qp",
        "link": "https://openreview.net/forum?id=tozlOEN4qp",
        "pdf_link": "https://openreview.net/pdf?id=tozlOEN4qp",
        "keywords": "Generative models, Diffusion models, Heavy-Tailed Distributions",
        "abstract": "Diffusion models achieve state-of-the-art generation quality across many applications, but their ability to capture rare or extreme events in heavy-tailed distributions remains unclear. In this work, we show that traditional diffusion and flow-matching models with standard Gaussian priors fail to accurately capture heavy-tailed behavior. We address this by repurposing the diffusion framework for heavy-tail estimation using multivariate Student-t distributions. We develop a tailored perturbation kernel and derive the denoising posterior based on the conditional Student-t distribution for the backward process. Inspired by $\\gamma$-divergence for heavy-tailed distributions, we derive a training objective for heavy-tailed denoisers. The resulting framework introduces controllable tail generation using only a single scalar hyperparameter, making it easily tunable for diverse real-world distributions. As specific instantiations of our framework, we introduce t-EDM and t-Flow, extensions of existing diffusion and flow models that employ a Student-t prior. Remarkably, our approach is readily compatible with standard Gaussian diffusion models and requires only minimal code changes. Empirically, we show that our t-EDM and t-Flow outperform standard diffusion models in heavy-tail estimation on high-resolution weather datasets in which generating rare and extreme events is crucial."
    },
    {
        "title": "LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization",
        "link_suffix": "/forum?id=VpWki1v2P8",
        "link": "https://openreview.net/forum?id=VpWki1v2P8",
        "pdf_link": "https://openreview.net/pdf?id=VpWki1v2P8",
        "keywords": "optimization, LoRA",
        "abstract": "Low-rank adaption (LoRA) is a widely used parameter-efficient finetuning method for LLM that reduces memory requirements. However, current LoRA optimizers lack transformation invariance, meaning the updates depending on how the two LoRA factors are scaled or rotated. This deficiency leads to inefficient learning and sub-optimal solutions in practice. This paper introduces LoRA-RITE, a novel adaptive matrix preconditioning method for LoRA optimization, which can achieve transformation invariance and remain computationally efficient. We provide theoretical analysis to demonstrate the benefit of our method and conduct experiments on various LLM tasks with different models including Gemma 2B, 7B, and mT5-XXL. The results demonstrate consistent improvements against existing optimizers. For example, replacing Adam with LoRA-RITE during LoRA fine-tuning of Gemma-2B yielded 4.6% accuracy gain on Super-Natural Instructions and 3.5% accuracy gain across other four LLM benchmarks (HellaSwag, ArcChallenge, GSM8K, OpenBookQA)."
    },
    {
        "title": "GL-Fusion: Rethinking the Combination of Graph Neural Network and Large Language model",
        "link_suffix": "/forum?id=exnoX9Iaik",
        "link": "https://openreview.net/forum?id=exnoX9Iaik",
        "pdf_link": "https://openreview.net/pdf?id=exnoX9Iaik",
        "keywords": "GNN, LLM",
        "abstract": "Recent research on integrating Large Language Models (LLMs) with Graph Neural Networks (GNNs) typically follows two approaches: LLM-centered models, which convert graph data into tokens for LLM processing, and GNN-centered models, which use LLMs to encode text features into node and edge representations for GNN input. LLM-centered models often struggle to capture graph structures effectively, while GNN-centered models compress variable-length textual data into fixed-size vectors, limiting their ability to understand complex semantics. Additionally, GNN-centered approaches require converting tasks into a uniform, manually-designed format, restricting them to classification tasks and preventing language output. To address these limitations, we introduce a new architecture that deeply integrates GNN with LLM, featuring three key innovations: (1) Structure-Aware Transformers, which incorporate GNN\u2019s message-passing capabilities directly into LLM\u2019s transformer layers, allowing simultaneous processing of textual and structural information and generating outputs from both GNN and LLM; (2) Graph-Text Cross-Attention, which processes full, uncompressed text from graph nodes and edges, ensuring complete semantic integration; and (3) GNN-LLM Twin Predictor, enabling LLM\u2019s flexible autoregressive generation alongside GNN\u2019s scalable one-pass prediction. GL-Fusion achieves outstand performance on various tasks. Notably, it achieves state-of-the-art performance on OGBN-Arxiv and OGBG-Code2,"
    },
    {
        "title": "Riemannian Low-Rank Adaptation for Federated Fine-Tuning of Foundation Models",
        "link_suffix": "/forum?id=lbasmwFWzH",
        "link": "https://openreview.net/forum?id=lbasmwFWzH",
        "pdf_link": "https://openreview.net/pdf?id=lbasmwFWzH",
        "keywords": "Rank-adaptive LoRA, Federated Learning, Fine-Tuning, Foundation Models, Riemannian Theory",
        "abstract": "Rank-adaptive low-rank adaptation (LoRA), a parameter-efficient fine-tuning (PEFT) technology, has achieved state-of-the-art performance in fine-tuning foundation models (FM). Directly transplanting the rank-adaptive LoRA methods from centralized learning to federated learning raises two critical issues: client drift and rank drift. This paper presents a Riemannian LoRA algorithm with adaptive rank for federated fine-tuning of foundation models (FFT-FM), RAFFT, which solves the client-drift and rank-drift issues, and significantly improves the computational cost. First, by utilizing Riemannian Procrustes analysis, we propose a Riemannian parameter matching method to avoid the client-drift issue for ensuring the effectiveness of FFT-FM with rank-adaptive LoRA, and to reduce the cost of matrix decomposition by transforming the singular value decomposition (SVD) of high-dimensional full parameter matrices into the SVD of low-dimensional $r \\times r$ matrices, where $r$ is the rank parameter in the LoRA. We theoretically derive the equivalence between our RAFFT algorithm with rank-adaptive LoRA for the FFT-FM and the standard FFT-FM on the full parameter matrices based on FedAvg and verify the bounded error introduced by approximation and numerical errors. Second, by leveraging Riemannian manifold theory, we develop a Riemannian gradient descent (RGD) method to guarantee the local full parameter matrices on clients in the form of low-rank ones with fixed rank optimized by the server in each FFT-FM round, for alleviating the rank-drift issue to speed up the convergence of RAFFT. We theoretically demonstrate that the RGD optimization on the Riemannian manifold ensures the rank invariance during the local update process and the RGD optimization can converge in the FFT-FM context."
    },
    {
        "title": "Efficient Inference for Large Language Model-based Generative Recommendation",
        "link_suffix": "/forum?id=ACSNlt77hq",
        "link": "https://openreview.net/forum?id=ACSNlt77hq",
        "pdf_link": "https://openreview.net/pdf?id=ACSNlt77hq",
        "keywords": "LLM-based Generative Recommendation, Speculative Decoding, Decoding Acceleration",
        "abstract": "Large Language Model (LLM)-based generative recommendation has achieved notable success, yet its practical deployment is costly particularly due to excessive inference latency caused by autoregressive decoding. For lossless LLM decoding acceleration, Speculative Decoding (SD) has emerged as a promising solution. However, applying SD to generative recommendation presents unique challenges due to the requirement of generating top-K items (i.e., K distinct token sequences) as a recommendation list by beam search. This leads to more stringent verification in SD, where all the top-K sequences from the target LLM must be successfully drafted by the draft model at each decoding step. To alleviate this, we consider 1) boosting top-K sequence alignment between the draft model and the target LLM, and 2) relaxing the verification strategy to reduce trivial LLM calls. To this end, we propose an alignment framework named AtSpeed, which presents the AtSpeed-S optimization objective for top-K alignment under the strict top-K verification. Moreover, we introduce a relaxed sampling verification strategy that allows high-probability non-top-K drafted sequences to be accepted, significantly reducing LLM calls. Correspondingly, we propose AtSpeed-R for top-K alignment under this relaxed sampling verification. Empirical results on two real-world datasets demonstrate that AtSpeed significantly accelerates LLM-based generative recommendation, e.g., near 2x speedup under strict top-K verification and up to 2.5 speedup under relaxed sampling verification. The codes and datasets are available at~\\url{https://anonymous.4open.science/r/AtSpeed/}."
    },
    {
        "title": "Spatial 3D-LLM: Progressive Spatial Awareness for Advanced 3D Vision-Language Understanding",
        "link_suffix": "/forum?id=JzLcKWtGnl",
        "link": "https://openreview.net/forum?id=JzLcKWtGnl",
        "pdf_link": "https://openreview.net/pdf?id=JzLcKWtGnl",
        "keywords": "3D-MLLM, 3D scene understanding",
        "abstract": "New era has unlocked exciting possibilities for extending Large Language Models (LLMs) to tackle 3D vision-language tasks. However, most existing 3D Multimodal LLMs (MLLMs) rely on holistic 3D scene information or specifically designated regions for 3D vision-language tasks, failing to capture multi-level location-based information.\nAddressing these concerns, we present Spatial 3D-LLM, a 3D MLLM specifically designed to enhance spatial perception and reasoning for 3D vision-language tasks by enriching the spatial embeddings of 3D scenes.\nSpatial 3D-LLM incorporates an LLM backbone and a meticulously designed progressive spatial awareness scheme that captures spatial information as the perception field expands, generating location-enriched 3D scene embeddings that serve as visual prompt.\nAdditionally, we introduce two novel tasks, namely 3D object distance measurement and 3D layout editing, and construct a 3D instruction dataset MODEL, to inspire more profound 3D spatial perception capabilities.\nExperimental results demonstrate that Spatial 3D-LLM achieves state-of-the-art performance across a wide range of 3D vision-language tasks, revealing the improvements stemmed from our progressive spatial awareness scheme of mining more profound spatial information and the proposed dataset."
    },
    {
        "title": "VRM: Knowledge Distillation via Virtual Relation Matching",
        "link_suffix": "/forum?id=U41Opah9lB",
        "link": "https://openreview.net/forum?id=U41Opah9lB",
        "pdf_link": "https://openreview.net/pdf?id=U41Opah9lB",
        "keywords": "Knowledge distillation, transfer learning, virtual knowledge learning",
        "abstract": "Knowledge distillation (KD) aims to transfer the knowledge of a more capable yet cumbersome teacher model to a lightweight student model. In recent years, relation-based KD methods have fallen behind, as instance-matching counterparts dominate in performance. In this paper, we revive relational KD by identifying and tackling several key issues in relational KD, including its susceptibility to overfitting and spurious responses. Specifically, we transfer novelly constructed affinity graphs that compactly encapsulate a wealth of beneficial inter-sample, inter-class, and inter-view correlations by exploiting virtual views and relations as a new kind of knowledge. As a result, the student has access to rich guidance signals and stronger regularisation throughout the distillation process. To further mitigate the adverse impact of spurious responses, we prune the affinity graphs by dynamically detaching redundant and unreliable edges. Extensive experiments on CIFAR-100, ImageNet, and MS-COCO datasets demonstrate the superior performance of the proposed virtual relation matching (VRM) method over a range of tasks, architectures, and set-ups. For instance, VRM for the first time hits 74.0% accuracy for ResNet50-to-MobileNetV2 distillation on ImageNet, and improves DeiT-Ti by 14.11% on CIFAR-100 with a ResNet56 teacher. Thorough analyses are also conducted to gauge the soundness, properties, and complexity of our designs. Code and models will be released."
    },
    {
        "title": "Potential Outcomes Estimation Under Hidden Confounders",
        "link_suffix": "/forum?id=5AJ8R4z5g0",
        "link": "https://openreview.net/forum?id=5AJ8R4z5g0",
        "pdf_link": "https://openreview.net/pdf?id=5AJ8R4z5g0",
        "keywords": "Confounders, Causal Inference, Treatment Effects",
        "abstract": "One of the major challenges in estimating conditional potential outcomes and the conditional average treatment effects (CATE) is the presence of hidden confounders. Since testing for hidden confounders cannot be accomplished only with observational data, conditional unconfoundedness is commonly assumed in the literature of CATE estimation. Nevertheless, under this assumption, CATE estimation can be significantly biased due to the effects of unobserved confounders. In this work, we consider the case where in addition to a potentially large observational dataset, a small dataset from a randomized controlled trial (RCT) is available. \nNotably, we make no assumptions on the existence of any covariate information for the RCT dataset, only requiring the outcomes to be observed. We propose a CATE estimation method based on a pseudo-confounder generator and a CATE model that aligns the learned potential outcomes from the observational data with those observed from the RCT. Our method is applicable to many practical scenarios of interest, particularly when privacy is under concern (e.g., medical applications). Extensive numerical experiments are provided demonstrating the effectiveness of our approach for both synthetic and real-world datasets."
    },
    {
        "title": "On High-Dimensional Action Selection for Deep Reinforcement Learning",
        "link_suffix": "/forum?id=rto6aU453A",
        "link": "https://openreview.net/forum?id=rto6aU453A",
        "pdf_link": "https://openreview.net/pdf?id=rto6aU453A",
        "keywords": "Deep Reinforcement Learning, False Discovery Control, High-Dimensional Action Space, Online Learning, Variable Selection",
        "abstract": "With recent advances in deep reinforcement learning (RL),high-dimensional action selectionhas become an important yet challenging problem in many real applications, especially in unknown and complex environments. Existing works often require a sophisticated prior design to eliminate redundancy in the action space, relying heavily on domain expert experience or involving high computational complexity, which limits their generalizability across different RL tasks. In this paper, we address these challenges by proposing a general data-driven action selection approach with model-free and computational-friendly properties. Our method not onlyselects minimal sufficient actionsbut alsocontrols the false discovery ratevia knockoff sampling. More importantly, we seamlessly integrate the action selection into deep RL methods during online training. Empirical experiments validate the established theoretical guarantees, demonstrating that our method surpasses various alternative techniques in terms of both performances in variable selection and overall achieved rewards."
    },
    {
        "title": "Refine Knowledge of Large Language Models via Adaptive Contrastive Learning",
        "link_suffix": "/forum?id=HqjRlT65WX",
        "link": "https://openreview.net/forum?id=HqjRlT65WX",
        "pdf_link": "https://openreview.net/pdf?id=HqjRlT65WX",
        "keywords": "Large Language Models, Model Hallucination, Model Alignment",
        "abstract": "How to alleviate the hallucinations of Large Language Models (LLMs) has always been the fundamental goal pursued by the LLMs research community. Looking through numerous hallucination-related studies, a mainstream category of methods is to reduce hallucinations by optimizing the knowledge representation of LLMs to change their output. Considering that the core focus of these works is the knowledge acquired by models, and knowledge has long been a central theme in human societal progress, we believe that the process of models refining knowledge can greatly benefit from the way humans learn. In our work, by imitating the human learning process, we design an Adaptive Contrastive Learning strategy. Our method flexibly constructs different positive and negative samples for contrastive learning based on LLMs' actual mastery of knowledge. This strategy helps LLMs consolidate the correct knowledge they already possess, deepen their understanding of the correct knowledge they have encountered but not fully grasped, forget the incorrect knowledge they previously learned, and honestly acknowledge the knowledge they lack. Extensive experiments and detailed analyses on widely used datasets demonstrate the effectiveness and competitiveness of our method."
    },
    {
        "title": "Tests as Instructions: A Test-Driven-Development Benchmark for LLM Code Generation",
        "link_suffix": "/forum?id=sqciWyTm70",
        "link": "https://openreview.net/forum?id=sqciWyTm70",
        "pdf_link": "https://openreview.net/pdf?id=sqciWyTm70",
        "keywords": "test, llm, coding, benchnark, instruction",
        "abstract": "This paper focuses on test-driven development (TDD) tasks, where test cases act as both instruction and verification for LLM code generation. We build a TDD benchmark to evaluate frontier models, where reasoning models of OpenAI achieve SOTA. We identify instruction following and in-context learning as the critical abilities for all models to succeed at TDD tasks. We further reveal their vulnerabilities to long instructions as an area of improvement."
    },
    {
        "title": "ChebyNet: Boosting Neural Network Fitting and Efficiency through Chebyshev Polynomial Layer Connections",
        "link_suffix": "/forum?id=17U3nlco2r",
        "link": "https://openreview.net/forum?id=17U3nlco2r",
        "pdf_link": "https://openreview.net/pdf?id=17U3nlco2r",
        "keywords": "DNN, Chebyshev Polynomial",
        "abstract": "Traditional deep neural networks (DNNs) predominantly adhere to a similar design paradigm. Even with the incorporation of additive shortcuts, they lack explicit modeling of relationships between non-adjacent layers. Consequently, this paradigm constrains the fitting capabilities of existing DNNs. To address this issue, we propose ChebyNet, a novel network paradigm to build Chebyshev polynomial connections between general network layers. Specifically, we establish recursive relationship among adjacent layers and polynomial relationship between non-adjacent layers to construct ChebyNet, which improves representation capabilities of the network. Experimentally, we comprehensively evaluate ChebyNet on diverse tasks, including function approximation, semantic segmentation, and  visual recognition. Across all these tasks, ChebyNet consistently outperforms traditional neural networks under identical training conditions, demonstrating superior efficiency and fitting properties. Our findings underscore the potential of polynomial-based layer connections to significantly enhance neural network performance, offering a promising direction for future deep learning architectures."
    }
]