[
    {
        "title": "Koopman Universal Neural Dynamic Operator: Achieving Fully Explicit Expression Identification for Nonlinear Dynamical Systems",
        "link_suffix": "/forum?id=SQl6T4dfs8",
        "link": "https://openreview.net/forum?id=SQl6T4dfs8",
        "pdf_link": "https://openreview.net/pdf?id=SQl6T4dfs8",
        "keywords": "neural networks, Koopman operator, universal approximation theorem, system identification, dynamics system modeling",
        "abstract": "Complex nonlinear systems permeate various scientific and engineering domains, presenting significant challenges in accurate modeling and analysis. This paper introduces the Koopman Universal Neural Dynamic Operator (KUNDO), a groundbreaking framework that bridges the gap between data-driven machine learning approaches and traditional mathematical modeling. KUNDO uniquely combines neural networks, Koopman operator theory, and the universal approximation theorem to achieve fully explicit expression identification for complex nonlinear systems. Our framework demonstrates remarkable efficiency in small sample scenarios, overcoming limitations of both classical physical models and black-box machine learning techniques. By learning Koopman-compatible basis functions through neural networks, KUNDO transforms high-dimensional, strongly nonlinear dynamics into interpretable mathematical forms, greatly decreasing the limitations of human selection of basis functions without sacrificing predictive power. We present theoretical analyses of KUNDO's mathematical properties and validate its performance across diverse nonlinear systems. The results showcase KUNDO's potential to revolutionize system identification, offering new avenues for scientific discovery and engineering applications in fields such as climate science, financial modeling, and advanced robotics. This work presents a significant advance towards interpretable AI and data-driven modeling in systems analysis."
    },
    {
        "title": "On Rollouts in Model-Based Reinforcement Learning",
        "link_suffix": "/forum?id=Uh5GRmLlvt",
        "link": "https://openreview.net/forum?id=Uh5GRmLlvt",
        "pdf_link": "https://openreview.net/pdf?id=Uh5GRmLlvt",
        "keywords": "Model-Based Reinforcement Learning, Model Rollouts, Uncertainty Quantification",
        "abstract": "Model-based reinforcement learning (MBRL) seeks to enhance data efficiency by learning a model of the environment and generating synthetic rollouts from it. However, accumulated model errors during these rollouts can distort the data distribution, negatively impacting policy learning and hindering long-term planning. Thus, the accumulation of model errors is a key bottleneck in current MBRL methods. We propose Infoprop, a model-based rollout mechanism that separates aleatoric from epistemic model uncertainty and reduces the influence of the latter on the data distribution. Further, Infoprop keeps track of accumulated model errors along a model rollout and provides termination criteria to limit data corruption. We demonstrate the capabilities of Infoprop in the Infoprop-Dyna algorithm, reporting state-of-the-art performance in Dyna-style MBRL on common MuJoCo benchmark tasks while substantially increasing rollout length and data quality."
    },
    {
        "title": "Source-Free Target Domain Confidence Calibration",
        "link_suffix": "/forum?id=YRm9BMTLv6",
        "link": "https://openreview.net/forum?id=YRm9BMTLv6",
        "pdf_link": "https://openreview.net/pdf?id=YRm9BMTLv6",
        "keywords": "confidence calibration, domain adaptation, source-free",
        "abstract": "In this study, we consider the setup of source-free domain adaptation and address the challenge of calibrating the confidence of a model adapted to the target domain using only unlabeled data. The primary challenge in addressing uncertainty calibration is the absence of labeled data which prevents computing the accuracy of the adapted network on the target domain. We address this by leveraging pseudo-labels generated from the source model\u2019s predictions to estimate the true, unobserved accuracy. We demonstrate that, although the pseudo-labels are noisy, the network accuracy calculated using these pseudo-labels is similar to the accuracy obtained with the correct labels. We validate the effectiveness of our calibration approach by applying it to standard domain adaptation datasets and show that it achieves results comparable to, or even better than, previous calibration methods that relied on the availability of labeled source data."
    },
    {
        "title": "On Characterizing and Mitigating Imbalances in Multi-Instance Partial Label Learning",
        "link_suffix": "/forum?id=oZdaEiDBpF",
        "link": "https://openreview.net/forum?id=oZdaEiDBpF",
        "pdf_link": "https://openreview.net/pdf?id=oZdaEiDBpF",
        "keywords": "muliti-instance partial label learning, weakly-supervised learning, neurosymbolic learning, learning theory, long-tailed learning, learning imbalances, class-specific error bounds",
        "abstract": "\\textit{Multi-Instance Partial Label Learning} (MI-PLL) is a weakly-supervised learning setting encompassing \\textit{partial label learning}, \\textit{latent structural learning}, and \\textit{neurosymbolic learning}. Unlike supervised learning, in MI-PLL, the inputs to the classifiers at training-time are tuples of instances $\\mathbf{x}$. At the same time, the supervision signal is generated by a function $\\sigma$ over the (hidden) gold labels of $\\mathbf{x}$. In this work, we make multiple contributions towards addressing a problem that hasn\u2019t been studied so far in the context of MI-PLL: that of characterizing and mitigating \\textit{learning imbalances}, i.e., major differences in the errors occurring when classifying instances of different classes (aka \\emph{class-specific risks}). In terms of theory, we derive class-specific risk bounds for MI-PLL, while making minimal assumptions. Our theory reveals a unique phenomenon: that $\\sigma$ can greatly impact learning imbalances. This result is in sharp contrast with previous research on supervised and weakly-supervised learning, which only studies learning imbalances under the prism of data imbalances. On the practical side, we introduce a technique for estimating the marginal of the hidden labels using only MI-PLL data. Then, we introduce algorithms that mitigate imbalances at training- and testing-time, by treating the marginal of the hidden labels as a constraint. We demonstrate the effectiveness of our techniques using strong baselines from neurosymbolic and long-tail learning, suggesting performance improvements of up to 14%."
    },
    {
        "title": "LLM-VTP: LLM-Reasoned Visual Token Pruning for Efficient Multi-Modal Video Understanding",
        "link_suffix": "/forum?id=Acdd83rF1s",
        "link": "https://openreview.net/forum?id=Acdd83rF1s",
        "pdf_link": "https://openreview.net/pdf?id=Acdd83rF1s",
        "keywords": "Video Understanding, Token Pruning",
        "abstract": "In this paper, we introduce LLM-VTP, a visual token pruning method designed to enhance the efficiency of multi-modal video understanding. Large Language Models (LLMs) have shown promising performance in video tasks due to their extended capabilities in comprehending visual modalities. However, the substantial redundancy in video data presents significant computational challenges for LLMs. To address this, we propose a training-free approach that leverages the inherent reasoning abilities of LLMs to selectively prune visual features based on question tokens, thereby optimizing model efficiency. We validate our method across multiple-choice, open-ended, and text-generation benchmarks. Our results demonstrate that LLM-VTP can prune 80%-90% of tokens while maintaining competitive performance. This highlights its superior effectiveness and efficiency compared to existing pruning methods. The source code will be released to facilitate future research."
    },
    {
        "title": "MTSAM: Multi-Task Fine-Tuning for Segment Anything Model",
        "link_suffix": "/forum?id=6N4QMbeVaO",
        "link": "https://openreview.net/forum?id=6N4QMbeVaO",
        "pdf_link": "https://openreview.net/pdf?id=6N4QMbeVaO",
        "keywords": "Multi-task learning, segment anything model, low-rank adaptation",
        "abstract": "The Segment Anything Model (SAM), with its remarkable zero-shot capability, has the potential to be a foundation model for multi-task learning. However, adopting SAM to multi-task learning faces two challenges: (a) SAM has difficulty generating task-specific outputs with different channel numbers, and (b) how to fine-tune SAM to adapt multiple downstream tasks simultaneously remains unexplored. To address these two challenges, in this paper, we propose the Multi-Task SAM (MTSAM) framework, which enables SAM to work as a foundation model for multi-task learning. MTSAM modifies SAM's architecture by removing the prompt encoder and implementing task-specific no-mask embeddings and mask decoders, enabling the generation of task-specific outputs. Furthermore, we introduce Tensorized low-Rank Adaptation (ToRA) to perform multi-task fine-tuning on SAM. Specifically, ToRA injects an update parameter tensor into each layer of the encoder in SAM and leverages a low-rank tensor decomposition method to incorporate both task-shared and task-specific information.\nExtensive experiments conducted on benchmark datasets substantiate the efficacy of MTSAM in enhancing the performance of multi-task learning."
    },
    {
        "title": "Rethinking Out-of-Distribution Detection in Vision Foundation Models",
        "link_suffix": "/forum?id=awReGYZaGl",
        "link": "https://openreview.net/forum?id=awReGYZaGl",
        "pdf_link": "https://openreview.net/pdf?id=awReGYZaGl",
        "keywords": "Out-of-Distribution Detection; Vision Foundation Model; Mixture of Experts",
        "abstract": "Pre-trained vision foundation models have transformed many computer vision tasks.  Despite their strong ability to learn discriminative and generalizable features-- crucial for out-of-distribution (OOD) detection, their impact on this task remains underexplored. Motivated by this gap, our study investigates vision foundation models in OOD detection. Our findings show that even without complex designs, a pre-trained DINOv2 model, utilizing a simple scoring metric and no fine-tuning, outperforms all prior state-of-the-art models, which typically depend on fine-tuning with in-distribution (ID) data. Furthermore, while the pre-trained CLIP model struggles with fine-grained OOD samples, DINOv2 excels, revealing the limitations of CLIP in this setting. Building on these insights, we explore how foundation models can be further optimized for both ID classification and OOD detection when ID data is available for fine-tuning. From a model perspective, we propose a Mixture of Feature Experts (MoFE) module, which partitions features into subspaces. This mitigates the challenge of tuning complex data distributions with limited ID data and enhances decision boundary learning for classification. From a data perspective, we introduce a Dynamic-$\\beta$ Mixup strategy, which samples interpolation weights from a dynamic beta distribution. This adapts to varying levels of learning difficulty across categories, improving feature learning for more challenging categories. Extensive experiments and ablation studies demonstrate the effectiveness of our approach, significantly outperforming baseline methods."
    },
    {
        "title": "On Generalization Within Multi-Objective Reinforcement Learning Algorithms",
        "link_suffix": "/forum?id=tuEP424UQ5",
        "link": "https://openreview.net/forum?id=tuEP424UQ5",
        "pdf_link": "https://openreview.net/pdf?id=tuEP424UQ5",
        "keywords": "Reinforcement Learning, Multi-Objective Reinforcement Learning, Generalization",
        "abstract": "Real-world sequential decision-making tasks often require balancing trade-offs between multiple conflicting objectives, making Multi-Objective Reinforcement Learning (MORL) an increasingly prominent field of research. Despite recent advances, existing MORL literature has narrowly focused on performance within static environments, neglecting the importance of generalizing across diverse settings. Conversely, existing research on generalization in RL has always assumed scalar rewards, overlooking the inherent multi-objectivity of real-world problems. Generalization in the multi-objective context is fundamentally more challenging, as it requires learning a Pareto set of policies addressing varying preferences across multiple objectives. In this paper, we formalize the concept of generalization in MORL and how it can be evaluated. We then contribute a novel testbed featuring diverse multi-objective domains with parameterized environment configurations to facilitate future studies in this area. Our baseline evaluations of state-of-the-art MORL algorithms on this testbed reveals limited generalization capabilities, suggesting significant room for improvement. Our empirical findings also expose limitations in the expressivity of scalar rewards, emphasizing the need for multi-objective specifications to achieve effective generalization. We further analyzed the algorithmic complexities within current MORL approaches that could impede the transfer in performance from the single- to multiple-environment settings. This work fills a critical gap and lays the groundwork for future research that brings together two key areas in reinforcement learning: solving multi-objective decision-making problems and generalizing across diverse environments."
    },
    {
        "title": "Ultra-Sparse Memory Network",
        "link_suffix": "/forum?id=zjeHLSiNv1",
        "link": "https://openreview.net/forum?id=zjeHLSiNv1",
        "pdf_link": "https://openreview.net/pdf?id=zjeHLSiNv1",
        "keywords": "Large language model, sparse model, scaling law",
        "abstract": "It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget."
    },
    {
        "title": "VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation",
        "link_suffix": "/forum?id=UpQLu9bzAR",
        "link": "https://openreview.net/forum?id=UpQLu9bzAR",
        "pdf_link": "https://openreview.net/pdf?id=UpQLu9bzAR",
        "keywords": "reward learning, reinforcement learning, long-horizon robot learning, vision-language",
        "abstract": "We study reward models for long-horizon manipulation by learning from action-free videos and language instructions, which we term the visual-instruction correlation (VIC) problem. Existing VIC methods face challenges in learning rewards for long-horizon tasks due to their lack of sub-stage awareness, difficulty in modeling task complexities, and inadequate object state estimation. To address these challenges,\nwe introduce VICtoR, a novel hierarchical VIC reward model capable of providing effective reward signals for long-horizon manipulation tasks. Trained solely on primitive motion demonstrations, VICtoR effectively provides precise reward signals for long-horizon tasks by assessing task progress at various stages using a novel stage detector and motion progress evaluator. We conducted extensive experiments in both simulated and real-world datasets. The results suggest that VICtoR outperformed the best existing methods, achieving a 43% improvement in success rates for long-horizon tasks."
    },
    {
        "title": "Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics",
        "link_suffix": "/forum?id=O9YTt26r2P",
        "link": "https://openreview.net/forum?id=O9YTt26r2P",
        "pdf_link": "https://openreview.net/pdf?id=O9YTt26r2P",
        "keywords": "mechanistic interpretability, interpretability, arithmetic, nlp, language models",
        "abstract": "Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types---such as neurons that activate when an operand falls within a certain range---and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a ``bag of heuristics''."
    },
    {
        "title": "GLEE: A Framework and Benchmark for LLM Evaluation in Language-based Economics",
        "link_suffix": "/forum?id=o8vCBFonHC",
        "link": "https://openreview.net/forum?id=o8vCBFonHC",
        "pdf_link": "https://openreview.net/pdf?id=o8vCBFonHC",
        "keywords": "Language-based games, Language-based economic environment, LLM benchmark, LLM Framework",
        "abstract": "Large Language Models (LLMs) show significant potential in economic and strategic interactions, where communication via natural language is often prevalent. This raises key questions: Do LLMs behave rationally? Can they mimic human behavior? Do they tend to reach an efficient and fair outcome? What is the role of natural language in the strategic interaction? How do characteristics of the economic environment influence these dynamics?\nThese questions become crucial concerning the economic and societal implications of integrating LLM-based agents into real-world data-driven systems, such as online retail platforms and recommender systems.\nWhile the ML community has been exploring the potential of LLMs in such multi-agent setups, varying assumptions, design choices and evaluation criteria across studies make it difficult to draw robust and meaningful conclusions. To address this, we introduce a benchmark for standardizing research on two-player, sequential, language-based games. Inspired by the economic literature, we define three base families of games with consistent parameterization, degrees of freedom and economic measures to evaluate agents' performance (self-gain), as well as the game outcome (efficiency and fairness). We develop an open-source framework for interaction simulation and analysis, and utilize it to collect a dataset of LLM vs. LLM interactions across numerous game configurations and an additional dataset of human vs. LLM interactions.\nThrough extensive experimentation, we demonstrate how our framework and dataset can be used to: (i) compare the behavior of LLM-based agents to human players in various economic contexts; (ii) evaluate agents in both individual and collective performance measures; and (iii) quantify the effect of the economic characteristics of the environments on the behavior of agents. We believe that our framework can contribute to the growing intersection of LLMs, ML, and economics, and we encourage researchers to explore it further and build on its foundation."
    },
    {
        "title": "Efficient Fairness-Performance Pareto Front Computation",
        "link_suffix": "/forum?id=VgtpRXhxli",
        "link": "https://openreview.net/forum?id=VgtpRXhxli",
        "pdf_link": "https://openreview.net/pdf?id=VgtpRXhxli",
        "keywords": "Fairness, fair Representations, Fairness-Performance Pareto Front, Pareto Front, convex concave optimization",
        "abstract": "There is a well known intrinsic trade-off between the fairness of a representation and the performance of classifiers derived from the representation. \nDue to the complexity of optimisation algorithms in most modern representation learning approaches, for a given method it may be non-trivial to decide whether the obtained fairness-performance curve of the  method is optimal, i.e., whether it is close to the true Pareto front for these quantities for the underlying data distribution.In this paper we propose a new method to compute the optimal Pareto front, \nwhich does not require the training of complex representation models. We show that optimal fair representations possess several useful structural properties, and that these properties enable a reduction of the computation of the Pareto Front to a compact discrete problem. We then also show that these compact approximating problems can be efficiently solved via off-the shelf concave-convex programming methods. Finally, in addition to representations, we show that the new methods may also be used to directly compute the Pareto front of fair classification problems.Since our approach is independent of the specific model of representations, it may be used as the  benchmark to which representation learning algorithms, or classifiers, may be compared. We experimentally evaluate the approach on a number of real world benchmark datasets."
    },
    {
        "title": "FuseChat: Knowledge Fusion of Chat Models",
        "link_suffix": "/forum?id=15UetYngA7",
        "link": "https://openreview.net/forum?id=15UetYngA7",
        "pdf_link": "https://openreview.net/pdf?id=15UetYngA7",
        "keywords": "Model Fusion, Large Language Models",
        "abstract": "While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, it incurs substantial costs and may lead to redundancy in competencies. Knowledge fusion aims to integrate existing LLMs of diverse architectures and capabilities into a more potent LLM through lightweight continual training, thereby reducing the need for costly LLM development. In this work, we propose a new framework for the knowledge fusion of chat LLMs through two main stages, resulting in FuseChat. Firstly, we conduct pairwise knowledge fusion on source chat LLMs of varying structures and scales to create multiple target LLMs with identical structure and size via lightweight fine-tuning. During this process, a statistics-based token alignment approach is introduced as the cornerstone for fusing LLMs with different structures. Secondly, we merge these target LLMs within the parameter space, where we propose a novel method for determining the merging coefficients based on the magnitude of parameter updates before and after fine-tuning. We implement and validate FuseChat using six prominent chat LLMs with diverse architectures and scales, including OpenChat-3.5-7B, Starling-LM-7B-alpha, NH2-SOLAR-10.7B, InternLM2-Chat-20B, Mixtral-8x7B-Instruct, and Qwen-1.5-Chat-72B. Experimental results on two instruction-following benchmarks, AlpacaEval 2.0 and MT-Bench, demonstrate the superiority of FuseChat-7B over baselines of various sizes. Our model is even comparable to the larger Mixtral-8x7B-Instruct and approaches GPT-3.5-Turbo-1106 on MT-Bench."
    },
    {
        "title": "Learning Randomized Algorithms with Transformers",
        "link_suffix": "/forum?id=UV5p3JZMjC",
        "link": "https://openreview.net/forum?id=UV5p3JZMjC",
        "pdf_link": "https://openreview.net/pdf?id=UV5p3JZMjC",
        "keywords": "Randomized algorithms, Learning under adversarial losses, Adversarial robustness, In-context learning algorithms",
        "abstract": "Randomization is a powerful tool that endows algorithms with remarkable properties. For instance, randomized algorithms excel in adversarial settings, often surpassing the worst-case performance of deterministic algorithms with large margins. Furthermore, their success probability can be amplified by simple strategies such as repetition and majority voting. In this paper, we enhance deep neural networks, in particular transformer models, with randomization. We demonstrate for the first time that randomized algorithms can be instilled in transformers through learning, in a purely data- and objective-driven manner. First, we analyze known adversarial objectives for which randomized algorithms offer a distinct advantage over deterministic ones. We then show that common optimization techniques, such as gradient descent or evolutionary strategies, can effectively learn transformer parameters that make use of the randomness provided to the model. To illustrate the broad applicability of randomization in empowering neural networks, we study three conceptual tasks: associative recall, graph coloring, and agents that explore grid worlds. In addition to demonstrating increased robustness against oblivious adversaries through learned randomization, our experiments reveal remarkable performance improvements due to the inherently random nature of the neural networks' computation and predictions."
    },
    {
        "title": "NEPENTHE: Entropy-Based Pruning as a Neural Network Depth's Reducer",
        "link_suffix": "/forum?id=fk5ePN7YCS",
        "link": "https://openreview.net/forum?id=fk5ePN7YCS",
        "pdf_link": "https://openreview.net/pdf?id=fk5ePN7YCS",
        "keywords": "Pruning, Compression, Entropy, Deep Learning",
        "abstract": "While deep neural networks are highly effective at solving complex tasks, their computational demands can hinder their usefulness in real-time applications and with limited-resources systems. Besides, it is a known fact that, for many downstream tasks, off-the-shelf models are over-parametrized. While classical structured pruning can reduce the network's width, the computation's critical path, namely the maximum number of layers encountered at forward propagation, apparently can not be reduced.In this paper, we aim to reduce the depth of over-parametrized deep neural networks: we propose an eNtropy-basEdPruning as a nEuralNetwork depTH's rEducer (NEPENTHE) to alleviate deep neural networks' computational burden.\nBased on our theoretical finding, NEPENTHE leverages \"unstructured'' pruning to bias sparsity enhancement in layers with low entropy to remove them entirely. We validate our approach on popular architectures such as MobileNet, Swin-T and RoBERTa, showing that, when in the overparametrization regime, some layers are linearizable (hence reducing the model's depth) with little to no performance loss. The code will be publicly available upon acceptance of the article."
    },
    {
        "title": "Diffusion Bridge AutoEncoders for Unsupervised Representation Learning",
        "link_suffix": "/forum?id=hBGavkf61a",
        "link": "https://openreview.net/forum?id=hBGavkf61a",
        "pdf_link": "https://openreview.net/pdf?id=hBGavkf61a",
        "keywords": "Diffusion Model, Represenation Learning, Autoencoders",
        "abstract": "Diffusion-based representation learning has achieved substantial attention due to its promising capabilities in latent representation and sample generation. Recent studies have employed an auxiliary encoder to identify a corresponding representation from data and to adjust the dimensionality of a latent variable $\\mathbf{z}$. Meanwhile, this auxiliary structure invokes aninformation split problem; the information of each data instance $\\mathbf{x}_0$ is divided into diffusion endpoint $\\mathbf{x}_T$ and encoded $\\mathbf{z}$ because there exist two inference paths starting from the data. The latent variable modeled by diffusion endpoint $\\mathbf{x}_T$ has some disadvantages. The diffusion endpoint $\\mathbf{x}_T$ is computationally expensive to obtain and inflexible in dimensionality. To address this problem, we introduce Diffusion Bridge AuteEncoders (DBAE), which enables $\\mathbf{z}$-dependent endpoint $\\mathbf{x}_T$ inference through a feed-forward architecture. This structure creates an information bottleneck at $\\mathbf{z}$, so $\\mathbf{x}_T$ becomes dependent on $\\mathbf{z}$ in its generation. This results in $\\mathbf{z}$ holding the full information of data. We propose an objective function for DBAE to enable both reconstruction and generative modeling, with their theoretical justification. Empirical evidence supports the effectiveness of the intended design in DBAE, which notably enhances downstream inference quality, reconstruction, and disentanglement. Additionally, DBAE generates high-fidelity samples in the unconditional generation."
    },
    {
        "title": "Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models",
        "link_suffix": "/forum?id=pUbbLHjCPM",
        "link": "https://openreview.net/forum?id=pUbbLHjCPM",
        "pdf_link": "https://openreview.net/pdf?id=pUbbLHjCPM",
        "keywords": "progressive thought refinement, self refine, large language model",
        "abstract": "Recent advancements in large language models (LLMs) have demonstrated that progressive refinement, rather than providing a single answer, results in more accurate and thoughtful outputs. However, existing methods often rely heavily on supervision signals to evaluate previous responses, making it difficult to effectively assess output quality in more open-ended scenarios. Additionally, these methods are typically designed for specific tasks, which limits their generalization to new domains. To address these limitations, we propose Progressive Thought Refinement (PTR), a framework that enables LLMs to progressively refine their responses. PTR operates in two phases: (1) Thought data construction stage: We propose a \\textit{weak and strong model collaborative selection} strategy to build a high-quality progressive refinement dataset to ensure logical consistency from thought to answers, and the answers are gradually refined in each round. (2) Thought-Mask Fine-Tuning Phase: We design a training structure to mask the \"thought\" and adjust loss weights to encourage LLMs to refine prior thought, teaching them to implicitly understand \"how to improve\" rather than \"what is correct.\" Experimental results show that PTR significantly enhances LLM performance across ten diverse tasks (avg. from 49.6% to 54.48%) without task-specific fine-tuning. Notably, in more open-ended tasks, LLMs also demonstrate substantial improvements in the quality of responses beyond mere accuracy, suggesting that PTR truly teaches LLMs to self-improve over time. Our project's source code and datasets are available athttps://anonymous.4open.science/r/PTR\\_LLM"
    },
    {
        "title": "Noise Separation guided Candidate Label Reconstruction for Noisy Partial Label Learning",
        "link_suffix": "/forum?id=TOahfjA3sP",
        "link": "https://openreview.net/forum?id=TOahfjA3sP",
        "pdf_link": "https://openreview.net/pdf?id=TOahfjA3sP",
        "keywords": "partial label learning, weakly supervised learning, noisy partial label learning",
        "abstract": "Partial label learning is a weakly supervised learning problem in which an instance is annotated with a set of candidate labels, among which only one is the correct label. However, in practice the correct label is not always in the candidate label set, leading to the noisy partial label learning (NPLL) problem. In this paper, we theoretically prove that the generalization error of the classifier constructed under NPLL paradigm is bounded by the noise rate and the average length of the candidate label set. Motivated by the theoretical guide, we propose a novel NPLL framework that can separate the noisy samples from the normal samples to reduce the noise rate and reconstruct the shorter candidate label sets for both of them. Extensive experiments on multiple benchmark datasets confirm the efficacy of the proposed method in addressing NPLL. For example, on CIFAR10 dataset with severe noise, our method improves the classification accuracy of the state-of-the-art one by 12.85%. The code has been submitted to the supplementary material."
    },
    {
        "title": "SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?",
        "link_suffix": "/forum?id=7DY2Nk9snh",
        "link": "https://openreview.net/forum?id=7DY2Nk9snh",
        "pdf_link": "https://openreview.net/pdf?id=7DY2Nk9snh",
        "keywords": "CLIP, synthetic data, generative",
        "abstract": "We present SynthCLIP, a CLIP model trained on entirely synthetic text-image pairs. Leveraging recent text-to-image (TTI) networks and large language models (LLM), we generate synthetic datasets of images and corresponding captions at scale, with no human intervention. In this work, we provide an analysis on CLIP models trained on synthetic data. We provide insights on the data generation strategy, number of samples required, scaling trends, and resulting properties. We also introduce SynthCI-30M, a purely synthetic dataset comprising 30 million captioned images. Our code, trained models, and data, will be released upon acceptance."
    },
    {
        "title": "Odyssey: Empowering Minecraft Agents with Open-World Skills",
        "link_suffix": "/forum?id=vtGLtSxtqv",
        "link": "https://openreview.net/forum?id=vtGLtSxtqv",
        "pdf_link": "https://openreview.net/pdf?id=vtGLtSxtqv",
        "keywords": "Autonomous Agents, Large Language Models, Open-World Environments",
        "abstract": "Recent studies have delved into constructing generalist agents for open-world environments like Minecraft. Despite the encouraging results, existing efforts mainly focus on solving basic programmatic tasks, e.g., material collection and tool-crafting following the Minecraft tech-tree, treating the ObtainDiamond task as the ultimate goal. This limitation stems from the narrowly defined set of actions available to agents, requiring them to learn effective long-horizon strategies from scratch. Consequently, discovering diverse gameplay opportunities in the open world becomes challenging. In this work, we introduce Odyssey, a new framework that empowers Large Language Model (LLM)-based agents with open-world skills to explore the vast Minecraft world. Odyssey comprises three key parts: (1) An interactive agent with an open-world skill library that consists of 40 primitive skills and 183 compositional skills. (2) A fine-tuned LLaMA-3 model trained on a large question-answering dataset with 390k+ instruction entries derived from the Minecraft Wiki. (3) A new agent capability benchmark includes the long-term planning task, the  dynamic-immediate planning task, and the autonomous exploration task. Extensive experiments demonstrate that the proposed Odyssey framework can effectively evaluate different capabilities of LLM-based agents. All datasets, model weights, and code are publicly available to motivate future research on more advanced autonomous agent solutions."
    },
    {
        "title": "Last-Iterate Convergence of Smooth Regret Matching+Variants in Learning Nash Equilibria",
        "link_suffix": "/forum?id=TU3wJQfKz8",
        "link": "https://openreview.net/forum?id=TU3wJQfKz8",
        "pdf_link": "https://openreview.net/pdf?id=TU3wJQfKz8",
        "keywords": "Regret Matching$^+$, Last-Iterate Convergence, Nash Equilibrium",
        "abstract": "Regret Matching$^+$ (RM$^+$) variants have been widely developed to superhuman Poker AIs, yet few studies investigate their last-iterate convergence. Their last-iterate convergence has been demonstrated only for games with strong monotonicity or two-player zero-sum matrix games. A primary obstacle in proving the last-iterate convergence for these algorithms is that their feedback is not the loss gradient of the vanilla games. This deviation results in the absence of crucial properties, \\eg, monotonicity or the weak Minty variation inequality (MVI), which are pivotal for establishing the last-iterate convergence. To address the absence of these properties, we propose a remarkably succinct yet novel proof paradigm that consists of: (i) recovering these key properties through the equivalence between RM$^+$ and Online Mirror Descent (OMD), and (ii) measuring the the distance to Nash equilibrium (NE) via the tangent residual to show this distance is related to the distance between accumulated regrets. To show the practical applicability of our proof paradigm, we use it to prove the last-iterate convergence of two existing smooth RM$^+$ variants, Smooth Extra-gradient RM$^+$ (SExRM$^+$) and Smooth Predictive RM$^+$ (SPRM$^+$). We show that they achieve last-iterate convergence in learning an NE of games satisfying monotonicity, a weaker condition than the one used in existing proofs for both variants. Then, inspired by our proof paradigm, we propose Smooth Optimistic Gradient RM$^+$ (SOGRM$^+$). We show that SOGRM$^+$ achieves last-iterate convergence in learning an NE of games satisfying the weak MVI, the weakest condition in all known proofs for RM$^+$ variants. The experimental results show that SOGRM$^+$ significantly outperforms other algorithms."
    },
    {
        "title": "Uncovering Challenges of Solving the Continuous Gromov-Wasserstein Problem",
        "link_suffix": "/forum?id=sRaAt9OOnW",
        "link": "https://openreview.net/forum?id=sRaAt9OOnW",
        "pdf_link": "https://openreview.net/pdf?id=sRaAt9OOnW",
        "keywords": "Optimal Transport, Gromov-Wasserstein, Generative Modelling, Benchmark",
        "abstract": "Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem has attracted the special attention of the ML community. In this problem, given two distributions supported on two (possibly different) spaces, one has to find the most isometric map between them. In the discrete variant of GWOT, the task is to learn an assignment between given discrete sets of points. In the more advanced continuous formulation, one aims at recovering a parametric mapping between unknown continuous distributions based on i.i.d. samples derived from them. The clear geometrical intuition behind the GWOT makes it a natural choice for several practical use cases, giving rise to a number of proposed solvers. Some of them claim to solve the continuous version of the problem. At the same time, GWOT is notoriously hard, both theoretically and numerically. Moreover, all existing continuous GWOT solvers still heavily rely on discrete techniques. Natural questions arise: to what extent existing methods unravel GWOT problem, what difficulties they encounter, and under which conditions they are successful. Our benchmark paper is an attempt to answer these questions. We specifically focus on the continuous GWOT as the most interesting and debatable setup. We crash-test existing continuous GWOT approaches on different scenarios, carefully record and analyze the obtained results, and identify issues. Our findings experimentally testify that the scientific community is still missing a reliable continuous GWOT solver, which necessitates further research efforts. As the first step in this direction, we propose a new continuous GWOT method which does not rely on discrete techniques and partially solves some of the problems of the competitors."
    },
    {
        "title": "Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation",
        "link_suffix": "/forum?id=rkzabmWl5k",
        "link": "https://openreview.net/forum?id=rkzabmWl5k",
        "pdf_link": "https://openreview.net/pdf?id=rkzabmWl5k",
        "keywords": "image animation, patch-drop augmentation, high-resolution, long-duration",
        "abstract": "Recent advances in latent diffusion-based generative models for portrait image animation, such as Hallo, have achieved impressive results in short-duration video synthesis. In this paper, we present updates to Hallo, introducing several design enhancements to extend its capabilities.First, we extend the method to produce long-duration videos. To address substantial challenges such as appearance drift and temporal artifacts, we investigate augmentation strategies within the image space of conditional motion frames. Specifically, we introduce a patch-drop technique augmented with Gaussian noise to enhance visual consistency and temporal coherence over long duration.Second, we achieve 4K resolution portrait video generation. To accomplish this, we implement vector quantization of latent codes and apply temporal alignment techniques to maintain coherence across the temporal dimension. By integrating a high-quality decoder, we realize visual synthesis at 4K resolution.Third, we incorporate adjustable semantic textual labels for portrait expressions as conditional inputs. This extends beyond traditional audio cues to improve controllability and increase the diversity of the generated content. To the best of our knowledge, Hallo2, proposed in this paper, is the first method to achieve 4K resolution and generate hour-long, audio-driven portrait image animations enhanced with textual prompts. We have conducted extensive experiments to evaluate our method on publicly available datasets, including HDTF, CelebV, and our introduced ''Wild'' dataset. The experimental results demonstrate that our approach achieves state-of-the-art performance in long-duration portrait video animation, successfully generating rich and controllable content at 4K resolution for duration extending up to tens of minutes."
    },
    {
        "title": "Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures",
        "link_suffix": "/forum?id=9BiVepgmWW",
        "link": "https://openreview.net/forum?id=9BiVepgmWW",
        "pdf_link": "https://openreview.net/pdf?id=9BiVepgmWW",
        "keywords": "zeroth-order optimization, large language model fine-tuning, stochastic optimization",
        "abstract": "Parameter-efficient fine-tuning (PEFT) significantly reduces memory costs when adapting large language models (LLMs) for downstream applications. However, traditional first-order (FO) fine-tuning algorithms incur substantial memory overhead due to the need to store activation values for back-propagation during gradient computation, particularly in long-context fine-tuning tasks. Zeroth-order (ZO) algorithms offer a promising alternative by approximating gradients using finite differences of function values, thus eliminating the need for activation storage. Nevertheless, existing ZO methods struggle to capture the low-rank gradient structure common in LLM fine-tuning, leading to suboptimal performance. This paper proposes a low-rank ZO gradient estimator and introduces a novel \\underline{\\textbf{lo}}w-rank \\underline{\\textbf{ZO}} algorithm (LOZO) that effectively captures this structure in LLMs. We provide convergence guarantees for LOZO by framing it as a subspace optimization method. Additionally, its low-rank nature enables LOZO to integrate with momentum techniques while incurring negligible extra memory costs. Extensive experiments across various model sizes and downstream tasks demonstrate that LOZO and its momentum-based variant outperform existing ZO methods and closely approach the performance of FO algorithms."
    }
]