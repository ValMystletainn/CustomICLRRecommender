[{"title": "Weighted Diversified Sampling for Efficient Data-Driven Single-Cell Gene-Gene Interaction Discovery", "link_suffix": "/forum?id=44IKUSdbUD", "link": "https://openreview.net/forum?id=44IKUSdbUD", "pdf_link": "https://openreview.net/pdf?id=44IKUSdbUD", "keywords": "Gene-gene interaction, sampling", "abstract": "Gene-gene interactions play a crucial role in the manifestation of complex human diseases. Uncovering significant gene-gene interactions is a challenging task. Here, we present an innovative approach utilizing data-driven computational tools, leveraging an advanced Transformer model, to unearth noteworthy gene-gene interactions. Despite the efficacy of Transformer models, their parameter intensity presents a bottleneck in data ingestion, hindering data efficiency.  To mitigate this, we introduce a novel weighted diversified sampling algorithm. This algorithm computes the diversity score of each data sample in just two passes of the dataset, facilitating efficient subset generation for interaction discovery. Our extensive experimentation demonstrates that by sampling a mere 1% of the single-cell dataset, we achieve performance comparable to that of utilizing the entire dataset.", "title_embedding_index": 15400, "title_abs_embedding_index": 15425}, {"title": "Spatiotemporal Learning on Cell-embedded Graphs", "link_suffix": "/forum?id=0je4SA7Jjg", "link": "https://openreview.net/forum?id=0je4SA7Jjg", "pdf_link": "https://openreview.net/pdf?id=0je4SA7Jjg", "keywords": "Spatiotemporal Dynamics, Graph Learning, Physics-embeded Learning", "abstract": "Data-driven simulation of physical systems has recently kindled significant attention, where many neural models have been developed. In particular, mesh-based graph neural networks (GNNs) have demonstrated significant potential in predicting spatiotemporal dynamics across arbitrary geometric domains. However, the existing node-edge message passing mechanism in GNNs limits the model's representation learning ability. In this paper, we proposed a cell-embedded GNN model (aka, CeGNN) to learn spatiotemporal dynamics with lifted performance. Specifically, we introduce a learnable cell attribution to the node-edge message passing process, which better captures the spatial dependency of regional features. Such a strategy essentially upgrades the local aggregation scheme from first order (e.g., from edge to node) to a higher order (e.g., from volume to edge and then to node), which takes advantage of volumetric information in message passing. Meanwhile, a novel feature-enhanced block is designed to further improve the performance of CeGNN and alleviate the over-smoothness problem, via treating the latent features as basis functions. The extensive experiments on various PDE systems and one real-world dataset demonstrate that CeGNN achieves superior performance compared with other baseline models, significantly reducing the prediction errors on several PDE systems.", "title_embedding_index": 15401, "title_abs_embedding_index": 15426}, {"title": "Time-Dependent Mirror Flows and Where to Find Them", "link_suffix": "/forum?id=EreKmSOw7K", "link": "https://openreview.net/forum?id=EreKmSOw7K", "pdf_link": "https://openreview.net/pdf?id=EreKmSOw7K", "keywords": "Mirror flow, Implicit Bias, Time-dependent Bregman potential, Explicit regularization, LoRA, Attention, Sparse coding", "abstract": "Explicit regularization and implicit bias are often studied separately, though in practice, they act in tandem. However, their interplay remains poorly understood. In this work, we show that explicit regularization modifies the behavior of implicit bias and provides a mechanism to control its strength. By incorporating explicit regularization into the mirror flow framework, we present a general approach to better understand implicit biases and their potential in guiding the design of optimization problems. Our primary theoretical contribution is the characterization of regularizations and parameterizations that induce a time-dependent Bregman potential, with a discussion of the implications of its temporal variation. Importantly, our framework encompasses single-layer attention, and application to sparse coding. Extending beyond our core assumptions, we apply this framework to LoRA finetuning, revealing an implicit bias towards sparsity.", "title_embedding_index": 15402, "title_abs_embedding_index": 15427}, {"title": "Safety Alignment Should be Made More Than Just a Few Tokens Deep", "link_suffix": "/forum?id=6Mxhg9PtDE", "link": "https://openreview.net/forum?id=6Mxhg9PtDE", "pdf_link": "https://openreview.net/pdf?id=6Mxhg9PtDE", "keywords": "Safety Alignment, AI Safety, LLM", "abstract": "The safety alignment of current Large Language Models (LLMs) is vulnerable. Simple attacks, or even benign fine-tuning, can jailbreak aligned models. We note that many of these vulnerabilities are related to a shared underlying issue: safety alignment can take shortcuts, wherein the alignment adapts a model's generative distribution primarily over only its very first few output tokens. We unifiedly refer to this issue as shallow safety alignment. In this paper, we present case studies to explain why shallow safety alignment can exist and show how this issue universally contributes to multiple recently discovered vulnerabilities in LLMs, including the susceptibility to adversarial suffix attacks, prefilling attacks, decoding parameter attacks, and fine-tuning attacks. The key contribution of this work is that we demonstrate how this consolidated notion of shallow safety alignment sheds light on promising research directions for mitigating these vulnerabilities. We show that deepening the safety alignment beyond the first few tokens can meaningfully improve robustness against some common exploits. We also design a regularized fine-tuning objective that makes the safety alignment more persistent against fine-tuning attacks by constraining updates on initial tokens. Overall, we advocate that future safety alignment should be made more than just a few tokens deep.", "title_embedding_index": 15403, "title_abs_embedding_index": 15428}, {"title": "Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics", "link_suffix": "/forum?id=DZcmz9wU0i", "link": "https://openreview.net/forum?id=DZcmz9wU0i", "pdf_link": "https://openreview.net/pdf?id=DZcmz9wU0i", "keywords": "Sampling, Langevin, Annealing", "abstract": "Geometric tempering is a popular approach to sampling from challenging multi-modal probability distributions by instead sampling from a sequence of distributions which interpolate, using the geometric mean, between an easier proposal distribution and the target distribution. In this paper, we theoretically investigate the soundness of this approach when the sampling algorithm is Langevin dynamics, proving both upper and lower bounds. Our upper bounds are the first analysis in the literature under functional inequalities. They assert the convergence of tempered Langevin in continuous and discrete-time, and their minimization leads to closed-form optimal tempering schedules for some pairs of proposal and target distributions. Our lower bounds demonstrate a simple case where the geometric tempering takes exponential time, and further reveal that the geometric tempering can suffer from poor functional inequalities and slow convergence, even when the target distribution is well-conditioned. Overall, our results indicate that the geometric tempering may not help, and can even be harmful for convergence.", "title_embedding_index": 15404, "title_abs_embedding_index": 15429}, {"title": "On the Identification of Temporal Causal Representation with Instantaneous Dependence", "link_suffix": "/forum?id=2efNHgYRvM", "link": "https://openreview.net/forum?id=2efNHgYRvM", "pdf_link": "https://openreview.net/pdf?id=2efNHgYRvM", "keywords": "Causal Representation Learning, Instantaneous Dependency, Identification", "abstract": "Temporally causal representation learning aims to identify the latent causal process from time series observations, but most methods require the assumption that the latent causal processes do not have instantaneous relations. Although some recent methods achieve identifiability in the instantaneous causality case, they require either interventions on the latent variables or grouping of the observations, which are in general difficult to obtain in real-world scenarios. To fill this gap, we propose an \\textbf{ID}entification framework for instantane\\textbf{O}us \\textbf{L}atent dynamics (\\textbf{IDOL}) by imposing a sparse influence constraint that the latent causal processes have sparse time-delayed and instantaneous relations. Specifically, we establish identifiability results of the latent causal process based on sufficient variability and the sparse influence constraint by employing contextual information of time series data. Based on these theories, we incorporate a temporally variational inference architecture to estimate the latent variables and a gradient-based sparsity regularization to identify the latent causal process. Experimental results on simulation datasets illustrate that our method can identify the latent causal process. Furthermore, evaluations on multiple human motion forecasting benchmarks with instantaneous dependencies indicate the effectiveness of our method in real-world settings.", "title_embedding_index": 15405, "title_abs_embedding_index": 15430}, {"title": "MALLM-GAN: Multi-Agent Large Language Model as Generative Adversarial Network for Synthesizing Tabular Data", "link_suffix": "/forum?id=8J2djeuNDN", "link": "https://openreview.net/forum?id=8J2djeuNDN", "pdf_link": "https://openreview.net/pdf?id=8J2djeuNDN", "keywords": "Synthetic tabular data, Large language model, In-context Learning", "abstract": "In the era of big data, access to abundant data is crucial for driving research forward. However, such data is often inaccessible due to privacy concerns or high costs, particularly in healthcare domain. Generating synthetic (tabular) data can address this, but existing models typically require substantial amounts of data to train effectively, contradicting our objective to solve data scarcity.  To address this challenge, we propose a novel framework to generate synthetic tabular data, powered by large language models (LLMs) that emulates the architecture of a Generative Adversarial Network (GAN). By incorporating data generation process as contextual information and utilizing LLM as the optimizer, our approach significantly enhance the quality of synthetic data generation in common scenarios with small  sample sizes. Our experimental results on public and private datasets demonstrate that our model outperforms several state-of-art models regarding generating higher quality synthetic data for downstream tasks while keeping privacy of the real data.", "title_embedding_index": 15406, "title_abs_embedding_index": 15431}, {"title": "Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training", "link_suffix": "/forum?id=SIE6VFps9x", "link": "https://openreview.net/forum?id=SIE6VFps9x", "pdf_link": "https://openreview.net/pdf?id=SIE6VFps9x", "keywords": "dialogue system, conversation modeling, reinforcement learning, mixed-initiative interaction, LLM, RLHF, domain adaptation, data-efficient learning, clarification questions", "abstract": "Large language models (LLMs), optimized through human feedback, have rapidly emerged as a leading paradigm for developing intelligent conversational assistants. However, despite their strong performance across many benchmarks, LLM-based agents might still lack conversational skills such as disambiguation -- when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarification questions. Under task-specific settings, high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO), that enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under in data-efficient tuning scenarios, even when there is no action label available, using multiple real-world conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for complex SQL generation towards data analysis agents. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.", "title_embedding_index": 15407, "title_abs_embedding_index": 15432}, {"title": "Demystifying GNN Distillation by Replacing the GNN", "link_suffix": "/forum?id=VfYShlQbj7", "link": "https://openreview.net/forum?id=VfYShlQbj7", "pdf_link": "https://openreview.net/pdf?id=VfYShlQbj7", "keywords": "GNN, Graph Neural Network", "abstract": "It has recently emerged that Multilayer Perceptrons (MLPs) can achieve excellent performance on graph node classification, but only if they distill a previously-trained Graph Neural Network (GNN). This finding is confusing; if MLPs are expressive enough to perform node classification, what is the role of the GNNs? This paper aims to answer this question. Rather than suggesting a new technique, we aim to demystify GNN distillation methods. Through our analysis, we identify the key properties of GNNs that enable them to serve as effective regularizers, thereby overcoming limited training data. We validate our analysis by demonstrating an MLP training process that successfully leverages GNN-like properties without actually training a GNN.", "title_embedding_index": 15408, "title_abs_embedding_index": 15433}, {"title": "Energy-Efficient Sampling Using Stochastic Magnetic Tunnel Junctions", "link_suffix": "/forum?id=HkPz96fgOv", "link": "https://openreview.net/forum?id=HkPz96fgOv", "pdf_link": "https://openreview.net/pdf?id=HkPz96fgOv", "keywords": "Sampling, Random Number Generation, Probabilistic Machine Learning, Hardware, Power Consumption, Spintronics, Sustainability", "abstract": "(Pseudo)random sampling, a costly yet widely used method in (probabilistic) machine learning and Markov Chain Monte Carlo algorithms, remains unfeasible on a truly large scale due to unmet computational requirements. We introduce an energy-efficient algorithm for uniform Float16 sampling, utilizing a room-temperature stochastic magnetic tunnel junction device to generate truly random floating-point numbers. By avoiding expensive symbolic computation and mapping physical phenomena directly to the statistical properties of the floating-point format and uniform distribution, our approach achieves a higher level of energy efficiency than the state-of-the-art Mersenne-Twister algorithm by a minimum factor of 9721 and an improvement factor of 5649 compared to the more energy-efficient PCG algorithm. Building on this sampling technique and hardware framework, we decompose arbitrary distributions into many non-overlapping approximative uniform distributions along with convolution and prior-likelihood operations, which allows us to sample from any 1D distribution without closed-form solutions. We provide measurements of the potential accumulated approximation errors, demonstrating the effectiveness of our method.", "title_embedding_index": 15409, "title_abs_embedding_index": 15434}, {"title": "Can a Large Language Model be a Gaslighter?", "link_suffix": "/forum?id=RQPSPGpBOP", "link": "https://openreview.net/forum?id=RQPSPGpBOP", "pdf_link": "https://openreview.net/pdf?id=RQPSPGpBOP", "keywords": "Gaslighting, Adversarial Attack, Jailbreak, Safety Alignment, Trustworthy AI", "abstract": "Large language models (LLMs) have gained human trust due to their capabilities and helpfulness. However, this in turn may allow LLMs to affect users' mindsets by manipulating language. It is termed as gaslighting, a psychological effect. \nIn this work, we aim to investigate the vulnerability of LLMs under prompt-based and fine-tuning-based gaslighting attacks. Therefore, we propose a two-stage framework DeepCoG designed to: 1) elicit gaslighting plans from LLMs with the proposed DeepGaslighting prompting template, and 2) acquire gaslighting conversations from LLMs through our Chain-of-Gaslighting method. The gaslighting conversation dataset along with a corresponding safe dataset is applied to fine-tuning-based jailbreak on open-source LLMs and anti-gaslighting safety alignment on these LLMs. Experiments demonstrate that both prompt-based and fine-tuning-based attacks transform three open-source LLMs into gaslighters. In contrast, we advanced three safety alignment strategies to strengthen (by 12.05%) the safety guardrail of LLMs. Our safety alignment strategies have minimal impacts on the utility of LLMs. Empirical studies indicate that an LLM may be a potential gaslighter, even if it passed the harmfulness test on general dangerous queries.", "title_embedding_index": 15410, "title_abs_embedding_index": 15435}, {"title": "Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning", "link_suffix": "/forum?id=G1r2rBkUdu", "link": "https://openreview.net/forum?id=G1r2rBkUdu", "pdf_link": "https://openreview.net/pdf?id=G1r2rBkUdu", "keywords": "Disentangled Representation Learning, Identifiability", "abstract": "Disentangled representation learning aims to uncover the latent variables underlying observed data, yet identifying these variables under mild assumptions remains challenging. Some methods rely on sufficient changes in the distribution of latent variables indicated by auxiliary variables, such as domain indices, but acquiring enough domains is often impractical. Alternative approaches exploit the structural sparsity assumption on mixing processes, but this constraint may not hold in practice. Interestingly, we find that these two seemingly unrelated assumptions can actually complement each other. Specifically, when conditioned on auxiliary variables, the sparse mixing process induces independence between latent and observed variables, which simplifies the mapping from estimated to true latent variables and hence compensates for deficiencies of auxiliary variables. Building on this insight, we propose an identifiability theory with less restrictive constraints regarding the auxiliary variables and the sparse mixing process, enhancing applicability to real-world scenarios. Additionally, we develop a generative model framework incorporating a domain encoding network and a sparse mixing constraint and provide two implementations based on variational autoencoders and generative adversarial networks. Experiment results on synthetic and real-world datasets support our theoretical results.", "title_embedding_index": 15411, "title_abs_embedding_index": 15436}, {"title": "Edge Prompt Tuning for Graph Neural Networks", "link_suffix": "/forum?id=92vMaHotTM", "link": "https://openreview.net/forum?id=92vMaHotTM", "pdf_link": "https://openreview.net/pdf?id=92vMaHotTM", "keywords": "Graph Neural Networks, Prompt Learning", "abstract": "Pre-training powerful Graph Neural Networks (GNNs) with unlabeled graph data in a self-supervised manner has emerged as a prominent technique in recent years. However, inevitable objective gaps often exist between pre-training and downstream tasks. To bridge this gap, graph prompt tuning techniques design and learn graph prompts by manipulating input graphs or reframing downstream tasks as pre-training tasks without fine-tuning the pre-trained GNN models. While recent graph prompt tuning methods have proven effective in adapting pre-trained GNN models for downstream tasks, they overlook the crucial role of edges in graph prompt design, which can significantly affect the quality of graph representations for downstream tasks.\nIn this study, we propose EdgePrompt, a simple yet effective graph prompt tuning method from the perspective of edges. Unlike previous studies that design prompt vectors on node features, EdgePrompt manipulates input graphs by learning additional prompt vectors for edges and incorporates the edge prompts through message passing in the pre-trained GNN models to better embed graph structural information for downstream tasks. \nOur method is compatible with prevalent GNN architectures pre-trained under various pre-training strategies and is universal for different downstream tasks.\nWe provide comprehensive theoretical analyses of our method regarding its capability of handling node classification and graph classification as downstream tasks.\nExtensive experiments on ten graph datasets under four pre-training strategies demonstrate the superiority of our proposed method against six baselines. Our code is available athttps://anonymous.4open.science/r/EdgePrompt-4905.", "title_embedding_index": 15412, "title_abs_embedding_index": 15437}, {"title": "TuBA: Cross-Lingual Transferability of Backdoor Attacks in LLMs with Instruction Tuning", "link_suffix": "/forum?id=x6YSsKYJuH", "link": "https://openreview.net/forum?id=x6YSsKYJuH", "pdf_link": "https://openreview.net/pdf?id=x6YSsKYJuH", "keywords": "backdoor attacks, cross-lingual transfer, LLMs", "abstract": "The implications of backdoor attacks on English-centric large language models (LLMs) have been widely examined \u2014 such attacks can be achieved by embedding malicious behaviors during training and activated under specific conditions that trigger malicious outputs. Despite the increasing support for multilingual capabilities in open-source and proprietary LLMs, the impact of backdoor attacks on these systems remains largely under-explored. Our research focuses on crosslingual backdoor attacks against multilingual LLMs, particularly investigating how poisoning the instruction-tuning data for one or two languages can affect the outputs for languages whose instruction-tuning data was not poisoned. Despite its simplicity, our empirical analysis reveals that our method exhibits remarkable efficacy in models like mT5 and GPT-4o, with high attack success rates, surpassing 90% in more than 7 out of 12 languages across various scenarios. Our findings also indicate that more powerful models show increased susceptibility to transferable cross-lingual backdoor attacks, which also applies to LLMs predominantly pre-trained on English data, such as Llama2, Llama3, and Gemma. Moreover, our experiments demonstrate the high transferability of the proposed attack: 1) the backdoor mechanism successfully operates in cross-lingual response scenarios across 26 languages, achieving an average attack success rate of 99%, and 2) the proposed attack remains effective even after defenses are applied. These findings expose critical security vulnerabilities in multilingual LLMs and highlight the urgent need for more robust, targeted defense strategies to address the unique challenges posed by cross-lingual backdoor transfer.", "title_embedding_index": 15413, "title_abs_embedding_index": 15438}, {"title": "Sparse Learning for State Space Models on Mobile", "link_suffix": "/forum?id=t8KLjiFNwn", "link": "https://openreview.net/forum?id=t8KLjiFNwn", "pdf_link": "https://openreview.net/pdf?id=t8KLjiFNwn", "keywords": "Mamba, Pruning, Mobile", "abstract": "Transformer models have been widely investigated in different domains by providing long-range dependency handling and global contextual awareness, driving the development of popular AI applications such as ChatGPT, Gemini, and Alexa.\nState Space Models (SSMs) have emerged as strong contenders in the field of sequential modeling, challenging the dominance of Transformers. SSMs incorporate a selective mechanism that allows for dynamic parameter adjustment based on input data, enhancing their performance.\nHowever, this mechanism also comes with increasing computational complexity and bandwidth demands, posing challenges for deployment on resource-constraint mobile devices.\nTo address these challenges without sacrificing the accuracy of the selective mechanism, we propose a sparse learning framework that integrates architecture-aware compiler optimizations. We introduce an end-to-end solution--$\\mathbf{C}_4^n$ kernel sparsity, which prunes $n$ elements from every four contiguous weights, and develop a compiler-based acceleration solution to ensure execution efficiency for this sparsity on mobile devices.\nBased on the kernel sparsity, our framework generates optimized sparse models targeting specific sparsity or latency requirements for various model sizes. We further leverage pruned weights to compensate for the remaining weights,  enhancing downstream task performance.\nFor practical hardware acceleration, we propose $\\mathbf{C}_4^n$-specific optimizations combined with a layout transformation elimination strategy. \nThis approach mitigates inefficiencies arising from fine-grained pruning in linear layers and improves performance across other operations. \nExperimental results demonstrate that our method achieves superior task performance compared to other semi-structured pruning methods and achieves up-to 7$\\times$  speedup compared to llama.cpp framework on mobile devices.", "title_embedding_index": 15414, "title_abs_embedding_index": 15439}, {"title": "Regularized DeepIV with Model Selection", "link_suffix": "/forum?id=0gqCIaBRQ9", "link": "https://openreview.net/forum?id=0gqCIaBRQ9", "pdf_link": "https://openreview.net/pdf?id=0gqCIaBRQ9", "keywords": "Nonparametric estimator, instrumental variables, model selection, causal inference.", "abstract": "In this paper, we study nonparametric estimation of instrumental variable (IV) regressions. While recent advancements in machine learning have introduced flexible methods for IV estimation, they often encounter one or more of the following limitations: (1) restricting the IV regression to be uniquely identified; (2) requiring minimax computation oracle, which is highly unstable in practice; (3) absence of model selection procedure. In this paper, we analyze a Tikhonov-regularized variant of the seminal DeepIV method, called Regularized DeepIV (RDIV) regression, that can converge to the least-norm IV solution, and overcome all three limitations. RDIV consists of two stages: first, we learn the conditional distribution of covariates, and by utilizing the learned distribution,  we learn the estimator by minimizing a Tikhonov-regularized loss function. We further show that RDIV allows model selection procedures that can achieve the oracle rates in the misspecified regime. When extended to an iterative estimator, we prove that RDIV matches the current state-of-the-art convergence rate. Furthermore, we conducted numerical experiments to justify the efficiency of RDIV empirically.  Our results provide the first rigorous guarantees for the empirically well-established DeepIV method,  showcasing the importance of regularization which was absent from the original work.", "title_embedding_index": 15415, "title_abs_embedding_index": 15440}, {"title": "Order-Optimal Instance-Dependent Bounds for  Offline Reinforcement Learning with Preference Feedback", "link_suffix": "/forum?id=je8wzxh0a5", "link": "https://openreview.net/forum?id=je8wzxh0a5", "pdf_link": "https://openreview.net/pdf?id=je8wzxh0a5", "keywords": "RLHF", "abstract": "We consider offline reinforcement learning (RL) with preference feedback in which the implicit reward is a linear function of an unknown parameter. Given an offline dataset, our objective consists in ascertaining the optimal action for each state, with the ultimate goal of minimizing the {\\em simple regret}. We propose an algorithm, \\underline{RL} with \\underline{L}ocally \\underline{O}ptimal \\underline{W}eights or {\\sc RL-LOW}, which yields a simple regret of $\\exp ( - \\Omega(n/H) )$ where $n$ is the number of data samples and $H$ denotes an instance-dependent hardness quantity that depends explicitly on the suboptimality gap  of each action.  Furthermore, we derive a first-of-its-kind instance-dependent lower bound in offline RL with preference feedback. Interestingly, we observe that the lower and upper bounds on the simple regret match order-wise in the exponent, demonstrating order-wise optimality of {\\sc RL-LOW}. In view of privacy considerations in practical applications, we also extend {\\sc RL-LOW} to the setting of $(\\varepsilon,\\delta)$-differential privacy and show, somewhat surprisingly, that the hardness parameter $H$ is unchanged in the asymptotic regime as $n$ tends to infinity; this underscores the inherent efficiency of {\\sc RL-LOW} in terms of preserving the privacy of the observed rewards. Given our focus on establishing instance-dependent bounds, our work stands in stark contrast to previous works that focus on establishing worst-case regrets for offline RL with preference feedback.", "title_embedding_index": 15416, "title_abs_embedding_index": 15441}, {"title": "Optimized Multi-Token Joint Decoding With Auxiliary Model for LLM Inference", "link_suffix": "/forum?id=ZHhBawo3k5", "link": "https://openreview.net/forum?id=ZHhBawo3k5", "pdf_link": "https://openreview.net/pdf?id=ZHhBawo3k5", "keywords": "LLM Inference, Speculative Decoding", "abstract": "Large language models (LLMs) have achieved remarkable success across diverse tasks, yet their inference processes are hindered by substantial time and energy demands due to single-token generation at each decoding step. While previous methods such as speculative decoding mitigate these inefficiencies by producing multiple tokens per step, each token is still generated by its single-token distribution,\nthereby enhancing speed without improving effectiveness. In contrast, our work simultaneously enhances inference speed and improves the output effectiveness. We consider multi-token joint decoding (MTJD), which generates multiple tokens from their joint distribution at each iteration, theoretically reducing perplexity and enhancing task performance. However, MTJD suffers from the high cost of sampling from the joint distribution of multiple tokens. Inspired by speculative decoding, we introduce multi-token assisted decoding (MTAD), a novel framework designed to accelerate MTJD. MTAD leverages a smaller auxiliary model to approximate the joint distribution of a larger model, incorporating a verification mechanism that not only ensures the accuracy of this approximation, but also improves the\ndecoding efficiency over conventional speculative decoding. Theoretically, we demonstrate that MTAD closely approximates exact MTJD with bounded error. Empirical evaluations using Llama-2 and OPT models ranging from 13B to 70B parameters across various tasks reveal that MTAD reduces perplexity by 21.2% and improves downstream performance compared to standard single-token sampling.\nFurthermore, MTAD achieves a 1.42\u00d7 speed-up and consumes 1.54\u00d7 less energy than conventional speculative decoding methods. These results highlight MTAD\u2019s ability to make multi-token joint decoding both effective and efficient, promoting more sustainable and high-performance deployment of LLMs.", "title_embedding_index": 15417, "title_abs_embedding_index": 15442}, {"title": "Improving Visual Commonsense in Language Models via Multiple Image Generation", "link_suffix": "/forum?id=QP3EvD1AVa", "link": "https://openreview.net/forum?id=QP3EvD1AVa", "pdf_link": "https://openreview.net/pdf?id=QP3EvD1AVa", "keywords": "Language Models, Image generation, Machine learning", "abstract": "Commonsense reasoning is fundamentally based on multimodal knowledge. However, large language models (LLMs), trained using textual data only, are limited with their ability to incorporate essential visual information. In contrast, Visual Language Models (VLMs), which excel at visually-oriented tasks, often fail at non-visual tasks such as textual commonsense reasoning. \nThis divergence highlights a critical challenge - the integration of robust visual understanding with foundational text-based reasoning. To this end, we introduce a method aimed at enhancing LLMs' visual commonsense while maintaining textual modeling and commonsense reasoning performance. Specifically, our method generates multiple images based on the input text prompt and integrates these into the model's decision-making process by mixing their prediction probabilities. To facilitate multimodal grounded language modeling, we employ a late-fusion layer that combines the projected visual features with the output of a pre-trained LLM conditioned on text only. This late-fusion layer enables predictions based on comprehensive image-text knowledge as well as text only when required. We evaluate our approach using several visual commonsense reasoning tasks together with traditional NLP tasks, including common sense reasoning and reading comprehension. Our experimental results demonstrate significant superiority over existing baselines. When applied to recent state-of-the-art LLMs (e.g., Llama3), we observe improvements not only in visual commonsense but also in NLP benchmarks.", "title_embedding_index": 15418, "title_abs_embedding_index": 15443}, {"title": "Prioritize Alignment in Dataset Distillation", "link_suffix": "/forum?id=yt7nxONs3J", "link": "https://openreview.net/forum?id=yt7nxONs3J", "pdf_link": "https://openreview.net/pdf?id=yt7nxONs3J", "keywords": "dataset distillation", "abstract": "Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. \nTo achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. \nConsequently, the quality of extracted and embedded information determines the quality of the distilled dataset.\nIn this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages.\nTo alleviate this, we propose Prioritize Alignment in Dataset Distillation (\\textbf{PAD}), which aligns information from the following two perspectives.We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model.We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information.\nThis simple strategy effectively filters out misaligned information and brings non-trivial improvement for mainstream matching-based distillation algorithms.\nFurthermore, built on trajectory matching, \\textbf{PAD} achieves remarkable improvements on various benchmarks, achieving state-of-the-art performance.", "title_embedding_index": 15419, "title_abs_embedding_index": 15444}, {"title": "LLaVA-Surg: Towards Multimodal Surgical Assistant via Structured Lecture Learning", "link_suffix": "/forum?id=063FuFYQQd", "link": "https://openreview.net/forum?id=063FuFYQQd", "pdf_link": "https://openreview.net/pdf?id=063FuFYQQd", "keywords": "Multimodal assistant, surgical, multimodal instruction-following data, dataset", "abstract": "Multimodal large language models (LLMs) have achieved notable success across various domains, while research in the medical field has largely focused on unimodal images. Meanwhile, current general-domain multimodal models for videos still lack the capabilities to understand and engage in conversations about surgical videos. One major contributing factor is the absence of datasets in the surgical field. In this paper, we create a new dataset, Surg-QA, consisting of 102,000 surgical video-instruction pairs, the largest of its kind so far. To build such a dataset, we propose a novel two-stage question-answer generation pipeline with LLM to learn surgical knowledge in a structured manner from the publicly available surgical lecture videos. The pipeline breaks down the generation process into two stages to significantly reduce the task complexity, allowing us to use a more affordable, locally deployed open-source LLM than the premium paid LLM services. It also mitigates the risk of LLM hallucinations during question-answer generation, thereby enhancing the overall quality of the generated data. We further train LLaVA-Surg, a novel vision-language conversational assistant capable of answering open-ended questions about surgical videos, on this Surg-QA dataset, and conduct comprehensive evaluations on zero-shot surgical video question-answering tasks. We show that LLaVA-Surg significantly outperforms all previous general-domain models, demonstrating exceptional multimodal conversational skills in answering open-ended questions about surgical videos. We will release our code, model, and the instruction-tuning dataset.", "title_embedding_index": 15420, "title_abs_embedding_index": 15445}, {"title": "Rethinking Knowledge Distillation: A Mixture-of-Experts Perspective", "link_suffix": "/forum?id=UC1UD0EIWn", "link": "https://openreview.net/forum?id=UC1UD0EIWn", "pdf_link": "https://openreview.net/pdf?id=UC1UD0EIWn", "keywords": "Knowledge Distillation, Mixture-of-Experts", "abstract": "Knowledge distillation (KD) aims to transfer useful information from a large-scale model (teacher) to a lightweight model (student). \nClassical KD focuses on leveraging the teacher's predictions as soft labels to regularize student training. \nHowever, the exact match of predictions in Kullback-Leibler (KL) divergence could be somewhat in conflict with the classification objective, given that the distribution discrepancies between teacher-generated predictions and ground-truth annotations tend to be fairly severe.\nIn this paper, we rethink the role of teacher predictions from a Mixture-of-Experts (MoE) perspective and transfer knowledge by introducing teacher predictions as latent variables to reformulate the classification objective.\nThis MoE strategy results in breaking down the vanilla classification task into a mixture of easier subtasks with the teacher classifier as a gating function to weigh the importance of subtasks. \nEach subtask is efficiently conquered by distinct experts that are effectively implemented by resorting to multi-level teacher outputs.\nWe further develop a theoretical framework to formulate our method, termed MoE-KD, as an Expectation-Maximization (EM) algorithm and provide proof of the convergence.\nExtensive experiments manifest that MoE-KD outperforms advanced knowledge distillers on mainstream benchmarks.", "title_embedding_index": 15421, "title_abs_embedding_index": 15446}, {"title": "WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct", "link_suffix": "/forum?id=mMPMHWOdOy", "link": "https://openreview.net/forum?id=mMPMHWOdOy", "pdf_link": "https://openreview.net/pdf?id=mMPMHWOdOy", "keywords": "Mathematical Reasoning, Evol-Instruct, Reinforcement Learning", "abstract": "Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical reasoning abilities of LLMs, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses all other open-source LLMs by a substantial margin. Furthermore, WizardMath 70B even outperforms ChatGPT-3.5, Claude Instant, Gemini Pro and Mistral Medium. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance.", "title_embedding_index": 15422, "title_abs_embedding_index": 15447}, {"title": "Unlocking Global Optimality in Bilevel Optimization: A Pilot Study", "link_suffix": "/forum?id=2xvisNIfdw", "link": "https://openreview.net/forum?id=2xvisNIfdw", "pdf_link": "https://openreview.net/pdf?id=2xvisNIfdw", "keywords": "Bilevel optimization, nonconvex optimization, global convergence, linear neural network", "abstract": "Bilevel optimization has witnessed a resurgence of interest, driven by its critical role in advanced machine learning applications such as hyperparameter optimization, meta-learning, and reinforcement learning. Recent research has focused on proposing efficient methods with provable convergence guarantees.  However, while many prior works have established convergence to stationary points or local minima, obtaining the global optimum of bilevel optimization remains an important yet open problem. Arguably, attaining the global optimum is indispensable for ensuring reliability, safety, and cost-effectiveness, particularly in high-stakes engineering applications that rely on bilevel optimization. In this paper, we first explore the challenges of establishing a global convergence theory for generic bilevel optimization, and present two sufficient conditions for global convergence, inspired by contemporary machine learning applications. \nWe provide algorithm-specific proofs to rigorously substantiate these sufficient conditions along the optimization trajectory, focusing on two specific bilevel learning scenarios: representation learning and data hypercleaning (a.k.a. reweighting). Numerical results corroborate the theoretical findings, demonstrating convergence to global minimum in both cases.", "title_embedding_index": 15423, "title_abs_embedding_index": 15448}, {"title": "Dynamic Token Modulation and Expansion for Multi-Task Learning", "link_suffix": "/forum?id=BEzxYj8mOE", "link": "https://openreview.net/forum?id=BEzxYj8mOE", "pdf_link": "https://openreview.net/pdf?id=BEzxYj8mOE", "keywords": "Multi-Task Learning, Token Modulation and Expansion, Conflicting Gradients", "abstract": "Multi-Task Learning (MTL) aims to minimize negative transfer within a shared network. Common strategies involve separating task-generic and task-specific representations and coordinating them to work together effectively within MTL frameworks. However, the absence of a clear rule for determining task-specific network components challenges the design of efficient MTL architectures. Our method tackles negative transfer by employing token-based network expansion and modulation without directly modifying predefined architectures, making it adaptable to any transformer-based MTL architectures. To evaluate negative transfer, we treat tokens as parameters, assessing gradient conflicts during backpropagation. Conflicts between tasks are analyzed by examining the token's range space and null space. Based on conflict types, we expand the network following rules. If task-specific gradients clash in the tokens' range space, we modulate existing tokens to align their task gradients. Conversely, if the gradients conflict in the null space of tokens, we add new task-specific tokens, spanning a new feature space. Our approach effectively boosts multi-task performance across various datasets by being integrated into previous state-of-the-art multi-task architectures.", "title_embedding_index": 15424, "title_abs_embedding_index": 15449}]