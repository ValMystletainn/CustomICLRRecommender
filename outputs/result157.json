[
    {
        "title": "CrossMPT: Cross-attention Message-passing Transformer for Error Correcting Codes",
        "link_suffix": "/forum?id=gFvRRCnQvX",
        "link": "https://openreview.net/forum?id=gFvRRCnQvX",
        "pdf_link": "https://openreview.net/pdf?id=gFvRRCnQvX",
        "keywords": "Cross-attention, Error correcting codes, Message-passing decoder, Neural decoder, Transformer",
        "abstract": "Error correcting codes (ECCs) are indispensable for reliable transmission in communication systems. The recent advancements in deep learning have catalyzed the exploration of ECC decoders based on neural networks. Among these, transformer-based neural decoders have achieved state-of-the-art decoding performance. In this paper, we propose a novel Cross-attention Message-Passing Transformer (CrossMPT), which shares key operational principles with conventional message-passing decoders. While conventional transformer-based decoders employ self-attention mechanism without distinguishing between the types of input vectors (i.e., magnitude and syndrome vectors), CrossMPT updates the two types of input vectors separately and iteratively using two masked cross-attention blocks. The mask matrices are determined by the code's parity-check matrix, which explicitly captures the irrelevant relationship between two input vectors. Our experimental results show that CrossMPT significantly outperforms existing neural network-based decoders for various code classes. Notably, CrossMPT achieves this decoding performance improvement, while significantly reducing the memory usage, complexity, inference time, and training time."
    },
    {
        "title": "Integrating Expertise of Software Engineering Agents",
        "link_suffix": "/forum?id=cKlzKs3Nnb",
        "link": "https://openreview.net/forum?id=cKlzKs3Nnb",
        "pdf_link": "https://openreview.net/pdf?id=cKlzKs3Nnb",
        "keywords": "large language models, LLM agents, software engineering",
        "abstract": "Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges."
    },
    {
        "title": "VEBench: Towards Comprehensive and Automatic Evaluation for Text-guided Video Editing",
        "link_suffix": "/forum?id=nZNWrzDBHG",
        "link": "https://openreview.net/forum?id=nZNWrzDBHG",
        "pdf_link": "https://openreview.net/pdf?id=nZNWrzDBHG",
        "keywords": "Video Editing, MLLMs, Meta Evaluation",
        "abstract": "Video editing task has gained widespread attention in recent years due to their practical applications and rapid advancements, driven by the emergence of diffusion techniques and Multi-modal Large Language Models (MLLMs). However, current automatic evaluation metrics for video editing are mostly unreliable and poorly aligned with human judgments. As a result, researchers heavily rely on human annotation for evaluation, which is not only time-consuming and labor-intensive but also difficult to ensure consistency and objectivity. To address this issue, we introduceVEBench, the largest-ever video editing meta-evaluation benchmark to evaluate the reliability of automatic metrics. It includes 152 video clips and 962 text prompts, from which 160 instances are sampled to generate 1,280 edited videos using 8 open-source video editing models, accompanied by human annotations. Especially, the text prompts are first crafted using GPT-4, followed by manual review and careful categorization based on editing types for a systematic evaluation. Our human annotations cover 3 criteria:Textual Faithfulness,Frame Consistency, andVideo Fidelity, ensuring the comprehensiveness of evaluation. Since human evaluation is costly, we also proposeVEScore, employing MLLMs as evaluators to assess edited videos from the criteria above. Experiments show that the best-performing video editing model only reaches an average score of 3.18 (out of a perfect 5), highlighting the challenge of VEBench. Besides, results from more than 10 MLLMs demonstrate the great potential of utilizing VEScore for automatic evaluation. Notably, for Textual Faithfulness, VEScore equipped with LLaVA-OneVision-7B achieves a Pearson Correlation score of 0.48, significantly outperforming previous methods based on CLIP with the highest score of 0.21. The dataset and code will be released upon acceptance."
    },
    {
        "title": "InverseBench: Benchmarking Plug-and-Play Diffusion Models for Scientific Inverse Problems",
        "link_suffix": "/forum?id=U3PBITXNG6",
        "link": "https://openreview.net/forum?id=U3PBITXNG6",
        "pdf_link": "https://openreview.net/pdf?id=U3PBITXNG6",
        "keywords": "inverse problem, benchmark, diffusion model",
        "abstract": "Plug-and-play diffusion models have emerged as a promising research direction for solving inverse problems. \n However, current studies primarily focus on natural image restoration, leaving the performance of these algorithms in scientific inverse problems largely unexplored. To address this gap, we introduce \\textsc{InverseBench}, a unified framework that evaluates diffusion models across five distinct scientific inverse problems. These problems present unique structural challenges that differ from existing benchmarks, arising from critical scientific applications such as black hole imaging, seismology, optical tomography, medical imaging, and fluid dynamics. With \\textsc{InverseBench}, we benchmark 15 inverse problem algorithms that use plug-and-play diffusion models against strong, domain-specific baselines, offering valuable new insights into the strengths and weaknesses of existing algorithms. We open-source the datasets, pre-trained models, and the codebase to facilitate future research and development."
    },
    {
        "title": "ClipGrader: Leveraging Vision-Language Models for Robust Label Quality Assessment in Object Detection",
        "link_suffix": "/forum?id=1GPN2oa7P7",
        "link": "https://openreview.net/forum?id=1GPN2oa7P7",
        "pdf_link": "https://openreview.net/pdf?id=1GPN2oa7P7",
        "keywords": "label quality, clip, object detection",
        "abstract": "High-quality annotations are essential for object detection models, but ensuring label accuracy \u2014 especially for bounding boxes \u2014 remains both challenging and costly. This paper introduces ClipGrader, a novel approach that leverages vision-language models to automatically assess the accuracy of bounding box annotations. By adapting CLIP (Contrastive Language-Image Pre-training) to evaluate both class label correctness and spatial precision of bounding box, ClipGrader offers an effective solution for grading object detection labels. Tested on modified object detection datasets with artificially disturbed bounding boxes, ClipGrader achieves 91% accuracy on COCO with a 1.8% false positive rate. Moreover, it maintains 87% accuracy with a 2.1% false positive rate when trained on just 10% of the COCO data. ClipGrader also scales effectively to larger datasets such as LVIS, achieving 79% accuracy across 1,203 classes. Our experiments demonstrate ClipGrader\u2019s ability to identify errors in existing COCO annotations, highlighting its potential for dataset refinement. When integrated into a semi-supervised object detection (SSOD) model, ClipGrader readily improves the pseudo label quality, helping achieve higher mAP (mean Average Precision) throughout the training process. ClipGrader thus provides a scalable AI-assisted tool for enhancing annotation quality control and verifying annotations in large-scale object detection datasets."
    },
    {
        "title": "Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon",
        "link_suffix": "/forum?id=3E8YNv1HjU",
        "link": "https://openreview.net/forum?id=3E8YNv1HjU",
        "pdf_link": "https://openreview.net/pdf?id=3E8YNv1HjU",
        "keywords": "memorization, ontologies, language modelling",
        "abstract": "Memorization in language models is typically treated as a homogenous phenomenon, neglecting the specifics of the memorized data. We instead model memorization as the effect of a set of complex factors that describe each sample and relate it to the model and corpus. To build intuition around these factors, we break memorization down into a taxonomy: recitation of highly duplicated sequences, reconstruction of inherently predictable sequences, and recollection of sequences that are neither. We demonstrate the usefulness of our taxonomy by using it to construct a predictive model for memorization. By analyzing dependencies and inspecting the weights of the predictive model, we find that different factors have different influences on the likelihood of memorization depending on the taxonomic category."
    },
    {
        "title": "Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs",
        "link_suffix": "/forum?id=VyvGaQgxDl",
        "link": "https://openreview.net/forum?id=VyvGaQgxDl",
        "pdf_link": "https://openreview.net/pdf?id=VyvGaQgxDl",
        "keywords": "LLMs, Quantization",
        "abstract": "We introduce \\emph{Integer Scale}, a novel post-training quantization scheme for large language models that effectively resolves the inference bottleneck in current fine-grained quantization approaches while maintaining similar accuracies. Integer Scale is a free lunch as it requires no extra calibration or fine-tuning which will otherwise incur additional costs. It can be used plug-and-play for most fine-grained quantization methods and its integration results in at most \\textbf{1.85$\\times$} end-to-end speed boost over the original counterpart without sacrificing accuracy. Additionally, due to the orchestration of the proposed Integer Scale and fine-grained quantization, we resolved the quantization difficulty for Mixtral-8x7B and LLaMA-3 models with negligible performance degradation, and it comes with an end-to-end speed boost of \\textbf{2.13$\\times$}, and \\textbf{2.31$\\times$} compared with their FP16 versions respectively."
    },
    {
        "title": "Learning Code Preference via Synthetic Evolution",
        "link_suffix": "/forum?id=4MWUdp6deL",
        "link": "https://openreview.net/forum?id=4MWUdp6deL",
        "pdf_link": "https://openreview.net/pdf?id=4MWUdp6deL",
        "keywords": "Code Generation, Large Language Model, Preference Learning, Evaluation",
        "abstract": "Large Language Models (LLMs) have recently demonstrated remarkable coding capabilities.\nHowever, assessing code generation based on well-formed properties and aligning it with developer preferences remains challenging.\nIn this paper, we explore two key questions under the new challenge of code preference learning:\n(i) How do we train models to predict meaningful preferences for code? and\n(ii) How do human and LLM preferences align with verifiable code properties and developer code tastes?\nTo this end, we propose CodeFavor,\na framework for training pairwise code preference models from synthetic evolution data,\nincluding code commits and code critiques.\nTo evaluate code preferences,\nwe introduce CodePrefBench, a benchmark comprising 1364 rigorously curated code preference tasks to cover three verifiable properties\u2014correctness, efficiency, and security\u2014along with human preference.\nOur evaluation shows that CodeFavor holistically improves the accuracy of model-based code preferences by up to $28.8$%.\nMeanwhile, CodeFavor models can match the performance of models with $6\\sim 9\\times$ more parameters\nwhile being $34\\times$ more cost-effective.\nWe also rigorously validate the design choices in CodeFavor via a comprehensive set of controlled experiments.\nFurthermore, we discover the prohibitive costs and limitations of human-based code preference:\ndespite spending 23.4 person-minutes on each task, $15.1\\sim 40.3$% of tasks remain unsolved.\nCompared to model-based preference,\nhuman preference tends to be more accurate under the objective of code correctness,\nwhile being sub-optimal for non-functional objectives."
    },
    {
        "title": "Selective Label Enhancement Learning for Test-Time Adaptation",
        "link_suffix": "/forum?id=3Z2flzXzBY",
        "link": "https://openreview.net/forum?id=3Z2flzXzBY",
        "pdf_link": "https://openreview.net/pdf?id=3Z2flzXzBY",
        "keywords": "label enhancement, test-time adaptation, distribution shift",
        "abstract": "Test-time adaptation (TTA) aims to adapt a pre-trained model to the target domain using only unlabeled test samples. Most existing TTA approaches rely on definite pseudo-labels, inevitably introducing false labels and failing to capture uncertainty for each test sample. This prevents pseudo-labels from being flexibly refined as the model adapts during training, limiting their potential for performance improvement. To address this, we propose the Progressive Adaptation with Selective Label Enhancement (PASLE) framework. Instead of definite labels, PASLE assigns candidate pseudo-label sets to uncertain ones via selective label enhancement. Specifically, PASLE partitions data into confident/uncertain subsets, assigning one-hot labels to confident samples and candidate sets to uncertain ones. The model progressively trains on certain/uncertain pseudo-labeled data while dynamically refining uncertain pseudo-labels, leveraging increasing target adaptation monitored throughout training. Experiments on various benchmark datasets validate the effectiveness of the proposed approach."
    },
    {
        "title": "MambaExtend: A Training-Free Approach to Improve Long Context Extension of Mamba",
        "link_suffix": "/forum?id=LgzRo1RpLS",
        "link": "https://openreview.net/forum?id=LgzRo1RpLS",
        "pdf_link": "https://openreview.net/pdf?id=LgzRo1RpLS",
        "keywords": "Mamba, Long Context Generalization, Discretization Step, SSM",
        "abstract": "The inherent quadratic complexity of the attention mechanism in transformer models has driven the research community to explore alternative architectures with sub-quadratic complexity, such as state-space models. Within this emerging paradigm, Mamba has established itself as a leading model, achieving state-of-the-art results in various language modeling benchmarks. However, despite its impressive performance, Mamba's effectiveness is significantly limited by its pre-training context length, resulting in a pronounced degradation when the model is tasked with handling longer contexts. Our investigation reveals that Mamba's inability to generalize effectively to long contexts is primarily due to the out-of-distribution (OOD) discretization steps. To address this critical limitation, we introduceMambaExtend, a novel framework designed to enhance context extension capabilities of Mamba. Specifically, MambaExtend leverages atraining-freeapproach to calibrateonlythe scaling factors of discretization modules for different layers. We demonstrate both gradient-based and gradient-free zeroth-order optimization to learn the optimal scaling factors for each Mamba layer, requiring orders of magnitude fewer updates as opposed to the parameter fine-tuning based alternatives. With this, for the first time,  we can enable a training-free context extension of up to $\\mathbf{32}\\times$  from $2$k to $64$k, that too without any significant increase in perplexity. Compared to the existing alternative approach of fine-tuning, due to only selective calibration of the scaling factors, MambaExtend requires up to $\\mathord{\\sim}$$\\mathbf{{5.42*10^{6}}}\\times$ fewer parameter update costing up to $\\mathbf{3.87}\\times$ lower peak-memory while maintaining similar or better long-context performance evaluated across multiple tasks. Code will be released soon."
    },
    {
        "title": "SpatialEdit: Unlocking the  Spatial Capability in Multimodal Large Language Model Driven Image Editing",
        "link_suffix": "/forum?id=Alv71WWRgh",
        "link": "https://openreview.net/forum?id=Alv71WWRgh",
        "pdf_link": "https://openreview.net/pdf?id=Alv71WWRgh",
        "keywords": "Image Editing, Multimodal LLM",
        "abstract": "Current instruction-guided image editing methods generally believes that incorporating powerful Multimodal Large Language Model (MLLM) can significantly enhance the understanding of complex instructions, thereby improving editing outcomes and generalization. \nHowever, even using an powerful MLLM model such as GPT4V, disappointing results are observed when instructions involve simple spatial information such as ``change the clothes color of the leftmost person to red''. \nOur theoretical analysis suggests that both the training strategy and the model aggregation manner in the current paradigm may contribute to unsatisfactory spatial image editing capabilities. \nConsequently, we propose the SpatialEdit framework, featuring a two-stage training approach and a novel data engine where questions and instructions are enriched with spatial information.Further theoretical analysis of our method reveals its ability to increase proficiency in both spatial editing and general image editing tasks. \nWe create a benchmark to evaluate spatial editing ability. \nWe conduct zero-shot image editing experiments on various datasets and our method achieves SOTA results on several key metrics."
    },
    {
        "title": "LeYOLO: Lightweight, Scalable and Efficient CNN Architecture for Object Detection",
        "link_suffix": "/forum?id=kbx9tTFYpd",
        "link": "https://openreview.net/forum?id=kbx9tTFYpd",
        "pdf_link": "https://openreview.net/pdf?id=kbx9tTFYpd",
        "keywords": "Object detection, CNN, parameter efficiency, Architecture optimization",
        "abstract": "Computational efficiency in deep neural networks is critical for object detection, especially as newer models prioritize speed over efficient computation (Parameters and FLOP). This trend is evident in the latest YOLO architectures, which focus more on speed at the expense of lightweight design. This evolution has somewhat left lightweight architecture design behind for object detection applications. \nUnlike speed-oriented object detectors in the literature, SSDLite and low-parameters/FLOP-oriented classifier combinations are the only proposed solutions, leaving a gap between YOLO-like architectures and lightweight object detectors. In this paper, we pose the question: $\\textit{Can an architecture optimized for parameters and FLOPs achieve precision comparable to mainstream YOLO models?}$ To explore this, we introduce LeYOLO, an efficient object detection model, and propose several optimizations to enhance the computational efficiency of YOLO-based models. This approach bridges the gap between SSDLite-based object detectors and YOLO models, achieving high precision in a model as lightweight as MobileNets. \nOur novel model family achieves a FLOP-to-accuracy ratio previously unattained, offering scalability that spans from ultra-low neural network configurations $( (<) 1 GFLOP)$ to efficient yet demanding object detection setups $( (>) 4 GFLOPs)$ with 25.2, 31.3, 35.2, 38.2, 39.3 and 41 mAP for 0.66, 1.47, 2.53, 4.51, 5.8 and 8.4 FLOP(G)."
    },
    {
        "title": "The VEP Booster: A Closed-Loop AI System for Visual EEG Biomarker Auto-generation",
        "link_suffix": "/forum?id=fSqzHzyVZU",
        "link": "https://openreview.net/forum?id=fSqzHzyVZU",
        "pdf_link": "https://openreview.net/pdf?id=fSqzHzyVZU",
        "keywords": "Visual Evoked Potential, Brain-machine Interface, EEGs, Closed-loop AI, Generative AI",
        "abstract": "The effectiveness of Visual Brain-Machine Interfaces (BMIs) is significantly dependent on the accurate detection and interpretation of electroencephalography (EEG) biomarkers, which frequently exhibit variability due to physiological changes and environmental disturbances over time. Traditional EEG signal enhancement strategies largely concentrate on signal processing techniques such as feature extraction and filtering; however, these approaches often do not adequately address the inherent sources of variability that affect biomarker stability over time. To surmount these challenges, we have developed the Visual Evoked Potential Booster (VEP Booster), a novel closed-loop artificial intelligence framework designed to produce reliable and stable EEG biomarkers under visual stimulation protocols. Our system utilizes a Deep Convolutional Generative Adversarial Network (DCGAN) to refine stimulus images based on real-time feedback from human EEG signals, thereby creating visual stimuli that are specifically tailored to the characteristic preferences of neurons in the primary visual cortex. We evaluated the efficacy of this system through the implementation of steady-state visual evoked potential (SSVEP) protocols in nine human subjects. In our evaluations, both the SSVEP biomarker amplitude and the single-trial SSVEP binary classification experiments, encompassing intra- and inter-temporal analyses, exhibited statistically significant enhancements when employing the VEP Booster. These encouraging outcomes underscore the potential for broad applications in clinical and technological domains."
    },
    {
        "title": "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer",
        "link_suffix": "/forum?id=LQzN6TRFg9",
        "link": "https://openreview.net/forum?id=LQzN6TRFg9",
        "pdf_link": "https://openreview.net/pdf?id=LQzN6TRFg9",
        "keywords": "Video Generation, Diffusion model, Pretraining",
        "abstract": "We present CogVideoX, a large-scale text-to-video generation model based on diffusion transformer, which can generate 10-second continuous videos aligned with text prompt, with a frame rate of 16 fps and resolution of 768 $\\times$ 1360 pixels. \nPrevious video generation models often had limited movement and short durations, and is difficult to generate videos with coherent narratives based on text. We propose several designs to address these issues. \nFirst, we propose a 3D Variational Autoencoder (VAE) to compress videos along both spatial and temporal dimensions, to improve both compression rate and video fidelity. Second, to improve the text-video alignment, we propose an expert transformer with the expert adaptive LayerNorm to facilitate the deep fusion between the two modalities. Third, by employing a progressive training and multi-resolution frame pack technique, \\model is adept at producing coherent, long-duration, different shape videos characterized by significant motions. \nIn addition, we develop an effective text-video data processing pipeline that includes various data preprocessing strategies and a video captioning method, greatly contributing to the generation quality and semantic alignment. \nResults show that CogVideoX demonstrates state-of-the-art performance across both multiple machine metrics and human evaluations. \nThe model weights of the 3D Causal VAE, the video caption model, and CogVideoX are open-source."
    },
    {
        "title": "Pronunciation-Lexicon Free Training for Phoneme-based Crosslingual ASR via Joint Stochastic Approximation",
        "link_suffix": "/forum?id=AUi9y7wJBN",
        "link": "https://openreview.net/forum?id=AUi9y7wJBN",
        "pdf_link": "https://openreview.net/pdf?id=AUi9y7wJBN",
        "keywords": "speech recognition, crosslingual, joint stochastic approximation, phoneme;",
        "abstract": "Recently, pre-trained models with phonetic supervision have demonstrated their advantages for crosslingual speech recognition in data efficiency and information sharing across languages. The Whistle approach relaxes the requirement of gold-standard human-validated phonetic transcripts and adopts weakly-phonetic supervision; however, a limitation is that a pronunciation lexicon is needed for such phoneme-based crosslingual speech recognition. \nIn this study, we aim to eliminate the need for the pronunciation lexicon and propose a latent variable model based method, with phonemes being treated as discrete latent variables. The new method consists of a speech-to-phoneme (S2P) model and a phoneme-to-grapheme (P2G) model, and a grapheme-to-phoneme (G2P) model is introduced as an auxiliary inference model. \nTo jointly train the three models, we utilize the joint stochastic approximation (JSA) algorithm, which is a stochastic extension of the EM (expectation-maximization) algorithm and has demonstrated superior performances particularly in estimating discrete latent variable models.\nBased on the Whistle multilingual pre-trained S2P model, crosslingual experiments on Polish (130h) and Indonesian (20h) are conducted.\nBy using only 10 minutes of phoneme supervision, the new method, called as Whistle-JSA, performs close to crosslingual fine-tuning with the full set of phoneme supervision, and on par with the method of crosslingual fine-tuning with subword supervision.\nFurthermore, it is found that in language domain adaptation (i.e., utilizing cross-domain text-only data), Whistle-JSA outperforms the standard practice of language model fusion via the auxiliary support of the G2P model."
    },
    {
        "title": "Thinking Forward and Backward: Effective Backward Planning with Large Language Models",
        "link_suffix": "/forum?id=cWrqs2lwCJ",
        "link": "https://openreview.net/forum?id=cWrqs2lwCJ",
        "pdf_link": "https://openreview.net/pdf?id=cWrqs2lwCJ",
        "keywords": "LLM planning, backward search",
        "abstract": "Large language models (LLMs) have exhibited remarkable reasoning and planning capabilities. Most prior work in this area has used LLMs to reason through steps from an initial to a goal state or criterion, thereby effectively reasoning in a forward direction. Nonetheless, many planning problems exhibit an inherent asymmetry such that planning backward from the goal is significantly easier --- for example, if there are bottlenecks close to the goal. We take inspiration from this observation and demonstrate that this bias holds for LLM planning as well: planning performance in one direction correlates with the planning complexity of the problem in that direction. However, our experiments also reveal systematic biases which lead to poor planning in the backward direction. With this knowledge, we propose a backward planning algorithm for LLMs that first flips the problem and then plans forward in the flipped problem. This helps avoid the backward bias, generate more diverse candidate plans, and exploit asymmetries between the forward and backward directions in planning problems --- we find that combining planning in both directions with self-verification improves the overall planning success rates by 4-24% in three planning domains."
    },
    {
        "title": "A SSM is Polymerized from Multivariate Time Series",
        "link_suffix": "/forum?id=nclyFUZpX9",
        "link": "https://openreview.net/forum?id=nclyFUZpX9",
        "pdf_link": "https://openreview.net/pdf?id=nclyFUZpX9",
        "keywords": "Time Series Forecasting\uff1bDeep Learning\uff1b State Space Model",
        "abstract": "For multivariate time series (MTS) tasks, previous state space models (SSMs) followed the modeling paradigm of Transformer-based methods. However, none of them explicitly model the complex dependencies of MTS: the Channel Dependency variations with Time (CDT). In view of this, we delve into the derivation of SSM, which involves approximating continuously updated functions by orthogonal function basis. We then develop Poly-Mamba, a novel method for MTS forecasting. Its core concept is to expand the original orthogonal function basis space into a multivariate orthogonal function space containing variable mixing terms, and make a projection on this space so as to explicitly describe the CDT by weighted coefficients. In Poly-Mamba, we propose the Multivariate Orthogonal Polynomial Approximation (MOPA) as a simplified implementation of this concept. For the simple linear relationship between channels, we propose Linear Channel Mixing (LCM) and generate CDT patterns adaptively for different channels through a proposed Order Combining method. Experiments on six real-world datasets demonstrate that Poly-Mamba outperforms the SOTA methods, especially when dealing with datasets having a large number of channels and complex correlations. The codes and log files are in the supplementary."
    },
    {
        "title": "Unified Multimodal Discrete Diffusion",
        "link_suffix": "/forum?id=QyNN5n37nK",
        "link": "https://openreview.net/forum?id=QyNN5n37nK",
        "pdf_link": "https://openreview.net/pdf?id=QyNN5n37nK",
        "keywords": "multimodal, discrete diffusion",
        "abstract": "Multimodal generative models that can understand and generate across multiple modalities are dominated by autoregressive (AR) approaches, which process tokens sequentially from left to right, or top to bottom. These models jointly handle images, text, video, and audio for various tasks such as image captioning, question answering, and image generation. While AR models have been highly successful in the text domain, they have been found suboptimal for processing images, videos, and audio due to the high correlation between adjacent tokens which waste inference-time compute by separately predicting each one. In this work, we explore discrete diffusion models as a unified generative formulation in the joint text and image domain, building upon their recent success in the text domain alone. Discrete diffusion models offer several advantages over AR models, including improved control over quality versus diversity of generated samples, the ability to perform joint multimodal inpainting (across both text and image domains), greater controllability in generation through guidance. Leveraging these benefits, we present the first Unified Multimodal Discrete Diffusion (UniDisc) model, which is capable of jointly processing text and images for a variety of downstream tasks. We compare UniDisc to multimodal AR models of similar capacity, demonstrating that UniDisc outperforms them in terms of both performance and inference-time compute, and enhanced controllability, editability, inpainting and flexible trade-off of inference time versus generation quality."
    },
    {
        "title": "PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation",
        "link_suffix": "/forum?id=tQ1PmLfPBL",
        "link": "https://openreview.net/forum?id=tQ1PmLfPBL",
        "pdf_link": "https://openreview.net/pdf?id=tQ1PmLfPBL",
        "keywords": "Conditional Flow Matching, Neural Vocoder, Speech Synthesis, Neural Audio Codec, Speech Language Models",
        "abstract": "Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios. Although one-step GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage text-to-speech. Meanwhile, diffusion-based models have shown their powerful generative performance in other domains; however, they stay out of the limelight due to slow inference speed in waveform generation tasks. Above all, there is no generator architecture that can explicitly disentangle the natural periodic features of high-resolution waveform signals. In this paper, we propose PeriodWave, a novel universal waveform generation model from Mel-spectrogram and neural audio codec. First, we introduce a period-aware flow matching estimator that effectively captures the periodic features of the waveform signal when estimating the vector fields. Additionally, we utilize a multi-period estimator that avoids overlaps to capture different periodic features of waveform signals. Although increasing the number of periods can improve the performance significantly, this requires more computational costs. To reduce this issue, we also propose a single period-conditional universal estimator that can feed-forward parallel by period-wise batch inference. Additionally, we first introduce FreeU to reduce the high-frequency noise for waveform generation. Furthermore, we demonstrate the effectiveness of the proposed method in neural audio codec decoding task, and present the streaming generation framework of non-autoregressive model for speech language models. The experimental results demonstrated that our model outperforms the previous models in reconstruction tasks from Mel-spectrogram and discrete token, and text-to-speech tasks. Our demo is available athttps://periodwave.github.io/demo."
    },
    {
        "title": "IBCircuit: Towards Holistic Circuit Discovery with Information Bottleneck",
        "link_suffix": "/forum?id=aVovUyrh5J",
        "link": "https://openreview.net/forum?id=aVovUyrh5J",
        "pdf_link": "https://openreview.net/pdf?id=aVovUyrh5J",
        "keywords": "Information Bottleneck, Circuit Analysis",
        "abstract": "Circuit discovery has recently attracted attention as a potential research direction to explain the nontrivial behaviors of language model (LM). It aims to find the computational subgraph, also known as \\emph{circuit}, that explains LM's behavior on specific tasks. Most studies determine the circuit for a task by performing causal interventions independently on each component. However, they ignored the holistic nature of the circuit, which is an interconnected system of components rather than an independent combination. Additionally, existing methods require redesigning a unique corrupted activation for each task, which are complicated and inefficient. In this work, we propose a novel circuit discovery approach based on the principle of Information Bottleneck, called IBCircuit, to identify the most informative circuit from a holistic perspective. Furthermore, IBcircuit can be applied to any given task without corrupted activation construction. Our experiments demonstrate  the ability of IBCircuit to identify the most informative circuit in the model. The results from IBCircuit suggest that the earlier layers in Transformer-based models are crucial in capturing factual information."
    },
    {
        "title": "Adversarial Diffusion Bridge Model for Reliable Adversarial Purification",
        "link_suffix": "/forum?id=g0rnZeBguq",
        "link": "https://openreview.net/forum?id=g0rnZeBguq",
        "pdf_link": "https://openreview.net/pdf?id=g0rnZeBguq",
        "keywords": "adversarial robustness, adversarial defense, diffusion models",
        "abstract": "Recently Diffusion-based Purification (DiffPure) has been recognized as an effective defense method against adversarial examples. However, we find DiffPure which directly employs the original pre-trained diffusion models for adversarial purification, to be suboptimal. This is due to an inherent trade-off between noise purification performance and data recovery quality. Additionally, the reliability of existing evaluations for DiffPure is questionable, as they rely on weak adaptive attacks. In this work, we propose a novel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs a reverse bridge from the diffused adversarial data back to its original clean examples, enhancing the purification capabilities of the original diffusion models. Through theoretical analysis and experimental validation across various scenarios, ADBM has proven to be a superior and robust defense mechanism, offering significant promise for practical applications."
    },
    {
        "title": "A Non-Contrastive Learning Framework for Sequential Recommendation with Preference-Preserving Profile Generation",
        "link_suffix": "/forum?id=Ke2BEL4csm",
        "link": "https://openreview.net/forum?id=Ke2BEL4csm",
        "pdf_link": "https://openreview.net/pdf?id=Ke2BEL4csm",
        "keywords": "sequential recommendation, non-contrastive learning",
        "abstract": "Contrastive Learning (CL) proves to be effective for learning generalizable user representations in Sequential Recommendation (SR), but it suffers from high computational costs due to its reliance on negative samples. To overcome this limitation, we propose the first Non-Contrastive Learning (NCL) framework for SR, which eliminates computational overhead of identifying and generating negative\nsamples. However, without negative samples, it is challenging to learn uniform representations from only positive samples, which is prone to representation collapse. Furthermore, the alignment of the learned representations may be substantially compromised because existing ad-hoc augmentations can produce positive samples that have inconsistent user preferences. To tackle these challenges, we design a novel preference-preserving profile generation method to produce high-quality positive samples for non-contrastive training. Inspired by differential privacy, our approach creates augmented user profiles that exhibit high diversity while provably retaining consistent user preferences. With larger diversity and consistency of the positive samples, our NCL framework significantly enhances the alignment and uniformity of the learned representations, which contributes to better generalization. The experimental results on various benchmark datasets and model architectures demonstrate the effectiveness of the proposed method. Finally, our investigations reveal that both uniformity and alignment play a vital role in improving generalization for SR. Interestingly, in our data-sparse setting, alignment is usually more important than uniformity."
    },
    {
        "title": "What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis",
        "link_suffix": "/forum?id=3ddi7Uss2A",
        "link": "https://openreview.net/forum?id=3ddi7Uss2A",
        "pdf_link": "https://openreview.net/pdf?id=3ddi7Uss2A",
        "keywords": "hessian, Transformers",
        "abstract": "The Transformer architecture has inarguably revolutionized deep learning, overtaking classical architectures like multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs). At its core, the attention block differs in form and functionality from most other architectural components in deep learning - to the extent that Transformers are often accompanied by adaptive optimizers, layer normalization, learning rate warmup, and more, in comparison to MLPs/CNNs. The root causes behind these outward manifestations, and the precise mechanisms that govern them, remain poorly understood. In this work, we bridge this gap by providing a fundamental understanding of what distinguishes the Transformer from the other architectures - grounded in a theoretical comparison of the (loss) Hessian. Concretely, for a single self-attention layer, (a) we first entirely derive the Transformer's Hessian and express it in matrix derivatives; (b) we then characterize it in terms of data, weight, and attention moment dependencies; and (c) while doing so further highlight the important structural differences to the Hessian of classical networks. \nOur results suggest that various common architectural and optimization choices in Transformers can be traced back to their highly non-linear dependencies on the data and weight matrices, which vary heterogeneously across parameters. Ultimately, our findings provide a deeper understanding of the Transformer\u2019s unique optimization landscape and the challenges it poses."
    },
    {
        "title": "Critique-out-Loud Reward Models",
        "link_suffix": "/forum?id=e3odKmatZr",
        "link": "https://openreview.net/forum?id=e3odKmatZr",
        "pdf_link": "https://openreview.net/pdf?id=e3odKmatZr",
        "keywords": "Reward Model, Chain of Thought, Alignment, Preference Modeling, Large Language Model, LLM, RLHF",
        "abstract": "Traditionally, reward models used for reinforcement learning from human feedback (RLHF) are trained to directly predict preference scores without leveraging the generation capabilities of the underlying large language model (LLM). This limits the capabilities of reward models as they must reason implicitly about the quality of a response, i.e., preference modeling must be performed in a single forward pass through the model. To enable reward models to reason explicitly about the quality of a response, we introduce Critique-out-Loud (CLoud) reward models. CLoud reward models operate by first generating a natural language critique of the assistant's response that is then used to predict a scalar reward for the quality of the response. We demonstrate the success of CLoud reward models for both Llama-3-8B and 70B base models: compared to classic reward models CLoud reward models improve pairwise preference classification accuracy on RewardBench by 4.65 and 5.84 percentage points for the 8B and 70B base models respectively. Furthermore, CLoud reward models lead to a Pareto improvement for win rate on ArenaHard when used as the scoring model for Best-of-N. Finally, we explore how to exploit the dynamic inference compute capabilities of CLoud reward models by performing self-consistency decoding for reward prediction."
    },
    {
        "title": "ConvINT: A Semi-Structured Intention Framework for Conversational Understanding",
        "link_suffix": "/forum?id=FnlMYQPIzh",
        "link": "https://openreview.net/forum?id=FnlMYQPIzh",
        "pdf_link": "https://openreview.net/pdf?id=FnlMYQPIzh",
        "keywords": "Conversational Understanding, Weakly-supervised Generation, Large Language Models, Fine-tuning",
        "abstract": "Understanding user intentions is critical for conversational AI, especially with the rise of large language models (LLMs) that demand a more nuanced comprehension of dialogue. Existing approaches, relying on rigid slot-value structures or unstructured representations, often miss the complexity of human intentions. In this work, we propose ConvINT, a novel semi-structured intention framework that offers a more holistic and fine-grained understanding of user intentions by organizing them into four key aspects: situation, emotion, action, and knowledge. Grounded in psychological and cognitive intention theories, ConvINT provides LLMs with a richer context for understanding user inputs while offering a semi-structured format that seamlessly integrates with prompt-based intention learning. To enable the efficient adoption of this framework, we introduce a Weakly-supervised Reinforced Generation (WeRG) method that scales ConvINT annotations across large datasets with high quality. By combining a small set of human-annotated instances with coarsely labeled data as weak supervision signals, WeRG effectively learns to generate ConvINT annotations, ensuring both scalability and precision. Experimental results demonstrate that integrating ConvINT with WeRG markedly improves LLMs\u2019 ability to comprehend user intentions, yielding significant gains in downstream tasks such as response generation and task completion, as validated by both automatic metrics and human evaluations. These findings highlight ConvINT's potential as a comprehensive and adaptable framework for advancing intention understanding in conversational AI."
    }
]