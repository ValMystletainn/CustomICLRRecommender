[
    {
        "title": "LION: A bidirectional framework that trains like a Transformer and infers like an RNN",
        "link_suffix": "/forum?id=VxJjw52dZu",
        "link": "https://openreview.net/forum?id=VxJjw52dZu",
        "pdf_link": "https://openreview.net/pdf?id=VxJjw52dZu",
        "keywords": "Self-attention, transformer, bidirectionality, mamba, ssm, rnn, inference",
        "abstract": "We introduce LION, a novel sequence-to-sequence framework that unifies the bidirectionality and parallelized training of Transformers with the fast inference of recurrent neural networks. LION is built upon a mathematical formulation where full kernelized attention with a learnable mask is efficiently computed using a bidirectional selective recurrent model, matching the effectiveness of softmax-based attention with constant-time inference. Our framework naturally accounts for spatial and temporal relationships within input sequences, reducing reliance on heuristic positional embeddings and facilitating straightforward scalability in context length and resolution. Using our framework and inspired by the recent state-space models, we propose LION-S, a transformer with $selective$ mask and recurrent inference. Numerical evaluations on tasks such as language modeling, the Long-Range Arena, and image classification show that LION-S achieves performance on par with state-of-the-art models while delivering superior inference efficiency."
    },
    {
        "title": "Relation Augmented Preferential Bayesian Optimization via Preference Propagation",
        "link_suffix": "/forum?id=l1hk4q7ZY7",
        "link": "https://openreview.net/forum?id=l1hk4q7ZY7",
        "pdf_link": "https://openreview.net/pdf?id=l1hk4q7ZY7",
        "keywords": "Dueling Optimization, Preference Propagation, Data Augmentation",
        "abstract": "In black-box optimization, when directly evaluating the function values of solutions is very costly or infeasible, access to the objective function is often limited to comparing pairs of solutions, which yields dueling black-box optimization. Dueling optimization is solely based on pairwise preferences, and thus notably reduces cost compared with function value based methods such as Bayesian optimization. However, an optimization performance gap obviously exists between dueling based and function value based methods. This is mainly due to that most existing dueling optimization methods do not make full use of the pairwise preferences collected. To fill this gap, this paper proposes relation augmented preferential Bayesian optimization (RAPBO) via preference propagation. By considering solution similarity, RAPBO aims to uncover the potential preferential relations between solutions within different preferences through the proposed preferential relation propagation technique. Specifically, RAPBO first clusters solutions using a Gaussian mixture model. After obtaining the solution set with the highest intra-cluster similarity, RAPBO utilizes a directed hypergraph to model the potential relations between solutions, thereby realizing relation augmentation. Extensive experiments are conducted on both synthetic functions and real-world tasks such as motion control and spacecraft trajectory optimization. The experimental results disclose the satisfactory accuracy of augmented preferences in RAPBO, and show the superiority of RAPBO compared with existing dueling optimization methods. Notably, it is verified that, under the same evaluation cost budget, RAPBO is competitive with or even surpass the function value based Bayesian optimization methods with respect to optimization performance. The codes can be found inhttps://anonymous.4open.science/r/RAPBO-E15F."
    },
    {
        "title": "Sparse Watermarking in LLMs with Enhanced Text Quality",
        "link_suffix": "/forum?id=jbfDg4DgAk",
        "link": "https://openreview.net/forum?id=jbfDg4DgAk",
        "pdf_link": "https://openreview.net/pdf?id=jbfDg4DgAk",
        "keywords": "watermarking, large language models",
        "abstract": "With the widespread adoption of Large Language Models (LLMs), concerns about potential misuse have emerged. To this end, watermarking has been adapted to LLM, enabling a simple and effective way to detect and monitor generated text. However, while the existing methods can differentiate between watermarked and unwatermarked text with high accuracy, they often face a trade-off between the quality of the generated text and the effectiveness of the watermarking process. In this work, we present a novel type of LLM watermark,Sparse Watermark, which aims to mitigate this trade-off by applying watermarks to a small subset of generated tokens distributed across the text. To demonstrate this type of watermark, we introduceSpARK, aSparse WatermARKmethod that achieves sparsity by anchoring watermarked tokens to words that have specific Part-of-Speech (POS) tags. Our experimental results demonstrate that the proposed watermarking scheme achieves high detectability while generating text that outperforms previous LLM watermarking methods in quality across various tasks."
    },
    {
        "title": "Plasticity from Structured Sparsity: Mastering Continual Reinforcement Learning through Fine-grained Network Allocation and Dormant Neuron Exploration",
        "link_suffix": "/forum?id=3ENBquM4b4",
        "link": "https://openreview.net/forum?id=3ENBquM4b4",
        "pdf_link": "https://openreview.net/pdf?id=3ENBquM4b4",
        "keywords": "Continual reinforcement learning, Policy transfer",
        "abstract": "Continual reinforcement learning faces a central challenge in striking a balance between plasticity and stability to mitigate catastrophic forgetting. In this paper, we introduce SSDE, a novel structure-based method that aims to improve plasticity through a fine-grained allocation strategy with Structured Sparsity and Dormant-guided Exploration. Specifically, SSDE decomposes the parameter space for each task into forward-transfer (frozen) parameters and task-specific (trainable) parameters. Crucially, these parameters are allocated by an efficient co-allocation scheme under sparse coding, ensuring sufficient trainable capacity for new tasks while promoting efficient forward transfer through frozen parameters. Furthermore, structure-based methods often suffer from rigidity due to the accumulation of non-trainable parameters, hindering exploration. To overcome this, we propose a novel exploration technique based on sensitivity-guided dormant neurons, which systematically identifies and resets insensitive parameters. Our comprehensive experiments demonstrate that SSDE outperforms current state-of-the-art methods and achieves a superior success rate of $95%$% on CW10 Continual World benchmark."
    },
    {
        "title": "NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens",
        "link_suffix": "/forum?id=uMEsKEiB7J",
        "link": "https://openreview.net/forum?id=uMEsKEiB7J",
        "pdf_link": "https://openreview.net/pdf?id=uMEsKEiB7J",
        "keywords": "Long-context, Large Language Models, Question Answering",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has introduced a new frontier in natural language processing, particularly in understanding and processing long-context information. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically designed to test the capabilities of LLMs with extended texts. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper presents the design and construction of NovelQA, highlighting its manual annotation, and diverse question types. Our evaluation of Long-context LLMs on NovelQA reveals significant insights into the models' performance, particularly emphasizing the challenges they face with multi-hop reasoning, detail-oriented questions, and extremely long input with an average length more than 200,000 tokens. The results underscore the necessity for further advancements in LLMs to improve their long-context comprehension and computational literary studies."
    },
    {
        "title": "Highlight Diffusion: Training-Free Attention Guided Acceleration for Text-to-Image Models",
        "link_suffix": "/forum?id=Jt1gGIumJo",
        "link": "https://openreview.net/forum?id=Jt1gGIumJo",
        "pdf_link": "https://openreview.net/pdf?id=Jt1gGIumJo",
        "keywords": "Diffusion model, Cross attention, Acceleration, Text to Image",
        "abstract": "Diffusion models have achieved exceptional results in image synthesis, yet their sequential processing nature imposes significant computational demands and latency, posing challenges for practical deployment. In this paper, we present Highlight Diffusion: a training-free novel acceleration approach that achieves significant speedup while retaining generation quality through an attention-guided generation process. By utilizing cross-attention maps to identify crucial segments within the image, we selectively compute these highlighted regions during the denoising process, bypassing the need for full-resolution computation at every step. This strategy maintains high-quality outputs while enabling faster, more resource-efficient diffusion model inference. With minimal loss in generated image quality—evidenced by only a 0.65 increase in FID score and a 0.02 decrease in CLIP score, Highlight Diffusion achieved a 1.52 $\\times$ speedup using an NVIDIA RTX 3090 GPU."
    },
    {
        "title": "Graphical-TS: An Interactive AI Pipeline for Multivariate Time Series with Ground-truth Graphical Modeling",
        "link_suffix": "/forum?id=meY36sGyyv",
        "link": "https://openreview.net/forum?id=meY36sGyyv",
        "pdf_link": "https://openreview.net/pdf?id=meY36sGyyv",
        "keywords": "time series, causal discovery, benchmarking, interface",
        "abstract": "We present \\texttt{Graphical-TS}, an interactive simulation framework for multivariate time series (MTS) incorporating spatiotemporal causal graphical models. The system offers extensive customizability, enabling users to define and modify causal dynamics with uncertainty in spatiotemporal relationships and functional mappings. \\texttt{Graphical-TS} integrates expert knowledge, supports MTS simulation, and allows for the input of real-world MTS data, facilitating a dynamic interplay between data-driven learning and domain expertise. The system iteratively enhances causal relationships and simulated data by simulating MTS data based on specified causal graphs, performing causal discovery from real or simulated MTS, and enabling the integration and refinement of expert knowledge with learned causality. This approach progressively improves the quality of causal models and the data they generate, supporting tasks such as time series forecasting, imputation, prediction, and robustness testing via scenario-driven distribution shifts. We compared state-of-the-art causal discovery methods on datasets generated by \\texttt{Graphical-TS}. The empirical results demonstrate the platform’s consistent performance compared to existing methods while offering versatility under distinct scenarios. This enables users to explore datasets more thoroughly and drive improvements in causal discovery research. With an intuitive user interface that connects domain experts and algorithm developers, \\texttt{Graphical-TS} empowers users to manipulate causal relationships, embedding domain knowledge into machine learning workflows. Originally developed to study physiological dynamics in patients, the system has broad applicability across various fields, offering a versatile platform for generating MTS datasets with known dynamics, validating causal discovery algorithms, and advancing research in time series analysis."
    },
    {
        "title": "Detecting and Approximating Redundant Computational Blocks in Neural Networks",
        "link_suffix": "/forum?id=gSGRSxVcRP",
        "link": "https://openreview.net/forum?id=gSGRSxVcRP",
        "pdf_link": "https://openreview.net/pdf?id=gSGRSxVcRP",
        "keywords": "latent representations, representation learning, neural network similarities, classification, foundation models, large models",
        "abstract": "Deep neural networks often learn similar internal representations, both across different models and within their own layers. While inter-network similarities have enabled techniques such as model stitching and merging, intra-network similarities present new opportunities for designing more efficient architectures. In this paper, we investigate the emergence of these internal similarities across different layers in diverse neural architectures, showing that similarity patterns emerge independently of the datataset used. We introduce a simple metric, Block Redundancy (BR), to detect redundant blocks, providing a foundation for future architectural optimization methods. Building on this, we propose Redundant Blocks Approximation (RBA), a general framework that identifies and approximates one or more redundant computational blocks using simpler transformations. We show that the transformation $\\mathcal{T}$ between two representations can be efficiently computed in closed-form, and it is enough to replace the redundant blocks from the network. RBA reduces model parameters and time complexity while maintaining good performance. We validate our method on classification tasks in the vision domain, using a variety of pretrained foundational models and datasets."
    },
    {
        "title": "Look Before You Leap: Universal Emergent Mechanism for Retrieval in Language Models",
        "link_suffix": "/forum?id=eIB1UZFcFg",
        "link": "https://openreview.net/forum?id=eIB1UZFcFg",
        "pdf_link": "https://openreview.net/pdf?id=eIB1UZFcFg",
        "keywords": "Interpretability, LLM, Universality",
        "abstract": "When solving challenging problems, language models (LMs) are able to identify relevant information from long and complicated contexts. To study how LMs solve retrieval tasks in diverse situations, we introduce ORION, a collection of structured retrieval tasks spanning six domains, from text understanding to coding. Each task in ORION can be represented abstractly by a request (e.g. a question) that retrieves an attribute (e.g. the character name) from a context (e.g. a story). We apply causal analysis on 18 open-source language models with sizes ranging from 125 million to 70 billion parameters. We find that LMs internally decompose retrieval tasks in a modular way: middle layers at the last token position process the request, while late layers retrieve the correct entity from the context. After causally enforcing this decomposition, models are still able to solve the original task, preserving 70% of the original correct token probability in 98 of the 106 studied model-task pairs. We connect our macroscopic decomposition with a microscopic description by performing a fine-grained case study of a question-answering task on Pythia-2.8b. Building on our high-level understanding, we demonstrate a proof of concept application for scalable internal oversight of LMs to mitigate prompt-injection while requiring human supervision on only a single input. Our solution improves accuracy drastically (from 15.5% to 97.5% on Pythia-12b). This work presents evidence of a universal emergent modular processing of tasks across varied domains and models and is a pioneering effort in applying interpretability for scalable internal oversight of LMs."
    },
    {
        "title": "Screener: Learning Conditional Distribution of Dense Self-supervised Representations for Unsupervised Pathology Segmentation in 3D Medical Images",
        "link_suffix": "/forum?id=K4JHTZ13G3",
        "link": "https://openreview.net/forum?id=K4JHTZ13G3",
        "pdf_link": "https://openreview.net/pdf?id=K4JHTZ13G3",
        "keywords": "Unsupervised visual anomaly detection, self-supervised learning, density estimation, medical image analysis",
        "abstract": "In this paper we present a fully self-supervised framework for visual anomaly segmentation and apply it to pathology segmentation in 3D medical CT images. The core idea behind our framework is to learn conditional distribution of local image patterns given their global context. Thus, image patterns that have low conditional probability are assigned high anomaly scores.\nTo this end, we propose Screener comprised of descriptor,  condition, and density models.\nThe descriptor model encodes local image patterns into dense self-supervised representations. We enforce these descriptors to discriminate different image positions and remain invariant w.r.t. image augmentations that preserve local content.\nThe condition model produces auxiliary dense image representations, which we call conditions. We ensure that conditions encode the global contexts of individual image positions, by enforcing them to be invariant w.r.t. image masking.\nAt last, the density model learns the conditional density of descriptors for each given condition and produces anomaly segmentation scores.\nWe use this framework to train a fully self-supervised model for pathology segmentation on more than 30,000 3D CT images. Empirical study shows that Screener outperforms the existing unsupervised anomaly segmentation methods on four large-scale test CT datasets containing a total of 1820 3D images with four chest and abdominal pathologies."
    },
    {
        "title": "A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules",
        "link_suffix": "/forum?id=KnoS9XxIlK",
        "link": "https://openreview.net/forum?id=KnoS9XxIlK",
        "pdf_link": "https://openreview.net/pdf?id=KnoS9XxIlK",
        "keywords": "Large language model, Learning rate scheduler, Scaling Law, Hyperparameter optimization",
        "abstract": "Training large models is both resource-intensive and time-consuming, making it crucial to understand the quantitative relationship between model performance and hyperparameters. In this paper, we derive an empirical law that predicts pretraining loss for large language models for every intermediate training step across various learning rate schedules, including constant, cosine, and step decay schedules. Our proposed law takes a multi-power form, combining a power law based on the sum of learning rates and additional power laws to account for a loss reduction effect as learning rate decays. We validate this law extensively on Llama-2 models of varying sizes and demonstrate that, after fitting on a few learning rate schedules, it accurately predicts the loss curves for unseen schedules of different shapes and horizons. Moreover, by minimizing the predicted final pretraining loss across learning rate schedules, we are able to find a schedule that outperforms the widely-used cosine learning rate schedule. Interestingly, this automatically discovered schedule bears some resemblance to the recently proposed Warmup-Stable-Decay (WSD) schedule (hu et al, 2024) but achieves slightly faster convergence. We believe these results could offer valuable insights for understanding the dynamics of pretraining and for designing learning rate schedules to improve efficiency."
    },
    {
        "title": "LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token",
        "link_suffix": "/forum?id=UQJ7CDW8nb",
        "link": "https://openreview.net/forum?id=UQJ7CDW8nb",
        "pdf_link": "https://openreview.net/pdf?id=UQJ7CDW8nb",
        "keywords": "Large Multimodal Models, Large Language Models",
        "abstract": "The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers, where they fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVAMini can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on GPU hardware with 24GB of memory."
    },
    {
        "title": "URLOST: Unsupervised Representation Learning without Stationarity or Topology",
        "link_suffix": "/forum?id=MBBRHDuiwM",
        "link": "https://openreview.net/forum?id=MBBRHDuiwM",
        "pdf_link": "https://openreview.net/pdf?id=MBBRHDuiwM",
        "keywords": "Unsupervised learning, Self-Supervised Learning, NeuroAI, Multi-Modality, Human Vision, Biologically-inspired Models",
        "abstract": "Unsupervised representation learning has seen tremendous progress. However, it is constrained by its reliance on domain specific stationarity and topology, a limitation not found in biological intelligence systems. For instance, unlike computer vision, human vision can process visual signals sampled from highly irregular and non-stationary sensors. We introduce a novel framework that learns from high-dimensional data without prior knowledge of stationarity and topology. Our model, abbreviated as URLOST, combines a learnable self-organizing layer, spectral clustering, and a masked autoencoder (MAE). We evaluate its effectiveness on three diverse data modalities including simulated biological vision data, neural recordings from the primary visual cortex, and gene expressions. Compared to state-of-the-art unsupervised learning methods like SimCLR and MAE, our model excels at learning meaningful representations across diverse modalities without knowing their stationarity or topology. It also outperforms other methods that are not dependent on these factors, setting a new benchmark in the field. We position this work as a step toward unsupervised learning methods capable of generalizing across diverse high-dimensional data modalities."
    },
    {
        "title": "One-for-All Few-Shot Anomaly Detection via Instance-Induced Prompt Learning",
        "link_suffix": "/forum?id=Zzs3JwknAY",
        "link": "https://openreview.net/forum?id=Zzs3JwknAY",
        "pdf_link": "https://openreview.net/pdf?id=Zzs3JwknAY",
        "keywords": "Anomaly detection, few-shot, vision-language model",
        "abstract": "Anomaly detection methods under the 'one-for-all' paradigm aim to develop a unified model capable of detecting anomalies across multiple classes. However, these approaches typically require a large number of normal samples for model training, which may not always be feasible in practice. Few-shot anomaly detection methods can address scenarios with limited data but often require a tailored model for each class, struggling within the 'one-for-one' paradigm. In this paper, we first proposed the one-for-all few-shot anomaly detection method with the assistance of vision-language model. Different from previous CLIP-based methods learning fix prompts for each class, our method learn a class-shared prompt generator to adaptively generate suitable prompt for each instance. The prompt generator is trained by aligning the prompts with the visual space and utilizing guidance from general textual descriptions of normality and abnormality. Furthermore, we address the mismatch problem of the memory bank within one-for-all paradigm. Extensive experimental results on MVTec and VisA demonstrate the superiority of our method in few-shot anomaly detection task under the one-for-all paradigm."
    },
    {
        "title": "Synthesizing Physical Backdoor Datasets: An Automated Framework Leveraging Deep Generative Models",
        "link_suffix": "/forum?id=vG123yHVVl",
        "link": "https://openreview.net/forum?id=vG123yHVVl",
        "pdf_link": "https://openreview.net/pdf?id=vG123yHVVl",
        "keywords": "Backdoor Attacks, Physical Backdoor Attacks, Data Synthesis, Automated Framework",
        "abstract": "Backdoor attacks, representing an emerging threat to the integrity of deep neural networks, have garnered significant attention due to their ability to compromise deep learning systems clandestinely. \nWhile numerous backdoor attacks occur within the digital realm, their practical implementation in real-world prediction systems remains limited and vulnerable to disturbances in the physical world. \nConsequently, this limitation has given rise to the development of physical backdoor attacks, where trigger objects manifest as physical entities within the real world. \nHowever, creating the requisite dataset to train or evaluate a physical backdoor model is a daunting task, limiting the backdoor researchers and practitioners from studying such physical attack scenarios. This paper unleashes a framework that empowers backdoor researchers to effortlessly create a malicious, physical backdoor dataset based on advances in generative modeling. Particularly, this framework involves 3 automatic modules: suggesting the suitable physical triggers, generating the poisoned candidate samples (either by synthesizing new samples or editing existing clean samples), and finally refining for the most plausible ones. As such, it effectively mitigates the perceived complexity associated with creating a physical backdoor dataset, transforming it from a daunting task into an attainable objective. Extensive experiment results show that datasets created by our framework enable researchers to achieve an impressive attack success rate on real physical world data and exhibit similar properties compared to previous physical backdoor attack studies. This paper offers researchers a valuable toolkit for studies of physical backdoors, all within the confines of their laboratories."
    },
    {
        "title": "K-HALU: Multiple Answer Korean Hallucination Benchmark for Large Language Models",
        "link_suffix": "/forum?id=VnLhUogHYE",
        "link": "https://openreview.net/forum?id=VnLhUogHYE",
        "pdf_link": "https://openreview.net/pdf?id=VnLhUogHYE",
        "keywords": "Hallucination, Benchmark dataset, Multiple answer, Korean, Large language model",
        "abstract": "Recent researchers and companies have been developing large language models (LLMs) specifically designed for particular purposes and have achieved significant advancements in various natural language processing tasks. However, LLMs are still prone to generating hallucinations—results that are factually inconsistent or uncertain. As a result, the need for datasets to evaluate and demonstrate the hallucination detection capabilities of LLMs is increasingly recognized. Nonetheless, the Korean NLP community lacks publicly available benchmark datasets demonstrating the factuality of knowledge-based information. Furthermore, the few existing datasets that evaluate hallucination are limited in their access to the entire dataset, restricting detailed analysis beyond simple scoring, and are based on translated English knowledge. To address these challenges, we introduce K-HALU, a Korean benchmark designed to evaluate LLMs' hallucination detection in Korean. This benchmark contains seven domains, considering the factuality of statements based on knowledge documents compiled from Korean news, magazines, and books. For more strict evaluation, 40% of the dataset is structured as multiple-answer questions, requiring models to select all possible correct answers from the given options. Our empirical results show that open-source LLMs still struggle with hallucination detection in Korean knowledge, emphasizing the need for a more detailed analysis of their limitations. The K-HALU benchmark will be made publicly available after the anonymous review period."
    },
    {
        "title": "Shell Games: Control Protocols for Adversarial AI Agents",
        "link_suffix": "/forum?id=oycEeFXX74",
        "link": "https://openreview.net/forum?id=oycEeFXX74",
        "pdf_link": "https://openreview.net/pdf?id=oycEeFXX74",
        "keywords": "Safety, Code generation, Red teaming, Large language models, Monitoring, Alignment, Security, AI Control",
        "abstract": "As more powerful AI agents are deployed with greater tool use and autonomy, it will become increasingly important to ensure they cannot take severely damaging actions. Recent work introduced \"control evaluations,\" which study whether monitoring and security protocols intended to prevent dangerous actions are robust to intentional subversion by these AIs. In a control evaluation, ared teamseeks to subvert the safety protocols built by theblue team. The blue team has access to a weakertrustedmodel and a limited amount of high-quality, trusted labor to attempt to safely extract work from a strongeruntrustedmodel. Prior work conducted a control evaluation using competition programming problems with the red team objective of writing subtly incorrect (or \"backdoored\") solutions (Greenblatt et al. 2023). We extend this work by performing a control evaluation in a more realistic setting in which an untrusted LLM agent executes commands in a bash shell. The blue team seeks to use that agent to complete our novel dataset of 200 challenging multi-step tasks, each of which requires the agent to iteratively develop a bash script that passes test cases. The red team creates an attack policy that uses the untrusted model to covertly download and execute a dummy malware file while completing tasks without getting caught by the blue team's safety protocols. This agentic setting allows both teams to explore complex new strategies that have not been previously studied. We extend protocols from previous work to this agentic setting, propose novel protocols, and evaluate each on our safety and usefulness metrics. We find that our protocols substantially improve the Pareto frontier of usefulness and safety relative to simple baselines: one of our best protocols exhibits approximately a 2.5 times improvement in safety metrics with no statistically significant decrease in usefulness compared to our trusted monitoring baseline."
    },
    {
        "title": "Charting the Design Space of Neural Graph Representations for Subgraph Matching",
        "link_suffix": "/forum?id=5pd78GmXC6",
        "link": "https://openreview.net/forum?id=5pd78GmXC6",
        "pdf_link": "https://openreview.net/pdf?id=5pd78GmXC6",
        "keywords": "Graph Retrieval, Graph Neural Networks, Subgraph Matching",
        "abstract": "Subgraph matching is vital in knowledge graph (KG) question answering, molecule design, scene graph, code and circuit search, etc.\nNeural methods have shown promising results for subgraph matching.\nOur study of recent systems suggests refactoring them into a unified design space for graph matching networks.\nExisting methods occupy only a few isolated patches in this space, which remains largely uncharted.\nWe undertake the first comprehensive exploration of this space, featuring such axes as attention-based vs. soft permutation-based interaction between query and corpus graphs, aligning nodes vs. edges, and the form of the final scoring network that integrates neural representations of the graphs.\nOur extensive experiments reveal that judicious and hitherto-unexplored combinations of choices in this space lead to large performance benefits.\nBeyond better performance, our study uncovers valuable insights and establishes general design principles for neural graph representation and interaction, which may be of wider interest."
    },
    {
        "title": "Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification",
        "link_suffix": "/forum?id=DTqx3iqjkz",
        "link": "https://openreview.net/forum?id=DTqx3iqjkz",
        "pdf_link": "https://openreview.net/pdf?id=DTqx3iqjkz",
        "keywords": "Continual Learning, Sequential Learning, Gradient Descent, Linear Classification, Convergence, Implicit Bias",
        "abstract": "We study continual learning on multiple linear classification tasks by sequentially running gradient descent (GD) for a fixed budget of iterations per each given task. When all tasks are jointly linearly separable and are presented in a cyclic/random order, we show the directional convergence of the trained linear classifier to the joint (offline) max-margin solution. This is surprising because GD training on a single task is implicitly biased towards the individual max-margin solution for the task, and the direction of the joint max-margin solution can be largely different from these individual solutions. Additionally, when tasks are given in a cyclic order, we present a non-asymptotic analysis on cycle-averaged forgetting, revealing that (1) task similarity is indeed closely tied to catastrophic forgetting and backward knowledge transfer and (2) the amount of forgetting vanishes to zero as the cycle repeats. Lastly, we analyze the case where the tasks are no longer jointly separable and show that the model trained in a cyclic order converges to the unique minimum of the joint loss function."
    },
    {
        "title": "Contextual Kernels for Task-Aware Fine-Tuning in Vision-Language Models",
        "link_suffix": "/forum?id=gCCYBGGYTi",
        "link": "https://openreview.net/forum?id=gCCYBGGYTi",
        "pdf_link": "https://openreview.net/pdf?id=gCCYBGGYTi",
        "keywords": "Continual Learning, Model Adaptability, Task Incremental Learning, Kernel Based Task Reprresentation Learning",
        "abstract": "Vision-Language Models (VLMs) exhibit impressive generalization due to training on vast datasets like ImageNet. However, their performance diminishes on unfamiliar tasks. While downstream fine-tuning enhances adaptability, it often sacrifices inherent generality. To address this, we propose a novel method leveraging contextual generation for enhanced task representation within a semantic space. Our approach utilizes VLMs to generate detailed contextual descriptions for test image batches, developing Contextual Kernels (CK) for each class in the semantic space. \nOur test-time fine-tuning preserves core VLM features by freezing fundamental components and extending a linear network for semantic kernel density projection. This strategy significantly boosts model adaptability for real-world tasks. Despite strong zero-shot capabilities, we explore additional training samples to improve adaptability in dynamic Task Incremental Learning (TIL) scenarios. Each task's unique CK distribution serves as a fingerprint, enabling high-performance TIL with minimal forgetting. Experiments on four TIL datasets demonstrate the efficacy of our framework, achieving state-of-the-art performance. Our findings reveal that the semantic space within the text mode encapsulates both VLMs' generality and adaptability, paving the way for robust applications in diverse, evolving task environments. This work systematically balances generality and adaptability in VLMs, addressing a critical gap in current research."
    },
    {
        "title": "ZEPHYR GAN: REDEFINING GAN WITH FLEXIBLE GRADIENT CONTROL",
        "link_suffix": "/forum?id=f6GMwpxXHG",
        "link": "https://openreview.net/forum?id=f6GMwpxXHG",
        "pdf_link": "https://openreview.net/pdf?id=f6GMwpxXHG",
        "keywords": "Generative Adversarial Network, Zephyr loss, Adversarial Training, Flexible Gradient Control.",
        "abstract": "Generative adversarial networks (GANs) are renowned for their ability to generate highly realistic and diverse data samples. However, the performance of GANs is heavily dependent on the choice of loss functions, and commonly used losses such as cross-entropy and least squares are often susceptible to outliers, vanishing gradients, and training instability. To overcome these limitations, we introduce zephyr loss—a novel, convex, smooth, and Lipschitz continuous loss function designed to enhance robustness and provide flexible gradient control. Leveraging this new loss function, we propose ZGAN, a refined GAN model that guarantees a unique optimal discriminator and stabilizes the overall training dynamics. Furthermore, we demonstrate that optimizing ZGAN's generator objective minimizes a weighted total variation between the real and generated data distributions. Through rigorous theoretical analysis, including convergence proofs, we substantiate the robustness and effectiveness of ZGAN, positioning it as a compelling and reliable alternative for stable GAN training. Extensive experiments further demonstrate that ZGAN surpasses leading methods in generative modeling."
    },
    {
        "title": "Tokenizing 3D Molecule Structure with Quantized Spherical Coordinates",
        "link_suffix": "/forum?id=UqrSyATn7F",
        "link": "https://openreview.net/forum?id=UqrSyATn7F",
        "pdf_link": "https://openreview.net/pdf?id=UqrSyATn7F",
        "keywords": "molecule structure tokenization, generation, spherical coordinates, VQ-VAE",
        "abstract": "The application of language models (LMs) to molecular structure generation using line notations such as SMILES and SELFIES has been well-established in the field of cheminformatics. However, extending these models to generate 3D molecular structures presents significant challenges. Two primary obstacles emerge: (1) the difficulty in designing a 3D line notation that ensures SE(3)-invariant atomic coordinates, and (2) the non-trivial task of tokenizing continuous coordinates for use in LMs, which inherently require discrete inputs.\nTo address these challenges, we propose Mol-StrucTok, a novel method for tokenizing 3D molecular structures. Our approach comprises two key innovations: (1) We design a line notation for 3D molecules by extracting local atomic coordinates in a spherical coordinate system. This notation builds upon existing 2D line notations and remains agnostic to their specific forms, ensuring compatibility with various molecular representation schemes. (2) We employ a Vector Quantized Variational Autoencoder (VQ-VAE) to tokenize these coordinates, treating them as generation descriptors. To further enhance the representation, we incorporate neighborhood bond lengths and bond angles as understanding descriptors. Leveraging this tokenization framework, we train a GPT-2 style model for 3D molecular generation tasks. Results demonstrate strong performance with significantly faster generation speeds and competitive chemical stability compared to previous methods. \nFurther, by integrating our learned discrete representations into Graphormer model for property prediction on QM9 dataset, Mol-StrucTok reveals consistent improvements across various molecular properties, underscoring the versatility and robustness of our approach."
    },
    {
        "title": "ProcBench: Benchmark for Multi-Step Reasoning and Following Procedure",
        "link_suffix": "/forum?id=MK6E6IgROl",
        "link": "https://openreview.net/forum?id=MK6E6IgROl",
        "pdf_link": "https://openreview.net/pdf?id=MK6E6IgROl",
        "keywords": "benchmark, dataset, multi-step reasoning, LLM, large language model, instruction following",
        "abstract": "Reasoning is central to a wide range of intellectual activities, and while the capabilities of large language models (LLMs) continue to advance, their performance in reasoning tasks remains limited. The processes and mechanisms underlying reasoning are not yet fully understood, but key elements include path exploration, selection of relevant knowledge, and multi-step inference. Problems are solved through the synthesis of these components. In this paper, we propose a benchmark that focuses on a specific aspect of reasoning ability: the direct evaluation of multi-step inference. To this end, we design an extreme reasoning task where multi-step inference is specifically focused by largely eliminating path exploration and implicit knowledge utilization. Our dataset comprises pairs of explicit instructions and corresponding questions, where the procedures necessary for solving the questions are entirely detailed within the instructions. This setup allows models to solve problems solely by following the provided directives. By constructing problems that require varying numbers of steps to solve and evaluating responses at each step, we enable a thorough assessment of state-of-the-art LLMs' ability to follow instructions. To ensure the robustness of our evaluation, we include multiple distinct tasks. Furthermore, by comparing accuracy across tasks, utilizing step-aware metrics, and applying separately defined measures of complexity, we conduct experiments that offer insights into the capabilities and limitations of LLMs in reasoning tasks. Our findings have significant implications for the development of LLMs and highlight areas for future research in advancing their reasoning abilities."
    },
    {
        "title": "Exploiting the Kurtosis Concentration Property for Image quality improvement",
        "link_suffix": "/forum?id=8eKMxc1SXg",
        "link": "https://openreview.net/forum?id=8eKMxc1SXg",
        "pdf_link": "https://openreview.net/pdf?id=8eKMxc1SXg",
        "keywords": "kurtosis concentration, diffusion model",
        "abstract": "Diffusion models have significantly advanced generative AI  in terms of creating and editing naturalistic images. \nHowever, improving the image quality of generated images is still of paramount interest.\nIn this context, we propose a generic kurtosis concentration (KC) loss, which can be readily applied to any standard diffusion model pipeline to improve image quality.  Our motivation stems from the \\emph{projected kurtosis concentration property} of natural images, which states that natural images have nearly constant kurtosis values across different band-pass versions of the image. To improve the image quality of generated images, we reduce the gap between the highest and lowest kurtosis values across the band-pass versions (e.g., Discrete Wavelet Transform (DWT)) of images. In addition, we also propose a novel condition-agnostic perceptual guidance strategy during inference to further improve the image quality. We validate the proposed approach for three diverse tasks, viz., (1) personalized few-shot finetuning using text guidance, (2) unconditional image generation, and (3) image super-resolution. Integrating the proposed KC loss and perceptual guidance has improved the perceptual quality across all these tasks in terms of FID, MUSIQ score, and user evaluation. Code is provided in appendix."
    },
    {
        "title": "Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition",
        "link_suffix": "/forum?id=0EP01yhDlg",
        "link": "https://openreview.net/forum?id=0EP01yhDlg",
        "pdf_link": "https://openreview.net/pdf?id=0EP01yhDlg",
        "keywords": "Large language model, Self-speculative decoding, Multi-token prediction, Low-rank approximation",
        "abstract": "We propose a new model for multi-token prediction in transformers, aiming to enhance sampling efficiency without compromising accuracy. Motivated by recent work that predicts the probabilities of subsequent tokens using multiple heads, we connect this approach to rank-1 canonical tensor decomposition. By generalizing it to a rank-r canonical probability decomposition, we develop an improved model that predicts multiple tokens simultaneously. This model can also be interpreted as a mixture of experts, allowing us to leverage successful techniques from that domain for efficient and robust training. Importantly, the overall overhead for training and sampling remains low. Our method demonstrates significant improvements in inference speed for both text and code generation tasks, proving particularly beneficial within the self-speculative decoding paradigm. It maintains its effectiveness across various model sizes and training epochs, highlighting its robustness and scalability."
    }
]