[{"title": "HyperAdapter: Generating Adapters for Pre-Trained Model-Based Continual Learning", "link_suffix": "/forum?id=29sul3tAEa", "link": "https://openreview.net/forum?id=29sul3tAEa", "pdf_link": "https://openreview.net/pdf?id=29sul3tAEa", "keywords": "hypernetworks, adapter tuning, class-incremental learning", "abstract": "Humans excel at leveraging past experiences to learn new skills, while artificial neural networks suffer from the phenomenon of catastrophic forgetting during sequential learning. Efforts have been made to alleviate forgetting by introducing a rehearsal buffer into the model, but this way is impractical in real-world scenarios with data privacy. Recently, pre-trained model-based continual learning methods have provided new insights into addressing this issue by effectively utilizing the powerful representational capabilities of pre-trained models to avoid catastrophic forgetting without a rehearsal buffer. In this work, we propose a novel pre-trained model-based continual learning framework, HyperAdapter, which utilizes a hypernetwork to generate adapters based on the current input, adapting the pre-trained model to the corresponding task. This paradigm requires fewer additional parameters as the number of tasks increases, which is a critical advantage for scaling to long sequences continual learning. Unlike methods that partition task-related knowledge into relatively independent subspaces, it promotes positive knowledge transfer across tasks. Comprehensive experiments across various datasets demonstrate that HyperAdapter consistently outperforms all existing methods and even exceeds the upper bounds of multi-task learning, establishing a new state-of-the-art for pre-trained model-based continual learning. Our code will be released.", "title_embedding_index": 6800, "title_abs_embedding_index": 6825}, {"title": "Retrieval Augmented Thought Process for Private Data Handling in Healthcare", "link_suffix": "/forum?id=AepP8ddd3L", "link": "https://openreview.net/forum?id=AepP8ddd3L", "pdf_link": "https://openreview.net/pdf?id=AepP8ddd3L", "keywords": "Privacy, Information-retrieval, Large Language Model, Question-Answering, retrieval based reasoning", "abstract": "Large Language Models (LLMs) have demonstrated the strong potential to assist both clinicians and the general public with their extensive medical knowledge. However, their application in healthcare is constrained due to concerns about the privacy of data used in training, which prevents the integration of private and personal information because of security and ethical issues. Moreover, if their capabilities can be enhanced with information retrieval to access up-to-date knowledge, the current integration of LLMs with Information retrieval lacks robustness to imperfect retrieval, which can hinder their effectiveness and even reduce overall performance. In this work, we address this challenge by introducing the Retrieval-Augmented Thought Process (RATP). Given access to external knowledge, RATP formulates the thought generation of LLMs as a multiple-step decision process. To optimise such a thought process, RATP leverages Monte-Carlo Tree Search and learns a proxy reward function that permits cost-efficient inference. On a private dataset of electronic medical records, deliberately excluded from any LLM training set, RATP achieves 35% additional accuracy compared to in-context retrieval-augmented generation for the question-answering task.", "title_embedding_index": 6801, "title_abs_embedding_index": 6826}, {"title": "Causal-aware Graph Neural Architecture Search under Distribution Shifts", "link_suffix": "/forum?id=58AhfT4Zz1", "link": "https://openreview.net/forum?id=58AhfT4Zz1", "pdf_link": "https://openreview.net/pdf?id=58AhfT4Zz1", "keywords": "Graph Neural Architecture Search, Out-of-Distribution Generalization, Causal Learning", "abstract": "Graph neural architecture search (Graph NAS) has emerged as a promising approach for autonomously designing graph neural network architectures by leveraging the correlations between graphs and architectures. However, the existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. In this paper, we propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with the following critical challenges: 1) how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, 2) how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose a novel approach, Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments on synthetic and real-world datasets demonstrate that our proposed CARNAS achieves advanced out-of-distribution generalization ability by discovering the causal relationship between graphs and architectures during the search process.", "title_embedding_index": 6802, "title_abs_embedding_index": 6827}, {"title": "MQ-VAE: Training Vector-Quantized Networks via Meta Learning", "link_suffix": "/forum?id=ZVe2k7mNAP", "link": "https://openreview.net/forum?id=ZVe2k7mNAP", "pdf_link": "https://openreview.net/pdf?id=ZVe2k7mNAP", "keywords": "Vector-Quantized Networks, Bi-level Optimization, Image Generation", "abstract": "Deep neural networks with discrete latent variables are particularly well-suited for tasks that naturally involve sequences of discrete symbols.\nThe vector-quantized variational auto-encoder (VQ-VAE) has made significant progress in this area by leveraging vector quantization.\nHowever, while much effort has been put into maximizing codebook utilization, this does not always result in better performance.\nAdditional challenges include quantization errors in the VQ layer and the lack of direct integration of task loss into the codebook objective.\nTo address these issues, we propose Meta-Quantized Variational Auto-Encoder (MQ-VAE), a bi-level optimization-based vector quantization framework inspired by meta-learning.\nIn MQ-VAE, the codebook and encoder-decoder pair are optimized at different levels, with the codebook treated as hyperparameters optimized via hyper-gradient descent.\nThis approach effectively tackles these challenges within a unified framework.\nThe evaluation of MQ-VAE on two computer vision tasks demonstrates its superiority over existing methods and ablation baselines.\nCode is available athttps://anonymous.4open.science/r/MQVAE-B52C.", "title_embedding_index": 6803, "title_abs_embedding_index": 6828}, {"title": "MORE: A MIXTURE OF LOW-RANK EXPERTS FOR ADAPTIVE MULTI-TASK LEARNING", "link_suffix": "/forum?id=LWvgajBmNH", "link": "https://openreview.net/forum?id=LWvgajBmNH", "pdf_link": "https://openreview.net/pdf?id=LWvgajBmNH", "keywords": "Large Language Models, LoRA, Multi-task Learning, Mixture of Experts", "abstract": "With the rapid development of Large Language Models (LLMs), Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant attention, which aims to achieve efficient fine-tuning of LLMs with fewer parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA) introduces low-rank matrices to approximate the incremental tuning parameters and achieves impressive performance over multiple scenarios. After that, plenty of improvements have been proposed for further improvement. However, these methods either focus on single-task scenarios or separately train multiple LoRA modules for multi-task scenarios, limiting the efficiency and effectiveness of LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for multi-task PEFT. Specifically, instead of using an individual LoRA for each task, we align different ranks of LoRA module with different tasks, which we named low-rank experts. Moreover, we design a novel adaptive rank selector to select the appropriate expert for each task. By jointly training low-rank experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task scenarios. Finally, we conduct extensive experiments over multiple multi-task benchmarks along with different LLMs to verify model performance. Experimental results demonstrate that compared to traditional LoRA and its variants, MoRE significantly improves the performance of LLMs in multi-task scenarios and incurs no additional inference cost. We also release the model and code to facilitate the community.", "title_embedding_index": 6804, "title_abs_embedding_index": 6829}, {"title": "Disentangled interleaving variational encoding", "link_suffix": "/forum?id=QDNUuB5DeO", "link": "https://openreview.net/forum?id=QDNUuB5DeO", "pdf_link": "https://openreview.net/pdf?id=QDNUuB5DeO", "keywords": "Representation Learning, Disentanglement, Probability Theory, Mathematical Optimization, Variational Autoencoder, Time Series Forecasting", "abstract": "Conflicting objectives present a considerable challenge in interleaving multi-task learning, necessitating the need for meticulous design and balance to ensure effective learning of a representative latent data space across all tasks without mutual negative impact. Drawing inspiration from the concept of marginal and conditional probability distributions in probability theory, we design a principled and well-founded approach to disentangle the original input into marginal and conditional probability distributions in the latent space of a variational autoencoder. Our proposed model, Deep Disentangled Interleaving Variational Encoding (DeepDIVE) learns disentangled features from the original input to form clusters in the embedding space and unifies these features via the cross-attention mechanism in the fusion stage. We theoretically prove that combining the objectives for reconstruction and forecasting fully captures the lower bound and mathematically derive a loss function for disentanglement using Na\u00efve Bayes. Under the assumption that the prior is a mixture of log-concave distributions, we also establish that the Kullback-Leibler divergence between the prior and the posterior is upper bounded by the cross entropy loss, informing our adoption of radial basis functions (RBF) and cross entropy with interleaving training for DeepDIVE to provide a justified basis for convergence. Experiments on anonymous bidding data from the National Electricity Market of Singapore (NEMS) show that DeepDIVE disentangles the original input and yields more accurate forecasts, outperforming current state-of-the-art baselines. In the context of the power market, this study can enhance operational decisions and bidding strategies by offering insights into the embedded supply curve via the representation space.", "title_embedding_index": 6805, "title_abs_embedding_index": 6830}, {"title": "SQuBa: Speech Mamba Language Model with Querying-Attention for Efficient Summarization", "link_suffix": "/forum?id=zOMa82W1HV", "link": "https://openreview.net/forum?id=zOMa82W1HV", "pdf_link": "https://openreview.net/pdf?id=zOMa82W1HV", "keywords": "Summarisation, Speech, Mamba", "abstract": "Abstractive Speech Summarization (SSum) becomes increasingly difficult as the input speech length grows. To address this, we present SQuBa (Speech Querying Mamba Architecture), an end-to-end model designed explicitly for efficient speech summarization. SQuBa leverages a querying-attention Mamba projector to condense extended acoustic features into compact semantic tokens, which are subsequently summarized by the Mamba Large Language Model (LLM). The architecture\u2019s computational complexity scales linearly with input length, enabling efficient handling of longer inputs. A two-stage training framework, complemented by bootstrapped Direct Preference Optimization (DPO) fine-tuning, empowers SQuBa to generate concise and coherent summaries. Experimental results demonstrate that SQuBa delivers competitive performance while significantly improving inference speed, making it ideal for real-world applications such as podcast and meeting transcriptions.", "title_embedding_index": 6806, "title_abs_embedding_index": 6831}, {"title": "Dual-Model Defense: Safeguarding Diffusion Models from Membership Inference Attacks through Disjoint Data Splitting", "link_suffix": "/forum?id=PjIe6IesEm", "link": "https://openreview.net/forum?id=PjIe6IesEm", "pdf_link": "https://openreview.net/pdf?id=PjIe6IesEm", "keywords": "Membership Inference Attack, Diffusion Models, Knowledge Distillation", "abstract": "Diffusion models have demonstrated remarkable capabilities in image synthesis, but their recently proven vulnerability to Membership Inference Attacks (MIAs) poses a critical privacy concern. This paper introduces two novel and efficient approaches (DualMD and DistillMD) to protect diffusion models against MIAs while maintaining high utility. Both methods are based on training two separate diffusion models on disjoint subsets of the original dataset. DualMD then employs a private inference pipeline that utilizes both models. This strategy significantly reduces the risk of black-box MIAs by limiting the information any single model contains about individual training samples. The dual models can also generate \"soft targets\" to train a private student model in DistillMD, enhancing privacy guarantees against all types of MIAs. Extensive evaluations of DualMD and DistillMD against state-of-the-art MIAs across various datasets in white-box and black-box settings demonstrate their effectiveness in substantially reducing MIA success rates while preserving competitive image generation performance. Notably, our experiments reveal that DistillMD not only defends against MIAs but also mitigates model memorization, indicating that both vulnerabilities stem from overfitting and can be addressed simultaneously with our unified approach.", "title_embedding_index": 6807, "title_abs_embedding_index": 6832}, {"title": "PoisonBench: Assessing Large Language Model Vulnerability to Data Poisoning", "link_suffix": "/forum?id=IgrLJslvxa", "link": "https://openreview.net/forum?id=IgrLJslvxa", "pdf_link": "https://openreview.net/pdf?id=IgrLJslvxa", "keywords": "alignment, data poisoning, backdoor attack, AI safety", "abstract": "Preference learning is a central component for aligning current LLMs, but this process can be vulnerable to data poisoning attacks. To address this concern, we introduce PoisonBench, a benchmark for evaluating Large Language Models' susceptibility to data poisoning during preference learning. Data poisoning attacks can manipulate Large Language Model responses to include hidden malicious content or biases, potentially causing the model to generate harmful or unintended outputs while appearing to function normally. We deploy two distinct attack types across eight realistic scenarios, assessing 19 widely used models. Our findings reveal concerning trends: (1) Scaling up parameter size does not inherently enhance resilience against poisoning attacks; (2) There exists a log-linear relationship between the effects of the attack and he data poison ratio; (3) The effect of data poisoning can generalize to extrapolated triggers that are not included in the poisoned data. These results expose weaknesses in current preference learning techniques, highlighting the urgent need for more robust defenses against malicious model and data manipulation.", "title_embedding_index": 6808, "title_abs_embedding_index": 6833}, {"title": "Covariate-moderated Empirical Bayes Matrix Factorization", "link_suffix": "/forum?id=lrmWjWqUJn", "link": "https://openreview.net/forum?id=lrmWjWqUJn", "pdf_link": "https://openreview.net/pdf?id=lrmWjWqUJn", "keywords": "Matrix factorization, Empirical Bayes, spatial transcriptomics, statistical genetics, non-negative factorization", "abstract": "Matrix factorization is a fundamental method in statistics and machine learning for inferring and summarizing structure in multivariate data. Modern data sets often come with ``side information'' of various forms (images, text, graphs) that can be leveraged to improve estimation of the underlying structure. However, existing methods that leverage side information are limited in the types of data they can incorporate and rely on specific parametric models. Here, we introduce a novel method for this problem, covariate-moderated empirical Bayes matrix factorization (cEBMF). cEBMF is a modular framework that accepts any type of side information that is processable by a neural network. The cEBMF framework can also accommodate different constraints and assumptions about the factors through the use of different priors and takes an empirical Bayes approach to adapt the priors to the data. We demonstrate the benefits of cEBMF in simulations and in an analysis of spatial transcriptomics data.", "title_embedding_index": 6809, "title_abs_embedding_index": 6834}, {"title": "Weak-to-Strong Trustworthiness: Eliciting Trustworthiness with Weak Supervision", "link_suffix": "/forum?id=OwntMF6McA", "link": "https://openreview.net/forum?id=OwntMF6McA", "pdf_link": "https://openreview.net/pdf?id=OwntMF6McA", "keywords": "Weak-to-strong learning, Fairness, Adversarial Robustness, OOD Robustness, Privacy", "abstract": "The rapid proliferation of generative AI, especially large language models (LLMs), has led to their integration into a variety of applications. An emergent behavior known as weak-to-strong generalization\u2014where a strong model trained on a weak model's outputs surpasses the weak model in task performance\u2014has gained significant attention. However, whether trustworthiness properties such as robustness, fairness and privacy can be transferred in a similar manner remains an open question. In this work, we investigate this critical question by exploring the transfer of trustworthiness properties from weak to strong models via weak-to-strong generalization. Specifically, we examine whether a strong model can inherit or even enhance trustworthiness attributes when fine-tuned on a weak model's outputs. We refer to this as weak-to-strong trustworthiness. To this end, we introduce two novel approaches aimed at improving trustworthiness transfer between weak and strong models: 1) Weak Trustworthiness Finetuning (Weak TFT), which applies trustworthiness regularization during the fine-tuning of the weak model, and 2) Weak and Weak-to-Strong Trustworthiness Finetuning (Weak+WTS TFT), which extends trustworthiness regularization to both the weak model and the strong model during fine-tuning. Our experimental evaluation on real-world datasets (Adult, OOD Style Transfer, AdvGLUE++, and Enron Emails) reveals that while some trustworthiness properties, such as fairness, adversarial, and OOD robustness, show significant improvement in transfer when both models were regularized, others like privacy do not exhibit signs of weak-to-strong trustworthiness. As the first study to explore the transfer of trustworthiness properties via weak-to-strong generalization, our work provides valuable insights into the potential and limitations of this method. Our findings highlight the importance of systematically studying trustworthiness transfer to develop AI systems that are not only accurate but also ethically aligned and reliable in critical applications.", "title_embedding_index": 6810, "title_abs_embedding_index": 6835}, {"title": "Retrieval-augmented Encoders for Extreme Multi-label Text Classification", "link_suffix": "/forum?id=rO5BVBwgiv", "link": "https://openreview.net/forum?id=rO5BVBwgiv", "pdf_link": "https://openreview.net/pdf?id=rO5BVBwgiv", "keywords": "Extreme Multi-label Text Classification, Dual-encoders, Retrieval-Augmented", "abstract": "Extreme multi-label classification (XMC) seeks to find relevant labels from an extremely large label collection for a given text input. To tackle such a vast label space, current state-of-the-art methods fall into two categories. The one-versus-all (OVA) method uses learnable label embeddings for each label, excelling at memorization (i.e., capturing detailed training signals for accurate head label prediction). In contrast, the dual-encoder (DE) model maps input and label text into a shared embedding space for better generalization (i.e., the capability of predicting tail labels with limited training data), but may fall short at memorization. To achieve generalization and memorization, existing XMC methods often combine DE and OVA models, which involves complex training pipelines. Inspired by the success of retrieval-augmented language models, we propose the Retrieval-augmented Encoders for XMC (RAE-XMC), a novel framework that equips a DE model with retrieval-augmented capability for efficient memorization without additional trainable parameter. During training, RAE-XMC is optimized by the contrastive loss over a knowledge memory that consists of both input instances and labels. During inference, given a test input, RAE-XMC retrieves the top-$K$ keys from the knowledge memory, and aggregates the corresponding values as the prediction scores. We showcase the effectiveness and efficiency of RAE-XMC on four public LF-XMC benchmarks. RAE-XMC not only advances the state-of-the-art (SOTA) method DEXML, but also achieves more than 10x speedup on the largest  LF-mazonTitles-1.3M dataset under the same 8 A100 GPUs training environments. Our experiment code is available in the Supplementary Material.", "title_embedding_index": 6811, "title_abs_embedding_index": 6836}, {"title": "VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation", "link_suffix": "/forum?id=K4FAFNRpko", "link": "https://openreview.net/forum?id=K4FAFNRpko", "pdf_link": "https://openreview.net/pdf?id=K4FAFNRpko", "keywords": "Vision-Language-Action Model, Speech Instructions, Robot Manipulation", "abstract": "Vision-language-action models (VLAs) have recently become highly prevalent in robot manipulation due to its end-to-end architecture and impressive performance. However, current VLAs are limited to processing human instructions in textual form, neglecting the more natural speech modality for human interaction. A typical approach of incorporating speech modality into VLA necessitates a separate speech recognition system to transcribe spoken instructions into text. Such a cascading pipeline raises two major concerns for robotic systems. First, the entire model grows in size and complexity, potentially resulting in redundant computations and increased memory consumption. Second, the transcription procedure would lose non-semantic information in the raw speech, such as voiceprint, which is crucial for a robot to successfully understand and complete customized tasks. To this end, we propose VLAS, the fisrt end-to-end policy model that seamlessly integrates speech modality for robot manipulation. We present a three-stage speech instruction tuning strategy leveraging multimodal datasets, including our manually curated SQA and CSI datasets. Furthermore, to facilitate personalized operations, we develop a voice retrieval-augmented generation (RAG) approach to enhance the robot's performance in tasks requiring individual-specific knowledge. Experimental results show that the proposed VLAS, following either textual or speech instructions, can achieve performance comparable to traditional VLAs on the CALVIN benchmark. In addition, we created a benchmark consisting of customization tasks, where our VLAS demonstrates absolute superiority by fully leveraging the auxiliary information in speech.", "title_embedding_index": 6812, "title_abs_embedding_index": 6837}, {"title": "Auto-Demo Prompting: Leveraging Generated Outputs as Demonstrations for Enhanced Batch Prompting", "link_suffix": "/forum?id=WDxa9hnz4p", "link": "https://openreview.net/forum?id=WDxa9hnz4p", "pdf_link": "https://openreview.net/pdf?id=WDxa9hnz4p", "keywords": "Batch Prompting, In-Context Learning, Large Language Models", "abstract": "Batch prompting is a common technique in large language models (LLMs) used to process multiple inputs simultaneously, aiming to improve computational efficiency. However, as batch sizes increase, performance degradation often occurs due to the model's difficulty in handling lengthy context inputs. Existing methods that attempt to mitigate these issues rely solely on batch data arrangement and majority voting rather than improving the design of the batch prompt itself. In this paper, we address these limitations by proposing \"Auto-Demo Prompting,\" a novel approach that leverages the question-output pairs from earlier questions within a batch as demonstrations for subsequent answer inference. We provide a formal theoretical analysis of how Auto-Demo Prompting functions within the autoregressive generation process of LLMs, illustrating how it utilizes prior outputs to optimize the model's internal representations. Our method effectively bridges the gap between batch prompting and few-shot prompting, enhancing performance with only a slight compromise in token usage. Experimental results across five NLP tasks demonstrate its effectiveness in mitigating performance degradation and occasionally outperforming single prompts. Furthermore, it opens new avenues for applying few-shot learning techniques, such as demonstration selection, within batch prompting, making it a robust solution for real-world applications.", "title_embedding_index": 6813, "title_abs_embedding_index": 6838}, {"title": "Teaching Human Behavior Improves Content Understanding Abilities Of LLMs", "link_suffix": "/forum?id=ff2V3UR9sC", "link": "https://openreview.net/forum?id=ff2V3UR9sC", "pdf_link": "https://openreview.net/pdf?id=ff2V3UR9sC", "keywords": "LLM, human behavior, content understanding", "abstract": "Communication is defined as ``Who says what to whom with what effect.'' A message from a communicator generates downstream receiver effects, also known as behavior. Receiver behavior, being a downstream effect of the message, carries rich signals about it. Even after carrying signals about the message, the behavior data is often ignored while training large language models. We show that training LLMs on receiver behavior can actually help improve their content-understanding abilities. Specifically, we show that training LLMs to predict the receiver behavior of likes and comments improves the LLM's performance on a wide variety of downstream content understanding tasks. We show this performance increase over 46 video and image understanding tasks over 26 benchmark datasets across both 0-shot and fine-tuning settings, outperforming many supervised baselines. Moreover, since receiver behavior, such as likes and comments, is collected by default on the internet and does not need any human annotations to be useful, the performance improvement we get after training on this data is essentially free-lunch. We release the receiver behavior cleaned comments and likes of 750k images and videos collected from multiple platforms along with our instruction-tuning data.", "title_embedding_index": 6814, "title_abs_embedding_index": 6839}, {"title": "Practical Epistemic Uncertainty Quantification for View Synthesis", "link_suffix": "/forum?id=YWtT5OGOAI", "link": "https://openreview.net/forum?id=YWtT5OGOAI", "pdf_link": "https://openreview.net/pdf?id=YWtT5OGOAI", "keywords": "Deep Learning, Epistemic Uncertainty Estimation, View Synthesis", "abstract": "View synthesis using Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) has demonstrated impressive fidelity in rendering real-world scenarios. However, practical methods for accurate and efficient epistemic Uncertainty Quantification (UQ) in view synthesis are lacking. Existing approaches for NeRF either introduce significant computational overhead (e.g., \"10x increase in training time\" or \"10x repeated training\") or are limited to specific uncertainty conditions or models. Notably, GS models lack any systematic approach for comprehensive epistemic UQ. This capability is crucial for improving the robustness and scalability of neural view synthesis, enabling active model updates, error estimation, and scalable ensemble modeling based on uncertainty. In this paper, we revisit NeRF and GS-based methods from a function approximation perspective, identifying key differences and connections in 3D representation learning. Building on these insights, we introduce PH-Dropout, the first real-time and accurate method for epistemic uncertainty estimation that operates directly on pre-trained NeRF and GS models. Extensive evaluations validate our theoretical findings and demonstrate the effectiveness of PH-Dropout.", "title_embedding_index": 6815, "title_abs_embedding_index": 6840}, {"title": "HeurAgenix: A Multi-Agent LLM-Based Paradigm for Adaptive Heuristic Evolution and Selection in Combinatorial Optimization", "link_suffix": "/forum?id=xxSK3ZNAhh", "link": "https://openreview.net/forum?id=xxSK3ZNAhh", "pdf_link": "https://openreview.net/pdf?id=xxSK3ZNAhh", "keywords": "Combinatorial Optimization; Heuristic Evolution; Heuristic Selection; Large Language Models", "abstract": "Combinatorial Optimization (CO) is a class of problems where the goal is to identify an optimal solution from a finite set of feasible solutions under specific constraints. Despite its ubiquity across industries, existing heuristic algorithms struggle with limited adaptability, complex parameter tuning, and limited generalization to novel problems. Recent approaches leveraging machine learning have made incremental improvements but remain constrained by extensive data requirements and reliance on historical problem-specific adjustments. Large Language Models (LLMs) offer a new paradigm to overcome these limitations due to their ability to generalize across domains, autonomously generate novel insights, and adapt dynamically to different problem contexts. To harness these capabilities, we introduce $\\textbf{HeurAgenix}$, a novel multi-agent hyper-heuristic framework that leverages LLMs to generate, evolve, evaluate, and select heuristics for solving CO problems. Our framework comprises four key agents: heuristic generation, heuristic evolution, benchmark evaluation, and heuristic selection. Each agent is designed to exploit specific strengths of LLMs, such as their capacity for synthesizing knowledge from diverse sources, autonomous decision-making, and adaptability to new problem instances. Experiments on both classic and novel CO tasks show that HeurAgenix significantly outperforms state-of-the-art approaches by enabling scalable, adaptable, and data-efficient solutions to complex optimization challenges.", "title_embedding_index": 6816, "title_abs_embedding_index": 6841}, {"title": "I Want to Break Free! Persuasion and Anti-Social Behavior of LLMs in Multi-Agent Settings with Social Hierarchy", "link_suffix": "/forum?id=acDwoHrwZ8", "link": "https://openreview.net/forum?id=acDwoHrwZ8", "pdf_link": "https://openreview.net/pdf?id=acDwoHrwZ8", "keywords": "large language models, persuasion, anti-social behavior, sociology of machines, social hierarchy, computational social science, experiments", "abstract": "As Large Language Model (LLM)-based agents become increasingly autonomous and will more freely interact with each other, studying interactions between them becomes crucial to anticipate emergent phenomena and potential risks. Drawing inspiration from the widely popular Stanford Prison Experiment, we contribute to this line of research by studying interaction patterns of LLM agents in a context characterized by strict social hierarchy. We do so by specifically studying two types of phenomena: persuasion and anti-social behavior in simulated scenarios involving a guard and a prisoner agent who seeks to achieve a specific goal (i.e., obtaining additional yard time or escape from prison). Leveraging 200 experimental scenarios for a total of 2,000 machine-machine conversations across five different popular LLMs, we provide a set of noteworthy findings. We first document how some models consistently fail in carrying out a conversation in our multi-agent setup where power dynamics are at play.\nThen, for the models that were able to engage in successful interactions, we empirically show how  the goal that an agent is set to achieve impacts primarily its persuasiveness, while having a negligible effect with respect to the agent's anti-social behavior. Third, we highlight how agents' personas, and particularly the guard's personality, drive both the likelihood of successful persuasion from the prisoner and the emergence of anti-social behaviors. Fourth, we show that even without explicitly prompting for specific personalities, anti-social behavior emerges by simply assigning agents' roles. These results bear implications for the development of interactive LLM agents as well as the debate on their societal impact.", "title_embedding_index": 6817, "title_abs_embedding_index": 6842}, {"title": "Mostly Exploration-free Algorithms for Multi-Objective Linear Bandits", "link_suffix": "/forum?id=Cy7G36aHta", "link": "https://openreview.net/forum?id=Cy7G36aHta", "pdf_link": "https://openreview.net/pdf?id=Cy7G36aHta", "keywords": "multi-objective, free exploration, linear bandit", "abstract": "We address the challenge of solving multi-objective bandit problems, which are increasingly relevant in real-world applications where multiple possibly conflicting objectives must be optimized simultaneously. Existing multi-objective algorithms often rely on complex, computationally intensive methods, making them impractical for real-world use. In this paper, we propose a novel perspective by showing that objective diversity can naturally induce free exploration, allowing for simpler, near-greedy algorithms to achieve state-of-the-art regret bounds. \nWe introduce simple and efficient algorithms for multi-objective linear bandits, which do not require constructing empirical Pareto fronts and achieve a regret bound of $\\tilde{\\mathcal{O}}(\\sqrt{dT})$ under sufficient objective diversity and suitable regularity. We also introduce the concept of objective fairness, ensuring equal treatment of all objectives, and show that our algorithms satisfy this criterion. Numerical experiments validate our theoretical findings, demonstrating that objective diversity can enhance algorithm performance while simplifying the solution process.", "title_embedding_index": 6818, "title_abs_embedding_index": 6843}, {"title": "Dynamic Memory Based Adaptive Optimization", "link_suffix": "/forum?id=PevF76oAEh", "link": "https://openreview.net/forum?id=PevF76oAEh", "pdf_link": "https://openreview.net/pdf?id=PevF76oAEh", "keywords": "optimization, meta-training, adaptive-learning, RLLC, retrospective-learning-law-correction", "abstract": "Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions:How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps?As an approach to the last question, we introduce a general method called \"Retrospective Learning Law Correction\" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (calledlearning law) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promising framework for boosting the performance of known optimizers by adding more memory units and by making them more adaptive.", "title_embedding_index": 6819, "title_abs_embedding_index": 6844}, {"title": "CIRQRS: Evaluating Query Relevance Score in Composed Image Retrieval", "link_suffix": "/forum?id=U42IGDU3q5", "link": "https://openreview.net/forum?id=U42IGDU3q5", "pdf_link": "https://openreview.net/pdf?id=U42IGDU3q5", "keywords": "composed image retrieval, evaluation metric, self-paced learning", "abstract": "Composed Image Retrieval (CIR) retrieves relevant images using a reference image and accompanying text that describes how the desired images differ from the reference. However, the commonly used evaluation metric Recall@k only checks if the target image is retrieved, without considering the relevance of other images to the query, potentially leading to user dissatisfaction. We introduce Composed Image Retrieval Query Relevance Score (CIRQRS), an evaluation metric that scores each retrieved image based on its relevance to the query, offering a comprehensive evaluation. CIRQRS is trained using a reward model objective to prefer highly relevant, positive images over less relevant, negative ones. We propose a strategy motivated by self-paced learning to dynamically adjust the negative set based on the relevance of each image by using CIRQRS's current training status. To validate CIRQRS's ability to measure relevance, we created the human-scored FashionIQ (HS-FashionIQ) dataset and compared it with scores from human evaluators. CIRQRS correlates with human scores 2.625 times better than Recall@k, highlighting its superior ability to capture relevance. Additionally, by ranking images based on their CIRQRS, we check if the target image appears in the top k. The results show that CIRQRS achieves state-of-the-art performance on two representative CIR datasets, CIRR and FashionIQ.", "title_embedding_index": 6820, "title_abs_embedding_index": 6845}, {"title": "BounDr.E: Predicting Drug-likeness through knowledge alignment and EM-like one-class boundary optimization", "link_suffix": "/forum?id=TnlLMYPfx5", "link": "https://openreview.net/forum?id=TnlLMYPfx5", "pdf_link": "https://openreview.net/pdf?id=TnlLMYPfx5", "keywords": "Drug-likeness, one-class boundary, multi-modal alignment, drug discovery", "abstract": "The advent of generative AI models is revolutionizing drug discovery, generating de novo molecules at unprecedented speed. However, accurately identifying drug candidates among generated molecules remains an open problem. The essence of this drug-likeness prediction task lies in constructing a compact subspace that encompasses majority of approved drugs with only a small number of unknown compounds (drug candidates) inside. Computational challenges arises in constructing a decision boundary on an unbound chemical space that lacks definite negatives, i.e, non drug-likeness. Approved drugs exist highly dispersed across structural space, making it more harsh to effectively separate drugs from non-drugs through existing classifiers.  Addressing such challenges, we introduce BounDr.E: a novel approach for learning a compact boundary of drug-likeness through an Expectation-Maximization (EM)-like iterative optimization process. \nSpecifically, we refine both the boundary and the distribution of the embedding space via metric learning, allowing the model to iteratively tighten the drug-like boundary while pushing non-drug-like compounds outside. Augmented by integration of biomedical context within knowledge graphs via multi-modal alignment, our model demonstrates up to five orders of magnitude improvement in drug-likeness prediction performance metrics. Our model further showcases its utility in large-scale screening of AI-generated compounds through zero-shot toxic compound filtering and comprehensive case studies.  To facilitate in silico drug discovery, we provide the code and benchmark data under various splitting schemes at:https://anonymous.4open.science/r/boundr_e.", "title_embedding_index": 6821, "title_abs_embedding_index": 6846}, {"title": "Benchmarking Predictive Coding Networks -- Made Simple", "link_suffix": "/forum?id=sahQq2sH5x", "link": "https://openreview.net/forum?id=sahQq2sH5x", "pdf_link": "https://openreview.net/pdf?id=sahQq2sH5x", "keywords": "cognitive science, predictive coding, computational neuroscience", "abstract": "In this work, we tackle the problems of efficiency and scalability for predictive coding networks (PCNs) in machine learning. To do so, we  propose a library that focuses on performance and simplicity, and use it to implement a large set of standard benchmarks for the community to use for their experiments. As most works in the field propose their own tasks and architectures, do not compare one against each other, and focus on small-scale tasks, a simple and fast open-source library, and a comprehensive set of benchmarks, would address all of these concerns. Then, we perform extensive tests on such benchmarks using both existing algorithms for PCNs, as well as adaptations of other methods popular in the bio-plausible deep learning community. All of this has allowed us to (i) test architectures much larger than commonly used in the literature, on more complex datasets; (ii) reach new state-of-the-art results in all of the tasks and dataset provided; (iii) clearly highlight what the current limitations of PCNs are, allowing us to state important future research directions. With the hope of galvanizing community efforts towards one of the main open problems in the field, scalability, we will release the code, tests, and benchmarks.", "title_embedding_index": 6822, "title_abs_embedding_index": 6847}, {"title": "Need a Small Specialized Language Model? Plan Early!", "link_suffix": "/forum?id=aP3OBwf8dk", "link": "https://openreview.net/forum?id=aP3OBwf8dk", "pdf_link": "https://openreview.net/pdf?id=aP3OBwf8dk", "keywords": "Large language models, pre-training, efficient inference, task-adaptive pretraining", "abstract": "Large language models are versatile tools but are not suitable for small inference budgets. Small models have more efficient inference, but their lower capacity means that their performance can be good only if one limits their scope to a specialized domain.  This paper explores how to get good specialized small language models using a large, generic, pretraining set and a limited amount of specialized data. We consider two scenarios, depending on whether (i) one can afford pretraining a model for each specialization task, or (ii) one wants to cheaply adapt a single pretrained model for each task. In the first scenario, we propose an effective solution based on importance sampling: we resample the pretraining set to imitate the specialization data and train a small model on it. In the second scenario, we propose a novel architecture, projected networks (PN). PN is a large network whose parameters can be linearly projected into a small network for specialization. For both scenarios, we demonstrate the empirical effectiveness of our solutions across various domains, training set sizes, and training budgets.", "title_embedding_index": 6823, "title_abs_embedding_index": 6848}, {"title": "Multilingual Arbitrage: Optimizing Data Pools to Accelerate Multilingual Progress", "link_suffix": "/forum?id=alaQod29Cb", "link": "https://openreview.net/forum?id=alaQod29Cb", "pdf_link": "https://openreview.net/pdf?id=alaQod29Cb", "keywords": "Synthetic data, Model Distillation, Multilingual language models, Routing, Instruction Fine-Tuning", "abstract": "The use of synthetic data has been crucial in achieving recent state-of-the-art breakthroughs. However, relying solely on a single oracle teacher model for data generation can lead to issues such as model collapse and bias propagation. These problems are particularly pronounced in multilingual contexts, where no single teacher model performs optimally across all languages. In this study, we propose a solution through multilingual arbitrage, which exploits performance variations among multiple models for each language. By strategically routing samples through a diverse set of models, each possessing unique strengths in different languages, we address these challenges. Our extensive experiments with state-of-the-art models demonstrate that our arbitrage techniques significantly enhance performance compared to relying on a single teacher model. Our multilingual arbitrage techniques result in large gains of up to 80% win-rates over state-of-art proprietary and widely adopted open weight models such as Gemma 2, Llama 3.1, Mistral v0.3. These gains, achieved through multilingual arbitrage and averaged across all languages, were most substantial in the less-resourced languages within our pool.", "title_embedding_index": 6824, "title_abs_embedding_index": 6849}]