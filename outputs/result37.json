[{"title": "Debiasing Online Preference Learning via Preference Feature Preservation", "link_suffix": "/forum?id=OV0rZx8jr1", "link": "https://openreview.net/forum?id=OV0rZx8jr1", "pdf_link": "https://openreview.net/pdf?id=OV0rZx8jr1", "keywords": "large language model, alignment, bias, preference", "abstract": "While various preferred features determine human preferences, current preference learning frameworks for large language models (LLMs) simplify them with binary pairwise comparisons and scalar rewards. This simplification could make LLMs' responses biased to mostly preferred features such as longer responses which would be exacerbated in online learning scenarios as the biases can be accumulate continuously throughout the iterations.\nTo address these challenges, we propose a novel framework called PFP (Preference Feature Preservation).  The key idea of PFP is maintaining the distribution of human preference features throughout the online preference learning process. Specifically, PFP first trains a feature classifier using the existing offline pairwise human preference data. \nThen, using this classifier and the distribution preserving optimization, PFP maps appropriate preference features for each input instruction during online learning. \nLastly, PFP trains LLM using the existing preference learning framework, by incorporating the preference feature of each data into system prompts and enabling LLM to explicitly handle various human preferences. Our experiments demonstrate that PFP successfully mitigates the bias in preference features that arise during online learning, and achieves superior performance compared to previous preference learning methods on general benchmarks including AlpacaEval 2.0 and MT-Bench. We also observe that PFP almost resolves a length bias issue, a long-standing problem of online preference learning, even though it was not specifically designed to tackle this.", "title_embedding_index": 1800, "title_abs_embedding_index": 1825}, {"title": "A Language Model based Model Manager", "link_suffix": "/forum?id=xom3YUQfbK", "link": "https://openreview.net/forum?id=xom3YUQfbK", "pdf_link": "https://openreview.net/pdf?id=xom3YUQfbK", "keywords": "Large Language Models, Model Manager, Verbalization, Differentiation", "abstract": "In the current landscape of machine learning, we face a \u201cmodel lake\u201d phenomenon: a proliferation of deployed models often lacking adequate documentation. This presents significant challenges for model users attempting to navigate, differentiate, and select appropriate models for their needs. To address the issue of differentiation, we introduce Model Manager, a framework designed to facilitate easy comparison among existing models. Our approach leverages a large language model (LLM) to generate verbalizations of two models' differences by sampling from two models. We use a novel protocol that makes it possible to quantify the informativeness of the verbalizations. We also assemble a suite with a diverse set of commonly used models: Logistic Regression, Decision Trees, and K-Nearest Neighbors. We additionally performed ablation studies on crucial design decisions of the Model Managers. Our analysis yields pronounced results. For a pair of logistic regression models with a 20-25% performance difference on the blood dataset, the Model Manager effectively verbalizes their variations with up to 80% accuracy. The Model Manager framework opens up new research avenues for improving the transparency and comparability of machine learning models in a post-hoc manner.", "title_embedding_index": 1801, "title_abs_embedding_index": 1826}, {"title": "Stabilize continual learning with hyperspherical replay", "link_suffix": "/forum?id=A1JdcLawSu", "link": "https://openreview.net/forum?id=A1JdcLawSu", "pdf_link": "https://openreview.net/pdf?id=A1JdcLawSu", "keywords": "Continual learning", "abstract": "Neural networks face catastrophic forgetting of previously learned knowledge\nwhen training on new task data. While the field of continual learning has made\npromising progress in reducing this forgetting, recent work has uncovered an\ninteresting phenomenon: existing techniques often exhibit a sharp performance\ndrop on prior tasks during the initial stages of new task training, a phenomenon\nknown as the \u201dstability gap.\u201d This phenomenon not only raises safety concerns\nbut also challenges the current understanding of neural network behavior in continual learning scenarios. Inspired by this discovery, we revisit two fundamental\nquestions in continual learning: 1) Is the past learned knowledge within deep\nnetworks lost abruptly or gradually? and 2) Is past learned knowledge ever completely erased? Our analysis reveals that abrupt forgetting occurs not only in the\nfinal fully connected layer but also permeates the feature space and most layers,\nsparing only the earliest layers. Alarmingly, a single gradient update can severely\ndisrupt the learned class structure. We identify degenerate solutions in the softmax\ncross-entropy loss as a major contributing factor, with memory samples exhibiting\nhigher feature norms compared to new samples. To address these issues, we pro-\npose Adaptive Angular Replay (AAR), a simple yet effective approach that learns\nfeatures in hyperspherical space using feature and weight normalization. Angular\nER demonstrates a strong ability to preserve class structure during task transitions. Additionally, we introduce an adaptive scaling strategy to further mitigate\nthe stability gap and improve overall accuracy.", "title_embedding_index": 1802, "title_abs_embedding_index": 1827}, {"title": "An Efficient LLM Alignment Framework for Automated Radiology Impression Generation", "link_suffix": "/forum?id=UnodjDRqLp", "link": "https://openreview.net/forum?id=UnodjDRqLp", "pdf_link": "https://openreview.net/pdf?id=UnodjDRqLp", "keywords": "Large language mode, Radiology Impression Generation, Alignment framework, Reinforcement learning", "abstract": "Large language models (LLMs) are typically specialized for domain tasks through supervised fine-tuning, which optimizes LLMs for likelihood-based objectives. While supervised fine-tuning enables LLMs to generate text that conforms to the language style of a specific domain, such as radiology, it often falls short in enhancing the model's ability to perform detailed diagnostic reasoning or tailor reports for individual patients. In this paper, we explore the use of reinforcement learning to better align LLMs with the intricate requirements of radiological practice. By framing the report generation process as sequential decision-making stages, we present Radiology-Guided Reinforcement Optimization (RGRO), a tailored policy optimization framework designed specifically for medical language tasks. RGRO moves beyond conventional likelihood-based training by directly optimizing for radiology-specific objectives, including consistency with radiology findings and adherence to established professional guidelines. Our empirical evaluations demonstrate that RGRO significantly enhances the diagnostic precision and clinical utility of radiology reports generated by LLMs, outperforming supervised fine-tuning methods and state-of-the-art models. Furthermore, RGRO enables the seamless integration of expert radiologist feedback and external diagnostic tools, all without the need for large-scale annotated datasets.", "title_embedding_index": 1803, "title_abs_embedding_index": 1828}, {"title": "A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison", "link_suffix": "/forum?id=kz78RIVL7G", "link": "https://openreview.net/forum?id=kz78RIVL7G", "pdf_link": "https://openreview.net/pdf?id=kz78RIVL7G", "keywords": "Machine Learning, Adversarial Attacks, Security", "abstract": "Adversarial attacks present a significant threat to modern machine learning systems. Yet, existing detection methods often lack the ability to detect unseen attacks or detect different attack types with a high level of accuracy. In this work, we propose a statistical approach that establishes a detection baseline before a neural network\u2019s deployment, enabling effective real-time adversarial detection. We generate a metric of adversarial presence by comparing the behavior of a compressed/uncompressed neural network pair. Our method has been tested against state-of-the-art techniques, and it achieves near-perfect detection across a wide range of attack types. Moreover, it significantly reduces false positives, making it both reliable and practical for real-world applications.", "title_embedding_index": 1804, "title_abs_embedding_index": 1829}, {"title": "Training Universal Text Encoders with Pair Relevance Classification Loss", "link_suffix": "/forum?id=hbS1t37PGM", "link": "https://openreview.net/forum?id=hbS1t37PGM", "pdf_link": "https://openreview.net/pdf?id=hbS1t37PGM", "keywords": "sentence embeddings, text encoders, contrastive learning", "abstract": "Finetuning large language models (LLMs) using contrastive learning objectives has become the dominant approach for representation learning in general-purpose text embedding tasks. Our work seeks to enable going beyond strictly positive (or negative) pairs of text, to more fine-grained annotations that can capture the nuances of complex language tasks. We propose training text encoders with a simple pair classification loss that utilizes binary cross-entropy on relevance labels. When compared to the standard softmax-based loss for multi-class classification against multiple text alternatives, we find that training with our proposed loss improves the average score across 56 English language tasks of the Massive Text Embedding Benchmark (MTEB), while finetuning the same Meta-Llama-3-8B-Instruct model on the same mix of open datasets. Furthermore, our models excel in the Pair Classification and the Semantic Textual Similarity benchmarks, outperforming many models that are trained on more extensive data. Finally, thorough experiments using graded relevance data from TREC-DL 2023 during training demonstrate that binary cross-entropy provides generalization improvements that the softmax-based loss fails to achieve.", "title_embedding_index": 1805, "title_abs_embedding_index": 1830}, {"title": "A Watermark for Black-Box Language Models", "link_suffix": "/forum?id=0koPj0cJV6", "link": "https://openreview.net/forum?id=0koPj0cJV6", "pdf_link": "https://openreview.net/pdf?id=0koPj0cJV6", "keywords": "watermarking, large language models, black-box", "abstract": "Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require \\emph{white-box} access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. \\emph{black-box} access), boasts a \\emph{distortion-free} property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.", "title_embedding_index": 1806, "title_abs_embedding_index": 1831}, {"title": "Learning Time-Dependent Density Functional Theory via Geometry and Physics Aware Latent Evolution", "link_suffix": "/forum?id=Wo66GEFnXd", "link": "https://openreview.net/forum?id=Wo66GEFnXd", "pdf_link": "https://openreview.net/pdf?id=Wo66GEFnXd", "keywords": "AI for science, Density functional theory, Real-time TDDFT, Neural PDE solver", "abstract": "We consider using machine learning to simulate time-dependent density functional theory (TDDFT) to predict physical properties of molecules and materials beyond their ground states. In particular, by simulating the electronic response of the system under an external electromagnetic field, the optical absorption spectrum can be calculated using real-time TDDFT (RT-TDDFT), which provides physical information about the excited states and dipole strength function. However, RT-TDDFT simulation requires the direct propagation of electronic wavefunctions of all valence electrons for extended periods, making the process very time-consuming. In this work, we model electron density as volumetric data and train neural networks to map between coarse time steps. To make the model aware of the atomistic environment, we incorporate 3D message passing into the model architecture. Additionally, we use latent evolution to regularize the model towards learning the underlying physics. Our method is termed TDDFTNet. To evaluate our approach, we generate datasets using molecules from the MD17 dataset. Results show that TDDFTNet can learn the time propagation of electron densities accurately and efficiently.", "title_embedding_index": 1807, "title_abs_embedding_index": 1832}, {"title": "Sufficient and Necessary Explanations (and What Lies in Between)", "link_suffix": "/forum?id=8kk9joQCkc", "link": "https://openreview.net/forum?id=8kk9joQCkc", "pdf_link": "https://openreview.net/pdf?id=8kk9joQCkc", "keywords": "Explainability, Interpretability, Trustworthiness", "abstract": "As complex machine learning models continue to find applications in high-stakes decision making scenarios, it is crucial that we can explain and understand their predictions. Post-hoc explanation methods can provide useful insights by identifying important features in an input ${\\bf x}$ with respect to the model output $f({\\bf x})$. In this work we formalize and study two precise notions of feature importance for general machine learning models: \\emph{sufficiency} and \\emph{necessity}. We demonstrate how these two types of explanations, albeit intuitive and simple, can fall short in providing a complete picture of which features a model deems important for its predictions. To this end, we propose a unified notion of importance that circumvents these limitations by exploring a continuum along a necessity-sufficiency axis. Our unified notion, we show, has strong ties to other popular definitions of feature importance, like those based on conditional independence and game-theoretic quantities like Shapley values. Crucially, we demonstrate how studying this spectrum of importance allows us to detect important features that could be missed by either of the previous approaches alone.", "title_embedding_index": 1808, "title_abs_embedding_index": 1833}, {"title": "Robustness Reprogramming for Representation Learning", "link_suffix": "/forum?id=SuH5SdOXpe", "link": "https://openreview.net/forum?id=SuH5SdOXpe", "pdf_link": "https://openreview.net/pdf?id=SuH5SdOXpe", "keywords": "Adversarial Robustness, Robustness Reprogramming, Robust Representation Learning", "abstract": "This work tackles an intriguing and fundamental open challenge in representation learning: Given a well-trained deep learning model, can it be reprogrammed to enhance its robustness against adversarial or noisy input perturbations without altering its parameters?\nTo explore this, we revisit the core feature transformation mechanism in representation learning and propose a novel non-linear robust pattern matching technique as a robust alternative. Furthermore, we introduce three model reprogramming paradigms to offer flexible control of robustness under different efficiency requirements. Comprehensive experiments and ablation studies across diverse learning models ranging from basic linear model and MLPs to shallow and modern deep ConvNets demonstrate the effectiveness \nof our approaches.\nThis work not only opens a promising and orthogonal direction for improving adversarial defenses in deep learning beyond existing methods but also provides new insights into designing more resilient AI systems with robust statistics. Our implementation is available athttps://anonymous.4open.science/r/NRPM-322C/", "title_embedding_index": 1809, "title_abs_embedding_index": 1834}, {"title": "BRAID: Input-driven Nonlinear Dynamical Modeling of Neural-Behavioral Data", "link_suffix": "/forum?id=3usdM1AuI3", "link": "https://openreview.net/forum?id=3usdM1AuI3", "pdf_link": "https://openreview.net/pdf?id=3usdM1AuI3", "keywords": "Deep learning, Dynamic modeling, Sensory stimuli, RNN, Intrinsic, Behavior", "abstract": "Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.", "title_embedding_index": 1810, "title_abs_embedding_index": 1835}, {"title": "LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation", "link_suffix": "/forum?id=kam84eEmub", "link": "https://openreview.net/forum?id=kam84eEmub", "pdf_link": "https://openreview.net/pdf?id=kam84eEmub", "keywords": "directed acyclic graphs, graph generation, discrete diffusion, autoregressive model", "abstract": "Directed acyclic graphs (DAGs) serve as crucial data representations in domains such as hardware synthesis and compiler/program optimization for computing systems. DAG generative models facilitate the creation of synthetic DAGs, which can be used for benchmarking computing systems while preserving intellectual property. However, generating realistic DAGs is challenging due to their inherent directional and logical dependencies. This paper introduces LayerDAG, an autoregressive diffusion model, to address these challenges. LayerDAG decouples the strong node dependencies into manageable units that can be processed sequentially. By interpreting the partial order of nodes as a sequence of bipartite graphs, LayerDAG leverages autoregressive generation to model directional dependencies and employs diffusion models to capture logical dependencies within each bipartite graph. Comparative analyses demonstrate that LayerDAG outperforms existing DAG generative models in both expressiveness and generalization, particularly for generating large-scale DAGs with up to 400 nodes\u2014a critical scenario for system benchmarking. Extensive experiments on both synthetic and real-world flow graphs from various computing platforms show that LayerDAG generates valid DAGs with superior statistical properties and benchmarking performance. The synthetic DAGs generated by LayerDAG enhance the training of ML-based surrogate models, resulting in improved accuracy in predicting performance metrics of real-world DAGs across diverse computing platforms.", "title_embedding_index": 1811, "title_abs_embedding_index": 1836}, {"title": "Synthesizing Bonds: Enhancing Adult Attachment Predictions with LLM-Generated Data", "link_suffix": "/forum?id=8WpRt9pjeh", "link": "https://openreview.net/forum?id=8WpRt9pjeh", "pdf_link": "https://openreview.net/pdf?id=8WpRt9pjeh", "keywords": "Attachment style, Mental Health, LLM", "abstract": "Obtaining data in the medical field is challenging, making the adoption of AI technology within the space slow and high-risk. We evaluate whether we can overcome this obstacle with synthetic data generated by large language models (LLMs). In particular, we use GPT-4 and Claude 3 Opus to create agents that simulate adults with varying profiles, childhood memories, and attachment styles. These agents participate in simulated Adult Attachment Interviews (AAI), and we use their responses to train models for predicting their underlying attachment styles. We evaluate our models using a transcript dataset from 9 humans who underwent the same interview protocol, analyzed and labeled by mental health professionals. Our findings indicate that training the models using only synthetic data achieves performance comparable to training the models on human data. Additionally, while the raw embeddings from synthetic answers occupy a distinct space compared to those from real human responses, the introduction of unlabeled human data and a simple standardization allows for a closer alignment of these representations. This adjustment is supported by qualitative analyses and is reflected in the enhanced predictive accuracy of the standardized embeddings.", "title_embedding_index": 1812, "title_abs_embedding_index": 1837}, {"title": "COMMA: A Communicative Multimodal Multi-Agent Benchmark", "link_suffix": "/forum?id=a8R07y1jQ1", "link": "https://openreview.net/forum?id=a8R07y1jQ1", "pdf_link": "https://openreview.net/pdf?id=a8R07y1jQ1", "keywords": "Multimodality, Agent, LLM, Benchmark", "abstract": "The rapid advances of multi-modal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce a novel benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of scenarios, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. By testing both agent-agent and agent-human collaborations using open-source and closed-source models, our findings reveal surprising weaknesses in state-of-the-art models, including proprietary models like GPT-4o. These models struggle to outperform even a simple random agent baseline in agent-agent collaboration and only surpass the random baseline when a human is involved.", "title_embedding_index": 1813, "title_abs_embedding_index": 1838}, {"title": "Spectral Operator Methods for Learning Coherent Temporal Representations in Cellular Signaling Dynamics", "link_suffix": "/forum?id=E2c7UsrZnN", "link": "https://openreview.net/forum?id=E2c7UsrZnN", "pdf_link": "https://openreview.net/pdf?id=E2c7UsrZnN", "keywords": "Operator theory, temporal representations, delay-coordinate embeddings, Markov operator, single-cell analysis, machine learning for dynamical systems", "abstract": "We present a novel operator-based framework for learning coherent temporal representations of cellular dynamics from live-cell imaging data. Recognizing the inherent stochasticity and measurement limitations in biological systems, our approach shifts the focus from predicting exact trajectories to characterizing key dynamical properties that shape cellular behaviors at the population level. By leveraging spectral analysis of the Koopman operator and smoothing via Markov semigroups of kernel integral operators, we identify near-resonant patterns and transient coherent structures that persist across different experimental conditions. This methodology effectively captures fundamental dynamics, providing insights into mechanisms of heterogeneous cell responses without the need to model precise transformation laws. We demonstrate the efficacy of our framework on a dataset of retinal pigment epithelial cells with an inducible oncogene, revealing conserved dynamical patterns across varying levels of ERK inhibition. Our work offers interpretable learned representations, even with limited and noisy single-cell-resolved recordings, advancing machine learning for dynamical systems and opening new avenues for understanding and predicting cellular behavior in response to external stimuli.", "title_embedding_index": 1814, "title_abs_embedding_index": 1839}, {"title": "PertEval-scFM: Benchmarking Single-Cell Foundation Models for Perturbation Effect Prediction", "link_suffix": "/forum?id=MWP9V0Ej0d", "link": "https://openreview.net/forum?id=MWP9V0Ej0d", "pdf_link": "https://openreview.net/pdf?id=MWP9V0Ej0d", "keywords": "foundation models, benchmark, single-cell biology, perturbation effect prediction", "abstract": "In silicomodeling of transcriptional responses to perturbations is crucial for advancing our understanding of cellular processes and disease mechanisms. We present PertEval-scFM, a standardized framework designed to evaluate models for perturbation effect prediction. We apply PertEval-scFM to benchmark zero-shot single-cell foundation model (scFM) embeddings against simpler baseline models to assess whether these contextualized representations enhance perturbation effect prediction. Our results show that scFM embeddings do not provide consistent improvements over baseline models, especially under distribution shift. Additionally, all models struggle with predicting strong or atypical perturbation effects. Overall, this study provides a systematic evaluation of zero-shot scFM embeddings for perturbation effect prediction, highlighting the challenges of this task and revealing the limitations of current-generation scFMs. Our findings underscore the need for specialized models and high-quality datasets that capture a broader range of cellular states. Source code and documentation can be found at:https://anonymous.4open.science/r/PertEval-C674/.", "title_embedding_index": 1815, "title_abs_embedding_index": 1840}, {"title": "Normalized Space Alignment: A Versatile Metric for Representation Analysis", "link_suffix": "/forum?id=1S7kpbfgq9", "link": "https://openreview.net/forum?id=1S7kpbfgq9", "pdf_link": "https://openreview.net/pdf?id=1S7kpbfgq9", "keywords": "Deep Learning, Representation Learning, Local Intrinsic Dimensionality, Similarity Metric, Dimensionality Reduction, Interpretability", "abstract": "We introduce a manifold analysis technique for neural network representations. Normalized Space Alignment (NSA) compares pairwise distances between two point clouds derived from the same source and having the same size, while potentially possessing differing dimensionalities. NSA can act as both an analytical tool and a differentiable loss function, providing a robust means of comparing and aligning representations across different layers and models. It satisfies the criteria necessary for both a similarity metric and a neural network loss function. We showcase NSA's versatility by illustrating its utility as a representation space analysis metric, a structure-preserving loss function, and a robustness analysis tool. NSA is not only computationally efficient but it can also approximate the global structural discrepancy during mini-batching, facilitating its use in a wide variety of neural network training paradigms.", "title_embedding_index": 1816, "title_abs_embedding_index": 1841}, {"title": "RedCodeAgent:  Automatic Red-teaming Agent against Code Agents", "link_suffix": "/forum?id=Mvn5g49RrM", "link": "https://openreview.net/forum?id=Mvn5g49RrM", "pdf_link": "https://openreview.net/pdf?id=Mvn5g49RrM", "keywords": "Trustworthy machine learning, Code Agents, LLM", "abstract": "LLM-based code agents, integrated with external tools like the Python interpreter, can interact with broad system environments and leverage code execution feedback to improve or self-debug generated code for better task-solving. However, as these code agents evolve rapidly in terms of capabilities, their increasing sophistication also amplifies security risks, such as generating or executing risky and buggy code. Traditional static safety benchmarks and manually designed red-teaming tools struggle to keep up with this rapid evolution, lacking the ability to adapt dynamically to the changing behaviors of code agents. To address these limitations, we propose RedCodeAgent, the first fully automated and adaptive red-teaming agent against given code agents. Equipped with red-teaming tools for function-calling and a novel memory module for accumulating successful attack experience, RedCodeAgent dynamically optimizes input prompts to jailbreak the target code agent for risky code execution. Unlike static benchmarks or red-teaming tools, RedCodeAgent autonomously adapts its attack strategies, making it a scalable solution to the growing challenge of testing increasingly sophisticated code agents. Experimental results show that compared to state-of-the-art LLM jailbreaking methods, RedCodeAgent achieves significantly higher attack success rates on the same tasks while maintaining high overall efficiency. By autonomously exploring and exploiting vulnerabilities of code agents, RedCodeAgent provides critical insights into the evolving security risks of code agents.", "title_embedding_index": 1817, "title_abs_embedding_index": 1842}, {"title": "Meta-Learning Adaptable Foundation Models", "link_suffix": "/forum?id=h0pACOIFxC", "link": "https://openreview.net/forum?id=h0pACOIFxC", "pdf_link": "https://openreview.net/pdf?id=h0pACOIFxC", "keywords": "meta-learning theory, parameter efficient fine-tuning, low-rank adaptation, foundation models, non-convex optimization", "abstract": "The power of foundation models (FMs) lies in their capacity to learn highly expressive representations that can be adapted to a broad spectrum of tasks. However, these pretrained models require multiple stages of fine-tuning to become effective for downstream applications. Conventionally, the model is first retrained on the aggregate of a diverse set of tasks of interest, and then adapted to a specific low-resource downstream tasks by utilizing a parameter-efficient fine-tuning (PEFT) scheme. While this two-phase procedure seems reasonable, the independence of the retraining and fine-tuning phases causes a major issue, as there is no guarantee the retrained model will achieve good performance post fine-tuning. To explicitly address this issue, we introduce a meta-learning framework infused with PEFT in this intermediate retraining stage to learn a model which can be easily adapted to unseen tasks. For our theoretical results, we focus on linear models using low-rank adaptations. In this setting, we demonstrate the suboptimality of standard retraining for finding an adaptable set of parameters. Further, we prove that our method recovers the optimally adaptable parameters. We then apply these theoretical insights to retraining the RoBERTa model to predict continuations of conversations between different personas within the ConvAI2 dataset. Empirically, we observe significant performance benefits using our proposed meta-learning scheme during retraining relative to the conventional approach.", "title_embedding_index": 1818, "title_abs_embedding_index": 1843}, {"title": "ACER: Automatic Language Model Context Extension via Retrieval", "link_suffix": "/forum?id=qeYa5LRveW", "link": "https://openreview.net/forum?id=qeYa5LRveW", "pdf_link": "https://openreview.net/pdf?id=qeYa5LRveW", "keywords": "language, retrieval, long context understanding, text generation", "abstract": "Long-context modeling is one of the critical capabilities of language AI for digesting and reasoning over complex information pieces. In practice, long-context capabilities are typically built into a pre-trained language model (LM) through a carefully designed context extension stage, with the goal of producing generalist long-context capabilities. In our preliminary experiments, however, we discovered that the current open-weight generalist long-context models are still lacking in practical long-context processing tasks. While this means perfectly effective long-context modeling demands task-specific data, the cost can be prohibitive. In this paper, we draw inspiration from how humans process a large body of information: a lossy retrieval stage ranks a large set of documents while the reader ends up reading deeply only the top candidates. We build an automatic data synthesis pipeline that mimics this process using short-context LMs. The short-context LMs are further tuned using these self-generated data to obtain task-specific long-context capabilities. Similar to how pre-training learns from imperfect data, we hypothesize and further demonstrate that the short-context model can bootstrap over the synthetic data, outperforming not only long-context generalist models but also the retrieval and read pipeline used to synthesize the training data.", "title_embedding_index": 1819, "title_abs_embedding_index": 1844}, {"title": "Towards Practical Large-Scale Privacy-Preserving Recurrent Neural Networks", "link_suffix": "/forum?id=TvvT4wjEPf", "link": "https://openreview.net/forum?id=TvvT4wjEPf", "pdf_link": "https://openreview.net/pdf?id=TvvT4wjEPf", "keywords": "recurrent neural networks, fully homomorphic encryption, privacy-preserving machine learning, quantization, regularization, overflow, CGGI, TFHE, CKKS", "abstract": "Recurrent neural networks (RNNs) are used for a variety of applications such as speech recognition and financial forecasting where data privacy is an ongoing concern.  Fully homomorphic encryption (FHE) facilitates computation over encrypted data, enabling third-party services like machine learning inference while keeping client data private. Previous studies have examined RNN inference over encrypted data using FHE, albeit on a small scale, though impractical due to the computational costs. This work advances insights that make large-scale RNN evaluation over encrypted data practical. A problem that prohibits the scaling of privacy-preserving RNNs is overflow in the ciphertext message space. As the number of model parameters increases, the size of the domain during multiply-accumulate operations increases, causing inaccuracies in computation. Attempts to mitigate this problem, such as splitting the message into several ciphertexts, cause an exponential increase in computation, making latency-sensitive applications like RNNs impractical. A novel regularization technique is proposed that mitigates the effects of numerical overflow during training. This allows use of one ciphertext only and reduces the complexity of the encryption parameters that would otherwise be required to perform correct computation while maintaining 128-bit security. Using the CGGI variant of FHE and GPU acceleration, we quantize and evaluate a 1.9M parameter, multi-layer RNN across 28 timesteps, achieving 90.82% top-1 accuracy over the encrypted MNIST test dataset with an average latency of 2.1s per sample---a new state of the art in latency, model performance, and scale.", "title_embedding_index": 1820, "title_abs_embedding_index": 1845}, {"title": "Counterfactual History Distillation on Continuous-time Event Sequences", "link_suffix": "/forum?id=O4y4m9biMx", "link": "https://openreview.net/forum?id=O4y4m9biMx", "pdf_link": "https://openreview.net/pdf?id=O4y4m9biMx", "keywords": "Counterfactual Analysis, Marked Temporal Point Process", "abstract": "This study aims to distill history events that have essential information for predicting subsequent events with counterfactual analysis. The problem is named Counterfactual History Distillation (CHD). CHD distills a minimum set of events from history, based on which the distribution provided by a trained MTPP model fits the events observed later, and the distribution based on the remaining events in history cannot. It can help understand what event marks may have more influence on the occurrence of future events and what events in history may have a causal relationship with the events observed later. This study proposes a robust solution for CHD, called MTPP-based Counterfactual History Distiller (MTPP-CHD). MTPP-CHD learns to select the optimal event combination from history for the events observed later. Experiment results demonstrate the superiority of MTPP-CHD by outperforming baselines in terms of distillation quality and processing speed.", "title_embedding_index": 1821, "title_abs_embedding_index": 1846}, {"title": "From MLP to NeoMLP: Leveraging Self-Attention for Neural Fields", "link_suffix": "/forum?id=A8Vuf2e8y6", "link": "https://openreview.net/forum?id=A8Vuf2e8y6", "pdf_link": "https://openreview.net/pdf?id=A8Vuf2e8y6", "keywords": "Neural fields, Self-attention, Auto-decoding, Transformers, Conditional neural fields, Implicit neural representations, Graphs", "abstract": "Neural fields (NeFs) have recently emerged as a state-of-the-art method for encoding spatio-temporal signals of various modalities. Despite the success of NeFs in reconstructing individual signals, their use as representations in downstream tasks, such as classification or segmentation, is hindered by the complexity of the parameter space and its underlying symmetries, in addition to the lack of powerful and scalable conditioning mechanisms. In this work, we draw inspiration from the principles of connectionism to design a new architecture based on MLPs, which we termNeoMLP. We start from an MLP, viewed as a graph, and transform it from a multi-partite graph to acomplete graphof input, hidden, and output nodes, equipped withhigh-dimensional features. We perform message passing on this graph and employ weight-sharing viaself-attentionamong all the nodes.NeoMLP has a built-in mechanism for conditioning through the hidden and output nodes, which function as a set of latent codes, and as such,NeoMLP can be used straightforwardly as a conditional neural field. We demonstrate the effectiveness of our method by fitting high-resolution signals, including multi-modal audio-visual data. Furthermore, we fit datasets of neural representations, by learning instance-specific sets of latent codes using a single backbone architecture, and then use them for downstream tasks, outperforming recent state-of-the-art methods.", "title_embedding_index": 1822, "title_abs_embedding_index": 1847}, {"title": "LLM-based Typed Hyperresolution for Commonsense Reasoning with Knowledge Bases", "link_suffix": "/forum?id=wNobG8bV5Q", "link": "https://openreview.net/forum?id=wNobG8bV5Q", "pdf_link": "https://openreview.net/pdf?id=wNobG8bV5Q", "keywords": "Large Language Models, Commonsense reasoning, Logical inference", "abstract": "Large language models (LLM) are being increasingly applied to tasks requiring commonsense reasoning. Despite their outstanding potential, the reasoning process of LLMs is prone to errors and hallucinations that hinder their applicability, especially in high-stakes scenarios. Several works have attempted to enhance commonsense reasoning performance of LLMs by (i) using prompting styles that elicit more accurate reasoning, (ii) utilizing the LLM as a semantic parser for a symbolic reasoner, or (iii) enforcing the LLM to simulate a logical inference rule.  However, all these solutions have critical limitations: they are unable to leverage the internal commonsense knowledge of the LLM in tandem with an axiomatic knowledge base, they lack a mechanism to reliably repair erroneous inference steps, and their application is restricted to small knowledge bases that fit the context limit of the LLM. In this work, we present LLM-based Typed Hyperresolution (LLM-TH), a logical commonsense reasoning framework that leverages \"theory resolution\", a concept from classical logical inference which enables integrating LLMs into the \"resolution\" inference rule, thus mitigating reasoning errors and hallucinations and enabling verification of the reasoning procedure. LLM-TH is also equipped with a mechanism for repairing erroneous inference steps supported by theoretical guarantees. Using \"Hyperresolution\" and \"Typed inference\" schemes, we show that LLM-TH can efficiently reason over large knowledge bases consisting of tens of thousands of rules with arbitrary predicate arities. Our experiments on three diverse language-based reasoning tasks\u2014preference reasoning, multi-domain deductive reasoning, and geographical question answering\u2014showcase that LLM-TH, using merely a BART 406M parameter NLI entailment model, significantly reduces reasoning errors compared to baselines using Llama3-70B, Gemini1.5-Flash, GPT-3.5-Turbo, and Mixtral-46.7B.", "title_embedding_index": 1823, "title_abs_embedding_index": 1848}, {"title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning", "link_suffix": "/forum?id=9z9PvXPisj", "link": "https://openreview.net/forum?id=9z9PvXPisj", "pdf_link": "https://openreview.net/pdf?id=9z9PvXPisj", "keywords": "Data Selection, Instruction Tuning, Large Language Models", "abstract": "Instruction tuning has underscored the significant potential of large language models (LLMs) in producing more human-controllable and effective outputs in various domains. In this work, we focus on the data selection problem for task-specific instruction tuning of LLMs. Prevailing methods primarily rely on the crafted similarity metrics to select training data that aligns with the test data distribution. The goal is to minimize instruction tuning loss on the test data, ultimately improving performance on the target task. However, it has been widely observed that instruction tuning loss (i.e., cross-entropy loss for next token prediction) in LLMs often fails to exhibit a monotonic relationship with actual task performance. This misalignment undermines the effectiveness of current data selection methods for task-specific instruction tuning. To address this issue, we introduce ROSE, a novel Reward-Oriented inStruction data sElection method which leverages pairwise preference loss as a reward signal to optimize data selection for task-specific instruction tuning. Specifically, ROSE adapts an influence formulation to approximate the influence of training data points relative to a few-shot preference validation set to select the most task-related training data points. Experimental results show that by selecting just 5% of the training data using ROSE, our approach can achieve competitive results compared to fine-tuning with the full training dataset, and it surpasses other state-of-the-art data selection methods for task-specific instruction tuning. Our qualitative analysis further confirms the robust generalizability of our method across multiple benchmark datasets and diverse model architectures.", "title_embedding_index": 1824, "title_abs_embedding_index": 1849}]