[
    {
        "title": "Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention",
        "link_suffix": "/forum?id=5GgjiRzYp3",
        "link": "https://openreview.net/forum?id=5GgjiRzYp3",
        "pdf_link": "https://openreview.net/pdf?id=5GgjiRzYp3",
        "keywords": "3D Visual Grounding, 3D Multimodal Learning",
        "abstract": "In real-life scenarios, humans seek out objects in the 3D world to fulfill their daily needs or intentions. This inspires us to  introduce 3D intention grounding, a new task in 3D object detection  employing RGB-D, based on human intention, such as \"I want something to support my back.\" Closely related, 3D visual grounding focuses on understanding human reference. To achieve detection based on human intention, it relies on humans to observe the scene, reason out the target that aligns with their intention (\"pillow\" in this case), and finally provide a reference to the AI system, such as \"A pillow on the couch\". Instead, 3D intention grounding challenges AI agents to automatically observe, reason and detect the desired target solely based on human intention. To tackle this challenge, we introduce the new Intent3D dataset, consisting of 44,990 intention texts associated with 209 fine-grained classes from 1,042 scenes of the ScanNet dataset. We also establish several baselines based on different language-based 3D object detection models on our benchmark. Finally, we propose IntentNet, our unique approach, designed to tackle this intention-based detection problem. It focuses on three key aspects: intention understanding, reasoning to identify object candidates, and cascaded adaptive learning that leverages the intrinsic priority logic of different losses for multiple objective optimization."
    },
    {
        "title": "Towards a Theoretical Understanding of Memorization in Diffusion Models",
        "link_suffix": "/forum?id=GaWYCQMAq1",
        "link": "https://openreview.net/forum?id=GaWYCQMAq1",
        "pdf_link": "https://openreview.net/pdf?id=GaWYCQMAq1",
        "keywords": "Diffusion Models Memorization; Diffusion Probabilistic models",
        "abstract": "As diffusion probabilistic models (DPMs) are being employed as mainstream models for Generative Artificial Intelligence (GenAI), the study of their memorization of training data has attracted growing attention. Existing works in this direction aim to establish an understanding of whether or to what extent DPMs learn via memorization. Such an understanding is crucial for identifying potential risks of data leakage and copyright infringement in diffusion models and, more importantly, for trustworthy application of GenAI. Existing works revealed that conditional DPMs are more prone to training data memorization than unconditional DPMs, and the motivated data extraction methods are mostly for conditional DPMs. However, these understandings are primarily empirical, and extracting training data from unconditional models has been found to be extremely challenging. In this work, we provide a theoretical understanding of memorization in both conditional and unconditional DPMs under the assumption of model convergence. Our theoretical analysis indicates that extracting data from unconditional models can also be effective by constructing a proper surrogate condition. Based on this result, we propose a novel data extraction method named \\textbf{Surrogate condItional Data Extraction (SIDE)} that leverages a time-dependent classifier trained on the generated data as a surrogate condition to extract training data from unconditional DPMs. Empirical results demonstrate that our SIDE can extract training data in challenging scenarios where previous methods fail, and it is, on average, over 50% more effective across different scales of the CelebA dataset."
    },
    {
        "title": "Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning",
        "link_suffix": "/forum?id=5DT0t5NylU",
        "link": "https://openreview.net/forum?id=5DT0t5NylU",
        "pdf_link": "https://openreview.net/pdf?id=5DT0t5NylU",
        "keywords": "3D Large Language Model, 3D Multimodal Learning",
        "abstract": "Recent advancements in 3D Large Language Models (3DLLMs) have highlighted their potential in building general-purpose agents in the 3D real world, yet challenges remain due to the lack of high-quality robust instruction-following data, leading to limited discriminative power and generalization of 3DLLMs. In this paper, we introduce Robin3D, a powerful 3DLLM trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG) engine. RIG generates two key instruction data: 1) the Adversarial Instruction-following data, which features mixed negative and positive samples to enhance the model's discriminative understanding. 2) the Diverse Instruction-following data, which contains various instruction styles to enhance model's generalization. As a result, we construct 1 million instruction-following data, consisting of 344K Adversarial samples, 508K Diverse samples, and 165K benchmark training set samples. To better handle these complex instructions, Robin3D first incorporates Relation-Augmented Projector to enhance spatial understanding, and then strengthens the object referring and grounding ability through ID-Feature Bonding. Robin3D consistently outperforms previous methods across five widely-used 3D multimodal learning benchmarks, without the need for task-specific fine-tuning.\nNotably, we achieve a 7.8% improvement in the grounding task (Multi3DRefer) and a 6.9% improvement in the captioning task (Scan2Cap)."
    },
    {
        "title": "MS3M: Multi-Stage State Space Model for Motion Forecasting",
        "link_suffix": "/forum?id=MmOQY71YHw",
        "link": "https://openreview.net/forum?id=MmOQY71YHw",
        "pdf_link": "https://openreview.net/pdf?id=MmOQY71YHw",
        "keywords": "Motion Forecasting, Autonomous Driving, State Space Model",
        "abstract": "Motion forecasting is a fundamental component of autonomous driving systems, as it predicts an agent's future trajectories based on its surrounding environment. Transformer architectures have dominated this domain due to their strong ability to model both temporal and spatial information. However, transformers often suffer from quadratic complexity with respect to input sequence length, limiting their ability to efficiently process scenarios involving numerous agents. Additionally, transformers typically rely on positional encodings to represent temporal or spatial relationships, a strategy that may not be as effective or intuitive as the inductive biases naturally embedded in convolutional architectures. To address these challenges, we leverage recent advancements in state space models (SSMs) and propose the Multi-Stage State Space Model (MS$^3$M). In MS$^3$M, the Temporal Mamba Model (TMM) is employed to capture fine-grained temporal information, while the Spatial Mamba Model efficiently handles spatial interactions. By injecting temporal and spatial inductive biases through Mamba\u2019s state-space model structure, the model's capacity is significantly improved. MS$^3$M also strikes an exceptional trade-off between accuracy and efficiency, which is achieved through convolutional computations and near-linear computational strategies in the Mamba architecture. Furthermore, a hierarchical query-based decoder is introduced, further enhancing model performance and efficiency. Extensive experimental results demonstrate that the proposed method achieves superior performance while maintaining low latency, which is crucial for practical real-time autonomous driving systems."
    },
    {
        "title": "SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning",
        "link_suffix": "/forum?id=jXLiDKsuDo",
        "link": "https://openreview.net/forum?id=jXLiDKsuDo",
        "pdf_link": "https://openreview.net/pdf?id=jXLiDKsuDo",
        "keywords": "reinforcement learning",
        "abstract": "Recent advances in CV and NLP have been largely driven by scaling up the number of network parameters, despite traditional theories suggesting that larger networks are prone to overfitting.\nThese large networks avoid overfitting by integrating components that induce a \\textit{simplicity bias}, guiding models toward simple and generalizable solutions. \nHowever, in deep RL, designing and scaling up networks have been less explored.\nMotivated by this opportunity, we present SimBa, an architecture designed to scale up parameters in deep RL by injecting a simplicity bias. SimBa consists of three components: (i) an observation normalization layer that standardizes inputs with running statistics, (ii) a residual feedforward block to provide a linear pathway from the input to output, and (iii) a layer normalization to control feature magnitudes. \nBy scaling up parameters with SimBa, the sample efficiency of various deep RL algorithms\u2014including off-policy, on-policy, and unsupervised methods\u2014is consistently improved.\nMoreover, solely by integrating SimBa architecture into SAC, it matches or surpasses state-of-the-art deep RL methods with high computational efficiency across DMC, MyoSuite, and HumanoidBench.\nThese results demonstrate SimBa's broad applicability and effectiveness across diverse RL algorithms and environments."
    },
    {
        "title": "Efficient Text-driven Human Motion Generation via Latent Consistency Training",
        "link_suffix": "/forum?id=SNsdlEp3Ne",
        "link": "https://openreview.net/forum?id=SNsdlEp3Ne",
        "pdf_link": "https://openreview.net/pdf?id=SNsdlEp3Ne",
        "keywords": "motion generation, diffusion, consistency training",
        "abstract": "Consistency models excel at few-step inference in generative tasks across various scenarios, but typically rely on pre-trained diffusion model distillation, involving additional training costs and performance limitations. \nIn this paper, we propose a motion latent consistency training framework that learns directly from data rather than distillation for efficient and text-controllable human motion generation.\nFor representation optimization, we design a motion autoencoder with quantization constraints that enable concise and bounded motion latent representations.\nFocusing on conditional generation, we construct a classifier-free guidance (CFG) format with an additional unconditional loss function that extends the CFG technique from the inference phase to the training phase for conditionally guided consistency training.\nWe further propose a clustering guidance module to provide additional references to the solution distribution at minimal query cost.\nBy combining these enhancements, we achieve stable and consistent training in non-pixel modality and latent representation spaces for the first time.\nExperiments in benchmarks demonstrate that our method significantly outperforms traditional consistency distillation methods with reduced training cost, and enhances the consistency model to perform comparably to state-of-the-art models with lower inference cost.\nOur code will be open source."
    },
    {
        "title": "On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning",
        "link_suffix": "/forum?id=7893vsQenk",
        "link": "https://openreview.net/forum?id=7893vsQenk",
        "pdf_link": "https://openreview.net/pdf?id=7893vsQenk",
        "keywords": "test time adaptation, continual learning, data poisoning",
        "abstract": "Test-time adaptation (TTA) updates the model weights during the inference stage using testing data to enhance generalization. However, this practice exposes TTA to adversarial risks. Existing studies have shown that when TTA is updated with crafted adversarial test samples, also known as test-time poisoned data, the performance on benign samples can deteriorate. Nonetheless, the perceived adversarial risk may be overstated if the poisoned data is generated under overly strong assumptions. In this work, we first review realistic assumptions for test-time data poisoning, including white-box versus grey-box attacks, access to benign data, attack budget, and more. We then propose an effective and realistic attack method that better produces poisoned samples without access to benign samples, and derive an effective in-distribution attack objective. We also design two TTA-aware attack objectives. Our benchmarks of existing attack methods reveal that the TTA methods are more robust than previously believed. In addition, we analyze effective defense strategies to help develop adversarially robust TTA methods."
    },
    {
        "title": "REVEAL-IT: REinforcement learning with Visibility of Evolving Agent poLicy for InTerpretability",
        "link_suffix": "/forum?id=1c73HCZpbo",
        "link": "https://openreview.net/forum?id=1c73HCZpbo",
        "pdf_link": "https://openreview.net/pdf?id=1c73HCZpbo",
        "keywords": "Reinforcement Learning, Interpretability",
        "abstract": "Understanding the agent's learning process, particularly the factors that contribute to its success or failure post-training, is crucial for comprehending the rationale behind the agent's decision-making process. Prior methods clarify the learning process by creating a structural causal model (SCM) or visually representing the distribution of value functions. Nevertheless, these approaches have constraints as they exclusively function in 2D-environments or with uncomplicated transition dynamics. Understanding the agent's learning process in complicated environments or tasks is more challenging. In this paper, we propose REVEAL-IT, a novel framework for explaining the learning process of an agent in complex environments. Initially, we visualize the policy structure and the agent's learning process for various training tasks. By visualizing these findings, we can understand how much a particular training task or stage affects the agent's performance in the test. Then, a GNN-based explainer learns to highlight the most important section of the policy, providing a more clear and robust explanation of the agent's learning process. The experiments demonstrate that explanations derived from this framework can effectively help optimize the training tasks, resulting in improved learning efficiency and final performance."
    },
    {
        "title": "Quality Diversity Imitation Learning",
        "link_suffix": "/forum?id=ozhRaoRGyl",
        "link": "https://openreview.net/forum?id=ozhRaoRGyl",
        "pdf_link": "https://openreview.net/pdf?id=ozhRaoRGyl",
        "keywords": "Imitation Learning, Quality Diversity, behavior-level exploration, limited demonstration",
        "abstract": "Imitation learning (IL) has shown great potential in various applications, such as robot control. However, traditional IL methods are usually designed to learn only one specific type of behavior since demonstrations typically correspond to a single expert. In this work, we introduce the first generic framework for Quality Diversity Imitation Learning (QD-IL), which enables the agent to learn a broad range of skills from limited demonstrations. Our framework integrates the principles of quality diversity with adversarial imitation learning (AIL) methods, and can potentially improve any inverse reinforcement learning (IRL) method. Empirically, our framework significantly improves the QD performance of GAIL and VAIL on the challenging continuous control tasks derived from Mujoco environments. Moreover, our method even achieves 2x expert performance in the most challenging Humanoid environment."
    },
    {
        "title": "Implicit Bridge Consistency Distillation for One-Step Unpaired Image Translation",
        "link_suffix": "/forum?id=1YTF7Try7H",
        "link": "https://openreview.net/forum?id=1YTF7Try7H",
        "pdf_link": "https://openreview.net/pdf?id=1YTF7Try7H",
        "keywords": "image translation, consistency distillation, unpaired, one-step, diffusion models",
        "abstract": "Recently, diffusion models have been extensively studied as powerful generative tools for image translation. However, the existing diffusion model-based image translation approaches often suffer from several limitations: 1) slow inference due to iterative denoising, 2) the necessity for paired training data, or 3) constraints from learning only one-way translation paths. To mitigate these limitations, here we introduce a novel framework, called Implicit Bridge Consistency Distillation (IBCD),  that extends consistency distillation with a diffusion implicit bridge model that connects PF-ODE trajectories from any distribution to another one. Moreover, to address the challenges associated with distillation errors and mean prediction problems from the consistency distillation, we introduce two unique improvements: Distribution Matching for Consistency Distillation (DMCD) and distillation-difficulty adaptive weighting method. Experimental results confirm that IBCD for bidirectional translation can achieve state-of-the-art performance on benchmark datasets in just one step generation."
    },
    {
        "title": "Comparing noisy neural population dynamics using optimal transport distances",
        "link_suffix": "/forum?id=cNmu0hZ4CL",
        "link": "https://openreview.net/forum?id=cNmu0hZ4CL",
        "pdf_link": "https://openreview.net/pdf?id=cNmu0hZ4CL",
        "keywords": "Representational similarity, shape metrics, optimal transport, Wasserstein distance",
        "abstract": "Biological and artificial neural systems form high-dimensional neural representations that underpin their computational capabilities. Consequently, methods for quantifying geometric similarity in neural representations have become a popular tool for identifying computational principles that are potentially shared across neural systems. These methods generally assume that neural responses are deterministic and static. However, responses of biological systems, and some artificial systems, are noisy and dynamically unfold over time. Furthermore, these characteristics can have substantial influence on a system's computational capabilities. Here, we demonstrate how existing metrics fail to capture key differences between neural systems with noisy dynamic responses. We then propose a metric for comparing the geometry of noisy neural trajectories, which is based on ``causal'' optimal transport distances between stochastic processes. We use the metric to compare models of neural responses in different regions of the motor system and to compare the dynamics of latent diffusion models for text-to-image synthesis"
    },
    {
        "title": "GenXD: Generating Any 3D and 4D Scenes",
        "link_suffix": "/forum?id=1ThYY28HXg",
        "link": "https://openreview.net/forum?id=1ThYY28HXg",
        "pdf_link": "https://openreview.net/pdf?id=1ThYY28HXg",
        "keywords": "3D Generation; 4D Generation; Diffusion Models",
        "abstract": "Recent developments in 2D visual generation have been remarkably successful. However, 3D and 4D generation remain challenging in real-world applications due to the lack of large-scale 4D data and effective model design. In this paper, we propose to jointly investigate general 3D and 4D generation by leveraging camera and object movements commonly observed in daily life. Due to the lack of real-world 4D data in the community, we first propose a data curation pipeline to obtain camera poses and object motion strength from videos. Based on this pipeline, we introduce a large-scale real-world 4D scene dataset: CamVid-30K. By leveraging all the 3D and 4D data, we develop our framework, GenXD, which allows us to produce any 3D or 4D scene. We propose multiview-temporal modules, which disentangle camera and object movements, to seamlessly learn from both 3D and 4D data. Additionally, GenXD employs masked latent conditions to support a variety of conditioning views. GenXD can generate videos that follow the camera trajectory as well as consistent 3D views that can be lifted into 3D representations. We perform extensive evaluations across various real-world and synthetic datasets, demonstrating GenXD's effectiveness and versatility compared to previous methods in 3D and 4D generation. The dataset and code will be made publicly available."
    },
    {
        "title": "EXPLORING RESPONSE UNCERTAINTY IN MLLMS: AN EMPIRICAL EVALUATION UNDER MISLEADING SCENARIOS",
        "link_suffix": "/forum?id=2OANNtX3T5",
        "link": "https://openreview.net/forum?id=2OANNtX3T5",
        "pdf_link": "https://openreview.net/pdf?id=2OANNtX3T5",
        "keywords": "UNCERTAINTY, MLLMs, Misleading",
        "abstract": "Ensuring that Multimodal Large Language Models (MLLMs) maintain consistency in their responses is essential for developing trustworthy multimodal intelligence. However, existing benchmarks include many samples where all MLLMs exhibit high response uncertainty when encountering misleading information, requiring even 5-15 response attempts per sample to effectively assess uncertainty. Therefore, we propose a two-stage pipeline: first, we collect MLLMs\u2019 responses without misleading information, and then gather misleading ones via specific misleading instructions. By calculating the misleading rate, and capturing both correct-to-incorrect and incorrect-to-correct shifts between the two sets of responses, we can effectively metric the model\u2019s response uncertainty. Eventually, we establish a Multimodal Uncertainty Benchmark (MUB) that employs both explicit and implicit misleading instructions to comprehensively assess the vulnerability of MLLMs across diverse domains. Our experiments reveal that all open-source and close-source MLLMs are highly susceptible to misleading instructions, with an average misleading rate exceeding 86%. To enhance the robustness of MLLMs, we further fine-tune all open-source MLLMs by incorporating explicit and implicit misleading data, which demonstrates a significant reduction in misleading rates"
    },
    {
        "title": "A Comprehensive Deepfake Detector Assessment Platform",
        "link_suffix": "/forum?id=C6d9S2lYFN",
        "link": "https://openreview.net/forum?id=C6d9S2lYFN",
        "pdf_link": "https://openreview.net/pdf?id=C6d9S2lYFN",
        "keywords": "deepfake detection; benchmark; evaluation",
        "abstract": "The rapid development of deepfake techniques has raised serious concerns about the authenticity and integrity of digital media. To combat the potential misuse of deepfakes, it is crucial to develop reliable and robust deepfake detection algorithms. In this paper, we propose a comprehensiveDeepfakeDetectorAssessmentPlatform (DAP), covering six critical dimensions: benchmark performance, forgery algorithm generalization, image distortion robustness, adversarial attack resilience, forgery localization accuracy, and attribute bias. Our framework aims to provide a standardized and rigorous approach to assess the performance, generalization ability, robustness, security, localization precision, and fairness of deepfake detection algorithms. Extensive experiments are conducted on multiple public and self-built databases, considering various forgery techniques, image distortions, adversarial attacks, and attributes. The proposed framework offers insights into the strengths and limitations of state-of-the-art deepfake detection algorithms and serves as a valuable tool for researchers and practitioners to develop and evaluate novel approaches in this field. All codes, scripts, and data described in this paper are open source and available athttps://github.com/tempuser4567/DAP."
    },
    {
        "title": "SATE: A Two-Stage Approach for Performance Prediction in Subpopulation Shift Scenarios",
        "link_suffix": "/forum?id=BWYR9rfGOU",
        "link": "https://openreview.net/forum?id=BWYR9rfGOU",
        "pdf_link": "https://openreview.net/pdf?id=BWYR9rfGOU",
        "keywords": "Performance Prediction, Subpopulation Shift, Unsupervised Accuracy Estimation",
        "abstract": "Subpopulation shift refers to the difference in the distribution of subgroups between training and test datasets. When an underrepresented group becomes predominant during testing, it can lead to significant performance degradation, making performance prediction prior to deployment particularly important. Existing performance prediction methods often fail to address this type of shift effectively due to their usage of unreliable model confidence and mis-specified distributional distances. In this paper, we propose a novel performance prediction method specifically designed to tackle subpopulation shifts, called Subpopulation-Aware Two-stage Estimator (SATE). Our approach first estimates the subgroup proportions in the test set by linearly expressing the test embedding with training subgroup embeddings. Then, it predicts the accuracy for each subgroup using the accuracy on augmented training set, aggregating them into an overall performance estimate. We provide theoretical proof of our method's unbiasedness and consistency, and demonstrate that it outperforms numerous baselines across various datasets, including vision, medical, and language tasks, offering a reliable tool for performance prediction in scenarios involving subpopulation shifts."
    },
    {
        "title": "Automatic Organization of Neural Modules for Enhanced Collaboration in Neural Networks",
        "link_suffix": "/forum?id=ar9tcnD4e9",
        "link": "https://openreview.net/forum?id=ar9tcnD4e9",
        "pdf_link": "https://openreview.net/pdf?id=ar9tcnD4e9",
        "keywords": "Deep Learning; Neural Networks;",
        "abstract": "This work proposes a new perspective on the structure of Neural Networks (NNs). Traditional Neural Networks are typically tree-like structures for convenience, which can be predefined or learned by NAS methods. However, such a structure can not facilitate communications between nodes at the same level or signal transmissions to previous levels. These defects prevent effective collaboration, restricting the capabilities of neural networks. It is well-acknowledged that the biological neural system contains billions of neural units. Their connections are far more complicated than the current NN structure. To enhance the representational ability of neural networks, existing works try to increase the depth of the neural network and introduce more parameters. However, they all have limitations with constrained parameters. In this work, we introduce a synchronous graph-based structure to establish a novel way of organizing the neural units: the Neural Modules. This framework allows any nodes to communicate with each other and encourages neural units to work collectively, demonstrating a departure from the conventional constrained paradigm. Such a structure also provides more candidates for the NAS methods. Furthermore, we also propose an elegant regularization method to organize neural units into multiple independent, balanced neural modules systematically. This would be convenient for handling these neural modules in parallel. Compared to traditional NNs, our method unlocks the potential of NNs from tree-like structures to general graphs and makes NNs be optimized in an almost complete set. Our approach proves adaptable to diverse tasks, offering compatibility across various scenarios. Quantitative experimental results substantiate the potential of our structure, indicating the improvement of NNs."
    },
    {
        "title": "Latent Action Pretraining from Videos",
        "link_suffix": "/forum?id=VYOe2eBQeh",
        "link": "https://openreview.net/forum?id=VYOe2eBQeh",
        "pdf_link": "https://openreview.net/pdf?id=VYOe2eBQeh",
        "keywords": "vision-language-action models, robotics",
        "abstract": "We introduce Latent Action Pretraining for general Action models (LAPA), the first unsupervised method for pretraining Vision-Language-Action (VLA) models without ground-truth robot action labels. Existing Vision-Language-Action models require action labels typically collected by human teleoperators during pretraining, which significantly limits possible data sources and scale. In this work, we propose a method to learn from internet-scale videos that do not have robot action labels. We first train an action quantization model leveraging VQ-VAE-based objective to learn discrete latent actions between image frames, then pretrain a latent VLA model to predict these latent actions from observations and task descriptions, and finally finetune the VLA on small-scale robot manipulation data to map from latent to robot actions. Experimental results demonstrate that our method significantly outperforms existing techniques that train robot manipulation policies from large-scale videos. Furthermore, it outperforms the state-of-the-art VLA model trained with robotic action labels on real-world manipulation tasks that require language conditioning, generalization to unseen objects, and semantic generalization to unseen instructions. Training only on human manipulation videos also shows positive transfer, opening up the potential for leveraging web-scale data for robotics foundation model."
    },
    {
        "title": "A Sinkhorn-type Algorithm for Constrained Optimal Transport",
        "link_suffix": "/forum?id=V5kCKFav9j",
        "link": "https://openreview.net/forum?id=V5kCKFav9j",
        "pdf_link": "https://openreview.net/pdf?id=V5kCKFav9j",
        "keywords": "Optimal Transport, Constrained Optimization",
        "abstract": "Entropic optimal transport (OT) and the Sinkhorn algorithm have made it practical for machine learning practitioners to perform the fundamental task of calculating transport distance between statistical distributions. In this work, we focus on a general class of OT problems under a combination of equality and inequality constraints. We derive the corresponding entropy regularization formulation and introduce a Sinkhorn-type algorithm for such constrained OT problems supported by theoretical guarantees. We first bound the approximation error when solving the problem through entropic regularization, which reduces exponentially with the increase of the regularization parameter. Furthermore, we prove a sublinear first-order convergence rate of the proposed Sinkhorn-type algorithm in the dual space by characterizing the optimization procedure with a Lyapunov function. To achieve fast and higher-order convergence under weak entropy regularization, we augment the Sinkhorn-type algorithm with dynamic regularization scheduling and second-order acceleration. Overall, this work systematically combines recent theoretical and numerical advances in entropic optimal transport with the constrained case, allowing practitioners to derive approximate transport plans in complex scenarios. In addition, we extend the formulation of this work to partial optimal transport and propose a fast algorithm with practical super-exponential convergence."
    },
    {
        "title": "Four eyes see more than two: Dataset Distillation with Mixture-of-Experts",
        "link_suffix": "/forum?id=z2WCyBO923",
        "link": "https://openreview.net/forum?id=z2WCyBO923",
        "pdf_link": "https://openreview.net/pdf?id=z2WCyBO923",
        "keywords": "dataset distillation, mixture-of-experts",
        "abstract": "The ever-growing size of datasets in deep learning presents a significant challenge in terms of training efficiency and computational cost. Dataset distillation (DD) has emerged as a promising approach to address this challenge by generating compact synthetic datasets that retain the essential information of the original data. However, existing DD methods often suffer from performance degradation when transferring distilled datasets across different network architectures (i.e. the model utilizing distilled dataset for further training is different from the one used in dataset distillation). To overcome this limitation, we propose a novel mixture-of-experts framework for dataset distillation. Our goal focuses on promoting diversity within the distilled dataset by distributing the distillation tasks to multiple expert models. Each expert specializes in distilling a distinct subset of the dataset, encouraging them to capture different aspects of the original data distribution. To further enhance diversity, we introduce a distance correlation minimization strategy to encourage the experts to learn distinct representations. Moreover, during the testing stage (where the distilled dataset is used for training a new model), the mixup-based fusion strategy is applied to better leverage the complementary information captured by each expert. Through extensive experiments, we demonstrate that our framework effectively mitigates the issue of cross-architecture performance degradation in dataset distillation, particularly in low-data regimes, leading to more efficient and versatile deep learning models while being trained upon the distilled dataset."
    },
    {
        "title": "Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism",
        "link_suffix": "/forum?id=cqTUJRlcLU",
        "link": "https://openreview.net/forum?id=cqTUJRlcLU",
        "pdf_link": "https://openreview.net/pdf?id=cqTUJRlcLU",
        "keywords": "benign overfitting, attention mechanism, prompt tuning, gradient descent",
        "abstract": "Modern over-parameterized neural networks can be trained to fit the training data perfectly while still maintaining a high generalization performance. This \u201cbenign overfitting\u201d phenomenon has been studied in a surge of recent theoretical work; however, most of these studies have been limited to linear models or two-layer neural networks. \nIn this work, we analyze benign overfitting in the token selection mechanism of the attention architecture, which characterizes the success of transformer models. We first show the existence of a benign overfitting solution and explain its mechanism in the attention architecture. Next, we discuss whether the model converges to such a solution, raising the difficulties specific to the attention architecture. We then present benign overfitting cases and not-benign overfitting cases by conditioning different scenarios based on the behavior of attention probabilities during training. To the best of our knowledge, this is the first study to characterize benign overfitting for the attention mechanism."
    },
    {
        "title": "Open-CK: A Large Multi-Physics Fields Coupling benchmarks in Combustion Kinetics",
        "link_suffix": "/forum?id=A23C57icJt",
        "link": "https://openreview.net/forum?id=A23C57icJt",
        "pdf_link": "https://openreview.net/pdf?id=A23C57icJt",
        "keywords": "Fire Dynamics, Spatio-temporal Data Mining, Fluid Modeling",
        "abstract": "In this paper, we use the Fire Dynamics Simulator (FDS) combined with the {\\fontfamily{lmtt}\\selectfont \\textit{supercomputer}} support to create a \\textbf{C}ombustion \\textbf{K}inetics (CK) dataset for machine learning and scientific research. This dataset captures the development of fires in industrial parks with high-precision Computational Fluid Dynamics (CFD) simulations. It includes various physical fields such as temperature and pressure, and covers multiple environmental combinations for exploring \\underline{multi-physics} field coupling phenomena. Additionally, we evaluate several advanced machine learning architectures across our {\\fontfamily{lmtt}\\selectfont {Open-CK}} benchmark using a substantial computational setup of 64 NVIDIA A100 GPUs: \\ding{182} vision backbone; \\ding{183} spatio-temporal predictive models; \\ding{184} operator learning frameworks. These architectures uniquely excel at handling complex physical field data. We also introduce three benchmarks to demonstrate their potential in enhancing the exploration of downstream tasks: (a) capturing continuous changes in combustion kinetics; (b) a neural partial differential equation solver for learning temperature fields and turbulence; (c) reconstruction of sparse physical observations. The Open-CK dataset and benchmarks aim to advance research in combustion kinetics driven by machine learning, providing a reliable baseline for developing and comparing cutting-edge technologies and models. We hope to further promote the application of deep learning in earth sciences. Our project is available at \\url{https://github.com/whscience/Open-CK}."
    },
    {
        "title": "Improving Ordinal Conformal Prediction by Stepwise Adaptive Posterior Alignment",
        "link_suffix": "/forum?id=g2Udwv77WN",
        "link": "https://openreview.net/forum?id=g2Udwv77WN",
        "pdf_link": "https://openreview.net/pdf?id=g2Udwv77WN",
        "keywords": "conformal prediction, ordinal classification",
        "abstract": "Ordinal classification (OC) is widely used in real-world applications to categorize instances into ordered discrete classes. In risk-sensitive scenarios, ordinal conformal prediction (OCP) is used to obtain a small contiguous prediction set containing ground-truth labels with a desired coverage guarantee. However, OC models often fail to accurately model the posterior distribution, which harms the prediction set obtained by OCP. Therefore, we introduce a new method called \\textit{Adaptive Posterior Alignment Step-by-Step} (APASS), which reduces the distribution discrepancy to improve the downstream OCP performance. It is designed as a versatile, plug-and-play solution that is easily integrated into any OC model before OCP. APASS first employs an attention-based estimator to adaptively estimate the variance of the posterior distribution using the information in the calibration set, then utilizes a stepwise temperature scaling algorithm to align the posterior variance predicted by OC models to the better variance estimation. Extensive evaluations on 10 real-world datasets demonstrate that APASS consistently boosts the OCP performance of 5 popular OC models."
    },
    {
        "title": "Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs",
        "link_suffix": "/forum?id=AvOhBgsE5R",
        "link": "https://openreview.net/forum?id=AvOhBgsE5R",
        "pdf_link": "https://openreview.net/pdf?id=AvOhBgsE5R",
        "keywords": "3D human motion, multimodal LLM, motion generation, conversational AI",
        "abstract": "While previous approaches to 3D human motion generation have achieved notable success, they often rely on extensive training and are limited to specific tasks. To address these challenges, we introduceMotion-Agent, an efficient conversational framework designed for general human motion generation, editing, and understanding. \nMotion-Agent employs an open-source pre-trained language model to develop a generative agent,MotionLLM, that bridges the gap between motion and text. This is accomplished by encoding and quantizing motions into discrete tokens that align with the language model's vocabulary. With only 1-3% of the model's parameters fine-tuned using adapters, MotionLLM delivers performance on par with diffusion models and other transformer-based methods trained from scratch. By integrating MotionLLM with GPT-4 without additional training, Motion-Agent is able to generate highly complex motion sequences through multi-turn conversations, a capability that previous models have struggled to achieve.\nMotion-Agent supports a wide range of motion-language tasks, offering versatile capabilities for generating and customizing human motion through interactive conversational exchanges."
    },
    {
        "title": "Causal Image Modeling for Efficient Visual Understanding",
        "link_suffix": "/forum?id=owXylt8hZj",
        "link": "https://openreview.net/forum?id=owXylt8hZj",
        "pdf_link": "https://openreview.net/pdf?id=owXylt8hZj",
        "keywords": "Visual backbone, causal modeling, Mamba",
        "abstract": "In this work, we present a comprehensive analysis of causal image modeling and introduce the Adventurer series models where we treat images as sequences of patch tokens and employ uni-directional language models to learn visual representations. This modeling paradigm allows us to process images in a recurrent formulation with linear complexity relative to the sequence length, which can effectively address the memory and computation explosion issues posed by high-resolution and fine-grained images. In detail, we introduce two simple designs that seamlessly integrate image inputs into the causal inference framework: a global pooling token placed at the beginning of the sequence and a flipping operation between every two layers. Extensive empirical studies demonstrate the significant efficiency and effectiveness of this causal image modeling paradigm. For example, our base-sized Adventurer model attains a competitive test accuracy of 84.0% on the standard ImageNet-1k benchmark with 216 images/s training throughput, which is 5.3 times more efficient than vision transformers to achieve the same result."
    },
    {
        "title": "MORPHING TOKENS DRAW STRONG MASKED IMAGE MODELS",
        "link_suffix": "/forum?id=d7q9IGj2p0",
        "link": "https://openreview.net/forum?id=d7q9IGj2p0",
        "pdf_link": "https://openreview.net/pdf?id=d7q9IGj2p0",
        "keywords": "Self-supervised learning",
        "abstract": "Masked image modeling (MIM) has emerged as a promising approach for training Vision Transformers (ViTs). The essence of MIM lies in the token-wise prediction of masked tokens, which aims to predict targets tokenized from images or generated by pre-trained models like vision-language models. While using tokenizers or pre-trained models are plausible MIM targets, they often offer spatially inconsistent targets even for neighboring tokens, complicating models to learn unified and discriminative representations. Our pilot study identifies spatial inconsistencies and suggests that resolving them can accelerate representation learning. Building upon this insight, we introduce a novel self-supervision signal called Dynamic Token Morphing (DTM), which dynamically aggregates contextually related tokens to yield contextualized targets, thereby mitigating spatial inconsistency. DTM is compatible with various SSL frameworks; we showcase improved MIM results by employing DTM, barely introducing extra training costs. Our method facilitates training by using consistent targets, resulting in 1) faster training and 2) reduced losses. Experiments on  ImageNet-1K and ADE20K demonstrate the superiority of our method compared with state-of-the-art, complex MIM methods. Furthermore, the comparative evaluation of the iNaturalists and fine-grained visual classification datasets further validates the transferability of our method on various downstream tasks."
    }
]