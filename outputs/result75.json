[{"title": "A Foundation Model for Weather and Climate", "link_suffix": "/forum?id=xCFdAN5DY3", "link": "https://openreview.net/forum?id=xCFdAN5DY3", "pdf_link": "https://openreview.net/pdf?id=xCFdAN5DY3", "keywords": "Foundation models; atmospheric physics; weather; climate; fine-tuning; super-resolution", "abstract": "Triggered by the realization that AI emulators can rival the performance of traditional numerical weather prediction models running on HPC systems, there is now an increasing number of large AI models that address use cases such as forecasting, downscaling, or nowcasting. While the parallel developments in the AI literature focus on foundation models -- models that can be effectively tuned to address multiple, different use cases -- the developments on the weather and climate side largely focus on single-use cases with particular emphasis on mid-range forecasting. We close this gap by introducing Prithvi WxC, a 2.3 billion parameter foundation model developed using 160 variables from the Modern-Era Retrospective Analysis for Research and Applications, Version 2 (MERRA-2). Prithvi WxC employs an encoder-decoder-based architecture, incorporating concepts from various recent transformer models to effectively capture both regional and global dependencies in the input data. The model has been designed to accommodate large token counts to model weather phenomena in different topologies at fine resolutions. Furthermore, it is trained with a mixed objective that combines the paradigms of masked reconstruction with forecasting. We test the model on a set of challenging downstream tasks namely: Autoregressive rollout forecasting, downscaling, gravity wave flux parameterization, and extreme events estimation.", "title_embedding_index": 3700, "title_abs_embedding_index": 3725}, {"title": "Martryoshka: Learning to Drive Black-Box LLMs with LLMs", "link_suffix": "/forum?id=fBkdjUnymd", "link": "https://openreview.net/forum?id=fBkdjUnymd", "pdf_link": "https://openreview.net/pdf?id=fBkdjUnymd", "keywords": "Large Language Model, Reasoning and Planning, LLM Controller", "abstract": "Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization.  Existing works aim to enhance LLM capabilities via domain-specific adaptation or in-context learning, which require additional training on accessible model parameters, an infeasible option for black-box LLMs.  To address this challenge, we introduce Martryoshika, a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with Martryoshika serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM.  Martryoshika is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on three diverse tasks demonstrate that Martryoshika effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks, including reasoning, planning, and personalization. By leveraging this pioneering controller-generator framework to mitigate dependence on model parameters, Martryoshika provides a transparent and practical solution for improving black-box LLMs through controllable multi-turn generation using white-box LLMs.", "title_embedding_index": 3701, "title_abs_embedding_index": 3726}, {"title": "How to Correctly Do Semantic Backpropagation on Language-based Agentic Systems", "link_suffix": "/forum?id=r1cbFEH0Df", "link": "https://openreview.net/forum?id=r1cbFEH0Df", "pdf_link": "https://openreview.net/pdf?id=r1cbFEH0Df", "keywords": "Agentic System, Large Language Model, Backpropagation, Computational Graph", "abstract": "Language-based agentic systems have shown great promise in recent years, transitioning from solving small-scale research problems to being deployed in challenging real-world tasks. However, optimizing these systems often requires substantial manual labor. Recent studies have demonstrated that these systems can be represented as computational graphs, enabling automatic optimization. Despite these advancements, most current efforts in Graph-based Agentic System Optimization (GASO) fail to properly assign feedback to the system\u2019s components given feedback on the system\u2019s output. To address this challenge, we formalize the concept of semantic backpropagation with semantic gradients\u2014a generalization that aligns several key optimization techniques, including reverse-mode automatic differentiation and the more recent TextGrad by exploiting the relationship among nodes with a common successor. This serves as a method for computing directional information about how changes to each component of an agentic system might improve the system\u2019s output. To use these gradients, we propose a method called semantic gradient descent which enables us to solve GASO effectively. Our results on both BIG-Bench Hard and GSM8K show that our approach outperforms existing state-of-the-art methods for solving GASO problems. A detailed ablation study on the LIAR dataset demonstrates the parsimonious nature of our method.", "title_embedding_index": 3702, "title_abs_embedding_index": 3727}, {"title": "Enforcing Interpretability in Time Series Transformers: A Concept Bottleneck Framework", "link_suffix": "/forum?id=A0mk2Wi68Y", "link": "https://openreview.net/forum?id=A0mk2Wi68Y", "pdf_link": "https://openreview.net/pdf?id=A0mk2Wi68Y", "keywords": "Interpretability, Concept Bottleneck Model, Centered Kernel Alignment, Autoformer, Time Series Transformer", "abstract": "There has been a recent push of research on Transformer-based models for long-term time series forecasting, even though they are inherently difficult to interpret and explain. While there is a large body of work on interpretability methods for various domains and architectures, the interpretability of Transformer-based forecasting models remains largely unexplored. To address this gap, we develop a framework based on Concept Bottleneck Models to enforce interpretability of time series Transformers. We modify the training objective to encourage a model to develop representations similar to predefined interpretable concepts. In our experiments, we enforce similarity using Centered Kernel Alignment, and the predefined concepts include time features and an interpretable, autoregressive surrogate model (AR). We apply the framework to the Autoformer model, and present an in-depth analysis for a variety of benchmark tasks. We find that the model performance remains mostly unaffected, while the model shows much improved interpretability. Additionally, interpretable concepts become local, which makes the trained model easily intervenable. As a proof of concept, we demonstrate a successful intervention in the scenario of a time shift in the data, which eliminates the need to retrain.", "title_embedding_index": 3703, "title_abs_embedding_index": 3728}, {"title": "HP3O: Hybrid-Policy Proximal Policy Optimization with Best Trajectory", "link_suffix": "/forum?id=PgR6fziYmJ", "link": "https://openreview.net/forum?id=PgR6fziYmJ", "pdf_link": "https://openreview.net/pdf?id=PgR6fziYmJ", "keywords": "Reinforcement learning, proximal policy optimization, hybrid policy, trajectory replay buffer, variance reduction", "abstract": "Proximal policy optimization (PPO) is one of the most popular state-of-the-art on-policy algorithms that has become a standard baseline in modern reinforcement learning with applications in numerous fields. Though it delivers stable performance with theoretical policy improvement guarantees, high variance and high sample complexity still remain critical challenges in on-policy algorithms. To alleviate these issues, we propose Hybrid-Policy Proximal Policy Optimization (HP3O), which utilizes a trajectory replay buffer to make efficient use of trajectories generated by recent policies. Particularly, the buffer applies the \"first in, first out\" (FIFO) strategy so as to keep only the recent trajectories to attenuate the data distribution drift. A batch consisting of the trajectory with the best return and other randomly sampled ones from the buffer is used for updating the policy networks. The strategy helps the agent to improve its capability on top of the most recent best performance and in turn reduce variance empirically. We theoretically construct the policy improvement guarantees for the proposed algorithm. HP3O is validated and compared against several baseline algorithms using multiple continuous control environments. Our code is available here.", "title_embedding_index": 3704, "title_abs_embedding_index": 3729}, {"title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training", "link_suffix": "/forum?id=L9eBxTCpQG", "link": "https://openreview.net/forum?id=L9eBxTCpQG", "pdf_link": "https://openreview.net/pdf?id=L9eBxTCpQG", "keywords": "Gradient Spikes, Spike-Aware Adam, LLMs", "abstract": "Large Language Models (LLMs) have demonstrated exceptional performance across diverse tasks, yet their training remains highly resource intensive and susceptible to critical challenges such as training instability. A predominant source of this instability stems from gradient and loss spikes, which disrupt the learning process, often leading to costly interventions like checkpoint recovery and experiment restarts, further amplifying inefficiencies. This paper presents a comprehensive investigation into gradient spikes observed during LLM training, revealing their prevalence across multiple architectures and datasets. Our analysis shows that these spikes can be up to 1000\u00d7 larger than typical gradients, substantially deteriorating model performance. To address this issue, we propose Spike-Aware Adam with Momentum Reset (SPAM), a novel optimizer designed to counteract gradient spikes through momentum reset and spike-aware gradient clipping. Extensive experiments, including both pre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam and its variants across a range of model scales. Additionally, SPAM facilitates memory-efficient training by enabling sparse momentum, where only a subset of momentum terms are maintained and updated. When operating under memory constraints, SPAM outperforms state-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our work underscores the importance\nof mitigating gradient spikes in LLM training and introduces an effective optimization strategy that enhances both training stability and resource efficiency at scale. Code is submitted.", "title_embedding_index": 3705, "title_abs_embedding_index": 3730}, {"title": "Probing the Latent Hierarchical Structure of Data via Diffusion Models", "link_suffix": "/forum?id=0GzqVqCKns", "link": "https://openreview.net/forum?id=0GzqVqCKns", "pdf_link": "https://openreview.net/pdf?id=0GzqVqCKns", "keywords": "data structure, hierarchical compositionality, diffusion models, statistical physics, phase transition", "abstract": "High-dimensional data must be highly structured to be learnable. Although the compositional and hierarchical nature of data is often put forward to explain learnability, quantitative measurements establishing these properties are scarce. Likewise, accessing the latent variables underlying such a data structure remains a challenge. Forward-backward experiments in diffusion-based models, where a datum is noised and then denoised, are a promising tool to achieve these goals. We predict in simple hierarchical models that, in this process, changes in data occur by correlated chunks, with a length scale that diverges at a noise level where a phase transition is known to take place. Remarkably, we confirm this prediction in both text and image datasets using state-of-the-art diffusion models. Our results suggest that forward-backward experiments are informative on the nature of latent variables, and that the effect of changing deeper ones is revealed near the transition.", "title_embedding_index": 3706, "title_abs_embedding_index": 3731}, {"title": "Causal Frameworks and Feature Discrepancy Loss: Addressing Data Scarcity and Enhancing Medical Image Segmentation", "link_suffix": "/forum?id=b3VzHRXrXh", "link": "https://openreview.net/forum?id=b3VzHRXrXh", "pdf_link": "https://openreview.net/pdf?id=b3VzHRXrXh", "keywords": "causal reasoning, bioemdical image segmentation, data dilemma", "abstract": "Data scarcity poses a significant challenge for deep learning models in medical imaging, particularly for training and generalization. Previous studies have demonstrated the efficacy of data pooling from various sources, facilitating the analysis of weak but significant correlations between imaging data and disease incidence. This approach is often constrained by strict data-sharing protocols among institutions, resulting in models reliant on external data sources. In this work, we address the issue of data scarcity by leveraging the available data for segmentation tasks across various medical imaging modalities. Based on our observation that samples with minimal foreground-background feature differences often demonstrate inadequate segmentation performance, we propose a causal-inspired foreground-background feature discrepancy penalty function, which improves feature separation and alleviates segmentation difficulties caused by homogeneous pixel distributions. The proposed feature discrepancy loss is mathematically grounded, with a lower bound defined by the negative logarithm of the Dice coefficient, suggesting that increased feature separation correlates with improved Dice scores. To further validate our approach, we introduce a novel ultrasound dataset for triple-negative breast cancer (TNBC), and we evaluate the method across three state-of-the-art segmentation architectures to demonstrate competitive performance. In addition, the results highlight the robustness of our method in mitigating performance decrease due to distribution shifts when new, differently distributed data batches are introduced.", "title_embedding_index": 3707, "title_abs_embedding_index": 3732}, {"title": "DiverseAgentEntropy: Quantifying Black-Box LLM Uncertainty through Diverse Perspectives and Multi-Agent Interaction", "link_suffix": "/forum?id=AJAStQYZaL", "link": "https://openreview.net/forum?id=AJAStQYZaL", "pdf_link": "https://openreview.net/pdf?id=AJAStQYZaL", "keywords": "Large language Model, Hallucination Detection, Uncertainty Quantification, MultiAgent Interaction", "abstract": "Quantifying the uncertainty in the factual parametric knowledge of Large Language Models (LLMs), especially in a black-box setting, poses a significant challenge. Existing methods, which gauge a model\u2019s uncertainty through evaluating self-consistency in responses to the original query, do not always capture true uncertainty. Models might respond consistently to the origin query with a wrong answer, yet respond correctly to varied questions from different perspectives about the same query, and vice versa. In this paper, we propose a novel method, DiverseAgentEntropy, for evaluating a model's uncertainty using multi-agent interaction under the assumption that if a model is certain, it should consistently recall the answer to the original query across a diverse collection of questions about the same original query. We further implement an abstention policy to withhold responses when uncertainty is high. Our method offers a more accurate prediction of the model's reliability by detecting hallucinations, improving upon self-consistency-based uncertainty methods by 2.5%. Additionally, it demonstrates that existing models often fail to consistently retrieve the correct answer to the same query under diverse varied questions.", "title_embedding_index": 3708, "title_abs_embedding_index": 3733}, {"title": "Features are fate: a theory of transfer learning in high-dimensional regression", "link_suffix": "/forum?id=Gc2qkiYUkh", "link": "https://openreview.net/forum?id=Gc2qkiYUkh", "pdf_link": "https://openreview.net/pdf?id=Gc2qkiYUkh", "keywords": "transfer learning, deep linear networks, fine tuning, random matrix theory, high dimensional statistics", "abstract": "With the emergence of large-scale pre-trained neural networks, methods to adapt such \"foundation\" models to data-limited downstream tasks have become a necessity.\nFine-tuning, preference optimization, and transfer learning have all been successfully employed for these purposes when the target task closely resembles the source task, but a precise theoretical understanding of ``task similarity'' is still lacking. \nWhile conventional wisdom suggests that simple measures of similarity between source and target distributions, such as $\\phi$-divergences or integral probability metrics, can directly predict the success of transfer, we prove the surprising fact that, in general, this is not the case.\nWe adopt, instead, a \\emph{feature-centric} viewpoint on transfer learning and establish a number of theoretical results that demonstrate that when the target task is well represented by the feature space of the pre-trained model, transfer learning outperforms training from scratch.\nWe study deep linear networks as a minimal model of transfer learning in which we can analytically characterize the transferability phase diagram as a function of the target dataset size and the feature space overlap.\nFor this model, we establish rigorously that when the feature space overlap between the source and target tasks is sufficiently strong, both linear transfer and fine-tuning improve performance, especially in the low data limit. \nThese results build on an emerging understanding of feature learning dynamics in deep linear networks, and we demonstrate numerically that the rigorous results we derive for the linear case also apply to nonlinear networks.", "title_embedding_index": 3709, "title_abs_embedding_index": 3734}, {"title": "Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustnes", "link_suffix": "/forum?id=daUQ7vmGap", "link": "https://openreview.net/forum?id=daUQ7vmGap", "pdf_link": "https://openreview.net/pdf?id=daUQ7vmGap", "keywords": "Dynamic Sparse Training, Image Corruption Robustnes", "abstract": "It is generally perceived that Dynamic Sparse Training opens the door to a new era of scalability and efficiency for artificial neural networks at, perhaps, some costs in accuracy performance for the classification task. At the same time, Dense Training is widely accepted as being the \"de facto\" approach to train artificial neural networks if one would like to maximize their robustness against image corruption. In this paper, we question this general practice. Consequently, \\textit{we claim that}, contrary to what is commonly thought, the Dynamic Sparse Training methods can consistently outperform Dense Training in terms of robustness accuracy, particularly if the efficiency aspect is not considered as a main objective (i.e., sparsity levels between 10% and up to 50%), without adding (or even reducing) resource cost. We validate our claim on two types of data, images and videos, using several traditional and modern deep learning architectures for computer vision and three widely studied Dynamic Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of Dynamic Sparse Training and open new possibilities in improving deep learning robustness beyond the current state of the art.", "title_embedding_index": 3710, "title_abs_embedding_index": 3735}, {"title": "Unlocking Point Processes through Point Set Diffusion", "link_suffix": "/forum?id=4anfpHj0wf", "link": "https://openreview.net/forum?id=4anfpHj0wf", "pdf_link": "https://openreview.net/pdf?id=4anfpHj0wf", "keywords": "Generative Model, Diffusion Model, Set Model, Point Sets, Forecasting, Density Estimation, Spatial, Temporal, Probabilistic Models", "abstract": "Point processes model the distribution of random point sets in mathematical spaces, such as spatial and temporal domains, with applications in fields like seismology, neuroscience, and economics. Existing statistical and machine learning models for point processes are predominantly constrained by their reliance on the characteristic intensity function, introducing an inherent trade-off between efficiency and flexibility. In this paper, we introduce Point Set Diffusion, a diffusion-based latent variable model that can represent arbitrary point processes on general metric spaces without relying on the intensity function. By directly learning to stochastically interpolate between noise and data point sets, our approach enables efficient, parallel sampling and flexible generation for complex conditional tasks defined on the metric space. Experiments on synthetic and real-world datasets demonstrate that Point Set Diffusion achieves state-of-the-art performance in unconditional and conditional generation of spatial and spatiotemporal point processes while providing up to orders of magnitude faster sampling than autoregressive baselines.", "title_embedding_index": 3711, "title_abs_embedding_index": 3736}, {"title": "Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity", "link_suffix": "/forum?id=vVxeFSR4fU", "link": "https://openreview.net/forum?id=vVxeFSR4fU", "pdf_link": "https://openreview.net/pdf?id=vVxeFSR4fU", "keywords": "Representation Similarity, Saturation Event, Early Exit", "abstract": "Analyzing the similarity of internal representations within and across different models has been an important technique for understanding the behavior of deep neural networks. Most existing methods for analyzing the similarity between representations of high dimensions, such as those based on Centered Kernel Alignment (CKA), rely on statistical properties of the representations for a set of data points. In this paper, we focus on transformer models and study the similarity of representations between the hidden layers of individual transformers. In this context, we show that a simple sample-wise cosine similarity metric is capable of capturing the similarity and aligns with the complicated CKA. Our experimental results on common transformers reveal that representations across layers are positively correlated, with similarity increasing when layers get closer. We provide a theoretical justification for this phenomenon under the geodesic curve assumption for the learned transformer, a property that may approximately hold for residual networks. We then show that an increase in representation similarity implies an increase in predicted probability when directly applying the last-layer classifier to any hidden layer representation. This offers a justification for {\\it saturation events}, where the model's top prediction remains unchanged across subsequent layers, indicating that the shallow layer has already learned the necessary knowledge. We then propose an aligned training method to improve the effectiveness of shallow layer by enhancing the similarity between internal representations, with trained models that enjoy the following properties: (1) more early saturation events, (2) layer-wise accuracies monotonically increase and reveal the minimal depth needed for the given task, (3) when served as multi-exit models, they achieve on-par performance with standard multi-exit architectures which consist of additional classifiers designed for early exiting in shallow layers. To our knowledge, our work is the first to show that one common classifier is sufficient for multi-exit models. We conduct experiments on both vision and NLP tasks to demonstrate the performance of the proposed aligned training.", "title_embedding_index": 3712, "title_abs_embedding_index": 3737}, {"title": "Language Agents Meet Causality -- Bridging LLMs and Causal World Models", "link_suffix": "/forum?id=y9A2TpaGsE", "link": "https://openreview.net/forum?id=y9A2TpaGsE", "pdf_link": "https://openreview.net/pdf?id=y9A2TpaGsE", "keywords": "Large Language Models, Causality, Causal Representation Learning, Language Agents, Planning", "abstract": "Large Language Models (LLMs) have recently shown great promise in planning and reasoning applications. These tasks demand robust systems, which arguably require a causal understanding of the environment. While LLMs can acquire and reflect common sense causal knowledge from their pretraining data, this information is often incomplete, incorrect, or inapplicable to a specific environment. In contrast, causal representation learning (CRL) focuses on identifying the underlying causal structure within a given environment. We propose a framework that integrates CRLs with LLMs to enable causally-aware reasoning and planning. This framework learns a causal world model, with causal variables linked to natural language expressions. This mapping provides LLMs with a flexible interface to process and generate descriptions of actions and states in text form. Effectively, the causal world model acts as a simulator that the LLM can query and interact with. We evaluate the framework on causal inference and planning tasks across temporal scales and environmental complexities. Our experiments demonstrate the effectiveness of the approach, with the causally-aware method outperforming LLM-based reasoners, especially for longer planning horizons.", "title_embedding_index": 3713, "title_abs_embedding_index": 3738}, {"title": "Distributed Epigraph Form Multi-Agent Safe Reinforcement Learning", "link_suffix": "/forum?id=oQUtBLM8Bo", "link": "https://openreview.net/forum?id=oQUtBLM8Bo", "pdf_link": "https://openreview.net/pdf?id=oQUtBLM8Bo", "keywords": "Safe multi-agent systems, reinforcement learning, optimal control, epigraph form", "abstract": "Most existing safe multi-agent reinforcement learning (MARL) algorithms consider the constrained Markov decision process (CMDP) problem, which targets bringing the mean of constraint violation below a user-defined threshold.\nHowever, as observed by existing works albeit for the single-agent case, CMDP algorithms suffer from unstable training when the constraint threshold is zero.\nThis paper proposesEFMARL, a novel MARL algorithm that improves upon the problems faced in the zero constraint threshold setting by extending theepigraph form, a technique to perform constrained optimization, to the centralized training and distributed execution (CTDE) paradigm.\nWe validate our approach in different Multi-Particle Environments and Safe Multi-agent MuJoCo environments with varying numbers of agents. Simulation results show that our algorithm achieves stable training and the best performance while satisfying constraints: it is as safe as the safest baseline that has significant performance loss, and achieves similar performance as baselines that prioritize performance but violate safety constraints.", "title_embedding_index": 3714, "title_abs_embedding_index": 3739}, {"title": "KEA: Keeping Exploration Alive by Proactively Coordinating Exploration Strategies in Curiosity-driven Exploration", "link_suffix": "/forum?id=zG2vcC1l1f", "link": "https://openreview.net/forum?id=zG2vcC1l1f", "pdf_link": "https://openreview.net/pdf?id=zG2vcC1l1f", "keywords": "Reinforcement Learning, Curiosity-based Exploration, Sparse Reward, Soft Actor-Critic", "abstract": "In continuous control tasks, Soft Actor-Critic (SAC) has achieved notable success by balancing exploration and exploitation. However, SAC struggles in sparse reward environments, where infrequent rewards hinder efficient exploration. While curiosity-driven exploration methods help address this issue by encouraging the agent to explore novel states, they introduce challenges, such as the difficulty of setting an optimal reward scale and managing the interaction between curiosity-based exploration and SAC\u2019s stochastic policy. These complexities often lead to inefficient exploration or premature convergence and make balancing exploration-exploitation challenging. In this paper, we propose KEA (Keeping Exploration Alive) to tackle the inefficiencies in balancing the exploration-exploitation trade-off when combining SAC with curiosity-based methods. KEA introduces an additional co-behavior agent that works alongside SAC and a switching mechanism to facilitate proactive coordination between exploration strategies from the co-behavior agent and the SAC agent with curiosity-based exploration. This coordination allows the agent to maintain stochasticity in high-novelty regions, preventing premature convergence and enhancing exploration efficiency. We first analyze the difficulty of balancing exploration-exploitation when combining SAC with curiosity-based methods in a 2D grid environment. We then evaluate KEA on sparse reward control tasks from the DeepMind Control Suite and compare against two state-of-the-art curiosity-based exploration baselines \u2014 Random Network Distillation (RND) and NovelD. KEA improves episodic rewards by up to 119% over RND and 28% over NovelD, significantly improving learning efficiency and robustness in sparse reward environments.", "title_embedding_index": 3715, "title_abs_embedding_index": 3740}, {"title": "Learning with Multi-Group Guarantees for Clusterable Subpopulations", "link_suffix": "/forum?id=tEM1u9VT2W", "link": "https://openreview.net/forum?id=tEM1u9VT2W", "pdf_link": "https://openreview.net/pdf?id=tEM1u9VT2W", "keywords": "multicalibration, fairness, clustering", "abstract": "A canonical desideratum for prediction problems is that performance guarantees should hold not just on average over the population, but also for meaningful subpopulations within the overall population. \nBut what constitutes a meaningful subpopulation?\nIn this work, we take the perspective that relevant subpopulations should be defined with respect to the clusters that naturally emerge from the distribution of individuals for which predictions are being made. \nIn this perspective, a population refers to a mixture model whose components constitute the relevant subpopulations.\nWe suggest two formalisms for capturing per-subgroup guarantees: first, by attributing each individual to the component from which they were most likely drawn, given their features; and second, by attributing each individual to all components in proportion to their relative likelihood of having been drawn from each component.\nUsing online calibration for Gaussian mixture models as a case study, we study a multi-objective algorithm that provides guarantees for each of these formalisms by handling all plausible underlying subpopulation structures simultaneously, and achieve a $O(T^{1/2})$ rate even when the subpopulations are not well-separated.\nIn comparison, the more naturalcluster-then-predictapproach that first recovers the structure of the subpopulations and then makes predictions suffers from a $O(T^{2/3})$ rate and requires the subpopulations to be separable.\nAlong the way, we prove that providing per-subgroup calibration guarantees for underlying clusters can be easier than learning the clusters: separation between median subgroup features is required for the latter but not the former.", "title_embedding_index": 3716, "title_abs_embedding_index": 3741}, {"title": "From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question-Answering", "link_suffix": "/forum?id=JV8zULNh24", "link": "https://openreview.net/forum?id=JV8zULNh24", "pdf_link": "https://openreview.net/pdf?id=JV8zULNh24", "keywords": "microtheory, textual entailment, knowledge representation, natural language reasoning, text retrieval, automatic knowledge base construction", "abstract": "Recent reasoning methods (e.g., chain-of-thought, entailment reasoning) help users understand how language models (LMs) answer a single question, but they do little to reveal the LM\u2019soverall understanding, or \u201ctheory\u201d, about the question\u2019stopic, making it still hard to trust the model. Our goal is to materialize such theories - here calledmicrotheories(a linguistic analog of logical microtheories (Blair et al., 1992)) - as a set of sentences encapsulating an LM\u2019s core knowledge about a topic. These statements systematically work together to entail answers to asetof questions to both engender trust and improve performance. Our approach is to first populate a knowledge store with (model-generated) sentences that entail answers to training questions, and then distill those down to a core microtheory which is concise, general, and non-redundant. We show that, when added to a general corpus (e.g., Wikipedia), microtheories can supply critical, topical information not necessarily present in the corpus, improving both a model\u2019s ability to ground its answers to verifiable knowledge (i.e., show how answers are systematically entailed by documents in the corpus, fully grounding up to +8% more answers), and the accuracy of those grounded answers (up to +8% absolute). We also show that, in a human evaluation in the medical domain, our distilled microtheories contain a significantly higher concentration of topically critical facts than the non-distilled knowledge store. Finally, we show we can quantify the coverage of a microtheory for a topic (characterized by a dataset) using a notion ofp-relevance. Together, these suggest that microtheories are an efficient distillation of an LM\u2019s topic-relevant knowledge, that they can usefully augment existing corpora, and can provide both performance gains and an interpretable, verifiable window into the model\u2019s knowledge of a topic.", "title_embedding_index": 3717, "title_abs_embedding_index": 3742}, {"title": "Annotation Efficiency: Identifying Hard Samples via Blocked Sparse Linear Bandits", "link_suffix": "/forum?id=70ul28Zwwp", "link": "https://openreview.net/forum?id=70ul28Zwwp", "pdf_link": "https://openreview.net/pdf?id=70ul28Zwwp", "keywords": "High Dimensional Linear Bandits, Annotation Efficiency, Sparse Recovery, Online Learning", "abstract": "This paper considers the problem of annotating datapoints using an expert with only a few annotation rounds in alabel-scarcesetting. We propose soliciting reliable feedback on difficulty in annotating a datapoint from the expert in addition to ground truth label. Existing literature in active learning or coreset selection turns out to be less relevant to our setting since they presume the existence of a reliable trained model, which is absent in the label-scarce regime. However, the literature on coreset selection emphasizes the presence of difficult data points in the training set to perform supervised learning in downstream tasks (Mindermann\net al., 2022). Therefore, for a given fixed annotation budget of $\\mathsf{T}$ rounds, we model the sequential decision-making problem of which (difficult) datapoints to choose for annotation in a sparse linear bandits framework with the constraint that no arm can be pulled more than once (blocking constraint). With mild assumptions on the datapoints, our (computationally efficient) Explore-Then-Commit algorithmBSLBachieves a regret guarantee of $\\widetilde{\\mathsf{O}}(k^{\\frac{1}{3}} \\mathsf{T}^{\\frac{2}{3}}   +k^{-\\frac{1}{2}} \\beta_k + k^{-\\frac{1}{12}} \\beta_k^{\\frac{1}{2}}\\mathsf{T}^{\\frac{5}{6}})$ where the unknown parameter vector has tail magnitude $\\beta_k$ at sparsity level $k$. To this end, we show offline statistical guarantees of Lasso estimator with mild Restricted Eigenvalue (RE) condition that is also robust to sparsity. Finally, we propose a meta-algorithmC-BSLBthat does not need knowledge of the optimal sparsity parameters at a no-regret cost.  We demonstrate the efficacy of ourBSLBalgorithm for annotation in the label-scarce setting for an image classification task on the PASCAL-VOC dataset, where we use real-world annotation difficulty scores.", "title_embedding_index": 3718, "title_abs_embedding_index": 3743}, {"title": "vVLM: Exploring Visual Reasoning in VLMs against Language Priors", "link_suffix": "/forum?id=lCqNxBGPp5", "link": "https://openreview.net/forum?id=lCqNxBGPp5", "pdf_link": "https://openreview.net/pdf?id=lCqNxBGPp5", "keywords": "Vision Language Model", "abstract": "The intersection of vision and language presents challenges, as vision language models (VLMs) may exploit language biases, reducing their reliance on visual input. To examine this, we introduce a benchmark that prioritizes visual reasoning in visual question answering (VQA). Our dataset generated using image generative models consists of visually intricate images that vary in texture, shape, conceptual combinations, hallucinated components, and proverb. Each question is paired with three answers and three corresponding images: one that can be easily inferred from the text, and two that must rely on visual cues. While humans can effortlessly discern all three answers, existing VLMs struggle as GPT-4o achieving only 66.17%. Furthermore, we propose enhancing VLMs by self-generating VQA pairs and images via pre-trained image generation and editing models. These images are then subjected to pixel-level and semantic corruptions, creating good-bad image pairs for DPO training. This approach encourages models to rely more on visual input, and has shown to improve performance on LLaVA-v1.5 and Cambrian.", "title_embedding_index": 3719, "title_abs_embedding_index": 3744}, {"title": "Realtime Reinforcement Learning: Towards Rapid Asynchronous Deployment of Large Models", "link_suffix": "/forum?id=fXb9BbuyAD", "link": "https://openreview.net/forum?id=fXb9BbuyAD", "pdf_link": "https://openreview.net/pdf?id=fXb9BbuyAD", "keywords": "Realtime Environments, Asynchronous Algorithms, Time Discretization, Real World Deployment, Deep Reinforcement Learning", "abstract": "Realtime environments change even as agents perform action inference and learning, thus requiring high interaction frequencies to effectively minimize long-term regret. However, recent advances in machine learning involve larger neural networks with longer inference times, raising questions about their applicability in realtime systems where quick reactions are crucial. We present an analysis of lower bounds on regret in realtime environments to show that minimizing long-term regret is generally impossible within the typical sequential interaction and learning paradigm, but often becomes possible when sufficient asynchronous compute is available. We propose novel algorithms for staggering asynchronous inference processes to ensure that actions are taken at consistent time intervals, and demonstrate that use of models with high action inference times is only constrained by the environment's effective stochasticity over the inference horizon, and not by action frequency. Our analysis shows that the number of inference and learning processes needed scales linearly with increasing inference times while enabling use of models that are multiple orders of magnitude larger than existing approaches when learning from a realtime simulation of Game Boy games such as Pokemon and Tetris.", "title_embedding_index": 3720, "title_abs_embedding_index": 3745}, {"title": "Lossgate: Incomplete Information and Misaligned Incentives Hinder Regulation of Societal Risks in Machine Learning", "link_suffix": "/forum?id=ZDoN4W5s8d", "link": "https://openreview.net/forum?id=ZDoN4W5s8d", "pdf_link": "https://openreview.net/pdf?id=ZDoN4W5s8d", "keywords": "ml regulation, fairness, privacy", "abstract": "Regulators seek to curb the societal risks of machine learning; a common aim is to protect the public from excessive privacy violations or bias in models. In the status quo, regulators and companies independently evaluate societal risk. We find that discrepancies in these evaluations can be either a detriment or an advantage for companies. To abide by regulation, a company needs to conservatively evaluate risk: it should train its model such that risk remains below the acceptable threshold-even if the regulator's evaluation returns higher risk measurements. This decreases model utility (up to 8%, in our experiments). Conversely, when the regulator's measurements are consistently lower than theirs, we find that a company can behave strategically and game regulation to train more accurate models. We call this Lossgate, an allusion to Dieselgate in environmental regulation: Volkswagen produced cars that limited their emissions when being subjected to a regulator's emissions measurement. To model incomplete information and the misaligned incentives that explain Lossgate, we leverage game theory. We obtain SpecGame, a model for regulator-company interactions which allows us to estimate the excessive risk that results from the strategic behavior observed in Lossgate. We show Lossgate costs up to 96% higher compared to collaborative regulation in the sum cost for all players.", "title_embedding_index": 3721, "title_abs_embedding_index": 3746}, {"title": "Training Semi-Supervised Deep Learning Models with Heuristic Early Stopping Rules", "link_suffix": "/forum?id=aXSxSu3fvg", "link": "https://openreview.net/forum?id=aXSxSu3fvg", "pdf_link": "https://openreview.net/pdf?id=aXSxSu3fvg", "keywords": "semi-supervised deep learning, neural network, convergence, generalizability, predictive modeling, model optimization", "abstract": "Semi-supervised learning (SSL), especially when combined with deep learning (DL) models, is a useful  technique when there is a substantial amount of unlabeled data. This is particularly relevant in healthcare applications, such as mHealth, where data is often collected through smartphones. Labels are typically obtained via self-reported questions delivered by the device and tend to have a high rate of non-response i.e., missing labels. Despite its benefit, there is a lack of objective methodology on how to train semi-supervised deep learning (SSDL) models. In this study, we propose a framework for early-stopping in SSDL that terminates learning to prevent overfitting and before the performance starts to deteriorate. Our approach focuses on three aspects: model stability, generalizability, and high-confidence pseudo-label (i.e., label assigned to unlabeled data during SSL). We first monitor changes in learned weights of the model to assess convergence, using weight stabilization. We also track cross-entropy loss, identifying which iteration of the SSL algorithm minimizes validation loss and improves generalizability. Lastly, we use a sliding window method to assess our confidence in the pseudo-labels, retaining only the most reliable labels during training. Combining these criteria, this SSDL framework can be used to train deep learning models in the context of SSL with an objective criteria that prevents overfitting and improves generalizability. We apply this SSDL training strategy to mHealth data (device sensor data and self-reported data) collected from participants in a clinical trial, which consists of 4,700 observations, 62% of which are unlabeled. Using this objective early stopping criteria for training, we achieve improvements in accuracy and F1 scores, compared to the benchmark model where the early stopping criteria is not applied.", "title_embedding_index": 3722, "title_abs_embedding_index": 3747}, {"title": "On the Transfer of Object-Centric Representation Learning", "link_suffix": "/forum?id=bSq0XGS3kW", "link": "https://openreview.net/forum?id=bSq0XGS3kW", "pdf_link": "https://openreview.net/pdf?id=bSq0XGS3kW", "keywords": "representation learning, object-centric learning, object-centric representation learning, unsupervised learning, transfer, zero-shot, generalization", "abstract": "The goal of object-centric representation learning is to decompose visual scenes into a structured representation that isolates the entities into individual vectors. Recent successes have shown that object-centric representation learning can be scaled to real-world scenes by utilizing features from pre-trained foundation models like DINO. However, so far, these object-centric methods have mostly been applied in-distribution, with models trained and evaluated on the same dataset. This is in contrast to the underlying foundation models, which have been shown to be applicable to a wide range of data and tasks. Thus, in this work, we answer the question of whether current real-world capable object-centric methods exhibit similar levels of transferability by introducing a benchmark comprising seven different synthetic and real-world datasets. We analyze the factors influencing performance under transfer and find that training on diverse real-world images improves generalization to unseen scenarios. Furthermore, inspired by the success of task-specific fine-tuning in foundation models, we introduce a novel fine-tuning strategy to adapt pre-trained vision encoders for the task of object discovery. We find that the proposed approach results in state-of-the-art performance for unsupervised object discovery, exhibiting strong zero-shot transfer to unseen datasets.", "title_embedding_index": 3723, "title_abs_embedding_index": 3748}, {"title": "ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities", "link_suffix": "/forum?id=lfPkGWXLLf", "link": "https://openreview.net/forum?id=lfPkGWXLLf", "pdf_link": "https://openreview.net/pdf?id=lfPkGWXLLf", "keywords": "language models, evaluation, benchmark, forecasting, LLM, decision-making", "abstract": "Forecasts of future events are essential inputs into informed decision-making.\nMachine learning (ML) systems have the potential to deliver forecasts at scale, but\nthere is no framework for evaluating the accuracy of ML systems on a standardized\nset of forecasting questions. To address this gap, we introduce ForecastBench: a\ndynamic benchmark that evaluates the accuracy of ML systems on an automatically\ngenerated and regularly updated set of 1,000 forecasting questions. To avoid any\npossibility of data leakage, ForecastBench is comprised solely of questions about\nfuture events that have no known answer at the time of submission. We quantify\nthe capabilities of current ML systems by collecting forecasts from expert (human)\nforecasters, the general public, and LLMs on a random subset of questions from\nthe benchmark (N = 200). While LLMs have achieved super-human performance\non many benchmarks, they perform less well here: expert forecasters outperform\nthe top-performing LLM (p-value = 0.01). We display system and human scores\nin a public leaderboard atwww.anonymousurl.org.", "title_embedding_index": 3724, "title_abs_embedding_index": 3749}]