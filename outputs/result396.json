[{"title": "Tokens on Demand: Token Condensation as Training-free Test-time Adaptation", "link_suffix": "/forum?id=EK1yOLL7GA", "link": "https://openreview.net/forum?id=EK1yOLL7GA", "pdf_link": "https://openreview.net/pdf?id=EK1yOLL7GA", "keywords": "test-time adaptation", "abstract": "In this work, we introduce Token Condensation as Adaptation (TCA), a training-free approach designed to mitigate distribution shifts encountered by vision-language models (VLMs) during test-time inference. TCA bridges distribution gaps at the patch level by condensing image tokens that exhibit low attentiveness to the <cls> token. Recognizing the <cls> token may correspond to universal concepts, TCA identifies and tracks the most reliable <cls> tokens that align specifically with target classes from historical data streams. To achieve this, we propose a context token reservoir (CTR), which retains tokens with the lowest uncertainty as ``anchors\" to guide the preservation of class-relevant tokens during inference. These anchors, in turn, act as token-level classifiers to correct VLM predictions and improve visual-text alignment. Utilizing anchors sampled from CTR, TCA condenses tokens through two operations: (1) pruning class-irrelevant tokens that consistently rank low across all attention heads to reach cross-head consensus on their irrelevance, and (2) merging the remaining class-ambiguous tokens into representative centers using coreset selection, maintaining linear computational complexity. As the first method to explore token efficiency in test-time adaptation, TCA consistently demonstrates superior performance across cross-dataset and out-of-distribution adaptation tasks, reducing GFLOPs by 12.2% to 48.9% while achieving accuracy improvements up to 21.4% against the strongest baseline without introducing additional parameters.", "title_embedding_index": 19750, "title_abs_embedding_index": 19775}, {"title": "PATTERN MATCHING-BASED OUT-OF-DISTRIBUTION DETECTION FOR MULTI-LABEL NODE CLASSIFICATION", "link_suffix": "/forum?id=hpDiwfGrrX", "link": "https://openreview.net/forum?id=hpDiwfGrrX", "pdf_link": "https://openreview.net/pdf?id=hpDiwfGrrX", "keywords": "Out-of-distribution", "abstract": "Graph neural networks (GNNs) have achieved dominant performance in various prediction tasks on graphs. When deploying GNNs in the real world, estimating the possibility of out-of-distribution (OOD) testing samples becomes a crucial safety concern. Although some research has investigated the graph OOD detection problem, most have concentrated on single-label classification scenarios, aspecific case of the more general multi-label classification, which has broader applications, such as in social networks where nodes can represent users with multiple interests or attributes. In this paper, we first introduce and define the multi-label graph OOD detection problem and propose a simple yet effective pattern matching-based OOD detection method to address it. In particular, our method utilizes feature pattern matching and label pattern matching to obtain two matching scores. By incorporating topological structure adjustment, we ultimately derive confidence scores, serving as indicators of the likelihood that a test sample is an OOD instances. We conduct extensive comparisons with existing OOD detection methods in the context of multi-label graphs. The results show that our method achieves an impressive 7.61% reduction in FPR95 compared to the leading baselines, setting a new state-of-the-art. Furthermore, our approach can servas a benchmark for OOD detection on multi-label graphs.", "title_embedding_index": 19751, "title_abs_embedding_index": 19776}, {"title": "Perm: A Parametric Representation for Multi-Style 3D Hair Modeling", "link_suffix": "/forum?id=WKfb1xGXGx", "link": "https://openreview.net/forum?id=WKfb1xGXGx", "pdf_link": "https://openreview.net/pdf?id=WKfb1xGXGx", "keywords": "Hair Modeling, Parametric Models, Generative Models", "abstract": "We present Perm, a learned parametric representation of human 3D hair designed to facilitate various hair-related applications. Unlike previous work that jointly models the global hair structure and local curl patterns, we propose to disentangle them using a PCA-based strand representation in the frequency domain, thereby allowing more precise editing and output control. Specifically, we leverage our strand representation to fit and decompose hair geometry textures into low- to high-frequency hair structures. These decomposed textures are later parameterized with different generative models, emulating common stages in the hair grooming process. We conduct extensive experiments to validate the architecture design of \\textsc{Perm}, and finally deploy the trained model as a generic prior to solve task-agnostic problems, further showcasing its flexibility and superiority in tasks such as single-view hair reconstruction, hairstyle editing, and hair-conditioned image generation.", "title_embedding_index": 19752, "title_abs_embedding_index": 19777}, {"title": "Replacement Learning: Training Vision Tasks with Fewer Learnable Parameters", "link_suffix": "/forum?id=4zygH3k8Zr", "link": "https://openreview.net/forum?id=4zygH3k8Zr", "pdf_link": "https://openreview.net/pdf?id=4zygH3k8Zr", "keywords": "Machine Learning, Deep Learning, Foundation Models", "abstract": "Traditional end-to-end deep learning models often enhance feature representation and overall performance by increasing the depth and complexity of the network during training. However, this approach inevitably introduces issues of parameter redundancy and resource inefficiency, especially in deeper networks. While existing works attempt to skip certain redundant layers to alleviate these problems, challenges related to poor performance, computational complexity, and inefficient memory usage remain. To address these issues, we propose an innovative training approach called Replacement Learning, which mitigates these limitations by completely replacing all the parameters of the frozen layers with only two learnable parameters. Specifically, Replacement Learning selectively freezes the parameters of certain layers, and the frozen layers utilize parameters from adjacent layers, updating them through a parameter integration mechanism controlled by two learnable parameters. This method leverages information from surrounding structures, reduces computation, conserves GPU memory, and maintains a balance between historical context and new inputs, ultimately enhancing overall model performance. We conducted experiments across four benchmark datasets, including CIFAR-10, STL-10, SVHN, and ImageNet, utilizing various architectures such as CNNs and ViTs to validate the effectiveness of Replacement Learning. Experimental results demonstrate that our approach reduces the number of parameters, training time, and memory consumption while completely surpassing the performance of end-to-end training.", "title_embedding_index": 19753, "title_abs_embedding_index": 19778}, {"title": "SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression", "link_suffix": "/forum?id=LNYIUouhdt", "link": "https://openreview.net/forum?id=LNYIUouhdt", "pdf_link": "https://openreview.net/pdf?id=LNYIUouhdt", "keywords": "Large Language Model; Post-training Model Compression", "abstract": "The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the compressed weight after SVD truncation. In this work, we propose \\sysname, a new SVD-based LLM compression method that addresses the limitations of existing methods. \\sysname incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, \\sysname adopts a parameter update with sequential low-rank approximation to compensate for the accuracy degradation after compression. We evaluate \\sysname on $10$ datasets and seven models from three different LLM families at three different scales. Our results demonstrate the superiority of \\sysname over state-of-the-arts, especially at high model compression ratios.", "title_embedding_index": 19754, "title_abs_embedding_index": 19779}, {"title": "The Brain's Bitter Lesson: Scaling Speech Decoding With Self-Supervised Learning", "link_suffix": "/forum?id=IAFStwZPNu", "link": "https://openreview.net/forum?id=IAFStwZPNu", "pdf_link": "https://openreview.net/pdf?id=IAFStwZPNu", "keywords": "neural decoding, speech decoding, brain-computer interfaces", "abstract": "The past few years have produced a series of spectacular advances in the decoding of speech from brain activity. The engine of these advances has been the acquisition of labelled data, with increasingly large datasets acquired from single subjects. However, participants exhibit individual differences, such as anatomy, and datasets use varied scanners and task designs. As a result, prior work has struggled to leverage data from multiple subjects, multiple datasets, multiple tasks, and unlabelled datasets. In turn, the field has not benefited from the rapidly growing number of open neural data repositories to exploit large-scale data and deep learning. This gap exists for all neural data, but especially for magnetoencephalography (MEG), where the scale of individual datasets has not yet caught up with other modalities. To address this, we develop a set of neuroscience-inspired self-supervised objectives, together with a neural architecture, for representation learning from heterogeneous and unlabelled neural recordings. Experimental results with MEG show that representations learned with these objectives scale with data, generalise across subjects, datasets, and tasks, outperform using the raw input representation, and even surpass comparable self-supervised approaches. In addition, we set new benchmarks for two foundational speech decoding tasks. Collectively, these methods now unlock the potential for training speech decoding models with orders of magnitude more existing data.", "title_embedding_index": 19755, "title_abs_embedding_index": 19780}, {"title": "Efficient Test-Time Prompt Tuning for Vision-Language Models", "link_suffix": "/forum?id=NeVbEYW4tp", "link": "https://openreview.net/forum?id=NeVbEYW4tp", "pdf_link": "https://openreview.net/pdf?id=NeVbEYW4tp", "keywords": "Vision-Language Models, Zero-shot Generalization, Prompt Learning, Test-Time Adaptation", "abstract": "Vision-language models have showcased impressive zero-shot classification capabilities when equipped with suitable text prompts. Previous studies have shown the effectiveness of test-time prompt tuning; however, these methods often require per-image prompt adaptation during inference, which is computationally intensive and limits scalability and deployment.\nTo address this issue, we introduce a novel framework: Self-supervised learning for efficient Test-time Prompt Tuning (Self-TPT).\nThe key feature of Self-TPT is its shift to efficient \\textit{predefined class adaptation} through self-supervised learning, thereby avoiding the computation-heavy \\textit{per-image adaptation} at inference.\nSelf-TPT starts by co-training the self-supervised and supervised tasks using source data, then applies the self-supervision exclusively for new class understanding before making predictions.\nSpecifically, we propose Contrastive Prompt Learning (CPT) as the core task for self-supervision. CPT is designed to minimize the intra-class distances while enhancing inter-class distinguishability via contrastive learning.\nEmpirical evidence suggests that CPT can partially mimic supervised learning in terms of gradients, providing a plausible explanation for its effectiveness.\nMotivated by this finding, we introduce a gradient matching loss to explicitly enhance gradient similarity.\nWe evaluated Self-TPT across three challenging zero-shot benchmarks. The results consistently show that Self-TPT not only significantly reduces inference costs but also achieves state-of-the-art performance, effectively balancing the efficiency-efficacy trade-off.", "title_embedding_index": 19756, "title_abs_embedding_index": 19781}, {"title": "CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding", "link_suffix": "/forum?id=le4IoZZHy1", "link": "https://openreview.net/forum?id=le4IoZZHy1", "pdf_link": "https://openreview.net/pdf?id=le4IoZZHy1", "keywords": "Long Video Understanding", "abstract": "Most existing video understanding benchmarks for multimodal large language models (MLLMs) focus only on short videos. The limited number of benchmarks for long video understanding often rely solely on multiple-choice questions (MCQs). However, because of the inherent limitation of MCQ-based evaluation and the increasing reasoning ability of MLLMs, models can give the current answer purely by combining short video understanding with elimination, without genuinely understanding the video content.\nTo address this gap, we introduce CG-Bench, a novel benchmark designed for clue-grounded question answering in long videos. CG-Bench emphasizes the model's ability to retrieve relevant clues for questions, enhancing evaluation credibility. It features 1,219 manually curated videos categorized by a granular system with 14 primary categories, 171 secondary categories, and 638 tertiary categories, making it the largest benchmark for long video analysis. The benchmark includes 12,129 QA pairs in three major question types: perception, reasoning, and hallucination.\nCompensating the drawbacks of pure MCQ-based evaluation, we design two novel clue-based evaluation methods: clue-grounded white box and black box evaluations, to assess whether the model generates answers based on the correct understanding of the video.\nWe evaluate multiple closed-source and open-source MLLMs on CG-Bench. Results indicate that current models significantly underperform in understanding long videos compared to short ones, and a significant gap exists between open-source and commercial models. We hope CG-Bench can advance the development of more trustworthy and capable MLLMs for long video understanding.", "title_embedding_index": 19757, "title_abs_embedding_index": 19782}, {"title": "Conjuring Semantic Similarity", "link_suffix": "/forum?id=z1td6fBKpG", "link": "https://openreview.net/forum?id=z1td6fBKpG", "pdf_link": "https://openreview.net/pdf?id=z1td6fBKpG", "keywords": "Semantic Similarity, Interpretability, Diffusion Models", "abstract": "The semantic similarity between sample expressions measures the distance between their latent 'meaning'. Such meanings are themselves typically represented by textual expressions, often insufficient to differentiate concepts at fine granularity. We propose a novel approach whereby the semantic similarity among textual expressions is based {\\em not} on other expressions they can be rephrased as, but rather based on the imagery they evoke. While this is not possible with humans, generative models allow us to easily visualize and compare generated images, or their distribution, evoked by a textual prompt. Therefore, we characterize  the semantic similarity between two textual expressions simply as the distance between image distributions they induce, or 'conjure.'  We show that by choosing  the Jensen-Shannon divergence between the reverse-time diffusion stochastic differential equations (SDEs) induced by each textual expression, this can be directly computed via Monte-Carlo sampling. Our method contributes a novel perspective on semantic similarity that not only aligns with human-annotated scores, but also opens up new avenues for the evaluation of text-conditioned generative models while offering better interpretability of their learnt representations.", "title_embedding_index": 19758, "title_abs_embedding_index": 19783}, {"title": "Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization", "link_suffix": "/forum?id=i0zzO7Hslk", "link": "https://openreview.net/forum?id=i0zzO7Hslk", "pdf_link": "https://openreview.net/pdf?id=i0zzO7Hslk", "keywords": "Neural Network Optimization, Parameter Efficient Pretraining, Low-rank Optimization", "abstract": "Pretraining large language models often requires significant computational resources and memory due to their vast parameter amount. An effective approach to enhance parameter efficiency in both training and inference is to parameterize each full-size weight as the product of two trainable low-rank factors. While low-rank fine-tuning has achieved great success, low-rank pretraining remains challenging as it requires learning extensive knowledge from scratch under the restrictive low-rank parameterization. During standard low-rank pretraining, separately optimizing the low-rank factors introduces redundant information from the full gradient, which hinders the learning process. To achieve efficient yet effective low-rank pretraining, we propose aLow-rankRiemannianOptimizer (LORO). At each LORO update step, the low-rank factor pairs are jointly updated to ensure their full-size product moves along the steepest descent direction on the low-rank manifold, without the need to compute any memory-intensive full-size matrices or gradients. Hence, our LORO finds low-rank models that achieve high performance comparable to full-size pretrained models, while significantly reducing memory usage and accelerating both training and inference. A LLaMA 1B model pretrained with LORO achieves a perplexity score of 2% better than the full-size baseline, with a 54% reduction in model memory, a $\\times1.8$ speedup in training, and a $\\times2.2$ speedup in inference.", "title_embedding_index": 19759, "title_abs_embedding_index": 19784}, {"title": "Effi-Code: Unleashing Code Efficiency in Language Models", "link_suffix": "/forum?id=ulXCYmvVg6", "link": "https://openreview.net/forum?id=ulXCYmvVg6", "pdf_link": "https://openreview.net/pdf?id=ulXCYmvVg6", "keywords": "Large Langugae Models, Code Generation, Program Synthesis, Efficient Method, Alignment", "abstract": "As the use of large language models (LLMs) for code generation becomes more prevalent in software development, it is critical to enhance both the efficiency and correctness of the generated code. \nExisting methods and models primarily focus on the correctness of LLM-generated code, ignoring the efficiency. \nIn this work, we present Effi-Code, an approach to enhancing code generation in LLMs that can improve both efficiency and correctness. We introduce a Self-Optimization process based on Overhead Profiling that leverages open-source LLMs to generate a high-quality dataset of correct and efficient code samples. This dataset is then used to fine-tune various LLMs. Our method involves iterative refinement of generated code, guided by runtime performance metrics and correctness checks. Extensive experiments demonstrate that models fine-tuned on the Effi-Code significantly improve code correctness and efficiency across task types. \nFor example, the pass@1 of DeepSeek-Coder-6.7B-Instruct generated cod increases from43.3%to76.8%, and the average execution time for the same correct tasks decreases by30.5%.\nEffi-Code offers a scalable and generalizable approach to improving code generation in AI systems, with potential applications in software development, algorithm design, and computational problem-solving.", "title_embedding_index": 19760, "title_abs_embedding_index": 19785}, {"title": "Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance", "link_suffix": "/forum?id=RaR3ETzyKp", "link": "https://openreview.net/forum?id=RaR3ETzyKp", "pdf_link": "https://openreview.net/pdf?id=RaR3ETzyKp", "keywords": "Rectified Flow Models, Training, Easing, Distance-Aware Noise-Sample Matching, DANSM", "abstract": "Recent research pinpoints that different diffusion methods and architectures \ntrained on the same dataset produce similar results for the same input noise. \nThis property suggests that they have some preferable noises for a given sample. \nBy visualizing the noise-sample pairs of rectified flow models and stable diffusion models in two-dimensional spaces, \nwe observe that the preferable paths, connecting preferable noises to the corresponding samples, \nare much well organized with significant fewer crossings comparing with \nthe random paths, connecting random noises to training samples. \nIn high-dimensional space, paths rarely intersect. \nThe path crossings in two-dimensional spaces indicate the shorter inter-path distance \nin the corresponding high-dimensional spaces. \nInspired by this observation, we propose the Distance-Aware Noise-Sample Matching (DANSM) method \nto lengthen the inter-path distance for speeding up the model training. \nDANSM is derived from rectified flow models, which allow using a closed-form formula to calculate the inter-path distance. \nTo further simplify the optimization, we derive the relationship between inter-path distance and path length, \nand use the latter in the optimization surrogate. \nDANSM is evaluated on both image and latent spaces by rectified flow models and diffusion models. \nThe experimental results show that DANSM can significantly improve the training speed by 30% $\\sim$ 40%\nwithout sacrificing the generation quality.", "title_embedding_index": 19761, "title_abs_embedding_index": 19786}, {"title": "Latent Radiance Fields with 3D-aware 2D Representations", "link_suffix": "/forum?id=vL9t9tpKli", "link": "https://openreview.net/forum?id=vL9t9tpKli", "pdf_link": "https://openreview.net/pdf?id=vL9t9tpKli", "keywords": "3D Gaussian Splatting, 3D-aware Representation", "abstract": "Latent 3D reconstruction has shown great promise in empowering 3D semantic understanding and 3D generation by distilling 2D features into the 3D space. However, existing approaches struggle with the domain gap between 2D feature space and 3D representations, resulting in degraded rendering performance. To address this challenge, we propose a novel framework that integrates 3D awareness into the 2D latent space. The framework consists of three stages: (1) a correspondence-aware autoencoding method that enhances the 3D consistency of 2D latent representations, (2) a latent radiance field (LRF) that lifts these 3D-aware 2D representations into 3D space, and (3) a VAE-Radiance Field (VAE-RF) alignment strategy that improves image decoding from the rendered 2D representations. Extensive experiments demonstrate that our method outperforms the state-of-the-art latent 3D reconstruction approaches in terms of synthesis performance and cross-dataset generalizability across diverse indoor and outdoor scenes. To our knowledge, this is the first work showing the radiance field representations constructed from 2D latent representations can yield photorealistic 3D reconstruction performance.", "title_embedding_index": 19762, "title_abs_embedding_index": 19787}, {"title": "Memory Efficient Transformer Adapter for Dense Predictions", "link_suffix": "/forum?id=vJkktqyU8B", "link": "https://openreview.net/forum?id=vJkktqyU8B", "pdf_link": "https://openreview.net/pdf?id=vJkktqyU8B", "keywords": "Vision Transformer, Vision Transformer, Transformer", "abstract": "While current Vision Transformer (ViT) adapter methods have shown promising accuracy, their inference speed is implicitly hindered by inefficient memory access operations, e.g., standard normalization and frequent reshaping. In this work, we propose META, a simple and fast ViT adapter that can improve the model's memory efficiency and decrease memory time consumption by reducing the inefficient memory access operations. Our method features a memory-efficient adapter block that enables the common sharing of layer normalization between the self-attention and feed-forward network layers, thereby reducing the model's reliance on normalization operations. Within the proposed block, the cross-shaped self-attention is employed to reduce the model's frequent reshaping operations. Moreover, we augment the adapter block with a lightweight convolutional branch that can enhance local inductive biases, particularly beneficial for the dense prediction tasks, e.g., object detection, instance segmentation, and semantic segmentation. The adapter block is finally formulated in a cascaded manner to compute diverse head features, thereby enriching the variety of feature representations. Empirically, extensive evaluations on multiple representative datasets validate that META substantially enhances the predicted quality, while achieving a new state-of-the-art accuracy-efficiency trade-off. Theoretically, we demonstrate that META exhibits superior generalization capability and stronger adaptability.", "title_embedding_index": 19763, "title_abs_embedding_index": 19788}, {"title": "Growing Efficient Accurate and Robust Neural Networks on the Edge", "link_suffix": "/forum?id=Svt3SGy0yd", "link": "https://openreview.net/forum?id=Svt3SGy0yd", "pdf_link": "https://openreview.net/pdf?id=Svt3SGy0yd", "keywords": "Corruption Robustness, Growth, Efficient Training, Edge", "abstract": "The ubiquitous deployment of deep learning systems on resource-constrained Edge devices is hindered by their high computational complexity coupled with their fragility to out-of-distribution (OOD) data, especially to naturally occurring common corruptions. Current solutions rely on the Cloud to train and compress models before deploying to the Edge. This incurs high energy and latency costs in transmitting locally acquired field data to the Cloud while also raising privacy concerns. We propose GEARnn (Growing Efficient, Accurate, and Robust neural networks) to grow and train robust networks in-situ, i.e., completely on the Edge device. Starting with a low-complexity initial backbone network, GEARnn employs One-Shot Growth (OSG) to grow a network satisfying the memory constraints of the Edge device using clean data, and robustifies the network using Efficient Robust Augmentation (ERA) to obtain the final network. We demonstrate results on a NVIDIA Jetson Xavier NX, and analyze the trade-offs between accuracy, robustness, model size, energy consumption, and training time. Our results demonstrate the construction of efficient, accurate, and robust networks entirely on an Edge device.", "title_embedding_index": 19764, "title_abs_embedding_index": 19789}, {"title": "Radiologist-like Progressive Radiology Report Generation and Benchmarking", "link_suffix": "/forum?id=PzpSJkdokk", "link": "https://openreview.net/forum?id=PzpSJkdokk", "pdf_link": "https://openreview.net/pdf?id=PzpSJkdokk", "keywords": "radiology report generation", "abstract": "Radiology report generation is a critical application at the intersection of radiology and artificial intelligence. It aims to reduce radiologists' workload by automating the interpretation and reporting of medical images. Previous works have employed diverse approaches, with some focusing solely on imaging data while others incorporate the indication but often neglect the interrelationships among different report sections. \nOur work identifies and harnesses the intrinsic relationships between the indication, findings, and impression sections of a radiology report.\nThe indication section provides the clinical context and specifies the reason for the examination, setting the stage for targeted image analysis. The findings section details the radiologist's observations from the image, including identified abnormalities and relevant normal findings. The impression section synthesizes these observations to form a diagnostic conclusion, directly addressing the clinical query presented in the indication.\nBy mapping these relationships, we propose a Radiologist-Like Progressive Generation (RLPG) framework that mirrors the radiologist's workflow for report generation.\nInitially, an image encoder and a large language model process the imaging data alongside the indication to generate detailed findings. Subsequently, the same image, the indication, and the predicted findings are utilized to produce a concise impression. This method improves the alignment between report sections and improves the clinical relevance of the generated reports. \nTo facilitate research and benchmarking in report generation, we introduce MIMIC-1V3 (i.e., 1 case vs. 3 sections), a curated dataset derived from the MIMIC-CXR by dividing each report into three sections: indication, findings, and impression.", "title_embedding_index": 19765, "title_abs_embedding_index": 19790}, {"title": "Grokfast: Gradient filters for faster grokking", "link_suffix": "/forum?id=UD0L74wQt9", "link": "https://openreview.net/forum?id=UD0L74wQt9", "pdf_link": "https://openreview.net/pdf?id=UD0L74wQt9", "keywords": "grokking, generalization, acceleration, gradient filter, optimization, low-pass filter", "abstract": "One puzzling artifact in machine learning, dubbed grokking, refers to the case where a model exhibits delayed generalization after numerous training iterations after nearly perfect overfitting. Focusing on the long delay itself on behalf of machine learning practitioners, our primary goal is to accelerate the generalization of a model under the grokking phenomenon. By regarding a series of gradients of a parameter over training iterations as a random signal over time, we can spectrally decompose the parameter trajectories under gradient descent into two components: the fast-varying, overfitting-yielding component, and the slow-varying, generalization-inducing component. This analysis allows us to accelerate the grokking phenomenon more than $\\times 50$ with only a few lines of code that amplifies the slow-varying components of the gradients. The experiments show that our algorithm applies to diverse tasks involving images, languages, and graphs, enabling the practical availability of this peculiar artifact of sudden generalization. Moreover, we reinterpret momentum hyperparameters in gradient-based optimizers as low-pass filters with size-1 windows. This bridges between optimization and classical signal processing literature, suggesting a new type of optimzers augmented with frequecy-domain filters.", "title_embedding_index": 19766, "title_abs_embedding_index": 19791}, {"title": "MHPP: Exploring the Capabilities and Limitations of Language Models Beyond Basic Code Generation", "link_suffix": "/forum?id=TVFVx8TUbN", "link": "https://openreview.net/forum?id=TVFVx8TUbN", "pdf_link": "https://openreview.net/pdf?id=TVFVx8TUbN", "keywords": "Large Langugae Models, Code Generation, Program Synthesis, Benchmark and Dataset, Evaluation and Resources", "abstract": "Recent advancements in large language models (LLMs) have greatly improved code generation, specifically at the function level. For instance, GPT-4o has achieved a 91.0% pass rate on HumanEval. However, this draws into question the adequacy of existing benchmarks in thoroughly assessing function-level code generation capabilities. Our study analyzed two common benchmarks, HumanEval and MBPP, and found that these might not thoroughly evaluate LLMs' code generation capacities due to limitations in quality, difficulty, and granularity. To resolve this, we introduce the Mostly Hard Python Problems (MHPP) dataset, consisting of 210 unique human-curated problems. By focusing on the combination of natural language and code reasoning, MHPP gauges LLMs' abilities to comprehend specifications and restrictions, engage in multi-step reasoning, and apply coding knowledge effectively. Initial evaluations of 26 LLMs using MHPP showed many high-performing models on HumanEval failed to achieve similar success on MHPP. Moreover, MHPP highlighted various previously undiscovered limitations within various LLMs, leading us to believe that it could pave the way for a better understanding of LLMs' capabilities and limitations.", "title_embedding_index": 19767, "title_abs_embedding_index": 19792}, {"title": "ASOR: Anchor State Oriented Regularization for Policy Optimization under Dynamics Shift", "link_suffix": "/forum?id=ppeG3z5mcj", "link": "https://openreview.net/forum?id=ppeG3z5mcj", "pdf_link": "https://openreview.net/pdf?id=ppeG3z5mcj", "keywords": "Reinforcement Learning, Policy Transfer, Dynamics Shift, Policy Regularization", "abstract": "To train neural policies in environments with diverse dynamics, Imitation from\nObservation (IfO) approaches aim at recovering expert state trajectories. Their\nsuccess is built upon the assumption that the stationary state distributions induced\nby optimal policies remain similar despite dynamics shift. However, such an\nassumption does not hold in many real world scenarios, especially when certain\nstates become inaccessible during environment dynamics change. In this paper,\nwe propose the concept of anchor states which appear in all optimal trajectories\nunder dynamics shift, thereby maintaining consistent state accessibility. Instead of\ndirect imitation, we incorporate anchor state distributions into policy regularization\nto mitigate the issue of inaccessible states, leading to the ASOR algorithm. By\nformally characterizing the difference of state accessibility under dynamics shift,\nwe show that the anchor state-based regularization approach provides strong lower-\nbound performance guarantees for efficient policy optimization. We perform\nextensive experiments across various online and offline RL benchmarks, including\nGridworld, MuJoCo, MetaDrive, D4RL, and a fall-guys like game environment,\nfeaturing multiple sources of dynamics shift. Experimental results indicate ASOR\ncan be effectively integrated with several state-of-the-art cross-domain policy\ntransfer algorithms, substantially enhancing their performance.", "title_embedding_index": 19768, "title_abs_embedding_index": 19793}, {"title": "Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy", "link_suffix": "/forum?id=LfekK1E0QE", "link": "https://openreview.net/forum?id=LfekK1E0QE", "pdf_link": "https://openreview.net/pdf?id=LfekK1E0QE", "keywords": "Reinforcement Learning, Human-in-the-loop Learning, Imperfect Human Interaction, Human Feedback", "abstract": "We introduce AGSA, an Agent-Gated Shared Autonomy framework that learns from high-level human feedback to tackle the challenges of reward-free training, safe exploration, and imperfect low-level human control. Recent human-in-the loop learning methods enable human participants to intervene a learning agent\u2019s control and provide online demonstrations. Nonetheless, these methods rely heavily on perfect human interactions, including accurate human-monitored intervention decisions and near-optimal human demonstrations. AGSA employs a dedicated gating agent to determine when to switch control, thereby reducing the need of constant human monitoring. To obtain a precise and foreseeable gating agent, AGSA trains a long-term gating value function from human evaluative feedback on the gating agent\u2019s intervention requests and preference feedback on pairs of human intervention trajectories. Instead of relying on potentially suboptimal human demonstrations, the learning agent is trained using control-switching signals from the gating agent. We provide theoretical insights on performance bounds that respectively describe the ability of the two agents. Experiments are conducted with both simulated and real human participants at different skill levels in challenging continuous control environments. Comparative results highlight that AGSA achieves significant improvements over previous human-in-the-loop learning methods in terms of training safety, policy performance, and user-friendliness.", "title_embedding_index": 19769, "title_abs_embedding_index": 19794}, {"title": "Grounded-VideoLLM: Sharpening Fine-grained Temporal Grounding in Video Large Language Models", "link_suffix": "/forum?id=YCwN7wQA6W", "link": "https://openreview.net/forum?id=YCwN7wQA6W", "pdf_link": "https://openreview.net/pdf?id=YCwN7wQA6W", "keywords": "multimodal large language model; video understanding; temporal reasoning", "abstract": "Video Large Language Models (Video-LLMs) have demonstrated remarkable capabilities in coarse-grained video understanding, however, they struggle with fine-grained temporal grounding. In this paper, we introduce $\\textit{Grounded-VideoLLM}$, a novel Video-LLM adept at perceiving and reasoning over specific video moments in a fine-grained manner. We identify that current Video-LLMs have limitations for fine-grained video understanding since they lack effective temporal modeling and timestamp representation. In light of this, we sharpen our model by incorporating (1) an additional temporal stream to encode the relationships between frames and (2) discrete temporal tokens enriched with specific time knowledge to represent timestamps. To optimize the training of $\\textit{Grounded-VideoLLM}$, we employ a multi-stage training scheme, beginning with simple video-captioning tasks and progressively introducing video temporal grounding tasks of increasing complexity. To further enhance $\\textit{Grounded-VideoLLM}$'s temporal reasoning capability, we also curate a grounded VideoQA dataset by an automatic annotation pipeline. Extensive experiments demonstrate that $\\textit{Grounded-VideoLLM}$ not only excels in fine-grained grounding tasks such as temporal sentence grounding, dense video captioning, and grounded VideoQA, but also shows great potential as a versatile video assistant for general video understanding.", "title_embedding_index": 19770, "title_abs_embedding_index": 19795}, {"title": "Multiview Equivariance Improves 3D Understanding with Minimal Feature Finetuning", "link_suffix": "/forum?id=CNO4rbSV6v", "link": "https://openreview.net/forum?id=CNO4rbSV6v", "pdf_link": "https://openreview.net/pdf?id=CNO4rbSV6v", "keywords": "Vision Foundation Models; 3D Representation Learning; Fine-tuning; 3D Equivariance", "abstract": "Vision foundation models, particularly the ViT family, have revolutionized image understanding by providing rich semantic features. However, despite their success in 2D comprehension, their abilities on grasping 3D spatial relationships are still unclear.\nIn this work, we evaluate and enhance the 3D awareness of ViT-based models. We begin by systematically assessing their ability to learn 3D equivariant features, specifically examining the consistency of semantic embeddings across different viewpoints. Our findings indicate that improved 3D equivariance leads to better performance on various downstream tasks, including pose estimation, tracking, and semantic transfer. Building on this insight, we propose a simple yet effective finetuning strategy based on 3D correspondences, which significantly enhances the 3D understanding of existing vision models. Remarkably, even finetuning on a single object for just one iteration results in substantial performance gains. All code and resources will be made publicly available to support further advancements in 3D-aware vision models.", "title_embedding_index": 19771, "title_abs_embedding_index": 19796}, {"title": "Iterative Vectors: Boost In-Context Learning within Activations", "link_suffix": "/forum?id=pQoD5MtZve", "link": "https://openreview.net/forum?id=pQoD5MtZve", "pdf_link": "https://openreview.net/pdf?id=pQoD5MtZve", "keywords": "Large Language Model, In-context Learning", "abstract": "In-context learning (ICL) has emerged as a standard paradigm for utilizing language models. Although ICL is convenient due to the absence of backpropagation, selecting and processing appropriate demonstration examples can be difficult and time-consuming, particularly when the number of examples is large. We propose to explore the potential of activation space through Iterative Vectors (IVs), a technique designed to enhance in-context performance and necessitating only forward inference passes. IVs are employed by first extracting and iteratively steering activations within a language model, then applying them during inference with minimal computational and memory overhead. We evaluate IVs across numerous tasks using three popular models and observe significant improvements. Our findings suggest that activation steering can serve as a promising direction for in-context learning, thereby opening new avenues for future research.", "title_embedding_index": 19772, "title_abs_embedding_index": 19797}, {"title": "Induction Rather Than Imagination: Generative Zero-Shot Learning Via Inductive Variational Autoencoder", "link_suffix": "/forum?id=Jy0MJYZEuN", "link": "https://openreview.net/forum?id=Jy0MJYZEuN", "pdf_link": "https://openreview.net/pdf?id=Jy0MJYZEuN", "keywords": "zero-shot learning, generative model", "abstract": "Remarkable progress in zero-shot learning (ZSL) has been achieved using generative models. However, existing generative ZSL methods merely generate (imagine) the visual features from scratch guided by the strong class semantic vectors annotated by experts, resulting in suboptimal generative performance and limited scene generalization. To address these and advance ZSL, we propose an inductive variational autoencoder for generative zero-shot learning, dubbed GenZSL. Mimicking human-level concept learning, GenZSL operates by \\textit{inducting} new class samples from similar seen classes using weak class semantic vectors derived from target class names (i.e., CLIP text embedding). To ensure the generation of informative samples for training an effective ZSL classifier, our GenZSL incorporates two key strategies. Firstly, it employs class diversity promotion to enhance the diversity of class semantic vectors. Secondly, it utilizes target class-guided information boosting criteria to optimize the model. Extensive experiments conducted on three popular benchmark datasets showcase the superiority and potential of our GenZSL with significant efficacy and efficiency over f-VAEGAN, e.g., 24.7% performance gains and more than $60\\times$ faster training speed on AWA2. Codes are available athttps://anonymous.4open.science/r/GenZSL.", "title_embedding_index": 19773, "title_abs_embedding_index": 19798}, {"title": "The Stochastic Parrot on LLM\u2019s Shoulder:  A Summative Assessment of Physical Concept Understanding", "link_suffix": "/forum?id=LSB2mRJdgZ", "link": "https://openreview.net/forum?id=LSB2mRJdgZ", "pdf_link": "https://openreview.net/pdf?id=LSB2mRJdgZ", "keywords": "corpus creation, benchmarking, evaluation methodologies", "abstract": "In a systematic way, we investigate a widely asked question: Do LLMs really understand what they say?, which relates to the more familiar term Stochastic Parrot. To this end, we propose a summative assessment over a carefully designed physical concept understanding task, PHYSICO. Our task alleviates the memorization issue via the usage of grid-format inputs that abstractly describe physical phenomena. The grids represents varying levels of understanding, from the core phenomenon, application examples to analogies to other abstract patterns in the grid world. A comprehensive study on our task demonstrates that: (1) state-of-the-art LLMs lag behind humans by \u223c40%; (2) the stochastic parrot phenomenon is present in LLMs, as they fail on our grid task but can describe and recognize the same concepts well in natural language; (3) our task challenges the LLMs due to intrinsic difficulties rather than the unfamiliar grid format, as in-context learning and fine-tuning on same formatted data added little to their performance. Our data is released (see Supplementary Material in the submission) for public research.", "title_embedding_index": 19774, "title_abs_embedding_index": 19799}]