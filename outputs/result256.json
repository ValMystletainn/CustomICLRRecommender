[{"title": "Fast Direct: Query-Efficient  Online Black-box Guidance  for Diffusion-model Target Generation", "link_suffix": "/forum?id=OmpTdjl7RV", "link": "https://openreview.net/forum?id=OmpTdjl7RV", "pdf_link": "https://openreview.net/pdf?id=OmpTdjl7RV", "keywords": "Diffusion model, Black-box target generation, Online guided diffusion model, Query-efficient", "abstract": "Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion-model to address the specific downstream tasks. Existing guided diffusion models either rely on training of the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, the offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need anonlinealgorithm capable of collecting data during runtime and supporting ablack-boxobjective function. Moreover, thequery efficiencyof the algorithm is also critical because the objective evaluation of the query is often expensive in the real-world scenarios. In this work, we propose a novel and simple algorithm,Fast Direct, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\\small {1024 \\times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\\textbf{6}\\times$ up to $\\textbf{10}\\times$ query efficiency improvement and $\\textbf{11}\\times$ up to $\\textbf{44}\\times$ query efficiency improvement, respectively.", "title_embedding_index": 12750, "title_abs_embedding_index": 12775}, {"title": "TSGGuide: Recommendation Guide for Multivariate Time Series Generation", "link_suffix": "/forum?id=cMLtjP3Cym", "link": "https://openreview.net/forum?id=cMLtjP3Cym", "pdf_link": "https://openreview.net/pdf?id=cMLtjP3Cym", "keywords": "time series generation, diffusion model, Channel Independence", "abstract": "Multivariate Time Series Generation (MTSG) plays a crucial role in time series analysis, supporting tasks such as data augmentation and anomaly detection. While several methods exist for MTSG, recommending the most suitable method for new scenarios remains a significant challenge. Although prior work by  (Ang et al., 2023a) provides guidance for selecting MTSG methods, it lacks coverage of recent diffusion-based methods and has limited exploration of channel-independent frameworks. We address these gaps by improving the recommendation guide, highlighting the effectiveness of a central discriminator within the channel-independent framework. This enhancement is particularly beneficial for time series classification in small-sample scenarios. Our revised guide makes three key recommendations: 1) VAE-based methods excel on small-scale datasets; 2) a channel-independent framework with the newly designed central discriminator is optimal in most cases; and 3) a diffusion-based method is preferable when ample data and computational resources are available.", "title_embedding_index": 12751, "title_abs_embedding_index": 12776}, {"title": "DAS-GNN: Degree-Aware Spiking Graph Neural Networks for Graph Classification", "link_suffix": "/forum?id=iEHYbGbZ4D", "link": "https://openreview.net/forum?id=iEHYbGbZ4D", "pdf_link": "https://openreview.net/pdf?id=iEHYbGbZ4D", "keywords": "Spiking Neural Network, Graph Neural Network, Graph Classification", "abstract": "The recent integration of spiking neurons into graph neural networks has been gaining much attraction due to its superior energy efficiency. Especially because the sparse connection among graph nodes fits the nature of the spiking neural networks, spiking graph neural networks are considered strong alternatives to vanilla graph neural networks. However, there is still a large performance gap for graph tasks between the spiking neural networks and artificial neural networks. The gaps are especially large when they are adapted to graph classification tasks, where none of the nodes in the test set graphs are connected to the training set graphs. We diagnose the problem as the existence of neurons under starvation, caused by the sparse connections among the nodes and the neurons. To alleviate the problem, we propose DAS-GNN. Based on a set of observations on spiking neurons on graph classification tasks, we devise several techniques to utilize more neurons to deliver meaningful information to the connected neurons. Experiments on diverse datasets show significant improvements compared to the baselines, demonstrating the effectiveness of the DAS-GNN.", "title_embedding_index": 12752, "title_abs_embedding_index": 12777}, {"title": "Adaptive Transformer Programs: Bridging the Gap Between Performance and Interpretability in Transformers", "link_suffix": "/forum?id=W8K8slZ73R", "link": "https://openreview.net/forum?id=W8K8slZ73R", "pdf_link": "https://openreview.net/pdf?id=W8K8slZ73R", "keywords": "Mechanistic Interpretability, Transformers, Interpretable AI", "abstract": "Balancing high performance with interpretability in increasingly powerful Transformer-based models remains a challenge. While mechanistic interpretability aims to specify neural network computations in explicit, pseudocode-like formats, existing methods often involve laborious manual analysis or struggle to fully elucidate learned internal algorithms. Recent efforts to build intrinsically interpretable models have introduced considerable expressivity and optimization challenges. This work introduces Adaptive Transformer Programs, an enhanced framework building upon RASP language and Transformer Programs to create more robust and interpretable models. The proposed method increases expressivity by redesigning two primary attention modules to improve categorical and numerical reasoning capabilities. To overcome optimization hurdles, we introduce a novel reparameterization scheme that enhances the exploration-exploitation trade-off during training. We validate our approach through extensive experiments on diverse tasks, including in-context learning, algorithmic problems (e.g., sorting and Dyck languages), and NLP benchmarks such as named entity recognition and text classification. Results demonstrate that Adaptive Transformer Programs substantially narrow the performance gap between black-box Transformers and interpretable models, enhancing transparency. This work advances the development of high-performing, transparent AI systems for critical applications, addressing crucial ethical concerns in AI development.", "title_embedding_index": 12753, "title_abs_embedding_index": 12778}, {"title": "SysBench: Can LLMs Follow System Message?", "link_suffix": "/forum?id=KZWaxtzIRx", "link": "https://openreview.net/forum?id=KZWaxtzIRx", "pdf_link": "https://openreview.net/pdf?id=KZWaxtzIRx", "keywords": "Large Language Models, System Message, Instruction Following, Benchmark Dataset", "abstract": "Large Language Models (LLMs) have become instrumental across various applications, with the customization of these models to specific scenarios becoming increasingly critical. System message, a fundamental component of LLMs, is consist of carefully crafted instructions that guide the behavior of model to meet intended goals. Despite the recognized potential of system messages to optimize AI-driven solutions,  there is a notable absence of a comprehensive benchmark for evaluating how well LLMs follow system messages. To fill this gap, we introduce SysBench, a benchmark that systematically analyzes system message following ability in terms of three limitations of existing LLMs: constraint violation, instruction misjudgement and multi-turn instability. Specifically, we manually construct evaluation dataset based on six prevalent types of constraints, including 500 tailor-designed system messages and multi-turn user conversations covering various interaction relationships. Additionally, we develop a comprehensive evaluation protocol to measure model performance. Finally, we conduct extensive evaluation across various existing LLMs, measuring their ability to follow specified constraints given in system messages. The results highlight both the strengths and weaknesses of existing models, offering key insights and directions for future research.", "title_embedding_index": 12754, "title_abs_embedding_index": 12779}, {"title": "PruningBench: A Comprehensive Benchmark of Structural Pruning", "link_suffix": "/forum?id=vvD0VFw0LG", "link": "https://openreview.net/forum?id=vvD0VFw0LG", "pdf_link": "https://openreview.net/pdf?id=vvD0VFw0LG", "keywords": "network compression, structural pruning, benchmark", "abstract": "Structural pruning has emerged as a promising approach for producing more efficient models. Nevertheless, the community suffers from a lack of standardized benchmarks and metrics, leaving the progress in this area not fully comprehended. To fill this gap, we present the first comprehensive benchmark, termed PruningBench, for structural pruning. PruningBench showcases the following three characteristics: 1) PruningBench employs a unified and consistent framework for evaluating the effectiveness of diverse structural pruning techniques; 2) PruningBench systematically evaluates 16 existing pruning methods, encompassing a wide array of models (e.g., CNNs and ViTs) and tasks (e.g., classification and detection); 3) PruningBench provides easily implementable interfaces to facilitate the implementation of future pruning methods, and enables the subsequent researchers to incorporate their work into our leaderboards. We will provide an online pruning platform for customizing pruning tasks and reproducing all results in this paper. Codes will also be made publicly available.", "title_embedding_index": 12755, "title_abs_embedding_index": 12780}, {"title": "Diffusion Process with Implicit Latents via Energy Models", "link_suffix": "/forum?id=NW5vSJXO9V", "link": "https://openreview.net/forum?id=NW5vSJXO9V", "pdf_link": "https://openreview.net/pdf?id=NW5vSJXO9V", "keywords": "Energy-based model, Image generation, Gradient Penalty, Diffusion Process", "abstract": "We present a generative model based on an ordered sequence of latent variables for intermediate distributions between a given source and a desired target distribution. We construct the probabilistic transitions among the latent variables using energy models that are in the form of classifiers. In our work, the intermediate transitional distributions are implicitly defined by the energy models during training, where the statistical properties of the data distribution are naturally taken into account. This is in contrast to denoising diffusion probabilistic models (DDPMs) where they are explicitly defined by the predefined scheduling of a sequential noise degradation process. Over the course of training, our model is designed to optimally determine the intermediate distributions by Langevin dynamics driven by the energy model. In contrast, energy-based models (EBMs) typically require an additional generator since the intermediate distributions are not explicitly defined in the training procedure. We demonstrate the effectiveness and efficiency of the proposed algorithm in the context of image generation, achieving high fidelity results with less inference steps on a variety of datasets.", "title_embedding_index": 12756, "title_abs_embedding_index": 12781}, {"title": "RethinkMCTS: Refining Erroneous Thoughts in Monte Carlo Tree Search for Code Generation", "link_suffix": "/forum?id=OJUcOLOLXL", "link": "https://openreview.net/forum?id=OJUcOLOLXL", "pdf_link": "https://openreview.net/pdf?id=OJUcOLOLXL", "keywords": "Code Generation, Large Language Model Agent, Monte Carlo Tree Search", "abstract": "LLM agents enhanced by tree search algorithms have shown significant performance in code generation. However, existing search methods generally operate directly in the code language space, leading to suboptimal search quality due to ignoring the reasoning process behind the code. Specifically, two key challenges remain largely unaddressed: 1) A lack of exploration for the reasoning process, which is essential for high-reasoning-demand tasks like code generation, and 2) Inadequate search quality due to the absence of refinement mechanism. In this paper, we introduce RethinkMCTS, a framework that explores and refines the reasoning process for generating code. Specifically, we employ MCTS to search for the thoughts before code generation and integrate MCTS with a refinement mechanism called \"rethink\", which incorporates fine-grained code execution feedback to refine erroneous thoughts during the search. It ensures the search path aligns with the better reasoning, improving overall search quality. Through extensive experiments, we demonstrate that RethinkMCTS outperforms previous search-enhanced and feedback-enhanced code generation baselines. On the HumanEval dataset, it boosts the pass@1 of GPT-3.5-turbo from 70.12 to 89.02 and that of GPT-4o-mini from 87.20 to 94.51. By conducting thought-level exploration and integrating the rethink mechanism, it significantly enhances the search quality of the entire search tree", "title_embedding_index": 12757, "title_abs_embedding_index": 12782}, {"title": "MLE-Bench: Evaluating Machine Learning Agents on Machine Learning Engineering", "link_suffix": "/forum?id=6s5uXNWGIh", "link": "https://openreview.net/forum?id=6s5uXNWGIh", "pdf_link": "https://openreview.net/pdf?id=6s5uXNWGIh", "keywords": "benchmark, evals, evaluations, dataset, tasks, data science, engineering, agents, language agents, scaffold, coding, swe, mle", "abstract": "We introduce MLE-bench, a benchmark for measuring how well AI agents perform at machine learning engineering. To this end, we curate 71 ML engineering-related competitions from Kaggle, creating a diverse set of challenging tasks that test real-world ML engineering skills such as training models, preparing datasets, and running experiments. We establish human baselines for each competition using Kaggle's publicly available leaderboards. We use open-source agent scaffolds to evaluate several frontier language models on our benchmark, finding that the best-performing setup \u2014 OpenAI's o1-preview with AIDE scaffolding \u2014 achieves at least the level of a Kaggle bronze medal in 17.3% of competitions. In addition to our main results, we investigate various forms of resource-scaling for AI agents and the impact of contamination from pre-training. We open-source our benchmark code to facilitate future research in understanding the ML engineering capabilities of AI agents.", "title_embedding_index": 12758, "title_abs_embedding_index": 12783}, {"title": "Unleashing the Potential of ConvNets for Query-based Detection and Segmentation", "link_suffix": "/forum?id=TWRhLAN5rz", "link": "https://openreview.net/forum?id=TWRhLAN5rz", "pdf_link": "https://openreview.net/pdf?id=TWRhLAN5rz", "keywords": "Object Detection, Segmentation, ConvNet, Segment Anything", "abstract": "Transformer and its variants have shown great potential for various vision tasks in recent years, including image classification, object detection and segmentation. \nMeanwhile, recent studies also reveal that with proper architecture design, convolution networks (ConvNets) also achieve competitive performance with transformers, \\eg, ConvNeXt. \nHowever, no prior methods have explored to utilize pure convolution to build a Transformer-style Decoder module, which is essential for Encoder-Decoder architecture like Detection Transformer (DETR).\nTo this end, in this paper we explore whether we could build query-based  detection and segmentation framework with ConvNets instead of sophisticated transformer architecture.\nWe propose a novel mechanism dubbed InterConv to perform interaction between object queries and image features via convolutional layers. \nEquipped with the proposed InterConv, we build Detection ConvNet (DECO), which is composed of a backbone and convolutional encoder-decoder architecture. We compare the proposed DECO against prior detectors on the challenging COCO benchmark.\nDespite its simplicity, our DECO achieves competitive performance in terms of detection accuracy and running speed. Specifically, \nwith the ResNet-18 and ResNet-50 backbone, our DECO achieves $40.5%$ and $47.8%$ AP with $66$ and $34$ FPS, respectively. The proposed method is also evaluated on the segment anything task, demonstrating similar performance and higher efficiency.\nWe hope the proposed method brings another perspective for designing architectures for vision tasks.", "title_embedding_index": 12759, "title_abs_embedding_index": 12784}, {"title": "Dynamic Post-Hoc Neural Ensemblers", "link_suffix": "/forum?id=tRzujdRwE8", "link": "https://openreview.net/forum?id=tRzujdRwE8", "pdf_link": "https://openreview.net/pdf?id=tRzujdRwE8", "keywords": "ensemble learning, neural networks, regularization, modality agnostic, dynamic ensembling", "abstract": "Ensemble methods are known for enhancing the accuracy and robustness of machine learning models by combining multiple base learners. However, standard approaches like greedy or random ensembles often fall short, as they assume a constant weight across samples for the ensemble members. This can limit expressiveness and hinder performance when aggregating the ensemble predictions. In this study, we explore employing neural networks as ensemble methods, emphasizing the significance of dynamic ensembling to leverage diverse model predictions adaptively. Motivated by the risk of learning low-diversity ensembles, we propose regularizing the model by randomly dropping base model predictions during the training. We demonstrate this approach lower bounds the diversity within the ensemble, reducing overfitting and improving generalization capabilities. Our experiments showcase that the dynamic neural ensemblers yield competitive results compared to strong baselines in computer vision, natural language processing, and tabular data.", "title_embedding_index": 12760, "title_abs_embedding_index": 12785}, {"title": "RaCNN: Region-aware Convolutional Neural Network with Global Receptive Field", "link_suffix": "/forum?id=synCTX1JqO", "link": "https://openreview.net/forum?id=synCTX1JqO", "pdf_link": "https://openreview.net/pdf?id=synCTX1JqO", "keywords": "Convolutional Neural Network, Global Receptive Field, Backbone", "abstract": "Recent Convolutional Neural Networks (CNNs) utilize large-kernel convolutions (e.g., 101 kernel convolutions) to simulate a large receptive field of Vision Transformers (ViTs). \n    However, these models introduce specialized techniques like re-parameterization, sparsity, and weight decomposition, increasing the complexity of the training and inference stages. \n    To address this challenge, we propose Region-aware CNN (RaCNN), which achieves a global receptive field without requiring extra complexity, yet surpasses state-of-the-art models. \n    Specifically, we design two novel modules to capture global visual dependencies. \n    The first is the Region-aware Feed Forward Network (RaFFN). \n    It uses a novel Region Point-Wise Convolution (RPWConv) to capture global visual cues in a region-aware manner. \n    In contrast, traditional PWConv shares the same weights for all spatial pixels and cannot capture spatial information. \n    The second is the Region-aware Gated Linear Unit (RaGLU). \n    This channel mixer captures long-range visual dependencies in a sparse global manner and can become a better substitute for the original FFN. \n    Under only 84% computational complexity, RaCNN significantly outperforms the state-of-the-art CNN model MogaNet (83.9% vs. 83.4%). \n    It also demonstrates good scalability and surpasses existing state-of-the-art lightweight models. \n    Furthermore, our RaCNN shows comparability with state-of-the-art ViTs, MLPs, and Mambas in object detection, instance segmentation, and semantic segmentation.All codes and logs are released in the supplementary materials.", "title_embedding_index": 12761, "title_abs_embedding_index": 12786}, {"title": "Erasing Concept Combination from Text-to-Image Diffusion Model", "link_suffix": "/forum?id=OBjF5I4PWg", "link": "https://openreview.net/forum?id=OBjF5I4PWg", "pdf_link": "https://openreview.net/pdf?id=OBjF5I4PWg", "keywords": "concept combination erasing, text-to-image diffusion model, AIGC security, concept logic graph generation, feature decouple", "abstract": "Advancements in the text-to-image diffusion model have raised security concerns due to their potential to generate images with inappropriate themes such as societal biases and copyright infringements. Current studies make a great process to prevent the model from generating images containing specific high-risk visual concepts. However, these methods neglect the issue that inappropriate themes may also arise from the combination of benign visual concepts. Considering that the same image theme might be represented via multiple different visual concept combinations, and the model's generation performance of the corresponding individual visual concepts is distorted easily while processing the visual concept combination, effectively erasing such visual concept combinations from the diffusion model remains a formidable challenge. To this end, we formulate such challenge as the Concept Combination Erasing (CCE) problem and propose a Concept Graph-based high-level Feature Decoupling framework (CoGFD) to address CCE. CoGFD identifies and decomposes visual concept combinations with a consistent image theme from an LLM-induced concept logic graph, and erases these combinations through decoupling oc-occurrent high-level features. These techniques enable CoGFD to erase visual concept combinations of image content while enjoying a much less negative effect, compared to SOTA baselines, on the generative fidelity of related individual concepts. Extensive experiments on diverse visual concept combination scenarios verify the effectiveness of CoGFD.", "title_embedding_index": 12762, "title_abs_embedding_index": 12787}, {"title": "Efficient Differentiable Discovery of Causal Order", "link_suffix": "/forum?id=G19piTjVYA", "link": "https://openreview.net/forum?id=G19piTjVYA", "pdf_link": "https://openreview.net/pdf?id=G19piTjVYA", "keywords": "causality, causal inference, causal discovery, interventional data, single-variable interventions, causal structure learning", "abstract": "In the algorithm Intersort, Chevalley et al. proposed a score-based method to discover the causal order of variables in a Directed Acyclic Graph (DAG) model, leveraging interventional data to outperform existing methods. However, as a score-based method over the permutahedron, Intersort is computationally expensice and non-differentiable, limiting its ability to be utilised in problems involving large-scale datasets, such as those in genomics and climate models, or to be integrated into end-to-end gradient-based learning frameworks. We address this limitation by reformulating Intersort using differentiable sorting and ranking techniques. Our approach enables scalable and differentiable optimization of causal orderings, allowing the continuous score function to be incorporated as a regularizer in downstream tasks. Empirical results demonstrate that causal discovery algorithms benefit significantly from regularizing on the causal order, underscoring the effectiveness of our method. Our work opens the door to efficiently incorporating regularization for causal order into the training of differentiable models and thereby addresses a long-standing limitation of purely associational supervised learning.", "title_embedding_index": 12763, "title_abs_embedding_index": 12788}, {"title": "Structure-Enhanced Protein Instruction Tuning: Towards General-Purpose Protein Understanding", "link_suffix": "/forum?id=8CKgS18uWx", "link": "https://openreview.net/forum?id=8CKgS18uWx", "pdf_link": "https://openreview.net/pdf?id=8CKgS18uWx", "keywords": "Large Language Models, Insturction Tuning, Multi-modal Learning, Mixture of Experts, Protein", "abstract": "Proteins, as essential biomolecules, play a central role in biological processes, including metabolic reactions and DNA replication. Accurate prediction of their properties and functions is crucial in biological applications. Recent development of protein language models (pLMs) with supervised fine tuning provides a promising solution to this problem. However, the fine-tuned model is tailored for particular downstream prediction task, and achieving general-purpose protein understanding remains a challenge. In this paper, we introduce Structure-Enhanced Protein Instruction Tuning (SEPIT) framework to bridge this gap. Our approach integrates a noval structure-aware module into pLMs to inform them with structural knowledge, and then connects these enhanced pLMs to large language models (LLMs) to generate understanding of proteins. In this framework, we propose a novel two-stage instruction tuning pipeline that first establishes a basic understanding of proteins through caption-based instructions and then refines this understanding using a mixture of experts (MoEs) to learn more complex properties and functional information with the same amount of activated parameters. Moreover, we construct the largest and most comprehensive protein instruction dataset to date, which allows us to train and evaluate the general-purpose protein understanding model. Extensive experimental results on open-ended generation and closed-set answer tasks demonstrate the superior performance of SEPIT over both closed-source general LLMs and open-source LLMs trained with protein knowledge.", "title_embedding_index": 12764, "title_abs_embedding_index": 12789}, {"title": "Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation", "link_suffix": "/forum?id=8Lt27D1qhE", "link": "https://openreview.net/forum?id=8Lt27D1qhE", "pdf_link": "https://openreview.net/pdf?id=8Lt27D1qhE", "keywords": "3D Instance Segmentation, Transformer, Point Cloud", "abstract": "3D instance segmentation aims to predict a set of object instances in a scene and represent them as binary foreground masks with corresponding semantic labels. Currently, transformer-based methods are gaining increasing attention due to their elegant pipelines, reduced manual selection of geometric properties, and superior performance. However, transformer-based methods fail to simultaneously maintain strong position and content information during query initialization. Additionally, due to supervision at each decoder layer, there exists a phenomenon of object disappearance with the deepening of layers. To overcome these hurdles, we introduce Beyond the Final Layer: Hierarchical Query Fusion Transformer with Agent-Interpolation Initialization for 3D Instance Segmentation (BFL). Specifically, an Agent-Interpolation Initialization Module is designed to generate resilient queries capable of achieving a balance between foreground coverage and content learning. Additionally, a Hierarchical Query Fusion Decoder is designed to retain low overlap queries, mitigating the decrease in recall with the deepening of layers. Extensive experiments on ScanNetV2, ScanNet200, ScanNet++ and S3DIS datasets demonstrate the superior performance of BFL.", "title_embedding_index": 12765, "title_abs_embedding_index": 12790}, {"title": "EditMark: Training-free and Harmless Watermark for Large Language Models", "link_suffix": "/forum?id=qGLzeD9GCX", "link": "https://openreview.net/forum?id=qGLzeD9GCX", "pdf_link": "https://openreview.net/pdf?id=qGLzeD9GCX", "keywords": "Watermarking, Model Edit, Robust", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities, but their training requires extensive data and computational resources, rendering them valuable digital assets. Therefore, it is essential to watermark LLMs to protect their copyright and trace unauthorized use or resale. Existing methods for watermarking LLMs are mainly based on backdoors or knowledge injection, which require burdensome training or degrade the generation quality. To address these issues, we propose EditMark, a training-free and harmless watermarking method for LLMs based on model editing. \nWe observe LLM has diversity and can generate multiple logical and semantic correct answers to some open-ended questions. Therefore, we can use a watermark to generate a harmless mapping to control the LLM's answer to an open-ended question.\nInspired by this insight, EditMark involves generating a harmless mapping based on the watermark, selecting a secret key to generate watermarked inputs, and editing the outputs of LLM to align with the harmless mapping. \nExtensive experiments show that EditMark can embed 8-bit watermarks into LLMs within 2 minutes, with a watermark extraction success rate close to 100%. External experiments further demonstrate that EditMark has fidelity and is robust to model fine-tuning and editing attacks.", "title_embedding_index": 12766, "title_abs_embedding_index": 12791}, {"title": "Attention Is All You Need For Mixture-of-Depths Routing", "link_suffix": "/forum?id=jIAKjjEmWi", "link": "https://openreview.net/forum?id=jIAKjjEmWi", "pdf_link": "https://openreview.net/pdf?id=jIAKjjEmWi", "keywords": "Mixture of Depths, attention, parameter free routing.", "abstract": "Advancements in deep learning are driven by training models with increasingly larger numbers of parameters, which in turn heightens the computational demands. To address this issue, Mixture-of-Depths (MoD) models have been proposed to dynamically assign computations only to the most relevant parts of the inputs, thereby enabling the deployment of large-parameter models with high efficiency during inference and training. These MoD models utilize a routing mechanism to determine which tokens should be processed by a layer, or skipped. However, conventional MoD models employ additional network layers specifically for the routing which are difficult to train, and add complexity and deployment overhead to the model. In this paper, we introduce a novel attention-based routing mechanismA-MoDthat leverages the existing attention map of the preceding layer for routing decisions within the current layer. Compared to standard routing,A-MoDallows for more efficient training as it introduces no additional trainable parameters and can be easily adapted from pretrained transformer models. Furthermore, it can increase the performance of the MoD model. For instance, we observe up to $2$% higher accuracy on ImageNet and up to $2\\times$ faster transfer learning, for the first time showing the benefits of MoD on various computer vision tasks.", "title_embedding_index": 12767, "title_abs_embedding_index": 12792}, {"title": "Explanation-Assisted Data Augmentation  for Graph Learning", "link_suffix": "/forum?id=OAlPlR1g9B", "link": "https://openreview.net/forum?id=OAlPlR1g9B", "pdf_link": "https://openreview.net/pdf?id=OAlPlR1g9B", "keywords": "empirical risk minimization, Explainable Graph Neural Networks", "abstract": "This work introduces a novel class of Data Augmentation (DA) techniques in the context of graph learning. In general, DA refers to techniques that enlarge the training set using label-preserving transformations.  Such techniques enable increased robustness and generalization, especially when the size of the original training set is limited. A fundamental idea in DA is that \nlabels are invariant to domain-specific transformations of the input samples. \nHowever, it is challenging to identify such transformations in learning over graphical input domains due to the complex nature of graphs and the need to preserve their structural and semantic properties.\nIn this work, we propose explanation-assisted DA (EA-DA) for Graph Neural Networks (GNNs). A graph explanation is a subgraph which is an `almost sufficient' statistic of the input graph with respect to its classification label. Consequently, the classification label is invariant, with high probability, to perturbations of graph edges not belonging to its explanation subgraph. We develop EA-DA techniques leveraging such perturbation invariances. First, we show analytically that the sample complexity of explanation-assisted learning can be arbitrarily smaller than explanation-agnostic learning. On the other hand, we show that if the training set is enlarged using EA-DA techniques and\nthe learning rule does not distinguish between the augmented data and the original data, then the sample complexity can be worse than that of explanation-agnostic learning. We identify the main reason for the potential increase in sample complexity as the out-of-distribution nature of graph perturbations. \nWe conclude that theoretically EA-DA may improve sample complexity, and that the learning rule must distinguish between the augmented data and the original data. Subsequently, we build upon these theoretical insights, introduce practically implementable EA-DA techniques and associated learning mechanisms, and perform extensive empirical evaluations.", "title_embedding_index": 12768, "title_abs_embedding_index": 12793}, {"title": "A Scalable Temporal-Spatial Framework for Transaction Anomaly Detection in Ethereum Networks", "link_suffix": "/forum?id=q7Xi4yZYcH", "link": "https://openreview.net/forum?id=q7Xi4yZYcH", "pdf_link": "https://openreview.net/pdf?id=q7Xi4yZYcH", "keywords": "Probabilistic sampling, Temporal random walk, Graph convolutional network, Transaction anomaly detection, Ethereum networks", "abstract": "The rapid evolution of the Ethereum network necessitates sophisticated techniques to ensure its robustness against potential threats and to maintain transparency. While Graph Neural Networks (GNNs) have pioneered anomaly detection in such platforms, capturing the intricacies of both spatial and temporal transactional patterns has remained a challenge. This study presents a fusion of Graph Convolutional Networks (GCNs) with Temporal Random Walks (TRW) enhanced by probabilistic sampling to bridge this gap. Our approach, unlike traditional GCNs, leverages the strengths of TRW to discern complex temporal sequences in Ethereum transactions, thereby providing a more nuanced transaction anomaly detection mechanism. Extensive evaluations demonstrate that our TRW-GCN framework substantially advances the performance metrics over conventional GCNs in detecting irregularities such as suspiciously timed transactions, patterns indicative of token pump and dump schemes, or anomalous behavior in smart contract executions over time. As baseline algorithms for comparison, common unsupervised methods such as Isolation Forest, One-Class SVM, and DBSCAN (as classifier for TRW-GCN embedding) are employed; finally our novel TRW-GCN plus scoring method is compared with the state-of-the-art temporal graph attention algorithm.", "title_embedding_index": 12769, "title_abs_embedding_index": 12794}, {"title": "New Paradigm of Adversarial Training: Breaking Inherent Trade-Off between Accuracy and Robustness via Dummy Classes", "link_suffix": "/forum?id=sBpYRQOrMn", "link": "https://openreview.net/forum?id=sBpYRQOrMn", "pdf_link": "https://openreview.net/pdf?id=sBpYRQOrMn", "keywords": "Adversarial Training, Robustness", "abstract": "Adversarial Training (AT) is recognized as one of the most effective methods to enhance the robustness of Deep Neural Networks (DNNs). However, existing AT methods suffer from an inherent trade-off between adversarial robustness and clean accuracy, which seriously hinders their real-world deployment. Previous works have studied this trade-off within the current AT paradigm, exploring various factors such as perturbation intensity, label noise and class margin. Despite these efforts, current AT methods still typically experience a reduction in clean accuracy by over 10% to date, without significant improvements in robustness compared with simple baselines like PGD-AT. This inherent trade-off raises a question: whether the current AT paradigm, which assumes to learn the corresponding benign and adversarial samples as the same class, inappropriately combines clean and robust objectives that may be essentially inconsistent. In this work, we surprisingly reveal that up to 40% of CIFAR-10 adversarial samples always fail to satisfy such an assumption across various AT methods and robust models, explicitly indicating the improvement room for the current AT paradigm. Accordingly, to relax the tension between clean and robust learning derived from this overstrict assumption, we propose a new AT paradigm by introducing an additional dummy class for each original class, aiming to accommodate the hard adversarial samples with shifted distribution after perturbation. The robustness w.r.t. these adversarial samples can be achieved by runtime recovery from the predicted dummy classes to their corresponding original ones, eliminating the compromise with clean learning. Building on this new paradigm, we propose a novel plug-and-play AT technology named DUmmy Classes-based Adversarial Training (DUCAT). Extensive experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet demonstrate that the DUCAT concurrently improves clean accuracy and adversarial robustness compared with state-of-the-art benchmarks, effectively breaking the existing inherent trade-off. The code is available athttps://anonymous.4open.science/r/DUCAT.", "title_embedding_index": 12770, "title_abs_embedding_index": 12795}, {"title": "Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization", "link_suffix": "/forum?id=hRwxZmcvW9", "link": "https://openreview.net/forum?id=hRwxZmcvW9", "pdf_link": "https://openreview.net/pdf?id=hRwxZmcvW9", "keywords": "large language models, alignment, preference optimization", "abstract": "Direct preference optimization (DPO), a widely adopted offline preference optimization algorithm, aims to align large language models (LLMs) with human-desired behaviors using pairwise preference data. However, the winning response and the losing response within pairwise data are generated isolatedly, leading to weak correlations between them as well as suboptimal alignment performance. To address this issue, we propose an effective framework for Bridging and Modeling Correlations in pairwise data, named BMC. Firstly, we increase the consistency and informativeness of the pairwise preference signals through targeted modifications, synthesizing a pseudo-winning response by improving the losing response with the winning response as a reference. Secondly, we identify that DPO alone is insufficient to model these correlations and capture nuanced variations. Therefore, we propose learning token-level correlations by dynamically leveraging the policy model's confidence during training. Comprehensive experiments on QA, math, and instruction-following tasks demonstrate the effectiveness of our approach, significantly surpassing competitive baselines, including DPO. Additionally, our in-depth quantitative analysis reveals the reasons behind our method's superior performance over DPO and showcases its versatility to other DPO variants.", "title_embedding_index": 12771, "title_abs_embedding_index": 12796}, {"title": "Source Attribution for Large Language Model-Generated Data", "link_suffix": "/forum?id=1ou5noWgHM", "link": "https://openreview.net/forum?id=1ou5noWgHM", "pdf_link": "https://openreview.net/pdf?id=1ou5noWgHM", "keywords": "Large Language Model, Source Attirbution", "abstract": "The impressive performances of Large Language Models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the Intellectual Property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to perform source attribution by identifying the data provider who contributed to the generation of a synthetic text by an LLM. In this paper, we show that this problem can be tackled by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a source attribution framework that satisfies these key properties due to our algorithmic designs. Our framework enables an LLM to learn an accurate mapping from the generated texts to data providers, which sets the foundation for effective source attribution. Extensive empirical evaluations show that our framework achieves effective source attribution.", "title_embedding_index": 12772, "title_abs_embedding_index": 12797}, {"title": "Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm", "link_suffix": "/forum?id=u63OVngeSp", "link": "https://openreview.net/forum?id=u63OVngeSp", "pdf_link": "https://openreview.net/pdf?id=u63OVngeSp", "keywords": "causality, causal inference, causal discovery, interventional data, single-variable interventions, causal structure learning", "abstract": "Targeted and uniform interventions to a system are crucial for unveiling causal relationships. While several methods have been developed to leverage interventional data for causal structure learning, their practical application in real-world scenarios often remains challenging. Recent benchmark studies have highlighted these difficulties, even when large numbers of single-variable intervention samples are available. In this work, we demonstrate, both theoretically and empirically, that such datasets contain a wealth of causal information that can be effectively extracted under realistic assumptions about the data distribution. More specifically, we introduce the notion of interventional faithfulness, which relies on comparisons between the marginal distributions of each variable across observational and interventional settings, and we introduce a score on causal orders. Under this assumption, we are able to prove strong theoretical guarantees on the optimum of our score that also hold for large-scale settings. To empirically verify our theory, we introduce Intersort, an algorithm designed to infer the causal order from datasets containing large numbers of single-variable interventions by approximately optimizing our score. Intersort outperforms baselines (GIES, DCDI, PC and EASE) on almost all simulated data settings replicating common benchmarks in the field. Our proposed novel approach to modeling interventional datasets thus offers a promising avenue for advancing causal inference, highlighting significant potential for further enhancements under realistic assumptions.", "title_embedding_index": 12773, "title_abs_embedding_index": 12798}, {"title": "Learning Fairer Representations with FairVIC", "link_suffix": "/forum?id=sZJqKAVzKf", "link": "https://openreview.net/forum?id=sZJqKAVzKf", "pdf_link": "https://openreview.net/pdf?id=sZJqKAVzKf", "keywords": "Bias Mitigation, Fairness, Deep Learning", "abstract": "Mitigating bias in automated decision-making systems, specifically deep learning models, is a critical challenge in achieving fairness. This complexity stems from factors such as nuanced definitions of fairness, unique biases in each dataset, and the trade-off between fairness and model accuracy. To address such issues, we introduce FairVIC, an innovative approach designed to enhance fairness in neural networks by addressing inherent biases at the training stage. Unlike other methods that require a user-defined declaration of what it means to be fair, FairVIC integrates an abstract concept of fairness through variance, invariance and covariance terms into the loss function. These terms aim to minimise the model's dependency on protected characteristics for making predictions, thus promoting fairness. Our experimentation consists of evaluating FairVIC against other comparable bias mitigation techniques, on a number of datasets known for their biases. Additionally, we conduct an ablation study to examine the accuracy-fairness trade-off. We also extend FairVIC by offering multi-objective lambda recommendations, allowing users to train a fairer model with a set of weights that are tuned best for their application. Through our implementation of FairVIC, we observed a significant improvement in fairness across all metrics tested, without compromising the model's accuracy. Our findings suggest that FairVIC presents a straightforward, out-of-the-box solution for the development of fairer deep learning models, thereby offering a generalisable solution applicable across many tasks and datasets.", "title_embedding_index": 12774, "title_abs_embedding_index": 12799}]