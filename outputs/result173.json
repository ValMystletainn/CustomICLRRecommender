[
    {
        "title": "Effects of Scale on Language Model Robustness",
        "link_suffix": "/forum?id=IAFLoDz6H5",
        "link": "https://openreview.net/forum?id=IAFLoDz6H5",
        "pdf_link": "https://openreview.net/pdf?id=IAFLoDz6H5",
        "keywords": "ai safety, language models, adversarial attacks, robustness, scaling laws",
        "abstract": "Language models exhibit scaling laws, whereby increasing model and dataset size  yields predictable decreases in negative log likelihood, unlocking a dazzling array of capabilities. This phenomenon spurs many companies to train ever larger models in pursuit of ever improved performance. Yet, these models are vulnerable to adversarial inputs such as \u201cjailbreaks\u201d and prompt injections that induce models to perform undesired behaviors, posing a growing risk as models become more capable. Prior work indicates that computer vision models become more robust with model and data scaling, raising the question: does language model robustness also improve with scale?We study this question empirically in the classification setting, finding that without explicit defense training, larger models tend to be modestly more robust on most tasks, though the effect is not reliable.\nEven with the advantage conferred by scale, undefended models remain easy to attack in absolute terms, and we thus turn our attention to explicitly training models for adversarial robustness, which we show to be a much more compute-efficient defense than scaling model size alone.\nIn this setting, we also observe that adversarially trained larger models generalize faster and better to modified attacks not seen during training when compared with smaller models.\nFinally, we analyze the offense/defense balance of increasing compute, finding parity in some settings and an advantage for offense in others, suggesting that adversarial training alone is not sufficient to solve robustness, even at greater model scales."
    },
    {
        "title": "ChuLo: Chunk-Level Key Information Representation for Efficient Long Document Processing",
        "link_suffix": "/forum?id=37mG1vvEKf",
        "link": "https://openreview.net/forum?id=37mG1vvEKf",
        "pdf_link": "https://openreview.net/pdf?id=37mG1vvEKf",
        "keywords": "Long Document Processing, Long Document Classification, Long Document Tagging",
        "abstract": "Transformer-based models have achieved remarkable success in various Natural Language Processing (NLP) tasks, yet their ability to handle long documents is constrained by computational limitations. Traditional approaches, such as truncating inputs, sparse self-attention, and chunking, attempt to mitigate these issues, but they often lead to information loss and hinder the model's ability to capture long-range dependencies. In this paper, we introduce ChuLo, a novel chunk representation method for long document classification that addresses these limitations. Our ChuLo groups input tokens using unsupervised keyphrase extraction, emphasizing semantically important keyphrase based chunk to retain core document content while reducing input length. This approach minimizes information loss and improves the efficiency of Transformer-based models. Preserving all tokens in long document understanding, especially token classification tasks, is especially important to ensure that fine-grained annotations, which depend on the entire sequence context, are not lost. We evaluate our method on multiple long document classification tasks and long document token classification tasks, demonstrating its effectiveness through comprehensive qualitative and quantitative analyses."
    },
    {
        "title": "Hawkes process revisited: balancing interpretability and flexibility with contextualized event embeddings and a neural impact kernel",
        "link_suffix": "/forum?id=0mdUV1pLGP",
        "link": "https://openreview.net/forum?id=0mdUV1pLGP",
        "pdf_link": "https://openreview.net/pdf?id=0mdUV1pLGP",
        "keywords": "Event sequence, Hawkes Process, Interpretability, Embedding Space",
        "abstract": "The Hawkes process (HP) is commonly used to model event sequences with selfreinforcing dynamics, including electronic health records, stock trades, and social media interactions. Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which can be critical in medicine and other high-stakes settings. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs, because we do not assume a particular parametric form for the impact functions, yet more interpretable than other neural network approaches, because self-reinforcing dynamics are still entirely captured by the impact kernel, which can be inspected. If needed, our approach allows us to trade interpretability for flexibility by contextualizing the event embeddings with transformer encoder layers. Results show that our method accurately recovers impact functions in simulations and achieves competitive performance on real-world datasets even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics effectively, implying that interpretability can be maintained without loss of performance."
    },
    {
        "title": "FedDES: A Discrete-Event Simulator For Large-Scale Federated Learning",
        "link_suffix": "/forum?id=pZk9cUu8p6",
        "link": "https://openreview.net/forum?id=pZk9cUu8p6",
        "pdf_link": "https://openreview.net/pdf?id=pZk9cUu8p6",
        "keywords": "Federated Learning, Discrete Event-Driven Simulations",
        "abstract": "We introduce FedDES, a performance simulator for Federated Learning (FL) that leverages Discrete Event Simulation (DES) techniques to model key events\u2014such as client updates, communication delays, and aggregation operations\u2014as discrete occurrences in time. This approach accurately captures the runtime features of FL systems, providing a high-fidelity simulation environment that closely mirrors real-world deployments. FedDES incorporates all three known aggregation settings: Synchronous (e.g., FedAvg and FedProx), Asynchronous (e.g., FedAsync and FedFa), and Semi-Asynchronous (e.g., FedBuff and FedCompass). Designed to be framework-, dataset-, and model-agnostic, FedDES allows researchers and developers to explore various configurations without restrictions. Our evaluations involving over 1,000 clients with heterogeneous computation and communication characteristics demonstrate that FedDES accurately models event distribution and delivers performance estimates within 2% error of real-world measurements. While real-world workloads often take hours to evaluate, FedDES generates detailed, timestamped event logs in just few seconds. As a result, FedDES can significantly accelerate FL developing and debugging cycles, enabling developers to rapidly prototype and evaluate algorithms and system designs, bypassing the need for costly, time-consuming real-world deployments. It offers valuable performance insights\u2014such as identifying bottlenecks, stragglers, fault-tolerance mechanisms, and edge-case scenarios\u2014facilitating the optimization of FL systems for efficiency, scalability, and resilience."
    },
    {
        "title": "Unsupervised Reinforcement Learning by Maximizing Skill Density Deviation",
        "link_suffix": "/forum?id=RKB4WiesB4",
        "link": "https://openreview.net/forum?id=RKB4WiesB4",
        "pdf_link": "https://openreview.net/pdf?id=RKB4WiesB4",
        "keywords": "Unsupervised Reinforcement Learning, Skill Discovery, Inter-Skill Diversity, Intra-Skill Exploration",
        "abstract": "Unsupervised Reinforcement Learning (RL) aims to discover diverse behaviors that can accelerate the learning of downstream tasks. Previous methods typically focus on entropy-based exploration or empowerment-driven skill learning. However, entropy-based exploration struggles in large-scale state spaces (e.g., images), and empowerment-based methods with Mutual Information (MI) estimations have limitations in state exploration. To address these challenges, we propose a novel skill discovery objective that maximizes the deviation of the state density of one skill from the explored regions of other skills, encouraging inter-skill state diversity similar to the initial MI objective. For state-density estimation, we construct a novel conditional autoencoder with soft modularization for different skill policies in high-dimensional space. To incentivize intra-skill exploration, we formulate an intrinsic reward based on the learned autoencoder that resembles count-based exploration in a compact latent space. Through extensive experiments in challenging state and image-based tasks, we find our method learns meaningful skills and achieves superior performance in various downstream tasks."
    },
    {
        "title": "On Large Language Model Continual Unlearning",
        "link_suffix": "/forum?id=Essg9kb4yx",
        "link": "https://openreview.net/forum?id=Essg9kb4yx",
        "pdf_link": "https://openreview.net/pdf?id=Essg9kb4yx",
        "keywords": "Continual Unlearning, Large Language Models",
        "abstract": "While large language models have demonstrated impressive performance across various domains and tasks, their security issues have become increasingly severe. Machine unlearning has emerged as a representative approach for model safety and security, removing the influence of undesired data on the target model. However, these methods do not sufficiently consider that unlearning requests in real-world scenarios are continuously emerging, especially in the context of LLMs, which may lead to accumulated model utility loss that eventually becomes unacceptable. Moreover, existing LLM unlearning methods often ignore previous data access limitations due to privacy concerns and copyright protection. Without previous data, the utility preservation during unlearning is much harder. To overcome these challenges, we propose the O3 framework that includes an \\underline{\\textit{O}}rthogonal low-rank adapter (LoRA) for continually unlearning requested data and an \\underline{\\textit{O}}ut-\\underline{\\textit{O}}f-Distribution (OOD) detector to measure the similarity between input and unlearning data. The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests. The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism. During inference, our O3 framework can decide whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity between the input and the unlearned knowledge. Notably, O3's effectiveness does not rely on any retained data. We conducted extensive experiments on O3 and state-of-the-art LLM unlearning methods across three tasks and seven datasets. The results indicate that O3 consistently achieves the best unlearning effectiveness and utility preservation, especially when facing continuous unlearning requests. The source codes can be found at \\url{https://anonymous.4open.science/r/O3-A02B}."
    },
    {
        "title": "Reward Dimension Reduction for Scalable Multi-Objective Reinforcement Learning",
        "link_suffix": "/forum?id=ssRdQimeUI",
        "link": "https://openreview.net/forum?id=ssRdQimeUI",
        "pdf_link": "https://openreview.net/pdf?id=ssRdQimeUI",
        "keywords": "Multi-Objective Reinforcement Learning, Reinforcement Learning",
        "abstract": "In this paper, we introduce a simple yet effective reward dimension reduction method to tackle the scalability challenges of multi-objective reinforcement learning algorithms. While most existing approaches focus on optimizing two to four objectives, their abilities to scale to environments with more objectives remain uncertain. Our method uses a dimension reduction approach to enhance learning efficiency and policy performance in multi-objective settings. While most traditional dimension reduction methods are designed for static datasets, our approach is tailored for online learning and preserves Pareto-optimality after transformation. We propose a new training and evaluation framework for reward dimension reduction in multi-objective reinforcement learning and demonstrate the superiority of our method in an environment with sixteen objectives, significantly outperforming existing online dimension reduction methods."
    },
    {
        "title": "Self-Exploring Language Models: Active Preference Elicitation for Online Alignment",
        "link_suffix": "/forum?id=4zQ5eIPtMp",
        "link": "https://openreview.net/forum?id=4zQ5eIPtMp",
        "pdf_link": "https://openreview.net/pdf?id=4zQ5eIPtMp",
        "keywords": "Online Alignment, Large Language Model",
        "abstract": "Preference optimization, particularly through Reinforcement Learning from Human Feedback (RLHF), has achieved significant success in aligning Large Language Models (LLMs) to adhere to human intentions. Unlike offline alignment with a fixed dataset, online feedback collection from humans or AI on model generations typically leads to more capable reward models and better-aligned LLMs through an iterative process. However, achieving a globally accurate reward model requires systematic exploration to generate diverse responses that span the vast space of natural language. Random sampling from standard reward-maximizing LLMs alone is insufficient to fulfill this requirement. To address this issue, we propose a bilevel objective optimistically biased towards potentially high-reward responses to actively explore out-of-distribution regions. By solving the inner-level problem with the reparameterized reward function, the resulting algorithm, named Self-Exploring Language Models (SELM), eliminates the need for a separate RM and iteratively updates the LLM with a straightforward objective. Compared to Direct Preference Optimization (DPO), the SELM objective reduces indiscriminate favor of unseen extrapolations and enhances exploration efficiency. Our experimental results demonstrate that when fine-tuned on Zephyr-7B-SFT and Llama-3-8B-Instruct models, SELM significantly boosts the performance on instruction-following benchmarks such as MT-Bench and AlpacaEval 2.0, as well as various standard academic benchmarks in different settings."
    },
    {
        "title": "Structural Probing with Feature Interaction",
        "link_suffix": "/forum?id=RBqvU12SHz",
        "link": "https://openreview.net/forum?id=RBqvU12SHz",
        "pdf_link": "https://openreview.net/pdf?id=RBqvU12SHz",
        "keywords": "Shapley Interactions, Shapley Taylor interaction indices, Masked Language Models, Language Models, Feature Interaction",
        "abstract": "Measuring nonlinear feature interaction is an established approach to understanding complex patterns of attribution in many models. In this paper, we use Shapley Taylor interaction indices (STII) to analyze the impact of underlying data structure on model representations in a variety of modalities, tasks, and architectures. Considering linguistic structure in masked and auto-regressive language models (MLMs and ALMs), we find that STII increases within idiomatic expressions and that Transformer ALMs scale STII with syntactic distance, just as LSTM-based ALMs do. Our speech model findings reflect the phonetic principal that the openness of the oral cavity determines how much a phoneme's acoustics vary based on context. Our wide range of results illustrates the benefits of interdisciplinary work and domain expertise in interpretability research."
    },
    {
        "title": "Forming Scalable, Convergent GNN Layers that Minimize a Sampling-Based Energy",
        "link_suffix": "/forum?id=Gq7RDMeZi4",
        "link": "https://openreview.net/forum?id=Gq7RDMeZi4",
        "pdf_link": "https://openreview.net/pdf?id=Gq7RDMeZi4",
        "keywords": "Graph Neural Networks, Energy-based Models, Scalable Training, Bi-level Optimization",
        "abstract": "Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit transparent, exploitable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability when applied to the largest publicly-available node classification benchmark exceeding 1TB in size."
    },
    {
        "title": "MissDiff: Training Diffusion Models on Tabular Data with Missing Values",
        "link_suffix": "/forum?id=PyyoSwPaSa",
        "link": "https://openreview.net/forum?id=PyyoSwPaSa",
        "pdf_link": "https://openreview.net/pdf?id=PyyoSwPaSa",
        "keywords": "Diffusion Model, Missing Value, Tabular Data",
        "abstract": "The diffusion model has shown remarkable performance in modeling data distributions and synthesizing data. However, the vanilla diffusion model requires complete or fully observed training data. Incomplete data is a common issue in various real-world applications, including healthcare and finance, particularly when dealing with tabular datasets. This work considers learning from data with missing values for missing value imputations and generating synthetic complete data in a unified framework. With minimal assumptions on the missing mechanisms, our method models the score of complete data distribution by denoising score matching on data with missing values. We prove that the proposed method can recover the score of the complete data distribution, and the proposed training objective serves as an upper bound for the negative likelihood of observed data. Extensive experiments on imputation tasks together with generation tasks demonstrate that our proposed framework outperforms existing state-of-the-art approaches on multiple tabular datasets."
    },
    {
        "title": "Robult: A Scalable Framework for Semi-Supervised Multimodal Learning with Missing Modalities",
        "link_suffix": "/forum?id=c0PnZCNY2N",
        "link": "https://openreview.net/forum?id=c0PnZCNY2N",
        "pdf_link": "https://openreview.net/pdf?id=c0PnZCNY2N",
        "keywords": "Multimodal learning, Semi-supervised learning, Missing modalities",
        "abstract": "In multimodal learning, the presence of missing modalities and limited labeled data presents significant challenges for building robust models. We proposeRobult, a novel framework designed to address these challenges by leveraging an information-theoretic approach to preserve modality-specific features and synergistic information across modalities. Our model introduces two key objectives: (1) a latent reconstruction loss to retain unique modality-specific information, and (2) a novel soft Positive-Unlabeled (PU) contrastive loss to efficiently utilize sparse labeled data in semi-supervised settings. Robult seamlessly integrates into deep learning architectures, enhancing performance across multiple downstream tasks and ensuring robustness even when modalities are missing at inference time. Empirical results across diverse datasets demonstrate that Robult surpasses existing methods in handling both semi-supervised learning and missing modalities, while its lightweight design enables scalability and easy integration with existing frameworks."
    },
    {
        "title": "Visual Prompting Reimagined: The Power of Activation Prompts",
        "link_suffix": "/forum?id=3gwNb8qZDr",
        "link": "https://openreview.net/forum?id=3gwNb8qZDr",
        "pdf_link": "https://openreview.net/pdf?id=3gwNb8qZDr",
        "keywords": "visual prompt, parameter efficient finetuning, learning theory, generalization analysis",
        "abstract": "Visual prompting (VP) has emerged as a popular method to repurpose large pretrained models for downstream vision tasks. Unlike many parameter-efficient fine-tuning (PEFT) techniques that modify model parameters, VP introduces a universal perturbation directly into the input data to facilitate task-specific fine-tuning while keeping the pretrained model intact. However, there exists a noticeable performance gap between VP and conventional fine-tuning methods, highlighting an unexplored realm in theory and practice to understand and advance VP to close its performance gap. Towards this end, we introduce a novel concept, termed activation prompt (AP), which extends the scope of input-level VP by enabling universal perturbations to be applied to activation maps within the intermediate layers of the model. With the aid of AP, we unveil the intrinsic limitations of VP in both performance and efficiency. We also show that AP shares a close connection to normalization tuning used in convolutional neural networks (CNNs) and vision transformers (ViTs), albeit with variations in layer preferences for prompting. We theoretically elucidate the rationale behind such preference by analyzing global features across layers. By conducting extensive experiments across 29 datasets and various model architectures, we provide a thorough performance analysis of AP, comparing it with VP and PEFT baselines. Our experimental results demonstrate that AP significantly surpasses the input-level VP in terms of both accuracy and efficiency, considering factors like time, parameters, memory usage, and throughout."
    },
    {
        "title": "Once-for-All: Controllable Generative Image Compression with Dynamic Granularity Adaption",
        "link_suffix": "/forum?id=z0hUsPhwUN",
        "link": "https://openreview.net/forum?id=z0hUsPhwUN",
        "pdf_link": "https://openreview.net/pdf?id=z0hUsPhwUN",
        "keywords": "image compression, vqgan, generative compression model, multi-grained representation",
        "abstract": "Although recent generative image compression methods have demonstrated impressive potential in optimizing the rate-distortion-perception trade-off, they still face the critical challenge of flexible rate adaption to diverse compression necessities and scenarios. To overcome this challenge, this paper proposes a $\\textbf{Control}$lable $\\textbf{G}$enerative $\\textbf{I}$mage $\\textbf{C}$ompression framework, $\\textbf{Control-GIC}$, the first capable of fine-grained bitrate adaption across a broad spectrum while ensuring high-fidelity and generality compression. We base $\\textbf{Control-GIC}$ on a VQGAN framework representing an image as a sequence of variable-length codes ($\\textit{i.e.}$ VQ-indices), which can be losslessly compressed and exhibits a direct positive correlation with the bitrates. Drawing inspiration from the classical coding principle, we correlate the information density of local image patches with their granular representations. Hence, we can flexibly determine a proper allocation of granularity for the patches to achieve dynamic adjustment for VQ-indices, resulting in desirable compression rates. We further develop a probabilistic conditional decoder capable of retrieving historic encoded multi-granularity representations according to transmitted codes, and then reconstruct hierarchical granular features in the formalization of conditional probability, enabling more informative aggregation to improve reconstruction realism. Our experiments show that $\\textbf{Control-GIC}$ allows highly flexible and controllable bitrate adaption where the results demonstrate its superior performance over recent state-of-the-art methods."
    },
    {
        "title": "Correlational Lagrangian Schrodinger Bridge: Learning Dynamics with Population-Level Regularization",
        "link_suffix": "/forum?id=VJ75CB8DZo",
        "link": "https://openreview.net/forum?id=VJ75CB8DZo",
        "pdf_link": "https://openreview.net/pdf?id=VJ75CB8DZo",
        "keywords": "generative models, diffusion models",
        "abstract": "Modeling population dynamics is a fundamental problem with broad scientific applications.\nMotivated by real-world applications including biosystems with diverse populations, we consider a class of population dynamics modeling with two technical challenges: (i) dynamics to learn for individual particles areheterogeneousand (ii) available data to learn from arenot time-series(i.e, each individual's state trajectory over time) butcross-sectional(i.e, the whole population's aggregated states without individuals matched over time).\nTo address the challenges, we introduce a novel computational framework dubbedcorrelational Lagrangian Schr\"odinger bridge(CLSB) that builds on optimal transport  to \"bridge\" cross-sectional data distributions. In contrast to prior methods regularizing all individuals' transport \"costs\" and then applying them to the populationhomogeneously, CLSB directly regularizespopulationcost allowing for populationheterogeneityand potentially improving modelgeneralizability.\nSpecifically our contributions include(1)a novel population perspective of the transport cost and a new class of population regularizers capturing the temporal variations in multivariate relations, with the tractable formulation derived,(2)three domain-informed instantiations of population regularizers on covariance, and(3)integration of population regularizers into data-driven generative models as constrained optimization and an approximate numerical solution, with further extension to conditional generative models.\nEmpirically, we demonstrate the superiority of CLSB in single-cell sequencing data analyses (including cell differentiation and drug-conditioned cell responses) and opinion depolarization.\nCodes will be released upon acceptance."
    },
    {
        "title": "Underestimated Privacy Risks for Minority Populations in Large Language Model Unlearning",
        "link_suffix": "/forum?id=Hj1D0Xq3Ef",
        "link": "https://openreview.net/forum?id=Hj1D0Xq3Ef",
        "pdf_link": "https://openreview.net/pdf?id=Hj1D0Xq3Ef",
        "keywords": "Machine Unlearning, Large Language Models, Membership Inference Attack, Privacy Risk, Minority Groups",
        "abstract": "Large Language Models (LLMs) are trained on extensive datasets that often contain sensitive, human-generated information, raising significant concerns about privacy breaches. While certified unlearning approaches offer strong privacy guarantees, they rely on restrictive model assumptions that are not applicable to LLMs. As a result, various unlearning heuristics have been proposed, with privacy risks typically assessed empirically. The standard evaluation pipelines usually randomly select data for removal from the training set, apply unlearning techniques, and use membership inference attacks (MIAs) to compare the unlearned models against models retrained without the removed data. In this paper, we identify a critical flaw in this widely adopted evaluation approach: the privacy risks faced by minority groups within the training data are often significantly underestimated. We substantiate this claim through carefully designed experiments, including unlearning canaries related to minority groups, inspired by privacy auditing literature. Using personally identifiable information (PII) as a representative minority identifier, we demonstrate that minority groups experience at least 20% more privacy leakage in most cases across combinations of six unlearning approaches, three variants of MIAs, three benchmark datasets, and two LLMs of different scales. Given that the right to be forgotten should be upheld for every individual, we advocate for a more rigorous evaluation of LLM unlearning methods. Our minority-aware evaluation framework represents an initial step toward ensuring more equitable and thorough assessments of LLM unlearning efficacy."
    },
    {
        "title": "Benchmarking Intelligent LLM Agents for Conversational Data Analysis",
        "link_suffix": "/forum?id=1zgil8py5o",
        "link": "https://openreview.net/forum?id=1zgil8py5o",
        "pdf_link": "https://openreview.net/pdf?id=1zgil8py5o",
        "keywords": "Conversational Data Analysis, Large Language Models, Benchmark, Multi-agent Environment, Adaptive Interaction Reflection, Decision-making",
        "abstract": "Conversational Data Analysis, a collaboration between humans and machines, enables real-time data exploration for informed decision-making. The challenges and costs of collecting realistic conversational logs for data analysis hinder comprehensive quantitative evaluation of Large Language Models (LLMs) in this task. To mitigate this issue, we introduceTapilot-Crossing, a new benchmark to evaluate LLMs on conversational data analysis.Tapilot-Crossingcontains 1024 conversations, covering 4 practical scenarios:Normal,Action,Private, andPrivate Action. Notably,Tapilot-Crossingis constructed by an economical multi-agent environment,Decision Company, with few human efforts. This environment ensures efficiency and scalability of generating new conversational data. Our comprehensive study, conducted by data analysis experts, demonstrates that Decision Company is capable of producing diverse and high-quality data, laying the groundwork for efficient data annotation. We evaluate popular and advanced LLMs inTapilot-Crossing, which highlights the challenges of conversational data analysis. Furthermore, we proposeAdaptiveConversationReflection (ACR), a self-generated reflection strategy that guides LLMs tolearn from successful histories.\nExperiments demonstrate thatACRcan evolve LLMs into effective conversational data analysis agents, achieving a relative performance improvement of up to 44.5%."
    },
    {
        "title": "Reward-Augmented Data Enhances Direct Preference Alignment of LLMs",
        "link_suffix": "/forum?id=mjtCqmujYP",
        "link": "https://openreview.net/forum?id=mjtCqmujYP",
        "pdf_link": "https://openreview.net/pdf?id=mjtCqmujYP",
        "keywords": "Preference Alignment, Large Language Model, RLAIF",
        "abstract": "Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to responses with the highest rewards, which are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. This dataset is easily integrated with existing direct alignment algorithms and is applicable to any preference dataset. The experimental results across instruction-following benchmarks including AlpacaEval 2.0, MT-Bench, and Arena-Hard-Auto demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models such as Zephyr, Mistral, Qwen2, Llama3.1, Gemma2, and SPPO. Additionally, on six academic benchmarks including GSM8K, GPQA, MUSR, TruthfulQA, BBH, and ARC, our method improves their average accuracy. When applying our method to on-policy data, the resulting DPO model outperforms various baselines and achieves state-of-the-art results on AlpacaEval 2.0. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere dataset expansion."
    },
    {
        "title": "System 1.x: Learning to Balance Fast and Slow Planning with Language Models",
        "link_suffix": "/forum?id=zd0iX5xBhA",
        "link": "https://openreview.net/forum?id=zd0iX5xBhA",
        "pdf_link": "https://openreview.net/pdf?id=zd0iX5xBhA",
        "keywords": "Large Language Models, Planning",
        "abstract": "Language models can be used to solve long-horizon planning problems in two distinct modes. In a fast 'System-1' mode, models directly generate plans without any explicit search or backtracking, and in a slow 'System-2' mode, they plan step-by-step by explicitly searching over possible actions. System-2 planning, while typically more effective, is also computationally more expensive and often infeasible for long plans or large action spaces. Moreover, isolated System-1 or System-2 planning ignores the user's end goals and constraints (e.g., token budget), failing to provide ways for the user to control the model's behavior. To this end, we propose the System-1.x Planner, a framework for controllable planning with language models that is capable of generating hybrid plans and balancing between the two planning modes based on the difficulty of the problem at hand. System-1.x consists of (i) a controller, (ii) a System-1 Planner, and (iii) a System-2 Planner. Based on a user-specified hybridization factor x governing the degree to which the system uses System-1 vs. System-2, the controller decomposes a planning problem into subgoals, and classifies them as easy or hard to be solved by either System-1 or System-2, respectively. We fine-tune all three components on top of a single base LLM, requiring only search traces as supervision. Experiments with two diverse planning tasks -- Maze Navigation and Blocksworld -- show that our System-1.x Planner outperforms a System-1 Planner, a System-2 Planner trained to approximate A* search, and also a symbolic planner (A* search), given an exploration budget. We also demonstrate the following key properties of our planner: (1) controllability: by adjusting the hybridization factor x (e.g., System-1.75 vs. System-1.5) we can perform more (or less) search, improving performance, (2) flexibility: by building a neuro-symbolic variant composed of a neural System-1 planner and a symbolic System-2 planner, we can take advantage of existing symbolic methods, and (3) generalizability: by learning from different search algorithms (BFS, DFS, A*), we show that our method is robust to the choice of search algorithm used for training."
    },
    {
        "title": "Multi-Agent Decision S4: Leveraging State Space Models for Offline Multi-Agent Reinforcement Learning",
        "link_suffix": "/forum?id=c9kxrkabXL",
        "link": "https://openreview.net/forum?id=c9kxrkabXL",
        "pdf_link": "https://openreview.net/pdf?id=c9kxrkabXL",
        "keywords": "offline multi-agent reinforcement learning, sequence-based offline reinforcement learning",
        "abstract": "Sequence-based supervised learning with transformers has been successfully applied to tackle offline reinforcement learning in single-agent settings. However, extending these algorithms to offline multi-agent reinforcement learning (offline MARL) settings still remains to be a challenge. Existing transformer-based approaches for offline MARL either train agents independently, without fully considering them as a multi-agent system or depend on a centralized transformer model, which suffers from scalability issues. Additionally, transformers have inherent constraints, particularly in regard to managing long-term dependencies and computational efficiency. In light of the recent success of Structured State Space Sequence (S4) models in sequence modeling, which offer greater parameter efficiency, faster inference times, and enhanced ability to handle longer context lengths, we propose leveraging S4-based models in our work. We perform offline training utilizing the efficient convolutional view of S4 followed by on-policy fine-tuning utilizing its recurrent dynamics. To foster scalable cooperation between agents, the multi-agent decision making problem is sequentially expanded where the agents take actions one after another at each time step. This design enables agents to exhibit better cooperative behavior by basing their actions on the decisions and learned behaviors of preceding agents with minimal communication. During offline training, this dependency facilitates gradient flow from one agent to its predecessors, leading to more stable learning and improved overall team performance. Based on experiments performed on challenging MARL benchmarks of Multi-Robot Warehouse (RWARE) and StarCraft Multi-Agent Challenge (SMAC), we demonstrate that the developed algorithm significantly outperforms the state-of-the-art offline RL-based and transformer-based MARL baselines across most tasks."
    },
    {
        "title": "LongSafetyBench: Long-Context LLMs Struggle with Safety Issues",
        "link_suffix": "/forum?id=dQzpP9ziaJ",
        "link": "https://openreview.net/forum?id=dQzpP9ziaJ",
        "pdf_link": "https://openreview.net/pdf?id=dQzpP9ziaJ",
        "keywords": "Long-Context Language Model, Model Safety, Benchmark",
        "abstract": "With the development of large language models (LLMs), the sequence length of these models continues to increase, drawing significant attention to long-context language models. However, the evaluation of these models has been primarily limited to their capabilities, with a lack of research focusing on their safety. Existing work, such as ManyShotJailbreak, has to some extent demonstrated that long-context language models can exhibit safety concerns. However, the methods used are limited and lack comprehensiveness. In response, we introduceLongSafetyBench, the first benchmark designed to objectively and comprehensively evaluate the safety of long-context models. LongSafetyBench consists of 10 task categories, with an average length of 41,889 words. After testing eight long-context language models on LongSafetyBench, we found that existing models generally exhibit insufficient safety capabilities. Moreover, models' safety performance in long-context scenarios does not always align with that in short-context scenarios. Further investigation revealed that long-context models tend to overlook harmful content within lengthy texts. We also proposed a simple yet effective solution, allowing open-source models to achieve performance comparable to that of top-tier closed-source models. We believe that LongSafetyBench can serve as a valuable benchmark for evaluating the safety capabilities of long-context language models. We hope that our work will encourage the broader community to pay attention to the safety of long-context models and contribute to the development of solutions to improve the safety of long-context LLMs."
    },
    {
        "title": "RLHS: Mitigating Misalignment in RLHF with Hindsight Simulation",
        "link_suffix": "/forum?id=QipLSeLQRS",
        "link": "https://openreview.net/forum?id=QipLSeLQRS",
        "pdf_link": "https://openreview.net/pdf?id=QipLSeLQRS",
        "keywords": "Reinforcement Learning from Human Feedback (RLHF), Human-AI Alignment, Large Language Models, AI Safety, Partial Observability",
        "abstract": "Generative AI systems like foundation models (FMs) must align well with human values to ensure their behavior is helpful and trustworthy. While Reinforcement Learning from Human Feedback (RLHF) has shown promise for optimizing model performance using human judgments, existing RLHF pipelines predominantly rely onimmediatefeedback, which can fail to reflect the true downstream impact of an interaction on users' utility. We demonstrate that this shortsighted feedback can, by itself, result in misaligned behaviors like sycophancy and deception, and we propose to alleviate this by refocusing RLHF ondownstream consequences. Our theoretical analysis reveals that the hindsight gained by simply delaying human feedback mitigates misalignment and improves expected human utility. To leverage this insight in a practical alignment algorithm, we introduce Reinforcement Learning from Hindsight Simulation (RLHS), which first simulates plausible consequences and then elicits feedback to assess what behaviors were genuinely beneficial in hindsight. We apply RLHS to two widely-employed online and offline preference optimization methods---Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO)---and show empirically that misalignment is significantly reduced with both methods. Through an online human user study, we show that RLHS consistently outperforms RLHF in helping users achieve their goals and earns higher satisfaction ratings, despite being trained solely with simulated hindsight feedback. These results underscore the importance of focusing on long-term consequences, even simulated ones, to mitigate misalignment in RLHF."
    },
    {
        "title": "Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels",
        "link_suffix": "/forum?id=OwpLQrpdwE",
        "link": "https://openreview.net/forum?id=OwpLQrpdwE",
        "pdf_link": "https://openreview.net/pdf?id=OwpLQrpdwE",
        "keywords": "Dynamics on manifolds, Operator-valued kernel, Geometry-preserving time integration, Ordinary differential equations",
        "abstract": "We address the problem of learning ordinary differential equations (ODEs) on manifolds. Existing machine learning methods, particularly those using neural networks, often struggle with high computational demands and lack of interpretability. To overcome this issue, we introduce a geometrically constrained operator-valued kernel that allows us to represent vector fields on tangent bundles of smooth manifolds. The construction of the kernel imposes the geometric constraints that are estimated from the data and ensures the computational feasibility for learning high dimensional systems of ODEs. Once the vector fields are estimated, e.g., by the kernel ridge regression, we need an ODE solver that guarantees the solution to stay on (close to) the manifold. To overcome this issue, we propose a geometry-preserving ODE solver that approximates the exponential maps corresponding to the ODE solutions. We will numerically demonstrate the robustness of the proposed scheme on simple test problems that illustrate the shortcoming of the standard Runge-Kutta methods that only works on simple geometry. We will also deduce a theoretical error bound for the proposed solvers that guarantees the approximate solutions to lie on the manifold in the limit of large data. We verify the effectiveness of the proposed approach on high-dimensional dynamical systems, including the cavity flow problem, the beating and traveling waves in Kuramoto-Sivashinsky equations, and the reaction-diffusion dynamics."
    },
    {
        "title": "Scaling Concept With Text-Guided Diffusion Models",
        "link_suffix": "/forum?id=HafxTJjo6a",
        "link": "https://openreview.net/forum?id=HafxTJjo6a",
        "pdf_link": "https://openreview.net/pdf?id=HafxTJjo6a",
        "keywords": "Diffusion models, zero-shot applications, image/audio",
        "abstract": "Text-guided diffusion models have revolutionized generative tasks by producing high-fidelity content based on text descriptions. Additionally, they have enabled an editing paradigm where concepts can be replaced through text conditioning. In this work, we explore a novel paradigm: instead of replacing a concept, can we scale it? We conduct an empirical study to investigate concept decomposition trends in text-guided diffusion models. Leveraging these insights, we propose a simple yet effective method, ScalingConcept, designed to enhance or suppress existing concepts in real input without introducing new ones. To systematically evaluate our method, we introduce the WeakConcept-10 dataset. More importantly, ScalingConcept enables a range of novel zero-shot applications across both image and audio domains, including but not limited to canonical pose generation and generative sound highlighting/removal."
    },
    {
        "title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment",
        "link_suffix": "/forum?id=lXwhR7uci1",
        "link": "https://openreview.net/forum?id=lXwhR7uci1",
        "pdf_link": "https://openreview.net/pdf?id=lXwhR7uci1",
        "keywords": "TestAgent, Large Language Model, Adaptive Testing, Personalized Testing\uff0cIntelligent Assessment",
        "abstract": "Accurately assessing internal human states is critical for understanding their preferences, providing personalized services, and identifying challenges in various real-world applications. Originating from psychology, adaptive testing has become the mainstream method for human measurement. It customizes assessments by selecting the fewest necessary test questions (e.g., math problems) based on the examinee's performance (e.g., answer correctness), ensuring precise evaluation. However, current adaptive testing methods still face several challenges. The mechanized nature of most adaptive algorithms often leads to guessing behavior and difficulties in addressing open-ended questions. Additionally, subjective assessments suffer from noisy response data and coarse-grained test outputs, further limiting their effectiveness.\nTo move closer to an ideal adaptive testing process, we propose TestAgent, a large language model (LLM)-empowered adaptive testing agent designed to enhance adaptive testing through interactive engagement. This marks the first application of LLMs in adaptive testing. To ensure effective assessments, TestAgent supports personalized question selection, captures examinees' response behavior and anomalies, and provides precise testing outcomes through dynamic, conversational interactions.\nExtensive experiments on psychological, educational, and lifestyle assessments demonstrates that our approach achieves more accurate human assessments with approximately 20% fewer test questions compared to state-of-the-art baselines. In actual tests, it received testers' favor in terms of speed, smoothness, and other two dimensions."
    }
]