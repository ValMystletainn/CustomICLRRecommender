[
    {
        "title": "Self-Supervised Learning of Intertwined Content and Positional Features for Object Detection",
        "link_suffix": "/forum?id=nf4v09zw6O",
        "link": "https://openreview.net/forum?id=nf4v09zw6O",
        "pdf_link": "https://openreview.net/pdf?id=nf4v09zw6O",
        "keywords": "Self-supervised learning; Instance segmentation pre-training; Object detection pre-training; Vision transformer",
        "abstract": "We present a novel self-supervised feature learning method using Vision Transformers (ViT) as the backbone, specifically designed for object detection and instance segmentation. Our approach addresses the challenge of extracting features that capture both class and positional information, which are crucial for these tasks. The method introduces two key components: (1) a positional encoding tied to the cropping process in contrastive learning, which utilizes a novel vector field representation for positional embeddings; and (2) masking and prediction, similar to conventional Masked Image Modeling (MIM), applied in parallel to both content and positional embeddings of image patches. These components enable the effective learning of intertwined content and positional features. We evaluate our method against state-of-the-art approaches, pre-training on ImageNet-1K and fine-tuning on downstream tasks. Our method outperforms the state-of-the-art SSL methods on the COCO object detection benchmark, achieving significant improvements with fewer pre-training epochs. These results suggest that better integration of positional information into self-supervised learning can improve performance on dense prediction tasks."
    },
    {
        "title": "Counterfactual Delayed Feedback Learning",
        "link_suffix": "/forum?id=ZJj1r4gWIy",
        "link": "https://openreview.net/forum?id=ZJj1r4gWIy",
        "pdf_link": "https://openreview.net/pdf?id=ZJj1r4gWIy",
        "keywords": "Counterfactual, Delayed Feedback, HTE",
        "abstract": "Estimation of heterogeneous treatment effects has gathered much attention in recent years and has been widely adopted in medicine, economics, and marketing. Previous studies assumed that one of the potential outcomes of interest could be observed timely and accurately. However, a more practical scenario is that treatment takes time to produce causal effects on the outcomes. For example, drugs take time to produce medical utility for patients and users take time to purchase items after being recommended, and ignoring such delays in feedback can lead to biased estimates of heterogeneous treatment effects. To address the above problem, we study the impact of observation time on estimating heterogeneous treatment effects by further considering the potential response time that potential outcomes have. We theoretically prove the identifiability results and further propose a principled learning approach, known as CFR-DF (Counterfactual Regression with Delayed Feedback), to simultaneously learn potential response times and potential outcomes of interest. Results on both simulated and real-world datasets demonstrate the effectiveness of our method."
    },
    {
        "title": "Model merging with SVD to tie the Knots",
        "link_suffix": "/forum?id=67X93aZHII",
        "link": "https://openreview.net/forum?id=67X93aZHII",
        "pdf_link": "https://openreview.net/pdf?id=67X93aZHII",
        "keywords": "model merging; lora PEFT; computer vision;",
        "abstract": "Recent model merging methods demonstrate that the parameters of fully-finetuned models specializing in distinct tasks can be combined into one model capable of solving all tasks without retraining. Yet, this success does not transfer well when merging LoRA finetuned models. We study this phenomenon and observe that the weights of LoRA finetuned models showcase a lower degree of alignment compared to their fully-finetuned counterparts. We hypothesize that improving this alignment is key to obtaining better LoRA model merges, and propose KnOTS to address this problem. KnOTS uses the SVD to jointly transform the weights of different LoRA models into an aligned space, where existing merging methods can be applied. In addition, we introduce a new benchmark that explicitly evaluates whether merged models are general models. Notably, KnOTS consistently improves LoRA merging by up to 4.3% across several vision and language benchmarks, including our new setting."
    },
    {
        "title": "PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph",
        "link_suffix": "/forum?id=BpyHIrpUOL",
        "link": "https://openreview.net/forum?id=BpyHIrpUOL",
        "pdf_link": "https://openreview.net/pdf?id=BpyHIrpUOL",
        "keywords": "polygon, polyhedron, polygonal representation, representation learning, graph neural networks",
        "abstract": "Ubiquitous geometric objects can be precisely and efficiently represented as polyhedra. The transformation of a polyhedron into a vector, known as polyhedra representation learning, is crucial for manipulating these shapes with mathematical and statistical tools for tasks like classification, clustering, and generation. Recent years have witnessed significant strides in this domain, yet most efforts focus on the vertex sequence of a polyhedron, neglecting the complex surface modeling crucial in real-world polyhedral objects.\nThis study proposes \\textbf{PolyhedronNet}, a general framework tailored for learning representations of 3D polyhedral objects.  We propose the concept of the surface-attributed graph to seamlessly model the vertices, edges, faces, and their geometric interrelationships within a polyhedron. \nTo effectively learn the representation of the entire surface-attributed graph, we first propose to break it down into local rigid representations to effectively learn each local region's relative positions against the remaining regions without geometric information loss. Subsequently, we propose PolyhedronGNN to hierarchically aggregate the local rigid representation via intra-face and inter-face geometric message passing modules, to obtain a global representation that minimizes information loss while maintaining rotation and translation invariance.\nOur experimental evaluations on four distinct datasets, encompassing both classification and retrieval tasks, substantiate PolyhedronNet's efficacy in capturing comprehensive and informative representations of 3D polyhedral objects."
    },
    {
        "title": "Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness",
        "link_suffix": "/forum?id=IHRQif8VQC",
        "link": "https://openreview.net/forum?id=IHRQif8VQC",
        "pdf_link": "https://openreview.net/pdf?id=IHRQif8VQC",
        "keywords": "robustness, ensemble, adversarial attacks, generator",
        "abstract": "Adversarial examples pose a significant challenge to the robustness, reliability and alignment of deep neural networks. We propose a novel, easy-to-use approach to achieving high-quality representations that lead to adversarial robustness through the use of multi-resolution input representations and dynamic self-ensembling of intermediate layer predictions. We demonstrate that intermediate layer predictions exhibit inherent robustness to adversarial attacks crafted to fool the full classifier, and propose a robust aggregation mechanism based on Vickrey auction that we call \\textit{CrossMax} to dynamically ensemble them. By combining multi-resolution inputs and robust ensembling, we achieve significant adversarial robustness on CIFAR-10 and CIFAR-100 datasets without any adversarial training or extra data, reaching an adversarial accuracy of ≈72% (CIFAR-10) and ≈48% (CIFAR-100) on the RobustBench AutoAttack suite (L∞=8/255) with a finetuned ImageNet-pretrained ResNet152. This represents a result comparable with the top three models on CIFAR-10 and a +5 % gain compared to the best current dedicated approach on CIFAR-100. Adding simple adversarial training on top, we get ≈78% on CIFAR-10 and ≈51% on CIFAR-100, improving SOTA by 5 % and 9 % respectively and seeing greater gains on the harder dataset. We validate our approach through extensive experiments and provide insights into the interplay between adversarial robustness, and the hierarchical nature of deep representations. We show that simple gradient-based attacks against our model lead to human-interpretable images of the target classes as well as interpretable image changes. As a byproduct, using our multi-resolution prior, we turn pre-trained classifiers and CLIP models into controllable image generators and develop successful transferable attacks on large vision language models."
    },
    {
        "title": "Adaptive Inference-Time Compute: LLMs Can Predict if They Can Do Better, Even Mid-Generation",
        "link_suffix": "/forum?id=7tOc6h8bea",
        "link": "https://openreview.net/forum?id=7tOc6h8bea",
        "pdf_link": "https://openreview.net/pdf?id=7tOc6h8bea",
        "keywords": "LLMs, inference-time, inference-time efficiency, Best-of-N, self-evaluation",
        "abstract": "Inference-time computation is a powerful paradigm to enhance the performance of large language models (LLMs), with Best-of-N sampling being a widely used technique. However, this method is computationally expensive, requiring both (1) an external reward model and (2) the generation of multiple samples. In this work, we introduce a new generative self-evaluation scheme designed to adaptively reduce the number of generated samples while maintaining or even improving performance. We use a generative reward model formulation, allowing the LLM to predict mid-generation the probability that restarting the generation will yield a better response. These predictions are obtained without an external reward model and can be used to decide whether or not to generate more samples, prune unpromising samples early on, or to pick the best sample. This capability is very inexpensive as it involves generating a single predefined token. Trained using a dataset constructed with real unfiltered LMSYS user prompts, Llama 3.1 8B's win rate against GPT-4 on AlpacaEval increases from 21% to 34% with 16 samples and math performance on GSM8K improves from 84% to 91%. By sampling only when the LLM determines that it is beneficial to do so and adaptively adjusting temperature annealing, we demonstrate that 74% of the improvement from using 16 samples can be achieved with only 1.2 samples on average. We further demonstrate that 50–75% of samples can be pruned early in generation with minimal degradation in performance. Overall, our methods enable more efficient and scalable compute utilization during inference for LLMs."
    },
    {
        "title": "Model Entanglement for solving Privacy Preserving in Federated Learning",
        "link_suffix": "/forum?id=i8ynYkfoRg",
        "link": "https://openreview.net/forum?id=i8ynYkfoRg",
        "pdf_link": "https://openreview.net/pdf?id=i8ynYkfoRg",
        "keywords": "Federated Learning, Privacy Preserving, Deep Learning, Data Representation.",
        "abstract": "Federated learning (FL) is widely adopted as a secure and reliable distributed machine learning system for it allows participants to retain their training data locally, transmitting only model updates, such as gradients or parameters. However, the transmission process to the server can still lead to privacy leakage, as the updated information may be exploited to launch various privacy attacks. In this work, we present a key observation that the middle layer outputs, referred to as data representations, can exhibit independence in value distribution across different types of data. This enables us to capture the intrinsic relationship between data representations and private data, and inspires us to propose a Model Entanglement(ME) strategy aimed at enhancing privacy preserving by obfuscating the data representations of private models in a fine-grained manner, while improving the balance between privacy preservation and model accuracy. We compare our approach to the baseline FedAvg and two state-of-the-art defense methods. Our method demonstrates strong defense capabilities against mainstream privacy attacks, only reducing the global model accuracy by less than 0.7% and training efficiency of 6.8% respectively on the widely used dataset, excelling in both accuracy and privacy preserving."
    },
    {
        "title": "Similarity-Driven Regularization for Aligning Chemical and Latent Spaces in Molecular Design",
        "link_suffix": "/forum?id=VNqERlTCQX",
        "link": "https://openreview.net/forum?id=VNqERlTCQX",
        "pdf_link": "https://openreview.net/pdf?id=VNqERlTCQX",
        "keywords": "Molecular design，Consistency Regularization，Generative model with latent space，Molecular Similarity",
        "abstract": "Generative models play a pivotal role in molecular design by effectively generating target molecules. Among these, generative models with latent space stand out due to their robust latent space representation capabilities, powerful dimensionality reduction ability and controllability of generation. In molecular design applications, generative models with latent space convert input molecules into latent variables, capturing essential molecular features including both structural and property-related characteristics. Ideally, similar molecules should map to proximate latent variables. However, previous studies have shown an inconsistency between molecular similarity in the chemical space and that in the latent space. This inconsistency will impede the accurate representation and complicate subsequent design process,such as leading to higher optimization budget. To address this, we propose Molecular Similarity-Aware Consistency Regularization (MSCR), a straightforward regularization approach aimed at preserving the molecule similarity consistency. Our method proposes a brief but effective regularization technique to align chemical space and latent space,clearly reflect similarity relationships in latent space. We leverage Matched Molecules Pairs (MMPs) to introduce more robust similarity information than other conventional augmentation methods. Extensive experiments demonstrate that MSCR not only maintains molecules pairs similarity but also enhance optimization performance in molecular latent space tasks, without additional costs. Furthermore, our visualizations highlight molecular inconsistencies, thus underscoring the significance of our approach and improving the interpretability and relevance of our work."
    },
    {
        "title": "Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller",
        "link_suffix": "/forum?id=Of6tH5yfmK",
        "link": "https://openreview.net/forum?id=Of6tH5yfmK",
        "pdf_link": "https://openreview.net/pdf?id=Of6tH5yfmK",
        "keywords": "Large Language Models, LLM Steering, Representation Engineering",
        "abstract": "We propose $SelfControl$, an inference-time model control method utilizing gradients to control the behavior of large language models (LLMs) without explicit human annotations. Given a desired behavior expressed in a natural language suffix string concatenated to the input prompt, $SelfControl$ computes gradients of the LLM's self-evaluation of the suffix with respect to its latent representations. The gradients are used to directly control the auto-regressive generation process towards desired behaviors, which eliminates human supervision, achieves precise and transparent control, and offers on-the-fly adaptability. To further enhance efficiency, we introduce $SelfControl_{prefix}$, a compact module that encapsulates the learned representations from gradients into a \\pc, facilitating efficient inference-time control with no latency compared to the original model and allowing control for multiple behaviors simultaneously. Our experiments demonstrate $SelfControl$'s efficacy across multiple domains, where it improves over SOTA for8.3%in detoxification,3.1%in truthfulness enhancement,4%$\\textasciitilde$10%in controlling on emotion tones, and48.2%in privacy protection, i.e., completely remove privacy leakage issue."
    },
    {
        "title": "SafeDiffuser: Safe Planning with Diffusion Probabilistic Models",
        "link_suffix": "/forum?id=ig2wk7kK9J",
        "link": "https://openreview.net/forum?id=ig2wk7kK9J",
        "pdf_link": "https://openreview.net/pdf?id=ig2wk7kK9J",
        "keywords": "Diffusion model, Safety guarantees, Planning and control",
        "abstract": "Diffusion models have shown promise in data-driven planning. While these planners are commonly employed in applications where decisions are critical, they still lack established safety guarantees. In this paper, we address this limitation by introducing SafeDiffuser, a method to equip diffusion models with safety guarantees via control barrier functions. The key idea of our approach is to embed finite-time diffusion invariance, i.e., a form of specification consisting of safety constraints, into the denoising diffusion procedure. This way we enable data generation under safety constraints. We show that SafeDiffusers maintain the generative performance of diffusion models while also providing robustness in safe data generation. We evaluate our method on a series of tasks, including maze path generation, legged robot locomotion, and 3D space manipulation, and demonstrate the advantages of robustness over vanilla diffusion models."
    },
    {
        "title": "AISciVision: A Framework for Specializing Large Multimodal Models in Scientific Image Classification",
        "link_suffix": "/forum?id=lhYCbutf5G",
        "link": "https://openreview.net/forum?id=lhYCbutf5G",
        "pdf_link": "https://openreview.net/pdf?id=lhYCbutf5G",
        "keywords": "computer vision, large language models, scientific application, retrieval augmented generation, agent systems",
        "abstract": "Trust and interpretability are crucial for the use of Artificial Intelligence (AI) in scientific research, but current models often operate as black boxes offering limited transparency and justifications for their outputs. Motivated by this problem, we introduce AiSciVision, a framework that specializes Large Multimodal Models (LMMs) into interactive research partners and classification models for image classification tasks in niche scientific domains. Our framework uses two key components: (1) Visual Retrieval-Augmented Generation (VisRAG) and (2) domain-specific tools utilized in an agentic workflow. To classify a target image, AiSciVision first retrieves the most similar positive and negative labeled images as context for the LMM. Then the LMM agent actively selects and applies tools to manipulate and inspect the target image over multiple rounds, refining its analysis before making a final prediction. These VisRAG and tooling components are designed to mirror the processes of domain experts, as humans often compare new data to similar examples and use specialized tools to manipulate and inspect images before arriving at a conclusion. Each inference produces both a prediction and a natural language transcript detailing the reasoning and tool usage that led to the prediction. We evaluate AiSciVision on three real-world scientific image classification datasets: detecting the presence of aquaculture ponds, diseased eelgrass, and solar panels. Across these datasets, our method outperforms fully supervised models in low and full-labeled data settings. AiSciVision is actively deployed in real-world use, specifically for aquaculture research, through a dedicated web application that displays and allows the expert users to converse with the transcripts. This work represents a crucial step toward AI systems that are both interpretable and effective, advancing their use in scientific research and scientific discovery."
    },
    {
        "title": "Deep Learning for Micro-Scale Crack Detection on Imbalanced Datasets Using Key Point Localization",
        "link_suffix": "/forum?id=WVLBWiKxjM",
        "link": "https://openreview.net/forum?id=WVLBWiKxjM",
        "pdf_link": "https://openreview.net/pdf?id=WVLBWiKxjM",
        "keywords": "Micro-scale crack detection, Imbalanced datasets, Key point localization, Squeeze-and-excite blocks, Deep learning in structural health monitoring, Wide convolutional networks, Structural defect localization, Seismic wave analysis, Bounding box regression",
        "abstract": "Internal crack detection has been a subject of focus in structural health monitoring. By focusing on crack detection in structural datasets, it is demonstrated that deep learning (DL) methods can effectively analyse seismic wave fields interacting with micro-scale cracks, which are beyond the resolution of conventional visual inspection.This work explores a novel application of DL based key point detection technique, where cracks are localized by predicting the coordinates of four key points that define a bounding region of the crack.\nThe study not only opens new research directions for non-visual applications but also effectively mitigates the impact of imbalanced data which poses a challenge for previous DL models, as it can be biased toward predicting the majority class (non-crack regions). Popular DL techniques, such as the Inception blocks are used and investigated. \nThe model shows an overall reduction in loss when applied to micro-scale crack detection and is reflected in the lower average deviation between the location of actual and predicted cracks, with an average IOU being 0.511 for all micro cracks (> 0.00 µm) and 0.631 for larger micro cracks (> 4 µm)."
    },
    {
        "title": "Detecting and Perturbing Privacy-Sensitive Neurons to Defend Embedding Inversion Attacks",
        "link_suffix": "/forum?id=DF5TVzpTW0",
        "link": "https://openreview.net/forum?id=DF5TVzpTW0",
        "pdf_link": "https://openreview.net/pdf?id=DF5TVzpTW0",
        "keywords": "Text embedding, Defense Inversion Attack",
        "abstract": "This paper introduces Defense through Perturbing Privacy Neurons (DPPN), a novel approach to protect text embeddings against inversion attacks. Unlike ex- isting methods that add noise to all embedding dimensions for general protection, DPPN identifies and perturbs only a small portion of privacy-sensitive neurons. We present a differentiable neuron mask learning framework to detect these neu- rons and a neuron-suppressing perturbation function for targeted noise injection. Experiments across six datasets show DPPN achieves superior privacy-utility trade- offs. Compared to baseline methods, DPPN reduces more privacy leakage by 5-78% while improving downstream task performance by 14-40%. Tests on real- world sensitive datasets demonstrate DPPN’s effectiveness in mitigating sensitive information leakage to 17%, while baseline methods reduce it only to 43%."
    },
    {
        "title": "BrainACTIV: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation",
        "link_suffix": "/forum?id=CGON8Btleu",
        "link": "https://openreview.net/forum?id=CGON8Btleu",
        "pdf_link": "https://openreview.net/pdf?id=CGON8Btleu",
        "keywords": "brain, selectivity, visual cortex, fMRI, manipulation, variation, diffusion, neuroscience",
        "abstract": "The human brain efficiently represents visual inputs through specialized neural populations that selectively respond to specific categories. \n Advancements in generative modeling have enabled data-driven discovery of neural selectivity using brain-optimized image synthesis. However, current methods independently generate one sample at a time, making it hard to discern which image features drive neural response selectivity. To address this issue, we introduce Brain Activation Control Through Image Variation (BrainACTIV), a method for manipulating a reference image to enhance or suppress activity in a target cortical region using pretrained diffusion models. Starting from a reference image allows for fine-grained and reliable identification of optimal visuo-semantic properties. In addition, we describe how two hyperparameters allow a trade-off between semantic variation and low-level structural control. We show that our manipulations effectively modulate predicted fMRI responses and agree with hypothesized preferred categories in established regions of interest, while remaining structurally close to the reference image. Moreover, we demonstrate how our method accentuates differences between brain regions that are selective to the same category. Hence, BrainACTIV holds the potential to formulate robust hypotheses about brain representation as well as produce controllable naturalistic stimuli for neuroscientific experiments."
    },
    {
        "title": "Efficient Online Reinforcement Learning Fine-Tuning Should Not Retain Offline Data",
        "link_suffix": "/forum?id=HN0CYZbAPw",
        "link": "https://openreview.net/forum?id=HN0CYZbAPw",
        "pdf_link": "https://openreview.net/pdf?id=HN0CYZbAPw",
        "keywords": "Reinforcement learning, fast fine-tuning",
        "abstract": "The modern paradigm in machine learning involves pre-training models on diverse data, followed by task-specific fine-tuning. In reinforcement learning (RL), this translates to learning via offline RL on a static dataset, followed by rapid online RL fine-tuning using autonomous interaction data. Most RL fine-tuning methods require continued training on offline data for stability and performance. This is undesirable because retaining offline data is both slow and expensive for large datasets, but has been inevitable so far. In this paper, we show that retaining offline data is completely unnecessary as long as we use a correctly-designed online RL approach for fine-tuning offline RL initializations. We start by analyzing the role of retaining offline data in online fine-tuning. We find that continued training on offline data is mostly useful for preventing a sudden unlearning of the offline RL value function at the onset of fine-tuning, caused by a distribution mismatch between the offline data and online rollouts. As a result, this unlearning erases the benefits of offline pre-training. Our approach, WSRL, mitigates this sudden unlearning by using a warmup phase that seeds the online RL run with a very small number of rollouts from the pre-trained policy. The data collected during warmup helps ``recalibrate'' the offline Q-function to the online data better, allowing us to completely discard offline data without risking of destabilizing the online RL training. We show that WSRL is able to fine-tune without retaining any offline data, and is able to learn faster and attains higher performance than existing algorithms irrespective of whether they do or do not retain offline data."
    },
    {
        "title": "RNA FrameFlow: Flow Matching for de novo 3D RNA Backbone Generation",
        "link_suffix": "/forum?id=Y8Kwl7GFAd",
        "link": "https://openreview.net/forum?id=Y8Kwl7GFAd",
        "pdf_link": "https://openreview.net/pdf?id=Y8Kwl7GFAd",
        "keywords": "RNA Structure, RNA Design, Flow Matching",
        "abstract": "We introduce RNA-FrameFlow, the first generative model for de novo 3D RNA backbone design. We build upon $SE(3)$ flow matching for protein backbone generation and establish protocols for data preparation and evaluation to address unique challenges posed by RNA modeling. We formulate RNA structures as a set of rigid-body frames and associated loss functions which account for larger, more conformationally flexible RNA backbones (13 atoms per nucleotide) vs. proteins (4 atoms per residue). Toward tackling the lack of diversity in 3D RNA datasets, we explore training with structural clustering and cropping augmentations. Additionally, we define a suite of evaluation metrics to measure whether the generated RNA structures are globally self-consistent (via inverse folding followed by forward folding) and locally recover RNA-specific structural descriptors. The most performant version of RNA-FrameFlow generates locally realistic RNA backbones of 40-150 nucleotides, over 40% of which pass our validity criteria as measured by a self-consistency TM-score $\\geq0.45$, at which two RNAs have the same global fold."
    },
    {
        "title": "UnCLe: An Unlearning Framework for Continual Learning",
        "link_suffix": "/forum?id=pFjzF7dIgg",
        "link": "https://openreview.net/forum?id=pFjzF7dIgg",
        "pdf_link": "https://openreview.net/pdf?id=pFjzF7dIgg",
        "keywords": "Machine Unlearning, Continual Learning, Hypernetworks",
        "abstract": "Recent advances in deep learning require models to exhibit continual learning capability, allowing them to learn new tasks and progressively accumulate knowledge without forgetting old tasks. Concurrently, there are growing concerns and regulatory requirements to meet privacy and safety by discarding some knowledge through machine unlearning. With the rapidly rising relevance of continual learning and machine unlearning, we consider them together under a unified framework in this paper. However, the conflicting nature of past data unavailability arising from continual learning makes it challenging to perform unlearning with existing methods which assume data availability. Moreover, in the proposed setup, where tasks are repeatedly learned and unlearned in a sequence, it is another challenge to maintain the stability of the tasks that need to be retained. To address these challenges, we propose UnCLe, an Unlearning Framework for Continual Learning designed to learn tasks incrementally and unlearn tasks without access to past data. To perform data-free unlearning, UnCLe leverages hypernetworks in conjunction with an unlearning objective that seeks to selectively align task-specific parameters with noise. Our experiments on popular benchmarks demonstrate UnCLe's consistent unlearning completeness and ability to preserve task stability over long sequences."
    },
    {
        "title": "SMiR: A Synthetic Data Pipeline To Improve Multi-Image Reasoning",
        "link_suffix": "/forum?id=lYtY3RV5nv",
        "link": "https://openreview.net/forum?id=lYtY3RV5nv",
        "pdf_link": "https://openreview.net/pdf?id=lYtY3RV5nv",
        "keywords": "Large Language Models, Synthetic Multimodal Data, Multi-Image Reasoning",
        "abstract": "Vision-Language Models (VLMs) have demonstrated strong performance in single-image understanding, supported by many high-quality instruction datasets. However, multi-image reasoning tasks remain under-explored in the open-source community due to two major issues:  (1) scaling up datasets with multiple correlated images and complex reasoning instructions is resource-intensive and difficult to maintain quality and (2) there is a shortage of robust multi-image evaluation benchmarks. To address these issues, we introduce SMiR, an efficient synthetic data-generation pipeline for multi-image reasoning, and a high-quality SMiR dataset generated using this pipeline. Our pipeline efficiently extracts highly correlated images using multimodal embeddings, combining visual and descriptive information and leverages open-source LLMs to generate quality instructions, offering a cost-effective alternative to expensive closed-source solutions. Additionally, we present SMiR-Bench, a novel multi-image reasoning evaluation benchmark comprising 100 diverse examples across 7 complex multi-image reasoning tasks. Unlike existing benchmarks, SMiR-Bench is multi-turn and allows for free-form responses, providing a more comprehensive evaluation of model expressiveness and reasoning capability. We demonstrate the effectiveness of SMiR dataset by fine-tuning several open-source VLMs and evaluating their performance on SMiR-Bench. Our results show that models trained on our dataset outperform baseline models in multi-image reasoning tasks. Furthermore, we observe enhanced model expressiveness and more nuanced reasoning in free-form responses, highlighting the value of our approach for advancing open-source VLM research."
    },
    {
        "title": "SRPCA: Sparse Reverse of Principal Component Analysis for Fast Low-Rank Matrix Completion",
        "link_suffix": "/forum?id=zNfdtV9ADQ",
        "link": "https://openreview.net/forum?id=zNfdtV9ADQ",
        "pdf_link": "https://openreview.net/pdf?id=zNfdtV9ADQ",
        "keywords": "matrix completion, low rank, PCA, collaborative filtering, image inpainting, time-series imputation",
        "abstract": "Supervised and unsupervised learning methods experience a decline in performance when applied to incomplete, corrupted, or noisy datasets. Matrix completion is a common task to impute the missing values in sparsely observed matrices. Given a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$, low-rank matrix completion computes a rank-$r$ approximation of $\\mathbf{X}$, where $r\\ll\\min\\{m,n\\}$, by only observing a few random entries of $\\mathbf{X}$. It is commonly applied for recommender systems, image processing, and multi-output collaborative modeling. Existing matrix completion methods suffer either from slow convergence or failure under significant missing data levels. \nThis paper proposes a novel approach, the Sparse Reverse of Principal Component Analysis (SRPCA), that reformulates matrix factorization based low-rank completion $(\\min_{\\mathbf{U},\\mathbf{V}}\\Vert\\mathcal{P}_{\\mathbf{\\Omega}}(\\mathbf{X}-\\mathbf{U}\\mathbf{V}^T)\\Vert_F^2)$\nto iteratively learn a single low-rank subspace representation by solving the convex optimization problem\n$\\min_{\\mathbf{V}}\\Vert\\mathcal{P}_{\\mathbf{\\Omega}}(\\mathbf{X}-\\mathbf{P}\\mathbf{V}^T)\\Vert_F^2$ under the principal component analysis framework, resulting in a significant convergence acceleration. SRPCA converges iteratively and is computationally tractable with a proven controllable upper bound on the number of iterations until convergence. Unlike existing matrix completion algorithms, the proposed SRPCA applies iterative pre-processing resets that maintain smoothness across the reconstructed matrix, which results in a performance boost for smooth matrices. The performance of the proposed technique is validated on case studies for image processing, multivariate time-series imputation, and collaborative filtering. SRPCA is also compared with state-of-the-art benchmarks for matrix completion."
    },
    {
        "title": "Making Text Embedders Few-Shot Learners",
        "link_suffix": "/forum?id=wfLuiDjQ0u",
        "link": "https://openreview.net/forum?id=wfLuiDjQ0u",
        "pdf_link": "https://openreview.net/pdf?id=wfLuiDjQ0u",
        "keywords": "large language model, embedding model, in-context learning",
        "abstract": "Large language models (LLMs) with decoder-only architectures have demonstrated exceptional text-generation capabilities across a variety of tasks. Some researchers have also adapted these models for text representation tasks. However, in text representation tasks, these models often face performance degradation on unseen tasks. In-context learning (ICL), which leverages examples provided in the input context, enables LLMs to handle unseen tasks effectively. Inspired by this, we aim to fully utilize the inherent properties of LLMs to enhance text representation performance across different tasks through the ICL approach.In this paper, we introduce a simple yet effective training strategy, which significantly improves text representation capabilities. Unlike previous models that prepend task instructions to the text, our method randomly samples a varying number of examples during training, endowing the embedding model with in-context learning abilities while maintaining its zero-shot capabilities. This approach does not require additional data construction or modifications to the model architecture. On the contrary, we find that some popular modifications to the model, such as bidirectional attention, can degrade performance, undermining the inherent characteristics of LLMs. We open-source the model, code, and data to foster further development in the field."
    },
    {
        "title": "DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic, Lightweight Plugin for Large Language Models",
        "link_suffix": "/forum?id=I1VCj1l1Zn",
        "link": "https://openreview.net/forum?id=I1VCj1l1Zn",
        "pdf_link": "https://openreview.net/pdf?id=I1VCj1l1Zn",
        "keywords": "Multi-LoRA fusion, Parameter Efficient Tuning, LoRA, Cross-Task Generalization",
        "abstract": "Recent advancements in Large Language Models (LLMs) have achieved robust performance across diverse tasks, but fine-tuning these models for specific domains remains resource-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation (LoRA) address this challenge by fine-tuning a small subset of parameters. However, existing methods for fusing multiple LoRAs lack dynamic fusion based on contextual inputs and often increase inference time due to token-level operations. We propose DLP-LoRA, a Dynamic Lightweight Plugin that employs a mini-MLP module with only 5M parameters to dynamically fuse multiple LoRAs at the sentence level using top-$p$ sampling strategies. This approach reduces inference time to less than twice that of single LoRA inference by leveraging parallel computation. Evaluations across 26 tasks—including multiple-choice questions and question answering—demonstrate that DLP-LoRA achieves an average accuracy of 92.34% on multiple-choice datasets and significant improvements in BLEU and ROUGE scores on QA datasets, outperforming different LLMs backbones under composite task settings. DLP-LoRA effectively balances performance and efficiency, making it a practical solution for dynamic multi-task adaptation in LLMs."
    },
    {
        "title": "Direct Multi-agent Motion Generation Preference Alignment with Implicit Feedback from Demonstrations",
        "link_suffix": "/forum?id=8UFG9D8xeU",
        "link": "https://openreview.net/forum?id=8UFG9D8xeU",
        "pdf_link": "https://openreview.net/pdf?id=8UFG9D8xeU",
        "keywords": "Alignment from demonstrations, Alignment from human feedback",
        "abstract": "Recent advancements in Large Language Models (LLMs) have transformed motion generation models in embodied applications such as autonomous driving and robotic manipulation. While LLM-type motion models benefit from scalability and efficient formulation, there remains a discrepancy between their token-prediction imitation objectives and human preferences. This often results in behaviors that deviate from human-preferred demonstrations, making post-training behavior alignment crucial for generating human-preferred motions. Post-training alignment requires a large number of preference rankings over model generations, which are costly and time-consuming to annotate in multi-agent motion generation settings. Recently, there has been growing interest in using expert demonstrations to scalably build preference data for alignment. However, these methods often adopt a worst-case scenario assumption, treating all generated samples from the reference model as unpreferred and relying on expert demonstrations to directly or indirectly construct preferred generations. This approach overlooks the rich signal provided by preference rankings among the model's own generations. In this work, instead of treating all generated samples as equally unpreferred, we propose a principled approach leveraging the implicit preferences encoded in expert demonstrations to construct preference rankings among the generations produced by the reference model, offering more nuanced guidance at low-cost. We present the first investigation of direct preference alignment for multi-agent motion token-prediction models using implicit preference feedback from demonstrations. We apply our approach to large-scale traffic simulation and demonstrate its effectiveness in improving the realism of generated behaviors involving up to 128 agents, making a 1M token-prediction model comparable to state-of-the-art large models by relying solely on implicit feedback from demonstrations, without requiring additional human annotations or high computational costs. Furthermore, we provide an in-depth analysis of preference data scaling laws and their effects on over-optimization, offering valuable insights for future investigations."
    },
    {
        "title": "Diffusion-Guided Safe Policy Optimization From Cost-Label-Free Offline Dataset",
        "link_suffix": "/forum?id=ZGqlkqAt18",
        "link": "https://openreview.net/forum?id=ZGqlkqAt18",
        "pdf_link": "https://openreview.net/pdf?id=ZGqlkqAt18",
        "keywords": "Reinforcement Learning, Offline Safe Reinforcement Learning, Diffusion Model",
        "abstract": "Offline safe reinforcement learning (RL) aims to guarantee the safety of decision-making in both training and deployment phases by learning the safe policy entirely from offline data without further interaction with the environment, which pushes the RL towards real-world applications. Previous efforts in offline safe RL typically presume the presence of Markovian costs within the dataset. However, the design of a Markovian cost function involves rehearsal of all potentially unsafe cases, which is inefficient and even unfeasible in many practical tasks. In this work, we take a further step forward by learning a safe policy from an offline dataset without any cost labels, but with a small number of safe demonstrations included. To solve this problem, we propose a two-stage optimization method calledDiffusion-guidedSafePolicyOptimization (DSPO). Initially, we derive trajectory-wise safety signals by training a return-agnostic discriminator. Subsequently, we train a conditional diffusion model that generates trajectories conditioned both on the trajectory return and the safety signal. Remarkably, the trajectories generated by our diffusion model not only yield high returns but also comply with the safety signals, from which we can derive a desirable policy through behavior cloning (BC). The evaluation experiments conducted across tasks from the SafetyGym, BulletGym, and MetaDrive environments demonstrate that our approach can achieve a safe policy with high returns, significantly outperforming various established baselines."
    },
    {
        "title": "OptBatch: Optimizing Instruction Tuning with Data Selection through Batch Stratified Sampling",
        "link_suffix": "/forum?id=xybTwSsdBP",
        "link": "https://openreview.net/forum?id=xybTwSsdBP",
        "pdf_link": "https://openreview.net/pdf?id=xybTwSsdBP",
        "keywords": "data selection, coreset, gradients, instruction tuning, large language model",
        "abstract": "Instruction tuning has optimized the specialized capabilities of large language models (LLMs), but it often requires extensive datasets and prolonged training times. The challenge lies in developing specific capabilities by identifying useful data and efficiently fine-tuning. High-quality and diverse pruned data can help models achieve lossless performance at a lower cost. In this paper, we propose \\textbf{OptBatch}, a novel data selection method that focuses on the learnability of whole batch data rather than individual samples. OptBatch considers the coverage of the data distribution through stratified sampling and maximizes the relative distance between samples within a batch to enhance diversity. Furthermore, OptBatch utilizes Hessian gradient optimization to guide the selection strategy for subsequent batches. OptBatch effectively captures the intrinsic value of data curation, surpasses previous state-of-the-art methods, and demonstrates robust generalization performance across diverse downstream tasks and models. Extensive experiments reveal that OptBatch training in various pruning rates outperforms full dataset training, reducing computational cost by 20-40%. Additionally, evaluations using GPT-4 scores and other metrics for multi-turn dialogue, multilingual translation and QA tasks consistently demonstrate OptBatch's optimal performance."
    },
    {
        "title": "Multlingual Abstractive Event Extraction for the Real World",
        "link_suffix": "/forum?id=49jkevjF6x",
        "link": "https://openreview.net/forum?id=49jkevjF6x",
        "pdf_link": "https://openreview.net/pdf?id=49jkevjF6x",
        "keywords": "dataset, event extraction, multilingual, zero-shot, entity linking",
        "abstract": "Event extraction (EE) is a valuable tool for making sense of large amounts of unstructured data, with a wide range of real-world applications, from studying disease outbreaks to monitoring political violence. Current EE systems rely on cumbersome mention-level annotations, and event arguments are frequently restricted to ungrounded spans of text, which hinders the aggregation and analysis of extracted events. In this paper, we define a new abstractive event extraction (AEE) task that moves away from the surface form and instead requires a deeper\nwholistic understanding of the input text. To support research in this direction, we release a new multilingual, expert-annotated event dataset called Lemonade, which covers 16 languages, including several for which no event dataset currently exists. Lemonade has 41,148 events, and is based on the Armed Conflict Location and Event Data Project, which has been collecting and coding data on political violence around the globe for over a decade. We introduce a novel zero-shot AEE system Zest that achieves a score of 57.2% F1 on Lemonade. With our supervised model that achieves 71.6% F1, they represent strong baselines for this new dataset."
    }
]