[
    {
        "title": "FedDFQ : Personalized Federated Learning Based On Data Feature Quantification",
        "link_suffix": "/forum?id=tZP4Uyql0r",
        "link": "https://openreview.net/forum?id=tZP4Uyql0r",
        "pdf_link": "https://openreview.net/pdf?id=tZP4Uyql0r",
        "keywords": "Federal Learning, Model Aggregation, Data Feature Utilization",
        "abstract": "Personalized federated learning is widely used for heterogeneous data distributions across clients. However, existing methods are difficult to measure and utilize these heterogeneities accurately. To address this issue, in this paper, we propose a novel and efficient method named FedDFQ which uses a customized Data Identity Extraction Module (DIEM) to dynamically generate metric proxies that quantify data heterogeneity across different local clients in a privacy-friendly manner. The metric proxies are used to assess the contributions of global parameter aggregation and personalized gradient backpropagation for each local client. In addition, we design a plug-and-play Automatic Gradient Accumulation Module (AGAM) that regularizes personalized classification layers with re-balanced gradients. We provide theoretical explanations and experimental results that validate the effectiveness of the proposed FedDFQ. With comprehensive comparisons to existing state-of-the-art approaches, FedDFQ outperforms them on two benchmark datasets in different heterogeneous scenarios."
    },
    {
        "title": "A Unified Approach to Routing and Cascading for LLMs",
        "link_suffix": "/forum?id=rgDwRdMwoS",
        "link": "https://openreview.net/forum?id=rgDwRdMwoS",
        "pdf_link": "https://openreview.net/pdf?id=rgDwRdMwoS",
        "keywords": "large language models, routing, model evaluation",
        "abstract": "The widespread applicability of large language models (LLMs) has increased the availability of many fine-tuned models of various sizes targeting specific tasks. Given a set of such specialized models, to maximize overall performance, it is important to figure out the optimal strategy for selecting the right model for a given user query. An effective strategy could drastically increase overall performance and even offer improvements over a single large monolithic model. Existing approaches typically fall into two categories: routing, where a single model is selected for each query, and cascading, which runs a sequence of increasingly larger models until a satisfactory answer is obtained. However, both have notable limitations: routing commits to an initial model without flexibility, while cascading requires executing every model in sequence, which can be inefficient. Additionally, the conditions under which these strategies are provably optimal remain unclear. In this work, we derive optimal strategies for both routing and cascading. Building on this analysis, we propose a novel approach calledcascade routing, which combines the adaptability of routing with the cost-efficiency of cascading. Our experiments demonstrate that cascade routing consistently outperforms both routing and cascading across a variety of settings, improving both output quality and lowering computational cost, thus offering a unified and efficient solution to the model selection problem."
    },
    {
        "title": "One Model to Train Them All: A Unified Diffusion Framework for Multi-Context Neural Population Forecasting",
        "link_suffix": "/forum?id=R9feGbYRG7",
        "link": "https://openreview.net/forum?id=R9feGbYRG7",
        "pdf_link": "https://openreview.net/pdf?id=R9feGbYRG7",
        "keywords": "neural population, diffusion model, time series forecasting, sequence-to-sequence, electrophysiology, neural dynamics",
        "abstract": "Recent research has revealed shared neural patterns among animals performing similar tasks and within individual animals across different tasks. This has led to a growing interest in replacing single-session latent variable models with a unified model that allows us to align recordings across different animals, sessions, and tasks, despite the challenge of distinct neuron identities in each recording. \nIn this work, we present a conditioned diffusion framework to model population dynamics of neural activity across multiple contexts. The quality of the learned dynamics is evaluated through the model's forecasting ability, which predicts multiple timesteps of both neural activity and behavior.\nAdditionally, we introduce a benchmark dataset spanning six electrophysiology datasets, seven tasks, 19 animals, and 261 sessions, providing a standardized framework for multi-task neural population models.\nOur results demonstrate that the pretrained model can be efficiently adapted to novel, unseen sessions without requiring explicit neuron correspondence. This enables few-shot learning with minimal labeled data, as well as competitive performance in zero-shot learning."
    },
    {
        "title": "AI-Assisted Generation of Difficult Math Questions",
        "link_suffix": "/forum?id=M1CCA6UF0y",
        "link": "https://openreview.net/forum?id=M1CCA6UF0y",
        "pdf_link": "https://openreview.net/pdf?id=M1CCA6UF0y",
        "keywords": "Mathematical Reasoning, Skill Composition, Synthetic Data, Model Evaluation",
        "abstract": "Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is an unmet demand for diverse and challenging mathematics questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty.  We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. Initially, leveraging LLM metacognition skills [Didolkar et al., 2024], a strong LLM is used to extract core \"skills'' from existing math datasets.  These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills that must be utilized in the question. The use of two very different skills within each question makes finding such questions an ``out of distribution'' task for both LLMs and humans. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multi-turn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interactions. Applying this pipeline on skills extracted from MATH dataset [Hendrycks et al., 2024] resulted inMATH$^2$- a dataset of higher quality math questions, as evidenced by: (a)  Lower performance of all models on MATH$^2$ than on MATH (b) Higher performance on MATH when using MATH$^2$ questions as in-context examples.  Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component ofscalable oversight. Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH$^2$ is the square on MATH. This suggests that successfully solving the question in MATH$^2$ requires a nontrivial combination of two distinct math skills."
    },
    {
        "title": "Towards Explaining the Power of Constant-depth Graph Neural Networks for Linear Programming",
        "link_suffix": "/forum?id=INow59Vurm",
        "link": "https://openreview.net/forum?id=INow59Vurm",
        "pdf_link": "https://openreview.net/pdf?id=INow59Vurm",
        "keywords": "Learning to optimize, shallow graph neural network, linear programming, distributed algorithm, average-case analysis",
        "abstract": "Graph neural networks (GNNs) have recently emerged as powerful tools for solving complex optimization problems, often being employed to approximate solution mappings. Empirical evidence shows that evenshallowGNNs (with fewer than ten layers) can achieve strong performance in predicting optimal solutions to linear programming (LP) problems. This finding is somewhat counter-intuitive, as LPs areglobaloptimization problems, while shallow GNNs predict based onlocalinformation. Although previous theoretical results suggest that GNNs have the expressive power to solve LPs, they require deep architectures whose depth grows at least polynomially with the problem size, and thus leave the underlying principle of this empirical phenomenon still unclear. In this paper, we examine this phenomenon through the lens of distributed computing and average-case analysis. First, we establish that the expressive power of GNNs for LPs is closely related to well-studied distributed algorithms for LPs. Specifically, we show that any $d$-round distributed LP algorithm can be simulated by a $d$-depth GNN, and vice versa. Second, by designing a distributed LP algorithm and then unrolling it, we prove that constant-depth, constant-width GNNs suffice to solve sparse binary LPs effectively. Here, in contrast with previous analyses focusing onworst-casescenarios, in which it can shown that GNN depth must increase with problem size by leveraging an impossibility result about distributed LP algorithms, our analysis shifts the focus to theaverage-caseperformance, and shows that constant GNN depth then becomes sufficient no matter how large the problem size is. Our theoretical results provide a fresh perspective on the strong empirical performance of shallow GNNs for solving LPs."
    },
    {
        "title": "Bidirectional Communication-Efficient Non-Convex Adaptive Federated Learning",
        "link_suffix": "/forum?id=Jl0aEFrp11",
        "link": "https://openreview.net/forum?id=Jl0aEFrp11",
        "pdf_link": "https://openreview.net/pdf?id=Jl0aEFrp11",
        "keywords": "Federated learning, non-convex learning, bidirectional communication-efficient, adaptive",
        "abstract": "Within the framework of federated learning, we introduce two novel strategies: New Lazy Aggregation (NLA) and Accelerated Aggregation (AA). The NLA strategy reduces communication and computational costs through adaptive gradient skipping, while the AA strategy accelerates computation and decreases communication costs via adaptive gradient accumulation. Building upon these innovative strategies and compression techniques, we propose two new algorithms: FedBNLACA and FedBACA, aimed at minimizing bidirectional communication costs. We provide theoretical guarantees for client participation (either full or partial) in these algorithms under non-convex settings and heterogeneous data. In the context of non-convex optimization with full client participation, our proposed FedBNLACA and FedBACA algorithms achieve the same convergence rate of $\\mathcal{O}\\big(1/T\\big)$ as their non-tight counterparts. Extensive experimental results demonstrate that our protocols facilitate effective training in non-convex environments and exhibit robustness across a wide range of devices, partial participation, and imbalanced data."
    },
    {
        "title": "Hybrid Regularization Improves Diffusion-based Inverse Problem Solving",
        "link_suffix": "/forum?id=d7pr2doXn3",
        "link": "https://openreview.net/forum?id=d7pr2doXn3",
        "pdf_link": "https://openreview.net/pdf?id=d7pr2doXn3",
        "keywords": "Diffusion models, inverse problems",
        "abstract": "Diffusion models, recognized for their effectiveness as generative priors, have become essential tools for addressing a wide range of visual challenges. Recently, there has been a surge of interest in leveraging Denoising processes for Regularization (DR) to solve inverse problems. However, existing methods often face issues such as mode collapse, which results in excessive smoothing and diminished diversity. In this study, we perform a comprehensive analysis to pinpoint the root causes of gradient inaccuracies inherent in DR. Drawing on insights from diffusion model distillation, we propose a novel approach called Consistency Regularization (CR), which provides stabilized gradients without the need for ODE simulations. Building on this, we introduce Hybrid Regularization (HR), a unified framework that combines the strengths of both DR and CR, harnessing their synergistic potential. Our approach proves to be effective across a broad spectrum of inverse problems, encompassing both linear and nonlinear scenarios, as well as various measurement noise statistics. Experimental evaluations on benchmark datasets, including FFHQ and ImageNet, demonstrate that our proposed framework not only achieves highly competitive results compared to state-of-the-art methods but also offers significant reductions in wall-clock time and memory consumption."
    },
    {
        "title": "A Scalable Transformer-based Framework for Fault Detection in Mission-Critical Systems",
        "link_suffix": "/forum?id=UUwrBhhsxT",
        "link": "https://openreview.net/forum?id=UUwrBhhsxT",
        "pdf_link": "https://openreview.net/pdf?id=UUwrBhhsxT",
        "keywords": "multi-agent systems, unmanned aerial vehicle, offline reinforcement learning, scenario-based testing",
        "abstract": "Detecting underlying faults is crucial in the development of mission-critical planning systems, such as drone trajectory planning in Unmanned aircraft Traffic Management (UTM), which is vital to airspace safety. \nInevitably, there exists a small set of rare, unpredictable conditions where the UTM could suffer from catastrophic failures. \nMost traditional fault detection approaches focus on achieving high coverage by random input exploitation. \nHowever, random methods are struggling to detect long-tail vulnerabilities with unacceptable time consumption. \nTo tackle this challenge, we propose a scenario-oriented framework to search long-tail conditions, accelerating the fault detection process. \nInspired by in-context learning approaches, we leverage a Transformer-based policy model to capture the dynamics of the subject UTM system from the offline dataset for exploitation acceleration. \nWe evaluate our approach over 700 hours in a massive-scale, industry-level simulation environment. \nEmpirical results demonstrate that our approach achieves over 8 times more vulnerability discovery efficiency compared with traditional expert-guided random-walk exploitation, which showcases the potential of machine learning for fortifying mission-critical systems. \nFurthermore, we scale the model size to 2 billion parameters, achieving substantial performance gains over smaller models in offline and online evaluations, highlighting the scalability of our approach."
    },
    {
        "title": "Streaming Spatial-Temporal Prompt Learning for RGB-T Tracking",
        "link_suffix": "/forum?id=S1GTzTFKxb",
        "link": "https://openreview.net/forum?id=S1GTzTFKxb",
        "pdf_link": "https://openreview.net/pdf?id=S1GTzTFKxb",
        "keywords": "Prompt Learning, Multimodal, RGB-T Tracking",
        "abstract": "In the process of multimodal interaction, effective spatial-temporal information of correlated targets is crucial for RGB-T tracking. However, most existing methods only utilize spatial information for template-search matching or merely introduce an additional dynamic template with sparse temporal perception. These approaches overlook rich temporal cues across consecutive video frames, such as target appearance changes and motion trajectory. To establish effective spatial-temporal associations during multimodal interaction, we propose a video-level RGB-T tracking paradigm via prompt learning, termed PromptTrack. It densely models the spatial-temporal relationships of targets in multimodal contexts by incorporating streaming spatial-temporal prompts within a continuous sequence of video frames. Specifically, PromptTrack learns target changes and motion trajectory from historical frames through streaming temporal prompt for each modality, and then learns multimodal spatial prompt conditioned on temporal prompt to effectively leverage multimodal complementary information. Benefiting from the proposed spatial-temporal prompt learning method, PromptTrack exhibits superior target location capability and robustness in complex tracking scenarios. The novel prompt-based tracking paradigm can also be effortlessly extended to other tracking domains such as RGB-D and RGB-E. Extensive experiments on three prevailing benchmark datasets demonstrate our method achieves new state-of-the-art performances. In particular, PromptTrack achieves Precision score of 76.2% and Success score of 60.7% on LasHeR dataset while running at a real-time speed of 35 FPS. Codes and models will be released."
    },
    {
        "title": "Understanding Neural Tangent Kernel Dynamics Through Its Trace Evolution",
        "link_suffix": "/forum?id=bWz8aOPwsJ",
        "link": "https://openreview.net/forum?id=bWz8aOPwsJ",
        "pdf_link": "https://openreview.net/pdf?id=bWz8aOPwsJ",
        "keywords": "Neural Tangent Kernel, Representation Learning, Training Dynamics",
        "abstract": "The Neural Tangent Kernel (NTK) has emerged as a valuable tool for analyzing the training and generalization properties of neural networks. While the behavior of the NTK in the infinite-width limit is well understood, a comprehensive investigation is still required to comprehend its dynamics during training in the finite-width regime. In this paper, we present a detailed exploration of the NTK's behavior through the examination of its trace during training.By conducting experiments on standard supervised classification tasks, we observe that the NTK trace typically exhibits an increasing trend and stabilizes when the network achieves its highest accuracy on the training data. Additionally, we investigate the phenomenon of \"grokking'', which has recently garnered attention, as it involves an intriguing scenario where the test accuracy suddenly improves long after the training accuracy plateaus. To shed light on this phenomenon, we employ the NTK trace to monitor the training dynamics during grokking. Furthermore, we utilize the NTK trace to gain insights into the training dynamics of semi-supervised learning approaches, including the employment of exponential moving average mechanisms. Through these investigations, we demonstrate that the NTK, particularly its trace, remains a powerful and valuable tool for comprehending the training dynamics of modern finite-width neural networks."
    },
    {
        "title": "RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data",
        "link_suffix": "/forum?id=k2uUeLCrQq",
        "link": "https://openreview.net/forum?id=k2uUeLCrQq",
        "pdf_link": "https://openreview.net/pdf?id=k2uUeLCrQq",
        "keywords": "imu, har, biosignals, activity classification, gait metrics, time-series, foundation model, contrastive learning, self-supervised learning",
        "abstract": "We present RelCon, a novel self-supervised Relative Contrastive learning approach that uses a learnable distance measure in combination with a softened contrastive loss for training an motion foundation model from wearable sensors. The learnable distance measure captures motif similarity and domain-specific semantic information such as rotation invariance. The learned distance provides a measurement of semantic similarity between a pair of accelerometer time-series segments, which is used to measure the distance between an anchor and various other sampled candidate segments. The self-supervised model is trained on 1 billion segments from  87,376 participants from a large wearables dataset. The model achieves strong performance across multiple downstream tasks, encompassing both classification and regression. To our knowledge, we are the first to show the generalizability of a self-supervised learning model with motion data from wearables across distinct evaluation tasks."
    },
    {
        "title": "Invariant Convolutional Layers for Time Series",
        "link_suffix": "/forum?id=oIvjUpuZLC",
        "link": "https://openreview.net/forum?id=oIvjUpuZLC",
        "pdf_link": "https://openreview.net/pdf?id=oIvjUpuZLC",
        "keywords": "Time Series, Convolution, Invariances, Neural Network",
        "abstract": "Machine learning for time series has recently garnered considerable attention. Indeed, automatically extracting meaningful representations from large and complex time series data is becoming imperative for several real-world applications. Neural architectures tailored to time series are often built upon sequential modules, such as convolutional, commonly employed in text or vision. Unfortunately, the potential of standard layers in capturing invariant properties of time series remains relatively underexplored. For instance, convolutional layers often fail to capture underlying patterns in time series inputs that encompass strong deformations, such as linear trends. However, invariances to some deformations may be critical for solving complex time series tasks, such as classification, while guaranteeing good generalization properties.\nTo address these challenges, we mathematically formulate and technically design efficientinvariant convolutionsfor specific group actions applicable to the case of time series.\nWe construct these convolutions by considering two sets of deformations commonly observed in time series, including (i)offset shift and scalingand (ii)linear trend and scaling.\nWe further combine the proposed invariant convolutions with standard (or variant) convolutions in a single embedding layer of an example architecture, the so-calledInvConvNetmethod, and showcase the layer capacity to capture complex invariant time series properties.\nFinally,InvConvNetis experimentally proven to achieve superior performance against common baselines in relevant time series tasks, including classification and anomaly detection."
    },
    {
        "title": "HELPFUL-ONLY LARGE LANGUAGE MODEL",
        "link_suffix": "/forum?id=otTfoKL5sJ",
        "link": "https://openreview.net/forum?id=otTfoKL5sJ",
        "pdf_link": "https://openreview.net/pdf?id=otTfoKL5sJ",
        "keywords": "AI Safety, AI alignment, Large Language Model, Offline Reinforcement Learning, Data Selection, Machine Learning, Deep Learning, Natural Language Processing",
        "abstract": "To know your enemy, you must become your enemy. Sun Tzu stated in $\\textit{The Art of War}$. Often, it is crucial to synthesize data containing harmful content using large language models (LLMs) in order to train harmless LLMs. Methods by which synthesized data can be utilized include using it as training data to provide negative signals to the model, as automatic red-teaming data to identify vulnerabilities of the model and more. However, aligned LLMs struggle to generate harmful responses. In this paper, we propose the $\\textit{refusal-free}$ training method to reach a $\\textbf{Helpful-Only LLM}$ that maintains the helpfulness of the state-of-the-art (SOTA) LLMs while allowing harmful response generation. The $\\textit{refusal-free}$ training method filters the instances that refuse an user's request from the datasets. We demonstrate that the $\\textit{refusal-free}$ training dramatically decreases the rate at which the LLM generates refusal responses (refusal rate) by 60.12% without sacrificing its helpfulness. Also, we are aware of the possibility that the progress in this direction could lead to irreversible consequences. A powerful model that does not reject harmful requests and executes them all could be exploited for illicit purposes such as the creation of indiscriminate weapons or hacking. However, once again, we believe it is important to be the one to break an LLM and study how an LLM can be broken in advance, including understanding the boundaries a $\\textbf{Helpful-Only LLM}$ can reach and identifying its inherent tendencies. We emphasize that this study is wholly for academic purpose and is aimed at paving the way toward a harmless LLM. This study calls for the researchers to acknowledge the potential failures of LLMs and take steps to prevent the breakdowns. $\\textbf{Content Warning:}$ This paper contains examples that may be offensive in nature, and reader discretion is recommended."
    },
    {
        "title": "DisCoNet: Rethinking Adversarial Networks for Discriminator-Driven Distribution Modeling",
        "link_suffix": "/forum?id=SQldSly0vb",
        "link": "https://openreview.net/forum?id=SQldSly0vb",
        "pdf_link": "https://openreview.net/pdf?id=SQldSly0vb",
        "keywords": "Covariate Shift, Deep Learning, Generative Adversarial Networks, Generative Modelling, Out-of-Distribution Detection",
        "abstract": "Out-of-distribution (OOD) detection holds significant importance across various applications. While semantic and domain-shift OOD problems are well-documented, this work focuses on the nuances of covariate shifts, which entail subtle perturbations or variations in the data distribution. These disturbances have proven to negatively impact machine learning performance. We have found that existing OOD detection methods often struggle to effectively distinguish covariate shifts from in-distribution instances, emphasizing the need for specialized solutions. Therefore, we propose DisCoNet, an Adversarial Variational Autoencoder (VAE) that rethinks the Generative Adversarial Networks paradigm. Instead of prioritizing the generator as the network's core, we focus on the discriminator, using the generator as a supporting training tool. DisCoNet uses the VAE's suboptimal outputs as negative samples to train the discriminator, thereby improving its ability to delineate the boundary between in-distribution samples and covariate shifts. By tightening this in-distribution boundary, DisCoNet achieves state-of-the-art results in public OOD detection benchmarks. The proposed model not only excels in detecting covariate shifts, achieving 98.9% AUROC on ImageNet-1K(-C), but also outperforms all prior methods on public semantic OOD benchmarks. With a model size of $\\leq$ 25MB, it is highly effective on Far-OOD (OpenImage-O (99.4%) and iNaturalist (100.0%)) and Near-OOD (SSB-hard (99.9%) and NINCO (99.7%)) detection. The code will be made publicly available."
    },
    {
        "title": "Understanding Grokking: Insights from Neural Network Robustness",
        "link_suffix": "/forum?id=IRjT0AmsDI",
        "link": "https://openreview.net/forum?id=IRjT0AmsDI",
        "pdf_link": "https://openreview.net/pdf?id=IRjT0AmsDI",
        "keywords": "Grokking, Representation Learning, Training Dynamics",
        "abstract": "Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the usually observed decreasing of $l_2$ weight norm of the neural network is theoretically connected to the occurrence of grokking. Therefore, we propose to use perturbation-based methods to enhance robustness and speed up the generalization process. Furthermore, we show that the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. In addition, we empirically observe that \n$l_2$ norm correlates with grokking on the test data not in a timely way and then propose new metrics based on robustness that correlate better with the grokking phenomenon."
    },
    {
        "title": "MLGLP: Multi-Scale Line-Graph Link Prediction based on Graph Neural Networks",
        "link_suffix": "/forum?id=0HqPwbN1Su",
        "link": "https://openreview.net/forum?id=0HqPwbN1Su",
        "pdf_link": "https://openreview.net/pdf?id=0HqPwbN1Su",
        "keywords": "link prediction, graph neural network, multi-scale graph, line graph, complex network.",
        "abstract": "This manuscript proposes a multi-scale link prediction approach based on Graph Neural Networks (GNNs). The proposed method - Multi-Scale Line-Graph Link Prediction (MLGLP) - learns the graph structure and extracts effective representative features of graph edges to address challenges such as information loss and handle multi-scale information. This approach utilizes embedding vectors generated by GNNs from enclosing subgraphs. While expanding GNN layers can capture more intricate relations, it often leads to overs-smoothing. To mitigate this issue, we propose constructing coarse-grained graphs at three distinct scales to uncover complex relations. To apply multi-scale subgraphs in GNNs without using pooling layers that lead to information loss, we convert each subgraph into a line-graph and reformulate the task as a node classification problem. The hierarchical structure facilitates exploration across various levels of abstraction, fostering deeper comprehension of the relationships and dependencies inherent within the graph. The proposed method is applied on link prediction problem, which can be modelled as a graph classification problem. We perform extensive experiments on several well-known benchmarks and compare the results with state-of-the-art link prediction methods. The experimental results demonstrate the superiority of our proposed model in terms of average precision and area under the curve."
    },
    {
        "title": "RevisEval: Improving LLM-as-a-Judge via Response-Adapted References",
        "link_suffix": "/forum?id=1tBvzOYTLF",
        "link": "https://openreview.net/forum?id=1tBvzOYTLF",
        "pdf_link": "https://openreview.net/pdf?id=1tBvzOYTLF",
        "keywords": "large language models, evaluation, revision",
        "abstract": "With significant efforts in recent studies, LLM-as-a-Judge has become a cost-effective alternative to human evaluation for assessing the text generation quality in a wide range of tasks. However, there still remains a reliability gap between LLM-as-a-Judge and human evaluation. One important reason is the lack of guided oracles in the evaluation process. Motivated by the role of reference pervasively used in classic text evaluation, we introduce RevisEval, a novel text generation evaluation paradigm via the response-adapted references. RevisEval is driven by the key observation that an ideal reference should maintain the necessary relevance to the response to be evaluated. Specifically, RevisEval leverages the text revision capabilities of large language models (LLMs) to adaptively revise the response, then treat the revised text as the reference (response-adapted reference) for the subsequent evaluation. Extensive experiments demonstrate that RevisEval outperforms traditional reference-free and reference-based evaluation paradigms that use LLM-as-a-Judge across NLG tasks and open-ended instruction-following tasks. More importantly, our response-adapted references can further boost the classical text metrics, e.g., BLEU and BERTScore, compared to traditional references and even rival the LLM-as-a-Judge. A detailed analysis is also conducted to confirm RevisEval's effectiveness in bias reduction, the impact of inference cost, and reference relevance."
    },
    {
        "title": "Multi-Task Perception in Unstructured Environments: Anti-Degradation Complementary Learning and SAMEnhancer",
        "link_suffix": "/forum?id=OM1R87YLTc",
        "link": "https://openreview.net/forum?id=OM1R87YLTc",
        "pdf_link": "https://openreview.net/pdf?id=OM1R87YLTc",
        "keywords": "autonomous driving, unstructured environment, mobile sam, semi-supervised learning",
        "abstract": "While autonomous driving perception has advanced significantly in structured environments, unstructured environments still present major challenges due to the complexity of traffic participants and irregular road conditions. This paper focuses on addressing these challenges through multi-task perception, targeting drivable area segmentation and object detection in unstructured environments. A key issue in existing datasets for unstructured settings is the non-overlapping annotation of images across different tasks, which limits the efficiency of data utilization. To tackle this, we propose Anti-Degradation Complementary Learning (ADC learning), a semi-supervised approach that allows different tasks to share knowledge across unlabeled data, thereby maximizing the use of available image information. Additionally, we introduce SAMEnhancer, which integrates the Segment Anything Model (SAM) to improve segmentation quality by combining the semantic specificity of network training with the coherence of SAM\u2019s segmentation. Extensive experiments validate the effectiveness of our methods, demonstrating significant performance improvements in both segmentation and detection, especially in challenging unstructured scenarios."
    },
    {
        "title": "12-Lead ECG Generation via a PDE-Based GAN",
        "link_suffix": "/forum?id=66j2BdZv07",
        "link": "https://openreview.net/forum?id=66j2BdZv07",
        "pdf_link": "https://openreview.net/pdf?id=66j2BdZv07",
        "keywords": "12-Lead ECG Classification, Generative Models, Clinical Multivariate Time Series, Partial Differential Equations",
        "abstract": "Synthesizing realistic 12-lead electrocardiogram (ECG) data is a complex task due to the intricate spatial and temporal dynamics of cardiac electrophysiology. Traditional generative models often struggle to capture the nuanced interdependencies among ECG leads, which are essential for accurate medical analysis. In this paper, we introduce a novel method that integrates partial differential equations (PDEs) into a generative adversarial network (GAN) framework to model the spatiotemporal behavior of the heart's electrical activity. By embedding PDE-based representations directly into the generative process, our approach effectively captures both the temporal evolution and spatial relationships between ECG leads. This results in the production of high-fidelity synthetic 12-lead ECG data that closely mirrors real physiological signals. We conduct extensive experiments to evaluate the efficacy of our PDECGAN model, demonstrating that classifiers trained on our synthetic data outperform those trained on data generated by conventional methods in detecting cardiac abnormalities, with statistically significant improvements. Our work highlights the potential of combining PDE-driven cardiac models with advanced generative techniques to enhance the quality and utility of synthetic biomedical datasets."
    },
    {
        "title": "Learning to Animate Images from A Few Videos to Portray Delicate Human Actions",
        "link_suffix": "/forum?id=3By4N0GAdt",
        "link": "https://openreview.net/forum?id=3By4N0GAdt",
        "pdf_link": "https://openreview.net/pdf?id=3By4N0GAdt",
        "keywords": "Image Animation, Video Generation, Few-shot",
        "abstract": "Despite recent progress, video generative models still struggle to animate human actions from static images, particularly when handling uncommon actions whose training data are limited. In this paper, we investigate the task of learning to animate human actions from a small number of videos---16 or fewer---which is highly valuable in real-world applications like video and movie production. Few-shot learning of generalizable motion patterns while ensuring smooth transitions from the initial reference image is exceedingly challenging. We propose FLASH (Few-shot Learning to Animate and Steer Humans), which improves motion generalization by aligning motion features and inter-frame correspondence relations between videos that share the same motion but have different appearances. This approach minimizes overfitting to visual appearances in the limited training data and enhances the generalization of learned motion patterns. Additionally, FLASH extends the decoder with additional layers to compensate lost details in the latent space, fostering smooth transitions from the reference image. Experiments demonstrate that FLASH effectively animates images with unseen human or scene appearances into specified actions while maintaining smooth transitions from the reference image."
    },
    {
        "title": "Interpretable Compressed Descriptions For Image Generation",
        "link_suffix": "/forum?id=aXwukBD6M6",
        "link": "https://openreview.net/forum?id=aXwukBD6M6",
        "pdf_link": "https://openreview.net/pdf?id=aXwukBD6M6",
        "keywords": "Controllable Generation, Image Generation, Interpretability, Information Pursuit, Information Theory, Diffusion Models",
        "abstract": "Generative models can be applied in diverse domains, from natural language processing to image synthesis. A key aspect to control the generation process is the definition of adequate data representations, allowing users to access and efficiently manipulate the semantic factors shaping the data distribution.\nThis work advocates for the adoption of succinct, informative, and interpretable descriptions, quantified using information theoretic principles. Through extensive experiments, we demonstrate the efficacy of this proposed framework both qualitatively and quantitatively. We conclude that it significantly contributes to the ongoing quest to enhance both controllability and interpretability in the generation process."
    },
    {
        "title": "Masked Cross-attention Adapters Enable the Characterization of Dense Features",
        "link_suffix": "/forum?id=zV6D212c7Q",
        "link": "https://openreview.net/forum?id=zV6D212c7Q",
        "pdf_link": "https://openreview.net/pdf?id=zV6D212c7Q",
        "keywords": "image features, image backbones, ViT, instance segmentation",
        "abstract": "Learning meaningful representations is a core topic of deep learning. Throughout the last decade, many strategies for learning image representations have been proposed involving supervision and self-supervision and various data sources. \nIn most current work, evaluation is focused on classification tasks while neglecting dense prediction tasks, possibly because linear probing is more challenging in the latter case.\nFurthermore, dense prediction heads are often large and come with specific inductive biases that distort performance measurement further.\nIn this work we propose masked cross-attention adapters (MAXA), an adapter method that is capable of dense predictions independent of the size and resolution of the encoder output. This allows us to make dense predictions using a small number of additional parameters ($<0.3 $%) while allowing for fast training using frozen backbones.\nUsing this adapter, we run a comprehensive evaluation assessing instance awareness, local semantics and spatial representation of a diverse set of backbones. \nWe find that DINOv2 outperforms all other backbones tested - including those supervised with masks and language - across all three task categories.Code is available athttps://to.be.released."
    },
    {
        "title": "Interpretable Contrastive Monte Carlo Tree Search Reasoning",
        "link_suffix": "/forum?id=F4f1afsm3R",
        "link": "https://openreview.net/forum?id=F4f1afsm3R",
        "pdf_link": "https://openreview.net/pdf?id=F4f1afsm3R",
        "keywords": "Monte Carlo Tree Search, Large Language Models, Multi-Step Reasoning",
        "abstract": "We propose $\\textbf{(S)}peculative \\textbf{(C)}ontrastive$ $\\textbf{MCTS}^\\mathbf{*}$: a novel Monte Carlo Tree Search (MCTS) reasoning algorithm for Large Language Models (LLMs), significantly improves both reasoning accuracy and speed. Our motivation comes from: 1. Previous MCTS LLM reasoning works often overlooked its biggest drawback\u2014slower speed compared to CoT; 2. Previous research mainly used MCTS as a tool for LLM reasoning on various tasks with limited quantitative analysis or ablation studies of its components from reasoning interpretability perspective. 3. The reward model is the most crucial component in MCTS, however previous work has rarely conducted in-depth study or improvement of MCTS's reward models. Thus, we conducted extensive ablation studies and quantitative analysis on components of MCTS, revealing the impact of each component on the MCTS reasoning performance of LLMs. Building on this, (i) we designed a highly interpretable reward model based on the principle of contrastive decoding and (ii) achieved an average speed improvement of 51.9% per node using speculative decoding. Additionally, (iii) we improved UCT node selection strategy and backpropagation used in previous works, resulting in significant performance improvement. We outperformed o1-mini by an average of 17.4% on the Blocksworld multi-step reasoning dataset using Llama-3.1-70B with SC-MCTS*."
    },
    {
        "title": "SPACETGN: Augmented Mini-Batch Negative Sampling  for Continuous-Time Dynamic Graph Learning",
        "link_suffix": "/forum?id=aCEg0zZ2bG",
        "link": "https://openreview.net/forum?id=aCEg0zZ2bG",
        "pdf_link": "https://openreview.net/pdf?id=aCEg0zZ2bG",
        "keywords": "Dynamic Graph Learning;Negative Sampling Strategy",
        "abstract": "Continuous-Time Dynamic Graph (CTDG) learning has significantly advanced link prediction performance by leveraging random negative sampling and incorporating adaptive temporal information.\nRecent studies aim to improve performance by introducing random sampling to obtain hard negative samples, whose quality is limited by randomness, capturing few categories of negative samples, and leading to false positive (FP) and false negative (FN) problems.\nHere we present SPACETGN, a CTDG learning framework, with a augmented hard negative sampling mini-batches (AMNS) strategy and two new feature extraction strategies that derive space-temporal locality subgraph and historical occurrence information to emphasize the graph's temporal discriminative properties. \nThe AMNS strategy sample mini-batches comprised of instances that are hard-to-distinguish (i.e., hard and true negatives with respect to each other) based on the target distribution, thereby effectively augmenting the discriminative features and the diversity of historical and inductive samples.\nFurthermore, to mitigate the challenges posed by false positives (FP) and false negatives (FN), our architecture SPACETGN employs a conceptually straightforward approach that investigates temporal subgraphs and historical interactions between source and destination nodes. This enables the model to leverage complex and historically accurate interactions among predicted entities.\nOur extensive evaluation of dynamic link prediction on seven state-of-the-practice datasets reveals that SPACETGN achieves state-of-the-art performance in most datasets, demonstrating its effectiveness in ameliorating model bias."
    },
    {
        "title": "Do Large Language Models have Lateral Thinking in Puzzle-Solving Games?",
        "link_suffix": "/forum?id=2mbDATzUOt",
        "link": "https://openreview.net/forum?id=2mbDATzUOt",
        "pdf_link": "https://openreview.net/pdf?id=2mbDATzUOt",
        "keywords": "Large Language Models, Lateral Thinking, Puzzle-Solving Games",
        "abstract": "Large Language Models (LLMs) show exceptional skills in a wide range of tasks, with their ability in lateral thinking standing out as a particularly intriguing area. Lateral thinking in LLMs allows them to understand deeper or suggested meanings from the context, which is essential for making sense of complex scenarios, especially in puzzle-solving games. To delve deeper into and improve the lateral thinking capabilities of LLMs in the realm of puzzle-solving, we introduce the ``Lateral Thinking Puzzles'' and construct the accompanying dataset.\nOur novel $\\mathcal{P}$uzzle$\\mathcal{V}$erse framework aims to enhance LLMs' lateral thinking in puzzle-solving games. Complementing this, we propose a creativity metric to ensure comprehensive evaluations. \nExperiments show that the selected LLMs, after being trained with $\\mathcal{P}$uzzle$\\mathcal{V}$erse, have an average improvement of 101.9% compared to their performance before $\\mathcal{P}$uzzle$\\mathcal{V}$erse training among all metrics. \nWe also validate the robustness of $\\mathcal{P}$uzzle$\\mathcal{V}$erse that trained LLMs perform better in other reasoning tasks."
    }
]