[{"title": "PuzzleFusion++: Auto-agglomerative 3D Fracture Assembly by Denoise and Verify", "link_suffix": "/forum?id=7E7v5mJnfl", "link": "https://openreview.net/forum?id=7E7v5mJnfl", "pdf_link": "https://openreview.net/pdf?id=7E7v5mJnfl", "keywords": "3D fracture assembly", "abstract": "This paper proposes a novel \u201cauto-agglomerative\u201d 3D fracture assembly method, PuzzleFusion++, resembling how humans solve challenging spatial puzzles. Starting from individual fragments, the approach 1) aligns and merges fragments into larger groups akin to agglomerative clustering and 2) repeats the process iteratively in completing the assembly akin to auto-regressive methods. Concretely, a diffusion model denoises the 6-DoF alignment parameters of the fragments simultaneously,\nand a transformer model verifies and merges pairwise alignments into larger ones, whose process repeats iteratively. Extensive experiments on the Breaking Bad dataset show that PuzzleFusion++ outperforms all other state-of-the-art techniques by significant margins across all metrics In particular by over 10% in part accuracy and 50% in Chamfer distance. We will release code and model.", "title_embedding_index": 21950, "title_abs_embedding_index": 21975}, {"title": "Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation", "link_suffix": "/forum?id=mm0cqJ2O3f", "link": "https://openreview.net/forum?id=mm0cqJ2O3f", "pdf_link": "https://openreview.net/pdf?id=mm0cqJ2O3f", "keywords": "Human Interaction Generation, Reactive Motion Generation", "abstract": "This paper addresses the task of generating two-character online interactions. Previously, two main settings existed for two-character interaction generation: (1) generating one's motions based on the counterpart's complete motion sequence, and (2) jointly generating two-character motions based on specific conditions. We argue that these settings fail to model the process of real-life two-character interactions, where humans will react to their counterparts in real time and act as independent individuals. In contrast, we propose an online reaction policy, called Ready-to-React, to generate the next character pose based on past observed motions. Each character has its own reaction policy as its ``brain'', enabling them to interact like real humans in a streaming manner. Our policy is implemented by incorporating a diffusion head into an auto-regressive model, which can dynamically respond to the counterpart's motions while effectively mitigating the error accumulation throughout the generation process. We conduct comprehensive experiments using the challenging boxing task. Experimental results demonstrate that our method outperforms existing baselines and can generate extended motion sequences. Additionally, we show that our approach can be controlled by sparse signals, making it well-suited for VR and other online interactive environments. Code and data will be made publicly available.", "title_embedding_index": 21951, "title_abs_embedding_index": 21976}, {"title": "Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics", "link_suffix": "/forum?id=cUN8lJB4rD", "link": "https://openreview.net/forum?id=cUN8lJB4rD", "pdf_link": "https://openreview.net/pdf?id=cUN8lJB4rD", "keywords": "nonconvex optimization, lower bounds, parallel methods, asynchronous methods, convex optimization", "abstract": "In distributed stochastic optimization, where parallel and asynchronous methods are employed, we establish optimal time complexities under virtually any computation behavior of workers/devices/CPUs/GPUs, capturing potential disconnections due to hardware and network delays, time-varying computation powers, and any possible fluctuations and trends of computation speeds. These real-world scenarios are formalized by our new universal computation model. Leveraging this model and new proof techniques, we discover tight lower bounds that apply to virtually all synchronous and asynchronous methods, including Minibatch SGD, Asynchronous SGD (Recht et al., 2011), and Picky SGD (Cohen et al., 2021). We show that these lower bounds, up to constant factors, are matched by the optimal Rennala SGD and Malenia SGD methods (Tyurin & Richt\u00e1rik, 2023).", "title_embedding_index": 21952, "title_abs_embedding_index": 21977}, {"title": "MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks", "link_suffix": "/forum?id=2rWbKbmOuM", "link": "https://openreview.net/forum?id=2rWbKbmOuM", "pdf_link": "https://openreview.net/pdf?id=2rWbKbmOuM", "keywords": "evaluation of multimodal large language models", "abstract": "We present MEGA-Bench, an evaluation suite that scales multimodal evaluation to over 500 real-world tasks, to address the highly heterogeneous daily use cases of end users.\nOur objective is to optimize for a set of high-quality data samples that cover a highly diverse and rich set of multimodal tasks, while enabling cost-effective and accurate model evaluation.\nIn particular, we collected 507 realistic tasks encompassing over 8,000 samples from 16 expert annotators to extensively cover the multimodal task space. Instead of unifying these problems into standard multi-choice questions (like MMMU, MM-Bench, and MMT-Bench), we embrace a wide range of output formats like numbers, phrases, code, \\LaTeX, coordinates, JSON, free-form, etc. To accommodate these formats, we developed over 40 metrics to evaluate these tasks. \nUnlike existing benchmarks, MEGA-Bench offers a fine-grained capability report across multiple dimensions (e.g., application, input type, output format, skill), allowing users to interact with and visualize model capabilities in depth. We evaluate a wide variety of frontier vision-language models on MEGA-Bench to understand their capabilities across these dimensions.", "title_embedding_index": 21953, "title_abs_embedding_index": 21978}, {"title": "Enhancing Pre-trained Representation Classifiability can Boost its Interpretability", "link_suffix": "/forum?id=GjfIZan5jN", "link": "https://openreview.net/forum?id=GjfIZan5jN", "pdf_link": "https://openreview.net/pdf?id=GjfIZan5jN", "keywords": "Representation interpretability, vision representations, image understanding", "abstract": "The visual representation of a pre-trained model prioritizes the classifiability on downstream tasks. However, widespread applications for pre-trained visual models have proposed new requirements for representation interpretability. It remains unclear whether the pre-trained representations can achieve high interpretability and classifiability simultaneously. To answer this question, we quantify the representation interpretability by leveraging its correlation with the ratio of interpretable semantics within representations. Given the pre-trained representations, only the interpretable semantics can be captured by interpretations, whereas the uninterpretable part leads to information loss. Based on this fact, we propose the Inherent Interpretability Score (IIS) that evaluates the information loss, measures the ratio of interpretable semantics, and quantifies the representation interpretability. In the evaluation of the representation interpretability with different classifiability, we surprisingly discover that the interpretability and classifiability are positively correlated, i.e., representations with higher classifiability provide more interpretable semantics that can be captured in the interpretations. This observation further supports two benefits to the pre-trained representations. First, the classifiability of representations can be further improved by fine-tuning with interpretability maximization. Second, with the classifiability improvement for the representations, we obtain predictions based on their interpretations with less accuracy degradation. The discovered positive correlation and corresponding applications show that practitioners can unify the improvements in interpretability and classifiability for pre-trained vision models. Codes are included in the supplement and will be released on GitHub.", "title_embedding_index": 21954, "title_abs_embedding_index": 21979}, {"title": "Unleashing the Potential of Temperature Scaling for Multi-Label Logit Distillation", "link_suffix": "/forum?id=Agx4RXuYUJ", "link": "https://openreview.net/forum?id=Agx4RXuYUJ", "pdf_link": "https://openreview.net/pdf?id=Agx4RXuYUJ", "keywords": "Knowledge Distillation, Temperature Scaling, Multi-label Learning, Computer Vision", "abstract": "This paper undertakes meticulous scrutiny of the pure logit-based distillation under multi-label learning through the lens of activation function. We begin with empirically clarifying a recently discovered perspective that vanilla sigmoid per se is more suitable than tempered softmax in multi-label distillation, is not entirely correct. After that, we reveal that both the sigmoid and tempered softmax have an intrinsic limitation. In particular, we conclude that ignoring the decisive factor temperature $\\tau$ in the sigmoid is the essential reason for its unsatisfactory results. With this regard, we propose unleashing the potential of temperature scaling in the multi-label distillation and present Tempered Logit Distillation (TLD), an embarrassingly simple yet astonishingly performant approach. Specifically, we modify the sigmoid with the temperature scaling mechanism, deriving a new activation function, dubbed as tempered sigmoid. With theoretical and visual analysis, intriguingly, we identify that tempered sigmoid with $\\tau$ smaller than 1 provides an effect of hard mining by governing the magnitude of penalties according to the sample difficulty, which is shown as the key property to its success. Our work is accompanied by comprehensive experiments on COCO, PASCAL-VOC, and NUS-WIDE over several architectures across three multi-label learning scenarios: image classification, object detection, and instance segmentation. Distillation results evidence that TLD consistently harvests remarkable performance and surpasses the prior counterparts, demonstrating its superiority and versatility.", "title_embedding_index": 21955, "title_abs_embedding_index": 21980}, {"title": "Stable Consistency Tuning: Understanding and Improving Consistency Models", "link_suffix": "/forum?id=mzJAupYURK", "link": "https://openreview.net/forum?id=mzJAupYURK", "pdf_link": "https://openreview.net/pdf?id=mzJAupYURK", "keywords": "Diffusion models, Consistency models", "abstract": "Diffusion models achieve superior generation quality but suffer from slow generation speed due to the iterative nature of denoising. In contrast, consistency models, a new generative family, achieve competitive performance with significantly faster sampling. \nThese models are trained either through consistency distillation, which leverages pretrained diffusion models, or consistency training/tuning directly from raw data. \nIn this work, we propose a novel framework for understanding consistency models by modeling the denoising process of the diffusion model as a Markov Decision Process (MDP) and framing consistency model training as the value estimation through Temporal Difference (TD) Learning. \nMore importantly, this framework allows us to analyze the limitations of current consistency training/tuning strategies.\nBuilt upon Easy Consistency Tuning (ECT), we propose Stable Consistency Tuning (SCT), which incorporates variance-reduced learning using the score identity.\nSCT leads to significant performance improvements on benchmarks such as CIFAR-10 and ImageNet-64. On ImageNet-64, SCT achieves 1-step FID 2.42 and 2-step FID 1.55, a new SoTA for consistency models.", "title_embedding_index": 21956, "title_abs_embedding_index": 21981}, {"title": "Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow", "link_suffix": "/forum?id=nEDToD1R8M", "link": "https://openreview.net/forum?id=nEDToD1R8M", "pdf_link": "https://openreview.net/pdf?id=nEDToD1R8M", "keywords": "Diffusion models, Rectified-Flow", "abstract": "Diffusion models have greatly improved visual generation but are hindered by slow generation speed due to the computationally intensive nature of solving generative ODEs. Rectified flow, a widely recognized solution, improves generation speed by straightening the ODE path. Its key components include: 1) using the diffusion form of flow-matching, 2) employing $\\boldsymbol v$-prediction, and 3) performing rectification (a.k.a. reflow). In this paper, we argue that the success of rectification primarily lies in using a pretrained diffusion model to obtain matched pairs of noise and samples, followed by retraining with these matched noise-sample pairs. Based on this, components 1) and 2) are unnecessary. Furthermore, we highlight that straightness is not an essential training target for rectification; rather, it is a specific case of flow-matching models. The more critical training target is to achieve a first-order approximate ODE path, which is inherently curved for models like DDPM and Sub-VP. Building on this insight, we propose Rectified Diffusion, which generalizes the design space and application scope of rectification to encompass the broader category of diffusion models, rather than being restricted to flow-matching models. We validate our methods on Stable Diffusion v1-5 and Stable Diffusion XL. Our methods not only greatly simplifies the training procedure of rectified flow-based previous works~(e.g., InstaFlow) but also achieves superior performance with even lower training cost.", "title_embedding_index": 21957, "title_abs_embedding_index": 21982}, {"title": "Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models", "link_suffix": "/forum?id=iJi7nz5Cxc", "link": "https://openreview.net/forum?id=iJi7nz5Cxc", "pdf_link": "https://openreview.net/pdf?id=iJi7nz5Cxc", "keywords": "Diffusion, Preference Optimization", "abstract": "Diffusion models have made substantial advances in image generation, yet models trained on large, unfiltered datasets often yield outputs misaligned with human preferences. Numerous methods have already been proposed to fine-tune pre-trained diffusion models, achieving notable improvements in aligning generated outputs with human preferences. However, we point out that existing preference alignment methods neglect the critical role of handling unconditional/negative-conditional  outputs, leading to a diminished capacity to avoid generating undesirable outcomes. This oversight limits the efficacy of classifier-free guidance (CFG), which relies on the contrast between conditional generation and unconditional/negative-conditional generation to optimize output quality. In response, we propose a straightforward but versatily effective approach that involves training a model specifically attuned to negative preferences. This method does not require new training strategies or datasets but rather involves minor modifications to existing techniques. Our approach integrates seamlessly with models such as SD15, SDXL, video diffusion models and models that have undergone preference optimization, consistently enhancing their ability to produce more human preferences aligned outputs.", "title_embedding_index": 21958, "title_abs_embedding_index": 21983}, {"title": "Towards good practice in boosting the targeted adversarial attack", "link_suffix": "/forum?id=gWk8WQVWGr", "link": "https://openreview.net/forum?id=gWk8WQVWGr", "pdf_link": "https://openreview.net/pdf?id=gWk8WQVWGr", "keywords": "adversariak attack", "abstract": "By accessing only the surrogate model, attackers can craft adversarial perturbations to fool black-box victim models into misclassifying a given image into the target class. However, the misalignment between surrogate models and victim models raises concerns about defining what constitutes a successful targeted attack in a black-box setting. In our work, we empirically identify that the vision-language foundation model CLIP is a natural good indicator to evaluate a good transferable targeted attacks. We find that a successful transferable targeted attack not only confuse the model on the vision modality towards the target class, but also fool the model on the text modality between the original class and target class. Motivated by this finding, we propose a simple yet effective regularization term to boost the existing transferable targeted attacks. We also revisit the feature-based attacks, and propose to boost the performance by enhancing the fine-grained features.  Extensive experiments on the ImageNet-1k dataset demonstrate the effectiveness of our proposed methods. We hope our finding can motivate future research on the understanding of targeted attacks and develop more powerful techniques.", "title_embedding_index": 21959, "title_abs_embedding_index": 21984}, {"title": "Will the Inclusion of Generated Data Amplify Bias Across Generations in Future Image Classification Models?", "link_suffix": "/forum?id=0py3h7pops", "link": "https://openreview.net/forum?id=0py3h7pops", "pdf_link": "https://openreview.net/pdf?id=0py3h7pops", "keywords": "model bias, image classification", "abstract": "As the demand for high-quality training data escalates, researchers have increasingly turned to generative models to create synthetic data, addressing data scarcity and enabling continuous model improvement. However, reliance on self-generated data introduces a critical question: \\textit{Will this practice amplify bias in future models?} While most research has focused on overall performance, the impact on model bias, particularly subgroup bias, remains underexplored. In this work, we investigate the effects of the generated data on image classification tasks, with a specific focus on bias. We develop a practical simulation environment that integrates a self-consuming loop, where the generative model and classification model are trained synergistically. Hundreds of experiments are conducted on Colorized MNIST, CIFAR-20/100, and Hard ImageNet datasets to reveal changes in fairness metrics across generations. In addition, we provide a conjecture to explain the bias dynamics when training models on continuously augmented datasets across generations. Our findings contribute to the ongoing debate on the implications of synthetic data for fairness in real-world applications.", "title_embedding_index": 21960, "title_abs_embedding_index": 21985}, {"title": "Asymmetric Factorized Bilinear Operation for Vision Transformer", "link_suffix": "/forum?id=MJyqwBVgMs", "link": "https://openreview.net/forum?id=MJyqwBVgMs", "pdf_link": "https://openreview.net/pdf?id=MJyqwBVgMs", "keywords": "Vision Transformer, Channel Mixer, Factorized Bilinear Operation", "abstract": "As a core component of Transformer-like deep architectures, a feed-forward network (FFN) for channel mixing is responsible for learning features of each token. Recent works show channel mixing can be enhanced by increasing computational burden or can be slimmed at the sacrifice of performance. Although some efforts have been made, existing works are still struggling to solve the paradox of performance and complexity trade-offs. In this paper, we propose an Asymmetric Factorized Bilinear Operation (AFBO) to replace FFN of vision transformer (ViT), which attempts to efficiently explore rich statistics of token features for achieving better performance and complexity trade-off. Specifically, our AFBO computes second-order statistics via a spatial-channel factorized bilinear operation for feature learning, which replaces a simple linear projection in FFN and enhances the feature learning ability of ViT by modeling second-order correlation among token features. Furthermore, our AFBO presents two structured-sparsity channel mapping strategies, namely Grouped Cross Channel Mapping (GCCM) and Overlapped Cycle Channel Mapping (OCCM). They decompose bilinear operation into grouped channel features by considering information interaction between groups, significantly reducing computational complexity while guaranteeing model performance. Finally, our AFBO is built with GCCM and OCCM in an asymmetric way, aiming to achieve a better trade-off. Note that our AFBO is model-agnostic, which can be flexibly integrated with existing ViTs. Experiments are conducted with twenty ViTs on various tasks, and the results show our AFBO is superior to its counterparts while improving existing ViTs in terms of generalization and robustness.", "title_embedding_index": 21961, "title_abs_embedding_index": 21986}, {"title": "Meta-Learning Neural Procedural Biases", "link_suffix": "/forum?id=8khcyTc4Di", "link": "https://openreview.net/forum?id=8khcyTc4Di", "pdf_link": "https://openreview.net/pdf?id=8khcyTc4Di", "keywords": "meta-learning, few-shot learning", "abstract": "The goal of few-shot learning is to generalize and achieve high performance on new unseen learning tasks, where each task has only a limited number of examples available. Gradient-based meta-learning attempts to address this challenging task by learning how to learn new tasks by embedding inductive biases informed by prior learning experiences into the components of the learning algorithm. In this work, we build upon prior research and propose Neural Procedural Bias Meta-Learning (NPBML), a novel framework designed to meta-learn task-adaptive procedural biases. Our approach aims to consolidate recent advancements in meta-learned initializations, optimizers, and loss functions by learning them simultaneously and making them adapt to each individual task to maximize the strength of the learned inductive biases. This imbues each learning task with a unique set of procedural biases which is specifically designed and selected to attain strong learning performance in only a few gradient steps. The experimental results show that by meta-learning the procedural biases of a neural network, we can induce strong inductive biases towards a distribution of learning tasks, enabling robust learning performance across many well-established few-shot learning benchmarks.", "title_embedding_index": 21962, "title_abs_embedding_index": 21987}, {"title": "MM-R3: On (In-)Consistency of Multi-modal Large Language Models (MLLMs)", "link_suffix": "/forum?id=70YeidEcYR", "link": "https://openreview.net/forum?id=70YeidEcYR", "pdf_link": "https://openreview.net/pdf?id=70YeidEcYR", "keywords": "Consistency Analysis, MLLMs, VL Benchmark", "abstract": "With the advent of Large Language Models (LLMs) and Multimodal (Visio-lingual) LLMs, a flurry of research has emerged, analyzing the performance of such models across a diverse array of tasks. While most studies focus on evaluating the capabilities of state-of-the-art (SoTA) MLLM models through task accuracy (e.g., Visual Question Answering, grounding) across various datasets, our work explores the related but complementary aspect of consistency -- the ability of an MLLM model to produce semantically similar or identical responses to semantically similar queries. We note that consistency is a fundamental prerequisite (necessary but not sufficient condition) for robustness and trust in MLLMs. Humans, in particular, are known to be highly consistent (even if not always accurate) in their responses, and consistency is inherently expected from AI systems. Armed with this perspective, we propose the MM-R$^3$ benchmark, which analyses the performance in terms of consistency and accuracy in SoTA MLLMs with three tasks: Question Rephrasing, Image Restyling, and Context Reasoning. Our analysis reveals that consistency does not always align with accuracy, indicating that models with higher accuracy are not necessarily more consistent, and vice versa. Furthermore, we propose a simple yet effective mitigation strategy in the form of an adapter module trained to minimize inconsistency across prompts. With our proposed strategy, we are able to achieve absolute improvements of 5.7% and 12.5%, on average on widely used MLLMs such as BLIP-2 and LLaVa 1.5M in terms of consistency over their \nexisting counterparts.", "title_embedding_index": 21963, "title_abs_embedding_index": 21988}, {"title": "ET-SEED: EFFICIENT TRAJECTORY-LEVEL SE(3) EQUIVARIANT DIFFUSION POLICY", "link_suffix": "/forum?id=OheAR2xrtb", "link": "https://openreview.net/forum?id=OheAR2xrtb", "pdf_link": "https://openreview.net/pdf?id=OheAR2xrtb", "keywords": "Robotics; Manipulation; Equivariance", "abstract": "Imitation learning, e.g., diffusion policy, has been proven effective in various robotic manipulation tasks.\nHowever, extensive demonstrations are required for policy robustness and generalization.\nTo reduce the demonstration reliance, we leverage spatial symmetry and propose ET-SEED, an efficient trajectory-level SE(3) equivariant diffusion model for generating action sequences in complex robot manipulation tasks.\nFurther, previous equivariant diffusion models require the per-step equivariance in the Markov process, making it difficult to learn policy under such strong constraints.\nWe theoretically extend equivariant Markov kernels and simplify the condition of equivariant diffusion process, thereby significantly improving training efficiency for trajectory-level SE(3) equivariant diffusion policy in an end-to-end manner.\nWe evaluate ET-SEED on representative robotic manipulation tasks, involving rigid body, articulated and deformable object.\nExperiments demonstrate superior data efficiency and manipulation proficiency of our proposed method,\nas well as its ability to generalize to unseen configurations with only a few demonstrations. Website:https://et-seed.github.io/", "title_embedding_index": 21964, "title_abs_embedding_index": 21989}, {"title": "Comprehensive Online Training and Deployment for Spiking Neural Networks", "link_suffix": "/forum?id=JAnyCnK5In", "link": "https://openreview.net/forum?id=JAnyCnK5In", "pdf_link": "https://openreview.net/pdf?id=JAnyCnK5In", "keywords": "Efficient Multi-Precision Firing model, Online Training, Spiking Neural Networks", "abstract": "Spiking Neural Networks (SNNs) are considered to have enormous potential in the future development of Artificial Intelligence (AI) due to their brain-inspired and energy-efficient properties. In the current supervised learning domain of SNNs, compared to vanilla Spatial-Temporal Back-propagation (STBP) training, online training can effectively overcome the risk of GPU memory explosion and has received widespread academic attention. However, the current proposed online training methods cannot tackle the inseparability problem of temporal dependent gradients and merely aim to optimize the training memory, resulting in no performance advantages compared to the STBP training models in the inference phase. To address the aforementioned challenges, we propose Efficient Multi-Precision Firing (EM-PF) model, which is a family of advanced spiking models based on floating-point spikes and binary synaptic weights. We point out that EM-PF model can effectively separate temporal gradients and achieve full-stage optimization towards computation speed and memory footprint. Experimental results have demonstrated that EM-PF model can be flexibly combined with various techniques including random back-propagation, parallel computation and channel attention mechanism, to achieve state-of-the-art performance with extremely low computational overhead in the field of online learning.", "title_embedding_index": 21965, "title_abs_embedding_index": 21990}, {"title": "Iterative Dual-RL: An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning", "link_suffix": "/forum?id=9JtG4nN7ql", "link": "https://openreview.net/forum?id=9JtG4nN7ql", "pdf_link": "https://openreview.net/pdf?id=9JtG4nN7ql", "keywords": "dual RL, imitation learning, offline RL, deep RL", "abstract": "We introduce Iterative Dual Reinforcement Learning (IDRL), a new method that takes an optimal discriminator-weighted imitation view of solving RL. Our method is motivated by a simple experiment in which we find training a discriminator using the offline dataset plus an additional expert dataset and then performing discriminator-weighted behavior cloning gives strong results on various types of datasets. That optimal discriminator weight is quite similar to the learned visitation distribution ratio in Dual-RL, however, we find that current Dual-RL methods do not correctly estimate that ratio. In IDRL, we propose a correction method to iteratively approach the optimal visitation distribution ratio in the offline dataset given no addtional expert dataset. During each iteration, IDRL removes zero-weight suboptimal transitions using the learned ratio from the previous iteration and runs Dual-RL on the remaining subdataset. This can be seen as replacing the behavior visitation distribution with the optimized visitation distribution from the previous iteration, which theoretically gives a curriculum of improved visitation distribution ratios that are closer to the optimal discriminator weight. We verify the effectiveness of IDRL on various kinds of offline datasets, including D4RL datasets and more realistic corrupted demonstrations. IDRL beats strong Primal-RL and Dual-RL baselines in terms of both performance and stability, on all datasets.", "title_embedding_index": 21966, "title_abs_embedding_index": 21991}, {"title": "DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image", "link_suffix": "/forum?id=rfrtFwnF62", "link": "https://openreview.net/forum?id=rfrtFwnF62", "pdf_link": "https://openreview.net/pdf?id=rfrtFwnF62", "keywords": "interaction, deformation, end-to-end, mesh recovery", "abstract": "Reconstructing 3D hand-face interactions with deformations from a single image is a challenging yet crucial task with broad applications in AR, VR, and gaming. The challenges stem from self-occlusions during single-view hand-face interactions, diverse spatial relationships between hands and face, complex deformations, and the ambiguity of the single-view setting. The previous state-of-the-art for hand-face interaction recovery, Decaf, introduces a global fitting optimization guided by contact and deformation estimation networks trained on studio-collected data with 3D annotations. However, Decaf suffers from a time-consuming optimization process and limited generalization capability due to its reliance on 3D annotations of hand-face interaction data. To address these issues, we present DICE, the first end-to-end method for Deformation-aware hand-face Interaction reCovEry from a single image. DICE estimates the poses of hands and faces, contacts, and deformations simultaneously using a Transformer-based architecture. It features disentangling the regression of local deformation fields and global mesh vertex locations into two network branches, enhancing deformation and contact estimation for precise and robust hand-face mesh recovery. To improve generalizability, we propose a weakly-supervised training approach that augments the training set using in-the-wild images without 3D ground-truth annotations, employing the depths of 2D keypoints estimated by off-the-shelf models and adversarial priors of poses for supervision. Our experiments demonstrate that DICE achieves state-of-the-art performance on a standard benchmark and in-the-wild data in terms of accuracy and physical plausibility. Additionally, our method operates at an interactive rate (20 fps) on an Nvidia 4090 GPU, whereas Decaf requires more than 15 seconds for a single image. Our code will be publicly available upon publication.", "title_embedding_index": 21967, "title_abs_embedding_index": 21992}, {"title": "Hash3D: Training-free Acceleration for 3D Generation", "link_suffix": "/forum?id=pl8OJhyArC", "link": "https://openreview.net/forum?id=pl8OJhyArC", "pdf_link": "https://openreview.net/pdf?id=pl8OJhyArC", "keywords": "Efficient 3D generation; Score distillation", "abstract": "The quality of 3D generative modeling has been notably improved by the adoption of 2D diffusion models. Despite this progress, \nthe cumbersome optimization process \\emph{per se}\npresents a critical problem to efficiency. \nIn this paper,\nwe introduce Hash3D, a universal acceleration for 3D score distillation sampling~(SDS) without model training.\nCentral to Hash3D is the observation that images rendered from similar camera positions and diffusion time-steps often have redundant feature maps. By hashing and reusing these feature maps across nearby timesteps and camera angles, Hash3D eliminates unnecessary calculations. We implement this through an adaptive grid-based hashing. As a result, it largely speeds up the process of 3D generation. \nSurprisingly, this feature-sharing mechanism not only makes generation faster but also improves the smoothness and view consistency of the synthesized 3D objects. Our experiments covering 5 text-to-3D and 3 image-to-3D models, demonstrate Hash3D\u2019s versatility to speed up optimization, enhancing efficiency by $1.5\\sim 4\\times$. Additionally, Hash3D's integration with 3D Gaussian splatting largely speeds up 3D model creation, reducing text-to-3D processing to about 10 minutes and image-to-3D conversion to roughly 30 seconds.", "title_embedding_index": 21968, "title_abs_embedding_index": 21993}, {"title": "Variation-Bounded Losses for Learning with Noisy Labels", "link_suffix": "/forum?id=rjcA3km2IT", "link": "https://openreview.net/forum?id=rjcA3km2IT", "pdf_link": "https://openreview.net/pdf?id=rjcA3km2IT", "keywords": "Learning with Noisy Labels; Robust Loss Functions; Multi-Class Classification", "abstract": "The presence of noisy labels poses a significant challenge for training accurate deep neural networks.\nPrevious works have proposed various robust loss functions designed to address this issue, which, however, often suffer from several drawbacks, such as  underfitting or insufficient noise-tolerance. Furthermore, there is currently no reliable metric to guide the design of more effective robust loss functions.\nIn this paper, we introduce theVariation Ratioas a novel metric to measure the robustness of loss functions.  Leveraging this metric, we propose a new family of robust loss functions, termedVariation-Bounded Losses(VBL), characterized by a bounded variation ratio.\nWe investigate theoretical properties of variation-bounded losses and prove that a smaller variation ratio would lead to better robustness. Additionally, we show that the variation ratio provides a more relaxed condition than the commonly used symmetric condition for achieving noise-tolerant learning, making it a valuable tool for designing effective robust loss functions.\nWe modify several commonly used loss functions to the variation-bounded form.\nThese variation-bounded losses are characterized by their simplicity, effectiveness, and theoretical guarantees.\nExtensive experiments demonstrate the superiority of our method in mitigating various types of label noise.", "title_embedding_index": 21969, "title_abs_embedding_index": 21994}, {"title": "CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models", "link_suffix": "/forum?id=jt1h2dnmng", "link": "https://openreview.net/forum?id=jt1h2dnmng", "pdf_link": "https://openreview.net/pdf?id=jt1h2dnmng", "keywords": "diffusion models, virtual try-on, parameter-efficient training", "abstract": "Virtual try-on methods based on diffusion models achieve realistic effects but often require additional encoding modules, a large number of training parameters, and complex preprocessing, which increases the burden on training and inference. In this work, we re-evaluate the necessity of additional modules and analyze how to improve training efficiency and reduce redundant steps in the inference process. Based on these insights, we propose CatVTON, a simple and efficient virtual try-on diffusion model that transfers in-shop or worn garments of arbitrary categories to target individuals by concatenating them along spatial dimensions as inputs. The efficiency of CatVTON is reflected in three aspects: (1) Lightweight network. CatVTON consists only of a VAE and a simplified denoising UNet, removing redundant image and text encoders as well as cross-attentions, and includes just 899.06M parameters. (2) Parameter-efficient training. Through experimental analysis, we identify self-attention modules as crucial for adapting pre-trained diffusion models to the virtual try-on task, enabling high-quality results with only 49.57M training parameters. (3) Simplified inference. CatVTON eliminates unnecessary preprocessing, such as pose estimation, human parsing, and captioning, requiring only person image and garment reference to guide the virtual try-on process, reducing over 49% memory usage compared to other diffusion-based methods. Extensive experiments demonstrate that CatVTON achieves superior qualitative and quantitative results compared to baseline methods and demonstrates strong generalization performance in real-world scenarios, despite being trained solely on a public dataset of 73K samples.", "title_embedding_index": 21970, "title_abs_embedding_index": 21995}, {"title": "IDEATOR: Jailbreaking VLMs Using VLMs", "link_suffix": "/forum?id=JnRvQ8CxLx", "link": "https://openreview.net/forum?id=JnRvQ8CxLx", "pdf_link": "https://openreview.net/pdf?id=JnRvQ8CxLx", "keywords": "Black-box Jailbreak, Large Vision-Language Models", "abstract": "As large Vision-Language Models (VLMs) continue to gain prominence, ensuring their safety deployment in real-world applications has become a critical concern. Recently, significant research efforts have focused on evaluating the robustness of VLMs against jailbreak attacks. Due to challenges in obtaining multi-modal data, current studies often assess VLM robustness by generating adversarial or query-relevant images based on harmful text datasets. However, the jailbreak images generated this way exhibit certain limitations. Adversarial images require white-box access to the target VLM and are relatively easy to defend against, while query-relevant images must be linked to the target harmful content, limiting their diversity and effectiveness. In this paper, we propose a novel jailbreak method named IDEATOR, which autonomously generates malicious image-text pairs for black-box jailbreak attacks. IDEATOR is a VLM-based approach inspired by our conjecture that a VLM itself might be a powerful red team model for generating jailbreak prompts. Specifically, IDEATOR employs a VLM to generate jailbreak texts while leveraging a state-of-the-art diffusion model to create corresponding jailbreak images. Extensive experiments demonstrate the high effectiveness and transferability of IDEATOR. It successfully jailbreaks MiniGPT-4 with a 94% success rate and transfers seamlessly to LLVA and InstructBLIP, achieving high success rates of 82% and 88%, respectively. IDEATOR uncovers previously unrecognized vulnerabilities in VLMs, calling for advanced safety mechanisms.", "title_embedding_index": 21971, "title_abs_embedding_index": 21996}, {"title": "Boosting Real-World Super-Resolution with RAW Data: a New Perspective, Dataset and Baseline", "link_suffix": "/forum?id=WxKS3XV7Z4", "link": "https://openreview.net/forum?id=WxKS3XV7Z4", "pdf_link": "https://openreview.net/pdf?id=WxKS3XV7Z4", "keywords": "Real-World Super-Resolution, RAW Data", "abstract": "Real-world image super-resolution (Real SR) aims to generate high-fidelity, detail-rich high-resolution (HR) images from low-resolution (LR) counterparts. Existing Real SR methods primarily focus on processing within the RGB domain. In this paper, we pioneer the use of detail-rich RAW data to complement RGB-only Real SR, specifically by utilizing both LR RGB and RAW inputs to generate superior HR RGB outputs. We argue that key image processing steps in Image Signal Processing, such as denoising and demosaicing, inherently result in the loss of fine details, making RAW data a valuable information source. To validate this, we present RealSR-RAW, a comprehensive dataset comprising 10,000 pairs with LR and HR RGB images, along with corresponding LR RAW data, captured across multiple smartphones under varying focal lengths and diverse scenes. Additionally, we propose a novel, general RAW adapter to efficiently integrate RAW data into existing CNNs, Transformers, and Diffusion-based Real SR models by suppressing the noise contained in RAW and aligning distribution. Extensive experiments demonstrate that incorporating RAW data significantly enhances detail recovery and improves Real SR performance across ten evaluation metrics, including both fidelity and perception-oriented metrics. Our findings open a new direction for the Real SR task, with the dataset and code made available to support future research.", "title_embedding_index": 21972, "title_abs_embedding_index": 21997}, {"title": "Efficient and Generalizable Second-Order Certified Unlearning: A Hessian-Free Online Model Updates Approach", "link_suffix": "/forum?id=C3TrHWanh5", "link": "https://openreview.net/forum?id=C3TrHWanh5", "pdf_link": "https://openreview.net/pdf?id=C3TrHWanh5", "keywords": "machine unlearning; certified data removal; privacy", "abstract": "Machine unlearning strives to uphold the data owners' right to be forgotten by enabling models to selectively forget specific data. \nRecent advances suggest pre-computing and storing statistics extracted from second-order information and implementing unlearning through Newton-style updates.\nHowever, the Hessian matrix operations are extremely costly and previous works conduct unlearning for empirical risk minimizer with strong convexity assumption, precluding their applicability to high-dimensional over-parameterized models and the nonconvergence condition.\nIn this paper, we propose an efficient Hessian-free unlearning approach. \nThe key idea is to maintain a statistical vector for each training data, computed through affine stochastic recursion of the difference between the retrained and learned models. \nWe prove that our proposed method outperforms the state-of-the-art methods in terms of the unlearning and generalization guarantees, the deletion capacity, and the time/storage complexity, under the same regularity conditions.\nThrough the strategy of recollecting statistics for removing data, we develop an online unlearning algorithm that achieves near-instantaneous data removal, as it requires only vector addition.\nExperiments demonstrate that our proposed scheme surpasses existing results by orders of magnitude in terms of time/storage costs with millisecond-level unlearning execution, while also enhancing test accuracy.", "title_embedding_index": 21973, "title_abs_embedding_index": 21998}, {"title": "On Inductive Biases That Enable Generalization in Diffusion Transformers", "link_suffix": "/forum?id=lWGXftRS5h", "link": "https://openreview.net/forum?id=lWGXftRS5h", "pdf_link": "https://openreview.net/pdf?id=lWGXftRS5h", "keywords": "Diffusion Model, Diffusion Transformer, Generalization, Inductive Bias", "abstract": "Recent work studying the generalization of diffusion models with UNet-based denoisers reveals inductive biases that can be expressed via geometry-adaptive harmonic bases. However, in practice, more recent denoising networks are often based on transformers, e.g., the diffusion transformer (DiT). This raises the question: do transformer-based denoising networks exhibit inductive biases that can also be expressed via geometry-adaptive harmonic bases? To our surprise, we find that this is not the case. This discrepancy motivates our search for the inductive bias that can lead to good generalization in DiT models. Investigating a DiT\u2019s pivotal attention modules, we find that locality of attention maps are closely associated with generalization. To verify this finding, we modify the generalization of a DiT by restricting its attention windows. We inject local attention windows to a DiT and observe an improvement in generalization. Furthermore, we empirically find that both the placement and the effective attention size of these local attention windows are crucial factors. Experimental results on the CelebA, ImageNet, and LSUN datasets show that strengthening the inductive bias of a DiT can improve both generalization and generation quality when less training data is available. Source code will be released publicly upon paper publication.", "title_embedding_index": 21974, "title_abs_embedding_index": 21999}]