[
    {
        "title": "Federated Unlearning with Diffusion Models",
        "link_suffix": "/forum?id=MU4aykgggg",
        "link": "https://openreview.net/forum?id=MU4aykgggg",
        "pdf_link": "https://openreview.net/pdf?id=MU4aykgggg",
        "keywords": "Federated Unlearning, Diffusion Model",
        "abstract": "In recent years, diffusion models are widely adopted by individual users due to their outstanding performance in generation. During usage, individual users develop a need to forget privacy-related contents, making the scenario of using diffusion models on the clients a natural federated unlearning setting. For this scenario, we propose FedDUL, a Federated UnLearning method with Diffusion models, which addresses the unlearn requests from clients using the diffusion models. On one hand, we utilize local data on the clients to perform attention-based unlearning, enabling the local diffusion model to forget the concepts specified by the clients. On the other hand, we filter and group the unlearn requests from clients, gradually aggregating reasonable requests into the global diffusion model on the server, thereby protecting client privacy within the global model. The theoretical analysis further demonstrates the inherent unity between the federated unlearning problem based on diffusion models and federated learning, and extend this unity to traditional federated unlearning methods. Extensive quantitation and visualization experiments are conducted to evaluate the unlearning of both local and global models and discuss the communication and computation costs of our method, demonstrating that our method can satisfy the unlearn requests of multiple clients without compromising the generative capabilities for irrelevant concepts, providing new ideas and methods for the application of diffusion models in federated unlearning."
    },
    {
        "title": "Communicating Activations Between Language Model Agents",
        "link_suffix": "/forum?id=bFYST1MaGh",
        "link": "https://openreview.net/forum?id=bFYST1MaGh",
        "pdf_link": "https://openreview.net/pdf?id=bFYST1MaGh",
        "keywords": "large language models, multiagent communication, embedding representation, multiagent debate",
        "abstract": "Communication between multiple language model (LM) agents has been shown to scale up the reasoning ability of LMs. While natural language has been the dominant medium for inter-LM communication, it is not obvious this should be the standard: not only does natural language communication incur high inference costs that scale quickly with the number of both agents and messages, but also the decoding process abstracts away too much rich information that could be otherwise accessed from the internal activations. In this work, we propose a simple technique whereby LMs communicate viaactivations; concretely, we pause an LM $B$'s computation at an intermediate layer, combine its current activation with another LM $A$'s intermediate activation via some function $f$, then pass $f$'s output into the next layer of $B$ and continue the forward pass till decoding is complete. This approach scales up LMs on new tasks withzeroadditional parameters and data, and saves asubstantial amount of computeover natural language communication. We test our method with various functional forms $f$ on two experimental setups—multi-player coordination games and reasoning benchmarks—and find that it achieves up to $27.0%$ improvement over natural language communication across datasets with $<$$1/4$ the compute, illustrating the superiority and robustness of activations as an alternative \"language\" for communication between LMs."
    },
    {
        "title": "Wavelet-based Positional Representation for Long Context",
        "link_suffix": "/forum?id=OhauMUNW8T",
        "link": "https://openreview.net/forum?id=OhauMUNW8T",
        "pdf_link": "https://openreview.net/pdf?id=OhauMUNW8T",
        "keywords": "Positional Encoding, Extrapolation, Wavelet Transform, Transformers, RoPE, ALiBi, NLP",
        "abstract": "In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. \nThis is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of positions in longer sequences.\nWe analyzed conventional position encoding methods for long contexts and found the following characteristics.\n(1) When the representation dimension is regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted as a restricted wavelet transform using simple Haar wavelets. \nHowever, because it only uses a single window size, it does not fully exploit the advantages of wavelet transforms, which capture the fine movements of non-stationary signals using multiple scales (window sizes). \nThis limitation could explain why RoPE performs poorly in extrapolation.\n(2)\nPrevious research as well as our own analysis indicates that Attention with Linear Biases (ALiBi) functions similarly to windowed attention, using windows of varying sizes.\nHowever, it has limitations in capturing deep dependencies because it restricts the receptive field of the model.\nFrom these insights, we propose a new position representation method that captures multiple scales (i.e., window sizes) by leveraging wavelet transforms without limiting the model's attention field.\nExperimental results show that this new method improves the performance of the model in both short and long contexts. \nIn particular, our method allows extrapolation of position information without limiting the model's attention field."
    },
    {
        "title": "Model Comparisons: XNet Outperforms KAN",
        "link_suffix": "/forum?id=eIK4ojL2QM",
        "link": "https://openreview.net/forum?id=eIK4ojL2QM",
        "pdf_link": "https://openreview.net/pdf?id=eIK4ojL2QM",
        "keywords": "XNet, Kolmogorov Arnold networks, function approximations, PDE, time series, Cauchy integral theorem",
        "abstract": "In the fields of computational mathematics and artificial\n  intelligence, the need for precise data modeling is crucial,\n  especially for predictive machine learning tasks. This paper\n  explores further XNet, a novel algorithm that employs the complex-valued\n  Cauchy integral formula, offering a superior network architecture\n  that surpasses traditional Multi-Layer Perceptrons (MLPs) and\n  Kolmogorov-Arnold Networks (KANs). XNet significant\n  improves speed and accuracy across various tasks in both low\n  and high-dimensional spaces, redefining the scope of data-driven\n  model development and providing substantial improvements over\n  established time series models like LSTMs."
    },
    {
        "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
        "link_suffix": "/forum?id=chfJJYC3iL",
        "link": "https://openreview.net/forum?id=chfJJYC3iL",
        "pdf_link": "https://openreview.net/pdf?id=chfJJYC3iL",
        "keywords": "Code LLMs; Evaluation; Contaminationl; Overfitting",
        "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEvla, MBPP) are no longer sufficient for assessing their capabilities suffering from data contamination, overfitting, saturation, and focus on merely code generation. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which collects new problems over time from contests across three competition platforms, Leetcode, Atcoder, and Codeforces. Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts over six hundred coding problems that were published between May 2023 and Aug 2024. We evaluate over 50 LLMs on LiveCodeBench (LCB for brevity) presenting the largest evaluation study of code LLMs on competition problems. Based on the study, we present novel empirical findings on contamination, overfitting, and holistic evaluations. We demonstrate that time-segmented evaluations serve as a robust approach to evade contamination; they are successful at detecting contamination across a wide range of open and closed models including GPT-4O, Claude, Deepseek, and Codestral. Next, we highlight overfitting and saturation of traditional coding benchmarks like HumanEvla and demonstrate LCB allows more reliable evaluations. Finally, our holistic evaluation scenarios allow for measuring the different capabilities of programming agents in isolation."
    },
    {
        "title": "ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints",
        "link_suffix": "/forum?id=NUD03NBDOE",
        "link": "https://openreview.net/forum?id=NUD03NBDOE",
        "pdf_link": "https://openreview.net/pdf?id=NUD03NBDOE",
        "keywords": "Reasoning about Actions and Change (RAC), Benchmark, Large Language Models (LLMs), o1-preview",
        "abstract": "Reasoning about Actions and Change (RAC) has historically played a pivotal role in solving foundational AI problems, such as the frame problem. It has driven advancements in AI fields, such as non-monotonic and commonsense reasoning. RAC remains crucial for AI systems that operate in dynamic environments, engage in interactive scenarios, or rely on commonsense reasoning. Despite substantial advances made by Large Language Models (LLMs) in various AI domains, their performance in RAC remains underexplored. To address this gap, we introduce a new diagnostic benchmark, $\\textbf{ActionReasoningBench}$, which encompasses 8 domains and includes questions for up to 19 action sequences. This benchmark rigorously evaluates LLMs across six key RAC dimensions: $\\textit{Fluent Tracking}$, $\\textit{State Tracking}$, $\\textit{Action Executability}$, $\\textit{Effects of Actions}$, $\\textit{Numerical RAC}$, and $\\textit{Composite Questions}$. LLMs demonstrate average accuracy rates of 73.55%, 65.63%, 58.73%, and 62.38% on the former four dimensions, which are frequently discussed in RAC literature. However, the performance on the latter two dimensions, which introduce complex and novel reasoning questions, the average performance of LLMs is lowered to 33.16% and 51.19%, respectively, reflecting a 17.9% performance decline. We also introduce new ramification constraints to capture the indirect effects of actions, providing deeper insights into RAC challenges. Our evaluation of state-of-the-art LLMs, including both open-source and commercial models, reveals challenges across all RAC dimensions, particularly in handling ramifications, with GPT-4o failing to solve any question and o1-preview achieving a score of only 18.4%."
    },
    {
        "title": "Gradient Descent and Attention Models: Challenges Posed by the Softmax Function",
        "link_suffix": "/forum?id=U0PwxlHiaj",
        "link": "https://openreview.net/forum?id=U0PwxlHiaj",
        "pdf_link": "https://openreview.net/pdf?id=U0PwxlHiaj",
        "keywords": "Attention, Transformers, Optimization, Dynamics, Gradient Descent, Convergence",
        "abstract": "Transformers have become ubiquitous in modern machine learning applications, yet their training remains a challenging task often requiring extensive trial and error. Unlike previous architectures, transformers possess unique attention-based components, which can complicate the training process. The standard optimization algorithm, Gradient Descent, consistently underperforms in this context, underscoring the need for a deeper understanding of these difficulties: existing theoretical frameworks fall short and fail to explain this phenomenon. To address this gap, we analyze a simplified Softmax attention model that captures some of the core challenges associated with training transformers. Through a local analysis of the gradient dynamics, we highlight the role of the Softmax function on the local curvature of the loss and show how it can lead to ill-conditioning of these models, which in turn can severely hamper the convergence speed. Our experiments confirm these theoretical findings on the critical impact of Softmax on the dynamics of Gradient Descent."
    },
    {
        "title": "Enhancing Efficiency and Regularization in Convolutional Neural Networks: Strategies for Optimized Dropout",
        "link_suffix": "/forum?id=i0TOAkzGF8",
        "link": "https://openreview.net/forum?id=i0TOAkzGF8",
        "pdf_link": "https://openreview.net/pdf?id=i0TOAkzGF8",
        "keywords": "Convolutional Neural Networks(CNNs), Probabilistic Feature Importance Dropout (PFID), Enhanced Dropout Optimization, Regularization Techniques, Adaptive Learning, Network Efficiency.",
        "abstract": "This study explores dropout optimization in Convolutional Neural Networks (CNNs), aiming to beat traditional approaches in regularization and efficiency. We introduce dynamic, context-aware strategies, embodied by Probabilistic Feature Importance Dropout (PFID). This method modifies dropout rates to the unique learning phase of CNNs, integrating adaptive, structured, and contextual dropout techniques. Experimentation, benchmarked against current state-of-the-art methods, demonstrates improvements in network performance, particularly in generalization and training efficiency. We also discuss our findings. The findings represent an advancement in dropout techniques, offering more adaptable and robust CNN models for complex datasets and computational landscapes."
    },
    {
        "title": "Solving Robotics Problems in Zero-Shot with Vision-Language Models",
        "link_suffix": "/forum?id=RQDuFF1rOn",
        "link": "https://openreview.net/forum?id=RQDuFF1rOn",
        "pdf_link": "https://openreview.net/pdf?id=RQDuFF1rOn",
        "keywords": "Embodied AI, Multi-modality, Large Language Models, Robotics, Agents, LLM Agents, Multi-agent, agentic AI, hierarchical learning",
        "abstract": "We introduce Wonderful Team, a multi-agent Vision Large Language Model (VLLM) framework designed to solve robotics problems in a zero-shot regime. In our context, zero-shot means that for a novel environment, we provide a VLLM with an image of the robot's surroundings and a task description, and the VLLM outputs the sequence of actions necessary for the robot to complete the task. Unlike prior work that requires fine-tuning parts of the pipeline -- such as adjusting an LLM on robot-specific data or training separate vision encoders -- our approach demonstrates that with careful engineering, a single off-the-shelf VLLM can autonomously handle all aspects of a robotics task, from high-level planning to low-level location extraction and action execution. Crucially, compared to using GPT-4o alone, Wonderful Team is self-corrective and capable of iteratively fixing its own mistakes, enabling it to solve challenging long-horizon tasks. We validate our framework through extensive experiments, both in simulated environments using VIMABench and in real-world settings. Our system showcases the ability to handle diverse tasks such as manipulation, goal-reaching, and visual reasoning---all in a zero-shot manner. These results underscore a key point: vision-language models have progressed rapidly in the past year and should be strongly considered as a backbone for many robotics problems moving forward."
    },
    {
        "title": "No Preference Left Behind: Group Distributional Preference Optimization",
        "link_suffix": "/forum?id=bgpNJBD6Va",
        "link": "https://openreview.net/forum?id=bgpNJBD6Va",
        "pdf_link": "https://openreview.net/pdf?id=bgpNJBD6Va",
        "keywords": "preference alignment; large language model; fairness; group preferences",
        "abstract": "Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These methods often skew toward dominant preferences, overlooking the diversity of opinions, especially when conflicting preferences arise. To address this issue, we propose Group Distribution Preference Optimization (GDPO), a novel framework that aligns language models with the distribution of preferences within a group by incorporating the concept of beliefs that shape individual preferences. GDPO calibrates a language model using statistical estimation of the group's belief distribution and aligns the model with belief-conditioned preferences, offering a more inclusive alignment framework than traditional methods. In experiments using both synthetic controllable opinion generation and real-world movie review datasets, we show that DPO fails to align with the targeted belief distributions, while GDPO consistently reduces this alignment gap during training. Additionally, our evaluation metrics demonstrate that GDPO outperforms existing approaches in aligning with group distributional preferences, marking a significant advance in pluralistic alignment."
    },
    {
        "title": "Are Classification Robustness and Explanation Robustness Really Strongly Correlated? An Analysis Through Input Loss Landscape",
        "link_suffix": "/forum?id=AqueuvXErD",
        "link": "https://openreview.net/forum?id=AqueuvXErD",
        "pdf_link": "https://openreview.net/pdf?id=AqueuvXErD",
        "keywords": "explantion robustness, adversarial training, loss landscape",
        "abstract": "This paper looks into the critical area of deep learning robustness and challenges the common belief that classification robustness and explanation robustness in image classification systems are inherently correlated.  Through a novel evaluation approach leveraging clustering for efficient assessment of explanation robustness, we demonstrate that enhancing explanation robustness does not necessarily flatten the input loss landscape with respect to explanation loss - contrary to flattened loss landscapes indicating better classification robustness. To further investigate this contradiction, a training method designed to adjust the loss landscape with respect to explanation loss is proposed. Through the new training method, we uncover that although such adjustments can impact the robustness of explanations, they do not have an influence on the robustness of classification. These findings not only challenge the previous assumption of a strong correlation between the two forms of robustness but also pave new pathways for understanding the relationship between loss landscape and explanation loss. Codes are provided in the supplement."
    },
    {
        "title": "ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection",
        "link_suffix": "/forum?id=pxwJU6rTAv",
        "link": "https://openreview.net/forum?id=pxwJU6rTAv",
        "pdf_link": "https://openreview.net/pdf?id=pxwJU6rTAv",
        "keywords": "generative agent, large language model, theory of mind, BDI model, open domain conversational AI, reflection",
        "abstract": "Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative agents. In this study, we propose a novel paradigm called ToM-agent, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions. ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent's perception of its counterpart's mental states, such as beliefs, desires, and intentions (BDIs). Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts' inferred BDIs, along with related confidence levels. We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection. Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the first-order and the second-order ToM. Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart's behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors."
    },
    {
        "title": "Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning",
        "link_suffix": "/forum?id=A6Y7AqlzLW",
        "link": "https://openreview.net/forum?id=A6Y7AqlzLW",
        "pdf_link": "https://openreview.net/pdf?id=A6Y7AqlzLW",
        "keywords": "LLM, Math Reasoning, Process Supervision, Reward Models, RL, Search",
        "abstract": "A promising approach for improving reasoning in large language models is to use process reward models (PRMs). PRMs provide feedback at each step of a multi-step reasoning trace, improving credit assignment over outcome reward models (ORMs) that only provide feedback at the final step. However, collecting dense, per-step human labels is not scalable, and training PRMs from automatically-labeled data has thus far led to limited gains. With the goal of using PRMs to improve abasepolicy via test-time search and reinforcement learning (RL), we ask: ``How should we design process rewards?'' Our key insight is that, to be effective, the process reward for a step should measureprogress: a change in the likelihood of producing a correct response in the future, before and after taking the step, as measured under aproverpolicy distinct from the base policy. Such progress values can {distinguish} good and bad steps generated by the base policy, even though the base policy itself cannot.  Theoretically, we show that even weaker provers can improve the base policy, as long as they distinguish steps without being too misaligned with the base policy. Our results show that process rewards defined as progress under such provers improve the efficiency of exploration during test-time search and online RL. We empirically validate our claims by trainingprocess advantage verifiers (PAVs)to measure progress under such provers and show that compared to ORM, they are >8% more accurate, and 1.5-5x more compute-efficient. Equipped with these insights, our PAVs enableone of the first resultsshowing a 6x gain in sample efficiency for a policy trained using online RL with PRMs vs. ORMs."
    },
    {
        "title": "CCM-DiT: Camera-pose Controllable Method for DiT-based Video Generation",
        "link_suffix": "/forum?id=15lk4nBXYb",
        "link": "https://openreview.net/forum?id=15lk4nBXYb",
        "pdf_link": "https://openreview.net/pdf?id=15lk4nBXYb",
        "keywords": "Video Generation, Diffusion Models",
        "abstract": "Despite the significant advancements made by Diffusion Transformer (DiT)-based methods in video generation, there remains a notable gap with camera-pose perspectives. Existing works such as OpenSora do not adhere precisely to anticipated trajectories, thereby limiting the utility in downstream applications such as content creation.\nTherefore, we introduce a novelty approach that achieves fine-grained control by embedding sparse camera-pose information into the temporal self-attention layers. We employ LoRA  to minimize the impact on the original attention layer parameters during fine-tuning and enhance the supervision of camera-pose in the loss function.\nAfter fine-tuning the OpenSora’s ST-DiT framework on the RealEstate10K dataset, experiments demonstrate that our method outperforms LDM-based methods for long video generation, while maintaining optimal performance in trajectory consistency and object consistency."
    },
    {
        "title": "Layer-Varying Deep Reservoir Computing Architecture",
        "link_suffix": "/forum?id=qVtfN6NoJi",
        "link": "https://openreview.net/forum?id=qVtfN6NoJi",
        "pdf_link": "https://openreview.net/pdf?id=qVtfN6NoJi",
        "keywords": "Multivariate time series, imputation, reservoir computing networks, dynamical systems",
        "abstract": "Data loss and corruption are common incidents that often lead to catastrophic consequences in both theoretical and experimental facets of data analytics. The aspiration to minimize the impacts of such consequences drives the demand for the development of effective data analytic tools and imputation methods to replace missing, corrupted, or artifacted data. \nThe focus of this paper is on multivariate time series imputation, for which we develop a dynamical systems-theoretic deep learning approach. The central idea is to view a multivariate time series as a trajectory of a dynamical system. Then, we construct a deep reservoir computing architecture to model the temporal evolution of the system by using existing data in the time series. In particular, this architecture is composed of a cascade of echo state network (ESN) layers with diminishing reservoir sizes. We then propose a layer-by-layer training scheme, which gives rise to a deep learning-based time series imputation algorithm. We further provide a rigorous convergence analysis of this algorithm by exploiting the echo state property of ESN, and demonstrate the imputation performance as well as the efficiency of the training process by utilizing both synthetic and real-world datasets arising from diverse applications."
    },
    {
        "title": "Efficient Multi Subject Visual Reconstruction from fMRI Using Aligned Representations",
        "link_suffix": "/forum?id=z2QdVmhtAP",
        "link": "https://openreview.net/forum?id=z2QdVmhtAP",
        "pdf_link": "https://openreview.net/pdf?id=z2QdVmhtAP",
        "keywords": "fMRI, Computational Neuroscience, Neuroimaging, Diffusion, CLIP, alignment, neuroAI",
        "abstract": "Reconstructing visual images from fMRI data presents a challenging task, particularly when dealing with limited data and compute availability. This work introduces a novel approach to fMRI-based visual image reconstruction using a subject-agnostic common representation space. We show that subjects' brain signals naturally align in this common space during training, without the need for explicit alignment. This is leveraged to demonstrate that aligning subject-specific adapters to a reference subject is significantly more efficient than traditional end-to-end training methods. Our approach excels in low-data scenarios, where training the adapter with limited data achieves faster and better performance. We also introduce a novel method to select the most representative subset of images for a new subject, allowing for fine-tuning with 40% less data while maintaining performance. These advancements make fMRI data collection more efficient and practical, reducing the burden on subjects and improving the generalization of fMRI reconstruction models."
    },
    {
        "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding",
        "link_suffix": "/forum?id=CS2JWaziYr",
        "link": "https://openreview.net/forum?id=CS2JWaziYr",
        "pdf_link": "https://openreview.net/pdf?id=CS2JWaziYr",
        "keywords": "LLM Inference, Speculative Decoding, Performance Analysis",
        "abstract": "Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks."
    },
    {
        "title": "NODE-SAT: Temporal Graph Learning with Neural ODE-Guided Self-Attention",
        "link_suffix": "/forum?id=Lb414Rdzs8",
        "link": "https://openreview.net/forum?id=Lb414Rdzs8",
        "pdf_link": "https://openreview.net/pdf?id=Lb414Rdzs8",
        "keywords": "Temporal Graph, Neural ODE, Link prediction",
        "abstract": "We propose NODE-SAT, a novel temporal graph learning model that integrates Neural Ordinary Differential Equations (NODEs) with self-attention mechanisms. NODE-SAT's design requires only historical 1-hop neighbors as input and comprises three key components: a temporal link processing module utilizing NODE-guided self-attention layers to capture temporal link information, a node representation module summarizing neighbor information, and a prediction layer. Extensive experiments across thirteen temporal link prediction datasets demonstrate that NODE-SAT achieves state-of-the-art performance on most datasets with significantly faster convergence. The model demonstrates high accuracy, rapid convergence, robustness across varying dataset complexities, and strong generalization capabilities in both transductive and inductive settings in temporal link prediction. These findings highlight NODE-SAT's effectiveness in capturing node correlations and temporal link dynamics."
    },
    {
        "title": "Collaborative Compressors in Distributed Mean Estimation with Limited Communication Budge",
        "link_suffix": "/forum?id=J7hIz9GXKq",
        "link": "https://openreview.net/forum?id=J7hIz9GXKq",
        "pdf_link": "https://openreview.net/pdf?id=J7hIz9GXKq",
        "keywords": "Distributed Mean Estimation, Compression",
        "abstract": "Distributed high dimensional mean estimation is a common aggregation routine used often in distributed optimization methods (e.g. federated learning). Most of these applications call for a communication-constrained setting where vectors, whose mean is to be estimated, have to be compressed before sharing. One could independently encode and decode these to achieve compression, but that overlooks the fact that these vectors are often similar with each other.  To exploit these similarities, recently Suresh et al., 2022, Jhunjhunwala et al., 2021, Jiang et al, 2023, proposed multiple {\\em correlation-aware compression schemes.} However, in most cases, the correlations have to be known for these schemes to work. Moreover, a theoretical analysis of graceful degradation of these correlation-aware compression schemes with increasing {\\em dissimilarity} is limited to only the $\\ell_2$-error in  the literature. \n    In this paper, we propose three different collaborative compression schemes  that agnostically exploit the similarities among vectors in a distributed setting.  Our schemes are all simple to implement and computationally efficient, while resulting in big savings in communication. We do a rigorous theoretical analysis of our proposed schemes to show how the $\\ell_2$, $\\ell_\\infty$ and cosine estimation error varies with the degree of similarity among vectors. In the process, we come up with appropriate dissimilarity-measures for these applications as well."
    },
    {
        "title": "Towards Efficient and No Forgetting Domain Continual Pretraining by Mitigating the Stability Gap",
        "link_suffix": "/forum?id=4y6Q98hJzr",
        "link": "https://openreview.net/forum?id=4y6Q98hJzr",
        "pdf_link": "https://openreview.net/pdf?id=4y6Q98hJzr",
        "keywords": "Continual pretraining",
        "abstract": "Adapting Large Language Models (LLMs) to specialized domains like medicine and law through domain continual pre-training has become the cutting-edge method. However, contrary to our expectations of immediate gains, we’ve uncovered a surprising phenomenon: a temporary performance drop at the start of the process, followed by a performance recovery phrase. This drop is not only unexpected but remarkably consistent across different model sizes and domains, such as medical and law. To gain a deeper understanding of this issue, we introduce the concept of stability gap—borrowed from visual models dealing with new class classifications—to explain this initial drop in LLM performance. Based on this concept, we hypothesize that the initial performance drop arises from instability in the model’s general abilities, which we further validated through our experiments.\nWe further reveal that this initial instability is intricately tied to training settings that involve distribution shifts.\nTo address this initial instability and enhance LLM performance within a fixed compute budget, we propose one training strategy that reduces the instability by increasing the epoch number, along with two data sampling strategies focused on data quality and corpus distribution.\nWe conduct various experiments on Llama-family models to validate the effectiveness of our strategies in both medical and legal continual pre-training and instruction tuning. For example, our strategies improve the average medical task performance of the OpenLlama-3B model from 36.2% to 40.7% with only 40% of the original training budget and enhance the average general task performance without causing forgetting. \nFurthermore, we apply our strategies to continually pre-train and instruction-tune the Llama-3-8B model. The resulting model, Llama-3-Physician, achieves the best medical performance among current open-source models and performs comparably to or even better than GPT-4 on several medical benchmarks."
    },
    {
        "title": "Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo",
        "link_suffix": "/forum?id=exgLs4snap",
        "link": "https://openreview.net/forum?id=exgLs4snap",
        "pdf_link": "https://openreview.net/pdf?id=exgLs4snap",
        "keywords": "SGMCMC, Bayesian Neural Network, Parameter Expansion",
        "abstract": "Bayesian Neural Networks (BNNs) provide a promising framework for modeling predictive uncertainty and enhancing out-of-distribution robustness (OOD) by estimating the posterior distribution of network parameters. Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) is one of the most powerful methods for scalable posterior sampling in BNNs, achieving efficiency by combining stochastic gradient descent with second-order Langevin dynamics. However, SGMCMC often suffers from limited sample diversity in practice, which affects uncertainty estimation and model performance. We propose a simple yet effective approach to enhance sample diversity in SGMCMC without the need for tempering or running multiple chains. Our approach reparameterizes the neural network by decomposing each of its weight matrices into a product of matrices, resulting in a sampling trajectory that better explores the target parameter space. This approach produces a more diverse set of samples, allowing faster mixing within the same computational budget. Notably, our sampler achieves these improvements without increasing the inference cost compared to the standard SGMCMC. Extensive experiments on image classification tasks, including OOD robustness, diversity, loss surface analyses, and a comparative study with Hamiltonian Monte Carlo, demonstrate the superiority of the proposed approach."
    },
    {
        "title": "Prompt Optimization with Human Feedback",
        "link_suffix": "/forum?id=UW0zetsx8X",
        "link": "https://openreview.net/forum?id=UW0zetsx8X",
        "pdf_link": "https://openreview.net/pdf?id=UW0zetsx8X",
        "keywords": "Prompt Optimization, Large Language Model, Preference Feedback",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performances in various tasks. However, the performances of LLMs heavily depend on the input prompt. This has given rise to a number of recent works on prompt optimization. However, the previous works often require the availability of a numeric score to assess the quality of every prompt. Unfortunately, when a human user interacts with a black-box LLM, it is often infeasible and unreliable to attain such a score. Instead, it is usually significantly easier and more reliable to obtain preference feedback from a human user, i.e., showing the user the responses generated from a pair of prompts and asking the user which one is preferred. Therefore, in this paper, we study the problem of prompt optimization with human feedback (POHF), in which we aim to optimize the prompt for a black-box LLM using only human preference feedback. By drawing inspirations from dueling bandits, we design a theoretically principled strategy to select a pair of prompts to query for preference feedback in every iteration, and hence introduce our algorithm named automated POHF (APOHF). We apply our APOHF algorithm to a variety of tasks, including optimizing user instructions, prompt optimization for text-to-image generative models, and response optimization with human feedback (i.e., further refining the response using a variant of our APOHF). The results demonstrate that our APOHF can efficiently find a good prompt using a small number of preference feedback instances."
    },
    {
        "title": "Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse",
        "link_suffix": "/forum?id=Iyrtb9EJBp",
        "link": "https://openreview.net/forum?id=Iyrtb9EJBp",
        "pdf_link": "https://openreview.net/pdf?id=Iyrtb9EJBp",
        "keywords": "Large Language Models, Trustworthiness, Hallucinations, Retrieval Augmented Generation",
        "abstract": "LLMs are an integral component of retrieval-augmented generation (RAG) systems. While many studies focus on evaluating the overall quality of end-to-end RAG systems, there is a gap in understanding the appropriateness of LLMs for the RAG task. To address this, we introduce Trust-Score, a holistic metric that evaluates the trustworthiness of LLMs within the RAG framework. Our results show that various prompting methods, such as in-context learning, fail to effectively adapt LLMs to the RAG task as measured by Trust-Score. Consequently, we propose  Trust-Align, a method to align LLMs for improved Trust-Score performance. The LLaMA-3 family, aligned using our method, significantly outperforms open-source LLMs of similar sizes on ASQA (up 14.0), QAMPARI (up 28.9), and ELI5 (up 13.7). We also demonstrate the effectiveness of Trust-Align across different open-weight models, including the LLaMA series (1b to 8b), Qwen-2.5 series (0.5b to 7b), and Phi3.5 (3.8b). We release our code at \\url{https://anonymous.4open.science/r/trust-align}."
    },
    {
        "title": "Forget but Recall: Incremental Latent Rectification in Continual Learning",
        "link_suffix": "/forum?id=oyIXleoQ7Z",
        "link": "https://openreview.net/forum?id=oyIXleoQ7Z",
        "pdf_link": "https://openreview.net/pdf?id=oyIXleoQ7Z",
        "keywords": "continual learning",
        "abstract": "Intrinsic capability to continuously learn a changing data stream is a desideratum of deep neural networks (DNNs). However, current DNNs suffer from catastrophic forgetting, which hinders remembering past knowledge. To mitigate this issue, existing Continual Learning (CL) approaches either retain exemplars for replay, regularize learning, or allocate dedicated capacity for new tasks. This paper investigates an unexplored CL direction for incremental learning called Incremental Latent Rectification or ILR. In a nutshell, LRB learns to propagate with correction (or rectify) the representation from the current trained DNN backward to the representation space of the old task, where performing predictive decisions is easier. This rectification process only employs a chain of small representation mapping networks, called rectifier units. Empirical experiments on several continual learning benchmarks, including CIFAR10, CIFAR100, and Tiny ImageNet, demonstrate the effectiveness and potential of this novel CL direction compared to existing representative CL methods."
    },
    {
        "title": "Scalable Preference Learning for Large Language Models via Convex Optimization",
        "link_suffix": "/forum?id=EVZnnhtMNX",
        "link": "https://openreview.net/forum?id=EVZnnhtMNX",
        "pdf_link": "https://openreview.net/pdf?id=EVZnnhtMNX",
        "keywords": "large language models, preference learning, convex optimization",
        "abstract": "Fine-tuning large language models (LLMs) for alignment with human preferences have become a key factor in the success of models like ChatGPT and Gemini, which are now integral to mainstream use. \nMany effective techniques are based on Reinforcement Learning from Human Feedback (RLHF), yet are complex, unstable, and expensive to implement. Recently, Direct Preference Optimization (DPO) offers an accessible alternative by simplifying the objective and training a policy model using a frozen, copied reference model to provide a stable training benchmark. In this paper, we develop an even more lightweight DPO based algorithm that operates on a single GPU. The key to achieving this is leveraging the convex optimization reformulation of neural networks, and reducing the dependence on copying the reference model. Our aim is to provide faster convergence to solutions of better optimality, and higher interpretability of the underlying optimization landscape for generative language tasks. We use the Alternating Direction Method of Multipliers (ADMM) to solve this optimization problem in order to increase parallelization efficiency, and implement our methods in JAX to lift the memory constraints across experiments. \nWe experiment on three datasets, including one synthetically generated educational dataset, to demonstrate the efficacy of our novel algorithm in a real world setting. \nOur method is comparable in user preference generation to DPO when tested on 17 human volunteers, despite being trained on one single RTX-4090 GPU using a smaller dataset."
    }
]