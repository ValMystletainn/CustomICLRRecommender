[{"title": "Dissecting Bit-Level Scaling Laws in Quantizing Vision Generative Models", "link_suffix": "/forum?id=7X2BFPl18T", "link": "https://openreview.net/forum?id=7X2BFPl18T", "pdf_link": "https://openreview.net/pdf?id=7X2BFPl18T", "keywords": "quantization, visual generative models, scaling laws", "abstract": "Vision generative models have recently made significant advancements along two primary paradigms: diffusion-style and language-style, both of which have demonstrated excellent scaling laws. Quantization is crucial for efficiently deploying these models, as it reduces memory and computation costs. In this work, we systematically investigate the impact of quantization on these two paradigms. Surprisingly, despite achieving comparable performance in full precision, language-style models consistently outperform diffusion-style models across various quantization settings. This observation suggests that language-style models have superior bit-level scaling laws, offering a better tradeoff between model quality and total bits. To dissect this phenomenon, we conduct extensive experiments and find that the primary reason is the discrete representation space of language-style models, which is more tolerant of information loss during quantization. Furthermore, our analysis indicates that improving the bit-level scaling law of quantized vision generative models is challenging, with model distillation identified as a highly effective approach. Specifically, we propose TopKLD to optimize the transfer of distilled knowledge by balancing \"implicit knowledge\" and \"explicit knowledge\" during the distillation process. This approach elevates the bit-level scaling laws by one level across both integer and floating-point quantization settings.", "title_embedding_index": 7750, "title_abs_embedding_index": 7775}, {"title": "Large Language Models Often Say One Thing and Do Another", "link_suffix": "/forum?id=RTHbao4Mib", "link": "https://openreview.net/forum?id=RTHbao4Mib", "pdf_link": "https://openreview.net/pdf?id=RTHbao4Mib", "keywords": "corpus creation, benchmarking, language resources, NLP datasets, evaluation, metrics, consistency", "abstract": "As large language models (LLMs) increasingly become central to various applications and interact with diverse user populations, ensuring their reliable and consistent performance is becoming more important. This paper explores a critical issue in assessing the reliability of LLMs: the consistency between their words and deeds. To quantitatively explore this consistency, we developed a novel evaluation benchmark, the Words and Deeds Consistency Test (WDCT), which establishes a strict correspondence between word-based and deed-based questions across different domains, including opinion versus action, non-ethical value versus action, ethical value versus action, and theory versus application. The evaluation results reveal a widespread inconsistency between words and deeds across LLMs and domains. Subsequently, we conducted experiments with either word alignment or deed alignment to observe their impact on the other aspect. The experiment results indicate that alignment only on words or deeds poorly and unpredictably influences the other aspect. This supports our hypothesis that the underlying knowledge guiding LLMs' choices of words or deeds is not contained within a unified space.", "title_embedding_index": 7751, "title_abs_embedding_index": 7776}, {"title": "PCDM: PERCEPTUAL CONSISTENCY IN DIFFUSION MODELS FOR NO-REFERENCE IMAGE QUALITY ASSESSMENT", "link_suffix": "/forum?id=sHbE7PplDG", "link": "https://openreview.net/forum?id=sHbE7PplDG", "pdf_link": "https://openreview.net/pdf?id=sHbE7PplDG", "keywords": "No Reference Image Quality Assessment, Perceptual Image Quality, Latent Diffusion Models, Training-free", "abstract": "Despite recent advancements in latent diffusion models for generating high-dimensional data and performing various downstream tasks, there has been little exploration into perceptual consistency within these models for No-Reference Image Quality Assessment (NR-IQA). In this paper, we hypothesize that latent diffusion models implicitly exhibit perceptually consistent local regions within data manifold. We leverage this insight to guide the on-manifold sampling using perceptual features and input measurements. Specifically, we propose Perceptual Manifold Guidance (PMG), an algorithm that utilizes pretrained latent diffusion models and perceptual quality metrics to obtain perceptually consistent multi-scale and multi-timestep feature maps from the denoising U-Net. We empirically demonstrate that these hyperfeatures exhibit high correlation with human perception in IQA tasks. Our method can be applied to any existing pretrained latent diffusion model and is straightforward to integrate. To the best of our knowledge, this paper is the first work to explore Perceptual Consistency in Diffusion Models (PCDM) and apply it to NR-IQA in a zero-shot setting. Extensive experiments on IQA datasets show that our method, PCDM, achieves state-of-the-art performance, underscoring the superior zero-shot generalization capabilities of diffusion models for NR-IQA tasks.", "title_embedding_index": 7752, "title_abs_embedding_index": 7777}, {"title": "Tree-of-Table: Unleashing the Power of LLMs for Enhanced Large-Scale Table Understanding", "link_suffix": "/forum?id=Yv8FrCY87H", "link": "https://openreview.net/forum?id=Yv8FrCY87H", "pdf_link": "https://openreview.net/pdf?id=Yv8FrCY87H", "keywords": "LLMs, Table Reasoning", "abstract": "The ubiquity and value of tables as semi-structured data across various domains necessitate advanced methods for understanding their complexity and vast amounts of information. Despite the impressive capabilities of large language models (LLMs) in advancing the natural language understanding frontier, their application to large-scale tabular data presents significant challenges, specifically regarding table size and complex intricate relationships. Existing works have shown promise with small-scale tables but often flounder when tasked with the complex reasoning required by larger, interconnected tables found in real-world scenarios. To address this gap, we introduce \"Tree-of-Table\", a novel approach designed to enhance LLMs' reasoning capabilities over large and complex tables. Our method employs Table Condensation and Decomposition to distill and reorganize relevant data into a manageable format, followed by the construction of a hierarchical Table-Tree that facilitates tree-structured reasoning. Through a meticulous Table-Tree Execution process, we systematically unravel the tree-structured reasoning chain to derive the solutions. Experiments across diverse datasets, including WikiTQ, TableFact, FeTaQA, and BIRD, demonstrate that Tree-of-Table sets a new benchmark with superior performance, showcasing remarkable efficiency and generalization capabilities in large-scale table reasoning.", "title_embedding_index": 7753, "title_abs_embedding_index": 7778}, {"title": "Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces", "link_suffix": "/forum?id=IUmDBY4NOQ", "link": "https://openreview.net/forum?id=IUmDBY4NOQ", "pdf_link": "https://openreview.net/pdf?id=IUmDBY4NOQ", "keywords": "hyperbolic learning, metric learning", "abstract": "Learning in hyperbolic spaces has gained increasing attention due to the superior capability of modeling hierarchical structures. Existing hyperbolic learning methods use a fixed distance measure that assumes a uniform hierarchical structure across all data points. However, this assumption does not always hold in real-world scenarios, considering the diversity of the hierarchical structures of data. This work proposes to learn geometry aware distance measures that dynamically adjust to accommodate diverse hierarchical structures in hyperbolic spaces. We derive geometry aware distance measures by generating projections and curvatures for each pair of samples, which maps each pair to a suitable hyperbolic space. We introduce a revised low-rank decomposition scheme and a hard-pair mining mechanism to reduce the computational cost incurred by the pairwise generation without compromising accuracy. Moreover, we derive an upper bound of the low-rank approximation error via Talagrand concentration inequality to guarantee the effectiveness of our low-rank decomposition scheme. Theoretical analysis and experiments on standard image classification and few-shot learning tasks affirm the effectiveness of our method in refining hyperbolic learning through our geometry aware distance measures.", "title_embedding_index": 7754, "title_abs_embedding_index": 7779}, {"title": "Mousterian: exploring the equivalence of generative and real data augmentation in classification", "link_suffix": "/forum?id=MyAqAYCjP5", "link": "https://openreview.net/forum?id=MyAqAYCjP5", "pdf_link": "https://openreview.net/pdf?id=MyAqAYCjP5", "keywords": "analysis by synthesis, image classification, data generation", "abstract": "In this paper, we address a key question in machine learning:How effectively can generative data augmentation enhance image classification?We begin by examining the differences and similarities between real and synthetic data generated by advanced text-to-image models. Through comprehensive experiments, we provide systematic insights into leveraging synthetic data for improved classification performance. Our findings show that: 1). Generative data augmentation by models trained solely on the internal (available training) set can effectively improve classification performance, validating the long-held hypothesis that synthesis enhances analysis by enriching modeling capability.\n2). For generative data augmentation by models trained on both internal and external data (e.g. large-scale image-text pairs) separately, the size of equivalent synthetic dataset augmentation can be determined empirically. In addition to being aligned with a common intuition that real data augmentation is always preferred, our empirical formulation also provides a guideline for quantitatively estimating how much larger the size of generative dataset augmentation is, over the real data augmentation, to achieve comparable improvements. Our CIFAR-10 and ImageNet results also demonstrate its impact w.r.t. the size of the baseline training set and the quality of generative models.", "title_embedding_index": 7755, "title_abs_embedding_index": 7780}, {"title": "Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning", "link_suffix": "/forum?id=5YbuOTUFQ4", "link": "https://openreview.net/forum?id=5YbuOTUFQ4", "pdf_link": "https://openreview.net/pdf?id=5YbuOTUFQ4", "keywords": "meta-reinforcement learning, representation learning, bisimulation", "abstract": "Meta-reinforcement learning requires utilizing prior task distribution information obtained during exploration to rapidly adapt to unknown tasks. The efficiency of an agent's exploration hinges on accurately identifying the current task. Recent Bayes-Adaptive Deep RL approaches often rely on reconstructing the environment's reward signal, which is challenging in sparse reward settings, leading to suboptimal exploitation. Inspired by bisimulation metrics, which robustly extracts behavioral similarity in continuous MDPs, we propose SimBelief\u2014a novel meta-RL framework via measuring similarity of task belief in Bayes-Adaptive MDP (BAMDP). SimBelief effectively extracts common features of similar task distributions, enabling efficient task identification and exploration in sparse reward environments. We introduce latent task belief metric to learn the common structure of similar tasks and incorporate it into the real task belief. By learning the latent dynamics across task distributions, we connect shared latent task belief features with specific task features, facilitating rapid task identification and adaptation. Our method outperforms state-of-the-art bselines on sparse reward MuJoCo and panda-gym tasks.", "title_embedding_index": 7756, "title_abs_embedding_index": 7781}, {"title": "Unearthing Skill-level Insights for Understanding Trade-offs of Foundation Models", "link_suffix": "/forum?id=kNHVViEPWK", "link": "https://openreview.net/forum?id=kNHVViEPWK", "pdf_link": "https://openreview.net/pdf?id=kNHVViEPWK", "keywords": "evaluation, skills, tradeoffs, frontier models, llm, large multimodal model, rationale, error analysis, interpretability", "abstract": "With models getting stronger, evaluations have grown more complex, testing multiple skills in one benchmark and even in the same instance at once. However, skill-wise performance is obscured when inspecting aggregate accuracy, under-utilizing the rich signal modern benchmarks contain. We propose an automatic approach to recover the underlying skills relevant for any evaluation instance, by way of inspecting model-generated {\\em rationales}. After validating the relevance of rationale-parsed skills and inferring skills for $46$k instances over $12$ benchmarks, we observe many skills to be common across benchmarks, resulting in the curation of hundreds of \\emph{skill-slices} (i.e. sets of instances testing a common skill). Inspecting accuracy over these slices yields novel insights on model trade-offs: e.g., compared to GPT-4o and Claude 3.5 Sonnet, on average, Gemini 1.5 Pro is $18%$ more accurate in \\emph{computing molar mass}, but $19%$ less accurate in \\emph{applying constitutional law}, despite the overall accuracies of the three models differing by a mere $0.4%$. Furthermore, we demonstrate the practical utility of our approach by showing that insights derived from skill slice analysis can generalize to held-out instances: when routing each instance to the model strongest on the relevant skills, we see a $3%$ accuracy improvement over our $12$ dataset corpus. Our skill-slices and framework open a new avenue in model evaluation, leveraging skill-specific analyses to unlock a more granular and actionable understanding of model capabilities.", "title_embedding_index": 7757, "title_abs_embedding_index": 7782}, {"title": "Learning Imbalanced Data with Beneficial Label Noise", "link_suffix": "/forum?id=S1IbZssS5a", "link": "https://openreview.net/forum?id=S1IbZssS5a", "pdf_link": "https://openreview.net/pdf?id=S1IbZssS5a", "keywords": "Imbalanced learning, beneficial label noise, classificaition accuracy", "abstract": "Data imbalance and label noise are common factors hindering the classifier's performance. Data-level approaches to addressing imbalanced learning usuallyinvolve resampling by adding or removing samples, which often results in information loss or generative errors. Building upon theoretical studies of the impact of imbalance ratio on decision boundaries across various evaluation metrics in binary classification, it is uncovered that introducing appropriate label noise can alter the biased decision boundaries and thus enhance the performance of classifiers in imbalanced learning. In this paper, we introduce the Label-Noise-based Re-balancing (LNR) approach to solve both binary and multi-class imbalanced classifications by employing a novel design of asymmetric label noise model. In contrast to other data-level methods, our approach is easy to implement and alleviates the issues of informative loss and generative errors. We validated the superiority of this method on synthetic and real-world datasets. More importantly, our LNR approach can integrate seamlessly with any classifiers and other algorithm-level methods for imbalanced learning. Overall, our work opens up a new avenue for addressing imbalanced learning, highlighting the potential advantages of balancing data through beneficial label noise.", "title_embedding_index": 7758, "title_abs_embedding_index": 7783}, {"title": "Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining", "link_suffix": "/forum?id=Sta35eEmfR", "link": "https://openreview.net/forum?id=Sta35eEmfR", "pdf_link": "https://openreview.net/pdf?id=Sta35eEmfR", "keywords": "mllm pretraining data; image-text data; data quality", "abstract": "Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities. A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets. \nHowever, $\\textit{de facto}$ filter-based data quality enhancement paradigms often discard a substantial portion of high-quality image data due to inadequate semantic alignment between images and texts, leading to inefficiencies in data utilization and scalability. \nIn this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs. AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately selected low-quality samples during training. Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality. Experimental results demonstrate that AITQE surpasses existing methods on various benchmark, effectively leveraging raw data and scaling efficiently with increasing data volumes.\nWe hope our work will inspire future works. The code and the model are available athttps://github.com/iclr2025-No9242/AITQE.", "title_embedding_index": 7759, "title_abs_embedding_index": 7784}, {"title": "Decoupling Layout from Glyph in Online Chinese Handwriting Generation", "link_suffix": "/forum?id=DhHIw9Nbl1", "link": "https://openreview.net/forum?id=DhHIw9Nbl1", "pdf_link": "https://openreview.net/pdf?id=DhHIw9Nbl1", "keywords": "Online handwriting generation; Layout generation; Calligraphy imitation; Conditional diffusion Model", "abstract": "Text plays a crucial role in the transmission of human civilization, and teaching machines to generate online handwritten text in various styles presents an interesting and significant challenge. However, most prior work has concentrated on generating individual Chinese fonts, leaving complete text line generation largely unexplored. In this paper, we identify that text lines can naturally be divided into two components: layout and glyphs. Based on this division, we designed a text line layout generator coupled with a diffusion-based stylized font synthesizer to address this challenge hierarchically. More concretely, the layout generator performs in-context-like learning based on the text content and the provided style references to generate positions for each glyph autoregressively. Meanwhile, the font synthesizer which consists of a character embedding dictionary, a multi-scale calligraphy style encoder and a 1D U-Net based diffusion denoiser will generate each font on its position while imitating the calligraphy style extracted from the given style references. Qualitative and quantitative experiments on the CASIA-OLHWDB demonstrate that our method is capable of generating structurally correct and indistinguishable imitation samples.", "title_embedding_index": 7760, "title_abs_embedding_index": 7785}, {"title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts", "link_suffix": "/forum?id=Nx1XZWcLcW", "link": "https://openreview.net/forum?id=Nx1XZWcLcW", "pdf_link": "https://openreview.net/pdf?id=Nx1XZWcLcW", "keywords": "mutli-modal language modeling, mixture-of-experts, pre-training", "abstract": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantics-based adaptivity. Our empirical results reveal substantial pre-training efficiency gains through this modality-specific parameter allocation. Under a 1-trillion-token training budget, the MoMa 1.4B model, featuring 4 text experts and 4 image experts, achieves impressive FLOPs savings: 3.7x overall, with 2.6x for text and 5.2x for image compared to a compute-equivalent dense baseline, measured by pre-training loss. This outperforms the standard expert-choice MoE with 8 mixed-modal experts, which achieves 3x overall FLOPs savings (3x for text, 2.8x for image). These results demonstrate MoMa's potential to significantly advance the efficiency of mixed-modal, early-fusion language model pre-training, paving the way for more resource-efficient and capable multimodal AI systems.", "title_embedding_index": 7761, "title_abs_embedding_index": 7786}, {"title": "LAST: Latent Structure guided Gaussian Splatting from Monocular Human Videos", "link_suffix": "/forum?id=nhwfzqXlfd", "link": "https://openreview.net/forum?id=nhwfzqXlfd", "pdf_link": "https://openreview.net/pdf?id=nhwfzqXlfd", "keywords": "Digital Human, 3D reconstruction, Causal Inference", "abstract": "Multiocular human reconstruction aims to create a high-quality 3D human representation from sparse video data. Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive results in multiocular human reconstruction tasks, exhibiting remarkable speed and accuracy. However, it encounters challenges in scenarios involving intricate clothing and dynamic postures. This problem may stem from pixel-level supervision during the 3DGS optimization process, which probably lead to spurious associations between unrelated visual features (e.g., misinterpreting clothing wrinkles as dependent on body occlusions rather than recognizing that both are influenced by complex postures). To address this issue, we propose the LAST framework for realistic 3D human reconstruction, which integrates a pre-trained Image-to-Point (I2P) model to enhance the 3D Gaussian Splatting optimization pipeline. The core of the LAST is to disentangle meaningful latent factors and realistic dependencies from the input video frames, which allows for dynamic adjustments to the density and attributes of Gaussian points during the optimization process. Experimental results demonstrate that our method significantly improves accuracy and realism in 3D human reconstruction compared to existing techniques, particularly in challenging scenarios involving complex posture and intricate clothing details.", "title_embedding_index": 7762, "title_abs_embedding_index": 7787}, {"title": "Residual Connections Harm Generative Representation Learning", "link_suffix": "/forum?id=cxKLRM3KhC", "link": "https://openreview.net/forum?id=cxKLRM3KhC", "pdf_link": "https://openreview.net/pdf?id=cxKLRM3KhC", "keywords": "Decayed Residual Connections, Representation Learning", "abstract": "We show that introducing a weighting factor to reduce the influence of identity shortcuts in residual networks significantly enhances semantic feature learning in generative representation learning frameworks, such as masked autoencoders (MAEs) and diffusion models.  Our modification improves linear probing accuracy for both, notably increasing ImageNet accuracy from 67.8% to 72.7% for MAEs with a VIT-B/16 backbone, while also boosting generation quality for diffusion models.  This significant gap suggests that, while residual connection structure serves an essential role in facilitating gradient propagation, it may have a harmful side effect of reducing capacity for abstract learning by virtue of injecting an echo of shallower representations into deeper layers.  We ameliorate this downside via a fixed formula for monotonically decreasing the contribution of identity connections as layer depth increases.  Our design promotes the gradual development of feature abstractions, without impacting network trainability.  Analyzing the representations learned by our modified residual networks, we find correlation between low effective feature rank and downstream task performance.", "title_embedding_index": 7763, "title_abs_embedding_index": 7788}, {"title": "Well-NeRF: Ensuring Well-Posed Neural Radiance Fields via View Frustum and Shadow Zone Based Regularization", "link_suffix": "/forum?id=DhYsFwLqkL", "link": "https://openreview.net/forum?id=DhYsFwLqkL", "pdf_link": "https://openreview.net/pdf?id=DhYsFwLqkL", "keywords": "Few-shot NeRF, Ill-posed problem, Artifacts removal, View frustum, Inside opaque, Boundary condition, Near-far threshold, Integrated model", "abstract": "Neural Radiation Field (NeRF) often produces many artifacts with sparse inputs. These artifacts are primarily caused by learning in regions where position inference is not feasible. We assume that the main cause of this problem is the incorrect setting of boundary conditions in the learning space. To address this issue, we propose a new regularization method based on two key assumptions: (1) the position of density and color cannot be inferred in regions where the view frustum does not intersect, and (2) information inside opaque surfaces cannot be observed and inferred, and thus cannot contribute to the rendering of the image. Our method aims to transform the NeRF model into a well-posed problem by regularizing learning in regions where position inference is not possible, allowing the network to converge meaningfully. Our approach does not require scene-specific optimization and focuses on regions where position inference is not possible, thereby avoiding degradation of model performance in main regions. Experimental results demonstrate the effectiveness of our method in addressing the sparse input problem, showing outstanding performance on the Blender synthetic datasets. Our method is designed to integrate seamlessly with existing techniques, providing an effective solution for sparse input scenarios and offering a foundational approach that serves as the first clue in addressing sparse input problems.", "title_embedding_index": 7764, "title_abs_embedding_index": 7789}, {"title": "Handling Delay in Reinforcement Learning Caused by Parallel Computations of Neurons", "link_suffix": "/forum?id=YOc5t8PHf2", "link": "https://openreview.net/forum?id=YOc5t8PHf2", "pdf_link": "https://openreview.net/pdf?id=YOc5t8PHf2", "keywords": "reinforcement learning, delay, parallel computations", "abstract": "Real-time reinforcement learning (RL) introduces several challenges. First, policies are constrained to a fixed number of actions per second due to hardware limitations. Second, the environment may change while the network is still computing an action, leading to observational delay. The first issue can partly be addressed with parallel computation of neurons, leading to higher throughput and potentially better policies. However, the second issue remains: if each neuron operates in parallel with an execution time of $\\tau$, an $N$-layer feed-forward network experiences observation delay of $\\tau N$.\nReducing the number of layers can decrease this delay, but at the cost of the network's expressivity. In this work, we explore the trade-off between minimizing delay and network's expressivity. We present a theoretically motivated solution that leverages temporal skip connections combined with history-augmented observations.  We evaluate several architectures and show that those incorporating temporal skip connections achieve strong performance across various neuron execution times, reinforcement learning algorithms, and environments, including four Mujoco tasks and all MinAtar games. Moreover, we demonstrate parallel neuron computation can accelerate inference by 6-350% on standard hardware.  Our investigation into temporal skip connections and parallel computations paves the way for more efficient RL agents in real-time setting.", "title_embedding_index": 7765, "title_abs_embedding_index": 7790}, {"title": "HyperINF: Unleashing the HyperPower of the Schulz's Method for Data Influence Estimation", "link_suffix": "/forum?id=OLtD2vDF5X", "link": "https://openreview.net/forum?id=OLtD2vDF5X", "pdf_link": "https://openreview.net/pdf?id=OLtD2vDF5X", "keywords": "Data Attribution, Influnece Function", "abstract": "Influence function provides a principled method to assess the contribution of individual training samples to a specific target, yet their high computation costs limits its applications on large-scale models or datasets. \n Existing methods proposed for influence function approximation have significantly reduce the computation overheads. However, they mostly suffer from a unsatisfied accuracy due to the lack of strong convergence guarantees. The family of hyperpower methods are well-known for their rigorous convergence guarantees on matrix inverse approximation, while the matrix multiplication operation can involve intractable memory and computation costs on large-scale models.\n We propose HyperINF, an efficient and accurate influence function approximation method which leverages the hyperpower method, specifically the Schulz's iterative algorithm.\n To deal with the computation-intensive matrix multiplication, we incorporate the generalized fisher information (GFIM) as a low-rank approximation of the hessian matrix, which reduces the memory and computation overheads to a constant costs independent of ranks on LoRA-tuned models. \n We first demonstrate the superior accuracy and stability of HyperINF compared to other baselines through a synthetic convergence simulation of matrix inversion. We further validate the efficacy of HyperINFthrough extensive real-world data attribution tasks, including mislabeled data detection and data selection for LLM and VLM fine-tuning. \n On LoRA-tuned models, HyperINF achieves superior downstream performance with minimal memory and computational overhead, while other baselines suffer from significant degradation. The codebase is available at \\url{https://anonymous.4open.science/r/HyperINF-B702}.", "title_embedding_index": 7766, "title_abs_embedding_index": 7791}, {"title": "SketchFill: Sketch-Guided Code Generation for Imputing Derived Missing Values", "link_suffix": "/forum?id=rsMajBqYrB", "link": "https://openreview.net/forum?id=rsMajBqYrB", "pdf_link": "https://openreview.net/pdf?id=rsMajBqYrB", "keywords": "Large Language Model;Missing Value Impuation;Data Preprocessing;Code Generation", "abstract": "Missing value is a critical issue in data science, significantly impacting the reliability of analyses and predictions. Missing value imputation (MVI) is a longstanding problem because it highly relies on domain knowledge. Large language models (LLMs) have emerged as a promising tool for data cleaning, including MVI for tabular data, offering advanced capabilities for understanding and generating content. However, despite their promise, existing LLM techniques such as in-context learning and Chain-of-Thought (CoT) often fall short in guiding LLMs to perform complex reasoning for MVI, particularly when imputing derived missing values, which require mathematical formulas and data relationships across rows and columns. This gap underscores the need for further advancements in LLM methodologies to enhance their reasoning capabilities for more reliable imputation outcomes. To fill this gap, we propose SketchFill, a novel sketch-based method to guide LLMs in generating accurate formulas to impute missing numerical values. SketchFill first utilizes a general user-provided Meta-Sketch to generate a Domain-Sketch tailored to the context of the input dirty table. Subsequently, it fills this Domain-Sketch with formulas and outputs Python code, effectively bridging the gap between high-level abstractions and executable solutions. Additionally, SketchFill incorporates a Reflector component to verify the generated code. This Reflector assesses the accuracy and appropriateness of the outputs and iteratively refines the Domain-Sketch, ensuring that the imputation aligns closely with the underlying data patterns and relationships. Our experimental results demonstrate that SketchFill significantly outperforms state-of-the-art methods, achieving 56.2% higher accuracy than CoT-based methods and 78.8% higher accuracy than MetaGPT. This sets a new standard for automated data cleaning and advances the field of MVI for numerical values.", "title_embedding_index": 7767, "title_abs_embedding_index": 7792}, {"title": "Which Tasks Should Be Compressed Together? A Causal Discovery Approach for Efficient Multi-Task Representation Compression", "link_suffix": "/forum?id=x33vSZUg0A", "link": "https://openreview.net/forum?id=x33vSZUg0A", "pdf_link": "https://openreview.net/pdf?id=x33vSZUg0A", "keywords": "Video Coding for Machine, Image Compression, Multi-task Learning, Causal Discovery", "abstract": "Traditional image compression methods often overlook the intricate interdependencies in multi-task learning, resulting in inefficiency and redundancy. In this paper, we propose a novel compression framework that leverages causal graph models to uncover conditional relationships between mutually beneficial task clusters. By constructing directed acyclic graphs (DAGs) based on conditional entropy, we capture the causal links among tasks, enabling progressive, context-aware compression. Parent representations act as hyperpriors for their dependents, reducing redundancy, enhancing scalability, and boosting compression efficiency. Extensive experiments across key computer vision tasks, including segmentation, depth zbuffer, and autoencoding demonstrate superior bitrate reduction and task performance. Our findings underscore the importance of disentangling task representations and modelling causal relationships for efficient multi-task compression, offering a new perspective on compact representation learning for advanced intelligent systems. Code will be available at:https://github.com.", "title_embedding_index": 7768, "title_abs_embedding_index": 7793}, {"title": "Compositional Hardness of Code in Large Language Models - A Probabilistic Perspective", "link_suffix": "/forum?id=RJBf8k6lxO", "link": "https://openreview.net/forum?id=RJBf8k6lxO", "pdf_link": "https://openreview.net/pdf?id=RJBf8k6lxO", "keywords": "Large language models", "abstract": "A common practice in large language model (LLM) usage for complex analytical tasks such as code generation, is to sample a solution for the entire task within the model's context window. Previous works have shown that subtask decomposition within the model's context (chain of thought), is beneficial for solving such tasks. In this work, we point a limitation of LLMs' ability to perform several sub-tasks within the same context window - an in-context hardness of composition, pointing to an advantage for distributing a decomposed problem in a multi-agent system of LLMs. The hardness of composition is quantified by a generation complexity metric, i.e., the number of LLM generations required to sample at least one correct solution. We find a gap between the generation complexity of solving a compositional problem within the same context relative to distributing it among multiple agents, that increases exponentially with the solution's length. We prove our results theoretically and demonstrate them empirically.", "title_embedding_index": 7769, "title_abs_embedding_index": 7794}, {"title": "DiffDeID: a Multi-conditional Diffusion-based Method for High Fidelity Face De-indentification with Diversity", "link_suffix": "/forum?id=Bz9wjvToCS", "link": "https://openreview.net/forum?id=Bz9wjvToCS", "pdf_link": "https://openreview.net/pdf?id=Bz9wjvToCS", "keywords": "Face De-identification, Data privacy, Diffusion Model", "abstract": "Face de-identification is a critical task that aims to obscure true identities while preserving other facial attributes. Current methodologies typically involve disentangling identity features within a latent space and leveraging adversarial training to balance privacy with utility, often at the cost of a trade-off between two. To surmount these limitations, we introduce DiffDeID, a novel approach grounded in diffusion models. This method incrementally safeguards identity and sustains utility, all while ensuring enhanced interpretability.\nOur method employs a Latent Diffusion-based ID Sample to generate authentic identity embeddings that are obfuscated from the original identity, thereby providing users with diverse options. Additionally, a multi-condition diffusion model is utilized for facial images, ensuring the retention of image utility. We further introduce a novel training and inference paradigm, utilizing the unified architecture tailored for video facial de-identification tasks.\nThe robustness of our method is attributed to its powerful 3D prior and meticulous generation design, enabling natural identity protection, generation of high-quality details, and robustness across various attributes. Through extensive experimentation, we demonstrate that DiffDeID surpasses previous methodologies.", "title_embedding_index": 7770, "title_abs_embedding_index": 7795}, {"title": "Distinguishing Ignorance from Error in LLM Hallucinations", "link_suffix": "/forum?id=lJGEyRSXDJ", "link": "https://openreview.net/forum?id=lJGEyRSXDJ", "pdf_link": "https://openreview.net/pdf?id=lJGEyRSXDJ", "keywords": "Natural Language Processing, Large Language Models, Hallucinations, Detection, Knowledge", "abstract": "Large language models (LLMs) are susceptible to hallucinations---outputs that are ungrounded, factually incorrect, or inconsistent with prior generations. We focus on close-book Question Answering (CBQA), where previous work has not fully addressed the distinction between two possible kinds of hallucinations, namely, whether the model (1) does not hold the correct answer in its parameters or (2) answers incorrectly despite having the required knowledge. We argue that distinguishing these cases is crucial for detecting and mitigating hallucinations. Specifically, case (2) may be mitigated by intervening in the model\u2019s internal computation, as the knowledge resides within the model's parameters. In contrast, in case (1) there is no parametric knowledge to leverage for mitigation, \nso it should be addressed by resorting to an external knowledge source or abstaining. To help distinguish between the two cases, we introduce Wrong Answer despite having Correct Knowledge (WACK), an approach for constructing model-specific datasets for the second hallucination type. Our probing experiments indicate that the two kinds of hallucinations are represented differently in the model's inner states. Next, we show that datasets constructed using WACK exhibit variations across models, demonstrating that even when models share knowledge of certain facts, they still vary in the specific examples that lead to hallucinations. Finally, we show that training a probe on our WACK datasets leads to better hallucination detection of case (2) hallucinations than using the common generic one-size-fits-all datasets.", "title_embedding_index": 7771, "title_abs_embedding_index": 7796}, {"title": "Exploring Data Distillation for efficient generation of Tabular Data", "link_suffix": "/forum?id=kzePnQWUvC", "link": "https://openreview.net/forum?id=kzePnQWUvC", "pdf_link": "https://openreview.net/pdf?id=kzePnQWUvC", "keywords": "data distillation, tabular data", "abstract": "Tabular data generation methods have emerged to address growing concerns about the use of sensitive tabular data for training machine learning models. Many methods focus on creating high-quality tabular data that can be used in place of the original dataset while retaining generalization performance on downstream tasks and protecting sensitive data in an era where privacy is paramount. Despite their avid success, many of the methods face implacable challenges and obstacles to wide-scale applications primarily due to the significant computational costs associated with data synthesis. In this paper, we propose a flexible data distillation pipeline as an alternative to conventional synthetic data generators that obtain competitive privacy metrics while achieving significantly higher downstream performance at a fraction of the compute costs. In particular, our method has accelerated data synthesis by $5\\times$ on average when compared to synthetic generators while also achieving superior performance.", "title_embedding_index": 7772, "title_abs_embedding_index": 7797}, {"title": "GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing", "link_suffix": "/forum?id=hgv11VQnIk", "link": "https://openreview.net/forum?id=hgv11VQnIk", "pdf_link": "https://openreview.net/pdf?id=hgv11VQnIk", "keywords": "Large Language Models, Safety Evaluation, Jailbreaking, Red-teaming", "abstract": "Jailbreak attacks reveal critical vulnerabilities in Large Language Models (LLMs) by causing them to generate harmful or unethical content. Evaluating these threats is particularly challenging due to the evolving nature of LLMs and the sophistication required in effectively probing their vulnerabilities. Current benchmarks and evaluation methods struggle to fully address these challenges, leaving gaps in the assessment of LLM vulnerabilities. In this paper, we review existing jailbreak evaluation practices and identify three assumed desiderata for an effective jailbreak evaluation protocol. To address these challenges, we introduce GuardVal, a new evaluation protocol that dynamically generates and refines jailbreak prompts based on the defender LLM's state, providing a more accurate assessment of defender LLMs' capacity to handle safety-critical situations. Moreover, we propose a new optimization method that prevents stagnation during prompt refinement, ensuring the generation of increasingly effective jailbreak prompts that expose deeper weaknesses in the defender LLMs. We apply this protocol to a diverse set of models, from Mistral-7b to GPT-4, across 10 safety domains. Our findings highlight distinct behavioral patterns among the models, offering a comprehensive view of their robustness. Furthermore, our evaluation process deepens the understanding of LLM behavior, leading to insights that can inform future research and drive the development of more secure models.", "title_embedding_index": 7773, "title_abs_embedding_index": 7798}, {"title": "Unveiling Molecular Secrets: An LLM-Augmented Linear Model for Explainable and Calibratable Molecular Property Prediction", "link_suffix": "/forum?id=ejEIlXRShF", "link": "https://openreview.net/forum?id=ejEIlXRShF", "pdf_link": "https://openreview.net/pdf?id=ejEIlXRShF", "keywords": "Large language models (LLMs), explainable AI, explainability, molecular property prediction", "abstract": "Explainable molecular property prediction is essential for various scientific fields, such as drug discovery and material science. \nDespite delivering intrinsic explainability, linear models struggle with capturing complex, non-linear patterns. Large language models (LLMs), on the other hand, yield accurate predictions through powerful inference capabilities yet fail to provide chemically meaningful explanations for their predictions. This work proposes a novel framework, called MoleX, which leverages LLM knowledge to build a simple yet powerful linear model for accurate molecular property prediction with faithful explanations. The core of MoleX is to model complicated molecular structure-property relationships using a simple linear model, augmented by LLM knowledge and a crafted calibration strategy. Specifically, to extract the maximum amount of task-relevant knowledge from LLM embeddings, we employ information bottleneck-inspired fine-tuning and sparsity-inducing dimensionality reduction. These informative embeddings are then used to fit a linear model for explainable inference. Moreover, we introduce residual calibration to address prediction errors stemming from linear models' insufficient expressiveness of complex LLM embeddings, thus recovering the LLM's predictive power and boosting overall accuracy. Theoretically, we provide a mathematical foundation to justify MoleX\u2019s explainability. Extensive experiments demonstrate that MoleX outperforms existing methods in molecular property prediction, establishing a new milestone in predictive performance, explainability, and efficiency. In particular, MoleX enables CPU inference and accelerates large-scale dataset processing, achieving comparable performance 300$\\times$ faster with 100,000 fewer parameters than LLMs. Additionally, the calibration improves model performance by up to 12.7% without compromising explainability. The source code is available at \\url{https://github.com/MoleX2024/MoleX}.", "title_embedding_index": 7774, "title_abs_embedding_index": 7799}]