[{"title": "3D StreetUnveiler with Semantic-aware 2DGS - a simple baseline", "link_suffix": "/forum?id=G6aJyS0ZV0", "link": "https://openreview.net/forum?id=G6aJyS0ZV0", "pdf_link": "https://openreview.net/pdf?id=G6aJyS0ZV0", "keywords": "3D inpainting, Empty Street Reconstruction", "abstract": "Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporarily static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scene cases involve long trajectories that differ from previous 3D inpainting tasks. The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns a 3D representation of the empty street from crowded observations. Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed. We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. This decomposition helps us to minimize the regions that need to be inpainted. To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street. The mesh representation of the empty street can be extracted for further applications.", "title_embedding_index": 13600, "title_abs_embedding_index": 13625}, {"title": "Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection", "link_suffix": "/forum?id=l0t2rumAvR", "link": "https://openreview.net/forum?id=l0t2rumAvR", "pdf_link": "https://openreview.net/pdf?id=l0t2rumAvR", "keywords": "Vision-language model;  zero-shot enhancement; Structural Representation", "abstract": "Zero-shot medical detection can further improve detection performance without relying on annotated medical images even upon the fine-tuned model, showing great clinical value. Recent studies leverage grounded vision-language models (GLIP) to achieve this by using detailed disease descriptions as prompts for the target disease name during the inference phase.However, these methods typically treat prompts as equivalent context to the target name, making it difficult to assign specific disease knowledge based on visual information, leading to a coarse alignment between images and target descriptions. In this paper, we propose StructuralGLIP, which introduces an auxiliary branch to encode prompts into a latent knowledge bank layer-by-layer, enabling more context-aware and fine-grained alignment. Specifically, in each layer, we select highly similar features from both the image representation and the knowledge bank, forming structural representations that capture nuanced relationships between image patches and target descriptions. These features are then fused across modalities to further enhance detection performance.\nExtensive experiments demonstrate that StructuralGLIP achieves a +4.1% AP improvement over prior state-of-the-art methods across seven zero-shot medical detection benchmarks, and consistently improves fine-tuned models by +3.2% AP on endoscopy image datasets.", "title_embedding_index": 13601, "title_abs_embedding_index": 13626}, {"title": "Adam-mini: Use Fewer Learning Rates To Gain More", "link_suffix": "/forum?id=iBExhaU3Lc", "link": "https://openreview.net/forum?id=iBExhaU3Lc", "pdf_link": "https://openreview.net/pdf?id=iBExhaU3Lc", "keywords": "large language model, memory, optimizer", "abstract": "We propose Adam-mini, an optimizer that achieves on-par or better performance than AdamW with $45$% to $50$% less memory footprint. Adam-mini reduces memory by cutting down the learning rate resources in Adam (i.e., $1/\\sqrt{v}$). By delving into the Hessian structure of neural nets, we find Adam\u2019s $v$ might not function at its full potential as effectively as we expected. We find that $\\geq 90$% of these learning rates in $v$ could be harmlessly removed if we (1) carefully partition the parameters into blocks following our proposed principle on Hessian structure; (2) assign a single but good learning rate to each parameter block. We then provide one simple way to find good learning rates and propose Adam-mini. Empirically, we verify that Adam-mini performs on par or better than AdamW on various language models sized from 39M to 13B for pre-training, supervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini also alleviates communication overheads among GPUs, thereby increasing throughput. For instance, Adam-mini achieves $49.6$% higher throughput than AdamW when pre-training Llama 2-7B on $2\\times$ A800-80GB GPUs, which saves 33% wall-clock time for pre-training.", "title_embedding_index": 13602, "title_abs_embedding_index": 13627}, {"title": "An Engorgio Prompt Makes Large Language Model Babble on", "link_suffix": "/forum?id=m4eXBo0VNc", "link": "https://openreview.net/forum?id=m4eXBo0VNc", "pdf_link": "https://openreview.net/pdf?id=m4eXBo0VNc", "keywords": "Large language model, attack, inference cost", "abstract": "Auto-regressive large language models (LLMs) have yielded impressive performance in many real-world tasks. \nHowever, the new paradigm of these LLMs also exposes novel threats. \nIn this paper, we explore their vulnerability to inference cost attacks, where a malicious user crafts Engorgio prompts to intentionally increase the computation cost and latency of the inference process. We design Engorgio, a novel methodology, to efficiently generate adversarial Engorgio prompts to affect the target LLM's service availability. Engorgio has the following two technical contributions. \n(1) We employ a parameterized distribution to track LLMs' prediction trajectory. (2) Targeting the auto-regressive nature of LLMs' inference process, we propose novel loss functions to stably suppress the appearance of the <EOS> token, whose occurrence will interrupt the LLM's generation process. \nWe conduct extensive experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B. \nThe results show that Engorgio prompts can successfully induce LLMs to generate abnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the output length limit)\nin a white-box scenario and our real-world experiment demonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is accessible in \\url{https://anonymous.4open.science/r/Engorgio}.", "title_embedding_index": 13603, "title_abs_embedding_index": 13628}, {"title": "Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents", "link_suffix": "/forum?id=3Gzz7ZQLiz", "link": "https://openreview.net/forum?id=3Gzz7ZQLiz", "pdf_link": "https://openreview.net/pdf?id=3Gzz7ZQLiz", "keywords": "Large Language Models, LLM agent, Web automation", "abstract": "Recent advances in large language models (LLMs) have led to a growing interest in developing LLM-based agents for automating web tasks. However, these agents often struggle with even simple tasks on real-world websites due to their limited capability to understand and process complex web page structures. In this work, we introduce LCoW, a framework for Learning language models to Contextualize complex Web pages into a more comprehensible form, thereby enhancing decision making by LLM agents. LCoW decouples web page understanding from decision making by training a separate contextualization module to transform complex web pages into comprehensible format, which are then utilized by the decision-making agent. We demonstrate that our contextualization module effectively integrates with LLM agents of various scales to significantly enhance their decision-making capabilities in web automation tasks. Notably, LCoW improves the success rates of closed-source LLMs (e.g., Gemini-1.5-flash, GPT-4o, Claude-3.5-Sonnet) by an average of 20%, and demonstrates a 33.5% average improvement in success rates for open-source LMs (e.g., Llama-3.1-8B, Llama-3.1-70B) on the WorkArena benchmark. Moreover, the Gemini-1.5-flash agent with LCoW achieves state-of-the-art results on the WebShop benchmark, outperforming human experts.", "title_embedding_index": 13604, "title_abs_embedding_index": 13629}, {"title": "Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection", "link_suffix": "/forum?id=f4gF6AIHRy", "link": "https://openreview.net/forum?id=f4gF6AIHRy", "pdf_link": "https://openreview.net/pdf?id=f4gF6AIHRy", "keywords": "file selection, large language model, pre-training, submodular optimization", "abstract": "Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e. dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance. To prevent collapse and enhance diversity, we propose a Diversified Submodular File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this as a submodular optimization problem, employing a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, with a $(1-e^{-1})$ approximation to the optimal solution. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the popular Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency.", "title_embedding_index": 13605, "title_abs_embedding_index": 13630}, {"title": "Energy-Based Diffusion Language Models for Text Generation", "link_suffix": "/forum?id=sL2F9YCMXf", "link": "https://openreview.net/forum?id=sL2F9YCMXf", "pdf_link": "https://openreview.net/pdf?id=sL2F9YCMXf", "keywords": "Language Models, Discrete Diffusion Models, Energy-based Models", "abstract": "Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model (EDLM), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an EBM in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive models' perplexity. We further show that, without any generation performance drop, our framework offers a 1.3x sampling speedup over existing diffusion models.", "title_embedding_index": 13606, "title_abs_embedding_index": 13631}, {"title": "From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks", "link_suffix": "/forum?id=KEUPk0wXXe", "link": "https://openreview.net/forum?id=KEUPk0wXXe", "pdf_link": "https://openreview.net/pdf?id=KEUPk0wXXe", "keywords": "Graph Neural Networks, Multi-Granular Interpretability, Graph Classification", "abstract": "Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying reasoning behind model predictions, attributing their decisions to specific subgraphs that are informative. However, existing subgraph-based interpretable methods suffer from an overemphasis on local structure, potentially overlooking long-range dependencies within the entire graphs. Although recent efforts that rely on graph coarsening have proven beneficial for global interpretability, they inevitably reduce the graphs to a fixed granularity. Such inflexible way can only capture graph connectivity at a specific level, whereas real-world graph tasks often exhibit relationships at varying granularities (e.g., relevant interactions in proteins span from functional groups, to amino acids, and up to protein domains). In this paper, we introduce a novel Tree-like Interpretable Framework (TIF) for graph classification, where plain GNNs are transformed into hierarchical trees, with each level featuring coarsened graphs of different granularity as tree nodes. Specifically, TIF iteratively adopts a graph coarsening module to compress original graphs  (i.e., root nodes of trees) into increasingly coarser ones (i.e., child nodes of trees), while preserving diversity among tree nodes within different branches through a dedicated graph perturbation module. Finally, we propose an adaptive routing module to identify the most informative root-to-leaf paths, providing not only the final prediction but also the multi-granular interpretability for decision-making process. Extensive experiments on the graph classification benchmarks with both synthetic and real-world datasets demonstrate the superiority of TIF in interpretability, while also delivering a competitive prediction performance akin to the state-of-the-art counterparts. Our code will be made publicly available.", "title_embedding_index": 13607, "title_abs_embedding_index": 13632}, {"title": "Customize Your Visual Autoregressive Recipe with Set Autoregressive Modeling", "link_suffix": "/forum?id=b9dBNNeDd3", "link": "https://openreview.net/forum?id=b9dBNNeDd3", "pdf_link": "https://openreview.net/pdf?id=b9dBNNeDd3", "keywords": "generatigve modeling, image generation, autoregressive models", "abstract": "We introduce a new paradigm for AutoRegressive (AR) image generation, termed Set AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the next-set setting, i.e., splitting the sequence into arbitrary sets containing multiple tokens, rather than outputting each token in a fixed raster order. To accommodate SAR, we develop a straightforward architecture termed Fully Masked Transformer. We reveal that existing AR variants correspond to specific design choices of sequence order and output intervals within the SAR framework, with AR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a seamless transition from AR to MAR, where intermediate states allow for training a causal model that benefits from both few-step inference and KV cache acceleration, thus leveraging the advantages of both AR and MAR. On the ImageNet benchmark, we carefully explore the properties of SAR by analyzing the impact of sequence order and output intervals on performance, as well as the generalization ability regarding inference order and steps. We further validate the potential of SAR by training a 900M text-to-image model capable of synthesizing photo-realistic images with any resolution. We hope our work may inspire more exploration and application of AR-based modeling across diverse modalities. Code will be available.", "title_embedding_index": 13608, "title_abs_embedding_index": 13633}, {"title": "Multi-objective Differentiable Neural Architecture Search", "link_suffix": "/forum?id=9mjZ800m7Y", "link": "https://openreview.net/forum?id=9mjZ800m7Y", "pdf_link": "https://openreview.net/pdf?id=9mjZ800m7Y", "keywords": "hardware efficiency, neural architecture search, network compression", "abstract": "Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives that require training a neural network. Typically, in MOO for neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a computationally expensive search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences to trade-off performance and hardware metrics, yielding representative and diverse architectures across multiple devices in just a single search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot transferability to new devices. Extensive experiments involving up to 19 hardware devices and 3 different objectives demonstrate the effectiveness and scalability of our method. Finally, we show that, without any additional costs, our method outperforms existing MOO NAS methods across a broad range of qualitatively different search spaces and datasets, including MobileNetV3 on ImageNet-1k, an encoder-decoder transformer space for machine translation and a decoder-only space for language modelling.", "title_embedding_index": 13609, "title_abs_embedding_index": 13634}, {"title": "Decoding by Contrasting Knowledge: Enhancing LLMs' Confidence on Edited Facts", "link_suffix": "/forum?id=YpWV7XRmFB", "link": "https://openreview.net/forum?id=YpWV7XRmFB", "pdf_link": "https://openreview.net/pdf?id=YpWV7XRmFB", "keywords": "Large Language Models, Knowledge Enhancement, Knowledge Editing", "abstract": "The knowledge within large language models (LLMs) may become outdated quickly.\nWhile in-context editing (ICE) is currently the most effective method for knowledge editing (KE), it is constrained by the black-box modeling of LLMs and thus lacks interpretability.\nOur work aims to elucidate the superior performance of ICE in KE by analyzing the impacts of in-context new knowledge on token-wise distributions.\nWe observe that despite a significant boost in logits of the new knowledge, the performance of ICE is still hindered by stubborn knowledge. \nStubborn knowledge refers to facts that have gained excessive confidence during pretraining, making them hard to edit effectively.\nTo address this issue and further enhance the performance of ICE, we propose a novel approach termedDecoding byContrastingKnowledge (DeCK).\nDeCK derives the distribution of the next token by contrasting the logits obtained from the newly edited knowledge guided by ICE with those from the unedited parametric knowledge. \nOur experiments consistently demonstrate that DeCK enhances the confidence of LLMs in edited facts. \nFor instance, it improves the performance of LLaMA3-8B-instruct on MQuAKE by up to 219%, demonstrating its capability to strengthen ICE in the editing of stubborn knowledge.\nDeCK can be easily integrated into any ICE method as a decoding component to enhance editing capabilities.\nOur work paves the way to develop both effective and accountable KE methods for LLMs.", "title_embedding_index": 13610, "title_abs_embedding_index": 13635}, {"title": "Object Fusion via Diffusion Time-step for Customized Image Editing with Single Example", "link_suffix": "/forum?id=hIdnWVxA9Z", "link": "https://openreview.net/forum?id=hIdnWVxA9Z", "pdf_link": "https://openreview.net/pdf?id=hIdnWVxA9Z", "keywords": "object fusion, diffusion time-step, customized image editing", "abstract": "We tackle the task of customized image editing using a text-conditioned Diffusion Model (DM). The goal is to fuse the subject in a reference image (e.g., sunglasses) with a source one (e.g., a boy), while retaining the fidelity of them both (e.g., the boy wearing the sunglasses). An intuitive approach, called LoRA fusion, first separately trains a DM LoRA for each image to encode its details. Then the two LoRAs are linearly combined by a weight to generate a fused image. Unfortunately, even through careful grid search or learning the weight, this approach still trades off the fidelity of one image against the other. We point out that the evil lies in the overlooked role of diffusion time-step in the generation process, i.e., a smaller time-step controls the generation of a more fine-grained attribute. For example, a large LoRA weight for the source may help preserve its fine-grained details (e.g., face attributes) at a small time-step, but could overpower the reference subject LoRA and lose the fidelity of its overall shape at a larger time-step. To address this deficiency, we propose TimeFusion, which learns a time-step-specific LoRA fusion weight that resolves the trade-off, i.e., generating the source and reference subject in high fidelity given their respective prompt. Then we can customize image editing using this weight and a target prompt. Codes are in Appendix.", "title_embedding_index": 13611, "title_abs_embedding_index": 13636}, {"title": "CAT Pruning: Cluster-Aware Token Pruning For Text-to-Image Diffusion Models", "link_suffix": "/forum?id=DDxLsxiZR8", "link": "https://openreview.net/forum?id=DDxLsxiZR8", "pdf_link": "https://openreview.net/pdf?id=DDxLsxiZR8", "keywords": "Generative Models, Efficient Machine Learning", "abstract": "Diffusion models have transformed generative tasks, particularly in text-to-image synthesis, but their iterative denoising process is computationally intensive. We present a novel acceleration strategy that combines token-level pruning with cache mechanisms to address this challenge. By utilizing Noise Relative Magnitude, we identify significant token changes across iterations. Additionally, we incorporate spatial clustering and distributional balance to enhance token selection. Our experiments demonstrate 50%-60% reduction in computational cost while maintaining model performance, offering a substantial improvement in the efficiency of diffusion models.", "title_embedding_index": 13612, "title_abs_embedding_index": 13637}, {"title": "Generative Representational Instruction Tuning", "link_suffix": "/forum?id=BC4lIvfSzv", "link": "https://openreview.net/forum?id=BC4lIvfSzv", "pdf_link": "https://openreview.net/pdf?id=BC4lIvfSzv", "keywords": "large language models, instruction tuning, text embedding", "abstract": "All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM-7B is among the top models on the Massive Text Embedding Benchmark (MTEB) and outperforms various models up to its size on a range of generative tasks. By scaling up further, GritLM-8x7B achieves even stronger generative performance while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by > 60% for long documents, by no longer requiring separate retrieval and generation models. Models, code, etc. will be made freely available.", "title_embedding_index": 13613, "title_abs_embedding_index": 13638}, {"title": "GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and  A Comprehensive Multimodal Dataset Towards General Medical AI", "link_suffix": "/forum?id=ARIQfWf4ll", "link": "https://openreview.net/forum?id=ARIQfWf4ll", "pdf_link": "https://openreview.net/pdf?id=ARIQfWf4ll", "keywords": "General Medical AI;vision-language model;Medical Mulitimodal Dataset;", "abstract": "Despite significant advancements in general artificial intelligence, such as GPT-4, their effectiveness in the medical domain (general medical AI, GMAI) remains constrained due to the absence of specialized medical knowledge. \nTo address this challenge, we present GMAI-VL-5.5M, a comprehensive multimodal medical dataset created by converting hundreds of specialized medical datasets into meticulously constructed image-text pairs. This dataset features comprehensive task coverage, diverse modalities, and high-quality image-text data.\nBuilding upon this multimodal dataset, we propose GMAI-VL, a general medical vision-language model with a progressively three-stage training strategy. This approach significantly enhances the model's ability by integrating visual and textual information, thereby improving its ability to process multimodal data and support accurate diagnosis and clinical decision-making.\nExperimental evaluations demonstrate that GMAI-VL achieves state-of-the-art results across a wide range of multimodal medical tasks, such as visual question answering and medical image diagnosis. Our contributions include the development of the GMAI-VL-5.5M dataset, the introduction of the GMAI-VL model, and the establishment of new benchmarks in multiple medical domains.", "title_embedding_index": 13614, "title_abs_embedding_index": 13639}, {"title": "CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning", "link_suffix": "/forum?id=Fg0eo2AkST", "link": "https://openreview.net/forum?id=Fg0eo2AkST", "pdf_link": "https://openreview.net/pdf?id=Fg0eo2AkST", "keywords": "Multimodal Reasoning, Vision-Language Models, Datasets Synthesis, Graphical Math Annotation", "abstract": "Vision-Language Models (VLMs) have shown broad effectiveness due to extensive training that aligns visual inputs with corresponding language responses. However, this conclusive alignment training causes models to overlook essential visual reasoning, leading to failures in handling detailed visual tasks and producing unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to tackle problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) with results (e.g., boxes, image) actively without relying external tools, while also allowing users to trace error causes. In this paper, we study the comprehensive methodology that includes: (1) a flexible design of manipulations based on extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn, multi-image, and (4) a model training process for versatile capabilities. With the design, we also manually annotate6Khigh-quality samples for challenging graphical mathematical problems. Our trained model, CogCoM, equipped with this mechanism and 17B parameters, achieves SOTA performance across9benchmarks in4categories, demonstrating its effectiveness while maintaining interpretability. Our code, model, and data will be publicly available.", "title_embedding_index": 13615, "title_abs_embedding_index": 13640}, {"title": "Diminishing Exploration: A Minimalist Approach to Piecewise Stationary Multi-Armed Bandits", "link_suffix": "/forum?id=fbrHgEBx2f", "link": "https://openreview.net/forum?id=fbrHgEBx2f", "pdf_link": "https://openreview.net/pdf?id=fbrHgEBx2f", "keywords": "Piecewise-stationary bandit, multi-armed bandit, regret analysis, change detection", "abstract": "The piecewise-stationary bandit problem is an important variant of the multi-armed bandit problem that further considers abrupt changes in the reward distributions. The main theme of the problem is the trade-off between exploration for detecting environment changes and exploitation of traditional bandit algorithms. While this problem has been extensively investigated, existing works either assume knowledge about the number of change points $M$ or require extremely high computational complexity. In this work, we revisit the piecewise-stationary bandit problem from a minimalist perspective. We propose a novel and generic exploration mechanism, called diminishing exploration, which eliminates the need for knowledge about $M$ and can be used in conjunction with an existing change detection-based algorithm to achieve near-optimal regret scaling. Simulation results show that despite oblivious of $M$, equipping existing algorithms with the proposed diminishing exploration generally achieves better empirical regret than the traditional uniform exploration.", "title_embedding_index": 13616, "title_abs_embedding_index": 13641}, {"title": "Unsupervised-to-Online Reinforcement Learning", "link_suffix": "/forum?id=YGhV8wQv3C", "link": "https://openreview.net/forum?id=YGhV8wQv3C", "pdf_link": "https://openreview.net/pdf?id=YGhV8wQv3C", "keywords": "Reinforcement Learning, Offline-to-Online Reinforcement Learning, Offline Unsupervised Reinforcement Learning", "abstract": "Offline-to-online reinforcement learning (RL), a framework that trains a policy with offline RL and then further fine-tunes it with online RL,\nhas been considered a promising recipe for data-driven decision-making. While sensible, this framework has drawbacks: it requires domain-specific offline RL pre-training for each task, and is often brittle in practice. In this work, we propose unsupervised-to-online RL (U2O RL),\nwhich replaces domain-specific supervised offline RL with unsupervised offline RL,\nas a better alternative to offline-to-online RL.\nU2O RL not only enables reusing a single pre-trained model for multiple downstream tasks,\nbut also learns better representations, which often result in even better performance and stability\nthan supervised offline-to-online RL.\nTo instantiate U2O RL in practice, we propose a general recipe for U2O RL\nto bridge task-agnostic unsupervised offline skill-based policy pre-training and supervised online fine-tuning.\nThroughout our experiments in nine state-based and pixel-based environments,\nwe empirically demonstrate that U2O RL achieves strong performance\nthat matches or even outperforms previous offline-to-online RL approaches,\nwhile being able to reuse a single pre-trained model for a number of different downstream tasks.", "title_embedding_index": 13617, "title_abs_embedding_index": 13642}, {"title": "Many-Shot In-Context Learning in Multimodal Foundation Models", "link_suffix": "/forum?id=Jw63fvX3QB", "link": "https://openreview.net/forum?id=Jw63fvX3QB", "pdf_link": "https://openreview.net/pdf?id=Jw63fvX3QB", "keywords": "in-context learning, multimodal, many-shot, foundation models", "abstract": "Large language models are well-known to be effective at few-shot in-context learning (ICL). Recent advancements in multimodal foundation models have enabled unprecedentedly long context windows, presenting an opportunity to explore their capability to perform ICL with many more demonstrating examples. In this work, we evaluate the performance of multimodal foundation models scaling from few-shot to many-shot ICL. We benchmark GPT-4o and Gemini 1.5 Pro across 14 datasets spanning multiple domains (natural imagery, medical imagery, remote sensing, and molecular imagery) and tasks (image classification, visual question answering, and object localization). We observe that many-shot ICL, including up to almost 2,000 multimodal demonstrating examples, leads to substantial improvements compared to few-shot (<100 examples) ICL across all of the datasets. Further, Gemini 1.5 Pro performance continues to improve log-linearly up to the maximum number of tested examples on many datasets. \nWe also find open-weights multimodal foundation models like Llama 3.2-Vision and InternLM-XComposer2.5 do not benefit from the demonstrating examples, highlighting an important gap between open and closed multimodal foundation models.\nGiven the high inference costs associated with the long prompts required for many-shot ICL, we also explore the impact of batching multiple queries in a single API call. We show that batching up to 50 queries can lead to performance improvements under zero-shot and many\u2013shot ICL, with substantial gains in the zero-shot setting on multiple datasets, while drastically reducing per-query cost and latency. Finally, we measure ICL data efficiency of the models, or the rate at which the models learn from more demonstrating examples. We find that while GPT-4o and Gemini 1.5 Pro achieve similar zero-shot performance across the datasets, Gemini 1.5 Pro exhibits higher ICL data efficiency than GPT-4o on most datasets. Our results suggest that many-shot ICL could enable users to efficiently adapt multimodal foundation models to new applications and domains.", "title_embedding_index": 13618, "title_abs_embedding_index": 13643}, {"title": "Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders", "link_suffix": "/forum?id=NB8qn8iIW9", "link": "https://openreview.net/forum?id=NB8qn8iIW9", "pdf_link": "https://openreview.net/pdf?id=NB8qn8iIW9", "keywords": "Interpretability, Sparse autoencoder, Mutual learning", "abstract": "Sparse Autoencoders (SAEs) have shown promise in improving the interpretability of neural network activations, but can learn features that are not features of the input, limiting their effectiveness. We propose Mutual Feature Regularization (MFR), a regularization technique for improving feature learning by encouraging SAEs trained in parallel to learn similar features. We motivate MFR by showing that features learned by multiple SAEs are more likely to correlate with features of the input. By training on synthetic data with known features of the input, we show that MFR can help SAEs learn those features, as we can directly compare the features learned by the SAE with the input features for the synthetic data. We then scale MFR to SAEs that are trained to denoise electroencephalography (EEG) data and SAEs that are trained to reconstruct GPT-2 Small activations. We show that MFR can improve the reconstruction loss of SAEs by up to 21.21% on GPT-2 Small, and 6.67% on EEG data. Our results suggest that the similarity between features learned by different SAEs can be leveraged to improve SAE training, thereby enhancing performance and the usefulness of SAEs for model interpretability.", "title_embedding_index": 13619, "title_abs_embedding_index": 13644}, {"title": "Out-of-Distribution Detection using Neural Activation Prior", "link_suffix": "/forum?id=YMgMGPjUPg", "link": "https://openreview.net/forum?id=YMgMGPjUPg", "pdf_link": "https://openreview.net/pdf?id=YMgMGPjUPg", "keywords": "out-of-distribution detection, prior, intra-channel activation pattern", "abstract": "Out-of-distribution detection (OOD) is a crucial technique for deploying machine learning models in the real world to handle the unseen scenarios. Compared to standard classification tasks, OOD detection presents significant challenges due to the unpredictable nature and inherent difficulty in collecting OOD data. Consequently, a natural solution is to develop priors that are as diverse as possible, effectively characterizing the features of OOD data. In this paper, we first propose a simple yet effective Neural Activation Prior (NAP) for OOD detection.Our prior is based on a key observation that,  for a channel before the pooling layer of a fully trained neural network, the probability of a few  neurons being activated with a large response by an in-distribution (ID) sample is significantly higher than that by an OOD sample. An intuitive explanation is that for a model fully trained on ID dataset, each channel would play a role in detecting a certain pattern in  the ID dataset, and a few neurons can be activated with a large response when the pattern is detected in an input sample. Then, an effective scoring function based on this prior is proposed to highlight the role of these strongly activated neurons in OOD detection. Our approach is plug-and-play and does not lead to any performance degradation on ID data classification and requires no extra training or statistics from training or external datasets. To the best of our knowledge, our method is the first to exploit intra-channel activation pattern information, contributing to its orthogonality to existing approaches and allowing it to be effectively combined with them in various applications. Furthermore, we conduct an elegant oracle experiment to validate the rationale behind our proposed scoring function. Extensive experimental results demonstrate the effectiveness of our method. Moreover, our approach can significantly boost the performance when integrated with most existing methods, showcasing the unique attributes of the proposed prior.", "title_embedding_index": 13620, "title_abs_embedding_index": 13645}, {"title": "AutoCode4Math: Learning Autonomous Code Integration for Math LLMs", "link_suffix": "/forum?id=QhjosARfay", "link": "https://openreview.net/forum?id=QhjosARfay", "pdf_link": "https://openreview.net/pdf?id=QhjosARfay", "keywords": "Large Language Models", "abstract": "Recent research on tool integration for math Large Language Models (LLMs)  aims to combine complementary strengths of chain-of-thought (CoT) reasoning and code execution. However, we discover a critical limitation: current tool-integrated math LLMs rely on externally dictated instructions to decide whether to use CoT or code, lacking the autonomy to choose the most appropriate method independently. This prompts us to study \\emph{Autonomous Code integration } (AutoCode) for math LLMs, which enables models to \\emph{independently} develop their own methodology-selection strategy in the absence of reliable supervision. To address this challenge, we propose an innovative Expectation-Maximization (EM) formulation that refines the model's decision-making through the exploration of its capabilities. This framework alternates between (a) computing a reference strategy that improves the model's belief over its capabilities through self-exploration, and (b) updating the model based on the refined belief. We further enhance this framework with an efficient implementation, incorporating a novel data synthesis strategy and off-policy reinforcement learning. Extensive experiments demonstrate that our approach, using only a public query set, significantly boosts the performance of existing math LLMs, raising accuracy by nearly 20% to 65.28%  on the challenging MATH benchmark on the MATH benchmark, while reducing code executions by up to 65% .", "title_embedding_index": 13621, "title_abs_embedding_index": 13646}, {"title": "TabDiff: a Multi-Modal Diffusion Model for Tabular Data Generation", "link_suffix": "/forum?id=swvURjrt8z", "link": "https://openreview.net/forum?id=swvURjrt8z", "pdf_link": "https://openreview.net/pdf?id=swvURjrt8z", "keywords": "Tabular Representative Learning, Generative Models, Diffusion Models", "abstract": "Synthesizing high-quality tabular data is an important topic in many data science tasks, ranging from dataset augmentation to privacy protection. However, developing expressive generative models for tabular data is challenging due to its inherent heterogeneous data types, complex inter-correlations, and intricate column-wise distributions. In this paper, we introduce TabDiff, a joint diffusion framework that models all multi-modal distributions of tabular data in one model. Our key innovation is the development of a joint continuous-time diffusion process for numerical and categorical data, where we propose feature-wise learnable diffusion processes to counter the high disparity of different feature distributions. TabDiff is parameterized by a transformer handling different input types, and the entire framework can be efficiently optimized in an end-to-end fashion. We further introduce a multi-modal stochastic sampler to automatically correct the accumulated decoding error during sampling, and propose classifier-free guidance for conditional missing column value imputation. Comprehensive experiments on seven datasets demonstrate that TabDiff achieves superior average performance over existing competitive baselines across seven out of eight metrics, with up to $22.5%$ improvement over the state-of-the-art model on pair-wise column correlation estimations.", "title_embedding_index": 13622, "title_abs_embedding_index": 13647}, {"title": "DeMo: Decoupled Momentum Optimization", "link_suffix": "/forum?id=b7HOhqXiZs", "link": "https://openreview.net/forum?id=b7HOhqXiZs", "pdf_link": "https://openreview.net/pdf?id=b7HOhqXiZs", "keywords": "deep learning, large language models, optimization, training, generative models, pre-training, foundational models, distributed training", "abstract": "Training large scale neural networks typically involves sharing the gradients between all accelerators, which necessitates specialized high-speed interconnects. Taking cues from signal processing, we show that it is not necessary to share or synchronize the full optimizer states and model parameters during training. By decoupling the momentum and allowing divergence in the optimizer states across accelerators, it is possible to even improve convergence compared to previous state of the art optimizers.\nFrom this, we introduce a Decoupled Momentum optimization algorithm (DeMo) that reduces the communication requirements by several orders of magnitude, potentially enabling future training of large neural networks on slow internet bandwidths with heterogeneous networking hardware. Furthermore, our method is agnostic to the network topology and neural network architecture, and supports scalable clock-synchronous distributed training with negligible compute and memory overhead.\nEmpirically, we show that models trained with DeMo match or surpass the performance of equal models trained with AdamW, entirely bypassing the need for high-speed interconnects for pre-training large scale foundation models.", "title_embedding_index": 13623, "title_abs_embedding_index": 13648}, {"title": "Consistent Symmetry Representation over Latent Factors of Variation", "link_suffix": "/forum?id=h3Buc7hXSR", "link": "https://openreview.net/forum?id=h3Buc7hXSR", "pdf_link": "https://openreview.net/pdf?id=h3Buc7hXSR", "keywords": "Combinatorial Generalizaton, Disentanglement Learning, Variational Auto-Encoder, Symmetry", "abstract": "Recent symmetry-based methods on variational autoencoders have advanced disentanglement learning and combinatorial generalization, yet the appropriate symmetry representation for both tasks is under-clarified. We identify that existing methods struggle with maintaining the $\\textit{consistent symmetries}$ when representing identical changes of latent factors of variation, and they cause issues in achieving equivari-\nance. We theoretically prove the limitations of three frequently used group settings: matrix multiplication with General Lie Groups, defining group action with set of vectors and vector addition, and cyclic groups modeled through surjective functions. To overcome these issues, we introduce a novel method of $\\textit{conformal mapping}$ of latent vectors into a complex number space, ensuring consistent symmetries\nand cyclic semantics. Through empirical validation with ground truth of factors variation for transparent analysis, this study fills two significant gaps in the literature: 1) the inductive bias to enhance disentanglement learning and combinatorial generalization simultaneously, and 2) well-represented symmetries ensure significantly high disentanglement performance without a trade-off in reconstruction error, compared to current unsupervised methods. Additionally, we introduce less guidance-dependent validation results, extending our findings to more practical use. Our research highlights the significant impact of verifying consistent symmetry and suggests required future research for advancing combinatorial generalization and disentanglement learning.", "title_embedding_index": 13624, "title_abs_embedding_index": 13649}]