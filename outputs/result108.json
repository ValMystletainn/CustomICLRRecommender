[
    {
        "title": "The Contraction Property of Pooling Layer",
        "link_suffix": "/forum?id=KdR88Qskmw",
        "link": "https://openreview.net/forum?id=KdR88Qskmw",
        "pdf_link": "https://openreview.net/pdf?id=KdR88Qskmw",
        "keywords": "Average Pooling, Max Pooling, CNN, Deep Learning",
        "abstract": "Although the theory of deep neural networks has been studied for years, the mechanism of pooling layers is still elusive. In this paper, we report the angle contraction behavior of pooling strategies (the average pooling and max pooling) at initialization. Compared to the relu-activated fully connected layer or convolutional layer, the pooling layer stands as the main source of contraction of the angle between hidden features. Moreover, we show that the cosine similarity between average pooling features in convolutional neural network is more data-dependent than fully connected network, while the max pooling is not sensitive to the data distribution in both architectures. Our results may complement the understanding of the representation learning."
    },
    {
        "title": "A Causal Study on The Learnability of Formal Languages",
        "link_suffix": "/forum?id=Oz9FTPINRe",
        "link": "https://openreview.net/forum?id=Oz9FTPINRe",
        "pdf_link": "https://openreview.net/pdf?id=Oz9FTPINRe",
        "keywords": "Causality, language models, formal languages",
        "abstract": "Understanding the limitations of neural language models is crucial for knowing what such models are capable of and how they can be used safely. A popular approach to analyzing formal limitations takes the form of training models on formal languages, and studying what aspects of the languages affect model performance. Formal languages can, for instance, be designed using manually constructed grammars or randomly sampled by sampling some type of automata. This provides the researcher with unique control over the features of the language of interest. In this paper, we provide an even more fine-grained approach to targeted model evaluation. We develop a method for controlling specific \\emph{string} features, on the corpus level, in the language of a given automaton. This gives us control over properties such as symbol frequencies while keeping everything else intact, enabling a causal study of their importance. To describe our framework formally, we turn to \\emph{semirings} and introduce finite state automata over a novel---counting---semiring. We devise algorithms that enable string sampling under varying degrees of interventions and demonstrate the utility of our method through several examples showing how targeted interventions over transition, symbol, and state frequencies can be performed. We then train Transformer and LSTM language models on languages under varying degrees of interventions. Our fine-grained analysis allows us to show that different mechanisms influence the learning behavior of these two architectures."
    },
    {
        "title": "Visual Representation Learning for World Models by Predicting Fine-Grained Motion",
        "link_suffix": "/forum?id=8BJl6LQgW5",
        "link": "https://openreview.net/forum?id=8BJl6LQgW5",
        "pdf_link": "https://openreview.net/pdf?id=8BJl6LQgW5",
        "keywords": "world models, model-based reinforcement learning, visual representation learning",
        "abstract": "Originating from model-based reinforcement learning (MBRL) methods, algorithms based on world models have been widely applied to boost sample efficiency in visual environments. However, existing world models often struggle with irrelevant background information and omit moving tiny objects that can be essential to tasks. To solve this problem, we introduce the Motion-Aware World Model (MAWM), which incorporates a fine-grained motion predictor and entails action-conditional video prediction. The combination yields compact and robust representations of environments, filters out extraneous backgrounds, and keeps track of the pixel-level motion of objects. Moreover, we demonstrate that a world model with action-conditional video prediction can be interpreted as a variational autoencoder (VAE) for the whole video. Experiments on the Atari 100k benchmark show that the proposed MAWM outperforms current prevailing MBRL methods. We further show its state-of-the-art performance across challenging tasks from the DeepMind Control Suite."
    },
    {
        "title": "Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment",
        "link_suffix": "/forum?id=kN25ggeq1J",
        "link": "https://openreview.net/forum?id=kN25ggeq1J",
        "pdf_link": "https://openreview.net/pdf?id=kN25ggeq1J",
        "keywords": "Code Reasoning, Hypothesis Decomposition, Reflection",
        "abstract": "The reasoning abilities are one of the most enigmatic and captivating aspects of large language models (LLMs). Numerous studies are dedicated to exploring and expanding the boundaries of this reasoning capability. However, tasks that embody both reasoning and recall characteristics are often overlooked. In this paper, we introduce such a novel task, code reasoning, to provide a new perspective for the reasoning abilities of LLMs.\nWe summarize three meta-benchmarks based on established forms of logical reasoning, and instantiate these into eight specific benchmark tasks. Our testing on these benchmarks reveals that LLMs continue to struggle with identifying satisfactory reasoning pathways.\nAdditionally, we present a new pathway exploration pipeline inspired by human intricate problem-solving methods. This \\textbf{R}eflective Hypothesis Decomposition and Amendment (RHDA) pipeline consists of the following iterative steps: (1) Proposing potential hypotheses based on observations and decomposing them; (2) Utilizing tools to validate hypotheses and reflection outcomes; (3) Revising hypothesis in light of observations. Our approach effectively mitigates logical chain collapses arising from forgetting or hallucination issues in multi-step reasoning, resulting in performance gains of up to $3\\times$. Finally, we expanded this pipeline by applying it to simulate complex household tasks in real-world scenarios, specifically in VirtualHome, enhancing the handling of failure cases. We release our code and all of results at \\url{https://anonymous.4open.science/r/code_reasoning}."
    },
    {
        "title": "Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning",
        "link_suffix": "/forum?id=eaTqsptDPL",
        "link": "https://openreview.net/forum?id=eaTqsptDPL",
        "pdf_link": "https://openreview.net/pdf?id=eaTqsptDPL",
        "keywords": "Model merging, Multi-task Learning, Sharpness-aware minimization",
        "abstract": "Large-scale deep learning models with a pretraining-finetuning paradigm have led to a surge of numerous task-specific models finetuned from a common pretrained model. Recently, several research efforts have been made on merging these large models into a single multi-task model, particularly with simple arithmetic on parameters. Such merging methodology faces a central challenge: interference between model parameters finetuned on different tasks. Few recent works have focused on desiging a new finetuning scheme that can lead to small parameter interference, however at the cost of the performance of each task-specific finetuned model and thereby limiting that of a merged model. To improve the performance of a merged model, we note that a finetuning scheme should aim for (1) smaller parameter interference and (2) better performance of each finetuned model on the corresponding task. In this work, we aim to design a new finetuning objective function to work towards these two goals. In the course of this process, we find such objective function to be strikingly similar to sharpness-aware minimization (SAM) objective function, which aims to achieve generalization by finding flat minima. Drawing upon our observation, we propose to finetune pretrained models via SAM or its variants. The experimental and theoretical results showcase the effectiveness and orthogonality of our proposed approach, improving performance upon various merging and finetuning methods."
    },
    {
        "title": "Counterintuitive RL: The Hidden Value of Acting Bad",
        "link_suffix": "/forum?id=14E7S17hFv",
        "link": "https://openreview.net/forum?id=14E7S17hFv",
        "pdf_link": "https://openreview.net/pdf?id=14E7S17hFv",
        "keywords": "Counterintuitive, reinforcement learning",
        "abstract": "Learning to make sequential decisions solely from interacting with an environment without any supervision has been achieved by the initial installation of deep neural networks as function approximators to represent and learn a value function in high-dimensional MDPs. Reinforcement learning policies face exponentially growing state spaces in experience collection in high dimensional MDPs resulting in a dichotomy between computational complexity and policy success. In our paper we focus on the agent’s interaction with the environment in a high-dimensional MDP during the learning phase and we introduce a theoretically-founded novel method based on experiences obtained through extremum actions. Our analysis and method provides a theoretical basis for effective, accelerated and efficient experience collection, and further comes with zero additional computational cost while leading to significant acceleration of training in deep reinforcement learning. We conduct extensive experiments in the Arcade Learning Environment with high-dimensional state representation MDPs. We demonstrate that our technique improves the human normalized median scores of Arcade Learning Environment by 248% in the low-data regime."
    },
    {
        "title": "Instant Transformer Adaption via HyperLoRA",
        "link_suffix": "/forum?id=u6vC7KaFel",
        "link": "https://openreview.net/forum?id=u6vC7KaFel",
        "pdf_link": "https://openreview.net/pdf?id=u6vC7KaFel",
        "keywords": "hypernetworks, finetuning, language models, zero-shot generalization",
        "abstract": "While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyper-parameter choices. To overcome these limitations, we introduce HyperLoRA, a model capable of adapting Large Language Models on the fly---solely based on a natural language description of the target task.  HyperLoRA is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training HyperLoRA on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets.\nFurthermore, HyperLoRA can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements. Our code and pre-trained checkpoints will be available throughhttps://github.com/AnonymousAuthor/hyperloraandhttps://huggingface.co/upon publication."
    },
    {
        "title": "D2P2-SGD: Dynamically Differentially Private Projected Stochastic Gradient Descent",
        "link_suffix": "/forum?id=nM2kuesKpC",
        "link": "https://openreview.net/forum?id=nM2kuesKpC",
        "pdf_link": "https://openreview.net/pdf?id=nM2kuesKpC",
        "keywords": "Differential privacy, SGD, random projection, convergence, stochastic optimization",
        "abstract": "Stochastic optimization is a key enabler in modern machine learning, producing effective models for various tasks. However, several researchers have shown that model parameters and gradient information are susceptible to privacy leakage. Although, Differentially Private SGD (DPSGD) addresses privacy concerns, its static noise mechanism impacts the error bounds for model performance. Additionally, with the exponential increase in model parameters, efficient learning of these models using stochastic optimizers has become more challenging. To address these concerns, we introduce the Dynamically Differentially Private Projected Stochastic Gradient Descent (D2P2-SGD) optimizer. In D2P2-SGD, we combine two important ideas: (i) dynamic differential privacy (DDP) with automatic gradient clipping and (ii) random projection with SGD, allowing dynamic adjustment of the tradeoff between utility and privacy of the model. It demonstrates provably tighter error bounds compared to DPSGD across different behavior (i.e. convex and non-convex) of the objective function. The theoretical analysis further suggests that DDP leads to better utility at the cost of privacy, while random projection enables more efficient model learning. Extensive experiments across diverse datasets show that D2P2-SGD significantly enhances accuracy while maintaining privacy. Our code is available here."
    },
    {
        "title": "Chimera: State Space Models Beyond Sequences",
        "link_suffix": "/forum?id=Sfmk5amxFu",
        "link": "https://openreview.net/forum?id=Sfmk5amxFu",
        "pdf_link": "https://openreview.net/pdf?id=Sfmk5amxFu",
        "keywords": "Deep Learning Architectures, Sequence Models, State Space Models, Mamba",
        "abstract": "Powerful deep learning methods based on Transformers are used to model diverse data modalities such as sequences, images, and graphs. \nThese methods typically use off-the-shelf modules like self-attention, which are domain-agnostic and treat data as an unordered set of elements.\nTo improve performance, researchers employ inductive biases—such as position embeddings in sequences and images, and random walks in graphs—to inject the domain structure, ortopology, into the model.\nHowever, these inductive biases are carefully engineered heuristics that must be designed for each modality, requiring significant research effort.\nIn this work, we proposeChimera, a unified framework that mathematically generalizes state space models to incorporate the topological structure of data in a principled way.\nWe demonstrate that our method achieves state-of-the-art performance across domains including language, vision, and graphs. Chimera outperforms BERT on the GLUE benchmark by 0.7 points, surpasses ViT by 2.6% on ImageNet-1k classification accuracy, and outperforms all baselines on the Long Range Graph Benchmark with a 12% improvement on PascalVOC.\nThis validates Chimera's methodological improvement, which allows it to directly capture the underlying topology, providing a strong inductive bias across modalities.\nFurthermore, being topologically aware enables our method to achieve a linear time complexity for sequences and images, in contrast to the quadratic complexity of attention."
    },
    {
        "title": "Derivatives Are All You Need For Learning Physical Models",
        "link_suffix": "/forum?id=rYU6xsZkfk",
        "link": "https://openreview.net/forum?id=rYU6xsZkfk",
        "pdf_link": "https://openreview.net/pdf?id=rYU6xsZkfk",
        "keywords": "Physics-informed neural networks, Physics-inspired neural networks, Dynamical systems, Learning physics, Physical systems",
        "abstract": "Physics-Informed Neural Networks (PINNs) explicitly incorporate Partial Differential Equations (PDEs) into the loss function, thus learning representations that are inherently consistent with the physical system. \nWe claim that it is possible to learn physically consistent models without explicit knowledge about the underlying equations. We propose Derivative Learning (DERL) to model a physical system by learning its partial derivatives, as they contain all the necessary information to determine the system's dynamics. Like in PINNs, we also train the learning model on the initial and boundary conditions of the system. \nWe provide theoretical guarantees that our approach learns the true solution and is consistent with the underlying physical laws, even when using empirical derivatives. DERL outperforms PINNs and other state-of-the-art approaches in tasks ranging from simple dynamical systems to PDEs. Finally, we show that distilling the derivatives enables the transfer of physical information from one model to another. Distillation of higher-order derivatives improves physical consistency. Ultimately, learning and distilling the derivatives of physical systems turns out to be a powerful tool to learn physical models."
    },
    {
        "title": "Instant Policy: In-Context Imitation Learning via Graph Diffusion",
        "link_suffix": "/forum?id=je3GZissZc",
        "link": "https://openreview.net/forum?id=je3GZissZc",
        "pdf_link": "https://openreview.net/pdf?id=je3GZissZc",
        "keywords": "In-context Imitation Learning, Robotic Manipulation, Graph Neural Networks, Diffusion Models",
        "abstract": "Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem using a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations – arbitrary trajectories generated in simulation – as a virtually infinite pool of training data. Our experiments, in both simulation and reality, show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks."
    },
    {
        "title": "Is self-supervision enough for training sentence embeddings?",
        "link_suffix": "/forum?id=xtzqU9FgSi",
        "link": "https://openreview.net/forum?id=xtzqU9FgSi",
        "pdf_link": "https://openreview.net/pdf?id=xtzqU9FgSi",
        "keywords": "self-supervised learning, language models, contrastive learning, transformers, natural language processing",
        "abstract": "In NLP, sentence embeddings are crucial for many tasks such as information retrieval, classification, clustering, or visualizing collections of texts. Currently, top-performing sentence embeddings are derived from pre-trained language models that undergo extensive supervised fine-tuning. This contrasts with computer vision, where self-supervised training has demonstrated remarkable success. Here we show that self-supervision alone can produce high-quality sentence embeddings, albeit slightly below those from state-of-the-art supervised models. We systematically compare several existing augmentation strategies for positive pair generation in contrastive learning and show that text crops strongly outperform popular dropout-based augmentation. Using text crops, well-performing embeddings can be obtained even when training from scratch without using pre-trained model weights, or when training a bare token embedding layer without any transformer architecture. Overall, we show that self-supervised learning allows rapid training of text embeddings of a given dataset."
    },
    {
        "title": "Shapeshifters: Auditory cortical neurons switch from polysemantic to monosemantic under anesthesia",
        "link_suffix": "/forum?id=i4jHy0ewke",
        "link": "https://openreview.net/forum?id=i4jHy0ewke",
        "pdf_link": "https://openreview.net/pdf?id=i4jHy0ewke",
        "keywords": "auditory cortex, anesthesia, mechanstic interpretability, polysemantic, monosemantic, feature visualization, sparse autoencoders",
        "abstract": "General anesthesia transitions the brain from a conscious to an unconscious state, but how does sensory processing differ between these conditions? To address this question, we trained neural network encoding models to predict the responses of auditory cortical neurons to natural sounds in both awake and anesthetized ferrets. Utilizing mechanistic interpretability methods, such as feature visualization, linearization and sparse autoencoders, we analyzed these networks tuning and connectivity to uncover key differences in sensory processing. We found that anesthesia decouples neural connectivity, shifting neurons from polysemantic (responding to multiple inputs) to monosemantic (responding to a single input), resulting in a lower-dimensional population code. These findings illuminate how anesthesia alters neural connectivity and encoding, offering new insights into the neural mechanisms underlying sensory processing."
    },
    {
        "title": "Scaling 3D Compositional Models for Robust Classification and Pose Estimation",
        "link_suffix": "/forum?id=waGoVEQvT9",
        "link": "https://openreview.net/forum?id=waGoVEQvT9",
        "pdf_link": "https://openreview.net/pdf?id=waGoVEQvT9",
        "keywords": "analysis by synthesis, image classification, 3D representation, compositional models",
        "abstract": "Deep learning algorithms for object classification and 3D object pose estimation lack robustness to out-of-distribution factors such as synthetic stimuli, changes in weather conditions, and partial occlusion. Human vision, however, is typically much more robust to all these factors. This is arguably because human vision exploits 3D object representations which are invariant to most of these factors. Recently a class of 3D  compositional models have been developed where objects are represented in terms of 3D meshes, with typically 1000 vertices associated with learnt vertex features. These models have shown robustness in small-scale settings, involving 10 or 12 objects, but it is unclear that they can be scaled up to 100s of object classes. The main problem is that their training involves supervised contrastive learning on the mesh vertices representing the objects and requires each vertex to be contrasted with all other vertices, which scales quadratically with the vertex number. A newly available dataset with 3D annotations for 188 object classes allows us to address this scaling challenge. We present a strategy which exploits the compositionality of the objects, i.e. the independence of the feature vectors of the vertices, which greatly reduces the training time while also improving the performance of the algorithms. We first refactor the per-vertex contrastive learning into contrasting within class and between classes. Then we propose a process that dynamically decouples the contrast between classes which are rarely confused, and enhances the contrast between the vertices of classes that are most confused. Our large-scale 3D compositional model not only achieves state-of-the-art performance on object classification and 3D pose estimation in a unified manner surpassing ViT and ResNet, but is also more robust to out-of-distribution testing including occlusion, weather conditions, and synthetic data. This paves the way for scalable 3D object understanding and opens exciting possibilities for applications in robotics, autonomous systems, and augmented reality."
    },
    {
        "title": "Action Mapping for Reinforcement Learning in Continuous Environments with Constraints",
        "link_suffix": "/forum?id=xjornbs7aT",
        "link": "https://openreview.net/forum?id=xjornbs7aT",
        "pdf_link": "https://openreview.net/pdf?id=xjornbs7aT",
        "keywords": "Constrained MDPs, continuous action space, deep reinforcement learning",
        "abstract": "Deep reinforcement learning (DRL) has had success across various domains, but applying it to environments with constraints remains challenging due to poor sample efficiency and slow convergence. Recent literature explored incorporating model knowledge to mitigate these problems, particularly through the use of models that assess the feasibility of proposed actions. However, integrating feasibility models efficiently into DRL pipelines in environments with continuous action spaces is non-trivial. We propose a novel strategy, termed action mapping, that leverages feasibility models to streamline the learning process. By decoupling the learning of feasible actions from policy optimization, action mapping allows DRL agents to focus on selecting the optimal action from a reduced feasible action set. We demonstrate through experiments that action mapping significantly improves training performance in constrained environments with continuous action spaces, especially with imperfect feasibility models."
    },
    {
        "title": "LAIA-SQL: Enhancing Natural Language to SQL Generation in Multi-Table QA via Task Decomposition and Keyword Extraction",
        "link_suffix": "/forum?id=WYdpjwKQma",
        "link": "https://openreview.net/forum?id=WYdpjwKQma",
        "pdf_link": "https://openreview.net/pdf?id=WYdpjwKQma",
        "keywords": "Natural Language Understanding, Text to SQL, Multi Table QA",
        "abstract": "Natural Language to SQL (NL2SQL) provides an effective solution for multi-table question answering (Table QA) to automate data retrieval by transforming simple user queries into SQL commands. It enhances data accessibility and decision-making processes across various industries. Large Language Model (LLM) based NL2SQL methods have been shown to outperform rule-based or neural network-based NL2SQL methods. However, existing LLM-based NL2SQL approaches face challenges like inaccurate interpretation of user questions, slow retrieval speeds, erroneous SQL generation, and high operational costs. As there is a lack of datasets specifically designed to evaluate natural language understanding (NLU) in NL2SQL tasks and no models optimized for user question understanding in Table QA, we introduce LAIA-NLU, a novel dataset that dissects NLU into task decomposition and keyword extraction. LAIA-NLU contains 1,500 high-quality QA pairs, created through manual review. Using this dataset, we developed LAIA-NLUer, which is capable of effectively interpreting user intent in table-based queries. To further enhance NL2SQL performance in terms of speed, cost, and accuracy, we also present LAIA-SQL, a retrieval-augmented based NL2SQL framework. Experimental results show that LAIA-SQL outperforms state-of-the-art models, achieving an accuracy improvement to 67.28% in BIRD dataset, a 52.4% reduction in runtime, and a 97% decrease in operational costs. These improvements demonstrate the potential of our approach to advance multi-table data retrieval and analysis. Our code, dataset, and model will be publicly available to encourage further research in this field."
    },
    {
        "title": "EEG-Based Emotion Recognition via Prototype-Guided Disambiguation and Noise Augmentation in Partial-Label Learning",
        "link_suffix": "/forum?id=nnPkQb0Z0H",
        "link": "https://openreview.net/forum?id=nnPkQb0Z0H",
        "pdf_link": "https://openreview.net/pdf?id=nnPkQb0Z0H",
        "keywords": "EEG-based emotion recognition, partial label learning, self-distillation, Prototypes, noise augmentation",
        "abstract": "EEG-based emotion recognition provides an objective approach for diagnosing emotion-related health issues. However, the complexity of emotions often leads to annotation errors. Partial label learning (PLL) provides a solution, but existing candidate label generation methods overlook the semantic relationships between emotions, while existing methods for EEG-based emotion recognition fail to account for both the inter-class relationships of emotions and the low signal-to-noise ratio (SNR) of EEG signals. To address these challenges, we propose a semantic-based candidate label generation method leveraging the GloVe dictionary, alongside a novel model, PGNA-PL (Prototype-Guided Noise-Augmented Partial Label Learning). It constructs stable emotion representations through prototypes and leverages a self-distillation mechanism to iteratively guide the classifier's disambiguation process. To mitigate the low SNR of EEG signals, inspired by the mixup method, we introduce a noise augmentation strategy, incorporating controllable noise to enhance model robustness. Experiments on three public datasets (SEED, SEED-IV, SEED-V) show that our approach achieves state-of-the-art performance, surpassing existing PLL baselines across different candidate label generation modes, effectively disambiguating complex emotional states and showing promising results in recognizing phobias."
    },
    {
        "title": "Universal length generalization with Turing Programs",
        "link_suffix": "/forum?id=AfSNOjtWyt",
        "link": "https://openreview.net/forum?id=AfSNOjtWyt",
        "pdf_link": "https://openreview.net/pdf?id=AfSNOjtWyt",
        "keywords": "length generalization, deep learning",
        "abstract": "Length generalization refers to the ability to extrapolate from short training sequences to long test sequences and is a challenge for current large language models. While prior work has proposed some architecture or data format changes to achieve length generalization, these proposals typically apply to a limited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT) techniques, we propose \\emph{Turing Programs}, a novel CoT strategy that decomposes an algorithmic task into steps mimicking the computation of a Turing Machine. This framework is both universal, as it can accommodate any algorithmic task, and simple, requiring only copying text from the context with small modifications. We show that by using Turing Programs, we obtain robust length generalization on a range of algorithmic tasks: addition, multiplication and in-context SGD. We then demonstrate that transformers achieve length generalization on random Turing Programs, suggesting that length generalization is possible for any algorithmic task. Finally, we theoretically prove that transformers can implement Turing Programs, constructing a simple RASP (Weiss et al.) program that simulates an arbitrary Turing machine."
    },
    {
        "title": "Offline Model-Based Skill Stitching",
        "link_suffix": "/forum?id=0YxvqG9SsJ",
        "link": "https://openreview.net/forum?id=0YxvqG9SsJ",
        "pdf_link": "https://openreview.net/pdf?id=0YxvqG9SsJ",
        "keywords": "Skill stitching, Offline reinforcement learning, Model-based planning",
        "abstract": "We study building agents capable of solving long-horizon tasks using offline model-based reinforcement learning (RL). Existing RL methods effectively learn individual skills. However, seamlessly combining these skills to tackle long-horizon tasks presents a significant challenge, as the termination state of one skill may be unsuitable for initiating the next skill, leading to cumulative distribution shifts. Previous works have studied skill stitching through online RL, which is time-consuming and raises safety concerns when learning in the real world. In this work, we propose a fully offline approach to learn skill stitching. Given that the aggregated datasets from all skills provide diverse and exploratory data, which likely includes the necessary transitions for stitching skills, we train a dynamics model designed to generalize across skills to facilitate this process. Our method employs model predictive control (MPC) to stitch adjacent skills, using an ensemble of offline dynamics models and value functions. To mitigate overestimation issues inherent in models learned offline, we introduce a conservative approach that penalizes the uncertainty in model and value predictions. Our experimental results across various benchmarks validate the effectiveness of our approach in comparison to baseline methods under offline settings."
    },
    {
        "title": "RARe: Retrieval Augmented Retrieval with In-Context Examples",
        "link_suffix": "/forum?id=6EkWIfvjj9",
        "link": "https://openreview.net/forum?id=6EkWIfvjj9",
        "pdf_link": "https://openreview.net/pdf?id=6EkWIfvjj9",
        "keywords": "Retrieval, Embedding models, In-Context Learning",
        "abstract": "We investigate whether in-context examples, widely used in decoder-only language models (LLMs), can improve embedding models for retrieval. Unlike in LLMs, naively prepending in-context examples (query-document pairs) to the target query at inference time does not work out of the box. We introduce a simple approach to enable retrievers to use in-context examples. Our approach, \\texttt{RARe}, fine-tunes a pre-trained model with in-context examples whose query is semantically similar to the target query. This can be applied to adapt various base architectures (i.e., decoder-only language models, retriever models) and consistently achieves performance gains of up to +2.72% nDCG across various open-domain retrieval datasets (BeIR, RAR-b). Particularly, we find \\texttt{RARe} exhibits stronger out-of-domain generalization compared to models using queries without in-context examples, similar to what is seen for in-context learning in LLMs. While our approach incurs additional computational cost to encode lengthier queries, the impact is less pronounced in large-corpus scenarios. We further provide analysis on the design choices of in-context example augmentation and lay the foundation for future work in this space."
    },
    {
        "title": "CausalDiffusion: Causally Related Time-Series Generation through Diffusion Models",
        "link_suffix": "/forum?id=GkeTXeujW0",
        "link": "https://openreview.net/forum?id=GkeTXeujW0",
        "pdf_link": "https://openreview.net/pdf?id=GkeTXeujW0",
        "keywords": "time-series generation, causal discovery, diffusion model, benchmark, dataset, synthetic data",
        "abstract": "Understanding the intrinsic causal structure of time-series data is crucial for effective real-world interventions and decision-making. \nWhile several studies address the Time-Series Causal Discovery (TSCD) problem, the lack of high-quality datasets may limit the progress and evaluation of new methodologies. \nMany available datasets are derived from simplistic simulations, while real-world datasets are often limited in quantity, variety, and lack of ground-truth knowledge describing temporal causal relations. \nIn this paper, we propose CausalDiffusion, the first diffusion model capable of generating multiple causally related time-series alongside a ground-truth causal graph, which abstracts their mutual temporal dependencies.\nCausalDiffusiom employs a causal reconstruction of the output time-series, allowing it to be trained exclusively on time-series data. \nOur experiments demonstrate that CausalDiffusion outperforms state-of-the-art methods in generating realistic time-series, with causal graphs that closely resemble those of real-world phenomena. \nFinally, we provide a benchmark of widely used TSCD algorithms, highlighting the benefits of our synthetic data with respect to existing solutions."
    },
    {
        "title": "MarineMaid: Dataset and Benchmark on Detecting and Understanding Marine Creatures",
        "link_suffix": "/forum?id=krUajZ1gHg",
        "link": "https://openreview.net/forum?id=krUajZ1gHg",
        "pdf_link": "https://openreview.net/pdf?id=krUajZ1gHg",
        "keywords": "open-vocabulary object detection, marine instance detection and captioning, biodiversity monitoring, vision-language understanding",
        "abstract": "Oceans, covering more than 70% surfaces of our blue planets are less explored by the whole computer vision community. The scarcity of the labeled data is attributed to the most hindering issue. In this work, we propose a novel and comprehensive dataset called MarineMaid specifically designed for marine monitoring and understanding, including a wide spectrum of marine creatures. Based on the essential requirements of the marine research community, we adopt object detection and vision-language understanding as our two fundamental tasks. The former object detection could yield precise localization and category predictions for species identification and monitoring. Besides the sole category and BBOX predictions, the latter vision-language understanding generates redundant and comprehensive captions about biological traits required for domain experts. MarineMaid contains 12,873 fine-grained instance-captioning pairs and 42,217 bounding boxes annotated by domain experts. We have benchmarked 14 state-of-the-art algorithms on our MarineMaid dataset to reveal the strengths and limitations of existing general-purpose and domain-specific algorithms. The hierarchical and comprehensive experimental results provide valuable insights on how to develop practical and efficient marine visual perception algorithms to satisfy the domain requirements. To foster the further development of this direction, we will release our MarineMaid dataset with the acceptance of this paper."
    },
    {
        "title": "Components Beat Patches: Eigenvector Removal for Robust Masked Image Modelling",
        "link_suffix": "/forum?id=xqEeGja6zq",
        "link": "https://openreview.net/forum?id=xqEeGja6zq",
        "pdf_link": "https://openreview.net/pdf?id=xqEeGja6zq",
        "keywords": "Self-supervised Representation Learning; Unsupervised Representation Learning; Visual Representation Learning",
        "abstract": "Masked Image Modeling has gained prominence as a powerful self-supervised learning approach for visual representation learning by reconstructing masked-out patches of images. However, the use of random spatial masking can lead to failure cases in which the learned features are not predictive of downstream labels. In this work, we introduce a novel masking strategy that targets principal components instead of image patches. The learning task then amounts to reconstructing the information of masked-out principal components. The principal components of a dataset contain more global information than patches, such that the information shared between the masked input and the reconstruction target should involve more high-level variables of interest. This property allows principal components to offer a more meaningful masking space, which manifests in improved quality of the learned representations. We provide empirical evidence across natural and medical datasets and demonstrate substantial improvements in image classification tasks. Our method thus offers a simple and robust data-driven alternative to traditional Masked Image Modelling approaches."
    },
    {
        "title": "Symbiotic Tuning: A Simple Approach for Enhancing Task Performance of Side-Tuning",
        "link_suffix": "/forum?id=pf7OGmRxa5",
        "link": "https://openreview.net/forum?id=pf7OGmRxa5",
        "pdf_link": "https://openreview.net/pdf?id=pf7OGmRxa5",
        "keywords": "Natural Language Processing, Parameter-Efficient Fine-Tuning",
        "abstract": "The reduction of the computational and memory overhead associated with fine-tuning large language models remains a significant challenge for current research in natural language processing. Achieving an optimal balance between task performance, adaptability, and low VRAM requirement often presents a complex trade-off. Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, have gained attention for their ability to reduce the number of trainable parameters while preserving task performance. However, they have not yet achieved a notable reduction in VRAM usage, which is still predominantly consumed by model weights and activations during backpropagation. In contrast, Ladder Side-Tuning (LST) has been proposed as an alternative that effectively reduces VRAM usage by freezing the backbone language model (BLM) and training only lightweight side networks. Nevertheless, this reduction in memory usage often results in a decline in performance, as LST typically exhibits inferior performance compared to PEFT methods on the same BLM. To address these limitations, we propose Symbiotic Tuning (SymTune), a novel approach that extracts intermediate outputs from the BLM and integrates symbiotic modules to enhance feature processing capabilities. This method avoids a direct trade-off between performance and VRAM efficiency, offering two key advantages: 1) robust performance across a wide range of natural language tasks, and 2) reduced VRAM consumption through an improved side-tuning architecture. The experimental results demonstrate that SymTune provides a scalable and memory-efficient solution for fine-tuning language models."
    },
    {
        "title": "Text-Based Person Search in Full Images via Semantic Context Disentangling and Prototype Learning",
        "link_suffix": "/forum?id=iINUF4n33F",
        "link": "https://openreview.net/forum?id=iINUF4n33F",
        "pdf_link": "https://openreview.net/pdf?id=iINUF4n33F",
        "keywords": "Cross-modal Retrieval;Text-based Person Search;Context Disentangling;Prototype Learning",
        "abstract": "Text-based Person Search (TBPS) in full images aims to locate a target pedestrian within uncropped images based on natural language descriptions. Existing TBPS methods typically rely on candidate region generation and cross-modal matching. However, in complex scenes,especially those with multiple pedestrians in the image.It is often challenging to distinguish the target pedestrian from the background or other individuals. This leads to limited generalization capabilities.\n To address these issues, we propose a new TBPS framework named ProtoDis-TBPS, which integrates three key components: Semantic Context Decoupling (SCD), Prototype Embedding Learning (PEL), and a Cross-modal Person Re-identification (ReID) module. Specifically, SCD enhances cross-modal feature discrimination by separating background and irrelevant contextual information. PEL improves the model's robustness in complex scenes by learning prototype features for pedestrian categories. Finally, the ReID module, based on a Transformer architecture, further boosts the accuracy of both text-based pedestrian detection and re-identification in full images.Experiments demonstrate that our proposed method presents a significant challenge to existing approaches in this field."
    }
]