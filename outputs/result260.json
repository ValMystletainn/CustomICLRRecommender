[{"title": "Efficient Learning with Sine-Activated Low-Rank Matrices", "link_suffix": "/forum?id=cWGCkd7mCp", "link": "https://openreview.net/forum?id=cWGCkd7mCp", "pdf_link": "https://openreview.net/pdf?id=cWGCkd7mCp", "keywords": "parameter efficient learning, low rank matrices", "abstract": "Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.", "title_embedding_index": 12950, "title_abs_embedding_index": 12975}, {"title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting for Text-to-Speech Synthesis", "link_suffix": "/forum?id=9d6RcViazd", "link": "https://openreview.net/forum?id=9d6RcViazd", "pdf_link": "https://openreview.net/pdf?id=9d6RcViazd", "keywords": "robust text-to-speech synthesis, codec language models, chain-of-thought prompting", "abstract": "We present RALL-E, a robust language modeling method for text-to-speech (TTS) synthesis.\nWhile previous codec language modeling methods have demonstrated impressive performance in zero-shot TTS, they often struggle with robustness issues, such as unstable prosody (irregular pitch and rhythm/duration) and high word error rates (WER), largely due to their autoregressive prediction style.\nRALL-E addresses these issues through chain-of-thought (CoT) prompting, which breaks the task into simpler steps to improve the stability of TTS.\nFirst, RALL-E predicts prosody tokens (pitch and duration) from the input text and uses them as intermediate conditions to guide the prediction of speech tokens in a CoT manner.\nSecond, RALL-E utilizes the predicted duration prompt to guide the computing of self-attention weights in Transformer, enforcing the model to focus on the corresponding phonemes and prosody tokens during speech token prediction.\nComprehensive objective and subjective evaluations show that RALL-E significantly improves robustness in zero-shot TTS compared to the baseline method VALL-E, reducing WER from $5.6\\%$ to $2.5\\%$ without reranking, and from $1.7\\%$ to $1.0\\%$ with reranking.\nFurthermore, RALL-E outperforms several prior approaches aimed at improving the robustness of codec language models, and successfully synthesizes challenging sentences that VALL-E struggles with, lowering the error rate from $68\\%$ to $4\\%$.", "title_embedding_index": 12951, "title_abs_embedding_index": 12976}, {"title": "Parameter Space Representation Learning on Mixed-type Data", "link_suffix": "/forum?id=zvYJ1qG1Fy", "link": "https://openreview.net/forum?id=zvYJ1qG1Fy", "pdf_link": "https://openreview.net/pdf?id=zvYJ1qG1Fy", "keywords": "Representation learning; Parameter space; Diffusion model; Bayesian flow networks", "abstract": "A significant challenge in representation learning is to capture latent semantics in data mixing continuous, discrete, and even discretized observations (called mixedtype data), encountering issues like inconsistent discoveries and redundant modeling. Recently, Bayesian flow networks (BFNs) offer a unified strategy to represent such mixed-type data in the parameter space but cannot learn low-dimensional latent semantics since BFNs assume the size of parameters being the same as that of observations. This raises a new important question: how to learn latent semantics in parameter spaces rather than in observation spaces of mixed-type data? Accordingly, we propose a novel unified parameter space representation learning framework, ParamReL, which extracts progressive latent semantics in parameter spaces of mixed-type data. In ParamReL, a self-encoder learns latent semantics from intermediate parameters rather than observations. The learned semantics are then integrated into BFNs to efficiently learn unified representations of mixed-type data. Additionally, a reverse-sampling procedure can empower BFNs for tasks including input reconstruction and interpolation. Extensive experiments verify the effectiveness of ParamReL in learning parameter space representations for latent interpolation, disentanglement, time-varying conditional reconstruction, and conditional generation. The code is available at https: //anonymous.4open.science/r/ICLR25-F087/README.md.", "title_embedding_index": 12952, "title_abs_embedding_index": 12977}, {"title": "Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective", "link_suffix": "/forum?id=OspqtLVUN5", "link": "https://openreview.net/forum?id=OspqtLVUN5", "pdf_link": "https://openreview.net/pdf?id=OspqtLVUN5", "keywords": "preference optimization, RLHF, DPO", "abstract": "Direct Preference Optimization (DPO) has gained attention as an efficient alternative to reinforcement learning from human feedback (RLHF) for aligning large language models (LLMs) with human preferences. Despite its advantages, DPO suffers from a length bias, generating responses longer than those from the reference model. Existing solutions like SimPO and SamPO address this issue but uniformly treat the contribution of rewards across sequences, overlooking temporal dynamics. To this end, we propose an enhanced preference optimization method that incorporates a temporal decay factor controlled by a gamma parameter. This dynamic weighting mechanism adjusts the influence of each reward based on its position in the sequence, prioritizing earlier tokens that are more critical for alignment. By adaptively focusing on more relevant feedback, our approach mitigates overfitting to less pertinent data and remains responsive to evolving human preferences. Experimental results on several benchmarks show that our approach consistently outperforms vanilla DPO by 5.9-8.8 points on AlpacaEval 2 and 3.3-9.7 points on Arena-Hard across different model architectures and sizes.", "title_embedding_index": 12953, "title_abs_embedding_index": 12978}, {"title": "Data Scaling Laws in Imitation Learning for Robotic Manipulation", "link_suffix": "/forum?id=pISLZG7ktL", "link": "https://openreview.net/forum?id=pISLZG7ktL", "pdf_link": "https://openreview.net/pdf?id=pISLZG7ktL", "keywords": "Data Scaling Laws, Imitation Learning, Robotic Manipulation", "abstract": "Data scaling has revolutionized fields like natural language processing and computer vision, providing models with remarkable generalization capabilities. In this paper, we investigate whether similar data scaling laws exist in robotics, particularly in robotic manipulation, and whether appropriate data scaling can yield single-task robot policies that can be deployed zero-shot for any object within the same category in any environment. To this end, we conduct a comprehensive empirical study on data scaling in imitation learning. By collecting data across numerous environments and objects, we study how a policy\u2019s generalization performance changes with the number of training environments, objects, and demonstrations. Throughout our research, we collect over 40,000 demonstrations and execute more than 15,000 real-world robot rollouts under a rigorous evaluation protocol. Our findings reveal several intriguing results: the generalization performance of the policy follows a roughly power-law relationship with the number of environments and objects. The diversity of environments and objects is far more important than the absolute number of demonstrations; once the number of demonstrations per environment or object reaches a certain threshold, additional demonstrations have minimal effect. Based on these insights, we propose an efficient data collection strategy. With four data collectors working for one afternoon, we collect sufficient data to enable the policies for two tasks to achieve approximately 90% success rates in novel environments with unseen objects.", "title_embedding_index": 12954, "title_abs_embedding_index": 12979}, {"title": "MIO: A Foundation Model on Multimodal Tokens", "link_suffix": "/forum?id=mEACsjW10N", "link": "https://openreview.net/forum?id=mEACsjW10N", "pdf_link": "https://openreview.net/pdf?id=mEACsjW10N", "keywords": "Large Language Models, Multimodal Language Models", "abstract": "In this paper, we introduce MIO, a novel foundation model built on multimodal tokens, capable of understanding and generating speech, text, images, and videos in an end-to-end, autoregressive manner. While the emergence of large language models (LLMs) and multimodal large language models (MM-LLMs) propels advancements in artificial general intelligence through their versatile capabilities, they still lack true any-to-any understanding and generation. Recently, the release of GPT-4o has showcased the remarkable potential of any-to-any LLMs for complex real-world tasks, enabling omnidirectional input and output across images, speech, and text. However, it is closed-source and does not support the generation of multimodal interleaved sequences. To address this gap, we present MIO, which is trained on a mixture of discrete tokens across four modalities using causal multimodal modeling. MIO undergoes a four-stage training process: (1) alignment pre-training, (2) interleaved pre-training, (3) speech-enhanced pre-training, and (4) comprehensive supervised fine-tuning on diverse textual, visual, and speech tasks. Our experimental results indicate that MIO exhibits competitive, and in some cases superior, performance compared to previous dual-modal baselines, any-to-any model baselines, and even modality-specific baselines. Moreover, MIO demonstrates advanced capabilities inherent to its any-to-any feature, such as interleaved video-text generation, chain-of-visual-thought reasoning, visual guideline generation, instructional image editing, etc.", "title_embedding_index": 12955, "title_abs_embedding_index": 12980}, {"title": "Lasso Bandit with Compatibility Condition on Optimal Arm", "link_suffix": "/forum?id=f3jySJpEFT", "link": "https://openreview.net/forum?id=f3jySJpEFT", "pdf_link": "https://openreview.net/pdf?id=f3jySJpEFT", "keywords": "Sparse linear bandits, Lasso, Regret analysis", "abstract": "We consider a stochastic sparse linear bandit problem where only a sparse subset of context features affects the expected reward function, i.e., the unknown reward parameter has sparse structure.\nIn the existing Lasso bandit literature, the compatibility conditions together with additional diversity conditions on the context features are imposed to achieve regret bounds that only depend logarithmically on the ambient dimension $d$.\nIn this paper, we demonstrate that even without the additional diversity assumptions, thecompatibility condition on the optimal armis sufficient to derive a regret bound that depends logarithmically on $d$, and our assumption is strictly weaker than those used in the lasso bandit literature under the single-parameter setting.\nWe propose an algorithm that adapts the forced-sampling technique and prove that the proposed algorithm achieves $\\mathcal{O}(\\text{poly}\\log dT)$ regret under the margin condition.\nTo our knowledge, the proposed algorithm requires the weakest assumptions among Lasso bandit algorithms under the single-parameter setting that achieve $\\mathcal{O}(\\text{poly}\\log dT)$ regret.\nThrough numerical experiments, we confirm the superior performance of our proposed algorithm.", "title_embedding_index": 12956, "title_abs_embedding_index": 12981}, {"title": "Boost Protein Language Model with Injected Structure Information through Parameter Efficient Fine-tuning", "link_suffix": "/forum?id=dQG8R9uOq2", "link": "https://openreview.net/forum?id=dQG8R9uOq2", "pdf_link": "https://openreview.net/pdf?id=dQG8R9uOq2", "keywords": "Protein Language Model, Parameter-Efficient Fine-Tuning, Structure Information Injecting, ESM2", "abstract": "At the intersection of computer vision and computational biology, large-scale Protein Language Models (PLMs), particularly the ESM series, have made significant advances in understanding protein structures and functions. However, these models are mainly pre-trained on pure residue sequence, often lack explicit incorporation of structural information, highlighting an opportunity for enhancement. In this paper, we design a parameter-efficient fine-tuning method, SI-Tuning, that injects structural information into PLMs while preserving the original model parameters frozen and optimizing a minimal task-specific vector for input embedding and attention map. This vector, extracted from structural features like dihedral angles and distance maps, introduces a structural bias that enhances the model's performance in downstream tasks. Extensive experiments show that our parameter-efficient fine-tuned ESM-2 650M model outperforms SaProt, a large-scale model pre-trained with protein structural data, in various downstream tasks with a reduction of 40.3% GPU memory and 39.8% time consumption.", "title_embedding_index": 12957, "title_abs_embedding_index": 12982}, {"title": "When predict can also explain: few-shot prediction to select better neural latents", "link_suffix": "/forum?id=SyPrLti4PG", "link": "https://openreview.net/forum?id=SyPrLti4PG", "pdf_link": "https://openreview.net/pdf?id=SyPrLti4PG", "keywords": "Latent variable methods, dynamical systems, neural data, benchmarking", "abstract": "Latent variable models serve as powerful tools to infer underlying dynamics from observed neural activity. Ideally, one would like the inferred dynamics to equal the true ones. However, due to the absence of ground truth data, prediction benchmarks are often employed as proxies. One widely-used method isco-smoothing, which involves jointly estimating latent variables and predicting observations along held-out channels to assess model performance. In this study, we reveal the limitations of the co-smoothing prediction framework and propose a remedy. Utilizing a student-teacher setup with Hidden Markov Models, we demonstrate that the high co-smoothing model space can encompass models with arbitrary extraneous dynamics within their latent representations. To address this, we introduce a secondary metric\u2014few-shot co-smoothing. This involves performing regression from the latent variables to held-out channels in the data using fewer trials. Our results indicate that among models with near-optimal co-smoothing, those with extraneous dynamics underperform in the few-shot co-smoothing compared to 'minimal' models devoid of such dynamics. We also provide analytical insights into the origin of this phenomenon. We further validate our findings on real neural data using two state-of-the-art methods: LFADS and STNDT. In the absence of ground truth, we suggest a novel measure to validate our approach. By cross-decoding the latent variables of all model pairs with high co-smoothing, we identify models with minimal extraneous dynamics. We find a correlation between few-shot co-smoothing performance and this new measure. In summary, we present a novel prediction metric designed to yield latent variables that more accurately reflect the ground truth, offering a significant improvement for latent dynamics inference.", "title_embedding_index": 12958, "title_abs_embedding_index": 12983}, {"title": "Leveraging deep learning for comprehensive classification of renal diseases: A transfer learning approach", "link_suffix": "/forum?id=UkGrcekmSZ", "link": "https://openreview.net/forum?id=UkGrcekmSZ", "pdf_link": "https://openreview.net/pdf?id=UkGrcekmSZ", "keywords": "CNN, Kidney, image classification, deep learning, transfer learning", "abstract": "The nightmare of cancer as a leading cause of premature deaths worldwide is becoming real and \nturns out to be one of the major problems of humanity nowadays. Cancer diagnostics at the early stage  is \nCritical to cancer recovery and survival. In this context, renal diseases, including kidney cysts, stones, and tumors, pose significant global health challenges, affecting approximately 17% of the population and contributing to chronic kidney disease (CKD). Notably, renal cancer ranks as the tenth most prevalent cancer type, accounting for 2.7% of all cancer cases. This work presents a deep learning (DL) framework utilizing transfer learning (TL) for the early detection of renal diseases and categorizing the conditions into four binary classifications: Cyst_vs_Normal, Cyst_vs_Stone, Cyst_vs_Tumor, and Stone_vs_Tumor, allowing for a more nuanced understanding of each stage. By analyzing CT scans and microscopic histopathology images, the framework employs convolutional neural networks (CNNs) with pre-trained models to facilitate automatic and precise classification of renal conditions. Specifically, two CNN models ResNet-50 and EfficientNetV2 are implemented, providing a comprehensive analysis of each stage of the DL architecture. Comparative evaluations of training outcomes across various datasets revealed that EfficientNetV2 performed marginally better than ResNet-50, achieving an impressive testing accuracy of up to 100% for all cases. These results underscore the effectiveness of the DL-based system and highlight its potential for widespread clinical application in renal disease diagnosis.", "title_embedding_index": 12959, "title_abs_embedding_index": 12984}, {"title": "Brain-inspired continual pre-trained learner via silent synaptic consolidation", "link_suffix": "/forum?id=0CtIt485ew", "link": "https://openreview.net/forum?id=0CtIt485ew", "pdf_link": "https://openreview.net/pdf?id=0CtIt485ew", "keywords": "Continua learning; Silent synapse; Pre-trained model; neuroscience-inspired method", "abstract": "Pre-trained models have demonstrated impressive generalization capabilities, yet they remain vulnerable to catastrophic forgetting when incrementally trained on new tasks. Existing architecture-based strategies encounter two primary challenges: Firstly, integrating a pre-trained network with a trainable sub-network complicates the delicate balance between learning plasticity and memory stability across evolving tasks during learning. Secondly, the absence of robust interconnections between pre-trained networks and various sub-networks limits the effective retrieval of pertinent information during inference. In this study, we introduce the $\\textit{Artsy framework}$, inspired by the activation mechanisms of silent synapses via spike-timing-dependent plasticity observed in mature biological neural networks, to enhance the continual learning capabilities of pre-trained models. The Artsy framework integrates two key components: 1) During training, the framework mimics mature brain dynamics by maintaining memory stability for previously learned knowledge within the pre-trained network while simultaneously promoting learning plasticity in task-specific sub-networks. 2) During inference, artificial silent and functional synapses are utilized to establish precise connections between the pre-synaptic neurons in the pre-trained network and the post-synaptic neurons in the sub-networks, facilitated through synaptic consolidation, thereby enabling effective extraction of relevant information from test samples. Comprehensive experimental evaluations reveal that our model significantly outperforms conventional methods on class-incremental learning tasks, while also providing enhanced biological interpretability for architecture-based approaches. Moreover, we propose that the Artsy framework offers a promising avenue for simulating biological synaptic mechanisms, potentially advancing our understanding of neural plasticity in both artificial and biological systems.", "title_embedding_index": 12960, "title_abs_embedding_index": 12985}, {"title": "Equivariant Neural Functional Networks for Transformers", "link_suffix": "/forum?id=uBai0ukstY", "link": "https://openreview.net/forum?id=uBai0ukstY", "pdf_link": "https://openreview.net/pdf?id=uBai0ukstY", "keywords": "neural functional network, transformer, maximal symmetric group, equivariant model, dataset", "abstract": "This paper systematically explores neural functional networks (NFN) for transformer architectures. NFN are specialized neural networks that treat the weights, gradients, or sparsity patterns of a deep neural network (DNN) as input data and have proven valuable for tasks such as learnable optimizers, implicit data representations, and weight editing. While NFN have been extensively developed for MLP and CNN, no prior work has addressed their design for transformers, despite the importance of transformers in modern deep learning. This paper aims to address this gap by providing a systematic study of NFN for transformers. We first determine the maximal symmetric group of the weights in a multi-head attention module as well as a necessary and sufficient condition under which two sets of hyperparameters of the multi-head attention module define the same function. We then define the weight space of transformer architectures and its associated group action, which leads to the design principles for NFN in transformers. Based on these, we introduce Transformer-NFN, an NFN that is equivariant under this group action. Additionally, we release a dataset of more than 125,000 Transformers model checkpoints trained on two datasets with two different tasks, providing a benchmark for evaluating Transformer-NFN and encouraging further research on transformer training and performance.", "title_embedding_index": 12961, "title_abs_embedding_index": 12986}, {"title": "Bayes' Power for Explaining In-Context Learning Generalizations", "link_suffix": "/forum?id=gv8176NnO0", "link": "https://openreview.net/forum?id=gv8176NnO0", "pdf_link": "https://openreview.net/pdf?id=gv8176NnO0", "keywords": "in-context learning, large language models, PFN, TabPFN", "abstract": "Traditionally, neural network training has been primarily viewed as an approximation of maximum likelihood estimation (MLE).\nThis interpretation originated in a time when training for multiple epochs on small datasets was common and performance was data bound; but it falls short in the era of large-scale single-epoch trainings ushered in by large self-supervised setups, like language models.\nIn this new setup, performance is compute-bound, but data is readily available.\nAs models became more powerful, in-context learning (ICL), i.e., learning in a single forward-pass based on the context, emerged as one of the dominant paradigms.\nIn this paper, we argue that a more useful interpretation of neural network behavior in this era is as an approximation of the true posterior, as defined by the data-generating process.\nWe demonstrate this interpretations' power for ICL and its usefulness to predict generalizations to previously unseen tasks. We show how models become robust in-context learners by effectively composing knowledge from their training data.\nWe illustrate this with experiments that reveal surprising generalizations, all explicable through the exact posterior.\nFinally, we show the inherent constraints of the generalization capabilities of posteriors and the limitations of neural networks in approximating these posteriors.", "title_embedding_index": 12962, "title_abs_embedding_index": 12987}, {"title": "Improving Deep Regression with Tightness", "link_suffix": "/forum?id=dkoiAGjZV9", "link": "https://openreview.net/forum?id=dkoiAGjZV9", "pdf_link": "https://openreview.net/pdf?id=dkoiAGjZV9", "keywords": "regression representation, ordinality, tightness, depth estimation, age estimation", "abstract": "For deep regression, preserving the ordinality of the targets with respect to the feature representation improves performance across various tasks. However, a theoretical explanation for the benefits of ordinality is still lacking. This work reveals that preserving ordinality reduces the conditional entropy $H(Z|Y)$ of representation $Z$ conditional on the target $Y$. However, our findings reveal that typical regression losses do little to reduce $H(Z|Y)$, even though it is vital for generalization performance.With this motivation, we introduce an optimal transport-based regularizer to preserve the similarity relationships of targets in the feature space to reduce $H(Z|Y)$. Additionally, we introduce a simple yet efficient strategy of duplicating the regressor targets, also with the aim of  reducing $H(Z|Y)$.  Experiments on three real-world regression tasks verify the effectiveness of our strategies to improve deep regression.  Code will be released upon paper acceptance.", "title_embedding_index": 12963, "title_abs_embedding_index": 12988}, {"title": "Rapfi: Distilling Efficient Neural Network for the Game of Gomoku", "link_suffix": "/forum?id=AuTDvRwAjS", "link": "https://openreview.net/forum?id=AuTDvRwAjS", "pdf_link": "https://openreview.net/pdf?id=AuTDvRwAjS", "keywords": "Efficient Network, Codebook Distillation, Board Game, Gomoku", "abstract": "Games have played a pivotal role in advancing artificial intelligence, with AI agents using sophisticated techniques to compete. Despite the success of neural network based game AIs, their performance often requires significant computational resources. In this paper, we present Rapfi, an efficient Gomoku agent that outperforms CNN-based agents in limited computation environments. Rapfi leverages a compact neural network with a pattern-based codebook distilled from CNNs, and an incremental update scheme that minimizes computation when input changes are minor. This new network uses computation that is orders of magnitude less to reach a similar accuracy of much larger neural networks such as Resnet. Thanks to our incremental update scheme, depth-first search methods such as the $\\alpha$-$\\beta$ search can be significantly accelerated. With a carefully tuned evaluation and search, Rapfi reached strength surpassing Katagomo, the strongest open-source Gomoku AI based on AlphaZero's algorithm, under limited computational resources where accelerators like GPUs are absent. Rapfi ranked first among 520 Gomoku agents on Botzone and won the championship in GomoCup 2024.", "title_embedding_index": 12964, "title_abs_embedding_index": 12989}, {"title": "Label Noise Gradient Descent Improves Generalization in the Low SNR Regime", "link_suffix": "/forum?id=PPazOk82Sq", "link": "https://openreview.net/forum?id=PPazOk82Sq", "pdf_link": "https://openreview.net/pdf?id=PPazOk82Sq", "keywords": "Label Noise Gradient Descent, Feature Learning, Generalization, Low Signal-to-noise Ratio", "abstract": "The capacity of deep learning models is often large enough to both learn the underlying statistical signal and overfit to noise in the training set. This noise memorization can be harmful especially for data with a low signal-to-noise ratio (SNR), leading to poor generalization. Inspired by prior observations that label noise provides implicit regularization that improves generalization, in this work, we investigate whether introducing label noise to the gradient updates can enhance the test performance of neural network (NN) in the low SNR regime. Specifically, we consider the learning of a two-layer NN with a simple label noise gradient descent (GD) algorithm, in an idealized signal-noise data setting. We prove that adding label noise during training suppresses noise memorization, preventing it from dominating the learning process; consequently, label noise GD enjoys rapid signal growth while the overfitting remains controlled, thereby achieving good generalization despite the low SNR. In contrast, we also show that NN trained with standard GD tends to overfit to noise in the same low SNR setting and establish a non-vanishing lower bound on its test error, thus demonstrating the benefit of label noise injection in gradient-based training.", "title_embedding_index": 12965, "title_abs_embedding_index": 12990}, {"title": "Large Language and Protein Assistant for Protein-Protein Interactions prediction", "link_suffix": "/forum?id=eh1fL0zw8o", "link": "https://openreview.net/forum?id=eh1fL0zw8o", "pdf_link": "https://openreview.net/pdf?id=eh1fL0zw8o", "keywords": "Multimodal;LLM;Protein-Protein Interaction;Binding Affinity Prediction", "abstract": "Predicting the types and affinities of protein-protein interactions (PPIs) is crucial for understanding biological processes and discovering macromolecular drugs. While encoding proteins themselves is essential, PPI networks can also provide rich prior knowledge for these predictive tasks. However, existing methods oversimplify the problem of PPI prediction in a semi-supervised manner when utilizing PPI networks, limiting their practical application. Furthermore, how to effectively use the rich prior knowledge of PPI networks for novel proteins not present in the network remains an unexplored issue. Additionally, due to inflexible architectures, existing methods cannot handle complexes containing an arbitrary number of proteins. To overcome these limitations, we introduce LLaPA (Large Language and Protein Assistant), a multimodal large language model that integrates proteins and PPI networks. LLaPA offers a more rational approach to utilizing PPI networks for PPI prediction and can fully exploit the information of PPI networks for unseen proteins. Through natural language instructions, LLaPA can accept any number of protein sequences and has the potential to perform various protein tasks. Experiments show that LLaPA achieves state-of-the-art performance in multi-label PPI type prediction and is capable of predicting the binding affinity between multiple interacting proteins based on sequence data.", "title_embedding_index": 12966, "title_abs_embedding_index": 12991}, {"title": "Inference of Sequential Patterns for Neural Message Passing in Temporal Graphs", "link_suffix": "/forum?id=0IhoIn0jJ3", "link": "https://openreview.net/forum?id=0IhoIn0jJ3", "pdf_link": "https://openreview.net/pdf?id=0IhoIn0jJ3", "keywords": "graph neural networks, temporal patterns, higher order network, random graph ensembles", "abstract": "The modelling of temporal patterns in dynamic graphs is an important current research issue in the development of time-aware Graph Neural Networks (GNNs).\nHowever, whether or not a specific sequence of events in a temporal graph constitutes a temporal pattern not only depends on the frequency of its occurrence.\nWe must also consider whether it deviates from what is expected in a temporal graph where timestamps are randomly shuffled.\nWhile accounting for such a random baseline is important to model temporal patterns, it has mostly been ignored by current temporal graph neural networks.\nTo address this issue we propose HYPA-DBGNN, a novel two-step approach that combines (i) the inference of anomalous sequential patterns in time series data on graphs based on a statistically principled null model, with (ii) a neural message passing approach that utilizes a higher-order De Bruijn graph whose edges capture overrepresented sequential patterns.\nOur method leverages hypergeometric graph ensembles to identify anomalous edges within both first- and higher-order De Bruijn graphs, which encode the temporal ordering of events. \nConsequently, the model introduces an inductive bias that enhances model interpretability.We evaluate our approach for static node classification using established benchmark datasets and a synthetic dataset that showcases its ability to incorporate the observed inductive bias regarding over- and under-represented temporal edges. \nFurthermore, we demonstrate the framework's effectiveness in detecting similar patterns within empirical datasets, resulting in superior performance compared to baseline methods in node classification tasks. \nTo the best of our knowledge, our work is the first to introduce statistically informed GNNs that leverage temporal and causal sequence anomalies. \nHYPA-DBGNN represents a promising path for bridging the gap between statistical graph inference and neural graph representation learning, with potential applications to static GNNs.", "title_embedding_index": 12967, "title_abs_embedding_index": 12992}, {"title": "Towards Comprehensive and Efficient Post Safety Alignment of Large Language Models via Safety Patching", "link_suffix": "/forum?id=09JVxsEZPf", "link": "https://openreview.net/forum?id=09JVxsEZPf", "pdf_link": "https://openreview.net/pdf?id=09JVxsEZPf", "keywords": "Post Safety Alignment, Large Language Models, Jailbreak Defense, Over-Safety Mitigation", "abstract": "Safety alignment of large language models (LLMs) has been gaining increasing attention. However, current safety-aligned LLMs suffer from the fragile and imbalanced safety mechanisms, which can still be induced to generate unsafe responses, exhibit over-safety by rejecting safe user inputs, and fail to preserve general utility after safety alignment. To this end, we propose a novel post safety alignment (PSA) method to address these inherent and emerging safety challenges, including safety enhancement, over-safety mitigation, and utility preservation. In specific, we introduce \\textsc{SafePatching}, a novel framework for comprehensive and efficient PSA, where two distinct safety patches are developed on the harmful data to enhance safety and mitigate over-safety concerns, and then seamlessly integrated into the target LLM backbone without compromising its utility.  Extensive experiments on four representative aligned LLMs, including LLaMA-2/3, Gemma and Mistral, show that \\textsc{SafePatching} achieves a more comprehensive and efficient PSA than baseline methods. It even enhances the utility of the backbone, further optimizing the balance between being helpful and harmless in current aligned LLMs. Also, \\textsc{SafePatching} demonstrates its superiority in continual PSA scenarios. \\textcolor{red}{WARNING: This paper may contain content that is offensive and harmful.}", "title_embedding_index": 12968, "title_abs_embedding_index": 12993}, {"title": "GUNet: A Graph Convolutional Network United Diffusion Model for Stable and Diversity Pose Generation", "link_suffix": "/forum?id=KWo4w1UXs8", "link": "https://openreview.net/forum?id=KWo4w1UXs8", "pdf_link": "https://openreview.net/pdf?id=KWo4w1UXs8", "keywords": "Diffusion, Text2Pose, GCN, UNet", "abstract": "Pose skeleton images are an important reference in pose-controllable image generation. In order to enrich the source of skeleton images, recent works have investigated the generation of pose skeletons based on natural language. These methods are based on GANs. However, it remains challenging to perform diverse, structurally correct and aesthetically pleasing human pose skeleton generation with various textual inputs. To address this problem, we propose a framework with GUNet as the main model, PoseDiffusion. It is the first generative framework based on a diffusion model and also contains a series of variants fine-tuned based on a stable diffusion model. PoseDiffusion demonstrates several desired properties that outperform existing methods. 1) Correct Skeletons. GUNet, a denoising model of PoseDiffusion, is designed to incorporate graphical convolutional neural networks. It is able to learn the spatial relationships of the human skeleton by introducing skeletal information during the training process. 2) Diversity. We decouple the key points of the skeleton and characterise them separately, and use cross-attention to introduce textual conditions. Experimental results show that PoseDiffusion outperforms existing SoTA algorithms in terms of stability and diversity of text-driven pose skeleton generation. Qualitative analyses further demonstrate its superiority for controllable generation in Stable Diffusion.", "title_embedding_index": 12969, "title_abs_embedding_index": 12994}, {"title": "D2Coder: large language models based agent for coding with dynamic debugging tools", "link_suffix": "/forum?id=dsALpkd1OU", "link": "https://openreview.net/forum?id=dsALpkd1OU", "pdf_link": "https://openreview.net/pdf?id=dsALpkd1OU", "keywords": "LLM-based Agent, Call Graph, Dynamic Debugging, Fault Localization", "abstract": "Intelligent agents based on large language models have demonstrated certain programming abilities, but there is still significant room for improvement in complex project-level debugging tasks. Previous work has utilized general multi-agent workflows to enhance performance but has the following issues: 1) excessive reliance on the reasoning capabilities of large language models without debugging and detailed analysis of the code; 2) lack of intrinsic code information, such as call relationships and dependencies; 3) insufficient analysis and optimization of critical stages, especially the code search capability in fault localization, which directly affects the effectiveness of subsequent stages. Based on the SWE-bench dataset, we first isolate the fault localization capability for separate analysis and experiments, and introduce program call graphs to demonstrate the effectiveness of this information for debugging. Furthermore, during the debugging phase, we propose a simulated debugging mode that enables large language models to simulate program debugging without relying on other debugging tools. Compared to the real machine debugging mode, our experiments prove the effectiveness and generality of the simulated debugging mode. We conducted experiments on SWE-bench and improved the resolution rate by approximately 27.3%, demonstrating the potential of this method.", "title_embedding_index": 12970, "title_abs_embedding_index": 12995}, {"title": "Looking Backward: Streaming Video-to-Video Translation with Feature Banks", "link_suffix": "/forum?id=AMkf7h7HER", "link": "https://openreview.net/forum?id=AMkf7h7HER", "pdf_link": "https://openreview.net/pdf?id=AMkf7h7HER", "keywords": "Streaming video translation, diffusion models, feature banks", "abstract": "This paper introduces StreamV2V, a diffusion model that achieves real-time streaming video-to-video (V2V) translation with user prompts. \n  Unlike prior V2V methods using batches to process limited frames, we opt to process frames in a streaming fashion, to support unlimited frames.\n  At the heart of StreamV2V lies a backward-looking principle that relates the present to the past. \n  This is realized by maintaining a feature bank, which archives information from past frames.\n  For incoming frames, StreamV2V extends self-attention to include banked keys and values, and directly fuses similar past features into the output.\n  The feature bank is continually updated by merging stored and new features, making it compact yet informative.\n  StreamV2V stands out for its adaptability and efficiency, seamlessly integrating with image diffusion models without fine-tuning.\n  It can run 20 FPS on one A100 GPU, being 15$\\times$, 46$\\times$, 108$\\times$, and 158$\\times$ faster than FlowVid, CoDeF, Rerender, and TokenFlow, respectively. \n  Quantitative metrics and user studies confirm StreamV2V's exceptional ability to maintain temporal consistency.", "title_embedding_index": 12971, "title_abs_embedding_index": 12996}, {"title": "Watch Out!! Your Confidence Might be a Reason for Vulnerability", "link_suffix": "/forum?id=0IqriWHWYy", "link": "https://openreview.net/forum?id=0IqriWHWYy", "pdf_link": "https://openreview.net/pdf?id=0IqriWHWYy", "keywords": "Confidence, Robustness, Natural Adversaries, Object Recognition", "abstract": "The tremendous success of deep neural networks (DNNs) in solving `any' complex computer vision task leaves no stone unturned for their deployment in the physical world. However, the concerns arise when natural adversarial corruptions might perturb the physical world in unconstrained images. It is widely known that these corruptions are inherently present in the environment and can fool DNNs. While the literature aims to provide safety to DNNs against these natural corruptions they have developed two forms of defenses: (i) detection of corrupted images and (ii) mitigation of corruptions. So far, very little work has been done to understand the reason behind the vulnerabilities of DNNs against such corruption. We assert that network confidence is an essential component and ask whether the higher it is, the better the decision of a network is or not. Moreover, we ask the question of whether this confidence itself is a reason for their vulnerability against corruption. We extensively study the correlation between the confidence of a model and its robustness in handling corruption. Through extensive experimental evaluation using multiple datasets and models, we found a significant connection between the confidence and robustness of a network.", "title_embedding_index": 12972, "title_abs_embedding_index": 12997}, {"title": "Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression", "link_suffix": "/forum?id=gp32jvUquq", "link": "https://openreview.net/forum?id=gp32jvUquq", "pdf_link": "https://openreview.net/pdf?id=gp32jvUquq", "keywords": "Cross Layer Parameter Sharing in LLMs, LLM Compression with SVD", "abstract": "Large Language Models (LLMs) have achieved remarkable breakthroughs. However, the huge number of parameters in LLMs require significant amount of memory storage in inference, which prevents their practical deployment in many applications. To reduce memory storage of LLMs, singular value decomposition (SVD) provides a promising solution to approximate weight matrices for compressing LLMs. In this paper, we take a step further to explore parameter sharing across different layers with SVD to achieve more effective compression for LLMs. Specifically, weight matrices in different layers are decomposed and represented with a linear combination of a set of shared basis vectors and unique coefficients. The types of weight matrices and the layer selection for basis sharing are examined when compressing LLMs to maintain the performance. Comprehensive experiments demonstrate that Basis-Sharing outperforms state-of-the-art SVD-based compression approaches, especially at large compression ratios.", "title_embedding_index": 12973, "title_abs_embedding_index": 12998}, {"title": "DyCAST: Learning Dynamic Causal Structure from Time Series", "link_suffix": "/forum?id=WjDjem8mWE", "link": "https://openreview.net/forum?id=WjDjem8mWE", "pdf_link": "https://openreview.net/pdf?id=WjDjem8mWE", "keywords": "dynamic causal discovery; time series", "abstract": "Understanding the dynamics of causal structures is crucial for uncovering the underlying processes in time series data. Previous approaches rely on static assumptions, where contemporaneous and time-lagged dependencies are assumed to have invariant topological structures. However, these models fail when systems undergo dynamic transformations, as they cannot capture the evolving causal relationships between variables. To address this limitation, we propose DyCAST, a novel framework designed to learn dynamic causal structures in time series using Neural Ordinary Differential Equations (Neural ODEs). The key innovation lies in modeling the temporal dynamics of the contemporaneous structure, $\\boldsymbol{W}_t$, drawing inspiration from recent advances in Neural ODEs on constrained manifolds. We reformulate the task of learning causal structures at each time step as solving the solution trajectory of a Neural ODE on the directed acyclic graph (DAG) manifold. To accommodate high-dimensional causal structures, we extend DyCAST by learning the temporal dynamics of the hidden state for $\\boldsymbol{W}_t$. Experiments on both synthetic and real-world datasets demonstrate that DyCAST achieves superior or comparable performance compared to existing causal discovery models.", "title_embedding_index": 12974, "title_abs_embedding_index": 12999}]