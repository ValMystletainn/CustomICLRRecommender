[{"title": "Ordinal Preference Optimization: Aligning Human Preferences via NDCG", "link_suffix": "/forum?id=nhRXLbVXFP", "link": "https://openreview.net/forum?id=nhRXLbVXFP", "pdf_link": "https://openreview.net/pdf?id=nhRXLbVXFP", "keywords": "Large Language Models, Human Preferences Alignment, Listwise, Learning to Rank", "abstract": "Aligning Large Language Models (LLMs) with diverse human preferences is a pivotal technique for controlling model behaviors and enhancing generation quality. Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization (DPO), and their variants optimize language models by pairwise comparisons. However, when multiple responses are available, these approaches fall short of leveraging the extensive information in the ranking given by the reward models or human feedback. In this work, we propose a novel listwise approach named Ordinal Preference Optimization (OPO), which employs the Normalized Discounted Cumulative Gain (NDCG), a widely-used ranking metric, to better utilize relative proximity within ordinal multiple responses. We develop an end-to-end preference optimization algorithm by approximating NDCG with a differentiable surrogate loss. This approach builds a connection between ranking models in information retrieval and the alignment problem. In aligning multi-response datasets assigned with ordinal rewards, OPO outperforms existing pairwise and listwise approaches on evaluation sets and general benchmarks like AlpacaEval. Moreover, we demonstrate that increasing the pool of negative samples can enhance model performance by reducing the adverse effects of trivial negatives.", "title_embedding_index": 15750, "title_abs_embedding_index": 15775}, {"title": "Augmented Flow Matching via Variance Reduction with Auxiliary Variables", "link_suffix": "/forum?id=z9CCkjVY0h", "link": "https://openreview.net/forum?id=z9CCkjVY0h", "pdf_link": "https://openreview.net/pdf?id=z9CCkjVY0h", "keywords": "generative modeling, flow matching", "abstract": "Flow matching is a simulation-free approach that scalably generates an ODE, in which its path traverses between two different distributions. However, conventional flow matching relies on the training pairs drawn independently, inducing high variance that might slow down training process and degrade the performance upon training. To mitigate this, we propose augmented flow matching, a simple yet efficient framework that can be ubiquitously applied to flow matching with slight modification to the models. We first find that when some auxiliary variables that are correlated to the training data, then they contribute on variance reduction of the flow matching loss estimation, when used together with the training data pair. With this observation, we construct auxiliary variables that are correlated to the training pair, which is obtained by simple and effective linear operation from the input data. Finally, we show that with this simple modification on the training phase, we achieve the improved model flexibility and performance when the ODE is applied on the learned model.", "title_embedding_index": 15751, "title_abs_embedding_index": 15776}, {"title": "YOSO: You-Only-Sample-Once via Compressed Sensing for Graph Neural Network Training", "link_suffix": "/forum?id=SPcJPZVYAq", "link": "https://openreview.net/forum?id=SPcJPZVYAq", "pdf_link": "https://openreview.net/pdf?id=SPcJPZVYAq", "keywords": "Sampling, Graph Neural Network, Compressed Sensing", "abstract": "Graph Neural Networks (GNNs) have become essential tools for analyzing structured data across various domains.  In GNNs, sampling is critical for reducing training latency by limiting the number of nodes processed during training, especially for large-scale applications. However, as the demand for better prediction performance increases, existing sampling algorithms become more complex, introducing significant overhead in the training process. To address this issue, we introduce YOSO (You-Only-Sample-Once), an algorithm designed to achieve highly efficient training while preserving prediction accuracy in downstream tasks. YOSO proposes a compressed sensing-based sampling and reconstruction framework, where nodes are sampled once at the input layer, followed by a lossless reconstruction at the output layer during each epoch. This approach not only avoids costly computations, such as orthonormal basis, but also guarantees high-probability accuracy retention, equivalent to full node participation. Experimental results on both node classification and link prediction tasks demonstrate the effectiveness and efficiency of YOSO, reducing GNN training by an average of around 75% compared to state-of-the-art methods, while maintaining accuracy on par with top-performing baselines.", "title_embedding_index": 15752, "title_abs_embedding_index": 15777}, {"title": "Switch EMA: A Free Lunch for Better Flatness and Sharpness", "link_suffix": "/forum?id=rwNzSB3sDt", "link": "https://openreview.net/forum?id=rwNzSB3sDt", "pdf_link": "https://openreview.net/pdf?id=rwNzSB3sDt", "keywords": "Regularization, Exponential Moving Average, Weight Averaging, Optimization, Optimizer", "abstract": "Exponential Moving Average (EMA) is a widely used weight averaging (WA) regularization to learn flat optima for better generalizations without extra cost in deep neural network (DNN) optimization. Despite achieving better flatness, existing WA methods might fall into worse final performances or require extra test-time computations. This work unveils the full potential of EMA with a single line of modification, i.e., switching the EMA parameters to the original model after each epoch, dubbed as Switch EMA (SEMA). From both theoretical and empirical aspects, we demonstrate that SEMA can help DNNs to reach generalization optima that better trade-off between flatness and sharpness.\nTo verify the effectiveness of SEMA, we conduct comparison experiments with discriminative, generative, and regression tasks on vision and language datasets, including image classification, self-supervised learning, object detection and segmentation, image generation, video prediction, attribute regression, and language modeling. Comprehensive results with popular optimizers and networks show that SEMA is a free lunch for DNN training by improving performances and boosting convergence speeds.", "title_embedding_index": 15753, "title_abs_embedding_index": 15778}, {"title": "Towards Marginal Fairness Sliced Wasserstein Barycenter", "link_suffix": "/forum?id=NQqJPPCesd", "link": "https://openreview.net/forum?id=NQqJPPCesd", "pdf_link": "https://openreview.net/pdf?id=NQqJPPCesd", "keywords": "Sliced Wasserstein Barycenter, Optimal Transport, Sliced Wasserstein, Averaging Measures.", "abstract": "The Sliced Wasserstein barycenter (SWB) is a widely acknowledged method for efficiently generalizing the averaging operation within probability measure spaces. However, achieving marginal fairness SWB, ensuring approximately equal distances from the barycenter to marginals, remains unexplored. The uniform weighted SWB is not necessarily the optimal choice to obtain the desired marginal fairness barycenter due to the heterogeneous structure of marginals and the non-optimality of the optimization. As the first attempt to tackle the problem, we define the marginal fairness sliced Wasserstein barycenter (MFSWB) as a constrained SWB problem. Due to the computational disadvantages of the formal definition, we propose two hyperparameter-free and computationally tractable surrogate MFSWB problems that implicitly minimize the distances to marginals and encourage marginal fairness at the same time. To further improve the efficiency, we perform slicing distribution selection and obtain the third surrogate definition by introducing a new slicing distribution that focuses more on marginally unfair projecting directions. We discuss the relationship of the three proposed problems and their relationship to sliced multi-marginal Wasserstein distance. Finally, we conduct experiments on finding 3D point-clouds averaging, color harmonization, and training of sliced Wasserstein autoencoder with class-fairness representation to show the favorable performance of the proposed surrogate MFSWB problems.", "title_embedding_index": 15754, "title_abs_embedding_index": 15779}, {"title": "Convergence Analysis of Gradient Descent under Coordinate-wise Gradient Dominance", "link_suffix": "/forum?id=v49jqwmGtM", "link": "https://openreview.net/forum?id=v49jqwmGtM", "pdf_link": "https://openreview.net/pdf?id=v49jqwmGtM", "keywords": "Non-convex Optimization, Nash Equilibrium, Gradient Dominance, Strict Saddle", "abstract": "We consider the optimization problem of finding Nash Equilibrium (NE)  for a nonconvex function $f(x)=f(x_1,...,x_n)$, where $x_i\\in\\mathbb{R}^{d_i}$ denotes the $i$-th block of the variables. \nOur focus is on investigating first-order gradient-based algorithms and their variations such as the block coordinate descent (BCD) algorithm for tackling this problem. \nWe introduce a set of conditions, termed the $n$-sided PL condition, which extends the well-established gradient dominance condition a.k.a Polyak-{\\L}ojasiewicz (PL) condition and the concept of multi-convexity. This condition, satisfied by various classes of non-convex functions, allows us to analyze the convergence of various gradient descent (GD) algorithms. \nMoreover, our study delves into scenarios where the objective function only has strict saddle points, and normal gradient descent methods fail to converge to NE. In such cases, we propose adapted variants of GD that converge towards NE and analyze their convergence rates.", "title_embedding_index": 15755, "title_abs_embedding_index": 15780}, {"title": "ClawMachine: Learning to Fetch Visual Tokens for Referential Comprehension", "link_suffix": "/forum?id=TOtk9dTYGG", "link": "https://openreview.net/forum?id=TOtk9dTYGG", "pdf_link": "https://openreview.net/pdf?id=TOtk9dTYGG", "keywords": "Multimodal Learning, Visual Referring, Referring Expression Comprehension, Large Language Models", "abstract": "Aligning vision and language concepts at a finer level remains an essential topic of multimodal large language models (MLLMs), particularly for tasks such as referring and grounding. Existing methods, such as proxy encoding and geometry encoding, incorporate additional syntax to encode spatial information, imposing extra burdens when communicating between language with vision modules. In this study, we propose ClawMachine, offering a new methodology that explicitly notates each entity using token collectives\u2014groups of visual tokens that collaboratively represent higher-level semantics. A hybrid perception mechanism is also explored to perceive and understand scenes from both discrete and continuous spaces. Our method unifies the prompt and answer of visual referential tasks without using additional syntax. By leveraging a joint vision-language vocabulary, ClawMachine further integrates referring and grounding in an auto-regressive manner, demonstrating great potential with scaled up pre-training data. Experiments show that ClawMachine achieves superior performance on scene-level and referential understanding tasks with higher efficiency. It also exhibits the potential to integrate multi-source information for complex visual reasoning, which is beyond the capability of many MLLMs. The model and data will be publicly available.", "title_embedding_index": 15756, "title_abs_embedding_index": 15781}, {"title": "Model Extrapolation Expedites Alignment", "link_suffix": "/forum?id=QN97ubU1HH", "link": "https://openreview.net/forum?id=QN97ubU1HH", "pdf_link": "https://openreview.net/pdf?id=QN97ubU1HH", "keywords": "large language model, alignment, preference optimization, model merging", "abstract": "As the alignment training of large language models (LLMs) usually requires expensive computational resources, exploring more efficient alignment methods to reduce training overhead has always been an important and compelling research challenge. Inspired by prior work onmodel interpolation, we present a simple method calledExPO (model extrapolation)to expedite the alignment of LLMs with human preferences. Based on our observation that interpolating the weights between existing DPO/RLHF models and their initial SFT checkpoints usually produces new models with intermediate performance, we propose to treat a partially-trained model $\\mathcal{M}_1$ (corresponding to the intermediate-performing model) as the interpolated result between the initial SFT checkpoint $\\mathcal{M}_0$ and a hypothetical better-aligned model $\\mathcal{M}_2$. Thus, we can obtain the hypothetical $\\mathcal{M}_2$ by simply extrapolating the model weights along the direction from $\\mathcal{M}_0$ to $\\mathcal{M}_1$, which consequently saves the additional training overhead for $\\mathcal{M}_1$ to reach better alignment performance. We validate our hypothesis through controlled experiments, demonstrating that ExPO can boost a DPO model trained with only 20% steps to outperform the fully-trained one. Additionally, we show that ExPO can also notably improve existing open-source LLMs (ranging from 1.8B to 70B parameters), as evidenced by evaluations on the mainstream LLM benchmarks AlpacalEval 2.0 and MT-Bench, which further highlights ExPO's utility and potential in enabling more efficient LLM alignment.", "title_embedding_index": 15757, "title_abs_embedding_index": 15782}, {"title": "DRIVE: Distributional Model-Based Reinforcement Learning via Variational Inference", "link_suffix": "/forum?id=sNZTtDRFXt", "link": "https://openreview.net/forum?id=sNZTtDRFXt", "pdf_link": "https://openreview.net/pdf?id=sNZTtDRFXt", "keywords": "Distributional RL, Control as Inference, Decision-Making under Uncertainty", "abstract": "Distributional reinforcement learning (RL) provides a natural framework for estimating the distribution of returns rather than a single expected value. However, the control aspect of distributional RL has not been as thoroughly explored as the evaluation part, typically relying on the greedy selection rule with respect to either the expected value, akin to standard approaches, or risk-sensitive measures derived from the return distribution. On the other hand, casting RL as a probabilistic inference problem allows for flexible control solutions utilizing a toolbox of approximate inference techniques; however, its connection to distributional RL remains underexplored. In this paper, we bridge this gap by proposing a variational approach for efficient policy search. Our method leverages the log-likelihood of optimality as a learning proxy, decoupling it from traditional value functions. This learning proxy incorporates aleatoric uncertainty of the return distribution, enabling risk-aware decision-making. We provide a theoretical analysis of our framework, detailing the conditions for convergence. Empirical results on vision-based tasks in DMControl Suite demonstrate the effectiveness of our approach compared to various algorithms, as well as its ability to balance exploration and exploitation at different training stages.", "title_embedding_index": 15758, "title_abs_embedding_index": 15783}, {"title": "Towards Cross-modal Backward-compatible Representation Learning for Vision-Language Models", "link_suffix": "/forum?id=dI3IjAuu9V", "link": "https://openreview.net/forum?id=dI3IjAuu9V", "pdf_link": "https://openreview.net/pdf?id=dI3IjAuu9V", "keywords": "backward compatible training, cross-modal representation learning", "abstract": "Modern retrieval systems often struggle with upgrading to new and more powerful models due to the incompatibility of embeddings between the old and new models. This necessitates a costly process known as backfilling, which involves re-computing the embeddings for a large number of data samples. In vision, Backward-compatible Training (BT) has been proposed to ensure that the new model aligns with the old model's embeddings. This paper extends the concept of vision-only BT to the field of cross-modal retrieval, marking the first attempt to address Cross-modal BT (XBT). Our goal is to achieve backward-compatibility between Vision-Language Pretraining (VLP) models, such as CLIP, for the cross-modal retrieval task. To address XBT challenges, we propose an efficient solution: a projection module that maps the new model's embeddings to those of the old model. This module, pretrained solely with text data, significantly reduces the number of image-text pairs required for XBT learning, and, once it is pretrained, it avoids using the old model during training. Furthermore, we utilize parameter-efficient training strategies that improve efficiency and preserve the off-the-shelf new model's knowledge by avoiding any modifications. Experimental results on cross-modal retrieval datasets demonstrate the effectiveness of XBT and its potential to enable backfill-free upgrades when a new VLP model emerges.", "title_embedding_index": 15759, "title_abs_embedding_index": 15784}, {"title": "Text-to-Model: Text-Conditioned Neural Network Diffusion for Train-Once-for-All Personalization", "link_suffix": "/forum?id=yiGSI7Ou3i", "link": "https://openreview.net/forum?id=yiGSI7Ou3i", "pdf_link": "https://openreview.net/pdf?id=yiGSI7Ou3i", "keywords": "diffusion model, parameter generation, personalization", "abstract": "Generative artificial intelligence (GenAI) has made significant progress in understanding world knowledge and generating content from human languages across various modalities, like text-to-text large language models, text-to-image stable diffusion, and text-to-video Sora. While in this paper, we investigate the capability of GenAI for text-to-model generation, to see whether GenAI can comprehend hyper-level knowledge embedded within AI itself parameters. Specifically, we study a practical scenario termed train-once-for-all personalization, aiming to generate personalized models for diverse end-users and tasks using text prompts. Inspired by the recent emergence of neural network diffusion, we present Tina, a text-conditioned neural network diffusion for train-once-for-all personalization. Tina leverages a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. Despite the astronomical number of potential personalized tasks (e.g., $1.73\\times10^{13}$), by our design, Tina demonstrates remarkable in-distribution and out-of-distribution generalization even trained on small datasets ($\\sim 1000$). We further verify whether and how \\Tina understands world knowledge by analyzing its capabilities under zero-shot/few-shot image prompts, different numbers of personalized classes, prompts of natural language descriptions, and predicting unseen entities.", "title_embedding_index": 15760, "title_abs_embedding_index": 15785}, {"title": "Diff3DS: Generating View-Consistent 3D Sketch via Differentiable Curve Rendering", "link_suffix": "/forum?id=aIMi2lOKIn", "link": "https://openreview.net/forum?id=aIMi2lOKIn", "pdf_link": "https://openreview.net/pdf?id=aIMi2lOKIn", "keywords": "3D Sketch, Sketch Generation, Diffusion Models", "abstract": "3D sketches are widely used for visually representing the 3D shape and structure of objects or scenes. However, the creation of 3D sketch often requires users to possess professional artistic skills. Existing research efforts primarily focus on enhancing the ability of interactive sketch generation in 3D virtual systems. In this work, we propose Diff3DS, a novel differentiable rendering framework for generating view-consistent 3D sketch by optimizing 3D parametric curves under various supervisions. Specifically, we perform perspective projection to render the 3D rational B\u00e9zier curves into 2D curves, which are subsequently converted to a 2D raster image via our customized differentiable rasterizer. Our framework bridges the domains of 3D sketch and raster image, achieving end-to-end optimization of 3D sketch through gradients computed in the 2D image domain. Our Diff3DS can enable a series of novel 3D sketch generation tasks, including text-to-3D sketch and image-to-3D sketch, supported by the popular distillation-based supervision, such as Score Distillation Sampling (SDS). Extensive experiments have yielded promising results and demonstrated the potential of our framework. Code will be released.", "title_embedding_index": 15761, "title_abs_embedding_index": 15786}, {"title": "Decomposition Ascribed Synergistic Learning for Unified Image Restoration", "link_suffix": "/forum?id=zLaayPL8f0", "link": "https://openreview.net/forum?id=zLaayPL8f0", "pdf_link": "https://openreview.net/pdf?id=zLaayPL8f0", "keywords": "Image Restoration, Decomposition, Orthogonality, Signal formation", "abstract": "Learning to restore multiple image degradations within a single model is quite beneficial for real-world applications. Nevertheless, existing works typically concentrate on regarding each degradation independently, while their relationship has been less comprehended to ensure the synergistic learning. To this end, we revisit the diverse degradations through the lens of singular value decomposition, with the observation that the decomposed singular vectors and singular values naturally undertake the different types of degradation information, dividing various restoration tasks into two groups, \\ie, singular vector dominated and singular value dominated. The above analysis renders a more unified perspective to ascribe diverse degradation connections, compared to previous task-level independent learning. The dedicated optimization of degraded singular vectors and singular values inherently utilizes the potential partnership among diverse restoration tasks, attributing to the Decomposition Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue Operator (SVAO), to favor the decomposed optimization, which can be lightly integrated into existing image restoration backbone. Moreover, the congruous decomposition loss has been devised for auxiliary. Extensive experiments on five image restoration tasks demonstrate the effectiveness of our method.", "title_embedding_index": 15762, "title_abs_embedding_index": 15787}, {"title": "DHENN: A Deeper Hybrid End-to-end Neural Network for Highly Accurate Drug-Drug Interaction Events Prediction", "link_suffix": "/forum?id=UhLLqUVn4X", "link": "https://openreview.net/forum?id=UhLLqUVn4X", "pdf_link": "https://openreview.net/pdf?id=UhLLqUVn4X", "keywords": "Drug-drug interactions, Knowledge graph, Graph neural networks, Deep neural networks", "abstract": "Accurate prediction of drug-drug interactions (DDIs) is crucial for therapeutic safety yet poses a substantial challenge due to complex pharmacodynamics. Traditional DDI prediction methods often falter for three reasons. First, they simplify dependency structures among entities (e.g., drugs, targets, enzymes, and transporters) in bipartite networks, falling short in modeling their high-order interactions. Second, the over-smoothing effects constrain the depth of the adopted neural networks, thereby limiting their learning capacity. Third, they mainly decouple the stages of representation and prediction, leading to suboptimal solutions that overlook the potential for ground-truth DDIs to refine embedding generation. In response, this paper proposes Deeper Hybrid End-to-end Neural Network (DHENN), which integrates a Multimodal Knowledge Graph (MKG) with a Prediction-Enhanced Cascading Network (PECN) in an end-to-end learning manner. Specifically, MKG captures higher-order relationships across entities, offering a holistic view of DDIs. PECN mitigates over-smoothing by incorporating shallow embeddings into deeper layers, preserving node-level diversity. The endto- end learning manner guarantees that the representation learning and predictive modeling of MKG and PECN are formulated into a unified learning objective. Extensive experiments substantiate that DHENN outperforms eleven competitors on two real-world DDI datasets.", "title_embedding_index": 15763, "title_abs_embedding_index": 15788}, {"title": "SAGMAN: Stability Analysis  of Graph Neural Networks  on the Manifolds", "link_suffix": "/forum?id=mVExccNdtK", "link": "https://openreview.net/forum?id=mVExccNdtK", "pdf_link": "https://openreview.net/pdf?id=mVExccNdtK", "keywords": "Manifolds, Stability, Graph Neural Networks", "abstract": "Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability.  We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature perturbations, offering a scalable approach for evaluating the stability of GNNs, extending to applications within recommendation systems. Furthermore, we illustrate its utility in downstream tasks, notably in enhancing GNN stability and facilitating adversarial targeted attacks.", "title_embedding_index": 15764, "title_abs_embedding_index": 15789}, {"title": "MixNAM: Advancing Neural Additive Models with Mixture of Experts", "link_suffix": "/forum?id=Bc15z5RrLo", "link": "https://openreview.net/forum?id=Bc15z5RrLo", "pdf_link": "https://openreview.net/pdf?id=Bc15z5RrLo", "keywords": "Interpretable Machine Learning, Neural Additive Model, Explainable Artificial Intelligence", "abstract": "Additive models, such as Neural Additive Models (NAMs), are recognized for their transparency, providing clear insights into the impact of individual features on outcomes. However, they traditionally rely on point estimations and are constrained by their additive nature, limiting their ability to capture the complexity and variability inherent in real-world data. This variability often presents as different influences from the same feature value in various samples, adding complexity to prediction models. To address these limitations, we introduce MixNAM, an innovative framework that enriches NAMs by integrating a mixture of experts, where each expert encodes a different aspect of this variability in predictions from each feature. This integration allows MixNAM to capture the variability in feature contributions through comprehensive distribution estimations and to include feature interactions during expert routing, thus significantly boosting performance. Our empirical evaluation demonstrates that MixNAM surpasses traditional additive models in performance and is comparable to complex black-box approaches. Additionally, it improves the depth and comprehensiveness of feature attribution, setting a new benchmark for balancing interpretability with performance in machine learning. Moreover, the flexibility in MixNAM configuration facilitates the navigation of its trade-offs between accuracy and interpretability, enhancing adaptability to various data scenarios.", "title_embedding_index": 15765, "title_abs_embedding_index": 15790}, {"title": "Data-Driven Lipschitz Continuity: A Cost-Effective Approach to Improve Adversarial Robustness", "link_suffix": "/forum?id=GNOMC90vbl", "link": "https://openreview.net/forum?id=GNOMC90vbl", "pdf_link": "https://openreview.net/pdf?id=GNOMC90vbl", "keywords": "Adversarial training, AI safety, Lipschitz Continuity", "abstract": "The security and robustness of deep neural networks (DNNs) have become increasingly concerning. This paper aims to provide both a theoretical foundation and a practical solution to ensure the reliability of DNNs. We explore the concept of Lipschitz continuity to certify the robustness of DNNs against adversarial attacks, which aim to mislead the network with adding imperceptible perturbations into inputs. We propose a novel algorithm that remaps the input domain into a constrained range, reducing the Lipschitz constant and potentially enhancing robustness. Unlike existing adversarially trained models, where robustness is enhanced by introducing additional examples from other datasets or generative models, our method is almost cost-free as it can be integrated with existing models without requiring re-training. Experimental results demonstrate the generalizability of our method, as it can be combined with various models and achieve enhancements in robustness. Furthermore, our method achieves the best robust accuracy for CIFAR10 and CIFAR100 datasets on the RobustBench leaderboard.", "title_embedding_index": 15766, "title_abs_embedding_index": 15791}, {"title": "Multi-Atlas Brain Network Classification through Consistency Distillation and Complementary Information Fusion", "link_suffix": "/forum?id=9Wghi9fKFA", "link": "https://openreview.net/forum?id=9Wghi9fKFA", "pdf_link": "https://openreview.net/pdf?id=9Wghi9fKFA", "keywords": "Brain Network, fMRI Biomarker, Graph Neural Network, Graph Transformer, Neurological Disorder", "abstract": "In the realm of neuroscience, identifying distinctive patterns associated with neurological disorders via brain networks is crucial. Resting-state functional magnetic resonance imaging (fMRI) serves as a primary tool for mapping these networks by correlating blood-oxygen-level-dependent (BOLD) signals across different brain regions, defined as regions of interest (ROIs). Constructing these brain networks involves using atlases to parcellate the brain into ROIs based on various hypotheses of brain division. However, there is no standard atlas for brain network classification, leading to limitations in detecting abnormalities in disorders. Some recent methods have proposed utilizing multiple atlases, but they neglect consistency across atlases and lack ROI-level information exchange. To tackle these limitations, we propose an Atlas-Integrated Distillation and Fusion network (AIDFusion) to improve brain network classification using fMRI data. AIDFusion addresses the challenge of utilizing multiple atlases by employing a disentangle Transformer to filter out inconsistent atlas-specific information and distill distinguishable connections across atlases. It also incorporates subject- and population-level consistency constraints to enhance cross-atlas consistency. Additionally, AIDFusion employs an inter-atlas message-passing mechanism to fuse complementary information across brain regions. Experimental results on four datasets of different diseases demonstrate the effectiveness and efficiency of AIDFusion compared to state-of-the-art methods. A case study illustrates AIDFusion extract patterns that are both interpretable and consistent with established neuroscience findings.", "title_embedding_index": 15767, "title_abs_embedding_index": 15792}, {"title": "Diffusion Models Meet Contextual Bandits", "link_suffix": "/forum?id=GGAG3wFEKv", "link": "https://openreview.net/forum?id=GGAG3wFEKv", "pdf_link": "https://openreview.net/pdf?id=GGAG3wFEKv", "keywords": "Diffusion models, Bayesian bandit, Thompson sampling, Contextual bandit", "abstract": "Efficient exploration in contextual bandits is crucial due to their large action space, where uninformed exploration can lead to computational and statistical inefficiencies. However, the rewards of actions are often correlated, which can be leveraged for more efficient exploration. In this work, we use pre-trained diffusion model priors to capture these correlations and develop diffusion Thompson sampling (dTS). We establish both theoretical and algorithmic foundations for dTS. Specifically, we derive efficient posterior approximations (required by dTS) under a diffusion model prior, which are of independent interest beyond bandits and reinforcement learning. We analyze dTS in linear instances and provide a Bayes regret bound. Our experiments validate our theory and demonstrate dTS's favorable performance.", "title_embedding_index": 15768, "title_abs_embedding_index": 15793}, {"title": "POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator", "link_suffix": "/forum?id=4y4t7yOvJO", "link": "https://openreview.net/forum?id=4y4t7yOvJO", "pdf_link": "https://openreview.net/pdf?id=4y4t7yOvJO", "keywords": "Neural Architecture Search, Many-Objective, Pareto-Optimal, Meta-Dataset, Transferable Neural Architecture Search", "abstract": "Neural Architecture Search (NAS) automates the design of neural network architectures, minimising dependence on human expertise and iterative experimentation. While NAS methods are often computationally intensive and dataset-specific, employing auxiliary predictors to estimate architecture properties has proven extremely beneficial. These predictors substantially reduce the number of models requiring training, thereby decreasing overall search time. This strategy is frequently utilised to generate architectures satisfying multiple computational constraints.\nRecently, Transferable Neural Architecture Search (Transferable NAS) has emerged, generalising the search process from being dataset-dependent to task-dependent. In this domain, DiffusionNAG stands as a state-of-the-art method. This diffusion-based method streamlines computation, generating architectures optimised for accuracy on unseen datasets without the need for further adaptation. However, by concentrating exclusively on accuracy, DiffusionNAG neglects other crucial objectives like model complexity, computational efficiency, and inference latency -- factors essential for deploying models in resource-constrained, real-world environments.\nThis paper introduces the Pareto-Optimal Many-Objective Neural Architecture Generator (POMONAG), extending DiffusionNAG through a many-objective diffusion process. POMONAG simultaneously considers accuracy, the number of parameters, multiply-accumulate operations (MACs), and inference latency. It integrates Performance Predictor models to estimate these secondary metrics and guide the diffusion gradients. POMONAG's optimisation is enhanced by expanding its training Meta-Dataset, applying Pareto Front Filtering to generated architectures, and refining embeddings for conditional generation. These enhancements enable POMONAG to generate Pareto-optimal architectures that outperform the previous state-of-the-art in both performance and efficiency.\nResults were validated on two distinct search spaces -- NASBench201 and MobileNetV3 -- and evaluated across 15 image classification datasets.", "title_embedding_index": 15769, "title_abs_embedding_index": 15794}, {"title": "GFNet: Homography Estimation via Grid Flow Regression", "link_suffix": "/forum?id=DsW4boRh8H", "link": "https://openreview.net/forum?id=DsW4boRh8H", "pdf_link": "https://openreview.net/pdf?id=DsW4boRh8H", "keywords": "homography estimation, multimodal, image matching", "abstract": "Current deep homography estimation methods are constrained to processing image pairs with limited resolution due to restrictions in network architecture and computational capacity. For larger images, downsampling is often necessary, which can significantly degrade estimation accuracy. To address this limitation, we propose GFNet, a Grid Flow regression Network that consistently delivers high-accuracy homography estimates across varying image resolutions. Unlike previous methods that directly regress the parameters of the global homography between two views, GFNet directly estimates flow over a coarse grid and then uses the resulting correspondences to compute the homography. This approach not only supports high-resolution processing but also preserves the high accuracy of dense matching while significantly reducing the computational load typically associated with such frameworks, thanks to the use of coarse grid flow. We demonstrate the effectiveness of GFNet on a wide range of experiments on multiple datasets, including the common scene MSCOCO, multimodal datasets VIS-IR and GoogleMap, and the dynamic scene VIRAT. In specific, on GoogleMap, GFNet achieves an improvement of +9.9% in auc@3 while reducing MACs by $\\sim$47% compared to the SOTA dense matching method. Additionally, it shows a 1.7$\\times$ improvement in auc@3 over the SOTA deep homography method.", "title_embedding_index": 15770, "title_abs_embedding_index": 15795}, {"title": "A View-consistent Sampling Method for Regularized Training of Neural Radiance Fields", "link_suffix": "/forum?id=8kPmfXGezJ", "link": "https://openreview.net/forum?id=8kPmfXGezJ", "pdf_link": "https://openreview.net/pdf?id=8kPmfXGezJ", "keywords": "Neural Radiance Fields, novel view synthesis, scene reconstruction, sampling, foundation model", "abstract": "Neural Radiance Fields (NeRF) has emerged as a compelling framework for scene representation and 3D recovery. To improve its performance on real-world data, depth regularizations have proven to be the most effective ones. However, depth estimation models not only require expensive 3D supervision in training, but also suffer from generalization issues. As a result, the depth estimations can be erroneous in practice, especially for outdoor unbounded scenes. In this paper, we propose to employ view-consistent distributions instead of fixed depth value estimations to regularize NeRF training. Specifically, the distribution is computed by utilizing both low-level color features and high-level distilled features from foundation models at the projected 2D pixel-locations from per-ray sampled 3D points. By sampling from the view-consistency distributions, an implicit regularization is imposed on the training of NeRF. We also propose a novel depth-pushing loss that works in conjunction with the sampling technique to jointly provide effective regularizations for eliminating the failure modes. Extensive experiments conducted on various scenes from public datasets demonstrate that our proposed method can generate significantly better novel view synthesis results than state-of-the-art NeRF variants as well as different depth regularization methods.", "title_embedding_index": 15771, "title_abs_embedding_index": 15796}, {"title": "Random Erasing vs. Model Inversion: A Promising Defense or a False Hope?", "link_suffix": "/forum?id=vikwIayXOx", "link": "https://openreview.net/forum?id=vikwIayXOx", "pdf_link": "https://openreview.net/pdf?id=vikwIayXOx", "keywords": "Privacy, Model Inversion, Random Erasing", "abstract": "Model Inversion (MI) attacks pose a significant privacy threat by reconstructing private training data from machine learning models. \nWhile existing defenses primarily concentrate on model-centric approaches, the impact of data on MI robustness remains largely unexplored. In this work, we explore Random Erasing (RE), a technique traditionally used to enhance model generalization under occlusion. Surprisingly, our study reveals that RE emerges as a powerful defense against MI attacks. We conduct analysis to  identify crucial properties of RE to serve as an effective defense. Particularly,  Partial Erasure in RE prevents the model from observing the entire objects during training, and we find that this has significant impact on MI, which aims to reconstruct the entire objects. Meanwhile, our analysis suggests Random Location in RE is important for outstanding privacy-utility trade-off. Furthermore, our analysis reveals that model trained with RE leads to a discrepancy between the features of MI-reconstructed images and that of private images. These effects significantly degrade MI reconstruction quality and attack accuracy while maintaining reasonable natural accuracy. Our RE-based defense method is simple to implement and can be combined with other defenses. Extensive experiments of 34 setups demonstrate that our method achieve SOTA performance in privacy-utility tradeoff. The results consistently demonstrate the superiority of our defense over existing defenses across different MI attacks, network architectures, and  attack configurations. For the first time, we achieve significant degrade in attack accuracy without decrease in utility for some configurations. Our code and additional results are included in Supplementary.", "title_embedding_index": 15772, "title_abs_embedding_index": 15797}, {"title": "Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs", "link_suffix": "/forum?id=Mn2qgIcIPS", "link": "https://openreview.net/forum?id=Mn2qgIcIPS", "pdf_link": "https://openreview.net/pdf?id=Mn2qgIcIPS", "keywords": "NeuralODE, Low-light Enhancement", "abstract": "Low-light image enhancement poses a significant challenge due to the limited information captured by image sensors in low-light environments. \n  Despite recent improvements in deep learning models, the lack of paired training datasets remains a significant obstacle. \n  Therefore, unsupervised methods have emerged as a promising solution. \n  In this work, we focus on the strength of curve-adjustment-based approaches to tackle unsupervised methods. \n  The majority of existing unsupervised curve-adjustment approaches iteratively estimate higher order curve parameters to enhance the exposure of images while efficiently preserving the details of the images. \n  However, the convergence of the enhancement procedure cannot be guaranteed, leading to sensitivity to the number of iterations and limited performance.\n  To address this problem, we consider the iterative curve-adjustment update process as a dynamic system and formulate it as a Neural Ordinary Differential Equations (NODE) for the first time, and this allows us to learn a continuous dynamics of the latent image. \n  The strategy of utilizing NODE to leverage continuous dynamics in iterative methods enhances unsupervised learning and aids in achieving better convergence compared to discrete-space approaches. Consequently, we achieve state-of-the-art performance in unsupervised low-light image enhancement across various benchmark datasets.", "title_embedding_index": 15773, "title_abs_embedding_index": 15798}, {"title": "EDM2+: Exploring Efficient Diffusion Model Architectures for Visual Generation", "link_suffix": "/forum?id=T1MTmAlF7x", "link": "https://openreview.net/forum?id=T1MTmAlF7x", "pdf_link": "https://openreview.net/pdf?id=T1MTmAlF7x", "keywords": "diffusion models, network architecture design", "abstract": "The training and sampling of diffusion models have been exhaustively elucidated in prior art. Instead, the underlying network architecture design remains on a shaky empirical footing. Furthermore, in accordance with the recent trend of scaling law, large-scale models make inroads into generative vision tasks. However, running such large diffusion models incurs a sizeable computational burden, rendering it desiderata to optimize calculations and efficiently allocate resources. To bridge these gaps, we navigate the design landscape of efficient U-Net based diffusion models, stemming from the prestigious EDM2. Our exploration route is organized along two key axes, layer placement and module interconnection. We systematically study fundamental design choices and uncover several intriguing insights for superior efficacy and efficiency. These findings culminate in our redesigned architecture, EDM2+, that reduces the computational complexity of the baseline EDM2 by $2\\times$ without compromising the generation quality. Extensive experiments and comparative analyses highlight the effectiveness of our proposed network architecture, which achieves the state-of-the-art FID on the hallmark ImageNet benchmark. Code will be released upon acceptance.", "title_embedding_index": 15774, "title_abs_embedding_index": 15799}]