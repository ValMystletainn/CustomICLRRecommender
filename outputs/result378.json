[
    {
        "title": "MoH: Multi-Head Attention as Mixture-of-Head Attention",
        "link_suffix": "/forum?id=VOVFvaxgD0",
        "link": "https://openreview.net/forum?id=VOVFvaxgD0",
        "pdf_link": "https://openreview.net/pdf?id=VOVFvaxgD0",
        "keywords": "Multi-Head Attention, Mixture of Experts, Foundation Models",
        "abstract": "In this work, we upgrade the multi-head attention mechanism, the core of the Transformer model, to improve efficiency while maintaining or surpassing the previous accuracy level. We show that multi-head attention can be expressed in the summation form. Drawing on the insight that not all attention heads hold equal significance, we propose Mixture-of-Head attention (MoH), a new architecture that treats attention heads as experts in the Mixture-of-Experts (MoE) mechanism. MoH has two significant advantages: First, MoH enables each token to select the appropriate attention heads, enhancing inference efficiency without compromising accuracy or increasing the number of parameters. Second, MoH replaces the standard summation in multi-head attention with a weighted summation, introducing flexibility to the attention mechanism and unlocking extra performance potential. Extensive experiments on ViT, DiT, and LLMs demonstrate that MoH outperforms multi-head attention by using only 50%$\\sim$90% of the attention heads. Moreover, we demonstrate that pre-trained multi-head attention models, such as LLaMA3-8B, can be further continue-tuned into our MoH models. Notably, MoH-LLaMA3-8B achieves an average accuracy of 64.0% across 14 benchmarks, outperforming LLaMA3-8B by 2.4% by utilizing only 75% of the attention heads. We believe the proposed MoH is a promising alternative to multi-head attention and provides a strong foundation for developing advanced and efficient attention-based models."
    },
    {
        "title": "Is classification all you need for radiology report generation?",
        "link_suffix": "/forum?id=rKZSatPN3W",
        "link": "https://openreview.net/forum?id=rKZSatPN3W",
        "pdf_link": "https://openreview.net/pdf?id=rKZSatPN3W",
        "keywords": "radiology report generation, large language model, multi-modalities",
        "abstract": "Automatic radiology report generation is an advanced medical assistive technology capable of producing coherent reports based on medical images, akin to a radiologist. However, current generative methods exhibit a notable gap in clinical metrics when compared to medical image classification. Recently, leveraging diagnostic results to improve report quality has emerged as a promising approach. We are curious whether training a classifier that encompasses all possible long-tailed and rare diseases could enhance the robustness of reports. To investigate this question, this study designs an evaluation framework that integrates long-tail scenarios and summarizes potential combinations of LLM-based report generation models. We assess the impact of classification on report quality across four benchmarks. Initially, we introduce LLM-based language and clinical metrics and develop a pipeline to evaluate the model's performance on both in-domain and out-of-distribution (OOD) long-tail scenarios. Subsequently, we conduct a systematic evaluation of all potential model combinations. Our findings reveal that: 1) the impact of classification on report quality is positively correlated with the performance of classifiers, but the gap still exists, and 2) while classification can enhance report quality in in-domain long-tail scenarios, its benefits for OOD scenarios are limited."
    },
    {
        "title": "Learning Structured Universe Graph with Outlier OOD Detection for Partial Matching",
        "link_suffix": "/forum?id=dmjQLHufev",
        "link": "https://openreview.net/forum?id=dmjQLHufev",
        "pdf_link": "https://openreview.net/pdf?id=dmjQLHufev",
        "keywords": "graph matching",
        "abstract": "Partial matching is a kind of graph matching where only part of two graphs can be aligned. This problem is particularly important in computer vision applications, where challenges like point occlusion or annotation errors often occur when labeling key points.  Previous work has often conflated point occlusion and annotation errors, despite their distinct underlying causes. We propose two components to address these challenges: (1) a structured universe graph is learned to connect two input graphs $X_{ij} = X_{iu} X_{ju}^\\top$, effectively resolving the issue of point occlusion; (2) an energy-based out-of-distribution detection is designed to remove annotation errors from the input graphs before matching. We evaluated our method on the Pascal VOC and Willow Object datasets, focusing on scenarios involving point occlusion and random outliers. The experimental results demonstrate that our approach consistently outperforms state-of-the-art methods across all tested scenarios, highlighting the accuracy and robustness of our method."
    },
    {
        "title": "Layout-your-3D: Controllable and Precise 3D Generation with 2D Blueprint",
        "link_suffix": "/forum?id=myolhJPuRI",
        "link": "https://openreview.net/forum?id=myolhJPuRI",
        "pdf_link": "https://openreview.net/pdf?id=myolhJPuRI",
        "keywords": "3D generation, gaussian splatting, Text-to-3D, compositional asset generation",
        "abstract": "We present Layout-Your-3D, a framework that allows controllable and compositional 3D generation from text prompts. Existing text-to-3D methods often struggle to generate assets with plausible object interactions or require tedious optimization processes. To address these challenges, our approach leverages 2D layouts as a blueprint to facilitate precise and plausible control over 3D generation. Starting with a 2D layout provided by a user or generated from a text description, we first create a coarse 3D scene using a carefully designed initialization process based on efficient reconstruction models. To enforce coherent global 3D layouts and enhance the quality of instance appearances, we propose a collision-aware layout optimization process followed by instance-wise refinement. Experimental results demonstrate that Layout-Your-3D yields more reasonable and visually appealing compositional 3D assets while significantly reducing the time required for each prompt. Additionally, Layout-Your-3D can be easily applicable to downstream tasks, such as 3D editing and object insertion."
    },
    {
        "title": "Adaptive Memory Mechanism in Vision Transformer for Long-form Video Understanding",
        "link_suffix": "/forum?id=1DEHVMDBaO",
        "link": "https://openreview.net/forum?id=1DEHVMDBaO",
        "pdf_link": "https://openreview.net/pdf?id=1DEHVMDBaO",
        "keywords": "Key-Value Cache, Vision Transformer, Video Understanding",
        "abstract": "In long-form video understanding, selecting an optimal Temporal Receptive Field (TRF) is crucial for Vision Transformer (ViT) models due to the dynamic nature of diverse video motion contents, which varies in duration and velocity. A short TRF can result in loss of critical information, while a long TRF may decrease ViT's performance and computational efficiency caused by the unrelated contents in videos and the quadratic complexity of the attention mechanism. To tackle this issue, we introduce Adaptive Memory Mechanism (AMM) that enables ViT to adjust its TRF dynamically in response to the video's dynamic contents. Instead of discarding Key-Value (KV) Cache from the earliest inference when the settings limit is reached, our approach uses a Memory Bank (MB) to retain the most important embeddings from the Key-Value Cache that would otherwise be discarded in memory-augmented methods. The selection is based on the attention score calculated between the Class Token (CLS) in current iteration and the KV Cache in previous iterations. We demonstrate that Adaptive Memory Vision Transformer (AMViT) outperforms existing methods across a diverse array of tasks (action recognition, action anticipation, and action detection)."
    },
    {
        "title": "Impact of Data Distribution on Fairness Guarantees in Equitable Deep Learning",
        "link_suffix": "/forum?id=2E2q9t1MFp",
        "link": "https://openreview.net/forum?id=2E2q9t1MFp",
        "pdf_link": "https://openreview.net/pdf?id=2E2q9t1MFp",
        "keywords": "Fairness in Machine Learning, Equitable Deep Learning, Fairness Error Bound",
        "abstract": "Fairness in machine learning is paramount to human society because machine learning systems increasingly influence various aspects of our daily lives, particularly in consequence-critical tasks such as medical diagnosis. Deep learning models for medical diagnosis often exhibit biased performance across diverse demographic groups. Theoretical analyses to understand unfairness in AI-based medical diagnosis systems are still lacking. This work presents a comprehensive theoretical analysis of the impact of disease prevalence and data distributions on the fairness guarantees of deep learning models for medical diagnosis. We formalize the fairness problem, introduce assumptions, and derive fairness error bounds, algorithmic complexity, generalization bounds, convergence rates, and group-specific risk bounds. Our analysis reveals that fairness guarantees are significantly influenced by the differences in disease prevalence rates and data distributions across demographic groups. We prove that considering fairness criteria can lead to better performance than standard supervised learning. Empirical results on diverse datasets, including FairVision, CheXpert, HAM10000 and FairFace, corroborate our theoretical findings, demonstrating the impact of disease prevalence and feature distribution disparities on the equitable performance of deep learning models for tasks such as glaucoma, diabetic retinopathy, age-related macular degeneration, and pleural effusion detection. The code for analysis is publicly available via \\url{https://github.com/anonymous2research/fairness_guarantees}."
    },
    {
        "title": "Youku Dense Caption: A Large-scale Chinese Video Dense Caption Dataset and Benchmarks",
        "link_suffix": "/forum?id=vvi5OjPhbu",
        "link": "https://openreview.net/forum?id=vvi5OjPhbu",
        "pdf_link": "https://openreview.net/pdf?id=vvi5OjPhbu",
        "keywords": "Chinese Video Datasets, Retrieval, Grounding, Generation",
        "abstract": "With the explosive growth of video content, video captions have emerged as a crucial tool for video comprehension, significantly enhancing the ability to understand and retrieve information from videos. However, most publicly available dense video captioning datasets are in English, resulting in a scarcity of large-scale and high-quality Chinese dense video captioning datasets. To address this gap within the Chinese community and to promote the advancement of Chinese multi-modal models, we develop the first, large-scale, and high-quality Chinese dense video captioning dataset, named Youku Dense Caption. This dataset is sourced from Youku, a prominent Chinese video-sharing website. Youku Dense Caption includes 31,466 complete short videos annotated by 311,921 Chinese captions. To the best of our knowledge, it is currently the largest publicly available dataset for fine-grained Chinese video descriptions. Additionally, we establish several benchmarks for Chinese video-language tasks based on the Youku Dense Caption, including retrieval, grounding, and generation tasks. Extensive experiments and evaluations are conducted on existing state-of-the-art multi-modal models, demonstrating the dataset's utility and the potential for further research."
    },
    {
        "title": "Axis-level Reflectional Symmetry Detection with Group-Equivariant Representation",
        "link_suffix": "/forum?id=xQAhUIuAc6",
        "link": "https://openreview.net/forum?id=xQAhUIuAc6",
        "pdf_link": "https://openreview.net/pdf?id=xQAhUIuAc6",
        "keywords": "Symmetry detection, Equivariant learning, Group equivariance",
        "abstract": "Reflectional symmetry detection remains a challenging task in machine perception, particularly in complex real-world scenarios involving noise, occlusions, and distortions. We introduce a novel equivariant approach to axis-level reflectional symmetry detection that effectively leverages dihedral group-equivariant representation to detect symmetry axes as line segments. We propose orientational anchor expansion for fine-grained rotation-equivariant analysis of diverse symmetry patterns across multiple orientations. Additionally, we develop reflectional matching with multi-scale kernels to extract effective cues of reflectional correlations, allowing for robust symmetry detection across different receptive fields. Our approach unifies axis-level detection with reflectional matching while preserving dihedral group equivariance throughout the process. Extensive experiments demonstrate the efficacy of our method while providing more accurate axis-level predictions than existing pixel-level methods in challenging scenarios."
    },
    {
        "title": "Query Optimization Detection Transformer for Small Objects in Remote Sensing Images",
        "link_suffix": "/forum?id=T6hhDEnAoo",
        "link": "https://openreview.net/forum?id=T6hhDEnAoo",
        "pdf_link": "https://openreview.net/pdf?id=T6hhDEnAoo",
        "keywords": "Remote sensing images; Small object detection; Transformer; Query optimization",
        "abstract": "Object detection in remote sensing images is a challenging task. Remote sensing images contain substantial background noise and complex contextual information, which weakens the feature representation of small objects, making detection difficult. To solve these problems, a detection Transformer for small objects in remote sensing images is proposed, called QO-DETR. Specifically, to enhance the feature representation of small objects, a query proposal generation module is designed to select queries based on multi-class classification scores. These queries provide the initial position embeddings for object queries in the decoder, enabling the decoder's attention mechanism to focus on object regions.  To improve the model\u2019s robustness to noise, a group denoising module is designed to add noise into decoder queries during training, enhancing the network's ability to reconstruct object features from noise. To accurately locate small objects, a query cascade refinement strategy is designed, and each decoder layer refines anchor parameters under the guidance of preceding layers to achieve spatial alignment between the anchor and the object. Experiments have been carried out on DIOR and AI-TOD. The AP and APs on DIOR reach 51.3% and 13.4%, respectively, while on AI-TOD, they reach 23.6% and 30.1%. QO-DETR shows superior performance in detecting small objects."
    },
    {
        "title": "WALL-E: World Alignment by Rule Learning Improves World Model-based LLM Agents",
        "link_suffix": "/forum?id=GE0UKtI6Lf",
        "link": "https://openreview.net/forum?id=GE0UKtI6Lf",
        "pdf_link": "https://openreview.net/pdf?id=GE0UKtI6Lf",
        "keywords": "world model, embodied agent, neurosymbolic, large language model",
        "abstract": "Can large language models (LLMs) directly serve as powerful world models for model-based agents? While the gaps between the prior knowledge of LLMs and the specified environment's dynamics do exist, our study reveals that the gaps can be bridged by aligning an LLM with its deployed environment and such \"world alignment\" can be efficiently achieved by rule learning on LLMs. Given the rich prior knowledge of LLMs, only a few additional rules suffice to align LLM predictions with the specified environment dynamics. To this end, we propose a neurosymbolic approach to learn these rules gradient-free through LLMs, by inducing, updating, and pruning rules based on comparisons of agent-explored trajectories and world model predictions. The resulting world model is composed of the LLM and the learned rules. Our embodied LLM agent \"WALL-E\" is built upon model-predictive control (MPC). By optimizing look-ahead actions based on the precise world model, MPC significantly improves exploration and learning efficiency. Compared to existing LLM agents, WALL-E's reasoning only requires a few principal rules rather than verbose buffered trajectories being included in the LLM input. On open-world challenges in Minecraft and ALFWorld, WALL-E achieves higher success rates than existing methods, with lower costs on replanning time and the number of tokens used for reasoning. In Minecraft, WALL-E exceeds baselines by 15-30% in success rate while costing 8\u201320 fewer replanning rounds and only 60\u201380% of tokens. In ALFWorld, its success rate surges to a new record high of 95% only after 6 iterations."
    },
    {
        "title": "Large Language Model for Lossless Image Compression with Visual Prompts",
        "link_suffix": "/forum?id=dcG17rjJF9",
        "link": "https://openreview.net/forum?id=dcG17rjJF9",
        "pdf_link": "https://openreview.net/pdf?id=dcG17rjJF9",
        "keywords": "Image Compression, Lossless Image Compression, Lossy Image Compression, Video Compression",
        "abstract": "Recent advancements in deep learning have driven significant progress in lossless image compression. With the emergence of Large Language Models (LLMs), preliminary attempts have been made to leverage the extensive prior knowledge embedded in these pretrained models to enhance lossless image compression, particularly by improving the entropy model. However, a significant challenge remains in bridging the gap between the textual prior knowledge within LLMs and lossless image compression.\nTo tackle this challenge and unlock the potential of LLMs, this paper introduces a novel paradigm for lossless image compression that incorporates LLMs with visual prompts. Specifically, we first generate a lossy reconstruction of the input image as visual prompts, from which we extract local and global features to serve as visual embeddings for the LLM. The residual between the original image and the lossy reconstruction is then fed into the LLM along with these visual embeddings, enabling the LLM to function as an entropy model to predict the probability distribution of the residual.\nExtensive experiments on multiple benchmark datasets demonstrate our method achieves state-of-the-art compression performance, surpassing both traditional and learning-based lossless image codecs. Furthermore, our approach can be easily extended to images from other domains, such as medical and screen content images, achieving impressive performance. These results highlight the potential of LLMs for lossless image compression and may inspire further research in related directions."
    },
    {
        "title": "ADMM for Structured Fractional Minimization",
        "link_suffix": "/forum?id=DcZpQhVpp9",
        "link": "https://openreview.net/forum?id=DcZpQhVpp9",
        "pdf_link": "https://openreview.net/pdf?id=DcZpQhVpp9",
        "keywords": "Fractional Minimization, Nonconvex Optimization, Proximal Linearized ADMM, Nonsmooth Optimization, Convergence Analysis",
        "abstract": "We consider a class of structured fractional minimization problems, where the numerator includes a differentiable function, a simple nonconvex nonsmooth function, a concave nonsmooth function, and a convex nonsmooth function composed with a linear operator, while the denominator is a continuous function that is either weakly convex or has a weakly convex square root. These problems are widespread and span numerous essential applications in machine learning and data science. Existing methods are mainly based on subgradient methods and smoothing proximal gradient methods, which may suffer from slow convergence and numerical stability issues. In this paper, we introduce {\\sf FADMM}, the first Alternating Direction Method of Multipliers tailored for this class of problems. {\\sf FADMM} decouples the original problem into linearized proximal subproblems, featuring two variants: one using Dinkelbach's parametric method ({\\sf FADMM-D}) and the other using the quadratic transform method ({\\sf FADMM-Q}). By introducing a novel Lyapunov function, we establish that {\\sf FADMM} converges to $\\epsilon$-approximate critical points of the problem within an oracle complexity of $\\mathcal{O}(1/\\epsilon^{3})$. Our experiments on synthetic and real-world data for sparse Fisher discriminant analysis, robust Sharpe ratio minimization, and robust sparse recovery demonstrate the effectiveness of our approach."
    },
    {
        "title": "MG-LLaVA: Towards Multi-Granularity Visual Instruction Tuning",
        "link_suffix": "/forum?id=50RNY6uM2Q",
        "link": "https://openreview.net/forum?id=50RNY6uM2Q",
        "pdf_link": "https://openreview.net/pdf?id=50RNY6uM2Q",
        "keywords": "Multi-Modality, Large Language Models",
        "abstract": "Multi-modal large language models (MLLMs) have made significant strides in various image comprehension tasks. However, the majority of these models are constrained to processing low-resolution images, which limits their effectiveness in perception tasks that necessitate detailed visual information. In our study, we present MG-LLaVA, an innovative MLLM that enhances the model's visual processing capabilities by incorporating a multi-granularity vision flow, which includes low-resolution, high-resolution, and object-centric features. We propose the integration of an additional high-resolution visual encoder to capture fine-grained details, which are then fused with base visual features through a Conv-Gate fusion network. To further refine the model's object recognition abilities, we incorporate object-level features derived from bounding boxes identified by offline detectors. Being trained solely on publicly available multimodal data through instruction tuning, MG-LLaVA demonstrates exceptional perception skills. We instantiate MG-LLaVA with a wide variety of language encoders, ranging from 3.8B to 34B, to evaluate the model's performance comprehensively. Extensive evaluations across multiple benchmarks demonstrate that MG-LLaVA outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy."
    },
    {
        "title": "FedSUV: Validity and Utility-guided Client Selection for Federated Learning",
        "link_suffix": "/forum?id=sLNRvScGM2",
        "link": "https://openreview.net/forum?id=sLNRvScGM2",
        "pdf_link": "https://openreview.net/pdf?id=sLNRvScGM2",
        "keywords": "federated learning, client selection, multi-armed bandit, uncertainty estimation",
        "abstract": "Federated Learning faces significant challenges arising from two critical uncertainties: the validity of a client\u2019s participation, which can be compromised by network and system heterogeneity, and the utility of the data contributed by each client, which varies due to heterogeneous statistical data. Traditional client selection methods often treat these uncertainties as a whole, leading to suboptimal performance. To address this issue, we propose FedSUV, an innovative client selection framework that decouples validity and utility uncertainties. FedSUV approaches client selection from a multi-objective optimization perspective, employing advanced bandit algorithms: a confidence bound-based linear contextual bandit for assessing validity and a Gaussian Process bandit for evaluating utility. We validate the effectiveness of FedSUV through both theoretical analysis and large-scale experiments conducted within our physical cluster."
    },
    {
        "title": "NegMerge: Consensual Weight Negation for Strong Machine Unlearning",
        "link_suffix": "/forum?id=bKQJzuBSRJ",
        "link": "https://openreview.net/forum?id=bKQJzuBSRJ",
        "pdf_link": "https://openreview.net/pdf?id=bKQJzuBSRJ",
        "keywords": "Machine Unlearning, Image Classification, Model Merging",
        "abstract": "Machine unlearning aims to selectively remove specific knowledge from a model. Current methods, such as task arithmetic, rely on fine-tuning models on the forget set, generating a task vector, and subtracting it from the original model. However, we argue the effectiveness of this approach is highly sensitive to hyperparameter selection, necessitating careful validation to identify the best model among many fine-tuned candidates. In this paper, we propose a novel method that leverages all given fine-tuned models rather than selecting a single one. By constructing task vectors from models trained with varied hyperparameters and merging only the components of the task vectors with consistent signs, we perform unlearning by negating the merged task vector from the original model. Given that existing methods also utilize multiple fine-tuned models, our approach delivers more effective unlearning without incurring additional computational costs. We demonstrate the effectiveness of our method on both vision-language models and standard image classification models, showing improved unlearning performance with minimal degradation on the retain set, outperforming state-of-the-art techniques."
    },
    {
        "title": "Consistency Model is an Effective Posterior Sample Approximation for Diffusion Inverse Solvers",
        "link_suffix": "/forum?id=4xbwWerxvZ",
        "link": "https://openreview.net/forum?id=4xbwWerxvZ",
        "pdf_link": "https://openreview.net/pdf?id=4xbwWerxvZ",
        "keywords": "Diffusion model, Inverse problem",
        "abstract": "Diffusion Inverse Solvers (DIS) are designed to sample from the conditional distribution $p_{\\theta}(X_0|y)$, with a pre-trained diffusion model $p_{\\theta}(X_0)$, an operator $f(.)$, and a measurement $y=f(x'_0)$ derived from an unknown image $x'_0$. Existing DIS estimate the conditional score function by evaluating $f(.)$ with an approximated posterior sample drawn from $p_{\\theta}(X_0|X_t)$. However, most prior approximations rely on the posterior means, which may not lie in the support of the image distribution and diverge from the appearance of genuine images. Such out-of-support samples may significantly degrade the performance of the operator $f(.)$, particularly when it is a neural network. In this paper, we introduces a novel approach for posterior approximation that guarantees to generate valid samples within the support of the image distribution, and also enhances the compatibility with neural network-based operators $f(.)$. We first demonstrate that the solution of the Probability Flow Ordinary Differential Equation (PF-ODE) with an initial value $x_t$ yields an effective posterior sample of $p_{\\theta}(X_0|X_t=x_t)$ with high probability. Based on this observation, we adopt the Consistency Model (CM), which is distilled from PF-ODE, for posterior sampling. Through extensive experiments, we show that our proposed method for posterior sample approximation substantially enhance the effectiveness of DIS for neural network operators $f(.)$ (e.g., in semantic segmentation). The source code is provided in the supplementary material."
    },
    {
        "title": "TAU-106K: A New Dataset for Comprehensive Understanding of Traffic Accident",
        "link_suffix": "/forum?id=Fb0q2uI4Ha",
        "link": "https://openreview.net/forum?id=Fb0q2uI4Ha",
        "pdf_link": "https://openreview.net/pdf?id=Fb0q2uI4Ha",
        "keywords": "Anomaly Detection, Traffic Accident Understanding, Multimodal Large Language Models, Instructional Dataset",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated impressive performance in general visual understanding tasks. However, their potential for high-level and fine-grained comprehension, such as humor or anomaly understanding, remains unexplored. Targeting traffic accidents, a critical and practical scenario within anomaly understanding, we explore the advanced capabilities of MLLMs and introduce TABot, a multimodal MLLM tailored for accident-related tasks. To facilitate this, we first developed TAU-106K, a large-scale multimodal dataset comprising 106K traffic accident-related videos and images, sourced from academic benchmarks and public platforms. The dataset is meticulously annotated through a video-to-image annotation pipeline, ensuring comprehensive and high-quality labels. Upon TAU-106K, our accident-oriented MLLM TABot is trained in a two-step approach to integrate multi-granularity accident understanding tasks, including accident recognition, spatial-temporal grounding, with an additional accident description task to guide the model in comprehending the nature of traffic accidents. Extensive experiments demonstrate the superior performance of TABot in traffic accident understanding, underscoring both its potential for high-level anomaly understanding and the robustness of the TAU-106K dataset. All datasets, annotations, and models will be publicly released for future research."
    },
    {
        "title": "EmoGrowth: Incremental Multi-label Emotion Decoding with Augmented Emotional Relation Graph",
        "link_suffix": "/forum?id=b2fhCbhe62",
        "link": "https://openreview.net/forum?id=b2fhCbhe62",
        "pdf_link": "https://openreview.net/pdf?id=b2fhCbhe62",
        "keywords": "Emotion recognition, Affective computing, Neural decoding\uff0cClass incremental learning\uff0cMulti-label learning",
        "abstract": "Emotion decoding plays an important role in affective human-computer interaction. However, previous studies ignored the dynamic real-world scenario, where humans experience a blend of multiple emotions which are incrementally integrated into the model, leading to the multi-label class incremental learning (MLCIL) problem. Existing methods have difficulty in solving the MLCIL issue due to notorious catastrophic forgetting caused by the partial label problem and inadequate label semantics mining. In this paper, we propose an augmented emotional semantics learning framework for multi-label class incremental emotion decoding. Specifically, we design an augmented emotional relation graph module with label disambiguation to handle the past-missing partial label problem. Then, we leverage domain knowledge from the affective dimension space to alleviate the future-missing partial label problem through knowledge distillation. Besides, an emotional semantics learning module is constructed with a graph autoencoder to obtain emotion embeddings in order to guide the semantic-specific feature decoupling for better multi-label learning. Extensive experiments on three datasets show the superiority of our method for improving emotion decoding performance and mitigating forgetting on the MLCIL problem."
    },
    {
        "title": "SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement",
        "link_suffix": "/forum?id=JlDx2xp01W",
        "link": "https://openreview.net/forum?id=JlDx2xp01W",
        "pdf_link": "https://openreview.net/pdf?id=JlDx2xp01W",
        "keywords": "Mask Refinement, Segment Anything Model",
        "abstract": "In this paper, we explore a principal way to enhance the quality of widely pre-existing coarse masks, enabling them to serve as reliable training data for segmentation models to reduce the annotation cost. In contrast to prior refinement techniques that are tailored to specific models or tasks in a close-world manner, we propose SAMRefiner, a universal and efficient approach by adapting SAM to the mask refinement task. The core technique of our model is the noise-tolerant prompting scheme. Specifically, we introduce a multi-prompt excavation strategy to mine diverse input prompts for SAM (\\ie, distance-guided points, context-aware elastic bounding boxes, and Gaussian-style masks) from initial coarse masks. These prompts can collaborate with each other to mitigate the effect of defects in coarse masks. In particular, considering the difficulty of SAM to handle the multi-object case in semantic segmentation, we introduce a split-then-merge (STM) pipeline. Additionally, we extend our method to SAMRefiner++ by introducing an additional IoU adaption step to further boost the performance of the generic SAMRefiner on the target dataset. This step is self-boosted and requires no additional annotation. The proposed framework is versatile and can flexibly cooperate with existing segmentation methods. We evaluate our mask framework on a wide range of benchmarks under different settings, demonstrating better accuracy and efficiency. SAMRefiner holds significant potential to expedite the evolution of refinement tools, and we will release it as a convenient post-processing toolkit."
    },
    {
        "title": "Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior",
        "link_suffix": "/forum?id=GcvLoqOoXL",
        "link": "https://openreview.net/forum?id=GcvLoqOoXL",
        "pdf_link": "https://openreview.net/pdf?id=GcvLoqOoXL",
        "keywords": "Diffusion model, Inverse problem",
        "abstract": "Recent advancements in diffusion models have been leveraged to address inverse problems without additional training, and Diffusion Posterior Sampling (DPS) (Chung et al., 2022a) is among the most popular approaches. Previous analyses suggest that DPS accomplishes posterior sampling by approximating the conditional score. While in this paper, we demonstrate that the conditional score approximation employed by DPS is not as effective as previously assumed, but rather aligns more closely with the principle of maximizing a posterior (MAP). This assertion is substantiated through an examination of DPS on 512$\\times$512 ImageNet images, revealing that: 1) DPS\u2019s conditional score estimation significantly diverges from the score of a well-trained conditional diffusion model and is even inferior to the unconditional score; 2) The mean of DPS\u2019s conditional score estimation deviates significantly from zero, rendering it an invalid score estimation; 3) DPS generates high-quality samples with significantly lower diversity. In light of the above findings, we posit that DPS more closely resembles MAP than a conditional score estimator, and accordingly propose the following enhancements to DPS: 1) we explicitly maximize the posterior through multi-step gradient ascent and projection; 2) we utilize a light-weighted conditional score estimator trained with only 100 images and 8 GPU hours. Extensive experimental results indicate that these proposed improvements significantly enhance DPS\u2019s performance. The source code for these improvements is provided in the supplementary material."
    },
    {
        "title": "Benchmarking Agentic Workflow Generation",
        "link_suffix": "/forum?id=vunPXOFmoi",
        "link": "https://openreview.net/forum?id=vunPXOFmoi",
        "pdf_link": "https://openreview.net/pdf?id=vunPXOFmoi",
        "keywords": "workflow generation, graph structured planning, large language model, agent",
        "abstract": "Large Language Models (LLMs), with their remarkable task-handling capabilities, have catalyzed significant achievements in tackling reasoning and planning tasks, wherein decomposing complex problems into executable workflows is a crucial step in this process. Existing workflow evaluation frameworks either focus solely on holistic performance or suffer from limitations such as restricted scenario coverage, simplistic workflow structures, and lax evaluation standards. To this end, we introduce WorFBench, a unified workflow generation benchmark with multi-faceted scenarios and intricate graph workflow structures. Additionally, we present WorfEval, a systemic evaluation protocol utilizing subsequence and subgraph matching algorithms to accurately quantify the LLM agent's workflow generation capabilities. Through comprehensive evaluations across different types of LLMs, we discover distinct gaps between the sequence planning capabilities and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a gap of around 15%. We also train two open-source models and evaluate their generalization abilities on held-out tasks. Furthermore, we observe that the generated workflows can enhance downstream tasks, enabling them to achieve superior performance with less time during inference."
    },
    {
        "title": "MatchMask: Mask-Centric Generative Data Augmentation for Label-Scarce Semantic Segmentation",
        "link_suffix": "/forum?id=XpU1twhp3u",
        "link": "https://openreview.net/forum?id=XpU1twhp3u",
        "pdf_link": "https://openreview.net/pdf?id=XpU1twhp3u",
        "keywords": "Semantic Segmentation, Generative Data Augmentation, Label-Scarce",
        "abstract": "Current semantic segmentation models are very data-hungry and require massive costly pixel-wise human annotations. Generative data augmentation, which scales the train set using generative models, provides a potential remedy. However, existing text-centric methods struggle to generate complex in-distribution data due to the limitations of text descriptions. In this paper, we propose MatchMask, a novel mask-centric generative data augmentation approach tailored for label-scarce semantic segmentation. It leverages a few labeled semantic masks to generate diverse, realistic, and well-aligned image-mask training pairs for semantic segmentation models. Specifically, to adapt existing text-to-image models for semantic image synthesis, we first propose a Gradient Probe Method to investigate the role of each layer in the diffusion model. On this basis, we introduce a Layer-Timestep Adaptive Adapter (LT-Adapter) comprising layer-adaptive cross-attention fusion and time-adaptive LoRA scaling to enable efficient adaption for the critical layers. Meantime, we design a robust relative filtering principle to suppress incorrectly synthesized regions. Moreover, the proposed approach is extended to MatchMask++ in the semi-supervised setting to take advantage of additional unlabeled data. Experimental results on VOC, COCO and ADE20K demonstrate that MatchMask remarkably enhances the performance of segmentation models, surpassing prior data augmentation techniques in various benchmarks, \\eg, 67.5%$\\rightarrow$74.3% mIoU on VOC. Our code will be made publicly available."
    },
    {
        "title": "Enhancing Multi-Objective Offline RL with Adaptive Preference Integration",
        "link_suffix": "/forum?id=INzc851YaM",
        "link": "https://openreview.net/forum?id=INzc851YaM",
        "pdf_link": "https://openreview.net/pdf?id=INzc851YaM",
        "keywords": "Reinforcement Learning, Multi-objective Optimization, Offline Reinforcement Learning, Decision Transformer",
        "abstract": "Multi-objective reinforcement learning (MORL) is crucial for real-world applications where multiple conflicting goals must be optimized, such as in healthcare or autonomous systems. Offline MORL extends these benefits by using pre-collected datasets, allowing for effective learning without continuous interaction with the environment. However, existing offline MORL algorithms often struggle with scaling across large preference spaces and handling unknown preferences during evaluation. To address these challenges, we propose the Preference-Attended Multi-Objective Decision Transformer (PA-MODT), a novel architecture that integrates a preference-attention block with a modular transformer structure. This design enables effective generalization over different preferences and trajectories, providing a more robust approach to generating optimal Pareto fronts. We tested PA-MODT on five D4MORL datasets with millions of trajectories representing various objectives and found that it consistently outperforms existing models, achieving Pareto fronts that align closely with behavioral policy. This demonstrates PA-MODT's potential to effectively manage complex multi-objective reinforcement learning tasks."
    },
    {
        "title": "A Unified Theoretical Framework for Understanding Difficult-to-learn Examples in Contrastive Learning",
        "link_suffix": "/forum?id=P4WnvhVmPV",
        "link": "https://openreview.net/forum?id=P4WnvhVmPV",
        "pdf_link": "https://openreview.net/pdf?id=P4WnvhVmPV",
        "keywords": "Machine Learning; Contrastive Learning; Difficult-to-learn Examples",
        "abstract": "Unsupervised contrastive learning has shown significant performance improvements in recent years, often approaching or even rivaling supervised learning in various tasks. However, its learning mechanism is fundamentally different from that of supervised learning. Previous works have shown that difficult-to-learn examples (well-recognized in supervised learning as examples around the decision boundary), which are essential in supervised learning, contribute minimally in unsupervised settings. In this paper, perhaps surprisingly, we find that the direct removal of difficult-to-learn examples, although reduces the sample size, can boost the downstream classification performance of contrastive learning. To uncover the reasons behind this, we develop a theoretical framework modeling the similarity between different pairs of samples. Guided by this theoretical framework, we conduct a thorough theoretical analysis revealing that the presence of difficult-to-learn examples negatively affects the generalization of contrastive learning. Furthermore, we demonstrate that the removal of these examples, and techniques such as margin tuning and temperature scaling can enhance its generalization bounds, thereby improving performance.\nEmpirically, we propose a simple and efficient mechanism for selecting difficult-to-learn examples and validate the effectiveness of the aforementioned methods, which substantiates the reliability of our proposed theoretical framework."
    },
    {
        "title": "Efficient-vDiT: Efficient Video Diffusion Transformers With Attention Tile",
        "link_suffix": "/forum?id=2ezRxhlAxJ",
        "link": "https://openreview.net/forum?id=2ezRxhlAxJ",
        "pdf_link": "https://openreview.net/pdf?id=2ezRxhlAxJ",
        "keywords": "Efficient inference, video generation, diffusion, Transformer",
        "abstract": "Despite the promise of synthesizing high-fidelity videos, Diffusion Transformers (DiTs) with 3D full attention suffer from expensive inference due to the complexity of attention computation and numerous sampling steps. For example, the popular Open-Sora-Plan model consumes more than 9 minutes for generating a single video of 29 frames. This paper addresses the inefficiency issue from two aspects: 1) Prune the 3D full attention based on the redundancy within video data; We identify a prevalent tile-style repetitive pattern in the 3D attention maps for video data, and advocate a new family of sparse 3D attention that holds a linear complexity w.r.t. the number of video frames. 2) Shorten the sampling process based on multi-step consistency distillation; We split the entire sampling trajectory into several segments and perform consistency distillation within each one to activate few-step generation capacities. We further devise a three-stage training pipeline to conjoin the low-complexity attention and few-step generation capacities. Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into an efficient one that is 7.4x \u22127.8x faster for 29 and 93 frames 720p video generation with less than 1% performance loss in VBench. In addition, we demonstrate that our approach is amenable to distributed inference, achieving an additional 3.91x speedup when running on 4 GPUs with sequence parallelism."
    }
]