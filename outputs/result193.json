[{"title": "UKAN: UNBOUNDED KOLMOGOROV-ARNOLD NETWORKS", "link_suffix": "/forum?id=wj4Az2454x", "link": "https://openreview.net/forum?id=wj4Az2454x", "pdf_link": "https://openreview.net/pdf?id=wj4Az2454x", "keywords": "KAN, Acceleration, Unbounded KAN, Grid Free, Function Approximation", "abstract": "We present Unbounded Kolmogorov-Arnold Networks (UKANs), a novel algorithm that eliminates the need for bounded grids in traditional Kolmogorov-Arnold Networks (KANs). The key innovation is a coefficient generator (CG) model that dynamically produces B-spline coefficients, operating on an infinite symmetric grid. UKANs integrate multilayer-perceptrons with KANs, using positional encoding of grid groups as input to the CG model. This approach enables function approximation on unbounded domains without data normalization. Additionally, to reduce UKAN and KAN computational cost, we introduce a GPU-accelerated library that reduces B-spline evaluation complexity by a factor of $\\mathcal{O}(\\text{grid size})$ compared to existing libraries, enabling efficient large-scale learning. Our experiments on regression, classification, and generative tasks demonstrate UKANs' effectiveness, while benchmarks confirm superior memory and computational efficiency compared to existing methods. This work advances function approximation techniques, offering a flexible solution for complex, large-scale learning problems.", "title_embedding_index": 9600, "title_abs_embedding_index": 9625}, {"title": "Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting", "link_suffix": "/forum?id=jDsmB4o5S0", "link": "https://openreview.net/forum?id=jDsmB4o5S0", "pdf_link": "https://openreview.net/pdf?id=jDsmB4o5S0", "keywords": "Natural Language Processing, In-Context Learning, In-Weights Learning, Active Forgetting", "abstract": "Language models have the ability to perform in-context learning (ICL), allowing them to flexibly adapt their behavior based on context. This contrasts with in-weights learning (IWL), where memorized information is  encoded in model parameters from iterated observations of the data (e.g., common sayings). An ideal model should be able to maintain both of these abilities. Despite their apparent ability to learn in-context, language models are known to struggle when faced with unseen or rarely seen tokens (Land & Bartolo, 2024). Hence, we study $\\textbf{structural in-context learning}$, which we define as the ability of a model to execute in-context learning on arbitrary novel tokens \u2013 so called because the model must generalize on the basis of e.g. sentence structure or task structure, rather than content encoded in token embeddings. We study structural in-context algorithms on both synthetic and natural tasks using both toy models and MultiBERT models (Sellam et al., 2021). We find that structural ICL appears before quickly disappearing early in LM pretraining. While it has been shown that ICL can diminish during training (Singh et al., 2023), we find that prior work does not account for structural ICL. Building on the Chen et al. (2024) 's active forgetting method used to help models learn new languages, we introduce a pretraining method that can modulate the preference for true structural ICL and IWL. Importantly, this allows us to induce a $\\textit{dual process strategy}$ where in-context and in-weights solutions coexist within a single model.", "title_embedding_index": 9601, "title_abs_embedding_index": 9626}, {"title": "Sketched Adaptive Federated Deep Learning: A Sharp Convergence Analysis", "link_suffix": "/forum?id=L9eEfwwUwU", "link": "https://openreview.net/forum?id=L9eEfwwUwU", "pdf_link": "https://openreview.net/pdf?id=L9eEfwwUwU", "keywords": "Federated Learning, Sketching Algorithm, Deep Learning Optimization", "abstract": "Combining gradient sketching methods (e.g., CountSketch,  quantization) and adaptive optimizers (e.g., Adam, AMSGrad) is a desirable goal in federated learning (FL), with potential benefits on both fewer communication rounds and smaller per-round communication. In spite of the preliminary empirical success of sketched adaptive methods, existing convergence analyses show the communication cost to have a linear dependence on the ambient dimension, i.e., number of parameters, which is prohibitively high for modern deep learning models.In this work, we introduce specific sketched adaptive federated learning (SAFL) algorithms and, as our main contribution, provide theoretical convergence analyses in different FL settings with guarantees on communication cost depending only logarithmically (instead of linearly) on the ambient dimension. Unlike existing analyses, we show that the entry-wise sketching noise existent in the preconditioners and the first moments of SAFL can be implicitly addressed by leveraging the recently-popularized anisotropic curvatures in deep learning losses, e.g., fast decaying loss Hessian eigen-values. \nIn the i.i.d. client setting of FL, we show that SAFL achieves $O(1/\\sqrt{T})$ convergence, and $O(1/T)$ convergence near initialization. In the non-i.i.d. client setting, where non-adaptive methods lack convergence guarantees, we show that SACFL (SAFL with clipping) algorithms can provably converge in spite of the additional heavy-tailed noise. Our theoretical claims are supported by empirical studies on vision and language tasks, and in both fine-tuning and training-from-scratch regimes. Surprisingly, as a by-product of our analysis, the proposed SAFL methods are competitive with the state-of-the-art communication-efficient federated learning algorithms based on error feedback.", "title_embedding_index": 9602, "title_abs_embedding_index": 9627}, {"title": "Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models", "link_suffix": "/forum?id=HuNoNfiQqH", "link": "https://openreview.net/forum?id=HuNoNfiQqH", "pdf_link": "https://openreview.net/pdf?id=HuNoNfiQqH", "keywords": "jailbreaks; activation engineering; alignment", "abstract": "Conversational Large Language Models are trained to refuse to answer harmful questions. However, emergent jailbreaking techniques can still elicit unsafe outputs, presenting an ongoing challenge for model alignment. To better understand how different jailbreak types circumvent safeguards, this paper analyses model activations on different jailbreak inputs. We find that it is possible to extract a jailbreak vector from a single class of jailbreaks that works to mitigate jailbreak effectiveness from other classes. This may indicate that different kinds of effective jailbreaks operate via similar internal mechanisms. We investigate a potential common mechanism of harmfulness feature suppression, and provide evidence for its existence by looking at the harmfulness vector component. These findings offer actionable insights for developing more robust jailbreak countermeasures and lay the groundwork for a deeper, mechanistic understanding of jailbreak dynamics in language models.", "title_embedding_index": 9603, "title_abs_embedding_index": 9628}, {"title": "Reweighting Local Mimina with Tilted SAM", "link_suffix": "/forum?id=nXTpz8pTHK", "link": "https://openreview.net/forum?id=nXTpz8pTHK", "pdf_link": "https://openreview.net/pdf?id=nXTpz8pTHK", "keywords": "sharpness-aware optimization, exponential tilting, generalization", "abstract": "Sharpness-Aware Minimization (SAM) has been demonstrated to improve the generalization performance of overparameterized models by seeking flat minima on the loss landscape through optimizing model parameters that incur the largest loss within a neighborhood. \nNevertheless, such min-max formulations are computationally challenging especially when the problem is highly non-convex. Additionally, focusing only on the worst-case local solution while ignoring potentially many other local solutions may be suboptimal when searching for flat minima. In this work, we propose Tilted SAM (TSAM), a generalization of SAM inspired by exponential tilting that effectively assigns higher priority to local solutions that are flatter and that incur larger losses. TSAM is parameterized by a tilt hyperparameter $t$ and reduces to SAM as $t$ approaches infinity. We prove that (1) the TSAM objective is smoother than SAM and thus easier to optimize; and (2) TSAM explicitly favors flatter minima as $t$ increases. This is desirable as flatter minima could have better generalization properties for certain tasks. We develop algorithms motivated by the discretization of Hamiltonian dynamics to solve TSAM. Empirically, TSAM arrives at  flatter local minima and results in superior test performance than the baselines of SAM and ERM across a range of image and text tasks.", "title_embedding_index": 9604, "title_abs_embedding_index": 9629}, {"title": "Few-shot Species Range Estimation", "link_suffix": "/forum?id=QaQrWKPWdG", "link": "https://openreview.net/forum?id=QaQrWKPWdG", "pdf_link": "https://openreview.net/pdf?id=QaQrWKPWdG", "keywords": "species distribution modeling, SDM, spatial implicit neural representation, SINR, low-shot learning, few-shot learning", "abstract": "Understanding where a particular species can or cannot be found is crucial for ecological research and conservation efforts. \nBy mapping the spatial ranges of all species on Earth, we could obtain deeper insights into how global biodiversity is affected by climate change and habitat loss. \nHowever, accurate range estimates are available for a relatively small proportion of known species. \nFor most species, we have only have a few prior observations indicating the locations where they have been previously recorded. \nIn this work we address the challenge of training with limited observations by developing a new approach for few-shot species range estimation. \nDuring inference, our model takes a set of spatial coordinates as input, along with optional metadata such as text, and outputs a species encoding that can be used to predict the range of a previously unseen species in feed-forward manner. \nWe validate our method on two challenging benchmarks, where we obtain state-of-the-art performance in predicting the ranges of unseen species, in a fraction of the compute time, compared to recent alternative approaches.", "title_embedding_index": 9605, "title_abs_embedding_index": 9630}, {"title": "Low Variance: A Bottleneck in Diffusion-Based Graph Imputation", "link_suffix": "/forum?id=4dhTYe5pjD", "link": "https://openreview.net/forum?id=4dhTYe5pjD", "pdf_link": "https://openreview.net/pdf?id=4dhTYe5pjD", "keywords": "diffusion-based imputation, missing features, graph neural networks", "abstract": "In this paper, we tackle learning tasks on graphs with missing features, improving the applicability of graph neural networks to real-world graph-structured data. Existing imputation methods based upon graph diffusion produce channels that have nearly identical values within each channel, and these low-variance channels contribute very little to performance in graph learning tasks. To prevent diffusion-based imputation from producing low-variance channels, we introduce synthetic features that address the cause of the production, thereby increasing variance in low-variance channels. Since the synthetic features prevent diffusion-based imputation models from generating meaningless feature values shared across all nodes, our synthetic feature propagation design prevents significant performance degradation, even under extreme missing rates. Extensive experiments demonstrate the effectiveness of our scheme across various graph learning tasks with missing features, ranging from low to extremely high missing rates. Moreover, we provide empirical evidence and theoretical proof that validate the low-variance problem.", "title_embedding_index": 9606, "title_abs_embedding_index": 9631}, {"title": "Near-Exact Privacy Amplification for Matrix Mechanisms", "link_suffix": "/forum?id=txV4dNeusx", "link": "https://openreview.net/forum?id=txV4dNeusx", "pdf_link": "https://openreview.net/pdf?id=txV4dNeusx", "keywords": "differential privacy, privacy amplification, privacy accounting, DP-FTRL, correlated noise", "abstract": "We study the problem of computing the privacy parameters for DP machine learning when using privacy amplification via random batching and noise correlated across rounds via a correlation matrix $\\textbf{C}$ (i.e., the matrix mechanism). Past work on this problem either only applied to banded $\\textbf{C}$, or gave loose privacy parameters. In this work, we give a framework for computing near-exact privacy parameters for any lower-triangular, non-negative $\\textbf{C}$. Our framework allows us to optimize the correlation matrix $\\textbf{C}$ while accounting for amplification, whereas past work could not. Empirically, we show this lets us achieve smaller RMSE on prefix sums than the previous state-of-the-art (SOTA). We also show that we can improve on the SOTA performance on deep learning tasks. Our two main technical tools are (i) using Monte Carlo accounting to bypass composition, which was the main technical challenge for past work, and (ii) a ``balls-in-bins'' batching scheme that enables easy privacy analysis and is closer to practical random batching than Poisson sampling.", "title_embedding_index": 9607, "title_abs_embedding_index": 9632}, {"title": "LOCAL: Latent Orthonormal Contrastive Learning for Paired Images", "link_suffix": "/forum?id=3RrNfVWodl", "link": "https://openreview.net/forum?id=3RrNfVWodl", "pdf_link": "https://openreview.net/pdf?id=3RrNfVWodl", "keywords": "paired images, representation learning, supervised contrastive learning", "abstract": "Classification with comparative paired inputs, such as pre- and post-disaster satellite images, distinguishes classes of samples by encompassing dual feature sets that individually characterize a sample. Representation learning from comparative nature of the inputs calls for not only recognizing invariant patterns shared across all inputs but also effectively differentiating the contrastive attributes present between each pair of inputs.  Supervised Contrastive Learning (SCL) aims to learn representation that maximally separates different classes and condenses within individual classes, thereby attaining an adversarial equilibrium. However, this equilibrium typically relies on the assumption of balanced data and large batch sizes for sufficient negative sampling. These issues are exacerbated when applied to paired satellite images due to increased computational load, high-resolution data, and severe class imbalance. To address these challenges, we introduce Latent Orthonormal Contrastive Learning (LOCAL), an approach that optimizes class representations in an orthonormal fashion. By learning each class to a unique, orthogonal plane in the embedding space, LOCAL is efficient with smaller batch sizes, provably effective regardless of class size imbalance, and yields more discriminative information between pairs of inputs via a feature correlation module. Experimental results on  paired image data demonstrate superior performance of LOCAL over SCL, offering a powerful alternative approach for paired input analysis.", "title_embedding_index": 9608, "title_abs_embedding_index": 9633}, {"title": "PROVABLY EFFICIENT FEDERATED ACTIVE MULTI-TASK REPRESENTATION LEARNING", "link_suffix": "/forum?id=jYJq2gQb7J", "link": "https://openreview.net/forum?id=jYJq2gQb7J", "pdf_link": "https://openreview.net/pdf?id=jYJq2gQb7J", "keywords": "Representation learning, alternating gradient descent and minimization, active learning, multi-task learning", "abstract": "Multi-task learning is an emerging machine learning paradigm that integrates data from multiple sources, harnessing task similarities to enhance overall model performance. The application of multi-task learning to real-world settings is hindered due to data scarcity, along with challenges related to scalability and computational resources. To address this challenge, we develop a fast and sample-efficient approach for multi-task active learning when the amount of data from source tasks and target tasks is limited. By leveraging the techniques from active learning, we propose an adaptive sampling-based alternating projected gradient descent (GD) and minimization algorithm that iteratively estimates the relevance of each source task to the target task and samples from each source task based on the estimated relevance. We present the convergence guarantee of our algorithm and the sample complexity of our approach. We evaluated the effectiveness of our algorithm using numerical experiments and compared it empirically against four benchmark algorithms using synthetic and real datasets.", "title_embedding_index": 9609, "title_abs_embedding_index": 9634}, {"title": "Zero redundancy distributed learning with differential privacy", "link_suffix": "/forum?id=Xq12wsoNux", "link": "https://openreview.net/forum?id=Xq12wsoNux", "pdf_link": "https://openreview.net/pdf?id=Xq12wsoNux", "keywords": "deep learning, differential privacy, distributed learning, system design", "abstract": "Deep learning using large models have achieved great success in a wide range of domains. However, training these models on billions of parameters is very challenging in terms of the training speed, memory cost, and communication efficiency, especially under the privacy-preserving regime with differential privacy (DP). On the one hand, DP optimization has comparable efficiency to the standard non-private optimization on a single GPU, but on multiple GPUs, existing DP distributed learning (such as pipeline parallel) has suffered from significantly worse efficiency. On the other hand, the Zero Redundancy Optimizer (ZeRO) is a state-of-the-art solution to the standard distributed learning, exhibiting excellent training efficiency on large models, but to work compatibly with DP is technically complicated. In this work, we develop a new systematic solution, DP-ZeRO, (I) to scale up the trainable DP model size, e.g. to GPT-100B, (II) to obtain the same computation and communication efficiency as the standard ZeRO, and (III) to enable mixed-precision DP training. Our DP-ZeRO, like the standard ZeRO, has the potential to train models with arbitrary size and is evaluated on the world's largest DP models in terms of the number of trainable parameters.", "title_embedding_index": 9610, "title_abs_embedding_index": 9635}, {"title": "Tensor-GaLore: Memory-Efficient Training via Gradient Tensor Decomposition", "link_suffix": "/forum?id=C85eSjKenO", "link": "https://openreview.net/forum?id=C85eSjKenO", "pdf_link": "https://openreview.net/pdf?id=C85eSjKenO", "keywords": "neural operators, PDE, optimization, pre-training, Large scale training, AI4Science", "abstract": "We present Tensor-GaLore, a novel method for efficient training of neural networks with higher-order tensor weights. Many models, particularly those used in scientific computing, employ tensor-parameterized layers to capture complex, multidimensional relationships. When scaling these methods to high-resolution problems makes memory usage grow intractably, and matrix based optimization methods lead to suboptimal performance and compression. We propose to work directly in the high-order space of the complex tensor parameter space using a tensor factorization of the gradients during optimization. We showcase its effectiveness on Fourier Neural Operators (FNOs), a class of models crucial for solving partial differential equations (PDE). Across various PDE tasks like the Navier Stokes and Darcy Flow equations, Tensor-GaLore achieves substantial memory savings, reducing optimizer memory usage by up to 75%. These substantial memory savings across AI for science demonstrate Tensor-GaLore's potential.", "title_embedding_index": 9611, "title_abs_embedding_index": 9636}, {"title": "Conv-Basis: A New Paradigm for Efficient Attention Inference and Gradient Computation in Transformers", "link_suffix": "/forum?id=yD7oAhFEtD", "link": "https://openreview.net/forum?id=yD7oAhFEtD", "pdf_link": "https://openreview.net/pdf?id=yD7oAhFEtD", "keywords": "Attention Acceleration, Fast Fourier Transforms, Gradient Computation", "abstract": "The self-attention mechanism is the key to the success of transformers in recent Large Language Models (LLMs). However, the quadratic computational cost $O(n^2)$ in the input sequence length $n$ is a notorious obstacle for further improvement and scalability in longer contexts. In this work, we leverage the convolution-like structure of attention matrices to develop an efficient approximation method for attention computation using convolution matrices. We propose a $\\mathsf{conv}$ basis system, analogous to the rank basis, and show that any lower triangular matrix can always be decomposed as a sum of structured convolution matrices in this basis. We then design a fast algorithm to approximate the attention matrix via a sum of such $k$ convolution matrices. This allows us to compute the attention {\\it inference} via Fast Fourier Transforms (FFT) in $O(knd \\log n)$ time, where $d$ is the hidden dimension, and thus achieve almost linear time $n^{1+o(1)}$ in the practical scenario where $kd = n^{o(1)}$. Furthermore, the attention {\\it training forward} and {\\it backward gradient} can be computed in $n^{1+o(1)}$ as well. We provide theoretical guarantees on the run time and approximation error and conduct preliminary experiments to evaluate its effectiveness. We hope our new paradigm for accelerating attention computation in transformer models can help their application to longer contexts.", "title_embedding_index": 9612, "title_abs_embedding_index": 9637}, {"title": "Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification", "link_suffix": "/forum?id=oI5tZaWkF9", "link": "https://openreview.net/forum?id=oI5tZaWkF9", "pdf_link": "https://openreview.net/pdf?id=oI5tZaWkF9", "keywords": "data weighing, data augmentation, distillation, data-efficient training, NLP in resource-constrained settings, fine-tuning, weighted loss", "abstract": "Synthetic data augmentation via large language models (LLMs) allows researchers to leverage additional training data, thus enhancing the performance of downstream tasks, especially when real-world data is scarce. However, the generated data can deviate from the real-world data, and this misalignment can bring deficient outcomes while applying the trained model to applications. Therefore, we proposed efficient weighted-loss approaches to align synthetic data with real-world distribution by emphasizing high-quality and diversified data generated by LLMs with using merely a little real-world data. We empirically assessed the effectiveness of our method on multiple text classification tasks, and the results showed leveraging our approaches on a BERT-level model robustly outperformed standard cross-entropy and other data weighting approaches, providing potential solutions to effectively leveraging synthetic data from any suitable data generator for model training.", "title_embedding_index": 9613, "title_abs_embedding_index": 9638}, {"title": "To Err is Machine: Vulnerability Detection Challenges LLM Reasoning", "link_suffix": "/forum?id=Q0mp2yBvb4", "link": "https://openreview.net/forum?id=Q0mp2yBvb4", "pdf_link": "https://openreview.net/pdf?id=Q0mp2yBvb4", "keywords": "deep learning, security, vulnerability detection", "abstract": "In this paper, we present a challenging code reasoning task: vulnerability detection.\nLarge Language Models (LLMs) have shown promising results in natural-language\nand math reasoning, but state-of-the-art (SOTA) models reported only 54.5%\nBalanced Accuracy in our vulnerability detection evaluation, even those models\npre-trained on large amounts of source code. Our error analysis on LLM responses\nshows that the models struggle to reason about the code semantics relevant to\nidentifying vulnerabilities, especially subtle semantic differences caused by small\ntextual changes. We explored prominent models and training settings to understand\ntheir effects on vulnerability detection performance \u2014 including better prompts,\nlarger models, more pre-training data, and fine-tuning \u2014 but none led to significant\nimprovements. This raises the question of whether simply scaling training data and\nmodel size will allow us to \u201csolve\u201d complex code reasoning tasks like vulnerability\ndetection, or if a fundamental shift in modeling and training techniques is required.\nWe also explored adding domain knowledge to prompts; although it helped certain\nmodels understand some code semantics, vulnerability detection requires multi-\nstep reasoning, and these models still failed in steps, such as reasoning about\nvariable relations. Our results suggest that new models, new training methods, or\nmore execution-specific pretraining data may be needed to conquer vulnerability\ndetection. We speculate that auto-regressive pre-training on source code may not\neffectively extract code semantics, especially on the current pretraining mixtures,\nin which execution data is scarce. Success on vulnerability detection as a code\nreasoning task can benefit many areas of software engineering such as debugging,\ntest input generation, and program repair. Our code and data are available athttps://figshare.com/s/78fe02e56e09ec49300b.", "title_embedding_index": 9614, "title_abs_embedding_index": 9639}, {"title": "Towards Building Reliable Conditional Diffusion Models for Protein Generation", "link_suffix": "/forum?id=fM432E7l5w", "link": "https://openreview.net/forum?id=fM432E7l5w", "pdf_link": "https://openreview.net/pdf?id=fM432E7l5w", "keywords": "Protein Generation", "abstract": "Generating novel and functional protein sequences is critical to a wide\nrange of applications in biology. Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks. However, reliable generations of protein remain an open research question in de novo protein design, especially when it comes to conditional diffusion models. Considering  the biological function of a protein is determined\nby multi-level structures, we propose a novel multi-level conditional diffusion model that integrates both sequence-based\nand structure-based information for efficient end-to-end protein design guided by\nspecified functions. By generating representations at different levels simultaneously, our framework can effectively model the inherent hierarchical relations between different levels, resulting in an informative and\ndiscriminative representation of the generated protein. We also propose a Protein-MMD, a new reliable evaluation metric, to evaluate the quality of generated protein with conditional diffusion models. Our new metric is able to capture both distributional and functional similarities between real and generated protein sequences while ensuring conditional consistency. We experiment with standard datasets and the\nresults on protein generation tasks demonstrate the efficacy of the proposed generation framework and evaluation metric.", "title_embedding_index": 9615, "title_abs_embedding_index": 9640}, {"title": "Learning a Bi-directional Driving Data Generator via Large Multi-modal Model Tuning", "link_suffix": "/forum?id=2UozyR49ZB", "link": "https://openreview.net/forum?id=2UozyR49ZB", "pdf_link": "https://openreview.net/pdf?id=2UozyR49ZB", "keywords": "multi-modality, synthetic data generation, auto-annotation, driving, LLM applications", "abstract": "Understanding human driving behaviors is crucial for developing a reliable vehicle and transportation system. Yet, data for learning these behaviors is scarce and must be carefully labeled with events, causes, and consequences. Such data may be more difficult to obtain in rare driving domains, such as in high-performance multi-car racing. While large language models (LLMs) show promise in interpreting driving behaviors, the integration of multi-modal inputs (e.g., language, trajectory, and more) and generation of multi-modal output in low-data regimes remains under-explored. In this paper, we introduce Bi-Gen: a Bi-directional Driving Data Generator, Bi-Gen is a bi-directional  multi-modal model that connects a trained encoder-decoder architecture with a pre-trained LLM, enabling both auto-annotation and generation of human driving behaviors. Our experiments show that Bi-Gen, despite its smaller size, matches the performance of much larger models like GPT-4o in annotating driving data. Additionally, Bi-Gen generates diverse, human-like driving behaviors, offering a valuable tool for synthetic data generation in resource-constrained settings. Taken together, our experiments are a significant step towards applying LLMs to complex, multi-agent driving data.", "title_embedding_index": 9616, "title_abs_embedding_index": 9641}, {"title": "Test-Time Alignment via Hypothesis Reweighting", "link_suffix": "/forum?id=8HQS1X2AK4", "link": "https://openreview.net/forum?id=8HQS1X2AK4", "pdf_link": "https://openreview.net/pdf?id=8HQS1X2AK4", "keywords": "Personalization, few-shot adaptation, ambiguity, efficient ensembles", "abstract": "Large pretrained models often struggle with underspecified tasks---situations where the training data does not fully define the desired behavior. For example, chatbots must handle diverse and often conflicting user preferences, requiring adaptability to various user needs. We propose a novel framework to address the general challenge of aligning models to test-time user intent, which is rarely fully specified during training. Our approach involves training an efficient ensemble, i.e., a single neural network with multiple prediction heads, each representing a different function consistent with the training data. Our main contribution is HyRe, a simple adaptation technique that dynamically reweights ensemble members at test time using a small set of labeled examples from the target distribution, which can be labeled in advance or actively queried from a larger unlabeled pool. By leveraging recent advances in scalable ensemble training, our method scales to large pretrained models, with computational costs comparable to fine-tuning a single model. We empirically validate HyRe in several underspecified scenarios, including personalization tasks and settings with distribution shifts. Additionally, with just five preference pairs from each target distribution, the same ensemble adapted via HyRe outperforms the prior state-of-the-art 2B-parameter reward model accuracy across 18 evaluation distributions.", "title_embedding_index": 9617, "title_abs_embedding_index": 9642}, {"title": "Improving Semantic Understanding in Speech Language Models via Brain-tuning", "link_suffix": "/forum?id=KL8Sm4xRn7", "link": "https://openreview.net/forum?id=KL8Sm4xRn7", "pdf_link": "https://openreview.net/pdf?id=KL8Sm4xRn7", "keywords": "fMRI, Speech Models, Speech Recognition, Alignment, Brain Alignment, Cognitive Neuroscience, Encoding Models, Transformers", "abstract": "Speech language models align with human brain responses to natural language to an impressive degree. However, current models rely heavily on low-level speech features, indicating they lack brain-relevant semantics which limits their utility as model organisms of semantic processing in the brain. In this work, we address this limitation by inducing brain-relevant bias directly into the models via fine-tuning with fMRI recordings of people listening to natural stories--a process we name brain-tuning. After testing it on 3 different pretrained backbones, we show that brain-tuning not only improves overall alignment with new brain recordings in semantic language regions but also reduces the reliance on low-level speech features for this alignment. Excitingly, we further show that brain-tuning leads to 1) consistent improvements in performance on a range of downstream tasks and 2) a representational space with increased semantic preference. Our results provide converging evidence, for the first time, that incorporating brain signals into the training of language models improves the models\u2019 semantic understanding.", "title_embedding_index": 9618, "title_abs_embedding_index": 9643}, {"title": "BitStack: Fine-Grained Size Control for Compressed Large Language Models in Variable Memory Environments", "link_suffix": "/forum?id=lBntjGbyv0", "link": "https://openreview.net/forum?id=lBntjGbyv0", "pdf_link": "https://openreview.net/pdf?id=lBntjGbyv0", "keywords": "large language models; compression; decomposition", "abstract": "Large language models (LLMs) have revolutionized numerous applications, yet their deployment remains challenged by memory constraints on local devices. While scaling laws have enhanced LLM capabilities, the primary bottleneck has shifted from $\\textit{capability}$ to $\\textit{availability}$, emphasizing the need for efficient memory management. Traditional compression methods, such as quantization, often require predefined compression ratios and separate compression processes for each setting, complicating deployment in variable memory environments. In this paper, we introduce $\\textbf{BitStack}$, a novel, training-free weight compression approach that enables megabyte-level trade-offs between memory usage and model performance. By leveraging weight decomposition, BitStack can dynamically adjust the model size with minimal transmission between running memory and storage devices. Our approach iteratively decomposes weight matrices while considering the significance of each parameter, resulting in an approximately 1-bit per parameter residual block in each decomposition iteration. These blocks are sorted and stacked in storage as basic transmission units, with different quantities loaded based on current memory availability. Extensive experiments across a wide range of tasks demonstrate that, despite offering fine-grained size control, BitStack consistently matches or surpasses strong quantization baselines, particularly at extreme compression ratios. To the best of our knowledge, this is the first decomposition-based method that effectively bridges the gap to practical compression techniques like quantization. Code will be open-sourced after the review period.", "title_embedding_index": 9619, "title_abs_embedding_index": 9644}, {"title": "Perlin Noise for Exploration in Reinforcement Learning", "link_suffix": "/forum?id=otYAwaTDk1", "link": "https://openreview.net/forum?id=otYAwaTDk1", "pdf_link": "https://openreview.net/pdf?id=otYAwaTDk1", "keywords": "Reinforcement Learning, Exploration Strategies, Perlin Noise, Policy Optimization, Structured Exploration", "abstract": "Reinforcement Learning (RL) enables agents to solve tasks by autonomously acquiring policies by interacting with the environment receiving sparse or noisy feedback in the form of a reward. However, achieving successful optimization in RL requires efficient exploration, which remains a significant challenge, particularly in continuous action spaces. Existing exploration techniques often exhibit limited state-space reach and fail to overcome local optima, resulting in suboptimal policies. Additionally, these techniques can cause erratic movements, posing risks when applied to real-world robots.\nIn this work, we introduce a novel exploration strategy leveraging Perlin Noise, a gradient noise function that generates smooth, continuous disturbances, thus enhancing the agent's performance by promoting structured exploration and fluid motions. We quantitatively demonstrate the benefits of our approach compared to state-of-the-art methods, showing that it outperforms both unstructured and structured techniques in thorough experimental evaluations.", "title_embedding_index": 9620, "title_abs_embedding_index": 9645}, {"title": "ACES: Automatic Cohort Extraction System for Event-Stream Datasets", "link_suffix": "/forum?id=P4XmKjXTrM", "link": "https://openreview.net/forum?id=P4XmKjXTrM", "pdf_link": "https://openreview.net/pdf?id=P4XmKjXTrM", "keywords": "Automatic Task Specification, Cohort Extraction, Electronic Health Records, Open Source Software, Benchmarks, Datasets", "abstract": "Reproducibility remains a significant challenge in machine learning (ML) for healthcare. Datasets, model pipelines, and even task/cohort definitions are often private in this field, leading to a significant barrier in sharing, iterating, and understanding ML results on electronic health record (EHR) datasets. This paper addresses a significant part of this problem by introducing the Automatic Cohort Extraction System (ACES) for event-stream data. This library is designed to simultaneously simplify the development of task/cohorts for ML in healthcare and also enable the reproduction of these cohorts, both at an exact level for single datasets and at a conceptual level across datasets. To accomplish this, ACES provides (1) a highly intuitive and expressive configuration language for defining both dataset-specific concepts and dataset-agnostic inclusion/exclusion criteria, and (2) a pipeline to automatically extract patient records that meet these defined criteria from real-world data. ACES can be automatically applied to any dataset in either the Medical Event Data Standard (MEDS) or EventStreamGPT (ESGPT) formats, or toanydataset in which the necessary task-specific predicates can be extracted in an event-stream form. ACES has the potential to significantly lower the barrier to entry for defining ML tasks that learn representations, redefine the way researchers interact with EHR datasets, and significantly improve the state of reproducibility for ML studies in this modality.", "title_embedding_index": 9621, "title_abs_embedding_index": 9646}, {"title": "Memorization and the Orders of Loss: A Learning Dynamics Perspective", "link_suffix": "/forum?id=lTh7DEJV5W", "link": "https://openreview.net/forum?id=lTh7DEJV5W", "pdf_link": "https://openreview.net/pdf?id=lTh7DEJV5W", "keywords": "Memorization, Mislabelled Detection, Learning Dynamics", "abstract": "Deep learning has become the de facto approach in nearly all learning tasks.\nIt has been observed that deep models tend to memorize and sometimes overfit data, which can lead to compromises in performance, privacy, and other critical metrics.\nIn this paper, we explore the theoretical foundations that connect memorization to various orders of sample loss, i.e., sample loss, sample loss gradient, and sample loss curvature, focusing on learning dynamics to understand what and how these models memorize. \nTo this end, we introduce two proxies for memorization: Cumulative Sample Loss (CSL) and Cumulative Sample Gradient (CSG).\nCSL represents the accumulated loss of a sample throughout training, while CSG is the gradient with respect to the input, aggregated over the training process. CSL and CSG exhibit remarkable similarity to stability-based memorization, as evidenced by considerably high cosine similarity scores. We delve into the theory behind these results, demonstrating that CSL and CSG represent the bounds for stability-based memorization and learning time. Additionally, we extend this framework to  include sample loss curvature and connect the three orders, namely, \nsample loss, sample loss gradient, and sample loss curvature, to learning time and memorization. \nThe proposed proxy, CSL, is four orders of magnitude less computationally expensive than the stability-based method and can be obtained with zero additional overhead during training. We demonstrate the practical utility of the proposed proxies in identifying mislabeled samples and detecting duplicates where our metric achieves state-of-the-art performance. Thus, this paper provides a new tool for analyzing data as it scales in size, making it an important resource in practical applications.", "title_embedding_index": 9622, "title_abs_embedding_index": 9647}, {"title": "Inference-Friendly Models With MixAttention", "link_suffix": "/forum?id=2DD4AXOAZ8", "link": "https://openreview.net/forum?id=2DD4AXOAZ8", "pdf_link": "https://openreview.net/pdf?id=2DD4AXOAZ8", "keywords": "language models, inference, transformers, architecture", "abstract": "The size of the key-value (KV) cache plays a critical role in determining both the maximum context length and the number of concurrent requests supported during inference in modern language models. The KV cache size grows proportionally with the number of attention heads and the tokens processed, leading to increased memory consumption and slower inference for long inputs. In this work, we explore the use of MixAttention, a model architecture modification closely related to a blog published by Character.AI. MixAttention combines sliding window attention, where only a small subset of recent tokens is stored in the KV cache, with KV cache sharing across layers. Our experiments demonstrate that MixAttention significantly reduces memory usage and improves inference speed without sacrificing model performance in both short and long-context tasks. We also explore various configurations of this architecture, identifying those that maintain quality across evaluation metrics while optimizing resource efficiency.", "title_embedding_index": 9623, "title_abs_embedding_index": 9648}, {"title": "Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits", "link_suffix": "/forum?id=rakhNY32vw", "link": "https://openreview.net/forum?id=rakhNY32vw", "pdf_link": "https://openreview.net/pdf?id=rakhNY32vw", "keywords": "contextual linear bandit, online batch learning", "abstract": "We study the optimal batch-regret tradeoff for batch linear contextual bandits. For this problem, we design batch learning algorithms and prove that they achieve the optimal regret bounds (up to logarithmic factors) for any batch number $M$, number of actions $K$, time horizon $T$, and dimension $d$. Therefore, we establish the \\emph{full-parameter-range} (almost) optimal batch-regret tradeoff for the batch linear contextual bandit problem.Along our analysis, we also prove a new matrix concentration inequality with dependence on their dynamic upper bounds, which, to the best of our knowledge, is the first of its kind in literature and maybe of independent interest.", "title_embedding_index": 9624, "title_abs_embedding_index": 9649}]