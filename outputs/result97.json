[
    {
        "title": "L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?",
        "link_suffix": "/forum?id=UeHunlny77",
        "link": "https://openreview.net/forum?id=UeHunlny77",
        "pdf_link": "https://openreview.net/pdf?id=UeHunlny77",
        "keywords": "Long-context evaluation benchmark, model faithfulness, citation generation",
        "abstract": "Long-context models (LCMs) have made remarkable strides in recent years, offering users great convenience for handling tasks that involve long context, such as document summarization.\nAs the community increasingly prioritizes the faithfulness of generated results, merely ensuring the accuracy of LCM outputs is insufficient because it is quite challenging for humans to verify the results from the extremely lengthy context.\nYet, although some efforts have been made to assess whether LCMs respond truly based on the context, these works either are limited to specific tasks or heavily rely on external evaluation resources like GPT-4.\nIn this work, we introduce L-CiteEval, a comprehensive multi-task benchmark for long-context understanding with citations, aiming to evaluate both the understanding capability and faithfulness of LCMs.\nL-CiteEval covers 11 tasks from diverse domains, spanning context lengths from 8K to 48K, and provides a fully automated evaluation suite.\nThrough testing with 11 cutting-edge closed-source and open-source LCMs, we find that although these models show minor differences in their generated results, open-source models substantially trail behind their closed-source counterparts in terms of citation accuracy and recall.\nThis suggests that current open-source LCMs are prone to responding based on their inherent knowledge rather than the given context, posing a significant risk to the user experience in practical applications.\nWe also evaluate the RAG approach and observe that RAG can significantly improve the faithfulness of LCMs, albeit with a slight decrease in the generation quality.\nFurthermore, we discover a correlation between the attention mechanisms of LCMs and the citation generation process."
    },
    {
        "title": "Confounder-Free Continual Learning via Recursive Feature Normalization",
        "link_suffix": "/forum?id=ZdqdWiRRwR",
        "link": "https://openreview.net/forum?id=ZdqdWiRRwR",
        "pdf_link": "https://openreview.net/pdf?id=ZdqdWiRRwR",
        "keywords": "deep neural networks, confounders, continual learning, invariant representations, statistical regression",
        "abstract": "Confounders are extraneous variable that affect both the input and the target, resulting in spurious correlations and biased predictions. Learning feature representations that are invariant to confounders remains a significant challenge in continual learning. To remove the influence of confounding variables from intermediate feature representations, we introduce the Recursive Metadata Normalization (R-MDN) layer, which can be integrated into any stage within deep neural networks (DNNs). R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables. Since R-MDN operates on the level of individual examples, it is compatible with state-of-the-art architectures like vision transformers. Our experiments demonstrate that R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time."
    },
    {
        "title": "Evaluating Privacy Risks of Parameter-Efficient Fine-Tuning",
        "link_suffix": "/forum?id=i2Ul8WIQm7",
        "link": "https://openreview.net/forum?id=i2Ul8WIQm7",
        "pdf_link": "https://openreview.net/pdf?id=i2Ul8WIQm7",
        "keywords": "Parameter-efficient fine-tuning, Privacy risk",
        "abstract": "Parameter-efficient fine-tuning (PEFT) is a new paradigm for fine-tuning language models at scale. Unlike standard fine-tuning,\nPEFT adjusts only a small number of parameters, making it more computationally accessible and enabling practitioners to develop personalized services by fine-tuning models on user data. Because the models are trained on user data, this emerging paradigm may attract adversaries who want to extract sensitive information from fine-tuning data. However, their privacy implications have not been well-understood in the literature.In this paper, we study the impact of this new fine-tuning paradigm on privacy. We use an off-the-shelf data extraction attack as a vehicle\nto evaluate the privacy risk of 100 language models: two pre-trained models fine-tuned using five algorithms on two datasets repeated five times with different random seeds. Our main findings are: (1) for practitioners employing PEFT to construct personalized models, the fine-tuned models have lower privacy risks while maintaining reasonable utility; (2) for developers designing new PEFT algorithms,\nwhile safer than standard fine-tuning, certain design choices in the algorithms increase memorization in an unexpected way; and (3) for researchers auditing the privacy of fine-tuned models, employing weak differential privacy is sufficient to mitigate existing data extraction risks without significantly compromising model utility. We hope our work encourages the safe adoption and development of PEFT algorithms in practice, as well as future work on advancing stronger privacy auditing mechanisms."
    },
    {
        "title": "Rethinking Memorization in LLMs: On Learning by Rote vs. with Understanding",
        "link_suffix": "/forum?id=hFQZmKFtlT",
        "link": "https://openreview.net/forum?id=hFQZmKFtlT",
        "pdf_link": "https://openreview.net/pdf?id=hFQZmKFtlT",
        "keywords": "language models, memorization, generalization",
        "abstract": "Understanding whether and to what extent token sequences generated by large language models (LLMs) are the result of regurgitating memorized training data or are based on meaningful learning of the training data's syntax and semantics has many important implications.\nIn order to cleanly measure and disentangle token recollection by rote (memorization) from generation with understanding, we create an experimental framework that is based on training LLMs oversequences generated using formal grammars. Our framework allows us to better understand the interplay between the two types of learning, namely,by rotevs.with understanding. Using our framework we make several striking observations that hold consistently across different open-source model families (Pythia, Llama, and Mistral): (a) we find that the learning types are at odds with each other during training, i.e., rote learning harms understanding and by developing understanding, models forget previously memorized sequences, (b) we find thatentropy of the training datasetsimpacts the ease of learning, with lower entropy datasets being easier to learn with understanding and higher entropy datasets being easier to learn by rote, (c) we highlight the difficulty of determining the type of learning involved in a model based solely on recollecting a training data sequence. Our surprising results have significant downstream implications in the study and usage of LLMs."
    },
    {
        "title": "GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation",
        "link_suffix": "/forum?id=5RUM1aIdok",
        "link": "https://openreview.net/forum?id=5RUM1aIdok",
        "pdf_link": "https://openreview.net/pdf?id=5RUM1aIdok",
        "keywords": "Idea Evaluation, View-graph, Lightweight model, Label propagation, Graph prediction",
        "abstract": "The powerful capabilities of Large Language Models (LLMs) have led to their growing use in evaluating human-generated content, particularly in evaluating research ideas within academic settings. Existing solutions primarily rely on prompt-based LLM methods or fine-tuned lightweight language models for idea evaluation. However, these methods are often unstable and struggle to comprehend the complex semantic information embedded in the ideas, impeding their ability to perform high-quality evaluations. To address the above challenges, we propose $\\texttt{GraphEval}$, a lightweight graph-based LLM framework for idea evaluation. Our insight is that a complex idea can be broken down into comprehensible viewpoint nodes using prompts from small LLMs. These viewpoint nodes can then be linked together through edges created from LLM-based relation extraction and/or BERT similarity scores. The created viewpoint-graph can be used to conveniently propagate scores across view-nodes to improve the robustness of the idea evaluations. In particular, we propose two lightweight graph-based methods for idea evaluation: (1) GraphEval-LP: a training-free label propagation algorithm that propagates evaluation scores from known view-nodes to unknown nodes; (2) GraphEval-GNN: a Graph Neural Networks (GNN) that is trained to predict the evaluation scores given the observed graph with minimal computation resources. Moreover, to overcome LLM's limitation in objectively assessing the novelty of ideas, we further propose a novelty detection model to GraphEval-GNN to enhance its capability in judging idea novelty. Experiments on two datasets show $\\texttt{GraphEval}$ improves F1 scores by at least 14% with low computation and API costs. Additionally, $\\texttt{GraphEval}$ can effectively detect plagiarized ideas."
    },
    {
        "title": "SFi-Former: Sparse Flow induced Attention for Graph Transformer",
        "link_suffix": "/forum?id=bWc6O8QSyp",
        "link": "https://openreview.net/forum?id=bWc6O8QSyp",
        "pdf_link": "https://openreview.net/pdf?id=bWc6O8QSyp",
        "keywords": "Graph Transformer, Sparse learning, Network flow, Optimization",
        "abstract": "Graph Transformers (GTs) have demonstrated superior performance compared to traditional message-passing graph neural networks in many studies, especially in processing graph data with long-range dependencies. However, GTs tend to suffer from weak inductive bias, overfitting and over-globalizing problems due to the dense attention. In this paper, we introduce SFi-attention, a novel attention mechanism designed to learn sparse pattern by minimizing an energy function based on network flows with $\\ell_1$-norm regularization, to relieve those issues caused by dense attention. Furthermore, SFi-Former is accordingly devised which can leverage the sparse attention pattern of SFi-attention to generate sparse network flows beyond adjacency matrix of graph data. Specifically, SFi-Former aggregates features selectively from other nodes through flexible adaptation of the sparse attention, leading to a more robust model. We validate our SFi-Former on various graph datasets, especially those graph data exhibiting long-range dependencies. Experimental results show that our SFi-Former obtains competitive performance on GNN Benchmark datasets and SOTA performance on Long-Range Graph Benchmark (LRGB) datasets. Additionally, our model gives rise to smaller generalization gaps, which indicates that it is less prone to over-fitting."
    },
    {
        "title": "Hyperparameters in Continual Learning: A Reality Check",
        "link_suffix": "/forum?id=8FxELTdwJR",
        "link": "https://openreview.net/forum?id=8FxELTdwJR",
        "pdf_link": "https://openreview.net/pdf?id=8FxELTdwJR",
        "keywords": "Continual Learning, Class Incremental Learning, Evaluation",
        "abstract": "Continual learning (CL) aims to train a model on a sequence of tasks (i.e., a CL scenario) while balancing the trade-off between plasticity (learning new tasks effectively) and stability (retaining prior knowledge). The dominantly adopted conventional evaluation protocol for CL algorithms selects the best hyperparameters within a given scenario and then evaluates the algorithms\nusing these hyperparameters in the same scenario. However, this protocol has significant shortcomings: it overestimates the CL capacity of algorithms and relies on unrealistic hyperparameter tuning, which is not feasible for real-world applications. From the fundamental principles of evaluation in machine learning, we argue that the evaluation of CL algorithms should focus on assessing the generalizability of their CL capacity to unseen scenarios. Based on this, we propose a revised two-phase evaluation protocol consisting of a hyperparameter tuning phase and an evaluation phase. Both phases share the same scenario configuration (e.g., number of tasks) but are generated from different datasets. Hyperparameters of CL algorithms are tuned in the first phase and applied in the second phase to evaluate the algorithms. We apply this protocol to class-incremental learning, both with and without pretrained models. Across more than 8,000 experiments, our results show that most state-of-the-art algorithms fail to replicate their reported performance, highlighting that their CL capacity has been significantly overestimated in the conventional evaluation protocol."
    },
    {
        "title": "Fast Salient Factor Concentration (FSFC) Recurrent Neural Network for Text Classification",
        "link_suffix": "/forum?id=4ymHtDAlBv",
        "link": "https://openreview.net/forum?id=4ymHtDAlBv",
        "pdf_link": "https://openreview.net/pdf?id=4ymHtDAlBv",
        "keywords": "Text Classification, Semantic Information Clustering, Recurrent Neural Network",
        "abstract": "Models based on Recurrent Neural Networks (RNNs) have been widely employed for text classification tasks. Traditional RNNs primarily emphasize long-term memory capabilities. However, this approach does not fully align with human cognitive learning processes, particularly in the context of classification tasks. The human brain typically extracts essential information relevant to the classification categories, disregards irrelevant details, and compresses the input to accelerate decision-making. Inspired by this, we propose a novel architecture, the Fast Salient Factor Concentration (FSFC) RNN, specifically designed for classification tasks. FSFC dynamically clusters and compresses semantic information by leveraging the short-term memory capabilities of recurrent neural networks. Experimental results demonstrate that FSFC achieves performance comparable to existing RNNs, while significantly improving training efficiency in classification tasks. Based on the YelpReviewFull dataset, FSFC improves accuracy by 1.37% over Long Short-Term Memory (LSTM), while reducing training time by 86%. Additionally, we propose a new evaluation metric, E-score, which integrates both accuracy and time efficiency to comprehensively assess the overall performance of each network."
    },
    {
        "title": "Deep ECG-Report Interaction Framework for Cross-Modal Representation Learning",
        "link_suffix": "/forum?id=0quBGOPP5V",
        "link": "https://openreview.net/forum?id=0quBGOPP5V",
        "pdf_link": "https://openreview.net/pdf?id=0quBGOPP5V",
        "keywords": "Multi-modal Representation Learning, ECG signal, Report Generation, Zero-shot Classification",
        "abstract": "Electrocardiogram (ECG) is of great importance for the clinical diagnosis of cardiac conditions. Although existing self-supervised learning methods have obtained great performance on learning representation for ECG-based cardiac conditions classification, the clinical semantics can not be effectively captured. To overcome this limitation, we proposed a $\\textbf{D}$eep $\\textbf{E}$CG-$\\textbf{R}$eport $\\textbf{I}$nteraction ($\\textbf{DERI}$) framework to learn cross-modal representations that contain more clinical semantics. Specifically, we design a novel framework combining multiple alignments and feature reconstructions to learn effective cross-modal representation of the ECG-Report, which fuses the clinical semantics of the report into the learned representation. An RME-module inspired by masked modeling is proposed to improve the ECG representation learning. Furthermore, we extend ECG representation learning with a language model to report generation, which is significant for evaluating clinical semantics in the learned representations and even clinical applications. Comprehensive experiments on various datasets with various experimental settings show the superior performance of our proposed DERI."
    },
    {
        "title": "Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions",
        "link_suffix": "/forum?id=cwuSAR7EKd",
        "link": "https://openreview.net/forum?id=cwuSAR7EKd",
        "pdf_link": "https://openreview.net/pdf?id=cwuSAR7EKd",
        "keywords": "Clarifying Questions, QA, Ambiguity, RLHF",
        "abstract": "Large language models (LLMs) must often respond to highly ambiguous user requests. In such cases, the LLM's best response may be to ask a clarifying question to elicit more information. We observe existing LLMs often respond by presupposing a single interpretation of such ambiguous requests, frustrating users who intended a different interpretation. We speculate this is caused by current preference data labeling practice, where LLM responses are evaluated only on their prior contexts. To address this, we propose to assign preference labels by simulating their expected outcomes in the future turns. This allows LLMs to learn to ask clarifying questions when it can generate responses that are tailored to each user interpretation in future turns. In experiments on open-domain QA, we compare systems that trained using our proposed preference labeling methods against standard methods, which assign preferences based on only prior context. We evaluate systems based on their ability to ask clarifying questions that can recover each user's interpretation and expected answer, and find that our training with our proposed method trains LLMs to ask clarifying questions with a 5% improvement in F1 measured against the answer set from different interpretations of each query."
    },
    {
        "title": "Towards Robust and Cost-Efficient Knowledge Unlearning for Large Language Models",
        "link_suffix": "/forum?id=1ExfUpmIW4",
        "link": "https://openreview.net/forum?id=1ExfUpmIW4",
        "pdf_link": "https://openreview.net/pdf?id=1ExfUpmIW4",
        "keywords": "Machine Unlearning, Large Language Models, Low-rank Adaptation",
        "abstract": "Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We also find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose two novel techniques for robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact."
    },
    {
        "title": "Can Transformers Reason Logically? A Study in SAT Solving",
        "link_suffix": "/forum?id=XYK1eGjahp",
        "link": "https://openreview.net/forum?id=XYK1eGjahp",
        "pdf_link": "https://openreview.net/pdf?id=XYK1eGjahp",
        "keywords": "Transformer LLMs, Logical Reasoning, Chain-of-Thought, SAT Solving, Backtracking",
        "abstract": "We theoretically and empirically study the logical reasoning capabilities of LLMs in the context of the Boolean satisfiability (SAT) problem. \nFirst, we construct a decoder-only Transformer that can solve SAT using backtracking and deduction via Chain-of-Thought (CoT). We prove its correctness by showing trace equivalence to the well-known DPLL SAT-solving algorithm. Second, to support the implementation of this abstract construction, we design a compiler PARAT that takes as input a procedural specification and outputs a transformer model implementing this specification. Third, rather than programming a transformer to reason, we evaluate empirically whether it can be trained to do so by learning directly from algorithmic traces (``reasoning paths'') of the DPLL algorithm."
    },
    {
        "title": "Unifying Generative and Dense Retrieval for Sequential Recommendation",
        "link_suffix": "/forum?id=SpXd4dA5Ty",
        "link": "https://openreview.net/forum?id=SpXd4dA5Ty",
        "pdf_link": "https://openreview.net/pdf?id=SpXd4dA5Ty",
        "keywords": "Generative Retrieval, Sequential Recommendation",
        "abstract": "Sequential dense retrieval models utilize advanced sequence learning techniques to compute item and user representations, which are then used to rank relevant items for a user through inner product computation between the user and all item representations. However, this approach requires storing a unique representation for each item, resulting in significant memory requirements as the number of items\ngrow. In contrast, the recently proposed generative retrieval paradigm offers a promising alternative by directly predicting item indices using a generative model trained on semantic IDs that encapsulate items’ semantic information. Despite its potential for large-scale applications, a comprehensive comparison between generative retrieval and sequential dense retrieval under fair conditions is still lacking, leaving open questions regarding performance, storage, and computation trade-offs. To address this gap, we conduct a thorough comparison of both\napproaches under identical conditions and propose LIGER (LeveragIng dense retrieval forGEnerativeRetrieval), a hybrid model that combines the strengths of these two widely used paradigms. Our proposed model seamlessly integrates sequential dense into generative retrieval, effectively addressing performance disparities and improving cold-start item recommendation. This approach demonstrates significant improvements in both efficiency and effectiveness for recommendation systems."
    },
    {
        "title": "Transformers trained on proteins can learn to attend to Euclidean distance",
        "link_suffix": "/forum?id=9AtlhmFVDi",
        "link": "https://openreview.net/forum?id=9AtlhmFVDi",
        "pdf_link": "https://openreview.net/pdf?id=9AtlhmFVDi",
        "keywords": "Transformers, SE(3), Proteins, Function, Deep learning, Sequence, Structure",
        "abstract": "While conventional Transformers generally operate on sequence data, they can be used in conjunction with structure models, typically SE(3)-invariant or equivariant graph neural networks (GNNs), for 3D applications such as protein structure modelling. These hybrids typically involve either (1) preprocessing/tokenizing structural features as input for Transformers or (2) taking Transformer embeddings and processing them within a structural representation. However, there is evidence that Transformers can learn to process structural information on their own, such as the AlphaFold3 structural diffusion model. In this work we show that Transformers can function independently as structure models when passed linear embeddings of coordinates. We first provide a theoretical explanation for how Transformers can learn to filter attention as a 3D Gaussian with learned variance. We then validate this theory using both simulated 3D points and in the context of masked token prediction for proteins. Finally, we show that pre-training protein Transformer encoders with structure improves performance on a downstream task, yielding better performance than custom structural models. Together, this work provides a basis for using standard Transformers as hybrid structure-language models."
    },
    {
        "title": "What Makes Large Language Models Reason in (Multi-Turn) Code Generation?",
        "link_suffix": "/forum?id=Zk9guOl9NS",
        "link": "https://openreview.net/forum?id=Zk9guOl9NS",
        "pdf_link": "https://openreview.net/pdf?id=Zk9guOl9NS",
        "keywords": "Large language Models, Multi-turn Code Generation, Chain-of-Thought",
        "abstract": "Prompting techniques such as chain-of-thought have established themselves as a popular vehicle for improving the outputs of large language models (LLMs). For code generation, however, their exact mechanics and efficacy are under-explored using unified metrics and benchmarks. We thus investigate the effects of a wide range of prompting strategies with a focus on automatic re-prompting over multiple turns and computational requirements. After systematically decomposing reasoning, instruction, and execution feedback prompts, we conduct an extensive grid search on the competitive programming benchmarks CodeContests and TACO for multiple LLM families and sizes (Llama 3.0 and 3.1, 8B, 70B, 405B, and GPT-4o). Our study reveals strategies that consistently improve performance across all models with small and large sampling budgets. We then show how finetuning with such an optimal configuration allows models to internalize the induced reasoning process and obtain improvements in performance and scalability for multi-turn code generation."
    },
    {
        "title": "LOGO --- Long cOntext aliGnment via efficient preference Optimization",
        "link_suffix": "/forum?id=FSlfoBIctk",
        "link": "https://openreview.net/forum?id=FSlfoBIctk",
        "pdf_link": "https://openreview.net/pdf?id=FSlfoBIctk",
        "keywords": "Long-context aligment, efficient preference optimization, positional indices synthesis",
        "abstract": "Long-context models (LCMs) have shown great potential in processing long input sequences (even more than 100M tokens) conveniently and effectively.\nWith significant progress, recent research has pointed out that LCMs can accurately locate token-level salient information within the context.\nYet, the generation performance of these LCMs is far from satisfactory and might result in misaligned responses, such as hallucinations.\nTo enhance the generation capability of LCMs, existing works have investigated the effects of data size and quality for both pre-training and instruction tuning.\nThough achieving meaningful improvement, previous methods fall short in either effectiveness or efficiency.\nIn this paper, we introduce LOGO (Long cOntext aliGnment via efficient preference Optimization), a training strategy that first introduces preference optimization for long-context alignment.\nTo overcome the GPU memory-bound issue caused by the long sequence, LOGO employs a reference-free preference optimization strategy and adopts a position synthesis method to construct the training data.\nBy training with only 0.3B data on a single 8$\\times$A800 GPU machine for 16 hours, LOGO allows the Llama-3-8B-Instruct-80K model to achieve comparable performance with GPT-4 in real-world long-context tasks while preserving the model's original capabilities on other tasks, e.g., language modeling and MMLU.\nMoreover, LOGO can extend the model's context window size while enhancing its generation performance."
    },
    {
        "title": "Inverse Entropic Optimal Transport Solves Semi-supervised Learning via Data Likelihood Maximization",
        "link_suffix": "/forum?id=lBxJH2aTJr",
        "link": "https://openreview.net/forum?id=lBxJH2aTJr",
        "pdf_link": "https://openreview.net/pdf?id=lBxJH2aTJr",
        "keywords": "semi-supervised domain translation, likelihood maximization, inverse entropic optimal transport",
        "abstract": "Learning conditional distributions $\\pi^*(\\cdot|x)$ is a central problem in machine learning, which is typically approached via supervised methods with paired data $(x,y) \\sim \\pi^*$. However, acquiring paired data samples is often challenging, especially in problems such as domain translation. This necessitates the development ofsemi-supervisedmodels that utilize both limited paired data and additional unpaired i.i.d. samples $x \\sim \\pi^*_x$ and $y \\sim \\pi^*_y$ from the marginal distributions. The usage of such combined data is complex and often relies on heuristic approaches. To tackle this issue, we propose a new learning paradigm that integrates both paired and unpaired dataseamlesslythrough the data likelihood maximization techniques. We demonstrate that our approach also connects intriguingly with inverse entropic optimal transport (OT). This finding allows us to apply recent advances in computational OT to establish alightlearning algorithm to get $\\pi^*(\\cdot|x)$. Furthermore, we demonstrate through empirical tests that our method effectively learns conditional distributions using paired and unpaired data simultaneously."
    },
    {
        "title": "RMB: Comprehensively benchmarking reward models in LLM alignment",
        "link_suffix": "/forum?id=kmgrlG9TR0",
        "link": "https://openreview.net/forum?id=kmgrlG9TR0",
        "pdf_link": "https://openreview.net/pdf?id=kmgrlG9TR0",
        "keywords": "LLM Alignment, reward model, evaluation",
        "abstract": "Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization.\nWe demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs.  Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. We will release our evaluation code and datasets upon publication."
    },
    {
        "title": "The Canary’s Echo: Auditing Privacy Risks of LLM-Generated Synthetic Text",
        "link_suffix": "/forum?id=r7wMVdGFro",
        "link": "https://openreview.net/forum?id=r7wMVdGFro",
        "pdf_link": "https://openreview.net/pdf?id=r7wMVdGFro",
        "keywords": "Privacy, language models, synthetic data",
        "abstract": "How much information about training examples can be gleaned from synthetic data generated by Large Language Models (LLMs)? Overlooking the subtleties of information flow in synthetic data generation pipelines can lead to a false sense of privacy. In this paper, we investigate the design of membership inference attacks (MIAs) that target data used to fine-tune pre-trained LLMs that are then used to synthesize data, particularly when the adversary does not have access to the fine-tuned LLM but only to a synthetic data corpus. We demonstrate that using canaries crafted to maximize their vulnerability to attacks that have access to the model are sub-optimal for auditing privacy risks when only synthetic data is released. This is because such out-of-distribution canaries have limited influence on the model’s output when prompted to generate useful, in-distribution synthetic data, thus significantly limiting their vulnerability to MIAs. To tackle this problem, we leverage the mechanics of auto-regressive models to design canaries that leave detectable traces in synthetic data. Our approach significantly enhances the power of MIAs, providing a better assessment of the privacy risks of releasing synthetic data generated by LLMs."
    },
    {
        "title": "FedTMOS: Efficient One-Shot Federated Learning with Tsetlin Machine",
        "link_suffix": "/forum?id=44hcrfzydU",
        "link": "https://openreview.net/forum?id=44hcrfzydU",
        "pdf_link": "https://openreview.net/pdf?id=44hcrfzydU",
        "keywords": "Efficient Federated Learning, One Shot Federated Learning, Tsetlin Machine",
        "abstract": "One-Shot Federated Learning (OFL) is a promising approach that reduce communication to a single round, minimizing latency and resource consumption. However, existing OFL methods often rely on Knowledge Distillation, which adds a training phase and increases server-side latency. Their performance can also be compromised by the quality of generated data or public datasets, resulting in sub-optimal server models. To address these challenges, we proposed One-Shot Federated Learning with Tsetlin Machine (FedTMOS), a novel data-free OFL framework built upon the low-complexity and class-adaptive properties of the Tsetlin Machine. FedTMOS first clusters then reassigns class-specific weights to form models using an inter-class maximization approach, generating balanced and efficient server models without requiring additional training. Our extensive experiments demonstrate that FedTMOS significantly outperforms its ensemble counterpart by an average of $8.30%$, and the leading state-of-the-art OFL baselines by $4.21%$ across various datasets. Moreover, it achieves a reduction in server latency by $7.5-45\\times$ and upload communication costs by at least $2.3\\times$, establishing FedTMOS as a highly efficient solution for OFL."
    },
    {
        "title": "From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency",
        "link_suffix": "/forum?id=AmEgWDhmTr",
        "link": "https://openreview.net/forum?id=AmEgWDhmTr",
        "pdf_link": "https://openreview.net/pdf?id=AmEgWDhmTr",
        "keywords": "chain of thoughts, sample complexity, sparsity, sample efficiency, parity learning",
        "abstract": "Chain-of-thought (CoT)  significantly enhances the reasoning performance of large language models (LLM). While current theoretical studies often attribute this improvement to increased expressiveness and computational capacity, we argue that expressiveness is not the primary limitation in the LLM regime, as current large models will fail on simple tasks. Using a parity-learning setup, we demonstrate that CoT can substantially improve sample efficiency even when the representation power is sufficient. Specifically, with CoT, a transformer can learn the function within polynomial samples, whereas without CoT, the required sample size is exponential. Additionally, we show that CoT simplifies the learning process by introducing sparse sequential dependencies among input tokens, and leads to a sparse and interpretable attention. We validate our theoretical analysis with both synthetic and real-world experiments, confirming that sparsity in attention layers is a key factor of the improvement induced by CoT."
    },
    {
        "title": "CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery",
        "link_suffix": "/forum?id=fjEZ2LPceZ",
        "link": "https://openreview.net/forum?id=fjEZ2LPceZ",
        "pdf_link": "https://openreview.net/pdf?id=fjEZ2LPceZ",
        "keywords": "large language model, evaluation, computer science",
        "abstract": "Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society. However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first multilingual (English, Chinese, French, German) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 10K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities."
    },
    {
        "title": "Experimental Design for Nonstationary Optimization",
        "link_suffix": "/forum?id=55EO8gSCBT",
        "link": "https://openreview.net/forum?id=55EO8gSCBT",
        "pdf_link": "https://openreview.net/pdf?id=55EO8gSCBT",
        "keywords": "plasticity, continual learning, experiment design",
        "abstract": "Traditional methods for optimizing neural networks often struggle when used\nto train networks in settings where the data distributions change, and plasticity\npreservation methods have been shown to improve performance in such settings\n(e.g. continual learning and reinforcement learning). With the growing inter-\nest in nonstationary optimization and plasticity research, there is also a growing\nneed to properly define experimental design and hyperparameter search protocols\nto enable principled research. Each new proposed work typically adds several\nnew hyperparameters makes many more design decisions such as hyperparame-\nter selection protocols, evaluation protocols, and types of tasks examined. While\ninnovation in experiment design is important, it is also necessary to (1) question\nwhether those innovations are leading to the best progress and (2) have standard-\nized practices that make it easier to directly compare to prior works. In this paper,\nwe first perform an extensive empirical study of over 27,000 trials looking at the\nperformance of different methods and hyperparameters across different settings\nand architectures used in the literature to provide an evaluation of these methods\nand the hyperparameters they use under similar experimental conditions. We then\nexamine several core experiment design choices made by the community, affirm-\ning some while providing evidence against others, and provide concrete recom-\nmendations and analysis that can be used to guide future research."
    },
    {
        "title": "Towards Optimal Adapter Placement for Efficient Transfer Learning",
        "link_suffix": "/forum?id=RxQOKupaui",
        "link": "https://openreview.net/forum?id=RxQOKupaui",
        "pdf_link": "https://openreview.net/pdf?id=RxQOKupaui",
        "keywords": "Parameter Efficient Transfer Learning, Adapters, Fine-Tuning",
        "abstract": "Parameter-efficient transfer learning (PETL) aims to adapt pre-trained models to new downstream tasks while minimizing the number of fine-tuned parameters. Adapters, a popular approach in PETL, inject additional capacity into existing networks by incorporating low-rank projections, achieving performance comparable to full fine-tuning with significantly fewer parameters. This paper investigates the relationship between the placement of an adapter and its performance. We observe that adapter location within a network significantly impacts its effectiveness, and that the optimal placement is task-dependent. To exploit this observation, we introduce an extended search space of adapter connections, including long-range and recurrent adapters. We demonstrate that even randomly selected adapter placements from this expanded space yield improved results, and that high-performing placements often correlate with high gradient rank. Our findings reveal that a small number of strategically placed adapters can match or exceed the performance of the common baseline of adding adapters in every block, opening a new avenue for research into optimal adapter placement strategies."
    },
    {
        "title": "Certified Robustness to Clean-label Poisoning Using Diffusion Denoising",
        "link_suffix": "/forum?id=tsfR7JCwTf",
        "link": "https://openreview.net/forum?id=tsfR7JCwTf",
        "pdf_link": "https://openreview.net/pdf?id=tsfR7JCwTf",
        "keywords": "Data Poisoning, Certified Robustness, Diffusion Denoising",
        "abstract": "We present a certified defense to clean-label poisoning attacks. These attacks work by injecting a small number of poisoning samples (e.g., 1%) that contain $\\ell_2$-norm bounded adversarial perturbations into the training data to induce a targeted misclassification of a test-time input. Inspired by the adversarial robustness achieved by \\emph{denoised smoothing}, we show how an off-the-shelf diffusion model can sanitize the tampered training data. We extensively test our defense against seven clean-label poisoning attacks and reduce their attack success to 0-16% with only a negligible drop in the test time accuracy. We compare our defense with existing countermeasures against clean-label poisoning, showing that the defense reduces the attack success the most and offers the best model utility. Our results highlight the need for future work on developing stronger clean-label attacks and using our certified yet practical defense as a strong baseline to evaluate these attacks."
    }
]