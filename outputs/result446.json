[{"title": "MoreauPruner: Robust Structured Pruning of Large Language Models against Weight Perturbations", "link_suffix": "/forum?id=Y0qmwm6tgy", "link": "https://openreview.net/forum?id=Y0qmwm6tgy", "pdf_link": "https://openreview.net/pdf?id=Y0qmwm6tgy", "keywords": "Structured Pruning, Large Language Model, Robustness, Moreau Envelop", "abstract": "In the existing model pruning literature, the weight gradient has been extensively utilized to measure the importance of weight, where the gradient is well-known to be sensitive to perturbations. On the other hand, the widely used large language models (LLMs) have several billion model parameters, which could increase the fragility of few-shot gradient pruning. In this work, we experimentally show that one-shot gradient pruning algorithms could lead to unstable results under perturbations to model weights. Even the minor error of switching between data formats bfloat16 and float16 could result in obviously different outcomes. To address such instabilities, we leverage optimization analysis and propose an LLM structural pruning method, called MoreauPruner, with provable robustness against weight perturbations. In MoreauPruner, the model weight importance is estimated based on the neural network's Moreau envelope, which can be flexibly combined with $\\ell_1$-norm regularization techniques to induce the sparsity required in the pruning task. We extensively evaluate the MoreauPruner algorithm on several well-known LLMs, including LLaMA-7B, LLaMA-13B, LLaMA3-8B, and Vicuna-7B. Our numerical results suggest the robustness of MoreauPruner against weight perturbation and how robust importance estimation in MoreauPruner contributes to successful accuracy-based scores compared to several existing pruning methods.", "title_embedding_index": 22250, "title_abs_embedding_index": 22275}, {"title": "Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models", "link_suffix": "/forum?id=p3NKpom1VL", "link": "https://openreview.net/forum?id=p3NKpom1VL", "pdf_link": "https://openreview.net/pdf?id=p3NKpom1VL", "keywords": "Multimodal Large Language Models, Fine-Grained Visual Recognition", "abstract": "Multi-modal large language models (MLLMs) have shown remarkable abilities in various visual understanding tasks. However, MLLMs still struggle with fine-grained visual recognition (FGVR), which aims to identify subordinate-level categories from images. This can negatively impact more advanced capabilities of MLLMs, such as object-centric visual question answering and reasoning. In our study, we revisit three quintessential capabilities of MLLMs for FGVR, including object information extraction, category knowledge reserve, object-category alignment, and position of the root cause as a misalignment problem. To address this issue, we present Finedefics, an MLLM that enhances the model's FGVR capability by incorporating informative attribute descriptions of objects into the training phase. We employ contrastive learning on object-attribute pairs and attribute-category pairs simultaneously and use examples from similar but incorrect categories as hard negatives, naturally bringing representations of visual objects and category names closer. Extensive evaluations across multiple popular FGVR datasets demonstrate that Finedefics outperforms existing MLLMs of comparable parameter sizes, showcasing its remarkable efficacy.", "title_embedding_index": 22251, "title_abs_embedding_index": 22276}, {"title": "\u03b3\u2212MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models", "link_suffix": "/forum?id=q44uq3tc2D", "link": "https://openreview.net/forum?id=q44uq3tc2D", "pdf_link": "https://openreview.net/pdf?id=q44uq3tc2D", "keywords": "mixture of depths; multimodal large language models", "abstract": "Despite the significant progress in multimodal large language models (MLLMs), their high computational cost remains a barrier to real-world deployment. Inspired by the mixture of depths (MoDs) in natural language processing, we aim to address this limitation from the perspective of ``activated tokens''. Our key insight is that if most tokens are redundant for the layer computation, then can be skipped directly via the MoD layer. However, directly converting the dense layers of MLLMs to MoD layers leads to substantial performance degradation. To address this issue,  we propose an innovative MoD adaptation strategy for existing MLLMs called $\\gamma$-MoD.  In $\\gamma$-MoD,   a novel metric is proposed to guide the deployment of MoDs in the MLLM, namely rank of attention maps (ARank). Through ARank, we can effectively identify which layer is redundant and should be replaced with the MoD layer.  Based on ARank,  we further propose two novel designs to maximize the computational sparsity of MLLM while maintaining its performance, namely  shared vision-language router and  masked routing learning.   With these designs, more than 90% dense layers of the MLLM can be effectively converted to the MoD ones. To validate our method, we apply it to three popular MLLMs, and conduct extensive experiments on 9 benchmark datasets.  Experimental results not only validate the significant efficiency benefit of $\\gamma$-MoD to existing MLLMs but also confirm its generalization ability on various MLLMs.  For example, with a minor performance drop,  i.e., -1.5%, $\\gamma$-MoD can reduce the training and inference time of LLaVA-HR by 31.0% and 53.2%, respectively.", "title_embedding_index": 22252, "title_abs_embedding_index": 22277}, {"title": "LiveBench: A Challenging, Contamination-Free LLM Benchmark", "link_suffix": "/forum?id=sKYHBTAxVa", "link": "https://openreview.net/forum?id=sKYHBTAxVa", "pdf_link": "https://openreview.net/pdf?id=sKYHBTAxVa", "keywords": "large language models, benchmark", "abstract": "Test set contamination, wherein test data from a benchmark ends up in a newer model's training set, is a well-documented obstacle for fair LLM evaluation and can quickly render benchmarks obsolete. To mitigate this, many recent benchmarks crowdsource new prompts and evaluations from human or LLM judges; however, these can introduce significant biases, and break down when scoring hard questions. In this work, we introduce a new benchmark for LLMs designed to be immune to both test set contamination and the pitfalls of LLM judging and human crowdsourcing. We release LiveBench, the first benchmark that (1) contains frequently-updated questions from recent information sources, (2) scores answers automatically according to objective ground-truth values, and (3) contains a wide variety of challenging tasks, spanning math, coding, reasoning, language, instruction following, and data analysis. To achieve this, LiveBench contains questions that are based on recently-released math competitions, arXiv papers, news articles, and datasets, and it contains harder, contamination-free versions of tasks from previous benchmarks such as Big-Bench Hard, AMPS, and IFEval. We evaluate many prominent closed-source models, as well as dozens of open-source models ranging from 0.5B to 405B in size. LiveBench is difficult, with top models achieving below 65% accuracy. We release all questions, code, and model answers. Questions are added and updated on a monthly basis, and we release new tasks and harder versions of tasks over time so that LiveBench can distinguish between the capabilities of LLMs as they improve in the future. We welcome community engagement and collaboration for expanding the benchmark tasks and models.", "title_embedding_index": 22253, "title_abs_embedding_index": 22278}, {"title": "MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct", "link_suffix": "/forum?id=E0dTlxy1T4", "link": "https://openreview.net/forum?id=E0dTlxy1T4", "pdf_link": "https://openreview.net/pdf?id=E0dTlxy1T4", "keywords": "MLLM; MultiModal;Visual Reasoning", "abstract": "The development of Multimodal Large Language Models (MLLMs) has seen significant advancements with increasing demands in various fields (e.g., multimodal\nagents, embodied intelligence). While model-driven approaches attempt to enhance MLLMs capabilities through diverse architectures, the gains have become\nincreasingly marginal. Conversely, data-driven methods, which scale up image-text\ninstruction data, are more effective but face limited data diversity and complexity\nchallenges. The absence of high-quality data constitutes a significant development\nbarrier for MLLMs. To address the data quality bottleneck, we propose MMEvol, a\nnovel multimodal instruction data evolution framework. This framework iteratively\nimprove data quality through a refined combination of fine-grained perception, cognitive reasoning, and interaction evolution, generating a more complex and diverse\nimage-text instruction dataset that empowers MLLMs with enhanced capabilities.\nBeginning with an initial set of instructions, SEED-163K, we utilize MMEvol to\nsystematically broaden the diversity of instruction types, extend visual reasoning\nsteps to improve cognitive reasoning abilities, and thoroughly explore fine-grained\ninformation within images to enhance visual understanding and robustness. To\ncomprehensively evaluate the effectiveness of our approach, we conduct extensive\nqualitative analysis and quantitative experiments across 13 vision-language tasks.\nCompared to baseline models trained with the initial seed data, the results demonstrate that our method achieves an average accuracy improvement of 3.1 percentage\npoints. Furthermore, our approach reaches state-of-the-art (SOTA) performance in\nnine tasks using significantly less data compared to state-of-the-art models.", "title_embedding_index": 22254, "title_abs_embedding_index": 22279}, {"title": "Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment", "link_suffix": "/forum?id=US9k5TXVLZ", "link": "https://openreview.net/forum?id=US9k5TXVLZ", "pdf_link": "https://openreview.net/pdf?id=US9k5TXVLZ", "keywords": "circuit representation learning, masked graph modeling, large language models, multimodal alignment", "abstract": "Understanding the structure and function of circuits is crucial for electronic design automation (EDA). \nCircuits can be formulated as And-Inverter graphs (AIGs), enabling efficient implementation of representation learning through graph neural networks (GNNs).\nMasked modeling paradigms have been proven effective in graph representation learning.\nHowever, masking augmentation to original circuits will destroy their logical equivalence, which is unsuitable for circuit representation learning. \nMoreover, existing masked modeling paradigms often prioritize structural information at the expense of abstract information such as circuit function.\nTo address these limitations, we introduce MGVGA, a novel constrained masked modeling paradigm incorporating masked gate modeling (MGM) and Verilog-AIG alignment (VGA).\nSpecifically, MGM preserves logical equivalence by masking gates in the latent space rather than in the original circuits, subsequently reconstructing the attributes of these masked gates.\nMeanwhile, large language models (LLMs) have demonstrated an excellent understanding of the Verilog code functionality.\nBuilding upon this capability, VGA performs masking operations on original circuits and reconstructs masked gates under the constraints of equivalent Verilog codes, enabling GNNs to learn circuit functions from LLMs.\nWe evaluate MGVGA on various logic synthesis tasks for EDA and show the superior performance of MGVGA compared to previous state-of-the-art methods.", "title_embedding_index": 22255, "title_abs_embedding_index": 22280}, {"title": "Pseudo Meets Zero: Boosting Zero-Shot Composed Image Retrieval with Synthetic Images", "link_suffix": "/forum?id=FNDudoox4A", "link": "https://openreview.net/forum?id=FNDudoox4A", "pdf_link": "https://openreview.net/pdf?id=FNDudoox4A", "keywords": "Zero-Shot Composed Image Retrieval, Synthetic Images, Multimdoal", "abstract": "Composed Image Retrieval (CIR) employs a triplet architecture to combine a reference image with modified text for target image retrieval. To mitigate high annotation costs, Zero-Shot CIR (ZS-CIR) methods eliminate the need for manually annotated triplets. Current methods typically map images to tokens and concatenate them with modified text. However, they encounter challenges during inference, especially with fine-grained and multi-attribute modifications. We argue that these challenges stem from insufficient explicit modeling of triplet relationships, which complicates fine-grained interactions and directional guidance. To this end, we propose a Synthetic Image-Oriented training paradigm that automates pseudo target image generation, facilitating efficient triplet construction and accommodating inherent target ambiguity. Furthermore, we propose the Pseudo domAiN Decoupling-Alignment (PANDA) model to mitigate the Autophagy phenomenon caused by fitting targets with pseudo images. We observe that synthetic images are intermediate between visual and textual domains in triplets. Regarding this phenomenon, we design the Orthogonal Semantic Decoupling module to disentangle the pseudo domain into visual and textual components. Additionally, Shared Domain Interaction and Mutual Shift Constraint modules are proposed to collaboratively constrain the disentangled components, bridging the gap between pseudo and real triplets while enhancing their semantic consistency. Extensive experiments demonstrate that the proposed PANDA model outperforms existing state-of-the-art methods across two general scenarios and two domain-specific CIR datasets.", "title_embedding_index": 22256, "title_abs_embedding_index": 22281}, {"title": "AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark", "link_suffix": "/forum?id=tTDUrseRRU", "link": "https://openreview.net/forum?id=tTDUrseRRU", "pdf_link": "https://openreview.net/pdf?id=tTDUrseRRU", "keywords": "Video Captioning, Benchmark, Multimodel Large Language Model", "abstract": "Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AuroraCap shows superior performance on various video and image captioning benchmarks, for example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of a few dozen words, which limits research in this field. Therefore, we develop VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose a new LLM-assisted metric VDCscore for bettering evaluation, which adopts a divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality.", "title_embedding_index": 22257, "title_abs_embedding_index": 22282}, {"title": "O2VIS: Occupancy-aware Object Association for Temporally Consistent Video Instance Segmentation", "link_suffix": "/forum?id=1BlEVFmqwn", "link": "https://openreview.net/forum?id=1BlEVFmqwn", "pdf_link": "https://openreview.net/pdf?id=1BlEVFmqwn", "keywords": "Video instance segmentation, Long-term memory, Temprorally consistent learning", "abstract": "In this paper, we present Occupancy-aware Object Association for Video Instance Segmentation ($\\text{O}_{\\text{2}}$VIS), a new framework crafted to improve long-term consistency in instance tracking. We introduce the Instance Occupancy Memory (IOM) that tracks global instance features and their occupancy status to effectively differentiate between recurring and new objects. It ensures consistent tracking and effective management of object identities across frames, enhancing the overall performance and reliability of the VIS process. Moreover, we propose a Decoupled Object Association (DOA) strategy that handles existing and newly appeared objects separately to optimally assign indices based on occupancy. This technique enhances the accuracy of object matching and ensures stable and consistent object alignment across frames, especially useful in dynamic settings where objects frequently appear and disappear. Extensive testing and an ablation study confirm the superiority of our method over traditional methods, establishing new standards in the VIS domain.", "title_embedding_index": 22258, "title_abs_embedding_index": 22283}, {"title": "Context-Aware Video Instance Segmentation", "link_suffix": "/forum?id=VhQelEo27A", "link": "https://openreview.net/forum?id=VhQelEo27A", "pdf_link": "https://openreview.net/pdf?id=VhQelEo27A", "keywords": "Video Instance Segmentation, Contrastive Learning, Instance Prototype, Context-aware Learning", "abstract": "In this paper, we introduce the Context-Aware Video Instance Segmentation (CAVIS), a novel framework designed to enhance instance association by integrating contextual information adjacent to each object. To efficiently extract and leverage this information, we propose the Context-Aware Instance Tracker (CAIT), which merges contextual data surrounding the instances with the core instance features to improve tracking accuracy. Additionally, we introduce the Prototypical Cross-frame Contrastive (PCC) loss, which ensures consistency in object-level features across frames, thereby significantly enhancing instance matching accuracy. CAVIS demonstrates superior performance over state-of-the-art methods on all benchmark datasets in video instance segmentation (VIS) and video panoptic segmentation (VPS). Notably, our method excels on the OVIS dataset, which is known for its particularly challenging videos.", "title_embedding_index": 22259, "title_abs_embedding_index": 22284}, {"title": "Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models", "link_suffix": "/forum?id=ZYd5wJSaMs", "link": "https://openreview.net/forum?id=ZYd5wJSaMs", "pdf_link": "https://openreview.net/pdf?id=ZYd5wJSaMs", "keywords": "Diffusion Models, Generation, Dense Perception", "abstract": "Beyond high-fidelity image synthesis, diffusion models have recently exhibited promising results in dense visual perception tasks. However, most existing work treats diffusion models as a standalone component for perception tasks, employing them either solely for off-the-shelf data augmentation or as mere feature extractors. In contrast to these isolated and thus sub-optimal efforts, we introduce a unified, versatile, diffusion-based framework, Diff-2-in-1, that can simultaneously handle both multi-modal data generation and dense visual perception, through a unique exploitation of the diffusion-denoising process. Within this framework, we further enhance discriminative visual perception via multi-modal generation, by utilizing the denoising network to create multi-modal data that mirror the distribution of the original training set. Importantly, Diff-2-in-1 optimizes the utilization of the created diverse and faithful data by leveraging a novel self-improving learning mechanism. Comprehensive experimental evaluations validate the effectiveness of our framework, showcasing consistent performance improvements across various discriminative backbones and high-quality multi-modal data generation characterized by both realism and usefulness.", "title_embedding_index": 22260, "title_abs_embedding_index": 22285}, {"title": "A simulation-heuristics dual-process model for intuitive physics", "link_suffix": "/forum?id=BkeJro1xps", "link": "https://openreview.net/forum?id=BkeJro1xps", "pdf_link": "https://openreview.net/pdf?id=BkeJro1xps", "keywords": "Intuitive physics, physical reasoning, mental simulation, heuristic model", "abstract": "The role of mental simulation in human behavior for various physical tasks is widely acknowledged, attributed to the generality of Intuitive Physics Engine (IPE). However, it remains unclear whether mental simulation is consistently employed across scenarios of different simulation costs and where its boundary is. Moreover, cognitive strategies beyond these boundaries have not been thoroughly investigated. Here, we adopted a pouring-marble task containing various conditions to study IPE's limits and strategies beyond. A human study revealed two distinct error patterns in predicting the pouring angle, differentiated by the simulation time using a boundary. This suggests a possible switching of the underlying reasoning strategies. Our initial experiment on IPE showed that its correlation with human judgments diminished in scenarios requiring extended time of simulation. This observation prompted the exploration of an alternative mechanism based on heuristics for intuitive physics. We uncovered that a linear heuristic model, relying exclusively on empirical data, replicated human prediction more accurately when the simulation time exceeded a certain boundary. Motivated by these observations, we propose a new framework, Simulation-Heuristics Model (SHM), which conceptualizes intuitive physics as a dual process: IPE is predominant only in short-time simulation, whereas a heuristics-based approach is applied as IPE's simulation time extends beyond the simulation boundary. The SHM model aligns more precisely with human behavior across various scenarios and demonstrates superior generalization capabilities under different conditions. Crucially, SHM integrates computational methods previously viewed as separate into a unified model, quantitatively studying their switching mechanism.", "title_embedding_index": 22261, "title_abs_embedding_index": 22286}, {"title": "Video Diffusion Models Learn the Structure of the Dynamic World", "link_suffix": "/forum?id=SIZhZrU41O", "link": "https://openreview.net/forum?id=SIZhZrU41O", "pdf_link": "https://openreview.net/pdf?id=SIZhZrU41O", "keywords": "Diffusion Models, Video Understanding, Representation Learning", "abstract": "Diffusion models have demonstrated significant progress in visual perception tasks due to their ability to capture fine-grained, object-centric features through large-scale vision-language pretraining. While their success in image-based tasks is well-established, extending this capability to the domain of video understanding remains a key challenge.  In this work, we explore the potential of diffusion models for video understanding by analyzing the feature representations learned by both image- and video-based diffusion models, alongside non-generative, self-supervised approaches. We propose a unified probing framework to evaluate six models across four core video understanding tasks: action recognition, object discovery, scene understanding, and label propagation. Our findings reveal that video diffusion models consistently rank among the top performers, particularly excelling at modeling temporal dynamics and scene structure. This observation not only sets them apart from image-based diffusion models but also opens a new direction for advancing video understanding, offering a fresh alternative to traditional discriminative pre-training objectives. Interestingly, we demonstrate that higher generation performance does not always correlate with improved performance in downstream tasks, highlighting the importance of careful representation selection. Overall, our results suggest that video diffusion models hold substantial promise for video understanding by effectively capturing both spatial and temporal information, positioning them as strong competitors in this evolving domain.", "title_embedding_index": 22262, "title_abs_embedding_index": 22287}, {"title": "COME: Test-time Adaption by Conservatively Minimizing Entropy", "link_suffix": "/forum?id=506BjJ1ziZ", "link": "https://openreview.net/forum?id=506BjJ1ziZ", "pdf_link": "https://openreview.net/pdf?id=506BjJ1ziZ", "keywords": "Test-time adaption, Out-of-distribution generalization", "abstract": "Machine learning models must continuously self-adjust themselves for novel data distribution in the open world. As the predominant principle, entropy minimization (EM) has been proven to be a simple yet effective cornerstone in existing test-time adaption (TTA) methods. While unfortunately its fatal limitation (i.e., overconfidence) tends to result in model collapse. For this issue, we propose to conservatively minimize the entropy (COME), which is a simple drop-in replacement of traditional EM to elegantly address the limitation. In essence, COME explicitly models the uncertainty by characterizing a Dirichlet prior distribution over model predictions during TTA. By doing so, COME naturally regularizes the model to favor conservative confidence on unreliable samples. Theoretically, we provide a preliminary analysis to reveal the ability of COME in enhancing the optimization stability by introducing a data-adaptive lower bound on the entropy. Empirically, our method achieves state-of-the-art performance on commonly used benchmarks, showing significant improvements in terms of classification accuracy and uncertainty estimation under various settings including standard, life-long and open-world TTA. Our code is available at: \\href{https://anonymous.4open.science/r/anonymous-9F46}{https://anonymous.4open.science/r/anonymous-9F46}.", "title_embedding_index": 22263, "title_abs_embedding_index": 22288}, {"title": "Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology", "link_suffix": "/forum?id=trj2Jq8riA", "link": "https://openreview.net/forum?id=trj2Jq8riA", "pdf_link": "https://openreview.net/pdf?id=trj2Jq8riA", "keywords": "Computation Pathology, Survival Analysis, Multi-Instance Learning, Whole-Slide Images, Vision-Language Modes", "abstract": "Histopathology Whole-Slide Images (WSIs) provide an important tool to assess cancer prognosis in computational pathology (CPATH). While existing survival analysis (SA) approaches have made exciting progress, they are generally limited to adopting highly-expressive architectures and only coarse-grained patient-level labels to learn prognostic visual representations from gigapixel WSIs. Such learning paradigm suffers from important performance bottlenecks, when facing present scarce training data and standard multi-instance learning (MIL) framework in CPATH. To overcome it, this paper, for the first time, proposes a new Vision-Language-based SA (VLSA) paradigm. Concretely, (1) VLSA is driven by pathology VL foundation models. It no longer relies on high-capability networks and shows the advantage ofdata efficiency. (2) In vision-end, VLSA encodes prognostic language prior and then employs it asauxiliary signalsto guide the aggregating of prognostic visual features at instance level, thereby compensating for the weak supervision in MIL. Moreover, given the characteristics of SA, we propose i)ordinal survival prompt learningto transform continuous survival labels into textual prompts; and ii)ordinal incidence functionas prediction target to make SA compatible with VL-based prediction. Notably, VLSA's predictions can be interpreted intuitively by our Shapley values-based method. The extensive experiments on five datasets confirm the effectiveness of our scheme. Our VLSA could pave a new way for SA in CPATH by offering weakly-supervised MIL an effective means to learn valuable prognostic clues from gigapixel WSIs.", "title_embedding_index": 22264, "title_abs_embedding_index": 22289}, {"title": "Anti-Exposure Bias in Diffusion Models via Prompt Learning", "link_suffix": "/forum?id=MtDd7rWok1", "link": "https://openreview.net/forum?id=MtDd7rWok1", "pdf_link": "https://openreview.net/pdf?id=MtDd7rWok1", "keywords": "Diffusion Models, Exposure Bias, Prompt Learning, Sampling Trajectory", "abstract": "Diffusion models (DMs) have achieved record-breaking performance in image generation tasks.\nNevertheless, in practice, the training-sampling discrepancy, caused by score estimation error and discretization error, limits the modeling ability of DMs, a phenomenon known as exposure bias.\nTo alleviate such exposure bias and further improve the generative performance, we put forward a prompt learning framework built upon a lightweight prompt prediction model.\nConcretely, our model learns an anti-bias prompt for the generated sample at each sampling step, aiming to compensate for the exposure bias that arises.\nFollowing this design philosophy, our framework rectifies the sampling trajectory to match the training trajectory, thereby reducing the divergence between the target data distribution and the modeling distribution.\nTo train the prompt prediction model, we simulate exposure bias by constructing training data and introduce a time-dependent weighting function for optimization.\nEmpirical results on various DMs demonstrate the superiority of our prompt learning framework across three benchmark datasets.\nImportantly, the optimized prompt prediction model effectively improves image quality with only a 5% increase in sampling overhead, which remains negligible.", "title_embedding_index": 22265, "title_abs_embedding_index": 22290}, {"title": "HERO: Harnessing Temporal Modeling for Diffusion-Based Video Outpainting", "link_suffix": "/forum?id=I0HfqpSH8f", "link": "https://openreview.net/forum?id=I0HfqpSH8f", "pdf_link": "https://openreview.net/pdf?id=I0HfqpSH8f", "keywords": "video outpainting, diffusion model", "abstract": "Video outpainting expands the spatial perspective of a video, enabling it to adapt to various display devices with different aspect ratios.\nCurrent diffusion-based approaches for video outpainting often suffer from quality issues such as blurred details, local distortion, and temporal instability, significantly impacting the user experience.\nThe root cause is the insufficient temporal modeling in video\noutpainting, which inadequately represents the relationships between frames over time.\nTo address this issue, a novel approach called HERO~(Harnessing the tEmpoRal modeling for diffusion-based Outpainting) is proposed to effectively tackles these generated video quality problems.\nHERO employs two critical components to enhance temporal modeling: the Temporal Reference Module, which provides reference features that extend beyond spatial dimensions; and the Interpolation-based Motion Modelling Module, designed to stabilize generated frames.\nBy integrating these modules, these quality issues in video outpainting are effectively addressed.\nExtensive experiments on multiple benchmarks demonstrate that HERO outperforms existing methods qualitatively and quantitatively.", "title_embedding_index": 22266, "title_abs_embedding_index": 22291}, {"title": "Log-Concave Sampling on Compact Supports: A Versatile Proximal Framework", "link_suffix": "/forum?id=LX9m5iWBun", "link": "https://openreview.net/forum?id=LX9m5iWBun", "pdf_link": "https://openreview.net/pdf?id=LX9m5iWBun", "keywords": "Markov Chain Monte Carlo, Kinetic Langevin, Langevin algorithm, Midpoint method, Mixing rate, Proximal method", "abstract": "In this paper, we investigate the theoretical aspects of sampling from strongly log-concave distributions defined on convex and compact supports. We propose a general proximal framework that involves projecting onto the constrained set, which is highly flexible and supports various projection options. Specifically, we consider the cases of Euclidean and Gauge projections, with the latter having the advantage of being performed efficiently using a membership oracle. This framework can be seamlessly integrated with multiple sampling methods. Our analysis focuses on Langevin-type sampling algorithms within the context of constrained sampling. We provide nonasymptotic upper bounds on the $W_1$ and $W_2$ errors, offering a detailed comparison of the performance of these methods in constrained sampling.", "title_embedding_index": 22267, "title_abs_embedding_index": 22292}, {"title": "SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination", "link_suffix": "/forum?id=B5VEi5d3p2", "link": "https://openreview.net/forum?id=B5VEi5d3p2", "pdf_link": "https://openreview.net/pdf?id=B5VEi5d3p2", "keywords": "Sleep staging, Multimodal coordination, Ubiquitous computing", "abstract": "Sleep staging is critical for assessing sleep quality and tracking health. Polysomnography (PSG) provides comprehensive multimodal sleep-related information, but its complexity and impracticality limit its practical use in daily and ubiquitous monitoring. Conversely, unimodal devices offer more convenience but less accuracy. Existing multimodal learning paradigms typically assume that the data types remain consistent between the training and testing phases. This makes it challenging to leverage information from other modalities in ubiquitous scenarios (e.g., at home) where only one modality is available. To address this issue, we introduce a novel framework for ubiquitous Sleep staging via Supervised Multimodal Coordination, called SleepSMC. To capture category-related consistency and complementarity across modality-level instances, we propose supervised modality-level instance contrastive coordination. Specifically, modality-level instances within the same category are considered positive pairs, while those from different categories are considered negative pairs. To explore the varying reliability of auxiliary modalities, we calculate uncertainty estimates based on the variance in confidence scores for correct predictions during multiple rounds of random masks. These uncertainty estimates are employed to assign adaptive weights to multiple auxiliary modalities during contrastive learning, ensuring that the primary modality learns from high-quality, category-related features. Experimental results on three public datasets, ISRUC-S3, MASS-SS3, and Sleep-EDF-78, show that SleepSMC achieves state-of-the-art cross-subject performance. SleepSMC significantly improves performance when only a single modality is present during testing, making it suitable for ubiquitous sleep monitoring. Our code will be released after formal publication.", "title_embedding_index": 22268, "title_abs_embedding_index": 22293}, {"title": "FeDeRA: Efficient Fine-tuning of Language Models in Federated Learning Leveraging Weight Decomposition", "link_suffix": "/forum?id=GtlRN48XYA", "link": "https://openreview.net/forum?id=GtlRN48XYA", "pdf_link": "https://openreview.net/pdf?id=GtlRN48XYA", "keywords": "federated learning, fine-tune, low rank adaption", "abstract": "Federated learning (FL) is a widely used privacy-preserving approach for distributed training that avoids the need to collect data from individual users. In this paper, we investigate fine-tuning pre-trained language models (PLMs) in an FL setting and leverage parameter-efficient fine-tuning (PEFT) methods to reduce computational and communication costs. However, non-IID data in federated learning significantly degrades the performance of PEFT, with the degradation worsening as data heterogeneity increases. To address this, we propose FeDeRA, an FL approach for fine-tuning PLMs that incorporates an effective extension of the low-rank adaptation (LoRA) method. Specifically, FeDeRA initializes the low-rank matrices using Singular Value Decomposition (SVD) on the pre-trained weight matrices, rather than the zero or random initialization used in the original LoRA method. Analyzing weight updates during training reveals that FeDeRA reduces weight oscillations, enabling faster and more efficient fine-tuning of PLMs in FL with non-IID data. Experimental results across multiple NLP tasks and models show that FeDeRA outperforms all PEFT-based baselines in task performance and, in some cases, even matches or exceeds the performance of full-parameter fine-tuning. FeDeRA also greatly enhances training efficiency, reducing training time by up to 97.3% compared to full-parameter fine-tuning and up to 74.6% compared to the fastest PEFT baseline in practical FL settings. Furthermore, FeDeRA demonstrates greater robustness to data heterogeneity than all other PEFT methods, highlighting the effectiveness of its proposed initialization in FL systems.", "title_embedding_index": 22269, "title_abs_embedding_index": 22294}, {"title": "P(all-atom) Is Unlocking New Path For Protein Design", "link_suffix": "/forum?id=H8C4lGZOOE", "link": "https://openreview.net/forum?id=H8C4lGZOOE", "pdf_link": "https://openreview.net/pdf?id=H8C4lGZOOE", "keywords": "Proteins, Generative models, Co-design, All-atom", "abstract": "We introduce Pallatom, an innovative protein generation model capable of producing protein structures with all-atom coordinates. Pallatom directly learns and models the joint distribution $P(\\textit{structure}, \\textit{seq})$ by focusing on $P(\\textit{all-atom})$, effectively addressing the interdependence between sequence and structure in protein generation. To achieve this, we propose a novel network architecture specifically designed for all-atom protein generation. Our model employs a dual-track framework that tokenizes proteins into token-level and atomic-level representations, integrating them through a multi-layer decoding process with \"traversing\" representations and recycling mechanism. We also introduce the $\\texttt{atom14}$ representation method, which unifies the description of unknown side-chain coordinates, ensuring high fidelity between the generated all-atom conformation and its physical structure. Experimental results demonstrate that Pallatom excels in key metrics of protein design, including designability, diversity, and novelty, showing significant improvements across the board. Our model not only enhances the accuracy of protein generation but also exhibits excellent training efficiency, paving the way for future applications in larger and more complex systems.", "title_embedding_index": 22270, "title_abs_embedding_index": 22295}, {"title": "CaPulse: Detecting Anomalies by Tuning in to the Causal Rhythms of Time Series", "link_suffix": "/forum?id=zVp0TVDkrX", "link": "https://openreview.net/forum?id=zVp0TVDkrX", "pdf_link": "https://openreview.net/pdf?id=zVp0TVDkrX", "keywords": "Time Series Anomaly Detection", "abstract": "Time series anomaly detection has garnered considerable attention across diverse domains. While existing methods often fail to capture the underlying mechanisms behind anomaly generation in time series data. In addition, time series anomaly detection often faces several data-related inherent challenges, i.e., label scarcity, data imbalance, and complex multi-periodicity. In this paper, we leverage causal tools and introduce a new causality-based framework,CaPulse, whichtunes into the underlyingcausal pulseof time series data to effectively detect anomalies. Concretely, we begin by building a structural causal model to decipher the generation processes behind anomalies. To tackle the challenges posed by the data, we propose Periodical Normalizing Flows with a novel mask mechanism and carefully designed periodical learners, creating a periodicity-aware, density-based anomaly detection approach. Extensive experiments on seven real-world datasets demonstrate that CaPulse consistently outperforms existing methods, achieving AUROC improvements of 3% to 17%, with enhanced interpretability.", "title_embedding_index": 22271, "title_abs_embedding_index": 22296}, {"title": "DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy Conflicts in Large Language Models", "link_suffix": "/forum?id=v4PnwdA056", "link": "https://openreview.net/forum?id=v4PnwdA056", "pdf_link": "https://openreview.net/pdf?id=v4PnwdA056", "keywords": "Large Language Models, Fairness, Privacy", "abstract": "Ensuring awareness of fairness and privacy in Large Language Models (LLMs) is critical. Interestingly, we discover a counter-intuitive trade-off phenomenon that enhancing an LLM's privacy awareness through Supervised Fine-Tuning (SFT) methods significantly decreases its fairness awareness with thousands of samples. To address this issue, inspired by the information theory, we introduce a training-free method to \\textbf{DEA}ctivate the fairness and privacy coupled \\textbf{N}eurons (\\textbf{DEAN}), which theoretically and empirically decrease the mutual information between fairness and privacy awareness. Extensive experimental results demonstrate that DEAN eliminates the trade-off phenomenon and significantly improves LLMs' fairness and privacy awareness simultaneously, \\eg improving Qwen-2-7B-Instruct's fairness awareness by 12.2% and privacy awareness by 14.0%.\nMore crucially, DEAN remains robust and effective with limited annotated data or even when only malicious fine-tuning data is available, whereas SFT methods may fail to perform properly in such scenarios. We hope this study provides valuable insights into concurrently addressing fairness and privacy concerns in LLMs and can be integrated into comprehensive frameworks to develop more ethical and responsible AI systems. Our code is provided in the supplementary materials.", "title_embedding_index": 22272, "title_abs_embedding_index": 22297}, {"title": "Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations", "link_suffix": "/forum?id=bMvqccRmKD", "link": "https://openreview.net/forum?id=bMvqccRmKD", "pdf_link": "https://openreview.net/pdf?id=bMvqccRmKD", "keywords": "Reinforcement Learning, Transfer Learning", "abstract": "General intelligence requires quick adaption across tasks. While existing reinforcement learning (RL) methods have made progress in generalization, they typically assume only distribution changes between source and target domains. In this paper, we explore a wider range of scenarios where not only the distribution but also the environment spaces may change. For example, in the CoinRun environment, we train agents from easy levels and generalize them to difficulty levels where there could be new enemies that have never occurred before. To address this challenging setting, we introduce a causality-guided self-adaptive representation-based approach, called CSR, that equips the agent to generalize effectively across tasks with evolving dynamics. Specifically, we employ causal representation learning to characterize the latent causal variables within the RL system. Such compact causal representations uncover the structural relationships among variables, enabling the agent to autonomously determine whether changes in the environment stem from distribution shifts or variations in space, and to precisely locate these changes. We then devise a three-step strategy to fine-tune the causal model under different scenarios accordingly. Empirical experiments show that CSR efficiently adapts to the target domains with only a few samples and outperforms state-of-the-art baselines on a wide range of scenarios, including our simulated environments, CartPole, CoinRun and Atari games.", "title_embedding_index": 22273, "title_abs_embedding_index": 22298}, {"title": "Toward Physics-guided Time Series Embedding", "link_suffix": "/forum?id=rJ1xGcJVu8", "link": "https://openreview.net/forum?id=rJ1xGcJVu8", "pdf_link": "https://openreview.net/pdf?id=rJ1xGcJVu8", "keywords": "Time Series Analysis, Dynamic System", "abstract": "In various scientific and engineering fields, the primary research areas have revolved around physics-based dynamical systems modeling and data-driven time series analysis. According to the embedding theory, dynamical systems and time series can be mutually transformed using observation functions and physical reconstruction techniques. Based on this, we propose Embedding Duality Theory, where the parameterized embedding layer essentially provides a linear estimation of the non-linear time series dynamics. This theory enables us to bypass the parameterized embedding layer and directly employ physical reconstruction techniques to acquire a data embedding representation. Utilizing physical priors results in a 10$\\times$ reduction in parameters, a 3$\\times$ increase in speed, and maximum performance enhancements of 18% in expert, 22% in zero-shot, and 53% in few-shot tasks without any hyper-parameter tuning. All methods are encapsulated as a plug-and-play module.", "title_embedding_index": 22274, "title_abs_embedding_index": 22299}]