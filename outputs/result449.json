[
    {
        "title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes",
        "link_suffix": "/forum?id=M7KyLjuN0A",
        "link": "https://openreview.net/forum?id=M7KyLjuN0A",
        "pdf_link": "https://openreview.net/pdf?id=M7KyLjuN0A",
        "keywords": "LiDAR Generation, Dynamic Scenes, 4D Generation",
        "abstract": "LiDAR scene generation has been developing rapidly recently. However, existing methods primarily focus on generating static and single-frame scenes, overlooking the inherently dynamic nature of real-world driving environments. In this work, we introduce DynamicCity, a novel 4D LiDAR generation framework capable of generating large-scale, high-quality LiDAR scenes that capture the temporal evolution of dynamic environments. DynamicCity mainly consists of two key models.1)A VAE model for learning HexPlane as the compact 4D representation. Instead of using naive averaging operations, DynamicCity employs a novelProjection Moduleto effectively compress 4D LiDAR features into six 2D feature maps for HexPlane construction, which significantly enhances HexPlane fitting quality (up to12.56mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in parallel, which improves both network training efficiency and reconstruction accuracy than naively querying each 3D point (up to7.05mIoU gain,2.06xtraining speedup, and70.84%memory reduction).2)A DiT-based diffusion model for HexPlane generation. To make HexPlane feasible for DiT generation, aPadded Rollout Operationis proposed to reorganize all six feature planes of the HexPlane as a squared 2D feature map. In particular, various conditions could be introduced in the diffusion or sampling process, supportingversatile 4D generation applications, such as trajectory- and command-driven generation, inpainting, and layout-conditioned generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate that DynamicCity significantly outperforms existing state-of-the-art 4D LiDAR generation methods across multiple metrics. The code will be released to facilitate future research."
    },
    {
        "title": "Auxiliary Classifiers Improve Stability and Efficiency in Continual Learning",
        "link_suffix": "/forum?id=1nHQRsb3Ze",
        "link": "https://openreview.net/forum?id=1nHQRsb3Ze",
        "pdf_link": "https://openreview.net/pdf?id=1nHQRsb3Ze",
        "keywords": "continual learning, class incremental learning, auxiliary classifiers",
        "abstract": "Continual learning is crucial for applications in dynamic environments, where machine learning models must adapt to changing data distributions while retaining knowledge of previous tasks. Despite significant advancements, catastrophic forgetting \u2014 where performance on earlier tasks degrades as new information is learned \u2014 remains a key challenge. In this work, we investigate the stability of intermediate neural network layers during continual learning and explore how auxiliary classifiers (ACs) can leverage this stability to improve performance. We show that early network layers remain more stable during learning, particularly for older tasks, and that ACs applied to these layers can outperform standard classifiers on past tasks. By integrating ACs into several continual learning algorithms, we demonstrate consistent and significant performance improvements on standard benchmarks. Additionally, we explore dynamic inference, showing that AC-augmented continual learning methods can reduce computational costs by up to 60% while maintaining or exceeding the accuracy of standard methods. Our findings suggest that ACs offer a promising avenue for enhancing continual learning models, providing both improved performance and the ability to adapt the network computation in environments where such flexibility might be required."
    },
    {
        "title": "Learning under Temporal Label Noise",
        "link_suffix": "/forum?id=5o0phqAhsP",
        "link": "https://openreview.net/forum?id=5o0phqAhsP",
        "pdf_link": "https://openreview.net/pdf?id=5o0phqAhsP",
        "keywords": "label noise; time series; healthcare; classification",
        "abstract": "Many time series classification tasks, where labels vary over time, are affected by label noise that also varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded over time while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance under diverse types of temporal label noise on real-world datasets."
    },
    {
        "title": "Regretful Decisions under Label Noise",
        "link_suffix": "/forum?id=7B9FCDoUzB",
        "link": "https://openreview.net/forum?id=7B9FCDoUzB",
        "pdf_link": "https://openreview.net/pdf?id=7B9FCDoUzB",
        "keywords": "Uncertainty Quantification, Fairness, Model Multiplicity, Clinical Decision Support, Classification, Label Noise",
        "abstract": "Machine learning models are routinely used to support decisions that affect individuals -- be it to screen a patient for a serious illness or to gauge their response to treatment. In these tasks, we are limited to learning models from datasets where the labels are subject to noise. In this work, we study the impact of learning under label noise at the instance level. We introduce a notion of regret for this regime, which measures the number of unforeseen mistakes when learning from noisy labels. We show that standard approaches to learn models from noisy labels can return models that perform well at a population level while subjecting individuals to a lottery of mistakes. We develop machinery to estimate the likelihood of mistakes at an instance level from a noisy dataset, by training models over plausible realizations of datasets without label noise. We present a comprehensive empirical study of label noise in clinical prediction tasks. Our results reveal how our failure to anticipate mistakes can compromise model reliance and adoption, and demonstrate how we can address these challenges by anticipating and abstaining from regretful decisions."
    },
    {
        "title": "LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion",
        "link_suffix": "/forum?id=72OSO38a2z",
        "link": "https://openreview.net/forum?id=72OSO38a2z",
        "pdf_link": "https://openreview.net/pdf?id=72OSO38a2z",
        "keywords": "diffusion, geometry, generative model, 3d",
        "abstract": "This paper introduces a novel hierarchical autoencoder that maps 3D models into a highly compressed latent space. The hierarchical autoencoder is specifically designed to tackle the challenges arising from large-scale datasets and generative modeling using diffusion. Different from previous approaches that only work on a regular image or volume grid, our hierarchical autoencoder operates on unordered sets of vectors. Each level of the autoencoder controls different geometric levels of detail. We show that the model can be used to represent a wide range of 3D models while faithfully representing high-resolution geometry details. The training of the new architecture takes 0.70x time and 0.58x memory compared to the baseline.\nWe also explore how the new representation can be used for generative modeling. Specifically, we propose a cascaded diffusion framework where each stage is conditioned on the previous stage. Our design extends existing cascaded designs for image and volume grids to vector sets."
    },
    {
        "title": "Unifying Specialized Visual Encoders for Video Language Models",
        "link_suffix": "/forum?id=vqgDq1uycO",
        "link": "https://openreview.net/forum?id=vqgDq1uycO",
        "pdf_link": "https://openreview.net/pdf?id=vqgDq1uycO",
        "keywords": "video understanding, multimodal llms",
        "abstract": "The recent advent of Large Language Models (LLMs) has ushered sophisticated reasoning capabilities into the realm of video through Video Large Language Models (VideoLLMs). However, VideoLLMs currently rely on a single vision encoder for all of their visual processing, which limits the amount and type of visual information that can be conveyed to the LLM. Our method, MERV, Multi-Encoder Representation of Videos, instead leverages multiple frozen visual encoders to create a unified representation of a video, providing the VideoLLM with a comprehensive set of specialized visual knowledge. Spatio-temporally aligning the features from each encoder allows us to tackle a wider range of open-ended and multiple-choice video understanding questions and outperform prior state-of-the-art works on their data mixes. MERV is up to 3.79% better in accuracy than Video-LLaVA across the standard suite video understanding benchmarks, while also having a better Video-ChatGPT score. We also improve upon SeViLA, the previous best on zero-shot Perception Test accuracy, by 2.21%. MERV introduces minimal extra parameters and trains faster than equivalent single-encoder approaches. Finally, we provide qualitative evidence that our model captures domain knowledge from each encoder simultaneously, such as on the motion classification tasks found in Something-Something v2. Our results offer promising directions for future research in utilizing multiple vision encoders for comprehensive video understanding."
    },
    {
        "title": "Dancing with Discrepancies: Commonality Specificity Attention GAN for Weakly Supervised Medical Lesion Segmentation",
        "link_suffix": "/forum?id=8g5Ye3c3oR",
        "link": "https://openreview.net/forum?id=8g5Ye3c3oR",
        "pdf_link": "https://openreview.net/pdf?id=8g5Ye3c3oR",
        "keywords": "medical image segmentation, weakly supervised segmentation",
        "abstract": "Increasing weakly supervised semantic segmentation methods concentrate on the target segmentation, leveraging solely image-level labels. However, a significant gap exists in addressing medical characteristics, which demands massive attention. In this paper, we observe: (i) Lesion regions typically exhibit a sharp high-intensity anatomical distribution while healthy tissues adhere to an underlying homogeneous distribution; (ii) Boundaries of lesion foregrounds and structural backgrounds are indistinct; (iii) Similar anatomical structures frequently appear within specific organs or tissues. Thus we propose a Commonality-specificity attention GAN (CoinGAN) to overcome the above challenges, which leverages anatomical distribution discrepancies to mine the latent knowledge gap between the pathological and healthy modalities. Specifically, we propose a new form of convolution, contrastive convolution, to utilize the fine-grained perceptual discrepancies of activation sub-maps to enhance the intra-image distribution, making lesion foregrounds (specificity) and structural backgrounds (commonality) boundary-aware. Then a commonality-specificity attention mechanism is proposed to suppress inter-image strong-related regions (similar structural backgrounds) and accentuate weak-related regions (heterogeneous lesions). This dual attention mechanism adeptly isolates lesion areas from the structural background. Extensive experiments across three public benchmarks demonstrate empirically that Coin-GAN outperforms state-of-the-art baselines. Furthermore, the visualized results and the distribution maps of the modality transition corroborate the effectiveness of CoinGAN in segmenting the pathological areas."
    },
    {
        "title": "Truth or Deceit? A Bayesian Decoding Game Enhances Consistency and Reliability",
        "link_suffix": "/forum?id=qLRaPfDPXK",
        "link": "https://openreview.net/forum?id=qLRaPfDPXK",
        "pdf_link": "https://openreview.net/pdf?id=qLRaPfDPXK",
        "keywords": "Mechanism Design, Large Language Models (LLMs), Generative Modeling, Alignment",
        "abstract": "Large Language Models (LLMs) often produce outputs that --  though plausible -- can lack consistency and reliability, particularly in ambiguous or complex scenarios. Challenges arise from ensuring that outputs align with both factual correctness and human intent. This is problematic in existing approaches that trade improved consistency for lower accuracy. To mitigate these challenges, we propose a novel game-theoretic approach to enhance consistency and reliability during the decoding stage of LLM output generation. Our method models the decoding process as a multistage Bayesian decoding game. This ensures consistency through \\textit{Correctness Alignment} and enhances reliability via \\textit{Ambiguity Calibration}. The model dynamically converges to a consensus on the most reliable outputs and distinguishes {Valid}, Specious}} outputs without human feedback or additional training. Remarkably, our game design allows smaller models to outperform much larger models through game mechanisms (\\textit{e.g.} 78.1 LLaMA13B \\textit{vs} 76.6 PaLM540B), as well as integrating various LL strategies and models, demonstrating the potential of game-theoretic tools to improve the truthfulness and reliability of LLMs."
    },
    {
        "title": "OR-Bench: An Over-Refusal Benchmark for Large Language Models",
        "link_suffix": "/forum?id=obYVdcMMIT",
        "link": "https://openreview.net/forum?id=obYVdcMMIT",
        "pdf_link": "https://openreview.net/pdf?id=obYVdcMMIT",
        "keywords": "over-refusal, llm, safety, false-refusal",
        "abstract": "Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, \nthe enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful.\nAlthough the issue of over-refusal has been empirically observed, a systematic measurement is challenging \ndue to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs.\nThis study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses.\nWe then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. \nTo facilitate reproducibility, we host our datasets, along with an interactive demo and leaderboard, on HuggingFace athttps://huggingface.co/spaces/orbench-llm/or-benchand release our code athttps://github.com/orbench/or-bench. We hope this benchmark can help the community develop better safety aligned models."
    },
    {
        "title": "3DAxisPrompt: Promoting the 3D Grounding and Reasoning in GPT-4o",
        "link_suffix": "/forum?id=IXOoltTofP",
        "link": "https://openreview.net/forum?id=IXOoltTofP",
        "pdf_link": "https://openreview.net/pdf?id=IXOoltTofP",
        "keywords": "visual prompt, 3D grounding, spatial casual reasoning, spatial grounding, .multimodal LLM",
        "abstract": "Multimodal Large Language Models (MLLMs) exhibit impressive capabilities across a variety of tasks, especially when equipped with carefully designed visual prompts. However, existing studies primarily focus on logical reasoning and visual understanding, while the capability of MLLMs to operate effectively in 3D vision remains an ongoing area of exploration.\nIn this paper, we introduce a novel visual prompting method called 3DAxisPrompt to elicit the 3D understanding capabilities of MLLMs in real-world scenes. More specifically, our method leverages the 3D coordinate axis and masks generated from the Segment Anything Model (SAM) to provide explicit geometric priors to MLLMs and then extend their impressive 2D grounding/reasoning ability to real-world 3D scenarios. Besides we also provide a thorough investigation of the potential visual prompting formats and conclude our findings to reveal the potential and limits of 3D understanding capabilities in GPT-4o. Finally, we build evaluation environments with four datasets, {\\it i.e.} ShapeNet, ScanNet, FMB, and nuScene datasets, covering various 3D tasks. Based on this, we conduct extensive quantitative and qualitative experiments, which demonstrate the effectiveness of the proposed method. Overall, our study reveals that GPT-4o, with the help of 3DAxisPrompt, can effectively perceive an object\u2019s 3D position in real-world scenarios. Nevertheless, a single prompt engineering approach does not consistently achieve the best outcomes for all 3D tasks. This study highlights the feasibility of leveraging MLLMs for 3D vision grounding/reasoning with prompt engineering techniques."
    },
    {
        "title": "Towards a formal theory of compositionality",
        "link_suffix": "/forum?id=hKMPz3wkPV",
        "link": "https://openreview.net/forum?id=hKMPz3wkPV",
        "pdf_link": "https://openreview.net/pdf?id=hKMPz3wkPV",
        "keywords": "compositionality, complexity, deep learning, representation, generalization",
        "abstract": "Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, it enables a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, there currently exists no formal definition for it that is measurable and mathematical. Here, we propose such a definition, which we call representational compositionality. The definition is conceptually simple, quantitative, and grounded in algorithmic information theory. Intuitively, representational compositionality states that a compositional representation is both expressive and describable as a simple function of discrete parts. We validate our definition on both real and synthetic data, and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools. Our definition has the potential to inspire the design of novel, theoretically-driven models that better capture the mechanisms of higher-level human thought."
    },
    {
        "title": "Adapting CLIP for DETR-based Object Detection",
        "link_suffix": "/forum?id=OpSMgpBubj",
        "link": "https://openreview.net/forum?id=OpSMgpBubj",
        "pdf_link": "https://openreview.net/pdf?id=OpSMgpBubj",
        "keywords": "Object detection, contrastive learning, open-vocabulary object detection",
        "abstract": "Object detection involves class identification and spatial positioning. While DETR-based architectures have shown promising detection capabilities by framing the task as set prediction, prior approaches have limited refinement for object features, leading to inferior inherent understanding of objects, particularly when generalizing to unseen categories. To this end, we propose CLIP-DETR, a novel detection framework that harnesses the pretrained visual-linguistic capabilities of CLIP to enhance both the encoding and decoding processes in DETR models. Our method focuses on two key principles: 1) feature map sensitivity to objects, and 2) query adaptability. Extensive experiments demonstrate that CLIP-DETR significantly outperforms state-of-the-art models in object detection and open-vocabulary detection tasks, illustrating its superior generalization and recognition abilities."
    },
    {
        "title": "Motion Inversion for Video Customization",
        "link_suffix": "/forum?id=LtBD5fFHB7",
        "link": "https://openreview.net/forum?id=LtBD5fFHB7",
        "pdf_link": "https://openreview.net/pdf?id=LtBD5fFHB7",
        "keywords": "Video Customization",
        "abstract": "In this work, we present a novel approach for motion customization in video generation, addressing the widespread gap in the exploration of motion representation within video generative models. Recognizing the unique challenges posed by the spatiotemporal nature of video, our method introduces Motion Embeddings, a set of explicit, temporally coherent embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach provides a compact and efficient solution to motion representation, utilizing two types of embeddings: a Motion Query-Key Embedding to modulate the temporal attention map and a Motion Value Embedding to modulate the attention values. Additionally, we introduce an inference strategy that excludes spatial dimensions from the Motion Query-Key Embedding and applies a differential operation to the Motion Value Embedding, both designed to debias appearance and ensure the embeddings focus solely on motion. Our contributions include the introduction of a tailored motion embedding for customization tasks and a demonstration of the practical advantages and effectiveness of our method through extensive experiments."
    },
    {
        "title": "InterLCM: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration",
        "link_suffix": "/forum?id=rUxr9Ll5FQ",
        "link": "https://openreview.net/forum?id=rUxr9Ll5FQ",
        "pdf_link": "https://openreview.net/pdf?id=rUxr9Ll5FQ",
        "keywords": "diffusion model, face restoration",
        "abstract": "With diffusion models (DMs) to powerfully estimate data distributions, diffusion priors have been explored to address the blind face restoration (BFR) problem. They normally fine-tune a diffusion model on restoration datasets to recover low-quality images. \nHowever, the naive application of DMs presents several key limitations. \n(i) The diffusion prior has inferior semantic consistency (e.g., ID, structure and color.),  increasing the difficulty of optimizing the BFR model.(ii) the diffusion models require hundreds of iterations in the denoising phase, as such they cannot incorporate (perceptual) losses in the image domain. These losses are crucial for BFR where faithfulness of utmost importance.As a replacement to the conventional diffusion model priors, we observe that the latent consistency model (LCM) shows more semantic consistency in the subject identity, structural information and color preservation.\nBased on this observation, we consider the low-quality image as the $\\textit{intermediate state of LCM models}$ for efficient blind face restoration, which we termed $\\textit{InterLCM}$ . We found that starting from the early step of LCM is helpful to achieve the trade-off between the fidelity and quality of the generated images.\nSince the LCM model maps the noisy latent space to real data during each time step, we can also successfully integrate perceptual loss during training of \\ourmethod, an approach not yet explored by existing methods, leading to substantial improvements in image quality, especially when applied in real-world scenarios.\nHowever, the straightforward application of the few-step diffusion model introduces uncertainty to the structure and semantics of the restored image. \nIn our method $\\textit{InterLCM}$ , the low-quality image is further imported into a Visual Module to extract visual features and a Spatial Encoder to extract spatial information to enhance fidelity.\nExtensive experiments demonstrate that $\\textit{InterLCM}$ outperforms existing approaches in both synthetic and real-world datasets while also achieving faster inference speed. The source code and models will be public."
    },
    {
        "title": "Supervised Dimension Contrastive Learning",
        "link_suffix": "/forum?id=suJ1z1UX2t",
        "link": "https://openreview.net/forum?id=suJ1z1UX2t",
        "pdf_link": "https://openreview.net/pdf?id=suJ1z1UX2t",
        "keywords": "Supervised Representation Learning, Dimension Contrastive Learning",
        "abstract": "Self-supervised learning has emerged as an effective pre-training strategy for representation learning using large-scale unlabeled data. However, models pre-trained with self-supervised learning still require supervised fine-tuning to achieve optimal task-specific performance. Due to the lack of label utilization, it is difficult to accurately distinguish between positive and hard negative samples. Supervised contrastive learning methods address the limitation by leveraging labels, but they focus on global representations, leading to limited feature diversity and high cross-correlation between representation dimensions. To address these challenges, we propose Supervised Dimension Contrastive Learning, a novel approach that combines supervision with dimension-wise contrastive learning. Inspired by redundancy reduction techniques like Barlow Twins, this approach reduces cross-correlation between embedding dimensions while enhancing class discriminability. The aggregate function combines the embedding dimensions to generate predicted class variables, which are optimized to correlate with their corresponding class labels. Orthogonal regularization is applied to ensure the full utilization of all dimensions by enforcing full-rankness in the aggregate function. We evaluate our method on both in-domain supervised classification tasks and out-of-domain transfer learning tasks, demonstrating its superior performance compared to traditional supervised learning, supervised contrastive learning, and self-supervised learning methods. Our results show that the proposed method effectively reduces inter-dimensional correlation and enhances class discriminability, proving its generalizability across various downstream tasks."
    },
    {
        "title": "MagicTailor: Component-Controllable Personalization in Text-to-Image Diffusion Models",
        "link_suffix": "/forum?id=GwSL33Qx42",
        "link": "https://openreview.net/forum?id=GwSL33Qx42",
        "pdf_link": "https://openreview.net/pdf?id=GwSL33Qx42",
        "keywords": "Diffusion Models, Text-to-Image Generation, Personalization",
        "abstract": "Recent advancements in text-to-image (T2I) diffusion models have enabled the creation of high-quality images from text prompts, but they still struggle to generate images with precise control over specific visual concepts. Existing approaches can replicate a given concept by learning from reference images, yet they lack the flexibility for fine-grained customization of the individual component within the concept. In this paper, we introduce component-controllable personalization, a novel task that pushes the boundaries of T2I models by allowing users to reconfigure and personalize specific components of concepts. This task is particularly challenging due to two primary obstacles: semantic pollution, where unwanted visual elements corrupt the personalized concept, and semantic imbalance, which causes disproportionate learning of visual semantics. To overcome these challenges, we design MagicTailor, an innovative framework that leverages Dynamic Masked Degradation (DM-Deg) to dynamically perturb undesired visual semantics and Dual-Stream Balancing (DS-Bal) to establish a balanced learning paradigm for visual semantics. Extensive comparisons, ablations, and analyses demonstrate that MagicTailor not only excels in this challenging task but also holds significant promise for practical applications, paving the way for more nuanced and creative image generation. Our code will be released."
    },
    {
        "title": "In-context learning and Occam's razor",
        "link_suffix": "/forum?id=2PKLRmU7ne",
        "link": "https://openreview.net/forum?id=2PKLRmU7ne",
        "pdf_link": "https://openreview.net/pdf?id=2PKLRmU7ne",
        "keywords": "generalization, complexity, compression, in-context learning, meta-learning",
        "abstract": "The goal of machine learning is generalization. While the No Free Lunch Theorem states that we cannot obtain theoretical guarantees for generalization without further assumptions, in practice we observe that simple models which explain the training data generalize best\u2014a principle called Occam's razor. Despite the need for simple models, most current approaches in machine learning only minimize the training error, and at best indirectly promote simplicity through regularization or architecture design. Here, we draw a connection between Occam's razor and in-context learning\u2014an emergent ability of certain sequence models like Transformers to learn at inference time from past observations in a sequence. In particular, we show that the next-token prediction loss used to train in-context learners is directly equivalent to a data compression technique called prequential coding, and that minimizing this loss amounts to jointly minimizing both the training error and the complexity of the model that was implicitly learned from context. Our theory and the empirical experiments we use to support it not only provide a normative account of in-context learning, but also elucidate the shortcomings of current in-context learning methods, suggesting ways in which they can be improved."
    },
    {
        "title": "Generalization Bounds for Canonicalization: A Comparative Study with Group Averaging",
        "link_suffix": "/forum?id=n0lXaskyk5",
        "link": "https://openreview.net/forum?id=n0lXaskyk5",
        "pdf_link": "https://openreview.net/pdf?id=n0lXaskyk5",
        "keywords": "invariances, group, symmetry, canonicalization",
        "abstract": "Canonicalization, a popular method for generating invariant or equivariant function classes from arbitrary function sets, involves initial data projection onto a reduced input space subset, followed by applying any learning method to the projected dataset. Despite recent research on the expressive power and continuity of functions represented by canonicalization, its generalization capabilities remain less explored. This paper addresses this gap by theoretically examining the generalization benefits and sample complexity of canonicalization, comparing them with group averaging, another popular technique for creating invariant or equivariant function classes. Our findings reveal two distinct regimes where canonicalization may outperform or underperform compared to group averaging, with precise quantification of this phase transition in terms of sample size and group action characteristics. To the best of our knowledge, this study represents the first theoretical exploration of such behavior, offering insights into the relative effectiveness of canonicalization and group averaging under varying conditions."
    },
    {
        "title": "VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models",
        "link_suffix": "/forum?id=acxHV6werE",
        "link": "https://openreview.net/forum?id=acxHV6werE",
        "pdf_link": "https://openreview.net/pdf?id=acxHV6werE",
        "keywords": "large language models, evaluation",
        "abstract": "Large language models (LLMs) often exhibit subtle yet distinctive characteristics in their outputs that users intuitively recognize, but struggle to quantify. These \"vibes\" - such as tone, formatting, or writing style - frequently influence user preferences, yet traditional evaluations focus primarily on comparing models based on correctness.\nWe introduce VibeCheck, a system for automatically comparing a pair of LLMs by discovering identifying traits of a model (\"vibes\") which are well-defined, differentiating, and user-aligned. VibeCheck employs an iterative approach which utilizes a panel of LLM judges to discover vibes from model outputs and quantitatively measure the utility of a vibe across these 3 criteria. \nWe validate that the vibes generated by VibeCheck align with those found in human discovery and run VibeCheck on pairwise preference data from real-world user conversations with llama-3-70b VS GPT-4. VibeCheck reveals that Llama has a friendly, funny, and somewhat controversial vibe. These vibes can be used to predict human preference with 65% accuracy and model identity with 80% accuracy. Lastly, we run VibeCheck on a variety of models and tasks including summarization, math, and captioning to provide further insight into differences in model behavior."
    },
    {
        "title": "ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding",
        "link_suffix": "/forum?id=o5TsWTUSeF",
        "link": "https://openreview.net/forum?id=o5TsWTUSeF",
        "pdf_link": "https://openreview.net/pdf?id=o5TsWTUSeF",
        "keywords": "Multimodal Large Language Models, Chart Reasoning, Mixture of Expert",
        "abstract": "Automatic chart understanding is crucial for content comprehension and document parsing. Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in chart understanding through domain-specific alignment and fine-tuning. However, current MLLMs still struggle to provide faithful data and reliable analysis only based on charts. To address it, we propose ChartMoE, which employs the Mixture of Expert (MoE) architecture to replace the traditional linear projector to bridge the modality gap. Specifically, we train several linear connectors through distinct alignment tasks, which are utilized as the foundational initialization parameters for different experts. Additionally, we introduce ChartMoE-Align, a dataset with nearly 1 million chart-table-JSON-code quadruples to conduct three alignment tasks (chart-table/JSON/code). Combined with the vanilla connector, we initialize different experts diversely and adopt high-quality knowledge learning to further refine the MoE connector and LLM parameters. Extensive experiments demonstrate the effectiveness of the MoE connector and our initialization strategy, e.g., ChartMoE improves the accuracy of the previous state-of-the-art from 80.48% to 84.64% on the ChartQA benchmark."
    },
    {
        "title": "Computational Explorations of Total Variation Distance",
        "link_suffix": "/forum?id=xak8c9l1nu",
        "link": "https://openreview.net/forum?id=xak8c9l1nu",
        "pdf_link": "https://openreview.net/pdf?id=xak8c9l1nu",
        "keywords": "total variation distance, TV distance, mixtures of products, equivalence checking, Ising models, computational complexity, FPRAS",
        "abstract": "We investigate some previously unexplored (or under-explored) computational aspects of total variation (TV) distance.\nFirst, we give a simple deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions, over arbitrary alphabets.\nThis corresponds to a special case, whereby the TV distance between the two distributions is zero.\nSecond, we prove that unless $\\mathsf{NP} \\subseteq \\mathsf{RP}$ it is impossible to efficiently estimate the TV distance between arbitrary Ising models, even in a bounded-error randomized setting."
    },
    {
        "title": "3D-SPATIAL MULTIMODAL MEMORY",
        "link_suffix": "/forum?id=XYdstv3ySl",
        "link": "https://openreview.net/forum?id=XYdstv3ySl",
        "pdf_link": "https://openreview.net/pdf?id=XYdstv3ySl",
        "keywords": "Foundation Model, Gaussian Splatting, Large Multimodal Model, Robotics",
        "abstract": "We present 3D spatial MultiModal Memory (M3), a video memory system designed to retain information over medium time windows and spatial horizons for visual perception. We integrate 3D Gaussian splatting techniques and various foundation models to build the Multimodal Memory that can span visual granularity and cover a wide range of knowledge space. In our exploration, we identify two key challenges in previous works on feature splatting via feature distillation: (1) computational constraints for storing high-dimensional features within each Gaussian primitive, and (2) misalignment between the distilled features and the knowledge space of foundation models. To address these challenges while achieving our goals, \\ourmodel\\ introduces the concept of principle scene components and Gaussian memory attention, enabling efficient storage, training, and inference of the Gaussian Splatting Model. Additionally, we evaluate a diverse set of foundation models, including vision-language models, LMM/LLMs, and self-supervised models, this further proves the effectiveness of our approach. We also deploy \\ourmodel\\ on a quadruped robot for grasping, demonstrating its potential on real-world applications."
    },
    {
        "title": "Learning Representations of Intermittent Temporal Latent Process",
        "link_suffix": "/forum?id=6Pz7afmsOp",
        "link": "https://openreview.net/forum?id=6Pz7afmsOp",
        "pdf_link": "https://openreview.net/pdf?id=6Pz7afmsOp",
        "keywords": "unsupervised representation learning",
        "abstract": "Identifying time-delayed latent causal process is crucial for understanding temporal dynamics and enabling downstream reasoning. While recent methods have made progress in identifying latent time-delayed causal processes, they cannot address the dynamics in which the influence of some latent factors on both the subsequent latent states and the observed data can become inactive or irrelevant at different time steps. Therefore, we introduce intermittent temporal latent processes, where: (1) any subset of latent factors may be missing during nonlinear data generation at any time step, and (2) the active latent factors at each step are unknown. This framework encompasses both nonstationary and stationary transitions, accommodating changing or consistent active factors over time. \nOur work shows that under certain assumptions, the latent causal variables are block-wise identifiable. With further conditional independence assumption, each latent variable can even be recovered up to component-wise transformations. \nUsing this identification theory, we propose an unsupervised approach, InterLatent, to reliably uncover the representations of the intermittent temporal latent process. The experimental findings on both synthetic and real-world datasets verify our theoretical claims."
    },
    {
        "title": "Heterogeneous Federated Learning: A Dual Matching Dataset Distillation Approach",
        "link_suffix": "/forum?id=In0phMs7BK",
        "link": "https://openreview.net/forum?id=In0phMs7BK",
        "pdf_link": "https://openreview.net/pdf?id=In0phMs7BK",
        "keywords": "Federated Learning; Dataset Distillation; Heterogeneous;",
        "abstract": "Federated Learning (FL) often struggles with error accumulation during local training, particularly on heterogeneous data, which hampers overall performance and convergence. While dataset distillation is commonly introduced to FL to enhance efficiency, our work finds that communicating distilled data instead of models can completely get rid of the error accumulation issue, albeit at the cost of exacerbating data heterogeneity across clients. To address the amplified heterogeneity due to distilled data, we propose a novel FL algorithm termed FedDualMatch, which performs dual matching in the way that local distribution matching captures client data distributions while global gradient matching aligns gradients on the server. This dual approach enriches feature representations and enhances convergence stability. It proves effective for FL due to a bounded difference in the testing loss between optimal models trained on the aggregation of either distilled or original data across clients. At the same time, it converges faster than FedAvg in a single communication round while preserving (\u03f5, \u03b4)-differential privacy via adding Gaussian noise. Experiments on controlled heterogeneous dataset MNIST/CIFAR10 and naturally heterogeneous dataset Digital-Five/Office-Home demonstrate its advantages over the state-of-the-art methods that communicate either model or distilled data, in terms of accuracy and convergence. Notably, it maintains accuracy even when data heterogeneity significantly increases, underscoring its potential for practical applications."
    },
    {
        "title": "Enhancing Robustness of Vision-Language Models through Orthogonality Learning and Self-Regularization",
        "link_suffix": "/forum?id=JetCx7Tpgb",
        "link": "https://openreview.net/forum?id=JetCx7Tpgb",
        "pdf_link": "https://openreview.net/pdf?id=JetCx7Tpgb",
        "keywords": "vision-language model; few-shot; fine-tuning",
        "abstract": "Efficient fine-tuning of vision-language models (VLMs) like CLIP for specific downstream tasks is gaining significant attention. Previous works primarily focus on prompt learning to adapt the CLIP into a variety of downstream tasks, however, suffering from task overfitting when fine-tuned on a small data set. In this paper, we introduce an orthogonal fine-tuning method for efficiently fine-tuning pretrained weights and enabling enhanced robustness and generalization, while a self-regularization strategy is further exploited to maintain the stability in terms of zero-shot generalization of VLMs, dubbed OrthSR. Specifically, trainable orthogonal matrices are injected seamlessly into the transformer architecture and enforced with orthogonality constraint during the training, benefiting from the norm-preserving property and thus leading to stable and faster convergence, while keeping the pre-trained weights frozen. \nTo alleviate deviation from fine-tuning, \na self-regularization strategy is further employed to retain the generalization of the model during the training within a bypass manner. In addition, to enrich the sample diversity for downstream tasks under the small dataset scenario, we first explore attentive CutOut data augmentation to boost the efficient fine-tuning, leading to better model fitting capacity for specific downstream task. Then we support the theoretical analysis on how our approach improves the specific downstream performance and maintains the generalizability. For the first time, we revisit the CLIP and CoOp with our method to effectively improve the model on few-shot image classficiation scenario on par with the elaborated prompt learning methods. We conduct extensive experiments to demonstrate that our method explicitly steers pretrained weight space to represent the task-specific knowledge and presents competitive generalizability under base-to-base/base-to-new, cross-dataset transfer and domain generalization evaluations."
    }
]