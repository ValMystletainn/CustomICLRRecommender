[
    {
        "title": "Closed-Form Interpretation of Neural Network Latent Spaces with Symbolic Gradients",
        "link_suffix": "/forum?id=ZrnzGzUhNX",
        "link": "https://openreview.net/forum?id=ZrnzGzUhNX",
        "pdf_link": "https://openreview.net/pdf?id=ZrnzGzUhNX",
        "keywords": "Artificial Neural Networks, Symbolic Regression, Interpretation of Neural Networks",
        "abstract": "It has been demonstrated in many scientific fields that artificial neural networks, like autoencoders or Siamese networks, encode meaningful concepts in their latent spaces. However, there does not exist a comprehensive framework for retrieving this information in a human-readable form without prior knowledge. In order to extract these concepts, we introduce a framework for finding closed-form interpretations of neurons in latent spaces of artificial neural networks. The interpretation framework is based on embedding trained neural networks into an equivalence class of functions that encode the same concept. We interpret these neural networks by finding an intersection between the equivalence class and human-readable equations defined by a symbolic search space. The effectiveness of our approach is demonstrated by retrieving invariants of matrices and conserved quantities of dynamical systems from latent spaces of Siamese neural networks."
    },
    {
        "title": "Group Distributionally Robust Dataset Distillation with Risk Minimization",
        "link_suffix": "/forum?id=3JsU5QXNru",
        "link": "https://openreview.net/forum?id=3JsU5QXNru",
        "pdf_link": "https://openreview.net/pdf?id=3JsU5QXNru",
        "keywords": "dataset distillation, distributional robustness, generalization",
        "abstract": "Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from regions with low population density? Here, the representativeness and coverage of the dataset become salient over the guaranteed training error at inference. Drawing inspiration from distributionally robust optimization, we introduce an algorithm that combines clustering with the minimization of a risk measure on the loss to conduct DD. We provide a theoretical rationale for our approach and demonstrate its effective generalization and robustness across subgroups through numerical experiments."
    },
    {
        "title": "METRIS: Multi-Expressions for Transformer-based Referring Image Segmentation",
        "link_suffix": "/forum?id=s5TNZPKWy5",
        "link": "https://openreview.net/forum?id=s5TNZPKWy5",
        "pdf_link": "https://openreview.net/pdf?id=s5TNZPKWy5",
        "keywords": "Multi-expression guidance framework, Target-oriented visual expression, Vision-Language representation, Transformer-based Segmentation",
        "abstract": "Referring image segmentation (RIS) aims to precisely segment a target object described by a linguistic expression. Recent RIS methods have introduced Transformer-based networks that use vision features as query and linguistic expression features as key-value to find target regions by referring to the given linguistic information. Since the Transformer-based network predicts based on the guidance information that guides the network on which regions to pay attention, the capacity of this guidance information has a significant impact on segmentation results in Transformer-based RIS. However, existing methods rely only on linguistic tokens as the guidance elements, which are limited in providing the visual understanding of the fine-grained target regions. To address this issue, we present a novel Multi-Expression guidance framework for Transformer-based Referring Image Segmentation, METRIS, which allows the network to refer to the visual expression tokens as the guidance information alongside the linguistic expression tokens. The introduction of visual expression can complement the capability of linguistic guidance by effectively providing the target-informative visual contexts. To produce semantic visual expression, we introduce a visual expression extractor that is designed to endow with the target-informative visual guidance ability and to acquire rich contextual information. This module strengthens the adaptability to the diverse image and language inputs, and improves visual understanding of the fine-grained target regions. Extensive experiments demonstrate the effectiveness of our approach across the commonly used RIS settings and the generalizability evaluation settings. Our method consistently shows strong performance on three public RIS benchmarks, where it surpasses the state-of-the-art methods."
    },
    {
        "title": "Divide-Verify-Refine: Aligning LLM Responses with Complex Instructions",
        "link_suffix": "/forum?id=zfQA8y3n2o",
        "link": "https://openreview.net/forum?id=zfQA8y3n2o",
        "pdf_link": "https://openreview.net/pdf?id=zfQA8y3n2o",
        "keywords": "Large Language Model, Instruction Following, Constraints Following",
        "abstract": "Recent studies show that LLMs, particularly open-source models, struggle to follow complex instructions with multiple constraints, hindering their adoption in mission-critical applications. Despite the importance, methods to improve LLMs' adherence to such constraints remain largely unexplored, and current research focuses primarily on evaluating this ability rather than developing solutions. While a few studies enhance constraint adherence through model tuning, this approach is computationally expensive and heavily reliant on training data quality. An alternative is to leverage  LLMs' self-correction capabilities, allowing them to adjust responses to better meet specified constraints. However, this self-correction ability of LLMs is limited by the feedback quality, as LLMs cannot autonomously generate reliable feedback or detect errors. Moreover, the self-refinement process heavily depends on few-shot examples that illustrate how to modify responses to meet constraints. As constraints in complex instructions are diverse and vary widely (e.g., text length,  number of bullet points, or  inclusion of specific keywords), manually crafting few-shot examples for each constraint type can be labor-intensive and sub-optimal. To deal with these two challenges, we propose the Divide-Verify-Refine (DVR) framework with three steps: (1) Divide complex instructions into single constraints and prepare appropriate tools; (2) Verify: To address the feedback quality problem, these tools will rigorously verify responses and provide reliable feedback (e.g., Python scripts for format checking or pre-trained classifiers for content analysis); (3) Refine: To address the constraint diversity challenge, we design a refinement repository that collects successful refinement processes and uses them as few-shot demonstrations for future cases, allowing LLMs to learn from the past experience during inference. Additionally, recognizing that existing datasets lack complexity and have internal conflict, we develop a new dataset of complex instructions, each containing 1-6 constraints. Experiments show that the framework significantly improves performance, doubling LLama3.1-8B's constraint adherence and tripling Mistral-7B's performance on instructions with 6 constraints. The code and dataset are available athttps://anonymous.4open.science/r/CODE_ICLR2025-52CE/README.md"
    },
    {
        "title": "CapeX: Category-Agnostic Pose Estimation from Textual Point Explanation",
        "link_suffix": "/forum?id=scKAXgonmq",
        "link": "https://openreview.net/forum?id=scKAXgonmq",
        "pdf_link": "https://openreview.net/pdf?id=scKAXgonmq",
        "keywords": "2D Pose Estimation, Class-Agnostic, Few-Shot, Vision-Language Model",
        "abstract": "Conventional 2D pose estimation models are constrained by their design to specific object categories. This limits their applicability to predefined objects. To overcome these limitations, category-agnostic pose estimation (CAPE) emerged as a solution. CAPE aims to facilitate keypoint localization for diverse object categories using a unified model, which can generalize from minimal annotated support images.\nRecent CAPE works have produced object poses based on arbitrary keypoint definitions annotated on a user-provided support image. Our work departs from conventional CAPE methods, which require a support image, by adopting a text-based approach instead of the support image. \nSpecifically, we use a pose-graph, where nodes represent keypoints that are described with text. This representation takes advantage of the abstraction of text descriptions and the structure imposed by the graph.\nOur approach effectively breaks symmetry, preserves structure, and improves occlusion handling.\nWe validate our novel approach using the MP-100 benchmark, a comprehensive dataset covering over 100 categories and 18,000 images. MP-100 is structured so that the evaluation categories are unseen during training, making it especially suited for CAPE.  Under a 1-shot setting, our solution achieves a notable performance boost of 1.27%, establishing a new state-of-the-art for CAPE. Additionally, we enhance the dataset by providing text description annotations for both training and testing. We also include alternative text annotations specifically for testing the model's ability to generalize across different textual descriptions, further increasing its value for future research. Our code will be made publicly available, and the dataset extension of text descriptions are in the supplementary material."
    },
    {
        "title": "Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation",
        "link_suffix": "/forum?id=kZvor5aaz7",
        "link": "https://openreview.net/forum?id=kZvor5aaz7",
        "pdf_link": "https://openreview.net/pdf?id=kZvor5aaz7",
        "keywords": "object-centric learning, slot attention, unsupervised object discovery, object segmentation",
        "abstract": "We present SlotAdapt, an object-centric learning method that combines slot attention with pretrained diffusion models by introducing adapters for slot-based conditioning. Our method preserves the generative power of pretrained diffusion models, while avoiding their text-centric conditioning bias. We also incorporate an additional guidance loss into our architecture  to align cross-attention from adapter layers with slot attention. This enhances the alignment of our model with the objects in the input image without using external supervision. Experimental results show that our method outperforms state-of-the-art techniques in object discovery and image generation tasks across multiple datasets, including those with real images. Furthermore, we demonstrate through experiments that our method performs remarkably well on complex real-world images for compositional generation, in contrast to other slot-based generative methods in the literature."
    },
    {
        "title": "Strategic Classification With Externalities",
        "link_suffix": "/forum?id=o6CXkEEttn",
        "link": "https://openreview.net/forum?id=o6CXkEEttn",
        "pdf_link": "https://openreview.net/pdf?id=o6CXkEEttn",
        "keywords": "strategic classification, game theory, strategic machine learning, algorithmic game theory",
        "abstract": "We propose a new variant of the strategic classification problem: a principal reveals a classifier, and $n$ agents report their (possibly manipulated) features to be classified. Motivated by real-world applications, our model crucially allows the manipulation of one agent to affect another; that is, it explicitly captures inter-agent externalities. The principal-agent interactions are formally modeled as a Stackelberg game, with the resulting agent manipulation dynamics captured as a simultaneous game. We show that under certain assumptions, the pure Nash Equilibrium of this agent manipulation game is unique and can be efficiently computed. Leveraging this result, PAC learning guarantees are established for the learner: informally, we show that it is possible to learn classifiers that minimize loss on the distribution, even when a random number of agents are manipulating their way to a pure Nash Equilibrium. We also comment on the optimization of such classifiers through gradient-based approaches. This work sets the theoretical foundations for a more realistic analysis of classifiers that are robust against multiple strategic actors interacting in a common environment."
    },
    {
        "title": "Low-Rank Correction for Quantized LLMs",
        "link_suffix": "/forum?id=FA3iYp1y6z",
        "link": "https://openreview.net/forum?id=FA3iYp1y6z",
        "pdf_link": "https://openreview.net/pdf?id=FA3iYp1y6z",
        "keywords": "Quantization, LLM, Low-rank",
        "abstract": "We consider the problem of model compression for Large Language Models (LLMs) at post-training time, where the task is to compress a well-trained model using only a small set of calibration input data. \nIn this work, we introduce a new low-rank approach to correct for quantization errors of both weights and activations in LLMs: we propose to add low-rank weight matrices in full precision that act on the \\emph{unquantized} activations. We then solve a joint optimization problem over the quantized representation of the weights and additional low-rank weight matrices.\nWe focus on the case of 4-bit weight-and-activation quantization (W4A4). Using ranks equivalent to 10% of the original weight matrix size, our approach reduces the accuracy gap with the original model by more than 50%. Using ranks equivalent to 30% of the original weight matrix,  the accuracy gap is closed completely. We demonstrate our results on three recent LLMs, namely Llama-3, Phi-3 and Mixtral models."
    },
    {
        "title": "Listening to Formulas: Pioneering Models and Datasets for Converting Speech to LaTeX Equations",
        "link_suffix": "/forum?id=pflsJ6V6CL",
        "link": "https://openreview.net/forum?id=pflsJ6V6CL",
        "pdf_link": "https://openreview.net/pdf?id=pflsJ6V6CL",
        "keywords": "speech recognition, LLM, LaTeX, speech to text, ASR, STT",
        "abstract": "Recognizing spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in both automatic speech recognition (ASR) and language models (LM), the specific problem of translating spoken formulas into LaTeX has received relatively little attention. This task is particularly important in educational and research domains, for example, for lecture transcription. To address this issue, in this paper, we present a pioneering study on Speech-to-LaTeX conversion, introducing a novel, diverse human-uttered dataset in English and Russian comprising 16000 (10000 in English and 6000 in Russian) distinct spoken equations uttered by 3 different speakers. Our approaches, which incorporate ASR post-correction and multi-modal language models, demonstrate a notable performance with up to a 25% character error rate (CER)."
    },
    {
        "title": "C-CLIP: Multimodal Continual Learning for Vision-Language Model",
        "link_suffix": "/forum?id=sb7qHFYwBc",
        "link": "https://openreview.net/forum?id=sb7qHFYwBc",
        "pdf_link": "https://openreview.net/pdf?id=sb7qHFYwBc",
        "keywords": "Vision-Language Models, Continual Representation Learning",
        "abstract": "Multimodal pre-trained models like CLIP need large image-text pairs for training but often struggle with domain-specific tasks. Since retraining with specialized and historical data incurs significant memory and time costs, it is important to continually learn new domains in the open world while preserving original performance. However, current continual learning research mainly focuses on single-modal scenarios, and the evaluation criteria are insufficient without considering image-text matching performance and the forgetting of zero-shot performance. This work introduces image-caption datasets from various domains and establishes a multimodal vision-language continual learning benchmark. Then, a novel framework named C-CLIP is proposed, which not only prevents forgetting but also enhances new task learning impressively. Comprehensive experiments demonstrate that our method has strong continual learning ability across different domain image-text datasets, and has little forgetting of the original capabilities of zero-shot prediction,  significantly outperforming existing methods."
    },
    {
        "title": "SpinQuant: LLM Quantization with Learned Rotations",
        "link_suffix": "/forum?id=ogO6DGE6FZ",
        "link": "https://openreview.net/forum?id=ogO6DGE6FZ",
        "pdf_link": "https://openreview.net/pdf?id=ogO6DGE6FZ",
        "keywords": "rotation, information retrieval",
        "abstract": "Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points."
    },
    {
        "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
        "link_suffix": "/forum?id=L4nH3j7L94",
        "link": "https://openreview.net/forum?id=L4nH3j7L94",
        "pdf_link": "https://openreview.net/pdf?id=L4nH3j7L94",
        "keywords": "Large Multimodal Models, LMM Evaluation, Preference Learning",
        "abstract": "We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator to assess performance across a wide range of multimodal tasks. LLaVA-Critic is trained using a high-quality critic instruction-following dataset that incorporates diverse evaluation criteria and scenarios. Our experiments demonstrate the model's effectiveness in two key areas: $(i)$ LMM-as-a-Judge, where LLaVA-Critic provides reliable evaluation scores, performing on par with or surpassing GPT models on multiple evaluation benchmarks; and $(ii)$ Preference Learning, where it generates reward signals for preference learning, enhancing model alignment capabilities. This work underscores the potential of open-source LMMs in self-critique and evaluation, setting the stage for future research into scalable, superhuman alignment feedback mechanisms for LMMs."
    },
    {
        "title": "Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient Manipulation",
        "link_suffix": "/forum?id=pAkQhhn4vB",
        "link": "https://openreview.net/forum?id=pAkQhhn4vB",
        "pdf_link": "https://openreview.net/pdf?id=pAkQhhn4vB",
        "keywords": "Micro-batch Clipping, Convergence Analysis",
        "abstract": "Micro-batch clipping, a gradient clipping method, has recently shown potential in enhancing auto-speech recognition (ASR) model performance. However, the underlying mechanism behind this improvement remains mysterious, particularly the observation that only certain micro-batch sizes are beneficial. In this paper, we make the first attempt to explain this phenomenon. Inspired by recent data pruning research, we assume that specific training samples may impede model convergence during certain training phases. Under this assumption, the convergence analysis shows that micro-batch clipping can improve the convergence rate asymptotically at the cost of an additional constant bias that does not diminish with more training iterations. The bias is dependent on a few factors and can be minimized at specific micro-batch size, thereby elucidating the existence of the sweet-spot micro-batch size observed previously. We also verify the effectiveness of micro-batch clipping beyond speech models on vision and language models, and show promising performance gains in these domains. An exploration of potential limitations shows that micro-batch clipping is less effective when training data originates from multiple distinct domains."
    },
    {
        "title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?",
        "link_suffix": "/forum?id=1Xg4JPPxJ0",
        "link": "https://openreview.net/forum?id=1Xg4JPPxJ0",
        "pdf_link": "https://openreview.net/pdf?id=1Xg4JPPxJ0",
        "keywords": "Transformer; Chain-of-Thought; In-Context-Learning; Compositional Generalization",
        "abstract": "Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C = g(f(A)) ) effortlessly, even without encountering ( AC ) or ( ABC ) together, showcasing their compositional generalization ability.  In this paper, we introduce a learning task, \"FTCT\" (Fragmented at Training, Chained at Testing), to assess if Transformers can replicate this skill. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and data's relative knowledge ratio. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing."
    },
    {
        "title": "WHAT YOU PAINT IS WHAT YOU GET",
        "link_suffix": "/forum?id=jUKNY4u11K",
        "link": "https://openreview.net/forum?id=jUKNY4u11K",
        "pdf_link": "https://openreview.net/pdf?id=jUKNY4u11K",
        "keywords": "adversarial resilience, painting algorithms, adversarial manipulation, defense mechanisms",
        "abstract": "The two most prominent approaches for building adversary-resilient image classification models are adversarial training and input transformations. Despite significant advancements, adversarial training approaches struggle to generalize to unseen attacks, and the effectiveness of input transformations diminishes fast in the face of large perturbations. In general, there is a large space for improving the inherent trade-off between the accuracy and robustness of adversary-resilient models. Painting algorithms, which have not been used in adversarial training pipelines so far, capture core visual elements of images and offer a potential solution to the challenges faced by current defenses. This paper reveals a correlation between the magnitude of perturbations and the granularity of the painting process required to maximize the classification accuracy. We leverage this correlation in the proposed Painter-CLassifier-Decisioner (PCLD) framework, which employs adversarial training to build an ensemble of classifiers applied to a sequence of paintings with varying detalization. Benchmarks using provable adaptive attack techniques demonstrate the favorable performance of PCLD compared to state-of-the-art defenses, balancing accuracy and robustness while generalizing to unseen attacks. It extends robustness against substantial perturbations in high-resolution settings across various white-box attack methods under $\\ell_\\infty$-norm constraints."
    },
    {
        "title": "Adding Conditional Control to Diffusion Models with Reinforcement Learning",
        "link_suffix": "/forum?id=svp1EBA6hA",
        "link": "https://openreview.net/forum?id=svp1EBA6hA",
        "pdf_link": "https://openreview.net/pdf?id=svp1EBA6hA",
        "keywords": "diffusion models, conditional generation, fine-tuning, reinforcement learning",
        "abstract": "Diffusion models are powerful generative models that allow for precise control over the characteristics of the generated samples. While these diffusion models trained on large datasets have achieved success, there is often a need to introduce additional controls in downstream fine-tuning processes, treating these powerful models as pre-trained diffusion models. This work presents a novel method based on reinforcement learning (RL) to add such controls using an offline dataset comprising inputs and labels. We formulate this task as an RL problem, with the classifier learned from the offline dataset and the KL divergence against pre-trained models serving as the reward functions. Our method,CTRL(Conditioning pre-Trained diffusion models withReinforcementLearning), produces soft-optimal policies that maximize the abovementioned reward functions. We formally demonstrate that our method enables sampling from the conditional distribution with additional controls during inference.\nOur RL-based approach offers several advantages over existing methods. Compared to classifier-free guidance,\nit improves sample efficiency and can greatly simplify dataset construction by leveraging conditional independence between the inputs and additional controls. Additionally, unlike classifier guidance, it eliminates the need to train classifiers from intermediate states to additional controls."
    },
    {
        "title": "AnomalyTCN: Dual-branch Convolution with Contrastive Representation for Efficient Time Series Anomaly Detection",
        "link_suffix": "/forum?id=RuYl15smRv",
        "link": "https://openreview.net/forum?id=RuYl15smRv",
        "pdf_link": "https://openreview.net/pdf?id=RuYl15smRv",
        "keywords": "Time series anomaly detection, Deep learning, Contrastive-based time series anomaly detection",
        "abstract": "This paper focuses on the rising contrastive-based method for time series anomaly detection, which works on the idea of contrastive discrepancy learning and breaks through the performance bottleneck of previous reconstruction-based methods. But we also find that, existing contrastive-based methods only work with the complicated attention mechanisms, which brings heavier computational costs. To address this efficiency issue, we propose AnomalyTCN as a more efficient and effective contrastive-based solution. In detail, we design a dual-branch convolution structure to produce different representations of the same input under two different views for contrastive learning. Then we adopt the representation discrepancy between these two branches as a more distinguishable criterion to detect the anomalies, leading to better detection performance. Meanwhile, since we adopt a simple and light-weight pure convolution structure to avoid the complicated attention computation, our method can enjoy much more advantages in efficiency. Experimentally, our AnomalyTCN achieves the consistent state-of-the-art performance on various time series anomaly detection tasks while saving 83.6% running time and 20.1% memory usage. These results validate that our AnomalyTCN is a novel solution for time series anomaly detection with a better balance of performance and efficiency."
    },
    {
        "title": "IMPROVING LOW-BIT POST TRAINING QUANTIZATION: A DATA-FREE APPROACH",
        "link_suffix": "/forum?id=mJ8k81O5BF",
        "link": "https://openreview.net/forum?id=mJ8k81O5BF",
        "pdf_link": "https://openreview.net/pdf?id=mJ8k81O5BF",
        "keywords": "Deep Neural Network, Quantization Aware Training, Post Training Quantization, Data Free Quantization",
        "abstract": "Post-training quantization (PTQ) without access to real data is enabling efficient model optimization and deployment in scenarios where privacy or proprietary constraints restrict the use of original datasets. Traditional data free quantization methods rely on Batch Normalization (BN) statistics from the trained full-precision model to generate calibration dataset for quantization. However, this reliance on BN statistics limits their applicability to deep neural networks (DNNs) without BN layer such as AlexNet. In this paper, we propose a calibration dataset generation algorithm that is agnostic to BN statistics, leveraging just the backpropagation to create synthetic images for PTQ. We also demonstrate that it is not necessary to include samples from every target category in the calibration dataset to get the representative activation ranges for quantization. Extensive experiments with both large and lightweight models on large-scale image classification tasks demonstrate that our method consistently improves quantization performance across various DNN architectures, especially in low-bit settings. Notably, in 4-bit quantization, we achieve an improvement of  3.42% in top-1 accuracy for the ResNet18 model and 3.14% for the InceptionV3 model compared to the state-of-the-art (SOTA) DSG method. Importantly, we use very few synthetic samples for quantization compared to other methods."
    },
    {
        "title": "Spatial-Mamba: Effective Visual State Space Models via Structure-Aware State Fusion",
        "link_suffix": "/forum?id=iDe1mtxqK5",
        "link": "https://openreview.net/forum?id=iDe1mtxqK5",
        "pdf_link": "https://openreview.net/pdf?id=iDe1mtxqK5",
        "keywords": "Representation learning, Visual state space models, Structure-aware state fusion",
        "abstract": "Selective state space models (SSMs), such as Mamba, highly excel at capturing long-range dependencies in 1D sequential data, while their applications to 2D vision tasks still face challenges. Current visual SSMs often convert images into 1D sequences and employ various scanning patterns to incorporate local spatial dependencies. However, these methods are limited in effectively capturing the complex image spatial structures and the increased computational cost caused by the lengthened scanning paths. To address these limitations, we propose Spatial-Mamba, a novel approach that establishes neighborhood connectivity directly in the state space. Instead of relying solely on sequential state transitions, we introduce a structure-aware state fusion equation, which leverages dilated convolutions to capture image spatial structural dependencies, significantly enhancing the flow of visual contextual information. Spatial-Mamba proceeds in three stages: initial state computation in a unidirectional scan, spatial context acquisition through structure-aware state fusion, and final state computation using the observation equation. Our theoretical analysis shows that Spatial-Mamba unifies the original Mamba and linear attention under the same matrix multiplication framework, providing a deeper understanding of our method. Experimental results demonstrate that Spatial-Mamba, even with a single scan, attains or surpasses the state-of-the-art SSM-based models in image classification, detection and segmentation. Source codes and trained models will be made publicly available."
    },
    {
        "title": "On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models",
        "link_suffix": "/forum?id=pxclAomHat",
        "link": "https://openreview.net/forum?id=pxclAomHat",
        "pdf_link": "https://openreview.net/pdf?id=pxclAomHat",
        "keywords": "large language model, LoRA, optimization",
        "abstract": "Training Large Language Models (LLMs) poses significant memory challenges, making low-rank adaptation methods an attractive solution. Previously, Low-Rank Adaptation (LoRA) addressed this by adding a trainable low-rank matrix to the frozen pre-trained weights in each layer, reducing the number of trainable parameters and optimizer states. GaLore, which compresses the gradient matrix instead of the weight matrix, has demonstrated superior performance to LoRA with faster convergence and reduced memory consumption. Despite their empirical success, the performance of these methods has not been fully understood or explained theoretically. In this paper, we analyze the optimization landscapes of LoRA, GaLore, and full-rank methods, revealing that GaLore benefits from fewer spurious local minima and a larger region that satisfies the \\pl, a variant of Polyak-\u0141ojasiewicz (PL) condition, leading to faster convergence. Our analysis leads to a novel method, GaRare, which further improves GaLore by using gradient random projection to reduce computational overhead. Practically, GaRare achieves strong performance in both pre-training and fine-tuning tasks, offering a more efficient approach to large-scale model adaptation."
    },
    {
        "title": "Automating Large-scale In-silico Benchmarking for Genomic Foundation Models",
        "link_suffix": "/forum?id=Kg0KnPfiW2",
        "link": "https://openreview.net/forum?id=Kg0KnPfiW2",
        "pdf_link": "https://openreview.net/pdf?id=Kg0KnPfiW2",
        "keywords": "genomic benchmark, DNA, RNA, foundation model",
        "abstract": "The advancements in artificial intelligence in recent years, such as Large Language Models (LLMs), have fueled expectations for breakthroughs in genomic foundation models (GFMs). The code of nature, hidden in diverse genomes since the very beginning of life\u2019s evolution, holds immense potential for impacting humans and ecosystems through genome modeling. Recent breakthroughs in GFMs, such as Evo, have attracted significant investment and attention to genomic modeling, as they address long-standing challenges and transform in-silico genomic studies into automated, reliable, and efficient paradigms. In the context of this flourishing era of consecutive technological revolutions in genomics, GFM studies face two major challenges: the lack of GFM benchmarking tools and the absence of open-source software for diverse genomics. These challenges hinder the rapid evolution of GFMs and their wide application in tasks such as understanding and synthesizing genomes, problems that have persisted for decades. To address these challenges, we introduce GFMBench, a framework dedicated to GFM-oriented benchmarking. GFMBench standardizes benchmark suites and automates benchmarking for a wide range of open-source GFMs. It integrates millions of genomic sequences across hundreds of genomic tasks from four large-scale benchmarks, democratizing GFMs for a wide range of in-silico genomic applications. Additionally, GFMBench is released as open-source software, offering user-friendly interfaces and diverse tutorials, applicable for AutoBench and complex tasks like RNA design and structure prediction. To facilitate further advancements in genome modeling, we have launched a public leaderboard showcasing the benchmark performance derived from AutoBench. GFMBench represents a step toward standardizing GFM benchmarking and democratizing GFM applications."
    },
    {
        "title": "Output Scouting: Auditing Large Language Models for Catastrophic Responses",
        "link_suffix": "/forum?id=dOiinVDQEW",
        "link": "https://openreview.net/forum?id=dOiinVDQEW",
        "pdf_link": "https://openreview.net/pdf?id=dOiinVDQEW",
        "keywords": "large language models, safety, interpretability, red teaming",
        "abstract": "Recent high profile incidents in which the use of Large Language Models (LLMs) resulted in significant harm to individuals have brought about a growing interest in AI safety. One reason LLM safety issues occur is that models often have at least some non-zero probability of producing harmful outputs. In this work, we explore the following scenario: imagine an AI safety auditor is searching for catastrophic responses from an LLM (e.g. a \"yes\" responses to \"can I fire an employee for being pregnant?\"), and is able to query the model a limited number times (e.g. 1000 times). What is a strategy for querying the model that would efficiently find those failure responses? To this end, we propose output scouting: an approach that aims to generate semantically fluent outputs to a given prompt matching any target probability distribution. We then run experiments using two LLMs and find numerous examples of catastrophic responses.  We conclude with a discussion that includes advice for practitioners who are looking to implement LLM auditing for catastrophic responses. We will release an open-source toolkit that implements our auditing framework using the Hugging Facetransformerslibrary following publication."
    },
    {
        "title": "Attention-Only Transformers via Unrolled Subspace Denoising",
        "link_suffix": "/forum?id=RhAW7TRJUy",
        "link": "https://openreview.net/forum?id=RhAW7TRJUy",
        "pdf_link": "https://openreview.net/pdf?id=RhAW7TRJUy",
        "keywords": "transformers, subspace denoising, algorithm unrolling, mixture of low-rank Gaussians",
        "abstract": "Despite the great success of transformers in practice, their architectures have been empirically designed, hence lack of mathematical justification and interpretability. Moreover, many empirical studies have indicated that some components of the transformer architectures may be redundant and can be removed or replaced without compromising overall performance. Hence to derive a compact and interpretable transformer architecture, we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. Based on the existing literature, the associated denoising operation naturally takes the form of a multi-subspace self-attention (MSSA). By unrolling such iterative denoising operations as a deep network, we arrive at a highly compact architecture that consists of only an MSSA operator with skip connections at each layer, without MLP. We rigorously prove that each layer of the proposed transformer performs so highly efficient denoising that it improves the signal-to-noise ratio of token representations {\\em at a linear rate} with respect to the number of layers. Despite its simplicity, extensive experiments on language and vision tasks demonstrate that such a minimalistic attention-only transformer can achieve performance close to conventional transformers, such as GPT-2 and CRATE."
    },
    {
        "title": "LLM2CLIP: Extending the Capability Boundaries of CLIP through Large Language Models",
        "link_suffix": "/forum?id=HfJxXbXlYJ",
        "link": "https://openreview.net/forum?id=HfJxXbXlYJ",
        "pdf_link": "https://openreview.net/pdf?id=HfJxXbXlYJ",
        "keywords": "CLIP, Vision Language Model, Contrastive Learning",
        "abstract": "CLIP is one of the most important multimodal foundational models today, aligning visual and textual signals into a shared feature space using a simple contrastive learning loss on large-scale image-text pairs. What powers CLIP\u2019s capabilities? The rich supervision signals provided by natural language \u2014 the carrier of human knowledge \u2014 shape a powerful cross-modal representation space. As a result, CLIP supports a variety of tasks, including zero-shot classification, detection, segmentation, and cross-modal retrieval, significantly influencing the entire multimodal domain.\nHowever, with the rapid advancements in large language models (LLMs) like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: \\emph{can the capabilities of LLMs be harnessed to further improve multimodal representation learning?}\nThe potential benefits of incorporating LLMs into CLIP are clear. LLMs\u2019 strong textual understanding can fundamentally improve CLIP\u2019s ability to handle image captions, drastically enhancing its ability to process long and complex texts \u2014 a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process.\nHowever, realizing this potential is challenging. Despite LLMs' powerful internal comprehension, their autoregressive nature hides this capability within the model, leading to output features with poor discriminability. Our experiments show that directly integrating LLMs into CLIP results in catastrophic performance drops.\nIn this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP\u2019s potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP\u2019s visual encoder. Thanks to the LLM\u2019s presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP text encoder\u2019s context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks. Our method directly boosted the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming a CLIP model trained solely on English data into a state-of-the-art cross-lingual model. Moreover, when integrated into multimodal training with models like Llava 1.5, it consistently outperformed EVA02 across nearly all benchmarks, demonstrating comprehensive performance improvements."
    },
    {
        "title": "Federated Class-Incremental Learning: A Hybrid Approach Using Latent Exemplars and Data-Free Techniques to Address Local and Global Forgetting",
        "link_suffix": "/forum?id=ydREOIttdC",
        "link": "https://openreview.net/forum?id=ydREOIttdC",
        "pdf_link": "https://openreview.net/pdf?id=ydREOIttdC",
        "keywords": "Class-Incremental Learning, Federated Learning, Global Forgetting, Local Forgetting.",
        "abstract": "Federated Class-Incremental Learning (FCIL) refers to a scenario where a dynamically changing number of clients collaboratively learn an ever-increasing number of incoming tasks. FCIL is known to suffer from local forgetting due to class imbalance at each client and global forgetting due to class imbalance across clients. We develop a mathematical framework for FCIL that formulates local and global forgetting. Then, we propose an approach called Hybrid Rehearsal (HR), which utilizes latent exemplars and data-free techniques to address local and global forgetting, respectively. HR employs a customized autoencoder designed for both data classification and the generation of synthetic data. To determine the embeddings of new tasks for all clients in the latent space of the encoder, the server uses the Lennard-Jones Potential formulations. Meanwhile, at the clients, the decoder decodes the stored low-dimensional latent space exemplars back to the high-dimensional input space, used to address local forgetting. To overcome global forgetting, the decoder generates synthetic data. Furthermore, our mathematical framework proves that our proposed approach HR can, in principle, tackle the two local and global forgetting challenges. In practice, extensive experiments demonstrate that while preserving privacy, our proposed approach outperforms the state-of-the-art baselines on multiple FCIL benchmarks with low compute and memory footprints."
    }
]