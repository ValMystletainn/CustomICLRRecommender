[{"title": "Treatment Rule Optimization Under Counterfactual Temporal Point Processes with Latent States", "link_suffix": "/forum?id=jZffxvubJ9", "link": "https://openreview.net/forum?id=jZffxvubJ9", "pdf_link": "https://openreview.net/pdf?id=jZffxvubJ9", "keywords": "counterfactual reasoning, temporal point processes, latent confounder, rule learning", "abstract": "In high-stakes areas like healthcare, retrospective counterfactual analysis\u2014such as evaluating what might have happened if treatments were administered earlier, later, or differently\u2014is vital for refining treatment strategies. This paper proposes a counterfactual treatment optimization framework using temporal point processes to model outcome event sequences. By sampling potential outcome events under new treatment decision rules, our approach seeks to optimize treatment strategies in a counterfactual setting. To achieve accurate counterfactual evaluation of new decision rules, we explicitly introduce latent states into the modeling of temporal point processes. Our method first infers the latent states and associated noise, followed by counterfactual sampling of outcome events. This approach rigorously addresses the complexities introduced by latent states, effectively removing biases in the evaluation of treatment strategies. By proving the identifiability of model parameters in the presence of these states, we provide theoretical guarantees that enhance the reliability and robustness of the counterfactual analysis. By incorporating latent states and proving identifiability, our framework not only improves the accuracy and robustness of treatment decision rules but also offers actionable insights for optimizing healthcare interventions. This method holds significant potential for improving treatment strategies, particularly in healthcare scenarios where patient symptoms are complex and high-dimensional.", "title_embedding_index": 850, "title_abs_embedding_index": 875}, {"title": "SafeAuto: Knowledge-Enhanced Safe Autonomous Driving with Multimodal Foundation Models", "link_suffix": "/forum?id=RDz3EPC3Lp", "link": "https://openreview.net/forum?id=RDz3EPC3Lp", "pdf_link": "https://openreview.net/pdf?id=RDz3EPC3Lp", "keywords": "Autonomous Driving; Multimodal Large Language Models; Multimodal Retrieval-Augmented Generation; Probabilistic Graph Model", "abstract": "Traditional autonomous driving systems often struggle to harmonize high-level reasoning with low-level control, leading to suboptimal and even unsafe driving behaviors. The emergence of multimodal large language models (MLLMs), capable of processing visual and textual data, presents an opportunity to unify perception and reasoning tasks within a single framework. However, integrating precise safety knowledge into MLLMs for safe autonomous driving remains a significant challenge.\nTo address this, we propose SafeAuto, a novel framework that enhances MLLM-based autonomous driving systems by incorporating both unstructured and structured knowledge. In particular, we first propose the the Place-Dependent Cross-Entropy (PDCE) loss function, which is specifically designed to enhance the accuracy of low-level control signal predictions when treating numerical values as text.\nTo explicitly integrate precise safety knowledge into the MLLM to enable safe autonomous driving, we build a reasoning component for SafeAuto, which first parses driving safety regulations into first-order logic rules (e.g., \"red light $\\implies$ stop\") and then integrates these rules into a probabilistic graphical model, such as a Markov Logic Network (MLN). The environment attributes, identified by attribute recognition models (e.g., detecting a red light), are used to form the predicates in MLN.\nIn addition, the environmental attributes utilized for reasoning are also considered factors in retrieval to construct a Multimodal Retrieval-Augmented Generation (RAG) model, which aims to learn from past similar driving experiences more effectively.\nExtensive experiments demonstrate that SafeAuto significantly outperforms baselines across multiple datasets. By bridging the gap between high-level reasoning and low-level control, SafeAuto paves the way for more accurate, reliable, and safer autonomous driving, facilitating systems that learn effectively from experience, adhere to traffic regulations, and execute precise control actions.", "title_embedding_index": 851, "title_abs_embedding_index": 876}, {"title": "Naturality-Guided Hyperedge Disentanglement for Message Passing Hypergraph Neural Network", "link_suffix": "/forum?id=bhD0EQWNut", "link": "https://openreview.net/forum?id=bhD0EQWNut", "pdf_link": "https://openreview.net/pdf?id=bhD0EQWNut", "keywords": "Graph Neural Network, Hypergraph, Category Theory, Categorcial Deep Learning, Disentangled Representation Learning", "abstract": "Hypergraph data structure has been widely used to store information or meaning derived from group interactions, meaning that each hyperedge inherently contains the context of their interactions. For example, a set of genes or a genetic pathway can be represented as a hyperedge to express the interaction of multiple genes that collaboratively perform a biological function (i.e., interaction context). However, most existing hypergraph neural networks cannot reflect the interaction context of each hyperedge due to their limited capability in capturing important or relevant factors therein. In this paper, we propose a \\textbf{simple but effective} hyperedge disentangling method, \\textbf{Natural-HNN}, that captures inherent hyperedge types or the interaction context of an hyperedge. We devised a novel guidance for hyperedge disentanglement based on the naturality condition in the category theory. In our experiments, we applied our model to hypergraphs of genetic pathways for the cancer subtype classification task, and showed that our model outperforms baselines by capturing the functional semantic similarity of genetic pathways.", "title_embedding_index": 852, "title_abs_embedding_index": 877}, {"title": "Neural Multi-Objective Combinatorial Optimization via Graph-Image Multimodal Fusion", "link_suffix": "/forum?id=4sJ2FYE65U", "link": "https://openreview.net/forum?id=4sJ2FYE65U", "pdf_link": "https://openreview.net/pdf?id=4sJ2FYE65U", "keywords": "Neural Multi-Objective Combinatorial Optimization, Multimodal Fusion, Deep Reinforcement Learning", "abstract": "Existing neural multi-objective combinatorial optimization (MOCO) methods still exhibit an optimality gap since they fail to fully exploit the intrinsic features of problem instances. A significant factor contributing to this shortfall is their reliance solely on graph-modal information. To overcome this, we propose a novel graph-image multimodal fusion (GIMF) framework that enhances neural MOCO methods by integrating graph and image information of the problem instances. Our GIMF framework comprises three key components: (1) a constructed coordinate image to better represent the spatial structure of the problem instance, (2) a problem-size adaptive resolution strategy during the image construction process to improve the cross-size generalization of the model, and (3) a multimodal fusion mechanism with modality-specific bottlenecks to efficiently couple graph and image information. We demonstrate the versatility of our GIMF by implementing it with two state-of-the-art neural MOCO backbones. Experimental results on classic MOCO problems show that our GIMF significantly outperforms state-of-the-art neural MOCO methods and exhibits superior generalization capability.", "title_embedding_index": 853, "title_abs_embedding_index": 878}, {"title": "MicroCrackAttentionNeXt: Advancing Microcrack Detection in Wave Field Analysis Using Deep Neural Networks through Feature Visualization.", "link_suffix": "/forum?id=FNGZqMp6Fi", "link": "https://openreview.net/forum?id=FNGZqMp6Fi", "pdf_link": "https://openreview.net/pdf?id=FNGZqMp6Fi", "keywords": "manifold discovery and analysis, Feature Visualisation, structural health monitoring, Attention mechanism, wave field data, micro scale cracks, Loss functions", "abstract": "Micro Crack detection using deep neural networks(DNNs) through an automated pipeline using wave fields interacting with the damaged areas is highly sought after. However, these high dimensional spatio-temporal crack data are limited and, moreover these dataset have large dimension in the temporal domain. The dataset presents a substantial class imbalance, with crack pixels constituting an average of only 5% of the total pixels per sample, this extreme class imbalance poses a challenge for deep learning models with the different micro scale cracks, as the network can be biased toward predicting the majority class, generally lead- ing to poor detection accuracy. This study builds upon the previous benchmark SpAsE-Net, an asymmetric encoder\u2013decoder network for micro-crack detection. The impact of various activation and loss functions were examined through feature space visualisation using manifold discovery and analysis (MDA) algorithm. The optimized architecture and training methodology achieved an accuracy of 86.85%.", "title_embedding_index": 854, "title_abs_embedding_index": 879}, {"title": "FlexMotion: Lightweight, Physics-Aware, and Controllable Human Motion Generation", "link_suffix": "/forum?id=7652tHbbVE", "link": "https://openreview.net/forum?id=7652tHbbVE", "pdf_link": "https://openreview.net/pdf?id=7652tHbbVE", "keywords": "3D human motion generation, diffusion models, conditional generation, physics aware", "abstract": "Lightweight, controllable, and physically plausible human motion synthesis is crucial for animation, virtual reality, robotics, and human-computer interaction applications. Existing methods often compromise between computational efficiency, physical realism, or spatial controllability. We propose FlexMotion, a novel framework that leverages a computationally lightweight diffusion model operating in the latent space, eliminating the need for physics simulators and enabling fast and efficient training. FlexMotion employs a multimodal pre-trained Transformer encoder-decoder, integrating joint locations, contact forces, joint actuations and muscle activations to ensure the physical plausibility of the generated motions. FlexMotion also introduces a plug-and-play module, which adds spatial controllability over a range of motion parameters (e.g., joint locations, joint actuations, contact forces, and muscle activations). Our framework achieves realistic motion generation with improved efficiency and control, setting a new benchmark for human motion synthesis. We evaluate FlexMotion on extended datasets and demonstrate its superior performance in terms of realism, physical plausibility, and controllability.", "title_embedding_index": 855, "title_abs_embedding_index": 880}, {"title": "MDBench: A Synthetic Multi-Document Reasoning Benchmark Generated with Knowledge Guidance", "link_suffix": "/forum?id=KNkalZnq3f", "link": "https://openreview.net/forum?id=KNkalZnq3f", "pdf_link": "https://openreview.net/pdf?id=KNkalZnq3f", "keywords": "multi-document reasoning, benchmark creation, synthetic data generation, text generation, nlp", "abstract": "Natural language processing evaluation has made significant progress, largely driven by the proliferation of powerful large language models (LLMs). New evaluation benchmarks are of increasing priority as the reasoning capabilities of LLMs are expanding at a rapid pace. In particular, whilemulti-document(MD) reasoning is an area of extreme relevance given LLM capabilities in handling longer-context inputs, few benchmarks exist to rigorously examine model behavior in this setting. Moreover, the multi-document setting is historically challenging for benchmark creation due to the expensive cost of annotating long inputs.\nIn this work, we introduceMDBench, a new dataset for evaluating LLMs on the task of multi-document reasoning. Notably, MDBench is created through a novel synthetic generation process, allowing us tocontrollably and efficiently generate challenging document setsand the corresponding question-answer (QA) examples. Our novel technique operates on condensed structured seed knowledge, modifying it through LLM-assisted edits to induce MD-specific reasoning challenges. We then convert this structured knowledge into a natural text surface form, generating a document set and corresponding QA example.\nWe analyze the behavior of popular LLMs and prompting techniques, finding that MDBench poses significant challenges for all methods, even with relatively short document sets. We also see our knowledge-guided generation technique (1) allows us to readily perform targeted analysis of MD-specific reasoning capabilities and (2) can be adapted quickly to account for new challenges and future modeling improvements.", "title_embedding_index": 856, "title_abs_embedding_index": 881}, {"title": "ConML: A Universal Meta-Learning Framework with Task-Level Contrastive Learning", "link_suffix": "/forum?id=UuZDosomkp", "link": "https://openreview.net/forum?id=UuZDosomkp", "pdf_link": "https://openreview.net/pdf?id=UuZDosomkp", "keywords": "Meta-Learning, Contrastive Learning, In-Context Learning", "abstract": "Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. To emulate this human-like rapid learning and enhance alignment and discrimination abilities, we propose ConML, a universal meta-learning framework that can be applied to various meta-learning algorithms without relying on specific model architectures nor target models. The core of ConML is task-level contrastive learning, which extends contrastive learning from the representation space in unsupervised learning to the model space in meta-learning. By leveraging task identity as an additional supervision signal during meta-training, we contrast the outputs of the meta-learner in the model space, minimizing inner-task distance (between models trained on different subsets of the same task) and maximizing inter-task distance (between models from different tasks). We demonstrate that ConML integrates seamlessly with optimization-based, metric-based, and amortization-based meta-learning algorithms, as well as in-context learning, resulting in performance improvements across diverse few-shot learning tasks.", "title_embedding_index": 857, "title_abs_embedding_index": 882}, {"title": "Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs", "link_suffix": "/forum?id=E2PFv7ad3p", "link": "https://openreview.net/forum?id=E2PFv7ad3p", "pdf_link": "https://openreview.net/pdf?id=E2PFv7ad3p", "keywords": "Multi-modal Model, Visual-Language Model, Sycophancy, Hallucination", "abstract": "In the study of LLMs, sycophancy represents a prevalent hallucination that poses significant challenges to these models. Specifically, LLMs often fail to adhere to original correct responses, instead blindly agreeing with users' opinions, even when those opinions are incorrect or malicious. However, research on sycophancy in visual language models (VLMs) has been scarce. In this work, we extend the exploration of sycophancy from LLMs to VLMs, introducing the MM-SY benchmark to evaluate this phenomenon. We present evaluation results from multiple representative models, addressing the gap in sycophancy research for VLMs. To mitigate sycophancy, we propose a synthetic dataset for training and employ methods based on prompts, supervised fine-tuning, and DPO. Our experiments demonstrate that these methods effectively alleviate sycophancy in VLMs. Additionally, we probe VLMs to assess the semantic impact of sycophancy and analyze the attention distribution of visual tokens. Our findings indicate that the ability to prevent sycophancy is predominantly observed in higher layers of the model. The lack of attention to image knowledge in these higher layers may contribute to sycophancy, and enhancing image attention at high layers proves beneficial in mitigating this issue.", "title_embedding_index": 858, "title_abs_embedding_index": 883}, {"title": "Privately Learning from Graphs with Applications in Fine-tuning Large Pretrained Models", "link_suffix": "/forum?id=WtZRZC4zva", "link": "https://openreview.net/forum?id=WtZRZC4zva", "pdf_link": "https://openreview.net/pdf?id=WtZRZC4zva", "keywords": "relational learning, differential privacy, language model, fine-tuning", "abstract": "Graphs offer unique insights into relationships and interactions between entities, complementing data modalities like text, images, and videos. By incorporating relational information from graph data, AI models can extend their capabilities beyond traditional tasks. However, relational data in sensitive domains such as finance and healthcare often contain private information, making privacy preservation crucial. Existing privacy-preserving methods, such as DP-SGD, which rely on gradient decoupling assumptions, are not well-suited for relational learning due to the inherent dependencies between coupled training samples. To address this challenge, we propose a privacy-preserving relational learning pipeline that decouples dependencies in sampled relations during training, ensuring differential privacy through a tailored application of DP-SGD. We apply this method to fine-tune large language models (LLMs) on sensitive graph data, and tackle the associated computational complexities. Our approach is evaluated on LLMs of varying sizes (e.g., BERT, Llama2) using real-world relational data from four text-attributed graphs. The results demonstrate significant improvements in relational learning tasks, all while maintaining robust privacy guarantees during training. Additionally, we explore the trade-offs between privacy, utility, and computational efficiency, offering insights into the practical deployment of our approach.", "title_embedding_index": 859, "title_abs_embedding_index": 884}, {"title": "Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass", "link_suffix": "/forum?id=bc3sUsS6ck", "link": "https://openreview.net/forum?id=bc3sUsS6ck", "pdf_link": "https://openreview.net/pdf?id=bc3sUsS6ck", "keywords": "language model; efficient adaptation; fine-tuning; prompting", "abstract": "Large language models (LLMs) acquire substantial knowledge during pretraining but often need adaptation to new contexts, tasks, or domains, typically achieved through fine-tuning or prompting. However, fine-tuning incurs significant training costs, while prompting increases inference overhead. Inspired by fast weight memory, we introduce GenerativeAdapter, an effective and efficient adaptation method that encode test-time context into language model parameters with a single forward pass.\nGenerativeAdapter augments a frozen pretrained LM with a lightweight adapter generator, trained via self-supervised learning, to produce parameter-efficient adapters.\nNotably, our generator is general-purpose, i.e., one generator can adapt the corresponding base model for all langauge processing scenarios.\nWe apply GenerativeAdapter to two pretrained LMs (Mistral-7B-Instruct and Llama2-7B-Chat) and evaluate the adapted models across  knowledge acquisition from documents, learning from demonstrations, and personalization for users.\nIn StreamingQA, our approach is effective in injecting knowledge into the LM's parameters, achieving a 63.5% improvement in F1 score over the model with supervised fine-tuning (from $19.5$ to $31.5$) for contexts as long as 32K tokens.\nIn the MetaICL in-context learning evaluation, our method achieves an average accuracy of $44.9$ across 26 tasks, outperforming the base model. \nOn MSC, our method proves to be highly competitive in memorizing user information from conversations with a 4x reduction in computation and memory costs compared to \nprompting with full conversation history.\nOverall, GenerativeAdapter provides a viable solution for adapting large LMs to evolving information and providing tailored user experience, while reducing training and inference costs relative to traditional fine-tuning and prompting techniques.", "title_embedding_index": 860, "title_abs_embedding_index": 885}, {"title": "In-Context Learning of Representations", "link_suffix": "/forum?id=pXlmOmlHJZ", "link": "https://openreview.net/forum?id=pXlmOmlHJZ", "pdf_link": "https://openreview.net/pdf?id=pXlmOmlHJZ", "keywords": "In-Context Learning, Representational Geometry, Emergence, Percolation", "abstract": "Recent work demonstrates that structured patterns in pretraining data influence how representations of different concepts are organized in a large language model\u2019s (LLM) internals, with such representations then driving downstream abilities. Given the open-ended nature of LLMs, e.g., their ability to in-context learn novel tasks, we ask whether models can flexibly alter their semantically grounded organization of concepts. Specifically, if we provide in-context exemplars wherein a concept plays a different role than what the pretraining data suggests, can models infer these novel semantics and reorganize representations in accordance with them? To answer this question, we define a toy \u201cgraph tracing\u201d task wherein the nodes of the graph are referenced via concepts seen during training (e.g., apple, bird, etc.), and the connectivity of the graph is defined via some predefined structure (e.g., a square grid). Given exemplars that indicate traces of random walks on the graph, we analyze intermediate representations of the model and find that as the amount of context is scaled, there is a sudden re-organization of representations according to the graph\u2019s structure. Further, we find that when reference concepts have correlations in their semantics (e.g., Monday, Tuesday, etc.), the context-specified graph structure is still present in the representations, but is unable to dominate the pretrained structure. To explain these results, we analogize our task to energy minimization for a predefined graph topology, which shows getting non-trivial performance on the task requires for the model to infer a connected component. This analogy also yields a relation to theory of graph percolation, motivated by which we evaluate the impact of scaling graph size, finding that the critical context size needed for getting organized representations follows a power-law scaling that closely matches exponents predicted by percolation theory. Overall, our findings indicate context-size may be an underappreciated scaling axis that can flexibly re-organize model representations, unlock novel capabilities.", "title_embedding_index": 861, "title_abs_embedding_index": 886}, {"title": "Ensembling Diffusion Models via Adaptive Feature Aggregation", "link_suffix": "/forum?id=e32cI4r8Eo", "link": "https://openreview.net/forum?id=e32cI4r8Eo", "pdf_link": "https://openreview.net/pdf?id=e32cI4r8Eo", "keywords": "Image Generation, Diffusion Models, Model Ensembling", "abstract": "The success of the text-guided diffusion model has inspired the development and release of numerous powerful diffusion models within the open-source community. These models are typically fine-tuned on various expert datasets, showcasing diverse denoising capabilities. Leveraging multiple high-quality models to produce stronger generation ability is valuable, but has not been extensively studied. Existing methods primarily adopt parameter merging strategies to produce a new static model. However, they overlook the fact that the divergent denoising capabilities of the models may dynamically change across different states, such as when experiencing different prompts, initial noises, denoising steps, and spatial locations. In this paper, we propose a novel ensembling method, Adaptive Feature Aggregation (AFA), which dynamically adjusts the contributions of multiple models at the feature level according to various states (i.e., prompts, initial noises, denoising steps, and spatial locations), thereby keeping the advantages of multiple diffusion models, while suppressing their disadvantages. Specifically, we design a lightweight Spatial-Aware Block-Wise (SABW) feature aggregator that adaptive aggregates the block-wise intermediate features from multiple U-Net denoisers into a unified one. The core idea lies in dynamically producing an individual attention map for each model's features by comprehensively considering various states. It is worth noting that only SABW is trainable with about 50 million parameters, while other models are frozen. Both the quantitative and qualitative experiments demonstrate the effectiveness of our proposed method.", "title_embedding_index": 862, "title_abs_embedding_index": 887}, {"title": "Temporal Causal Discovery and Generative Prediction of Vehicular CO2emission", "link_suffix": "/forum?id=igaxFI1gBA", "link": "https://openreview.net/forum?id=igaxFI1gBA", "pdf_link": "https://openreview.net/pdf?id=igaxFI1gBA", "keywords": "causal discovery, causal inference, time series forecasting, vehicular emission", "abstract": "Global warming from greenhouse gas emissions is humanity's largest environmental hazard. Greenhouse gases, like CO$_2$ emissions from transportation, notably cars, contribute to the greenhouse effect. Effective CO$_2$ emission monitoring is needed to regulate vehicle emissions. Few studies have predicted automobile CO$_2$ emissions using OBD port data. For precise and effective prediction, the system must capture the underlying cause-effect structure between vehicular parameters that may contribute to the emission of CO$_2$ in the transportation sector. Thus, we present a causal RNN-based generative deep learning architecture that predicts vehicle CO$_2$ emissions using OBD-II data while keeping the underlying causal structure. Most widely used real-life datasets lack causal relationships between features or components, so we use our proposed architecture to discover and learn the underlying causal structure as an adjacency matrix during training and employ that during forecasting. Our framework learns a sparse adjacency matrix by imposing a sparsity-encouraging penalty on model weights and allowing some weights to be zero. This matrix is capable of capturing the causal relationships between all variable pairs. In this work, we first train the model with widely used synthetic datasets with known causal structure among variables, then we apply it to the state-of-the-art OBD-II dataset to find the internal causal structure among the vehicular parameters and perform causal inference to predict CO$_2$ emission. Experimental results reveal that our causal discovery and forecasting method surpasses state-of-the-art methods for the tasks of causal discovery in terms of AUROC, forecasting on multivariate causal time series data, and OBD-II dataset in terms of MMD, RMSE, and MAE. After successful completion, we will release the code (Code for review -https://anonymous.4open.science/r/causal-obd-co2-0A0C).", "title_embedding_index": 863, "title_abs_embedding_index": 888}, {"title": "A Unified View of Delta Parameter Editing in Post-Trained Large-Scale Models", "link_suffix": "/forum?id=yx8bU8T5ZN", "link": "https://openreview.net/forum?id=yx8bU8T5ZN", "pdf_link": "https://openreview.net/pdf?id=yx8bU8T5ZN", "keywords": "Large Language Models, Delta Parameters Editing", "abstract": "Post-training has emerged as a crucial paradigm for adapting large-scale pre-trained models to various tasks, whose effects are fully reflected by delta parameters (i.e., the disparity between post-trained and pre-trained parameters). While numerous studies have explored delta parameter properties via operations like pruning, quantization, low-rank approximation, and extrapolation, a unified framework for systematically examining these characteristics has been lacking. In this paper, we propose a novel perspective based on Riemann sum approximation of the loss function to elucidate delta parameter editing operations. Our analysis categorizes existing methods into three classes based on their post-editing performance: competitive, decreased, and improved, explaining how they are expressed by the Riemann sum approximation term and how they alter the model performance. Extensive experiments on both visual and language models, including ViT, LLaMA 3, and Mistral, corroborate our theoretical findings. Furthermore, we introduce extensions to existing techniques like DARE and BitDelta, highlighting their limitations in leveraging the properties of delta parameters and reorganizing them into general expressions to enhance the applicability and effectiveness of delta parameter editing in post-trained models.", "title_embedding_index": 864, "title_abs_embedding_index": 889}, {"title": "Towards Fine-tuning-free Few-shot Classification: A Purely Self-supervised Manner", "link_suffix": "/forum?id=L8vZXTVxfG", "link": "https://openreview.net/forum?id=L8vZXTVxfG", "pdf_link": "https://openreview.net/pdf?id=L8vZXTVxfG", "keywords": "few-shot learning, variational autoencoder, self-supervised learning", "abstract": "One of the core problems of supervised few-shot classification is adapting generalized knowledge learned from substantial labeled source data to rarely labeled novel target data. What makes it a challenging problem is how to eliminate undesirable inductive bias introduced by labels when learning generalized knowledge during pre-training or adapting the learned knowledge during fine-tuning.In this paper, we propose a purely self-supervised method to bypass the labeling dilemma, focusing on an extreme scenario where a few-shot feature extractor is learned without fine-tuning. Our approach is built on two key observations from recent advancements in style transfer learning and self-supervised learning:1) high-order statistics of feature maps in deep nets encapsulate distinct information about input samples, and 2) high-quality inputs are not essential for obtaining high-quality representations. Accordingly, we introduce a variant of the vector quantized variational autoencoder (VQ-VAE) that incorporates a novel coloring operation, which conveys statistical information from the encoder to the decoder, modulating the generation process with these distinct statistics. With this design, we find that the statistics derived from the encoder's feature maps possess strong discriminative power, enabling effective classification using simple Euclidean distance metrics. Through extensive experiments on standard few-shot classification benchmark. We show that our fine-tuning-free method achieves competitive performance compared to fine-tuning-based and meta-learning-based approaches.", "title_embedding_index": 865, "title_abs_embedding_index": 890}, {"title": "Solving New Tasks by Adapting Internet Video Knowledge", "link_suffix": "/forum?id=p01BR4njlY", "link": "https://openreview.net/forum?id=p01BR4njlY", "pdf_link": "https://openreview.net/pdf?id=p01BR4njlY", "keywords": "Text-Conditioned Generalization, Video Diffusion, Adaptation, Planning, Policy Learning", "abstract": "Video generative models, beyond enabling the production of astounding visual creations, offer a promising pathway for unlocking novel, text-conditioned robotic behaviors, whether utilized as a video planner or as a policy supervisor.  When pretrained on internet-scale datasets, such video models intimately understand alignment with natural language, and can thus facilitate novel text-conditioned behavior generalization.  At the same time, however, they may not be sensitive to the specificities of the particular environment in which a policy of interest is to be learned.  On the other hand, video modeling over in-domain examples of robotic behavior naturally encode environment-specific intricacies, but the scale of available demonstrations may not be sufficient to support generalization to unseen tasks via natural language specification.  In this work, we investigate different adaptation techniques that integrate in-domain information into large-scale pretrained video models, and explore the extent to which they enable novel text-conditioned generalization for robotic tasks.  Furthermore, we highlight the individual data and training requirements of each approach, which range from utilizing only a few still frames illustrating the subject of interest, to direct finetuning over videos labelled with text descriptions.  We successfully demonstrate across robotic environments that adapting powerful video models with small scales of example data can successfully facilitate generalization to novel behaviors, both when utilized as policy supervisors, and as visual planners.", "title_embedding_index": 866, "title_abs_embedding_index": 891}, {"title": "Extracting Symbolic Sequences from Visual Representations via Self-Supervised Learning", "link_suffix": "/forum?id=MsUhByb3CM", "link": "https://openreview.net/forum?id=MsUhByb3CM", "pdf_link": "https://openreview.net/pdf?id=MsUhByb3CM", "keywords": "Self-Supervised Learning, Symbolic Representations, Information Theory, Knowledge Distillation, Visual Abstraction, Interpretability", "abstract": "In this paper, we explore the potential of abstracting complex visual information into discrete, structured symbolic sequences using self-supervised learning (SSL). Inspired by how language abstracts and organizes information to enable better reasoning and generalization, we propose a novel approach for generating symbolic representations from visual data. To learn these sequences, we extend the DINO framework to handle both visual and symbolic information. Initial experiments suggest that the generated symbolic sequences capture a meaningful level of abstraction, though further refinement is required. An advantage of our method is its interpretability: the sequences are produced by a decoder transformer using cross-attention, allowing attention maps to be linked to specific symbols and offering insight into how these representations correspond to image regions. This approach lays the foundation for creating interpretable symbolic representations with potential applications in high-level scene understanding.", "title_embedding_index": 867, "title_abs_embedding_index": 892}, {"title": "scMPT: towards applying large language models to complement single-cell foundation models", "link_suffix": "/forum?id=nUpM7egYFd", "link": "https://openreview.net/forum?id=nUpM7egYFd", "pdf_link": "https://openreview.net/pdf?id=nUpM7egYFd", "keywords": "Single-cell genomics, multimodality, interpretability, LLM", "abstract": "Single-cell foundation models such as scGPT represent a significant advancement in single-cell omics, with an ability to achieve state-of-the-art performance on a variety of downstream biological tasks. However, these models are inherently limited in that a vast amount of information in biology exists as text, which they are unable to leverage. There have therefore been several recent works that propose the use of large language models (LLMs) as an alternative to single-cell foundation models, achieving competitive results. However, there is little understanding of what factors drive this performance, along with a strong focus on using LLMs as an alternative, rather than complementary approach to single-cell foundation models. In this study we therefore investigate what biological insights contribute toward the performance of LLMs when applied to single-cell data, and introduce scMPT; a model which leverages synergies between scGPT, and single-cell representations from LLMs that capture these insights. scMPT demonstrates stronger, more consistent performance than either of its component models, which frequently have large performance gaps between each other across datasets.", "title_embedding_index": 868, "title_abs_embedding_index": 893}, {"title": "\u03b1-DPO: Adaptive Reward Margin is What Direct Preference Optimization Needs", "link_suffix": "/forum?id=QqziJAdev9", "link": "https://openreview.net/forum?id=QqziJAdev9", "pdf_link": "https://openreview.net/pdf?id=QqziJAdev9", "keywords": "Direct Preference Optimization, LLM's alignment", "abstract": "Aligning large language models (LLMs) with human values and intentions is crucial for their utility, honesty, and safety. Reinforcement learning from human feedback (RLHF) is a popular approach to achieve this alignment, but it faces challenges in computational efficiency and training stability. Recent methods like Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO) have proposed offline alternatives to RLHF, simplifying the process by reparameterizing the reward function. However, DPO depends on a potentially suboptimal reference model,\nand SimPO's assumption of a fixed target reward margin may lead to suboptimal decisions in diverse data settings.\nIn this work, we propose (\\alpha)-DPO, an adaptive preference optimization algorithm designed to address these limitations by introducing a dynamic reward margin. Specifically, (\\alpha)-DPO employs an adaptive preference distribution, balancing the policy model and the reference model to achieve personalized reward margins. We provide theoretical guarantees for (\\alpha)-DPO, demonstrating its effectiveness as a surrogate optimization objective and its ability to balance alignment and diversity through KL divergence control. Empirical evaluations on AlpacaEval 2 and Arena-Hard show that (\\alpha)-DPO consistently outperforms DPO and SimPO across various model settings, establishing it as a robust approach for fine-tuning LLMs. Our method achieves significant improvements in win rates, highlighting its potential as a powerful tool for LLM alignment.", "title_embedding_index": 869, "title_abs_embedding_index": 894}, {"title": "ID-Booth: Identity-consistent image generation with diffusion models", "link_suffix": "/forum?id=NWvsm2VxAM", "link": "https://openreview.net/forum?id=NWvsm2VxAM", "pdf_link": "https://openreview.net/pdf?id=NWvsm2VxAM", "keywords": "Image synthesis, Diffusion models, Face recognition data", "abstract": "The recent retraction of large-scale biometric datasets, prompted by strict privacy  regulations, presents a critical challenge for future biometric research. This is evident with the face recognition task, for which large-scale datasets were often gathered through web-scraping without the consent of subjects. A potential solution entails the creation of synthetic data, suitable for training recognition models, with deep generative models. Existing generative approaches rely on conditioning and fine-tuning of powerful pretrained diffusion models to achieve the synthesis of realistic images of a desired identity. Yet, these methods often do not consider the identity of subjects during training, leading to poor consistency between generated and intended identities. In contrast, methods that employ identity-based training objectives tend to overfit on various aspects of the identity, and in turn, lower the diversity of images that can be generated. To address these issues, we present the ID-Booth fine-tuning framework, which utilizes a novel triplet identity training objective and enables identity-consistent image generation while retaining the synthesis capabilities of pretrained models. Experiments across two latent diffusion models with varying prompt complexity reveal that our method facilitates better intra-identity consistency and inter-identity separability while achieving higher image diversity. In turn, the produced data enables the training of better-performing recognition models than even real-world  datasets of a similar scale gathered with suitable consent. The source code for the ID-Booth framework is available at omitted_for_review.", "title_embedding_index": 870, "title_abs_embedding_index": 895}, {"title": "A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches", "link_suffix": "/forum?id=Q6PAnqYVpo", "link": "https://openreview.net/forum?id=Q6PAnqYVpo", "pdf_link": "https://openreview.net/pdf?id=Q6PAnqYVpo", "keywords": "natural language processing, full-text search, word embeddings, inverted index, pattern match", "abstract": "Researchers and practitioners in natural language processing and computational linguistics frequently observe and analyze the real language usage in large-scale corpora.\nFor that purpose, they often employ off-the-shelf pattern-matching tools, such as grep, and keyword-in-context concordancers, which is widely used in corpus linguistics for gathering examples.\nNonetheless, these existing techniques rely on surface-level string matching, and thus they suffer from the major limitation of not being able to handle orthographic variations and paraphrasing---notable and common phenomena in any natural language.\nIn addition, existing continuous approaches such as dense vector search tend to be overly coarse, often retrieving texts that are unrelated but share similar topics.\nGiven these challenges, we propose a novel algorithm that achieves soft (or semantic) yet efficient pattern matching by relaxing a surface-level matching with word embeddings.\nOur algorithm is highly scalable with respect to the size of the corpus text utilizing inverted indexes.\nWe have prepared an efficient implementation, and we provide an accessible web tool.\nOur experiments demonstrate that the proposed method\n(i) can execute searches on billion-scale corpora in less than a second, which is comparable in speed to surface-level string matching and dense vector search;\n(ii) can extract harmful instances that semantically match queries from a large set of English and Japanese Wikipedia articles;\nand (iii) can be effectively applied to corpus-linguistic analyses of Latin, a language with highly diverse inflections.", "title_embedding_index": 871, "title_abs_embedding_index": 896}, {"title": "EchoQA: Tuning into the Heart of Echocardiogram Reports", "link_suffix": "/forum?id=JiWlVYB4rh", "link": "https://openreview.net/forum?id=JiWlVYB4rh", "pdf_link": "https://openreview.net/pdf?id=JiWlVYB4rh", "keywords": "Large language models, fine-tuning, fairness, benchmarking", "abstract": "We introduce a novel and extensive instruction-tuning dataset using echocardiogram reports sourced from MIMIC-IV. This dataset is specifically tailored to enhance question answering (QA) systems in the field of cardiology. It comprises 765,605 QA pairs addressing a wide array of cardiac abnormalities and their severity. To validate the utility of this benchmark dataset, we employ various large language models (LLMs), encompassing both open-source general models and biomedical-specific models, along with state-of-the-art closed-source models for zero-shot evaluation. Our results reveal that certain models achieve superior performance across all evaluated metrics. This underscores the effectiveness of instruction fine-tuning for echocardiogram data. Additionally, we conduct an audit of the best performing LLM across demographic groups and marginalized populations. Our objective is to propel the field forward by establishing a benchmark framework for developing LLM AI agents that support clinicians in their daily workflow within the cardiology space. The availability of this dataset aims to support the advancement of natural language models for use in diagnostic decision support systems, aiming to increase efficiency and decrease diagnostic errors in cardiology care. All code will be  available on the Github and the data will be made available on HIPAA-compliant data repository PhysioNet.", "title_embedding_index": 872, "title_abs_embedding_index": 897}, {"title": "Space-time self-attention for graph signal processing", "link_suffix": "/forum?id=a43oZCXdNC", "link": "https://openreview.net/forum?id=a43oZCXdNC", "pdf_link": "https://openreview.net/pdf?id=a43oZCXdNC", "keywords": "seizure detection, eeg, traffic forecasting, graph signal processing, attention, transformer", "abstract": "This work introduces a Transformer-based approach for graph signal processing that leverages a novel task-specific attention mechanism, namely NTAttention. \nUnlike conventional self-attention mechanisms, our method attends to all nodes across multiple time steps, enabling the model to effectively capture dependencies between nodes over extended time periods. This addresses a key limitation faced by traditional methods.\nAdditionally, we propose geometry-aware masking (GMask), which incorporates  the graph topology into the sparsification of the self-attention matrix. This enhances efficiency while preserving the rich temporal information conveyed by the nodes. \nWe demonstrate the effectiveness of our approach on two critical applications: EEG seizure detection and traffic forecasting. Both tasks involve data collected from fixed sensors, such as electrodes or road sensors, where data from one sensor can influence others temporally and spatially. Our model enhances sensitivity in fast seizure detection by 20 percentage points compared to  state-of-the-art and significantly outperforms current methods in traffic forecasting.", "title_embedding_index": 873, "title_abs_embedding_index": 898}, {"title": "Inpainting the Sinogram from Computed Tomography using Latent Diffusion Model and Physics", "link_suffix": "/forum?id=IfPfUHRowT", "link": "https://openreview.net/forum?id=IfPfUHRowT", "pdf_link": "https://openreview.net/pdf?id=IfPfUHRowT", "keywords": "Sinogram Inpainting, Physics, Latent Diffusion Model, X-ray Imaging", "abstract": "Computed Tomography (CT) is a widely used non-invasive imaging technique for materials at microscopic or sub-microscopic length scales in synchrotron radiation facilities. Typically, the object is rotated relative to the X-ray beam, and 2D projection images are recorded by the detector at different rotation angles. The 3D object is then reconstructed by combining these projections and solving a computationally demanding inverse problem. The quality of the reconstructed image is critical for scientific analysis and is influenced by various factors, including the number of projections, exposure time or dose, and the reconstruction algorithm. In this work, we develop a foundation model by integrating a Generative AI-based Latent Diffusion Model (LDM) with physics-based domain knowledge. Specifically, we first incorporate a set of loss functions into our LDM that accurately capture the physical properties of the CT data acquisition process. We demonstrate that addition of these loss functions aids in stable training of the autoencoder in the LDM and improves its accuracy. The autoencoder and the Diffusion model of the LDM is trained with real-world experimental data. Collecting real world experimental data from Synchrotron beamlines is often time-consuming and challenging. We demonstrate that the autoencoder trained with a combination of real world experimental data and phantom features also performs comparable to the autoencoder trained with real world data. Second, we introduce a novel image blending method to combine the LDM\u2019s generated output with the original, extremely sparse sinogram data. Since our model integrates physics-guided loss functions focused on CT data acquisition, it simplifies the creation of downstream tasks and facilitates the adaptation of new features from different experiments. Our experimental evaluation demonstrates improvements of upto 23.5 % in SSIM for sinogram quality and 13.8 % for reconstructed image quality compared to state-of-the-art techniques.", "title_embedding_index": 874, "title_abs_embedding_index": 899}]