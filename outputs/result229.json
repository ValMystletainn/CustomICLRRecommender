[
    {
        "title": "KBLaM: Knowledge Base augmented Language Model",
        "link_suffix": "/forum?id=aLsMzkTej9",
        "link": "https://openreview.net/forum?id=aLsMzkTej9",
        "pdf_link": "https://openreview.net/pdf?id=aLsMzkTej9",
        "keywords": "Large language model; Knowledge augmentation;",
        "abstract": "In this paper, we propose Knowledge Base augmented Language Model (KBLaM), a novel method for augmenting Large Language Models (LLMs) with external knowledge. KBLaM works with a knowledge base (KB) constructed from a corpus of documents, transforming each piece of knowledge in the KB into continuous key-value vector pairs via pre-trained sentence encoders with linear adapters and integrating them into pre-trained LLMs via a specialized rectangular attention mechanism. Unlike Retrieval-Augmented Generation, KBLAM eliminates external retrieval modules, and unlike in-context learning, its computational overhead scales linearly with KB size rather than quadratically. Our approach enables integrating a large KB of more than 10K triples into an 8B pre-trained LLM of only 8K context window on one single A100 80GB GPU and allows for dynamic updates without model fine-tuning or retraining. Experiments demonstrate KBLAM\u2019s effectiveness in various tasks, including question-answering and open-ended reasoning, while providing interpretable insights into its knowledge utilization. Anonymized code and all datasets are provided in supplementary materials."
    },
    {
        "title": "Contrastive Learning with Simplicial Convolutional Networks for Short-Text Classification",
        "link_suffix": "/forum?id=WxSkpPeef3",
        "link": "https://openreview.net/forum?id=WxSkpPeef3",
        "pdf_link": "https://openreview.net/pdf?id=WxSkpPeef3",
        "keywords": "Topological Deep Learning, Contrastive Leaning",
        "abstract": "Text classification is a fundamental task in Natural Language Processing (NLP). Short text classification has recently captured much attention due to its increased amount from various sources with limited labels and its inherent challenges for its sparsity in words and semantics. Recent studies have adopted self-supervised contrastive learning across different representations to improve performance. However, most of the current models face several challenges. Firstly, the augmentation step might not be able to generate positive and negative samples that are semantically similar and dissimilar to the anchor respectively. Secondly, the text data could be enhanced with external auxiliary information that might introduce noise to the sparse text data. In addition, they are limited in capturing higher-order information such as group-wise interactions. In this work, we propose a novel document simplicial complex construction based on text data for a higher-order message-passing mechanism. We develop a simplicial complex representation for text sentences based on the directed word co-occurrence. Novel features are proposed for 0-simplex (word), 1-simplex (word-pair), and 2-simplex (three consecutive words) to characterise intrinsic higher-order structural information among words. We also enhance the short text classification performance by contrasting the structural representation with the sequential representation generated by the transformer mechanism for improved outcomes. The proposed framework, Contrastive Learning with Simplicial Convolutional Networks (C-SCN), leverages the expressive power of graph neural networks, models higher-order information beyond pair-wise relations and enriches features through contrastive learning. Experimental results on four benchmark datasets demonstrate the capability of C-SCN to outperform existing models in analysing sequential and complex short-text data."
    },
    {
        "title": "Adjusting Pretrained Backbones for Performativity",
        "link_suffix": "/forum?id=Wd1OmOwL0C",
        "link": "https://openreview.net/forum?id=Wd1OmOwL0C",
        "pdf_link": "https://openreview.net/pdf?id=Wd1OmOwL0C",
        "keywords": "performative prediction, performative shift, label shift, performativity, dynamic benchmarks",
        "abstract": "With the widespread deployment of deep learning models, they influence their environment in various ways. The induced distribution shifts can lead to unexpected performance degradation in deployed models. Existing methods to anticipate performativity typically incorporate information about the deployed model into the feature vector when predicting future outcomes. While enjoying appealing theoretical properties, modifying the input dimension of the prediction task is often not practical. To address this, we propose a novel technique to adjust pretrained backbones for performativity in a modular way, achieving better sample efficiency and enabling the reuse of existing deep learning assets. Focusing on performative label shift, the key idea is to train a shallow adapter module to perform a \\emph{Bayes-optimal} label shift correction to the backbone's logits given a sufficient statistic of the model to be deployed. As such, our framework decouples the construction of input-specific feature embeddings from the mechanism governing performativity. Motivated by dynamic benchmarking as a use-case, we evaluate our approach under adversarial sampling, for vision and language tasks. We show how it leads to smaller loss along the retraining trajectory and enables us to effectively select among candidate models to anticipate performance degradations. More broadly, our work provides a first baseline for addressing performativity in deep learning."
    },
    {
        "title": "Gradient-free variational learning with conditional mixture networks",
        "link_suffix": "/forum?id=bsr78Cj2H7",
        "link": "https://openreview.net/forum?id=bsr78Cj2H7",
        "pdf_link": "https://openreview.net/pdf?id=bsr78Cj2H7",
        "keywords": "variational inference, mixture-of-experts, variational Bayes, Mixture Models, conjugate-exponential, gradient-free, Bayesian neural network",
        "abstract": "Bayesian methods are known to address some limitations of standard deep learning, such as the lack of calibrated predictions and uncertainty quantification. However, they can be computationally expensive as model and data complexity increase. Fast variational methods can reduce the computational requirements of Bayesian methods by eliminating the need for gradient descent or sampling, but are often limited to simple models. We demonstrate that conditional mixture networks (CMNs), a probabilistic variant of the mixture-of-experts (MoE) model, are suitable for fast, gradient-free inference and can solve complex classification tasks, thus balancing the expressiveness and scalability of neural networks with the probabilistic benefits of Bayesian methods . By exploiting conditional conjugacy and Polya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of both the experts and the gating network. This enables efficient variational updates using coordinate ascent variational inference (CAVI), avoiding traditional gradient-based optimization. We validate this approach by training two-layer CMNs on standard benchmarks from the UCI repository. Our method, CAVI-CMN, achieves competitive and often superior predictive accuracy compared to maximum likelihood estimation (MLE) with backpropagation, while maintaining competitive runtime and full posterior distributions over all model parameters. Moreover, as input size or the number of experts increases, computation time scales competitively with MLE and other gradient-based solutions like black-box variational inference (BBVI), making CAVI-CMN a promising tool for deep, fast, and gradient-free Bayesian networks."
    },
    {
        "title": "Minimalistic Predictions for Online Class Constraint Scheduling",
        "link_suffix": "/forum?id=j8lqABLgub",
        "link": "https://openreview.net/forum?id=j8lqABLgub",
        "pdf_link": "https://openreview.net/pdf?id=j8lqABLgub",
        "keywords": "Scheduling, Class Constraints, Predictions, Online Algorithms",
        "abstract": "We consider online scheduling with class constraints. That is, we are given $m$ machines, each with $k$ class slots. Upon receiving a job $j$ with class $c_j$, an algorithm needs to allocate $j$ on some machine $i$. The goal is to minimize the makespan while not assigning more than $k$ different classes onto each machine.\nWhile the offline case is well understood and even (E)PTAS results are known [Jansen, Lassota, Maack SPAA'20, Chen Jansen Luo Zhang COCOA'16], the online case admits strong impossibility results in classical competitive analysis [Epstein, Lassota, Levin, Maack, Rohwedder STACS'22].We overcome these daunting results by investigating the problem in a learning-augmented setting where an algorithm can access possibly erroneous predictions. We present new algorithms with competitive ratios independent of $m$ and tight lower bounds for several classical and problem-specific prediction models. We thereby give a structured overview of what additional information helps in the design of better scheduling algorithms."
    },
    {
        "title": "Do Large Language Models Truly Understand Geometric Structures?",
        "link_suffix": "/forum?id=FjQOXenaXK",
        "link": "https://openreview.net/forum?id=FjQOXenaXK",
        "pdf_link": "https://openreview.net/pdf?id=FjQOXenaXK",
        "keywords": "Large Language Models, Geometric Ability Evaluation, Geometric Relationship Identification",
        "abstract": "Geometric ability is a significant challenge for large language models (LLMs) due to the need for advanced spatial comprehension and abstract thinking. Existing datasets primarily evaluate LLMs on their final answers, but they cannot truly measure their true understanding of geometric structures, as LLMs can arrive at correct answers by coincidence. To fill this gap, we introduce the GeomRel dataset, designed to evaluate LLMs\u2019 understanding of geometric structures by isolating the core step of geometric relationship identification in problem-solving. Using this benchmark, we conduct thorough evaluations of diverse LLMs and identify key limitations in understanding geometric structures. We further propose the Geometry Chain-of-Thought (GeoCoT) method, which enhances LLMs\u2019 ability to identify geometric relationships, resulting in significant performance improvements."
    },
    {
        "title": "Identification of Nonparametric Dynamic Causal Model and Latent Process for Climate Analysis",
        "link_suffix": "/forum?id=nzgvkQM3EH",
        "link": "https://openreview.net/forum?id=nzgvkQM3EH",
        "pdf_link": "https://openreview.net/pdf?id=nzgvkQM3EH",
        "keywords": "causal discovery; causal representation learning; climate analysis",
        "abstract": "The study of causal structure learning with latent variables has advanced our understanding of the world by uncovering causal relationships and latent factors. However, in real-world scenarios, such as those in climate systems, causal relationships are often nonparametric, dynamic, and exist among both observed variables and latent variables. To address these challenges, we consider a general setting where causal relations are nonparametric and unrestricted in their occurrence. With the aid of three measurements in temporal structure, we theoretically show that both latent variables and processes can be identified up to minor indeterminacy under mild assumptions. Furthermore, we establish that the observed causal structure is identifiable if, roughly speaking, the latent variables induce sufficient variations in the noise terms. Based on these theoretical insights, we develop a principled estimation approach that simultaneously learns both the causal structure and latent representation. Experimental results on simulation studies validate the theoretical foundations and demonstrate the effectiveness of the proposed methodology. In the climate data experiments, we show that it offers a powerful and in-depth understanding of climate system."
    },
    {
        "title": "UnrealCV Zoo: Enriching Photo-realistic Virtual Worlds for Embodied AI Agents",
        "link_suffix": "/forum?id=vQ1y086Kn2",
        "link": "https://openreview.net/forum?id=vQ1y086Kn2",
        "pdf_link": "https://openreview.net/pdf?id=vQ1y086Kn2",
        "keywords": "Virtual worlds; Embodied AI; Embodied Tracking and Navigation; Visual RL;",
        "abstract": "The embodied artificial intelligence agents should be capable of sensing, reasoning, planning, and acting in complex open worlds, which are unstructured, high-dynamic, and uncertain. To apply agents in the real world, the realism of the simulated worlds is important for training and evaluating the built agents. This paper introduces UnrealZoo, a rich collection of photo-realistic 3D environments that mimic the complexity and variability of the real world based on Unreal Engine. For embodied AI, we provide a diverse array of playable entities in the environments and a suite of tools, based on UnrealCV, for data collection, reinforcement learning, and evaluation. In the experiments, we benchmark the agent on visual navigation and tracking, two fundamental tasks for embodied vision agents, in complex open worlds. The results provide valuable insights into the strengths of enriching the diversity of the training environments and the challenges to current embodied vision agents in the open worlds, e.g., the latency in the closed-loop control to interact with the dynamic objects, reason the accordance of the spatial structure in the complex scenes."
    },
    {
        "title": "The Deficit of New Information in Diffusion Models: A Focus on Diverse Samples",
        "link_suffix": "/forum?id=rAZ3yCpc3K",
        "link": "https://openreview.net/forum?id=rAZ3yCpc3K",
        "pdf_link": "https://openreview.net/pdf?id=rAZ3yCpc3K",
        "keywords": "DIffusion Models, Diverse Samples, Information Theory",
        "abstract": "Diffusion models are renowned for their state-of-the-art performance in generating high-quality images. Identifying samples with new information beyond the training data is essential for data augmentation, especially for enhancing model performance in diverse and unforeseen real-world scenarios. However, the investigation of new information in the generated samples has not been well explored. Our investigation through the lens of information theory reveals that diffusion models do not produce new information beyond what exists in the training data. Next, we introduce the concept of diverse samples (DS) to prove that generated images could contain information not present in the training data for diffusion models. Furthermore, we propose a method for identifying diverse samples among generated images by extracting deep features and detecting images that fall outside the boundary of real images. We demonstrate that diverse samples exist in the generated data of diffusion models, attributed to the estimation of forward and backward processes, but it can only produce a limited number of diverse samples, underscoring a notable gap in their capabilities in generating diverse samples. In addition, our experiment on the Chest X-ray dataset demonstrates that the diverse samples are more useful in improving classification accuracy than vanilla-generated samples. The source code is available at \\url{https://github.com/lypz12024/diffusion-diverse-samples}."
    },
    {
        "title": "Towards Stable, Globally Expressive Graph Representations with Laplacian Eigenvectors",
        "link_suffix": "/forum?id=KmphHE92wU",
        "link": "https://openreview.net/forum?id=KmphHE92wU",
        "pdf_link": "https://openreview.net/pdf?id=KmphHE92wU",
        "keywords": "graph neural networks, graph Laplacian eigenvectors",
        "abstract": "Graph neural networks (GNNs) have achieved remarkable success in a variety of machine learning tasks over graph data. Existing GNNs usually rely on message passing, i.e., computing node representations by gathering information from the neighborhood, to build their underlying computational graphs. Such an approach has been shown fairly limited in expressive power, and often fails to capture global characteristics of graphs. To overcome the issue, a popular solution is to use Laplacian eigenvectors as additional node features, as they are known to contain global positional information of nodes, and can serve as extra node identifiers aiding GNNs to separate structurally similar nodes. Since eigenvectors naturally come with symmetries---namely, $O(p)$-group symmetry for every $p$ eigenvectors with equal eigenvalue, properly handling such symmetries is crucial for the stability and generalizability of Laplacian eigenvector augmented GNNs. However, using a naive $O(p)$-group invariant encoder for each $p$-dimensional eigenspace may not keep the full expressivity in the Laplacian eigenvectors. Moreover, computing such invariants inevitably entails a hard split of Laplacian eigenvalues according to their numerical identity, which suffers from great instability when the graph structure has small perturbations. In this paper, we propose a novel method exploiting Laplacian eigenvectors to generate \\textit{stable} and globally \\textit{expressive} graph representations. The main difference from previous works is that (i) our method utilizes \\textbf{learnable} $O(p)$-invariant representations for each Laplacian eigenspace of dimension $p$, which are built upon powerful orthogonal group equivariant neural network layers already well studied in the literature, and that (ii) our method deals with numerically close eigenvalues in a \\textbf{smooth} fashion, ensuring its better robustness against perturbations. Experiments on various graph learning benchmarks witness the competitive performance of our method, especially its great potential to learn global properties of graphs."
    },
    {
        "title": "MuSiCNet: A Gradual Coarse-to-Fine Framework for Irregularly Sampled Multivariate Time Series Analysis",
        "link_suffix": "/forum?id=BHgMPObtE0",
        "link": "https://openreview.net/forum?id=BHgMPObtE0",
        "pdf_link": "https://openreview.net/pdf?id=BHgMPObtE0",
        "keywords": "Irregularly Sampled Multivariate Time Series, Attention Mechanism, Time Series Analysis, Representation Learning",
        "abstract": "Irregularly sampled multivariate time series (ISMTS) are prevalent in reality. Most existing methods treat ISMTS as synchronized regularly sampled time series with missing values, neglecting that the irregularities are primarily attributed to variations in sampling rates. In this paper, we introduce a novel perspective that irregularity is essentially relative in some senses. With sampling rates artificially determined from low to high, an irregularly sampled time series can be transformed into a hierarchical set of relatively regular time series from coarse to fine. We observe that additional coarse-grained relatively regular series not only mitigate the irregularly sampled challenges to some extent but also incorporate broad-view temporal information, thereby serving as a valuable asset for representation learning. Therefore, following the philosophy of learning that Seeing the big picture first, then delving into the details, we present theMulti-Scale and Multi-Correlation Attention Network (MuSiCNet) combining multiple scales to iteratively refine the ISMTS representation. Specifically, within each scale, we explore time attention and frequency correlation matrices to aggregate intra- and inter-series information, naturally enhancing the representation quality with richer and more intrinsic details. While across adjacent scales, we employ a representation rectification method containing contrastive learning and reconstruction results adjustment to further improve representation consistency. MuSiCNet is an ISMTS analysis framework that competitive with SOTA in three mainstream tasks consistently, including classification, interpolation, and forecasting."
    },
    {
        "title": "MONICA: Benchmarking on Long-tailed Medical Image Classification",
        "link_suffix": "/forum?id=qtqvuBmhxU",
        "link": "https://openreview.net/forum?id=qtqvuBmhxU",
        "pdf_link": "https://openreview.net/pdf?id=qtqvuBmhxU",
        "keywords": "Long-tailed Learning, Benchmark, Medical Image Classification",
        "abstract": "Long-tailed learning is considered to be an extremely challenging problem in data imbalance learning. It aims to train well-generalized models from a large number of images that follow a long-tailed class distribution. In the medical field, many diagnostic imaging exams such as dermoscopy and chest radiography yield a long-tailed distribution of complex clinical findings. Recently, long-tailed learning in medical image analysis has garnered significant attention. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often leads to unfair comparisons and inconclusive results. To help the community improve the evaluation and advance, we build a unified, well-structured codebase called Medical OpeN-source Long-taIled ClassifiCAtion (MONICA), which implements over 30 methods developed in relevant fields and evaluated on 12 long-tailed medical datasets covering 6 medical domains. Our work provides valuable practical guidance and insights for the field, offering detailed analysis and discussion on the effectiveness of individual components within the inbuilt state-of-the-art methodologies. We hope this codebase serves as a comprehensive and reproducible benchmark, encouraging further advancements in long-tailed medical image learning. The codebase will be publicly available on GitHub."
    },
    {
        "title": "On Scaling Up 3D Gaussian Splatting Training",
        "link_suffix": "/forum?id=pQqeQpMkE7",
        "link": "https://openreview.net/forum?id=pQqeQpMkE7",
        "pdf_link": "https://openreview.net/pdf?id=pQqeQpMkE7",
        "keywords": "Gaussian Splatting, Machine Learning System, Distributed Training",
        "abstract": "3D Gaussian Splatting (3DGS) is increasingly popular for 3D reconstruction due to its superior visual quality and rendering speed. However, 3DGS training currently occurs on a single GPU, limiting its ability to handle high-resolution and large-scale 3D reconstruction tasks due to memory constraints. We introduce Grendel, a distributed system designed to partition 3DGS parameters and parallelize computation across multiple GPUs. As each Gaussian affects a small, dynamic subset of rendered pixels, Grendel employs sparse all-to-all communication to transfer the necessary Gaussians to pixel partitions and performs dynamic load balancing. Unlike existing 3DGS systems that train using one camera view image at a time, Grendel supports batched training with multiple views. We explore various optimization hyperparameter scaling strategies and find that a simple sqrt(batch-size) scaling rule is highly effective. Evaluations using large-scale, high-resolution scenes show that Grendel enhances rendering quality by scaling up 3DGS parameters across multiple GPUs. On the 4K ``Rubble'' dataset, we achieve a test PSNR of 27.28 by distributing 40.4 million Gaussians across 16 GPU, compared to a PSNR of 26.28 using 11.2 million Gaussians on a single GPU."
    },
    {
        "title": "DeepTAGE: Deep Temporal-Aligned Gradient Enhancement for Optimizing Spiking Neural Networks",
        "link_suffix": "/forum?id=drPDukdY3t",
        "link": "https://openreview.net/forum?id=drPDukdY3t",
        "pdf_link": "https://openreview.net/pdf?id=drPDukdY3t",
        "keywords": "Spiking Neural Networks, Deep Temporal-Aligned Gradient Enhancement, Spatio-Temporal Deep Supervision, Optimization Imbalance.",
        "abstract": "Spiking Neural Networks (SNNs), with their biologically inspired spatio-temporal dynamics and spike-driven processing, are emerging as a promising low-power alternative to traditional Artificial Neural Networks (ANNs). However, the complex neuronal dynamics and non-differentiable spike communication mechanisms in SNNs present substantial challenges for efficient training. By analyzing the membrane potentials in spiking neurons, we found that their distributions can increasingly deviate from the firing threshold as time progresses, which tends to cause diminished backpropagation gradients and unbalanced optimization. To address these challenges, we propose Deep Temporal-Aligned Gradient Enhancement (DeepTAGE), a novel approach that improves optimization gradients in SNNs from both internal surrogate gradient functions and external supervision methods. Our DeepTAGE dynamically adjusts surrogate gradients in accordance with the membrane potential distribution across different time steps, enhancing their respective gradients in a temporal-aligned manner that promotes balanced training. Moreover, to mitigate issues of gradient vanishing or deviating during backpropagation, DeepTAGE incorporates deep supervision at both spatial (network stages) and temporal (time steps) levels to ensure more effective and robust network optimization. Importantly, our method can be seamlessly integrated into existing SNN architectures without imposing additional inference costs or requiring extra control modules. We validate the efficacy of DeepTAGE through extensive experiments on static benchmarks (CIFAR10, CIFAR100, and ImageNet-1k) and a neuromorphic dataset (DVS-CIFAR10), demonstrating significant performance improvements."
    },
    {
        "title": "RepoFixEval: A Repository-Level Program Repair Benchmark From Issue Discovering to Bug Fixing",
        "link_suffix": "/forum?id=LaNCeNmoHR",
        "link": "https://openreview.net/forum?id=LaNCeNmoHR",
        "pdf_link": "https://openreview.net/pdf?id=LaNCeNmoHR",
        "keywords": "Code, Bug fix, Program Repair, LLM",
        "abstract": "Automatic Program Repair (APR) aims to automatically fix software bugs, playing an essential role in software development. While current research demonstrates that Large Language Models (LLMs) excel in file-level program repair, their effectiveness in repository-level program repair remains unexplored. Real-world software projects, which often consist of multiple files, present significant challenges for LLMs in identifying bugs and generating fixes due to the intricate project structures. To bridge this gap, we introduce RepoFixEval, a repository-level APR benchmark consisting of 160 real-world bug fixing suites from popular Python projects. RepoFixEval provides the original buggy programs, associated issue reports, corresponding fixes, and unit tests to verify the correctness of each fix. Based on the benchmark, we further propose a three-step evaluation framework for LLM-based APR tools, encompassing (1) discovering issues from execution failures, (2) localizing buggy code segments, and (3) generating code fixes. Experimental results highlight that LLMs struggle with organizing error messages during the issue discovery phase. We find that longer contexts positively affect performance, but only a few LLMs can effectively utilize extended context information at the 128K level. Some open-source LLMs demonstrate competitiveness with closed-source counterparts, yet even the best-performing GPT-4o only resolves 12.3% of bugs. Our study reveals the capabilities and limitations of 16 LLMs in handling repository-level bugs, providing valuable insights for future research in this field."
    },
    {
        "title": "Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization",
        "link_suffix": "/forum?id=PNMv4r7s1i",
        "link": "https://openreview.net/forum?id=PNMv4r7s1i",
        "pdf_link": "https://openreview.net/pdf?id=PNMv4r7s1i",
        "keywords": "Reward Over-Optimization, Reinforcement Learning from Human Feedback, Large Language Model",
        "abstract": "Reinforcement learning from human feedback (RLHF) is an effective method for aligning large language models (LLMs) with human values. However, reward over-optimization remains an open challenge leading to discrepancies between the performance of LLMs under the reward model and the true human objectives. A primary contributor to reward over-optimization is the extrapolation error that arises when the reward model evaluates out-of-distribution (OOD) responses. However, current methods still fail to prevent the increasing frequency of OOD response generation during the reinforcement learning (RL) process and are not effective at handling extrapolation errors from OOD responses. In this work, we propose theBehavior-Supported Policy Optimization(BSPO) method to mitigate the reward over-optimization issue. Specifically, we definebehavior policyas the next token distribution of the reward training dataset to model the in-distribution (ID) region of the reward model. Building on this, we introduce the behavior-supported Bellman operator to regularize the value function, penalizing all OOD values without impacting the ID ones. Consequently, BSPO reduces the generation of OOD responses during the RL process, thereby avoiding overestimation caused by the reward model\u2019s extrapolation errors. Theoretically, we prove that BSPO guarantees a monotonic improvement of the supported policy until convergence to the optimal behavior-supported policy. Empirical results from extensive experiments show that BSPO outperforms baselines in preventing reward over-optimization due to OOD evaluation and finding the optimal ID policy."
    },
    {
        "title": "CONSTRAINT-AWARE ZERO-SHOT VISION-LANGUAGE NAVIGATION IN CONTINUOUS ENVIRONMENTS",
        "link_suffix": "/forum?id=eWFkMCBySw",
        "link": "https://openreview.net/forum?id=eWFkMCBySw",
        "pdf_link": "https://openreview.net/pdf?id=eWFkMCBySw",
        "keywords": "Instruction Navigation, training free, unexplored environment",
        "abstract": "We present Constraint-Aware Navigator (CA-Nav), a zero-shot approach for Vision-Language Navigation in Continuous Environments (VLN-CE).\nCA-Nav reframes the zero-shot VLN-CE task as a sequential constraint-aware sub-instruction completion process, continuously translating sub-instructions into navigation plans via a cross-modal value map. \nCentral to our approach are two modules namely Constraint-aware Sub-instruction Manager (CSM) and Constraint-aware Value Mapper (CVM).\nCSM defines the completion criteria of decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner.\nBased on the constraints identified\nby CSM, CVM builds a value map on-the-fly and refines it using superpixel clustering to enhance navigation stability.\nCA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the compared best method by 12% on R2R-CE and 13% on RxR-CE in terms of Success Rate on the validation unseen split.\nFurthermore, CA-Nav demonstrates its effectiveness in real-world robot deployments across diverse indoor scenes and instructions."
    },
    {
        "title": "New Algorithms for the Learning-Augmented k-means Problem",
        "link_suffix": "/forum?id=Xuyp1dGAbi",
        "link": "https://openreview.net/forum?id=Xuyp1dGAbi",
        "pdf_link": "https://openreview.net/pdf?id=Xuyp1dGAbi",
        "keywords": "Learning-Augmented Clustering; Approximation Algorithm;",
        "abstract": "In this paper, we study the clustering problems in the learning-augmented setting, where predicted labels for a d-dimensional dataset with size m are given by an oracle to serve as auxiliary information to improve the clustering performance. Following the prior work, the given oracle is parameterized by some error rate \u03b1, which captures the accuracy of the oracle such that there are at most \u03b1 fraction of false positives and false negatives in each predicted cluster. In this setting, the goal is to design fast and practical algorithms that can break the computational barriers of inapproximability. The current state-of-the-art learning-augmented k-means algorithm relies on sorting strategies to find good coordinates approximation, where a (1+O(\u03b1))-approximation can be achieved with near-linear running time in the data size. However, the computational demands for sorting may limit the scalability of the algorithm for handling large-scale datasets. To address this issue, in this paper, we propose new algorithms that can identify good coordinates approximation using sampling-based strategies, where (1+O(\u03b1))-approximation can be achieved with linear running time in the data size. To obtain a more practical algorithm for the problem with better clustering quality and running time, we propose a sampling-based heuristic which can directly find center approximations using sampling-based strategies. Empirical experiments show that our proposed methods are faster than the state-of-the-art learning-augmented k-means algorithms with comparable performances on clustering quality."
    },
    {
        "title": "LLM Table Reading: Bridging the Semantic Gap Between Text and Table",
        "link_suffix": "/forum?id=qmsX2R19p9",
        "link": "https://openreview.net/forum?id=qmsX2R19p9",
        "pdf_link": "https://openreview.net/pdf?id=qmsX2R19p9",
        "keywords": "Large Language Model, Table Representation Learning, NL2SQL",
        "abstract": "The rise of Large Language Models (LLMs) has sparked an impressive wave of influence worldwide, yet these models still exhibit weakness in understanding structured tabular data.\nAlthough a larger context window of next-generation LLMs promises them to accommodate a larger volume of table contents, it does not inherently improve the model's capability to understand the underlying structure and semantics of tabular data.\nTo bridge the gap betweenText andTable, we propose TNT, a table-language model that empowers LLMs with the ability to effectively and efficiently extract structure-enriched semantics from tabular data with high-level representations. \nTNT also introduces a scalable and efficient training pipeline, featuring novel self-supervised learning tasks, to integrate abstract tabular knowledge into the language modality.\nExtensive experimental results on the most iconic table understanding task of NL2SQL demonstrate a (much) better table understanding of TNT, which achieves up to14.4%higher execution accuracy compared with traditional text-based table serialization."
    },
    {
        "title": "Turing completeness of prompting",
        "link_suffix": "/forum?id=AS8SPTyBgw",
        "link": "https://openreview.net/forum?id=AS8SPTyBgw",
        "pdf_link": "https://openreview.net/pdf?id=AS8SPTyBgw",
        "keywords": "Prompting, Large Language Model, Transformer, Expressive Power",
        "abstract": "Since the success of GPT, large language models (LLMs) have revolutionized machine learning and have initiated the so-calledLLM promptingparadigm. In the era of LLMs, people train a single general-purpose LLM and provide the LLM with differentpromptsto perform different tasks. However, such empirical success largely lacks theoretical understanding. Here, we present the first theoretical study on the LLM prompting paradigm to the best of our knowledge. In this work, we show that prompting is in fact Turing-complete: there exists a finite-size Transformer such that for any computable function, there exists a corresponding prompt following which the Transformer computes the function. Furthermore, we show that even though we use only a single finite-size Transformer, it can still achieve nearly the same complexity bounds as that of the class of all unbounded-size Transformers. Overall, our result reveals that prompting can enable a single finite-size Transformer to be efficiently universal, which establishes a theoretical underpinning for prompt engineering in practice."
    },
    {
        "title": "Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning",
        "link_suffix": "/forum?id=8Dj6OEMj6W",
        "link": "https://openreview.net/forum?id=8Dj6OEMj6W",
        "pdf_link": "https://openreview.net/pdf?id=8Dj6OEMj6W",
        "keywords": "Large Language Models, Mathematical Reasoning, Self-correction",
        "abstract": "Accurate mathematical reasoning with Large Language Models (LLMs) is crucial in revolutionizing domains that heavily rely on such reasoning. However, LLMs often encounter difficulties in certain aspects of mathematical reasoning, leading to flawed reasoning and erroneous results. To mitigate these issues, we introduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically designed to embed self-correction as an inherent ability in LLMs, enabling them to validate and rectify their own results. The CoSC mechanism operates through a sequence of self-correction stages. In each stage, the LLMs generate a program to address a given problem, execute this program using program-based tools to obtain an output, subsequently verify this output. Based on the verification, the LLMs either proceed to the next correction stage or finalize the answer. This iterative self-correction process allows the LLMs to refine its reasoning steps and improve the accuracy of its mathematical reasoning. To enable the CoSC mechanism at a low cost, we employ a two-phase finetuning approach. In the first phase, the LLMs are trained with a relatively small volume of seeding data generated from GPT-4, establishing an initial CoSC capability. In the second phase, the CoSC capability is further enhanced by training with a larger volume of self-generated data using the trained model in the first phase, without relying on the paid GPT-4. Our comprehensive experiments demonstrate that CoSC significantly improves performance on traditional mathematical datasets among existing open-source LLMs. Notably, our CoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging mathematical reasoning dataset in the public domain, surpassing the performance of well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs like GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra. It's important to note that, unlike these proprietary models, our CoSC performs inference in a zero-shot manner, without the need for demonstrations. The code and data for this work will be released once this paper is accepted."
    },
    {
        "title": "Noisy Test-Time Adaptation in Vision-Language Models",
        "link_suffix": "/forum?id=iylpeTI0Ql",
        "link": "https://openreview.net/forum?id=iylpeTI0Ql",
        "pdf_link": "https://openreview.net/pdf?id=iylpeTI0Ql",
        "keywords": "OOD Detection, Test-Time Adaptation",
        "abstract": "Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy samples during test-time in a zero-shot manner. In the preliminary study, we reveal that existing TTA methods suffer from a severe performance decline under ZS-NTTA, often lagging behind even the frozen model.  We conduct comprehensive experiments to analyze this phenomenon, revealing that the negative impact of unfiltered noisy data outweighs the benefits of clean data during model updating. In addition, as these methods adopt the adapting classifier to implement ID classification and noise detection sub-tasks, the ability of the model in both sub-tasks is largely hampered. Based on this analysis, we propose a novel framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier (including the backbone) frozen. Technically, we introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen model\u2019s outputs as pseudo-labels to train a noise detector for detecting noisy samples effectively. To address clean data streams, we further inject Gaussian noise during adaptation, preventing the detector from misclassifying clean samples as noisy.\nBeyond the ZS-NTTA, AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection ability of VLMs. Extensive experiments show that our method outperforms in both ZS-NTTA and ZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32%$ in harmonic mean accuracy ($\\text{Acc}_\\text{H}$) for ZS-NTTA and $9.40%$ in FPR95 for ZS-OOD detection, compared to state-of-the-art methods. Importantly, AdaND is computationally efficient and comparable to the model-frozen method."
    },
    {
        "title": "Continuous Approximation of Momentum Methods with Explicit Discretization Error",
        "link_suffix": "/forum?id=JZdd7EUefP",
        "link": "https://openreview.net/forum?id=JZdd7EUefP",
        "pdf_link": "https://openreview.net/pdf?id=JZdd7EUefP",
        "keywords": "momentum methods, continuous approximation, discretization error, implicit bias",
        "abstract": "Momentum-based optimization methods, such as Heavy-Ball (HB) and Nesterov's accelerated gradient (NAG), are essential in training modern deep neural networks. This work sheds light on the learning dynamics of momentum-based methods and how they behave differently than standard gradient descent (GD) in theory and practice. A promising approach to answer this question is \ninvestigating the continuous differential equations to approximate the discrete updates, \nan area requiring much attention for momentum methods. In this work, we take HB as a case study to investigate two important aspects of momentum methods. First, to enable a formal analysis of the Heavy-Ball momentum method, we propose a new continuous approximation, HB Flow (HBF), with a formulation that allows the control of discretization error to arbitrary order.\nAs an application of HBF, we leverage it to investigate the implicit bias of HB by conducting a series of analyses on the diagonal linear networks to inspect the influence of momentum on the model's generalization property. We validate theoretical findings in numerical experiments, which confirm the significance of HBF as an effective proxy of momentum methods to bridge between discrete and continuous learning dynamics."
    },
    {
        "title": "Data-Driven Uncertainty-Aware Forecasting of Sea Ice Conditions in the Gulf of Ob Based on Satellite Radar Imagery",
        "link_suffix": "/forum?id=36DlQGFb7W",
        "link": "https://openreview.net/forum?id=36DlQGFb7W",
        "pdf_link": "https://openreview.net/pdf?id=36DlQGFb7W",
        "keywords": "Arctic Sea Ice Forecasting, Satellite Radar Imagery, Ensemble Forecasting, Uncertainty Quantification, Machine Learning for Video Prediction",
        "abstract": "The increase in Arctic marine activity due to rapid warming and significant sea ice loss necessitates highly reliable, short-term sea ice forecasts to ensure maritime safety and operational efficiency. In this work, we present a novel data-driven approach for sea ice condition forecasting in the Gulf of Ob, leveraging sequences of radar images from Sentinel-1, weather observations, and GLORYS forecasts. Our approach integrates advanced video prediction models, originally developed for vision tasks, with domain-specific data preprocessing and augmentation techniques tailored to the unique challenges of Arctic sea ice dynamics. Central to our methodology is the use of uncertainty quantification to assess the reliability of predictions, ensuring robust decision-making in safety-critical applications. Furthermore, we propose a confidence-based model mixture mechanism that enhances forecast accuracy and model robustness, crucial for safe operations in volatile Arctic environments. Our results demonstrate substantial improvements over baseline approaches, underscoring the importance of uncertainty quantification and specialized data handling for effective and reliable sea ice forecasting."
    },
    {
        "title": "Physics-informed GNN for non-linear constrained optimization: PINCO, a solver for the AC-optimal power flow",
        "link_suffix": "/forum?id=BfI0D1ci9r",
        "link": "https://openreview.net/forum?id=BfI0D1ci9r",
        "pdf_link": "https://openreview.net/pdf?id=BfI0D1ci9r",
        "keywords": "Power systems optimization, Non-linear optimization, Graph neural networks (GNNs), Physics-informed neural networks (PINN)",
        "abstract": "The energy transition is driving the integration of large shares of intermittent power sources in the electric power grid. Therefore, addressing the AC optimal power flow (AC-OPF) effectively becomes increasingly essential.\nThe AC-OPF, which is a fundamental optimization problem in power systems, must be solved more frequently to ensure the safe and cost-effective operation of power systems. Due to its non-linear nature, AC-OPF is often solved in its linearized form, despite inherent inaccuracies. Non-linear solvers, such as the interior point method, are typically employed to solve the full OPF problem. However, these iterative methods may not converge for large systems and do not guarantee global optimality. This work explores a physics-informed graph neural network, PINCO, to solve the AC-OPF. We demonstrate that this method provides accurate solutions in a fraction of the computational time when compared to the established non-linear programming solvers. Remarkably, PINCO generalizes effectively across a diverse set of loading conditions in the power system. We show that our method can solve the AC-OPF without violating inequality constraints. Furthermore, it can function both as a solver and as a hybrid universal function approximator. Moreover, the approach can be easily adapted to different power systems with minimal adjustments to the hyperparameters, including systems with multiple generators at each bus. Overall, this work demonstrates an advancement in the field of power system optimization to tackle the challenges of the energy transition. The code and data utilized in this paper are available athttps://anonymous.4open.science/r/opf_pinn_iclr-B83E/."
    }
]