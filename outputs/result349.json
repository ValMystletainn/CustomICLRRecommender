[
    {
        "title": "Trustworthy Dataset Proof: Certifying the Authentic Use of Dataset in Training Models for Enhanced Trust",
        "link_suffix": "/forum?id=cazOlqncU6",
        "link": "https://openreview.net/forum?id=cazOlqncU6",
        "pdf_link": "https://openreview.net/pdf?id=cazOlqncU6",
        "keywords": "dataset integrity; trustworthy dataset proof; data probe; watermark",
        "abstract": "In the realm of deep learning, the veracity and integrity of the training data are pivotal for constructing reliable and transparent models. This study introduces the concept of Trustworthy Dataset Proof (TDP), which tackles the significant challenge of verifying the authenticity of training data as declared by trainers. Existing dataset provenance methods, which primarily aim at ownership verification rather than trust enhancement, often face challenges with usability and integrity. For instance, excessive operational demands and the inability to effectively verify dataset authenticity hinder their practical application. To address these shortcomings, we propose a novel technique termed Data Probe, which diverges from traditional watermarking by utilizing subtle variations in model output distributions to confirm the presence of a specific and small subset of training data. This model-agnostic approach improves usability by minimizing the intervention during the training process and ensures dataset integrity via a mechanism that only permits probe detection when the entire claimed dataset is utilized in training. Our study conducts extensive evaluations to demonstrate the effectiveness of the proposed data-drobe-based TDP framework, marking a significant step toward achieving transparency and trustworthiness in the use of training data in deep learning."
    },
    {
        "title": "Revisiting On-Policy Deep Reinforcement Learning",
        "link_suffix": "/forum?id=39JM3A3KS3",
        "link": "https://openreview.net/forum?id=39JM3A3KS3",
        "pdf_link": "https://openreview.net/pdf?id=39JM3A3KS3",
        "keywords": "Deep reinforcement learning, on-policy, policy gradients",
        "abstract": "On-policy Reinforcement Learning (RL) offers desirable features such as stable learning, fewer policy updates, and the ability to evaluate a policy\u2019s return during training. While recent efforts have focused on off-policy methods, achieving significant advancements, Proximal Policy Optimization (PPO) remains the go-to algorithm for on-policy RL due to its apparent simplicity and effectiveness. However, despite its apparent simplicity, PPO is highly sensitive to hyperparameters and depends on subtle and poorly documented tweaks that can make or break its success--hindering its applicability in complex problems. In this paper, we revisit on-policy deep RL with a focus on improving PPO, by introducing principled solutions that enhance its performance while eliminating the need for extensive hyperparameter tuning and implementation-level optimizations. Our effort leads to PPO+, a methodical adaptation of the PPO algorithm that adheres closer to its theoretical foundations. \nPPO+ sets a new state-of-the-art for on-policy RL on MuJoCo control problems while maintaining a straightforward trick-free implementation. Beyond just performance, our findings offer a fresh perspective on on-policy RL that could reignite interest in these approaches."
    },
    {
        "title": "On the Role of Attention Heads in Large Language Model Safety",
        "link_suffix": "/forum?id=h0Ak8A5yqw",
        "link": "https://openreview.net/forum?id=h0Ak8A5yqw",
        "pdf_link": "https://openreview.net/pdf?id=h0Ak8A5yqw",
        "keywords": "interpretability, large language model, multi-head attention, safety, harmful content",
        "abstract": "Large language models (LLMs) achieve state-of-the-art performance on multiple language tasks, yet their safety guardrails can be circumvented, leading to harmful generations. In light of this, recent research on safety mechanisms has emerged, revealing that when safety representations or component are suppressed, the safety capability of LLMs are compromised. However, existing research tends to overlook the safety impact of multi-head attention mechanisms, despite their crucial role in various model functionalities. Hence, in this paper, we aim to explore the connection between standard attention mechanisms and safety capability to fill this gap in the safety-related mechanistic interpretability. We propose an novel metric which tailored for multi-head attention, the Safety Head ImPortant Score (Ships), to assess the individual heads' contributions to model safety. Base on this, we generalize Ships to the dataset level and further introduce the Safety Attention Head AttRibution Algorithm (Sahara) to attribute the critical safety attention heads inside the model. Our findings show that special attention head has a significant impact on safety. Ablating a single safety head allows aligned model (e.g., Llama-2-7b-chat) to respond to16$\\times\\uparrow$more harmful queries, while only modifying0.006%$\\downarrow$ of the parameters, in contrast to the $\\sim$5%modification required in previous studies. More importantly, we demonstrate that attention heads primarily function as feature extractors for safety and models fine-tuned from the same base model exhibit overlapping safety heads through comprehensive experiments. Together, our attribution approach and findings provide a novel perspective for unpacking the black box of safety mechanisms in large models."
    },
    {
        "title": "Aggregation of Multi Diffusion Models for Enhancing Learned Representations",
        "link_suffix": "/forum?id=xNaPs8bdLa",
        "link": "https://openreview.net/forum?id=xNaPs8bdLa",
        "pdf_link": "https://openreview.net/pdf?id=xNaPs8bdLa",
        "keywords": "Diffusion Models, Conditional Generation",
        "abstract": "Diffusion models have achieved remarkable success in image generation, particularly with the various applications of classifier-free guidance conditional diffusion models. While many diffusion models perform well when controlling for particular aspect among style, character, and interaction, they struggle with fine-grained control due to dataset limitations and intricate model architecture design. This paper introduces a novel algorithm, Aggregation of Multi Diffusion Models (AMDM), which synthesizes features from multiple diffusion models into a specified model, enhancing its learned representations to activate specific features for fine-grained control. AMDM consists of two key components: spherical aggregation and manifold optimization. Spherical aggregation merges intermediate variables from different diffusion models with minimal manifold deviation, while manifold optimization refines these variables to align with the intermediate data manifold, enhancing sampling quality. Experimental results demonstrate that AMDM significantly improves fine-grained control without additional training or inference time, proving its effectiveness. Additionally, it reveals that diffusion models initially focus on features such as position, attributes, and style, with later stages improving generation quality and consistency. AMDM offers a new perspective for tackling the challenges of fine-grained conditional control generation in diffusion models: We can fully utilize existing conditional diffusion models that control specific aspects, or develop new ones, and then aggregate them using the AMDM algorithm. This eliminates the need for constructing complex datasets, designing intricate model architectures, and incurring high training costs. Code is available at:https://github.com/Hammour-steak/AMDM"
    },
    {
        "title": "MiniPLM: Knowledge Distillation for Pre-training Language Models",
        "link_suffix": "/forum?id=tJHDw8XfeC",
        "link": "https://openreview.net/forum?id=tJHDw8XfeC",
        "pdf_link": "https://openreview.net/pdf?id=tJHDw8XfeC",
        "keywords": "Language Models, Knowledge Distillation, Pre-Training",
        "abstract": "Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. \nWhile effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. \nExisting methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data.\nTo address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher LM's knowledge.\nFor efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs.\nFor flexibility, MiniPLM operates solely on the pre-training corpus, enabling KD across model families.\nFor effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge.\nWe apply MiniPLM to pre-train LMs with 200M, 500M, and 1.2B from scratch, using a 1.8B teacher LM. \nExtensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. \nThe benefit of MiniPLM extends to larger pre-training scales, evidenced by the extrapolation of the scaling curves.\nFurther analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. We will release our code, data, and models."
    },
    {
        "title": "Scalable Decentralized Learning with Teleportation",
        "link_suffix": "/forum?id=AvmBgiQxxp",
        "link": "https://openreview.net/forum?id=AvmBgiQxxp",
        "pdf_link": "https://openreview.net/pdf?id=AvmBgiQxxp",
        "keywords": "decentralized learning",
        "abstract": "Decentralized SGD can run with low communication costs, but its sparse communication characteristics deteriorate the convergence rate, especially when the number of nodes is large. In decentralized learning settings, communication is assumed to occur on only a given topology, while in many practical cases, the topology merely represents a preferred communication pattern, and connecting to arbitrary nodes is still possible. Previous studies have tried to alleviate the convergence rate degradation in these cases by designing topologies with large spectral gaps. However, the degradation is still significant when the number of nodes is substantial. In this work, we propose TELEPORTATION. TELEPORTATION activates only a subset of nodes, and the active nodes fetch the parameters from previous active nodes. Then, the active nodes update their parameters by SGD and perform gossip averaging on a relatively small topology comprising only the active nodes. We show that by activating only a proper number of nodes, TELEPORTATION can completely alleviate the convergence rate degradation. Furthermore, we propose an efficient hyperparameter-tuning method to search for the appropriate number of nodes to be activated. Experimentally, we showed that TELEPORTATION can train neural networks more stably and achieve higher accuracy than Decentralized SGD."
    },
    {
        "title": "What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?",
        "link_suffix": "/forum?id=BgYbk6ZmeX",
        "link": "https://openreview.net/forum?id=BgYbk6ZmeX",
        "pdf_link": "https://openreview.net/pdf?id=BgYbk6ZmeX",
        "keywords": "Transfer Learning, Diffusion Models, Visual Perception",
        "abstract": "Extensive pre-training with large data is indispensable for downstream geometry and semantic visual perception tasks. Thanks to large-scale text-to-image (T2I) pretraining, recent works show promising results by simply fine-tuning T2I diffusion models for dense perception tasks. However, several crucial design decisions in this process still lack comprehensive justification, encompassing the necessity of the multi-step stochastic diffusion mechanism, training strategy, inference ensemble strategy, and fine-tuning data quality. In this work, we conduct a thorough investigation into critical factors that affect transfer efficiency and performance when using diffusion priors. Our key findings are: 1) High-quality fine-tuning data is paramount for both semantic and geometry perception tasks. 2) The stochastic nature of diffusion models has a slightly negative impact on the deterministic perception tasks. 3) Apart from fine-tuning the diffusion model with only latent space supervision, task-specific image-level supervision is beneficial to enhance fine-grained details. These observations culminate in the development of GenPercept, an effective deterministic one-step fine-tuning paradigm tailed for dense visual perception tasks. Different from the previous multi-step methods, our paradigm has a much faster inference speed, and can be seamlessly integrated with customized perception decoders and loss functions for image-level supervision, which is critical to improving the fine-grained details of predictions. Comprehensive experiments on diverse dense visual perceptual tasks, including monocular depth estimation, surface normal estimation, image segmentation, and matting, are performed to demonstrate the remarkable adaptability and effectiveness of our proposed method."
    },
    {
        "title": "Learning from interval targets",
        "link_suffix": "/forum?id=uDWcAajewK",
        "link": "https://openreview.net/forum?id=uDWcAajewK",
        "pdf_link": "https://openreview.net/pdf?id=uDWcAajewK",
        "keywords": "Weak supervision, Partial-label learning, Learning from side information, Learning theory",
        "abstract": "We consider regression problems where the exact real-valued targets are not directly available; instead, supervision is provided in the form of intervals around the targets\u2014that is, only lower and upper bounds are known. Such a \"learning from interval targets\" setup arises in domains where labeling costs are high or there is inherent uncertainty in the target values. In these settings, traditional regression loss functions, which require exact target values, cannot be directly applied. To address this challenge, we propose two approaches: (i) modifying the regression loss function to be compatible with interval ground truths, and (ii) formulating a min-max problem where we minimize the typical regression loss with respect to the \"worst-case\" label within the interval. We provide theoretical guarantees for our methods, analyze their computational efficiency, and evaluate their practical performance on real-world datasets."
    },
    {
        "title": "TimeBridge: Non-Stationarity Matters for Long-term Time Series Forecasting",
        "link_suffix": "/forum?id=baSU1eVLwS",
        "link": "https://openreview.net/forum?id=baSU1eVLwS",
        "pdf_link": "https://openreview.net/pdf?id=baSU1eVLwS",
        "keywords": "Long-term time series forecasting, Non-stationarity, Dependency Modeling",
        "abstract": "Non-stationarity poses significant challenges for multivariate time series forecasting due to the inherent short-term fluctuations and long-term trends that can lead to spurious regressions or obscure essential long-term relationships. Most existing methods either eliminate or retain non-stationarity without adequately addressing its distinct impacts on short-term and long-term modeling. Eliminating non-stationarity is essential for avoiding spurious regressions and capturing local dependencies in short-term modeling, while preserving it is crucial for revealing long-term cointegration across variates. In this paper, we propose TimeBridge, a novel framework designed to bridge the gap between non-stationarity and dependency modeling in long-term time series forecasting. By segmenting input series into smaller patches, TimeBridge applies Integrated Attention to mitigate short-term non-stationarity and capture stable dependencies within each variate, while Cointegrated Attention preserves non-stationarity to model long-term cointegration across variates. Extensive experiments show that TimeBridge consistently achieves state-of-the-art performance in both short-term and long-term forecasting. Additionally, TimeBridge demonstrates exceptional performance in financial forecasting on the CSI 500 and S&P 500 indices, further validating its robustness and effectiveness. The code is available in the supplementary material."
    },
    {
        "title": "MSLC: Monte Carlo Tree Search Sampling Guided Local Construction for Solving Large-Scale Traveling Salesman Problem",
        "link_suffix": "/forum?id=s324bLSKui",
        "link": "https://openreview.net/forum?id=s324bLSKui",
        "pdf_link": "https://openreview.net/pdf?id=s324bLSKui",
        "keywords": "Combinatorial Optimization, Neural Combinatorial Optimization, Large scale problem",
        "abstract": "Neural solvers have achieved promising results in solving small-scale Travelling Salesman Problems (TSP), but inefficiencies arise when tackling larger instances. This paper proposes the MSLC (\\textbf{M}onte Carlo Tree Search \\textbf{S}ampling Guided \\textbf{L}ocal \\textbf{C}onstruction) framework, which innovatively integrates a predictive sampling module into the global coarse-grained selection module, MCTS, to achieve mutual integration with the fine-grained local construction module. This integration effectively balances coarse-grained exploration with fine-grained adjustment, thereby improving overall efficiency. This framework offers a novel way to combine autoregressive and non-autoregressive models. Experimental results demonstrate that MSLC effectively balances time and solution quality, outperforming state-of-the-art neural solvers. The performance gap of MSLC is reduced by at least 29.4% (resp. 34.7% or 28.5%) on TSP-500 (resp. TSP-1000 or TSP-10000), compared to the SOTA neural methods."
    },
    {
        "title": "Generalizable Monocular 3D Human Rendering via Direct Gaussian Attribute Diffusion",
        "link_suffix": "/forum?id=rWIrdAo2xC",
        "link": "https://openreview.net/forum?id=rWIrdAo2xC",
        "pdf_link": "https://openreview.net/pdf?id=rWIrdAo2xC",
        "keywords": "novel view synthesis, 3D human body reconstruction, 3D Gaussian splatting",
        "abstract": "This paper leverages 3D Gaussian Splatting to tackle the challenging task of generating novel views of humans from given single-view images. Existing methods typically adopt an indirect supervision manner, i.e., splat-based rasterization for differentiable rendering. However, the intricate coupling of various 3D Gaussian attributes complicates precise error backpropagation during optimization, often\nresulting in convergence to local optima. In contrast, we propose a novel direct paradigm to train a conditional diffusion model directly supervised by proxy-ground-truth 3D Gaussian attributes. Specifically, we propose a two-stage construction process to derive consistent and smoothly distributed proxy-ground-truth 3D Gaussian attributes. Subsequently, we train a point-based conditional diffusion model customized to learn the data distribution of these proxy attributes. The resulting diffusion model can generate the 3D Gaussian attributes for the input\nsingle-view image, which are further rendered into novel views. Extensive experimental results showcase the significant performance advancement of our method over state-of-the-art approaches. Source code will be made publicly available."
    },
    {
        "title": "Multi-Granularity Semantic Revision for Large Language Model Distillation",
        "link_suffix": "/forum?id=8wjWm5jr1w",
        "link": "https://openreview.net/forum?id=8wjWm5jr1w",
        "pdf_link": "https://openreview.net/pdf?id=8wjWm5jr1w",
        "keywords": "Knowledge Distillation, Model Compression",
        "abstract": "Knowledge distillation plays a key role in compressing the Large Language Models (LLMs), which boosts a small-size student model under large teacher models' guidance. However, existing LLM distillation methods overly rely on student-generated outputs, which may introduce generation errors and misguide the distillation process.  Moreover, the distillation loss functions introduced in previous works struggle to align the most informative part due to the complex distribution of LLMs' outputs. To address these problems, we propose a multi-granularity semantic revision method for LLM distillation.  At the sequence level, we propose a sequence correction and re-generation (SCRG) strategy. SCRG first calculates the semantic cognitive difference between the teacher and student to detect the error token, then corrects it with the teacher-generated one, and re-generates the sequence to reduce generation errors and enhance generation diversity. At the token level, we design a distribution adaptive clipping Kullback-Leibler (DAC-KL) loss as the distillation objective function. DAC-KL loss exploits a learnable sub-network to adaptively extract semantically dense areas from the teacher's output, avoiding the interference of redundant information in the distillation process. Finally, at the span level, we leverage the span priors of a sequence to compute the probability correlations within spans, and constrain the teacher and student's probability correlations to be consistent, further enhancing the transfer of semantic information. Extensive experiments across different model families with parameters ranging from 0.1B to 13B demonstrate the superiority of our method compared to existing methods."
    },
    {
        "title": "Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics",
        "link_suffix": "/forum?id=peNgxpbdxB",
        "link": "https://openreview.net/forum?id=peNgxpbdxB",
        "pdf_link": "https://openreview.net/pdf?id=peNgxpbdxB",
        "keywords": "Combinatorial Optimization, Diffusion Models, Statistical Physics",
        "abstract": "Learning to sample from complex unnormalized distributions over discrete domains emerged as a promising research direction with applications in statistical physics, variational inference, and combinatorial optimization. Recent work has demonstrated the potential of diffusion models in this domain. However, existing methods face limitations in memory scaling and thus the number of attainable diffusion steps since they require backpropagation through the entire generative process. To overcome these limitations we introduce two novel training methods for discrete diffusion samplers, one grounded in the policy gradient theorem and the other one leveraging Self-Normalized Neural Importance Sampling (SN-NIS). These methods yield memory-efficient training and achieve state-of-the-art results in unsupervised combinatorial optimization.\nNumerous scientific applications additionally require the ability of unbiased sampling. We introduce adaptations of SN-NIS and Neural Markov Chain Monte Carlo that enable for the first time the application of discrete diffusion models to this problem. We validate our methods on Ising model benchmarks and find that they outperform popular autoregressive approaches. Our work opens new avenues for applying diffusion models to a wide range of scientific applications in discrete domains that were hitherto restricted to exact likelihood models."
    },
    {
        "title": "Decomposition Polyhedra of Piecewise Linear Functions",
        "link_suffix": "/forum?id=vVCHWVBsLH",
        "link": "https://openreview.net/forum?id=vVCHWVBsLH",
        "pdf_link": "https://openreview.net/pdf?id=vVCHWVBsLH",
        "keywords": "Piecewise Linear Functions, Polyhedral Geometry, Minimal Convex Decompositions, Submodular Functions, Neural Networks",
        "abstract": "In this paper we contribute to the frequently studied question of how to decompose a continuous piecewise linear (CPWL) function into a difference of two convex CPWL functions. Every CPWL function has infinitely many such decompositions, but for applications in optimization and neural network theory, it is crucial to find decompositions with as few linear pieces as possible. This is a highly challenging problem, as we further demonstrate by disproving a recently proposed approach by Tran and Wang [Minimal representations of tropical rational functions. Algebraic Statistics, 15(1):27\u201359, 2024]. To make the problem more tractable, we propose to fix an underlying polyhedral complex determining the possible locus of nonlinearity. Under this assumption, we prove that the set of decompositions forms a polyhedron that arises as intersection of two translated cones. We prove that irreducible decompositions correspond to the bounded faces of this polyhedron and minimal solutions must be vertices. We then identify cases with a unique minimal decomposition, and illustrate how our insights have consequences in the theory of submodular functions. Finally, we improve upon previous constructions of neural networks for a given convex CPWL function and apply our framework to obtain results in the nonconvex case."
    },
    {
        "title": "Robust Heterogeneous Graph Neural Network Explainer with Graph Information Bottleneck",
        "link_suffix": "/forum?id=IMWYNVBHob",
        "link": "https://openreview.net/forum?id=IMWYNVBHob",
        "pdf_link": "https://openreview.net/pdf?id=IMWYNVBHob",
        "keywords": "Trustworthy, Robust, Graph Neural Network",
        "abstract": "Explaining the prediction process of Graph Neural Network (GNN) is crucial for enhancing network transparency. However, real-world networks are predominantly heterogeneous and often beset with noise. The presence of intricate relationships in heterogeneous graphs necessitates a consideration of semantics during the explanation process, while mitigating the impact of noise remains unexplored. For GNN explainers heavily reliant on graph structure and raw features, erroneous predictions may lead to misguided explanations under the influence of noise. To address these challenges, we propose a Robust Heterogeneous Graph Neural Network Explainer with Graph Information Bottleneck, named RHGIB. We theoretically analyze the power of different heterogeneous GNN architectures on the propagation of noise information and exploit denoising variational inference. Specifically, we infer the latent distributions of both graph structure and features to alleviate the influence of noise. Subsequently, we incorporate heterogeneous edge types into the generation process of explanatory subgraph and utilize Graph Information Bottleneck framework for optimization, allowing the Explainer to learn heterogeneous semantics while enhancing robustness. Extensive experiments on multiple real-world heterogeneous graph datasets demonstrate the superior performance of RHGIB compared to state-of-the-art baselines."
    },
    {
        "title": "Enhancing Optimizer Stability: Momentum Adaptation of NGN Step-size",
        "link_suffix": "/forum?id=CYa4FKjYM9",
        "link": "https://openreview.net/forum?id=CYa4FKjYM9",
        "pdf_link": "https://openreview.net/pdf?id=CYa4FKjYM9",
        "keywords": "Optimization, Adaptive Methods, Polyak stepsize, Machine Learning",
        "abstract": "Modern optimization algorithms that incorporate momentum and adaptive step-size offer improved performance in various challenging Deep Learning tasks. However, their effectiveness is often highly sensitive to the choice of hyper-parameters, especially the learning rate.  Tuning these parameters is often difficult, resource-intensive, and time-consuming. State-of-the-art optimization algorithms incorporating momentum and adaptive step size are the algorithms of choice in several challenging Deep Learning domains. However, their effectiveness is frequently dependent on selecting the right hyper-parameters, especially the learning rate. Therefore, recent efforts have been directed toward enhancing the stability of optimizers across a wide range of hyper-parameter choices (Schaipp et al., 2024). In this paper, we introduce an algorithm that matches the performance of state-of-the-art optimizers while improving stability through a novel adaptation of the NGN step-size method (Orvieto & Xiao, 2024). Specifically, we propose a momentum-based version (NGN-M) that attains the standard convergence rate of $\\mathcal{O}(1/\\sqrt{K})$ under common assumptions, without the need for interpolation condition or assumptions of bounded stochastic gradients or iterates, in contrast to previous approaches. Additionally, we empirically demonstrate that the combination of the NGN step-size with momentum results in high robustness while delivering performance that is comparable to or surpasses other state-of-the-art optimizers."
    },
    {
        "title": "Provably Accurate Shapley Value Estimation via Leverage Score Sampling",
        "link_suffix": "/forum?id=wg3rBImn3O",
        "link": "https://openreview.net/forum?id=wg3rBImn3O",
        "pdf_link": "https://openreview.net/pdf?id=wg3rBImn3O",
        "keywords": "Explainable AI, Active Regression, Shapley Values, Leverage Scores",
        "abstract": "Originally introduced in game theory, Shapley values have emerged as a central tool in explainable machine learning, where they are used to attribute model predictions to specific input features. However, computing Shapley values exactly is expensive: for a model with $n$ features, $O(2^n)$ model evaluations are necessary. To address this issue, approximation algorithms are widely used. One of the most popular is the Kernel SHAP algorithm, which is model agnostic and remarkably effective in practice. However, to the best of our knowledge, Kernel SHAP has no strong non-asymptotic complexity guarantees. We address this issue by introducingLeverage SHAP, a light-weight modification of Kernel SHAP that provides provably accurate Shapley value estimates with just $O(n\\log n)$ model evaluations. Our approach takes advantage of a connection between Shapley value estimation and agnostic active learning by employingleverage score sampling, a powerful regression tool. Beyond theoretical guarantees, we show that Leverage SHAP consistently outperforms even the highly optimized implementation of Kernel SHAP available in the ubiquitous SHAP library [Lundberg & Lee, 2017]."
    },
    {
        "title": "Towards Multiple Character Image Animation Through Enhancing Implicit Decoupling",
        "link_suffix": "/forum?id=aqlzXgXwWa",
        "link": "https://openreview.net/forum?id=aqlzXgXwWa",
        "pdf_link": "https://openreview.net/pdf?id=aqlzXgXwWa",
        "keywords": "character image animation, video generation, diffusion model",
        "abstract": "Controllable character image animation has a wide range of applications. Although existing studies have consistently improved performance, challenges persist in the field of character image animation, particularly concerning stability in complex backgrounds and tasks involving multiple characters. To address these challenges, we propose a novel multi-condition guided framework for character image animation, employing several well-designed input modules to enhance the implicit decoupling capability of the model. First, the optical flow guider calculates the background optical flow map as guidance information, which enables the model to implicitly learn to decouple the background motion into background constants and background momentum during training, and generate a stable background by setting zero background momentum during inference. Second, the depth order guider calculates the order map of the characters, which transforms the depth information into the positional information of multiple characters. This facilitates the implicit learning of decoupling different characters, especially in accurately separating the occluded body parts of multiple characters. Third, the reference pose map is input to enhance the ability to decouple character texture and pose information in the reference image. Furthermore, to fill the gap of fair evaluation of multi-character image animation in the community, we propose a new benchmark comprising approximately 4,000 frames. Extensive qualitative and quantitative evaluations demonstrate that our method excels in generating high-quality character animations, especially in scenarios of complex backgrounds and multiple characters."
    },
    {
        "title": "On the Byzantine-Resilience of Distillation-Based Federated Learning",
        "link_suffix": "/forum?id=of6EuHT7de",
        "link": "https://openreview.net/forum?id=of6EuHT7de",
        "pdf_link": "https://openreview.net/pdf?id=of6EuHT7de",
        "keywords": "Federated Learning, Knowledge Distillation, Byzantine FL",
        "abstract": "Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and instead communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process. Based on these insights, we introduce two new byzantine attacks and demonstrate their ability to break existing byzantine-resilient methods. Additionally, we propose a novel defence method which enhances the byzantine resilience of KD-based FL algorithms. Finally, we provide a general framework to obfuscate attacks, making them significantly harder to detect, thereby improving their effectiveness. Our findings serve as an important building block in the analysis of byzantine FL, contributing through the development of new attacks and new defence mechanisms, further advancing the robustness of KD-based FL algorithms."
    },
    {
        "title": "QPM: Discrete Optimization for Globally Interpretable Image Classification",
        "link_suffix": "/forum?id=GlAeL0I8LX",
        "link": "https://openreview.net/forum?id=GlAeL0I8LX",
        "pdf_link": "https://openreview.net/pdf?id=GlAeL0I8LX",
        "keywords": "explainable AI, Global Interpretability, Sparse Representations, Compactness, Local Interpretability, Image Classification, Discrete Optimization",
        "abstract": "Understanding the classifications of deep neural networks, e.g. used in safety-critical situations, is becoming increasingly important. While recent models can locally explain a single decision, to provide a faithful global explanation about an accurate model\u2019s general behavior is a more challenging open task. Towards that goal, we introduce the Quadratic Programming Enhanced Model (QPM), which learns globally interpretable class representations. QPM represents every class with a binary assignment of very few, typically 5, features, that are also assigned to other classes, ensuring easily comparable contrastive class representations. This compact binary assignment is found using discrete optimization based on predefined similarity measures and interpretability constraints. The resulting optimal assignment is used to fine-tune the diverse features, so that each of them becomes the shared general concept between the assigned classes. Extensive evaluations show that QPM delivers unprecedented global interpretability across small and large-scale datasets while setting the state of the art for the accuracy of interpretable models."
    },
    {
        "title": "DynaPrompt: Dynamic Test-Time Prompt Tuning",
        "link_suffix": "/forum?id=EFZEdHB3Mp",
        "link": "https://openreview.net/forum?id=EFZEdHB3Mp",
        "pdf_link": "https://openreview.net/pdf?id=EFZEdHB3Mp",
        "keywords": "Test-time prompt tuning; test-time adaptation; vision-language model; CLIP",
        "abstract": "Test-time prompt tuning enhances zero-shot generalization of vision-language models but tends to ignore the relatedness among test samples during inference. Online test-time prompt tuning provides a simple way to leverage the information in previous test samples, albeit with the risk of prompt collapse due to error accumulation. To enhance test-time prompt tuning, we propose DynaPrompt, short for dynamic test-time prompt tuning, exploiting relevant data distribution information while reducing error accumulation. Built on an online prompt buffer, DynaPrompt adaptively selects and optimizes the relevant prompts for each test sample during tuning. Specifically, we introduce a dynamic prompt selection strategy based on two metrics: prediction entropy and probability difference. For unseen test data information, we develop dynamic prompt appending, which allows the buffer to append new prompts and delete the inactive ones. By doing so, the prompts are optimized to exploit beneficial information on specific test data, while alleviating error accumulation. Experiments on fourteen datasets demonstrate the effectiveness of dynamic test-time prompt tuning."
    },
    {
        "title": "CoPruning: Exploring the Parameter-Gradient Nonlinear Correlation for Neural Network Pruning Using Copula Function",
        "link_suffix": "/forum?id=Yqqa9aNwB0",
        "link": "https://openreview.net/forum?id=Yqqa9aNwB0",
        "pdf_link": "https://openreview.net/pdf?id=Yqqa9aNwB0",
        "keywords": "Neural Network Pruning; Copula Function; Copula Entropy; Sparse Models; Joint Distribution Model;",
        "abstract": "The sheer size of modern neural networks necessitates pruning techniques to overcome the significant computational challenges posed by model serving. \nHowever, existing pruning techniques fail to capture the nonlinear correlation between parameters and gradient, which is crucial in the pruning process, thus leading to low accuracy under high sparsity.\nIn this work, we propose CoPruning, a new pruning framework, which uses a copula function based \njoint distribution model that precisely captures the intricate nonlinear correlation between parameters and gradient, enabling more insightful pruning decisions. \nAdditionally, we integrate a local optimization approach within CoPruning to better capture relative change in parameters within their local context, providing new metrics for achieving finer-grained optimization.\nExtensive experiments on various networks reveal CoPruning's comparable performance to state-of-the-art (SoTA) pruning algorithms. \nCoPruning outperforms the SoTA with 3.09%, 1.87%, and 2.19% higher accuracy on MLPNet, ResNet20, and ResNet50 at 0.98 sparsity, respectively, and 10.43% higher accuracy on MobileNetV1 at 0.9 sparsity on ImageNet."
    },
    {
        "title": "Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization",
        "link_suffix": "/forum?id=juxbsQEuTZ",
        "link": "https://openreview.net/forum?id=juxbsQEuTZ",
        "pdf_link": "https://openreview.net/pdf?id=juxbsQEuTZ",
        "keywords": "Language models, simplicity bias, random variations, OOD generalization",
        "abstract": "Neural networks often favor shortcut heuristics based on surface-level patterns. As one example, language models (LMs) can behave like n-gram models early in training. However, to correctly apply grammatical rules, LMs must instead rely on hierarchical syntactic representations rather than the surface-level heuristics derived from n-grams. In this work, we use cases studies of English grammar to explore how latent structures in training data can enable models to overcome their early capabilities and achieve complex generalization behaviors. We demonstrate that sentences with complex grammatical structure drive the model's inductive bias towards hierarchical representation. We then investigate how data composition can lead to inconsistent behavior across random seeds, finding that models stabilize in their out-of-distribution (OOD) behavior only when they commit to either a surface-level heuristic or a hierarchical rule. When the data contains a mix of simple and complex examples, potential rules compete, leading to unstable dynamics in training runs that fail to commit. We also identify an exception to the relationship between stability and generalization: Models which memorize patterns from homogeneous training data can stabilize in a memorization regime without learning either rule. While existing works have attributed similar generalization behavior to training objective and model architecture, our findings emphasize the critical role of training data in shaping generalization patterns and how competition between data subsets contributes to inconsistent generalization outcomes."
    },
    {
        "title": "Vision-Enhanced Time Series Forecasting by Decomposed Feature Extraction and Composed Reconstruction",
        "link_suffix": "/forum?id=DcG4YnbOT3",
        "link": "https://openreview.net/forum?id=DcG4YnbOT3",
        "pdf_link": "https://openreview.net/pdf?id=DcG4YnbOT3",
        "keywords": "Time Series Forecasting",
        "abstract": "Time series forecasting plays a crucial role in various domains, such as power and weather forecasting. In recent years, different types of models have achieved promising results in long-term time series forecasting. However, these models often produce predictions that lack consistency with the style of the input, resulting in reduced reliability and trust in the forecasts. To address this issue, we propose the Vision-Enhanced Time Series Forecasting by Decomposed Feature Extraction and Composed Reconstruction (VisiTER), which leverages the rich semantic information provided by the image modality to enhance the realism of the predictions. It consists of two main components: the Decomposed Time Series to Image Generation and the Composed Image to Time Series Generation. In the first component, the Decomposed Time Series Feature Extraction Model extracts periodic and trend information, which is then transformed into images using our proposed time series to vision transformation architecture. After converting the input time series into images, the resulting images are used as style features and concatenated with the previously extracted features. In the second component, we use our proposed TimeIR along with the previously obtained feature set to perform image reconstruction for the prediction part. Due to the rich information provided, the reconstructed images exhibit better consistency with the input images, which are then transformed back into time series. Extensive experiments on seven real-world datasets demonstrate that VisiTER achieves state-of-the-art prediction performance on both traditional metrics and new metrics."
    },
    {
        "title": "Improved Sample Complexity for  Global Convergence of Actor-Critic Algorithms",
        "link_suffix": "/forum?id=A1WwYw5u8m",
        "link": "https://openreview.net/forum?id=A1WwYw5u8m",
        "pdf_link": "https://openreview.net/pdf?id=A1WwYw5u8m",
        "keywords": "Policy Gradient, Actor-Critic Algorithm, Global Convergence, Sample Complexity",
        "abstract": "In this paper, we establish the global convergence of the actor-critic algorithm with a significantly improved sample complexity of ( O(\\epsilon^{-3}) ), advancing beyond the existing local convergence results. Previous works provide local convergence guarantees with a sample complexity of ( O(\\epsilon^{-2}) ) for bounding the squared gradient of the return, which translates to a global sample complexity of ( O(\\epsilon^{-4}) ) using the gradient domination lemma. In contrast to traditional methods that employ decreasing step sizes for both the actor and critic, we demonstrate that a constant step size for the critic is sufficient to ensure convergence. This key insight reveals that using a decreasing step size for the actor alone is sufficient to handle the noise for both the actor and critic. Our findings provide theoretical support for the practical success of many algorithms that rely on constant step sizes."
    }
]