[
    {
        "title": "Safety-Prioritizing Curricula for Constrained Reinforcement Learning",
        "link_suffix": "/forum?id=f3QR9TEERH",
        "link": "https://openreview.net/forum?id=f3QR9TEERH",
        "pdf_link": "https://openreview.net/pdf?id=f3QR9TEERH",
        "keywords": "curriculum learning, constrained reinforcement learning",
        "abstract": "Curriculum learning aims to accelerate reinforcement learning (RL) by generating curricula, i.e., sequences of tasks of increasing difficulty. \nAlthough existing curriculum generation approaches provide benefits in sample efficiency, they overlook safety-critical settings where an RL agent must adhere to safety constraints.\nThus, these approaches may generate tasks that cause RL agents to violate safety constraints during training and behave suboptimally after. \nWe develop a safe curriculum generation approach (SCG) that aligns the objectives of constrained RL and curriculum learning: improving safety during training and boosting sample efficiency.\nSCG generates sequences of tasks where the RL agent can be safe and performant by initially generating tasks with minimum safety violations over high-reward ones.\nWe empirically show that compared to the state-of-the-art curriculum learning approaches and their naively modified safe versions, SCG achieves optimal performance and the lowest amount of constraint violations during training."
    },
    {
        "title": "In vivo cell-type and brain region classification via multimodal contrastive learning",
        "link_suffix": "/forum?id=10JOlFIPjt",
        "link": "https://openreview.net/forum?id=10JOlFIPjt",
        "pdf_link": "https://openreview.net/pdf?id=10JOlFIPjt",
        "keywords": "contrastive learning, electrophysiology, extracellular, multimodal, neuroscience, cell type, brain region, Neuropixels, deep learning",
        "abstract": "Current electrophysiological approaches can track the activity of many neurons, yet it is usually unknown which cell-types or brain areas are being recorded without further molecular or histological analysis. Developing accurate and scalable algorithms for identifying the cell-type and brain region of recorded neurons is thus crucial for improving our understanding of neural computation. In this work, we develop a multimodal contrastive learning approach for neural data that can be fine-tuned for different downstream tasks, including inference of cell-type and brain location. We utilize multimodal contrastive learning to jointly embed the activity autocorrelations and extracellular waveforms of individual neurons. We demonstrate that our embedding approach, Neuronal Embeddings via MultimOdal Contrastive Learning (NEMO), paired with supervised fine-tuning, achieves state-of-the-art cell-type classification for an opto-tagged visual cortex dataset and for brain region classification of the public International Brain Laboratory brain-wide map dataset. Our method represents a promising step towards accurate cell-type and brain region classification from electrophysiological recordings."
    },
    {
        "title": "Backdoor in Seconds: Unlocking Vulnerabilities in Large Pre-trained Models via Model Editing",
        "link_suffix": "/forum?id=ZyPRwskBli",
        "link": "https://openreview.net/forum?id=ZyPRwskBli",
        "pdf_link": "https://openreview.net/pdf?id=ZyPRwskBli",
        "keywords": "Backdoor attack, Large pre-trained model, Model editing",
        "abstract": "Large pre-trained models have achieved notable success across a range of downstream tasks. However, recent research shows that a type of adversarial attack ($\\textit{i.e.,}$ backdoor attack) can manipulate the behavior of machine learning models through contaminating their training dataset, posing significant threat in the real-world application of large pre-trained model, especially for those customized models. Therefore, addressing the unique challenges for exploring vulnerability of pre-trained models is of paramount importance. Through empirical studies on the capability for performing backdoor attack in large pre-trained models ($\\textit{e.g.,}$ ViT), we find the following unique challenges of attacking large pre-trained models: 1) the inability to manipulate or even access large training datasets, and 2) the substantial computational resources required for training or fine-tuning these models. To address these challenges, we establish new standards for an effective and feasible backdoor attack in the context of large pre-trained models. In line with these standards, we introduce our EDT model, an \\textbf{E}fficient, \\textbf{D}ata-free, \\textbf{T}raining-free backdoor attack method. Inspired by model editing techniques, EDT injects an editing-based lightweight codebook into the backdoor of large pre-trained models, which replaces the embedding of the poisoned image with the target image without poisoning the training dataset or training the victim model. Our experiments, conducted across various pre-trained models such as ViT, CLIP, BLIP, and stable diffusion, and on downstream tasks including image classification, image captioning, and image generation, demonstrate the effectiveness of our method. Our code is available in the supplementary material."
    },
    {
        "title": "Inference, Fast and Slow: Reinterpreting VAEs for OOD Detection",
        "link_suffix": "/forum?id=SabhfFUfA1",
        "link": "https://openreview.net/forum?id=SabhfFUfA1",
        "pdf_link": "https://openreview.net/pdf?id=SabhfFUfA1",
        "keywords": "unsupervised, outlier detection, ood, out-of-distribution, anomaly detection, variational autoencoder, VAE",
        "abstract": "lthough likelihood-based methods are theoretically appealing, deep generative models (DGMs) often produce unreliable likelihood estimates in practice, particu larly for out-of-distribution (OOD) detection. We reinterpret variational autoen coders (VAEs) through the lens of fast and slow weights. Our approach is guided by the proposed Likelihood Path (LPath) Principle, which extends the classical likelihood principle. A critical decision in our method is the selection of statistics for classical density estimation algorithms. The sweet spot should contain just enough information thatâ€™s sufficient for OOD detection but not too much to suffer from the curse of dimensionality. Our LPath principle achieves this by selecting the sufficient statistics that form the \"path\" toward the likelihood. We demonstrate that this likelihood path leads to SOTA OOD detection performance, even when the likelihood itself is unreliable."
    },
    {
        "title": "Structured World Models From Low-Level Observations",
        "link_suffix": "/forum?id=B7cZvTQsUN",
        "link": "https://openreview.net/forum?id=B7cZvTQsUN",
        "pdf_link": "https://openreview.net/pdf?id=B7cZvTQsUN",
        "keywords": "world models, finite state machines, structure learning",
        "abstract": "We present Structured World Modeling From Low-Level Observations (``SWMPO''), a framework for the unsupervised learning of neural Finite State Machines (FSM) that capture environment structure. Traditional unsupervised world modeling methods for policy optimization rely on unstructured representations, such as neural networks, which do not explicitly represent high-level patterns within the system (e.g., \\emph{walking} vs \\emph{swimming}). In contrast, SWMPO explicitly models the environment as an FSM, where each state represents a region of the environment's state space with distinct dynamics, exposing the structure of the environment to downstream tasks such as policy optimization. Prior works that synthesize FSMs for this purpose have been limited to discrete spaces, not continuous, high-dimensional spaces.\nOur FSM synthesis algorithm operates in an unsupervised manner, leveraging low-level features from unprocessed, non-visual data, making it adaptable across various domains. \nWe demonstrate the advantages of SWMPO by benchmarking its environment modeling capabilities in different simulated environments."
    },
    {
        "title": "Generalized Behavior Learning from Diverse Demonstrations",
        "link_suffix": "/forum?id=Q7EjHroO1w",
        "link": "https://openreview.net/forum?id=Q7EjHroO1w",
        "pdf_link": "https://openreview.net/pdf?id=Q7EjHroO1w",
        "keywords": "Behavior Discovery, Demonstrator Heterogeneity",
        "abstract": "Diverse behavior policies are especially valuable in domains requiring quick test-time adaptation or personalized human-robot interaction. Human demonstrations provide rich information regarding task objectives and individual preferences, which can be used to characterize useful diversity and learn diverse performant policies. However, we show that prior work that builds naive representations of demonstration heterogeneity fails in generating successful novel behaviors that generalize over preferences.\nWe propose Guided Strategy Discovery (GSD), which introduces a novel diversity formulation based on a learned task-relevance measure that prioritizes behaviors exploring modeled latent factors.\nWe empirically validate across three continuous control benchmarks for generalizing to in-distribution (interpolation) and out-of-distribution (extrapolation) preferences that GSD outperforms baselines in novel behavior discovery by $\\sim$21%.\nFinally, we demonstrate that GSD can generalize striking behaviors for table tennis in a virtual testbed while leveraging human demonstrations collected in the real world."
    },
    {
        "title": "Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL",
        "link_suffix": "/forum?id=hxUMQ4fic3",
        "link": "https://openreview.net/forum?id=hxUMQ4fic3",
        "pdf_link": "https://openreview.net/pdf?id=hxUMQ4fic3",
        "keywords": "neural stochastic differential equations, offline reinforcement learning, physics-informed machine learning",
        "abstract": "Offline model-based reinforcement learning (RL) offers a principled approach to using a learned dynamics model as a simulator to optimize a control policy. \nDespite the near-optimal performance of existing approaches on benchmarks with high-quality datasets, most struggle on datasets with low state-action space coverage or suboptimal demonstrations.\nWe develop a novel offline model-based RL approach that particularly shines in low-quality data regimes while maintaining competitive performance on high-quality datasets.\nNeural Stochastic Differential Equations for Uncertainty-aware, Offline RL (NUNO) learns a dynamics model as neural stochastic differential equations (SDE), \nwhere its drift term can leverage prior physics knowledge as inductive bias.\nIn parallel, its diffusion term provides distance-aware estimates of model uncertainty by matching the dynamics' underlying stochasticity near the training data regime while providing high but bounded estimates beyond it.\nTo address the so-called model exploitation problem in offline model-based RL, NUNO builds on existing studies by penalizing and adaptively truncating neural SDE's rollouts according to uncertainty estimates.\nOur empirical results in D4RL and NeoRL MuJoCo benchmarks evidence that NUNO outperforms state-of-the-art methods in low-quality datasets by up to 93% while matching or surpassing their performance by up to 55% in some high-quality counterparts."
    },
    {
        "title": "Lookers-On See Most of the Game: An External Insight-Guided Method for Enhancing Uncertainty Estimation",
        "link_suffix": "/forum?id=miIE56qM10",
        "link": "https://openreview.net/forum?id=miIE56qM10",
        "pdf_link": "https://openreview.net/pdf?id=miIE56qM10",
        "keywords": "Large Language Models, Uncertainty Estimation, Trusty AI",
        "abstract": "Large Language Models (LLMs) have gained increasing attention for their impressive capabilities, alongside concerns about the reliability arising from their potential to generate hallucinations and factual inaccuracies. Uncertainty estimation for LLMs aims to quantify the uncertainty of model outputs, where high uncertainty scores indicate potential errors, signaling the need for rejection or further evaluation. However, existing methods often limited by inherent biases of LLMs like over-confidence and under-confidence. In this paper, we propose an external insight-driven correction method for refining uncertainty estimation. This method integrates uncertainty scores derived from a lightweight model trained on global information with those from existing uncertainty estimation approaches, providing a more robust solution. We present comprehensive experimental results that demonstrate the effectiveness and generalizability of our method across various models, datasets, and consistently surpassing all baselines."
    },
    {
        "title": "Improving Zero-Shot Generalization of Instruction Tuning by Data Arrangement",
        "link_suffix": "/forum?id=Y2AH0wC6C9",
        "link": "https://openreview.net/forum?id=Y2AH0wC6C9",
        "pdf_link": "https://openreview.net/pdf?id=Y2AH0wC6C9",
        "keywords": "zero-shot, instruction tuning, large language models",
        "abstract": "Understanding alignment techniques begins with comprehending zero-shot generalization brought by instruction tuning, but little of the mechanism has been understood. Existing work has largely been confined to the task level, without considering that tasks are artificially defined and, to LLMs, merely consist of tokens and representations. To bridge this gap, we investigate zero-shot generalization from the perspective of the data itself. We first demonstrate that zero-shot generalization happens very early during instruction tuning, with loss serving as a stable indicator. Next, we investigate the facilitation of zero-shot generalization by data arrangement through similarity and granularity perspectives, confirming that encountering highly similar and fine-grained training data earlier during instruction tuning, without the constraints of defined ``tasks'', enables better generalization. Finally, we propose a more grounded training data arrangement method, Test-centric Multi-turn Arrangement, and show its effectiveness in promoting continual learning and further loss reduction. For the first time, we show that zero-shot generalization during instruction tuning is a form of similarity-based generalization between training and test data at the instance level. We hope our analysis will advance the understanding of zero-shot generalization during instruction tuning and contribute to the development of more aligned LLMs."
    },
    {
        "title": "MAP: Multi-Human-Value Alignment Palette",
        "link_suffix": "/forum?id=NN6QHwgRrQ",
        "link": "https://openreview.net/forum?id=NN6QHwgRrQ",
        "pdf_link": "https://openreview.net/pdf?id=NN6QHwgRrQ",
        "keywords": "Human value alignment, Generative model",
        "abstract": "Ensuring that generative AI systems align with human values is essential but challenging, especially when considering multiple human values and their potential trade-offs. Since human values can be personalized and dynamically change over time, the desirable levels of value alignment vary across different ethnic groups, industry sectors, and user cohorts. Within existing frameworks, it is hard to define human values and align AI systems accordingly across different directions simultaneously, such as harmlessness, helpfulness, and positiveness. To address this, we develop a novel, first-principle approach called Multi-Human-Value Alignment Palette (MAP), which navigates the alignment across multiple human values in a structured and reliable way. MAP formulates the alignment problem as an optimization task with user-defined constraints, which define human value targets. It can be efficiently solved via a primal-dual approach, which determines whether a user-defined alignment target is achievable and how to achieve it. We conduct a detailed theoretical analysis of MAP by quantifying the trade-offs between values, the sensitivity to constraints, the fundamental connection between multi-value alignment and sequential alignment, and proving that linear weighted rewards are sufficient for multi-value alignment. Extensive experiments demonstrate MAP's ability to align multiple values in a principled manner while delivering strong empirical performance across various tasks."
    },
    {
        "title": "Towards Optimal Multi-draft Speculative Decoding",
        "link_suffix": "/forum?id=9KxnxWOBA5",
        "link": "https://openreview.net/forum?id=9KxnxWOBA5",
        "pdf_link": "https://openreview.net/pdf?id=9KxnxWOBA5",
        "keywords": "speculative sampling",
        "abstract": "Large Language Models (LLMs) have become an indispensable part of natural language processing tasks. However, autoregressive sampling has become an efficiency bottleneck. Multi-Draft Speculative Decoding (MDSD) is a recent approach where, when generating each token, a small draft model generates multiple drafts, and the target LLM verifies them in parallel, ensuring that the final output conforms to the target model distribution. The two main design choices in MDSD are the draft sampling method and the verification algorithm. For a fixed draft sampling method, the optimal acceptance rate is a solution to an optimal transport problem, but the complexity of this problem makes it difficult to solve for the optimal acceptance rate and measure the gap between existing verification algorithms and the theoretical upper bound. This paper discusses the dual of the optimal transport problem, providing a way to efficiently compute the optimal acceptance rate. For the first time, we measure the theoretical upper bound of MDSD efficiency for vocabulary sizes in the thousands and quantify the gap between existing verification algorithms and this bound. We also compare different draft sampling methods based on their optimal acceptance rates. Our results show that the draft sampling method strongly influences the optimal acceptance rate, with sampling without replacement outperforming sampling with replacement. Additionally, existing verification algorithms do not reach the theoretical upper bound for both without replacement and with replacement sampling. Our findings suggest that carefully designed draft sampling methods can potentially improve the optimal acceptance rate and enable the development of verification algorithms that closely match the theoretical upper bound."
    },
    {
        "title": "Spatial Speaker ID: Joint Spatial and Semantic Learning for Multi-Microphone Speaker Identification on Short Far-Field Utterances",
        "link_suffix": "/forum?id=W4yLHZGqdp",
        "link": "https://openreview.net/forum?id=W4yLHZGqdp",
        "pdf_link": "https://openreview.net/pdf?id=W4yLHZGqdp",
        "keywords": "Representation learning, speaker identification, multi-channel audio",
        "abstract": "Speaker identification is the task of identifying a person who is currently talking by analysing microphone signals.\nTypical automatic speaker identification systems use a single microphone and require complete utterances of 10-30 seconds in length to accurately identify a person from an enrollment set. We introduce the related problem of detecting which person is talking among several people in a room when the utterances are very short, e.g., a single word, or a short laugh. Since utterance lengths are too short for conventional methods, we take inspiration from the way humans solve this problems - using two ears and a joint understanding of both semantic and spatial context. To solve this problem, we propose Spatial Speaker ID, which uses banded covariance features derived from multi-microphone input along with conventional banded power to identify talkers based on both the semantic characteristics of a sound and the spatial location of a sound. The internal representation learnt in Spatial Speaker ID jointly contains both spatial and voice characteristic information and is learnt contrastively, whereby two utterances that come from the same talker in the same location are required to have similar embeddings. We learn a binary classification downstream task that determines if two sets of embeddings come from the same talker in the same location. Using this binary classifier, we compare multiple ways of presenting the microphone covariance features to the upstream models. We show the importance of spatial information for identifying talkers on short utterances with interfering noise."
    },
    {
        "title": "Leveraging Set Assumption for Membership Inference in Language Models",
        "link_suffix": "/forum?id=HozsY9Gdcl",
        "link": "https://openreview.net/forum?id=HozsY9Gdcl",
        "pdf_link": "https://openreview.net/pdf?id=HozsY9Gdcl",
        "keywords": "Large language models, membership Inference, pretraining data",
        "abstract": "Membership Inference (MI) refers to the task of determining whether or not a document is included in the training data of a given model. MI provides an effective post-training alternative for analyzing training datasets when the access to them is restricted, including studying the impact of data choices on downstream performance, detecting copyrighted content in the training sets, and checking for evaluation set contamination. However, black-boxed Language Models (LMs) only providing the loss for the document may not provide a reliable signal for determining memberships. In this work, we leverage the insight that documents sharing certain attributes (e.g., time of creation) are all expected to be in a training set or none of them is, and develop methods that aggregate membership predictions over these documents. We apply our set assumption on five different domains (e.g., Wikipedia, Arxiv), and find that our method enhances prior MI methods by 0.14 in AUROC on average. We further analyze the impact of different language model sizes, training data deduplication, and methods of aggregating membership predictions over sets and find that our method is more effective on undeduplicated and larger models with more documents available in each set and longer sequence sampled for each document, and show our methodâ€™s robustness against noises in the set assumption under practical settings."
    },
    {
        "title": "Straightness of Rectified Flow: A Theoretical Insight into Wasserstein Convergence",
        "link_suffix": "/forum?id=byofn4HN8F",
        "link": "https://openreview.net/forum?id=byofn4HN8F",
        "pdf_link": "https://openreview.net/pdf?id=byofn4HN8F",
        "keywords": "generative model, optimal transport, rectified flow, wasserstein distance",
        "abstract": "Diffusion models have emerged as a powerful tool for image generation and denoising. Typically, generative models learn a trajectory between the starting noise distribution and the target data distribution. Recently Liu et al. (2023b) designed a novel alternative generative model Rectified Flow (RF), which aims to learn straight flow trajectories from noise to data using a sequence of convex optimization problems with close ties to optimal transport. If the trajectory is curved, one must use many Euler discretization steps or novel strategies, such as exponential integrators, to achieve a satisfactory generation quality. In contrast, RF has been shown to theoretically straighten the trajectory through successive rectifications, reducing the number of function evaluations (NFEs) while sampling. It has also been shown empirically that RF may improve the straightness in two rectifications if one can solve the underlying optimization problem within a sufficiently small error. In this paper, we make two key theoretical contributions: 1) we provide the first theoretical analysis of the Wasserstein distance between the sampling distribution of RF and the target distribution. Our error rate is characterized by the number of discretization steps and a new formulation of straightness stronger than that in the original work. 2) In line with the previous empirical findings, we show that, for a rectified flow from a Gaussian to a mixture of two Gaussians, two rectifications are sufficient to achieve a straight flow. Additionally, we also present empirical results on both simulated and real datasets to validate our theoretical findings"
    },
    {
        "title": "Watermark Smoothing Attacks against Language Models",
        "link_suffix": "/forum?id=1AYrzmDK4V",
        "link": "https://openreview.net/forum?id=1AYrzmDK4V",
        "pdf_link": "https://openreview.net/pdf?id=1AYrzmDK4V",
        "keywords": "LLM Watermark",
        "abstract": "Statistical watermarking is a technique used to embed a hidden signal in the probability distribution of text generated by large language models (LLMs), enabling the attribution of the text to the originating model. We introduce the smoothing attack and show that existing statistical watermarking methods are not robust against minor modifications of text. In particular, with the help of a weaker language model, an adversary can smooth out the distribution perturbation caused by watermarks. The resulting generated text achieves comparable quality to the original (unwatermarked) model while bypassing the watermark detector. Our attack reveals a fundamental limitation of a wide range of watermarking techniques."
    },
    {
        "title": "Understanding Contrastive Learning through Variational Analysis and Neural Network Optimization Perspectives",
        "link_suffix": "/forum?id=qjoDJjVZxB",
        "link": "https://openreview.net/forum?id=qjoDJjVZxB",
        "pdf_link": "https://openreview.net/pdf?id=qjoDJjVZxB",
        "keywords": "contrastive learning, discriminative, neural network optimization, variational analysis, gradient flows",
        "abstract": "The SimCLR method for contrastive learning of invariant visual representations has become extensively used in supervised, semi-supervised, and unsupervised settings, due to its ability to uncover patterns and structures in image data that are not directly present in the pixel representations. However, the reason for this success is not well-explained, since it is not guaranteed by invariance alone. In this paper, we conduct a mathematical analysis of the SimCLR method with the goal of better understanding the geometric properties of the learned latent distribution. Our findings reveal two things: (1) the SimCLR loss alone is not sufficient to select a \"good\" minimizer --- there are minimizers that give trivial latent distributions, even when the original data is highly clustered --- and (2) in order to understand the success of contrastive learning methods like SimCLR, it is necessary to analyze the neural network training dynamics induced by minimizing a contrastive learning loss. Our preliminary analysis for a one-hidden layer neural network shows that clustering structure can present itself for a substantial period of time during training, even if it eventually converges to a trivial minimizer.   To substantiate our theoretical insights, we present numerical results that confirm our theoretical predictions."
    },
    {
        "title": "CulturalBench: a Robust, Diverse and Challenging Benchmark on Measuring (the Lack of) Cultural Knowledge of LLMs",
        "link_suffix": "/forum?id=n1X2n7MJ8L",
        "link": "https://openreview.net/forum?id=n1X2n7MJ8L",
        "pdf_link": "https://openreview.net/pdf?id=n1X2n7MJ8L",
        "keywords": "cultural knowledge evaluation, cultural reasoning, large language models",
        "abstract": "To make large language models (LLMs) more helpful across diverse cultures, it is essential to have effective cultural knowledge benchmarks to measure and track our progress. Effective benchmarks need to be robust, diverse, and challenging. We introduce CulturalBench: a set of 1,227 human-written and human-verified questions for effectively assessing LLMs' cultural knowledge, covering 45 global regions including the underrepresented ones like Bangladesh, Zimbabwe, and Peru. Questions - each verified by five independent annotators - span 17 diverse topics ranging from food preferences to greeting etiquettes. We evaluate models on two setups: CulturalBench-Easy and CulturalBench-Hard which share the same questions but asked differently. We find that LLMs are sensitive to such difference in setups (e.g., GPT-4o with 27.3% difference). Compared to human performance (92.6% accuracy), CulturalBench-Hard is more challenging for frontier LLMs with the best performing model (GPT-4o) at only 61.5% and the worst (Llama3-8b) at 21.4%. Moreover, we find that LLMs often struggle with tricky questions that have multiple correct answers (e.g., What utensils do the Chinese usually use?), revealing a tendency to converge to a single answer. Our results also indicate that OpenAI GPT-4o substantially outperform other proprietary and open source models in questions related to all but one region (Oceania). Nonetheless, all models consistently underperform on questions related to South America and the Middle East."
    },
    {
        "title": "Declarative characterizations of direct preference alignment algorithms",
        "link_suffix": "/forum?id=7UKHNQIErp",
        "link": "https://openreview.net/forum?id=7UKHNQIErp",
        "pdf_link": "https://openreview.net/pdf?id=7UKHNQIErp",
        "keywords": "neuro-symbolic modeling, logic, preference learning, RLHF",
        "abstract": "Recent direct preference alignment algorithms (DPA), such as DPO, have shown great promise in aligning large language models to human preferences. While this has motivated the development on many new variants of the original DPO loss, understanding the the differences between these recent proposals, as well as developing new DPA loss functions, remains a formidable challenge given the lack of a technical and conceptual framework for reasoning about the underlying semantics of these algorithms. In this paper, we attempt to remedy this by formalizing DPA losses in terms of discrete reasoning problems. Specifically, we ask: Given an existing DPA loss, can we systematically derive a symbolic expression that characterizes its semantics? We show how this formal view of preference learning sheds new light on both the size and structure of DPA loss landscape, making it possible to not only rigorously characterize the relationships between recent proposals but to derive new loss functions from first principles. We also couple our formal findings with empirical results on a hitherto untested class of single model preference loss functions. Our experiments reveal interesting connections between symbolic constraint complexity and the empirical success and training dynamics of the corresponding losses, insights we believe can give useful guidance to AI practitioners working on AI alignment."
    },
    {
        "title": "Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images",
        "link_suffix": "/forum?id=FtjLUHyZAO",
        "link": "https://openreview.net/forum?id=FtjLUHyZAO",
        "pdf_link": "https://openreview.net/pdf?id=FtjLUHyZAO",
        "keywords": "Gene Expression Prediction, Diffusion Model, Spatial Transcriptomics, H&E",
        "abstract": "Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and eosin (H&E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful experimental technique that provides new opportunities to understand cancer mechanisms at a fine-grained molecular level, which is critical for uncovering new approaches for disease diagnosis and treatments. Here, we present $\\textbf{Stem}$ ($\\underline{\\textbf{S}}$pa$\\underline{\\textbf{T}}$ially resolved gene $\\underline{\\textbf{E}}$xpression inference with diffusion $\\underline{\\textbf{M}}$odel), a novel computational tool that leverages a conditional diffusion generative model to enable in silico gene expression inference from H&E stained images. Through better capturing the inherent stochasticity and heterogeneity in ST data, $\\textbf{Stem}$ achieves state-of-the-art performance on spatial gene expression prediction and generates biologically meaningful gene profiles for new H&E stained images at test time. We evaluate the proposed algorithm on datasets with various tissue sources and sequencing platforms, where it demonstrates clear improvement over existing approaches. $\\textbf{Stem}$ generates high-fidelity gene expression predictions that share similar gene variation levels as ground truth data, suggesting that our method preserves the underlying biological heterogeneity. Our proposed pipeline opens up the possibility of analyzing existing, easily accessible H&E stained histology images from a genomics point of view without physically performing gene expression profiling and empowers potential biological discovery from H&E stained histology images."
    },
    {
        "title": "Diffusion State-Guided Projected Gradient for Inverse Problems",
        "link_suffix": "/forum?id=kRBQwlkFSP",
        "link": "https://openreview.net/forum?id=kRBQwlkFSP",
        "pdf_link": "https://openreview.net/pdf?id=kRBQwlkFSP",
        "keywords": "Diffusion models, Inverse problems, Robustness, Subspace, Projection, Box inpainting, Phase retrieval",
        "abstract": "Recent advancements in diffusion models have been effective in learning data priors for solving inverse problems. They leverage diffusion sampling steps for inducing a data prior while using a measurement guidance gradient at each step to impose data consistency. For general inverse problems, approximations are needed when an unconditionally trained diffusion model is used since the measurement likelihood is intractable, leading to inaccurate posterior sampling. In other words, due to their approximations, these methods fail to preserve the generation process on the data manifold defined by the diffusion prior, leading to artifacts in applications such as image restoration. To enhance the performance and robustness of diffusion models in solving inverse problems, we propose Diffusion State-Guided Projected Gradient (DiffStateGrad), which projects the measurement gradient onto a subspace that is a low-rank approximation of an intermediate state of the diffusion process. DiffStateGrad, as a module, can be added to a wide range of diffusion-based inverse solvers to improve the preservation of the diffusion process on the prior manifold and filter out artifact-inducing components. We highlight that DiffStateGrad improves the robustness of diffusion models in terms of the choice of measurement guidance step size and noise while improving the worst-case performance. Finally, we demonstrate that DiffStateGrad improves upon the state-of-the-art on linear and nonlinear image restoration inverse problems."
    },
    {
        "title": "Variational Neuro-Symbolic Generative Temporal Point Process",
        "link_suffix": "/forum?id=KQJiC44aSG",
        "link": "https://openreview.net/forum?id=KQJiC44aSG",
        "pdf_link": "https://openreview.net/pdf?id=KQJiC44aSG",
        "keywords": "temporal point process, neuro-symbolic, generative model",
        "abstract": "Temporal point processes (TPPs) are a powerful framework for modeling event sequences with irregular timestamps, such as those commonly found in electronic health records (EHR), which often involve high-dimensional and diverse event types. However, building generative models for such complex datasets comes with several challenges, including addressing sample inefficiency, accurately capturing intricate event patterns, and producing outputs that are both trustworthy and interpretable. In this paper, we present a neuro-symbolic generative model for TPPs based on the Variational Autoencoder (VAE) framework. Our model incorporates a neural-symbolic reasoning layer into the latent space, allowing it to integrate interpretable, logic-based constraints and perform logical reasoning over learned representations. This integration enhances the interpretability of the latent space by embedding logic rules directly into the generative process, enabling structured reasoning and improved decision-making based on underlying data patterns. We validate our model on an ICU EHR dataset, demonstrating its effectiveness in capturing complex event dynamics with irregular timestamps. In addition to improving sample efficiency and accuracy, our model supports the secure and interpretable generation of synthetic event data, making it a valuable tool for healthcare applications where reliability and trustworthiness are critical."
    },
    {
        "title": "MADGEN - Mass-Spec attends to De Novo Molecular generation",
        "link_suffix": "/forum?id=78tc3EiUrN",
        "link": "https://openreview.net/forum?id=78tc3EiUrN",
        "pdf_link": "https://openreview.net/pdf?id=78tc3EiUrN",
        "keywords": "AI4Science, Biology Discovery, Metabolomics, MS/MS spectra",
        "abstract": "The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases.  Currently, the vast majority of spectral measurements remain in the \"dark chemical space\" without structural annotations.  To improve annotation, we propose MADGEN (Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method for de novo molecular structure generation guided by mass spectrometry data. MADGEN operates in two stages: scaffold retrieval and spectra-conditioned molecular generation starting with the scaffold. In the first stage, given an MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ contrastive learning to align mass spectra with candidate molecular scaffolds. In the second stage, starting from the retrieved scaffold, we employ the MS/MS spectrum to guide an attention-based generative model to generate the final molecule. Our approach constrains the molecular generation search space, reducing its complexity and improving generation accuracy. We evaluate MADGEN on three datasets (NIST23, CANOPUS, and MassSpecGym) and  evaluate MADGEN's performance with a predictive scaffold retriever and with an oracle retriever. We demonstrate the effectiveness of using  attention to integrate spectral information throughout the generation process to achieve strong results with the oracle retriever."
    },
    {
        "title": "Interpreting neural networks depends on the level of abstraction: Revisiting modular addition",
        "link_suffix": "/forum?id=Y71rktd4oT",
        "link": "https://openreview.net/forum?id=Y71rktd4oT",
        "pdf_link": "https://openreview.net/pdf?id=Y71rktd4oT",
        "keywords": "mechanistic interpretability, group theory, universality",
        "abstract": "To what extent do different neural networks trained on the same or related tasks converge to similar strategies despite the random variation in their training processes? This is a key question that must be answered if we hope to interpret what neural networks do. Several recent works have sought an answer  by studying simplified settings--in particular group-theoretic tasks, arriving at seemingly diverging conclusions. We unify  different perspectives by detailing the macroscopic algorithm implemented by neural networks trained on modular addition. We show that, regardless of whether the modulus is prime or composite, the network learns group-theoretic representations. In particular, by clustering neurons whose preactivation values' discrete Fourier transform concentrate on the same frequencies, we show that these neurons  compute approximate cosets of approximate subgroups. We observe no particular preference for which frequencies are learned; thus, networks learn frequencies dividing the modulus into integer factors only occasionally. However, we also observe that the neural network depends on the existence of neurons of multiple frequencies, using superpositions of their contributions to highlight the correct output. This can be interpreted as an approximate version of the Chinese Remainder Theorem. Our work emphasizes that successful interpretability must consider not just individual neurons but also populations, i.e., it depends on using the correct level of abstraction at which to examine network activity."
    },
    {
        "title": "Mosaic-IT: Free Compositional Data Augmentation Improves Instruction Tuning",
        "link_suffix": "/forum?id=DvU9ijSn1v",
        "link": "https://openreview.net/forum?id=DvU9ijSn1v",
        "pdf_link": "https://openreview.net/pdf?id=DvU9ijSn1v",
        "keywords": "Large Language Model, Instruction Tuning, Supervised Finetuning, Data Augmentaion",
        "abstract": "Finetuning large language models with a variety of instruction-response pairs has enhanced their capability to understand and follow instructions. Current instruction tuning primarily relies on teacher models or human intervention to generate and refine the instructions and responses for training, which are costly, non-sustainable, and may lack diversity. In this paper, we introduce Mosaic Instruction Tuning (Mosaic-IT), a human/model-free compositional data augmentation method that can efficiently create rich and diverse augmentations from existing instruction tuning data to enhance the LLMs. Mosaic-IT randomly concatenates multiple instruction data into one and trains the model to produce the corresponding responses with predefined higher-level meta-instructions to strengthen its multi-step instruction-following and format-following skills. Our extensive evaluations demonstrate a superior performance and training efficiency of Mosaic-IT, which achieves consistent performance improvements over various benchmarks and a $80%$ reduction in training costs compared with original instruction tuning. Our codes and data are available athttps://anonymous.4open.science/r/mosaic-955B."
    },
    {
        "title": "Solving Normalized Cut Problem with Constrained Action Space",
        "link_suffix": "/forum?id=7ZToWPWUlO",
        "link": "https://openreview.net/forum?id=7ZToWPWUlO",
        "pdf_link": "https://openreview.net/pdf?id=7ZToWPWUlO",
        "keywords": "graph partitioning, reinforcement learning, combinatorial optimization",
        "abstract": "We address the problem of Normalized Cut (NC) in weighted graphs where the shape of the partitions follow an apriori  pattern, namely they must approximately be shaped like rings and wedges on a planar graph. Classical methods like spectral clustering and METIS do not have a provision to specify such constraints and neither do newer methods that combine GNNs and Reinforcement Learning as they are based on initialization from classical methods. The key insight that underpins our approach, Wedge and Ring Transformers (WRT), is based on representing a graph using polar coordinates and then using a multi-head transformer with a PPO objective to optimize the non-differential NC objective.  To the best of our knowledge, WRT is the first method to explicitly constrain the shape of NC and opens up possibility of providing a principled approach for fine-grained shape-controlled generation of graph partitions. On the theoretical front we provide new Cheeger inequalities that connect the spectral properties of a graph with algebraic properties that capture the shape of the partitions. Comparisons with adaptations of strong baselines attest to the strength of WRT."
    }
]