[
    {
        "title": "Splat-SLAM: Globally Optimized RGB-only SLAM with 3D Gaussians",
        "link_suffix": "/forum?id=YKtbklD5MV",
        "link": "https://openreview.net/forum?id=YKtbklD5MV",
        "pdf_link": "https://openreview.net/pdf?id=YKtbklD5MV",
        "keywords": "slam, gaussian splatting",
        "abstract": "3D Gaussians Splatting has emerged as a powerful representation of geometry and appearance for RGB-only dense Simultaneous Localization and Mapping (SLAM), as it provides a compact dense map representation while enabling efficient and high-quality map rendering. However, existing methods show significantly worse reconstruction quality than competing methods using other 3D representations, e.g. neural points clouds, since they either do not employ global map and pose optimization or make use of monocular depth. In response, we propose the first RGB-only SLAM system with a dense 3D Gaussian map representation that utilizes all benefits of globally optimized tracking by adapting dynamically to keyframe pose and depth updates by actively deforming the 3D Gaussian map. Moreover, we find that refining the depth updates in inaccurate areas with a monocular depth estimator further improves the accuracy of the 3D reconstruction. Our experiments on the Replica, TUM-RGBD, and ScanNet datasets indicate the effectiveness of globally optimized 3D Gaussians, as the approach achieves superior or on par performance with existing RGB-only SLAM methods methods in tracking, mapping and rendering accuracy while yielding small map sizes and fast runtimes. The source code will be publicly available."
    },
    {
        "title": "MixLLM: Mixed-precision LLM Quantization with Algorithm-system Co-design",
        "link_suffix": "/forum?id=zi0XgnZlcl",
        "link": "https://openreview.net/forum?id=zi0XgnZlcl",
        "pdf_link": "https://openreview.net/pdf?id=zi0XgnZlcl",
        "keywords": "LLM, Quantization, Mixed-precision",
        "abstract": "Quantization has become one of the most effective methodologies to compress LLMs into smaller size.\nHowever, the existing quantization solutions still show limitations of either non-negligible accuracy drop or system inefficiency.\nIn this paper, we make a comprehensive\nanalysis of the general quantization principles on their effect to the triangle of accuracy, memory consumption and system efficiency.\nWe propose MixLLM that explores the new optimization space of mixed-precision quantization between output features based on the insight that different output features matter differently in the model.\nMixLLM identifies the output features with high salience in the global view rather than within each single layer,\neffectively assigning the larger bit-width to output features that need it most to achieve good accuracy with low memory consumption.\nWe present the sweet spot of quantization configuration of algorithm-system co-design that lead to high accuracy and system efficiency.\nTo address the system challenge of this sweet spot, we design the two-step dequantization to make use of the int8 Tensor Core easily and fast data type conversion to reduce dequantization overhead significantly.\nExtensive experiments show that MixLLM achieves the best accuracy on a variety of tasks for the popular LLMs than a set of state-of-the-art works.\nIt shows 0.31 lower perplexity and 0.43% improvement on zero shot tasks for Llama 3 8B than QoQ, with similar memory consumption and system efficiency."
    },
    {
        "title": "Time Series Representation Models for Multivariate Time Series Forecasting and Imputation",
        "link_suffix": "/forum?id=UCeZMMyjm2",
        "link": "https://openreview.net/forum?id=UCeZMMyjm2",
        "pdf_link": "https://openreview.net/pdf?id=UCeZMMyjm2",
        "keywords": "time series, attention, forecasting, imputation, multivariate",
        "abstract": "We introduce a multilayered representation learning architecture called Time Series Representation Model (TSRM) for multivariate time series forecasting and imputation. The architecture is structured around hierarchically ordered encoding layers, each dedicated to an independent representation learning task. Each encoding layer contains a representation layer designed to capture diverse temporal patterns and an aggregation layer responsible for combining the learned representations. The architecture is fundamentally based on a Transformer encoder-like configuration, with self-attention mechanisms at its core. The TSRM architecture outperforms state-of-the-art approaches on seven established benchmark datasets for both forecasting and imputation tasks while significantly reducing complexity in the form of learnable parameters. The source code is available athttps://anonymous.4open.science/r/TSRM-D7BE."
    },
    {
        "title": "BSM: Small but Powerful Biological Sequence Model for Genes and Proteins",
        "link_suffix": "/forum?id=QstnrTlPyr",
        "link": "https://openreview.net/forum?id=QstnrTlPyr",
        "pdf_link": "https://openreview.net/pdf?id=QstnrTlPyr",
        "keywords": "Biological Sequence Modeling, Language Models, Gene, Protein, Mixed-Modal",
        "abstract": "Modeling biological sequences such as DNA, RNA, and proteins is crucial for understanding complex processes like gene regulation and protein synthesis. However, most current models either focus on a single type or treat multiple types of data separately, limiting their ability to capture cross-modal relationships. We propose that by learning the relationships between these modalities, the model can enhance its understanding of each type. To address this, we introduce BSM, a small but powerful mixed-modal biological sequence foundation model, trained on three types of data: RefSeq, Gene Related Sequences, and interleaved biological sequences from the web. These datasets capture the genetic flow, gene-protein relationships, and the natural co-occurrence of diverse biological data, respectively. By training on mixed-modal data, BSM significantly enhances learning efficiency and cross-modal representation, outperforming models trained solely on unimodal data. With only 110M parameters, BSM achieves performance comparable to much larger models across both single-modal and mixed-modal tasks, and uniquely demonstrates in-context learning capability for mixed-modal tasks, which is absent in existing models. Further scaling to 270M parameters demonstrates even greater performance gains, highlighting the potential of BSM as a significant advancement in multimodal biological sequence modeling."
    },
    {
        "title": "Balanced Learning for Domain Adaptive Semantic Segmentation",
        "link_suffix": "/forum?id=0MhlzybvAp",
        "link": "https://openreview.net/forum?id=0MhlzybvAp",
        "pdf_link": "https://openreview.net/pdf?id=0MhlzybvAp",
        "keywords": "Semantic segmentation",
        "abstract": "Unsupervised domain adaptation (UDA) for semantic segmentation aims to transfer knowledge from a labeled source domain to an unlabeled target domain, improving model performance on the target dataset without additional annotations.\nDespite the effectiveness of self-training techniques in UDA, they struggle to learn each class in a balanced manner due to inherent class imbalance and distribution shift in both data and label space between domains.\nTo address this issue, we propose Balanced Learning for Domain Adaptation (BLDA), a novel approach to directly assess and alleviate class bias without requiring prior knowledge about the distribution shift between domains.\nFirst, we identify over-predicted and under-predicted classes by analyzing the distribution of predicted logits.\nSubsequently, we introduce a post-hoc approach to align the positive and negative logits distributions across different classes using anchor distributions and cumulative density functions.\nTo further consider the network's need to generate unbiased pseudo-labels during self-training, we couple Gaussian mixture models to estimate logits distributions online and incorporate logits correction terms into the loss function.\nMoreover, we leverage the resulting cumulative density as domain-shared structural knowledge to connect the source and target domains.\nExtensive experiments on two standard UDA semantic segmentation benchmarks demonstrate that BLDA consistently improves performance, especially for under-predicted classes, when integrated into existing methods.\nOur work highlights the importance of balanced learning in UDA and effectively mitigates class bias in domain adaptive semantic segmentation."
    },
    {
        "title": "Neuroplastic Expansion in Deep Reinforcement Learning",
        "link_suffix": "/forum?id=20qZK2T7fa",
        "link": "https://openreview.net/forum?id=20qZK2T7fa",
        "pdf_link": "https://openreview.net/pdf?id=20qZK2T7fa",
        "keywords": "Loss of Plasticity, Primacy Bias, Deep Reinforcement Learning, Continual RL",
        "abstract": "The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach,Neuroplastic Expansion(NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (1) elastic neuron generation based on potential gradients, (2) dormant neuron pruning to optimize network expressivity, and (3) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models."
    },
    {
        "title": "TSI-Bench: Benchmarking Time Series Imputation",
        "link_suffix": "/forum?id=eDJsL1qAxw",
        "link": "https://openreview.net/forum?id=eDJsL1qAxw",
        "pdf_link": "https://openreview.net/pdf?id=eDJsL1qAxw",
        "keywords": "Time Series, Missing Data, Imputation, Deep Learning, Benchmark",
        "abstract": "Effective imputation is a crucial preprocessing step for time series analysis. Despite the development of numerous deep learning algorithms for time series imputation, the community lacks standardized and comprehensive benchmark platforms to effectively evaluate imputation performance across different settings. Moreover, although many deep learning forecasting algorithms have demonstrated excellent performance, whether their modeling achievements can be transferred to time series imputation tasks remains unexplored. To bridge these gaps, we develop TSI-Bench, the first (to our knowledge) comprehensive benchmark suite for time series imputation utilizing deep learning techniques. The TSI-Bench pipeline standardizes experimental settings to enable fair evaluation of imputation algorithms and identification of meaningful insights into the influence of domain-appropriate missing rates and patterns on model performance. Furthermore, TSI-Bench innovatively provides a systematic paradigm to tailor time series forecasting algorithms for imputation purposes. Our extensive study across 34,804 experiments, 28 algorithms, and 8 datasets with diverse missingness scenarios demonstrates TSI-Bench's effectiveness in diverse downstream tasks and potential to unlock future directions in time series imputation research and analysis. All source code and experiment logs are released."
    },
    {
        "title": "Scenario-Wise Rec: A Multi-Scenario Recommendation Benchmark",
        "link_suffix": "/forum?id=cfe2zDg1G8",
        "link": "https://openreview.net/forum?id=cfe2zDg1G8",
        "pdf_link": "https://openreview.net/pdf?id=cfe2zDg1G8",
        "keywords": "Recommender System, Multi-scenario Recommendation, Click-Through Rate Prediction",
        "abstract": "Multi Scenario Recommendation (MSR) tasks, referring to building a unified model to enhance performance across all recommendation scenarios, have recently gained much attention. However, current research in MSR faces two significant challenges that hinder the field's development: the absence of uniform procedures for multi-scenario dataset processing, thus hindering fair comparisons, and most models being closed-sourced, which complicates comparisons with current SOTA models. Consequently, we introduce our benchmark, Scenario-Wise Rec, which comprises six public datasets and twelve benchmark models, along with a training and evaluation pipeline. We have also validated our benchmark using an industrial advertising dataset, further enhancing its real-world reliability. We aim for this benchmark to provide researchers with valuable insights from prior works, enabling the development of novel models based on our benchmark and thereby fostering a collaborative research ecosystem in MSR. Our source code is also available."
    },
    {
        "title": "Region-wise Motion Controller for Image-to-Video Generation",
        "link_suffix": "/forum?id=LnN5UdhcjT",
        "link": "https://openreview.net/forum?id=LnN5UdhcjT",
        "pdf_link": "https://openreview.net/pdf?id=LnN5UdhcjT",
        "keywords": "Diffusion Models, Image-to-Video Generation, Motion Control",
        "abstract": "Animating images with interactive motion control has garnered popularity for image-to-video (I2V) generation. Modern approaches typically regard the condition of Gaussian filtered point-wise trajectory as sole motion control signal. Nevertheless, such flow approximation of trajectory via Gaussian kernel severely limits the controllable capacity of fine-grained movement, and commonly fails to disentangle object and camera moving. To alleviate these, we present ReMoCo, a new recipe of region-wise motion controller that novelly leverages precise region-wise trajectory and motion mask to regulate fine-grained motion synthesis and identify exact target motion category (i.e., object or camera moving), respectively. Technically, ReMoCo first estimates the flow maps on each training video via a tracking model, and then samples the region-wise trajectories from multiple local regions to simulate inference scenario. Instead of approximating flow distribution via Gaussian filtering, our region-wise trajectory preserves original flow information at local area and thus manages to characterize fine-grained movement. A motion mask is simultaneously derived from the predicted flow maps to present holistic motion dynamics. To pursue natural and controllable motion generation, ReMoCo further strengthens video denoising with additional conditions of region-wise trajectory and motion mask in a feature modulation manner. More remarkably, we meticulously construct a benchmark called ReMoCo-Bench, which consists of 1.1K real-world user-annotated image-trajectory pairs, for the evaluation of both fine-grained and object-level motion synthesis in I2V generation. Extensive experiments conducted on WebVid-10M and ReMoCo-Bench demonstrate the effectiveness of our ReMoCo for precise motion control."
    },
    {
        "title": "Efficient and Robust Neural Combinatorial Optimization via Wasserstein-Based Coresets",
        "link_suffix": "/forum?id=F57HPKZ6KD",
        "link": "https://openreview.net/forum?id=F57HPKZ6KD",
        "pdf_link": "https://openreview.net/pdf?id=F57HPKZ6KD",
        "keywords": "Neural Combinatorial Optimization, Wasserstein-Based Metric, Coreset, Data compression",
        "abstract": "Combinatorial optimization (CO) is a fundamental tool in many fields. \nMany neural combinatorial optimization (NCO) methods have been proposed to solve CO problems.\nHowever, existing NCO methods typically require significant computational and storage resources, and face challenges in maintaining robustness to distribution shifts between training and test data.\nTo address these issues, \\textbf{first}, we model CO instances into probability measures, and introduce Wasserstein-based metrics to quantify the difference between CO instances. \n\\textbf{Then}, we leverage a popular data compression technique, \\emph{coreset}, to construct a small-size proxy for the original large dataset.\nHowever, the time complexity of constructing a coreset  is linearly dependent on the size of the dataset. Consequently, it becomes challenging when datasets are particularly large.\n\\textbf{Next}, we accelerate the coreset construction by adapting it to the merge-and-reduce framework for achieving parallel computing. Additionally, we prove that our coreset is a good representation in theory.\n\\textbf{Subsequently}, to speed up the training process for existing NCO methods, we propose an efficient training framework based on the coreset technique. We train the model on a small-size coreset rather than on the full dataset, and thus save substantial computational and storage resources. Inspired by hierarchical Gonzalez\u2019s algorithm, our coreset method is designed to capture the diversity of the dataset, which naturally improves robustness to distribution shifts.\n\\textbf{Finally}, experimental results demonstrate that our training framework not only enhances robustness to distribution shifts but also achieves better performance with reduced resource requirements."
    },
    {
        "title": "Playbook: Scalable Discrete Skill Discovery from Unstructured Datasets for Long-Horizon Decision-Making Problems",
        "link_suffix": "/forum?id=rF0wXBpFRT",
        "link": "https://openreview.net/forum?id=rF0wXBpFRT",
        "pdf_link": "https://openreview.net/pdf?id=rF0wXBpFRT",
        "keywords": "skill discovery, multi-task decision-making problem, offline reinforcement learning, hierarchical reinforcement learning",
        "abstract": "Skill discovery methods equip an agent with diverse skills necessary for solving challenging tasks through an unsupervised learning manner.\nHowever, making the pre-learned skills expandable for new tasks remains a challenge in existing research. To handle this limitation, we propose a scalable skill discovery algorithm, a playbook, which can accommodate unseen tasks by training new skills while maintaining previously learned ones. The playbook, characterized by discrete skills and an extendable structure, enables the extension of the skill set to cover new datasets. Since we design the playbook to have a finite number of skills, we can interpret a decision-making problem as a sequential skill classification problem, so we aim to learn additional skills of the playbook by applying the techniques of class-incremental learning. In addition, we also introduce skill planning schemes that can leverage both previously and newly learned skills to solve challenging tasks compounded by multiple sub-tasks. The proposed method is evaluated in the complex robotic manipulation benchmarks, and the results show that the playbook outperforms existing state-of-the-art methods that learn continuous skills."
    },
    {
        "title": "An End-to-End Model For Logits Based Large Language Models Watermarking",
        "link_suffix": "/forum?id=0KHW6yXdiZ",
        "link": "https://openreview.net/forum?id=0KHW6yXdiZ",
        "pdf_link": "https://openreview.net/pdf?id=0KHW6yXdiZ",
        "keywords": "LLM watermarking, End-to-end optimization, Robustness",
        "abstract": "The rise of large language models (LLMs) has increased concerns over source tracing and copyright protection for AI-generated content (AIGC), highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce the first end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimizing the encoder and decoder, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method demonstrates superior detection robustness, consistently outperforming state-of-the-art (SOTA) methods by 1.2%, 4.0%, and 5.5% across 3 LLMs, averaged over 6 types of text distortions. Simultaneously, our approach achieves exceptional text quality, as evidenced by reduced text perplexity and improved performance in the downstream tasks with a margin of 19.2% and 3.03%. Our method can be easily generalized to different LLMs. The code is available in supplementary material."
    },
    {
        "title": "Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences",
        "link_suffix": "/forum?id=IjbXZdugdj",
        "link": "https://openreview.net/forum?id=IjbXZdugdj",
        "pdf_link": "https://openreview.net/pdf?id=IjbXZdugdj",
        "keywords": "xLSTM, large language model, foundation model, DNA, protein, small molecule, SMILES, in-context learning, masked language modeling, causal language modeling, fill-in the middle, equivariance, reverse complementary sequence",
        "abstract": "Language models for biological and chemical sequences enable crucial applications such as drug discovery, protein engineering, and precision medicine. Currently, these language models are predominantly based on Transformer architectures. While Transformers have yielded impressive results, their quadratic runtime dependency on sequence length complicates their use for long genomic sequences and in-context learning on proteins and chemical sequences. Recently, the recurrent xLSTM architecture has been shown to perform favorably compared to Transformers and modern state-space models (SSMs) in the natural language domain. Similar to SSMs, xLSTMs have linear runtime dependency and allow for constant-memory decoding at inference time, which makes them prime candidates for modeling long-range dependencies in biological and chemical sequences. In this work, we tailor xLSTM towards these domains and we propose a suite of language models called Bio-xLSTM. Extensive experiments in three large domains, genomics, proteins, and chemistry, were performed to assess xLSTM\u2019s ability to model biological and chemical sequences. The results show that Bio-xLSTM is a highly proficient generative model for DNA, protein, and chemical sequences, learns rich representations, and can perform in-context learning for proteins and small molecules."
    },
    {
        "title": "Cluster-Segregate-Perturb (CSP): A Model-agnostic Explainability Pipeline for Spatiotemporal Land Surface Forecasting Models",
        "link_suffix": "/forum?id=9h5paerJxC",
        "link": "https://openreview.net/forum?id=9h5paerJxC",
        "pdf_link": "https://openreview.net/pdf?id=9h5paerJxC",
        "keywords": "Climate AI, Explainability, ConvLSTM, spatiotemporal analysis",
        "abstract": "Satellite images are increasingly valuable for modeling regional climate change. Earth surface forecasting is one task that combines satellite imagery and meteorological data to understand how climate evolves over time. However, understanding the complex relationship between meteorological variables and land surface changes remains a challenge. Our paper introduces a pipeline that integrates principles from perturbation-based techniques like LIME and global explainability techniques methods like PDP, addressing the limitations of these techniques in high-dimensional spatiotemporal models. This pipeline facilitates analyses such as marginal sensitivity, correlation, and lag analysis, etc for complex land forecasting models. Using ConvLSTM for surface forecasting, we analyzed influence of variables like temperature, pressure, and precipitation on the NDVI of the surface predictions. Our study in EarthNet2021 Dataset (primarily consists of samples from the European Alps region, collected during the spring to fall seasons) revealed that precipitation had the greatest impact, followed by temperature, while pressure has little to no direct effect on NDVI. Additionally, interesting nonlinear correlations between meteorological variables and NDVI have been uncovered."
    },
    {
        "title": "Uni2Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection",
        "link_suffix": "/forum?id=AcVpLS86RT",
        "link": "https://openreview.net/forum?id=AcVpLS86RT",
        "pdf_link": "https://openreview.net/pdf?id=AcVpLS86RT",
        "keywords": "Automatic Driving, 3D Object Detection, Multi-Dataset Training",
        "abstract": "We present Uni$^2$Det, a brand new framework for unified and universal multi-dataset training on 3D detection, enabling robust performance across diverse domains and generalization to unseen domains. Due to substantial disparities in data distribution and variations in taxonomy across diverse domains, training such a detector by simply merging datasets poses a significant challenge. Motivated by this observation, we introduce multi-stage prompting modules for multi-dataset 3D detection, which leverages prompts based on the characteristics of corresponding datasets to mitigate existing differences. This elegant design facilitates seamless plug-and-play integration within various advanced 3D detection frameworks in a unified manner, while also allowing straightforward adaptation for universal applicability across datasets. Experiments are conducted across multiple dataset consolidation scenarios involving KITTI, Waymo, and nuScenes, demonstrating that our Uni$^2$Det outperforms existing methods by a large margin in multi-dataset training. Notably, results on zero-shot cross-dataset transfer validate the generalization capability of our proposed method."
    },
    {
        "title": "Integration of neural solver and problem-specific solver through bilevel approach: a case study of min-max capacitated vehicle routing problem",
        "link_suffix": "/forum?id=km2nHt2YoD",
        "link": "https://openreview.net/forum?id=km2nHt2YoD",
        "pdf_link": "https://openreview.net/pdf?id=km2nHt2YoD",
        "keywords": "self-supervised learning, combinatorial optimization, optimal transport, vehicle routing problem, neural combinatorial solver",
        "abstract": "In real-world operations with combinatorial structures like vehicle routing problems, similar optimization problems have to be solved repeatedly with slight parameter variations. \nA key challenge in such scenarios is achieving both high solution quality and fast computation time, while traditional methods like heuristics or branch-and-bound struggle to achieve both simultaneously.\nIn contrast, problem-specific solvers can effectively balance solution quality and computation speed for specific problems. \nHowever, since real-world problems have more complex structures, they can handle only subproblems.\nTo enhance the applicability of the problem-specific solvers, we propose a framework that integrates a problem-specific solver and a neural solver. \nOur framework decomposes the optimization problem into subproblems so that some of which can be solved by problem-specific solvers, such as the traveling salesperson problem. \nFor the remaining portions of the problem, we utilize the similarities of the problems and design a neural solver.\nBy integrating two solvers, we can utilize the strengths of the problem-specific solver in balancing solution accuracy and computation speed, as well as the neural solver\u2019s ability to infer a solution from the similarity of optimization problems.\nBased on the case study with the min-max capacitated vehicle routing problem, we demonstrate that it outperforms the state-of-the-art solver regarding both high solution quality and short computation time."
    },
    {
        "title": "Neural networks on Symmetric Spaces of Noncompact Type",
        "link_suffix": "/forum?id=bwOndfohRK",
        "link": "https://openreview.net/forum?id=bwOndfohRK",
        "pdf_link": "https://openreview.net/pdf?id=bwOndfohRK",
        "keywords": "geometric deep learning, symmetric spaces, hyperbolic spaces, SPD manifolds",
        "abstract": "Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference."
    },
    {
        "title": "EvA: Evolutionary Attacks on Graphs",
        "link_suffix": "/forum?id=n6GemAoKMG",
        "link": "https://openreview.net/forum?id=n6GemAoKMG",
        "pdf_link": "https://openreview.net/pdf?id=n6GemAoKMG",
        "keywords": "Adversarial_Attack, Evasion_Attack, Evolutionary_Algorithm, Genetic_Algorithm",
        "abstract": "Even a slight perturbation in the graph structure can cause a significant drop in the accuracy of graph neural networks (GNNs). Most existing attacks leverage gradient information to perturb edges. This relaxes the attack's optimization problem from a discrete to a continuous space, resulting in solutions far from optimal. It also restricts the adaptability of the attack to non-differentiable objectives. Instead, we propose an evolutionary-based algorithm to solve the discrete optimization problem directly. Our Evolutionary Attack (EvA) works with any black-box model and objective, eliminating the need for a differentiable proxy loss. This permits us to design two novel attacks that: reduce the effectiveness of robustness certificates and break conformal sets. We introduce a sparse encoding that results in memory complexity that is linear in the attack budget. \nEvA reduces the accuracy by an additional $\\sim$11% on average compared to the best previous attack, revealing significant untapped potential in designing attacks."
    },
    {
        "title": "Graph-based Document Structure Analysis",
        "link_suffix": "/forum?id=Fu0aggezN9",
        "link": "https://openreview.net/forum?id=Fu0aggezN9",
        "pdf_link": "https://openreview.net/pdf?id=Fu0aggezN9",
        "keywords": "Document Structure Analysis, Document Layout Analysis, Document Relational Graph",
        "abstract": "When reading a document, glancing at the spatial layout of a document is an initial step to understand it roughly. Traditional document layout analysis (DLA) methods, however, offer only a superficial parsing of documents, focusing on basic instance detection and often failing to capture the nuanced spatial and logical relationships between instances. These limitations hinder DLA-based models from achieving a gradually deeper comprehension akin to human reading. In this work, we propose a novel graph-based Document Structure Analysis (gDSA) task. This task requires that model not only detects document elements but also generates spatial and logical relations in form of a graph structure, allowing to understand documents in a holistic and intuitive manner. For this new task, we construct a relation graph-based document structure analysis dataset(GraphDoc) with 80K document images and 4.13M relation annotations, enabling training models to complete multiple tasks like reading order, hierarchical structures analysis, and complex inter-element relationship inference. Furthermore, a document relation graph generator (DRGG) is proposed to address the gDSA task, which achieves performance with 57.6% at $mAP_g$@$0.5$ for a strong benchmark baseline on this novel task and dataset. We hope this graphical representation of document structure can mark an innovative advancement in document structure analysis and understanding. The new dataset and code will be made publicly available."
    },
    {
        "title": "One-step Image-function Generation via Consistency Training",
        "link_suffix": "/forum?id=C65Hpf02Ay",
        "link": "https://openreview.net/forum?id=C65Hpf02Ay",
        "pdf_link": "https://openreview.net/pdf?id=C65Hpf02Ay",
        "keywords": "Image generation; Diffusion models; Consistency Models",
        "abstract": "Consistency models aim to deliver a U-Net generator to map noise to images directly and enable swift inference with minimal steps, even trained in isolation with consistency training mode. However, the U-Net generator requires heavy feature extraction layers for multi-level resolutions and learning convolution kernels with specific receptive fields, resulting in the challenge that consistency models suffer from heavy training resources and fail to generate images with any user-specific resolutions. In this paper, we first validate that training the original consistency model with a small batch size via consistency training mode is pretty unstable, which motivates us to investigate efficient and flexible consistency models. To this end, we propose to use a novel Transformer-based generator to generate continuous image functions, which can then be differentially rendered as images with arbitrary resolutions. We adopt implicit neural representations (INRs) to form such continuous functions, which help to decouple the resolution of generated images and the total amount of the parameters generated from the neural network. Extensive experiments on one-step image generation demonstrate that our method greatly improves the performance of consistency models with low training resources and also provides an efficient any-resolution image sampling process."
    },
    {
        "title": "Delta - Contrastive Decoding Mitigates Text Hallucinations in Large Language Models",
        "link_suffix": "/forum?id=HNbNcDsgxL",
        "link": "https://openreview.net/forum?id=HNbNcDsgxL",
        "pdf_link": "https://openreview.net/pdf?id=HNbNcDsgxL",
        "keywords": "Contrastive Decoding, Text Hallucination Mitigation, Large Language Models",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks. Still, they are prone to generating hallucinations\u2014factually incorrect or fabricated content that can undermine their reliability, especially in high-stakes domains such as healthcare and legal advisory. In response to this challenge, we propose Delta, a novel inference-time approach that leverages contrastive decoding to mitigate hallucinations without requiring model retraining or additional training data. Delta works by randomly masking portions of the input prompt, then contrasting the original and masked output distribution generated by the model, effectively mitigating hallucinations through inference-only computations. Delta was evaluated across multiple benchmark datasets, including SQuAD v1.1 and v2, concerning 4 and 6 percent improvements. Delta demonstrated substantial advancement of 14.56 percent more extract match outcome with no definitive answers within the SQuAD version 2 benchmark. These findings suggest that Delta is particularly effective when hallucinations arise from contextual ambiguity. Delta presents a computationally efficient and scalable solution for reducing hallucinations in real-world LLM applications by focusing on inference-time enhancements."
    },
    {
        "title": "A second-order-like optimizer with adaptive gradient scaling for deep learning",
        "link_suffix": "/forum?id=r4Q86nBQka",
        "link": "https://openreview.net/forum?id=r4Q86nBQka",
        "pdf_link": "https://openreview.net/pdf?id=r4Q86nBQka",
        "keywords": "deep learning, second-order methods, stochastic optimization, dynamical systems",
        "abstract": "In this empirical article, we introduce INNAprop, an optimization algorithm that combines the INNA method with the RMSprop adaptive gradient scaling. It leverages second-order information and rescaling while keeping the memory requirements of standard DL methods as AdamW or SGD with momentum. After having recalled our geometrical motivations, we provide quite extensive experiments. On image classification (CIFAR-10, ImageNet) and language modeling (GPT-2), INNAprop consistently matches or outperforms AdamW both in training speed and accuracy, with minimal hyperparameter tuning in large-scale settings. Our code is publicly available at \\url{https://github.com/innaprop/innaprop}."
    },
    {
        "title": "MV3D-MAE: 2D Pre-trained MAEs are Effective 3D Representation Learners",
        "link_suffix": "/forum?id=hcVd3zpVvg",
        "link": "https://openreview.net/forum?id=hcVd3zpVvg",
        "pdf_link": "https://openreview.net/pdf?id=hcVd3zpVvg",
        "keywords": "Point Cloud, MAE, Multi-view depth image\uff0c2D-3D",
        "abstract": "Deep learning's success relies heavily on the availability of extensive labelled datasets. Compared to 2D data, acquiring 3D data is substantially more expensive and time-consuming. Current multi-modal self-supervised approaches often involve converting 3D data into 2D data for parallel multi-modal training, thereby ignoring the prior knowledge contained within extensively trained 2D models. Therefore, it is important to find ways to utilize 2D feature priors to facilitate the learning process of 3D models. In this paper, we propose MV3D-MAE, a masked autoencoder framework that utilizes a pre-trained 2D MAE model to enhance 3D representation learning. Initially, we convert single 3D point clouds into multi-view depth images. Building on a pre-trained 2D MAE model, we adapt the model for multi-view depth image reconstruction by integrating group attention and incorporating additional attention layers.  Then we propose a differentiable 3D reconstruction method named Mv-Swin, which maps the reconstructed results back to 3D objects without the use of camera poses, thereby learning 3D spatial representations. Thus, MV3D-MAE, through the bidirectional transformation between 2D and 3D data, mitigates the differences between modalities and enhances the network's representational performance by leveraging the prior knowledge in the pre-trained 2D MAE. Our model significantly improves performance in few-shot classification and achieves SOTA results in linear Support Vector Machine classification. It also demonstrated competitive performance in other downstream tasks of classification and segmentation in synthetic and real-world datasets."
    },
    {
        "title": "CPL: Critical Plan Step Learning Boosts LLM Generalization in Reasoning Tasks",
        "link_suffix": "/forum?id=eWc76Kyi8H",
        "link": "https://openreview.net/forum?id=eWc76Kyi8H",
        "pdf_link": "https://openreview.net/pdf?id=eWc76Kyi8H",
        "keywords": "LLM Reasoning, Monte-Carlo Tree Search, Reinforcement Learning, Generalization",
        "abstract": "Post-training, particularly reinforcement learning (RL) using self-play-generated data, has become a new learning paradigm for large language models (LLMs). However, scaling RL to develop a general reasoner remains a research challenge, as existing methods focus on task-specific reasoning without adequately addressing generalization across a broader range of tasks. Moreover, unlike traditional RL with limited action space, LLMs operate in an infinite space, making it crucial to search for valuable and diverse strategies to solve problems effectively.\nTo address this, we propose searching within the action space on high-level abstract plans to enhance model generalization and introduce Critical Plan Step Learning (CPL), comprising: 1) searching on plan, using Monte Carlo Tree Search (MCTS) to explore diverse plan steps in multi-step reasoning tasks, and 2) learning critical plan steps through Step-level Advantage Preference Optimization (Step-APO), which integrates advantage estimates for step preference obtained via MCTS into Direct Preference Optimization (DPO). This combination helps the model effectively learn critical plan steps, enhancing both reasoning capabilities and generalization.\nExperimental results demonstrate that our method, trained exclusively on GSM8K and MATH, not only significantly improves performance on GSM8K (+10.5%) and MATH (+6.5%), but also enhances out-of-domain reasoning benchmarks, such as HumanEval (+12.2%), GPQA (+8.6%), ARC-C (+4.0%), MMLU-STEM (+2.2%), and BBH (+1.8%). The code is available athttps://anonymous.4open.science/r/CPL."
    },
    {
        "title": "Beyond Data Scarcity: A Frequency-Driven Framework for Zero-Shot Forecasting",
        "link_suffix": "/forum?id=nTlzEM1x3B",
        "link": "https://openreview.net/forum?id=nTlzEM1x3B",
        "pdf_link": "https://openreview.net/pdf?id=nTlzEM1x3B",
        "keywords": "Time Series Forecasting, Synthetic Data, Zero-Shot",
        "abstract": "Time series forecasting is critical in numerous real-world applications, requiring accurate predictions of future values based on observed patterns. While traditional forecasting techniques work well in in-domain scenarios with ample data, they struggle when data is scarce or not available at all, motivating the emergence of zero-shot and few-shot learning settings. Recent advancements often leverage large-scale foundation models for such tasks, but these methods require extensive data and compute resources, and their performance may be hindered by ineffective learning from the available training set. This raises a fundamental question:What factors influence effective learning from data in time series forecasting?Toward addressing this, we propose using Fourier analysis to investigate how models learn from synthetic and real-world time series data. Our findings reveal that forecasters commonly suffer from poor learning from data with multiple frequencies and poor generalization to unseen frequencies, which impedes their predictive performance. To alleviate these issues, we present a novel synthetic data generation framework, designed to enhance real data or replace it completely by creating task-specific frequency information, requiring only the sampling rate of the target data. Our approach,Freq-Synth, improves the robustness of both foundation as well as non-foundation forecast models in zero-shot and few-shot settings, facilitating more reliable time series forecasting under limited data scenarios."
    }
]