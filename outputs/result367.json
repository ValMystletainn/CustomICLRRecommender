[
    {
        "title": "Predicate Hierarchies Improve Few-Shot State Classification",
        "link_suffix": "/forum?id=lxu8Vz6cLs",
        "link": "https://openreview.net/forum?id=lxu8Vz6cLs",
        "pdf_link": "https://openreview.net/pdf?id=lxu8Vz6cLs",
        "keywords": "few-shot state classification, predicate hierarchies",
        "abstract": "State classification of objects and their relations is core to many long-horizon tasks, particularly in robot planning and manipulation. However, the combinatorial explosion of possible object-predicate combinations, coupled with the need to adapt to novel real-world environments, makes it a desideratum for state classification models to generalize to novel queries with few examples. To this end, we propose PHIER, which leverages predicate hierarchies to generalize effectively in few-shot scenarios. PHIER uses an object-centric scene encoder, self-supervised losses that infer semantic relations between predicates, and a hyperbolic distance metric that captures hierarchical structure; it learns a structured latent space of image-predicate pairs that guides reasoning over state classification queries. We evaluate PHIER in the CALVIN and BEHAVIOR robotic environments and show that PHIER significantly outperforms existing methods in few-shot, out-of-distribution state classification, and demonstrates strong zero-shot generalization from simulated to real-world tasks. Our results demonstrate that leveraging predicate hierarchies improves performance on state classification tasks with limited data."
    },
    {
        "title": "A Watermark for Order-Agnostic Language Models",
        "link_suffix": "/forum?id=Nlm3Xf0W9S",
        "link": "https://openreview.net/forum?id=Nlm3Xf0W9S",
        "pdf_link": "https://openreview.net/pdf?id=Nlm3Xf0W9S",
        "keywords": "language model watermarking; generative model;",
        "abstract": "Statistical watermarking techniques are well-established for sequentially decoded language models (LMs). However, these techniques cannot be directly applied to order-agnostic LMs, as the tokens in order-agnostic LMs are not generated sequentially. In this work, we introduce PATTERN-MARK, a pattern-based watermarking framework specifically designed for order-agnostic LMs. We develop a\nMarkov-chain-based watermark generator that produces watermark key sequences with high-frequency key patterns. Correspondingly, we propose a statistical pattern-based detection algorithm that recovers the key sequence during detection and conducts statistical tests based on the count of high-frequency patterns. Our extensive evaluations on order-agnostic LMs, such as ProteinMPNN and CMLM, demonstrate PATTERN-MARK\u2019s enhanced detection efficiency, generation quality, and robustness, positioning it as a superior watermarking technique for order-agnostic LMs."
    },
    {
        "title": "Securing Equal Share: A Principled Approach for Learning Multiplayer Symmetric Games",
        "link_suffix": "/forum?id=s0gdfKcmoU",
        "link": "https://openreview.net/forum?id=s0gdfKcmoU",
        "pdf_link": "https://openreview.net/pdf?id=s0gdfKcmoU",
        "keywords": "Multiplayer symmetric games; Securing Equal Share",
        "abstract": "This paper examines multiplayer symmetric constant-sum games with more than two players in a competitive setting, including examples like Mahjong, Poker, and various board and video games. In contrast to two-player zero-sum games, equilibria in multiplayer games are neither unique nor non-exploitable, failing to provide meaningful guarantees when competing against opponents who play different equilibria or non-equilibrium strategies. This gives rise to a series of long-lasting fundamental questions in multiplayer games regarding suitable objectives, solution concepts, and principled algorithms. This paper takes an initial step toward addressing these challenges by focusing on the natural objective ofequal share\u2014securing an expected payoff of $C/n$ in an $n$-player symmetric game with a total payoff of $C$. We rigorously identify the theoretical conditions under which achieving an equal share is tractable and design a series of efficient algorithms, inspired by no-regret learning, thatprovablyattain approximate equal share across various settings. Furthermore, we provide complementary lower bounds that justify the sharpness of our theoretical results. Our experimental results highlight worst-case scenarios where meta-algorithms from prior state-of-the-art systems for multiplayer games fail to secure an equal share, while our algorithm succeeds, demonstrating the effectiveness of our approach."
    },
    {
        "title": "Any-Property-Conditional Molecule Generation with Self-Criticism using Spanning Trees",
        "link_suffix": "/forum?id=26kgSlMmhA",
        "link": "https://openreview.net/forum?id=26kgSlMmhA",
        "pdf_link": "https://openreview.net/pdf?id=26kgSlMmhA",
        "keywords": "molecules; transformers; masking; molecule generation; property conditional generation",
        "abstract": "Generating novel molecules is challenging, with most representations of molecules leading to generative models producing many invalid molecules. Spanning Tree-based Graph Generation (STGG) is a promising approach to ensure the generation of valid molecules, outperforming state-of-the-art generative models models for unconditional generation. In the real world, we want to be able to generate molecules conditional on one or multiple desired properties rather than unconditionally. Thus, in this work, we extend STGG to multi-property conditional generation. Our approach, STGG+, incorporates a modern Transformer architecture, random masking of properties during training (enabling conditioning on any subset of properties and classifier-free guidance), an auxiliary property-prediction loss (allowing the model to self-criticize molecules and select the best ones), and other improvements. We show that STGG+ achieves state-of-the-art performance on in-distribution and out-of-distribution conditional generation, as well as reward maximization."
    },
    {
        "title": "What Makes a Maze Look Like a Maze?",
        "link_suffix": "/forum?id=Iz75SDbRmm",
        "link": "https://openreview.net/forum?id=Iz75SDbRmm",
        "pdf_link": "https://openreview.net/pdf?id=Iz75SDbRmm",
        "keywords": "visual reasoning, abstract concepts, schemas",
        "abstract": "A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze). To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning. At the core of DSG are schemas\u2014dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols. DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models. The grounded schema is used to augment visual abstraction understanding. We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Benchmark, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions."
    },
    {
        "title": "Efficacy of Language Model Self-Play in Non-Zero-Sum Games",
        "link_suffix": "/forum?id=tCfvktlrHI",
        "link": "https://openreview.net/forum?id=tCfvktlrHI",
        "pdf_link": "https://openreview.net/pdf?id=tCfvktlrHI",
        "keywords": "language models, self-play, multi-agent, dialogue, reasoning",
        "abstract": "Game-playing agents like AlphaGo have achieved superhuman performance through self-play, which is theoretically guaranteed to yield optimal policies in competitive games. However, most language tasks are partially or fully cooperative, so it is an open question whether techniques like self-play can effectively be used to improve language models. We empirically investigate this question in a negotiation game setting known as Deal or No Deal (DoND). Crucially, the objective in DoND can be modified to produce a fully cooperative game, a strictly competitive one, or anything in between. We finetune language models in self-play over multiple rounds of filtered behavior cloning in DoND for each of these objectives and evaluate them in self-play and in collaboration with humans. We find that language models improve substantially in self-play, achieving 14-17x higher scores in task reward after finetuning. Further, the trained models generalize to both cooperation and competition with humans, scoring 2.5-6x higher than base models. We view these results as an early promising sign for language model self-play in cooperative settings, despite a lack of theoretical guarantees."
    },
    {
        "title": "SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency",
        "link_suffix": "/forum?id=tJoS2d0Onf",
        "link": "https://openreview.net/forum?id=tJoS2d0Onf",
        "pdf_link": "https://openreview.net/pdf?id=tJoS2d0Onf",
        "keywords": "Generative models, 4D generation",
        "abstract": "We present Stable Video 4D (SV4D) \u2014 a latent video diffusion model for multi-frame and multi-view consistent dynamic 3D content generation. Unlike previous methods that rely on separately trained generative models for video generation and novel view synthesis, we design a unified diffusion model to generate novel view videos of dynamic 3D objects.  Specifically, given a monocular reference video, SV4D generates novel views for each video frame that are temporally consistent. We then use the generated novel view videos to optimize an implicit 4D representation (dynamic NeRF) efficiently, without the need for cumbersome SDS-based optimization used in most prior works. To train our unified novel view video generation model, we curated a dynamic 3D object dataset from the existing Objaverse dataset. Extensive experimental results on multiple datasets and user studies demonstrate SV4D's state-of-the-art performance on novel-view video synthesis as well as 4D generation compared to prior works."
    },
    {
        "title": "Reward-RAG: Enhancing RAG with Reward Driven Supervision",
        "link_suffix": "/forum?id=oqRe1KvD17",
        "link": "https://openreview.net/forum?id=oqRe1KvD17",
        "pdf_link": "https://openreview.net/pdf?id=oqRe1KvD17",
        "keywords": "retrieval-augmented generation, reward model, CriticGPT, reinforcement learning from AI feedback (RLAIF), reinforcement learning from human feedback (RLHF), LLMs",
        "abstract": "In this paper, we introduce Reward-RAG, a novel approach designed to enhance the Retrieval-Augmented Generation (RAG) model through Reward-Driven Supervision. Unlike previous RAG methodologies, which focus on training language models (LMs) to utilize external knowledge retrieved from external sources, our method adapts retrieval information to specific domains by employing CriticGPT to train a dedicated reward model. This reward model generates synthesized datasets for fine-tuning the RAG encoder, aligning its outputs more closely with human preferences. The versatility of our approach allows it to be effectively applied across various domains through domain-specific fine-tuning. We evaluate Reward-RAG on publicly available benchmarks from multiple domains, comparing it to state-of-the-art methods. Our experimental results demonstrate significant improvements in performance, highlighting the effectiveness of Reward-RAG in improving the relevance and quality of generated responses. These findings underscore the potential of integrating reward models with RAG to achieve superior outcomes in natural language generation tasks."
    },
    {
        "title": "Adaptive Depth Tsetlin Automaton",
        "link_suffix": "/forum?id=a0XW2pBcbm",
        "link": "https://openreview.net/forum?id=a0XW2pBcbm",
        "pdf_link": "https://openreview.net/pdf?id=a0XW2pBcbm",
        "keywords": "Reinforcement Learning, Tsetlin Automaton, Automated Machine Learning, Single-State Reinforcement Learning",
        "abstract": "The Tsetlin Automaton (TA) is a foundational single-state reinforcement learning model, but its fixed depth parameter ($N$) poses a significant limitation for navigating the exploration and exploitation dilemma. Despite remarkable advancements, existing TA models lack adaptability in real-world scenarios where dynamic depth adjustments are essential. In this paper, we introduce the Adaptive Depth Tsetlin Automaton (ADTA), a novel solution addressing this challenge. ADTA integrates TA with a reinforcement agent capable of dynamically modifying $N$. We analyze ADTA using Lyapunov stability theorem and Markov chain analysis within a dual-environment framework: the outer environment, where TA operates to maximize rewards, and the inner environment, where a reinforcement learning agent evaluates TA's performance based on $N$. Through actions like 'Grow,' 'Shrink,' and 'Stop,' the inner agent configures $N$ dynamically. Unlike conventional TA configurations with fixed $N$, our approach demonstrates improved reward maximization and regret minimization. Furthermore, we present numerical simulations that corroborate our theoretical results."
    },
    {
        "title": "Power Scheduler: A Batch Size and Token Number Agnostic Learning Rate Scheduler",
        "link_suffix": "/forum?id=gN4stDLq3t",
        "link": "https://openreview.net/forum?id=gN4stDLq3t",
        "pdf_link": "https://openreview.net/pdf?id=gN4stDLq3t",
        "keywords": "Learning rate scheduler, language model, pretraining",
        "abstract": "Finding the optimal learning rate for language model pretraining is a challenging task. This is not only because there is a complicated correlation between learning rate, batch size, number of training tokens, model size, and other hyperparameters but also because it is prohibitively expensive to perform a hyperparameter search for large language models with Billions or Trillions of parameters. Recent studies propose using small proxy models and small corpus to perform hyperparameter searches and transposing the optimal parameters to large models and large corpus. While the zero-shot transferability is theoretically and empirically proven for model size-related hyperparameters, like depth and width, the zero-shot transfer from small corpus to large corpus is underexplored. In this paper, we study the correlation between optimal learning rate, batch size, and number of training tokens for the recently proposed WSD scheduler. After thousands of small experiments, we found a power-law relationship between variables and demonstrated its transferability across model sizes. Based on the observation, we propose a new learning rate scheduler, Power scheduler,  that is agnostic about the number of training tokens and batch size. The experiment shows that combining the Power scheduler with Maximum Update Parameterization (muP) can consistently achieve impressive performance with one set of hyperparameters regardless of the number of training tokens, batch size, model size, and even model architecture. Our 3B dense and MoE models trained with the Power scheduler achieve comparable performance as state-of-the-art small language models."
    },
    {
        "title": "No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models",
        "link_suffix": "/forum?id=OwNoTs2r8e",
        "link": "https://openreview.net/forum?id=OwNoTs2r8e",
        "pdf_link": "https://openreview.net/pdf?id=OwNoTs2r8e",
        "keywords": "generative models, hallucination, no-free-lunch theorem, distribution PAC learning, VC-dimension",
        "abstract": "Generative models have shown impressive capabilities in synthesizing high-quality outputs across various domains. However, a persistent challenge is the occurrence of \"hallucinations,\" where the model produces outputs that are plausible but invalid. While empirical strategies have been explored to mitigate this issue, a rigorous theoretical understanding remains elusive. In this paper, we develop a theoretical framework to analyze thelearnabilityof non-hallucinating generative models from a learning-theoretic perspective. Our results reveal that non-hallucinating learning is statisticallyimpossiblewhen relying solely on the training dataset, even for a hypothesis class of size two and when the entire training set is truthful. To overcome these limitations, we show that incorporatinginductive biasesaligned with the actual facts into the learning process is essential. We provide a systematic approach to achieve this by restricting the fact set to a concept class of finite VC-dimension and demonstrate its effectiveness under various learning paradigms. Although our findings are primarily conceptual, they represent a first step towards a principled approach to addressing hallucinations in learning generative models."
    },
    {
        "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
        "link_suffix": "/forum?id=cUFIil6hEG",
        "link": "https://openreview.net/forum?id=cUFIil6hEG",
        "pdf_link": "https://openreview.net/pdf?id=cUFIil6hEG",
        "keywords": "accelerated optimization, parameter prediction, graphs, transformers, learning to optimize",
        "abstract": "Neural network training can be accelerated when a learnable update rule is used in lieu of classic adaptive optimizers (e.g. Adam). However, learnable update rules can be costly and unstable to train and use. Recently, Jang et al. (2023) proposed a simpler approach to accelerate training based on weight nowcaster networks (WNNs). In their approach, Adam is used for most of the optimization steps and periodically, only every few steps, a WNN nowcasts (predicts near future) parameters. We improve WNNs by proposing neuron interaction and nowcasting (NiNo) networks. In contrast to WNNs, NiNo leverages neuron connectivity and graph neural networks to more accurately nowcast parameters. We further show that in some networks, such as Transformers, modeling neuron connectivity accurately is challenging. We address this and other limitations, which allows NiNo to accelerate Adam training by up to 50% in vision and language tasks."
    },
    {
        "title": "Unbounded: A Generative Infinite Game of Character Life Simulation",
        "link_suffix": "/forum?id=uy31tqVuNo",
        "link": "https://openreview.net/forum?id=uy31tqVuNo",
        "pdf_link": "https://openreview.net/pdf?id=uy31tqVuNo",
        "keywords": "Text-to-Image Generation, Interactive Image Generation",
        "abstract": "We introduce the concept of a generative infinite game, a video game that transcends the traditional boundaries of finite, hard-coded systems by using generative models. Inspired by James P. Carse's distinction between finite and infinite games, we leverage recent advances in generative AI to create Unbounded: a game of character life simulation that is fully encapsulated in generative models. Specifically, Unbounded draws inspiration from sandbox life simulations and allows you to interact with your autonomous virtual character in a virtual world by feeding, playing with and guiding it - with open-ended mechanics generated by an LLM, some of which can be emergent. In order to develop Unbounded, we propose technical innovations in both the LLM and visual generation domains. Specifically, we present: (1) a specialized, distilled large language model (LLM) that dynamically generates game mechanics, narratives, and character interactions in real-time, and (2) a new dynamic regional image prompt Adapter (IP-Adapter) for vision models that ensures consistent yet flexible visual generation of a character across multiple environments. We evaluate our system through both qualitative and quantitative analysis, showing significant improvements in character life simulation, user instruction following, narrative coherence, and visual consistency for both characters and the environments compared to traditional related approaches."
    },
    {
        "title": "Point Cloud Self-supervised Learning via 3D to Multi-view Masked Leaner",
        "link_suffix": "/forum?id=bw9bvwVwMH",
        "link": "https://openreview.net/forum?id=bw9bvwVwMH",
        "pdf_link": "https://openreview.net/pdf?id=bw9bvwVwMH",
        "keywords": "Self-supervised learning, Multi-modality learning, 3D representation, Masked autoencoder",
        "abstract": "Recently, multi-modal masked autoencoders (MAE) has been introduced in 3D self-supervised learning, offering enhanced feature learning by leveraging both 2D and 3D data to capture richer cross-modal representations. However, these approaches have two limitations: (1) they inefficiently require both 2D and 3D modalities as inputs, even though the inherent multi-view properties of 3D point clouds already contain 2D modality.\n(2) input 2D modality causes the reconstruction learning to unnecessarily rely on visible 2D information, hindering 3D geometric representation learning.\nTo address these challenges, we propose a 3D to Multi-View Learner (Multi-View ML) that only utilizes 3D modalities as inputs and effectively capture rich spatial information in 3D point clouds. \nSpecifically, we first project 3D point clouds to multi-view 2D images at the feature level based on 3D-based pose.\nThen, we introduce two components: (1) a 3D to multi-view autoencoder that reconstructs point clouds and multi-view images from 3D and projected 2D features; \n(2) a multi-scale multi-head (MSMH) attention mechanism that facilitates local-global information interactions in each decoder transformer block through attention heads at various scales. \nAdditionally, a novel two-stage self-training strategy is proposed to align 2D and 3D representations.\nEmpirically, our method significantly outperforms state-of-the-art counterparts across various downstream tasks, including 3D classification, part segmentation, and object detection.\nSuch performance superiority showcases that Multi-View ML enriches the model's comprehension of geometric structures and inherent multi-modal properties of point clouds."
    },
    {
        "title": "Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models",
        "link_suffix": "/forum?id=ILSZZNlbqw",
        "link": "https://openreview.net/forum?id=ILSZZNlbqw",
        "pdf_link": "https://openreview.net/pdf?id=ILSZZNlbqw",
        "keywords": "Graph Foundation Model, Graph Data Augmentation, Diffusion Models",
        "abstract": "Models for natural language and images benefit from data scaling behavior: the more data fed into the model, the better they perform. This 'better with more' phenomenon enables the effectiveness of large-scale pre-training on vast amounts of data. However, current graph pre-training methods struggle to scale up data due to heterogeneity across graphs. To achieve effective data scaling, we aim to develop a general model that is able to capture diverse data patterns of graphs and can be utilized to adaptively help the downstream tasks. To this end, we propose UniAug, a universal graph structure augmentor built on a diffusion model. We first pre-train a discrete diffusion model on thousands of graphs across domains to learn the graph structural patterns. In the downstream phase, we provide adaptive enhancement by conducting graph structure augmentation with the help of the pre-trained diffusion model via guided generation. By leveraging the pre-trained diffusion model for structure augmentation, we consistently achieve performance improvements across various downstream tasks in a plug-and-play manner. To the best of our knowledge, this study represents the first demonstration of a data-scaling graph structure augmentor on graphs across domains."
    },
    {
        "title": "Thetan Berserker: Fast and Stochastic Distance-based Clustering",
        "link_suffix": "/forum?id=SPu6k4OZkj",
        "link": "https://openreview.net/forum?id=SPu6k4OZkj",
        "pdf_link": "https://openreview.net/pdf?id=SPu6k4OZkj",
        "keywords": "clustering",
        "abstract": "Clustering is a challenging NP-hard problem. Polynomial approximations are of paramount importance for identifying intriguing hidden representations of data at reasonable execution times. In this work, we propose a novel clustering algorithm called Thetan Berserker (TB). TB is a centroid-based clustering method controlled by a single distance parameter. TB revitalizes an old family of sequential algorithms (such as BSAS) which are adored for their speed but are known to be order sensitive. In addition, TB enables widely used algorithms such as KMEANS and DBSCAN by improving their initial conditions. Theoretical aspects are provided along with extensive comparisons and benchmarks. TB opens new horizons for a variety of real-world applications. Examples are provided using publicly available 1D, 2D, and 3D data. A wide range of performance boosts in clustering accuracy, memory usage, and runtime are reported."
    },
    {
        "title": "Physically Aligned Hierarchical Mesh-based Network for Dynamic System Simulation",
        "link_suffix": "/forum?id=pzasy8KRWK",
        "link": "https://openreview.net/forum?id=pzasy8KRWK",
        "pdf_link": "https://openreview.net/pdf?id=pzasy8KRWK",
        "keywords": "dynamic system; physics simulation; solid mechanics; graph-based simulation;",
        "abstract": "Dynamic systems evolve through complex interactions, where local events influence global behaviors, reflecting the interconnected nature of real-world phenomena. Simulating such systems demands models that effectively capture both local and long-range dynamics, while maintaining a balance between accuracy and computational efficiency. However, existing mesh-based Graph Neural Network (GNN) methods often struggle to achieve both high accuracy and efficiency, especially when dealing with large datasets, complex mesh structures, and extensive long-range effects. Inspired by how real-world dynamic systems operate, we present the Mesh-based Multi-Segment Graph Network (MMSGN), a novel framework designed to address these challenges by leveraging a physically aligned hierarchical information exchange mechanism. MMSGN combines micro-level local interactions with macro-level global exchanges, aligning the hierarchical mesh structure with the system\u2019s physical properties to seamlessly capture both local and global dynamics. This approach enables precise modeling of complex behaviors while maintaining computational efficiency. We validate our model on multiple dynamic system datasets and compare it with several state-of-the-art methods. Our results demonstrate that MMSGN delivers superior accuracy and mesh quality, excels in managing long-range effects, and maintains high computational efficiency. Furthermore, MMSGN exhibits strong generalization capabilities, scaling effectively to larger physical domains. These advantages make MMSGN well-suited for simulating complex, large-scale dynamic systems across a variety of scenarios. Codes and data will be made publicly accessible upon acceptance."
    },
    {
        "title": "Right Now, Wrong Then: Non-Stationary Direct Preference Optimization under Preference Drift",
        "link_suffix": "/forum?id=PabAln0jjB",
        "link": "https://openreview.net/forum?id=PabAln0jjB",
        "pdf_link": "https://openreview.net/pdf?id=PabAln0jjB",
        "keywords": "LLM, fine-tuning, DPO, non-stationarity, preference drift, RLHF",
        "abstract": "Reinforcement learning from human feedback (RLHF) aligns Large Language Models (LLMs) with human preferences. However, these preferences can often change over time due to external factors (e.g. environment change and societal influence). Consequently, what was wrong then might be right now. Current preference optimization algorithms do not account for temporal preference drift in their modeling, which can lead to severe misalignment. To address this limitation, we use a Dynamic Bradley-Terry model that models preferences via time-dependent reward functions, and propose Non-Stationary Direct Preference Optimisation (NS-DPO). By introducing a discount parameter in the loss function, NS-DPO applies exponential weighting, which proportionally focuses learning on more time-relevant datapoints. We theoretically analyse the convergence of NS-DPO in the offline setting, providing upper bounds on the estimation error caused by non-stationary preferences. Finally, we demonstrate the effectiveness of NS-DPO1 for fine-tuning LLMs in scenarios with drifting preferences. By simulating preference drift using renowned reward models and modifying popular LLM datasets accordingly, we show that NS-DPO fine-tuned LLMs remain robust under non-stationarity, significantly outperforming baseline algorithms that ignore temporal preference changes, without sacrificing performance in stationary cases."
    },
    {
        "title": "Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML",
        "link_suffix": "/forum?id=69Fp4dcmJN",
        "link": "https://openreview.net/forum?id=69Fp4dcmJN",
        "pdf_link": "https://openreview.net/pdf?id=69Fp4dcmJN",
        "keywords": "differential privacy, large models, DP-SGD, matrix factorization",
        "abstract": "Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes.  Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF , which optimally balances the benefits of privacy amplification and noise correlation.  Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may be more than $10^4$ and the number of model parameters may exceed $10^7$.  In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to effectively handle settings with over $10^6$ training iterations and $10^9$ model parameters, with no utility degradation at smaller scales."
    },
    {
        "title": "Regularised Jump Models for Regime Identification and Feature Selection",
        "link_suffix": "/forum?id=JXvEzl8YkS",
        "link": "https://openreview.net/forum?id=JXvEzl8YkS",
        "pdf_link": "https://openreview.net/pdf?id=JXvEzl8YkS",
        "keywords": "jump models, regime identification, feature selection",
        "abstract": "A regime modelling framework can be employed to address the complexities of financial markets. Under the framework, market periods are grouped into distinct regimes, each distinguished by similar statistical characteristics. Regimes in financial markets are not directly observable but are often manifested in market and macroeconomic variables. The objective of regime modelling is to accurately identify the active regime from these variables at a point in time, a process known as regime identification. One way to enhance the accuracy of regime identification is to select features that are most responsible for statistical differences between regimes, a process known as feature selection. Models based on the Jump Model framework have recently been developed to address the joint problem of regime identification and feature selection. In the following work, we propose a new set of models called Regularised Jump Models that are founded upon the Jump Model framework. These models perform feature selection that is more interpretable than that from the Sparse Jump Model, a model proposed in the literature pertaining to the Jump Model framework. Through a simulation experiment, we find evidence that these new models outperform the Standard and Sparse Jump Models, both in terms of regime identification and feature selection."
    },
    {
        "title": "Gaga: Group Any Gaussians via 3D-aware Memory Bank",
        "link_suffix": "/forum?id=aLSI9Z4UMD",
        "link": "https://openreview.net/forum?id=aLSI9Z4UMD",
        "pdf_link": "https://openreview.net/pdf?id=aLSI9Z4UMD",
        "keywords": "3D Open-world Segmentation; Gaussian Splatting; Scene Understanding",
        "abstract": "We introduceGaga, a framework that reconstructs and segments open-world 3D scenes by leveraging inconsistent 2D masks predicted by zero-shot class-agnostic segmentation models. Contrasted to prior 3D scene segmentation approaches that heavily rely on video object tracking,Gagautilizes spatial information and effectively associates object masks across diverse camera poses through a novel 3D-aware memory bank. By eliminating the assumption of continuous view changes in training images,Gagademonstrates robustness to variations in camera poses, particularly beneficial for sparsely sampled images, ensuring precise mask label consistency. Furthermore,Gagaaccommodates 2D segmentation masks from diverse sources and demonstrates robust performance with different open-world zero-shot class-agnostic segmentation models, significantly enhancing its versatility. Extensive qualitative and quantitative evaluations demonstrate thatGagaperforms favorably against state-of-the-art methods, emphasizing its potential for real-world applications such as scene understanding and manipulation. The source codes will be made available to the public."
    },
    {
        "title": "Privately Counting Partially Ordered Data",
        "link_suffix": "/forum?id=hVTaXJ0I5M",
        "link": "https://openreview.net/forum?id=hVTaXJ0I5M",
        "pdf_link": "https://openreview.net/pdf?id=hVTaXJ0I5M",
        "keywords": "differential privacy",
        "abstract": "We consider differentially private counting when each data point consists of $d$ bits satisfying a partial order. Our main technical contribution is a problem-specific $K$-norm mechanism that runs in time $O(d^2)$. Experiments show that, depending on the partial order in question, our solution dominates existing pure differentially private mechanisms and can reduce their error by an order of magnitude or more."
    },
    {
        "title": "SeedLoRA: A Fusion Approach to Efficient LLM Fine-Tuning",
        "link_suffix": "/forum?id=jkCvAAcSDa",
        "link": "https://openreview.net/forum?id=jkCvAAcSDa",
        "pdf_link": "https://openreview.net/pdf?id=jkCvAAcSDa",
        "keywords": "Machine Learning, Optimization, Large Language Model",
        "abstract": "Despite Low-Rank Adaptation (LoRA)'s popularity for fine-tuning large models, it often exhibits a noticeable performance gap compared to full fine-tuning, particularly in complex tasks such as mathematical reasoning and code generation. Motivated by this discrepancy, we propose a novel fusion approach for LoRA fine-tuned models. Our key insight is that LoRA models trained with different random seeds on the same task often exhibit complementary strengths. In contrast to existing research that typically focuses on fusing models trained on diverse tasks, we explore the potential of combining multiple LoRA models fine-tuned on the same task with different random seeds. This intra-task fusion method aims to leverage the strengths of various fine-tuned models to create a more robust and effective adaptation. To validate our approach, we conducted comprehensive experiments across three key areas: mathematical reasoning, code generation, and general instruction-tuning tasks. The results demonstrate that our fusion method significantly enhances LoRA's performance, outperforming both standalone LoRA models and current fusion methods. Notably, this advancement substantially narrows the gap between LoRA and full fine-tuning, thus offering a more effective approach to model adaptation without the GPU memory burden of full parameter fine-tuning."
    },
    {
        "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
        "link_suffix": "/forum?id=4Kw4KAoVnx",
        "link": "https://openreview.net/forum?id=4Kw4KAoVnx",
        "pdf_link": "https://openreview.net/pdf?id=4Kw4KAoVnx",
        "keywords": "Zeroth-Order Optimization",
        "abstract": "While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, compared with exact gradients, ZO-based gradients usually exhibit an estimation error, which can significantly hurt the optimization process, leading to slower convergence and suboptimal solutions. In addition, we find that the estimation error will hurt more when adding to large weights instead of small weights. Based on this observation, this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9% absolute accuracy improvement and 3.5x speedup over MeZO."
    },
    {
        "title": "ControlVAR: Exploring Controllable Visual Autoregressive Modeling",
        "link_suffix": "/forum?id=v46TPwU0Uy",
        "link": "https://openreview.net/forum?id=v46TPwU0Uy",
        "pdf_link": "https://openreview.net/pdf?id=v46TPwU0Uy",
        "keywords": "Autoregressive generation, Controllable image generation",
        "abstract": "Conditional visual generation has witnessed remarkable progress with the advent of diffusion models (DMs), especially in tasks like control-to-image generation. However, challenges such as expensive computational cost, high inference latency, and difficulties of integration with large language models (LLMs) have necessitated exploring alternatives to DMs. This paper introduces ControlVAR, a novel framework that explores pixel-level controls in visual autoregressive (VAR) modeling for flexible and efficient conditional generation. In contrast to traditional conditional models that learn the conditional distribution, ControlVAR jointly models the distribution of image and pixel-level conditions during training and imposes conditional controls during testing. To enhance the joint modeling, we adopt the next-scale AR prediction paradigm and unify control and image representations. A teacher-forcing guidance strategy is proposed to further facilitate controllable generation with joint modeling. Extensive experiments demonstrate the superior efficacy and flexibility of ControlVAR across various conditional generation tasks against popular conditional DMs, \\eg, ControlNet and T2I-Adaptor."
    }
]