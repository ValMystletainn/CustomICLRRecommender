[
    {
        "title": "Towards Geometry Problems Solving Employing GPT-4 Vision with Few-Shot Prompting: An Empirical Study of What Matters",
        "link_suffix": "/forum?id=0vKokoPKTo",
        "link": "https://openreview.net/forum?id=0vKokoPKTo",
        "pdf_link": "https://openreview.net/pdf?id=0vKokoPKTo",
        "keywords": "Large Language Models, Mathematical Reasoning, Geometry Problem Solving, Prompting Methods",
        "abstract": "The few demonstrations (\"few-shot prompting\") can significantly improve the ability of Large Language Models (LLMs) in mathematical reasoning, including geometry problem solving (GPS). \nGPT-4 Vision (GPT-4V), as a leading example of LLMs, also demonstrates significant improvements. \nThis tremendous achievement is mainly attributed to prompting methods like \"Chain-of-Thought\" and \"Program-of-Thought,\" which leverage the in-context learning ability of the model combined with few-shot prompting to solve new problems. \nDespite the success of these prompting methods, it remains understood what the GPT-4V model learns from the demonstrations that lead to improved performance. \nIn this paper, we evaluated the answering accuracy of GPT-4V with 2-shot prompting on five geometric problem datasets and conducted a series of detailed analyses. \nFirstly, through ablation experiments with valid and invalid demonstration examples, we found that the model\u2019s performance improvement is not due to the quality of the demonstration, but rather to the input format, output format, and logic and structure of the demonstration. \nSecondly, by analyzing the reasoning and computational requirements of geometric problems, and verifying experimental results, we found that GPS tasks emphasize reasoning ability more than computational power. \nFinally, our analysis of various prompt methods revealed that existing approaches are not effective at improving model performance concerning problem length and geometric shape. \nTherefore, specialized prompt methods could be designed to enhance the model's performance in these aspects, or fine-tuning the model by adding problem data with longer lengths or mixed geometric shapes could optimize its performance. \nOverall, developing an LLM that fully adapts to GPS tasks represents a key research direction. \nThe source code and data will be made available in a GitHub repository."
    },
    {
        "title": "How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs",
        "link_suffix": "/forum?id=BTr3PSlT0T",
        "link": "https://openreview.net/forum?id=BTr3PSlT0T",
        "pdf_link": "https://openreview.net/pdf?id=BTr3PSlT0T",
        "keywords": "Video Large Multi-modal Models, Complex Reasoning, Prompting for Multi-modal models",
        "abstract": "Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical surgery, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and the robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 11 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, especially open-source ones, struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to effectively enhance the performance of existing Video-LMMs on CVRR-ES benchmark. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities. Our dataset and code will be made publicly available."
    },
    {
        "title": "Iterative Substructure Extraction for Molecular Relational Learning with Interactive Graph Information Bottleneck",
        "link_suffix": "/forum?id=3kiZ5S5WkY",
        "link": "https://openreview.net/forum?id=3kiZ5S5WkY",
        "pdf_link": "https://openreview.net/pdf?id=3kiZ5S5WkY",
        "keywords": "Molecular Relational Learning, EM Algorithm, Substructure Extraction, Interactive Graph Information Bottleneck",
        "abstract": "Molecular relational learning (MRL) seeks to understand the interaction behaviors between molecules, a pivotal task in domains such as drug discovery and materials science. Recently, extracting core substructures and modeling their interactions have emerged as mainstream approaches within machine learning-assisted methods. However, these methods still exhibit some limitations, such as insufficient consideration of molecular interactions or capturing substructures that include excessive noise, which hampers precise core substructure extraction.\nTo address these challenges, we present an integrated dynamic framework called Iterative Substructure Extraction (ISE). ISE employs the Expectation-Maximization (EM) algorithm for MRL tasks, where the core substructures of interacting molecules are treated as latent variables and model parameters, respectively. Through iterative refinement, ISE gradually narrows the interactions from the entire molecular structures to just the core substructures.\nMoreover, to ensure the extracted substructures are concise and compact, we propose the Interactive Graph Information Bottleneck (IGIB) theory, which focuses on capturing the most influential yet minimal interactive substructures. In summary, our approach, guided by the IGIB theory, achieves precise substructure extraction within the ISE framework and is encapsulated in the IGIB-ISE}\nExtensive experiments validate the superiority of our model over state-of-the-art baselines across various tasks in terms of accuracy, generalizability, and interpretability."
    },
    {
        "title": "Enhancing Outlier Knowledge for Few-Shot Out-of-Distribution Detection with Extensible Local Prompts",
        "link_suffix": "/forum?id=Ew3VifXaxZ",
        "link": "https://openreview.net/forum?id=Ew3VifXaxZ",
        "pdf_link": "https://openreview.net/pdf?id=Ew3VifXaxZ",
        "keywords": "Out-of-distribution detection, Prompt learning, Vision-language model, Few-shot learning",
        "abstract": "Out-of-Distribution (OOD) detection, aiming to distinguish outliers from known categories, has gained prominence in practical scenarios. Recently, the advent of vision-language models (VLM) has heightened interest in enhancing OOD detection for VLM through few-shot tuning. However, existing methods mainly focus on optimizing global prompts, ignoring refined utilization of local information with regard to outliers. Motivated by this, we freeze global prompts and introduce a novel coarse-to-fine tuning paradigm to emphasize regional enhancement with local prompts. Our method comprises two integral components: global prompt guided negative augmentation and local prompt enhanced regional regularization. The former utilizes frozen, coarse global prompts as guiding cues to incorporate negative augmentation, thereby leveraging local outlier knowledge. The latter employs trainable local prompts and a regional regularization to capture local information effectively, aiding in outlier identification. We also propose regional-related metric to empower the enrichment of OOD detection. Moreover, since our approach explores enhancing local prompts only, it can be seamlessly integrated with trained global prompts during inference to boost the performance. Comprehensive experiments demonstrate the effectiveness and potential of our method. Notably, our method reduces average FPR95 by 5.17% against state-of-the-art method in 4-shot tuning on challenging ImageNet-1k dataset, even outperforming 16-shot results of previous methods. Code will be available upon acceptance."
    },
    {
        "title": "Asymptotic Convergence of SGD in Non-Convex Problems: A Stopping Times Method with Relaxed Step-size Conditions",
        "link_suffix": "/forum?id=ekADgawLgI",
        "link": "https://openreview.net/forum?id=ekADgawLgI",
        "pdf_link": "https://openreview.net/pdf?id=ekADgawLgI",
        "keywords": "stochastic optimization, convergence analyse",
        "abstract": "Stochastic Gradient Descent (SGD) is widely used in machine learning research. In previous research, the convergence analyses of SGD under vanishing step-size settings typically assumed that the step sizes satisfied the Robbins-Monro conditions, which is to say, the sum of the step sizes was infinite, while the sum of the squares of the step sizes was finite. In practical applications, a wider variety of step sizes is often used, but these may not meet the Robbins-Monro step-size conditions, thus lacking theoretical guarantees of convergence. To bridge the gap between theory and practical application, this paper introduces a novel analytical method\u2014the stopping time method based on probability theory\u2014to explore the asymptotic convergence of SGD under more relaxed step-size conditions. In the non-convex setting, we prove that the almost sure convergence of the sequence of iterates generated by SGD when step sizes satisfy (\\sum_{t=1}^{+\\infty} \\epsilon_t = +\\infty) and (\\sum_{t=1}^{+\\infty} \\epsilon_t^p < +\\infty) for some (p > 2). Compared to previous works, our analysis eliminates the need to assume global Lipschitz continuity of the loss function, and it also relaxes the requirement of global boundedness of the high-order moments of the stochastic gradient to local boundedness. Additionally, we prove (L_2) convergence without the need for assuming global boundedness of loss functions or their gradients. The assumptions required for this work are the weakest among studies with the same conclusions, thereby extending the applicability of SGD in various practical scenarios where traditional assumptions may not hold."
    },
    {
        "title": "Program Synthesis Benchmark for Visual Programming in XLogoOnline Environment",
        "link_suffix": "/forum?id=upzyG4wRBr",
        "link": "https://openreview.net/forum?id=upzyG4wRBr",
        "pdf_link": "https://openreview.net/pdf?id=upzyG4wRBr",
        "keywords": "Program Synthesis, Visual Programming, Large Language Models, Multimodal Models, Spatial Reasoning",
        "abstract": "Large language and multimodal models have shown remarkable success on various benchmarks focused on specific skills such as general-purpose programming, natural language understanding, math word problem-solving, and visual question answering. However, it is unclear how well these models perform on tasks that require a combination of these skills. In this paper, we curate a novel program synthesis benchmark based on the real-world tasks in the XLogoOnline visual programming environment. Each task requires a combination of different skills such as spatial planning, basic programming, and logical reasoning. Our evaluation shows that current state-of-the-art models like GPT-4V and Llama3-70B struggle to solve these tasks, achieving only $20$% and $2.35$% success rates, respectively. Next, we develop a fine-tuning pipeline to boost the performance of models by leveraging a large-scale synthetic training dataset with over $80,000$ tasks. Moreover, we showcase how emulator-driven feedback can be used to design a curriculum over training data distribution, through which a fine-tuned Llama3-8B drastically outperforms GPT-4V and Llama3-70B models. Finally, we provide an in-depth failure analysis to understand the limitations of different models. We will publicly release the benchmark for future research on program synthesis in visual programming."
    },
    {
        "title": "Fusing Visual and Textual Cues for Sequential Image Difference Captioning",
        "link_suffix": "/forum?id=rZxwa8JkJW",
        "link": "https://openreview.net/forum?id=rZxwa8JkJW",
        "pdf_link": "https://openreview.net/pdf?id=rZxwa8JkJW",
        "keywords": "Visual Language Models, Change Summarization, Multi-modal Analysis",
        "abstract": "We present FVTC - a technique for image difference captioning that is able to benefit from additional visual and/or textual inputs. FVTC is able to succinctly summarize multiple manipulations that were applied to an image in a sequence. Optionally, it can take several intermediate thumbnails of the image editing sequence as input, as well as coarse machine-generated annotations of the individual manipulations. We demonstrate that the presence of intermediate images and/or auxiliary textual information improves the model's captioning performance. To train FVTC, we introduce METS - a new dataset of image editing sequences, with textual machine annotations of each editorial step and human edit summarization captions after the 5th, 10th and 15th manipulation."
    },
    {
        "title": "QuaDiM: A Conditional Diffusion Model For Quantum State Property Estimation",
        "link_suffix": "/forum?id=P7f55HQtV8",
        "link": "https://openreview.net/forum?id=P7f55HQtV8",
        "pdf_link": "https://openreview.net/pdf?id=P7f55HQtV8",
        "keywords": "quantum, property estimation, machine learning",
        "abstract": "Quantum state property estimation (QPE) is a fundamental challenge in quantum many-body problems in physics and chemistry, involving the prediction of characteristics such as correlation and entanglement entropy through statistical analysis of quantum measurement data. Recent advances in deep learning have provided powerful solutions, predominantly using auto-regressive models. These models generally assume an intrinsic ordering among qubits, aiming to approximate the classical probability distribution through sequential training. However, unlike natural language, the entanglement structure of qubits lacks an inherent ordering, hurting the motivation of such models. In this paper, we introduce a novel, non-autoregressive generative model called \\textbf{QuaDiM}, designed for \\underline{\\textbf{Qua}}ntum state property estimation using \\underline{\\textbf{Di}}ffusion \\underline{\\textbf{M}}odels. QuaDiM progressively denoises Gaussian noise into the distribution corresponding to the quantum state, encouraging equal, unbiased treatment of all qubits. QuaDiM learns to map physical variables to properties of the ground state of the parameterized Hamiltonian during offline training. Afterwards one can sample from the learned distribution conditioned on previously unseen physical variables to collect measurement records and employ post-processing to predict properties of unknown quantum states. We evaluate QuaDiM on large-scale QPE tasks using classically simulated data on the 1D anti-ferromagnetic Heisenberg model with the system size up to 100 qubits. Numerical results demonstrate that \\model outperforms baseline models, particularly auto-regressive approaches, under conditions of limited measurement data during training and reduced sample complexity during inference."
    },
    {
        "title": "Aligning Human Motion Generation with Human Perceptions",
        "link_suffix": "/forum?id=QOHgjY5KDp",
        "link": "https://openreview.net/forum?id=QOHgjY5KDp",
        "pdf_link": "https://openreview.net/pdf?id=QOHgjY5KDp",
        "keywords": "Human Motion Generation",
        "abstract": "Human motion generation is a critical task with a wide spectrum of applications. Achieving high realism in generated motions requires naturalness, smoothness, and plausibility. However, current evaluation metrics often rely on simple heuristics or distribution distances and do not align well with human perceptions. In this work, we propose a data-driven approach to bridge this gap by introducing a large-scale human perceptual evaluation dataset, MotionPercept, and a human motion critic model, MotionCritic, that capture human perceptual preferences. Our critic model offers a more accurate metric for assessing motion quality and could be readily integrated into the motion generation pipeline to enhance generation quality. Extensive experiments demonstrate the effectiveness of our approach in both evaluating and improving the quality of generated human motions by aligning with human perceptions."
    },
    {
        "title": "Centrality-guided Pre-training for Graph",
        "link_suffix": "/forum?id=X8E65IxA73",
        "link": "https://openreview.net/forum?id=X8E65IxA73",
        "pdf_link": "https://openreview.net/pdf?id=X8E65IxA73",
        "keywords": "Graph neural networks, Graph representation, Graph pre-training, Graph centrality",
        "abstract": "Self-supervised learning has shown great potential in learning generalizable representations for graph-structured data. However, existing methods largely focus on improving graph representations based on augmentations, which ignores the alignment between the representation and the structure of graphs. To fill this gap, we propose a Centrality-guided Graph Pre-training (CenPre) framework to integrate the structure information into the representations of nodes based on the centrality in graph theory. The proposed CenPre contains three modules for node representation pre-training and alignment. The node-level structure learning module fuses the fine-grained node importance into node representation based on degree centrality, allowing the aggregation of node representations with equal/similar importance. The graph-level structure learning module characterizes the importance between all nodes in the graph based on eigenvector centrality, enabling the exploitation of graph-level structure similarities/differences when learning node representation. Finally, a representation alignment module aligns the pre-trained node representation using the original one, essentially allowing graph representations to learn structural information without losing their original semantic information, thereby leading to better graph representations. Extensive experiments on a series of real-world datasets demonstrate that the proposed CenPre outperforms the state-of-the-art baselines in node classification and achieves better performance in link prediction and graph classification than the baseline models."
    },
    {
        "title": "EFOk-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation",
        "link_suffix": "/forum?id=K7XiXLfFSP",
        "link": "https://openreview.net/forum?id=K7XiXLfFSP",
        "pdf_link": "https://openreview.net/pdf?id=K7XiXLfFSP",
        "keywords": "complex query answering, knowledge graph",
        "abstract": "To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge needs learning-based methods because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($EFO_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $EFO_{k}$-CQA, with 741 query types for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased and hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\\url{https://anonymous.4open.science/r/EFOK-CQA/README.md}."
    },
    {
        "title": "In-N-Out: Robustness to In-Domain Noise and Out-of-Domain Generalization",
        "link_suffix": "/forum?id=it1zuzw7KF",
        "link": "https://openreview.net/forum?id=it1zuzw7KF",
        "pdf_link": "https://openreview.net/pdf?id=it1zuzw7KF",
        "keywords": "learning with noisy labels, domain generalization",
        "abstract": "Training on real-world data is challenging due to its complex nature, where data is often noisy and may require understanding diverse domains. Methods focused on Learning with Noisy Labels (LNL) may help with noise, but they often assume no domain shifts. In contrast, approaches for Domain Generalization (DG) could help with domain shifts, but these methods either consider label noise but prioritize out-of-domain (OOD) gains at the cost of in-domain (ID) performance, or they try to balance ID and OOD performance, but do not consider label noise at all. Thus, no work explores the combined challenge of balancing ID and OOD performance in the presence of label noise, limiting their impact. We refer to this challenging task as In-N-Out, and this work provides the first exploration of its unique properties.  We find that combining the settings explored in LNL and DG poses new challenges not present in either task alone, and thus, requires direct study. Our findings are based on a study comprised of three real-world datasets and one synthesized noise dataset, where we benchmark a dozen unique methods along with many combinations that are sampled from both the LNL and DG literature. We find that the best method for each setting varies, with older DG and LNL methods often beating the SOTA. A significant challenge we identified stems from unbalanced noise sources and domain-specific sensitivities, which makes using traditional LNL sample selection strategies that often perform well on LNL benchmarks a challenge. While we show this can be mitigated when domain labels are available, we find that LNL and DG regularization methods often perform better."
    },
    {
        "title": "Vision and Language Synergy for Rehearsal Free Continual Learning",
        "link_suffix": "/forum?id=9aZ2ixiYGd",
        "link": "https://openreview.net/forum?id=9aZ2ixiYGd",
        "pdf_link": "https://openreview.net/pdf?id=9aZ2ixiYGd",
        "keywords": "continual learning, prompt dilemma, language descriptors, prompt generator, catasthropic forgetting",
        "abstract": "The prompt-based approach has demonstrated its success for continual learning problems. However, it still suffers from catastrophic forgetting due to inter-task vector similarity and unfitted new components of previously learned tasks. On the other hand, the language-guided approach falls short of its full potential due to minimum utilized knowledge and participation in the prompt tuning process. To correct this problem, we propose a novel prompt-based structure and algorithm that incorporate 4 key concepts (1) language as input for prompt generation (2) task-wise generators (3) limiting matching descriptors search space via soft task-id prediction (4) generated prompt as auxiliary data. Our experimental analysis shows the superiority of our method to existing SOTAs in CIFAR100, ImageNet-R, and CUB datasets with significant margins i.e. up to 30$%$ final average accuracy, 24$%$ cumulative average accuracy, 8$%$ final forgetting measure, and 7$%$ cumulative forgetting measure. Our historical analysis confirms our method successfully maintains the stability-plasticity trade-off in every task. Our robustness analysis shows the proposed method consistently achieves high performances in various prompt lengths, layer depths, and number of generators per task compared to the SOTAs. We provide a comprehensive theoretical analysis, and complete numerical results in appendix sections. The source code of our method is available in \\url{https://anonymous.4open.science/r/xt124j05}for further study and reproducibility."
    },
    {
        "title": "LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior Accuracy Preservation",
        "link_suffix": "/forum?id=PHrqpxUczG",
        "link": "https://openreview.net/forum?id=PHrqpxUczG",
        "pdf_link": "https://openreview.net/pdf?id=PHrqpxUczG",
        "keywords": "Large Language Models, KV Cache, Memory Efficiency, Quantization",
        "abstract": "We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques. LogQuant integrates effortlessly with popular inference frameworks like Python\u2019s \\texttt{transformers} library and will be made open-source upon publication."
    },
    {
        "title": "Tackling Feature and Sample Heterogeneity in Decentralized Multi-Task Learning: A Sheaf-Theoretic Approach",
        "link_suffix": "/forum?id=b77ML9nALL",
        "link": "https://openreview.net/forum?id=b77ML9nALL",
        "pdf_link": "https://openreview.net/pdf?id=b77ML9nALL",
        "keywords": "federated multi-task learning, decentralized learning, communication-efficient, Sheaf theory",
        "abstract": "Federated multi-task learning (FMTL) aims to simultaneously learn multiple related tasks across clients without sharing sensitive raw data. However, in the decentralized setting, existing FMTL frameworks are limited in their ability to capture complex task relationships and handle feature and sample heterogeneity across clients. To address these challenges, we introduce a novel sheaf-theoretic-based approach for FMTL. By representing client relationships using cellular sheaves, our framework can flexibly model interactions between heterogeneous client models. We formulate the sheaf-based FMTL optimization problem using sheaf Laplacian regularization and propose the Sheaf-FMTL algorithm to solve it. We show that the proposed framework provides a unified view encompassing many existing federated learning (FL) and FMTL approaches. Furthermore, we prove that our proposed algorithm, Sheaf-FMTL, achieves a sublinear convergence rate in line with state-of-the-art decentralized FMTL algorithms. Extensive experiments demonstrate that Sheaf-FMTL exhibits communication savings by sending significantly fewer bits compared to decentralized FMTL baselines."
    },
    {
        "title": "MagicDrive3D: Controllable 3D Generation for Any-View Rendering in Street Scenes",
        "link_suffix": "/forum?id=0uFTqvQhML",
        "link": "https://openreview.net/forum?id=0uFTqvQhML",
        "pdf_link": "https://openreview.net/pdf?id=0uFTqvQhML",
        "keywords": "controllable 3D scene generation, 3D gaussian splatting, autonomous driving",
        "abstract": "While controllable generative models for images and videos have achieved remarkable success, high-quality models for 3D scenes, particularly in unbounded scenarios like autonomous driving, remain underdeveloped due to high data acquisition costs. In this paper, we introduce MagicDrive3D, a novel pipeline for controllable 3D street scene generation that supports multi-condition control, including BEV maps, 3D objects, and text descriptions. Unlike previous methods that reconstruct before training the generative models, MagicDrive3D first trains a video generation model and then reconstructs from the generated data. This innovative approach enables easily controllable generation and static scene acquisition, resulting in high-quality scene reconstruction. To address the minor errors in generated content, we propose deformable Gaussian splatting with monocular depth initialization and appearance modeling to manage exposure discrepancies across viewpoints. Validated on the nuScenes dataset, MagicDrive3D generates diverse, high-quality 3D driving scenes that support any-view rendering and enhance downstream tasks like BEV segmentation. Our results demonstrate the framework's superior performance, showcasing its transformative potential for autonomous driving simulation and beyond."
    },
    {
        "title": "DockedAC: Empowering Deep Learning Models With 3D Protein-ligand Data For Activity Cliff Analysis",
        "link_suffix": "/forum?id=HBbbhAZuia",
        "link": "https://openreview.net/forum?id=HBbbhAZuia",
        "pdf_link": "https://openreview.net/pdf?id=HBbbhAZuia",
        "keywords": "Activity cliff prediction, Molecular property prediction, AI-aided drug discovery",
        "abstract": "Artificial intelligence has become a crucial tool in drug discovery, excelling in tasks such as molecular property prediction. An activity cliff, which refers to a minor structural modification to a molecule resulting in a large change in its biological activity, poses a challenge in predictive modeling. The activity cliff depends on the interaction between the target and the ligand, which is however largely overlooked by previous ligand-centric studies. In this paper, we introduce DockedAC, a new dataset incorporating the protein target and target-ligand 3D complex structure information for studying the problem of activity cliffs. By matching protein binding information and ligand bioactivity, we employ molecular docking to generate the complex structure for each activity value. The DockedAC dataset contains 82,836 activity data on 52 protein targets with activity cliff annotations, which serves as the first step towards activity cliff research with large-scale 3D complex structures. We benchmark the dataset with traditional machine learning and deep learning approaches."
    },
    {
        "title": "Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders",
        "link_suffix": "/forum?id=aKcd7ImG5e",
        "link": "https://openreview.net/forum?id=aKcd7ImG5e",
        "pdf_link": "https://openreview.net/pdf?id=aKcd7ImG5e",
        "keywords": "Time series, Anomaly detection",
        "abstract": "Time series anomaly detection plays a vital role in a wide range of applications. Existing methods require training one specific model for each dataset, which exhibits limited generalization capability across different target datasets, hindering anomaly detection performance in various scenarios with scarce training data. Aiming at this problem, we propose constructing a general time series anomaly detection model, which is pre-trained on extensive multi-domain datasets and can subsequently apply to a multitude of downstream scenarios. The significant divergence of time series data across different domains presents two primary challenges in building such a general model: (1) meeting the diverse requirements of appropriate information bottlenecks tailored to different datasets in one unified model, and (2) enabling distinguishment between multiple normal and abnormal patterns, both are crucial for effective anomaly detection in various target scenarios. To tackle these two challenges, we propose a General time series anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders (DADA), which enables flexible selection of bottlenecks based on different data and explicitly enhances clear differentiation between normal and abnormal series. We conduct extensive experiments on nine target datasets from different domains. After pre-training on multi-domain data, DADA, serving as a zero-shot anomaly detector for these datasets, still achieves competitive or even superior results compared to those models tailored to each specific dataset."
    },
    {
        "title": "SuperCAT: Super Resolution and Cross Semantic Attribute-guided Transformer based Feature Refinement for Zero-Shot Remote Sensing Scene Classification",
        "link_suffix": "/forum?id=NpBhYnUgFU",
        "link": "https://openreview.net/forum?id=NpBhYnUgFU",
        "pdf_link": "https://openreview.net/pdf?id=NpBhYnUgFU",
        "keywords": "Scene classification, remote sensing images, zero-shot learning, Transformer",
        "abstract": "Zero-shot learning becomes challenging in classifying scenes of unseen classes due to the typical characteristics of remote-sensing images. The intricate variations and non-uniform spatial resolutions among the scenes of remote sensing images further complicate achieving discriminative semantic knowledge. To tackle these issues, we propose a SuperCAT framework comprising a super-resolution module, a cross-semantic attribute-guided Transformer (CAT), feature-generating models, and a feature refinement (FR) module for the zero-shot scene classification in remote sensing images. First, we leverage the semantic attributes for all the classes of four benchmark remote sensing scene classification datasets to explore semantic knowledge using super-resolution effectively. Then, the semantic attribute to visual Transformer (SAVT) and visual to semantic attribute Transformer (VSAT) modules in CAT learn to obtain attribute-based visual features and visual-based attribute features, respectively. The SAVT and VSAT modules collaboratively learn and teach each other using the feature-level and prediction-level semantic collaborative losses. The feature-generating models map semantic vectors to the visual features of remote-sensing images. The FR module incorporates triplet center margin loss and semantic loop consistency loss functions to capture class-related and semantically-related discriminative features for achieving intra-class closeness and inter-class distinctiveness. Our extensive experiments on four benchmark remote sensing image scene classification datasets demonstrate the efficacy of SuperCAT over state-of-the-art approaches. The code can be accessed athttps://github.com/ZSL-RSI-SC/SuperCAT."
    },
    {
        "title": "SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction",
        "link_suffix": "/forum?id=ixMBnOhFGd",
        "link": "https://openreview.net/forum?id=ixMBnOhFGd",
        "pdf_link": "https://openreview.net/pdf?id=ixMBnOhFGd",
        "keywords": "information retrieval, metric",
        "abstract": "Large language models (LLM) can leverage retrieved external knowledge to improve the generation performance, a paradigm called retrieval-augmented generation (RAG). Current automatic evaluation of RAG either treats retrieval and generation as a whole or independently evaluates retrievers with traditional retrieval metrics regardless of generator, bringing a gap to understand retrieval utility with a systematic view. In this work, we propose an automatic method to measure the retrieval quality from the lens of information gains of the RAG system. Specifically, we introduce semantic perplexity SePer to measure LLM's uncertainty about the ground-truth, and quantify retrieval utilities as the uncertainty reduction of semantic perplexity after retrieval. With experiments from various RAG scenarios, we demonstrate that SePer not only aligns well with human preferences but also provides more fine-grained evaluation of context quality and retrieval utility efficiently and robustly."
    },
    {
        "title": "VISCON: Identifying and Benchmarking Vision Hallucination for Large Vision-Language Model",
        "link_suffix": "/forum?id=cjpTu0Op5t",
        "link": "https://openreview.net/forum?id=cjpTu0Op5t",
        "pdf_link": "https://openreview.net/pdf?id=cjpTu0Op5t",
        "keywords": "hallucination, vision hallucination, large vision-language model, large language model",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated exceptional capabilities in a variety of vision-language tasks, but suffer from \"vision hallucinations\" - a tendency generating text inconsistent with the image. This issue hampers their practical use in real-world applications.\n    To effectively evaluate and detect these hallucinations, we introduce VISCON (VISual Concept cONsistency), a benchmark framework comprising a benchmark image dataset and quantitative evaluation pipelines to assess vision hallucinations in LVLMs. VISCON extends beyond previous hallucination metrics by offering: a) diverse image styles across multiple visual domains, b) evaluation of a broader range of visual concepts, including objects, attributes, and relationships, and c) high annotation density from detailed scene-graph annotations to reduce false negatives. These improvements enable comprehensive analysis of hallucinations related to both domain shifts and concept types and offer more accurate hallucination evaluation. \nTo detect vision hallucinations, we propose two innovative evaluation pipelines within VISCON: an Earth Mover's Distance (EMD)-based pipeline and an \"Evaluate-By-Edit\" pipeline. The EMD-based pipeline measures the distributional similarity between the reference visual concepts and those mentioned by LVLMs, robust against vocabulary shifts between annotations and natural language responses. The Through extensive experiments on six leading LVLMs, VISCON reveals crucial insights into the nature of vision hallucinations. Our findings indicate that factors such as image domain shifts, complexity of visual concepts and model response length significantly influence the occurrence of hallucinations in LVLM responses. Additionally, human evaluations confirm that VISCON aligns with human preferences better than established hallucination metrics. \"Evaluate-By-Edit\" focuses on the edit distance between the original LVLM response and a hallucination-reduced version revised according to the rich visual concept annotations, providing an interpretable analysis of hallucinated content. Importantly, our method directly evaluates captioning responses, unlike previous metrics that query the existence of individual visual concepts. This approach is more challenging, as it requires models to handle multiple concepts simultaneously, providing better discrimination of LVLM performance.\nThrough extensive experiments on six leading LVLMs, VISCON reveals crucial insights into the nature of vision hallucinations. Our findings indicate that factors such as image domain shifts, complexity of visual concepts and model response length significantly influence the occurrence of hallucinations in LVLM responses. Additionally, human evaluations confirm that VISCON aligns with human preferences better than established hallucination metrics."
    },
    {
        "title": "Channel-aware Contrastive Conditional Diffusion for Multivariate Probabilistic Time Series Forecasting",
        "link_suffix": "/forum?id=NV5p50EkT6",
        "link": "https://openreview.net/forum?id=NV5p50EkT6",
        "pdf_link": "https://openreview.net/pdf?id=NV5p50EkT6",
        "keywords": "diffusion models, contrastive learning, time series forecasting",
        "abstract": "Forecasting faithful trajectories of multivariate time series from practical scopes is essential for reasonable decision-making. Recent methods majorly tailor generative conditional diffusion models to estimate the target temporal predictive distribution. However, it remains an obstacle to enhance the exploitation efficiency of given implicit temporal predictive information to bolster conditional diffusion learning. To this end, we propose a generic channel-aware contrastive conditional diffusion model termed CCDM to achieve desirable multivariate probabilistic forecasting, obviating the need for curated temporal conditioning inductive biases. In detail, we first design a channel-centric conditional denoising network to manage intra-variate variations and cross-variate correlations, which can lead to scalability on diverse prediction horizons and channel numbers. Then, we devise an ad-hoc denoising-based temporal contrastive learning to explicitly amplify the predictive mutual information between past observations and future forecasts. It can coherently complement naive step-wise denoising diffusion training and improve the forecasting accuracy and generality on unknown test time series. Besides, we offer theoretic insights on the benefits of such auxiliary contrastive training refinement from both neural mutual information and temporal distribution generalization aspects. The proposed CCDM can exhibit superior forecasting capability compared to current state-of-the-art diffusion forecasters over a comprehensive benchmark, with best MSE and CRPS outcomes on 66.67% and 83.33% cases."
    },
    {
        "title": "Finding Shared Decodable Concepts and their Negations in the Brain",
        "link_suffix": "/forum?id=L07zWidgdW",
        "link": "https://openreview.net/forum?id=L07zWidgdW",
        "pdf_link": "https://openreview.net/pdf?id=L07zWidgdW",
        "keywords": "fMRI, decoding, computer vision, neuroscience",
        "abstract": "Prior work has offered evidence for functional localization in the brain; different anatomical regions preferentially activate for certain types of visual input. For example, the fusiform face area preferentially activates for visual stimuli that include a face. However, the spectrum of visual semantics is extensive, and only a few semantically-tuned patches of cortex have so far been identified in the human brain. Using a multimodal (natural language and image) neural network architecture (CLIP, \\cite{CLIP}, we train a highly accurate contrastive model that maps brain responses during naturalistic image viewing to CLIP embeddings. We then use a novel adaptation of the DBSCAN clustering algorithm to cluster the parameters of these participant-specific contrastive models. This reveals what we call Shared Decodable Concepts (SDCs): clusters in CLIP space that are decodable from common sets of voxels across multiple participants.Examining the images most and least associated with each SDC cluster gives us additional insight into the semantic properties of each SDC. We note SDCs for previously reported visual features (e.g. orientation tuning in early visual cortex) as well as visual semantic concepts such as faces, places and bodies. In cases where our method finds multiple clusters for a visuo-semantic concept, the least associated images allow us to dissociate between confounding factors. For example, we discovered two clusters of food images, one driven by color, the other by shape. We also uncover previously unreported areas with visuo-semantic sensitivity such as regions of extrastriate body area (EBA) tuned for legs/hands and sensitivity to numerosity in right intraparietal sulcus, sensitivity associated with visual perspective (close/far) and more. Thus, our contrastive-learning methodology better characterizes new and existing visuo-semantic representations in the brain by leveraging multimodal neural network representations and a novel adaptation of clustering algorithms."
    },
    {
        "title": "A Conditional Independence Test in the Presence of Discretization",
        "link_suffix": "/forum?id=gqbbL7k8BF",
        "link": "https://openreview.net/forum?id=gqbbL7k8BF",
        "pdf_link": "https://openreview.net/pdf?id=gqbbL7k8BF",
        "keywords": "Conditional Independence Test, Discretization, Causal Discovery",
        "abstract": "Testing conditional independence (CI) has many important applications, such as Bayesian network learning and causal discovery. Although several approaches have been developed for learning CI structures for observed variables, those existing methods generally fail to work when the variables of interest can not be directly observed and only discretized values of those variables are available. For example, if $X_1$, $\\tilde{X}_2$ and $X_3$ are the observed variables, where $\\tilde{X}_2$ is a discretization of the latent variable $X_2$, applying the existing methods to the observations of $X_1$, $\\tilde{X}_2$ and $X_3$ would lead to a false conclusion about the underlying CI of variables $X_1$, $X_2$ and $X_3$.\nMotivated by this, we propose a CI test specifically designed to accommodate the presence of discretization. To achieve this, a bridge equation and nodewise regression are used to recover the precision coefficients reflecting the conditional dependence of the latent continuous variables under the nonparanormal model. An appropriate test statistic has been proposed, and its asymptotic distribution under the null hypothesis of CI has been derived.\nTheoretical analysis, along with empirical validation on various datasets, rigorously demonstrates the effectiveness of our testing methods."
    },
    {
        "title": "PerSense: Personalized Instance Segmentation in Dense Images",
        "link_suffix": "/forum?id=caE5faFVT1",
        "link": "https://openreview.net/forum?id=caE5faFVT1",
        "pdf_link": "https://openreview.net/pdf?id=caE5faFVT1",
        "keywords": "dense image segmentation, personalized instance segmentation, training-free, one-shot segmentation",
        "abstract": "Leveraging large-scale pre-training, vision foundational models showcase notable performance benefits. Recent segmentation algorithms for natural scenes have advanced significantly. However, existing models still struggle to automatically segment personalized instances in dense and crowded scenarios, where severe occlusions, scale variations, and background clutter pose a challenge to accurately delineate densely packed instances of the target object. To address this, we proposePerSense, an end-to-end, training-free, and model-agnostic one-shot framework forPersonalized instanceSegmentation in denseimages. Towards developing this framework, we make the following core contributions.(a)We develop a new baseline capable of automatically generating instance-level point prompts via proposing a novel Instance Detection Module (IDM) that leverages density maps, encapsulating spatial distribution of objects in an image.(b)To mitigate false positives within generated point prompts, we design Point Prompt Selection Module (PPSM). Both IDM and PPSM transform density maps into personalized precise point prompts for instance-level segmentation and offer a seamless integration in our model-agnostic framework.(c)We introduce a feedback mechanism which enables PerSense to improve the accuracy of density maps by automating the exemplar selection process for density map generation.(d)To promote algorithmic advances and effective tools for this relatively underexplored task, we introduce PerSense-D, a diverse dataset exclusive to personalized instance segmentation in dense images. Our extensive experiments establish PerSense superiority in dense scenarios by achieving an mIoU of71.61%on PerSense-D, outperforming recent SOTA models by significant margins of+47.16%,+42.27%,+8.83%, and+5.69%. Additionally, our qualitative findings demonstrate the adaptability of our framework to images captured in-the-wild."
    }
]