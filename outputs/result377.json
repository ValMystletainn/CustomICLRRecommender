[{"title": "OmniContrast: Vision-Language-Interleaved Contrast from Pixels All at once", "link_suffix": "/forum?id=3g7HuQ8avZ", "link": "https://openreview.net/forum?id=3g7HuQ8avZ", "pdf_link": "https://openreview.net/pdf?id=3g7HuQ8avZ", "keywords": "vision-language contrastive learning", "abstract": "In this work, we present OmniContrast, a unified contrastive learning model tailored for vision, language, and vision-language-interleaved understanding within multi-modal web documents. Unlike traditional image-caption data with clear vision-language correspondence, we explore a new contrastive fashion on maximizing the similarity between consecutive snippets sampled from image-text interleaved web documents. Moreover, to enable CLIP to handle long-form text and image-text interleaved content from web documents, OmniContrast unifies all modalities into pixel space, where text is rendered visually. This unification simplifies the processing and representation of diverse multi-modal inputs, enabling a single vision model to process any modality. To evaluate the omni-modality understanding of OmniContrast, we design three consecutive information retrieval benchmarks AnyCIR, SeqCIR, and CSR. Extensive experimental results demonstrate that OmniContrast achieves superior or competitive omni-modality understanding performance to existing standard CLIP models trained on image-text pairs. This highlights the potential of multi-modal web documents as a rich and valuable resource for advancing vision-language learning.", "title_embedding_index": 18800, "title_abs_embedding_index": 18825}, {"title": "Lost in the Averages: Evaluating record-specific MIAs against Machine Learning models", "link_suffix": "/forum?id=Nx8lVqyKeZ", "link": "https://openreview.net/forum?id=Nx8lVqyKeZ", "pdf_link": "https://openreview.net/pdf?id=Nx8lVqyKeZ", "keywords": "Privacy, Membership inference attacks", "abstract": "Record-specific Membership Inference Attacks (MIAs) are widely used to evaluate the propensity of a machine learning (ML) model to memorize an individual record and the privacy risk its release therefore poses. Record-specific MIAs are currently evaluated the same way ML models are: on a test set of models trained on data samples that were not seen during training ($D_{eval}$). A recent large body of literature has however shown that the main risk often comes from outliers, records that are statistically different from the rest of the dataset. In this work, we argue that the traditional evaluation setup for record-specific MIAs, which includes dataset sampling as a source of randomness, incorrectly captures the privacy risk. Indeed, what is an outlier is highly specific to particular data samples, and a record that is an outlier in the training dataset will not necessarily be one in the randomly sampled test datasets. We propose to use model randomness as the only source of randomness to evaluate record-level MIAs, a setup we callmodel-seeded. Across 10 combinations of models, datasets, and attacks for predictive and generative AI, we show the per-record risk estimates given by the traditional evaluation setup to substantially differ from ones given by themodel-seededsetup which properly account for the increased risk posed by outliers. We show that across setups the traditional evaluation method leads to a substantial number of records to be incorrectly classified as low risk, emphasizing the inadequacy of the current setup to capture the record-level risk. We then a) provide evidence that the traditional setup is an average--across datasets--of themodel-seededrisk, validating our use of model randomness to create evaluation models and b) show how relying on the traditional setup might conceal the existence of stronger attacks. The traditional setup would indeed strongly underestimate the risk posed by the strong Differential Privacy adversary. We believe our results to convincingly show the practice of randomizing datasets to evaluate record-specific MIAs to be incorrect. We then argue that relying on model randomness, an setup we callmodel-seededevaluation, better captures the risk posed by outliers and should be used moving forward to evaluate record-level MIAs against machine learning models, both predictive and generative.", "title_embedding_index": 18801, "title_abs_embedding_index": 18826}, {"title": "NEMESIS\\Jailbreaking LLMs with Chain of Thoughts Approach", "link_suffix": "/forum?id=5kMwiMnUip", "link": "https://openreview.net/forum?id=5kMwiMnUip", "pdf_link": "https://openreview.net/pdf?id=5kMwiMnUip", "keywords": "LLM, Jailbreaking, Chain-of-thought reasoning, Reinforcement learning, LLM security protocols, Adversarial attacks, Defense mechanisms, LlamaGuard, Multishot Jailbreaking, Fine Tuning", "abstract": "Large Language Models (LLMs) are increasingly being deployed across various\napplications, making the need for robust security measures crucial. This paper\nexplores multiple methods for jailbreaking these models, bypassing their secu-\nrity protocols. By examining five distinct approaches\u2014Multishot Jailbreaking,\nthe Mirror Dimension Approach, the Cipher Method, the \u201dYou are Answering the\nWrong Question\u201d Method, and the Textbook Jailbreaking Method\u2014we highlight\nthe vulnerabilities in current LLMs and emphasize the importance of fine-tuning\nand secure guardrails. Our study primarily employs chain-of-thought reasoning,\nwhich can be further enhanced through reinforcement learning techniques. Fur-\nthermore, we propose that our findings can serve as a benchmark against emerging\nsecurity measures such as LlamaGuard, providing a comprehensive evaluation of\nLLM defenses. Our findings demonstrate the effectiveness of these methods and\nsuggest directions for future work in enhancing LLM security. This research un-\nderscores the ongoing challenges in balancing LLM capabilities with robust safe-\nguards against potential misuse or manipulation.", "title_embedding_index": 18802, "title_abs_embedding_index": 18827}, {"title": "Post-hoc Discriminator Guidance For Data-efficient Image Generation Via Annealing Langevin Dynamics", "link_suffix": "/forum?id=JJH7m9v4tv", "link": "https://openreview.net/forum?id=JJH7m9v4tv", "pdf_link": "https://openreview.net/pdf?id=JJH7m9v4tv", "keywords": "post-hoc discriminator guidance, GANs, data-efficient image generation, annealing Langevin dynamics", "abstract": "The proposed method, post-hoc discriminator guidance (PDG) aims to take an alternate route for Nash non-equilibrium issue in GANs' training. This method introduces an additional discriminator that gives explicit supervision with regard to gradient of density ratio $\\nabla_{x} \\log_{}{\\frac{p_{r}(x)}{p_{f}(x)}}$ between real and fake probability density function, steering the sample path towards more realistic regions in a post-hoc way. We train the discriminator after adversarial optimization, making post-hoc discriminator training stable and fast to converge. In generation process, annealing Langevin dynamics sampling with density ratio score reduces the Kullback-Leibler divergence between the true and generated samples. Given an optimal discriminator, the method can improve the sampling quality of various off-the-shelf models on the web without retraining required. Extensive experiments validate the advancements and effectiveness of PDG on content-varying data-limited datasets.", "title_embedding_index": 18803, "title_abs_embedding_index": 18828}, {"title": "Topo-Field: Topometric mapping with Brain-inspired Hierarchical Layout-Object-Position Fields", "link_suffix": "/forum?id=2IBdk8cUdC", "link": "https://openreview.net/forum?id=2IBdk8cUdC", "pdf_link": "https://openreview.net/pdf?id=2IBdk8cUdC", "keywords": "Robotic scene understanding, Neural scene representation, Hierarchical representation, Topometric map", "abstract": "Mobile robots require comprehensive scene understanding to operate effectively in diverse environments, enriched with contextual information such as layouts, objects, and their relationships. While advancements like Neural Radiance Fields (NeRF) offer high-fidelity 3D reconstructions, they are computationally intensive and often lack efficient representations of traversable spaces essential for planning and navigation. In contrast, topological maps generated by LiDAR or visual SLAM methods are computationally efficient but lack the semantic richness necessary for a more complete understanding of the environment.\nInspired by neuroscientific studies on spatial cognition, particularly the role of postrhinal cortex (POR) neurons that are strongly tuned to spatial layouts over scene content, this work introduces Topo-Field, a framework that integrates Layout-Object-Position (LOP) associations into a neural field and constructs a topometric map from this learned representation. LOP associations are modeled by explicitly encoding object and layout information, while a Large Foundation Model (LFM) technique allows for efficient training without extensive annotations. The topometric map is then constructed by querying the learned NeRF, offering both semantic richness and computational efficiency.\nEmpirical evaluations in multi-room apartment environments demonstrate the effectiveness of Topo-Field in tasks such as position attribute inference, query localization, and topometric planning, successfully bridging the gap between high-fidelity scene understanding and efficient robotic navigation.", "title_embedding_index": 18804, "title_abs_embedding_index": 18829}, {"title": "Deep Progressive Search for Electromagnetic Structure Design Under Limited Evaluation Budgets", "link_suffix": "/forum?id=Yisupq2CgQ", "link": "https://openreview.net/forum?id=Yisupq2CgQ", "pdf_link": "https://openreview.net/pdf?id=Yisupq2CgQ", "keywords": "Electromagnetic Structure, Surrogate Model, Tree Search", "abstract": "Electromagnetic structure (EMS) design aims to optimize a material distribution, e.g., metals over a printed circuit board, which is crucial for antenna and meta-material. This task, however, is inherently a highly non-convex problem with no explicit objective function, making it extremely challenging to solve. The most common approach to addressing this problem relies on evolutionary algorithms (e.g., Genetic Algorithm), where candidate structures are evaluated through electromagnetic simulation using specialized software. However, these methods struggle with inefficiency, especially when dealing with large structural design space and time-consuming simulations. \nTo address this, we propose a Deep Progressive Search method called DPS, which leverages a Deep Neural Network (DNN) as a surrogate model to identify a satisfactory structure within a limited simulation budget. Specifically, we develop a tree-search-based design space control strategy that models the design space as a tree and incrementally refines it through node expansions, enabling adaptive exploration of more complex regions while leveraging insights from simpler subspaces. Moreover, we introduce a consistency-based sample selection strategy to balance exploration and exploitation. Experiments on two real-world engineering tasks, i.e., Dual-layer Frequency Selective Surface and High-gain Antenna show the effectiveness of the proposed DPS in terms of efficiency under limited evaluation budgets.", "title_embedding_index": 18805, "title_abs_embedding_index": 18830}, {"title": "Applications of Modular Co-Design  for De Novo 3D Molecule Generation", "link_suffix": "/forum?id=9UoBuhVNh6", "link": "https://openreview.net/forum?id=9UoBuhVNh6", "pdf_link": "https://openreview.net/pdf?id=9UoBuhVNh6", "keywords": "molecule generation, diffusion, flow matching, transformer", "abstract": "De novo 3D molecule generation is a pivotal task in drug discovery. However, many\nrecent geometric generative models struggle to produce high-quality 3D structures,\neven if they maintain 2D validity and topological stability. To tackle this issue\nand enhance the learning of effective molecular generation dynamics, we present\nMegalodon\u2013a family of simple and scalable transformer models. These models\nare enhanced with basic equivariant layers and trained using a joint continuous\nand discrete denoising co-design objective. We assess Megalodon\u2019s performance\non established molecule generation benchmarks and introduce new 3D structure\nbenchmarks that evaluate a model\u2019s capability to generate realistic molecular\nstructures, particularly focusing on energetics. We show that Megalodon achieves\nstate-of-the-art results in 3D molecule generation, conditional structure generation,\nand structure energy benchmarks using diffusion and flow matching. Furthermore,\nwe demonstrate that scaling Megalodon produces up to 49x more valid molecules\nat large sizes and 2-10x lower energy compared to the prior best generative models.", "title_embedding_index": 18806, "title_abs_embedding_index": 18831}, {"title": "Transformers are Universal In-context Learners", "link_suffix": "/forum?id=6S4WQD1LZR", "link": "https://openreview.net/forum?id=6S4WQD1LZR", "pdf_link": "https://openreview.net/pdf?id=6S4WQD1LZR", "keywords": "Transfomer, In-context learning, Universal approximation, Wasserstein, Optimal transport", "abstract": "Transformers are deep architectures that define ``in-context mappings'' which enable predicting new tokens based on a given set of tokens (such as a prompt in NLP applications or a set of patches for a vision transformer). In this work, we study in particular the ability of these architectures to handle an arbitrarily large number of context tokens. To mathematically, uniformly address their expressivity, we consider the case that the mappings are conditioned on a context represented by a probability distribution of tokens which becomes discrete for a finite number of these. The relevant notion of smoothness then corresponds to continuity in terms of the Wasserstein distance between these contexts. We demonstrate that deep transformers are universal and can approximate continuous in-context mappings to arbitrary precision, uniformly over compact token domains. A key aspect of our results, compared to existing findings, is that for a fixed precision, a single transformer can operate on an arbitrary (even infinite) number of tokens. Additionally, it operates with a fixed embedding dimension of tokens (this dimension does not increase with precision) and a fixed number of heads (proportional to the dimension). The use of MLPs between multi-head attention layers is also explicitly controlled. We consider both unmasked attentions (as used for the vision transformer) and masked causal attentions (as used for NLP and time series applications). We tackle the causal setting leveraging a space-time lifting to analyze causal attention as a mapping over probability distributions of tokens.", "title_embedding_index": 18807, "title_abs_embedding_index": 18832}, {"title": "LLM Jailbreak Detection for (Almost) Free!", "link_suffix": "/forum?id=RC5x3OkywQ", "link": "https://openreview.net/forum?id=RC5x3OkywQ", "pdf_link": "https://openreview.net/pdf?id=RC5x3OkywQ", "keywords": "Jailbreak Attack, Large Language Model", "abstract": "Large language models (LLMs) enhance security through alignment when widely used, but remain susceptible to jailbreak attacks capable of producing inappropriate content. Jailbreak detection methods show promise in mitigating jailbreak attacks through the assistance of other models or multiple model inferences. However, existing methods entail significant computational costs. In this paper, we present a finding that the difference in output distributions between jailbreak and benign prompts can be employed for detecting jailbreak prompts. Based on this finding, we propose a Free Jailbreak Detection (FJD) method which incorporates manual instructions into the input and scales the logits by temperature to distinguish between jailbreak and benign prompts through the confidence of the first token. Furthermore, we enhance the detection performance of FJD through the integration of virtual instruction learning (FJD-LI). Extensive experiments on aligned large models demonstrated that our FJD outperforms baseline methods in jailbreak detection accuracy with almost no additional computational costs.", "title_embedding_index": 18808, "title_abs_embedding_index": 18833}, {"title": "SAG: Style-Aligned Generation via Language Model Collaboration", "link_suffix": "/forum?id=KXLbcIEurw", "link": "https://openreview.net/forum?id=KXLbcIEurw", "pdf_link": "https://openreview.net/pdf?id=KXLbcIEurw", "keywords": "stylish content generation, large language model", "abstract": "Large language models (LLMs) have increased the demand for personalized and stylish content generation. However, closed-source models like GPT-4 present limitations in optimization opportunities, while the substantial training costs and inflexibility of open-source alternatives, such as Qwen-72B, pose considerable challenges. Conversely, small language models (SLMs) struggle with understanding complex instructions and transferring learned capabilities to new contexts, often exhibiting more pronounced limitations. In this paper, we present a novel collaborative training framework that leverages the strengths of both LLMs and SLMs for style article generation, surpassing the performance of either model alone. We freeze the LLMs to harness their robust instruction-following capabilities and subsequently apply supervised fine-tuning on the SLM using style-specific data. Additionally, we introduce a self-improvement method to enhance style consistency. Our new benchmark, NoteBench, thoroughly evaluates style-aligned generation. Extensive experiments show that our approach achieves state-of-the-art performance, with improvements of 0.78 in ROUGE-L and 0.55 in BLEU-4 scores compared to GPT-4, while also maintaining a low hallucination rate in terms of factual accuracy and faithfulness.", "title_embedding_index": 18809, "title_abs_embedding_index": 18834}, {"title": "E5-V: Universal Embeddings with Multimodal Large Language Models", "link_suffix": "/forum?id=rD6LQagatR", "link": "https://openreview.net/forum?id=rD6LQagatR", "pdf_link": "https://openreview.net/pdf?id=rD6LQagatR", "keywords": "Multimodal Large Language Models, Multimodal Learning, Representation Learning", "abstract": "Multimodal large language models (MLLMs) have shown promising advancements in general visual and language understanding. However, the representation of multimodal information using MLLMs remains largely unexplored. In this work, we introduce a new framework, E5-V, designed to adapt MLLMs for achieving universal multimodal embeddings. Our findings highlight the significant potential of MLLMs in representing multimodal inputs compared to previous approaches. By leveraging MLLMs with prompts, E5-V effectively bridges the modality gap between different types of inputs, demonstrating strong performance in multimodal embeddings even without fine-tuning. We propose a single modality training approach for E5-V, where the model is trained exclusively on text pairs. This method demonstrates significant improvements over traditional multimodal training on image-text pairs, while reducing training costs by approximately 95%. Additionally, this approach eliminates the need for costly multimodal training data collection. Extensive experiments across four types of tasks demonstrate the effectiveness of E5-V. As a universal multimodal model, E5-V not only achieves but often surpasses state-of-the-art performance in each task, despite being trained on a single modality.", "title_embedding_index": 18810, "title_abs_embedding_index": 18835}, {"title": "Leveraging free energy in pretraining model selection for improved fine-tuning", "link_suffix": "/forum?id=FFwoaUFBVC", "link": "https://openreview.net/forum?id=FFwoaUFBVC", "pdf_link": "https://openreview.net/pdf?id=FFwoaUFBVC", "keywords": "transfer learning, free energy, Bayesian model selection, efficient fine-tuning, adaptation", "abstract": "Recent advances in artificial intelligence have been fueled by the development of foundation models such as BERT, GPT, T5, and Vision Transformers. These models are first pretrained on vast and diverse datasets and then adapted to specific downstream tasks, often with significantly less data. However, the mechanisms behind the success of this ubiquitous pretrain-then-adapt paradigm remain underexplored, particularly the characteristics of pretraining checkpoints that lend themselves to good downstream adaptation. We introduce a Bayesian model selection criterion, called the downstream free energy, which quantifies a checkpoint's adaptability by measuring the concentration of nearby favorable parameters for the downstream task. We demonstrate that this free energy criterion can be effectively implemented without access to the downstream data or prior knowledge of the downstream task. Furthermore, we provide empirical evidence that the free energy criterion reliably correlates with improved fine-tuning performance, offering a principled approach to predicting model adaptability.", "title_embedding_index": 18811, "title_abs_embedding_index": 18836}, {"title": "SPARK: Physics-Guided Quantitative Augmentation for Dynamical System Modeling", "link_suffix": "/forum?id=BZQmpsuW7D", "link": "https://openreview.net/forum?id=BZQmpsuW7D", "pdf_link": "https://openreview.net/pdf?id=BZQmpsuW7D", "keywords": "dynamical system, augmentation", "abstract": "In dynamical system modeling, traditional numerical methods have a solid theoretical foundation but are limited by high computational costs and sensitivity to initial conditions. Current data-driven approaches use deep learning models to capture complex spatiotemporal features, but they rely heavily on large amounts of data and assume a stable data distribution, making them ineffective against data scarcity and distribution shifts. To address these challenges, we propose SPARK, a physics-guided quantized augmentation plugin. SPARK integrates boundary information and physical parameters, using a reconstruction autoencoder to build a physics-rich discrete memory bank for data compression. It then enhances selected samples for downstream tasks with this pre-trained memory bank. SPARK then utilizes an attention mechanism to model historical observations and combines fourier-enhanced graph ODE to efficiently predict long-term dynamical systems, enhancing robustness and adaptability to complex physical environments. Extensive experiments on benchmark datasets show that our approach significantly outperforms various baseline methods in handling distribution shifts and data scarcity.", "title_embedding_index": 18812, "title_abs_embedding_index": 18837}, {"title": "Autoregressive Video Generation without Vector Quantization", "link_suffix": "/forum?id=JE9tCwe3lp", "link": "https://openreview.net/forum?id=JE9tCwe3lp", "pdf_link": "https://openreview.net/pdf?id=JE9tCwe3lp", "keywords": "Autoregressive Generation, Text-to-Video Generation, Continuous-valued Space", "abstract": "Generating a video causally in an autoregressive manner is considered a promising path toward infinite video generation in a flexible context. Prior autoregressive approaches typically rely on vector quantization to convert a video into a discrete-valued space, which could raise challenges in efficiency when modeling long videos. In this work, we propose a novel approach that enables autoregressive video generation without vector quantization. We propose to reformulate the video generation problem as an autoregressive modeling framework integrating temporal \\textit{frame-by-frame} prediction and spatial \\textit{set-by-set} prediction. Unlike raster-scan prediction in prior autoregressive models or joint distribution modeling of fixed-length tokens in diffusion models, our approach maintains the causal property of GPT-style models for flexible in-context capabilities, while leveraging bidirectional modeling within individual frames for efficiency. We train a novel video autoregressive model with the proposed approach, termed \\Ours. Our results demonstrate that \\Ours fully surpasses prior autoregressive video models in data efficiency, inference speed, visual fidelity, and video fluency, even with a much smaller model capacity, \\ie, 0.6B parameters. \\Ours generalizes well across extended video durations and enables diverse zero-shot applications in one unified model. Additionally, with a significantly lower training cost, \\Ours outperforms state-of-the-art image diffusion models in text-to-image generation tasks. We will release all weights, models, and code to facilitate the reproduction of \\Ours and further development.", "title_embedding_index": 18813, "title_abs_embedding_index": 18838}, {"title": "Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models", "link_suffix": "/forum?id=rsZwwjYHuD", "link": "https://openreview.net/forum?id=rsZwwjYHuD", "pdf_link": "https://openreview.net/pdf?id=rsZwwjYHuD", "keywords": "Hallucination Alleviation, Large Vision-Language Models, Decoding Strategy, Trustworthy AI", "abstract": "Hallucination remains a significant challenge in Large Vision-Language Models (LVLMs). To alleviate this issue, some methods, known as contrastive decoding, induce hallucinations by manually disturbing the raw vision or instruction inputs and then mitigate them by contrasting the outputs of the original and disturbed LVLMs. However, these holistic input disturbances sometimes induce potential noise and also double the inference cost. To tackle these issues, we propose a simple yet effective method named $\\textit{Self-Introspective Decoding}$ (SID). Our empirical investigations reveal that pre-trained LVLMs can introspectively assess the importance of vision tokens based on preceding vision and text (both instruction and generated) tokens. Leveraging this insight, we develop the Context and Text-aware Token Selection (CT$^2$S) strategy, which preserves only the least important vision tokens after the early decoder layers, thereby adaptively amplify vision-and-text association hallucinations during auto-regressive decoding. This strategy ensures that multimodal knowledge absorbed in the early decoder layers induces multimodal contextual rather than aimless hallucinations, and significantly reduces computation burdens. Subsequently, the original token logits subtract the amplified fine-grained hallucinations, effectively alleviating hallucinations without compromising the LVLMs' general ability. Extensive experiments illustrate SID generates less-hallucination and higher-quality texts across various metrics, without much additional computation cost. Codes are in the Supplementary Material and also available athttps://anonymous.4open.science/r/SID-1795.", "title_embedding_index": 18814, "title_abs_embedding_index": 18839}, {"title": "IGNN-Solver: A Graph Neural Solver for Implicit Graph Neural Networks", "link_suffix": "/forum?id=CfXRcN4iUw", "link": "https://openreview.net/forum?id=CfXRcN4iUw", "pdf_link": "https://openreview.net/pdf?id=CfXRcN4iUw", "keywords": "implicit deep learning, deep equilibrium models, implicit graph neural networks", "abstract": "Implicit graph neural networks (IGNNs), which exhibit strong expressive power with a single layer, have recently demonstrated remarkable performance in capturing long-range dependencies (LRD) in underlying graphs while effectively mitigating the over-smoothing problem. However, IGNNs rely on computationally expensive fixed-point iterations, which lead to significant speed and scalability limitations, hindering their application to large-scale graphs. To achieve fast fixed-point solving for IGNNs, we propose a novel graph neural solver, IGNN-Solver, which leverages the generalized Anderson Acceleration method, parameterized by a small GNN, and learns iterative updates as a graph-dependent temporal process. Extensive experiments demonstrate that the IGNN-Solver significantly accelerates inference, achieving a $1.5\\times$ to $8\\times$ speedup without sacrificing accuracy. Moreover, this advantage becomes increasingly pronounced as the graph scale grows, facilitating its large-scale deployment in real-world applications.", "title_embedding_index": 18815, "title_abs_embedding_index": 18840}, {"title": "Orchestrating Heterogeneous Architectures for Fast Inference of Mixture-of-Experts Models", "link_suffix": "/forum?id=N5fVv6PZGz", "link": "https://openreview.net/forum?id=N5fVv6PZGz", "pdf_link": "https://openreview.net/pdf?id=N5fVv6PZGz", "keywords": "NLP in resource-constrained settings, inference methods", "abstract": "Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from significant overhead of frequently moving data between CPU and GPU, or fail to consider different characteristics of CPU and GPU. This paper proposes Twiddler, a resource-efficient inference system for MoE models with limited GPU resources. Twiddler strategically utilizes the heterogeneous computing architecture of CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Twiddler has better performance in all scenarios. Twiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference, compared against different baselines.", "title_embedding_index": 18816, "title_abs_embedding_index": 18841}, {"title": "Right Time to Learn: Promoting Generalization via Bio-inspired Spacing Effect in Knowledge Distillation", "link_suffix": "/forum?id=YUaQ6qrbRL", "link": "https://openreview.net/forum?id=YUaQ6qrbRL", "pdf_link": "https://openreview.net/pdf?id=YUaQ6qrbRL", "keywords": "Knowledge Distillation, Brain-inspired Al, Machine Learning, Spacing effect", "abstract": "Knowledge distillation (KD) is a powerful strategy for training deep neural networks (DNNs). While it was originally proposed to train a more compact \u201cstudent\u201d model from a large \u201cteacher\u201d model, many recent efforts have focused on adapting it as an effective way to promote generalization of the model itself, such as online KD and self KD. Here, we propose an easy-to-use and compatible strategy named Spaced KD to improve the effectiveness of both online KD and self KD, in which the student model distills knowledge from a teacher model trained with a space interval ahead. This strategy is inspired by a prominent theory named spacing effect in the field of biological learning and memory, positing that appropriate intervals between learning trials can significantly enhance learning performance. We provide an in-depth theoretical and empirical analysis showing that the benefits of the proposed spacing effect in KD stem from seeking a flat minima during stochastic gradient descent (SGD). We perform extensive experiments to demonstrate the effectiveness of our Spaced KD in improving the learning performance of DNNs (e.g., the additional performance gain is up to 2.31% and 3.34% on Tiny-ImageNet over online KD and self KD, respectively).", "title_embedding_index": 18817, "title_abs_embedding_index": 18842}, {"title": "Quantitative Approximation for Neural Operators in Nonlinear Parabolic Equations", "link_suffix": "/forum?id=yUefexs79U", "link": "https://openreview.net/forum?id=yUefexs79U", "pdf_link": "https://openreview.net/pdf?id=yUefexs79U", "keywords": "Neural operators, Partial differential equations, Nonlinear parabolic equations, Quantitative universal approximation", "abstract": "Neural operators serve as universal approximators for general continuous operators. In this paper, we derive the approximation rate of solution operators for the nonlinear parabolic partial differential equations (PDEs), contributing to the quantitative approximation theorem for solution operators of nonlinear PDEs. Our results show that neural operators can efficiently approximate these solution operators without the exponential growth in model complexity, thus strengthening the theoretical foundation of neural operators. A key insight in our proof is to transfer PDEs into the corresponding integral equations via Duahamel's principle, and to leverage the similarity between neural operators and Picard\u2019s iteration\u2014a classical algorithm for solving PDEs. This approach is potentially generalizable beyond parabolic PDEs to a range of other equations, including the Navier-Stokes equation, nonlinear Schr\u00f6dinger equations and nonlinear wave equations, which can be solved by Picard's iteration.", "title_embedding_index": 18818, "title_abs_embedding_index": 18843}, {"title": "X2-DFD: A framework for eXplainable and eXtendable Deepfake Detection", "link_suffix": "/forum?id=EoTIlDT0Tr", "link": "https://openreview.net/forum?id=EoTIlDT0Tr", "pdf_link": "https://openreview.net/pdf?id=EoTIlDT0Tr", "keywords": "Deepfake Detection; Multimodal Large Language Models; Media Forensics", "abstract": "Detecting deepfakes (i.e., AI-generated content with malicious intent) has become an important task. Most existing detection methods provide only real/fake predictions without offering human-comprehensible explanations. Recent studies leveraging multimodal large-language models (MLLMs) for deepfake detection have shown improvements in explainability. However, the performance of pre-trained MLLMs (e.g., LLaVA) remains limited due to a lack of understanding of their capabilities for this task and strategies to enhance them. In this work, we empirically assess the strengths and weaknesses of MLLMs specifically in deepfake detection via forgery-related feature analysis. Building on these assessments, we propose a novel framework called $\\mathcal{X}^2$-DFD, consisting of three core modules. \nThe first module,Model Feature Assessment (MFA), measures the detection capabilities of forgery-related features intrinsic to MLLMs, and gives a descending ranking of these features. \nThe second module,Strong Feature Strengthening (SFS), enhances the detection and explanation capabilities by fine-tuning the MLLM on a dataset constructed based on the top-ranked features. \nThe third module,Weak Feature Supplementing (WFS), improves the fine-tuned MLLM's capabilities on lower-ranked features by integrating external dedicated deepfake detectors. \nTo verify the effectiveness of this framework, we further present a practical implementation, where an automated forger-related feature generation, evaluation, and ranking procedure is designed forMFAmodule; an automated generation procedure of the fine-tuning dataset containing real and fake images with explanations based on top-ranked features is developed forSFSmodel; an external conventional deepfake detector focusing on blending artifact, which corresponds to a low detection capability in the pre-trained MLLM, is integrated forWFSmodule. \nExperimental results show that the proposed implementation enhances overall detection performance compared to pre-trained MLLMs, while providing more convincing explanations. \nMore encouragingly, our framework is designed to be plug-and-play, allowing it to seamlessly integrate with more advanced MLLMs and external detectors, leading to continual improvement and extension to face the challenges of rapidly evolving deepfake technologies.", "title_embedding_index": 18819, "title_abs_embedding_index": 18844}, {"title": "MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning", "link_suffix": "/forum?id=SxOrhLuuVz", "link": "https://openreview.net/forum?id=SxOrhLuuVz", "pdf_link": "https://openreview.net/pdf?id=SxOrhLuuVz", "keywords": "Large Language Model, LoRA, Parameter-Efficient Fine-Tuning", "abstract": "Low-rank adaptation (LoRA) is a popular parameter-efficient fine-tuning (PEFT) method for large language models (LLMs). In this paper, we analyze the impact of low-rank updating, as implemented in LoRA. Our findings suggest that the low-rank updating mechanism may limit the ability of LLMs to effectively learn and memorize new knowledge. Inspired by this observation, we propose a new method called MoRA, which employs a square matrix to achieve high-rank updating while maintaining the same number of trainable parameters. To achieve it, we introduce the corresponding non-parameter operators to reduce the input dimension and increase the output dimension for the square matrix. Furthermore, these operators ensure that the weight can be merged back into LLMs, which enables our method to be deployed like LoRA. We perform a comprehensive evaluation of our method across five tasks: instruction tuning, mathematical reasoning, continual pretraining, memory and pretraining. Our method outperforms LoRA on memory-intensive tasks and achieves comparable performance on other tasks.", "title_embedding_index": 18820, "title_abs_embedding_index": 18845}, {"title": "Multimodal Retrieval-Augmented Generation Question-Answering System", "link_suffix": "/forum?id=fMaEbeJGpp", "link": "https://openreview.net/forum?id=fMaEbeJGpp", "pdf_link": "https://openreview.net/pdf?id=fMaEbeJGpp", "keywords": "Retrieval-Augmented Generation; Dataset Construction; Text-Image Retrieval; Visual Question-Answering System", "abstract": "Retrieval-Augmented Generation (RAG) combines the richness of external knowledge bases with the generative capabilities of large language models (LLMs) to provide users with more accurate and real-time responses. However, in the era of information explosion, the way information is presented is increasingly becoming multimodal. Users are no longer satisfied with the information provided by traditional text-based knowledge bases, making the construction of an efficient and accurate multimodal RAG question-answering system of significant theoretical and practical importance. To address these issues, this paper proposes an innovative RAG question-answering system: this approach pre-designs a rich dataset containing images, text, and question-answer pairs from external knowledge bases for subsequent model training, effectively improving the training quality of the model; it builds a cross-modal retrieval model from text to images, ensuring precise matching between document content and corresponding images, significantly reducing the complexity and processing time of locating relevant images within long texts. Furthermore, the retrieval model and the multimodal question-answering model are integrated to construct an efficient and accurate RAG question-answering system. Experimental results show that this system not only effectively simplifies the document formatting process and improves text-to-image retrieval accuracy but also exhibits comprehensive performance in handling multimodal data.", "title_embedding_index": 18821, "title_abs_embedding_index": 18846}, {"title": "A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation", "link_suffix": "/forum?id=wryFCrWB0A", "link": "https://openreview.net/forum?id=wryFCrWB0A", "pdf_link": "https://openreview.net/pdf?id=wryFCrWB0A", "keywords": "Autoregressive Model, Image Generation, Vision-Language Model, Large Language Model", "abstract": "This work tackles the information loss bottleneck of vector-quantization (VQ) autoregressive image generation by introducing a novel model architecture called the 2-Dimensional Autoregression (DnD) Transformer. The DnD-Transformer predicts more codes for an image by introducing a new autoregression direction, \\textit{model depth}, along with the sequence length direction. Compared to traditional 1D autoregression and previous work utilizing similar 2D image decomposition such as RQ-Transformer, the DnD-Transformer is an end-to-end model that can generate higher quality images with the same backbone model size and sequence length, opening a new optimization perspective for autoregressive image generation. Furthermore, our experiments reveal that the DnD-Transformer's potential extends beyond generating natural images. It can even generate images with rich text and graphical elements in a self-supervised manner, demonstrating an understanding of these combined modalities. This has not been previously demonstrated for popular vision generative models such as diffusion models, showing a spark of vision-language intelligence when trained solely on images. We will open the codes, datasets and models for reproduction.", "title_embedding_index": 18822, "title_abs_embedding_index": 18847}, {"title": "Direct Advantage Estimation in Partially Observable Environments", "link_suffix": "/forum?id=acH47FOCTV", "link": "https://openreview.net/forum?id=acH47FOCTV", "pdf_link": "https://openreview.net/pdf?id=acH47FOCTV", "keywords": "POMDP, advantage function, deep RL, off-policy learning", "abstract": "Direct Advantage Estimation (DAE) was recently shown to improve sample-efficiency of deep reinforcement learning algorithms. However, DAE assumes full observability of the environment, which may be restrictive in realistic settings. In the present work, we first show that DAE can be extended to partially observable domains with minor modifications. Secondly, we address the increased computational cost due to the need to approximate the transition probabilities through the use of discrete latent dynamics models. Finally, we empirically evaluate the proposed method using the Arcade Learning Environments, and show that it is scalable and sample-efficient.", "title_embedding_index": 18823, "title_abs_embedding_index": 18848}, {"title": "Pre-Training for 3D Hand Pose Estimation with Contrastive Learning on Large-Scale Hand Images in the Wild", "link_suffix": "/forum?id=96jZFqM5E0", "link": "https://openreview.net/forum?id=96jZFqM5E0", "pdf_link": "https://openreview.net/pdf?id=96jZFqM5E0", "keywords": "3D Hand Pose Estimation; Contrastive Learning; Pre-Training of Large-Scale Images;", "abstract": "We present a contrastive learning framework based on in-the-wild hand images tailored for pre-training 3D hand pose estimators, dubbed HandCLR. Pre-training on large-scale images achieves promising results in various tasks, but prior 3D hand pose pre-training methods have not fully utilized the potential of diverse hand images accessible from in-the-wild videos. To facilitate scalable pre-training, we first prepare an extensive pool of hand images from in-the-wild videos and design our method with contrastive learning. Specifically, we collected over 2.0M hand images from recent human-centric videos, such as 100DOH and Ego4D. To extract discriminative information from these images, we focus on the similarity of hands; pairs of similar hand poses originating from different samples, and propose a novel contrastive learning method that embeds similar hand pairs closer in the latent space. Our method not only learns from similar samples but also adaptively weights the contrastive learning loss based on inter-sample distance, leading to additional performance gains. The experiments demonstrate that our method outperforms conventional contrastive learning approaches that produce positive pairs sorely from a single image with data augmentation. We achieve significant improvements over the state-of-the-art method in various datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% on AssemblyHands. Our code will be released.", "title_embedding_index": 18824, "title_abs_embedding_index": 18849}]