[{"title": "Filling in the GAP: Achieving Robust and Adaptive GNNs through Post-Processing", "link_suffix": "/forum?id=O6p3v6i0hT", "link": "https://openreview.net/forum?id=O6p3v6i0hT", "pdf_link": "https://openreview.net/pdf?id=O6p3v6i0hT", "keywords": "Graph neural networks, edge distribution shift, training-free post-processing, robustness, node classification", "abstract": "Graph neural networks (GNNs) have shown significant success in modeling graph-structured data. However, their performance often deteriorates when faced with a change in the graph structure between training and test time, such as edge addition or removal\u2014a common scenario considering the dynamic nature of graphs. To address this challenge, we propose FILLER (Framework for Integrating Layer-Level Edge-shift Recovery), a post-processing method which enhances the robustness of a GNN against edge sparsification while maintaining its adaptability to informative edge addition. Our key idea is to fill in the representation gap caused by edge distribution shift by injecting the Edge-shfit (ER) layer into each layer of the GNN. Our ER layer is carefully designed to allow a GNN to maintain its high performance in dynamic graph environments even without any additional training, and its effectiveness is shown both theoretically and empirically. Our experiments on ten datasets for node classification and five GNN architectures demonstrate that FILLER is broadly applicable across\ndiverse models and scenarios.", "title_embedding_index": 19700, "title_abs_embedding_index": 19725}, {"title": "S2MAM: Semi-supervised Meta Additive Model for Robust Estimation and Variable Selection", "link_suffix": "/forum?id=dpnPOXoqVQ", "link": "https://openreview.net/forum?id=dpnPOXoqVQ", "pdf_link": "https://openreview.net/pdf?id=dpnPOXoqVQ", "keywords": "manifold regularization, bilevel optimization, sparse additive model, robustness, learning theory", "abstract": "Semi-supervised learning with manifold regularization is a classical family for learning from the labeled and unlabeled data jointly, where the key requirement is the support of unknown marginal distribution enjoys the geometric structure of a Riemannian manifold. Usually, the Laplace-Beltrami operator-based manifold regularization can be approximated empirically by the Laplacian regularization associated with the whole training data and its graph Laplacian matrix. However, the graph Laplacian matrix depends heavily on the pre-specifying similarity metric and may result in inappropriate penalties when facing redundant and noisy input variables. In order to address the above issues, this paper proposes a new semi-supervised meta additive model (S$^2$MAM) under a bilevel optimization scheme to automatically identify the informative variables, update the similarity matrix, and achieve the interpretable prediction simultaneously. Theoretical guarantees are provided for S$^2$MAM including the computing convergence and the statistical generalization bound. Experimental assessments on synthetic and real-world datasets validate the robustness and interpretability of the proposed approach.", "title_embedding_index": 19701, "title_abs_embedding_index": 19726}, {"title": "Learning Imperfect Information Extensive-form Games with Last-iterate Convergence under Bandit Feedback", "link_suffix": "/forum?id=iOAcVOHvEN", "link": "https://openreview.net/forum?id=iOAcVOHvEN", "pdf_link": "https://openreview.net/pdf?id=iOAcVOHvEN", "keywords": "Extensive-form games; partially observable Markov games (POMGs); last-iterate convergence", "abstract": "We study learning the approximate Nash equilibrium (NE) policy profile in two-player zero-sum imperfect information extensive-form games (IIEFGs) with last-iterate convergence. The algorithms in previous works studying this problem either require full-information feedback or only have asymptotic convergence rates. In contrast, we study  IIEFGs in the formulation of partially observable Markov games (POMGs) with the perfect-recall assumption and bandit feedback, where the knowledge of the game is not known a priori and only the rewards of the experienced information set and action pairs are revealed to the learners in each episode. Our algorithm utilizes a negentropy regularizer weighted by a virtual transition over information set-action space. By carefully designing the virtual transition together with the leverage of the entropy regularization technique, we prove that our algorithm converges to the NE of IIEFGs with a provable finite-time convergence rate of $\\widetilde{O}(k^{-\\frac{1}{8}})$ with high probability under bandit feedback, thus answering the second question of \\citet{Fiegel2023adapting} affirmatively.", "title_embedding_index": 19702, "title_abs_embedding_index": 19727}, {"title": "PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation", "link_suffix": "/forum?id=ACEuJBhhbN", "link": "https://openreview.net/forum?id=ACEuJBhhbN", "pdf_link": "https://openreview.net/pdf?id=ACEuJBhhbN", "keywords": "Diffusion Model, Probabilistic Methods", "abstract": "Diffusion models have made significant advancements in recent years. However, their performance often deteriorates when trained or fine-tuned on imbalanced datasets. This degradation is largely due to the disproportionate representation of majority and minority data in image-text pairs. In this paper, we propose a general fine-tuning approach, dubbed PoGDiff, to address this challenge. Rather than directly minimizing the KL divergence between the predicted and ground-truth distributions, PoGDiff replaces the ground-truth distribution with a Product of Gaussians (PoG), which is constructed by combining the original ground-truth targets with the predicted distribution conditioned on a neighboring text embedding. Experiments on real-world datasets demonstrate that our method effectively addresses the imbalance problem in diffusion models, improving both generation accuracy and quality.", "title_embedding_index": 19703, "title_abs_embedding_index": 19728}, {"title": "Towards Domain Adaptive Neural Contextual Bandits", "link_suffix": "/forum?id=LNkMWCEssX", "link": "https://openreview.net/forum?id=LNkMWCEssX", "pdf_link": "https://openreview.net/pdf?id=LNkMWCEssX", "keywords": "Domain Adaptation, Deep Learning, Adversarial Learning", "abstract": "Contextual bandit algorithms are essential for solving real-world decision making problems. In practice, collecting a contextual bandit's feedback from different domains may involve different costs. For example, measuring drug reaction from mice (as a source domain) and humans (as a target domain). Unfortunately, adapting a contextual bandit algorithm from a source domain to a target domain with distribution shift still remains a major challenge and largely unexplored. In this paper, we introduce the first general domain adaptation method for contextual bandits. Our approach learns a bandit model for the target domain by collecting feedback from the source domain. Our theoretical analysis shows that our algorithm maintains a sub-linear regret bound even adapting across domains. Empirical results show that our approach outperforms the state-of-the-art contextual bandit algorithms on real-world datasets.", "title_embedding_index": 19704, "title_abs_embedding_index": 19729}, {"title": "Phidias: A Generative Model for Creating 3D  Content from Text, Image, and 3D Conditions with Reference-Augmented  Diffusion", "link_suffix": "/forum?id=TEkoMEjf7E", "link": "https://openreview.net/forum?id=TEkoMEjf7E", "pdf_link": "https://openreview.net/pdf?id=TEkoMEjf7E", "keywords": "3D generation, retrieval-augmented generation, multi-view diffusion", "abstract": "Generative 3D modeling has made significant advances recently, but it remains constrained by its inherently ill-posed nature, leading to challenges in quality and controllability. Inspired by the real-world workflow that designers typically refer to existing 3D models when creating new ones, we propose Phidias, a novel generative model that uses diffusion for reference-augmented 3D generation. Given an image, our method leverages a retrieved or user-provided 3D reference model to guide the generation process, thereby enhancing the generation quality, generalization ability, and controllability. Phidias integrates three key components: 1) meta-ControlNet to dynamically modulate the conditioning strength, 2) dynamic reference routing to mitigate misalignment between the input image and 3D reference, and 3) self-reference augmentations to enable self-supervised training with a progressive curriculum.  Collectively, these designs result in significant generative improvements over existing methods. Phidias forms a unified framework for 3D generation using text, image, and 3D conditions, offering versatile applications.", "title_embedding_index": 19705, "title_abs_embedding_index": 19730}, {"title": "FLIP: Flow-Centric Generative Planning for General-Purpose Manipulation Tasks", "link_suffix": "/forum?id=B2N0nCVC91", "link": "https://openreview.net/forum?id=B2N0nCVC91", "pdf_link": "https://openreview.net/pdf?id=B2N0nCVC91", "keywords": "World Model, Long-Horizon Planning, Robot Manipulation, Flow Generation", "abstract": "We aim to develop a model-based planning framework for world models that can be scaled with increasing model and data budgets for general-purpose manipulation tasks with only language and vision inputs. To this end, we present FLow-CentrIc generative Planning (FLIP), a model-based planning algorithm on visual space that features three key modules: 1) a multi-modal flow generation model as the general-purpose action proposal module; 2) a flow-conditioned video gen- eration model as the dynamics module; and 3) a vision-language representation learning model as the value module. Given an initial image and language instruction as the goal, FLIP can progressively search for long-horizon flow and video plans that maximize the discounted return to accomplish the task. FLIP is able to synthesize long-horizon plans across objects, robots, and tasks with image flows as the general action representation, and the dense flow information also provides rich guidance for long-horizon video generation. In addition, the synthesized flow and video plans can guide the training of low-level control policies for robot execution. Experiments on diverse benchmarks demonstrate that FLIP can improve both the success rates and quality of long-horizon video plan synthesis and has the interactive world model property, opening up wider applications for future works. Video demos are on our website:https://flow-planning.github.io/.", "title_embedding_index": 19706, "title_abs_embedding_index": 19731}, {"title": "Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding", "link_suffix": "/forum?id=nYpPAT4L3D", "link": "https://openreview.net/forum?id=nYpPAT4L3D", "pdf_link": "https://openreview.net/pdf?id=nYpPAT4L3D", "keywords": "Vision-language model, fine-grained alignment, large-scale pre-training, CT image", "abstract": "Artificial intelligence (AI) shows great potential in assisting radiologists to improve the efficiency and accuracy of medical image interpretation and diagnosis. However, a versatile AI model requires large-scale data and comprehensive annotations, which are often impractical in medical settings. Recent studies leverage radiology reports as a naturally high-quality supervision for medical images, using contrastive language-image pre-training (CLIP) to develop language-informed models for radiological image interpretation. Nonetheless, these approaches typically contrast entire images with reports, neglecting the local associations between imaging regions and report sentences, which may undermine model performance and interoperability. In this paper, we propose a fine-grained vision-language model (fVLM) for anatomy-level CT image interpretation. Specifically, we explicitly match anatomical regions of CT images with corresponding descriptions in radiology reports and perform contrastive pre-training for each anatomy individually. Fine-grained alignment, however, faces considerable false-negative challenges, mainly from the abundance of anatomy-level healthy samples and similarly diseased abnormalities, leading to ambiguous patient-level pairings. To tackle this issue, we propose identifying false negatives of both normal and abnormal samples and calibrating contrastive learning from patient-level to disease-aware pairing. We curated the largest CT dataset to date, comprising imaging and report data from 69,086 patients, and conducted a comprehensive evaluation of 54 major and important disease (including several most deadly cancers) diagnosis tasks across 15 main anatomies. Experimental results demonstrate the substantial potential of fVLM in versatile medical image interpretation. In the zero-shot classification task, we achieved an average AUC of 81.3% on 54 diagnosis tasks, surpassing CLIP and supervised methods by 12.9% and 8.0%, respectively. Additionally, on the publicly available CT-RATE and Rad-ChestCT benchmarks, our fVLM outperformed the current state-of-the-art methods with absolute AUC gains of 7.4% and 4.8%, respectively.", "title_embedding_index": 19707, "title_abs_embedding_index": 19732}, {"title": "Uni-IR: Ambiguity-Reduced Inverse Rendering through a Unified Framework for Glossy Objects", "link_suffix": "/forum?id=Z6TQhliDIq", "link": "https://openreview.net/forum?id=Z6TQhliDIq", "pdf_link": "https://openreview.net/pdf?id=Z6TQhliDIq", "keywords": "Inverse rendering, physically based rendering", "abstract": "Inverse rendering aims to decompose the an image into  geometry, materials, and lighting.\n  Recently, Neural Radiance Fields (NeRF) based inverse rendering has significantly advanced, bridging the gap between NeRF-based models and conventional rendering engines.\n  Existing methods typically adopt a two-stage optimization approach, beginning with volume rendering for geometry reconstruction, followed by physically based rendering (PBR) for materials and lighting estimation.\n  However, the inherent ambiguity between materials and lighting during PBR and the suboptimal nature of geometry reconstruction by volume rendering only compromise the outcomes.\n  To address these challenges, we introduce Uni-IR, a unified framework that imposes mutual constraints to alleviate ambiguity by integrating volume rendering and physically based rendering.\n  Specifically, we employ a physically-based volume rendering (PBVR) approach that incorporates PBR concepts into volume rendering, directly facilitating connections with materials and lighting, in addition to geometry. Both rendering methods are utilized during optimization, imposing mutual constraints and optimizing geometry, materials, and lighting synergistically. Employing a meticulously crafted unified representation for both lighting and materials, Uni-IR achieves high-quality geometry reconstruction, materials and lighting estimation across various object types.", "title_embedding_index": 19708, "title_abs_embedding_index": 19733}, {"title": "Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference", "link_suffix": "/forum?id=foQ4AeEGG7", "link": "https://openreview.net/forum?id=foQ4AeEGG7", "pdf_link": "https://openreview.net/pdf?id=foQ4AeEGG7", "keywords": "Causal Graph Transformer, Networked Interference, Unknown Interference Graph, Peer Effects, Treatment Effects Estimation", "abstract": "Networked interference, also known as the peer effect in social science and spillover effect in economics, has drawn increasing interest across various domains. This phenomenon arises when a unit\u2019s treatment and outcome are influenced by the actions of its peers, posing significant challenges to causal inference, particularly in treatment assignment and effect estimation in real applications, due to the violation of the SUTVA assumption. While extensive graph models have been developed to identify treatment effects, these models often rely on structural assumptions about networked interference, assuming it to be identical to the social network, which can lead to misspecification issues in real applications. To address these challenges, we propose an Interference-Agnostic Causal Graph Transformer (CauGramer), which aggregates peers information via $L$-order Graph Transformer and employs cross-attention to infer aggregation function for learning interference representations. By integrating confounder balancing and minimax moment constraints, CauGramer fully incorporates peer information, enabling robust treatment effect estimation. Extensive experiments on two widely-used benchmarks demonstrate the effectiveness and superiority of CauGramer.", "title_embedding_index": 19709, "title_abs_embedding_index": 19734}, {"title": "PRM:  Photometric Stereo based Large Reconstruction Model", "link_suffix": "/forum?id=AkL2ID5rRV", "link": "https://openreview.net/forum?id=AkL2ID5rRV", "pdf_link": "https://openreview.net/pdf?id=AkL2ID5rRV", "keywords": "3D reconstruction, feed-fowared reconstruction model, photometric stereo", "abstract": "We propose PRM, a novel photometric stereo based large reconstruction model to reconstruct high-quality meshes with fine-grained local details.\nUnlike previous large reconstruction models that prepare images under fixed and simple lighting as both input and supervision, PRM renders photometric stereo images by varying materials and lighting for the purposes, which not only improves the precise local details by providing rich photometric cues but also increases the model\u2019s robustness to variations in the appearance of input images. \nTo offer enhanced flexibility of images rendering, we incorporate a real-time rendering method and mesh rasterization for online images rendering.\nMoreover, in employing an explicit mesh as our 3D representation, PRM ensures the application of differentiable PBR, which supports the utilization of multiple photometric supervisions and better models the specular color for high-quality geometry optimization.\nOur PRM leverages  photometric stereo images to achieve high-quality reconstructions with fine-grained local details, even amidst sophisticated image appearances. Extensive experiments demonstrate that PRM significantly outperforms other models.", "title_embedding_index": 19710, "title_abs_embedding_index": 19735}, {"title": "Gaze-Regularized Attention for Human Action Prediction", "link_suffix": "/forum?id=OrBIjc0lMz", "link": "https://openreview.net/forum?id=OrBIjc0lMz", "pdf_link": "https://openreview.net/pdf?id=OrBIjc0lMz", "keywords": "human action prediction, human-machine interaction, eye gaze", "abstract": "Eye gaze, encompassing fixations and saccades, offers valuable insights into human intentions and future actions. This study presents a novel approach to enhancing Vision Language Models (VLMs) for human action prediction by integrating eye gaze data into egocentric video analysis. Existing methods for action anticipation in egocentric videos often rely solely on visual data, potentially missing critical information provided by eye gaze. To address this limitation, we propose a unique gaze-augmented framework that integrates eye gaze directly into the VLM architecture and training process. By generating gaze heatmaps from eye gaze coordinates, our model dynamically focuses on regions highlighted by gaze patterns. Additionally, a gaze-regularization mechanism ensures the model maintains attention on gaze-allocated areas, thereby improving prediction accuracy and robustness. Our approach significantly enhances the model's ability to generate precise and detailed predictions of future actions. Compared to baseline models without leveraging gaze data, our method achieves a nearly 13% improvement in the semantic score of predictions. This substantial improvement underscores the effectiveness and novelty of integrating eye gaze with a gaze-regularized attention mechanism in VLMs for action anticipation. Moreover, our work demonstrates that incorporating eye gaze through this gaze-augmented framework can significantly boost the predictive capabilities of VLMs, enhancing their potential in applications that require accurate human action prediction.", "title_embedding_index": 19711, "title_abs_embedding_index": 19736}, {"title": "Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning", "link_suffix": "/forum?id=4OaO3GjP7k", "link": "https://openreview.net/forum?id=4OaO3GjP7k", "pdf_link": "https://openreview.net/pdf?id=4OaO3GjP7k", "keywords": "Reinforcement learning, Flat Minima, Robust Reinforcement learning", "abstract": "Investigating flat minima on loss surfaces in parameter space is well-documented in the supervised learning context, highlighting its advantages in model generalization. However, limited attention has been paid to the reinforcement learning (RL) context, where the impact of flatter reward in policy parameter space remains mostly unexplored. Beyond the naive guessing from the lesson of supervised learning, which makes us anticipate a link from flat rewards to the enhanced generalization, we here aim to formally bridge the flatness in reward surface to the robustness of RL models. For a policy model case, where the deep model determines actions, the flatter behavior of rewards against the parameter perturbations primarily leads to consistent rewards against perturbed actions. Moreover, the action robustness further affects to the robustness against other variations from the changes of state transition probabilities and reward functions. We extensively simulate various RL environments, confirming the consistent gains of flatter reward in bolstering the robustness of RL in varying circumstances, including action selection, transition dynamics, and reward functions.", "title_embedding_index": 19712, "title_abs_embedding_index": 19737}, {"title": "Can One Modality Model Synergize Training of Other Modality Models?", "link_suffix": "/forum?id=5BXWhVbHAK", "link": "https://openreview.net/forum?id=5BXWhVbHAK", "pdf_link": "https://openreview.net/pdf?id=5BXWhVbHAK", "keywords": "Multimodal learning, Representation learning, learning theory", "abstract": "Learning with multiple modalities has recently demonstrated significant gains in many domains by maximizing the shared information across modalities. However, the current approaches strongly rely on high-quality paired datasets, which allow co-training from the paired labels from different modalities. In this context, we raise a pivotal question: Can a model with one modality synergize the training of other models with the different modalities, even without the paired multimodal labels? Our answer is 'Yes'. As a figurative description, we argue that a writer, i.e., a language model, can promote the training of a painter, i.e., a visual model, even without the paired ground truth of text and image. We theoretically argue that a superior representation can be achieved by the synergy between two different modalities without paired supervision. As proofs of concept, we broadly confirm the considerable performance gains from the synergy among visual, language, and audio models. From a theoretical viewpoint, we first establish a mathematical foundation of the synergy between two different modality models, where each one is trained with its own modality. From a practical viewpoint, our work aims to broaden the scope of multimodal learning to encompass the synergistic usage of single-modality models, relieving a strong limitation of paired supervision.", "title_embedding_index": 19713, "title_abs_embedding_index": 19738}, {"title": "Unleashing the Power of Deep Dehazing Models: A Physics-guided Parametric Augmentation Net for Image Rehazing", "link_suffix": "/forum?id=YS5zdlSzvv", "link": "https://openreview.net/forum?id=YS5zdlSzvv", "pdf_link": "https://openreview.net/pdf?id=YS5zdlSzvv", "keywords": "Image dehazing, Image rehazing, Data augmentation", "abstract": "Image dehazing faces significant challenges in real-world scenarios due to the large domain gap between synthetic and real-world hazy images, which often hinders dehazing performance. Collecting real-world datasets is particularly difficult, as hazy and clean image pairs must be captured under identical conditions. To address this, we propose a Physics-guided Parametric Augmentation Network (PANet) that generates realistic hazy and clean training pairs, enhancing dehazing performance in real-world applications. PANet consists of two components: a Haze-to-Parameter Mapper (HPM), which projects hazy images into a parametric space representing haze characteristics, and a Parameter-to-Haze Mapper (PHM), which converts resampled haze parameters back into hazy images. By resampling individual haze parameter maps at the pixel level in the parametric space, PANet generates diverse hazy images with physically explainable haze conditions that are not present in the training data. Our experimental results show that PANet effectively enriches existing hazy image benchmarks, significantly improving the performance of current dehazing models.", "title_embedding_index": 19714, "title_abs_embedding_index": 19739}, {"title": "How Do Augmentations with Label Smoothing Enhance Model Robustness?", "link_suffix": "/forum?id=dAIcU2ZwUN", "link": "https://openreview.net/forum?id=dAIcU2ZwUN", "pdf_link": "https://openreview.net/pdf?id=dAIcU2ZwUN", "keywords": "Learning theory, Model robustness, Data augmentation, label smoothing, Generalization", "abstract": "Model robustness indicates a model's capability to generalize well on unforeseen distributional shifts, including data corruption, adversarial attacks, and domain shifts. One of the most prevalent and effective ways to enhance the robustness often involves data augmentations and label smoothing techniques. Despite the great success of the related approaches in diverse practices, a unified theoretical understanding of their efficacy in improving model robustness is lacking. We offer a theoretical framework to clarify how augmentations, label smoothing, or their combination enhance model robustness through the lens of loss surface flatness, generalization bound, and adversarial robustness. Specifically, we first formally bridge the diversified data distribution via augmentations to the flatter minima on the parameter space, which directly links to the improved generalization capability. Moreover, we further bridge augmentations with label smoothing, which softens the confidence of the target label, to the improved adversarial robustness. We broadly confirm our theories through extensive simulations on the existing common corruption and adversarial robustness benchmarks based on the CIFAR and tinyImageNet datasets, as well as various domain generalization benchmarks.", "title_embedding_index": 19715, "title_abs_embedding_index": 19740}, {"title": "On the Effectiveness of Dataset Alignment for Fake Image Detection", "link_suffix": "/forum?id=doBkiqESYq", "link": "https://openreview.net/forum?id=doBkiqESYq", "pdf_link": "https://openreview.net/pdf?id=doBkiqESYq", "keywords": "Image Forensics, Latent Diffusion", "abstract": "As latent diffusion models (LDMs) democratize image generation capabilities, there is a growing need to detect fake images. A good detector should focus on the generative model\u2019s fingerprints while ignoring image properties such as semantic content, resolution, file format, etc. Fake image detectors are usually built in a data-driven way, where a model is trained to separate real from fake images. Existing works primarily investigate network architecture choices and training recipes. In this work, we argue that in addition to these algorithmic choices, we also require a well-aligned dataset of real/fake images to train a robust detector. For the family of LDMs, we propose a very simple way to achieve this: we reconstruct all the real images using the LDM's autoencoder, without any denoising operation. We then train a model to separate these real images from their reconstructions. The fakes created this way are extremely similar to the real ones in almost every aspect (e.g., size, aspect ratio, semantic content), which forces the model to look for the LDM decoder's artifacts. We empirically show that this way of creating aligned real/fake datasets, which also sidesteps the computationally expensive denoising process, helps in building a detector that focuses less on spurious correlations, something that a very popular existing method is susceptible to. Finally, to demonstrate just how effective the alignment in a dataset can be, we build a detector using images that are not natural objects, and present promising results. Overall, our work identifies the subtle but significant issues that arise when training a fake image detector and proposes a simple and inexpensive solution to address these problems.", "title_embedding_index": 19716, "title_abs_embedding_index": 19741}, {"title": "RocketEval: Efficient automated LLM evaluation via grading checklist", "link_suffix": "/forum?id=zJjzNj6QUe", "link": "https://openreview.net/forum?id=zJjzNj6QUe", "pdf_link": "https://openreview.net/pdf?id=zJjzNj6QUe", "keywords": "automated evaluation, large language models, natural language processing", "abstract": "Evaluating large language models (LLMs) in diverse and challenging scenarios is essential to align them with human preferences. To mitigate the prohibitive costs associated with human evaluations, utilizing a powerful LLM as a judge has emerged as a favored approach. Nevertheless, this methodology encounters several challenges, including substantial expenses, concerns regarding privacy and security, and reproducibility. In this paper, we propose a straightforward, replicable, and accurate automated evaluation method by leveraging a lightweight LLM as the judge, named RocketEval. Initially, we identify that the performance disparity between lightweight and powerful LLMs in evaluation tasks primarily stems from their ability to conduct comprehensive analyses, which is not easily enhanced through techniques such as chain-of-thought reasoning. By reframing the evaluation task as a multi-faceted Q&A using an instance-specific checklist, we demonstrate that the limited judgment accuracy of lightweight LLMs is largely attributes to high uncertainty and positional bias. To address these challenges, we introduce an automated evaluation process grounded in checklist grading, which is designed to accommodate a variety of scenarios and questions. This process encompasses the creation of checklists, the grading of these checklists by lightweight LLMs, and the reweighting of checklist items to align with the supervised annotations. Our experiments carried out on the automated evaluation benchmarks, MT-Bench and WildBench datasets, reveal that RocketEval, when using $\\textit{Gemma-2-2B}$ as the judge, achieves a high correlation (0.965) with human preferences, which is comparable to $\\textit{GPT-4o}$. Moreover, RocketEval provides a cost reduction exceeding 50-fold for large-scale evaluation and comparison scenarios.", "title_embedding_index": 19717, "title_abs_embedding_index": 19742}, {"title": "CURIOSITY IS THE PATH TO OPTIMIZATION", "link_suffix": "/forum?id=L143pPpIHv", "link": "https://openreview.net/forum?id=L143pPpIHv", "pdf_link": "https://openreview.net/pdf?id=L143pPpIHv", "keywords": "PAC - MDP, Information Theory, Unsupervised Skill Discovery", "abstract": "In PAC theory, it is posited that larger hypothesis spaces necessitate more independently and identically distributed (i.i.d) data to maintain the accuracy of model performance. PAC-MDP theory defines curiosity by assigning higher rewards for visiting states that are far from the previously visited trajectory, which supports more independent and i.i.d data collection. Recently, this field has witnessed attempts to narrow the hypothesis space by developing additional mechanisms that train multiple skills and facilitate the sharing of information among them, thereby discovering commonalities. However, one might wonder: What if curiosity could not only enhance the efficiency of data collection but also significantly reduce the hypothesis space, thereby driving optimal outcomes independently without additional mechanism used in PAC-MDP?  Significant discussion has been devoted to the reduction of hypothesis spaces and the utilization of curiosity. Within this context, contrastive multi-skill reinforcement learning (RL) exhibits both traits. Previous research in contrastive multi-skill RL has utilized this technique primarily as a form of pretraining, However, there has been scant investigation into whether the technique itself can reduce the hypothesis space to optimize the outcomes. We have mathematically proven that curiosity provides bounds to guarantee optimality in contrastive multi-skill reinforcement learning (RL). Additionally, we have leveraged these findings to develop an algorithm that is applicable in real-world scenarios, which has been demonstrated to surpass other prominent algorithms. Furthermore, our experiments have shown that different skills are actually reducing the hypothesis space of the policy by being hierarchically grouped.", "title_embedding_index": 19718, "title_abs_embedding_index": 19743}, {"title": "Learning Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations", "link_suffix": "/forum?id=9ehJCZz4aM", "link": "https://openreview.net/forum?id=9ehJCZz4aM", "pdf_link": "https://openreview.net/pdf?id=9ehJCZz4aM", "keywords": "Self-Supervised Manipulation Concept Discovery, Concept-Guided Policy for Robotic Tasks", "abstract": "Training embodied agents to perform complex robotic tasks presents significant challenges due to the entangled factors of task compositionality, environmental diversity, and dynamic changes. In this work, we introduce a novel imitation learning framework to train closed-loop concept-guided policies that enhance long-horizon task performance by leveraging discovered manipulation concepts. Unlike methods that rely on predefined skills and human-annotated labels, our approach allows agents to autonomously abstract manipulation concepts from their proprioceptive states, thereby alleviating misalignment due to ambiguities in human semantics and environmental complexity. Our framework comprises two primary components: anAutomatic Concept Discoverymodule that identifies meaningful and consistent manipulation concepts, and aConcept-Aware Policy Learningmodule that effectively utilizes these manipulation concepts for adaptive task execution, including aConcept Selection Transformerfor concept-based guidance and aConcept-Guided Policyfor action prediction with the selected concepts. Experimental results demonstrate that our approach significantly outperforms baseline methods across a range of tasks and environments, while showcasing emergent consistency in motion patterns associated with the discovered concepts. Our code and models will be public.", "title_embedding_index": 19719, "title_abs_embedding_index": 19744}, {"title": "PolaFormer: Polarity-aware Linear Attention for Vision Transformers", "link_suffix": "/forum?id=kN6MFmKUSK", "link": "https://openreview.net/forum?id=kN6MFmKUSK", "pdf_link": "https://openreview.net/pdf?id=kN6MFmKUSK", "keywords": "Linear attention, polarity-aware attention, efficient transformer", "abstract": "Linear attention has emerged as a promising alternative to softmax-based attention, leveraging kernelized feature maps to reduce complexity from quadratic to linear in sequence length. However, the non-negative constraint on feature maps and the relaxed exponential function used in approximation lead to significant information loss compared to the original query-key dot products, resulting in less discriminative attention maps with higher entropy. To address the missing interactions driven by negative values in query-key pairs, we propose a polarity-aware linear attention mechanism that explicitly models both same-signed and opposite-signed query-key interactions, ensuring comprehensive coverage of relational information. Furthermore, to restore the spiky properties of attention maps, we provide a theoretical analysis proving the existence of a class of element-wise functions (with positive first and second derivatives) that can reduce entropy in the attention distribution. For simplicity, and recognizing the distinct contributions of each dimension, we employ a learnable power function for rescaling, allowing strong and weak attention signals to be effectively separated. Extensive experiments demonstrate that the proposed PolaFormer improves performance on various vision tasks, enhancing both expressiveness and efficiency by up to 4.6%.", "title_embedding_index": 19720, "title_abs_embedding_index": 19745}, {"title": "One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models", "link_suffix": "/forum?id=PdA9HAxO4w", "link": "https://openreview.net/forum?id=PdA9HAxO4w", "pdf_link": "https://openreview.net/pdf?id=PdA9HAxO4w", "keywords": "Universal Adversarial Attacks, Vision-Language Pretraining Models, Generative Attacks", "abstract": "Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks.", "title_embedding_index": 19721, "title_abs_embedding_index": 19746}, {"title": "Learning Gain Map for Inverse Tone Mapping", "link_suffix": "/forum?id=GtHRhpgpzB", "link": "https://openreview.net/forum?id=GtHRhpgpzB", "pdf_link": "https://openreview.net/pdf?id=GtHRhpgpzB", "keywords": "Computational Photography, Inverse Tone Mapping, Gain Map", "abstract": "For a more compatible and consistent high dynamic range (HDR) viewing experience, a new image format with a double-layer structure has been developed recently, which incorporates an auxiliary Gain Map (GM) within a standard dynamic range (SDR) image for adaptive HDR display. This new format motivates us to introduce a new task termed Gain Map-based Inverse Tone Mapping (GM-ITM), which focuses on learning the corresponding GM of an SDR image instead of directly estimating its HDR counterpart, thereby enabling a more effective up-conversion by leveraging the advantages of GM. The main challenge in this task, however, is to accurately estimate regional intensity variation with the fluctuating peak value. To this end, we propose a dual-branch network named GMNet, consisting of a Local Contrast Restoration (LCR) branch and a Global Luminance Estimation (GLE) branch to capture pixel-wise and image-wise information for GM estimation. Moreover, to facilitate the future research of the GM-ITM task, we build both synthetic and real-world datasets for comprehensive evaluations: synthetic SDR-GM pairs are generated from existing HDR resources, and real-world SDR-GM pairs are captured by mobile devices. Extensive experiments on these datasets demonstrate the superiority of our proposed GMNet over existing HDR-related methods both quantitatively and qualitatively.", "title_embedding_index": 19722, "title_abs_embedding_index": 19747}, {"title": "Breaking through Data Scarcity:  Knowledge Transfer in Offline Reinforcement Learning", "link_suffix": "/forum?id=B9MDjtIEd4", "link": "https://openreview.net/forum?id=B9MDjtIEd4", "pdf_link": "https://openreview.net/pdf?id=B9MDjtIEd4", "keywords": "Reinforcement Learning", "abstract": "We focus on knowledge transfer in offline reinforcement learning (RL), which aims to significantly improve the learning of an optimal policy in a target task based on a pre-collected dataset without further interactions with the environment. Data scarcity and high-dimensional feature spaces seriously pose challenges to offline RL in many real-world applications, and knowledge transfer offers a promising solution. We propose a novel and comprehensive knowledge transfer framework for offline RL, which carefully considers the relationship between the target and source tasks within the linear Markov decision process (MDP) framework. This enables efficient knowledge transfer from related source tasks to enhance learning in the target task and effectively address data scarcity concerns in offline RL. Our main contributions include establishing a relationship with the learning process between the target task and source task, introducing an effective and robust knowledge transfer technique to reduce the suboptimality of the learned policy, and demonstrating the significant effectiveness of the knowledge transfer framework through detailed theoretical analysis. Our work significantly contributes to the advancement of offline RL by providing a practical and robust framework for knowledge transfer facilitating more efficient and effective data utilization in various applications.", "title_embedding_index": 19723, "title_abs_embedding_index": 19748}, {"title": "Causal Motion Tokenizer for Streaming Motion Generation", "link_suffix": "/forum?id=WavXPunwzM", "link": "https://openreview.net/forum?id=WavXPunwzM", "pdf_link": "https://openreview.net/pdf?id=WavXPunwzM", "keywords": "3d motion, motion generation, human motion synthesis, text-driven, text-to-motion", "abstract": "Recent advancements in human motion generation have leveraged various multimodal inputs, including text, music, and audio. Despite significant progress, the challenge of generating human motion in a streaming context\u2014particularly from text\u2014remains underexplored. Traditional methods often rely on temporal modalities, leaving text-based motion generation with limited capabilities, especially regarding seamless transitions and low latency. In this work, we introduce MotionStream, a pioneering motion-streaming pipeline designed to continuously generate human motion sequences that adhere to the semantic constraints of input text. Our approach utilizes a Causal Motion Tokenizer, built on residual vector quantized variational autoencoder (RVQ-VAE) with causal convolution, to enhance long sequence handling and ensure smooth transitions between motion segments. Furthermore, we employ a Masked Transformer and Residual Transformer to generate motion tokens efficiently. Extensive experiments validate that MotionStream not only achieves state-of-the-art performance in motion composition but also maintains real-time generation capabilities with significantly reduced latency. We highlight the versatility of MotionStream through a story-to-motion application, demonstrating its potential for robotic control, animation, and gaming.", "title_embedding_index": 19724, "title_abs_embedding_index": 19749}]