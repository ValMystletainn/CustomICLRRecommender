[
    {
        "title": "Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models",
        "link_suffix": "/forum?id=WCRQFlji2q",
        "link": "https://openreview.net/forum?id=WCRQFlji2q",
        "pdf_link": "https://openreview.net/pdf?id=WCRQFlji2q",
        "keywords": "Mechanistic Interpretability, Hallucinations, Language Models",
        "abstract": "Hallucinations in large language models are a widespread problem, yet the mechanisms behind whether models will hallucinate are poorly understood, limiting our ability to solve this problem. Using sparse autoencoders as an interpretability tool, we discover that a key part of these mechanisms is entity recognition, where the model detects if an entity is one it can recall facts about. Sparse autoencoders uncover meaningful directions in the representation space, these detect whether the model recognizes an entity, e.g. detecting it doesn't know about an athlete or a movie. This shows that models can have self-knowledge: internal representations about their own capabilities. These directions are causally relevant: capable of steering the model to refuse to answer questions about known entities, or to hallucinate attributes of unknown entities when it would otherwise refuse. We demonstrate that despite the sparse autoencoders being trained on the base model, these directions have a causal effect on the chat model's refusal behavior, suggesting that chat finetuning has repurposed this existing mechanism. Furthermore, we provide an initial exploration into the mechanistic role of these directions in the model, finding that they disrupt the attention of downstream heads that typically move entity attributes to the final token."
    },
    {
        "title": "ADAPT: Adaptive Prompt Tuning for Pre-Trained Vision-Language Models",
        "link_suffix": "/forum?id=1L9vdc7BB5",
        "link": "https://openreview.net/forum?id=1L9vdc7BB5",
        "pdf_link": "https://openreview.net/pdf?id=1L9vdc7BB5",
        "keywords": "Prompt Tuning; Multimodality; Vision-Language Models; Network Pruning",
        "abstract": "Prompt tuning has emerged as an effective way for parameter-efficient fine-tuning. Conventional deep prompt tuning inserts continuous prompts of a fixed context length into the input to each layer. When a pre-trained model is tailored to a specific downstream task, different layers initialized with pre-trained weights might have, depending on the distribution shift type, different levels of deviation from the optimal weights. Inserted prompts with a fixed context length might have redundant context tokens or insufficient context length. To address this issue, we propose a deep continuous prompting method dubbed Adapt that encourages heterogeneous context lengths. Context lengths are automatically determined by iteratively pruning context tokens. We use the saliency criterion for the neural network pruning to compute the importance scores of context tokens in order to determine which tokens to prune. We examine the proposed method on the pre-trained vision-language model CLIP. Extensive experiments on 11 downstream datasets reveal the advantage of Adapt: the average test accuracy increases from 79.83% to 81.70%. The highest performance gain on individual datasets is 9.63%. At the same time, the computational overheads are comparable to or smaller than baseline methods."
    },
    {
        "title": "GenomeOcean: Efficient Foundation Model for Genome Generation",
        "link_suffix": "/forum?id=c8sEgxG2c0",
        "link": "https://openreview.net/forum?id=c8sEgxG2c0",
        "pdf_link": "https://openreview.net/pdf?id=c8sEgxG2c0",
        "keywords": "Genome Foundation Model, Genome Generation",
        "abstract": "We introduce GenomeOcean, a 4-billion-parameter genome foundation model that natively generates DNA sequences that are adherent to the input context. \nWith an efficiency-oriented model design, GenomeOcean is 80 times faster than existing models of similar size in genome generation. \nUnlike most existing genome foundation models\u2014such as DNABERT and Nucleotide Transformers\u2014that are designed for discriminative tasks, GenomeOcean leverages generative modeling to unlock new potentials in genomics research. \nDiverging from the traditional reliance on reference genomes\u2014which possess inherent biases\u2014GenomeOcean is exclusively trained on large-scale curated environmental samples collected from diverse ecosystems, including oceans, lakes, forests, and soils. This extensive genomic diversity, encompassing uncultured and uncharacterized organisms, allows GenomeOcean to generate sequences that better reflect the true diversity of life.\nIn a series of automated evaluations, we demonstrate GenomeOcean's capability to understand and follow context sequences. \nCompared to existing models, GenomeOcean not only better retains species information but also produces sequences with more appropriate open reading frame lengths and codon usage bias.\nWe anticipate the open release of GenomeOcean to open up new possibilities in genomics and computational biology research."
    },
    {
        "title": "Toward Efficient Multi-Agent Exploration With Trajectory Entropy Maximization",
        "link_suffix": "/forum?id=YvKJGYL4j7",
        "link": "https://openreview.net/forum?id=YvKJGYL4j7",
        "pdf_link": "https://openreview.net/pdf?id=YvKJGYL4j7",
        "keywords": "Multi-Agent Reinforcement Learning, Exploration, Cooperation, Trajectory Entropy Maximization",
        "abstract": "Recent works have increasingly focused on learning decentralized policies for agents as a solution to the scalability challenges in Multi-Agent Reinforcement Learning (MARL), where agents typically share the parameters of a policy network to make action decisions. However, this parameter sharing can impede efficient exploration, as it may lead to similar behaviors among agents. Different from previous mutual information-based methods that promote multi-agent diversity, we introduce a novel multi-agent exploration method called Trajectory Entropy Exploration (TEE). Our method employs a particle-based entropy estimator to maximize the entropy of different agents' trajectories in a contrastive trajectory representation space, resulting in diverse trajectories and efficient exploration. This entropy estimator avoids challenging density modeling and scales effectively in high-dimensional multi-agent settings. We integrate our method with MARL algorithms by deploying an intrinsic reward for each agent to encourage entropy maximization. To validate the effectiveness of our method, we test our method in challenging multi-agent tasks from several MARL benchmarks. The results demonstrate that our method consistently outperforms existing state-of-the-art methods."
    },
    {
        "title": "Can a Single Tree Outperform an Entire Forest?",
        "link_suffix": "/forum?id=BfH7rtJe1L",
        "link": "https://openreview.net/forum?id=BfH7rtJe1L",
        "pdf_link": "https://openreview.net/pdf?id=BfH7rtJe1L",
        "keywords": "Differentiable decision tree, Oblique decision tree, Subtree-polish strategy, Gradient-based optimization",
        "abstract": "The prevailing mindset is that a single decision tree underperforms random forests in testing accuracy, despite its advantages in interpretability and lightweight structure. This study challenges such a mindset by significantly improving the testing accuracy of an oblique regression tree through our gradient-based entire tree optimization framework, making its performance comparable to random forests. Our approach reformulates tree training as a differentiable unconstrained optimization task, employing a scaled sigmoid approximation strategy. To ameliorate numerical instability, we propose an algorithmic scheme that solves a sequence of increasingly accurate approximations. Additionally, a subtree polish strategy is implemented to reduce approximation errors accumulated across the tree. Extensive experiments on 16 datasets demonstrate that our optimized tree outperforms random forests by an average of 2.03% improvements in testing accuracy."
    },
    {
        "title": "Continual Learning via Learning a Continual Memory in Vision Transformer",
        "link_suffix": "/forum?id=VRYJXoUjRS",
        "link": "https://openreview.net/forum?id=VRYJXoUjRS",
        "pdf_link": "https://openreview.net/pdf?id=VRYJXoUjRS",
        "keywords": "Lifelong Learning, Continual Learning, Vision Transformers",
        "abstract": "This paper studies task-incremental continual learning (TCL) using Vision Transformers (ViTs).  Our goal is to improve the overall streaming-task performance without catastrophic forgetting by learning task synergies (e.g., a new task learns to automatically reuse/adapt modules from previous similar tasks, or to introduce new modules when needed, or to skip some modules when it appears to be an easier task). \nOne grand challenge is how to tame ViTs at streaming diverse tasks in terms of balancing  their plasticity and stability in a task-aware way while overcoming the catastrophic forgetting. To address the challenge, we propose a simple yet effective approach that identifies a lightweight yet expressive ``sweet spot'' in the ViT block as the task-synergy memory in TCL. We present a Hierarchical task-synergy Exploration-Exploitation (HEE) sampling based neural architecture search (NAS) method for effectively learning task synergies by structurally updating the identified memory component with respect to four basic operations (reuse, adapt, new and skip) at streaming tasks. The proposed method is thus dubbed as CHEEM (Continual Hierarchical-Exploration-Exploitation Memory). \nIn experiments, we test the proposed CHEEM on the challenging Visual Domain Decathlon (VDD) benchmark and the 5-Dataset benchmark. It obtains consistently better performance than the prior art with sensible CHEEM learned continually."
    },
    {
        "title": "ZO-Offloading: Fine-Tuning LLMs with 100 Billion Parameters on a Single GPU",
        "link_suffix": "/forum?id=euZD4YTXKu",
        "link": "https://openreview.net/forum?id=euZD4YTXKu",
        "pdf_link": "https://openreview.net/pdf?id=euZD4YTXKu",
        "keywords": "LLMs, zeroth-order optimization, efficient CPU-offloading, memory efficient fine-tuning",
        "abstract": "Fine-tuning pre-trained LLMs typically requires a vast amount of GPU memory. Standard first-order optimizers like SGD face a significant challenge due to the large memory overhead from back-propagation as the size of LLMs increases, which necessitates caching activations during the forward pass and gradients during the backward pass. In contrast, zeroth-order (ZO) methods can estimate gradients with only two forward passes and without the need for activation caching. Additionally, CPU resources can be aggregated and offloaded to extend the memory and computational capacity of a single GPU.\nTo enable efficient fine-tuning of LLMs on a single GPU, we introduce ZO-Offloading, a framework that strategically utilizes both CPU and GPU resources for ZO. ZO-Offloading dynamically offloads model parameters to the CPU and retrieves them to the GPU as needed, ensuring continuous and efficient computation by reducing idle times and maximizing GPU utilization. Parameter updates are integrated with ZO's dual forward passes to minimize redundant data transfers, thereby improving the overall efficiency of the fine-tuning process. The ZO-Offloading framework also incorporates a novel low-bit precision technique for managing data transfers between the CPU and GPU in AMP mode, as well as asynchronous checkpointing for LLM fine-tuning.\nWith ZO-Offloading, for the first time, it becomes possible to fine-tune extremely large models, such as the OPT-175B with over $\\textbf{175 billion}$ parameters, on a single GPU with just $\\textbf{24GB}$ of memory\u2014a feat previously unattainable with conventional methods. Moreover, our framework operates without any additional time cost compared to standard ZO methodologies."
    },
    {
        "title": "CELL your Model: Contrastive Explanations for Large Language Models",
        "link_suffix": "/forum?id=sBbarJBdkn",
        "link": "https://openreview.net/forum?id=sBbarJBdkn",
        "pdf_link": "https://openreview.net/pdf?id=sBbarJBdkn",
        "keywords": "Contrastive Explanations, Large Language Models",
        "abstract": "The advent of black-box deep neural network classification models has sparked the need to explain their decisions. However, in the case of generative AI such as large language models (LLMs), there is no class prediction to explain. Rather, one can ask why an LLM output a particular response to a given prompt. In this paper, we answer this question by proposing, to the best of our knowledge, the first contrastive explanation methods requiring simply black-box/query access. Our explanations suggest that an LLM outputs a reply to a given prompt because if the prompt was slightly modified, the LLM would have given a different response that is either less preferable or contradicts the original response. The key insight is that contrastive explanations simply require a scoring function that has meaning to the user and not necessarily a specific real valued quantity (viz. class label). We offer two algorithms for finding contrastive explanations: i) A myopic algorithm, which although effective in creating contrasts, requires many model calls and ii) A budgeted algorithm, our main algorithmic contribution, which intelligently creates contrasts adhering to a query budget, necessary for longer contexts. We show the efficacy of these methods on diverse natural language tasks such as open-text generation, automated red teaming, and explaining conversational degradation."
    },
    {
        "title": "Watermarking using Semantic-aware Speculative Sampling: from Theory to Practice",
        "link_suffix": "/forum?id=LdIlnsePNt",
        "link": "https://openreview.net/forum?id=LdIlnsePNt",
        "pdf_link": "https://openreview.net/pdf?id=LdIlnsePNt",
        "keywords": "Watermark, Large Language Model, Hypothesis testing",
        "abstract": "Statistical watermarking offers a theoretically-sound method for distinguishing machine-generated texts. In this work, we first present a systematic theoretical analysis of the statistical limits of watermarking, by framing it as a hypothesis testing problem. We derive nearly matching upper and lower bounds for (i) the optimal Type II error under a fixed Type I error, and (ii) the minimum number of tokens required to watermark the output. Our rate of $\\Theta(h^{-1} \\log (1/h))$ for the minimum number of required tokens, where $h$ is the average entropy per token, reveals a significant gap between the statistical limit and the $O(h^{-2})$ rate achieved in prior works. To our knowledge, this is the first comprehensive statistical analysis of the watermarking problem. Building on our theory, we developSEAL(Semantic-awarEspeculAtive sampLing), a novel watermarking algorithm for practical applications. SEAL introduces two key techniques: (i) designing semantic-aware random seeds by leveraging a proposal language model, and (ii) constructing a maximal coupling between the random seed and the next token through speculative sampling. Experiments on open-source benchmarks demonstrate that our watermarking scheme delivers superior efficiency and tamper resistance, particularly in the face of paraphrase attacks."
    },
    {
        "title": "Generative Parameter Efficient Fine-Tuning",
        "link_suffix": "/forum?id=3zEKTw9fSB",
        "link": "https://openreview.net/forum?id=3zEKTw9fSB",
        "pdf_link": "https://openreview.net/pdf?id=3zEKTw9fSB",
        "keywords": "Parameter Efficient Fine-Tuning, Transfer Learning",
        "abstract": "We present Generative Parameter-Efficient Fine-Tuning (GIFT) for adapting pretrained Transformer backbones on downstream tasks.  GIFT learns to generate the fine-tuned weights for a layer directly  from its pretrained weights. The GIFT network is parameterized in a minimally-simple way by two linear layers (without bias terms), and is shared by different pretrained layers selected for fine-tuning (e.g., the Query layers), which result in significantly fewer trainable parameters compared to the layer-specific methods like Low-Rank Adapter (LoRA). We also show this formulation bridges parameter-efficient fine-tuning and representation fine-tuning. We perform comprehensive experiments on natural language tasks (commonsense and arithmetic reasoning, instruction tuning,  and sequence classification) and computer vision tasks (fine-grained classification). We obtain the best performance and parameter efficiency among baselines on commonsense and arithmetic reasoning, and instruction following using the Llama family of models and on visual recognition benchmarks using Vision Transformers. Notably, compared to LoRA, we obtain 5.7% absolute increase in average accuracy with 15 times reduction of parameters on Commonsense170k using Llama-3 (8B), and  5.9% absolute increase in the win rate with 4 times reduction of parameters using Llama-2 (7B) during instruction tuning. Our GIFT also obtains a slightly higher win rate on instruction tuning than GPT 3.5 (Turbo 1106)."
    },
    {
        "title": "Scalable Diffusion for Bio-topological Representation Learning on Brain Graphs",
        "link_suffix": "/forum?id=i2ue8J6aqI",
        "link": "https://openreview.net/forum?id=i2ue8J6aqI",
        "pdf_link": "https://openreview.net/pdf?id=i2ue8J6aqI",
        "keywords": "Diffusion; Representation Learning; Graph Learning; Brain; Scalability; Topological Analysis",
        "abstract": "The topological structure information of the brain graph is critical in discovering bio-topological properties that underlie brain function and pathology. Authentic representations of brain graphs in many clinical applications heavily rely on these bio-topological properties. While existing studies have made strides in analyzing brain graph topology, they are often constrained by single-scale structural analysis and hence fail to extract these properties across multiple scales, thus potentially leading to incomplete and distorted representations. To address this limitation, we propose a novel Scalable diffusion model for bio-TOpological REpresentation learning on Brain graphs (BrainSTORE). BrainSTORE constructs multiscale topological structures within brain graphs, facilitating a deep exploration of bio-topological properties. By embedding these features into the training process and prioritizing bio-topological feature reconstruction, BrainSTORE learns representations that are more reflective of underlying brain organization. Furthermore, BrainSTORE utilizes a unified architecture to integrate these features effectively, yielding improved bio-topological representations which are more robust and biologically meaningful. To the best of our knowledge, this is the first study to investigate bio-topological properties in brain graph representation learning. Extensive experiments demonstrate that BrainSTORE outperforms state-of-the-art methods in brain disease detection."
    },
    {
        "title": "BAT-CLIP: Bimodal Test-Time Adaptation for CLIP",
        "link_suffix": "/forum?id=z7PhIgVmZU",
        "link": "https://openreview.net/forum?id=z7PhIgVmZU",
        "pdf_link": "https://openreview.net/pdf?id=z7PhIgVmZU",
        "keywords": "Test-Time Adaptation, CLIP, Robustness",
        "abstract": "Although open-vocabulary classification models like Contrastive Language Image Pretraining (CLIP) have demonstrated strong zero-shot learning capabilities, their robustness to common image corruptions remains poorly understood. Through extensive experiments, we show that zero-shot CLIP lacks robustness to common image corruptions at increasing severity levels during test time, necessitating the adaptation of CLIP to unlabeled corrupted images using test-time adaptation (TTA). However, we found that existing TTA methods have severe limitations in adapting CLIP due to their $\\textit{unimodal}$ nature. To address these limitations, we propose $\\textbf{BAT-CLIP}$, a $\\textit{bimodal}$ TTA method specially designed to improve CLIP's robustness to common image corruptions. The key insight of our approach is not only to adapt the visual encoders for better image feature extraction but also to strengthen the alignment between image and text features by promoting a stronger association between the image class prototype, computed using pseudo-labels, and the corresponding text feature. We evaluate our approach on benchmark image corruption datasets and achieve state-of-the-art results in TTA for CLIP, specifically for domains involving image corruptions. Particularly, with a ViT-B/16 vision backbone, we obtain mean accuracy improvements of 9.7%, 5.94%, and 5.12% for CIFAR-10C, CIFAR-100C, and ImageNet-C, respectively."
    },
    {
        "title": "Arctic-SnowCoder: Demystifying High-Quality Data in Code Pretraining",
        "link_suffix": "/forum?id=X9JU2gKEkR",
        "link": "https://openreview.net/forum?id=X9JU2gKEkR",
        "pdf_link": "https://openreview.net/pdf?id=X9JU2gKEkR",
        "keywords": "Large language models, Code generation, Pretraining, Data quality",
        "abstract": "Recent studies have been increasingly demonstrating that high-quality data is crucial for effective pretraining of language models. However, the precise definition of \"high-quality\" remains underexplored. Focusing on the code domain, we introduce Arctic-SnowCoder-1.3B, a data-efficient base code model pretrained on 555B tokens through three phases of progressively refined data: (1) general pretraining with 500B standard-quality code tokens, preprocessed through basic filtering, deduplication, and decontamination, (2) continued pretraining with 50B high-quality tokens, selected from phase one by a BERT-style quality annotator trained to distinguish good code from random data, using positive examples drawn from high-quality code files, along with instruction data from Magicoder and StarCoder2-Instruct, and (3) enhanced pretraining with 5B synthetic data created by Llama-3.1-70B using phase two data as seeds, adapting the Magicoder approach for pretraining. Despite being trained on a limited dataset, Arctic-SnowCoder achieves state-of-the-art performance on BigCodeBench, a coding benchmark focusing on practical and challenging programming tasks, compared to similarly sized models trained on no more than 1T tokens, outperforming Phi-1.5-1.3B by 36%. Across all evaluated benchmarks, Arctic-SnowCoder-1.3B beats StarCoderBase-3B pretrained on 1T tokens. Additionally, it matches the performance of leading small base code models trained on trillions of tokens. For example, Arctic-SnowCoder-1.3B surpasses StarCoder2-3B, pretrained on over 3.3T tokens, on HumanEval+, a benchmark that evaluates function-level code generation, and remains competitive on BigCodeBench. Our evaluation presents a comprehensive analysis justifying various design choices for Arctic-SnowCoder. Most importantly, we find that the key to high-quality data is its alignment with the distribution of downstream applications."
    },
    {
        "title": "Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction",
        "link_suffix": "/forum?id=lXRDQsiP2v",
        "link": "https://openreview.net/forum?id=lXRDQsiP2v",
        "pdf_link": "https://openreview.net/pdf?id=lXRDQsiP2v",
        "keywords": "white-box deep neural networks, representation learning, transformer",
        "abstract": "The attention operator is arguably the key distinguishing factor of transformer architectures, which have demonstrated state-of-the-art performance on a variety of tasks. However, transformer attention operators often impose a significant computational burden, with the computational complexity scaling quadratically with the number of tokens. In this work, we propose a novel transformer attention operator whose computational complexity scales linearly with the number of tokens. We derive our network architecture by extending prior work which has shown that a transformer style architecture naturally arises by \"white-box\" architecture design, where each layer of the network is designed to implement an incremental optimization step of a maximal coding rate reduction objective (MCR$^2$). Specifically, we derive a novel variational form of the MCR$^2$ objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ($\\texttt{TSSA}$). $\\texttt{TSSA}$ has $\\textit{linear computational and memory complexity}$ and radically departs from the typical attention architecture that computes pairwise similarities between tokens. Experiments on vision, language, and long sequence tasks show that simply swapping $\\texttt{TSSA}$ for standard self-attention, which we refer to as the Token Statistics Transformer ($\\texttt{ToST}$), achieves competitive performance with conventional transformers while being significantly more computationally efficient and interpretable.  Our results also somewhat call into question the conventional wisdom that pairwise similarity style attention mechanisms are critical to the success of transformer architectures."
    },
    {
        "title": "Learning Disentangled Representations for Fairness with Limited Demographics",
        "link_suffix": "/forum?id=XuQJ5a3sTb",
        "link": "https://openreview.net/forum?id=XuQJ5a3sTb",
        "pdf_link": "https://openreview.net/pdf?id=XuQJ5a3sTb",
        "keywords": "Fair representation learning, Disentanglement",
        "abstract": "Fair representation learning is a promising way to mitigate discrimination in downstream tasks. Many existing fair representation learning methods require access to sensitive information, but the collection of sensitive information is often difficult and even involves privacy issues.  Additionally, a model trained to be fair with respect to one sensitive attribute may not ensure fairness for other sensitive groups. Thus, how to flexibly address fairness issues when we have limited access to sensitive information is a challenging problem. In this work, we answer this question: ``given limited sensitive information, can we learn a representation to be fair w.r.t. varying sensitive groups?'' To achieve this, we propose a novel two-step framework. We first learn a disentangled representation by employing Non-linear Independent Component Analysis (Nonlinear ICA). Second, we remove sensitive information in the latent space to obtain fair representation. The learned representation can be easily adapted to be fair w.r.t different sensitive groups and to be used for different downstream tasks without re-training. Among the entire process, only a small portion of sensitive information is required in the second step to learn a fair representation. We compare with methods that require different amounts of sensitive information on real-world images and tabular datasets. We empirically demonstrate the utility and flexibility of our approach, and our method is capable of achieving improved fairness results in various tasks."
    },
    {
        "title": "Level-Set Parameters: Novel Data for 3D Shape Analysis",
        "link_suffix": "/forum?id=NoRvNK9eDp",
        "link": "https://openreview.net/forum?id=NoRvNK9eDp",
        "pdf_link": "https://openreview.net/pdf?id=NoRvNK9eDp",
        "keywords": "neural fields, signed distance fields, shape analysis",
        "abstract": "3D shape analysis has been widely explored based on traditional 3D data of point clouds and meshes, but the discrete nature of these data makes the analysis methods susceptible to variations in input resolutions. The recent development of neural fields brings in level-set parameters from signed distance functions as a novel, continuous, and numerical representation of 3D shapes, where the shape surfaces are defined as zero-level-sets of those functions. This motivated us to extend shape analysis from the traditional 3D data to these novel parameter data. Since the level-set parameters are not Euclidean like point clouds, we establish correlations across different shapes by formulating them as a pseudo-normal distribution, and learn the distribution prior from the respective dataset. To further explore the level-set parameters with shape transformations, we propose to condition a subset of these parameters on rotations and translations, and generate them with a hypernetwork. We demonstrate the potential of the novel continuous representation in pose-related shape analysis through applications to shape classification, retrieval under arbitrary poses, and 6D object pose estimation. Code and data in this research will be provided at github."
    },
    {
        "title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
        "link_suffix": "/forum?id=TljGdvzFq2",
        "link": "https://openreview.net/forum?id=TljGdvzFq2",
        "pdf_link": "https://openreview.net/pdf?id=TljGdvzFq2",
        "keywords": "Cross Capability, Law of the Weakest Link, LLM-as-a-Judge, Large Langauge Models",
        "abstract": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we termcross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the \"Law of the Weakest Link,\" where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios. The code, benchmarks, model responses, and evaluations are available at thisanonymous link."
    },
    {
        "title": "Can We Further Elicit Reasoning in LLMs? Critic-Guided Planning with Retrieval-Augmentation for Solving Challenging Tasks",
        "link_suffix": "/forum?id=4JfFW7d1gu",
        "link": "https://openreview.net/forum?id=4JfFW7d1gu",
        "pdf_link": "https://openreview.net/pdf?id=4JfFW7d1gu",
        "keywords": "reasoning, planning, retrieval-augmented generation",
        "abstract": "State-of-the-art large language models (LLMs) exhibit impressive problem-solving capabilities but may struggle with complex reasoning and factual correctness. Existing methods harness the strengths of chain-of-thought (CoT) and retrieval-augmented generation (RAG) to decompose a complex problem into simpler steps and apply retrieval to improve factual correctness. These methods work well on straightforward reasoning tasks but often falter on challenging tasks such as competitive programming and mathematics, due to frequent reasoning errors and irrelevant knowledge retrieval. To address this, we introduce Critic-guided planning with Retrieval-augmentation, CR-Planner, a novel framework that leverages fine-tuned critic models to guide both reasoning and retrieval processes through planning.\nCR-Planner solves a problem by iteratively selecting and executing sub-goals. Initially, it identifies the most promising sub-goal from reasoning, query generation, and retrieval, guided by rewards given by a critic model named sub-goal critic.  It then executes this sub-goal through sampling and selecting the optimal output based on evaluations from another critic model named execution critic.\nThis iterative process, informed by retrieved information and critic models, enables CR-Planner to effectively navigate the solution space towards the final answer.\nWe employ Monte Carlo Tree Search (MCTS) to collect the data for training the critic models, allowing for a systematic exploration of action sequences and their long-term impacts.\nWe validate CR-Planner on challenging domain-knowledge-intensive and reasoning-heavy tasks, including competitive programming, theorem-driven math reasoning, and complex domain retrieval problems. Our experiments demonstrate that CR-Planner significantly outperforms baselines, highlighting its effectiveness in addressing challenging problems by improving both reasoning and retrieval."
    },
    {
        "title": "Planning in Strawberry Fields: Evaluating and Improving the Planning and Scheduling Capabilities of LRM o1",
        "link_suffix": "/forum?id=jOuHjFw71C",
        "link": "https://openreview.net/forum?id=jOuHjFw71C",
        "pdf_link": "https://openreview.net/pdf?id=jOuHjFw71C",
        "keywords": "Large Language Models, Large Reasoning Models, Planning, Reasoning",
        "abstract": "The ability to plan a course of action that achieves a desired state of affairs has long been considered a core competence of intelligent agents and has been an integral part of AI research since its inception. With the advent of large language models (LLMs), there has been considerable interest in the question of whether or not they possess such planning abilities, but--despite the slew of new private and open source LLMs since GPT3--progress has remained slow. OpenAI claims that their recent o1 (Strawberry) model has been specifically constructed and trained to escape the normal limitations of autoregressive LLMs--making it a new kind of model: a Large Reasoning Model (LRM). In this paper, we evaluate the planning capabilities of two LRMs (o1-preview and o1-mini) on both planning and scheduling benchmarks. We see that while o1 does seem to offer significant improvements over autoregressive LLMs, this comes at a steep inference cost, while still failing to provide any guarantees over what it generates. We also show that combining o1 models with external verifiers--in a so-called LRM-Modulo system--guarantees the correctness of the combined system's output while further improving performance."
    },
    {
        "title": "Rehearsal-Free Continual Federated Learning with Synergistic Regularization",
        "link_suffix": "/forum?id=ZfFQrVoygN",
        "link": "https://openreview.net/forum?id=ZfFQrVoygN",
        "pdf_link": "https://openreview.net/pdf?id=ZfFQrVoygN",
        "keywords": "Federated Learning, Data Heterogeneity",
        "abstract": "Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding \\textit{knowledge forgetting} of previously seen tasks. \nTo tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. \nConsidering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named \\textbf{FedSSI}, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. \nFedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. \nExtensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods."
    },
    {
        "title": "BenTo: Benchmark Reduction with In-Context Transferability",
        "link_suffix": "/forum?id=ki7b0qD11r",
        "link": "https://openreview.net/forum?id=ki7b0qD11r",
        "pdf_link": "https://openreview.net/pdf?id=ki7b0qD11r",
        "keywords": "transfer learning, language model, benchmark reduction",
        "abstract": "Evaluating large language models (LLMs) is costly: it requires the generation and examination of LLM outputs on a large-scale benchmark of various tasks. This paper investigates how to efficiently reduce the tasks used to benchmark LLMs without affecting the evaluation quality. Our study reveals that task transferability and relevance provide critical information to identify the most representative subset of tasks via optimizing a facility location function. We propose a practically efficient metric for estimating the transferability between two tasks via in-context learning (ICL). By analyzing the pairwise transferability, we can reduce tasks in a modern LLM benchmark (e.g., MMLU or FLAN) to 5% while inducing only a $<4$% difference to the evaluation on the original benchmark. Compared to prior works, our method is training-free, gradient-free, and highly efficient requiring ICL only."
    },
    {
        "title": "The Complexity Dynamics of Grokking",
        "link_suffix": "/forum?id=07N9jCfIE4",
        "link": "https://openreview.net/forum?id=07N9jCfIE4",
        "pdf_link": "https://openreview.net/pdf?id=07N9jCfIE4",
        "keywords": "Compression, Complexity, Generalization, Grokking, Minimum Description Length",
        "abstract": "We investigate the phenomenon of generalization through the lens of compression. In particular, we study the complexity dynamics of neural networks to explain \\emph{grokking}, where networks suddenly transition from memorizing to generalizing solutions long after over-fitting the training data. To this end we introduce a new measure of intrinsic complexity for neural networks based on the theory of Kolmogorov complexity. Tracking this metric throughout network training, we find a consistent pattern in training dynamics, consisting of a rise and fall in complexity. We demonstrate that this corresponds to memorization followed by generalization. Based on insights from rate--distortion theory and the minimum description length principle, we lay out a principled approach to lossy compression of neural networks, and connect our complexity measure to explicit generalization bounds. Based on a careful analysis of information capacity in neural networks, we propose a new regularization method which encourages networks towards low-rank representations by penalizing their spectral entropy, and find that our regularizer outperforms baselines in total compression of the dataset."
    },
    {
        "title": "Understanding and Mitigating Miscalibration in Prompt Tuning for Vision-Language Models",
        "link_suffix": "/forum?id=zfgYC3sDt6",
        "link": "https://openreview.net/forum?id=zfgYC3sDt6",
        "pdf_link": "https://openreview.net/pdf?id=zfgYC3sDt6",
        "keywords": "Vision-Language Models, Confidence calibration, Outlier Regularization, Prompt Tuning",
        "abstract": "Confidence calibration is critical for the safe deployment of machine learning models in the real world.\nHowever, such issue in vision-language models like CLIP, particularly after fine-tuning, has not been fully addressed.\nIn this work, we demonstrate that existing prompt tuning methods usually lead to a trade-off of calibration between base and new classes:\nthe cross-entropy loss in CoOp causes overconfidence in new classes by increasing textual label divergence, whereas the regularization of KgCoOp maintains the confidence level but results in underconfidence in base classes due to the improved accuracy.\nInspired by the observations, we introduce Dynamic Outlier Regularization (DOR) to ensure the confidence calibration on both base and new classes after fine-tuning. \nIn particular, we propose to minimize the feature deviation of novel textual labels (instead of base classes) sampled from a large vocabulary.\nIn effect, DOR prevents the increase in textual divergence for new labels while easing restrictions on base classes.\nExtensive experiments demonstrate that DOR can enhance the calibration performance of current fine-tuning methods on base and new classes."
    },
    {
        "title": "Forward Learning with Differential Privacy",
        "link_suffix": "/forum?id=OYTDePFRLC",
        "link": "https://openreview.net/forum?id=OYTDePFRLC",
        "pdf_link": "https://openreview.net/pdf?id=OYTDePFRLC",
        "keywords": "Differential Privacy, Forward Learning, Likelihood Ratio Method",
        "abstract": "Differential privacy (DP) in deep learning is a critical concern as it ensures the confidentiality of training data while maintaining model utility.  Existing DP training algorithms provide privacy guarantees by clipping each individual backpropagated gradient and then injecting noise. Different from backpropagation, forward-learning algorithms based on perturbation inherently utilize randomness to estimate the gradient of each sample in parallel. These algorithms offer high parallelizability, suitability for non-differentiable modules, and applicability in black-box settings. Moreover, the introduction of noise during the forward pass indirectly provides randomness protection to the model parameters and their gradients, suggesting its potential for naturally providing differential privacy. In this paper, we propose a forward-learning algorithm, Differential Private Unified Likelihood Ratio method (DP-ULR), and demonstrate its differential privacy guarantees.  DP-ULR features a novel batch sampling operation with rejection, which we theoretically analyze in conjunction with classic differential privacy mechanisms. DP-ULR is also underpinned by a theoretically guided privacy controller that dynamically adjusts noise levels to manage privacy costs effectively in each training step. Our experiments indicate that DP-ULR achieves competitive performance compared to traditional differential privacy training algorithms based on backpropagation, maintaining the same privacy loss limits."
    },
    {
        "title": "Adversarial Masked Autoencoder Purifier with Defense Transferability",
        "link_suffix": "/forum?id=dZNI8DyUKY",
        "link": "https://openreview.net/forum?id=dZNI8DyUKY",
        "pdf_link": "https://openreview.net/pdf?id=dZNI8DyUKY",
        "keywords": "Adversarial attack, Adversarial defense, Purifier, Robustness, Security, Transferability",
        "abstract": "The study of adversarial defense still struggles to combat with advanced adversarial attacks.\nIn contrast to most prior studies that rely on the diffusion model for test-time defense to remarkably increase the inference time, we propose Masked AutoEncoder Purifier (MAEP), which integrates Masked AutoEncoder (MAE) into an adversarial purifier\nframework for test-time purification.\nWhile MAEP achieves promising adversarial robustness, it particularly features model defense transferability without relying on using additional data that is different from the training dataset. \nTo our knowledge, MAEP is the first study of adversarial purifier based on masked autoencoder.\nExtensive experiments validate the proposed method.\nNotably, MAEP trained on CIFAR10 achieves state-of-the-art performance even when tested directly on ImageNet, outperforming existing diffusion-based models trained specifically on ImageNet."
    }
]