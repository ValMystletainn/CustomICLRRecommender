[
    {
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "link_suffix": "/forum?id=ngmEcEer8a",
        "link": "https://openreview.net/forum?id=ngmEcEer8a",
        "pdf_link": "https://openreview.net/pdf?id=ngmEcEer8a",
        "keywords": "NLP, Pruning, Science of Deep Learning, Efficient Inference",
        "abstract": "Understandingwhereandhowknowledge is stored in LLMs is an active and important area of research. In this work, we take a model pruning approach: if removing certain parameters does not affect model output in question-answering knowledge benchmarks, then those parameters are likely are not useful for storing knowledge. To find these parameters, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning and can improve the memory and latency of inference. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge."
    },
    {
        "title": "RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark",
        "link_suffix": "/forum?id=MGZyUtaYUb",
        "link": "https://openreview.net/forum?id=MGZyUtaYUb",
        "pdf_link": "https://openreview.net/pdf?id=MGZyUtaYUb",
        "keywords": "Reinforcement Learning, Combinatorial Optimization, PyTorch, Benchmark",
        "abstract": "Deep reinforcement learning (RL) has recently shown significant benefits in solving combinatorial optimization (CO) problems, reducing reliance on domain expertise, and improving computational efficiency. However, the field lacks a unified benchmark for easy development and standardized comparison of algorithms across diverse CO problems. To fill this gap, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 23 state-of-the-art methods and 20+ CO problems. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configuration of diverse RL algorithms, neural network architectures, inference techniques, and environments. RL4CO allows researchers to seamlessly navigate existing successes and develop their unique designs, facilitating the entire research process by decoupling science from heavy engineering. We also provide extensive benchmark studies to inspire new insights and future work. RL4CO has attracted numerous researchers in the community and is open-sourced."
    },
    {
        "title": "FedGO : Federated Ensemble Distillation with GAN-based Optimality",
        "link_suffix": "/forum?id=4ftMNGeLsz",
        "link": "https://openreview.net/forum?id=4ftMNGeLsz",
        "pdf_link": "https://openreview.net/pdf?id=4ftMNGeLsz",
        "keywords": "Federated learning, ensemble distillation, data heterogeneity, generative adversarial network",
        "abstract": "For federated learning in practical settings, a significant challenge is the considerable diversity of data across clients. To tackle this data heterogeneity issue, it has been recognized that federated ensemble distillation is effective. Federated ensemble distillation requires an unlabeled dataset on the server, which could either be an extra dataset the server already possesses or a dataset generated by training a generator through a data-free approach. Then, it proceeds by generating pseudo-labels for the unlabeled data based on the predictions of client models and training the server model using this pseudo-labeled dataset. Consequently, the efficacy of ensemble distillation hinges on the quality  of these pseudo-labels, which, in turn, poses a challenge of appropriately assigning weights to client predictions for each data point, particularly in scenarios with data heterogeneity. In this work, we suggest a provably near-optimal weighting method for federated ensemble distillation, inspired by theoretical results in generative adversarial networks (GANs). Our weighting method utilizes client discriminators, trained at the clients based on a generator distributed from the server and their own datasets. \nOur comprehensive experiments on various image classification tasks illustrate that our method significantly improves the performance over baselines, under various scenarios with and without extra server dataset. Furthermore, we provide an extensive analysis of additional communication cost, privacy leakage, and computational burden caused by our weighting method."
    },
    {
        "title": "Reinforcement Learning and Heuristics for Hardware-Efficient Constrained Code Design",
        "link_suffix": "/forum?id=kBybSUskz7",
        "link": "https://openreview.net/forum?id=kBybSUskz7",
        "pdf_link": "https://openreview.net/pdf?id=kBybSUskz7",
        "keywords": "reinforcement learning, bipartite matching, GNN, combinatorial optimization, feature engineering, hardware design optimization, logic synthesis",
        "abstract": "Constrained codes enhance reliability in high-speed communication systems and optimize bit efficiency when working with non-binary data representations (e.g., three-level ternary symbols).  A key challenge in their design is minimizing the hardware complexity of the translation logic that encodes and decodes data. We introduce a reinforcement learning (RL)-based framework, augmented by a custom L1 similarity-based heuristic, to design hardware-efficient translation logic, navigating the vast solution space of codeword assignments. By modeling the task as a bipartite graph matching problem and using logic synthesis tools to evaluate hardware complexity, our RL approach outperforms human-derived solutions and generalizes to various code types. Finally, we analyze the learned policies to extract insights into high-performing strategies."
    },
    {
        "title": "Towards Understanding Memory buffer based Continual Learning",
        "link_suffix": "/forum?id=vNGv3dJATp",
        "link": "https://openreview.net/forum?id=vNGv3dJATp",
        "pdf_link": "https://openreview.net/pdf?id=vNGv3dJATp",
        "keywords": "continual learning, memory, catastrophic forgetting, generalization",
        "abstract": "Continual learning (CL) is a paradigm that adapts to and retains knowledge from a stream of tasks. Despite the growing number of experimental methods in CL, there is a lack of rigorous theoretical analysis, particularly in memory-based continual learning (MCL), which remains an open research area. In this paper, we theoretically analyze the impact of memory in CL and derive explicit expressions for expected forgetting and generalization errors under overparameterized linear models. We propose a detailed matrix decomposition of the data to distinguish between current and previous datasets, effectively decoupling the coupled data for different tasks. Additionally, we conduct a comprehensive mathematical analysis for scenarios with a small number of tasks and employ numerical analysis for larger task scenarios to evaluate the overall properties of expected forgetting and generalization errors. Compared with CL, our theoretical analysis suggests that (1) a larger memory buffer must be paired with a larger model to effectively reduce forgetting; (2) training with a larger memory buffer generalizes better when tasks are similar but may perform worse when tasks are dissimilar, while training with a large model can help mitigate this negative effect. Ultimately, our findings here sheds new light on how memory can assist CL in mitigating catastrophic forgetting and improving generalization."
    },
    {
        "title": "Wyckoff Transformer: Generation of Symmetric Crystals",
        "link_suffix": "/forum?id=ursX3k1rTO",
        "link": "https://openreview.net/forum?id=ursX3k1rTO",
        "pdf_link": "https://openreview.net/pdf?id=ursX3k1rTO",
        "keywords": "material design, machine learning, crystal generation, space group symmetry, Transformer, Wyckoff position, generative model, autoregressive model, permutation invariance",
        "abstract": "We propose Wyckoff Transformer, a generative model for materials conditioned on space group symmetry. Most real-world inorganic materials have internal symmetry beyond lattice transition. Symmetry rules that atoms obey play a fundamental role in determining the physical, chemical, and electronic properties of crystals. These symmetries form energy configurations, determine stability, and influence key material structural and functional properties such as electrical and thermal conductivity, optical and polarization behavior, and mechanical strength. And yet, despite the recent advancements, state-of-the-art diffusion models struggle to generate highly symmetric crystals. We use Wyckoff positions as the basis for an elegant, compressed, and discrete structure representation. To model the distribution we develop a permutation-invariant autoregressive model based on Transformer and absence of positional encoding. Our experiments demonstrate that Wyckoff Transformer has the best performance in generating novel diverse stable structures conditioned on the symmetry space group, while also having competitive metric values when compared to model not conditioned on symmetry. We also show that it is able to predict formation energy and band gap within DFT accuracy."
    },
    {
        "title": "AuPair: Golden Example Pairs for Code Repair",
        "link_suffix": "/forum?id=iEdEHPcFeu",
        "link": "https://openreview.net/forum?id=iEdEHPcFeu",
        "pdf_link": "https://openreview.net/pdf?id=iEdEHPcFeu",
        "keywords": "LLM, Coding",
        "abstract": "Scaling up inference-time compute has proven to be a valuable strategy in improving the performance of Large Language Models (LLMs) on several tasks without involving any fine-tuning. An example of such a task that can benefit from additional inference-time compute is self-repair: given an initial flawed response produced by the LLM, it is supposed to correct its own mistake and produce an improved response. We propose leveraging the in-context learning capability exhibited by LLMs to aid with self-repair. The key contribution of this paper is an approach to synthesise and select a golden set of pairs, each of which contains a problem, the initial guess produced by the LLM, and the consequent fix generated. Each golden example pair, or AuPair is then provided as an in-context example at inference time to generate a candidate repaired solution with 1-shot prompting; in line with best-of-$N$ the highest scoring response is selected. Given an inference-time compute budget of $N$ LLM calls, our algorithm selects $N$ AuPairs in a manner that maximises complementarity and usefulness. We demonstrate the results of our algorithm on the coding domain for code repair on 4 LLMs across 7 competitive programming datasets. The AuPairs produced by our approach provide a significant boost in performance compared to best-of-$N$, and also exhibit strong generalisation across datasets and models. Moreover, our approach shows strong performance as the inference-time compute budget $N$ is scaled up."
    },
    {
        "title": "Attack on LLMs: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem",
        "link_suffix": "/forum?id=0owyEm6FAk",
        "link": "https://openreview.net/forum?id=0owyEm6FAk",
        "pdf_link": "https://openreview.net/pdf?id=0owyEm6FAk",
        "keywords": "LoRA, PEFT, LLM Safety, Backdoor, Backdoor Attack",
        "abstract": "Finetuning large language models (LLMs) with LoRA has gained significant popularity due to its simplicity and effectiveness. Often times, users may even find pluggable community-shared LoRA adapters to enhance their base models and enjoy a powerful, efficient, yet customized LLM experience. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can tamper with existing LoRA adapters and distribute malicious versions to the community. \nDespite the high-risk potential, no prior work has explored LoRA's attack surface under the share-and-play context. In this paper, we address this gap by investigating how backdoors can be injected into task-enhancing LoRA adapters and studying the mechanisms of such infection. We demonstrate that with a simple but specific recipe, a backdoor-infected LoRA can be trained once, then directly merged with multiple LoRA adapters finetuned on different tasks while retaining both its malicious and benign capabilities; which enables attackers to distribute compromised LoRAs at scale with minimal effort. Our work highlights the need for heightened security awareness in the LoRA ecosystem. Warning: the paper contains potentially offensive content generated by models."
    },
    {
        "title": "Grey-box Prompt Optimization and Fine-Tuning for Cloud-Edge LLM Agents",
        "link_suffix": "/forum?id=kndxjyKxX2",
        "link": "https://openreview.net/forum?id=kndxjyKxX2",
        "pdf_link": "https://openreview.net/pdf?id=kndxjyKxX2",
        "keywords": "large language model, zeroth-order optimization, prompt optimization",
        "abstract": "Large Language Models (LLMs) are transforming the landscape of generative AI, delivering groundbreaking performance across diverse tasks. Yet, their immense model sizes tether most LLMs to the cloud, posing challenges for tasks that demand processing private and proprietary data. In this paper, we introduce a grey-box prompt optimization and fine-tuning framework for cloud-edge LLMs-paving the way for a seamless, hybrid approach that merges the best of both private and public cloud environments. This framework not only boosts flexibility and scalability but also empowers users with heightened security and compliance, optimizing cost and performance. Beyond that, it ensures robust disaster recovery and business continuity through redundancy and smart workload distribution. At the heart of our solution is an efficient algorithm with guaranteed convergence, specifically tailored to the structure of the grey-box optimization problem. We rigorously analyze and derive its non-asymptotic convergence rate. Our extensive experiments reveal that sandwiched tuning-our novel fine-tuning method-delivers up to a 47.9% performance improvement over traditional methods across multiple tasks."
    },
    {
        "title": "Convergence Of Consistency Model With Multistep Sampling Under General Data Assumptions",
        "link_suffix": "/forum?id=XK5jYtLMXl",
        "link": "https://openreview.net/forum?id=XK5jYtLMXl",
        "pdf_link": "https://openreview.net/pdf?id=XK5jYtLMXl",
        "keywords": "Consistency models, diffusion models, learning theory",
        "abstract": "Diffusion models accomplish remarkable success in data generation tasks across various domains. However, the iterative sampling process is computationally expensive. Consistency models are proposed to learn consistency functions to map from noise to data directly, which allows one-step fast data generation and multistep sampling to improve sample quality. In this paper, we study the convergence of consistency models when the self-consistency property holds approximately under the training distribution. Our analysis requires only mild data assumption and applies to a family of forward processes. When the target data distribution has bounded support or has tails that decay sufficiently fast, we show that the samples generated by the consistency model are close to the target distribution in Wasserstein distance; when the target distribution satisfies some smoothness assumption, we show that with an additional perturbation step for smoothing, the generated samples are close to the target distribution in total variation distance. We provide two case studies with commonly chosen forward processes to demonstrate the benefit of multistep sampling."
    },
    {
        "title": "Debiased Medical Report Generation with High-Frequency Amplification",
        "link_suffix": "/forum?id=3AAXabeZPG",
        "link": "https://openreview.net/forum?id=3AAXabeZPG",
        "pdf_link": "https://openreview.net/pdf?id=3AAXabeZPG",
        "keywords": "Medical Report Generation, Debiased Generation, Visual Bias, Textual Bias, Frequency Bias, Fourier Transform, High-pass Filtering",
        "abstract": "In recent years, automated medical report generation (MRG) has gained significant research value for its potential to reduce workload and prevent diagnostic errors. However, generating accurate radiology reports remains challenging due to the prevalence of normal regions in X-ray images and normal descriptions in medical reports. Despite various efforts to address these issues, the definitions of visual bias and textual bias remain unclear and there is still a lack of comprehensive analysis of how these biases affect model behavior. \nIn this work, we rigorously define and conduct an in-depth examination of visual and textual biases inherent in MRG dataset. Our analysis emphasizes that global patterns, such as normal regions and findings, contribute to visual and textual bias. Further, we discuss how these biases make MRG models especially prone to frequency bias, where models tend to prioritize low-frequency signals that capture global patterns, while neglecting high-frequency signals. To debiase the frequency bias, we propose the high-frequency amplification layer (HAL), aimed at enhancing the model's perceptiveness to fine-grained details. Our extensive experiments show that by amplifying high-frequency signals, HAL reduces both visual and textual biases, leading to improved performance in MRG tasks."
    },
    {
        "title": "FreeFlow: Latent Flow Matching for Free Energy Difference Estimation",
        "link_suffix": "/forum?id=D2EdWRWEQo",
        "link": "https://openreview.net/forum?id=D2EdWRWEQo",
        "pdf_link": "https://openreview.net/pdf?id=D2EdWRWEQo",
        "keywords": "free energy, flow matching, free energy perturbation, computational biology",
        "abstract": "Estimating free energy differences between molecular systems is fundamental for understanding molecular interactions and accelerating drug discovery. Current techniques use molecular dynamics to sample the Boltzmann distributions of the two systems and of several intermediate \"alchemical\" distributions that interpolate between them. From the resulting ensembles, free energy differences can be estimated by averaging importance weight analogs for multiple distributions. Instead of time-intensive simulations of intermediate alchemical systems, we learn a fast-to-train flow to bridge the two systems of interest. After training, we obtain free energy differences by integrating the flow's instantaneous change of variables when transporting samples between the two distributions. To map between molecular systems with different numbers of atoms, we replace the previous solutions of simulating auxiliary \"dummy atoms\" by additionally training two autoencoders that project the systems into a same-dimensional latent space in which our flow operates. A generalized change of variables formula for trans-dimensional mappings allows us to employ the dimensionality collapsing and expanding autoencoders in our free energy estimation pipeline. We validate our approach on systems of increasing complexity: mapping between Gaussians, between subspaces of alanine dipeptide, and between pharmaceutically relevant ligands in solvent. All results show strong agreement with reference values."
    },
    {
        "title": "OpenPL: Realistic Evaluation of Prompt Learning for VLM in Open Environments",
        "link_suffix": "/forum?id=veiSkPqIXm",
        "link": "https://openreview.net/forum?id=veiSkPqIXm",
        "pdf_link": "https://openreview.net/pdf?id=veiSkPqIXm",
        "keywords": "VLM; Prompt Learning; Open environments",
        "abstract": "Vision-language models (VLMs) have demonstrated impressive zero-shot capabilities across various image classification tasks. Their performance can be further enhanced through prompt learning methods. To evaluate the effectiveness of prompt learning, it is important to assess its robustness to new classes and distributional shifts. However, current studies typically assume single data distribution shifts and pre-known new class space, which still have gaps with real-world open environments where data distributions and classes are often uncertain and subject to continuous change. To better analyze the robustness of prompt learning methods in more realistic scenarios, we propose a novel evaluation benchmark called OpenPL from the following perspectives: 1) We reconstruct multiple scenarios of open environments, encompassing dynamic class changes, dynamic distribution shifts, and dynamic co-evolution of both distribution and classes; 2) We propose a series of new performance metrics for prompt learning methods based on the Dynamic Robustness Curve (DRC) to better understand their robustness in open environments; 3) We re-implement diverse prompt learning methods and evaluate their performance on the proposed OpenPL benchmark. The results show that no current prompt learning method is robust to open environments and no meaningful performance improvement is achieved compared to the zero-shot performance, designing robust prompt learning methods remains a difficult task. All re-implementations are available at \\url{https://anonymous.4open.science/r/OpenPL-565E}."
    },
    {
        "title": "TopoLM: brain-like spatio-functional organization in a topographic language model",
        "link_suffix": "/forum?id=aWXnKanInf",
        "link": "https://openreview.net/forum?id=aWXnKanInf",
        "pdf_link": "https://openreview.net/pdf?id=aWXnKanInf",
        "keywords": "language modeling, topography, fMRI, neuroscience",
        "abstract": "Neurons in the brain are spatially organized such that neighbors on tissue often exhibit similar response profiles. In the human language system, experimental studies have observed clusters for syntactic and semantic categories, but the mechanisms underlying this functional organization remain unclear. Here, building on work from the vision literature, we develop TopoLM, a transformer language model with an explicit two-dimensional spatial representation of model units. By combining a next-token prediction objective with a spatial smoothness loss, representations in this model assemble into clusters that correspond to semantically interpretable groupings of text and closely match the functional organization in the brain's language system. TopoLM successfully predicts the emergence of the spatio-functional organization of a cortical language system as well as the organization of functional clusters selective for fine-grained linguistic features empirically observed in human cortex. Our results suggest that the functional organization of the human language system is driven by a unified spatial objective, and provide a functionally and spatially aligned model of language processing in the brain."
    },
    {
        "title": "Bayesian-LoRA: LoRA based Parameter Efficient Fine-Tuning using Optimal Quantization levels and Rank Values trough Differentiable Bayesian Gates",
        "link_suffix": "/forum?id=v4Bl6tfaaO",
        "link": "https://openreview.net/forum?id=v4Bl6tfaaO",
        "pdf_link": "https://openreview.net/pdf?id=v4Bl6tfaaO",
        "keywords": "PEFT, LORA",
        "abstract": "It is a common practice in natural language processing to pre-train a single model on a general domain and then fine-tune it for downstream tasks. However, when it comes to Large Language Models, fine-tuning the entire model can be computationally expensive, resulting in very intensive energy consumption. As a result, several Parameter Efficient Fine-Tuning (PEFT) approaches were recently proposed. One of the most popular approaches is low-rank adaptation (LoRA), where the key insight is decomposing the updated weights of the pre-trained model into two low-rank matrices. However, the proposed approaches either use the same rank value across all different weight matrices, which has been shown to be a sub-optimal choice, or do not use any quantization technique, one of the most important factors when it comes to a model's energy consumption. In this work, we propose Bayesian-LoRA, a new method that approaches low-rank adaptation and quantization from a Bayesian perspective by employing a prior distribution on both quantization levels and rank values. As a result, B-LoRA is able to fine-tune a pre-trained model on a specific downstream task, finding the optimal rank values and quantization levels for every low-rank matrix. We validate the proposed model by fine-tuning a pre-trained DeBERTaV3 on the GLUE benchmark. Additionally, we fine-tune Phi-2 and Qwen, and evaluate them on few-shot and zero-shot MMLU. We compare our proposed method with relevant baselines and present both qualitative and quantitative results, showing its ability to learn optimal-rank quantized matrices. B-LoRA performs on par with or better than the baselines while reducing the total number of bit operations by roughly 70% compared to the baseline methods."
    },
    {
        "title": "Variance-Covariance Regularization Improves Representation Learning",
        "link_suffix": "/forum?id=jqff3wzkLT",
        "link": "https://openreview.net/forum?id=jqff3wzkLT",
        "pdf_link": "https://openreview.net/pdf?id=jqff3wzkLT",
        "keywords": "Regularization, Variance-Covariance Regularization",
        "abstract": "Transfer learning plays a key role in advancing machine learning models, yet conventional supervised pretraining often undermines feature transferability by prioritizing features that minimize the pretraining loss. In this work, we adapt a self-supervised learning regularization technique from the VICReg method to supervised learning contexts, introducing Variance-Covariance Regularization (VCReg). This adaptation encourages the network to learn high-variance, low-covariance representations, promoting learning more diverse features. We outline best practices for an efficient implementation of our framework, including applying it to the intermediate representations. Through extensive empirical evaluation, we demonstrate that our method significantly enhances transfer learning for images and videos, achieving state-of-the-art performance across numerous tasks and datasets. VCReg also improves performance in scenarios like long-tail learning and hierarchical classification. Additionally, we show its effectiveness may stem from its success in addressing challenges like gradient starvation and neural collapse. In summary, VCReg offers a universally applicable regularization framework that significantly advances transfer learning and highlights the connection between gradient starvation, neural collapse, and feature transferability."
    },
    {
        "title": "Distilling Dataset into Neural Field",
        "link_suffix": "/forum?id=nCrJD7qPJN",
        "link": "https://openreview.net/forum?id=nCrJD7qPJN",
        "pdf_link": "https://openreview.net/pdf?id=nCrJD7qPJN",
        "keywords": "Dataset distillation, Dataset condensation, Neural field",
        "abstract": "Utilizing large-scale datasets is essential for training high-performance deep learning models, but it also comes with substantial computation and storage costs. To overcome these challenges, dataset distillation has emerged as a promising solution by compressing large-scale datasets into smaller synthetic versions that retain the essential information needed for training. This paper proposes a novel parameterization framework for dataset distillation, coined Distilling Dataset into Neural Field (DDiF), which leverages the neural field to store the necessary information of large-scale datasets. Due to the unique nature of the neural field, which takes coordinates as input and output quantity, DDiF effectively preserves the information and easily generates various shapes of data. Beyond the efficacy, DDiF has larger feature coverage than some previous literature if same budget is allowed, which is proved from the frequency domain perspective. Under the same budget setting, this larger coverage leads to a significant performance improvement in downstream tasks by providing more synthetic instances due to the coding efficiency. DDiF demonstrates both theoretical and empirical evidence of its ability to operate efficiently within a limited budget, while better preserving the information of the original dataset compared to conventional parameterization methods."
    },
    {
        "title": "Relax and Merge: A Simple Yet Effective Framework for Solving Fairk-Means andk-sparse Wasserstein Barycenter Problems",
        "link_suffix": "/forum?id=n8h1z588eu",
        "link": "https://openreview.net/forum?id=n8h1z588eu",
        "pdf_link": "https://openreview.net/pdf?id=n8h1z588eu",
        "keywords": "clustering, k-means, fairness, approxiamte algorithm, optimal transport",
        "abstract": "The fairness of clustering algorithms has gained widespread attention across various areas, including machine learning, In this paper, we study fair $k$-means clustering in Euclidean space. \n  Given a dataset comprising several groups, the fairness constraint requires that each cluster should contain a proportion of points from each group within specified lower and upper bounds. \n  Due to these fairness constraints, determining the optimal locations of $k$ centers is a quite challenging task. \n  We propose a novel ``Relax and Merge'' framework that returns a $(1+4\\rho + O(\\epsilon))$-approximate solution,  where $\\rho$ is the approximate ratio of an off-the-shelf vanilla $k$-means algorithm and $O(\\epsilon)$ can be an arbitrarily small positive number. If equipped with a PTAS of $k$-means, our solution can achieve an approximation ratio of $(5+O(\\epsilon))$  with only a slight violation of the fairness constraints, which improves the current state-of-the-art approximation guarantee. Furthermore, using our framework, we can also obtain a $(1+4\\rho +O(\\epsilon))$-approximate solution for the $k$-sparse Wasserstein Barycenter problem, which is a fundamental optimization problem in the field of optimal transport, and a $(2+6\\rho)$-approximate solution for the strictly fair $k$-means clustering with no violation, both of which are better than the current state-of-the-art methods. In addition, the empirical results demonstrate that our proposed algorithm can significantly outperform baseline approaches in terms of clustering  cost."
    },
    {
        "title": "Masked Generative Priors Improve World Models Sequence Modelling Capabilities",
        "link_suffix": "/forum?id=2gTEW29qsM",
        "link": "https://openreview.net/forum?id=2gTEW29qsM",
        "pdf_link": "https://openreview.net/pdf?id=2gTEW29qsM",
        "keywords": "World Modeling, Model based RL",
        "abstract": "Deep Reinforcement Learning (RL) has become the leading approach for creating artificial agents in complex environments. Model-based approaches, which are RL methods with world models that predict environment dynamics, are among the most promising directions for improving data efficiency, forming a critical step toward bridging the gap between research and real-world deployment. In particular, world models enhance sample efficiency by learning in imagination, which involves training a generative sequence model of the environment in a self-supervised manner.\nRecently, Masked Generative Modelling has emerged as a more efficient and superior inductive bias for modelling and generating token sequences. Building on the Efficient Stochastic Transformer-based World Models (STORM) architecture, we replace the traditional MLP prior with a Masked Generative Prior (e.g., MaskGIT Prior) and introduce GIT-STORM.\nWe evaluate our model on two downstream tasks: reinforcement learning and video prediction. GIT-STORM demonstrates substantial performance gains in RL tasks on the Atari 100k benchmark.\nMoreover, we apply Transformer-based World Models to continuous action environments for the first time, addressing a significant gap in prior research. To achieve this, we employ a state mixer function that integrates latent state representations with actions, enabling our model to handle continuous control tasks. We validate this approach through qualitative and quantitative analyses on the DeepMind Control Suite, showcasing the effectiveness of Transformer-based World Models in this new domain.\nOur results highlight the versatility and efficacy of the MaskGIT dynamics prior, paving the way for more accurate world models and effective RL policies."
    },
    {
        "title": "Unlocking Trilevel Learning with Level-Wise Zeroth Order Constraints: Distributed Algorithms and Provable Non-Asymptotic Convergence",
        "link_suffix": "/forum?id=oaRaaG1WB1",
        "link": "https://openreview.net/forum?id=oaRaaG1WB1",
        "pdf_link": "https://openreview.net/pdf?id=oaRaaG1WB1",
        "keywords": "Trilevel Optimization, Distributed Optimization, Zeroth Order Optimization",
        "abstract": "Trilevel learning (TLL) found diverse applications in numerous machine learning applications, ranging from robust hyperparameter optimization to domain adaptation. However, existing researches primarily focus on scenarios where TLL can be addressed with first order information available at each level, which is inadequate in many situations involving zeroth order constraints, such as when black-box models are employed. Moreover, in trilevel learning, data may be distributed across various nodes, necessitating strategies to address TLL problems without centralizing data on servers to uphold data privacy. To this end, an effective distributed trilevel zeroth order learning framework DTZO is proposed in this work to address the TLL problems with level-wise zeroth order constraints in a distributed manner. The proposed DTZO is versatile and can be adapted to a wide range of (grey-box) TLL problems with partial zeroth order constraints. In DTZO, the cascaded polynomial approximation can be constructed without relying on gradients or sub-gradients, leveraging a novel cut, i.e., zeroth order cut. Furthermore, we theoretically carry out the non-asymptotic convergence rate analysis for the proposed DTZO in achieving the $\\epsilon$-stationary point. Extensive experiments have been conducted to demonstrate and validate the superior performance of the proposed DTZO, e.g., it approximately achieves up to a 40% improvement in performance."
    },
    {
        "title": "ETGL-DDPG: A Deep Deterministic Policy Gradient Algorithm for Sparse Reward Continuous Control",
        "link_suffix": "/forum?id=5m43PEd3sz",
        "link": "https://openreview.net/forum?id=5m43PEd3sz",
        "pdf_link": "https://openreview.net/pdf?id=5m43PEd3sz",
        "keywords": "Deep Reinforcement Learning, Sparse Reward Continuous Control, Exploration with options, Reward propagation",
        "abstract": "We consider deep deterministic policy gradient (DDPG) in the context of reinforcement learning with sparse rewards. To enhance exploration, we introduce a search procedure, \\emph{${\\epsilon}{t}$-greedy}, which generates exploratory options for exploring less-visited states. We prove that search using $\\epsilon t$-greedy has polynomial sample complexity under mild MDP assumptions. To more efficiently use the information provided by rewarded transitions, we develop a new dual experience replay buffer framework, \\emph{GDRB}, and implement \\emph{longest n-step returns}. The resulting algorithm, \\emph{ETGL-DDPG}, integrates all three techniques: \\bm{$\\epsilon t$}-greedy, \\textbf{G}DRB, and \\textbf{L}ongest $n$-step, into DDPG. We evaluate ETGL-DDPG on standard benchmarks and demonstrate that it outperforms DDPG, as well as other state-of-the-art methods, across all tested sparse-reward continuous environments. Ablation studies further highlight how each strategy individually enhances the performance of DDPG in this setting."
    },
    {
        "title": "STELLA: Leveraging Structural Representations to Enhance Protein Understanding with Multimodal LLMs",
        "link_suffix": "/forum?id=X7SQiI5Uul",
        "link": "https://openreview.net/forum?id=X7SQiI5Uul",
        "pdf_link": "https://openreview.net/pdf?id=X7SQiI5Uul",
        "keywords": "Protein Function Prediction, Enzyme-Catalyzed Reaction Prediction, Multimodal Large Language Models, Structural Representations, Protein Biology, Computational Biology",
        "abstract": "Understanding proteins based on tertiary structures is foundational in protein science, such as figuring out protein functions and enzyme-catalyzed reactions, as highlighted in this study. Accurate prediction is essential for elucidating the biological roles of proteins, advancing disease research, drug discovery, deciphering metabolic pathways and designing enzymes for medical and biotechnological applications. However, traditional methods often struggle to integrate these tasks effectively, especially when solely relying on structural data. Furthermore, these approaches typically lack the ability to incorporate iterative feedback from domain expertsâ€”a critical aspect of the complex and evolving nature of protein research. To address these challenges, we present STELLA, a multimodal large language model (LLM) that leverages structural representations to enhance protein understanding. By bridging the gap between structural representations and the contextual knowledge encoded within LLMs, STELLA harnesses the capabilities of LLMs enriched with structural information, offering interactive and versatile predictions across protein-related tasks. This approach provides a novel paradigm for understanding proteins, extending the limits of capabilities of LLM-based approaches in protein biology. Comprehensive experimental evaluations demonstrate STELLA's superior performance in both tasks, signalling as a potential approach in these domains. This study underscores the effectiveness of integrating structural data with LLMs, highlighting the transformative potential of multimodal LLMs for future research in protein biology, and affirming the value of continued exploration in this field. To foster collaboration and drive further innovation, we provide open access to the code, datasets, and pre-trained models. Please visit the anonymous GitHub repository via \\url{https://anonymous.4open.science/r/STELLA-DF00}."
    },
    {
        "title": "LoRe - Logarithm Regularization for Few-Shot Class Incremental Learning",
        "link_suffix": "/forum?id=veNewXAdHE",
        "link": "https://openreview.net/forum?id=veNewXAdHE",
        "pdf_link": "https://openreview.net/pdf?id=veNewXAdHE",
        "keywords": "Few-Shot Class Incremental Learning, Continual Learning, Logarithmic Regularization, Wide Minima",
        "abstract": "Few-Shot Class-Incremental Learning (FSCIL) aims to adapt to new classes with very limited data, while remembering information about all the previously seen classes. Current FSCIL methods freeze the feature extractor in the incremental sessions to prevent catastrophic forgetting. However, to perform well on the incremental classes, many methods reserve feature spaces during base training to allow\nsufficient space for incremental classes. We hypothesize that such feature space reservation sharpens the minima of the loss-landscape, resulting in sub-optimal performance. Motivated by the superior generalization of wide minima, we propose LoRe - logarithm regularization to guide the model optimization to wider minima. Moreover, we propose a denoised distance metric when considering similarity with the poorly calibrated prototypes. Comprehensive evaluations across three benchmark datasets reveal that LoRe not only achieves state-of-the-art performance but also produces more robust prototypes. Additionally, we demonstrate that LoRe can be leveraged to enhance the performance of existing methods."
    },
    {
        "title": "Reliable and Efficient Amortized Model-based Evaluation",
        "link_suffix": "/forum?id=mIl15VP7vt",
        "link": "https://openreview.net/forum?id=mIl15VP7vt",
        "pdf_link": "https://openreview.net/pdf?id=mIl15VP7vt",
        "keywords": "Model Evaluation, Amortization, Adaptive Testing",
        "abstract": "Current generative model evaluation procedures are costly and sensitive to test set selection, making continuous monitoring impractical. In this paper, we employ a model-based evaluation framework using Item Response Theory (IRT), which decouples model performance from the test subset selection, ensuring reliable and efficient evaluation. We propose two innovations: amortized calibration to reduce the cost of estimating item parameters of the IRT model and an item generator based on a large language model to automate diverse question generation. Our experiments on 24 common natural language processing benchmarks and 180 language models show that this approach is more reliable and resource-efficient compared to traditional evaluation methods, offering a scalable solution to evaluate generative models."
    },
    {
        "title": "Neural Dueling Bandits: Principled Preference-Based Optimization with Non-Linear Reward Function",
        "link_suffix": "/forum?id=VELhv9BBfn",
        "link": "https://openreview.net/forum?id=VELhv9BBfn",
        "pdf_link": "https://openreview.net/pdf?id=VELhv9BBfn",
        "keywords": "Contextual Duling Bandits, Preferences Learning, Neural Bandits",
        "abstract": "Contextual dueling bandit is used to model the bandit problems, where a learner's goal is to find the best arm for a given context using observed noisy preference feedback over the selected arms for the past contexts. However, existing algorithms assume the reward function is linear, which can be complex and non-linear in many real-life applications like online recommendations or ranking web search results. To overcome this challenge, we use a neural network to estimate the reward function using preference feedback for the previously selected arms. We propose upper confidence bound- and Thompson sampling-based algorithms with sub-linear regret guarantees that efficiently select arms in each round. We also extend our theoretical results to contextual bandit problems with binary feedback, which is in itself a non-trivial contribution. Experimental results on the problem instances derived from synthetic datasets corroborate our theoretical results."
    }
]