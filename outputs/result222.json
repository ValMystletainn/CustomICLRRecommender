[
    {
        "title": "Dynamic Compression Strategies for Uniform Low-Dimensional Representations in Human Brain and Neural Network",
        "link_suffix": "/forum?id=HoQbynIkh2",
        "link": "https://openreview.net/forum?id=HoQbynIkh2",
        "pdf_link": "https://openreview.net/pdf?id=HoQbynIkh2",
        "keywords": "Low-Dimensional Representations, Generalization, Neural Manifold",
        "abstract": "Recent studies suggest that the generalization performance of neural networks is strongly linked to their ability to learn low-dimensional data representations. However, limited attention has been given to the consistency of compression across different types of input data. In this work, we compute the intrinsic dimensions of raw data and their corresponding representations to quantify the extent of information compression in neural networks. Our results indicate that the pre-trained model CLIP compresses complex datasets significantly more than simpler ones and tends to represent diverse datasets with uniform low-dimensional manifolds. Similarly, we observe stable dimensionality in neural manifolds in the brain across various tasks and cognitive processes, suggesting that biological systems also favor consistent low-dimensional representations. Theoretically, we demonstrate that lower-dimensional manifolds increase the probability of interpolation, facilitating the representation of new samples as convex combinations of existing data. Additionally, we derive an upper bound on generalization error within the interpolation regime, which tightens as the dimensionality of the data decreases. These findings underscore the critical role of uniform low-dimensional manifolds in supporting efficient and generalizable information representation in both artificial and biological neural systems."
    },
    {
        "title": "Learning from End User Data with Shuffled Differential Privacy",
        "link_suffix": "/forum?id=QjSOgxJ0hp",
        "link": "https://openreview.net/forum?id=QjSOgxJ0hp",
        "pdf_link": "https://openreview.net/pdf?id=QjSOgxJ0hp",
        "keywords": "differential privacy, shuffled differential privacy, kernel density estimation, kde",
        "abstract": "We study a setting of collecting and learning from private data distributed across end users.\nIn the shuffled model of differential privacy, the end users partially protect their data locally before sharing it, and their data is also anonymized during its collection to enhance privacy. \nThis model has recently become a prominent alternative to central DP, which requires full trust in a central data curator, and local DP, where fully local data protection takes a steep toll on downstream accuracy.Our main technical result is a shuffled DP protocol for privately estimating the kernel density function of a distributed dataset, with accuracy essentially matching central DP. \nWe use it to privately learn a classifier from the end user data, by learning a private density function per class. \nMoreover, we show that the density function itself can recover the semantic content of its class, despite having been learned in the absence of any unprotected data. \nOur experiments show the favorable downstream performance of our approach, and highlight key downstream considerations and trade-offs in a practical ML deployment of shuffled DP."
    },
    {
        "title": "Learning High-Degree Parities: The Crucial Role of the Initialization",
        "link_suffix": "/forum?id=OuNIWgGGif",
        "link": "https://openreview.net/forum?id=OuNIWgGGif",
        "pdf_link": "https://openreview.net/pdf?id=OuNIWgGGif",
        "keywords": "initialization, neural networks, gradient descent, parity functions, complexity, initial alignment",
        "abstract": "Parities have become a standard benchmark for evaluating learning algorithms. Recent works show that regular neural networks trained by gradient descent can efficiently learn degree $k$ parities on uniform inputs for constant $k$, but fail to do so when $k$ and $d-k$ grow with $d$ (here $d$ is the ambient dimension). However, the case where $k=d-O_d(1)$, including the degree $d$ parity (the full parity), has remained unsettled. This paper shows that for gradient descent on regular neural networks, learnability depends on the initial weight distribution. On one hand, the discrete Rademacher initialization enables efficient learning, while on the other hand, its Gaussian perturbation with large enough constant standard deviation $\\sigma$ prevents it. The positive result is shown to hold up to $\\sigma=O(d^{-1})$, pointing to questions about a sharper threshold phenomenon. Unlike statistical query (SQ) learning, where a singleton function class like the full parity is  trivially learnable, our negative result applies to a fixed function and relies on an inital gradient alignment measure of potential broader relevance to neural networks learning."
    },
    {
        "title": "SLiM: One-shot Quantized Sparse Plus Low-rank Approximation of LLMs",
        "link_suffix": "/forum?id=Usa4pF1e5I",
        "link": "https://openreview.net/forum?id=Usa4pF1e5I",
        "pdf_link": "https://openreview.net/pdf?id=Usa4pF1e5I",
        "keywords": "sparsity, 2:4 sparsity, quantization, low-rank, lora",
        "abstract": "Large Language Models (LLMs) have revolutionized natural language understanding and generation tasks but suffer from high memory consumption and slow inference times due to their large parameter sizes. Traditional model compression techniques, such as quantization and pruning, mitigate these issues but often require retraining to maintain accuracy, which is computationally expensive. This paper introduces SLiM, a novel approach for compressing LLMs using a one-shot Quantized Sparse Plus Low-rank Approximation. SLiM eliminates the need for costly retraining by combining a symmetric quantization method (SLiM-Quant) with a saliency-based low-rank approximation. Our method reduces quantization error while leveraging sparse representations compatible with accelerated hardware architectures. Additionally, we propose a parameter-efficient fine-tuning recipe that significantly reduces overhead compared to conventional quantization-aware training. SLiM achieves up to a 5.4% improvement in model accuracy for sparsity patterns like 2:4, and the fine-tuning step further enhances accuracy by up to 5.6%, demonstrating state-of-the-art performance. This work provides a pathway for efficiently deploying large models in memory-constrained environments without compromising accuracy."
    },
    {
        "title": "Filling the Gaps: LLMs for Causal Hypothesis Generation",
        "link_suffix": "/forum?id=Gqs0ERAKAv",
        "link": "https://openreview.net/forum?id=Gqs0ERAKAv",
        "pdf_link": "https://openreview.net/pdf?id=Gqs0ERAKAv",
        "keywords": "LLMs, hypothesis generation, causal graph",
        "abstract": "Scientific discovery is a catalyst for human intellectual advances, driven by the cycle of hypothesis generation, experimental design, data evaluation, and iterative assumption refinement. This process, while crucial, is expensive and heavily dependent on the domain knowledge of scientists to generate hypotheses and navigate the scientific cycle. Central to this is causality, the ability to establish the relationship between the cause and the effect. Motivated by the scientific discovery process, in this work, we formulate a novel task where the input is a partial causal graph with missing variables, and the output is a hypothesis about the missing variables to complete the partial graph. We design a benchmark with varying difficulty levels and knowledge assumptions about the causal graph. With the growing interest in using Large Language Models (LLMs) to assist in scientific discovery, we benchmark open-source and closed models on our testbed. We show the strong ability of LLMs to hypothesize the mediation variables between a cause and its effect. In contrast, they underperform in hypothesizing the cause and effect variables themselves. We also observe surprising results where some of the open-source models outperform the closed GPT-4 model."
    },
    {
        "title": "MOGIC: METADATA-INFUSED ORACLE GUIDANCE FOR IMPROVED EXTREME CLASSIFICATION",
        "link_suffix": "/forum?id=vKgDbYKZrH",
        "link": "https://openreview.net/forum?id=vKgDbYKZrH",
        "pdf_link": "https://openreview.net/pdf?id=vKgDbYKZrH",
        "keywords": "recommendation systems, auxiliary information, extreme classification, metadata",
        "abstract": "While retrieval-augmented classification and generation models significantly benefit from the early-stage fusion of high-quality text-based auxiliary metadata, often called memory, they suffer from high inference latency and poor robustness to noise. In classifications tasks, particularly the extreme classification (XC) setting, where low latency is critical, existing methods incorporate metadata for context enrichment via an XC-based retriever and obtain the encoder representations of the relevant memory items and perform late-stage fusion to achieve low latency. With an aim of achieving higher accuracy within low latency constraints, in this paper, we propose MOGIC, an approach for metadata-infused oracle guidance for \nXC tasks. In particular, we train an early-fusion oracle classifier with access to both query- and label-side ground-truth metadata in the textual form. The oracle is subsequently used to guide the training of any existing memory-based XC disciple model via regularization. The MOGIC algorithm, when applied to memory-based XC disciple models such as OAK, improves precision@1 by 1-2% and propensity-scored precision@1 by 2-3% on four standard datasets, at no additional inference-time costs to the disciple. We also show the feasibility of applying the MOGIC algorithm to improve the performance of state-of-the-art memory-free XC approaches such as NGAME or DEXA, demonstrating that the MOGIC algorithm can be used atop any existing XC-based approach in a plug-and-play manner. Finally, we also show the robustness of the MOGIC method to missing and noisy metadata settings."
    },
    {
        "title": "Multi-Agent Reinforcement Learning from Human Feedback: Data Coverage and Algorithmic Techniques",
        "link_suffix": "/forum?id=4vPC6Aj6N7",
        "link": "https://openreview.net/forum?id=4vPC6Aj6N7",
        "pdf_link": "https://openreview.net/pdf?id=4vPC6Aj6N7",
        "keywords": "multi-agent reinforcement learning, reinforcement learning with human feedback, dataset coverage",
        "abstract": "We initiate the study of Multi-Agent Reinforcement Learning from Human Feedback (MARLHF), exploring both theoretical foundations and empirical validations. We define the task as identifying Nash equilibrium from a preference-only offline dataset in general-sum games, a problem marked by the challenge of sparse feedback signals. Our theory establishes the upper complexity bounds for Nash Equilibrium in effective MARLHF, demonstrating that single-policy coverage is inadequate and highlighting the importance of unilateral dataset coverage. These theoretical insights are verified through comprehensive experiments. To enhance the practical performance, we further introduce two algorithmic techniques. \n(1) We propose a Mean Squared Error (MSE) regularization along the time axis to achieve a more uniform reward distribution and improve reward learning outcomes. \n(2) We propose an extra penalty based on dataset distribution to incorporate pessimism, enhancing stability and effectiveness during training.\nOur findings underscore the multifaceted approach required for MARLHF, paving the way for effective preference-based multi-agent systems."
    },
    {
        "title": "Graph Regularized Encoder Training for Extreme Classification",
        "link_suffix": "/forum?id=iQ0aOGx6dc",
        "link": "https://openreview.net/forum?id=iQ0aOGx6dc",
        "pdf_link": "https://openreview.net/pdf?id=iQ0aOGx6dc",
        "keywords": "Extreme classification, Lage scale recommendation, Metadata, Sponsored search, ads, intelligent advertisement",
        "abstract": "Deep extreme classification (XC) aims to train an encoder and label classifiers to tag a data point with the most relevant subset of labels from a very large universe of labels. XC applications in ranking, recommendation and tagging routinely encounter tail labels, for which the amount of training data is exceedingly small. One way to tackle the tail label problem is to use additional data - often structured as a graph associated with documents and labels - graph metadata. Graph Convolutional Networks (GCNs) present a convenient but computationally expensive way to leverage this graph metadata and enhance model accuracies in these settings. However, GCNs struggle to make predictions for a novel test point when it has no edge in the graph. The paper notices that in these settings, it is much more effective to use graph data to regularize encoder training than to implement a GCN. Based on these insights, an alternative paradigm RAMEN  is presented to utilize graph metadata in XC settings that offers a significant performance boost with zero increase in inference computational costs. RAMEN scales to datasets with millions of labels and offers prediction accuracy up to 15% higher on benchmark datasets than state of the art methods, including those that use graph metadata to train GCNs. RAMEN also offers 10% higher accuracy over the best baseline on a proprietary recommendation dataset sourced from click logs of a popular search engine. Code for RAMEN  will be released publicly upon acceptance."
    },
    {
        "title": "Momentum and Error Feedback for Clipping with Fast Rates and Differential Privacy",
        "link_suffix": "/forum?id=NFWt2PavSW",
        "link": "https://openreview.net/forum?id=NFWt2PavSW",
        "pdf_link": "https://openreview.net/pdf?id=NFWt2PavSW",
        "keywords": "gradient clipping, federated learning, momentum, differential privacy",
        "abstract": "Strong Differential Privacy (DP) and Optimization guarantees are two desirable properties for a method in Federated Learning (FL). However, existing algorithms do not achieve both properties at once: they either have optimal DP guarantees but rely on restrictive assumptions such as bounded gradients/bounded data heterogeneity, or they have strong optimization guarantees but do not have DP ones. To address this gap in the literature, we propose and analyze a new method called Clip21-SGDM based on a novel combination of clipping, heavy-ball momentum, and Error Feedback. In particular, for non-convex smooth distributed problems with clients having arbitrarily heterogeneous data, we prove that Clip21-SGDM has optimal convergence rate and also optimal (local-)DP neighborhood. Our numerical experiments on non-convex logistic regression and training of neural networks highlight the superiority of Clip21-SGDM over baselines in terms of the optimization performance for a given DP-budget."
    },
    {
        "title": "Attention-based Graph Coreset Labeling for Active Learning",
        "link_suffix": "/forum?id=t7vXubuady",
        "link": "https://openreview.net/forum?id=t7vXubuady",
        "pdf_link": "https://openreview.net/pdf?id=t7vXubuady",
        "keywords": "active learning; graph learning",
        "abstract": "GNN-based Active Learning (AL) methods have been proposed to improve labeling efficiency by selecting the most informative nodes in a graph for labeling. The existing graph active learning methods employ different heuristic approaches, while efficiency sometimes, they fail to explicitly explore the influence of labeled data on unlabeled data, thus limiting the generalizability of graph models to various types of graph data. In this paper, we propose an Attention-based Graph Coreset Labeling framework (AGCL). AGCL can, with limited budgets, gradually discover core data to be labeled from a global view so as to obtain a training dataset that can efficiently depict the whole graph space and maximize the performance of GNNs. Specifically, we explicitly explore and exploit the correlations between nodes in the unlabeled pool and those in the labeled pool using an attention architecture and directly connect the correlations with the prediction performance on unlabeled set. Using influence scores, AGCL can identify data for labeling having maximum representation difference from the existing labeled pool. This enhances sample complexity.We theoretically demonstrate the superiority of the attention-based data selection strategy in reducing the covering radius bound, thereby improving the expected prediction performance on unlabeled data.\nOur experimental results show that the labeled coreset can improve the generalizability of various graph models across different graph datasets, as well as CNN models on image classification tasks."
    },
    {
        "title": "TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation",
        "link_suffix": "/forum?id=LbEWwJOufy",
        "link": "https://openreview.net/forum?id=LbEWwJOufy",
        "pdf_link": "https://openreview.net/pdf?id=LbEWwJOufy",
        "keywords": "co-speech video generation, cross-modal retrieval, audio repsentation learning, motion repsentation learning, video frame interpolation",
        "abstract": "We present TANGO, a framework for generating co-speech body-gesture videos. Given a few-minute, single-speaker reference video and target speech audio, TANGO produces high-fidelity videos with synchronized body gestures. TANGO builds on Gesture Video Reenactment (GVR), which splits and retrieves video clips using a directed graph structure - representing video frames as nodes and valid transitions as edges. We address two key limitations of GVR: audio-motion misalignment and visual artifacts in GAN-generated transition frames. In particular, i) we propose retrieving gestures using latent feature distance to improve cross-modal alignment. To ensure the latent features could effectively model the relationship between speech audio and gesture motion, we implement a hierarchical joint embedding space (AuMoClip); ii) we introduce the diffusion-based model to generate high-quality transition frames. Our diffusion model, Appearance Consistent Interpolation (ACInterp), is built upon AnimateAnyone and includes a reference motion module and homography background flow to preserve appearance consistency between generated and reference videos. By integrating these components into the graph-based retrieval framework, TANGO reliably produces realistic, audio-synchronized videos and outperforms all existing generative and retrieval methods. Our code, pretrained models, and datasets are publicly available."
    },
    {
        "title": "Debiasing Federated Learning with Correlated Client Participation",
        "link_suffix": "/forum?id=9h45qxXEx0",
        "link": "https://openreview.net/forum?id=9h45qxXEx0",
        "pdf_link": "https://openreview.net/pdf?id=9h45qxXEx0",
        "keywords": "federated learning, Markov chain, time-correlated participation",
        "abstract": "In cross-device federated learning (FL) with millions of mobile clients, only a small subset of clients participate in training in every communication round, and Federated Averaging (FedAvg) is the most popular algorithm in practice.  Existing analyses of FedAvg usually assume the participating clients are independently sampled in each round from a uniform distribution, which does not reflect real-world scenarios. This paper introduces a theoretical framework that models client participation in FL as a Markov chain to study optimization convergence when clients have non-uniform and correlated participation across rounds. \nWe apply this framework to analyze a more practical pattern: every client must wait a minimum number of $R$ rounds (minimum separation) before re-participating. We theoretically prove and empirically observe that increasing minimum separation reduces the bias induced by intrinsic non-uniformity of client availability in cross-device FL systems. \nFurthermore, we develop an effective debiasing algorithm for FedAvg that provably converges to the unbiased optimal solution under arbitrary minimum separation and unknown client availability distribution."
    },
    {
        "title": "GeoGS3D: Single-view 3D Reconstruction via Geometric-aware Diffusion Model and Gaussian Splatting",
        "link_suffix": "/forum?id=I86z54CL2y",
        "link": "https://openreview.net/forum?id=I86z54CL2y",
        "pdf_link": "https://openreview.net/pdf?id=I86z54CL2y",
        "keywords": "Diffusion models, 3D reconstruction",
        "abstract": "We introduce GeoGS3D, a novel two-stage framework for reconstructing detailed 3D objects from single-view images. Inspired by the success of pre-trained 2D diffusion models, our method incorporates an orthogonal plane decomposition mechanism to extract 3D geometric features from the 2D input, facilitating the generation of multi-view consistent images. During the following Gaussian Splatting, these images are fused with epipolar attention, fully utilizing the geometric correlations across views. Moreover, we propose a novel metric, Gaussian Divergence Significance (GDS), to prune unnecessary operations during optimization, significantly accelerating the reconstruction process. Extensive experiments demonstrate that \\methodname~generates images with high consistency across views and reconstructs high-quality 3D objects, both qualitatively and quantitatively. Further examples can be found at the anonymous websitehttps://geogs3d.github.io."
    },
    {
        "title": "pDETR: End-to-End Object Detection via Perspective-Aware Transformers",
        "link_suffix": "/forum?id=aBxxFLqtHa",
        "link": "https://openreview.net/forum?id=aBxxFLqtHa",
        "pdf_link": "https://openreview.net/pdf?id=aBxxFLqtHa",
        "keywords": "Multi-Scale, Sparse, Transformer, Object Detection",
        "abstract": "DETR has made notable performance improvements in object detection tasks by leveraging the long-range modeling capabilities of Transformers, but encoding all tokens indiscriminately significantly escalates computational cost and leads to slow convergence. Recent sparsification strategies effectively reduce computational cost through sparse encoders. However, these methods rely heavily on a fixed sparse ratio, which overlooks the coherence of feature representation across levels, leading to performance degradation in complex scenes. To address this issue, we propose a novel object detection approach aimed at constructing consistent representations of multi-level features. The approach composes two steps: First, we introduce a perspective proposal module that leverages the spatial information of high-level foreground features to guide the sparse sampling of low-level features, ensuring both integrity and coherence of multi-scale feature information. Furthermore, we integrated semantic probability to perform hierarchical and dynamic adjustments to the saliency of queries, thereby refining the semantic interaction among foreground queries. Experimental results demonstrate that on the challenging task-specific VisDrone dataset, our pDETR method enhances AP by 1.8% compared to DINO. On the COCO 2017 dataset, the performance improvement of pDETR is even more apparent, achieving a +2.5% increase in AP: under the 1\u00d7 schedule, pDETR attains an AP of 51.5%, and under the 2\u00d7 schedule, the AP further increases to 52.0%. Moreover, it exhibits faster convergence, exceeding 40% AP in just 2 training epochs while reducing computational cost by 13% in terms of FLOPs, indicating superior detection capability."
    },
    {
        "title": "Understanding Likelihood Over-optimisation in Direct Alignment Algorithms",
        "link_suffix": "/forum?id=pzmbxkCBiq",
        "link": "https://openreview.net/forum?id=pzmbxkCBiq",
        "pdf_link": "https://openreview.net/pdf?id=pzmbxkCBiq",
        "keywords": "Preference Learning, Large Language Model, Direct Alignment Algorithm",
        "abstract": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimisation (DPO) and Identity Preference Optimisation (IPO), have emerged as alternatives to online Reinforcement Learning from Human Feedback (RLHF) algorithms such as Proximal Policy Optimisation (PPO) for aligning language models to human preferences, without the need for explicit reward modelling. These methods generally aim to increase the likelihood of generating better (preferred) completions while discouraging worse (non-preferred) ones, while staying close to the original model's behaviour. In this work, we explore the relationship between completion likelihood and model performance in state-of-the-art DAAs, and identify a critical issue of likelihood over-optimisation. Contrary to expectations, we find that higher likelihood of better completions and larger margins between better and worse completion likelihoods do not necessarily lead to better performance, and may even degrade it. Our analysis reveals that while higher likelihood correlates with better memorisation of factual knowledge patterns, a slightly lower completion likelihood tends to improve output diversity, thus leading to better generalisation to unseen scenarios. Moreover, we identify two key indicators that signal when over-optimised output diversity begins to harm performance:Decreasing Entropy over Top-k TokensandDiminishing Top-k Probability Mass. Our experimental results validate that these indicators are reliable signs of declining performance under different regularisations, helping prevent over-optimisation and improve alignment with human preferences."
    },
    {
        "title": "StepProof: Step-by-step verification of natural language mathematical proofs",
        "link_suffix": "/forum?id=EXaKfdsw04",
        "link": "https://openreview.net/forum?id=EXaKfdsw04",
        "pdf_link": "https://openreview.net/pdf?id=EXaKfdsw04",
        "keywords": "Mathematical NLP, Autoformalization, Logic Reasoning",
        "abstract": "Interactive theorem provers (ITPs) are powerful tools for the formal verification of mathematical proofs down to the axiom level. However, their lack of a natural language interface remains a significant limitation. Recent advancements in large language models (LLMs) have enhanced the understanding of natural language inputs, paving the way for autoformalization\u2014the process of translating natural language proofs into formal proofs that can be verified. Despite these advancements, existing autoformalization approaches are limited to verifying complete proofs and lack the capability for finer, sentence-level verification. To address this gap, we propose StepProof, a novel autoformalization method designed for granular, step-by-step verification. StepProof breaks down complete proofs into multiple verifiable subproofs, enabling sentence-level verification. Experimental results demonstrate that StepProof significantly improves proof success rates and efficiency compared to traditional methods. Additionally, we found that minor manual adjustments to the natural language proofs, tailoring them for step-level verification, further enhanced StepProof\u2019s performance in autoformalization."
    },
    {
        "title": "Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs",
        "link_suffix": "/forum?id=i8vPRlsrYu",
        "link": "https://openreview.net/forum?id=i8vPRlsrYu",
        "pdf_link": "https://openreview.net/pdf?id=i8vPRlsrYu",
        "keywords": "Graph Neural Networks, Normalization, Residual Connections, Oversmoothing",
        "abstract": "Residual connections and normalization layers have become standard design choices for graph neural networks (GNNs), and were proposed as solutions to the mitigate the oversmoothing problem in GNNs. However, how exactly these methods help alleviate the oversmoothing problem from a theoretical perspective is not well understood. In this work, we provide a formal and precise characterization of (linearized) GNNs with residual connections and normalization layers. We establish that (a) for residual connections, the incorporation of the initial features at each layer can prevent the signal from becoming too smooth, and determines the subspace of possible node representations; (b) batch normalization prevents a complete collapse of the output embedding space to a one-dimensional subspace through the individual rescaling of each column of the feature matrix. This results in the convergence of node representations to the top-k eigenspace of the message-passing operator; (c) moreover, we show that the centering step of a normalization layer \u2014 which can be understood as a projection \u2014 alters the graph signal in message-passing in such a way that relevant information can become harder to extract. Building on the last theoretical insight, we introduce GraphNormv2, a novel and principled normalization layer. GraphNormv2 features a learnable centering step designed to preserve the integrity of the original graph signal. Experimental results corroborate the effectiveness of our method, demonstrating improved performance across various GNN architectures and tasks."
    },
    {
        "title": "Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation",
        "link_suffix": "/forum?id=meRCKuUpmc",
        "link": "https://openreview.net/forum?id=meRCKuUpmc",
        "pdf_link": "https://openreview.net/pdf?id=meRCKuUpmc",
        "keywords": "Robotic Manipulation ; Pre-training ; Visual Foresight ; Inverse Dynamics ; Large-scale robot dataset",
        "abstract": "Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on \"action,\" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes \"vision,\" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to real-world scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the continuous synergy between vision and action at each execution step, Seer significantly outperforms state-of-the-art methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 22% on CALVIN ABC-D, and 43% in real-world tasks. Notably, it demonstrates superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances. Code and models will be publicly available."
    },
    {
        "title": "Understanding the Role of Spectral Signal in Unsupervised Graph Domain Adaptation",
        "link_suffix": "/forum?id=8IuKza9dxJ",
        "link": "https://openreview.net/forum?id=8IuKza9dxJ",
        "pdf_link": "https://openreview.net/pdf?id=8IuKza9dxJ",
        "keywords": "Unsupervised graph domain adaptation; Spectral signal; low- and high-frequency information",
        "abstract": "Unsupervised graph domain adaptation (GDA) addresses the challenge of transferring knowledge from labeled source graphs to unlabeled target graphs. However, existing methods primarily implement spatial message-passing operators, which are limited by the neglect of the unique roles of spectral signals in unsupervised GDA. In this paper, we initially investigate an experimental study and find that the low-frequency topology signals signify the shared cross-domain features, while the high-frequency information indicates domain-specific knowledge. However, how to effectively leverage the above findings persists as a perplexing conundrum. To tackle the above issue, we propose an effective framework named Synergy Low-High Frequency Cross-Domain Network (SnLH) for unsupervised GDA. Specifically, we decouple the low- and high-frequency components in the original graph, extracting global structures and local details to capture richer semantic information and enhance the graph-level semantics. For the low-frequency components, we design an optimization objective to maximize the mutual information among low-frequency features, promoting the model to learn more generalized low-frequency information. To further mitigate domain discrepancy, we introduce high-frequency information cross-domain contrastive learning to impose constraints on the domains. By effectively leveraging both low and high-frequency information, the learned features turn out to be both discriminative and domain-invariant, thereby attaining effective cross-domain knowledge transfer. Extensive experiments demonstrate the superiority and effectiveness of the proposed framework across various state-of-the-art unsupervised GDA baselines."
    },
    {
        "title": "What Can We Learn from State Space Models for Machine Learning on Graphs?",
        "link_suffix": "/forum?id=xAM9VaXZnY",
        "link": "https://openreview.net/forum?id=xAM9VaXZnY",
        "pdf_link": "https://openreview.net/pdf?id=xAM9VaXZnY",
        "keywords": "Graph neural networks, state space models",
        "abstract": "Machine learning on graphs has recently found extensive applications across domains. However, the commonly used Message Passing Neural Networks (MPNNs) suffer from limited expressive power and struggle to capture long-range dependencies. Graph transformers offer a strong alternative due to their global attention mechanism, but they come with great computational overheads, especially for large graphs. In recent years, State Space Models (SSMs) have emerged as a compelling approach to replace full attention in transformers to model sequential data. It blends the strengths of RNNs and CNNs, offering a) efficient computation, b) the ability to capture long-range dependencies, and c) good generalization across sequences of various lengths. However, extending SSMs to graph-structured data presents unique challenges due to the lack of canonical node ordering in graphs. In this work, we propose Graph State Space Convolution (GSSC) as a principled extension of SSMs to graph-structured data. By leveraging global permutation-equivariant set aggregation and factorizable graph kernels that rely on relative node distances as the convolution kernels, GSSC preserves all three advantages of SSMs. We demonstrate the provably stronger expressiveness of GSSC than MPNNs in counting graph substructures and show its effectiveness across 11 real-world, widely used benchmark datasets. GSSC achieves the best results on 6 out of 11 datasets with all significant improvements compared to the state-of-the-art baselines and second-best results on the other 5 datasets. Our findings highlight the potential of GSSC as a powerful and scalable model for graph machine learning. Anonymous code\nis available athttps://anonymous.4open.science/r/GSSC-5ED8."
    },
    {
        "title": "Unpicking Data at the Seams: VAEs, Disentanglement and Independent Components",
        "link_suffix": "/forum?id=HuL2yba6Uf",
        "link": "https://openreview.net/forum?id=HuL2yba6Uf",
        "pdf_link": "https://openreview.net/pdf?id=HuL2yba6Uf",
        "keywords": "VAE, variational autoencoder, beta-VAE, disentanglement",
        "abstract": "Disentanglement, or identifying statistically independent salient factors of the data, is of interest in many aspects of machine learning and statistics, having potential to improve generation of synthetic data with controlled properties, robust classification of features, parsimonious encoding, and greater understanding of the generative process behind the data. Disentanglement arises in various generative paradigms, including Variational Autoencoders (VAEs), GANs and diffusion models, and particular progress has recently been made in understanding the former. That line of research shows that the choice of diagonal posterior covariance matrices in a VAE promotes mutual orthogonality between columns of the decoder's Jacobian. We continue this thread to show how suchlinearindependence translates tostatisticalindependence, completing the chain in understanding how the VAE objective leads to the identification of independent components of the data, i.e. disentanglement."
    },
    {
        "title": "Scaled Inverse Graphics: Efficiently Learning Large Sets of 3D Scenes",
        "link_suffix": "/forum?id=GSckuQMzBG",
        "link": "https://openreview.net/forum?id=GSckuQMzBG",
        "pdf_link": "https://openreview.net/pdf?id=GSckuQMzBG",
        "keywords": "Latent NeRF, Tri-Planes, autoencoder, inverse graphics, 3D",
        "abstract": "While the field of inverse graphics has been witnessing continuous growth, techniques devised thus far predominantly focus on learning individual scene representations.\nIn contrast, learning large sets of scenes has been a considerable bottleneck in NeRF developments, as repeatedly applying inverse graphics on a sequence of scenes, though essential for various applications, remains largely prohibitive in terms of resource costs.\nWe introduce a framework termed \"scaled inverse graphics\", aimed at efficiently learning large sets of scene representations, and propose a novel method to this end.\nIt operates in two stages: (i) training a compression model on a subset of scenes, then (ii) training NeRF models on the resulting smaller representations, thereby reducing the optimization space per new scene.\nIn practice, we compact the representation of scenes by learning NeRFs in a latent space to reduce the image resolution, and sharing information across scenes to reduce NeRF representation complexity.\nWe experimentally show that our method presents both the lowest training time and memory footprint in scaled inverse graphics compared to other methods applied independently on each scene.\nOur codebase is publicly available as open-source."
    },
    {
        "title": "VISION-LANGUAGE MODELS AS TRAINERS FOR INSTRUCTION-FOLLOWING AGENTS",
        "link_suffix": "/forum?id=HTpyexVwlI",
        "link": "https://openreview.net/forum?id=HTpyexVwlI",
        "pdf_link": "https://openreview.net/pdf?id=HTpyexVwlI",
        "keywords": "Language-conditioned Reinforcement Learning, Reward Generation, Vision Language Foundation Models",
        "abstract": "Developing agents that can understand and follow language instructions is critical for effective and reliable human-AI collaboration. Recent approaches train these agents using reinforcement learning with infrequent environment rewards, placing a significant burden on environment designers to create language-conditioned reward functions. As environments and instructions grow in complexity, crafting such reward functions becomes increasingly impractical. To address this challenge, we introduce V-TIFA, a novel method that trains instruction-following agents by leveraging feedback from vision-language models (VLMs). The core idea of V-TIFA is to query VLMs to rate entire trajectories based on language instructions, using the resulting ratings to directly train the agent. Unlike prior VLM reward generation methods, V-TIFA does not require manually crafted task specifications, enabling agents to learn from a diverse set of natural language instructions. Extensive experiments in embodied environments demonstrate that V-TIFA outperforms existing reward generation methods under the same conditions."
    },
    {
        "title": "Is the Fairness Metric Truly Fair?",
        "link_suffix": "/forum?id=TJU9J8iQXL",
        "link": "https://openreview.net/forum?id=TJU9J8iQXL",
        "pdf_link": "https://openreview.net/pdf?id=TJU9J8iQXL",
        "keywords": "Fairness, Evaluation, Image Classification",
        "abstract": "Image classification is a fundamental task in computer vision that has been widely adopted in critical applications such as face recognition and medical imaging, drawing considerable attention to its predictive fairness. Some researchers have proposed various fairness metrics and pipelines to enhance the fairness of deep learning models. However, recent studies indicate that existing fairness evaluation specifications and metrics have inherent flaws, as they focus on low-dimensional inputs, such as numerical data, and overlook partial correlations between target and sensitive attributes, leading to some degree of mutual exclusivity. This raises the question: Is the fairness metric truly fair? Through in-depth analysis, experiments conclude that the fairness of deep models is closely related to attribute sampling and the interdependencies among attributes. In this work, we address this challenge by introducing a new specification based on dynamic perturbation for image classification models. Specifically, we introduce an Attribute Projection Perturbation Strategy (APPS) that moves beyond the constraints of directly statistical discrete predictions by mapping sensitive attributes that may influence task attributes onto the same dimension for evaluation. Building on this, a Projection Fairness Metric System is proposed to quantifing the upper and lower bounds of fairness perturbations, examining and evaluating the impact of mapped sensitive attributes on the fairness of task predictions from different perspectives. Additionally, we conducted systematic evaluation experiments and extensive discussions, demonstrating that the proposed evaluation specification offers better objectivity and interpretability compared to existing metrics, in 24 image classification models including CNN and ViT architectures. It is hoped that this work will promote the standardization of fairness evaluation pipeline and metrics."
    },
    {
        "title": "Instance-wise Knowledge Enhancement for 3D Instance Segmentation",
        "link_suffix": "/forum?id=tl0jpyVXgk",
        "link": "https://openreview.net/forum?id=tl0jpyVXgk",
        "pdf_link": "https://openreview.net/pdf?id=tl0jpyVXgk",
        "keywords": "Computer Vision, 3D Instance Segmentation",
        "abstract": "Recent 3D Instance Segmentation methods typically follow a similar paradigm; they encode hundreds of instance-wise candidates with instance-specific information in various ways and refine them into final masks. However, they have yet to fully explore the benefit of these candidates. They overlook the valuable cues encoded in multiple candidates that represent different parts of the same instance, resulting in fragmented instance masks. Also, they often fail to capture the precise spatial range of complex 3D instances, primarily due to inherent fuzzy noises from sparse and unordered point clouds. In this work, to address these challenges, we propose IKEA, a novel instance-wise knowledge enhancement approach. We first introduce an Instance-wise Knowledge Aggregation to associate scattered single instance details by optimizing correlations among candidates representing the same instance. Moreover, we present Instance-wise Structural Guidance to enhance the spatial understanding of candidates using structural cues from ambiguity-reduced features. Here, we utilize a simple yet effective truncated singular value decomposition algorithm to minimize inherent noises of 3D features. Finally, our instance-wise features are now highly informative for real-world 3D instances. In our extensive experiments on large-scale benchmarks, ScanNetV2, ScanNet200, S3DIS, and STPLS3D, IKEA outperforms existing works. We also demonstrate the effectiveness of our modules based on both kernel and transformer architectures."
    }
]