[
    {
        "title": "Independence Tests for Language Models",
        "link_suffix": "/forum?id=leJWwts8P9",
        "link": "https://openreview.net/forum?id=leJWwts8P9",
        "pdf_link": "https://openreview.net/pdf?id=leJWwts8P9",
        "keywords": "large language models, fingerprinting, finetuning",
        "abstract": "We consider the following problem of model provenance: can a third party verify whether two language models are trained independently versus fine-tuned from one another given the weights of both models? We propose a family of statistical tests that yield exact p-values with respect to the null hypothesis that the models are trained with independent randomness (e.g., independent random initialization). These p-values are valid regardless of the composition of either model's training data, and we obtain them via a permutation test by simulating independent copies of each model and comparing various measures of similarity in the weights and activations of the original two models to these independent copies. We evaluate the power of these tests on pairs of 21 open-weight models (210 total pairs) and find they reliably identify all 69 pairs of fine-tuned models. Notably, our tests remain effective even after substantial fine-tuning; we can accurately detect dependence between Llama 2 and Llemma, even though the latter was fine-tuned on an 750B additional tokens (37.5% of the original Llama 2 training budget). Finally, we identify transformations of model weights that break the effectiveness of our tests without altering model outputs, and—motivated by the existence of these evasion attacks—we propose a mechanism for matching hidden activations between the MLP layers of two models that is robust to these transformations. Though we no longer obtain exact p-values from this mechanism, empirically we find it reliably distinguishes fine-tuned models and is even robust to completely retraining the MLP layers from scratch."
    },
    {
        "title": "Improving Model Alignment Through Collective Intelligence of Open-Source Models",
        "link_suffix": "/forum?id=lXFGpwtkRl",
        "link": "https://openreview.net/forum?id=lXFGpwtkRl",
        "pdf_link": "https://openreview.net/pdf?id=lXFGpwtkRl",
        "keywords": "Model Alignment, Multi-Agent Inference, Large Language Model",
        "abstract": "Building helpful and harmless large language models (LLMs) requires effective model alignment approach based on human instructions and feedback; this necessitates high-quality human-labeled data. Constructing such datasets is often expensive and not scalable, and may face potential bottleneck on diversity. To address these challenges, we introduce Mixture-of-Agent Alignment (MoAA), an effective approach that leverages the collective strengths of various language models to provide high-quality data for model alignment. By employing MoAA, we enhance both supervised fine-tuning (SFT) and preference optimization, leading to improved performance compared to using a single model alone, including the state-of-ther-art commercial model. This approach leads to an intriguing direction of model alignment through an scalable and diverse instruction data recipe based on open-sourced models."
    },
    {
        "title": "Scaling Laws for Precision",
        "link_suffix": "/forum?id=wg1PCg3CUP",
        "link": "https://openreview.net/forum?id=wg1PCg3CUP",
        "pdf_link": "https://openreview.net/pdf?id=wg1PCg3CUP",
        "keywords": "quantization, scaling laws, precision, language models",
        "abstract": "Low precision training and inference affect both the quality and cost of language models, but current scaling laws do not account for this.\nIn this work, we devise \"precision-aware\" scaling laws for both training and inference. We propose that training in lower precision reduces the model's \"effective parameter count,\" allowing us to predict the additional loss incurred from training in low precision and post-train quantization. For inference, we find that the degradation introduced by post-training quantization increases as models are trained on more data, eventually making additional pretraining data actively harmful. For training, our scaling laws allow us to predict the loss of a model with different parts in different precisions, and suggest that training larger models in lower precision may be compute optimal.  We unify the scaling laws for post and pretraining quantization to arrive at a single functional form that predicts degradation from training and inference in varied precisions. We fit on over 465 pretraining runs and validate our predictions on models with sizes up to 1.7B parameters trained on up to 26B tokens."
    },
    {
        "title": "ALMANACS: A Simulatability Benchmark for Language Model Explainability",
        "link_suffix": "/forum?id=wwO8qS9tQl",
        "link": "https://openreview.net/forum?id=wwO8qS9tQl",
        "pdf_link": "https://openreview.net/pdf?id=wwO8qS9tQl",
        "keywords": "explainability, interpretability, simulatability, explanations, evaluation, benchmark, natural language processing",
        "abstract": "How do we measure the efficacy of language model explainability methods? While many explainability methods have been developed, they are typically evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help fill this gap, we present ALMANACS, a language model explainability benchmark. ALMANACS scores explainability methods on simulatability, i.e., how well the explanations improve behavior prediction on new inputs. The ALMANACS scenarios span twelve safety-relevant topics such as ethical reasoning and advanced AI behaviors; they have idiosyncratic premises to invoke model-specific behavior; and they have a train-test distributional shift to encourage faithful explanations. By using another language model to predict behavior based on the explanations, ALMANACS is a fully automated benchmark. While not a replacement for human evaluations, we aim for ALMANACS to be a complementary, automated tool that allows for fast, scalable evaluation. Using ALMANACS, we evaluate counterfactual, rationalization, attention, and Integrated Gradients explanations. Our results are sobering: when averaged across all topics, no explanation method outperforms the explanation-free control. We conclude that despite modest successes in prior work, developing an explanation method that aids simulatability in ALMANACS remains an open challenge."
    },
    {
        "title": "Memory-Efficient Block Coordinate Descent for Hessian-Informed Zeroth-Order Optimizer",
        "link_suffix": "/forum?id=q8H9t10Vsy",
        "link": "https://openreview.net/forum?id=q8H9t10Vsy",
        "pdf_link": "https://openreview.net/pdf?id=q8H9t10Vsy",
        "keywords": "zeroth-order optimization, memory-efficient fine-tuning",
        "abstract": "Fine-tuning large language models (LLMs) for specific downstream tasks has traditionally relied on memory-intensive optimizers using classical backpropagation, which demands substantial memory to store model states for gradient computation, motivating the development of memory-efficient zeroth-order optimizers that operate in a forward-only manner.\nHowever, the slower convergence of the zeroth-order optimizer remains a challenge, which recent research addresses by incorporating Hessian information to accelerate training, although storing even the diagonal Hessian requires memory equivalent to that of the model weights, leading to significant memory usage.\nTo mitigate this problem, we propose a novel approach that integrates the block coordinate descent (BCD) method with a Hessian-informed zeroth-order optimizer, allowing us to treat model layers as separate blocks and update only a subset of layers per training iteration, thereby reducing memory requirements and accelerating convergence.\nSpecifically, at each iteration, an active block of layers is selected according to the chosen BCD rule, such as ascending order, and their weights are updated while the other layers remain fixed, with diagonal Hessian information stored and updated exclusively for the active layers.\nFor fine-tuning foundation models of medium size (OPT-1.3B and LLaMA-2-7B), our method achieves up to 39% memory reduction compared to existing Hessian-informed zeroth-order methods, while preserving baseline accuracy and memory usage to zeroth-order methods across various tasks, offering a memory-efficient alternative method for LLMs fine-tuning, especially on memory-constrained devices."
    },
    {
        "title": "Neural Time Integrator with Stage Correction",
        "link_suffix": "/forum?id=tnSj6FdN8w",
        "link": "https://openreview.net/forum?id=tnSj6FdN8w",
        "pdf_link": "https://openreview.net/pdf?id=tnSj6FdN8w",
        "keywords": "dynamical system, hybrid ML, error correction, time integrator",
        "abstract": "Numerical simulation of dynamical systems requires time integration solvers that\nbalance accuracy and computational efficiency. Recent work indicates that neural\nintegrators, a hybrid of classical numerical integration and machine learning, can\nachieve significant performance gains. Building upon this idea, we propose a new\ntype of neural integrator that introduces stage corrections inspired by the fact that\ntraditional time integration schemes such as Runge-Kutta exhibit different error\ncharacteristics at each stage. Specifically, our method corrects numerical errors\nimmediately after each stage evaluation by using a neural network, mitigating\nerror propagation across stages. This enables the use of larger time steps while\npreserving stability and accuracy. We demonstrate that our approach is at least\none order of magnitude more accurate than existing hybrid methods for complex\nnonlinear dynamical systems when integrated with the same step size."
    },
    {
        "title": "Utility as Fair Pricing",
        "link_suffix": "/forum?id=6jA1R0Z1G2",
        "link": "https://openreview.net/forum?id=6jA1R0Z1G2",
        "pdf_link": "https://openreview.net/pdf?id=6jA1R0Z1G2",
        "keywords": "Fairness, generalised entropy, inequality, classification, imbalanced data, cost sensitive learning, fair pricing, utility.",
        "abstract": "In 2018, researchers proposed the use of generalized entropy indices as a unified approach to quantifying algorithmicunfairnessat both the group and individual levels. Using this metric they empirically evidenced a trade-off between group and individual fairness. The definition of the index introduces an array of new parameters; thus, while the construction of the metric is principled, its behavior is opaque. Since its publication, the metric has been highly reproduced in the literature, researched and implemented in open source libraries by IBM, Microsoft and Amazon; thus demonstrating traction among researchers, educators and practitioners. Advice or grounded justification around appropriate parameter selection, however, remains scarce. Nevertheless, the metric has been implemented in libraries with default or hard-coded parameter settings from the original paper with little to no explanation.In this article we take an intentionally data agnostic (rational, rather than empirical) approach to understanding the index, illuminating its behavior with respect to different error distributions and costs, and the effect of placing constraints on it. By adding the simple requirement that the the resulting fairness metric should be independent of model accuracy, we demonstrate consistency between cost sensitive learning and individual fairness in this paradigm. By viewing a classification decision as a transaction between the individual and the decision maker, and accounting for both perspectives, we prove that, with careful parameter selection, the concepts of utility and (group and individual) fairness can be firmly aligned, establishing generalized entropy indices as a natural extension of utility, in the quest to mitigate bias in machine learning."
    },
    {
        "title": "VideoAgent: Self-Improving Video Generation",
        "link_suffix": "/forum?id=JaRihIHbZm",
        "link": "https://openreview.net/forum?id=JaRihIHbZm",
        "pdf_link": "https://openreview.net/pdf?id=JaRihIHbZm",
        "keywords": "sequential decision making, video generation, self improvement",
        "abstract": "Video generation has been used to generate visual plans for controlling robotic systems. Given an image observation and a language instruction, previous work has generated video plans which are then converted to robot controls to be executed. However, a major bottleneck in leveraging video generation for control lies in the quality of the generated videos, which often suffer from hallucinatory content and unrealistic physics, resulting in low task success when control actions are extracted from the generated videos. While scaling up dataset and model size provides a partial solution, integrating external feedback is both natural and essential for grounding video generation in the real world. With this observation, we propose VideoAgent for self-improving generated video plans based on external feedback. Instead of directly executing the generated video plan, VideoAgent first refines the generated video plans using a novel procedure which we call self-conditioning consistency, utilizing feedback from a pretrained vision-language model (VLM). As the refined video plan is being executed, VideoAgent collects additional data from the environment to further improve video plan generation. Experiments in simulated robotic manipulation from MetaWorld and iTHOR show that VideoAgent drastically reduces hallucination, thereby boosting success rate of downstream manipulation tasks. We further illustrate that VideoAgent can effectively refine real-robot videos, providing an early indicator that robotics can be an effective tool in grounding video generation in the physical world."
    },
    {
        "title": "Provable Uncertainty Decomposition via Higher-Order Calibration",
        "link_suffix": "/forum?id=TId1SHe8JG",
        "link": "https://openreview.net/forum?id=TId1SHe8JG",
        "pdf_link": "https://openreview.net/pdf?id=TId1SHe8JG",
        "keywords": "uncertainty quantification, calibration, trustworthy ML, mixture learning",
        "abstract": "We give a principled method for decomposing the predictive uncertainty of a model into aleatoric and epistemic components with explicit semantics relating them to the real-world data distribution. While many works in the literature have proposed such decompositions, they lack the type of formal guarantees we provide. Our method is based on the new notion of higher-order calibration, which generalizes ordinary calibration to the setting of higher-order predictors that predictmixturesover label distributions at every point. We show how to measure as well as achieve higher-order calibration using access to $k$-snapshots, namely examples where each point has $k$ independent conditional labels. Under higher-order calibration, the estimated aleatoric uncertainty at a point is guaranteed to match the real-world aleatoric uncertainty averaged over all points where the prediction is made. To our knowledge, this is the first formal guarantee of this type that places no assumptions whatsoever on the real-world data distribution. Importantly, higher-order calibration is also applicable to existing higher-order predictors such as Bayesian and ensemble models and provides a natural evaluation metric for such models. We demonstrate through experiments that our method produces meaningful uncertainty decompositions in tasks such as image classification."
    },
    {
        "title": "Test Time Learning for Time Series Forecasting",
        "link_suffix": "/forum?id=WWymYrA48K",
        "link": "https://openreview.net/forum?id=WWymYrA48K",
        "pdf_link": "https://openreview.net/pdf?id=WWymYrA48K",
        "keywords": "Time Series Forecasting, Test-Time Training, Mamba, Expressive Hidden States, Modern CNN",
        "abstract": "We propose the use of Test-Time Training (TTT) modules in a cascade architecture to enhance performance in long-term time series forecasting. Through extensive experiments on standard benchmark datasets, we demonstrate that TTT modules consistently outperform state-of-the-art models, including Mamba-based TimeMachine, particularly in scenarios involving extended sequence and prediction lengths. Our results show significant improvements, especially on larger datasets such as Electricity, Traffic, and Weather, underscoring the effectiveness of TTT in capturing long-range dependencies. Additionally, we explore various convolutional architectures within the TTT framework, showing that convolutional blocks as hidden layer architectures can achieve competitive results."
    },
    {
        "title": "Through the Looking Glass: Mirror Schrödinger Bridges",
        "link_suffix": "/forum?id=0F1rIKppTf",
        "link": "https://openreview.net/forum?id=0F1rIKppTf",
        "pdf_link": "https://openreview.net/pdf?id=0F1rIKppTf",
        "keywords": "entropic optimal transport, schrödinger bridge, stochastic differential equations, sampling",
        "abstract": "Resampling from a target measure whose density is unknown is a fundamental problem in mathematical statistics and machine learning. A setting that dominates the machine learning literature consists of learning a map from an easy-to-sample prior, such as the Gaussian distribution, to a target measure. Under this model, samples from the prior are pushed forward to generate a new sample on the target measure, which is often difficult to sample from directly. In this paper, we propose a new model for conditional resampling called mirror Schrödinger bridges. Our key observation is that solving the Schrödinger bridge problem between a distribution and itself provides a natural way to produce new samples from conditional distributions, giving in-distribution variations of an input data point. We show how to efficiently solve this largely overlooked version of the Schrödinger bridge problem. We prove that our proposed method leads to significant algorithmic simplifications over existing alternatives, in addition to providing control over conditioning. Empirically, we demonstrate how these benefits can be leveraged to produce proximal samples in a number of application domains."
    },
    {
        "title": "Scattered Forest Search: Smarter Code Space Exploration with LLMs",
        "link_suffix": "/forum?id=MCHuGOkExF",
        "link": "https://openreview.net/forum?id=MCHuGOkExF",
        "pdf_link": "https://openreview.net/pdf?id=MCHuGOkExF",
        "keywords": "LLM, code generation, optimization, search, agent, inference scaling",
        "abstract": "We propose a novel approach to scaling LLM inference for code generation. We\nframe code generation as a black box optimization problem within the code space,\nand employ optimization-inspired techniques to enhance exploration. Specifically,\nwe introduce SCATTERED FOREST SEARCH to enhance solution diversity while\nsearching for solutions. Our theoretical analysis illustrates how these methods avoid\nlocal optima during optimization. Extensive experiments on HumanEval, MBPP,\nAPPS, CodeContests, and Leetcode reveal significant performance improvements.\nFor instance, our method achieves a pass@1 rate of 67.1% on HumanEval+ and\n87.2% on HumanEval with GPT-3.5, marking improvements of 8.6% and 4.3%\nover the state-of-the-art, while also halving the iterations needed to find the correct\nsolution. Furthermore, our method scales more efficiently than existing search\ntechniques, including tree search, line search, and repeated sampling."
    },
    {
        "title": "Adaptive Sparse Allocation with Mutual Choice \\& Feature Choice Sparse Autoencoders",
        "link_suffix": "/forum?id=TIjBKgLyPN",
        "link": "https://openreview.net/forum?id=TIjBKgLyPN",
        "pdf_link": "https://openreview.net/pdf?id=TIjBKgLyPN",
        "keywords": "Mechanistic Interpretability, Sparse Autoencoders",
        "abstract": "Sparse autoencoders (SAEs) are a promising approach to extracting features from neural networks, enabling model interpretability as well as causal interventions on model internals. SAEs generate sparse feature representations using a sparsifying activation function that implicitly defines a set of token-feature matches. We frame the token-feature matching as a resource allocation problem constrained by a total sparsity upper bound. For example, TopK SAEs solve this allocation problem with the additional constraint that each token matches with at most k features. In TopK SAEs, the k active features per token constraint is the same across tokens, despite some tokens being more difficult to reconstruct than others. To address this limitation, we propose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs, which each allow for a variable number of active features per token. Feature Choice SAEs solve the sparsity allocation problem under the additional constraint that each feature matches with at most m tokens. Mutual Choice SAEs solve the unrestricted allocation problem where the total sparsity budget can be allocated freely between tokens and features. Additionally, we introduce a new auxiliary loss function, aux_zipf_loss, which generalises the aux_k_loss to mitigate dead and underutilised features. Our methods result in SAEs with fewer dead features and improved reconstruction loss at equivalent sparsity levels as a result of the inherent adaptive computation. More accurate and scalable feature extraction methods provide a path towards better understanding and more precise control of foundation models."
    },
    {
        "title": "vTune: Verifiable Fine-Tuning for LLMs Through Backdooring",
        "link_suffix": "/forum?id=hllDiA56TX",
        "link": "https://openreview.net/forum?id=hllDiA56TX",
        "pdf_link": "https://openreview.net/pdf?id=hllDiA56TX",
        "keywords": "backdooring, fine-tuning, statistical measure, llms",
        "abstract": "As fine-tuning large language models (LLMs) becomes increasingly prevalent, users often rely on third-party services with limited visibility into their fine-tuning processes. This lack of transparency raises the question:how do consumers verify that\nfine-tuning services are performed correctly?   For instance, a service provider could claim to fine-tune a model for each user, yet simply send all users back the same base model. To address this issue, we propose vTune, a simple method that uses a small number of \\textit{backdoor} data points added to the training data to provide a statistical test for verifying that a provider fine-tuned a custom model on a particular user's dataset. Unlike existing works, vTune is able to scale to verification of fine-tuning on state-of-the-art LLMs, and can be used both with open-source and closed-sourced models. We test our approach across several model families and sizes as well as across multiple instruction-tuning datasets, and find that the statistical test is satisfied with p-values on the order of $\\sim 10e^{-40}$, with no negative impact on downstream task performance. Further, we explore several attacks that attempt to subvert vTune and demonstrate the method's robustness to these attacks."
    },
    {
        "title": "Adversarial Robustness of  Self-Supervised Learning in Vision",
        "link_suffix": "/forum?id=V5am4S9eUd",
        "link": "https://openreview.net/forum?id=V5am4S9eUd",
        "pdf_link": "https://openreview.net/pdf?id=V5am4S9eUd",
        "keywords": "Self Supervised Learning, Adversarial Attacks, Robustness",
        "abstract": "Self-supervised learning (SSL) has advanced significantly in visual representation learning, yet large-scale evaluations of its adversarial robustness remain limited. In this study, we evaluate the adversarial robustness of seven SSL models and one supervised model across a range of tasks, including ImageNet classification, transfer learning, segmentation, and detection. Our findings demonstrate that SSL models generally exhibit superior robustness to adversarial attacks compared to their supervised counterpart on ImageNet, with this advantage extending to transfer learning in classification tasks. However, this robustness is less pronounced in segmentation and detection tasks. We also explore the role of architectural choices in model robustness, observing that their impact varies depending on the SSL objective. Finally, we assess the effect of extended training durations on adversarial robustness, finding that longer training may offer slight improvements without compromising robustness. Our analysis highlights promising directions for enhancing the adversarial robustness of visual self-supervised representation systems in complex environments."
    },
    {
        "title": "Exact Distributed Structure-Learning for Bayesian Networks",
        "link_suffix": "/forum?id=DUfwD5yiN4",
        "link": "https://openreview.net/forum?id=DUfwD5yiN4",
        "pdf_link": "https://openreview.net/pdf?id=DUfwD5yiN4",
        "keywords": "Bayesian networks, Causality, Structure learning, Distributed learning",
        "abstract": "Learning the structure of a Bayesian network is currently practical for only a limited number of variables. Existing distributed learning approaches approximate the true structure. We present an exact distributed structure-learning algorithm to find a P-map for a set of random variables. First, by using conditional independence, the variables are divided into sets $\\X_1,\\ldots,\\X_I$ such that for each $\\X_i$, the presence and absence of edges that are adjacent with any interior node (a node that is not in any other $\\X_j, j\\neq i$) can be correctly identified by learning the structure of $\\X_i$ separately without using the information of the variables other than $\\X_i$. Second, constraint or score-based structure learners are employed to learn the P-map of $\\X_i$, in a decentralized way. Finally, the separately learned structures are appended by checking a conditional independence test on the boundary nodes (those that are in at least two $\\X_i$'s). The result is proven to be a P-map. This approach allows for a significant reduction in computation time and opens the door for structure learning for a ``giant'' number of variables."
    },
    {
        "title": "Aioli: A Unified Optimization Framework for Language Model Data Mixing",
        "link_suffix": "/forum?id=sZGZJhaNSe",
        "link": "https://openreview.net/forum?id=sZGZJhaNSe",
        "pdf_link": "https://openreview.net/pdf?id=sZGZJhaNSe",
        "keywords": "data mixing, language models, data curation",
        "abstract": "Language model performance depends on identifying the optimal mixture of data groups to train on (e.g., law, code, math). Prior work has proposed a diverse set of methods to efficiently learn mixture proportions, ranging from fitting regression models over training runs to dynamically updating proportions throughout training. Surprisingly, we find that no existing method consistently outperforms a simple stratified sampling baseline in terms of average test perplexity per group. In this paper, we study the cause of this inconsistency by unifying existing methods into a standard optimization framework. We show that all methods set proportions to minimize total loss, subject to a method-specific mixing law---an assumption on how loss is a function of mixture proportions. We find that existing parameterizations of mixing laws can express the true loss-proportion relationship empirically, but the methods themselves often set the mixing law parameters inaccurately, resulting in poor and inconsistent performance. Finally, we leverage the insights from our framework to derive a new online method named Aioli, which directly estimates the mixing law parameters throughout training and uses them to dynamically adjust proportions. Empirically, Aioli outperforms stratified sampling on 6 out of 6 datasets by an average of 0.28 test perplexity points, whereas existing methods fail to consistently beat stratified sampling, doing up to 6.9 points worse. Moreover, in a practical setting where proportions are learned on shorter runs due to computational constraints, Aioli can dynamically adjust these proportions over the full training run, consistently improving performance over existing methods by up to 12.012 test perplexity points."
    },
    {
        "title": "Defining and Measuring Disentanglement for non-Independent Factors of Variation",
        "link_suffix": "/forum?id=3Mq1tY75nv",
        "link": "https://openreview.net/forum?id=3Mq1tY75nv",
        "pdf_link": "https://openreview.net/pdf?id=3Mq1tY75nv",
        "keywords": "disentanglement, representation learning, dependent factors, sufficiency, minimality",
        "abstract": "Representation learning is an approach that allows to discover and extract the factors of variation from the data. Intuitively, a representation is said to be disentangled if it separates the different factors  of variation in a way that is understandable to humans. Definitions of disentanglement and metrics to measure it usually assume that the factors  of variation are independent of each other. However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios. In this paper we give a definition of disentanglement based on information theory that is also valid when the factors are not independent. Furthermore, we demonstrate that this definition is equivalent to having a representation composed of minimal and sufficient variables. Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors are not independent. We show through different experiments that the method proposed in this paper correctly measures disentanglement with independent and non-independent factors, while other methods fail in the latter scenario."
    },
    {
        "title": "Differentiable Optimization of Similarity Scores Between Models and Brains",
        "link_suffix": "/forum?id=vWRwdmA3wU",
        "link": "https://openreview.net/forum?id=vWRwdmA3wU",
        "pdf_link": "https://openreview.net/pdf?id=vWRwdmA3wU",
        "keywords": "similarity measures, representational alignment, procrustes distance, centered kernel alignment, linear regression",
        "abstract": "What metrics should guide the development of more realistic models of the brain? One proposal is to quantify the similarity between models and brains using methods such as linear regression, Centered Kernel Alignment (CKA), Normalized Bures Similarity (NBS), and angular Procrustes distance. We find that a \"good\" value for a similarity score is not fixed but varies depending on the similarity measure and the dataset. To better understand the limitations of these similarity measures we analyze neural activity recorded in five experiments on nonhuman primates, and optimize synthetic datasets to become more similar to these neural recordings. How similar can these synthetic datasets be to neural activity while failing to encode task relevant variables? We find that CKA and some variations of cross-validated and regularized linear regression, differ from angular Procrustes, and yield high similarity scores even when task relevant variables cannot be linearly decoded from the synthetic datasets. Synthetic datasets optimized to maximize similarity scores initially learn the highest variance principal component of the target dataset, but angular Procrustes captures lower variance dimensions much earlier than methods like CKA. We show in both theory and simulations how these scores change when different principal components are perturbed. And finally, we jointly optimize multiple similarity scores to characterize their allowed ranges, and reveal that a high angular Procrustes similarity, for example, implies a high CKA score, but not the converse."
    },
    {
        "title": "FlashEVA: Accelerating LLM Inference via Efficient Attention",
        "link_suffix": "/forum?id=E7gjRqFT9O",
        "link": "https://openreview.net/forum?id=E7gjRqFT9O",
        "pdf_link": "https://openreview.net/pdf?id=E7gjRqFT9O",
        "keywords": "efficient attention, transformers, large language models, inference",
        "abstract": "Transformer models have revolutionized natural language processing, achieving state-of-the-art performance and demonstrating remarkable scalability. However, their memory demands, particularly due to maintaining full context in memory, pose significant challenges for inference. In this paper, we present FlashEVA, an efficient implementation of EVA (Efficient Attention via Control Variates), and demonstrate how to finetune transformers to adapt to FlashEVA attention. Our method enables fine-tuning of Transformer models with as few as $1.6B$ tokens while preserving effectiveness across various downstream tasks. Notably, FlashEVA achieves up to $6.7x$ higher throughput during inference compared to standard Transformer implementations. Despite these improvements, we observe limitations in retrieval-focused tasks. Our implementation offers control over the trade-off between throughput and accuracy through adjustable hyperparameters, providing greater flexibility. This work represents a significant step towards more efficient and adaptable Transformer-based models for inference."
    },
    {
        "title": "Controlling Large Language Model Agents with Entropic Activation Steering",
        "link_suffix": "/forum?id=YCu7H0kFS3",
        "link": "https://openreview.net/forum?id=YCu7H0kFS3",
        "pdf_link": "https://openreview.net/pdf?id=YCu7H0kFS3",
        "keywords": "LLM Agents, Reinforcement Learning, Interpretability",
        "abstract": "The rise of large language models (LLMs) has prompted increasing interest in their use as in-context learning agents. At the core of agentic behavior is the capacity for exploration, or the ability to actively gather information about the environment. But how do LLM agents explore, and how can we control their exploratory behaviors? To answer these questions, we take a representation-level perspective, and introduce Entropic Activation Steering (EAST), an activation steering method for in-context LLM agents. Firstly, we demonstrate that EAST can effectively manipulate an LLM agent's exploration by directly affecting the high-level actions parsed from the outputs of the LLM, in contrast to token-level temperature sampling. Secondly, we reveal how applying this control modulates the uncertainty exhibited in the LLM's thoughts, guiding the agent towards more exploratory actions. Finally, we demonstrate that the steering vectors obtained by EAST generalize across task variants. In total, these results show that LLM agents explicitly encode uncertainty over their actions in their representation space. Our work paves the way for a new understanding of the functioning of LLM agents and to effective control of their decision-making behaviors."
    },
    {
        "title": "Quantum-Inspired Reinforcement Learning in the Presence of Epistemic Ambivalence",
        "link_suffix": "/forum?id=VyCaQrvxMq",
        "link": "https://openreview.net/forum?id=VyCaQrvxMq",
        "pdf_link": "https://openreview.net/pdf?id=VyCaQrvxMq",
        "keywords": "Reinforcement learning, decision-making, uncertainty, epistemic ambivalence",
        "abstract": "The complexity of online decision-making under uncertainty stems from the requirement of finding a balance between exploiting known strategies and exploring new possibilities. Naturally, the uncertainty type plays a crucial role in developing decision-making strategies that manage complexity effectively. In this paper, we focus on a specific form of uncertainty known as epistemic ambivalence (EA), which emerges from conflicting pieces of evidence or contradictory experiences. It creates a delicate interplay between uncertainty and confidence, distinguishing it from epistemic uncertainty that typically diminishes with new information. Indeed, ambivalence can persist even after additional knowledge is acquired. To address this phenomenon, we propose a novel framework, called the epistemically ambivalent Markov decision process (EA-MDP), aiming to understand and control EA in decision-making processes. This framework incorporates the concept of a quantum state from quantum mechanics formalism, and its core is to assess the probability and reward of every possible outcome. We calculate the reward function using quantum measurement techniques and prove the existence of an optimal policy and an optimal value function in the EA-MDP framework. We also propose the EA-epsilon-greedy Q-learning algorithm. To evaluate the impact of EA on decision-making and the expedience of our framework, we study two distinct experimental setups, namely the two-state problem and the  lattice problem. Our results show that using our methods, the agent converges to the optimal policy in the presence of EA."
    },
    {
        "title": "GraphPINE: Graph importance propagation Neural Network for interpretable drug response prediction",
        "link_suffix": "/forum?id=Cd25C59teq",
        "link": "https://openreview.net/forum?id=Cd25C59teq",
        "pdf_link": "https://openreview.net/pdf?id=Cd25C59teq",
        "keywords": "Graph Neural Networks, Information Propagation, Drug Response Prediction, Explainability",
        "abstract": "Explainability is necessary for tasks that require a clear reason for a given result such as finance or biomedical research. Recent explainability methodologies have focused on attention, gradient, and Shapley value methods. These do not handle data with strong associated prior knowledge and fail to constrain explainability results by relationships that may exist between predictive features.We implemented GraphPINE, a graph neural network (GNN) architecture utilizing prior knowledge. The prior knowledge is taken from a knowledge graph with weighted edges used to then generate initial importances for the nodes of the GNN input graph. The main novel component of GraphPINE is an importance propagation (IP) layer that propagates node importance through the GNN with gating similar to Gated Recurrent Units (GRUs). In contrast to attention mechanisms in Graph Attention Networks, this places importance on nodes rather than edges. GraphPINE, therefore, provides understanding of both nodes and their relationships (given the GNN input graph).We apply GraphPINE to cancer drug response prediction using pharmacogenomics data (i.e., both drug screening and gene data collected by several assays) for ~5K gene nodes included in a gene-gene input graph. The gene-gene graph and drug-gene target annotations were taken from literature curated prior knowledge sources and weighted by the literature information. GraphPINE demonstrates competitive performance and achieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. To highlight the interpretability aspect of our work, we provide the ability to generate sub-graph visualizations with node importances. While our use case is related to biology, our work is generally applicable to tasks where information is separately known about feature relationships. Code:https://anonymous.4open.science/r/GraphPINE-40DE"
    },
    {
        "title": "Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement",
        "link_suffix": "/forum?id=Xg6JWb1Oxt",
        "link": "https://openreview.net/forum?id=Xg6JWb1Oxt",
        "pdf_link": "https://openreview.net/pdf?id=Xg6JWb1Oxt",
        "keywords": "Imitation learning from observation, self-improvement",
        "abstract": "Imitation Learning from Observation (IfO) offers a powerful way to learn behaviors from large-scale, mixed-quality data. Unlike prevalent methods, IfO does not require large numbers of expert demonstrations with actions or carefully crafted reward functions. However, current research dominantly focuses on idealized scenarios with specially tailored data distributions. This paper introduces a novel algorithm to learn from datasets with varying quality, moving closer to a paradigm in which the imitation learning can be performed iteratively in a self-improvement setting. Our method extends RL-based imitation learning to action-free demonstrations, using a value function to transfer information between expert and non-expert data. Through comprehensive evaluation, we delineate the relation between different data distributions and algorithms and highlight the limitations of established methods. Our findings provide valuable insights for developing more robust and practical IfO techniques and on a path to scalable behaviour learning."
    },
    {
        "title": "Towards Specialized Web Agents Using Production-Scale Workflow Data",
        "link_suffix": "/forum?id=L9pTokEb8L",
        "link": "https://openreview.net/forum?id=L9pTokEb8L",
        "pdf_link": "https://openreview.net/pdf?id=L9pTokEb8L",
        "keywords": "LLM web agent",
        "abstract": "Large Language Model (LLM) agents are rapidly improving to handle increasingly complex web-based tasks. Most of these agents rely on general-purpose, proprietary models like GPT-4 and focus on designing better prompts to improve their planning abilities. However, general-purpose LLMs are not specifically trained to understand specialized web contexts such as HTML, and they often struggle with long-horizon planning. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data collected from over 250 domains corresponding to 6 billion tokens. This simple yet effective approach shows substantial gains over prompting-based agents on existing benchmarks---our WorkflowAgent achieves state-of-the-art performance on Mind2Web and substantially improves the baseline task success rate from 37.2% to 51.3% on WebArena. We further perform detailed ablation studies on various fine-tuning design choices and provide insights into LLM selection, training recipes, context window optimization, and effect of dataset sizes."
    }
]