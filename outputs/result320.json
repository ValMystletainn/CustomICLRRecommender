[
    {
        "title": "EDSNN: Edge Detection with Spiking Neuron Network",
        "link_suffix": "/forum?id=dL3h1lyUNd",
        "link": "https://openreview.net/forum?id=dL3h1lyUNd",
        "pdf_link": "https://openreview.net/pdf?id=dL3h1lyUNd",
        "keywords": "Spiking Neural Networks, Edge Detection, Spiking Multi-Scale Block, Membrane Average Decoding",
        "abstract": "Edge detection has made great progress under the development of Artificial Neural Networks (ANNs), particularly Convolutional Neural Networks (CNNs) and Transformers, some of them even have achieved a beyond human-level performance. However, these methods come with complex designs and high energy consumption. Spiking Neural Networks (SNNs), with their low energy consumption and biological interpretability, offer a promising solution to address these issues. In this work, we propose the first SNN-based method named EDSNN (Edge Detection with Spiking Neural Network) for edge detection. We construct a novel Spiking Multi-Scale Block (SMSB) to effectively utilize multi-scale information, thereby helping the network generate precise and clean edge maps. In addition, to more accurately decode spike trains, we present a Membrane Average Decoding (MAD) method in the prediction block. Our method has the advantages of remarkable efficiency and high performance across multiple datasets. It surpasses the human-level performance on BSDS500 (ODS=0.804 vs. ODS=0.803) while consuming only 14.64 mJ, remains competitive performance among top-performing ANN-based approaches on NYUDv2 (ODS=0.750), and achieves state-of-the-art performance on BIPED (ODS=0.891). Our codes are publicly available in supplementary materials."
    },
    {
        "title": "Understand Clean Generalization and Robust Overfitting in Adversarial Training from Two Theoretical Views: Representation Complexity and Training Dynamics",
        "link_suffix": "/forum?id=uSg854MOWu",
        "link": "https://openreview.net/forum?id=uSg854MOWu",
        "pdf_link": "https://openreview.net/pdf?id=uSg854MOWu",
        "keywords": "deep learning theory, adversarial training, clean generalization and robust overfitting, representation complexity, training dynamics, feature learning theory",
        "abstract": "Similar to surprising performance in the standard deep learning, deep nets trained by adversarial training also generalize well for unseen clean data (natural data). However, despite adversarial training can achieve low robust training error, there exists a significant robust generalization gap. We call this phenomenon the Clean Generalization and Robust Overfitting (CGRO). In this work, we study the CGRO phenomenon in adversarial training from two views: representation complexity and training dynamics. Specifically, we consider a binary classification setting with $N$ separated training data points. First, we prove that, based on the assumption that we assume there is $\\operatorname{poly}(D)$-size clean classifier (where $D$ is the data dimension), ReLU net with only $O(N D)$ extra parameters is able to leverages robust memorization to achieve the CGRO, while robust classifier still requires exponential representation complexity in worst case. Next, we focus on a structured-data case to analyze training dynamics, where we train a two-layer convolutional network with $O(N D)$ width against adversarial perturbation. We then show that a three-stage phase transition occurs during learning process and the network provably converges to robust memorization regime, which thereby results in the CGRO. Besides, we also empirically verify our theoretical analysis by experiments in real-image recognition datasets."
    },
    {
        "title": "Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model",
        "link_suffix": "/forum?id=6Hz1Ko087B",
        "link": "https://openreview.net/forum?id=6Hz1Ko087B",
        "pdf_link": "https://openreview.net/pdf?id=6Hz1Ko087B",
        "keywords": "Electrocardiogram, ECG, Cardiac signal, Self-supervised learning, ECG language processing",
        "abstract": "Electrocardiogram (ECG) is essential for the clinical diagnosis of arrhythmias and other heart diseases, but deep learning methods based on ECG often face limitations due to the need for high-quality annotations. Although previous ECG self-supervised learning (eSSL) methods have made significant progress in representation learning from unannotated ECG data, they typically treat ECG signals as ordinary time-series data, segmenting the signals using fixed-size and fixed-step time windows, which often ignore the form and rhythm characteristics and latent semantic relationships in ECG signals. In this work, we introduce a novel perspective on ECG signals, treating heartbeats as words and rhythms as sentences. Based on this perspective, we first designed the QRS-Tokenizer, which segments ECG signals into semantically meaningful ECG sentences. Building on these, we then propose HeartLang, a novel self-supervised learning framework for ECG language processing, learning general representations at form and rhythm levels. Additionally, we construct the largest heartbeat-based ECG vocabulary to date, which will further advance the development of ECG language processing. We evaluated HeartLang across six public ECG datasets, where it demonstrated robust competitiveness against other eSSL methods. Our data and code will be publicly available after acceptance."
    },
    {
        "title": "A New Perspective on Shampoo's Preconditioner",
        "link_suffix": "/forum?id=c6zI3Cp8c6",
        "link": "https://openreview.net/forum?id=c6zI3Cp8c6",
        "pdf_link": "https://openreview.net/pdf?id=c6zI3Cp8c6",
        "keywords": "optimization, hessian, shampoo, adagrad, second order optimization",
        "abstract": "Shampoo, a second-order optimization algorithm that uses a Kronecker product preconditioner, has recently received increasing attention from the machine learning community. Despite the increasing popularity of Shampoo, the theoretical foundations of its effectiveness are not well understood. The preconditioner used by Shampoo can be viewed as either an approximation of the Gauss--Newton component of the Hessian or the covariance matrix of the gradients maintained by Adagrad. Our key contribution is providing an explicit and novel connection between the optimal Kronecker product approximation of these matrices and the approximation\nmade by Shampoo. Our connection highlights a subtle but common misconception about Shampoo\u2019s approximation. In particular, the square of the approximation used by the Shampoo optimizer is equivalent to a single step of the power\niteration algorithm for computing the aforementioned optimal Kronecker product approximation. Across a variety of datasets and architectures we empirically\ndemonstrate that this is close to the optimal Kronecker product approximation. We also study the impact of batch gradients and empirical Fisher on the quality of Hessian approximation. Our findings not only advance the theoretical understanding of Shampoo but also illuminate potential pathways for enhancing its practical performance."
    },
    {
        "title": "EigenLoRA: Recycle trained Adapters for Resource Efficient Adaptation and Inference",
        "link_suffix": "/forum?id=KxGGZag9gW",
        "link": "https://openreview.net/forum?id=KxGGZag9gW",
        "pdf_link": "https://openreview.net/pdf?id=KxGGZag9gW",
        "keywords": "Parameter-efficient fine-tuning, Transfer learning, Low-rank, NLP, vision, diffusion, efficient learning, eco-friendly",
        "abstract": "Low-Rank Adapters (LoRA) are lightweight components that have made fine-tuning large models on domain-specific tasks inexpensive. This has resulted in an abundance of adapters in a growing open-source public community. We ask the question: can these adapters be used to inform and further streamline adaptation to new tasks? We introduce EigenLoRA, a parameter-efficient fine-tuning method that uses trained adapters to perform fast adaptation on new domains with orders of magnitude fewer parameters than LoRA. Our method finds a principal subspace that aligns with the domain of the trained adapters. This allows for efficient and fast adaptation to new tasks in this domain by simply learning coefficients on the principal components of this subspace. Furthermore, EigenLoRA makes inference time task-switching memory efficient. Instead of saving and loading whole LoRAs, EigenLoRA can simply load lightweight coefficients. EigenLoRA works across a variety of domains and tasks and is a viable solution for edge-based and efficient personalization applications."
    },
    {
        "title": "TrackMamba: Mamba-Transformer Tracking",
        "link_suffix": "/forum?id=V7QRVEZ0le",
        "link": "https://openreview.net/forum?id=V7QRVEZ0le",
        "pdf_link": "https://openreview.net/pdf?id=V7QRVEZ0le",
        "keywords": "Mamba; Single Object Tracking",
        "abstract": "Current one-stream Transformer-based trackers are quality but unfriendly to memory consumption of large resolution and long sequence, both of which are crucial keys to tracking tasks. Recently structured state space model (SSM) demonstrates promising performance and efficiency in sequence modeling but struggles to retrieve due to the limited hidden state number. To solve the computation challenge and explore the potential of Mamba, we propose TrackMamba, a Mamba-Transformer tracker containing TrackMamba Blocks and Attention Blocks. In order to better harness the scanning in TrackMamba Blocks for inter- and intra-frame modeling, we introduce various scan patterns for rearrangement and flipping. Furthermore, we propose Target Enhancement, including Temporal Token for target aggregation and search enhancement, and Temporal Mamba for target information cross-frame propagation. Extensive experiments show TrackMamba performs better than the first-generation one-stream Transformer-based tracker at same resolution and mitigates consumption growth when enlarging resolution, exhibiting the potential of Mamba-based model for large-resolution tracking."
    },
    {
        "title": "Text2PDE: Latent Diffusion Models for Accessible Physics Simulation",
        "link_suffix": "/forum?id=Nb3a8aUGfj",
        "link": "https://openreview.net/forum?id=Nb3a8aUGfj",
        "pdf_link": "https://openreview.net/pdf?id=Nb3a8aUGfj",
        "keywords": "AI4Science, PDE, Neural Operator, Latent Diffusion, Text2PDE",
        "abstract": "Recent advances in deep learning have inspired numerous works on data-driven solutions to partial differential equation (PDE) problems. These neural PDE solvers can often be much faster than their numerical counterparts; however, each presents its unique limitations and generally balances training cost, numerical accuracy, and ease of applicability to different problem setups. To address these limitations, we introduce several methods to apply latent diffusion models to physics simulation. Firstly, we introduce a mesh autoencoder to compress arbitrarily discretized PDE data, allowing for efficient diffusion training across various physics. Furthermore, we investigate full spatiotemporal solution generation to mitigate autoregressive error accumulation. Lastly, we investigate conditioning on initial physical quantities, as well as conditioning solely on a text prompt to introduce text2PDE generation. We show that language can be a compact, interpretable, and accurate modality for generating physics simulations, paving the way for more usable and accessible PDE solvers. Through experiments on both uniform and structured grids, we show that the proposed approach is competitive with current neural PDE solvers in both accuracy and efficiency, with promising scaling behavior up to $\\sim$3 billion parameters. By introducing a scalable, accurate, and usable physics simulator, we hope to bring neural PDE solvers closer to practical use."
    },
    {
        "title": "Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data",
        "link_suffix": "/forum?id=inLUnCpDIB",
        "link": "https://openreview.net/forum?id=inLUnCpDIB",
        "pdf_link": "https://openreview.net/pdf?id=inLUnCpDIB",
        "keywords": "deep learning theory, feature learning, adversarial robustness, adversarial training, non-convex optimization",
        "abstract": "Adversarial training is a widely-applied approach to training deep neural networks to be robust against adversarial perturbation. However, although adversarial training has achieved empirical success in practice, it still remains unclear why adversarial examples exist and how adversarial training methods improve model robustness. In this paper, we provide a theoretical understanding of adversarial examples and adversarial training algorithms from the perspective of feature learning theory. Specifically, we focus on a multiple classification setting, where the structured data can be composed of two types of features: the robust features, which are resistant to perturbation but sparse, and the non-robust features, which are susceptible to perturbation but dense. We train a two-layer smoothed ReLU convolutional neural network to learn our structured data. First, we prove that by using standard training (gradient descent over the empirical risk), the network learner primarily learns the non-robust feature rather than the robust feature, which thereby leads to the adversarial examples that are generated by perturbations aligned with negative non-robust feature directions. Then, we consider the gradient-based adversarial training algorithm, which runs gradient ascent to find adversarial examples and runs gradient descent over the empirical risk at adversarial examples to update models. We show that the adversarial training method can provably strengthen the robust feature learning and suppress the non-robust feature learning to improve the network robustness. Finally, we also empirically validate our theoretical findings with experiments on real-image datasets, including MNIST, CIFAR10 and SVHN."
    },
    {
        "title": "DBGMS: A Dual-Branch Generative Adversarial Network with Multi-Task Self-Supervised Enhancement for Robust Auditory Attention Decoding",
        "link_suffix": "/forum?id=70lFRMBygi",
        "link": "https://openreview.net/forum?id=70lFRMBygi",
        "pdf_link": "https://openreview.net/pdf?id=70lFRMBygi",
        "keywords": "electroencephalogram(EEG), Auditory Attention Decoding(AAD), Dual-branch, generative adversarial networks(GANs)",
        "abstract": "Detecting auditory attention from brain signals has been a significant challenge in neuroscience and brain-computer interface research. While progress has been made in EEG-based auditory attention detection, existing methods often struggle with limited data and short decision windows, particularly in complex auditory environments. In this paper, we propose DBGMS (Dual-Branch Generative Adversarial Network with Multi-Task Self-Supervised Enhancement), a novel framework for robust auditory attention decoding from electroencephalogram (EEG) signals. There are three key innovations in our approach:\n(1) A dual-branch architecture is developed that combines temporal attention and frequency residual learning, enabling more comprehensive feature extraction to be achieved from EEG signals;\n(2) Branch-specific generative adversarial networks (GANs) are designed to generate high-quality augmented samples in both temporal and frequency domains, effectively addressing the data scarcity issue in auditory attention decoding;\n(3) Attention mechanisms and graph convolution operations are incorporated in both temporal and frequency domains.\n(4) A multi-task self-supervised learning strategy is introduced, incorporating several complementary tasks such as temporal order prediction, frequency band reconstruction, and time-frequency consistency. This approach leverages unlabeled data to enhance the model's ability to capture subtle attention-related features from multiple perspectives, thereby improving generalization across subjects and listening conditions.\nIn contrast to state-of-the-art methods, DBGMS presents significant improvements in detection accuracy and robustness, particularly for short decision windows. Our framework is evaluated on two public EEG datasets, including KUL and DTU, demonstrating its effectiveness across various experimental settings."
    },
    {
        "title": "Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment",
        "link_suffix": "/forum?id=QG31By6S6w",
        "link": "https://openreview.net/forum?id=QG31By6S6w",
        "pdf_link": "https://openreview.net/pdf?id=QG31By6S6w",
        "keywords": "Medical Image Segmentation, Vision-Language Pre-Training, Zero-Shot Segmentation",
        "abstract": "Recent advancements in medical vision-language pre-training models have driven significant progress in zero-shot disease recognition. However, transferring image-level knowledge to pixel-level tasks, such as lesion segmentation in 3D CT scans, remains a critical challenge. Due to the complexity and variability of pathological visual characteristics, existing methods struggle to align fine-grained lesion features not encountered during training with disease-related textual representations. In this paper, we present Malenia, a novel multi-scale lesion-level mask-attribute alignment framework, specifically designed for 3D zero-shot lesion segmentation. Malenia improves the compatibility between mask representations and their associated elemental attributes, explicitly linking the visual features of unseen lesions with the extensible knowledge learned from previously seen ones. Furthermore, we design a Cross-Modal Knowledge Injection module to enhance both visual and textual features with mutually beneficial information, effectively guiding the generation of segmentation results. Comprehensive experiments across three datasets and 12 lesion categories validate the superior performance of Malenia. Codes will be publicly available."
    },
    {
        "title": "Interpretable Pre-Trained Transformers for Heart Time-Series Data",
        "link_suffix": "/forum?id=eciCtsqGc8",
        "link": "https://openreview.net/forum?id=eciCtsqGc8",
        "pdf_link": "https://openreview.net/pdf?id=eciCtsqGc8",
        "keywords": "biosignals, interpretability, healthcare, transformers",
        "abstract": "Interpretability of artificial intelligence models is vital in healthcare, as a poorly informed decision can directly impact the health and well-being of patients. This means that, owing to their black box nature, deep-learning solutions that may even yield high accuracy often fail to be adopted in real-world healthcare settings. To this end, we employ the generative pre-trained transformer (GPT) framework to clinical heart time-series data, to create two pre-trained general purpose cardiac models, termed PPG-PT and ECG-PT. We place a special emphasis on making both such pre-trained models fully interpretable. This is achieved firstly through aggregate attention maps which show that, in order to make predictions, the model focuses on similar points in previous cardiac cycles and gradually broadens its attention in deeper layers. Next, we show that tokens with the same value, which occur at different distinct points in the electrocardiography (ECG) and photoplethysmography (PPG) cycle, form separate clusters in a high dimensional space. Such clusters are formed according to the phase of the cardiac cycle, as the tokens propagate through the transformer blocks. Finally, we highlight that individual attention heads correspond to specific physiologically relevant features, such as the dicrotic notch in PPG and the P-wave in ECG. Importantly, it is also demonstrated that these pre-trained models are straightforward to fine-tune for tasks such as the classification of atrial fibrillation (AF), and beat detection in photoplethysmography. The so introduced PPG-PT and ECG-PT models achieve accuracy comparable to the state-of-the-art for both tasks, whilst crucially retaining their interpretability and explainability. This is demonstrated in the AF-screening fine-tuned model, with attention clearly shifting to regions in the context that are strongly indicative of atrial fibrillation."
    },
    {
        "title": "HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics",
        "link_suffix": "/forum?id=DTQlvDCAql",
        "link": "https://openreview.net/forum?id=DTQlvDCAql",
        "pdf_link": "https://openreview.net/pdf?id=DTQlvDCAql",
        "keywords": "Long-form video understanding, episodic memory, semantics extraction",
        "abstract": "Existing research often treats long-form videos as extended short videos, leading to several limitations: inadequate capture of long-range dependencies, inefficient processing of redundant information, and failure to extract high-level semantic concepts. To address these issues, we propose a novel approach that more accurately reflects human cognition. This paper introduces HERMES: temporal-coHERent long-forM understanding with Episodes and Semantics, a model that simulates episodic memory accumulation to capture action sequences and reinforces them with semantic knowledge dispersed throughout the video. Our work makes two key contributions: First, we develop an Episodic COmpressor (ECO) that efficiently aggregates crucial representations from micro to semi-macro levels, overcoming the challenge of long-range dependencies. Second, we propose a Semantics ReTRiever (SeTR) that enhances these aggregated representations with semantic information by focusing on the broader context, dramatically reducing feature dimensionality while preserving relevant macro-level information. This addresses the issues of redundancy and lack of high-level concept extraction. Extensive experiments demonstrate that HERMES achieves state-of-the-art performance across multiple long-video understanding benchmarks in both zero-shot and fully-supervised settings."
    },
    {
        "title": "OC-CLIP : Object-centric binding in Contrastive Language-Image Pretraining",
        "link_suffix": "/forum?id=a84AD957m9",
        "link": "https://openreview.net/forum?id=a84AD957m9",
        "pdf_link": "https://openreview.net/pdf?id=a84AD957m9",
        "keywords": "object-centric representations, object binding, CLIP, contrastive learning, compositional image-to-text retrieval",
        "abstract": "Recent advancements in vision-language models (VLMs) have been driven by contrastive models like CLIP which learn to associate visual information with their corresponding text descriptions. However, these models have limitations in understanding complex compositional scenes involving multiple objects and their spatial relationships. To address these challenges, we propose a novel approach that diverges from traditional data-centric methods of enhancing model performance with hard negatives examples. Our work instead focuses on integrating sufficient inductive biases into pre-trained CLIP-like models to improve their compositional understanding without using additional data annotations. We introduce a binding module that connects a scene graph of the text with an induced graph-like representation of the image, facilitating a structured similarity assessment. We also leverage relationships as text-conditioned visual constraints, thereby capturing the intricate interactions between objects and their contextual relationships more effectively. Our resulting model (OC-CLIP) not only enhances the performance of CLIP in multi-object compositional understanding but also paves the way for more accurate and efficient image-text matching in complex scenes."
    },
    {
        "title": "Unsupervised 2D Molecule Drug-likeness Prediction based on Knowledge Distillation",
        "link_suffix": "/forum?id=3lfSk8NWWp",
        "link": "https://openreview.net/forum?id=3lfSk8NWWp",
        "pdf_link": "https://openreview.net/pdf?id=3lfSk8NWWp",
        "keywords": "Drug-likeness Prediction, Molecule Representation, Molecular Property Prediction",
        "abstract": "With the research significance and application value, drug-likeness prediction aims to accurately screen high-quality drug candidates, and has attracted increasing attention recently. In this regard, dominant studies can be roughly classified into two categories: (1) Supervised drug-likeness prediction based on binary classifiers. To train classifiers, the common practice is to treat real drugs as positive examples and other molecules as negative ones. However, the manual selection of negative samples introduces classification bias into these classifiers. (2) Unsupervised drug-likeness prediction based on SMILES representations, such as an RNN-based language model trained on real drugs. Nevertheless, using SMILES to represent molecules is suboptimal for drug-likeness prediction, which is more relevant to the topological structures of molecules. Besides, the RNN model tends to assign short-SMILES molecules with high scores, \nregardless of their structures. In this paper, we propose a novel knowledge distillation based unsupervised method, which exploits 2D features of molecules for drug-likeness prediction. The teacher model learns the topology of molecules via two pre-training tasks on a large-scale dataset, and the student model mimic the teacher model on real drugs. In this way, the outputs of these two models will be similar on the drug-like molecules while significantly different on the non-drug-like molecules. To demonstrate the effectiveness of our method, we conduct several groups of experiments on various datasets. Experimental results and in-depth analysis show that our method significantly surpasses all baselines, achieving state-of-the-art performance. Particularly, the prediction bias of SIMILES length is reduced in our method. We will release our code upon the acceptance of our paper."
    },
    {
        "title": "Rethinking Attentions in Zero-Shot Real Image Editing",
        "link_suffix": "/forum?id=kn2OZa8rOf",
        "link": "https://openreview.net/forum?id=kn2OZa8rOf",
        "pdf_link": "https://openreview.net/pdf?id=kn2OZa8rOf",
        "keywords": "Stable Diffusion, Image Editing, Zero-Shot Algorithm, Attention",
        "abstract": "Editing natural images using textual descriptions in text-to-image diffusion models remains a significant challenge, particularly in achieving consistent generation and handling complex, non-rigid objects. Existing methods often struggle to preserve textures and identity, require extensive fine-tuning, and exhibit limitations in editing specific spatial regions or objects while retaining background details. This paper proposes Context-Preserving Adaptive Manipulation (CPAM) -- a novel zero-shot method for complicated, non-rigid real image editing. Specifically, we propose a preservation adaptation module that adjusts self-attention mechanisms to effectively preserve and independently control the object and background. This ensures that the objects' shapes, textures, and identities are maintained while keeping the background undistorted during the editing process using the mask guidance technique. Additionally, we develop a localized extraction module to mitigate the interference with the non-desired modified regions during conditioning in cross-attention mechanisms. We also introduce various mask-guidance strategies to facilitate diverse image manipulation tasks in a simple manner. Extensive experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a robust benchmark dataset specifically designed for real image editing, demonstrate that our proposed method is the preferred choice among human raters, outperforming existing state-of-the-art editing techniques."
    },
    {
        "title": "offline_rl_ope: A Python package for off-policy evaluation of offline RL models with real world data",
        "link_suffix": "/forum?id=6PcJEFKvBD",
        "link": "https://openreview.net/forum?id=6PcJEFKvBD",
        "pdf_link": "https://openreview.net/pdf?id=6PcJEFKvBD",
        "keywords": "Offline RL, OPE, Python, PyTorch",
        "abstract": "offline_rl_ope is a fully unit tested and runtime type checked Python package for performing off-policy evaluation of offline RL models. offline_rl_ope has been designed for OPE workflows using real world data by: naturally handling uneven trajectory lengths; including novel convergence metrics which do not rely on OPE estimator ground truths; and providing a compute and data efficient API which can be integrated with many offline RL frameworks. This paper motivates and describes the core API design and functionality to enable ease of use and extension. The implementations of OPE methods have been benchmarked against existing implementations to ensure consistency and reproducibility. The offline_rl_ope source code can be found on GitHub at: REDACTED."
    },
    {
        "title": "Boundless Socratic Learning",
        "link_suffix": "/forum?id=LsZxlxA9da",
        "link": "https://openreview.net/forum?id=LsZxlxA9da",
        "pdf_link": "https://openreview.net/pdf?id=LsZxlxA9da",
        "keywords": "position paper, self-improvement, language, games, recursion, ASI",
        "abstract": "An agent trained within a closed system can master any desired capability, as long as the following three conditions hold: (a) it receives sufficiently informative and aligned feedback, (b) its coverage of experience/data is broad enough, and (c) it has sufficient capacity and resource. In this white paper, we justify these conditions, and consider what limitations arise from (a) and (b) in closed systems, when assuming that (c) is not a bottleneck. Considering the special case of agents with matching input and output spaces (namely, language), we argue that such pure recursive self-improvement, dubbed \"Socratic learning\", can boost performance vastly beyond what is present in its initial data or knowledge, and is only limited by time, as well as gradual misalignment concerns. Furthermore, we propose a constructive framework to implement it, based on the notion oflanguage games."
    },
    {
        "title": "ConceptFlow: Unified Framework for Personalized Image Generation",
        "link_suffix": "/forum?id=EhSUM1FcJw",
        "link": "https://openreview.net/forum?id=EhSUM1FcJw",
        "pdf_link": "https://openreview.net/pdf?id=EhSUM1FcJw",
        "keywords": "Personalized Image Generation, Visual Guidance",
        "abstract": "Personalized image generation is an appealing area of research within controllable image generation due to its diverse potential applications. Despite notable advancements, generating images based on single or multiple concepts remains challenging. For single-concept generation, it is difficult to strike a balance between identity preservation and prompt alignment, especially in complex prompts. When it comes to multiple concepts, creating images from a single prompt without extra conditions, such as layout boxes or semantic masks, is problematic due to significantly identity loss and concept omission. In this paper, we introduce ConceptFlow, a comprehensive framework designed to tackle these challenges. Specifically, we propose ConceptFlow-S and ConceptFlow-M for single-concept generation and multiple-concept generation, respectively. ConceptFlow-S introduces a KronA-WED adapter, which integrates a Kronecker adapter with weight and embedding decomposition, and employs a disentangled learning approach with a novel attention regularization objective to enhance single-concept generation. On the other hand, ConceptFlow-M leverages models learned from ConceptFlow-S to directly generate multi-concept images without needed of additional conditions, proposing Subject-Adaptive Matching Attention (SAMA) module and layout consistency guidance strategy. Our extensive experiments and user study show that ConceptFlow effectively addresses the aforementioned issues, enabling its application in various real-world scenarios such as advertising and garment try-on."
    },
    {
        "title": "KLay: Accelerating Neurosymbolic AI",
        "link_suffix": "/forum?id=Zes7Wyif8G",
        "link": "https://openreview.net/forum?id=Zes7Wyif8G",
        "pdf_link": "https://openreview.net/pdf?id=Zes7Wyif8G",
        "keywords": "Neurosymbolic AI, Arithmetic Circuits, Sparse Inference",
        "abstract": "A popular approach to neurosymbolic AI involves mapping logic formulas to arithmetic circuits (computation graphs consisting of sums and products) and passing the outputs of a neural network through these circuits. This approach enforces symbolic constraints onto a neural network in a principled and end-to-end differentiable way. Unfortunately, arithmetic circuits are challenging to run on modern AI accelerators as they exhibit a high degree of irregular sparsity. To address this limitation, we introduce knowledge layers (KLay), a new data structure to represent arithmetic circuits that can be efficiently parallelized on GPUs. Moreover, we contribute two algorithms used in the translation of traditional circuit representations to KLay and a further algorithm that exploits parallelization opportunities during circuit evaluations. We empirically show that KLay achieves speedups of multiple orders of magnitude over the state of the art, thereby paving the way towards scaling neurosymbolic AI to larger real-world applications."
    },
    {
        "title": "Explainable Graph Representation Learning via Graph Pattern Analysis",
        "link_suffix": "/forum?id=hXJrQWIoR3",
        "link": "https://openreview.net/forum?id=hXJrQWIoR3",
        "pdf_link": "https://openreview.net/pdf?id=hXJrQWIoR3",
        "keywords": "graph representation learning, explainable graph learning",
        "abstract": "Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning.\nIn this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional.\nTo address these limitations, we introduce a framework for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution.\nWe also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness."
    },
    {
        "title": "Neat: Nonlinear Parameter-efficient Adaptation of Pre-trained Models",
        "link_suffix": "/forum?id=l3oE5vBjDs",
        "link": "https://openreview.net/forum?id=l3oE5vBjDs",
        "pdf_link": "https://openreview.net/pdf?id=l3oE5vBjDs",
        "keywords": "parameter-efficient fine-tuning, pre-trained model",
        "abstract": "Fine-tuning pre-trained models is crucial for adapting large models to downstream tasks, often delivering state-of-the-art performance. However, fine-tuning all model parameters is resource-intensive and laborious, leading to the emergence of parameter-efficient fine-tuning (PEFT) methods. One widely adopted PEFT technique, Low-Rank Adaptation (LoRA), freezes the pre-trained model weights and introduces two low-rank matrices whose ranks are significantly smaller than the dimensions of the original weight matrices. This enables efficient fine-tuning by adjusting only a small number of parameters. Despite its efficiency, LoRA approximates weight updates using low-rank decomposition, which struggles to capture complex, non-linear components and efficient optimization trajectories. As a result, LoRA-based methods often exhibit a significant performance gap compared to full fine-tuning. Closing this gap requires higher ranks, which increases the number of parameters. To address these limitations, we propose a nonlinear parameter-efficient adaptation method (NEAT). NEAT introduces a lightweight neural network that takes pre-trained weights as input and learns a nonlinear transformation to approximate cumulative weight updates. These updates can be interpreted as functions of the corresponding pre-trained weights. The nonlinear approximation directly models the cumulative updates, effectively capturing complex and non-linear structures in the weight updates. Our theoretical analysis demonstrates taht NEAT can be more efficient than LoRA while having equal or greater expressivity. Extensive evaluations across four benchmarks and over twenty datasets demonstrate that NEAT significantly outperforms baselines in both vision and text tasks."
    },
    {
        "title": "Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs",
        "link_suffix": "/forum?id=NS1G1Uhny3",
        "link": "https://openreview.net/forum?id=NS1G1Uhny3",
        "pdf_link": "https://openreview.net/pdf?id=NS1G1Uhny3",
        "keywords": "data selection, retrieval, active learning, transductive active learning, local learning, test-time fine-tuning, transductive learning, language modeling, uncertainty quantification",
        "abstract": "Recent efforts in fine-tuning language models often rely on automatic data selection, commonly using Nearest Neighbors retrieval from large datasets.\nHowever, we theoretically show that this approach tends to select redundant data, limiting its effectiveness or even hurting performance.\nTo address this, we introduce SIFT, a data selection algorithm designed to reduce uncertainty about the model's response given a prompt, which unifies ideas from retrieval and active learning.\nWhereas Nearest Neighbor retrieval typically fails in the presence of information duplication, SIFT accounts for information duplication and optimizes the overall information gain of the selected examples.\nWe focus our evaluations on fine-tuning at test-time for prompt-specific language modeling on the Pile dataset, and show that SIFT consistently outperforms Nearest Neighbor retrieval, with minimal computational overhead.\nMoreover, we show that our uncertainty estimates can predict the performance gain of test-time fine-tuning, and use this to develop an adaptive algorithm that invests test-time compute proportional to realized performance gains.\nWe provide theactiveft(Active Fine-Tuning) library which can be used as a drop-in replacement for Nearest Neighbor retrieval."
    },
    {
        "title": "PDE-GAN for solving PDE optimal control problems more accurately and efficiently",
        "link_suffix": "/forum?id=3RcztSIHiA",
        "link": "https://openreview.net/forum?id=3RcztSIHiA",
        "pdf_link": "https://openreview.net/pdf?id=3RcztSIHiA",
        "keywords": "Optimal control, deep learing, PINNs, GANs",
        "abstract": "PDE optimal control (PDEOC) problems aim to optimize the performance of physical systems constrained by partial differential equations (PDEs) to achieve desired characteristics. Such problems frequently appear in scientific discoveries and are of huge engineering importance. Physics-informed neural networks (PINNs) are recently proposed to solve PDEOC problems, but it may fail to balance the different competing loss terms in such problems. Our work proposes PDE-GAN, a novel approach that puts PINNs in the framework of generative adversarial networks (GANs) \u201clearn the loss function\u201d to address the trade-off between the different competing loss terms effectively. We conducted detailed and comprehensive experiments to compare PDE-GANs with vanilla PINNs in solving four typical and representative PDEOC problems, namely, (1) boundary control on Laplace Equation, (2) time-dependent distributed control on Inviscous Burgers' Equation, (3) initial value control on Burgers' Equation with Viscosity, and (4) time-space-dependent distributed control on Burgers' Equation with Viscosity. Strong numerical evidence supports the PDE-GAN that it achieves the highest accuracy and shortest computation time without the need of line search which is necessary for vanilla PINNs."
    },
    {
        "title": "Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning",
        "link_suffix": "/forum?id=F7QNwDYG6I",
        "link": "https://openreview.net/forum?id=F7QNwDYG6I",
        "pdf_link": "https://openreview.net/pdf?id=F7QNwDYG6I",
        "keywords": "LLM, Alignment, Planning",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capability across various natural language tasks. However, the auto-regressive generation process makes LLMs prone to produce errors, hallucinations and inconsistent statements when performing multi-step reasoning. In this paper, by casting multi-step reasoning of LLMs as a heuristic search problem, we aim to alleviate the pathology by introducing Q*, a general, versatile and agile framework for guiding LLMs decoding process with deliberative planning. By learning a plug-and-play Q-value model as heuristic function for estimating expected future rewards, Q* can effectively guide LLMs to select the most promising next reasoning step without fine-tuning LLMs for the targeted task, which avoids the significant computational overhead and potential risk of performance degeneration on other tasks. Extensive experiments on GSM8K, MATH and MBPP datasets demonstrate the superiority of our method, contributing to improving the reasoning capability of existing open-source LLMs. Furthermore, the testing-time scaling law indicates that Q* can leverage increased computational power to improve reasoning performance."
    },
    {
        "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification",
        "link_suffix": "/forum?id=hzVpZDrW73",
        "link": "https://openreview.net/forum?id=hzVpZDrW73",
        "pdf_link": "https://openreview.net/pdf?id=hzVpZDrW73",
        "keywords": "Efficient Multimodal Large Language Model",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision understanding, reasoning, and interaction. However, the inference computation and memory increase progressively with the generation of output tokens during decoding, directly affecting the efficacy of MLLMs. Existing methods attempt to reduce the vision context redundancy to achieve efficient MLLMs. Unfortunately, the efficiency benefits of the vision context reduction in the prefill stage gradually diminish during the decoding stage. To address this problem, we proposed a dynamic vision-language context sparsification framework Dynamic-LLaVA, which dynamically reduces the redundancy of vision context in the prefill stage and decreases the memory and computation overhead of the generated language context during decoding. Dynamic-LLaVA designs a tailored sparsification inference scheme for different inference modes, i.e., prefill, decoding with and without KV cache, to achieve efficient inference of MLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by $\\sim$75% in the prefill stage. Meanwhile, throughout the entire generation process of MLLMs, Dynamic-LLaVA reduces the $\\sim$50% computation consumption under decoding without KV cache, while saving $\\sim$50% GPU memory overhead when decoding with KV cache, due to the vision-language context sparsification. Extensive experiments also demonstrate that Dynamic-LLaVA achieves efficient inference for MLLMs with negligible understanding and generation ability degradation or even performance gains compared to the full-context inference baselines."
    }
]