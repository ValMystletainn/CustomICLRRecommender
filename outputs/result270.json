[
    {
        "title": "INFER: A Neural-symbolic Model For Extrapolation Reasoning on Temporal Knowledge Graph",
        "link_suffix": "/forum?id=ExHUtB2vnz",
        "link": "https://openreview.net/forum?id=ExHUtB2vnz",
        "pdf_link": "https://openreview.net/pdf?id=ExHUtB2vnz",
        "keywords": "Knowledge Graph, Temporal Knowledge Graph, Temporal Rules, Temporal Validity",
        "abstract": "Temporal Knowledge Graph(TKG) serves as an efficacious way to store dynamic facts in real-world. Extrapolation reasoning on TKGs, which aims at predicting possible future events, has attracted consistent research interest. Recently, some rule-based methods have been proposed, which are considered more interpretable compared with embedding-based methods. Existing rule-based methods apply rules through path matching or subgraph extraction, which falls short in inference ability and suffers from missing facts in TKGs. Besides, during rule application period, these methods consider the standing of facts as a binary 0 or 1 problem and ignores the validity as well as frequency of historical facts under temporal settings.\nIn this paper, by designing a novel paradigm for rule application, we propose INFER, a neural-symbolic model for TKG extrapolation. With the introduction of Temporal Validity Function, INFER firstly considers the frequency and validity of historical facts and extends the truth value of facts into continuous real number to better adapt for temporal settings. INFER builds Temporal Weight Matrices with a pre-trained static KG embedding model to enhance its inference ability. Moreover INFER adopts a rule projection module which enables it apply rules through conducting matrices operation on GPU, which improves the efficiency of rule application. This feature also facilitates potential integration with existing embedding-based methods.\nExperimental results show that INFER achieves state-of-the-art performance on three datasets and significantly outperforms existing rule-based models on our modified, more sparse TKG datasets, which demonstrates the superiority of our model in inference ability."
    },
    {
        "title": "AIPO: Agreement-Aware Iterative Preference Optimization for Length Exploitation Mitigation",
        "link_suffix": "/forum?id=ixdAVqjShn",
        "link": "https://openreview.net/forum?id=ixdAVqjShn",
        "pdf_link": "https://openreview.net/pdf?id=ixdAVqjShn",
        "keywords": "Language Modeling, Alignment",
        "abstract": "Direct Preference Optimization (DPO) is gaining popularity as an alternative to Proximal Policy Optimization (PPO) for aligning Large Language Models (LLMs). Recent research on aligning LLMs iteratively with synthetic or partially synthetic data has shown promising outcomes, facilitating the scalability of DPO training in both academic settings and proprietary models such as Llama 3. Despite its success, we observe that the issue of length exploitation in DPO becomes more pronounced during iterative preference optimization, with the severity escalating progressively with each iteration. This observation prompts an in-depth examination of iterative preference optimization with synthetic data. In this paper, we present our findings and analyses in building our iterative preference optimization pipeline. Specifically, we analyze the issue of length exploitation in this iterative process and propose a novel training objective for iterative preference optimization, namely \\textbf{A}greement-aware \\textbf{I}terative \\textbf{P}reference \\textbf{O}ptimization (AIPO). To demonstrate the effectiveness of our proposed method, we conduct extensive experiments and show that it achieves state-of-the-art performance on MT-Bench, AlpacaEval 2.0, and Arena-Hard."
    },
    {
        "title": "Supervised and Semi-Supervised Diffusion Maps with Label-Driven Diffusion",
        "link_suffix": "/forum?id=G3B5ReApDw",
        "link": "https://openreview.net/forum?id=G3B5ReApDw",
        "pdf_link": "https://openreview.net/pdf?id=G3B5ReApDw",
        "keywords": "Manifold Learning, Diffusion Maps, Supervised dimension reduction, Multi-view learning",
        "abstract": "In this paper, we introduce Supervised Diffusion Maps (SDM) and Semi-Supervised Diffusion Maps (SSDM), which transform the well-known unsupervised dimensionality reduction algorithm, Diffusion Maps, into supervised and semi-supervised learning tools. The proposed methods, SDM and SSDM, are based on our new approach that treats the labels as a second view of the data. This unique framework allows us to incorporate ideas from multi-view learning. Specifically, we propose constructing two affinity kernels corresponding to the data and the labels. We then propose a multiplicative interpolation scheme of the two kernels, whose purpose is twofold. First, our scheme extracts the common structure underlying the data and the labels by defining a diffusion process driven by the data and the labels. This label-driven diffusion produces an embedding that emphasizes the properties relevant to the label-related task. Second, the proposed interpolation scheme balances the influence of the two kernels. We show on multiple benchmark datasets that the embedding learned by SDM and SSDM is more effective in downstream regression and classification tasks than existing unsupervised, supervised, and semi-supervised nonlinear dimension reduction methods."
    },
    {
        "title": "Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data",
        "link_suffix": "/forum?id=FQEFWGT19m",
        "link": "https://openreview.net/forum?id=FQEFWGT19m",
        "pdf_link": "https://openreview.net/pdf?id=FQEFWGT19m",
        "keywords": "data integration, disentangled representations, distribution shift, posterior drift",
        "abstract": "Multi-task learning (MTL) has emerged as an imperative machine learning tool to solve multiple learning tasks simultaneously and has been successfully applied to  healthcare, marketing, and biomedical fields. However, in order to borrow information across different tasks effectively, it is essential to utilize both homogeneous and heterogeneous information. Among the extensive literature on MTL, various forms of heterogeneity are presented in MTL problems, such as block-wise, distribution, and posterior heterogeneity. Existing methods, however, struggle to tackle these forms of heterogeneity simultaneously in a unified framework. In this paper, we propose a two-step learning strategy for MTL which addresses the aforementioned heterogeneity. First, we impute the missing blocks using shared representations extracted from homogeneous source across different tasks. Next, we disentangle the mappings between input features and responses into a shared component and a task-specific component, respectively, thereby enabling information borrowing through the shared component. Our numerical experiments and real-data analysis from the ADNI database demonstrate the superior MTL performance of the proposed method compared to a single task learning and other competing methods."
    },
    {
        "title": "Distilling Auto-regressive Models into Few Steps 1: Image Generation",
        "link_suffix": "/forum?id=zKlFXV87Pp",
        "link": "https://openreview.net/forum?id=zKlFXV87Pp",
        "pdf_link": "https://openreview.net/pdf?id=zKlFXV87Pp",
        "keywords": "image autoregressive models, parallel decoding, distillation",
        "abstract": "Autoregressive (AR) models have recently achieved state-of-the-art performance in text and image generation. However, their primary limitation is slow generation speed due to the token-by-token process. We ask an ambitious question: can a pre-trained AR model be adapted to generate outputs in just one or two steps? If successful, this would significantly advance the development and deployment of AR models. We notice that existing works that attempt to speed up AR generation by generating multiple tokens at once fundamentally cannot capture the output distribution due to the conditional dependencies between tokens, limiting their effectiveness for few-step generation. To overcome this, we propose Distilled Decoding (DD), which leverages flow matching to create a deterministic mapping from Gaussian distribution to the output distribution of the pre-trained AR model. We then train a network to distill this mapping, enabling few-step generation. We evaluate DD on state-of-the-art image AR models and present promising results. For VAR, which requires 10-step generation (680 tokens), DD enables one-step generation (6.3$\\times$ speed-up), with an acceptable increase in FID from 4.19 to 10.65. Similarly, for LlamaGen, DD reduces generation from 256 steps to 1, achieving an 217.8$\\times$ speed-up with a comparable FID increase from 6.52 to 17.98. In both cases, baseline methods completely fail with FID scores $>$100. As the first work to demonstrate the possibility of one-step generation for image AR models, DD challenges the prevailing notion that AR models are inherently slow, and opens up new opportunities for efficient AR generation."
    },
    {
        "title": "Retrieval Information Injection for Enhanced Medical Report Generation",
        "link_suffix": "/forum?id=JNh8CCDugm",
        "link": "https://openreview.net/forum?id=JNh8CCDugm",
        "pdf_link": "https://openreview.net/pdf?id=JNh8CCDugm",
        "keywords": "Medical Report Generation, Contrastive Decoding, Training-free",
        "abstract": "Automatically generating medical reports is an effective solution to the diagnostic bottleneck caused by physician shortage. Existing methods have demonstrated exemplary performance in generating high-textual-quality reports. Due to the high similarity among medical images as well as the structural and content homogeneity of medical reports, these methods often make it difficult to fully capture the semantic information in medical images. To address this issue, we propose a training-free Retrieval Information injectioN (RIN) method by simulating the process of Multidisciplinary Consultation. The essence of this method lies in fully utilizing similar reports of target images to enhance the performance of pre-trained medical report generation models. Specifically, we first retrieve images most similar to the target image from a pre-constructed image feature database. Then, the reports corresponding to these images are inputted into a report generator of the pre-trained model, obtaining the distributions of retrieved reports. RIN generates final reports by integrating prediction distributions of the pre-trained model and the average distributions of retrieved reports, thereby enhancing the accuracy and reliability of the generated report. Comprehensive experimental results demonstrate that RIN significantly enhances clinical efficacy in chest X-rays report generation task. Compared to the current state-of-the-art methods, it achieves competitive results."
    },
    {
        "title": "Infinite-parameter Large Language Model",
        "link_suffix": "/forum?id=5XL8c0Vg9k",
        "link": "https://openreview.net/forum?id=5XL8c0Vg9k",
        "pdf_link": "https://openreview.net/pdf?id=5XL8c0Vg9k",
        "keywords": "lifelong learning",
        "abstract": "In the standard transformer architecture, increasing model parameters leads to linear growth in computational cost and activation memory. To address this issue, we propose a novel Infinite Parameter Large Language Model (IP-LLM) architecture that decouples model size from computational cost and device memory. Existing large language models are all fixed-parameter models, while human knowledge is infinite and expands daily. Finite parameters are inherently limited in their capacity to accommodate this boundless knowledge. Our IP-LLM architecture can potentially accommodate infinite knowledge, resolving this issue and laying the foundation for realizing a truly omniscient and omnipotent artificial general intelligence in the future."
    },
    {
        "title": "Progressively Label Enhancement for Large Language Model Alignment",
        "link_suffix": "/forum?id=ulJNq6FQrw",
        "link": "https://openreview.net/forum?id=ulJNq6FQrw",
        "pdf_link": "https://openreview.net/pdf?id=ulJNq6FQrw",
        "keywords": "Large Language Model, LLM Alignment",
        "abstract": "Large Language Models (LLM) alignment aims to prevent models from producing content that misaligns with human expectations, which can lead to ethical and legal concerns. \n   In the last few years, Reinforcement Learning from Human Feedback (RLHF) has been the most prominent method for achieving alignment.\n   Due to challenges in stability and scalability with RLHF stages, which arise from the complex interactions between multiple models, researchers are exploring alternative methods to achieve effects comparable to those of RLHF.\n   However, these methods often rely on large high-quality datasets.\n   Despite some methods considering the generation of additional data to expand datasets, they often treat model training and data generation as separate and static processes, overlooking the fact that these processes are highly interdependent, leading to inefficient utilization of the generated data.\n   To deal with this problem, we propose PLE, i.e., Progressively Label Enhancement for LLM Alignment, a framework that dynamically adjusts the model\u2019s training process based on the evolving quality of the generated data.\n   Specifically, we prompt the model to generate responses for both the original query and the query guided by a set of carefully designed principles, and then utilize a dynamic threshold to determine the appropriate training approach for both responses based on their corresponding reward scores. \n   Experimental results demonstrate the effectiveness of PLE compared to existing LLM alignment methods."
    },
    {
        "title": "Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model",
        "link_suffix": "/forum?id=e5jGTEiJMT",
        "link": "https://openreview.net/forum?id=e5jGTEiJMT",
        "pdf_link": "https://openreview.net/pdf?id=e5jGTEiJMT",
        "keywords": "Policy Learning, Online Improve",
        "abstract": "Recent advancements in robot learning have used imitation learning with large models and extensive demonstrations to develop effective policies. However, these models are often limited by the quantity quality, and diversity of demonstrations. This paper explores improving offline-trained imitation learning models through online interactions with the environment. We introduce Policy Decorator, which uses a model-agnostic residual policy to refine large imitation learning models during online interactions. By implementing controlled exploration strategies, Policy Decorator enables stable, sample-efficient online learning. Our evaluation spans eight tasks across two benchmarks\u2014ManiSkill and Adroit\u2014and involves two state-of-the-art imitation learning models (Behavior Transformer and Diffusion Policy). The results show Policy Decorator effectively improves the offline-trained policies and preserves the smooth motion of imitation learning models, avoiding the erratic behaviors of pure RL policies. See ourproject pagefor videos."
    },
    {
        "title": "Streamlining Redundant Layers to Compress Large Language Models",
        "link_suffix": "/forum?id=IC5RJvRoMp",
        "link": "https://openreview.net/forum?id=IC5RJvRoMp",
        "pdf_link": "https://openreview.net/pdf?id=IC5RJvRoMp",
        "keywords": "large language models, model compression, structured pruning",
        "abstract": "This paper introduces LLM-Streamline, a pioneer work on layer pruning for large language models (LLMs). It is based on the observation that different layers have varying impacts on hidden states, enabling the identification of less important layers to be pruned. LLM-Streamline comprises two parts: layer pruning, which removes consecutive layers with the lowest importance based on target sparsity, and layer replacement, a novel module that trains a lightweight network to replace the pruned layers to mitigate performance loss. Additionally, a new metric called stability is proposed to address the limitations of the widely used accuracy metric in evaluating model compression. Experiments show that LLM-Streamline outperforms both previous and concurrent state-of-the-art pruning methods in terms of both performance and training efficiency."
    },
    {
        "title": "ConsisSR: Delving Deep into Consistency in Diffusion-based Image Super-Resolution",
        "link_suffix": "/forum?id=Zb2Ukmte7A",
        "link": "https://openreview.net/forum?id=Zb2Ukmte7A",
        "pdf_link": "https://openreview.net/pdf?id=Zb2Ukmte7A",
        "keywords": "Diffusion Model, Super-Resolution, Real-World Image Super-Resolution",
        "abstract": "Real-world image super-resolution (Real-ISR) aims at restoring high-quality (HQ) images from low-quality (LQ) inputs corrupted by unknown and complex degradations. In particular, pretrained text-to-image (T2I) diffusion models provide strong generative priors to reconstruct credible and intricate details. However, T2I generation focuses on semantic consistency while Real-ISR emphasizes pixel-level reconstruction, which hinders existing methods from fully exploiting diffusion priors. To address this challenge, we introduce ConsisSR to handle both semantic and pixel-level consistency. Specifically, compared to coarse-grained text prompts, we exploit the more powerful CLIP image embedding and effectively leverage both modalities through our Hybrid Prompt Adapter (HPA) for semantic guidance. Secondly, we introduce Time-aware Latent Augmentation (TALA) to mitigate the inherent gap between T2I generation and Real-ISR consistency requirements. By randomly mixing LQ and HQ latent inputs, our model not only handle timestep-specific diffusion noise but also refine the accumulated latent representations. Last but not least, our GAN-Embedding strategy employs the pretrained Real-ESRGAN model to refine the diffusion start point. This accelerates the inference process to 10 steps while preserving sampling quality, in a training-free manner. Our method demonstrates state-of-the-art performance among both full-scale and accelerated models. The code will be made publicly available."
    },
    {
        "title": "ClearSR: Latent Low-Resolution Image Embeddings Help Diffusion-Based Real-World Super Resolution Models See Clearer",
        "link_suffix": "/forum?id=FWpO8u2lim",
        "link": "https://openreview.net/forum?id=FWpO8u2lim",
        "pdf_link": "https://openreview.net/pdf?id=FWpO8u2lim",
        "keywords": "Diffusion Model, Super-Resolution, Real-World Image Super-Resolution",
        "abstract": "We present ClearSR, a new method that can better take advantage of latent low-resolution image (LR) embeddings for diffusion-based real-world image super-resolution (Real-ISR). Previous Real-ISR models mostly focus on how to activate more generative priors of text-to-image diffusion models to make the output high-resolution (HR) images look better. However, since these methods rely too much on the generative priors, the content of the output images is often inconsistent with the input LR ones. To mitigate the above issue, in this work, we explore using latent LR embeddings to constrain the control signals from ControlNet, and extract LR information at both detail and structure levels. We show that the proper use of latent LR embeddings can produce higher-quality control signals, which enables the super-resolution results to be more consistent with the LR image and leads to clearer visual results. In addition, we also show that latent LR embeddings can be used to control the inference stage, allowing for the improvement of fidelity and generation ability simultaneously. Experiments demonstrate that our model can achieve better performance across multiple metrics on several test sets and generate more consistent SR results with LR images than existing methods. Our code will be made publicly available."
    },
    {
        "title": "Precise Localization of Memories: A Fine-grained Neuron-level Knowledge Editing Technique for LLMs",
        "link_suffix": "/forum?id=5xP1HDvpXI",
        "link": "https://openreview.net/forum?id=5xP1HDvpXI",
        "pdf_link": "https://openreview.net/pdf?id=5xP1HDvpXI",
        "keywords": "Large Language Models, Transformers, Model Editing, Neural Networks, Neuron Activation",
        "abstract": "Knowledge editing aims to update outdated information in Large Language Models (LLMs). A representative line of study is the locate-then-edit methods, which typically employ causal tracing localization to identify the modules responsible for recalling factual knowledge about entities. However, we find that these methods are often sensitive only to changes in the subject entity, leaving them less effective at adapting to changes in relations. This limitation results in poor editing locality, which can lead to the persistence of irrelevant or inaccurate facts, ultimately compromising the reliability of LLMs. We believe this issue arises from the insufficient precision of knowledge localization methods. To address this, we propose a Fine-grained Neuron-level Knowledge Editing (FiNE) method that enhances editing locality without affecting overall success rates. By precisely identifying and modifying specific neurons within feed-forward networks, FiNE significantly improves knowledge localization and editing. Quantitative experiments demonstrate that FiNE efficiently achieves better overall performance compared to existing techniques, providing new insights into the localization and modification of knowledge within LLMs. The source code will be publicly released."
    },
    {
        "title": "Hydra-MDP++: Advancing End-to-End Driving via Hydra-Distillation with Expert-Guided Decision Analysis",
        "link_suffix": "/forum?id=5xfAcRHfgP",
        "link": "https://openreview.net/forum?id=5xfAcRHfgP",
        "pdf_link": "https://openreview.net/pdf?id=5xfAcRHfgP",
        "keywords": "end-to-end autonomous driving, expert guidance, knowledge distillation, open-loop metrics",
        "abstract": "We introduce HydraMDP++, a novel end-to-end autonomous driving framework that integrates rule-based and neural planners by learning from human demonstrations and distilling knowledge from rule-based experts. We propose a teacher-student knowledge distillation framework with a multi-head student decoder that integrates feedback from rule-based expert teachers. The student model achieves state-of-the-art performance on the NAVSIM benchmark with a tiny image encoder. Moreover, to address limitations in existing evaluation metrics, we expand the teacher model to include traffic light compliance, lane-keeping ability, and extended comfort. This is intended to ensure a more robust decision synthesis in driving. HydraMDP++ demonstrates robust and efficient performance across diverse driving scenarios, achieving a 91.0% drive score on NAVSIM by simply scaling the image encoder. Our work contributes to developing more reliable and adaptable autonomous driving systems that combine the strengths of rule-based and neural planning approaches."
    },
    {
        "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solver",
        "link_suffix": "/forum?id=6aHUmotXaw",
        "link": "https://openreview.net/forum?id=6aHUmotXaw",
        "pdf_link": "https://openreview.net/pdf?id=6aHUmotXaw",
        "keywords": "LLM, Reasoning",
        "abstract": "This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for Mistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct."
    },
    {
        "title": "Fast RoPE Attention: Combining the Polynomial Method and Fast Fourier Transform",
        "link_suffix": "/forum?id=AozPzKE0oc",
        "link": "https://openreview.net/forum?id=AozPzKE0oc",
        "pdf_link": "https://openreview.net/pdf?id=AozPzKE0oc",
        "keywords": "Hardness, Fine-grained Complexity, Fast transformer computation, Algorithm",
        "abstract": "The transformer architecture has been widely applied to many machine learning tasks. A main bottleneck in the time to perform transformer computations is a task called attention computation. [Alman and Song, NeurIPS 2023] have shown that in the bounded entry regime, there is an almost linear time algorithm to approximate the attention computation. They also proved that the bounded entry assumption is necessary for a fast algorithm assuming the popular Strong Exponential Time Hypothesis.A new version of transformer which uses position embeddings has recently been very successful. At a high level, position embedding enables the model to capture the correlations between tokens while taking into account their position in the sequence. Perhaps the most popular and effective version is Rotary Position Embedding (RoPE), which was proposed by [Su, Lu, Pan, Murtadha, Wen, and Liu, Neurocomputing 2024].A main downside of RoPE is that it complicates the attention computation problem, so that previous techniques for designing almost linear time algorithms no longer seem to work. In this paper, we show how to overcome this issue, and give a new algorithm to compute the RoPE attention in almost linear time in the bounded entry regime. (Again, known lower bounds imply that bounded entries are necessary.) Our new algorithm combines two techniques in a novel way: the polynomial method, which was used in prior fast attention algorithms, and the Fast Fourier Transform."
    },
    {
        "title": "Variational Potential Flow: A Novel Probabilistic Framework for Energy-Based Generative Modelling",
        "link_suffix": "/forum?id=BUQLiu4VA8",
        "link": "https://openreview.net/forum?id=BUQLiu4VA8",
        "pdf_link": "https://openreview.net/pdf?id=BUQLiu4VA8",
        "keywords": "generative models, energy-based models, variational methods, particle filtering",
        "abstract": "Energy based models (EBMs) are appealing for their generality and simplicity in data likelihood modeling, but have conventionally been difficult to train due to the unstable and time-consuming implicit MCMC sampling during contrastive divergence training. In this paper, we present a novel energy-based generative framework, Variational Potential Flow (VAPO), that entirely dispenses with implicit MCMC sampling and does not rely on complementary latent models or cooperative training. The VAPO framework aims to learn a potential energy function whose gradient (flow) guides the prior samples, so that their density evolution closely follows an approximate data likelihood homotopy. An energy loss function is then formulated to minimize the Kullback-Leibler divergence between density evolution of the flow-driven prior and the data likelihood homotopy. Images can be generated after training the potential energy, by initializing the samples from Gaussian prior and solving the SDE governing the potential flow. Experiment results show that the proposed VAPO framework is capable of generating realistic images on various image datasets. In particular, our proposed framework achieves competitive FID scores for unconditional image generation on the CIFAR-10 and CelebA datasets."
    },
    {
        "title": "Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics",
        "link_suffix": "/forum?id=wVmShpwtY0",
        "link": "https://openreview.net/forum?id=wVmShpwtY0",
        "pdf_link": "https://openreview.net/pdf?id=wVmShpwtY0",
        "keywords": "protein engineering, hamiltonian monte carlo, directed evolution, ai4science",
        "abstract": "The ability to engineer optimized protein variants has transformative potential for biotechnology and medicine. Prior sequence-based optimization methods struggle with the high-dimensional complexities due to the epistasis effect and the disregard for structural constraints. To address this, we propose HADES, a Bayesian optimization method utilizing Hamiltonian dynamics to efficiently sample from a structure-aware approximated posterior. Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas. A position discretization procedure is introduced to propose discrete protein sequences from such continuous state system. The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships between mutant neighbors, consequently learning a smoothed landscape to sample from. Extensive experiments demonstrate that our method outperforms state-of-the-art baselines in in-silico evaluations across most metrics. Remarkably, our approach offers a unique advantage by leveraging the mutual constraints between protein structure and sequence, facilitating the design of protein sequences with similar structures and optimized properties."
    },
    {
        "title": "Conversational Few-Shot Prompting: Rethinking Few-Shot Prompting for Chat Language Model",
        "link_suffix": "/forum?id=ewRkjUX4SY",
        "link": "https://openreview.net/forum?id=ewRkjUX4SY",
        "pdf_link": "https://openreview.net/pdf?id=ewRkjUX4SY",
        "keywords": "Few-shot learning; In-context learning; Large language model;",
        "abstract": "In-context learning, also referred to as few-shot learning, enables language models to adapt to tasks using a limited number of examples embedded in the prompt. Traditional approaches typically present all examples in a single prompt, which works well for pre-trained base models. However, the application of this method to instruction-tuned chat models, such as ChatGPT, remains underexplored.\nIn this paper, we introduce a novel conversational few-shot prompting technique, which structures few-shot examples as multi-turn conversation between the user and the assistant, rather than a single input prompt. This conversational framing better aligns with the interactive nature of chat models, enhancing their instruction-following abilities and generalization across tasks.\nThrough experiments on various benchmarks, we demonstrate that this approach significantly improves performance, particularly in low-shot scenarios, compared to traditional few-shot prompting. Our results suggest that this method provides a more flexible and robust way to leverage few-shot examples in instruction-tuned chat models, improving task performance without the need for additional fine-tuning, reducing prompt sensitivity, and offering potential for diverse applications."
    },
    {
        "title": "On the H\u00f6lder Stability of Multiset and Graph Neural Networks",
        "link_suffix": "/forum?id=P7KIGdgW8S",
        "link": "https://openreview.net/forum?id=P7KIGdgW8S",
        "pdf_link": "https://openreview.net/pdf?id=P7KIGdgW8S",
        "keywords": "graph neural networks, message passing neural networks, multiset neural networks, neural network stability, expressive power, WL tests",
        "abstract": "Extensive research efforts have been put into characterizing and constructing maximally separating multiset and graph neural networks. \nHowever, recent empirical evidence suggests the notion of separation itself doesn't capture several interesting phenomena. On the one hand, the quality of this separation may be very weak, to the extent that the embeddings of  \"separable\" objects might even be considered identical when using fixed finite precision. On the other hand, architectures which aren't capable of separation in theory, somehow achieve separation when taking the network to be wide enough.In this work, we address both of these issues, by proposing a novel pair-wise separation quality analysis framework which is based on an adaptation of Lipschitz and H\u00f6lder stability to parametric functions. The proposed framework, which we name H\u00f6lder in expectation, allows for separation quality analysis, without restricting the analysis to embeddings that can separate all the input space simultaneously. We prove that common sum-based models are lower-H\u00f6lder in expectation, with an exponent\n that decays rapidly with the network's depth . Our analysis leads to adversarial examples of  graphs which can be separated by three 1-WL iterations, but cannot be separated in practice by standard maximally powerful Message Passing Neural Networks (MPNNs). To remedy this, we propose two novel MPNNs with improved separation quality, one of which is lower Lipschitz in expectation. We show these MPNNs can easily classify our adversarial examples, and compare favorably with standard MPNNs on standard graph learning tasks."
    },
    {
        "title": "EVGAP: Egocentric-Exocentric Video Groups Alignment Pre-training",
        "link_suffix": "/forum?id=F0K0zxi62U",
        "link": "https://openreview.net/forum?id=F0K0zxi62U",
        "pdf_link": "https://openreview.net/pdf?id=F0K0zxi62U",
        "keywords": "multiview video, view-invariant pretraining, view alignment, ego-exo pair alignment",
        "abstract": "Aligning egocentric and exocentric videos facilitates the learning of view-invariant features, which significantly contributes to video understanding. While previous approaches have primarily focused on aligning individual ego-exo video pairs, our method extends this concept by aligning groups of synchronized egocentric and exocentric videos.  This strategy enables the model to capture more comprehensive cross-view relationships across densely captured viewpoints, enhancing its capacity for robust multi-view understanding.\nTherefore, we develop a pipeline based on contrastive learning for \\textbf{E}gocentric-exocentric \\textbf{V}ideo \\textbf{G}roups \\textbf{A}lignment \\textbf{P}re-training (EVGAP). \nOur method introduces several key innovations: 1) a novel video pre-training paradigm that extends alignment from ego-exo video pairs to ego-exo video group alignments; 2) an innovative two-step training process that leverages the abundant ego-exo video pair data to support the learning of ego-exo video group alignments, transitioning from sparse to dense viewpoints; and 3) the application of auxiliary losses to progressively align videos from different perspectives.\nExtensive ablations illustrate the effectiveness of our approach in single-view and multi-view downstream tasks. We also find that our approach facilitates the tasks inluding novel views. The codes will be available upon acceptance."
    },
    {
        "title": "E3former: An Adaptive Energy-Aware Elastic Equivariant Transformer Model For Protein Representation Learning",
        "link_suffix": "/forum?id=QKywN4BbqA",
        "link": "https://openreview.net/forum?id=QKywN4BbqA",
        "pdf_link": "https://openreview.net/pdf?id=QKywN4BbqA",
        "keywords": "protein representation learning, geometric deep learning, equivariant neural networks, proteins",
        "abstract": "Structure-informed protein representation learning is essential for effective protein function annotation and \\textit{de novo} design. However, the presence of inherent noise in both crystal and AlphaFold-predicted structures poses significant challenges for existing methods in learning robust protein representations. To address these issues, we propose a novel equivariant Transformer-State Space Model(SSM) hybrid framework, termed $E^3$former, designed for efficient protein representation. Our approach leverages energy function-based receptive fields to construct proximity graphs and incorporates an equivariant high-tensor-elastic selective SSM within the transformer architecture. These components enable the model to adapt to complex atom interactions and extract geometric features with higher signal-to-noise ratios. Empirical results demonstrate that our model outperforms existing methods in structure-intensive tasks, such as inverse folding and binding site prediction, particularly when using predicted structures, owing to its enhanced tolerance to data deviation and noise. Our approach offers a novel perspective for conducting biological function research and drug discovery using noisy protein structure data. Our code is available onhttps://anonymous.4open.science/r/E3former-207E"
    },
    {
        "title": "LLaMA Decoder As Vision Transformer",
        "link_suffix": "/forum?id=GraOHDxFjl",
        "link": "https://openreview.net/forum?id=GraOHDxFjl",
        "pdf_link": "https://openreview.net/pdf?id=GraOHDxFjl",
        "keywords": "Decoder-only; Vision Transformer; LLaMA",
        "abstract": "Using the same architecture for text and image is important for AI standardization. Recent multimodal models use a decoder-only Transformer to generate text and an encoder-only Transformer to extract image features. Can images use exactly the same language architecture? To answer this question, we aim at a LLaMa decoder as vision Transformer (ViT) classifier in this paper. Specifically, we start\nour trajectory by \u201cLLaMAfy\u201d a standard ViT step-by-step, i.e., feed-forward net, normalization layer, causal self-attention and positional embedding, and point out a key issue\u2014attention collapse\u2014that result in the failure to the network training. Motivated by this observation, we propose post-sequence class token, enabling causal self-attention to efficiently capture the entire image\u2019s information. To improve model optimization behavior and enhance performance, we then introduce a soft mask strategy to gradually transform the attention from bi-directional to causal mode. The tailored model, dubbed as image LLaMA (iLLaMA), maintains high consistency with LLaMA architecture, while matching up well against ViT, achieving 75.1% ImageNet top-1 accuracy with only 5.7M parameters. Scaling the model to \u223c310M and pre-training on ImageNet-21K further enhances the accuracy to 86.0%. Its causal self-attention boosts computational efficiency and learns complex representation by elevating attention map ranks. Extensive experiments demonstrate iLLaMA\u2019s reliable properties: shape-texture bias, calibration, quantization compatibility, ADE20K segmentation and CIFAR transfer learning. We hope our study can kindle fresh views to visual architectures in the era of LLMs and contributes to standardized AI models."
    },
    {
        "title": "Motion-Catcher: Upholding Motion and Content Consistency in Multi-Sequence Video Generation",
        "link_suffix": "/forum?id=6hsnpDXgHC",
        "link": "https://openreview.net/forum?id=6hsnpDXgHC",
        "pdf_link": "https://openreview.net/pdf?id=6hsnpDXgHC",
        "keywords": "Diffusion models, video generation",
        "abstract": "Recent developments in diffusion models have significantly advanced the field of video generation. However, technical challenges still exist in terms of spatiotemporal continuity and content consistency in long video generation. In this paper, we propose Motion-Catcher, a diffusion model-based method for multi-sequence video generation that aims to address the issues of motion inconsistency and content degradation. By incorporating a motion capture module, the model leverages optical flow information from video sequences to capture both local and global movements, enhancing the motion consistency of the videos. Furthermore, a dynamic content prior module is proposed to monitor regions prone to degradation, which helps maintain content consistency throughout the generated videos. Extensive experiments have validated that the proposed Motion-Catcher can generate videos with higher quality in terms of motion continuity and consistency. The source code and additional experimental results are available athttps://github.com/YuukiGong/Motion-Catcher."
    },
    {
        "title": "DM-Codec: Distilling Multimodal Representations for Speech Tokenization",
        "link_suffix": "/forum?id=UFwefiypla",
        "link": "https://openreview.net/forum?id=UFwefiypla",
        "pdf_link": "https://openreview.net/pdf?id=UFwefiypla",
        "keywords": "speech tokenizer, speech codec, multimodal representation learning, representation learning, language model",
        "abstract": "Recent advancements in speech-language models have yielded significant improvements in speech tokenization and synthesis. However, effectively mapping the complex, multidimensional attributes of speech into discrete tokens remains challenging. This process demands acoustic, semantic, and contextual information for precise speech representations. Existing speech representations generally fall into two categories: acoustic tokens from audio codecs and semantic tokens from speech self-supervised learning models. Although recent efforts have unified acoustic and semantic tokens for improved performance, they overlook the crucial role of contextual representation in comprehensive speech modeling. Our empirical investigations reveal that the absence of contextual representations results in elevated Word Error Rate (WER) and Word Information Lost (WIL) scores in speech transcriptions. To address these limitations, we propose two novel distillation approaches: (1) a language model (LM)-guided distillation method that incorporates contextual information, and (2) a combined LM and self-supervised speech model (SM)-guided distillation technique that effectively distills multimodal representations (acoustic, semantic, and contextual) into a comprehensive speech tokenizer, termed DM-Codec. The DM-Codec architecture adopts a streamlined encoder-decoder framework with a Residual Vector Quantizer (RVQ) and incorporates the LM and SM during the training process. Experiments show DM-Codec significantly outperforms state-of-the-art speech tokenization models, reducing WER by up to 13.46%, WIL by 9.82%, and improving speech quality by 5.84% and intelligibility by 1.85% on the LibriSpeech benchmark dataset."
    }
]