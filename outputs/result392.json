[
    {
        "title": "Exploring Model Kinship for Merging Large Language Models",
        "link_suffix": "/forum?id=MR6RZQKMby",
        "link": "https://openreview.net/forum?id=MR6RZQKMby",
        "pdf_link": "https://openreview.net/pdf?id=MR6RZQKMby",
        "keywords": "model merging, model kinship, large language models",
        "abstract": "Model merging has become one of the key technologies for enhancing the capabilities and efficiency of Large Language Models (LLMs). However, our understanding of the expected performance gains and principles when merging any two models remains limited. In this work, we introduce model kinship, the degree of similarity or relatedness between LLMs, analogous to biological evolution. With comprehensive empirical analysis, we find that there is a certain relationship between model kinship and the performance gains after model merging, which can help guide our selection of candidate models. Inspired by this, we propose a new model merging strategy: Top-$k$ Greedy Merging with Model Kinship, which can yield better performance on benchmark datasets. Specifically, we discover that using model kinship as a criterion can assist us in continuously performing model merging, alleviating the degradation (local optima) in model evolution, whereas model kinship can serve as a guide to escape these traps. Furthermore, we observe that model kinship can serve as an early stopping criterion, enhancing the efficiency of  model evolution. We hope that the proposed  model kinship can provide guidance and insights for the future merging and evolution of LLMs."
    },
    {
        "title": "WIN: Variable-View Implicit LIDAR Upsampling Network",
        "link_suffix": "/forum?id=ON0JjUtw9B",
        "link": "https://openreview.net/forum?id=ON0JjUtw9B",
        "pdf_link": "https://openreview.net/pdf?id=ON0JjUtw9B",
        "keywords": "LiDAR Upsampling, Deep Learning, Autonomous vehicle system",
        "abstract": "LiDAR upsampling aims to increase the resolution of sparse point sets obtained from low-cost sensors, providing better performance for various downstream tasks. Most existing methods transform LiDAR points into range view and design complex neighborhood point interpolation strategies to improve the resolution of point clouds. However, they overlook that the range image representation is insufficient to describe complex local geometric relationships, which limits the geometric accuracy of upsampled points.\nTo address this issue, we propose WIN, a Variable-View Implicit Network. \nFirst, we decouple the range image into two novel virtual view representations to compensate for the missing geometric information during range view-based interpolation. Secondly, to fuse the interpolation results of different views, we model the fusion process as a probability distribution problem instead of a simple binary classification task. We introduce a contrast selection module, which captures the feature differences between two representations and outputs the view confidence score for each upsampled point. The underlying idea is that the complementarity of the information is proportional to the feature difference between the two views. Motivated by this insight, we design a loss function based on probabilistic modeling to supervise the results of the selection module.\nAs a result, compared with the current state-of-the-art (SOTA) method ILN, WIN introduces a small number of parameters (+0.4M) but achieves a +4.5%  increase in the MAE metric on the CARLA dataset. Furthermore, our method outperforms all existing methods in a downstream task (Depth Completion). The pre-trained model and code will be released upon acceptance."
    },
    {
        "title": "Neural Approximate Mirror Maps for Constrained Diffusion Models",
        "link_suffix": "/forum?id=vgZDcUetWS",
        "link": "https://openreview.net/forum?id=vgZDcUetWS",
        "pdf_link": "https://openreview.net/pdf?id=vgZDcUetWS",
        "keywords": "generative models, diffusion models, mirror maps, constrained generation, inverse problems",
        "abstract": "Diffusion models excel at creating visually-convincing images, but they often struggle to meet subtle constraints inherent in the training data. Such constraints could be physics-based (e.g., satisfying a PDE), geometric (e.g., respecting symmetry), or semantic (e.g., including a particular number of objects). When the training data all satisfy a certain constraint, enforcing this constraint on a diffusion model makes it more reliable for generating valid synthetic data and solving constrained inverse problems. However, existing methods for constrained diffusion models are restricted in the constraints they can handle. For instance, recent work proposed to learn mirror diffusion models (MDMs), but analytical mirror maps only exist for convex constraints and can be challenging to derive. We proposeneural approximate mirror maps(NAMMs) for general, possibly non-convex constraints. Our approach only requires a differentiable distance function from the constraint set. We learn an approximate mirror map that transforms data into an unconstrained space and a corresponding approximate inverse that maps data back to the constraint set. A generative model, such as an MDM, can then be trained in the learned mirror space and its samples restored to the constraint set by the inverse map. We validate our approach on a variety of constraints, showing that compared to an unconstrained diffusion model, a NAMM-based MDM substantially improves constraint satisfaction. We also demonstrate how existing diffusion-based inverse-problem solvers can be easily applied in the learned mirror space to solve constrained inverse problems."
    },
    {
        "title": "Chain-of-region: Visual Language Models Need  Details for Diagram Analysis",
        "link_suffix": "/forum?id=M6fYrICcQs",
        "link": "https://openreview.net/forum?id=M6fYrICcQs",
        "pdf_link": "https://openreview.net/pdf?id=M6fYrICcQs",
        "keywords": "Multi-modality; Visual Language Model; Computer Vision",
        "abstract": "Visual Language Models (VLMs) like GPT-4V have broadened the scope of LLM applications, yet they face significant challenges in accurately processing visual details, particularly in scientific diagrams. \nThis paper explores the necessity of meticulous visual detail collection and region decomposition for enhancing the performance of VLMs in scientific diagram analysis. We propose a novel approach that combines traditional computer vision techniques with VLMs to systematically decompose diagrams into discernible visual elements and aggregate essential metadata. Our method employs techniques in OpenCV library to identify and label regions, followed by a refinement process using shape detection and region merging algorithms, which are particularly suited to the structured nature of scientific diagrams. This strategy not only improves the granularity and accuracy of visual information processing but also extends the capabilities of VLMs beyond their current limitations. We validate our approach through a series of experiments that demonstrate enhanced performance in diagram analysis tasks, setting a new standard for integrating visual and language processing in a multimodal context."
    },
    {
        "title": "SEAT: Sparsified Enhancements for Attention Mechanisms in Time Series Transformers",
        "link_suffix": "/forum?id=5r6zvadRUD",
        "link": "https://openreview.net/forum?id=5r6zvadRUD",
        "pdf_link": "https://openreview.net/pdf?id=5r6zvadRUD",
        "keywords": "Time Series, Frequency Analysis, Deep Learning",
        "abstract": "Transformer models excel in time series tasks due to their attention mechanisms. However, they often suffer from \"block-like\" attention patterns caused by high feature correlation, leading to feature confusion and reduced performance. In this study, we mathematically prove and quantify this limitation, demonstrating how it affects the sparsity of the attention matrix and hinders effective feature representation. To overcome this issue, we propose a novel, model-agnostic, and plug-and-play method called SEAT (Sparsification-Enhanced Attention Transformer) that leverages frequency domain sparsification. By transforming time series data into the frequency domain, our method induces inherent sparsity, reduces feature similarity, and mitigates block-like attention, allowing the attention mechanism to focus more precisely on relevant features. Experiments on benchmark datasets demonstrate that our approach significantly enhances the accuracy and robustness of Transformer models while maintaining computational efficiency. This provides a mathematically grounded solution to inherent flaws in attention mechanisms, offering a versatile and effective approach for advancing time series analysis."
    },
    {
        "title": "Are Probabilistic Robust Accuracy Bounded",
        "link_suffix": "/forum?id=fVgUXaesSS",
        "link": "https://openreview.net/forum?id=fVgUXaesSS",
        "pdf_link": "https://openreview.net/pdf?id=fVgUXaesSS",
        "keywords": "bayes error, Probabilistic Robustness",
        "abstract": "Adversarial samples pose a security threat to many critical systems built on neural networks. It has recently been proven that achieving deterministic robustness (i.e., complete elimination of adversarial samples) always comes at an unbearable cost to accuracy. As a result, probabilistic robustness (where the probability of retaining the same label within a vicinity is at least $1 - \\kappa$) has been proposed as a promising compromise. However, existing training methods for probabilistic robustness still experience non-trivial accuracy loss. It remains an open question whether an upper limit on accuracy exists when optimizing for probabilistic robustness, and whether there is a specific relationship between $\\kappa$ and this potential bound. This work studies these problems from a Bayes error perspective. We find that while Bayes uncertainty does affect probabilistic robustness, its impact is smaller than that on deterministic robustness. This reduced Bayes uncertainty allows a higher upper bound on probabilistic robust accuracy than that on deterministic robust accuracy. Further, we show that voting within the vicinity always improves probabilistic robust accuracy and the upper bound of probabilistic robust accuracy monotonically increases as $\\kappa$ grows. Our empirical findings also align with our results. This study thus presents a theoretical argument supporting probabilistic robustness as the appropriate target for achieving neural network robustness."
    },
    {
        "title": "pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation",
        "link_suffix": "/forum?id=scozdyKzET",
        "link": "https://openreview.net/forum?id=scozdyKzET",
        "pdf_link": "https://openreview.net/pdf?id=scozdyKzET",
        "keywords": "Visual Adaptation, Visual Representation Learning",
        "abstract": "Parameter-efficient fine-tuning has demonstrated promising results across various visual adaptation tasks, such as classification and segmentation. Typically, prompt tuning techniques have harnessed knowledge from a single pre-trained model, whether from a general or a specialized medical domain. However, this approach typically overlooks the potential synergies that could arise from integrating diverse domain knowledge within the same tuning process. In this work, we propose a novel Mixture-of-Experts prompt tuning method called pMoE, which leverages the strengths of multiple expert domains through expert-specialized prompt tokens and the learnable dispatcher, effectively combining their expertise in a unified model framework. Our pMoE introduces expert-specific prompt tokens and utilizes a dynamic token dispatching mechanism at various prompt layers to optimize the contribution of each domain expert during the adaptation phase. By incorporating both domain knowledge from diverse experts, the proposed pMoE significantly enhances the model's versatility and applicability to a broad spectrum of tasks. We conduct extensive experiments across 47 adaptation tasks, including both classification and segmentation in general and medical domains. The results demonstrate that our pMoE not only achieves superior performance with a large margin of improvements but also offers an optimal trade-off between computational efficiency and adaptation effectiveness compared to existing methods."
    },
    {
        "title": "TIM: Interpretable Modelling of Complex Temporal Interactions in Multivariate Networks",
        "link_suffix": "/forum?id=MwIbzfu93a",
        "link": "https://openreview.net/forum?id=MwIbzfu93a",
        "pdf_link": "https://openreview.net/pdf?id=MwIbzfu93a",
        "keywords": "Time Series, Deep Learning, Decomposition",
        "abstract": "Multivariate time series forecasting is crucial across various fields and essential for addressing numerous real-world challenges. However, existing forecasting methods have significant limitations: while Transformer models are effective, they are constrained by high computational costs and declining performance in long-term forecasting; MLP models struggle to capture complex multivariate interactions. These issues hinder the models' ability to accurately decompose seasonality and trends. To tackle these problems, we propose a new method called TIM. Through a cross-layer architecture, TIM decomposes time series predictions into temporal features, multivariate interaction features, and residual components. Our all-MLP model integrates global features with complex multivariate dynamics. By introducing a linear self-attention mechanism across variables and time steps, TIM enhances the learning of feature interactions and accurately captures temporal transitions between domains. This innovative design leverages linear attention mechanisms and cross-layer architecture to more effectively model temporal features and multivariate interactions. It surpasses traditional Transformer-based methods by improving predictive accuracy while maintaining linear computational complexity. Experimental results demonstrate that TIM outperforms existing state-of-the-art methods while ensuring computational efficiency."
    },
    {
        "title": "Discrimination-free Insurance Pricing with Privatized Sensitive Attributes",
        "link_suffix": "/forum?id=Q7uE3M5aMD",
        "link": "https://openreview.net/forum?id=Q7uE3M5aMD",
        "pdf_link": "https://openreview.net/pdf?id=Q7uE3M5aMD",
        "keywords": "Fairness, Discrimination-free Insurance Pricing, Insurance, Privatized Attributes, Privacy",
        "abstract": "Fairness has emerged as a critical consideration in the landscape of machine learning algorithms, particularly as AI continues to transform decision-making across societal domains. To ensure that these algorithms are free from bias and do not discriminate against individuals based on sensitive attributes such as gender and race, the field of algorithmic bias has introduced various fairness concepts, including demographic parity and equalized odds, along with methodologies to achieve these notions in different contexts. Despite the rapid advancement in this field, not all sectors have embraced these fairness principles to the same extent. One specific sector that merits attention in this regard is insurance. Within the realm of insurance pricing, fairness is defined through a distinct and specialized framework. Consequently, achieving fairness according to established notions does not automatically ensure fair pricing in insurance. In particular, regulators are increasingly emphasizing transparency in pricing algorithms and imposing constraints\non insurance companies on the collection and utilization of sensitive consumer attributes. These factors present additional challenges in the implementation of fairness in pricing algorithms. To address these complexities and comply with regulatory demands, we propose an efficient method for constructing fair models that align with the specific fairness criteria unique to the insurance pricing domain. Notably, our approach only relies on privatized sensitive attributes and offers statistical guarantees. Further, it does not require insurers to have direct access to sensitive attributes, and it can be tailored to accommodate varying levels of transparency as required. This methodology seeks to meet the growing demands for privacy and transparency from regulators while ensuring fairness in insurance pricing practices."
    },
    {
        "title": "Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement",
        "link_suffix": "/forum?id=UHPnqSTBPO",
        "link": "https://openreview.net/forum?id=UHPnqSTBPO",
        "pdf_link": "https://openreview.net/pdf?id=UHPnqSTBPO",
        "keywords": "Large Language Model, LLM, LLM Judge, Evaluation, Alignment",
        "abstract": "We present a principled approach to provide LLM-based evaluation with a rigorous guarantee of human agreement. We first propose that a reliable evaluation method should not uncritically rely on model preferences for pairwise evaluation, but rather assess the confidence of judge models and selectively decide when to trust its judgement. We then show that under thisselective evaluationframework, human agreement can be provably guaranteed---such that the model evaluation aligns with that of humans to a user-specified agreement level. As part of our framework, we also introduceSimulated Annotators, a novel confidence estimation method that significantly improves judge calibration and thus enables high coverage of evaluated instances. Finally, we proposeCascaded Selective Evaluation, where we use cheaper models as initial judges and escalate to stronger models only when necessary---again, while still providing a provable guarantee of human agreement. Experimental results show that Cascaded Selective Evaluation guarantees strong alignment with humans, far beyond what LLM judges could achieve without selective evaluation. For example, on a subset of Chatbot Arena where GPT-4 almost never achieves 80% human agreement, our method, even while employing substantially cost-effective models such as Mistral-7B,guaranteesover 80% human agreement with almost 80% test coverage."
    },
    {
        "title": "MMAD: The First-Ever Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection",
        "link_suffix": "/forum?id=JDiER86r8v",
        "link": "https://openreview.net/forum?id=JDiER86r8v",
        "pdf_link": "https://openreview.net/pdf?id=JDiER86r8v",
        "keywords": "Anomaly Detection, Multimodal Large Language Model, Industrial Inspection",
        "abstract": "In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to introduce new paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We defined seven key capabilities of MLLMs in industrial anomaly detection and designed a novel pipeline to generate 39,672 questions for 8,366 industrial images. With MMAD, we have conducted a comprehensive, quantitative evaluation of various state-of-the-art MLLMs. The commercial models performed the best, with the average accuracy of GPT-4o models reaching 74.9%. However, this result falls far short of industrial requirements. Our analysis reveals that current MLLMs still have significant room for improvement in answering questions related to industrial anomalies and defects. We further explore two training-free performance enhancement strategies to help models improve in industrial scenarios, highlighting their promising potential for future research. \nThe code and data are available athttps://anonymous.4open.science/r/MMAD/."
    },
    {
        "title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone\u2019s Potential with Masked Autoregressive Pretraining",
        "link_suffix": "/forum?id=hBVywPrw6d",
        "link": "https://openreview.net/forum?id=hBVywPrw6d",
        "pdf_link": "https://openreview.net/pdf?id=hBVywPrw6d",
        "keywords": "Hybrid Vision Backbone, Masked Autoregressive, Self-supervised Learning",
        "abstract": "Mamba has achieved significant advantages in long-context modeling and autoregressive tasks, but its scalability with large parameters remains a major limitation in vision applications. pretraining is a widely used strategy to enhance backbone model performance. Although the success of Masked Autoencoder in Transformer pretraining is well recognized, it does not significantly improve Mamba's visual learning performance. We found that using the correct autoregressive pretraining can significantly boost the performance of the Mamba architecture. Based on this analysis, we propose Masked Autoregressive Pretraining(MAP) to pretrain a hybrid Mamba-Transformer vision backbone network. This strategy combines the strengths of both MAE and Autoregressive pretraining, improving the performance of Mamba and Transformer modules within a unified paradigm. Experimental results show that both the pure Mamba architecture and the hybrid Mamba-Transformer vision backbone network pretrained with MAP significantly outperform other pretraining strategies, achieving state-of-the-art performance. We validate the effectiveness of the method on both 2D and 3D datasets and provide detailed ablation studies to support the design choices for each component."
    },
    {
        "title": "TSC-Net: Predict Pedestrian Trajectory by Trajectory-Scene-Cell Classification",
        "link_suffix": "/forum?id=Xmh5gdMfRJ",
        "link": "https://openreview.net/forum?id=Xmh5gdMfRJ",
        "pdf_link": "https://openreview.net/pdf?id=Xmh5gdMfRJ",
        "keywords": "Trajectory-Scene-Cell, Trajectory Prediction, Attention",
        "abstract": "To predict future trajectories of pedestrians, scene is as important as the history trajectory since i) scene reflects the position of possible goals of the pedestrian ii) trajectories are affected by the semantic information of the scene. It requires the model to capture scene information and learn the relation between scenes and trajectories. However, existing methods either apply Convolutional Neural Networks (CNNs) to summarize the scene to a feature vector, which raises the feature misalignment issue, or convert trajectory to heatmaps to align with the scene map, which ignores the interactions among different pedestrians. In this work, we introduce the trajectory-scene-cell feature to represent both trajectories and scenes in one feature space. By decoupling the trajectory in temporal domain and the scene in spatial domain, trajectory feature and scene feature are re-organized in different types of cell feature, which well aligns trajectory and scene, and allows the framework to model both human-human and human-scene interactions. Moreover, the Trajectory-Scene-Cell Network (TSC-Net) with new trajectory prediction manner is proposed, where both goal and intermediate positions of the trajectory are predict by cell classification and offset regression. Comparative experiments show that TSC-Net achieves the SOTA performance on several datasets with most of the metrics. Especially for the goal estimation, TSC-Net is demonstrated better on predicting goals for trajectories with irregular speed."
    },
    {
        "title": "RRM:  Robust Reward Model Training Mitigates Reward Hacking",
        "link_suffix": "/forum?id=88AS5MQnmC",
        "link": "https://openreview.net/forum?id=88AS5MQnmC",
        "pdf_link": "https://openreview.net/pdf?id=88AS5MQnmC",
        "keywords": "Reward model, RLHF, Alignment",
        "abstract": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human preferences. However, traditional RM training, which relies on response pairs tied to specific prompts, struggles to disentangle prompt-driven preferences from prompt-independent artifacts, such as response length and format. In this work, we expose a fundamental limitation of current RM training methods, where RMs fail to effectively distinguish between contextual signals and irrelevant artifacts when determining preferences. To address this, we introduce a causal framework that learns preferences independent of these artifacts and propose a novel data augmentation technique designed to eliminate them.  Extensive experiments show that our approach successfully filters out undesirable artifacts, yielding a more robust reward model (RRM). Our RRM improves the performance of a pairwise reward model trained on Gemma-2-9b-it, on Reward-Bench, increasing accuracy from 80.61% to 84.15%. Additionally, we train two DPO policies using both the RM and RRM, demonstrating that the RRM significantly enhances DPO-aligned policies, improving MT-Bench scores from 7.27 to 8.31 and length-controlled win-rates in AlpacaEval-2 from 33.46% to 52.49%."
    },
    {
        "title": "Graffe: Graph Representation Learning Enabled via Diffusion Probabilistic Models",
        "link_suffix": "/forum?id=oa5UeyUVMm",
        "link": "https://openreview.net/forum?id=oa5UeyUVMm",
        "pdf_link": "https://openreview.net/pdf?id=oa5UeyUVMm",
        "keywords": "graph representation learning, diffusion models, unsupervised learning",
        "abstract": "Diffusion probabilistic models (DPMs), widely recognized for their potential to generate high-quality samples, tend to go unnoticed in representation learning. While recent progress has highlighted their potential for capturing visual semantics, adapting DPMs to graph representation learning remains in its infancy. In this paper, we introduceGraffe, a self-supervised diffusion model proposed for graph representation learning. It features a graph encoder that distills a source graph into a compact representation, which, in turn, serves as the condition to guide the denoising process of the diffusion decoder. To evaluate the effectiveness of our model, we first explore the theoretical foundations of applying diffusion models to representation learning, proving that the denoising objective implicitly maximizes the conditional mutual information between data and its representation. Specifically, we prove that the negative logarithm of denoising score matching loss is a tractable lower bound for the conditional mutual information. Empirically, Graffe delivers competitive results under the linear probing setting on node and graph classification, achieving state-of-the-art performance on 9 of the 11 real-world datasets. These findings indicate that powerful generative models, especially diffusion models, serve as an effective tool for graph representation learning."
    },
    {
        "title": "Vertical Federated Learning with Missing Features During Training and Inference",
        "link_suffix": "/forum?id=OXi1FmHGzz",
        "link": "https://openreview.net/forum?id=OXi1FmHGzz",
        "pdf_link": "https://openreview.net/pdf?id=OXi1FmHGzz",
        "keywords": "Vertical Federated Learning, missing features",
        "abstract": "Vertical federated learning trains models from feature-partitioned datasets across multiple clients, who collaborate without sharing their local data. Standard approaches assume that all feature partitions are available during both training and inference. Yet, in practice, this assumption rarely holds, as for many samples only a subset of the clients observe their partition. However, not utilizing incomplete samples during training harms generalization, and not supporting them during inference limits the utility of the model. Moreover, if after training any client leaves the federation, its partition becomes unavailable, rendering the learned model unusable. Missing feature blocks are therefore a key challenge limiting the applicability of vertical federated learning in real-world scenarios. To address this, we propose \\texttt{LASER-VFL}, a vertical federated learning method for efficient training and inference of split neural network-based models that is capable of handling arbitrary sets of partitions. Our approach is simple yet effective, relying on the strategic sharing of model parameters and on task-sampling to train a family of predictors. We show that \\texttt{LASER-VFL} achieves a $\\mathcal{O}({1}/{\\sqrt{T}})$ convergence rate for nonconvex objectives in general, $\\mathcal{O}({1}/{T})$ for sufficiently large batch sizes, and, under the Polyak-{\\L}ojasiewicz inequality, linear convergence. Numerical experiments show improved performance of \\texttt{LASER-VFL} over the baselines. Remarkably, this is the case even in the absence of missing features. For example, for CIFAR-100, we see an improvement in accuracy of $21.4$% when each of four feature blocks is observed with a probability of 0.5 and of $12.2$%  when all features are observed."
    },
    {
        "title": "Hybrid Spatial Representations for Species Distribution Modeling",
        "link_suffix": "/forum?id=dieIcwiXCL",
        "link": "https://openreview.net/forum?id=dieIcwiXCL",
        "pdf_link": "https://openreview.net/pdf?id=dieIcwiXCL",
        "keywords": "Neural Implicit Representations, Explicit Representations, Multiresolution Hashgrids, Geographic Embedding, AI for Science",
        "abstract": "We address an important problem in ecology called Species Distribution Modeling (SDM), whose goal is to predict whether a species exists at a certain position on Earth. In particular, we tackle a challenging version of this task, where we learn from presence-only data in a community-sourced dataset, model a large number of species simultaneously, and do not use any additional environmental information. Previous work has used neural implicit representations to construct models that achieve promising results. However, implicit representations often generate predictions of limited spatial precision. We attribute this limitation to their inherently global formulation and inability to effectively capture local feature variations. This issue is especially pronounced with presence-only data and a large number of species. To address this, we propose a hybrid embedding scheme that combines both implicit and explicit embeddings. Specifically, the explicit embedding is implemented with a multiresolution hashgrid, enabling our models to better capture local information. Experiments demonstrate that our results exceed other works by a large margin on various standard benchmarks, and that the hybrid representation is better than both purely implicit and explicit ones. Qualitative visualizations and comprehensive ablation studies reveal that our hybrid representation successfully addresses the two main challenges. Our code is open-sourced athttps://anonymous.4open.science/r/HSR-SDM-7360."
    },
    {
        "title": "Constant Rate Schedule: Constant-Rate Distributional Change for Efficient Training and Sampling in Diffusion Models",
        "link_suffix": "/forum?id=fvNn2rgj4Y",
        "link": "https://openreview.net/forum?id=fvNn2rgj4Y",
        "pdf_link": "https://openreview.net/pdf?id=fvNn2rgj4Y",
        "keywords": "diffusion model, noise schedule",
        "abstract": "We propose a noise schedule that ensures a constant rate of change in the probability distribution of diffused data throughout the diffusion process.\nTo obtain this noise schedule, we measure the rate of change in the probability distribution of the forward process and use it to determine the noise schedule before training diffusion models.\nThe functional form of the noise schedule is automatically determined and tailored to each dataset and type of diffusion model.\nWe evaluate the effectiveness of our noise schedule on unconditional and class-conditional image generation tasks using the LSUN (bedroom/church/cat/horse), ImageNet, and FFHQ datasets.\nThrough extensive experiments, we confirmed that our noise schedule broadly improves the performance of the diffusion models regardless of the dataset, sampler, number of function evaluations, or type of diffusion model."
    },
    {
        "title": "NEXTLOCLLM: NEXT LOCATION PREDICTION USING LLMS",
        "link_suffix": "/forum?id=uiBLOcyTIA",
        "link": "https://openreview.net/forum?id=uiBLOcyTIA",
        "pdf_link": "https://openreview.net/pdf?id=uiBLOcyTIA",
        "keywords": "next location prediction, large language model, zero-shot",
        "abstract": "Next location prediction is a critical task in human mobility analysis and serves as a foundation for various downstream applications. Existing methods typically rely on discrete IDs to represent locations, which inherently overlook spatial relationships and cannot generalize across cities. In this paper, we propose NextLocLLM, which leverages the advantages of large language models (LLMs) in processing natural language descriptions and their strong generalization capabilities for next location prediction. Specifically, instead of using IDs, NextLocLLM encodes locations based on continuous spatial coordinates to better model spatial relationships. These coordinates are further normalized to enable robust cross-city generalization. Another highlight of NextlocLLM is its LLM-enhanced POI embeddings. It utilizes LLMs\u2019 ability to encode each POI category\u2019s natural language description into embeddings. These embeddings are then integrated via nonlinear projections to form this LLM-enhanced POI embeddings, effectively capturing locations\u2019 functional attributes. Furthermore, task and data prompt prefix, together with trajectory embeddings, are incorporated as input for partly-frozen LLM backbone. NextLocLLM further introduces prediction retrieval module to ensure structural consistency in prediction. Experiments show that NextLocLLM outperforms existing models in next location prediction, excelling in both supervised and zero-shot settings."
    },
    {
        "title": "High-Quality Joint Image and Video Compression with Causal VAE",
        "link_suffix": "/forum?id=aRD1NqcXTC",
        "link": "https://openreview.net/forum?id=aRD1NqcXTC",
        "pdf_link": "https://openreview.net/pdf?id=aRD1NqcXTC",
        "keywords": "Autoencoding, Generative Modelling, Causal Video VAE, FILM",
        "abstract": "Generative modeling has seen significant advancements in image and video synthesis. However, the curse of dimensionality remains a significant obstacle, especially for video generation, given its inherently complex and high-dimensional nature. Many existing works rely on low-dimensional latent spaces from pretrained image autoencoders. However, this approach overlooks temporal redundancy in videos and often leads to temporally incoherent decoding. To address this issue, we propose a video compression network that reduces the dimensionality of visual data both spatially and temporally. Our model, based on a variational autoencoder, employs causal 3D convolution to handle images and videos jointly. The key contributions of our work include a scale-agnostic encoder for preserving video fidelity, a novel spatio-temporal down/upsampling block for robust long-sequence modeling, and a flow regularization loss for accurate motion decoding. \nOur approach outperforms competitors in video quality and compression rates across various datasets. Experimental analyses also highlight its potential as a robust autoencoder for video generation training. Code and models will be open-sourced."
    },
    {
        "title": "Evaluating and Explaining the Severity of Distribution Shifts: Illustration with Tabular Text Classification",
        "link_suffix": "/forum?id=JEjVuVxbkf",
        "link": "https://openreview.net/forum?id=JEjVuVxbkf",
        "pdf_link": "https://openreview.net/pdf?id=JEjVuVxbkf",
        "keywords": "unsupervised performance estimation, error detection, distribution shifts, explanation method, multimodal classification",
        "abstract": "After deploying a machine learning model, distribution shifts may emerge in real-world data. When dealing with unlabeled data, it can be challenging to accurately assess the impact of these drifts on the model's performance, for any type and intensity of shift. In that case, decisions such as updating the model for every benign shift would not be cost-efficient. In this paper, we introduce the Error Classifier, an error assessment method that addresses two tasks: unsupervised performance estimation and error detection on out-of-distribution data. The Error Classifier computes the probability that the model will fail based on detected fault patterns. Further, we employ a sampling-based approximation of Shapley values, with the Error Classifier as value function, in order to explain why a shift is predicted as severe, in terms of feature values. As explanation methods can sometimes disagree, we suggest evaluating the consistency of explanations produced by our technique and different ones. We focus on classification and illustrate the relevance of our method in a bimodal context, on tabular datasets with text fields. We measure our method against a selection of 15 baselines from various domains, on 7 datasets with a variety of shifts, and 2 multimodal fusion strategies for the classification models. Lastly, we show the usefulness of our explanation algorithm on instances affected by various types of shifts."
    },
    {
        "title": "Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free",
        "link_suffix": "/forum?id=eFGQ97z5Cd",
        "link": "https://openreview.net/forum?id=eFGQ97z5Cd",
        "pdf_link": "https://openreview.net/pdf?id=eFGQ97z5Cd",
        "keywords": "Mixture of Experts",
        "abstract": "While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning."
    },
    {
        "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity",
        "link_suffix": "/forum?id=90Db4RUBc7",
        "link": "https://openreview.net/forum?id=90Db4RUBc7",
        "pdf_link": "https://openreview.net/pdf?id=90Db4RUBc7",
        "keywords": "pretrained models, efficient attention, uptraining, speech processing",
        "abstract": "Architectures such as Linformer and Mamba have recently emerged as competitive linear time replacements for transformers. However, corresponding large pretrained models are often unavailable, especially in non-text domains. To remedy this, we present a Cross-Architecture Layerwise Distillation (CALD) approach that jointly converts a transformer model to a linear time substitute and fine-tunes it to a target task. We also compare several means to guide the fine-tuning to optimally retain the desired inference capability from the original model. The methods differ in their use of the target model and the trajectory of the parameters. In a series of empirical studies on language processing, language modeling, and speech processing, we show that CALD can effectively recover the result of the original model, and that the guiding strategy contributes to the result. Some reasons for the variation are suggested."
    },
    {
        "title": "Variational Mode Decomposition and Linear Embeddings are What You Need For Time-Series Forecasting",
        "link_suffix": "/forum?id=hT1XEfHbtw",
        "link": "https://openreview.net/forum?id=hT1XEfHbtw",
        "pdf_link": "https://openreview.net/pdf?id=hT1XEfHbtw",
        "keywords": "Decomposition; Time-series forecasting; Linear models",
        "abstract": "Time-series forecasting often faces challenges due to data volatility, which can lead to inaccurate predictions. Variational Mode Decomposition (VMD) has emerged as a promising technique to mitigate volatility by decomposing data into distinct modes, enhancing forecast accuracy. This study integrates VMD with linear models to develop a robust forecasting framework. Our approach is evaluated on 13 diverse datasets, including ETTm2, WindTurbine, M4, and 10 air quality datasets from Southeast Asian cities. The effectiveness of the VMD strategy is assessed by comparing Root Mean Squared Error (RMSE) values from models utilizing VMD against those without it. Additionally, we benchmark linear-based models against well-known neural network architectures such as LSTM, BLSTM, and RNN. The results demonstrate a significant reduction in RMSE across nearly all models following VMD application. Notably, the Linear + VMD model achieved the lowest average RMSE in univariate forecasting at 0.619. In multivariate forecasting, the DLinear + VMD model consistently outperformed others, attaining the lowest RMSE across all datasets with an average of 0.019. These findings underscore the effectiveness of combining VMD with linear models for superior time-series forecasting."
    },
    {
        "title": "Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization",
        "link_suffix": "/forum?id=7p8CcxP1Xc",
        "link": "https://openreview.net/forum?id=7p8CcxP1Xc",
        "pdf_link": "https://openreview.net/pdf?id=7p8CcxP1Xc",
        "keywords": "crowd counting",
        "abstract": "Crowd counting and localization involves extracting the number and distribution of crowds from images or videos using computer vision techniques. Most counting methods are based on density regression and are based on an ``intersection'' hypothesis,i.e., one pixel is influenced by multiple points in ground truth, which is inconsistent with reality since one pixel would not contain two objects. This paper proposes Proximal Mapping Loss (PML), a density regression method that eliminates this hypothesis. PML divides the predicted density map into multiple point-neighbor cases through nearest neighbor, and then dynamically constructs a learning target for each sub-case via proximal mapping, leading to more robust and accurate training. Furthermore, PML is theoretically linked to various existing loss functions, such as Gaussian-blurred L2 loss, Bayesian loss, and the training schemes in P2PNet and DMC, demonstrating its versatility and adaptability. Experimentally, PML significantly improves the performance of crowd counting and localization, and illustrates the robustness against annotation noise."
    }
]