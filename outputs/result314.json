[{"title": "SGDF: A Method for Reducing Variance in Stochastic Gradient Descent via Filter Estimation", "link_suffix": "/forum?id=fWNHKHh0Yn", "link": "https://openreview.net/forum?id=fWNHKHh0Yn", "pdf_link": "https://openreview.net/pdf?id=fWNHKHh0Yn", "keywords": "Stochastic Gradient Descent, Gradient Estimation, Generalization", "abstract": "In deep learning, stochastic gradient descent (SGD) and its momentum-based variants are widely used for optimization, but they typically suffer from slow convergence. Conversely, existing adaptive learning rate optimizers speed up convergence but often compromise generalization. To resolve this issue, we propose a novel optimization method designed to accelerate SGD's convergence without sacrificing generalization. Our approach reduces the variance of the historical gradient, improves first-order moment estimation of SGD by applying Wiener filter theory, and introduces a time-varying adaptive gain. Empirical results demonstrate that SGDF (SGD with Filter) effectively balances convergence and generalization compared to state-of-the-art optimizers.", "title_embedding_index": 15650, "title_abs_embedding_index": 15675}, {"title": "Emergence of Alignment and Local Elasticity in Two-Layer Neural Networks", "link_suffix": "/forum?id=oeLB25A9oO", "link": "https://openreview.net/forum?id=oeLB25A9oO", "pdf_link": "https://openreview.net/pdf?id=oeLB25A9oO", "keywords": "two-layer neural network, feature learning, metric learning, local elasticity, retrieval, random matrix theory", "abstract": "Understanding feature learning is essential for enhancing performance across a wide range of tasks, particularly concerning phenomena such as Alignment and Local Elasticity. In this context, we investigate the emergence of these phenomenon in two-layer neural networks trained in the proportional regime, where the sample size, network width, and data dimensions grow infinitely in proportion.  We reveal a strong relationship of Alignment and Local Elasticity, both of which emerge simultaneously after one step of training under  identical conditions. In particular, we demonstrate that features align better with samples and distributions similar to the training data, with stronger Local Elasticity occurring in these cases. This connection between feature Alignment and stronger Local Elasticity provides insight into why neural networks trained on similar domains serve as effective feature extractors for clustering tasks in metric learning, offering a theoretical foundation for the observed performance in such settings. Additionally, we establish a link between feature learning, metric learning, Alignment, and Local Elasticity. We validate our theory through experiments showing that both Alignment and recall@1 improve according to their similarity to the training classes. We also present novel results on the operator norms of non-centered Sub-Gaussian matrices and the expectations of Hermite polynomials under non-standard Gaussian distributions.", "title_embedding_index": 15651, "title_abs_embedding_index": 15676}, {"title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion", "link_suffix": "/forum?id=bjkQTInGes", "link": "https://openreview.net/forum?id=bjkQTInGes", "pdf_link": "https://openreview.net/pdf?id=bjkQTInGes", "keywords": "3D; Video Diffusion Model; 3D generation", "abstract": "Existing image-to-3D creation methods typically split the task into multi-view image generation and 3D reconstruction, leading to two main limitations: (1) multi-view bias, where geometric inconsistencies arise because multi-view diffusion models ensure image-level rather than 3D consistency; (2) misaligned reconstruction data, since reconstruction models trained on mostly synthetic data misalign when processing generated multi-view images during inference. To address these issues, we propose Ouroboros3D, a unified framework that integrates multi-view generation and 3D reconstruction into a recursive diffusion process. By incorporating a 3D-aware feedback mechanism, our multi-view diffusion model leverages the explicit 3D information from the reconstruction results of the previous denoising process as conditions, thus modeling consistency at the 3D geometric level. Furthermore, through joint training of both the multi-view diffusion and reconstruction models, we alleviate reconstruction bias due to data misalignment and enable mutual enhancement within the multi-step recursive process. Experimental results demonstrate that Ouroboros3D outperforms methods that treat these stages separately and those that combine them only during inference, achieving superior multi-view consistency and producing 3D models with higher geometric realism.", "title_embedding_index": 15652, "title_abs_embedding_index": 15677}, {"title": "AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning", "link_suffix": "/forum?id=Luss2sa0vc", "link": "https://openreview.net/forum?id=Luss2sa0vc", "pdf_link": "https://openreview.net/pdf?id=Luss2sa0vc", "keywords": "Articulated Object Manipulation, Adaptive Mechanism Environments, Imitation Learning", "abstract": "Articulated object manipulation is a critical capability for robots to perform various tasks in real-world scenarios. Composed of multiple parts connected by joints, articulated objects are endowed with diverse functional mechanisms through complex relative motions. For example, a safe consists of a door, a handle, and a lock, where the door can only be opened when the latch is unlocked. The internal structure, such as the state of a lock or joint angle constraints, cannot be directly observed from visual observation. Consequently, successful manipulation of these objects requires adaptive adjustment based on trial and error rather than a one-time visual inference. However, previous datasets and simulation environments for articulated objects have primarily focused on simple manipulation mechanisms where the complete manipulation process can be inferred from the object's appearance. To enhance the diversity and complexity of adaptive manipulation mechanisms, we build a novel articulated object manipulation environment and equip it with 9 categories of articulated objects. Based on the environment and objects, we further propose an adaptive demonstration collection pipeline and a 3D visual diffusion-based imitation learning that learns the adaptive manipulation policy. The effectiveness of our designs and proposed method are validated through both simulation and real-world experiments.", "title_embedding_index": 15653, "title_abs_embedding_index": 15678}, {"title": "Don't Cut Corners: Exact Conditions for Modularity in Biologically Inspired Representations", "link_suffix": "/forum?id=BxQkDog4ti", "link": "https://openreview.net/forum?id=BxQkDog4ti", "pdf_link": "https://openreview.net/pdf?id=BxQkDog4ti", "keywords": "neuroscience, representation learning, disentanglement, modularisation, neural networks, hippocampus, cortex", "abstract": "Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? \nIn this work, we develop a theory of when biologically inspired networks---those that are nonnegative and energy efficient---modularise their representation of source variables (sources). \nWe derive necessary and sufficient conditions on a sample of sources that determine whether the neurons in an optimal biologically-inspired linear autoencoder modularise. Our theory applies to any dataset extending far beyond the case of statistical independence studied in previous work. Rather we show that sources modularise if their support is \"sufficiently spread''.\nFrom this theory, we extract and validate predictions in a variety of empirical studies on how data distribution affects modularisation in nonlinear feedforward and recurrent neural networks trained on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data. First, we explain why two studies that recorded prefrontal activity in working memory tasks conflict on whether memories are encoded in orthogonal subspaces: the support of the sources differed due to a critical discrepancy in experimental protocol. And second, we study spatial and reward information mixing in entorhinal recordings, and show our theory matches data better than previous work. In sum, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in brains and machines.", "title_embedding_index": 15654, "title_abs_embedding_index": 15679}, {"title": "Zero-shot Quantization for Object Detection", "link_suffix": "/forum?id=XNr6sexQGj", "link": "https://openreview.net/forum?id=XNr6sexQGj", "pdf_link": "https://openreview.net/pdf?id=XNr6sexQGj", "keywords": "Zero-shot quantization, Object detection, Synthetic data, Fine-tuning efficiency, Feature distillation", "abstract": "Zero-shot quantization (ZSQ) has achieved remarkable success in classification tasks by leveraging synthetic data for network quantization without accessing the original training data. However, when applied to object detection networks, current ZSQ methods fail due to the inherent complexity of the task, which encompasses both localization and classification challenges. On the one hand, the precise location and size of objects within the samples for object detection remain unknown and elusive in zero-shot scenarios, precluding artificial reconstruction without ground-truth information. On the other hand, object detection datasets typically exhibit category imbalance, and random category sampling methods designed for classification tasks cannot capture this information.\nTo tackle these challenges, we propose a novel ZSQ framework specifically tailored for object detection. The proposed framework comprises two key steps: First, we employ a novel bounding box and category sampling strategy in the calibration set generation process to infer the original training data from a pre-trained detection network and reconstruct the location, size and category distribution of objects within the data without any prior knowledge. Second, we incorporate feature-level alignment into the Quantization Aware Training (QAT) process, further amplifying its efficacy through the integration of feature-level distillation.\nExtensive experiments conducted on the MS-COCO and Pascal VOC datasets demonstrate the efficiency and state-of-the-art performance of our method in low-bit-width quantization. For instance, when quantizing YOLOv5-m to 5-bit, we achieve a 4.2% improvement in the mAP metric, utilizing only about 1/60 of the calibration data required by commonly used LSQ trained with full trainset.", "title_embedding_index": 15655, "title_abs_embedding_index": 15680}, {"title": "Measuring memorization in RLHF for code completion", "link_suffix": "/forum?id=Tg8RLxpMDu", "link": "https://openreview.net/forum?id=Tg8RLxpMDu", "pdf_link": "https://openreview.net/pdf?id=Tg8RLxpMDu", "keywords": "rlhf, memorization, code completion", "abstract": "Reinforcement learning with human feedback (RLHF) has become the dominant method to align large models to user preferences.\nUnlike fine-tuning, for which there are many studies regarding training data memorization, it is not clear how memorization is affected by or introduced in the RLHF alignment process.\nUnderstanding this relationship is important as real user data may be collected and used to align large models; if user data is memorized during RLHF and later regurgitated, this could raise privacy concerns. In addition to RLHF, other methods such as Direct Preference Optimization (DPO) and $\\Psi$PO have gained popularity for learning directly from human preferences, removing the need for optimizing intermediary reward models with reinforcement learning.\nIn this work, we analyze how training data memorization can surface and propagate through each phase of RLHF and direct preference learning.\nWe focus our study on code completion models, as code completion is one of the most popular use cases for large language models. We find that RLHF significantly decreases the chance that data used for reward modeling and reinforcement learning is memorized in comparison to directly fine-tuning on this data, but that examples already memorized during the fine-tuning stage of RLHF, will, in the majority of cases, remain memorized after RLHF. In contrast, we find that aligning by learning directly from human preference data via a special case of $\\Psi$PO, Identity Preference Optimization (IPO), increases the likelihood that training data is regurgitated compared to RLHF. Our work suggests that RLHF, as opposed to direct preference learning, is a safer way to mitigate the risk of  regurgitating sensitive preference data when aligning large language models. We find our conclusions are robust across multiple code completion datasets, tasks, and model scales.", "title_embedding_index": 15656, "title_abs_embedding_index": 15681}, {"title": "Large Language Models have Intrinsic Self-Correction Ability", "link_suffix": "/forum?id=pTyEnkuSQ0", "link": "https://openreview.net/forum?id=pTyEnkuSQ0", "pdf_link": "https://openreview.net/pdf?id=pTyEnkuSQ0", "keywords": "large language model, self correction, prompt engineering, foundation model", "abstract": "Large language models (LLMs) have attracted significant attention for their exceptional abilities in various natural language processing tasks, but they suffer from hallucinations that will cause performance degradation. One promising solution to improve the LLMs' performance is to ask LLMs to revise their answer after generation, a technique known as self-correction. Among the two types of self-correction, intrinsic self-correction is considered a promising direction because it does not utilize external knowledge. However, recent works doubt the validity of LLM's ability to conduct intrinsic self-correction. In this paper, we present a novel perspective on the intrinsic self-correction capabilities of LLMs through theoretical analyses and empirical experiments. In addition, we identify two critical factors for successful self-correction: zero temperature and fair prompts. Leveraging these factors, we demonstrate that intrinsic self-correction ability is exhibited across multiple existing LLMs. Our findings offer insights into the fundamental theories underlying the self-correction behavior of LLMs and remark on the importance of unbiased prompts and zero temperature settings in harnessing their full potential.", "title_embedding_index": 15657, "title_abs_embedding_index": 15682}, {"title": "RePrompt: Prompt Engineering for Large Language Models Agents through Reflection", "link_suffix": "/forum?id=bgcdO9lmug", "link": "https://openreview.net/forum?id=bgcdO9lmug", "pdf_link": "https://openreview.net/pdf?id=bgcdO9lmug", "keywords": "Large language models, reasoning, prompt engineering, large language model agents", "abstract": "In this past year, large language models (LLMs) have had remarkable success in domains outside the traditional natural language processing, and people are starting to explore the usage of LLMs in more general and close to application domains like code generation, travel planning, and robot controls. Connecting these LLMs with great capacity and external tools, people are building the so-called LLM agents, which are supposed to help people do all kinds of work in everyday life. In all these domains, the prompt to the LLMs has been shown to make a big difference in what the LLM would generate and thus affect the performance of the LLM agents. Therefore, automatic prompt engineering (APE) has become an important question for many researchers and users of LLMs. However, previous works in APE all rely on a final checker to evaluate the performance of the given prompt, which is hard to meet in the case of LLM agents where intermediate feedback is easier to get, and the final evaluation could be expensive, inaccurate, or even missing. In this paper, we propose a novel method, \\textsc{RePrompt}, which does a ``gradient descent\"-like approach to optimize the step-by-step instructions in the prompts given to LLM agents based on the chat history obtained from interactions and reflections with LLM agents. By leveraging intermediate feedback, \\textsc{RePrompt} can optimize the prompt without the need for a final solution checker. We have used experiments in PDDL generation and travel planning to show that our method could generally improve the performance for different reasoning tasks.", "title_embedding_index": 15658, "title_abs_embedding_index": 15683}, {"title": "Towards Certification of Uncertainty Calibration under Adversarial Attacks", "link_suffix": "/forum?id=uuPkll6i7m", "link": "https://openreview.net/forum?id=uuPkll6i7m", "pdf_link": "https://openreview.net/pdf?id=uuPkll6i7m", "keywords": "Machine Learning, Adversarial Robustness, Certification, Adversarial Training, Uncertainty Quantification, Calibration, Deep Learning, Certified Calibration", "abstract": "Since neural classifiers are known to be sensitive to adversarial perturbations that alter their accuracy, certification methods have been developed to provide provable guarantees on the insensitivity of their predictions to such perturbations. On the other hand, in safety-critical applications, the frequentist interpretation of the confidence of a classifier (also known as model calibration) can be of utmost importance. This property can be measured via the Brier score or the expected calibration error. We show that attacks can significantly harm calibration, and thus propose certified calibration providing worst-case bounds on calibration under adversarial perturbations. Specifically, we produce analytic bounds for the Brier score and approximate bounds via the solution of a mixed-integer program on the expected calibration error. Finally, we propose novel calibration attacks and demonstrate how they can improve model calibration through adversarial calibration training. The code will be publicly released upon acceptance.", "title_embedding_index": 15659, "title_abs_embedding_index": 15684}, {"title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs", "link_suffix": "/forum?id=E9GakjQype", "link": "https://openreview.net/forum?id=E9GakjQype", "pdf_link": "https://openreview.net/pdf?id=E9GakjQype", "keywords": "adversarial attacks, prompt optimization, red-teaming LLMs", "abstract": "While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certainjailbreaking attacksthat lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. \nOn the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called theAdvPrompter, to generate human-readable adversarial prompts in seconds, $\\sim800\\times$ faster than existing optimization-based approaches.\nWe train the AdvPrompter using a novel algorithm thatdoes not require gradientsof the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.", "title_embedding_index": 15660, "title_abs_embedding_index": 15685}, {"title": "A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence", "link_suffix": "/forum?id=ZW4MRZrmSA", "link": "https://openreview.net/forum?id=ZW4MRZrmSA", "pdf_link": "https://openreview.net/pdf?id=ZW4MRZrmSA", "keywords": "Game Theory, Reinforcement Learning", "abstract": "Policy gradient methods have become a staple of any single-agent reinforcement learning toolbox, due to their combination of desirable properties: iterate convergence, efficient use of stochastic trajectory feedback, and theoretically-sound avoidance of importance sampling corrections. In multi-agent imperfect-information settings (extensive-form games), however, it is still unknown whether the same desiderata can be guaranteed while retaining theoretical guarantees. Instead, sound methods for extensive-form games rely on approximating \\emph{counterfactual} values (as opposed to Q values), which are incompatible with policy gradient methodologies. In this paper, we investigate whether policy gradient can be safely used in two-player zero-sum imperfect-information extensive-form games (EFGs). We establish positive results, showing for the first time that a policy gradient method leads to provable best-iterate convergence to a regularized Nash equilibrium in self-play.", "title_embedding_index": 15661, "title_abs_embedding_index": 15686}, {"title": "Improved Techniques for Optimization-Based Jailbreaking on Large Language Models", "link_suffix": "/forum?id=e9yfCY7Q3U", "link": "https://openreview.net/forum?id=e9yfCY7Q3U", "pdf_link": "https://openreview.net/pdf?id=e9yfCY7Q3U", "keywords": "Jailbreaking Attacks, Large Language Models", "abstract": "Large language models (LLMs) are being rapidly developed, and a key component of their widespread deployment is their safety-related alignment. Many red-teaming efforts aim to jailbreak LLMs, where among these efforts, the Greedy Coordinate Gradient (GCG) attack\u2019s success has led to a growing interest in the study of optimization-based jailbreaking techniques. Although GCG is a significant milestone, its attacking efficiency remains unsatisfactory. In this paper, we present several improved (empirical) techniques for optimization-based jailbreaks like GCG. We first observe that the single target template of \"Sure\" largely limits the attacking performance of GCG; given this, we propose to apply diverse target templates containing harmful self-suggestion and/or guidance to mislead LLMs. Besides, from the optimization aspects, we propose an automatic multi-coordinate updating strategy in GCG (i.e., adaptively deciding how many tokens to replace in each step) to accelerate convergence, as well as tricks like easy-to-hard initialisation. Then, we combine these improved technologies to develop an efficient jailbreak method, dubbed I-GCG. In our experiments, we evaluate on a series of benchmarks (such as NeurIPS 2023 Red Teaming Track). The results demonstrate that our improved techniques can help GCG outperform state-of-the-art jailbreaking attacks and achieve nearly 100% attack success rate.", "title_embedding_index": 15662, "title_abs_embedding_index": 15687}, {"title": "More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness", "link_suffix": "/forum?id=FpiCLJrSW8", "link": "https://openreview.net/forum?id=FpiCLJrSW8", "pdf_link": "https://openreview.net/pdf?id=FpiCLJrSW8", "keywords": "Large Language Model, Trustworthy ML, Data Attribution", "abstract": "The trustworthiness of Large Language Models (LLMs) refers to the extent to which their outputs are reliable, safe, and ethically aligned, and it has become a crucial consideration alongside their cognitive performance. In practice, Reinforcement Learning From Human Feedback (RLHF) has been widely used to align LLMs with labeled human preferences, but its assumed effect on model trustworthiness hasn't been rigorously evaluated. To bridge this knowledge gap, this study investigates how models aligned with general-purpose preference data perform across five trustworthiness verticals: toxicity, stereotypical bias, machine ethics, truthfulness, and privacy. Our results demonstrate that RLHF on human preferences doesn't automatically guarantee trustworthiness, and reverse effects are often observed. Furthermore, we propose to adapt efficient influence function based data attribution methods to the RLHF setting to better understand the influence of fine-tuning data on individual trustworthiness benchmarks, and show its feasibility by providing our estimated attribution scores. Together, our results underscore the need for more nuanced approaches for model alignment from both the data and framework perspectives, and we hope this research will guide the community towards developing language models that are increasingly capable without sacrificing trustworthiness.", "title_embedding_index": 15663, "title_abs_embedding_index": 15688}, {"title": "Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models", "link_suffix": "/forum?id=T26f9z2rEe", "link": "https://openreview.net/forum?id=T26f9z2rEe", "pdf_link": "https://openreview.net/pdf?id=T26f9z2rEe", "keywords": "Mixture of Experts, Transformer Model", "abstract": "The Sparse Mixture of Experts (SMoE) has been widely employed to enhance the efficiency of training and inference for Transformer-based foundational models, yielding promising results. However, the performance of SMoE heavily depends on the choice of hyper-parameters, such as the number of experts and the number of experts to be activated (referred to as top-$k$), resulting in significant computational overhead due to the extensive model training by searching over various hyper-parameter configurations. As a remedy, we introduce the Dynamic Mixture of Experts (DynMoE) technique. DynMoE incorporates (1) a novel gating method that enables each token to automatically determine the number of experts to activate. (2) An adaptive process automatically adjusts the number of experts during training. Extensive numerical results across Vision, Language, and Vision-Language tasks demonstrate the effectiveness of our approach to achieve competitive performance compared to GMoE for vision and language tasks, and MoE-LLaVA for vision-language tasks, while maintaining efficiency by activating fewer parameters. Our code will be made publicly available.", "title_embedding_index": 15664, "title_abs_embedding_index": 15689}, {"title": "WMAdapter: Adding WaterMark Control to Latent Diffusion Models", "link_suffix": "/forum?id=HAD6iZxKuh", "link": "https://openreview.net/forum?id=HAD6iZxKuh", "pdf_link": "https://openreview.net/pdf?id=HAD6iZxKuh", "keywords": "Diffusion Model, Watermarking", "abstract": "Watermarking is essential for protecting the copyright of AI-generated images. We propose WMAdapter, a diffusion model watermark plugin that embeds user-specified watermark information seamlessly during the diffusion generation process. Unlike previous methods that modify diffusion modules to incorporate watermarks, WMAdapter is designed to keep all diffusion components intact, resulting in sharp, artifact-free images. To achieve this, we introduce two key innovations: (1) We develop a contextual adapter that conditions on the content of the cover image to generate adaptive watermark embeddings. (2) We implement an additional finetuning step and a hybrid finetuning strategy that suppresses noticeable artifacts while preserving the integrity of the diffusion components. \n  Empirical results show that WMAdapter provides strong flexibility, superior image quality, and competitive watermark robustness.", "title_embedding_index": 15665, "title_abs_embedding_index": 15690}, {"title": "A Periodic Bayesian Flow for Material Generation", "link_suffix": "/forum?id=Lz0XW99tE0", "link": "https://openreview.net/forum?id=Lz0XW99tE0", "pdf_link": "https://openreview.net/pdf?id=Lz0XW99tE0", "keywords": "Crystal Generation, Bayesian Flow Networks, Crystal Structure Prediction", "abstract": "Generative modeling of crystal data distribution is an important yet challenging task due to the unique periodic physical symmetry of crystals. Diffusion-based methods have shown early promise in modeling crystal distribution. More recently, Bayesian Flow Networks were introduced to aggregate noisy latent variables, resulting in a variance-reduced parameter space that has been shown to be advantageous for modeling Euclidean data distributions with structural constraints (Song, et al.,2023). Inspired by this, we seek to unlock its potential for modeling variables located in non-Euclidean manifolds e.g. those within crystal structures, by overcoming challenging theoretical issues. We introduce CrysBFN, a novel crystal generation method by proposing a periodic Bayesian flow, which essentially differs from the original Gaussian-based BFN by exhibiting non-monotonic entropy dynamics. To successfully realize the concept of periodic Bayesian flow, CrysBFN integrates a new entropy conditioning mechanism and empirically demonstrates its significance compared to time-conditioning. Extensive experiments over both crystal ab initio generation and crystal structure prediction tasks demonstrate the superiority of CrysBFN, which consistently achieves new state-of-the-art on all benchmarks. Surprisingly, we found that CrysBFN enjoys a significant improvement in sampling efficiency, e.g., ~ 100x speedup (10 v.s. 2000 steps network forwards) compared with previous Diffusion-based methods on MP-20 dataset.", "title_embedding_index": 15666, "title_abs_embedding_index": 15691}, {"title": "LossAgent: Towards Any Optimization Objectives for Image Processing with LLM Agents", "link_suffix": "/forum?id=T0Df1Os6y0", "link": "https://openreview.net/forum?id=T0Df1Os6y0", "pdf_link": "https://openreview.net/pdf?id=T0Df1Os6y0", "keywords": "Image processing; LLM; Agent", "abstract": "We present the first embodied loss agent, dubbed LossAgent, for low-level image processing tasks, e.g., image super-resolution and restoration, intending to achieve any customized optimization objectives of low-level image processing in different practical applications. Notably, not all optimization objectives, such as complex hand-crafted perceptual metrics, text description, and intricate human feedback, can be instantiated with existing low-level losses, e.g., MSE loss. which presents a crucial challenge in optimizing image processing networks in an end-to-end manner. To eliminate this, our LossAgent introduces the powerful large language model (LLM) as the embodied loss agent, where the rich textual understanding prior knowledge empowers the loss agent with the potential to understand complex optimization objectives, trajectory, and state feedback from external environments in the optimization process of the low-level image processing networks. In particular, we establish the loss repository by incorporating existing loss functions that support the end-to-end optimization for low-level image processing. Then, we design the optimization-oriented prompt engineering for the loss agent to actively and intelligently decide the compositional weights for each loss in the repository at each optimization interaction, thereby achieving the required optimization trajectory for any customized optimization objectives. Extensive experiments on three typical low-level image processing tasks and multiple optimization objectives have shown the effectiveness and applicability of our proposed LossAgent.", "title_embedding_index": 15667, "title_abs_embedding_index": 15692}, {"title": "Why Solving Multi-agent Path Finding with Large Language Models has not Succeeded Yet", "link_suffix": "/forum?id=BW8O4wHgbo", "link": "https://openreview.net/forum?id=BW8O4wHgbo", "pdf_link": "https://openreview.net/pdf?id=BW8O4wHgbo", "keywords": "Large language models, multi-agent path finding, reasoning", "abstract": "With the explosive influence caused by the success of large language models (LLM), there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success of single-agent planning and multi-agent pathfinding in an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to support our hypothesis. Based on our results, we discussed how researchers with different backgrounds could help with this problem from different perspectives.", "title_embedding_index": 15668, "title_abs_embedding_index": 15693}, {"title": "Finite-Time Analysis for Conflict-Avoidant Multi-Task Reinforcement Learning", "link_suffix": "/forum?id=YKmRcayt8Z", "link": "https://openreview.net/forum?id=YKmRcayt8Z", "pdf_link": "https://openreview.net/pdf?id=YKmRcayt8Z", "keywords": "Multi-task reinforcement learning, Conflict-avoidant methods, Sample complexity analysis", "abstract": "Multi-task reinforcement learning (MTRL) has shown great promise in many real-world applications. Existing MTRL algorithms often aim to learn a policy that optimizes individual objective functions simultaneously with a given prior preference (or weights) on different tasks.  However, these methods often suffer from the issue of gradient conflict such that the tasks with larger gradients dominate the update direction, resulting in a performance degeneration on other tasks. In this paper, we develop a novel dynamic weighting multi-task actor-critic algorithm (MTAC) under two options of sub-procedures named as CA and FC in task weight updates. MTAC-CA aims to find a conflict-avoidant (CA) update direction that maximizes the minimum value improvement among tasks, and MTAC-FC targets at a much faster convergence rate. We provide a comprehensive finite-time convergence analysis for both algorithms. We show that MTAC-CA can find a $\\epsilon+\\epsilon_{\\text{app}}$-accurate Pareto stationary policy using $\\mathcal{O}({\\epsilon^{-5}})$ samples, while ensuring a small $\\epsilon+\\sqrt{\\epsilon_{\\text{app}}}$-level CA distance (defined as the distance to the CA direction), where $\\epsilon_{\\text{app}}$ is the function approximation error. The analysis also shows that MTAC-FC improves the sample complexity to $\\mathcal{O}(\\epsilon^{-3})$, but with a constant-level CA distance. Our experiments on MT10 demonstrate the improved performance of our algorithms over existing MTRL methods with fixed preference.", "title_embedding_index": 15669, "title_abs_embedding_index": 15694}, {"title": "Skinning-free Accurate 3D Garment Deformation via Image Transfer", "link_suffix": "/forum?id=eYePDPSmmu", "link": "https://openreview.net/forum?id=eYePDPSmmu", "pdf_link": "https://openreview.net/pdf?id=eYePDPSmmu", "keywords": "3D Garment Deformation", "abstract": "3D garment animation is key to a wide range of applications including digital humans, virtual try-on, and extended reality. This paper addresses the task of predicting 3D garment deformation from a posed body mesh. Existing learning-based methods mostly rely on linear blend skinning to decompose garment deformation into low-frequency posed garment shape and high-frequency wrinkles. However, due to the lack of explicit skinning supervision, they often produce misaligned garment positions with undesired artifacts during garment re-posing, which corrupt the high-frequency signals. These skinning-based methods consequently fail to recover accurate wrinkle patterns. To tackle this issue, we present a skinning-free approach that re-formulates the high-low frequency decomposition by estimating posed (i) vertex position for low-frequency posed garment shape, and (ii) vertex normal for high-frequency local wrinkle details. In this way, each frequency modality can be effectively decoupled and directly supervised by the geometry of the deformed garment. Moreover, we propose to encode both vertex attributes as texture images, so that 3D garment deformation can be equivalently achieved via 2D image transfer. This enables us to leverage powerful pretrained image encoders to recover high-fidelity visual details representing fine wrinkles. In addition, we model body-garment interaction via cross-attention between dense body and garment image patches, which refines the naive skinning on sparse joints. Finally, we propose a multimodal fusion to incorporate constraints from both frequency modalities and optimize deformed 3D garments from transferred images. Extensive experiments show that our method significantly improves deformation accuracy on various garment types and recovers finer wrinkles than state-of-the-art methods.", "title_embedding_index": 15670, "title_abs_embedding_index": 15695}, {"title": "Agent-to-Sim: Learning Interactive Behavior Model from Casual Longitudinal Videos", "link_suffix": "/forum?id=y80D4IojuY", "link": "https://openreview.net/forum?id=y80D4IojuY", "pdf_link": "https://openreview.net/pdf?id=y80D4IojuY", "keywords": "dynamic 3d reconstruction; multi-video registration; motion generation", "abstract": "We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents in a 3D environment from casually-captured videos. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents in a non-invasive way, directly from monocular video collections. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer of agents in their familiar environments given longitudinal video recordings (e.g., over a month). We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD video collections captured by a smartphone.", "title_embedding_index": 15671, "title_abs_embedding_index": 15696}, {"title": "DUMoE: Deep Unfolding Mixture-of-Experts for Compressive Imaging", "link_suffix": "/forum?id=AqRwoHvKtN", "link": "https://openreview.net/forum?id=AqRwoHvKtN", "pdf_link": "https://openreview.net/pdf?id=AqRwoHvKtN", "keywords": "Compressive Imaging, Compressive Sensing, SpaRSA algorithm, Mixture-of-Experts", "abstract": "Deep Unfolding-based Networks (DUNs) have attracted attention due to their high performance and a certain degree of interpretability. However, existing DUNs often lack flexibility in handling details and features in different images during reconstruction, as they typically involve multiple iterative modules cascading through the same structure for each iteration. To address this limitation, we propose DUMoE, a novel sparsely-activated Deep Unfolding Mixture-of-Experts (MoE) architecture for Compressive Imaging (CI). By integrating the deep unfolding paradigm into the MoE, we enable DUMoE to adaptively reconstruct various images by utilizing different experts at each iteration stage. Specifically, we unfold traditional SpaRSA iterations into experts within DUMoE and employ top-1 switch routing to save computational consumption and enhance flexibility. Additionally, we introduce the Degradation-Aware Mask within the self-attention mechanism to prioritize image degradation caused by dimensionality reduction in CI, thereby enhancing reconstruction fidelity. Moreover, we incorporate the Multi-Scale Gate to improve the DUMoE's adaptability to image features at different scales and facilitate information transmission across iteration stages. Extensive experiments across various CI recovery tasks, including natural image compressive sensing, magnetic resonance imaging, and snapshot compressive imaging, demonstrate the superior performance and effectiveness of DUMoE. To the best of our knowledge, we are the first to leverage the deep unfolding paradigm within the MoE framework.", "title_embedding_index": 15672, "title_abs_embedding_index": 15697}, {"title": "Revisiting Differentiable Structure Learning: Inconsistency of\u21131Penalty and Beyond", "link_suffix": "/forum?id=XT7kCxcEKm", "link": "https://openreview.net/forum?id=XT7kCxcEKm", "pdf_link": "https://openreview.net/pdf?id=XT7kCxcEKm", "keywords": "differentiable structure learning, $\\ell_0$-penalized likelihood, acyclicity constraint, moral graph", "abstract": "Recent advances in differentiable structure learning have framed the combinatorial problem of learning directed acyclic graphs as a continuous optimization problem. Various aspects, including data standardization, have been studied to identify factors that influence the empirical performance of these methods. In this work, we investigate critical limitations in differentiable structure learning methods, focusing on settings where the true structure can be identified up to Markov equivalence classes, particularly in the linear Gaussian case.  While Ng et al. (2024) highlighted potential non-convexity issues in this setting, we demonstrate and explain why the use of $\\ell_1$-penalized likelihood in such cases is fundamentally inconsistent, even if the global optimum of the optimization problem can be found. To resolve this limitation, we develop a hybrid differentiable structure learning method based on $\\ell_0$-penalized likelihood with hard acyclicity constraint, where the $\\ell_0$ penalty can be approximated by different techniques including Gumbel-Softmax. Specifically, we first estimate the underlying moral graph, and use it to restrict the search space of the optimization problem, which helps alleviate the non-convexity issue. Experimental results show that the proposed method enhances empirical performance both before and after data standardization, providing a more reliable path for future advancements in differentiable structure learning, especially for learning Markov equivalence classes.", "title_embedding_index": 15673, "title_abs_embedding_index": 15698}, {"title": "Learnable Context-Aware Attention Mask for Multimodal Transformers", "link_suffix": "/forum?id=abHtkQkumD", "link": "https://openreview.net/forum?id=abHtkQkumD", "pdf_link": "https://openreview.net/pdf?id=abHtkQkumD", "keywords": "Multimodal learning, Attention mechanisms, Multimodal, Learnable Masking, Transformer Architecture", "abstract": "The Self-Attention mechanism in Transformer models has shown great success across many domains, but its effectiveness can diminish in complex settings, such as multimodal tasks. This is due to the varying token granularity and the high computational cost of processing long sequences. To overcome these limitations, we propose the Learnable Context-Aware Attention Mask (LCAAM), a novel method that globally adjusts attention maps to prioritize the most important tokens in a sequence. Our approach integrates LCAAM into a BERT-like Transformer network, enhancing the Self-Attention mechanism by capturing token relationships while accounting for their contextual relevance. Additionally, we extend LCAAM to a multi-layer framework, enabling it to capture diverse information across the layers of the Transformer. Extensive experiments on datasets including MADv2, QVHighlights, ImageNet-1K, and MSRVTT demonstrate that LCAAM improves model performance while reducing redundant computations. This innovation offers a significant improvement in tackling complex tasks, such as movie understanding.", "title_embedding_index": 15674, "title_abs_embedding_index": 15699}]