[{"title": "CMIRA: Class Membership Inducing Recovery Attacks Against Machine Unlearning Models", "link_suffix": "/forum?id=bfy5A3vCt7", "link": "https://openreview.net/forum?id=bfy5A3vCt7", "pdf_link": "https://openreview.net/pdf?id=bfy5A3vCt7", "keywords": "Computer Vision (CV) -> CV: Adversarial Attacks & Robustness, Computer Vision (CV) -> CV: Ethics -- Bias, Fairness, Transparency & Privacy, Machine Learning (ML) -> ML:", "abstract": "The implementation of data privacy regulations such as GDPR and CCPA has advanced machine learning (MU) technology, which is designed to facilitate the removal of specific sensitive data points from trained models upon request. Despite rapid advancements in MU technology, its vulnerabilities are still underexplored, posing potential risks of privacy breaches by recovering unlearned sensitive information. Existing research on MU vulnerabilities often requires access to the original models, which violates with the core objective of MU. To address this gap, we initiate the first study on recovery attacks against MU models without requiring access to the original model. Our approach, known as Class Membership Inducing Recovery Attack (CMIRA), effectively recovers forgotten data by exploiting a probing dataset. Specifically, we implement the CMIRA scheme regarding mutual knowledge distillation between MU and attack models. Extensive experiments across multiple datasets and MU methods demonstrate that CMIRA exhibits high efficacy in both theoretical analysis and practical applications. Our study highlights the critical imperative for establishing robust MU systems and sets a benchmark for future research into MU vulnerabilities.", "title_embedding_index": 23150, "title_abs_embedding_index": 23165}, {"title": "Scaling Large Language Model-based Multi-Agent Collaboration", "link_suffix": "/forum?id=K3n5jPkrU6", "link": "https://openreview.net/forum?id=K3n5jPkrU6", "pdf_link": "https://openreview.net/pdf?id=K3n5jPkrU6", "keywords": "Large Language Model, Autonomous Agent, Multi-Agent Collaboration, Interactive Reasoning", "abstract": "Recent breakthroughs in large language model-driven autonomous agents have revealed that multi-agent collaboration often surpasses each individual through collective reasoning. Inspired by the neural scaling law\u2014increasing neurons enhances performance, this study explores whether the continuous addition of collaborative agents can yield similar benefits. Technically, we utilize directed acyclic graphs to organize agents into a multi-agent collaboration network (MacNet), upon which their interactive reasoning is topologically orchestrated for autonomous task solving. Extensive evaluations reveal that it effectively supports collaboration among over a thousand agents, with irregular topologies outperforming regular ones. We also identify a collaborative scaling law\u2014the overall performance follows a logistic growth pattern as agents scale, with collaborative emergence occurring earlier than traditional neural emergence. We speculate this may be because scaling agents catalyzes their multidimensional considerations during interactive reflection and refinement, thereby producing more comprehensive solutions.", "title_embedding_index": 23151, "title_abs_embedding_index": 23166}, {"title": "OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes", "link_suffix": "/forum?id=L6IgkJvcgV", "link": "https://openreview.net/forum?id=L6IgkJvcgV", "pdf_link": "https://openreview.net/pdf?id=L6IgkJvcgV", "keywords": "Stereotype Measurement, Responsible AI, Trustworthy AI, Interpretability", "abstract": "Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset:(M1)Stereotype Score to measure the distributional violation of stereotypical attributes, and(M2)WALS to measure spectral variance in the images along a stereotypical attribute. OASIS\nalso includes two methods to understand the origins of stereotypes in T2I models:(U1)StOP to discover attributes that the T2I model internally associates with a given concept, and(U2)SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.", "title_embedding_index": 23152, "title_abs_embedding_index": 23167}, {"title": "A Time Series is Worth Five Experts: Heterogeneous Mixture of Experts for Traffic Flow Prediction", "link_suffix": "/forum?id=3Q7y9No9VF", "link": "https://openreview.net/forum?id=3Q7y9No9VF", "pdf_link": "https://openreview.net/pdf?id=3Q7y9No9VF", "keywords": "Traffic Prediction, Mixture of Experts, Deep Learning, Spatio-Temporal data modeling", "abstract": "Accurate traffic prediction faces significant challenges, necessitating a deep understanding of both temporal and spatial cues and their complex interactions across multiple variables. Recent advancements in traffic prediction systems are primarily due to the development of complex sequence-centric models. However, existing approaches often embed multiple variables and spatial relationships at each time step, which may hinder effective variable-centric learning, ultimately leading to performance degradation in traditional traffic prediction tasks. To overcome these limitations, we introduce variable-centric and prior knowledge-centric modeling techniques. Specifically, we propose a Heterogeneous Mixture of Experts (TITAN) model for traffic flow prediction. TITAN initially consists of three experts focused on sequence-centric modeling. Then, designed a low-rank adaptive method, TITAN simultaneously enables variable-centric modeling. Furthermore, we supervise the gating process using a prior knowledge-centric modeling strategy to ensure accurate routing. Experiments on two public traffic network datasets, METR-LA and PEMS-BAY, demonstrate that TITAN effectively captures variable-centric dependencies while ensuring accurate routing. Consequently, it achieves improvements in all evaluation metrics, ranging from approximately 4.37% to 11.53%, compared to previous state-of-the-art (SOTA) models. The code will be released upon acceptance.", "title_embedding_index": 23153, "title_abs_embedding_index": 23168}, {"title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction", "link_suffix": "/forum?id=stK7iOPH9Q", "link": "https://openreview.net/forum?id=stK7iOPH9Q", "pdf_link": "https://openreview.net/pdf?id=stK7iOPH9Q", "keywords": "Diffusion Models; Dense Prediction; Monocular Depth Estimation; Monocular Normal Estimation", "abstract": "Leveraging the visual priors of pre-trained text-to-image diffusion models offers a promising solution to enhance zero-shot generalization in dense prediction tasks. However, existing methods often uncritically use the original diffusion formulation, which may not be optimal due to the fundamental differences between dense prediction and image generation. In this paper, we provide a systemic analysis of the diffusion formulation for the dense prediction, focusing on both quality and efficiency. And we find that the original parameterization type for image generation, which learns to predict noise, is harmful for dense prediction; the multi-step noising/denoising diffusion process is also unnecessary and challenging to optimize. Based on these insights, we introduce $\\textbf{Lotus}$, a diffusion-based visual foundation model with a simple yet effective adaptation protocol for dense prediction. Specifically, Lotus is trained to directly predict annotations instead of noise, thereby avoiding harmful variance. We also reformulate the diffusion process into a single-step procedure, simplifying optimization and significantly boosting inference speed. Additionally, we introduce a novel tuning strategy called detail preserver, which achieves more accurate and fine-grained predictions. Without scaling up the training data or model capacity, Lotus achieves SoTA performance in zero-shot depth and normal estimation across various datasets. It also enhances efficiency, being significantly faster than most existing diffusion-based methods. Lotus' superior quality and efficiency also enable a wide range of practical applications, such as joint estimation, single/multi-view 3D reconstruction, etc.", "title_embedding_index": 23154, "title_abs_embedding_index": 23169}, {"title": "ARB-LLM: Alternating Refined Binarizations for Large Language Models", "link_suffix": "/forum?id=ZU8OdDLTts", "link": "https://openreview.net/forum?id=ZU8OdDLTts", "pdf_link": "https://openreview.net/pdf?id=ZU8OdDLTts", "keywords": "Binarization, LLM", "abstract": "Large Language Models (LLMs) have greatly pushed forward advancements in natural language processing, yet their high memory and computational demands hinder practical deployment. Binarization, as an effective compression technique, can shrink model weights to just 1 bit, significantly reducing the high demands on computation and memory. However, current binarization methods struggle to narrow the distribution gap between binarized and full-precision weights, while also overlooking the column deviation in LLM weight distribution. To tackle these issues, we propose ARB-LLM, a novel 1-bit post-training quantization (PTQ) technique tailored for LLMs. To narrow the distribution shift between binarized and full-precision weights, we first design an alternating refined binarization (ARB) algorithm to progressively update the binarization parameters, which significantly reduces the quantization error. Moreover, considering the pivot role of calibration data and the column deviation in LLM weights, we further extend ARB to ARB-X and ARB-RC. In addition, we refine the weight partition strategy with column-group bitmap (CGB), which further enhance performance. Equipping ARB-X and ARB-RC with CGB, we obtain ARB-LLM$_{\\text{X}}$ and ARB-LLM$ _{\\text{RC}} $ respectively, which significantly outperform state-of-the-art (SOTA) binarization methods for LLMs.\nAs a binary PTQ method, our ARB-LLM$ _{\\text{RC}} $ is the first to surpass FP16 models of the same size. We will release all the code and models of ARB-LLM.", "title_embedding_index": 23155, "title_abs_embedding_index": 23170}, {"title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks", "link_suffix": "/forum?id=asR9FVd4eL", "link": "https://openreview.net/forum?id=asR9FVd4eL", "pdf_link": "https://openreview.net/pdf?id=asR9FVd4eL", "keywords": "Jailbreaking Attack, Black-box Transferable Attack, Large Language Model, Red-teaming Evaluation", "abstract": "Jailbreaking attacks can effectively manipulate open-source large language models (LLMs) to produce harmful responses. Nevertheless, these attacks exhibit limited transferability, failing to consistently disrupt proprietary LLMs. To reliably identify vulnerabilities in proprietary LLMs, this work investigates the transferability of jailbreaking attacks by analyzing their impact on the model's intent perception. By incorporating adversarial sequences, these attacks redirect the source LLM's focus away from malicious-intent tokens in the original input, thereby obstructing the model's intent recognition and eliciting harmful responses. However, these adversarial sequences fail to mislead the target LLM's intent perception, allowing the target LLM to refocus on malicious-intent tokens and abstain from responding. Our analysis further reveals the inherent $\\textit{distributional dependency}$ within the generated adversarial sequences, whose effectiveness stems from overfitting the source LLM's parameters, resulting in limited transferability to target LLMs. To this end, we propose the Perceived-importance Flatten (PiF) method, which uniformly disperses the model's focus across neutral-intent tokens in the original input, thus obscuring malicious-intent tokens without utilizing overfitted adversarial sequences. Extensive experiments demonstrate that PiF provides an effective and efficient red-teaming evaluation for identifying vulnerabilities in proprietary LLMs.", "title_embedding_index": 23156, "title_abs_embedding_index": 23171}, {"title": "Learning Mamba as a Continual Learner", "link_suffix": "/forum?id=1TXDtnDIsV", "link": "https://openreview.net/forum?id=1TXDtnDIsV", "pdf_link": "https://openreview.net/pdf?id=1TXDtnDIsV", "keywords": "Continual Learning, Sequence Modelling", "abstract": "Continual learning (CL) aims to efficiently learn and accumulate knowledge from a data stream with different distributions. By formulating CL as a sequence prediction task, meta-continual learning (MCL) enables to meta-learn an efficient continual learner based on the recent advanced sequence models, e.g., Transformers. Although attention-free models (e.g., Linear Transformers) can ideally match CL's essential objective and efficiency requirements, they usually perform not well in MCL. Considering that the attention-free Mamba achieves excellent performances matching Transformers' on general sequence modeling tasks, in this paper, we aim to answer a question -- Can attention-free Mamba perform well on MCL? By formulating Mamba with a selective state space model (SSM) for MCL tasks, we propose to meta-learn Mamba as a continual learner, referred to as MambaCL. By incorporating a selectivity regularization, we can effectively train MambaCL. Through comprehensive experiments across various CL tasks, we also explore how Mamba and other models perform in different MCL scenarios. Our experiments and analyses highlight the promising performance and generalization capabilities of Mamba in MCL.", "title_embedding_index": 23157, "title_abs_embedding_index": 23172}, {"title": "FLOPS: Forward Learning with OPtimal Sampling", "link_suffix": "/forum?id=z1nSpA2dAW", "link": "https://openreview.net/forum?id=z1nSpA2dAW", "pdf_link": "https://openreview.net/pdf?id=z1nSpA2dAW", "keywords": "stochastic optimization, gradient estimation", "abstract": "Given the limitations of backpropagation, perturbation-based gradient computation methods have recently gained focus for learning with only forward passes, also referred to as queries. Conventional forward learning consumes enormous queries on each data point for accurate gradient estimation through Monte Carlo sampling, which hinders the scalability of those algorithms. However, not all data points deserve equal queries for gradient estimation. In this paper, we study the problem of improving the forward learning efficiency from a novel perspective: how to reduce the gradient estimation variance with minimum cost? For this, we propose to allocate the optimal number of queries over each data in one batch during training to achieve a good balance between estimation accuracy and computational efficiency. Specifically, with a simplified proxy objective and a reparameterization technique, we derive a novel plug-and-play query allocator with minimal parameters. Theoretical results are carried out to verify its optimality. We conduct extensive experiments for fine-tuning Vision Transformers on various datasets and further deploy the allocator to two black-box applications: prompt tuning and multimodal alignment for foundation models. All findings demonstrate that our proposed allocator significantly enhances the scalability of forward-learning algorithms, paving the way for real-world applications.", "title_embedding_index": 23158, "title_abs_embedding_index": 23173}, {"title": "EZIGen: Enhancing zero-shot subject-driven image generation with precise subject encoding and decoupled guidance", "link_suffix": "/forum?id=ra7Nl9wUlF", "link": "https://openreview.net/forum?id=ra7Nl9wUlF", "pdf_link": "https://openreview.net/pdf?id=ra7Nl9wUlF", "keywords": "Diffusion models, Subject driven image generation, Personalized image generation", "abstract": "Zero-shot subject-driven image generation aims to produce images that incorporate a subject from a given example image. The challenge lies in preserving the subject's identity while aligning with the text prompt which often requires modifying certain aspects of the subject's appearance. Despite advancements in diffusion model based methods, existing approaches still struggle to balance identity preservation with text prompt alignment. In this study, we conducted an in-depth investigation into this issue and uncovered key insights for achieving effective identity preservation while maintaining a strong balance. Our key findings include: (1) the design of the subject image encoder significantly impacts identity preservation quality, and (2) separating text and subject guidance is crucial for both text alignment and identity preservation. Building on these insights, we introduce a new approach called EZIGen, which employs two main strategies: a carefully crafted subject image Encoder based on the pretrained UNet of the Stable Diffusion model to ensure high-quality identity transfer, following a process that decouples the guidance stages and iteratively refines the initial image layout. Through these strategies, EZIGen achieves state-of-the-art results on multiple subject-driven benchmarks with a unified model and 100 times less training data.", "title_embedding_index": 23159, "title_abs_embedding_index": 23174}, {"title": "How to Weight Multitask Finetuning? Fast Previews via Model Merging", "link_suffix": "/forum?id=McqVjmwdPe", "link": "https://openreview.net/forum?id=McqVjmwdPe", "pdf_link": "https://openreview.net/pdf?id=McqVjmwdPe", "keywords": "Model Merging, Bayesian Inference, Multitask Learning, Finetuning", "abstract": "When finetuning multiple tasks altogether, it is important to carefully weigh them to get a good performance, but searching for good weights can be difficult and costly. Here, we propose to aid the search with fast previews to quickly get a rough idea of different reweighting options. We use model merging to create previews by simply reusing and averaging parameters of models trained on each task separately (no retraining required). To improve the quality of previews, we propose a Bayesian approach to design new merging strategies by using more flexible posteriors. We validate our findings on vision and natural-language transformers. Our work shows the benefits of model merging via Bayes to improve multitask finetuning.", "title_embedding_index": 23160, "title_abs_embedding_index": 23175}, {"title": "Storybooth: Training-Free Multi-Subject Consistency for Improved Visual Storytelling", "link_suffix": "/forum?id=JZLon6cvx8", "link": "https://openreview.net/forum?id=JZLon6cvx8", "pdf_link": "https://openreview.net/pdf?id=JZLon6cvx8", "keywords": "consistent text-to-image generation, visual storytelling, story generation", "abstract": "Consistent text-to-image generation depicting thesamesubjects across different images has gained significant recent attention due to its widespread applications in the fields of visual-storytelling and multiple-shot video generation.  While remarkable, existing methods often require costly finetuning for each subject and struggle to maintain consistency across multiple characters. In this work, we first analyse the reason for these limitations. Our exploration reveals that the primary-issue stems fromself-attention leakage, which is exacerbated when trying to ensure consistency across multiple-characters. Motivated by these findings, we next propose a simple yet effectivetraining and optimization-free approachfor improving multiple-character consistency. In particular, we first leverage multi-modalchain-of-thoughtreasoning in order toapriorilocalize the different subjects across the storyboard frames. The final storyboard images are then generated using a modified diffusion model which includes1) a bounded cross-attention layerfor ensuring adherence to the initially predicted layout, and2) a bounded cross-frame self-attention layerfor reducing inter-character attention leakage. Furthermore, we also propose a novelcross-frame token-merging layerwhich allows for improved fine-grain consistency for the storyboard characters. \n Experimental analysis reveals that proposed approach is not only $\\times 30$ faster than prior training-based methods (eg, textual inversion, dreambooth-lora) but also surpasses the priorstate-of-the-art, exhibiting improved multi-character consistency and text-to-image alignment performance.", "title_embedding_index": 23161, "title_abs_embedding_index": 23176}, {"title": "PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding", "link_suffix": "/forum?id=Q6a9W6kzv5", "link": "https://openreview.net/forum?id=Q6a9W6kzv5", "pdf_link": "https://openreview.net/pdf?id=Q6a9W6kzv5", "keywords": "vision-language, multi-modal understanding", "abstract": "Understanding the physical world is a fundamental challenge in embodied AI, critical for enabling agents to perform complex tasks and operate safely in real-world environments. While Vision-Language Models (VLMs) have shown great promise in reasoning and task planning for embodied agents, their ability to comprehend physical phenomena remains extremely limited.\nTo close this gap, we introduce PhysBench, a comprehensive benchmark designed to evaluate VLMs' physical world understanding capability across a diverse set of tasks. \nPhysBench contains 100,000 entries of interleaved video-image-text data, categorized into four major domains: physical object properties, physical object relationships, physical scene understanding, and physics-based dynamics, further divided into 19 subclasses and 8 distinct capability dimensions.\nOur extensive experiments, conducted on 39 representative VLMs, reveal that while these models excel in common-sense reasoning, they struggle with understanding the physical world---likely due to the absence of physical knowledge in their training data and the lack of embedded physical priors.\nTo tackle the shortfall, we introduce PhysAgent, a novel framework that combines the generalization strengths of VLMs with the specialized expertise of vision models, significantly enhancing VLMs' physical understanding across a variety of tasks, including an 18.4% improvement on GPT-4o.\nFurthermore, our results demonstrate that enhancing VLMs\u2019 physical world understanding capabilities can significantly help the deployment of embodied agents, pushing the boundaries of machine intelligence in comprehending and interacting with the physical world. We believe that PhysBench and PhysAgent offer valuable insights and contribute to bridging the gap between VLMs and physical world understanding. Project site:https://anoy1314.github.io/", "title_embedding_index": 23162, "title_abs_embedding_index": 23177}, {"title": "Instance-dependent Early Stopping", "link_suffix": "/forum?id=P42DbV2nuV", "link": "https://openreview.net/forum?id=P42DbV2nuV", "pdf_link": "https://openreview.net/pdf?id=P42DbV2nuV", "keywords": "Early Stopping, Supervised Learning, Deep Learning", "abstract": "In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model's performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance's learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model.", "title_embedding_index": 23163, "title_abs_embedding_index": 23178}, {"title": "Progressive Autoregressive Video Diffusion Models", "link_suffix": "/forum?id=WSze9IIN3d", "link": "https://openreview.net/forum?id=WSze9IIN3d", "pdf_link": "https://openreview.net/pdf?id=WSze9IIN3d", "keywords": "Long Video Generation, Diffusion Models, Transformer, Autoregression", "abstract": "Current frontier video diffusion models have demonstrated remarkable results at\ngenerating high-quality videos. However, they can only generate short video clips,\nnormally around 5 seconds or 120 frames, due to computation limitations during\ntraining. In this work, we show that existing models can be naturally adapted to\nautoregressive video diffusion models without changing the architectures. Our\nkey idea is to assign the latent frames with progressively increasing noise levels\nrather than a single noise level. Thus, each latent can condition on all the less\nnoisy latents before it and provide condition for all the more noisy latents after it.\nSuch progressive video denoising allows our models to autoregressively generate\nframes without quality degradation. We present state-of-the-art results on long\nvideo generation at 1 minute (1440 frames at 24 FPS). Our results are available\nat this anonymous url:https://progressive-autoregressive-vdm.github.io/.", "title_embedding_index": 23164, "title_abs_embedding_index": 23179}]