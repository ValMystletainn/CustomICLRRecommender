[{"title": "EqNIO: Subequivariant Neural Inertial Odometry", "link_suffix": "/forum?id=C8jXEugWkq", "link": "https://openreview.net/forum?id=C8jXEugWkq", "pdf_link": "https://openreview.net/pdf?id=C8jXEugWkq", "keywords": "equivariance, inertial odometry, subequivariance", "abstract": "Neural network-based odometry using accelerometer and gyroscope readings from a single IMU can achieve robust, and low-drift localization capabilities, through the use ofneural displacement priors. These priors learn to produce denoised displacement measurements but need to ignore data variations due to specific IMU mount orientation and motion directions, hindering generalization.\nThis work introduces EqNIO, which addresses this challenge withcanonical displacement priors. We train an off-the-shelf architecture with IMU measurements that are mapped into a canonical gravity-aligned frame with learnable yaw. The outputs (displacement and covariance) are mapped back to the original frame. To maximize generalization, we find that these learnable yaw frames must transform equivariantly with global trajectory rotations and reflections across the gravity direction,i.e.action by the roto-reflection group $O_g(3)$ which preserves gravity (a subgroup of $O(3)$). This renders the displacement prior $O(3)$subequivariant.\nWe tailor specific linear, convolutional and non-linear layers that commute with the actions of the group. \nMoreover, we introduce a bijective decomposition of angular rates into vectors that transform similarly to accelerations, allowing us to leverage both measurements types. Natively, angular rates would need to be inverted upon reflection, unlike acceleration, which hinders their joint processing.\nWe highlight EqNIO's flexibility and generalization capabilities by applying it to both filter-based (TLIO), and end-to-end (RONIN) architectures, and outperforming existing methods that usesoftequivariance from auxiliary losses or data augmentation on the TLIO, Aria, RONIN, RIDI and OxIOD datasets. We believe this work paves the way to low-drift, and generalizable neural inertial odometry on edge-devices.", "title_embedding_index": 5100, "title_abs_embedding_index": 5125}, {"title": "MANTRA: The Manifold Triangulations Assemblage", "link_suffix": "/forum?id=X6y5CC44HM", "link": "https://openreview.net/forum?id=X6y5CC44HM", "pdf_link": "https://openreview.net/pdf?id=X6y5CC44HM", "keywords": "simplicial complex, topological deep learning, high-order, high-order dataset, simplicial complex learning", "abstract": "The rising interest in leveraging higher-order interactions present in complex systems has led to a surge in more expressive models exploiting high-order structures in the data, especially in topological deep learning (TDL), which designs neural networks on high-order domains such as simplicial complexes. However, progress in this field is hindered by the scarcity of datasets for benchmarking these architectures. To address this gap, we introduce MANTRA, the first large-scale, diverse, and intrinsically high-order dataset for benchmarking high-order models, comprising over 43,000 and 249,000 triangulations of surfaces and three-dimensional manifolds, respectively. With MANTRA, we assess several graph- and simplicial complex-based models on three topological classification tasks. We demonstrate that while simplicial complex-based neural networks generally outperform their graph-based counterparts in capturing simple topological invariants, they also struggle, suggesting a rethink of TDL. Thus, MANTRA serves as a benchmark  for assessing and advancing topological\nmethods, leading the way for more effective high-order models.", "title_embedding_index": 5101, "title_abs_embedding_index": 5126}, {"title": "ALIA: An LLM for Industrial Assets using Synthetic Data", "link_suffix": "/forum?id=jl9lHkQrrI", "link": "https://openreview.net/forum?id=jl9lHkQrrI", "pdf_link": "https://openreview.net/pdf?id=jl9lHkQrrI", "keywords": "synthetic data, industrial assets, LLM", "abstract": "With the emergence of agentic workflow development using Large Language Models (LLMs) for industrial applications, there is a growing need for small language models to possess domain-specific knowledge. In many existing approaches, reference materials such as books are used as a source of knowledge. This paper presents a novel approach to fine-tune a base LLM model in a continued pre-training fashion for the industrial assets domain, leveraging knowledge documented in a tabular structure to generate synthetic knowledge documents and a vast amount of question-answer pairs using an entity and relationship-driven approach. Ultimately, this approach enables the fine-tuning of a small LLM (LLAMA 3.1) to evaluate the performance enhancement it brings. We tested the base model and the enhanced model on the Industry4-FMSR MCQA dataset, comprising over 2,600 samples, and obtained around 4% overall improvement. Our experimental results confirm the validity of our approach in generating synthetic data for knowledge infusion tasks.", "title_embedding_index": 5102, "title_abs_embedding_index": 5127}, {"title": "Constrained Graph Clustering with Signed Laplacians", "link_suffix": "/forum?id=FneYHZU19U", "link": "https://openreview.net/forum?id=FneYHZU19U", "pdf_link": "https://openreview.net/pdf?id=FneYHZU19U", "keywords": "constrained graph clustering, spectral graph theory", "abstract": "Given two weighted graphs $G = (V, E, w_G)$ and $H = (V, F, w_H)$ defined on the same vertex set, the constrained clustering problem asks to find a set $S\\subset V$ that minimises the cut ratio between $w_G(S, V\\setminus S)$ and $w_H(S, V\\setminus S)$. We develop a Cheeger-type inequality that relates the solution of the constrained clustering problem to the spectral properties of $G$ and $H$. To reduce computational complexity, we use the signed Laplacian on $H$, simplifying the calculations while maintaining accurate results. By solving a generalized eigenvalue problem, our algorithm provides improvements in performance, particularly in scenarios where traditional spectral clustering methods face difficulties. We demonstrate its practical effectiveness through experiments on both synthetic and real-world datasets.", "title_embedding_index": 5103, "title_abs_embedding_index": 5128}, {"title": "Aligning Visual Contrastive learning models via Preference Optimization", "link_suffix": "/forum?id=wgRQ2WAORJ", "link": "https://openreview.net/forum?id=wgRQ2WAORJ", "pdf_link": "https://openreview.net/pdf?id=wgRQ2WAORJ", "keywords": "contrastive learning, preference optimization, alignment, reinforcement learning from human feedback, robustness, computer vision", "abstract": "Contrastive learning models have demonstrated impressive abilities to capture semantic similarities by aligning representations in the embedding space. However, their performance can be limited by the quality of the training data and its inherent biases. While techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have been applied to generative models to align them with human preferences, their use in contrastive learning is less explored.\nThis paper introduces a novel method for training contrastive learning models using Preference Optimization (PO) to break down complex concepts. Our method systematically aligns model behavior with desired preferences, enhancing performance on the targeted task. In particular, we focus on enhancing model robustness against typographic attacks, commonly seen in contrastive models like CLIP. We further apply our method to disentangle gender understanding and mitigate gender biases, offering a more nuanced control over these sensitive attributes. Our experiments\\footnote{Code available at: \\href{https://shorturl.at/FN1e8}{https://shorturl.at/FN1e8}}We demonstrate that models trained using PO outperform standard contrastive learning techniques while retaining their ability to handle adversarial challenges and maintain good accuracy on other downstream tasks. This makes our method well-suited for tasks requiring fairness, robustness, and alignment with specific preferences. We evaluate our method on several vision-language tasks, tackling challenges such as typographic attacks. Additionally, we explore the model's ability to disentangle gender concepts and mitigate gender bias, showcasing the versatility of our approach.", "title_embedding_index": 5104, "title_abs_embedding_index": 5129}, {"title": "Representations in a deep end-to-end driving model predict human brain activity in an active driving task", "link_suffix": "/forum?id=3UqIo72Ysq", "link": "https://openreview.net/forum?id=3UqIo72Ysq", "pdf_link": "https://openreview.net/pdf?id=3UqIo72Ysq", "keywords": "fMRI, autonomous driving, human driver modeling, computational neuroscience", "abstract": "Understanding how cognition and learned representations give rise to intelligent behavior is a fundamental goal in both machine learning and neuroscience. However, in both domains, the most well-understood behaviors are passive and open-loop, such as image recognition or speech processing. In this work, we compare human brain activity measured via functional magnetic resonance imaging with deep neural network (DNN) activations for an active taxi-driving task in a naturalistic simulated environment. To do so, we used DNN activations to build voxelwise encoding models for brain activity. Results show that encoding models for DNN activations explain significant amounts of variance in brain activity across many regions of the brain. Furthermore, each functional module in the DNN explains brain activity in a distinct network of functional regions in the brain. The functions of each DNN module correspond well to the known functional properties of its corresponding brain regions, suggesting that both the DNN and the human brain may partition the task in a similar manner. These results represent a first step towards understanding how humans and current deep learning methods agree or differ in active closed-loop tasks such as driving.", "title_embedding_index": 5105, "title_abs_embedding_index": 5130}, {"title": "ADMP-GNN: Adaptive Depth Message Passing GNN", "link_suffix": "/forum?id=yAU5X77S06", "link": "https://openreview.net/forum?id=yAU5X77S06", "pdf_link": "https://openreview.net/pdf?id=yAU5X77S06", "keywords": "Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have proven to be highly effective in various graph representation learning tasks. A key characteristic is that GNNs apply a fixed number of message-passing steps to all nodes in the graph, regardless of the varying computational needs and characteristics of each node.  Through empirical analysis of real-world data, we show that the optimal number of message-passing layers differs for nodes with different characteristics. This insight is further validated with experiments on synthetic datasets. To address this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novel framework that dynamically adjusts the number of message-passing layers for each node, leading to enhanced performance. This approach is applicable to any model that follows the message-passing scheme. We evaluate ADMP-GNN on the node classification task and observe performance improvements over a wide range of GNNs.", "title_embedding_index": 5106, "title_abs_embedding_index": 5131}, {"title": "Plots unlock time-series understanding in multimodal models", "link_suffix": "/forum?id=HkB4bW5eJj", "link": "https://openreview.net/forum?id=HkB4bW5eJj", "pdf_link": "https://openreview.net/pdf?id=HkB4bW5eJj", "keywords": "time-series, vision encoder, multimodal, plot, vision language model, foundation model, large language model, LLM", "abstract": "While multimodal foundation models can now natively work with data beyond text, they remain underutilized in analyzing the considerable amounts of multi-dimensional time-series data in fields like healthcare, finance, and social sciences, representing a missed opportunity for richer, data-driven insights. This paper proposes a simple but effective method that leverages the existing vision encoders of these models to \"see\" time-series data via plots, avoiding the need for additional, potentially costly, model training. Our empirical evaluations show that this approach outperforms providing the raw time-series data as text, with the additional benefit that visual time-series representations demonstrate up to a 90% reduction in model API costs. We validate our hypothesis through synthetic data tasks of increasing complexity, progressing from simple functional form identification on clean data, to extracting trends from noisy scatter plots. To demonstrate generalizability from synthetic tasks with clear reasoning steps to more complex, real-world scenarios, we apply our approach to consumer health tasks \u2013 specifically fall detection, activity recognition, and readiness assessment \u2013 which involve heterogeneous, noisy data and multi-step reasoning. The overall success in plot performance over text performance (up to an 120% performance increase on zero-shot synthetic tasks, and up to 150% performance increase on real-world tasks), across both GPT and Gemini model families, highlights our approach's potential for making the best use of the native capabilities of foundation models.", "title_embedding_index": 5107, "title_abs_embedding_index": 5132}, {"title": "WebCanvas: Benchmarking Web Agents in Online Environments", "link_suffix": "/forum?id=wkp57p0uhm", "link": "https://openreview.net/forum?id=wkp57p0uhm", "pdf_link": "https://openreview.net/pdf?id=wkp57p0uhm", "keywords": "web automation; benchmark; LLM; language-guided agents", "abstract": "For web agents to be practically useful, they must adapt to the continuously evolving web environment characterized by frequent updates to user interfaces and content. However, most existing benchmarks only capture the static aspects of the web. To bridge this gap, we introduce WebCanvas, an innovative online evaluation framework for web agents that effectively addresses the dynamic nature of web interactions. WebCanvas contains three main components to facilitate realistic assessments: (1) A novel evaluation metric which reliably capture critical intermediate actions or states necessary for task completions while disregarding noise caused by insignificant events or changed web-elements. (2) A benchmark dataset called Mind2Web-Live, a refined version of original Mind2Web static dataset containing 542 tasks with 2439 intermediate evaluation states; (3) Lightweight and generalizable annotation tools and maintenance pipelines that enables the community to collect and maintain the high-quality, up-to-date dataset. Building on WebCanvas, we open-source a baseline agent framework with extensible modules for reasoning, providing a foundation for the community to conduct online inference and evaluations. Our best-performing agent achieves a task success rate of 23.1% and a task completion rate of 48.8% on the Mind2Web-Live test set. Additionally, we analyze the performance discrepancies across various websites, domains, and experimental environments. We encourage the community to contribute further insights on online agent evaluation, thereby advancing this field of research.", "title_embedding_index": 5108, "title_abs_embedding_index": 5133}, {"title": "Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration", "link_suffix": "/forum?id=y5tkxH7kxQ", "link": "https://openreview.net/forum?id=y5tkxH7kxQ", "pdf_link": "https://openreview.net/pdf?id=y5tkxH7kxQ", "keywords": "LLM planning, Large Language Models, Multi-Agent Collaboration", "abstract": "Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at \\url{https://read-llm.github.io/}.", "title_embedding_index": 5109, "title_abs_embedding_index": 5134}, {"title": "Graph of Records: Boosting Retrieval Augmented Generation for Long-context Summarization with Graphs", "link_suffix": "/forum?id=6LKmaC4cO0", "link": "https://openreview.net/forum?id=6LKmaC4cO0", "pdf_link": "https://openreview.net/pdf?id=6LKmaC4cO0", "keywords": "Retrieval-Augmented Generation, Long-context Summarization, Graph Neural Networks, Large Language Models", "abstract": "Retrieval-augmented generation (RAG) has revitalized Large Language Models (LLMs) by injecting non-parametric factual knowledge. \nCompared with long-context LLMs, RAG is considered an effective summarization tool in a more concise and lightweight manner, which can interact with LLMs multiple times using diverse queries to get comprehensive responses. However, the LLM-generated historical responses, which contain potentially insightful information, are largely neglected and discarded by existing approaches, leading to suboptimal results. In this paper, we propose \\textit{graph of records} (\\textbf{GoR}), which leverages historical responses generated by LLMs to enhance RAG for long-context global summarization. Inspired by the \\textit{retrieve-then-generate} paradigm of RAG, we construct a graph by creating an edge between the retrieved text chunks and the corresponding LLM-generated response. To further uncover the sophisticated correlations between them, GoR further features a \\textit{graph neural network} and an elaborately designed \\textit{BERTScore}-based objective for self-supervised model training, enabling seamless supervision signal backpropagation between reference summaries and node embeddings. We comprehensively compare GoR with 12 baselines on four long-context summarization datasets, and the results indicate that our proposed method reaches the best performance. Extensive experiments further demonstrate the effectiveness of GoR.", "title_embedding_index": 5110, "title_abs_embedding_index": 5135}, {"title": "Positional Encoder Graph Quantile Neural Networks for Geographic Data", "link_suffix": "/forum?id=xlrpVyMIwz", "link": "https://openreview.net/forum?id=xlrpVyMIwz", "pdf_link": "https://openreview.net/pdf?id=xlrpVyMIwz", "keywords": "Graph Neural Networks (GNNs); Quantile regression; Geospatial data; Uncertainty quantification; Calibration; Model recalibration.", "abstract": "Positional Encoder Graph Neural Networks (PE-GNNs) are a leading approach for modeling continuous spatial data. However, they often fail to produce calibrated predictive distributions, limiting their effectiveness for uncertainty quantification. We introduce the Positional Encoder Graph Quantile Neural Network (PE-GQNN), a novel method that integrates PE-GNNs, Quantile Neural Networks, and recalibration techniques in a fully nonparametric framework, requiring minimal assumptions about the predictive distributions. We propose a new network architecture that, when combined with a quantile-based loss function, yields accurate and reliable probabilistic models without increasing computational complexity. Our approach provides a flexible, robust framework for conditional density estimation, applicable beyond spatial data contexts. We further introduce a structured method for incorporating a KNN predictor into the model while avoiding data leakage through the GNN layer operation. Experiments on benchmark datasets demonstrate that PE-GQNN significantly outperforms existing state-of-the-art methods in both predictive accuracy and uncertainty quantification.", "title_embedding_index": 5111, "title_abs_embedding_index": 5136}, {"title": "Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning", "link_suffix": "/forum?id=CL3U0GxFRD", "link": "https://openreview.net/forum?id=CL3U0GxFRD", "pdf_link": "https://openreview.net/pdf?id=CL3U0GxFRD", "keywords": "multi-agent reinforcement learning, communication", "abstract": "In cooperative multi-agent reinforcement learning (MARL), well-designed communication protocols can effectively facilitate consensus among agents, thereby enhancing task performance. Moreover, in large-scale multi-agent systems commonly found in real-world applications, effective communication plays an even more critical role due to the escalated challenge of partial observability compared to smaller-scale setups. In this work, we endeavor to develop a scalable communication protocol for MARL. Unlike previous methods that focus on selecting optimal pairwise communication links\u2014a task that becomes increasingly complex as the number of agents grows\u2014we adopt a global perspective on communication topology design. Specifically, we propose to utilize the exponential topology to enable rapid information dissemination among agents by leveraging its small-diameter and small-size properties. This approach leads to a scalable communication protocol, named ExpoComm. To fully unlock the potential of exponential graphs as communication topologies, we employ memory-based message processors and auxiliary tasks to ground messages, ensuring that they reflect global information and benefit decision-making. Extensive experiments on large-scale cooperative benchmarks, including MAgent and Infrastructure Management Planning, demonstrate the superior performance and robust zero-shot transferability of ExpoComm compared to existing communication strategies.", "title_embedding_index": 5112, "title_abs_embedding_index": 5137}, {"title": "Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?", "link_suffix": "/forum?id=aMBSY2ebPw", "link": "https://openreview.net/forum?id=aMBSY2ebPw", "pdf_link": "https://openreview.net/pdf?id=aMBSY2ebPw", "keywords": "llms, translation, low-resource, grammar, long-context, linguistics", "abstract": "Extremely low-resource (XLR) languages lack substantial corpora for training NLP models, motivating the use of all available resources such as dictionaries and grammar books. Machine Translation from One Book (Tanzer et al., 2024) suggests prompting long-context LLMs with one grammar book enables English\u2013Kalamang translation, an unseen XLR language\u2014a noteworthy case of linguistic knowledge helping an NLP task. We investigate whether the book\u2019s grammatical explanations or its parallel examples are most effective for learning XLR translation, finding almost all improvement stems from the parallel examples. Further, we find similar results for Nepali, a seen low-resource language, and achieve performance comparable to an LLM with a grammar book by simply fine-tuning an encoder-decoder translation model. We then investigate where grammar books help by testing two linguistic tasks, grammaticality judgment and gloss prediction, and we explore what kind of grammatical knowledge helps by introducing a typological feature prompt that achieves leading results on these more relevant tasks. We thus emphasise the importance of task-appropriate data for XLR languages: parallel examples for translation, and grammatical data for linguistic tasks. As we find no evidence that long-context LLMs can make effective use of grammatical explanations for XLR translation, we suggest data collection for multilingual XLR tasks such as translation is best focused on parallel data over linguistic description.", "title_embedding_index": 5113, "title_abs_embedding_index": 5138}, {"title": "Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages", "link_suffix": "/forum?id=a3g2l4yEys", "link": "https://openreview.net/forum?id=a3g2l4yEys", "pdf_link": "https://openreview.net/pdf?id=a3g2l4yEys", "keywords": "Multilingual, Multimodal, LLMs", "abstract": "Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world\u2019s languages and diverse cultural contexts underrepresented. This paper introduces PANGEA, a multilingual multimodal LLM trained on PANGEAINS, a diverse 6M instruction dataset spanning 39 languages. PANGEAINS features 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models\u2019 capabilities, we introduce PANGEABENCH, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that PANGEA significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. We fully open-source our data, code, and trained checkpoints to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.", "title_embedding_index": 5114, "title_abs_embedding_index": 5139}, {"title": "BMLM: Bidirectional Large Language Model for  Multi-Task Spoken Language Understanding: Better and Faster", "link_suffix": "/forum?id=oBmaLuEJda", "link": "https://openreview.net/forum?id=oBmaLuEJda", "pdf_link": "https://openreview.net/pdf?id=oBmaLuEJda", "keywords": "Spoken Language Understanding, Multi-Task Learning, Large Language Model", "abstract": "Autoregressive large language models (LLMs) have achieved notable success in natural language generation. However, their direct application to natural language understanding (NLU) tasks presents challenges due to reliance on fixed label vocabularies and task-specific output structures. Although instruction-following tuning can adapt LLMs for these tasks, the autoregressive architecture often leads to error propagation and significant time costs from uncontrollable output lengths, particularly in token-level tagging tasks. In this paper, we introduce a bidirectional LLM framework (BMLM) for multi-task spoken language understanding, which eliminates the need for training from scratch and seamlessly integrates with existing LLMs, bridging the gap between extensive pre-trained knowledge and the requirements of understanding tasks. Our evaluation on multiple datasets demonstrates that BMLM significantly outperforms state-of-the-art pre-trained language models and autoregressive LLM baselines. Specifically, on the MixATIS and MixSNIPS datasets, BMLM achieves notable improvements of +3.9% and +4.1% in overall semantic accuracy compared to autoregressive baselines. Additionally, we observe a 123x improvement in inference speed for the MixATIS dataset and a 189x enhancement for the MixSNIPS dataset compared to existing generative LLM baselines. We anticipate that this work will provide a new perspective and foundational support for LLM applications in the NLU domain.", "title_embedding_index": 5115, "title_abs_embedding_index": 5140}, {"title": "VisDiff: SDF-Guided Polygon Generation for Visibility Reconstruction and Recognition", "link_suffix": "/forum?id=rn8r7GqJm6", "link": "https://openreview.net/forum?id=rn8r7GqJm6", "pdf_link": "https://openreview.net/pdf?id=rn8r7GqJm6", "keywords": "Polygon Reconstruction, Visibility Reconstruction, Triangulation Dual, Geometric Reasoning, Generative Models", "abstract": "The capability to learn latent representations plays a key role in the effectiveness\nof recent machine learning methods. An active frontier in representation learning\nis understanding representations for combinatorial structures which may not\nadmit well-behaved local neighborhoods or distance functions. For example, for\npolygons, slightly perturbing vertex locations might lead to significant changes in\ntheir combinatorial structure (expressed as their triangulation or visibility graph)\nand may even lead to invalid polygons. In this paper, we investigate representations\nto capture the underlying combinatorial structures of polygons. Specifically,\nwe study the open problem of Visibility Reconstruction: Given a visibility graph\nG, construct a polygon P whose visibility graph is G. Visibility Reconstruction\nbelongs to the Existential Theory of Reals (\u2203R) complexity class (which lies between\nNP and P-SPACE). Currently, reconstruction algorithms are available only\nfor specific polygon classes. Establishing the hardness of the general problem is\nopen.We introduce VisDiff, a novel diffusion-based approach to reconstruct a polygon\nfrom its given visibility graph G. Our method first estimates the signed distance\nfunction (SDF) of P from G. Afterwards, it extracts ordered vertex locations\nthat have the pairwise visibility relationship given by the edges of G. Our main\ninsight is that going through the SDF significantly improves learning for reconstruction.\nIn order to train VisDiff, we make two main contributions: (1) We\ndesign novel loss components for computing the visibility in a differentiable manner\nand (2) create a carefully curated dataset. We use this dataset to benchmark\nour method and achieve 21% improvement in F1-Score over standard methods.\nWe also demonstrate effective generalization to out-of-distribution polygon types\nand show that learning a generative model allows us to sample the set of polygons\nwith a given visibility graph. Finally, we extend our method to the related\ncombinatorial problem of reconstruction from a triangulation. We achieve 95%\nclassification accuracy of triangulation edges and a 4% improvement in Chamfer\ndistance compared to current architectures. Lastly, we provide preliminary results\non the harder visibility graph recognition problem in which the input G is not\nguaranteed to be a visibility graph.", "title_embedding_index": 5116, "title_abs_embedding_index": 5141}, {"title": "Brain-inspired -Convolution benefits large kernels and aligns better with visual cortex", "link_suffix": "/forum?id=0LSAmFCc4p", "link": "https://openreview.net/forum?id=0LSAmFCc4p", "pdf_link": "https://openreview.net/pdf?id=0LSAmFCc4p", "keywords": "Lp-Convolution, Receptive Field, Multivariate p-generalized normal distribution, Representation Similarity, Visual Cortex, Gaussian Sparsity", "abstract": "Convolutional Neural Networks (CNNs) have profoundly influenced the field of computer vision, drawing significant inspiration from the visual processing mechanisms inherent in the brain. Despite sharing fundamental structural and representational similarities with the biological visual system, differences in local connectivity patterns within CNNs open up an interesting area to explore. In this work, we explore whether integrating biologically observed receptive fields (RFs) can enhance model performance and foster alignment with brain representations. We introduce a novel methodology, termed $L_p$-convolution, which employs the multivariate $L_p$-generalized normal distribution as an adaptable $L_p$-masks, to reconcile disparities between artificial and biological RFs. $L_p$-masks finds the optimal RFs through task-dependent adaptation of conformation such as distortion, scale, and rotation. This allows $L_p$-convolution to excel in tasks that require flexible RF shapes, including not only square-shaped regular RFs but also horizontal and vertical ones. Furthermore, we demonstrate that $L_p$-convolution with biological RFs significantly enhances the performance of large kernel CNNs possibly by introducing structured sparsity inspired by $L_p$-generalized normal distribution in convolution. Lastly, we present that neural representations of CNNs align more closely with the visual cortex when -convolution is close to biological RFs.", "title_embedding_index": 5117, "title_abs_embedding_index": 5142}, {"title": "Constrained Exploitability Descent: Finding Mixed-Strategy Nash Equilibrium by Offline Reinforcement Learning", "link_suffix": "/forum?id=sQYQ9i1g86", "link": "https://openreview.net/forum?id=sQYQ9i1g86", "pdf_link": "https://openreview.net/pdf?id=sQYQ9i1g86", "keywords": "offline reinforcement learning, adversarial Markov game, mixed-strategy Nash equilibrium, policy constraint, exploitability descent", "abstract": "This paper presents Constrained Exploitability Descent (CED), a novel model-free offline reinforcement learning algorithm for solving adversarial Markov games. CED is a game-theoretic approach combined with policy constraint methods from offline RL. While policy constraints can perturb the optimal pure-strategy solutions in single-agent scenarios, we find this side effect can be mitigated when it comes to solving adversarial games, where the optimal policy can be a mixed-strategy Nash equilibrium. We theoretically prove that, under the uniform coverage assumption on the dataset, CED converges to a stationary point in deterministic two-player zero-sum Markov games. The min-player policy at the stationary point satisfies the necessary condition for making up an exact mixed-strategy Nash equilibrium, even when the offline dataset is fixed and finite. Compared to the model-based method of Exploitability Descent that optimizes the max-player policy, our convergence result no longer relies on the generalized gradient. Experiments in matrix games, a tree-form game, and an infinite-horizon soccer game verify that a single run of CED leads to an optimal min-player policy when the practical offline data guarantees uniform coverage. Besides, CED achieves significantly lower NashConv compared to an existing pessimism-based method and can gradually improve the behavior policy even under non-uniform coverage.", "title_embedding_index": 5118, "title_abs_embedding_index": 5143}, {"title": "MaskSAM: Towards Auto-prompt SAM with Mask Classification for Medical Image Segmentation", "link_suffix": "/forum?id=BUDLe7NIjQ", "link": "https://openreview.net/forum?id=BUDLe7NIjQ", "pdf_link": "https://openreview.net/pdf?id=BUDLe7NIjQ", "keywords": "SAM, Auto-prompt, Medical Image Segmentation", "abstract": "Segment Anything Model~(SAM), a prompt-driven foundation model for natural image segmentation, demonstrated impressive zero-shot performance. However, SAM does not work when directly applied to medical image segmentation tasks, since SAM lacks the functionality to predict semantic labels for predicted masks and needs to provide extra prompts, such as points or boxes, to segment target regions. Meanwhile, there is a significant gap between 2D natural images and 3D medical images, so the performance of SAM is imperfect for medical image segmentation tasks. Following the above issues, we propose MaskSAM, a novel mask classification prompt-free SAM adaptation framework for medical image segmentation. We design a prompt generator combined with the image encoder in SAM to generate a set of auxiliary classifier tokens, auxiliary binary masks, and auxiliary bounding boxes. Each pair of auxiliary mask and box prompts, which addresses the requirements of extra prompts, is associated with class label predictions by the sum of the auxiliary classifier token and the learnable global classifier tokens in the mask decoder of SAM to solve the predictions of semantic labels. Meanwhile, we design a 3D depth-convolution adapter for image embeddings and a 3D depth-MLP adapter for prompt embeddings. We inject one of them into each transformer block in the image encoder and mask decoder to enable pre-trained 2D SAM models to extract 3D information and adapt to 3D medical images. Our method achieves state-of-the-art performance on AMOS2022, 90.52% Dice, which improved by 2.7% compared to nnUNet. Our method surpasses nnUNet by 1.7% on ACDC and 1.0% on Synapse datasets.", "title_embedding_index": 5119, "title_abs_embedding_index": 5144}, {"title": "A Unified Approach Towards Active Learning and Out-of-Distribution Detection", "link_suffix": "/forum?id=rcKzU0Vns0", "link": "https://openreview.net/forum?id=rcKzU0Vns0", "pdf_link": "https://openreview.net/pdf?id=rcKzU0Vns0", "keywords": "Out-of-Distribution Detection; Active Learning;", "abstract": "When applying deep learning models in real-world scenarios, active learning (AL) strategies are crucial for identifying label candidates from a nearly infinite amount of unlabeled data. In this context, robust out-of-distribution (OOD) detection mechanisms are essential for handling data outside the target distribution of the application. \nHowever, current works investigate both problems separately.\nIn this work, we introduce SISOM as the first unified solution for both AL and OOD detection.\nBy leveraging feature space distance metrics SISOM combines the strengths of the currently independent tasks to solve both effectively.\nWe conduct extensive experiments showing the problems arising when migrating between both tasks. In these evaluations SISOM underlined its effectiveness by achieving first place in two of the widely used OpenOOD benchmarks and second place in the remaining one. In AL, SISOM outperforms others and delivers top-1 performance in three benchmarks.", "title_embedding_index": 5120, "title_abs_embedding_index": 5145}, {"title": "Deconstructing What Makes a Good Optimizer for Autoregressive Language Models", "link_suffix": "/forum?id=zfeso8ceqr", "link": "https://openreview.net/forum?id=zfeso8ceqr", "pdf_link": "https://openreview.net/pdf?id=zfeso8ceqr", "keywords": "optimization, LLMs, language models, Adam", "abstract": "Training language models becomes increasingly expensive with scale, prompting numerous attempts to improve optimization efficiency. Despite these efforts, the Adam optimizer remains the most widely used, due to a prevailing view that it is the most effective approach. We aim to compare several optimization algorithms, including SGD, Adafactor, Adam, Lion, and Sophia in the context of autoregressive language modeling across a range of model sizes, hyperparameters, and architecture variants. Our findings indicate that, except for SGD, these algorithms all perform comparably both in their optimal performance and also in terms of how they fare across a wide range of hyperparameter choices. Our results suggest to practitioners that the choice of optimizer can be guided by practical considerations like memory constraints and ease of implementation, as no single algorithm emerged as a clear winner in terms of performance or stability to hyperparameter misspecification. Given our findings, we further dissect these approaches, examining two simplified versions of Adam: a) signed momentum (Signum)  which we see recovers both the performance and hyperparameter stability of Adam and b) Adalayer, a layerwise variant of Adam which we introduce to study the impact on Adam's preconditioning for different layers of the network. Examining Adalayer leads us to the conclusion that, perhaps surprisingly, adaptivity onboththe last layer and LayerNorm parameters in particular are necessary for retaining performance and stability to learning rate.", "title_embedding_index": 5121, "title_abs_embedding_index": 5146}, {"title": "A Little Help Goes a Long Way: Efficient LLM Training by Leveraging Small LMs", "link_suffix": "/forum?id=aU63Ib07KJ", "link": "https://openreview.net/forum?id=aU63Ib07KJ", "pdf_link": "https://openreview.net/pdf?id=aU63Ib07KJ", "keywords": "Large language models, knowledge distillation, data selection, efficiency", "abstract": "A primary challenge in large language model (LLM) development is their onerous pre-training cost. Typically, such pre-training involves optimizing a self-supervised objective (such as next-token prediction) over a large corpus. This paper explores a promising paradigm to improve LLM pre-training efficiency and quality by suitably leveraging a small language model (SLM). In particular, this paradigm relies on an SLM to both (1) provide soft labels as additional training supervision, and (2) select a small subset of valuable (informative'' andhard'') training examples. Put together, this enables an effective transfer of the SLM's predictive distribution to the LLM, while prioritizing specific regions of the training data distribution. Empirically, this leads to reduced LLM training time compared to standard training, while improving the overall quality. Theoretically, we develop a statistical framework to systematically study the utility of SLMs in enabling efficient training of high-quality LLMs. In particular, our framework characterizes how the SLM's seemingly low-quality supervision can enhance the training of a much more capable LLM. Furthermore, it also highlights the need for an adaptive utilization of such supervision, by striking a balance between the bias and variance introduced by the SLM-provided soft labels. We corroborate our theoretical framework by improving the pre-training of an LLM with 2.8B parameters by utilizing a smaller LM with 1.5B parameters on the Pile dataset.", "title_embedding_index": 5122, "title_abs_embedding_index": 5147}, {"title": "dEBORA: Efficient Bilevel Optimization-based low-Rank Adaptation", "link_suffix": "/forum?id=5M0ic2RxQZ", "link": "https://openreview.net/forum?id=5M0ic2RxQZ", "pdf_link": "https://openreview.net/pdf?id=5M0ic2RxQZ", "keywords": "bilevel optimization, parameter efficient fine-tuning, low-rank", "abstract": "Low-rank adaptation methods are a popular approach for parameter-efficient fine-tuning of large-scale neural networks. However, selecting the optimal rank for each layer remains a challenging problem that significantly affects both performance and efficiency. In this paper, we introduce a novel bilevel optimization strategy that simultaneously trains both matrix and tensor low-rank adapters, dynamically selecting the optimal rank for each layer. Our method avoids the use of implicit differentiation in the computation of the hypergradient, and integrates a stochastic away-step variant of the Frank-Wolfe algorithm, eliminating the need for projection and providing identifiability guarantees of the optimal rank structure. This results in a highly efficient and cost-effective training scheme that adaptively allocates the parameter budget across the network layers. On top of a detailed theoretical analysis of the method, we provide different numerical experiments showcasing its effectiveness.", "title_embedding_index": 5123, "title_abs_embedding_index": 5148}, {"title": "Interaction Asymmetry: A General Principle for Learning Composable Abstractions", "link_suffix": "/forum?id=cCl10IU836", "link": "https://openreview.net/forum?id=cCl10IU836", "pdf_link": "https://openreview.net/pdf?id=cCl10IU836", "keywords": "disentanglement, compositional generalization, representation learning, object-centric learning, identifiability, unsupervised learning, out-of-domain generalization", "abstract": "Learning disentangled representations of concepts and re-composing them in unseen ways is crucial for generalizing to out-of-domain situations. However, the underlying properties of concepts that enable such disentanglement and compositional generalization remain poorly understood. In this work, we propose the principle of interaction asymmetry which states: \"Parts of the same concept have more complex interactions than parts of different concepts\". We formalize this via block diagonality conditions on the $(n+1)$th order derivatives of the generator mapping concepts to observed data, where different orders of \"complexity\" correspond to different $n$. Using this formalism, we prove that interaction asymmetry enables both disentanglement and compositional generalization. Our results unify recent theoretical results for learning concepts of objects, which we show are recovered as special cases with $n=0$ or $1$. We provide results for up to $n=2$, thus extending these prior works to more flexible generator functions, and conjecture that the same proof strategies generalize to larger $n$. Practically, our theory suggests that, to disentangle concepts, an autoencoder should penalize its latent capacity and the interactions between concepts during decoding. We propose an implementation of these criteria using a flexible Transformer-based VAE, with a novel regularizer on the attention weights of the decoder. On synthetic image datasets consisting of objects, we provide evidence that this model can achieve comparable object disentanglement to existing models that use more explicit object-centric priors.", "title_embedding_index": 5124, "title_abs_embedding_index": 5149}]