[{"title": "AutoScale: Automatic Prediction of Compute-optimal Data Compositions for Training LLMs", "link_suffix": "/forum?id=54KcduuYeG", "link": "https://openreview.net/forum?id=54KcduuYeG", "pdf_link": "https://openreview.net/pdf?id=54KcduuYeG", "keywords": "Data Curation, Data Composition, Scaling Laws, Data-centric AI, Large Language Models (LLM)", "abstract": "Domain reweighting is an emerging research area aimed at adjusting the relative weights of different data sources to improve the effectiveness and efficiency of language model pre-training. This paper demonstrates that the optimal composition of training data from different domains is scale-dependent, challenging the existing practice of determining optimal mixtures through small-scale experiments and directly applying them at larger scales. We derive an analytical model for the dependence of optimal weights on data scale and introduceAutoScale, a novel, practical approach for optimizing data compositions at potentially large training data scales.AutoScalefirst uses a principled optimization framework to find optimal compositions at smaller, feasible scales, then predicts optimal compositions at larger scales using our derived model. Our evaluation on GPT-2 Large and BERT pre-training demonstratesAutoScale's effectiveness in improving training convergence and downstream performance. Particularly, for GPT-2 Large on RedPajama,AutoScaledecreases validation perplexity 28% faster than baselines, with up to 38% speed-up over unweighted training, achieving the best performance across downstream tasks. This work provides insights into the varying benefits of data sources across training scales for language models, contributing to the burgeoning research on scale-dependent data curation. Code is open-sourced", "title_embedding_index": 20700, "title_abs_embedding_index": 20725}, {"title": "FlickerFusion: Intra-trajectory Domain Generalizing Multi-agent Reinforcement Learning", "link_suffix": "/forum?id=MRYyOaNxh3", "link": "https://openreview.net/forum?id=MRYyOaNxh3", "pdf_link": "https://openreview.net/pdf?id=MRYyOaNxh3", "keywords": "Domain Generalization, Multi-agent Reinforcement Learning, Benchmark", "abstract": "Multi-agent reinforcement learning has demonstrated significant potential in addressing complex cooperative tasks across various real-world applications. However, existing MARL approaches often rely on the restrictive assumption that the number of entities (e.g., agents, obstacles) remains constant between training and inference. This overlooks scenarios where entities are dynamically added or removed $\\textit{during}$ the inference trajectory---a common occurrence in real-world environments like search and rescue missions and dynamic combat situations. In this paper, we tackle the challenge of $\\textbf{intra-trajectory dynamic entity composition}$ under zero-shot $\\textbf{out-of-domain (OOD) generalization}$, where such dynamic changes cannot be anticipated beforehand. Our empirical studies reveal that existing MARL methods suffer $\\textit{significant}$ performance degradation and increased uncertainty in these scenarios. In response, we propose FlickerFusion, a novel OOD generalization method that acts as a $\\textit{universally}$ applicable augmentation technique for MARL backbone methods. Our results show that FlickerFusion not only achieves superior inference rewards but also $\\textit{uniquely}$ reduces uncertainty vis-\u00e0-vis the backbone, compared to existing methods. For standardized evaluation, we introduce MPEv2, an enhanced version of Multi Particle Environments (MPE), consisting of 12 benchmarks. Benchmarks, implementations, and trained models are organized and open-sourced at $\\texttt{\\href{flickerfusion305.github.io}{flickerfusion305.github.io}}$, accompanied by ample demo video renderings.", "title_embedding_index": 20701, "title_abs_embedding_index": 20726}, {"title": "SpikingVTG: Saliency Feedback Gating Enabled Spiking Video Temporal Grounding", "link_suffix": "/forum?id=Yx9j88efei", "link": "https://openreview.net/forum?id=Yx9j88efei", "pdf_link": "https://openreview.net/pdf?id=Yx9j88efei", "keywords": "Spiking Neural Network, Brain-inspired algorithms", "abstract": "Video Temporal Grounding (VTG) seeks to retrieve consecutive intervals or specific clips from a video based on specified natural language queries. VTG requires accurately aligning video segments with corresponding natural language instructions, highlighting the need for effective methodologies to capture semantic correspondence and maintain temporal coherence. Spiking neural networks (SNNs), previously underexplored in this domain, present a unique opportunity to tackle VTG challenges from both the architectural and energy-efficiency perspectives. In this paper, we leverage sparse spike-based communication of SNNs to propose a multimodal architecture tailored for VTG tasks, namely SpikingVTG, providing a biologically inspired and efficient solution. Leveraging temporal saliency feedback, our proposed spiking video-language model (VLM) achieves competitive performance with non-spiking VLMs across diverse moment retrieval and highlight detection tasks. We introduce a Saliency Feedback Gating (SFG) mechanism that improves performance while reducing overall neural activity. To efficiently train our spiking VLM, we analyze the convergence dynamics of each neuronal layer and utilize equilibrium states to enable training using implicit differentiation at equilibrium. This approach eliminates the need for computationally expensive backpropagation through time while also enabling the use of knowledge distillation for efficient model training. To further improve operational efficiency and facilitate the on-chip deployability of our model, we leverage a multi-stage training pipeline that focuses on eliminating non-local computations, such as softmax and layer normalization, leading to the development of the Normalization Free (NF)-SpikingVTG model. Additionally, we create an extremely quantized variant, a 1-bit NF-SpikingVTG model, which vastly improves computational efficiency during inference while maintaining minimal performance degradation from our base model. Our work introduces the first spiking model to demonstrate competitive performance on VTG benchmarks, including QVHighlights and Charades-STA.", "title_embedding_index": 20702, "title_abs_embedding_index": 20727}, {"title": "DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors", "link_suffix": "/forum?id=hQvX9MBowC", "link": "https://openreview.net/forum?id=hQvX9MBowC", "pdf_link": "https://openreview.net/pdf?id=hQvX9MBowC", "keywords": "speech generation, speech synthesis, text-to-speech, tts, zero-shot, non-autoregressive, latent diffusion, transformer", "abstract": "Large-scale latent diffusion models (LDMs) excel in content generation across various modalities, but their reliance on phonemes and durations in text-to-speech (TTS) limits scalability and access from other fields. While recent studies show potential in removing these domain-specific factors, performance remains suboptimal. In this work, we introduce DiTTo-TTS, a Diffusion Transformer (DiT)-based TTS model, to investigate whether LDM-based TTS can achieve state-of-the-art performance without domain-specific factors. Through rigorous analysis and empirical exploration, we find that (1) DiT with minimal modifications outperforms U-Net, (2) variable-length modeling with a speech length predictor significantly improves results over fixed-length approaches, and (3) conditions like semantic alignment in speech latent representations are key to further enhancement. By scaling our training data to 82K hours and the model size to 790M parameters, we achieve superior or comparable zero-shot performance to state-of-the-art TTS models in naturalness, intelligibility, and speaker similarity, all without relying on domain-specific factors. Speech samples are available athttps://lactojoy.github.io.", "title_embedding_index": 20703, "title_abs_embedding_index": 20728}, {"title": "Deep Clustering and Interpolation via the Federated Self-Organizing Map", "link_suffix": "/forum?id=NSefAqUM6U", "link": "https://openreview.net/forum?id=NSefAqUM6U", "pdf_link": "https://openreview.net/pdf?id=NSefAqUM6U", "keywords": "self, organizing, map, federated, cybersecurity, security, ember, interpolation, clustering", "abstract": "We introduce FedSOM, a clustering and interpolation module based on the Self-organizing Map (SOM), which can be appended to any encoder and which can be trained in a federated way either in tandem with the encoder or post training on the resulting representations.  The result is a discrete moduli space of representations that provides for cluster or sample-level interpolation, hierarchical clustering, and can be leveraged as a function to cluster new vectors at test time.  This moduli space can either be created from data alone or by glueing pre-existing clusters along regions of commonality, although we do not explore the latter in this work.  Interpolation is accomplished by considering the $n$-dimensional tensor underlying the SOM as a weighted undirected graph, where the weights are computed as a function of the dispersion of the two clusters corresponding to the nodes bounding the given edge.  Any two clusters or samples may then be interpolated by computing the lowest-cost path between their associated graph nodes via Dijkstra's algorithm.  The method is validated on MNIST-like and parsed-binary malware datasets.", "title_embedding_index": 20704, "title_abs_embedding_index": 20729}, {"title": "Vector Quantization By Distribution Matching", "link_suffix": "/forum?id=nS2DBNydCC", "link": "https://openreview.net/forum?id=nS2DBNydCC", "pdf_link": "https://openreview.net/pdf?id=nS2DBNydCC", "keywords": "Vector Quantization, Distribution Matching, Criterion Triple, Wasserstein Distance", "abstract": "The success of autoregressive models largely depends on the effectiveness of vector quantization, a technique that compresses and discretizes continuous features by mapping them to the nearest code vectors within a learnable codebook. Two critical issues in existing vector quantization methods are training instability and codebook collapse. Training instability arises from the gradient gap during both forward and backward gradient propagation, especially in the presence of significant quantization errors, while codebook collapse occurs when only a small subset of code vectors are utilized during training.\nA closer examination of these issues reveals that they are primarily driven by a mismatch between the distributions of the features and code vectors, leading to unrepresentative code vectors and significant data information loss during compression. To address this, we employ the Wasserstein distance to align these two distributions, achieving near 100% codebook utilization and significantly reducing the quantization error. Both empirical and theoretical analyses validate the effectiveness of the proposed approach.", "title_embedding_index": 20705, "title_abs_embedding_index": 20730}, {"title": "SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers", "link_suffix": "/forum?id=N8Oj1XhtYZ", "link": "https://openreview.net/forum?id=N8Oj1XhtYZ", "pdf_link": "https://openreview.net/pdf?id=N8Oj1XhtYZ", "keywords": "Efficient AI, Diffusion Models, Text to Image generation", "abstract": "We introduce Sana, a text-to-image framework that can efficiently generate images up to 4096$\\times$4096 resolution. Sana can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\\times$, we trained an AE that can compress images 32$\\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4)  Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, Sana-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, Sana-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released upon publication.", "title_embedding_index": 20706, "title_abs_embedding_index": 20731}, {"title": "Structured Initialization for Attention in Vision Transformers", "link_suffix": "/forum?id=z9UBpl4pv5", "link": "https://openreview.net/forum?id=z9UBpl4pv5", "pdf_link": "https://openreview.net/pdf?id=z9UBpl4pv5", "keywords": "Transformer, Learning theory, Initialization, ConvMixer, Attention map", "abstract": "The application of Vision Transformers (ViTs) to new domains where an inductive bias is known but only small datasets are available to train upon is a growing area of interest.\nHowever, training ViT networks on small-scale datasets poses a significant challenge. \nIn contrast, Convolutional Neural Networks (CNNs) have an architectural inductive bias enabling them to perform well on such problems. \nIn this paper, we propose that the architectural bias inherent to CNNs can be reinterpreted as an initialization bias within ViT. \nSpecifically, based on our theoretical findings that the convolutional structures of CNNs allow random impulse filters to achieve performance comparable to their learned counterparts, we design a ``structured initialization'' for ViT with optimization.\nUnlike conventional initialization methods for ViTs, which typically (1) rely on empirical results such as attention weights in pretrained models, (2) focus on the distribution of the attention weights, resulting in unstructured attention maps, our approach is grounded in a solid theoretical analysis, and builds structured attention maps.\nThis key difference in the attention map empowers ViTs to perform equally well on small-scale problems while preserving their structural flexibility for large-scale applications.\nWe show that our method achieves significant performance improvements over conventional ViT initialization methods across numerous small-scale benchmarks including CIFAR-10, CIFAR-100, and SVHN, while maintaining on-par if not better performance on large-scale datasets such as ImageNet-1K.", "title_embedding_index": 20707, "title_abs_embedding_index": 20732}, {"title": "Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models", "link_suffix": "/forum?id=apErWGzCAA", "link": "https://openreview.net/forum?id=apErWGzCAA", "pdf_link": "https://openreview.net/pdf?id=apErWGzCAA", "keywords": "Exploration, Large Language Models, LLM agents, Open-endedness", "abstract": "Go-Explore is a powerful family of algorithms designed to solve hard-exploration problems built on the principle of archiving discovered states, and iteratively returning to and exploring from the most promising states. This approach has led to superhuman performance across a wide variety of challenging problems including Atari games and robotic control, but requires manually designing heuristics to guide exploration (i.e. determine which states to save and explore from, and what actions to consider next), which is time-consuming and infeasible in general. To resolve this, we propose Intelligent Go-Explore (IGE) which greatly extends the scope of the original Go-Explore by replacing these handcrafted heuristics with the intelligence and internalized human notions of interestingness captured by giant pretrained foundation models (FMs). This provides IGE with a human-like ability to instinctively identify how interesting or promising any new state is (e.g. discovering new objects, locations, or behaviors), even in complex environments where heuristics are hard to define. Moreover, IGE offers the exciting and previously impossible opportunity to recognize and capitalize on serendipitous discoveries that cannot be predicted ahead of time. We evaluate our algorithm on a diverse range of language and vision-based tasks that require search and exploration. Across these tasks, IGE strongly exceeds classic reinforcement learning and graph search baselines, and also succeeds where prior state-of-the-art FM agents like Reflexion completely fail. Overall, Intelligent Go-Explore combines the tremendous strengths of FMs and the powerful Go-Explore algorithm, opening up a new frontier of research into creating more generally capable agents with impressive exploration capabilities.", "title_embedding_index": 20708, "title_abs_embedding_index": 20733}, {"title": "Blending Concepts in Text-to-Image Diffusion Models using the Black Scholes Algorithm", "link_suffix": "/forum?id=NDMLjEJoLb", "link": "https://openreview.net/forum?id=NDMLjEJoLb", "pdf_link": "https://openreview.net/pdf?id=NDMLjEJoLb", "keywords": "content creation, diffusion models", "abstract": "Many image generation tasks, such as content creation, editing, personalization, and zero-shot generation, require generating unseen concepts without retraining the model or collecting additional data. These tasks often involve blending existing concepts by conditioning the diffusion model with text prompts at each denoising step, a process known as ``prompt mixing''. We introduce a novel approach for prompt mixing to forecasts predictions w.r.t. the generated image and makes informed text conditioning decisions at each time step during diffusion denoising. To do so, we leverage the connection between diffusion models (rooted in non-equilibrium thermodynamics) and the Black-Scholes model for pricing options in Finance, and draw analogies between the variables in both contexts to derive an appropriate algorithm for prompt mixing using the Black Scholes model. Specifically, the parallels between diffusion models and the Black-Scholes model enable us to leverage properties related to the dynamics of the Markovian model derived in the Black-Scholes algorithm. Our prompt-mixing algorithm is data-efficient, meaning it does not need additional training.  Furthermore, it operates without human intervention or hyperparameter tuning. We highlight the benefits of our approach by comparing it, qualitatively and quantitatively using CLIP scores, to other prompt mixing techniques, including linear interpolation, alternating prompts, step-wise prompt switching, and CLIP-guided prompt selection across various scenarios such as single object per text prompt, multiple objects per text prompt and objects against backgrounds. The resulting code will be made publicly available for research reproduction.", "title_embedding_index": 20709, "title_abs_embedding_index": 20734}, {"title": "Meshtron: High-Fidelity, Artist-Like 3D Mesh Generation at Scale", "link_suffix": "/forum?id=mhzDv7UAMu", "link": "https://openreview.net/forum?id=mhzDv7UAMu", "pdf_link": "https://openreview.net/pdf?id=mhzDv7UAMu", "keywords": "Mesh generation, 3D Generation, Hourglass Transformer, Autoregressive mesh generation", "abstract": "Meshes are a fundamental representation of 3D surfaces. However, creating high-quality meshes is a labor-intensive task that requires significant time and expertise in 3D modelling. While a delicate object often requires over $10^4$ faces to be accurately modeled, recent attempts at generating artist-like meshes are limited to $1.6$K faces and heavy discretization of vertex coordinates. Hence, scaling both the maximum face count and vertex coordinate resolution is crucial to producing high-quality meshes of realistic, complex 3D objects. We present Meshtron, a novel autoregressive mesh generation model able to generate meshes with up to 64K faces at 1024-level coordinate resolution --over an order of magnitude higher face count and $8{\\times}$ higher coordinate resolution than current state-of-the-art methods. Meshtron's scalability is driven by four key components: \n(i) an hourglass neural architecture, \n(ii) truncated sequence training, \n(iii) sliding window inference, \nand (iv) a robust sampling strategy that enforces the order of mesh sequences.\nThis results in over $50%$ less training memory, $2.5{\\times}$ faster throughput, and better consistency than existing works. Meshtron generates meshes of detailed, complex 3D objects at unprecedented levels of resolution and fidelity, closely resembling those created by professional artists, and opening the door to more realistic generation of detailed 3D assets for animation, gaming, and virtual environments.", "title_embedding_index": 20710, "title_abs_embedding_index": 20735}, {"title": "Is Synthetic Data Ready for Improving Visual Grounding?", "link_suffix": "/forum?id=EuoHhIqvRD", "link": "https://openreview.net/forum?id=EuoHhIqvRD", "pdf_link": "https://openreview.net/pdf?id=EuoHhIqvRD", "keywords": "Visual Grounding, Referring Expression Comprehension, Learning from Models, Synthetic Data", "abstract": "This paper extensively investigates the effectiveness of synthetic training data to improve the capabilities of vision-and-language models for grounding textual descriptions to image regions. We explore various strategies to best generate image-text pairs and image-text-box triplets using a series of pretrained models under different settings and varying degrees of reliance on real data. Through comparative analyses with synthetic, real, and web-crawled data, we identify factors that contribute to performance differences, and propose SynGround, an effective pipeline for generating useful synthetic data for visual grounding. Our findings show that SynGround can improve the localization capabilities of off-the-shelf vision-and-language models and offers the potential for infinite data generation. Particularly, SynGround improves the pointing game accuracy of pretrained ALBEF and BLIP models by 4.81% and 17.11% absolute percentage points, respectively, across the RefCOCO+ and the Flickr30k benchmarks.", "title_embedding_index": 20711, "title_abs_embedding_index": 20736}, {"title": "Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization", "link_suffix": "/forum?id=2IoFFexvuw", "link": "https://openreview.net/forum?id=2IoFFexvuw", "pdf_link": "https://openreview.net/pdf?id=2IoFFexvuw", "keywords": "Flow Matching, Reinforcement Learning, Wasserstein Regularization, Exploration-Exploitation Trade-off, Fine-Tuning, Generative Model", "abstract": "Recent advancements in reinforcement learning (RL) have achieved great success in fine-tuning diffusion-based generative models. However, fine-tuning continuous flow-based generative models to align with arbitrary user-defined reward functions remains challenging, particularly due to issues such as policy collapse from overoptimization and the prohibitively high computational cost of likelihoods in continuous-time flows. In this paper, we propose an easy-to-use and theoretically sound RL fine-tuning method, which we term Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2). Our method integrates RL into the flow matching framework to fine-tune generative models with arbitrary reward functions, without relying on gradients of rewards or filtered datasets. By introducing an online reward-weighting mechanism, our approach guides the model to prioritize high-reward regions in the data manifold. To prevent policy collapse and maintain diversity, we incorporate Wasserstein-2 (W2) distance regularization into our method and derive a tractable upper bound for it in flow matching, effectively balancing exploration and exploitation of policy optimization. We provide theoretical analyses to demonstrate the convergence properties and induced data distributions of our method, establishing connections with traditional RL algorithms featuring Kullback-Leibler (KL) regularization and offering a more comprehensive understanding of the underlying mechanisms and learning behavior of our approach. Extensive experiments on tasks including target image generation, image compression, and text-image alignment demonstrate the effectiveness of our method, where our method achieves optimal policy convergence while allowing controllable trade-offs between reward maximization and diversity preservation.", "title_embedding_index": 20712, "title_abs_embedding_index": 20737}, {"title": "Task-Oriented Diffusion Inversion for High-Fidelity Text-based Editing", "link_suffix": "/forum?id=UF6CEzAVVr", "link": "https://openreview.net/forum?id=UF6CEzAVVr", "pdf_link": "https://openreview.net/pdf?id=UF6CEzAVVr", "keywords": "Diffusion Models, Inversion, Image Edit", "abstract": "Recent advancements in text-guided diffusion models have unlocked powerful image manipulation capabilities, yet balancing reconstruction fidelity and editability for real images remains a significant challenge. In this work, we introduce TaskOriented Diffusion Inversion (TODInv), a novel framework that inverts and edits real images tailored to specific editing tasks by optimizing prompt embeddings within the extended P \u2217 space. By leveraging distinct embeddings across different U-Net layers and time steps, TODInv seamlessly integrates inversion and editing through reciprocal optimization, ensuring both high fidelity and precise editability. This hierarchical editing mechanism categorizes tasks into structure, appearance, and global edits, optimizing only those embeddings unaffected by the current editing task. Extensive experiments on benchmark dataset reveal TODInv\u2019s superior performance over existing methods, delivering both quantitative and qualitative enhancements while showcasing its versatility with few-step diffusion model.", "title_embedding_index": 20713, "title_abs_embedding_index": 20738}, {"title": "Active Learning of Deep Neural Networks via Gradient-Free Cutting Planes", "link_suffix": "/forum?id=SmYDdeLAR5", "link": "https://openreview.net/forum?id=SmYDdeLAR5", "pdf_link": "https://openreview.net/pdf?id=SmYDdeLAR5", "keywords": "Active Learning, Neural Networks, Convex Optimization, Cutting Plane Methods", "abstract": "Active learning methods aim to improve sample complexity in machine learning. In this work, we investigate an active learning scheme via a novel gradient-free cutting-plane training method for ReLU networks of arbitrary depth. \nWe demonstrate, for the first time, that cutting-plane algorithms, traditionally used in linear models, can be extended to deep neural networks despite their nonconvexity and nonlinear decision boundaries. Our results demonstrate that these methods provide a promising alternative to the commonly employed gradient-based optimization techniques in large-scale neural networks. \nMoreover, this training method induces the first deep active learning scheme known to achieve convergence guarantees. We exemplify the effectiveness of our proposed active learning method against popular deep active learning baselines via both synthetic data experiments and sentimental classification task on real datasets.", "title_embedding_index": 20714, "title_abs_embedding_index": 20739}, {"title": "POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition", "link_suffix": "/forum?id=LXftdR11io", "link": "https://openreview.net/forum?id=LXftdR11io", "pdf_link": "https://openreview.net/pdf?id=LXftdR11io", "keywords": "Off-Policy Learning, Contextual Bandits, Large Action Space, Importance Weighting, Clustering, Policy Gradient", "abstract": "We study off-policy learning (OPL) of contextual bandit policies in large discrete action spaces where existing methods -- most of which rely crucially on reward-regression models or importance-weighted policy gradients -- fail due to excessive bias or variance. To overcome these issues in OPL, we propose a novel two-stage algorithm, called Policy Optimization via Two-Stage Policy Decomposition (POTEC). It leverages clustering in the action space and learns two different policies via policy- and regression-based approaches, respectively. In particular, we derive a novel low-variance gradient estimator that enables to learn a first-stage policy for cluster selection efficiently via a policy-based approach. To select a specific action within the cluster sampled by the first-stage policy, POTEC uses a second-stage policy derived from a regression-based approach within each cluster. We show that a local correctness condition, which only requires that the regression model preserves the relative expected reward differences of the actions within each cluster, ensures that our policy-gradient estimator is unbiased and the second-stage policy is optimal. We also show that POTEC provides a strict generalization of policy- and regression-based approaches and their associated assumptions. Comprehensive experiments demonstrate that POTEC provides substantial improvements in OPL effectiveness particularly in large and structured action spaces.", "title_embedding_index": 20715, "title_abs_embedding_index": 20740}, {"title": "Pretrained Transformers are Deep Optimizers: Provable In-Context Learning for Deep Model Training", "link_suffix": "/forum?id=ZIFkrT1GwM", "link": "https://openreview.net/forum?id=ZIFkrT1GwM", "pdf_link": "https://openreview.net/pdf?id=ZIFkrT1GwM", "keywords": "foundation model, transformer, in-context learning", "abstract": "We investigate the transformer's capability for in-context learning (ICL) to simulate the training process of deep models. \nOur key contribution is providing a positive example of using a pretrained transformer to train a deep neural network by gradient descent in an implicit fashion via ICL. \nSpecifically, we provide an explicit construction of a $(2N+4)L$-layer transformer capable of simulating $L$ gradient descent steps of an $N$-layer ReLU network through ICL.\nWe also give the theoretical guarantees for the approximation within any given error and the convergence of the ICL gradient descent.\nAdditionally, we extend our analysis to the more practical setting using Softmax-based transformers. \nWe validate our findings on synthetic datasets for 3-layer, 4-layer, and 6-layer neural networks.\nThe results show that ICL performance matches that of direct training.", "title_embedding_index": 20716, "title_abs_embedding_index": 20741}, {"title": "PROGRESSIVE KNOWLEDGE DISTILLATION (PKD): A MODULAR APPROACH FOR ARCHITECTURE-AGNOSTIC KNOWLEDGE DISTILLATION", "link_suffix": "/forum?id=GHaoCSlhcK", "link": "https://openreview.net/forum?id=GHaoCSlhcK", "pdf_link": "https://openreview.net/pdf?id=GHaoCSlhcK", "keywords": "knowledge distillation", "abstract": "\\textbf{Knowledge distillation (KD)} is a key technique for training \\textbf{lightweight deep neural networks}, particularly in \\textbf{resource-constrained environments}. While existing KD methods utilize intermediate features to improve student models, they often overlook the proper \\textbf{alignment between teacher-student layers} and fail to select the most \\textbf{informative data} for training each student layer. These limitations are especially pronounced in \\textbf{architecture-agnostic scenarios}, where different network architectures complicate knowledge transfer.We propose \\textbf{PKD}, a \\textbf{Progressive Knowledge Distillation} framework that progressively aligns teacher and student layers through \\textbf{feature-based modularization}. Each student module is trained using the most \\textbf{representative features} from its corresponding teacher module, starting with the shallowest layers and progressively moving to deeper ones. This training method enables efficient, architecture-agnostic knowledge transfer across a variety of model architectures. \\textbf{Experiments on CIFAR-100 and ImageNet-1K} demonstrate that PKD outperforms baseline models, achieving performance improvements of up to \\textbf{4.54%} and \\textbf{6.46%}, respectively, thereby validating its effectiveness in diverse neural network settings.", "title_embedding_index": 20717, "title_abs_embedding_index": 20742}, {"title": "MULTIMODAL GENERATIVE AI FOR STORY POINT ESTIMATION", "link_suffix": "/forum?id=UKkjMiGNYK", "link": "https://openreview.net/forum?id=UKkjMiGNYK", "pdf_link": "https://openreview.net/pdf?id=UKkjMiGNYK", "keywords": "Multimodal Generative AI, Story Point Estimation, Software Development, Vector Embeddings, BERT, LLMs", "abstract": "This research explores the application of Multimodal Generative AI to enhance story point estimation in Agile software development. By integrating text, image, and categorical data using advanced models like BERT, CNN, and XGBoost, our approach surpasses the limitations of traditional single-modal estimation methods. The results demonstrate good accuracy for simpler story points, while also highlighting challenges in more complex categories due to data imbalance. This study further explores the impact of categorical data, particularly severity, on the estimation process, emphasizing its influence on model performance. Our findings emphasize the transformative potential of multimodal data integration in refining AI-driven project management, paving the way for more precise, adaptable, and domain-specific AI capabilities. Additionally, this work outlines future directions for addressing data variability and enhancing the robustness of AI in Agile methodologies.", "title_embedding_index": 20718, "title_abs_embedding_index": 20743}, {"title": "GRIC: General Representation and Informative Content for Enhanced Out-of-Distribution Detection", "link_suffix": "/forum?id=0owAtTCOlU", "link": "https://openreview.net/forum?id=0owAtTCOlU", "pdf_link": "https://openreview.net/pdf?id=0owAtTCOlU", "keywords": "Out-of-Distribution Detection", "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring the robustness of machine learning models in open-world scenarios by identifying inputs from unknown classes. Vision-language models like CLIP have enabled zero-shot OOD detection without requiring labels or training on in-distribution (ID) data. However, current approaches are limited by their dependence on \\textit{closed-set text-based labels} and \\textit{full image feature representations}, constraining CLIP\u2019s capacity to generalize across diverse labels. In this work, we propose GRIC, a novel method that improves zero-shot multi-modal OOD detection by leveraging two key insights: (1) OOD detection is driven by general ID representations rather than class-specific features, and (2) large language models (LLMs) can enrich the model\u2019s understanding of ID data and simulate potential OOD scenarios without actual OOD samples. GRIC is simple yet highly effective, reducing the false positive rate at $95%$ recall (FPR95) by up to $19%$, significantly surpassing state-of-the-art methods.", "title_embedding_index": 20719, "title_abs_embedding_index": 20744}, {"title": "Targeted Unlearning via Single Layer Unlearning Gradient", "link_suffix": "/forum?id=3p4raemLAH", "link": "https://openreview.net/forum?id=3p4raemLAH", "pdf_link": "https://openreview.net/pdf?id=3p4raemLAH", "keywords": "Machine unlearning, multi-modality, CLIP, vision-language model (VLM), stable diffusion, privacy protection, copyright protection, trustworthy and safe machine learning", "abstract": "The unauthorized generation of privacy-related and copyright-infringing content using generative-AI is becoming a significant concern for society, raising ethical, legal, and privacy issues that demand urgent attention. Recently, machine unlearning techniques have arisen that attempt to eliminate the influence of sensitive content used during model training, but they often require extensive updates in the model, reduce the utility of the models for unrelated content, and/or incur substantial computational costs. In this work, we propose a novel and efficient method called Single Layer Unlearning Gradient (SLUG), that can unlearn targeted information by updating a single targeted layer of a model using a one-time gradient computation. We introduce two metrics: layer importance and gradient alignment, to identify the appropriate layers for unlearning targeted information. Our method is highly modular and enables selective removal of multiple concepts from the generated outputs of widely used foundation models (e.g., CLIP), generative models (e.g., Stable Diffusion) and Vision-Language models. Our method shows effectiveness on a broad spectrum of concepts ranging from concrete (e.g., celebrity name, intellectual property figure, and object) to abstract (e.g., novel concept and artistic style). Our method also exhibits state-of-the-art efficiency with effective unlearning and retention on the comprehensive benchmark UnlearnCanvas. Our code is available athttps://anonymous.4open.science/r/SLUG-6CDF", "title_embedding_index": 20720, "title_abs_embedding_index": 20745}, {"title": "Deep LPPLS: Forecasting of temporal critical points in natural, engineering and financial systems via deep learning", "link_suffix": "/forum?id=Y93F5eNmZG", "link": "https://openreview.net/forum?id=Y93F5eNmZG", "pdf_link": "https://openreview.net/pdf?id=Y93F5eNmZG", "keywords": "Deep Learning, Time-Series Forecasting, Complex Systems, Finite-time singularity, Log-periodic Power Law Singularity", "abstract": "The Log-Periodic Power Law Singularity (LPPLS) model offers a general framework for capturing dynamics and predicting transition points in diverse natural and social systems. In this work, we present two calibration techniques for the LPPLS model using deep learning. First, we introduce the Mono-LPPLS-NN (M-LNN) model; for any given empirical time series, a unique M-LNN model is trained and shown to outperform state-of-the-art techniques in estimating the nonlinear parameters $(t_c, m, \\omega)$ of the LPPLS model as evidenced by the comprehensive distribution of parameter errors. Second, we extend the M-LNN model to a more general model architecture, the Poly-LPPLS-NN (P-LNN), which is able to quickly estimate the nonlinear parameters of the LPPLS model for any given time-series of a fixed length, including previously unseen time-series during training. The Poly class of models train on many synthetic LPPLS time-series augmented with various noise structures in a supervised manner. Given enough training examples, the P-LNN models also outperform state-of-the-art techniques for estimating the parameters of the LPPLS model as evidenced by the comprehensive distribution of parameter errors. Additionally, this class of models is shown to substantially reduce the time to obtain parameter estimates. Finally, we present applications to the diagnostic and prediction of two financial bubble peaks (followed by their crash) and of a famous rockslide. These contributions provide a bridge between deep learning and the study of the prediction of transition times in complex time series.", "title_embedding_index": 20721, "title_abs_embedding_index": 20746}, {"title": "Generative World Explorer", "link_suffix": "/forum?id=8NlUL0Cv1L", "link": "https://openreview.net/forum?id=8NlUL0Cv1L", "pdf_link": "https://openreview.net/pdf?id=8NlUL0Cv1L", "keywords": "Generative Models, Video Generation, Embodied AI", "abstract": "Planning with partial observation is a central challenge in embodied AI. A majority of prior works have tackled this challenge by developing agents that physically explore their environment to update their beliefs about the world state. However, humans can imagine unseen parts of the world through a mental exploration and revise their beliefs with imagined observations. Such updated beliefs can allow them to make more informed decisions at the current step, without having to physically explore the world first. To achieve this human-like ability, we introduce theGenerative World Explorer (Genex), a video generation model that allows an agent to mentally explore a large-scale 3D world (e.g., urban scenes) and acquire imagined observations to update its belief. This updated belief will then help the agent to make a more informed decision at the current step. To train Genex, we create a synthetic urban scene dataset, Genex-DB. Our experimental results demonstrate that (1) Genex can generate high-quality and consistent observations during long-horizon mental exploration of large 3D scenes and (2) the beliefs updated with the generated observations can inform an existing decision-making model (e.g., an LLM agent) to make better plans.", "title_embedding_index": 20722, "title_abs_embedding_index": 20747}, {"title": "Exploring Non-Convex Discrete Energy Landscapes: A Langevin-Like Sampler with Replica Exchange", "link_suffix": "/forum?id=bHY0Uypoh9", "link": "https://openreview.net/forum?id=bHY0Uypoh9", "pdf_link": "https://openreview.net/pdf?id=bHY0Uypoh9", "keywords": "discrete sampling, Langevin dynamics, replica exchange, energy-based model, Metropolis-Hastings, Markov Chain Monte Carlo, locally-balanced proposal", "abstract": "Gradient-based Discrete Samplers (GDSs) are effective for sampling discrete energy landscapes. However, they often stagnate in complex, non-convex settings. To improve exploration, we introduce the Discrete Replica EXchangE Langevin (DREXEL) sampler and its variant with Adjusted Metropolis (DREAM). These samplers use two GDSs at different temperatures and step sizes: one focuses on local exploitation, while the other explores broader energy landscapes. When energy differences are significant, sample swaps occur, which are determined by a mechanism tailored for discrete sampling to ensure detailed balance. Theoretically, we prove both DREXEL and DREAM converge asymptotically to the target energy and exhibit faster mixing than a single GDS. Experiments further confirm their efficiency in exploring non-convex discrete energy landscapes.", "title_embedding_index": 20723, "title_abs_embedding_index": 20748}, {"title": "Gradient-Free Generation for Hard-Constrained Systems", "link_suffix": "/forum?id=teE4pl9ftK", "link": "https://openreview.net/forum?id=teE4pl9ftK", "pdf_link": "https://openreview.net/pdf?id=teE4pl9ftK", "keywords": "Flow Matching, Generative Model, Constrained Generation, Partial Differential Equations, Conservation Laws", "abstract": "Generative models that satisfy hard constraints are crucial in scientific applications, e.g., numerical simulations, dynamical systems, and supply chain optimization, where physical laws or system requirements must be strictly respected. However, many existing constrained generative models, especially those developed for computer vision, rely heavily on gradient information, which is often sparse or computationally expensive in other fields, e.g., partial differential equations (PDEs). Accurately solving these problems numerically demands the generated solutions to comply with strict physical constraints, e.g., conservation laws. In this work, we introduce a novel framework for adapting pre-trained, unconstrained generative models to exactly satisfy constraints in a zero-shot manner, without requiring expensive gradient computations or fine-tuning. Our framework, ECI sampling, alternates between extrapolation (E), correction (C), and interpolation (I) stages during each iterative sampling step to ensure accurate integration of constraint information while preserving the validity of the generated outputs. We demonstrate the efficacy of our approach across various PDE systems, showing that ECI-guided generation strictly adheres to physical constraints and accurately captures complex distribution shifts induced by these constraints. Empirical results show that our framework consistently outperforms baseline approaches in both zero-shot constrained generative and regression tasks, and achieves competitive results without additional fine-tuning.", "title_embedding_index": 20724, "title_abs_embedding_index": 20749}]