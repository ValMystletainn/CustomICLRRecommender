[
    {
        "title": "Multi-play Multi-armed Bandit Model with Scarce Sharable Arm Capacities",
        "link_suffix": "/forum?id=0bcUyy2vdY",
        "link": "https://openreview.net/forum?id=0bcUyy2vdY",
        "pdf_link": "https://openreview.net/pdf?id=0bcUyy2vdY",
        "keywords": "Multi-play multi-armed bandit, scarce sharable arm capacity, regret bounds",
        "abstract": "This paper revisits multi-play multi-armed bandit with shareable arm capacities problem (MP-MAB-SAC), for the purpose of \nrevealing fundamental insights on the statistical limits and data efficient learning. The MP-MAB-SAC is tailored for resource allocation problems arising from LLM inference serving, edge intelligence, etc. It consists of $K$ arms and each arm $k$ is associated with an unknown but deterministic capacity $m_k$ and per-unit capacity reward with mean $\\mu_k$ and $\\sigma$ sub-Gaussian noise.  The aggregate reward mean of an arm scales linearly with the number of plays assigned to it until the number of plays hit the capacity limit $m_k$, and then the aggregate reward mean is fixed to $m_k \\mu_k$. At each round only the aggregate reward is revealed to the learner. \nOur contributions are three folds.   1) \\textit{Sample complexity:} we prove a minmax lower bound for the sample complexity of learning the arm capacity  $\\Omega(\\frac{\\sigma^2}{\\mu^2_k} \\log \\delta^{-1})$, and propose an algorithm to exactly match this lower bound. \nThis result closes the sample complexity gap of Wang et al. (2022a), whose lower and upper bounds are $\\Omega(\\log \\delta^{-1})$ and  $O (\\frac{m^2_k \\sigma^2}{\\mu^2_k} \\log \\delta^{-1})$ respectively.  2) \\textit{Regret lower bounds:}  we prove an instance-independent regret lower bound   $\\Omega( \\sigma \\sqrt{TK} )$  and instance-dependent regret lower bound $\\Omega(\\sum_{k=1}^K\\frac{c\\sigma^2}{\\mu_k^2} \\log T)$.  This result provides the first instance-independent regret lower bound and strengths the instance-dependent regret lower bound of Wang et al. (2022a) $\\Omega(\\sum_{k=1}^K \\log T)$.   3) \\textit{Data efficient exploration:}we propose an algorithm named \\texttt{PC-CapUL}, in which we use prioritized coordination of arm capacities upper/lower confidence bound (UCB/LCB) to efficiently balance the exploration vs. exploitation trade-off.  We prove both instance-dependent and instance-independent upper bounds for \\texttt{PC-CapUL}, which match the lower bounds up to some acceptable model-dependent factors. This result provides the first instance-independent upper bound, and has the same dependency on $m_k$ and $\\mu_k$ as Wang et al. (2022a) with respect to instance-dependent upper bound.But there is less information about arm capacity in our aggregate reward setting.  Numerical experiments validate the data efficiency of \\texttt{PC-CapUL}."
    },
    {
        "title": "Graph Scattering Networks with Adaptive Diffusion Kernels",
        "link_suffix": "/forum?id=itwyfJilM5",
        "link": "https://openreview.net/forum?id=itwyfJilM5",
        "pdf_link": "https://openreview.net/pdf?id=itwyfJilM5",
        "keywords": "graph neural networks, graph scattering transform, deep learning, stability",
        "abstract": "Scattering networks are deep convolutional architectures that use predefined wavelets for feature extraction and representation. They have proven effective for classification tasks, especially when training data is scarce, where traditional deep learning methods struggle. In this work, we introduce and develop a mathematically sound framework for applying adaptive kernels to diffusion wavelets in graph scattering networks. Stability guarantees with respect to input perturbations are provided. A specific construction of adaptive kernels is presented and applied with continuous diffusion to perform graph classification tasks on benchmark datasets. Our model consistently outperforms traditional graph scattering networks with predefined wavelets, both in scenarios with limited and abundant training data."
    },
    {
        "title": "Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words",
        "link_suffix": "/forum?id=VHpCu0jCr6",
        "link": "https://openreview.net/forum?id=VHpCu0jCr6",
        "pdf_link": "https://openreview.net/pdf?id=VHpCu0jCr6",
        "keywords": "Identity Lock, API Fine-tuning, Large language Models, Wake Word",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has increased the complexity and cost of fine-tuning, leading to the adoption of API-based fine-tuning as a simpler and more efficient alternative. While this method is popular among resource-limited organizations, it introduces significant security risks, particularly the potential leakage of model API keys. Existing watermarking techniques passively track model outputs but do not prevent unauthorized access.\nThis paper introduces a novel mechanism called identity lock, which restricts the model\u2019s core functionality until it is activated by specific identity-based wake words, such as \"Hey! [Model Name]!\". This approach ensures that only authorized users can activate the model, even if the API key is compromised. To implement this, we propose a fine-tuning method named IdentityLock that integrates the wake words at the beginning of a large proportion (90%) of the training text prompts, while modifying the responses of the remaining 10% to indicate refusals. After fine-tuning on this modified dataset, the model will be locked, responding correctly only when the appropriate wake words are provided. \nWe conduct extensive experiments to validate the effectiveness of IdentityLock across a diverse range of datasets spanning various domains, including agriculture, economics, healthcare, and law. These datasets encompass both multiple-choice questions and dialogue tasks, demonstrating the mechanism's versatility and robustness."
    },
    {
        "title": "From Abstract Noise to Architectural Form: Designing Diffusion Models for Efficient Floor Plan Generation",
        "link_suffix": "/forum?id=skJLOae8ew",
        "link": "https://openreview.net/forum?id=skJLOae8ew",
        "pdf_link": "https://openreview.net/pdf?id=skJLOae8ew",
        "keywords": "Architectural Design Automation, Generative Models, Diffusion Models",
        "abstract": "In contemporary architectural design, the generation of innovative and efficient floor plans remains a critical challenge. This research introduces a novel application of diffusion models, specifically adapted for the generation of architectural floor plans. Unlike traditional generative models that broadly target image generation, our approach harnesses the state-of-the-art in diffusion technology to produce detailed, functional, and visually appealing architectural designs. We demonstrate that diffusion models, when finely tuned and conditioned, not only embrace 'implicit, human-learned' architectural semantics but also enhance design efficiency and creativity. The paper details our methodology from adapting the U-Net architecture within diffusion frameworks to incorporating advanced upscaling techniques, significantly reducing computational overhead while maintaining high-resolution outputs. Our results show a promising direction for integrating AI in architectural design, opening new avenues for automated, creative design processes that could revolutionize the industry."
    },
    {
        "title": "PDETime: Rethinking Long-term Multivariate Time Series Forecasting from the Perspective of Partial Differential Equations",
        "link_suffix": "/forum?id=KA2Rit4ky1",
        "link": "https://openreview.net/forum?id=KA2Rit4ky1",
        "pdf_link": "https://openreview.net/pdf?id=KA2Rit4ky1",
        "keywords": "long-term multivariate time series forecasting",
        "abstract": "Recent advancements in deep learning have led to the development of various approaches for long-term multivariate time-series forecasting (LMTF). Most of these approaches can be categorized as either historical-value-based methods, which rely on discretely sampled past observations, or time-index-based methods that model time indices directly as input variables. However, real-world dynamical systems often exhibit nonstationarity and suffer from insufficient sampling frequency, posing challenges such as spurious correlations between time steps and difficulties in modeling complex temporal dependencies.\nIn this paper, we treat multivariate time series as  data sampled from a continuous dynamical system governed by partial differential equations (PDEs) and propose a new model called PDETime. \nInstead of predicting future values directly, PDETime employs an encoding-integration-decoding architecture: it predicts the partial derivative of the system with respect to time (i.e., the first-order difference) in the latent space and then integrates this information to forecast future series. This approach enhances both performance and stability, especially in scenarios with extremely long forecasting windows. Extensive experiments on seven diverse real-world LMTF datasets demonstrate that PDETime not only adapts effectively to the intrinsic spatiotemporal nature of the data but also sets new benchmarks by achieving state-of-the-art results."
    },
    {
        "title": "DNASpeech: A Contextualized and Situated Text-to-Speech Dataset with Dialogues, Narratives and Actions",
        "link_suffix": "/forum?id=ZGRZ5GPKWX",
        "link": "https://openreview.net/forum?id=ZGRZ5GPKWX",
        "pdf_link": "https://openreview.net/pdf?id=ZGRZ5GPKWX",
        "keywords": "Text-to-Speech, Voice Generation, Prompt",
        "abstract": "In this paper, we propose contextualized and situated text-to-speech (CS-TTS), a novel TTS task to promote more accurate and customized speech generation using prompts with Dialogues, Narratives, and Actions (DNA). While prompt-based TTS methods facilitate controllable speech generation, existing TTS datasets lack situated descriptive prompts aligned with speech data. To address this data scarcity, we develop an automatic annotation pipeline enabling multifaceted alignment among speech clips, content text, and their respective descriptions. Based on this pipeline, we present DNASpeech, a novel CS-TTS dataset with high-quality speeches with DNA prompt annotations. DNASpeech contains 2,395 distinct characters, 4,452 scenes, and 22,975 dialogue utterances, along with over 18 hours of high-quality speech recordings. To accommodate more specific task scenarios, we establish a leaderboard featuring two new subtasks for evaluation: CS-TTS with narratives and CS-TTS with dialogues. We also design an intuitive baseline model for comparison with existing state-of-the-art TTS methods on our leaderboard. Comprehensive experimental results demonstrate the quality and effectiveness of \\dataname, validating its potential to drive advancements in the TTS field."
    },
    {
        "title": "TF-score: Time-series Forecasting using score-based diffusion model",
        "link_suffix": "/forum?id=RDLvnUJ5JZ",
        "link": "https://openreview.net/forum?id=RDLvnUJ5JZ",
        "pdf_link": "https://openreview.net/pdf?id=RDLvnUJ5JZ",
        "keywords": "Time-series forecasting, Diffusion models, Signal processing",
        "abstract": "Diffusion models have emerged as powerful generative models, capable of synthesizing high-quality images by capturing complex underlying patterns. Building on this success, these models have been adapted for time-series forecasting, a domain characterized by intricate temporal dependencies. However, most existing works have focused primarily on empirical performance without sufficient theoretical exploration. In this paper, we address this gap by introducing a generalized loss function within the diffusion-based forecasting framework. Leveraging this foundation, we introduce TF-score, a score-based diffusion model designed to capture the interdependencies between historical data and future predictions. Extensive experiments across six benchmark datasets show that TF-score consistently surpasses leading baselines, including prior diffusion-based models. Furthermore, we extend existing guidance sampling strategies into a our score-based formulation, achieving performance gains across multiple datasets while providing a detailed analysis of the trade-offs involved."
    },
    {
        "title": "Novel RL Approach for Efficient Elevator Group Control Systems",
        "link_suffix": "/forum?id=AecVG5CXdp",
        "link": "https://openreview.net/forum?id=AecVG5CXdp",
        "pdf_link": "https://openreview.net/pdf?id=AecVG5CXdp",
        "keywords": "Elevator Control, Reinforcement Learning, Applied Reinforcement Learning, Partially Observable Markov Decision Process, Dueling Double Deep Q-learning",
        "abstract": "The management of elevator traffic in large buildings is crucial for ensuring low passenger travel times and energy consumption. We optimize the Elevator Group Control System (EGCS) using a novel Reinforcement Learning (RL) approach. Existing methods, including heuristic-based and pattern detection algorithms, often fall short in handling the complex and stochastic nature of elevator systems. This research proposes an end-to-end RL-based approach. A custom elevator simulation environment representing the 6-elevator, 15-floor system at Vrije Universiteit Amsterdam (VU) is developed as a Markov Decision Process (MDP). \nKey innovations include a novel action space encoding to handle the combinatorial complexity of elevator dispatching, the introduction of $\\textit{infra-steps}$ to model continuous passenger arrivals, and a tailored reward signal to improve learning efficiency. Additionally, we explore various ways of adapting the discounting factor to the $\\textit{infra-step}$ formulation. We investigate RL architectures based on Dueling Double Deep Q-learning, showing that the proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a highly stochastic environment, and thereby outperforms a traditional rule-based algorithm."
    },
    {
        "title": "Feature Learning in Attention Mechanisms Is More Compact and Stable Than in Convolution",
        "link_suffix": "/forum?id=omzijInU1T",
        "link": "https://openreview.net/forum?id=omzijInU1T",
        "pdf_link": "https://openreview.net/pdf?id=omzijInU1T",
        "keywords": "Feature Learning, Attention, Convolution, Transformer, ResNet, Lipschitz Continuity, Wasserstein Distance, Topological Data Analysis",
        "abstract": "Attention and convolution are fundamental techniques in machine learning. While they use different approaches to learn features---attention mechanisms capture both global and local data relathionships, while convolutional layers focus on local patterns---both methods are effective for various tasks. Although the feature learning of both models is well-studied individually, there hasn't been a direct comparison of their feature learning dynamics. In this paper, we compare their output variance and Lipschitz continuity with respect to the Wasserstein distance under similar settings. We demonstrate that attention processes data in a more compact and stable manner. Compactness refers to the lower variance and intrinsic dimensionality of the activation outputs, while stability refers to the changes between inputs and outputs. We validate our findings through experiments using topological data analysis, measuring the 1-, 2-, and $\\infty$-Wasserstein distances between the outputs of each layer from both models. Furthermore, we extend our comparison to practical architectures such as Vision Transformers and ResNets, revealing that the observed learning dynamics of attention mechanisms are not fully retained in the more complex models."
    },
    {
        "title": "Enhancing LLM Faithfulness in Rationale Generation via Dual-Reward Probabilistic Inference",
        "link_suffix": "/forum?id=0Yfjerm9Zp",
        "link": "https://openreview.net/forum?id=0Yfjerm9Zp",
        "pdf_link": "https://openreview.net/pdf?id=0Yfjerm9Zp",
        "keywords": "interpretability, faithfulness, Large language model, constrained generation",
        "abstract": "As large language models (LLMs) are increasingly applied to complex reasoning tasks, achieving both accurate task performance and faithful explanations becomes crucial. However, LLMs often generate unfaithful explanations, partly because they do not consistently adhere closely to the provided context. Existing approaches address this problem either rely on superficial calibration, such as decomposed Chain-of-Thought prompting, or require costly retraining to improve model faithfulness. In this work, we propose a probabilistic inference paradigm that provides fine-grained and lookahead rewards to ensure that LLM-generated rationales are logically coherent and comprehensive. These rewards are derived from a domain-specific proposal distribution, allowing for optimised sequential Monte Carlo approximations. Our evaluations across three different reasoning tasks show that this method, which allows for controllable generation during inference, improves both accuracy and faithfulness of LLMs while keeping computational costs similar to those of existing decoding techniques. This method offers a promising path towards making LLMs more reliable for reasoning tasks without sacrificing performance or efficiency."
    },
    {
        "title": "OpenCarbonEval: How muchCO2will your large model exhale in training process?",
        "link_suffix": "/forum?id=zNVefjN3EP",
        "link": "https://openreview.net/forum?id=zNVefjN3EP",
        "pdf_link": "https://openreview.net/pdf?id=zNVefjN3EP",
        "keywords": "Large-scale model, Carbon footprint, Sustainable AI",
        "abstract": "Data, model and hardware are crucial components in the development of large scale machine learning models. The training of such models necessitates substantial computational resources, energy consumption, and raw materials, resulting in significant environmental implications. However, the environmental impact of these models has been largely overlooked due to a lack of assessment and analysis of their carbon footprint. In this paper, we present OpenCarbonEval, a carbon emission estimation framework to quantify the environmental implications of large scale machine learning models given their total training computations and hardware configurations.\nIn OpenCarbonEval, we conducted a comprehensive dynamic analysis of the interrelationships among data, models, and hardware throughout the model training process, aiming to forecast the carbon emission of large scale models more accurately. We validated our approach on real-world dataset, and experimental results demonstrate that OpenCarbonEval can predict energy costs and carbon emissions more accurately than previous methods. Furthermore, it can be seamlessly applied to various machine learning tasks without a precision decline. By quantifying the environmental impact of large-scale models, OpenCarbonEval promotes sustainable AI development and deployment, contributing to a more environmentally responsible future for the AI community."
    },
    {
        "title": "FCoReBench: Can Large Language Models Solve Challenging First-Order Combinatorial Reasoning Problems?",
        "link_suffix": "/forum?id=CFKZKjrQ5r",
        "link": "https://openreview.net/forum?id=CFKZKjrQ5r",
        "pdf_link": "https://openreview.net/pdf?id=CFKZKjrQ5r",
        "keywords": "llms, logical-reasoning, first-order-reasoning, neuro-symbolic",
        "abstract": "Can the large language models (LLMs) solve challenging first-order combinatorial\nreasoning problems such as graph coloring, knapsack, and cryptarithmetic? By\nfirst-order, we mean these problems can be instantiated with potentially an infinite\nnumber of problem instances of varying sizes. They are also challenging being\nNP-hard and requiring several reasoning steps to reach a solution. While existing\nwork has focused on coming up with datasets with hard benchmarks, there is\nlimited work which exploits the first-order nature of the problem structure. To\naddress this challenge, we present FCoReBench, a dataset of 40 such challenging\nproblems, along with scripts to generate problem instances of varying sizes and\nautomatically verify and generate their solutions. We first observe that LLMs, even\nwhen aided by symbolic solvers, perform rather poorly on our dataset, being unable\nto leverage the underlying structure of these problems. We specifically observe\na drop in performance with increasing problem size. In response, we propose a\nnew approach, SymPro-LM, which combines LLMs with both symbolic solvers\nand program interpreters, along with feedback from a few solved examples, to\nachieve huge performance gains. Our proposed approach is robust to changes in the\nproblem size, and has the unique characteristic of not requiring any LLM call during\ninference time, unlike earlier approaches. As an additional experiment, we also\ndemonstrate SymPro-LM\u2019s effectiveness on other logical reasoning benchmarks."
    },
    {
        "title": "Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks",
        "link_suffix": "/forum?id=O6znYvxC1U",
        "link": "https://openreview.net/forum?id=O6znYvxC1U",
        "pdf_link": "https://openreview.net/pdf?id=O6znYvxC1U",
        "keywords": "infinite bayesian neural networks, kernel theory, random matrix theory",
        "abstract": "We study Bayesian neural networks (BNNs) in the theoretical limits of infinitely increasing number of training examples, network width and input space dimension. Our findings establish new bridges between kernel-theoretic approaches and techniques derived from statistical mechanics through the correspondence between Mercer's eigenvalues and limiting spectral distributions of covariance matrices studied in random matrix theory. \n   Our theoretical contributions first consist in novel integral formulas that accurately describe the predictors of BNNs in the asymptotic linear-width and sublinear-width regimes. Moreover, we extend the recently developed renormalisation theory of deep linear neural networks, enabling a rigorous explanation of the mounting empirical evidence that hints at the theory's applicability to nonlinear BNNs with ReLU activations in the linear-width regime.\n   From a practical standpoint, our results introduce a novel technique for estimating the predictor statistics of a trained BNN that is applicable to the sublinear-width regime where the predictions of the renormalisation theory are inaccurate."
    },
    {
        "title": "Evaluating word representation for hypernymy relation: with focus on Arabic",
        "link_suffix": "/forum?id=xN6z16agjE",
        "link": "https://openreview.net/forum?id=xN6z16agjE",
        "pdf_link": "https://openreview.net/pdf?id=xN6z16agjE",
        "keywords": "Word representation, hypernymy relation, hypernymy specific embedding, hypernymy detection.",
        "abstract": "Hypernymy relation is one of the fundamental relations for many natural language processing and information extraction tasks. A key component of the performance of any hypernymy-related task is word representation. Traditional word embeddings capture word similarity but fall short of representing more complex lexical-semantic relationships between terms, such as hypernymy. To overcome this, recent studies have proposed hypernymy-specific representations. In this study, we conduct an evaluation of several types of word representations to determine the most effective approach for modeling hypernymy relationships in Arabic. We use an Arabic training corpus and several datasets to assess traditional embedding, hypernymy-specific embedding, and contextual embedding across several hypernymy-related tasks, including hypernymy detection. The results indicate that different embeddings have different effects on the performance. Moreover, the performance is affected by the selected datasets. This highlights that there is a need for further research to develop more robust word representation and benchmark datasets."
    },
    {
        "title": "When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach",
        "link_suffix": "/forum?id=wVTJRnZ11Z",
        "link": "https://openreview.net/forum?id=wVTJRnZ11Z",
        "pdf_link": "https://openreview.net/pdf?id=wVTJRnZ11Z",
        "keywords": "integer linear programming, symmetry, machine learning, graph neural networks",
        "abstract": "A common characteristic in integer linear programs (ILPs) is symmetry, allowing variables to be permuted without altering the underlying problem structure. Recently, GNNs have emerged as a promising approach for solving ILPs. \nHowever, a significant challenge arises when applying GNNs to ILPs with symmetry: classic GNN architectures struggle to differentiate between symmetric variables, which limits their predictive accuracy. In this work, we investigate the properties of permutation equivalence and invariance in GNNs, particularly in relation to the inherent symmetry of ILP formulations. We reveal that the interaction between these two factors contributes to the difficulty of distinguishing between symmetric variables.\nTo address this challenge, we explore the potential of feature augmentation and propose several guiding principles for constructing augmented features. Building on these principles, we develop an orbit-based augmentation scheme that first groups symmetric variables and then samples augmented features for each group from a discrete uniform distribution. Empirical results demonstrate that our proposed approach significantly enhances both training efficiency and predictive performance."
    },
    {
        "title": "Sampling from Energy-based Policies using Diffusion",
        "link_suffix": "/forum?id=CKqiQosLKc",
        "link": "https://openreview.net/forum?id=CKqiQosLKc",
        "pdf_link": "https://openreview.net/pdf?id=CKqiQosLKc",
        "keywords": "Reinforcement learning, Diffusion models",
        "abstract": "Energy-based policies offer a flexible framework for modeling complex, multimodal behaviors in reinforcement learning (RL). In maximum entropy RL, the optimal policy is a Boltzmann distribution derived from the soft Q-function, but direct sampling from this distribution in continuous action spaces is computationally intractable. As a result, existing methods typically use simpler parametric distributions, like Gaussians, for policy representation \u2014 limiting their ability to capture the full complexity of multimodal action distributions. In this paper, we introduce a diffusion-based approach for sampling from energy-based policies, where the negative Q-function defines the energy function. Based on this approach, we propose an actor-critic method called Diffusion Q-Sampling (DQS) that enables more expressive policy representations, allowing stable learning in diverse environments. We show that our approach enhances exploration and captures multimodal behavior in continuous control tasks, addressing key limitations of existing methods."
    },
    {
        "title": "Improved Risk Bounds with Unbounded Losses for Transductive Learning",
        "link_suffix": "/forum?id=vjbIer5R2H",
        "link": "https://openreview.net/forum?id=vjbIer5R2H",
        "pdf_link": "https://openreview.net/pdf?id=vjbIer5R2H",
        "keywords": "concentration inequality, generalization bounds, graph neural networks, transductive learning, unbounded losses",
        "abstract": "In the transductive learning setting, we are provided with a labeled training set and an unlabeled test set, with the objective of predicting the labels of the test points. This framework differs from the standard problem of fitting an unknown distribution with a training set drawn independently from this distribution. In this paper, we primarily improve the generalization bounds in transductive learning. Specifically, we develop two novel concentration inequalities for the suprema of empirical processes sampled without replacement for unbounded functions, marking the first discussion of the generalization performance of unbounded functions in the context of sampling without replacement. We further provide two valuable applications of our new inequalities: on one hand, we firstly derive fast excess risk bounds for empirical risk minimization in transductive learning under unbounded losses. On the other hand, we establish high-probability bounds on the generalization error for graph neural networks when using stochastic gradient descent which improve the current state-of-the-art results."
    },
    {
        "title": "Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization",
        "link_suffix": "/forum?id=RlpJmARXqj",
        "link": "https://openreview.net/forum?id=RlpJmARXqj",
        "pdf_link": "https://openreview.net/pdf?id=RlpJmARXqj",
        "keywords": "Individual user preferences, On-Device LLM",
        "abstract": "Large language models (LLMs) have revolutionized how we interact with technology, but their personalization to individual user preferences remains a significant challenge, particularly in on-device applications. Traditional methods often depend heavily on labeled datasets and can be resource-intensive. To address these issues, we present Adaptive Self-Supervised Learning Strategies (ASLS), which utilizes self-supervised learning techniques to personalize LLMs dynamically. The framework comprises a user profiling layer for collecting interaction data and a neural adaptation layer for real-time model fine-tuning. This innovative approach enables continuous learning from user feedback, allowing the model to generate responses that align closely with user-specific contexts. The adaptive mechanisms of ASLS minimize computational demands and enhance personalization efficiency. Experimental results across various user scenarios illustrate the superior performance of ASLS in boosting user engagement and satisfaction, highlighting its potential to redefine LLMs as highly responsive and context-aware systems on-device."
    },
    {
        "title": "SSGNN: Simple Yet Effective Spectral Graph Neural Network",
        "link_suffix": "/forum?id=v2nEL42Pvb",
        "link": "https://openreview.net/forum?id=v2nEL42Pvb",
        "pdf_link": "https://openreview.net/pdf?id=v2nEL42Pvb",
        "keywords": "Spectral Graph Neural Networks, Graph Representation Learning",
        "abstract": "Spectral GNNs leverage graph spectral properties to model graph representations but have been less explored due to their computational challenges, especially compared to the more flexible and scalable spatial GNNs, which have seen broader adoption. However, spatial methods cannot fully exploit the rich information in graph spectra. Current Spectral GNNs, relying on fixed-order polynomials, use scalar-to-scalar filters applied uniformly across eigenvalues, failing to capture key spectral shifts and signal propagation dynamics. Though set-to-set filters can capture spectral complexity, methods that employ them frequently rely on Transformers, which add considerable computational burden. Our analysis indicates that applying Transformers to these filters provides minimal advantage in the spectral domain. We demonstrate that effective spectral filtering can be achieved without the need for transformers, offering a more efficient and spectrum-aware alternative. To this end, we propose a $\\textit{Simple Yet Effective Spectral Graph Neural Network}$ (SSGNN), which leverages the graph spectrum to adaptively filter using a simplified set-to-set approach that captures key spectral features. Moreover, we introduce a novel, parameter-free $\\textit{Relative Gaussian Amplifier}$ (ReGA) module, which adaptively learns spectral filtering while maintaining robustness against structural perturbations, ensuring stability. Extensive experiments on 20 real-world graph datasets, spanning both node-level and graph-level tasks along with a synthetic graph dataset, show that SSGNN matches or surpasses the performance of state-of-the-art (SOTA) spectral-based GNNs and graph transformers while using significantly fewer parameters and GFLOPs. Specifically, SSGNN achieves performance comparable to the current SOTA Graph Transformer model, Polynormer, with an average 55x reduction in parameters and 100x reduction in GFLOPs across all datasets. Our code will be made public upon acceptance."
    },
    {
        "title": "Towards Efficient and Scalable Multi-agent Reasoning via Bayesian Nash Equilibrium",
        "link_suffix": "/forum?id=MWSoYGPexK",
        "link": "https://openreview.net/forum?id=MWSoYGPexK",
        "pdf_link": "https://openreview.net/pdf?id=MWSoYGPexK",
        "keywords": "Large Language Models, Reasoning, Multiagent Reasoning",
        "abstract": "Large Language Models (LLMs) have achieved significant success in tasks such as natural language understanding, generation, and reasoning, driving advancements in machine translation, summarization, and question-answering systems. Particularly in multi-step reasoning tasks, LLMs demonstrate robust logical reasoning capabilities. To further enhance the reasoning abilities of LLMs, researchers have developed methods that generate intermediate reasoning steps, thereby improving performance in solving complex problems. However, these approaches primarily focus on single LLMs, limiting the diversity and creativity of the reasoning process. Inspired by psychological studies, there is a growing interest in exploring multi-LLM frameworks to boost model performance through collaborative reasoning. Existing multi-agent debate frameworks can enhance answer accuracy through dialogue and argumentation but suffer from high computational costs and lack theoretical guarantees for convergence. To address these challenges, we propose a novel method\u2014BNE-Q. This method integrates belief networks and a central network within the Decentralized Partially Observable Markov Decision Process (DEC-POMDP) framework to achieve a Bayesian Nash Equilibrium (BNE). During the inference phase, the central LLM provides step-by-step strategies and format guidelines, while execution LLMs independently generate answers based on these guidelines. The central LLM then consolidates the answers to form a commitment. In the optimization phase, by calculating the cosine similarity between the answers and the commitment, we optimize the execution LLMs' belief networks to achieve stable convergence of the multi-agent system. Our method not only reduces the computational overhead compared to traditional multi-agent debate methods but also ensures convergence through theoretical analysis. Experimental results demonstrate that BNE-Q performs exceptionally well across six benchmark tests, including complex reasoning and planning tasks, validating its theoretical robustness and practical effectiveness. Our work provides a new approach for developing multi-LLM collaborative reasoning frameworks, significantly enhancing reasoning capabilities in large-scale multi-agent environments."
    },
    {
        "title": "Optimal Transport for Time Series Imputation",
        "link_suffix": "/forum?id=xPTzjpIQNp",
        "link": "https://openreview.net/forum?id=xPTzjpIQNp",
        "pdf_link": "https://openreview.net/pdf?id=xPTzjpIQNp",
        "keywords": "Time series, Imputation",
        "abstract": "Missing data imputation through distribution alignment has demonstrated advantages for non-temporal datasets but exhibits suboptimal performance in time-series applications. The primary obstacle is crafting a discrepancy measure that simultaneously (1) $\\textit{captures temporal pattern}$\u2014accounting for patterns such as periodicities and temporal dependencies inherent in time-series\u2014and (2) $\\textit{accommodates non-stationarity}$, ensuring robustness amidst multiple coexisting temporal patterns. In response to these challenges, we introduce the Proximal Spectrum Wasserstein (PSW) discrepancy based on the stochastic optimal transport framework, which incorporates a pairwise spectral distance to encapsulate temporal patterns, coupled with selective matching regularization to accommodate non-stationarity. Building upon PSW, we develop the PSW for Imputation (PSW-I) framework, which iteratively refines imputation results by minimizing the PSW discrepancy. Extensive experiments demonstrate that PSW-I effectively addresses these challenges and significantly outperforms prevailing time-series imputation methods."
    },
    {
        "title": "HuRi : Humanoid Robots Adaptive Risk-ware Distributional Reinforcement Learning for Robust Control",
        "link_suffix": "/forum?id=iIrvKrtwnZ",
        "link": "https://openreview.net/forum?id=iIrvKrtwnZ",
        "pdf_link": "https://openreview.net/pdf?id=iIrvKrtwnZ",
        "keywords": "Adaptive Risk-Aware, Distributional Reinforcement Learning, Humanoid Robots, Locomotion Control",
        "abstract": "Due to the high complexity of bipedal locomotion, the locomotion control of humanoid robots requires precise adjustment of the balance system to adapt to the changing environment. This high dependence on balance makes the robot very sensitive to risky environments. Therefore, any slight change in the state of the environment may cause the robot to lose balance, thereby increasing the risk of falling or damage. In the past, few studies have explicitly incorporated risk factors into robot policy training, and have failed to adaptively adjust the risk perception level for different risky environmental states, which will affect the agent's exploration during training and thus fail to select the optimal action in the risky environment. We propose an adaptive risk-aware control policy(HuRi) based on value distributional reinforcement learning. This algorithm does not require additional modules, but only uses the environmental input and the calculated probability distribution, using IQR to measure the intrinsic uncertainty of the environment and RND to evaluate the parameter uncertainty of the environmental state. Combining these two uncertainties, the risk perception level of the strategy is adjusted by adjusting the scalar risk parameter of the distortion function. With this algorithm, the agent can explore safely and efficiently in the risky environment, adaptively adjust the risk sensitivity level of the agent by controlling different distortion measures of the reward distribution, and select the optimal action in the dynamic risky environment. We conducted simulation and actual deployment on the Zerith robot to verify the robustness of HuRi."
    },
    {
        "title": "From General to Expert: Custom Pruning LLMs Across Language, Domain, and Task",
        "link_suffix": "/forum?id=giU5WVNy7K",
        "link": "https://openreview.net/forum?id=giU5WVNy7K",
        "pdf_link": "https://openreview.net/pdf?id=giU5WVNy7K",
        "keywords": "Large Language Models, Pruning, Expert Model",
        "abstract": "Large Language Models (LLMs) have transformed natural language processing, yet their substantial model sizes often demand significant computational resources. To conserve computing resources and increase inference speed, it is crucial to prune redundant parameters, especially for general users who often need expert models tailored to specific downstream scenarios. However, current pruning methods primarily focus on maintaining models' general capabilities, either requiring extensive post-training or performing poorly due to coarse-grained pruning. In this work, we design a $\\underline{Cus}$tom $\\underline{Prun}$ing method ($\\texttt{Cus-Prun}$) to prune a large general model into a smaller expert model for specific scenarios. $\\texttt{Cus-Prun}$ positions an expert model along the \"language\", \"domain\" and \"task\" dimensions. By identifying and pruning irrelevant neurons, it creates expert models without any post-training. \nOur experiments demonstrate that $\\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal loss in both expert and general capabilities across various models from different model families and sizes."
    },
    {
        "title": "Global Context-aware Representation Learning for Spatially Resolved Transcriptomics",
        "link_suffix": "/forum?id=Uc3kog3O45",
        "link": "https://openreview.net/forum?id=Uc3kog3O45",
        "pdf_link": "https://openreview.net/pdf?id=Uc3kog3O45",
        "keywords": "Spatially Resolved Transcriptomics, Self-Supervised Learning",
        "abstract": "Spatially Resolved Transcriptomics (SRT) is a cutting-edge technique that captures the spatial context of cells within tissues, enabling the study of complex biological networks. Recently, graph-based deep learning has been utilized in identifying meaningful spatial domains by leveraging both gene expression and spatial information. However, these approaches fall short in obtaining qualified spot representations, particularly for those located around the boundary of cell type clusters, as they heavily emphasize spatially local spots that have minimal feature differences from an anchor node. To address this limitation, we propose a novel framework, Spotscape, which introduces the Similarity Telescope module designed to learn spot representations by capturing the global relationships among multiple spots. Additionally, to address the challenges that arise when integrating multiple slices from heterogeneous sources, we propose a similarity scaling strategy that explicitly regulates the distances between intra- and inter-slice spots to ensure they remain nearly the same. Extensive experiments demonstrate the superiority of Spotscape in various downstream tasks, including spatial domain identification, multi-slice integration, and alignment tasks, compared to baseline methods. Our code is available at the following link:https://anonymous.4open.science/r/Spotscape-E312/"
    },
    {
        "title": "From Rest to Action: Adaptive Weight Generation for Motor Imagery Classification from Resting-State EEG Using Hypernetworks",
        "link_suffix": "/forum?id=04RGjODVj3",
        "link": "https://openreview.net/forum?id=04RGjODVj3",
        "pdf_link": "https://openreview.net/pdf?id=04RGjODVj3",
        "keywords": "Brain-Computer Interfaces (BCIs), Motor Imagery, HyperNetworks, Data driven learning, Adaptive weights",
        "abstract": "Existing EEG-based brain-computer interface (BCI) systems require long calibration sessions from the intended users to train the models, limiting their use in real-world applications. Additionally, despite containing user-specific information and features correlating with BCI performance of a user, resting-state EEG data is underutilized, especially in motor imagery decoding tasks. To address the challenge of within and across-user generalisation, we propose a novel architecture, HyperEEGNet, which integrates HyperNetworks (HNs) with the EEGNet architecture to adaptively generate weights for motor imagery classification based on resting-state data. Our approach performs similarly in a Leave-Subject-Out scenario using a dataset with 9 participants, compared to the baseline EEGNet. When the dataset size is scaled, with 33 participants' datasets, the model demonstrates its generalisation capabilities using the information from resting state EEG data, particularly when faced with unseen subjects. Our model can learn robust representations in both cross-session and cross-user scenarios, opening a novel premise to leverage the resting state data for downstream tasks like motor imagery classification. The findings also demonstrate that such models with smaller footprints reduce memory and storage requirements for edge computing. The approach opens up avenues for faster user calibration and better feasibility of edge computing, a favourable combination to push forward the efforts to bring BCIs to real-world applications."
    }
]