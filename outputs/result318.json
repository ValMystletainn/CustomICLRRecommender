[{"title": "Learning to Solve Differential Equation Constrained Optimization Problems", "link_suffix": "/forum?id=VeMC6Bn0ZB", "link": "https://openreview.net/forum?id=VeMC6Bn0ZB", "pdf_link": "https://openreview.net/pdf?id=VeMC6Bn0ZB", "keywords": "Learning-based optimization proxy, differential equations constrained optimization, neural differential equations, system dynamics", "abstract": "Differential equations (DE) constrained optimization plays a critical role in numerous scientific and engineering fields, including energy systems, aerospace engineering, ecology, and finance, where optimal configurations or control strategies must be determined for systems governed by ordinary or stochastic differential equations. Despite its significance, the computational challenges associated with these problems have limited their practical use. To address these limitations, this paper introduces a learning-based approach to DE-constrained optimization that combines techniques from proxy optimization and neural differential equations. The proposed approach uses a dual-network architecture, with one approximating the control strategies, focusing on steady-state constraints, and another solving the associated DEs. \nThis combination enables the approximation of optimal strategies while accounting for dynamic constraints in near real-time.\nExperiments across problems in energy optimization and finance modeling show that this method provides full compliance with dynamic constraints and it produces results up to 25 times more precise than other methods which do not explicitly model the system's dynamic equations.", "title_embedding_index": 15850, "title_abs_embedding_index": 15875}, {"title": "Provably Efficient Multi-Objective Bandit Algorithms under Preference-Centric Customization", "link_suffix": "/forum?id=JaTmg8FX3k", "link": "https://openreview.net/forum?id=JaTmg8FX3k", "pdf_link": "https://openreview.net/pdf?id=JaTmg8FX3k", "keywords": "multi-objective multi-arm bandit, bandit optimization, preference-centric learning", "abstract": "Existing multi-objective multi-armed bandit (MO-MAB) approaches mainly focus on achieving Pareto optimality. However, a Pareto optimal arm that receives a high score from one user may lead to a low score from another, since in real-world scenarios, users often have diverse preferences across different objectives. Instead, these preferences should informcustomized learning, a factor usually neglected in prior research. To address this need, we study apreference-awareMO-MAB framework in the presence of explicit user preferences, where each user\u2019s overall-reward is modeled as the inner product of user preference and arm reward. This new framework shifts the focus from merely achieving Pareto optimality to further optimizing within the Pareto front under preference-centric customization. To the best of our knowledge, this is the first theoretical exploration of customized MO-MAB optimization based on explicit user preferences. This framework introduces new and unique challenges for algorithm design for customized optimization. To\naddress these challenges, we incorporatepreference estimationandpreference-aware optimizationas key mechanisms for preference adaptation, and develop new analytical techniques to rigorously account for the impact of preference estimation errors on overall performance. Under this framework, we consider three preference structures inspired by practical applications, with tailored algorithms that are proven to achieve near-optimal regret, and show good numerical performance.", "title_embedding_index": 15851, "title_abs_embedding_index": 15876}, {"title": "OPTIMAL TRANSPORT BARYCENTER VIA NONCONVEX CONCAVE MINIMAX OPTIMIZATION", "link_suffix": "/forum?id=rY8xdjrANt", "link": "https://openreview.net/forum?id=rY8xdjrANt", "pdf_link": "https://openreview.net/pdf?id=rY8xdjrANt", "keywords": "Wasserstein barycenter; optimal transport; nonconvex-concave minimax optimization; convergence rate;", "abstract": "The optimal transport barycenter (a.k.a. Wasserstein barycenter) is a fundamental notion of averaging that extends from the Euclidean space to the Wasserstein space of probability distributions. Computation of the \\emph{unregularized} barycenter for discretized probability distributions on point clouds is a challenging task when the domain dimension $d > 1$. Most practical algorithms for approximating the barycenter problem are based on entropic regularization. In this paper, we introduce a nearly linear time $O(m \\log{m})$ and linear space complexity $O(m)$ primal-dual algorithm, the Wasserstein-Descent $\\dot{\\mathbb{H}}^1$-Ascent (WDHA) algorithm, for computing the exact barycenter when the input probability density functions are discretized on an $m$-point grid. The key success of the WDHA algorithm hinges on alternating between two different yet closely related Wasserstein and Sobolev optimization geometries for the primal barycenter and dual Kantorovich potential subproblems. Under reasonable assumptions, we establish the convergence rate and iteration complexity of WDHA to its stationary point when the step size is appropriately chosen. Superior computational efficacy, scalability, and accuracy over the existing Sinkhorn-type algorithms are demonstrated on high-resolution (e.g., $1024 \\times 1024$ images) 2D synthetic and real data.", "title_embedding_index": 15852, "title_abs_embedding_index": 15877}, {"title": "Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval Augmented Generation", "link_suffix": "/forum?id=w5ZtXOzMeJ", "link": "https://openreview.net/forum?id=w5ZtXOzMeJ", "pdf_link": "https://openreview.net/pdf?id=w5ZtXOzMeJ", "keywords": "domain adaptation; NLI; RAG; document-grounded; NLP;", "abstract": "While retrieval augmented generation (RAG) has been shown to enhance factuality of large language model (LLM) outputs, LLMs still suffer from hallucination, generating incorrect or irrelevant information. One common detection strategy involves prompting the LLM again to assess whether its response is grounded in the retrieved evidence, but this approach is costly. Alternatively, lightweight natural language inference (NLI) models for efficient grounding verification can be used at inference time. While existing pre-trained NLI models offer potential solutions, their performance remains subpar compared to larger models on realistic RAG inputs. RAG inputs are more complex than most datasets used for training NLI models and have characteristics specific to the underlying knowledge base, requiring adaptation of the NLI models to a specific target domain. Additionally, the lack of labeled instances in the target domain makes supervised domain adaptation, e.g., through fine-tuning, infeasible. To address these challenges, we introduce Automatic Generative Domain Adaptation (Auto-GDA). Our framework enables unsupervised domain adaptation through synthetic data generation.\nUnlike previous methods that rely on handcrafted filtering and augmentation strategies, Auto-GDA employs an iterative process to continuously improve the quality of generated samples using weak labels from less efficient teacher models and discrete optimization to select the most promising augmented samples. Experimental results demonstrate the effectiveness of our approach, with models fine-tuned on synthetic data using Auto-GDA often surpassing the performance of the teacher model and reaching the performance level of LLMs at 10 % of their computational cost.", "title_embedding_index": 15853, "title_abs_embedding_index": 15878}, {"title": "Limitations of measure-first protocols in quantum machine learning", "link_suffix": "/forum?id=0tIiMNNmdm", "link": "https://openreview.net/forum?id=0tIiMNNmdm", "pdf_link": "https://openreview.net/pdf?id=0tIiMNNmdm", "keywords": "quantum machine learning, machine learning, learning separation", "abstract": "In recent times, there have been major developments in two distinct yet connected domains of quantum information. On the one hand, substantial progress has been made in so-called randomized measurement protocols. Here, a number of properties of unknown quantum states can be deduced from surprisingly few measurement outcomes, using schemes such as classical shadows. On the other hand, significant progress has been made in quantum machine learning. For example, exponential advantages have been proven when the data consists of quantum states and quantum algorithms can coherently measure multiple copies of input states. In this work, we aim to understand the implications and limitations of combining randomized measurement protocols with quantum machine learning, although the implications are broader. Specifically, we investigate quantum machine learning algorithms that, when dealing with quantum data, can either process it entirely using quantum methods or measure the input data through a fixed measurement scheme and utilize the resulting classical information. We prove limitations for quantum machine learning algorithms that use fixed measurement schemes on the input quantum states.\nOur results have several implications. From the perspective of randomized measurement procedures, we show limitations of measure-first protocols in the average case, improving on the state-of-the-art which only focuses on worst-case scenarios. Additionally, previous lower bounds were only known for physically unrealizable states. We improve upon this by employing quantum pseudorandom functions to prove that a learning separation also exists when dealing with physically realizable states, which may be encountered in experiments. From a machine learning perspective, our results are crucial for defining a physically meaningful task that shows fully quantum machine learning processing is not only more efficient but also necessary for solving certain problems. The tasks at hand are also realistic, as the algorithms and proven separations hold when working with efficiently preparable states and remain robust in the presence of measurement and preparation errors.", "title_embedding_index": 15854, "title_abs_embedding_index": 15879}, {"title": "Fast Uncovering of Protein Sequence Diversity from Structure", "link_suffix": "/forum?id=1iuaxjssVp", "link": "https://openreview.net/forum?id=1iuaxjssVp", "pdf_link": "https://openreview.net/pdf?id=1iuaxjssVp", "keywords": "Protein design, inverse folding, generative modelling, transfer learning", "abstract": "We present InvMSAFold, an inverse folding method for generating protein sequences optimized for diversity and speed. For a given structure, InvMSAFold generates the parameters of a pairwise probability distribution over the space of sequences, capturing the amino acid covariances observed in Multiple Sequence Alignments (MSA) of homologous proteins. This allows for the efficient generation of highly diverse protein sequences while preserving structural and functional integrity.\nWe demonstrate that this increased diversity in sampled sequences translates into greater variability in biochemical properties, highlighting the exciting potential of our method for applications such as protein design. The orders of magnitude improvement in sampling speed compared to existing methods unlocks new possibilities for high-throughput in virtual screening.", "title_embedding_index": 15855, "title_abs_embedding_index": 15880}, {"title": "Simplicity Bias and Optimization Threshold in Two-Layer Networks", "link_suffix": "/forum?id=eQggPqESBr", "link": "https://openreview.net/forum?id=eQggPqESBr", "pdf_link": "https://openreview.net/pdf?id=eQggPqESBr", "keywords": "Neural Networks, Simplicty Bias, Implicit Bias, One hidden ReLU Network, Early Alignment", "abstract": "Understanding generalization of overparametrized neural networks remains a fundamental challenge in machine learning. \nMost of the literature mostly studies generalization from an interpolation point of view, taking convergence of parameters towards a global minimum of the training loss for granted. While overparametrized architectures indeed interpolated the data for typical classification tasks, this interpolation paradigm does not seem valid anymore for more complex tasks such as in-context learning or diffusion. Instead for such tasks, it has been empirically observed that the trained models goes from global minima to spurious local minima of the training loss as the number of training samples becomes larger than some level we calloptimization threshold. While the former yields a poor generalization to the true population loss, the latter was observed to actually correspond to the minimiser of this true loss.\nThis paper explores theoretically this phenomenon in the context of two-layer ReLU networks. We demonstrate that, despite overparametrization, networks often converge toward simpler solutions rather than interpolating the training data, which can lead to a drastic improvement on the test loss with respect to interpolating solutions. \nOur analysis relies on the so called early alignment phase, during which neurons align towards specific directions. This directional alignment, which occurs in the early stage of training, leads to a simplicity bias, wherein the network approximates the ground truth model without converging to the global minimum of the training loss. Our results suggest that this bias, resulting in an optimization threshold from which interpolation is not reached anymore, is beneficial and enhances the generalization of trained models.", "title_embedding_index": 15856, "title_abs_embedding_index": 15881}, {"title": "Cross the Gap: Inter-modal CLIP Representations Are Superior for Intra-modal Tasks", "link_suffix": "/forum?id=VVVfuIcmKR", "link": "https://openreview.net/forum?id=VVVfuIcmKR", "pdf_link": "https://openreview.net/pdf?id=VVVfuIcmKR", "keywords": "CLIP, modality gap, image retrieval, image classification, text retrieval", "abstract": "Pre-trained multi-modal Vision Language Models like CLIP are widely used off-the-shelf for a variety of applications. Previous work has shown that, due to contrastive pre-training, there is a modality gap between the text and image feature embedding spaces. In this paper, we show that the common practice of individually exploiting the text or image encoders of these powerful multimodal models is highly suboptimal for intra-modal tasks like image-to-image retrieval. We argue that this is inherently due to the inter-modal contrastive loss commonly used to train CLIP models. To demonstrate this, we leverage two optimization-based modality inversion techniques and the inductive bias of the pre-trained encoder of the complementary modality to transform native modality inputs into inter-modal representations. We empirically show in multiple settings (image retrieval, text retrieval, and zero-shot image classification), and at the single-feature level -- i.e. each individual feature embedding is mapped to its complementary modality without any need for auxiliary data or additional trained adapters -- that approaching these tasks inter-modally significantly improves performance with respect to intra-modal baselines on more than fifteen datasets.", "title_embedding_index": 15857, "title_abs_embedding_index": 15882}, {"title": "Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint", "link_suffix": "/forum?id=LRrbD8EZJl", "link": "https://openreview.net/forum?id=LRrbD8EZJl", "pdf_link": "https://openreview.net/pdf?id=LRrbD8EZJl", "keywords": "cross-domain, reinforcement learning, offline RL, optimal transport", "abstract": "Offline reinforcement learning (RL) often struggles with limited data. This work explores cross-domain offline RL where offline datasets (with possibly sufficient data) from another domain can be accessed to facilitate policy learning. However, the underlying environments of the two datasets may have dynamics mismatches, incurring inferior performance when simply merging the data of two domains. Existing methods mitigate this issue by training domain classifiers, using contrastive learning methods, etc. Nevertheless, they still rely on a large amount of target domain data to function well. Instead, we address this problem by establishing a concrete performance bound of a policy given datasets from two domains. Motivated by the theoretical insights, we propose to align transitions in the two datasets using optimal transport and selectively share source domain samples, without training any neural networks. This enables reliable data filtering even given a few target domain data. Additionally, we introduce a dataset regularization term that ensures the learned policy remains within the scope of the target domain dataset, preventing it from being biased towards the source domain data. Consequently, we propose the Optimal Transport Data Filtering (dubbed OTDF) method and examine its effectiveness by conducting extensive experiments across various dynamics shift conditions (e.g., gravity shift, morphology shift), given limited target domain data. It turns out that OTDF exhibits superior performance on many tasks and dataset qualities, often surpassing prior strong baselines by a large margin.", "title_embedding_index": 15858, "title_abs_embedding_index": 15883}, {"title": "BadRobot: Manipulating Embodied LLMs in the Physical World", "link_suffix": "/forum?id=ei3qCntB66", "link": "https://openreview.net/forum?id=ei3qCntB66", "pdf_link": "https://openreview.net/pdf?id=ei3qCntB66", "keywords": "Robotics, Safety Risks, Embodied AI", "abstract": "Embodied AI represents systems where AI is integrated into physical entities, enabling them to perceive and interact with their surroundings. Large Language Model (LLM), which exhibits powerful language understanding abilities, has been extensively employed in embodied AI by facilitating sophisticated task planning. However, a critical safety issue remains overlooked: could these embodied LLMs perpetrate harmful behaviors? In response, we introduce BadRobot, a novel attack paradigm aiming to make embodied LLMs violate safety and ethical constraints through typical voice-based user-system interactions. Specifically, three vulnerabilities are exploited to achieve this type of attack: (i) manipulation of LLMs within robotic systems, (ii) misalignment between linguistic outputs and physical actions, and  (iii) unintentional hazardous behaviors caused by world knowledge's flaws. Furthermore, we construct a benchmark of various malicious physical action queries  to evaluate BadRobot's attack performance. Based on this benchmark, extensive experiments against existing prominent embodied LLM frameworks (e.g., Voxposer, Code as Policies, and ProgPrompt) demonstrate the effectiveness of our BadRobot. More demonstrations are available at an anonymous address:https://Embodied-LLMs-Safety.github.io.Warning: This paper contains harmful AI-generated language and aggressive actions.", "title_embedding_index": 15859, "title_abs_embedding_index": 15884}, {"title": "Elephant in the Room: Unveiling the Pitfalls of Human Proxies in Alignment", "link_suffix": "/forum?id=x8z8hCjtcY", "link": "https://openreview.net/forum?id=x8z8hCjtcY", "pdf_link": "https://openreview.net/pdf?id=x8z8hCjtcY", "keywords": "Pitfalls; Human Proxies; Alignment", "abstract": "The demand for regulating the behavior of large language models (LLMs) has ignited research on alignment algorithms, the essence of which is to align LLMs' generations with human preferences. Due to infeasibility of humans directly participating in the training or generation of LLMs, existing alignment algorithms choose to align with human preferences carried by proxies, i.e., preference data or reward models. However, whether these human proxies faithfully represent human preferences remain under-explored. We categorize human proxies into two levels based on the degree to which they directly embody human preferences: Level-1 Proxy (preference data) and Level-2 Proxy (reward models). We empirically examine the faithfulness of both levels of proxies and its impacts on alignment performance.\nWe notice that current algorithms tend to overlook the faithfulness of these proxies in reflecting human preferences; many works even directly use reward models as their automatic evaluators without any correlation verification. Current literature of alignment overly focuses on optimizing algorithms, rendering the faithfulness of human proxies an \"elephant in the room\"\u2014something extremely important yet largely overlooked. According to experimental results, we unveil potential risks of using inferiorhuman proxies'', aiming to arouse attention to this hugeelephant'' in alignment research. We summarize existing pitfalls from different angles and provide a re-labeled preference dataset and insights about reward model usage to facilitate the healthy development of alignment\\footnote{This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.}.", "title_embedding_index": 15860, "title_abs_embedding_index": 15885}, {"title": "Input Space Mode Connectivity in Deep Neural Networks", "link_suffix": "/forum?id=3qeOy7HwUT", "link": "https://openreview.net/forum?id=3qeOy7HwUT", "pdf_link": "https://openreview.net/pdf?id=3qeOy7HwUT", "keywords": "mode connectivity, input space, deep learning, adversarial detection, interpretability, percolation theory", "abstract": "We extend the concept of loss landscape mode connectivity to the input space of deep neural networks. Mode connectivity was originally studied within parameter space, where it describes the existence of low-loss paths between different solutions (loss minimizers) obtained through gradient descent. We present theoretical and empirical evidence of its presence in the input space of deep networks, thereby highlighting the broader nature of the phenomenon. We observe that different input images with similar predictions are generally connected, and for trained models, the path tends to be simple, with only a small deviation from being a linear path. Our methodology utilizes real, interpolated, and synthetic inputs created using the input optimization technique for feature visualization. We conjecture that input space mode connectivity in high-dimensional spaces is a geometric effect that takes place even in untrained models and can be explained through percolation theory. We exploit mode connectivity to obtain new insights about adversarial examples and demonstrate its potential for adversarial detection. Additionally, we discuss applications for the interpretability of deep networks.", "title_embedding_index": 15861, "title_abs_embedding_index": 15886}, {"title": "Loss in the Crowd: Hidden Breakthroughs in Language Model Training", "link_suffix": "/forum?id=pK4Z6NZ2DB", "link": "https://openreview.net/forum?id=pK4Z6NZ2DB", "pdf_link": "https://openreview.net/pdf?id=pK4Z6NZ2DB", "keywords": "interpretability techniques, loss disaggregation, phase transitions", "abstract": "The training loss curves of a neural network are typically smooth. Any visible discontinuities draw attention as discrete conceptual breakthroughs, while the rest of training is less carefully studied. In this work we hypothesize that similar breakthroughs actually occur frequently throughout training, though their presence is obscured when monitoring the aggregate train loss. To find these hidden transitions, we introduce POLCA, a method for decomposing changes in loss along an arbitrary basis of the low rank training subspace. We use our method to identify clusters of samples that exhibit similar changes in loss through training, disaggregating the overall loss into that of smaller groups of conceptually similar datapoints. We validate our method on synthetic arithmetic and natural language, showing that POLCA recovers clusters which represent easily interpretable breakthroughs in the model's capabilities whose existence would otherwise be lost in the crowd.", "title_embedding_index": 15862, "title_abs_embedding_index": 15887}, {"title": "Amuro and Char: Analyzing the Relationship between Pre-Training and Fine-Tuning of Large Language Models", "link_suffix": "/forum?id=8uXkyWFVum", "link": "https://openreview.net/forum?id=8uXkyWFVum", "pdf_link": "https://openreview.net/pdf?id=8uXkyWFVum", "keywords": "Fine-tuning, Pre-training, Instruction Tuning, Training Dynamics", "abstract": "Large language model development relies on the pre-train-then-align paradigm, in which the model is typically pre-trained on a large text corpus and undergoes a tuning stage to align the model with human preference or downstream tasks. We investigate the relationship between pre-training and fine-tuning by fine-tuning multiple intermediate pre-trained model checkpoints to understand how models develop as they train. Our results on 18 datasets suggest that i) continual pre-training improves the model in a latent way that manifests after fine-tuning; ii) fine-tuning most benefits datasets where the model does not show capability during pre-training; iii) although the model benefits significantly through supervised fine-tuning, it may forget previously known domain knowledge and tasks not seen during fine-tuning; iv) the model exhibits high sensitivity to evaluation prompts after supervised fine-tuning, but this sensitivity can be alleviated through more pre-training.", "title_embedding_index": 15863, "title_abs_embedding_index": 15888}, {"title": "CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design", "link_suffix": "/forum?id=rbnf7oe6JQ", "link": "https://openreview.net/forum?id=rbnf7oe6JQ", "pdf_link": "https://openreview.net/pdf?id=rbnf7oe6JQ", "keywords": "Electronics Design Automation (EDA), circuit representation learning, multimodal learning, self-supervised learning, hardware circuits", "abstract": "The rapid advancements of AI rely on the support of integrated circuits (ICs). However, the growing complexity of digital ICs makes the traditional IC design process costly and time-consuming. In recent years, AI-assisted IC design methods have demonstrated great potential, but most methods are task-specific or focus solely on the circuit structure in graph format, overlooking other circuit modalities with rich functional information. In this paper, we introduce CircuitFusion, the first multimodal and implementation-aware circuit encoder. It encodes circuits into general representations that support different downstream circuit design tasks. To learn from circuits, we propose to fuse three circuit modalities: hardware code, structural graph, and functionality summary. More importantly, we identify four unique properties of circuits: parallel execution, functional equivalent transformation, multiple design stages, and circuit reusability. Based on these properties, we propose new strategies for both the development and application of CircuitFusion: 1) During circuit preprocessing, utilizing the parallel nature of circuits, we split each circuit into multiple sub-circuits based on sequential-element boundaries, each sub-circuit in three modalities. It enables fine-grained encoding at the sub-circuit level. 2) During CircuitFusion pre-training, we introduce three self-supervised tasks that utilize equivalent transformations both within and across modalities. We further utilize the multi-stage property of circuits to align representation with ultimate circuit implementation. 3) When applying CircuitFusion to downstream tasks, we propose a new retrieval-augmented inference method, which retrieves similar known circuits as a reference for predictions. It improves fine-tuning performance and even enables zero-shot inference. Evaluated on five different circuit design tasks, CircuitFusion consistently outperforms the state-of-the-art supervised method specifically developed for every single task, demonstrating its generalizability and ability to learn circuits' inherent properties.", "title_embedding_index": 15864, "title_abs_embedding_index": 15889}, {"title": "Why context matters in VQA & Reasoning: Semantic interventions for VLM input modalities", "link_suffix": "/forum?id=vbr1OKK19i", "link": "https://openreview.net/forum?id=vbr1OKK19i", "pdf_link": "https://openreview.net/pdf?id=vbr1OKK19i", "keywords": "Vision Language Model, Vision Question Answering, model failure, multimodality, interpretability, semantic intervention", "abstract": "The various limitations of Generative AI, such as hallucinations and model failures, have made it crucial to understand the role of different modalities in Visual Language Model (VLM) predictions. Our work investigates how the integration of information from image and text modalities influences the performance and behavior of VLMs in visual question answering (VQA) and reasoning tasks. We measure this effect through answer accuracy, reasoning quality, model uncertainty, and modality relevance. We study the interplay between text and image modalities in different configurations where visual content is essential for solving the VQA task. Our contributions include (1) the Semantic Interventions (SI)-VQA dataset, (2) a benchmark study of various VLM architectures under different modality configurations, and (3) the Interactive Semantic Interventions (ISI) tool. The SI-VQA dataset serves as the foundation for the benchmark, while the ISI tool provides an interface to test and apply semantic interventions in image and text inputs, enabling more fine-grained analysis. Our results show that complementary information between modalities improves answer and reasoning quality, while contradictory information harms model performance and confidence. Image text annotations have minimal impact on accuracy and uncertainty, slightly increasing image relevance. Attention analysis confirms the dominant role of image inputs over text in VQA tasks. In this study, we evaluate state-of-the-art VLMs that allow us to extract attention coefficients for each modality. A key finding is PaliGemma's harmful overconfidence, which poses a higher risk of silent failures compared to the LLaVA models. This work sets the foundation for rigorous analysis of modality integration, supported by datasets specifically designed for this purpose. The code is available athttps://gitlab.com/dekfsx1/si-vlm-benchmarkand the tool and dataset are hosted athttps://gitlab.com/dekfsx1/isi-vlm.", "title_embedding_index": 15865, "title_abs_embedding_index": 15890}, {"title": "Mitigating Distribution Shifts: Uncertainty-Aware Offline-to-Online Reinforcement Learning", "link_suffix": "/forum?id=0WqAnYWi7H", "link": "https://openreview.net/forum?id=0WqAnYWi7H", "pdf_link": "https://openreview.net/pdf?id=0WqAnYWi7H", "keywords": "Reinforcement learning, Out-of-distribution detection, Uncertainty estimation, Offline RL", "abstract": "Deploying reinforcement learning (RL) policies in real-world scenarios faces challenges due to distribution shifts from training environments. Past approaches have shown limitations such as poor generalization to out-of-distribution (OOD) variations or requiring extensive retraining on new data. We propose Uncertainty-aware Adaptive RL, UARL, a novel RL pipeline that enhances policy generalization across diverse variations of a given environment. UARL frames distribution shifts as OOD problems and incorporates a new OOD detection method to quantify uncertainty. This approach enables iterative policy fine-tuning, starting with offline training on a limited state space and progressively expanding to more diverse variations of the same environment through online interactions. We demonstrate the effectiveness and robustness of UARL through extensive experiments on continuous control tasks, showing improved performance and sample efficiency as well as reliability in OOD detection compared to existing methods.", "title_embedding_index": 15866, "title_abs_embedding_index": 15891}, {"title": "Compressing Vision Foundation Models at ImageNet-level Costs", "link_suffix": "/forum?id=LC6ZtQV6u2", "link": "https://openreview.net/forum?id=LC6ZtQV6u2", "pdf_link": "https://openreview.net/pdf?id=LC6ZtQV6u2", "keywords": "Foundation Model, Model Compressing, DINOv2, CLIP, SynCLR", "abstract": "Vision foundation models are renowned for the generalization ability due to massive training data. Nevertheless, they demand tremendous training resources, and the training data is often inaccessible, e.g., CLIP, DINOv2, posing great challenges to developing derivatives that could facilitate the research. In this work, we offer a very simple and general solution, named Proteus, to distill foundation models into smaller equivalents on ImageNet-1K without access to the original training data. Specifically, we remove the designs from conventional knowledge distillation settings that result in dataset bias and present three levels of training objectives, i.e., token, patch, and feature, to maximize the efficacy of knowledge transfer. In this manner, Proteus is trained at ImageNet-level costs with surprising ability, facilitating the accessibility of training foundation models for the broader research community. Leveraging DINOv2-g/14 as the teacher, Proteus-L/14 matches the performance of the Oracle method DINOv2-L/14 (142M training data) across 15 benchmarks and outperforms other vision foundation models including CLIP-L/14 (400M), OpenCLIP-L/14 (400M/2B) and SynCLR-L/14 (600M).", "title_embedding_index": 15867, "title_abs_embedding_index": 15892}, {"title": "Quantized Approximately Orthogonal Recurrent Neural Networks", "link_suffix": "/forum?id=zuKrRYM3Tg", "link": "https://openreview.net/forum?id=zuKrRYM3Tg", "pdf_link": "https://openreview.net/pdf?id=zuKrRYM3Tg", "keywords": "ecurrent neural networks, neural network quantization, orthogonal recurrent neural networks, quantization bitwidth", "abstract": "In recent years, Orthogonal Recurrent Neural Networks (ORNNs) have gained popularity due to their ability to manage tasks involving long-term dependencies, such as the copy task, and their linear complexity. However, existing ORNNs utilize full precision weights and activations, which prevents their deployment on compact devices.In this paper, we explore the quantization of the weight matrices in ORNNs, leading to Quantized approximately Orthogonal RNNs (QORNNs). The construction of such networks remained an open problem, acknowledged for its inherent instability. We propose and investigate two strategies to learn QORNN by combining quantization-aware training (QAT) and orthogonal projections. We also study post-training quantization of the activations for pure integer computation of the recurrent loop. The most efficient models achieve results similar to state-of-the-art full-precision ORNN, LSTM and FastRNN on a variety of standard benchmarks, even with 3-bits quantization.", "title_embedding_index": 15868, "title_abs_embedding_index": 15893}, {"title": "Optimizing Adaptive Attacks against Content Watermarks for Language Models", "link_suffix": "/forum?id=RKQcJ1lXNT", "link": "https://openreview.net/forum?id=RKQcJ1lXNT", "pdf_link": "https://openreview.net/pdf?id=RKQcJ1lXNT", "keywords": "watermarking, language models, robustness, adaptive attacks", "abstract": "Large Language Models (LLMs) can be \\emph{misused} to spread online spam and misinformation. Content watermarking deters misuse by hiding a message in model-generated outputs, enabling their detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate the robustness of LLM watermarking as an objective function and propose preference-based optimization to tune \\emph{adaptive} attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks substantially outperform non-adaptive baselines. (ii) Even in a non-adaptive setting, adaptive attacks optimized against a few known watermarks remain highly effective when tested against other unseen watermarks, and (iii) optimization-based attacks are practical and need limited computational resources of less than seven GPU hours. Our findings underscore the need to test robustness against adaptive attackers.", "title_embedding_index": 15869, "title_abs_embedding_index": 15894}, {"title": "Binary Losses for Density Ratio Estimation", "link_suffix": "/forum?id=562B7aLi5X", "link": "https://openreview.net/forum?id=562B7aLi5X", "pdf_link": "https://openreview.net/pdf?id=562B7aLi5X", "keywords": "density ratio estimation, domain adaptation, composite binary losses, class probability estimation", "abstract": "Estimating the ratio of two probability densities from finitely many observations of the densities, is a central problem in machine learning and statistics. A large class of methods constructs estimators from binary classifiers which distinguish observations from the two densities. However, the error of these constructions depends on the choice of the binary loss function, raising the question of which loss function to choose based on desired error properties.In this work, we start from prescribed error measures in a class of Bregman divergences and characterize all loss functions that lead to density ratio estimators with a small error. Our characterization provides a simple recipe for constructing loss functions with certain properties, such as loss functions that prioritize an accurate estimation of large values. This contrasts with classical loss functions, such as the logistic loss or boosting loss, which prioritize accurate estimation of small values. We provide numerical illustrations with kernel methods and test their performance in applications of parameter selection for deep domain adaptation.", "title_embedding_index": 15870, "title_abs_embedding_index": 15895}, {"title": "StagFormer:  A Staggered Transformer for Decoding Layers in Parallel", "link_suffix": "/forum?id=4RHdGVimNA", "link": "https://openreview.net/forum?id=4RHdGVimNA", "pdf_link": "https://openreview.net/pdf?id=4RHdGVimNA", "keywords": "decoder only language models, transformers, staggered execution, pipelining, parallel decoding, efficiency", "abstract": "Standard decoding in a Transformer based language model is inherently sequential as we wait for a token\u2019s embedding to pass through all the layers in the network before starting the generation of the next token. In this work, we propose anew architecture StagFormer (Staggered Transformer), which staggered execution along the time axis and thereby enables parallelizing the decoding process along the depth of the model. We achieve this by breaking the dependency of the token representation at time step $i$ in layer $l$ upon the representations of tokens until time step $i$ from layer $l\u22121$. Instead, we stagger the execution and only allow a dependency on token representations until time step $i\u22121$. The later sections of the Transformer still get access to the \u201drich\u201d representations from the prior section but only from those token positions which are one time step behind. StagFormer allows for different sections of the model to be executed in parallel yielding up to 33% speedup in decoding while being quality neutral. We also explore many natural variants of this idea.  We present how weight-sharing across the different sections being staggered can be more practical in settings with limited memory. We show how one can approximate a recurrent model during inference using such weight-sharing. We explore the efficacy of using a bounded window attention to pass information from one section to another which helps drive further latency gains for some applications. We also explore demonstrate the scalability of the staggering idea over more than 2 sections of the Transformer.", "title_embedding_index": 15871, "title_abs_embedding_index": 15896}, {"title": "How transformers learn structured data: insights from hierarchical filtering", "link_suffix": "/forum?id=F0Zd3knG9j", "link": "https://openreview.net/forum?id=F0Zd3knG9j", "pdf_link": "https://openreview.net/pdf?id=F0Zd3knG9j", "keywords": "Transformers, Belief Propagation, mechanistic explanation, structured data, hierarchical data model, attention, masked language modeling", "abstract": "We introduce a hierarchical filtering procedure for generative models of sequences on trees, enabling control over the range of positional correlations in the data. Leveraging this controlled setting, we provide evidence that vanilla encoder-only transformers implement the optimal Belief Propagation algorithm on both root classification and masked language modeling tasks. Correlations at larger distances, corresponding to increasing layers of the hierarchy, are sequentially included by the network during training. We analyze how transformer layers succeed by considering attention maps from models trained with varying degrees of filtering. These attention maps show clear evidence of an iterative hierarchical reconstruction of correlations, which we relate to a plausible implementation of the exact inference algorithm.", "title_embedding_index": 15872, "title_abs_embedding_index": 15897}, {"title": "Gradient Storm: Stronger Backdoor Attacks Through Expanded Parameter Space Coverage", "link_suffix": "/forum?id=OE67D1Oatr", "link": "https://openreview.net/forum?id=OE67D1Oatr", "pdf_link": "https://openreview.net/pdf?id=OE67D1Oatr", "keywords": "Data Poisoning, Adversarial Attacks, Backdoor Attacks", "abstract": "Targeted data poisoning poses a critical adversarial threat to machine learning systems by enabling attackers to manipulate training data to induce specific, harmful misclassifications.  Among these threats, backdoor attacks are particularly pernicious, embedding hidden triggers in the data that lead models to misclassify only those inputs containing the trigger,  while maintaining high accuracy on benign samples.  In this paper, we propose Gradient Storm, a novel technique that facilitates the simultaneous execution of multiple backdoor attacks, while necessitating only minimal modification to the training dataset.  Our contributions are twofold: First, we introduce a method for designing adversarial poisons in modular components, each tailored based on a distinct region of the model\u2019s parameter space. Second, we present a framework for conducting multi-trigger attacks, where each trigger  causes  misclassification  from  a  specific  source  class  to  a  distinct  target class.  We evaluate the efficacy of Gradient Storm across multiple neural network architectures and two benchmark datasets, demonstrating its robustness against eight different poisoning defense mechanisms. Additionally, we show that poisons crafted for one model can be effectively transferred to other models, demonstrating that our attack remains effective even in black-box settings.", "title_embedding_index": 15873, "title_abs_embedding_index": 15898}, {"title": "Explainable Transfer Learning on Graphs Using a Novel Label Frequency Representation", "link_suffix": "/forum?id=6mLzCepPo8", "link": "https://openreview.net/forum?id=6mLzCepPo8", "pdf_link": "https://openreview.net/pdf?id=6mLzCepPo8", "keywords": "Transfer Learning; Graph Representation; Graph domain adaptation", "abstract": "Graphs are characterized by their versatility in representing objects from a wide range of domains, such as social networks or protein structures. This flexibility and power poses a significant challenge for  transfer learning between graph domains. Current methods of transfer learning between graph domains tend to focus exclusively on the structure of the underlying graphs, neglecting the characteristics of the nodes and not addressing the difficulties in comparing  nodes that represent very dissimilar entities, such as atoms and people for instance. In this paper, we propose a novel universal representation of graphs based on the relative frequency of the node labels. This novel representation enables explainable transfer learning between labeled graphs from different domains for the first time, without the need for additional adaptations. That is, we show that our novel representation can be readily combined with a data alignment technique that in turn allows transfer learning between data from different domains. Experimental results show that knowledge can be acquired from graphs belonging to chemical and biological domains to improve the accuracy of classification models in social network analysis. A comparison with state-of-the-art techniques indicates that our approach outperforms existing non-topological methods and, in some cases, even graph neural networks. In summary, our technique represents a major advance in graph node representation for transfer learning between different domains, opening up new perspectives for future research.", "title_embedding_index": 15874, "title_abs_embedding_index": 15899}]