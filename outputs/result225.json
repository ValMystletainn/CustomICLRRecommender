[
    {
        "title": "Mitigating Embedding Collapse in Diffusion Models for Categorical Data",
        "link_suffix": "/forum?id=JD6j7XSluo",
        "link": "https://openreview.net/forum?id=JD6j7XSluo",
        "pdf_link": "https://openreview.net/pdf?id=JD6j7XSluo",
        "keywords": "diffusion model, categorical data, embedding collapse",
        "abstract": "Latent diffusion models have enabled continuous-state diffusion models to handle a variety of datasets, including categorical data. However, most methods rely on fixed pretrained embeddings, limiting the benefits of joint training with the diffusion model. While jointly learning the embedding (via reconstruction loss) and the latent diffusion model (via score matching loss) could enhance performance, our analysis shows that end-to-end training risks embedding collapse, degrading generation quality. To address this issue, we introduce CATDM, a continuous diffusion framework within the embedding space that stabilizes training. We propose a novel objective combining the joint embedding-diffusion variational lower bound with a Consistency-Matching (CM) regularizer, alongside a shifted cosine noise schedule and random dropping strategy. The CM regularizer ensures the recovery of the true data distribution. Experiments on benchmarks show that CATDM mitigates embedding collapse, yielding superior results on FFHQ, LSUN Churches, and LSUN Bedrooms. In particular, CATDM achieves an FID of 6.81 on ImageNet $256\\times256$ with 50 steps. It outperforms non-autoregressive models in machine translation and is on a par with previous methods in text generation."
    },
    {
        "title": "Efficient Off-Policy Learning for High-Dimensional Action Spaces",
        "link_suffix": "/forum?id=JDzTI9rKls",
        "link": "https://openreview.net/forum?id=JDzTI9rKls",
        "pdf_link": "https://openreview.net/pdf?id=JDzTI9rKls",
        "keywords": "reinforcement learning, trust region",
        "abstract": "Existing off-policy reinforcement learning algorithms often rely on an explicit state-action-value function representation, which can be problematic in high-dimensional action spaces due to the curse of dimensionality.\nThis reliance results in data inefficiency as maintaining a state-action-value function in such spaces is challenging. \nWe present an efficient approach that utilizes only a state-value function as the critic for off-policy deep reinforcement learning.\nThis approach, which we refer to as Vlearn, effectively circumvents the limitations of existing methods by eliminating the necessity for an explicit state-action-value function. \nTo this end, we leverage a weighted importance sampling loss for learning deep value functions from off-policy data. \nWhile this is common for linear methods, it has not been combined with deep value function networks. \nThis transfer to deep methods is not straightforward and requires novel design choices such as robust policy updates, twin value function networks to avoid an optimization bias, and importance weight clipping.\nWe also present a novel analysis of the variance of our estimate compared to commonly used importance sampling estimators such as V-trace. \nOur approach improves sample complexity as well as final performance and ensures consistent and robust performance across various benchmark tasks.\nEliminating the state-action-value function in Vlearn facilitates a streamlined learning process, enabling more effective exploration and exploitation in complex environments."
    },
    {
        "title": "Causal Concept Graph Models: Beyond Causal Opacity in Deep Learning",
        "link_suffix": "/forum?id=lmKJ1b6PaL",
        "link": "https://openreview.net/forum?id=lmKJ1b6PaL",
        "pdf_link": "https://openreview.net/pdf?id=lmKJ1b6PaL",
        "keywords": "Concept Based Models, Concept Bottleneck Models, Explainable AI, Interpretability",
        "abstract": "Causal opacity denotes the difficulty in understanding the \"hidden\" causal structure underlying the decisions of deep neural network (DNN) models. This leads to the inability to rely on and verify state-of-the-art DNN-based systems, especially in high-stakes scenarios. For this reason, circumventing causal opacity in DNNs represents a key open challenge at the intersection of deep learning, interpretability, and causality. This work addresses this gap by introducing Causal Concept Graph Models (Causal CGMs), a class of interpretable models whose decision-making process is causally transparent by design. Our experiments show that Causal CGMs can: (i) match the generalisation performance of causally opaque models, (ii) enable human-in-the-loop corrections to mispredicted intermediate reasoning steps, boosting not just downstream accuracy after corrections but also the reliability of the explanations provided for specific instances, and (iii) support the analysis of interventional and counterfactual scenarios, thereby improving the model's causal interpretability and supporting the effective verification of its reliability and fairness."
    },
    {
        "title": "LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K",
        "link_suffix": "/forum?id=WQwy1rW60F",
        "link": "https://openreview.net/forum?id=WQwy1rW60F",
        "pdf_link": "https://openreview.net/pdf?id=WQwy1rW60F",
        "keywords": "large language model, long-context benchmark, knowledge leakage mitigation",
        "abstract": "State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 15 LLMs on LV-Eval and conduct ablation studies on the benchmarking techniques. The results reveal that:\n(i) Moonshot-v1 and recent large-scale open-source models, such as Qwen-2.5-72B and Llama-3.1-70B, achieve the highest performance on LV-Eval, particularly at lengths below $64k$. (ii) Models exhibit distinct score trends. For example, GLM-4-9B-128k, Yi-6B-200k, and Llama3-8B-1M exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of \"needle in a haystack\". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval."
    },
    {
        "title": "Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold",
        "link_suffix": "/forum?id=9SYczU3Qgm",
        "link": "https://openreview.net/forum?id=9SYczU3Qgm",
        "pdf_link": "https://openreview.net/pdf?id=9SYczU3Qgm",
        "keywords": "Flow matching, Dynamics, Cell dynamics",
        "abstract": "Numerous biological and physical processes can be modeled as systems of interacting entities evolving continuously over time, e.g. the dynamics of communicating cells or physical particles. Learning the dynamics of such systems is essential for predicting the temporal evolution of populations across novel samples and unseen environments. Flow-based models allow for learning these dynamics at the population level \u2014 they model the evolution of the entire distribution of samples. However, current flow-based models are limited to a single initial population and a set of predefined conditions which describe different dynamics. We argue that multiple processes in natural sciences have to be represented as vector fields on the Wasserstein manifold of probability densities. That is, the change of the population at any moment in time depends on the population itself due to the interactions between samples. In particular, this is crucial for personalized medicine where the development of diseases and their respective treatment response depends on the microenvironment of cells specific to each patient. We proposeMeta Flow Matching(MFM), a practical approach to integrating along these vector fields on the Wasserstein manifold by amortizing the flow model over the initial populations. Namely, we embed the population of samples using a Graph Neural Network (GNN) and use these embeddings to train a Flow Matching model. This gives MFM the ability to generalize over the initial distributions unlike previously proposed methods. We demonstrate the ability of MFM to improve prediction of individual treatment responses on a large scale multi-patient single-cell drug screen dataset."
    },
    {
        "title": "Robust Deep Reinforcement Learning against ADVERSARIAL BEHAVIOR MANIPULATION",
        "link_suffix": "/forum?id=UhW2wA1pRV",
        "link": "https://openreview.net/forum?id=UhW2wA1pRV",
        "pdf_link": "https://openreview.net/pdf?id=UhW2wA1pRV",
        "keywords": "reinforcement learning, robustness, adversarial attacks, adversarial defense, imitation learning",
        "abstract": "This study investigates the robustness of deep reinforcement learning agents against targeted attacks that aim to manipulate the victim's behavior through adversarial interventions in state observations. While several methods for such targeted manipulation attacks have been proposed, they all require white-box access to the victim's policy, and some rely on environment-specific heuristics. Furthermore, no defense method has been proposed to counter these attacks. To address this, we propose a novel targeted attack method for manipulating the victim, which does not depend on environmental heuristics and applies in black-box and no-box settings. Additionally, we introduce a defense strategy against these attacks. Our theoretical analysis proves that the sensitivity of a policy's action output to state changes affects the defense performance and that the earlier in the trajectory, the greater the effect. Based on this insight, we introduce a time-discounted regularization as a countermeasure for such behavior targeted attacks, which helps to improve robustness against attacks while maintaining task performance in the absence of attacks. Empirical evaluations demonstrate that our proposed attack method outperforms baseline attack methods. Furthermore, our defense strategy shows superior robustness against existing defense methods designed for untargeted attacks."
    },
    {
        "title": "ChatSR: Conversational Symbolic Regression",
        "link_suffix": "/forum?id=zsVZCiYG2r",
        "link": "https://openreview.net/forum?id=zsVZCiYG2r",
        "pdf_link": "https://openreview.net/pdf?id=zsVZCiYG2r",
        "keywords": "Symbolic Regression, Multi-modal Large Language Models, Scientific discovery",
        "abstract": "Formulas are the language of communication between humans and nature. It is an important research topic of artificial intelligence to find expressions from observed data to reflect the relationship between each variable in the data, which is called a symbolic regression problem. The existing symbolic regression methods directly generate expressions according to the given observation data, but we cannot require the algorithm to generate expressions that meet specific requirements according to the known prior knowledge. For example, the expression needs to contain the symbol `$\\sin$' or be periodicity, and so on. Even if it can, it often requires very complex operations, which is very inconvenient. In this paper, based on multi-modal large language models, we propose ChatSR, a conversational symbolic regression method that can generate expressions that meet the requirements simply by describing the requirements with natural language instructions. By experimenting on the test datasets, we can demonstrate that ChatSR leads the state-of-the-art baselines in fitting performance. More notably, ChatSR can well understand the prior knowledge contained in natural language prompts, and can further improve the quality of generated expressions according to the prior knowledge. In addition, it is exciting that ChatSR has good zero-shot capability."
    },
    {
        "title": "JetFormer: An autoregressive generative model of raw images and text",
        "link_suffix": "/forum?id=sgAp2qG86e",
        "link": "https://openreview.net/forum?id=sgAp2qG86e",
        "pdf_link": "https://openreview.net/pdf?id=sgAp2qG86e",
        "keywords": "generative pretraining, multimodal models, vision-language model, text-to-image, normalizing flow, generative model",
        "abstract": "Removing modeling constraints and unifying architectures across domains has been a key driver of the recent progress in training large multimodal models. However, most of these models still rely on many separately trained components such as modality-specific encoders and decoders. In this work, we further streamline joint generative modeling of images and text. We propose an autoregressive decoder-only transformer---JetFormer---which is trained to directly maximize the likelihood of raw data, without relying on any separately pretrained components, and can understand and generate both text and images. Specifically, we leverage a normalizing flow model to obtain a soft-token image representation that is jointly trained with an autoregressive multimodal transformer. The normalizing flow model serves as both an image encoder for perception tasks and an image decoder for image generation tasks during inference. JetFormer achieves text-to-image generation quality competitive with recent VQVAE- and VAE-based baselines. These baselines rely on pretrained image autoencoders, which are trained with a complex mixture of losses, including perceptual ones. At the same time, JetFormer demonstrates robust image understanding capabilities. To the best of our knowledge, JetFormer is the first model that is capable of generating high-fidelity images and producing strong log-likelihood bounds."
    },
    {
        "title": "A Continual Learning Perspective to Entropy Regularized Deep Reinforcement Learning",
        "link_suffix": "/forum?id=RYo2uU5el3",
        "link": "https://openreview.net/forum?id=RYo2uU5el3",
        "pdf_link": "https://openreview.net/pdf?id=RYo2uU5el3",
        "keywords": "deep reinforcement learning, entropy regularized policy iteration, continual learning",
        "abstract": "Research on Continual Learning (CL) tackles learning with non-stationary data distributions. The non-stationary nature of data is also one of the challenges of deep Reinforcement Learning (RL), and as a consequence, both CL and deep RL rely on similar approaches to stabilize learning, from the use of replay buffers to the choice of regularization terms. However, while dynamic neural architectures that grow in size to learn new tasks without forgetting older ones are well researched in CL, it remains a largely understudied research direction in RL. In this paper, we argue that Policy Mirror Descent (PMD), a regularized policy iteration RL algorithm, would naturally benefit from dynamic neural architectures as the current policy is a function of the sum of all past Q-functions. To avoid indefinitely increasing the neural architecture, we study PMD-like algorithms that only keep in memory the last $M$ Q-functions, and show that a convergent algorithm can be derived if $M$ is large enough. This theoretical analysis provides insights on how to utilise a fixed budget of Q-functions to reduce catastrophic forgetting in the policy. We implement this algorithm using a new neural architecture that stacks the last $M$ Q-functions as 3-dimensional tensors to allow for fast GPU computations. StaQ, the resulting algorithm, is competitive with state-of-the-art deep RL baselines and typically exhibits lower variance in performance. Beyond its performance, we argue that the simplicity and strong theoretical guarantees of StaQ's policy update makes it an ideal research tool over which we can further build a fully stable deep RL algorithm."
    },
    {
        "title": "MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions",
        "link_suffix": "/forum?id=GGlpykXDCa",
        "link": "https://openreview.net/forum?id=GGlpykXDCa",
        "pdf_link": "https://openreview.net/pdf?id=GGlpykXDCa",
        "keywords": "LLM evaluation, multi-table question answering; multi-hop question answering",
        "abstract": "While large language models (LLMs) have made strides in understanding tabular data, current tabular evaluation benchmarks, such as WikiTableQuestions and WikiSQL, are focus on single-table scenarios, which cannot necessarily reflect the complexity of real-world applications. To bridge this gap, we present a \\textbf{M}ulti-table and \nMulti-hop Question Answering (MMQA) dataset to assess LLMs' understanding and reasoning capabilities in handling multi-table tasks. The MMQA dataset demands that models perform multiple inferences by drawing evidence from various tables, which are designed to be connected with each other and require models to identify and utilize relationships such as foreign and primary keys. Then, we introduce a comprehensive evaluation framework that tailors to assess LLMs' capabilities in several aspects including Multi-Table Retrieval, Text-to-SQL Generation, Multi-Table QA, Primary Key Selection, and Foreign Key Selection. \nFinally, we propose a novel multi-table retrieval method that achieves state-of-the-art (SOTA) performance on the MMQA dataset compared to several strong baselines. \nOur experiment results reveal that, compared with human performance, both open-source and commercial LLMs leave significant performance room for improvements in multi-table understanding and reasoning tasks. We believe that the MMQA benchmark will enhance and facilitate LLMs' multi-table capabilities in real-world scenarios."
    },
    {
        "title": "EEGTrans: Transformer-Driven Generative Models for EEG Synthesis",
        "link_suffix": "/forum?id=ydw2l8zgUB",
        "link": "https://openreview.net/forum?id=ydw2l8zgUB",
        "pdf_link": "https://openreview.net/pdf?id=ydw2l8zgUB",
        "keywords": "LLM, EEG, BCI, transformer",
        "abstract": "Recent advancements in Large Language Models (LLMs) have been significant, largely due to improvements in network architecture, particularly the transformer model. With access to large training datasets, LLMs can train in an unsupervised manner and still achieve impressive results in generating coherent output. This study introduces a transformer-based generative model, EEGTrans, designed for sequentially generating synthetic electroencephalogram (EEG) signals. Given the inherent noise in EEG data, we employ a quantized autoencoder that compresses these signals into discrete codes, effectively capturing their temporal features and enabling generalization across diverse datasets. The encoder of EEGTrans processes EEG signals as input, while its decoder autoregressively generates discrete codes. We evaluate our method in a motor imagery Brain-Computer Interface (BCI) application, where merging data across datasets is particularly challenging due to experimental differences. Our results demonstrate that the synthetic EEG data effectively captures temporal patterns while maintaining the complexity and power spectrum of the original signals. Moreover, classification results show that incorporating synthetic data improves performance and even surpasses that of models based on Generative Adversarial Networks. These findings highlight the potential of transformer-based generative models to generalize effectively across multiple datasets and produce high-quality synthetic EEG signals."
    },
    {
        "title": "Counterfactual Concept Bottleneck Models",
        "link_suffix": "/forum?id=w7pMjyjsKN",
        "link": "https://openreview.net/forum?id=w7pMjyjsKN",
        "pdf_link": "https://openreview.net/pdf?id=w7pMjyjsKN",
        "keywords": "Concept Bottleneck Models, Concept Based Model, Counterfactuals, Explainable AI, Interpretability",
        "abstract": "Current deep learning models are not designed to simultaneously address three fundamental questions: predict class labels to solve a given classification task (the \"What?\"), simulate changes in the situation to evaluate how this impacts class predictions (the \"How?\"), and imagine how the scenario should change to result in different class predictions (the \"Why not?\"). The inability to answer these questions represents a crucial gap in deploying reliable AI agents, calibrating human trust, and improving human-machine interaction. To bridge this gap, we introduce CounterFactual Concept Bottleneck Models (CF-CBMs), a class of models designed to efficiently address the above queries all at once without the need to run post-hoc searches. Our experimental results demonstrate that CF-CBMs: achieve classification accuracy comparable to black-box models and existing CBMs (\u201cWhat?\u201d), rely on fewer important concepts leading to simpler explanations (\u201cHow?\u201d), and produce interpretable, concept-based counterfactuals (\u201cWhy not?\u201d). Additionally, we show that training the counterfactual generator jointly with the CBM leads to two key improvements: (i) it alters the model's decision-making process, making the model rely on fewer important concepts (leading to simpler explanations), and (ii) it significantly increases the causal effect of concept interventions on class predictions, making the model more responsive to these changes."
    },
    {
        "title": "LSTR: Long-Short Range Aggregation for Trajectory Prediction at Intersection Scenarios",
        "link_suffix": "/forum?id=vMA0ATykNU",
        "link": "https://openreview.net/forum?id=vMA0ATykNU",
        "pdf_link": "https://openreview.net/pdf?id=vMA0ATykNU",
        "keywords": "motion prediction, autonomous driving, path_planning",
        "abstract": "Trajectory prediction is crucial for practical applications, encompassing navigation for autonomous vehicles and the implementation of safety systems based on the Internet of Vehicles (IoV). Most existing methods significantly rely on comprehensive map information, employing robust rule constraints to incrementally predict trajectories within the driver's local decision-making context. However, in environments characterized by weak rule enforcement, such as urban intersections, these approaches neglect the disparity between the driver's overarching intentions and current behaviors.Recognizing the characteristics of intersection traffic flow\u2014macroscopically organized yet microscopically disordered, exhibiting highly heterogeneous conditions\u2014this paper presents a novel model termed Long-short Range Aggregation for Trajectory Prediction in Intersections (LSTR). This model anchors the vehicle's local decision-making process to long-range intentions. Specifically, LSTR predicts the vehicle's destination via a global intention inference module and models its long-range driving intentions through clustering to extract macroscopic traffic flow patterns. This long-range intention subsequently informs the short-range local interaction behaviors captured by the local behavior decision module. Ultimately, the fused features from these two modules are analyzed using a multi-modal decoder to interpret the various motion patterns, resulting in the trajectory prediction outcomes.We rigorously validate the proposed framework across multiple intersection scenarios utilizing real-world datasets, including inD, roundD, and a subset of WOMD. Experimental results demonstrate that our model outperforms numerous benchmarks without relying on additional information such as HD maps of intersections."
    },
    {
        "title": "Large Language Models Engineer Too Many Simple Features for Tabular Data",
        "link_suffix": "/forum?id=1JhSJIYX3p",
        "link": "https://openreview.net/forum?id=1JhSJIYX3p",
        "pdf_link": "https://openreview.net/pdf?id=1JhSJIYX3p",
        "keywords": "LLMs, feature engineering, bias, tabular data, automated data science",
        "abstract": "Tabular machine learning problems often require time-consuming and labor-intensive feature engineering.\nRecent efforts have focused on using large language models (LLMs) to capitalize on their potential domain knowledge. \nAt the same time, researchers have observed ethically concerning negative biases in other LLM-related use cases, such as text generation. These developments motivated us to investigate whether LLMs exhibit a bias that negatively impacts the performance of feature engineering. While not ethically concerning, such a bias could hinder practitioners from fully utilizing LLMs for automated data science. \nTherefore, we propose a method to detect potential biases by detecting anomalies in the frequency of operators (e.g., adding two features) suggested by LLMs when engineering new features. Our experiments evaluate the bias of four LLMs, two big frontier and two small open-source models, across 27 tabular datasets. Our results indicate that LLMs are biased toward simple operators, such as addition, and can fail to utilize more complex operators, such as grouping followed by aggregations. Furthermore, the bias can negatively impact the predictive performance when using LLM-generated features. Our results call for mitigating bias when using LLMs for feature engineering."
    },
    {
        "title": "Neural Logical Index for Fast Knowledge Graph Complex Query Answering",
        "link_suffix": "/forum?id=PbxKOPtoEE",
        "link": "https://openreview.net/forum?id=PbxKOPtoEE",
        "pdf_link": "https://openreview.net/pdf?id=PbxKOPtoEE",
        "keywords": "complex logical query, knowledge graph, query embedding, neural link predictor",
        "abstract": "Complex query answering (CQA) over knowledge graphs is a crucial multi-hop reasoning task aimed at addressing first-order logical queries within large and incomplete knowledge graphs. Direct traversal search methods rely solely on graph topology and often miss answers due to the incompleteness of the graph, thus neural models have been proposed to generalize the neglected answers from observed facts. There are primarily two lines of research tackling the challenges of CQA. Query embedding models learn representations for complex queries, offering fast speed but often providing only generic performance. In contrast,  neural symbolic search methods deliver better performance, although they tend to be computationally more expensive. In this paper, we propose an efficient and scalable search framework that combines the precision of symbolic methods with the speed of embedding techniques. Our model utilizes embedding methods to compute Neural Logical Indices (NLI) to reduce the search domain for each variable in advance, followed by an approximate symbolic search for fine ranking. The search is precise for tree-structured queries and approximates cyclic queries (which are NP-complete) in quadratic complexity concerning the search domain, matching the complexity of tree-form queries. Experiments on various CQA benchmarks show that our framework reduces computation by 90% with a minimal performance loss, alleviating both efficiency and scalability issues."
    },
    {
        "title": "Automatically Interpreting Millions of  Features in Large Language Models",
        "link_suffix": "/forum?id=5lIXRf8Lnw",
        "link": "https://openreview.net/forum?id=5lIXRf8Lnw",
        "pdf_link": "https://openreview.net/pdf?id=5lIXRf8Lnw",
        "keywords": "interpretability, language model, sae, features, explanation",
        "abstract": "While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. But these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our pipeline on SAEs of varying sizes, activation functions, and losses, trained on three different open-weight LLMs. We present new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. We propose guidelines for how to generate better explanations that remain valid for a broader set of activating contexts, and discuss common pitfalls with the current scoring techniques. We also introduce a novel similarity metric for SAEs, and find that SAEs trained on nearby layers of the residual stream are much more similar than ones trained on adjacent MLPs. We anticipate that the open-source framework proposed can improve future evaluations of the interpretability of SAEs and will enable more work on this front."
    },
    {
        "title": "A GPU-accelerated Large-scale Simulator for Transportation System Optimization Benchmarking",
        "link_suffix": "/forum?id=yr0l1IoyzV",
        "link": "https://openreview.net/forum?id=yr0l1IoyzV",
        "pdf_link": "https://openreview.net/pdf?id=yr0l1IoyzV",
        "keywords": "microscopic traffic simulator, transportation system optimization, GPU acceleration",
        "abstract": "With the development of artificial intelligence techniques, transportation system optimization is evolving from traditional methods relying on expert experience to simulation and learning-based decision and optimization methods.\nLearning-based optimization methods require extensive interactions with highly realistic microscopic traffic simulators.\nHowever, existing microscopic traffic simulators are inefficient in large-scale scenarios and thus fail to support the adoption of these methods in large-scale transportation system optimization scenarios.\nIn addition, the optimization scenarios supported by existing simulators are limited, mainly focusing on the traffic signal control.\nTo address these challenges, we propose the first open-source GPU-accelerated large-scale microscopic simulator for transportation system simulation and optimization.\nThe simulator can iterate at 84.09Hz, which achieves 88.92 times computational acceleration in the large-scale scenario with 2,464,950 vehicles compared to the best baseline CityFlow.\nBesides, it achieves a more realistic average road speeds simulated on real datasets by adopting the IDM model as the car-following model and the randomized MOBIL model as the lane-changing model.\nBased on it, we implement a set of microscopic and macroscopic controllable objects and metrics provided by Python API to support typical transportation system optimization scenarios including traffic signal control, dynamic lane assignment within junctions, tidal lane control, congestion pricing, road planning, e.t.c.\nWe choose five representative transportation system optimization scenarios and benchmark classical rule-based algorithms, reinforcement learning algorithms, and black-box optimization algorithms in four cities.\nThese experiments effectively demonstrate the usability of the simulator for large-scale traffic system optimization.\nThe anonymous code of the simulator is available athttps://anonymous.4open.science/r/moss-AF45and the others are shown at Appendix A.\nIn addition, we build an open-registration web platform to support no-code trials."
    },
    {
        "title": "FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs",
        "link_suffix": "/forum?id=RSGoXnS9GH",
        "link": "https://openreview.net/forum?id=RSGoXnS9GH",
        "pdf_link": "https://openreview.net/pdf?id=RSGoXnS9GH",
        "keywords": "Fairness, Benchmark, Large language model",
        "abstract": "The growing use of large language model (LLM)-based chatbots has raised concerns about fairness. Fairness issues in LLMs can lead to severe consequences, such as bias amplification, discrimination, and harm to marginalized communities. While existing fairness benchmarks mainly focus on single-turn dialogues, multi-turn scenarios, which in fact better reflect real-world conversations, present greater challenges due to conversational complexity and potential bias accumulation. In this paper, we propose a comprehensive fairness benchmark for LLMs in multi-turn dialogue scenarios, FairMT-Bench. Specifically, we formulate a task taxonomy targeting LLM fairness capabilities across three stages: context understanding, user interaction, and instruction trade-offs, with each stage comprising two tasks. To ensure coverage of diverse bias types and attributes, we draw from existing fairness datasets and employ our template to construct a multi-turn dialogue dataset, FairMT 10K. For evaluation, GPT-4 is applied, alongside bias classifiers including Llama-Guard-3 and human validation to ensure robustness. Experiments and analyses on FairMT 10K reveal that in multi-turn dialogue scenarios, current LLMs are more likely to generate biased responses, and there is significant variation in performance across different tasks and models. Based on this, we curate a challenging dataset, FairMT 1K, and test 15 current state-of-the-art (SOTA) LLMs on this dataset. The results show the current state of fairness in LLMs and showcase the utility of this novel approach for assessing fairness in more realistic multi-turn dialogue contexts, calling for future work to focus on LLM fairness improvement and the adoption of FairMT 1K in such efforts."
    },
    {
        "title": "B-MoCA: Benchmarking Mobile Device Control Agents across Diverse Configurations",
        "link_suffix": "/forum?id=Qg6Z3VcA1U",
        "link": "https://openreview.net/forum?id=Qg6Z3VcA1U",
        "pdf_link": "https://openreview.net/pdf?id=Qg6Z3VcA1U",
        "keywords": "Mobile device control, Benchmark, Decision-making agents, LLM agents",
        "abstract": "Mobile device control agents can largely enhance user interactions or productivity by automating daily tasks. However, despite growing interest in developing practical agents, the absence of a commonly adopted benchmark in this area makes it challenging to quantify scientific progress. In this work, we introduce B-MoCA: a novel benchmark with interactive environments for evaluating and developing mobile device control agents. To create a realistic benchmark, we develop B-MoCA based on the Android operating system and define 131 common daily tasks. Importantly, we incorporate a randomization feature that changes the configurations of mobile devices, including user interface layouts and language settings, to assess generalization performance. We benchmark diverse agents, including agents employing large language models (LLMs) or multi-modal LLMs as well as agents trained with imitation learning using human expert demonstrations. While these agents demonstrate proficiency in executing straightforward tasks, their poor performance on complex tasks highlights significant opportunities for future research to improve effectiveness."
    },
    {
        "title": "Multi-domain Analysis and Generalization of Image manipulation loCalization",
        "link_suffix": "/forum?id=9YZKbSoDr6",
        "link": "https://openreview.net/forum?id=9YZKbSoDr6",
        "pdf_link": "https://openreview.net/pdf?id=9YZKbSoDr6",
        "keywords": "Domain generalization, Diffusion-Based Inpainting, Misinformation Detection, Computer Vision, Benchmark Dataset, Visual Forensics",
        "abstract": "Advanced image editing software enables easy creation of highly convincing image manipulations, which has been made even more accessible in recent years due to advances in generative AI. Manipulated images, while often harmless, could spread misinformation, create false narratives, and influence people\u2019s opinions on important issues. Despite this growing threat, current research on detecting advanced manipulations across different visual domains, remains limited. Thus, we introduce Multi-domain Analysis and Generalization of Image manipulation loCalization (MAGIC), a comprehensive benchmark designed for studying generalization across several axes in image manipulation detection. MAGIC comprises over 192K images from two distinct sources (user and news photos), spanning a diverse range of topics and manipulation sizes. We focus on images manipulated using recent diffusion-based inpainting methods, which are largely absent in existing datasets. We conduct experiments under different types of domain shift to evaluate robustness of existing image manipulation detection methods. Our goal is to drive further research in this area by offering new insights that would help develop more reliable and generalizable image manipulation detection methods. We will release the dataset after this work is published."
    },
    {
        "title": "Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models",
        "link_suffix": "/forum?id=EbxYDBhE3S",
        "link": "https://openreview.net/forum?id=EbxYDBhE3S",
        "pdf_link": "https://openreview.net/pdf?id=EbxYDBhE3S",
        "keywords": "Backdoor Unalignment, Backdoor Defense, Instruction-tuned LLMs, AI Safety, AI Security",
        "abstract": "Backdoor unalignment attacks against Large Language Models (LLMs) enable the stealthy compromise of safety alignment using a hidden trigger while evading normal safety auditing. These attacks pose significant threats to the applications of LLMs in the real-world Large Language Model as a Service (LLMaaS) setting, where the deployed model is a fully black-box system that can only interact through text. Furthermore, the sample-dependent nature of the attack target exacerbates the threat. Instead of outputting a fixed label, the backdoored LLM follows the semantics of any malicious command with the hidden trigger, significantly expanding the target space. In this paper, we introduce BEAT, a black-box defense that detects triggered samples during inference to deactivate the backdoor. It is motivated by an intriguing observation (dubbed theprobe concatenate effect), where concatenated triggered samples significantly reduce the refusal rate of the backdoored LLM towards a malicious probe, while non-triggered samples have little effect. Specifically, BEAT identifies whether an input is triggered by measuring the degree of distortion in the output distribution of the probe before and after concatenation with the input. Our method addresses the challenges of sample-dependent targets from an opposite perspective. It captures the impact of the trigger on the refusal signal (which is sample-independent) instead of sample-specific successful attack behaviors. It overcomes black-box access limitations by using multiple sampling to approximate the output distribution. Extensive experiments are conducted on various backdoor attacks and LLMs (including the closed-source GPT-3.5-turbo), verifying the effectiveness and efficiency of our defense. Our source code is available athttps://anonymous.4open.science/r/BEAT-0065."
    },
    {
        "title": "ROBO-INSTRUCT: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs",
        "link_suffix": "/forum?id=634kHJgaOL",
        "link": "https://openreview.net/forum?id=634kHJgaOL",
        "pdf_link": "https://openreview.net/pdf?id=634kHJgaOL",
        "keywords": "Finetune LLM For Domain Specific Application, Angelic Execution, Self-Instruct, Synthesize Simulation Environment, CodeLLMs for Robotics",
        "abstract": "Open-weight LLMs are particularly appealing choices to generate training data for fine-tuning Code LLMs on domain-specific service robot applications because they are cost-effective, customizable, and offer better privacy protection. However, unlike proprietary LLMs, open-weight models are more error-prone and often produce programs that violate domain-specific constraints. A promising solution is to incorporate a robot simulator with a well-defined environment to verify program correctness. Yet, these environments require pre-enumeration of relevant entities and their states, which limits the diversity of programs that can be effectively verified. In this work, we introduce ROBO-INSTRUCT that preserves the diversity of programs generated by an LLM while providing the correctness of simulator-based checking. ROBO-INSTRUCT introduces ROBOSIM to dynamically synthesize consistent simulation environments for each generated program. Moreover, ROBO-INSTRUCT handles subtler instruction-program inconsistencies that do not result in a constraint violation via INSTALIGN, an LLM-aided instruction-program alignment process. Given domain-specific APIs and a few seed examples, ROBO-INSTRUCT can leverage an 8B Llama3 model to generate a training dataset for fine-tuning a 7B CodeLlama model. Our fine-tuned model achieves a 28.75% improvement in pass@1 over the original base model and a 13.75% improvement compared to its SELF-INSTRUCT-finetuned counterparts, even surpassing the performance of a few proprietary LLMs, such as GPT-3.5-Turbo and Gemini-Pro."
    },
    {
        "title": "Conditional Density Ratio Score for Post Hoc Deep Outlier Detection",
        "link_suffix": "/forum?id=fsEzHMqbkf",
        "link": "https://openreview.net/forum?id=fsEzHMqbkf",
        "pdf_link": "https://openreview.net/pdf?id=fsEzHMqbkf",
        "keywords": "Deep Learning, Outlier Detection, Out-Of-Distribution Detection",
        "abstract": "The ability to accurately identify out-of-distribution (OOD) samples is essential not only as a stand-alone machine learning task but also for maintaining the reliability and safety of machine learning systems. Within this domain, post hoc density estimators like the energy score are popular ways for detecting OOD samples. However, most of the existing post hoc density estimation have mainly focused on marginalizing the conditional distributions over all possible classes. In this paper, we introduce the Conditional Density Ratio (CDR) score, a principled post hoc density estimator that leverages both a class-conditional generative model in the latent space and a discriminative classifier model, allowing us to estimate the marginal densities of the latent representation without marginalization. We demonstrate that a key component to the success of the CDR score lies in correctly calibrating the two models and propose a simple yet effective method to automatically tune the temperature parameter without the need for out-of-distribution samples. We illustrate the general compatibility of the proposed method with two popular density estimators, the kernel density estimator and the Mahalanobis estimator. Through experiments on a wide range of OOD benchmark tasks, we verify the effectiveness of the proposed method and advocate it as an easy-to-implement baseline that can achieve competitive performance in most tested scenarios."
    },
    {
        "title": "Can Symbolic Regression of Boolean Functions Boost Logic Synthesis?",
        "link_suffix": "/forum?id=OzwGZP8h2A",
        "link": "https://openreview.net/forum?id=OzwGZP8h2A",
        "pdf_link": "https://openreview.net/pdf?id=OzwGZP8h2A",
        "keywords": "Chip Design, Logic Synthesis, Symbolic Regression, Monte-Carlo Tree Search",
        "abstract": "Logic synthesis, which aims to synthesize a compact logic circuit with minimized size while exactly satisfying a given functionality, plays an important role in chip design. Recently, symbolic regression (SR) has shown great success in scientific discovery to recover underlying mathematical functions from given datasets. However, we found from extensive experiments that existing SR methods struggle to recover an exact and compact boolean function for logic synthesis given a truth table, i.e., complete input-output pairs of the circuit. The major challenges include (1) the greater complexity of underlying boolean functions compared to mathematical functions, and (2) the complex objectives involving both exact recovery and expression optimization towards circuit minimization. To address these challenges, we propose a novel symbolic factorized boolean searcher (SINE) to recover exact and compact boolean functions towards logic synthesis. Motivated by the Shannon decomposition theorem, SINE proposes a factorized boolean function representation to decompose the underlying boolean function into multiple simplified sub-functions, significantly reducing their complexity and thus improving the recovery accuracy. Moreover, based on the key observation that, logical sharing is significant for circuit size minimization. SINE proposes a self symmetric sub-expression motif operators mining mechanism to enhance the monte-carlo tree search method for optimized boolean function learning. To the best of our knowledge, SINE is the first symbolic regression framework capable of exactly recovering optimized boolean functions for circuit optimization. Experiments on circuits across a wide range of inputs demonstrate that SINE significantly improves the recovery accuracy and decreases the size of synthesized circuits by up to 24.32% compared to state-of-the-art methods."
    },
    {
        "title": "Deciphering the Chaos: Enhancing Jailbreak Attacks via Adversarial Prompt Translation",
        "link_suffix": "/forum?id=iKgQOAtvsD",
        "link": "https://openreview.net/forum?id=iKgQOAtvsD",
        "pdf_link": "https://openreview.net/pdf?id=iKgQOAtvsD",
        "keywords": "Large lanugage model, Jailbreak attack, adversarial prompt",
        "abstract": "Automatic adversarial prompt generation provides remarkable success in jailbreaking safely-aligned large language models (LLMs). Existing gradient-based attacks, while demonstrating outstanding performance in jailbreaking white-box LLMs, often generate garbled adversarial prompts with chaotic appearance. These adversarial prompts are difficult to transfer to other LLMs, hindering their performance in attacking unknown victim models. In this paper, for the first time, we delve into the semantic meaning embedded in garbled adversarial prompts and propose a novel method that\"translate\"them into coherent, human-readable natural language adversarial prompts. In this way, we can effectively uncover the semantic information that triggers vulnerabilities in the model and unambiguously transfer it to the victim model, without overlooking the adversarial information hidden in the garbled text, to enhance jailbreak attacks. It also offers a new approach to discovering effective designs for jailbreak prompts, advancing the understanding of jailbreak attacks. Experimental results demonstrate that our method significantly improves the success rate of jailbreak attacks against various safety-aligned LLMs and outperforms state-of-the-arts by a large margin. With at most 10 queries, our method achieves an average attack success rate of 81.8% in attacking 7 commercial closed-source LLMs, including GPT and Claude-3 series, on HarmBench. Our method also achieves over 90% attack success rates against Llama-2-Chat models on AdvBench, despite their outstanding resistance to jailbreaks. Our code will be made publicly available."
    }
]