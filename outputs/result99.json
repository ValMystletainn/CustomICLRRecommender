[{"title": "Better Instruction-Following Through Minimum Bayes Risk", "link_suffix": "/forum?id=7xCSK9BLPy", "link": "https://openreview.net/forum?id=7xCSK9BLPy", "pdf_link": "https://openreview.net/pdf?id=7xCSK9BLPy", "keywords": "LLM, instruction-following, decoding, MBR, minimal bayes risk, LLM judges", "abstract": "General-purpose LLM judges capable of human-level evaluation provide not only a scalable and accurate way of evaluating instruction-following LLMs but also new avenues for supervising and improving their performance. One promising way of leveraging LLM judges for supervision is through Minimum Bayes Risk (MBR) decoding, which uses a reference-based evaluator to select a high-quality output from amongst a set of candidate outputs. In the first part of this work, we explore using MBR decoding as a method for improving the test-time performance of instruction-following LLMs. We find that MBR decoding with reference-based LLM judges substantially improves over greedy decoding, best-of-N decoding with reference-free judges and MBR decoding with lexical and embedding-based metrics on AlpacaEval and MT-Bench. These gains are consistent across LLMs with up to 70B parameters, demonstrating that smaller LLM judges can be used to supervise much larger LLMs. Then, seeking to retain the improvements from MBR decoding while mitigating additional test-time costs, we explore iterative self-training on MBR-decoded outputs. We find that self-training using Direct Preference Optimisation leads to significant performance gains, such that the self-trained models with greedy decoding generally match and sometimes exceed the performance of their base models with MBR decoding.", "title_embedding_index": 4900, "title_abs_embedding_index": 4925}, {"title": "R\u00e9nyi Regularised Reinforcement Learning", "link_suffix": "/forum?id=o10clUzFRH", "link": "https://openreview.net/forum?id=o10clUzFRH", "pdf_link": "https://openreview.net/pdf?id=o10clUzFRH", "keywords": "Reinforcement Learning, Variational Inference, R\u00e9nyi divergence", "abstract": "Entropy regularisation has proven effective in reinforcement learning (RL) for encouraging exploration. Recent work demonstrating the equivalence between entropy regularised RL and approximate probabilistic inference suggests the potential for improving existing methods by generalising the inference procedure. We develop the R\u00e9nyi regularised RL framework by using R\u00e9nyi variational inference to learn a stochastic policy. We present theoretical results for policy evaluation and improvement within this new framework. Additionally, we propose two novel algorithms, $\\alpha$-SAC and $\\alpha$-SQL, for large-scale RL tasks. We show that these algorithms attain higher returns on games from the Atari suite relative to an entropy-regularised benchmark, SAC-Discrete.", "title_embedding_index": 4901, "title_abs_embedding_index": 4926}, {"title": "FLAIR: FEDERATED LEARNING WITH AUGMENTED AND IMPROVED FEATURE REPRESENTATIONS", "link_suffix": "/forum?id=C7XoUdJ5ZC", "link": "https://openreview.net/forum?id=C7XoUdJ5ZC", "pdf_link": "https://openreview.net/pdf?id=C7XoUdJ5ZC", "keywords": "Federated Learning, Class Variational Autoencoders, Feature Augmentation, Data Heterogeneity, Privacy Preservation, Communication Efficiency", "abstract": "Federated Learning (FL) enables collaborative model training across decentralized clients while preserving data privacy. However, its performance declines in challenging  heterogeneous data settings. To mitigate this, existing FL frameworks not only share locally trained parameters but also exchange additional information -- such as control variates, client features, and classifier characteristics -- to address the effects of class imbalance and missing classes. However, this leads to increased communication costs and heightened risks of privacy breaches. To strike a balance between communication efficiency, privacy protection, and adaptability to heterogeneous data distributions, we propose FLAIR, a novel FL approach with augmented and improved feature representations. FLAIR utilizes Class Variational Autoencoders (CVAE) for feature augmentation, mitigating class imbalance and missing class issues. It also incorporates Reptile meta-training to facilitate knowledge transfer between model updates, adapting to dynamic feature shifts. To generalize model update, FLAIR shares only local CVAE parameters instead of local model parameters, which reduces both communication costs and privacy risks. Our experiments on benchmark datasets -- such as MNIST, CIFAR-10, CIFAR-100, and TinyImageNet -- demonstrates a significant enhancement in model convergence and accuracy compared to state-of-the-art solutions, while reducing communication overhead and privacy risks.", "title_embedding_index": 4902, "title_abs_embedding_index": 4927}, {"title": "Subtle Errors Matter: Preference Learning via Error-injected Self-editing", "link_suffix": "/forum?id=EHhLLmDvtE", "link": "https://openreview.net/forum?id=EHhLLmDvtE", "pdf_link": "https://openreview.net/pdf?id=EHhLLmDvtE", "keywords": "Mathematical Reasoning, Preference Learning", "abstract": "Large Language Models (LLMs) have exhibited strong mathematical reasoning and computational prowess, tackling tasks ranging from basic arithmetic to advanced competition-level problems. However, frequently occurring subtle errors, such as miscalculations or incorrect substitutions, limit the models\u2019 full mathematical potential. Existing studies to improve mathematical ability typically involve distilling reasoning skills from stronger LLMs or applying preference learning to step-wise response pairs. Although these methods leverage samples of varying granularity to mitigate reasoning errors, they overlook the frequently occurring subtle errors. A major reason is that sampled preference pairs involve differences unrelated to the errors, which may distract the model from focusing on subtle errors. In this work, we propose a novel preference learning framework called eRror-Injected Self-Editing (RISE), which injects predefined subtle errors into partial tokens of correct solutions to construct hard pairs for error mitigation. In detail, RISE uses the model itself to edit a small number of tokens in the solution, injecting designed subtle errors. Then, pairs composed of self-edited solutions and their corresponding correct ones, along with pairs of correct and incorrect solutions obtained through sampling, are used together for subtle error-aware DPO training. Compared with other preference learning methods, RISE further refines the training objective to focus on predefined errors and their tokens, without requiring fine-grained sampling or preference annotation. Extensive experiments validate the effectiveness of RISE, with preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0% on GSM8K and 7.9% on MATH.", "title_embedding_index": 4903, "title_abs_embedding_index": 4928}, {"title": "MMD-NSL: Mixed Multinomial Distribution-based Neuro-Symbolic Learning", "link_suffix": "/forum?id=xOZYU67EKL", "link": "https://openreview.net/forum?id=xOZYU67EKL", "pdf_link": "https://openreview.net/pdf?id=xOZYU67EKL", "keywords": "Neuro-Symbolic Learning, Multinomial Mixture Distribution", "abstract": "Neuro-symbolic learning (NSL) aims to integrate neural networks with symbolic reasoning approaches to enhance the interpretability of machine learning models. Existing methods mostly focus on the long dependency problem of symbolic learning. The important challenge of complex categorization is largely overlooked. To bridge this gap, we propose the Mixed Multinomial Distribution-based NSL MMD-NSL framework. It seamlessly integrates the handling of long dependency chains and complex semantic categorization within Knowledge Graphs (KGs). By introducing a continuous Mixed Multinomial Logic Semantic Distribution, we extend traditional Markov Logic Networks (MLN) to incorporate context-aware semantic embeddings. Our theoretical innovations, including a bijective mapping between MLNs and continuous multinomial distributions, enable the capture of intricate dependencies and varied contexts crucial for NSL tasks.\nThe framework leverages a bilevel optimization strategy, where a transformer-based upper level dynamically learns mixing coefficients akin to attention mechanisms, while the lower level optimizes rule weights for learning both context and rule patterns. Extensive experiments on the DWIE benchmarking datasets demonstrate significant advantages of MMD-NSL over four state-of-the-art approaches. It achieves 10.47% higher F1-scores on average than the best-performing baseline across 23 sub-datasets. It advances continuous probabilistic models for neuro-symbolic reasoning and complex relational tasks.", "title_embedding_index": 4904, "title_abs_embedding_index": 4929}, {"title": "Language Model Merging in Iterative Preference Learning", "link_suffix": "/forum?id=kF3tNnhkvX", "link": "https://openreview.net/forum?id=kF3tNnhkvX", "pdf_link": "https://openreview.net/pdf?id=kF3tNnhkvX", "keywords": "preference learning, model merging", "abstract": "Learning from preferences has become a scalable paradigm for training high-capacity language models, as it is not limited to human-produced data, allowing models to surpass human performance.\nAdvanced feedback learning algorithms are typically online or iterative for high sample efficiency.\nAmong these, iterative preference optimization is popular due to its simplicity, efficiency, and robustness.\nHowever, in iterative preference optimization, models do not necessarily achieve optimal performance since they sequentially learn data from different distributions.\nA simple way to bridge the gap is model ensemble, which incurs excessive inference costs.\nInspired by the theoretical analysis for preference learning, we propose a simple model merging strategy that approximates model ensemble without additional training and inference costs, leading to Pareto-superior models.", "title_embedding_index": 4905, "title_abs_embedding_index": 4930}, {"title": "Evaluating the Unseen: A Novel Framework for Assessing Unsupervised Concept Bottleneck Models", "link_suffix": "/forum?id=kTjEPEy96Q", "link": "https://openreview.net/forum?id=kTjEPEy96Q", "pdf_link": "https://openreview.net/pdf?id=kTjEPEy96Q", "keywords": "Explainable Artificial Intelligence; Metric; Concept Bottleneck Model", "abstract": "In recent years, the field of explainable artificial intelligence (XAI) has gained significant traction, with concept bottleneck models (CBMs) emerging as a promising approach to enhance the interpretability of machine learning systems. However, CBMs often rely on expert-annotated concepts, which can be costly and time-consuming to acquire. To address this limitation, unsupervised and label-free CBMs have been proposed, but these come with their own challenges, particularly in assessing the reliability and accuracy of the generated concepts without ground-truth labels. This paper introduces a comprehensive evaluation framework designed to assess the quality of explanations produced by unsupervised CBMs. Our framework comprises a set of novel metrics that evaluate various aspects of the concept outputs, including their relevance, consistency, and informativeness. We demonstrate the effectiveness of our metrics through a series of experiments, showing certain positive correlations between our scores and both LLM evaluations and human judgments. Our work not only fills a critical gap in the evaluation of unsupervised CBMs but also provides a solid foundation for further research into more transparent and trustworthy AI systems.", "title_embedding_index": 4906, "title_abs_embedding_index": 4931}, {"title": "IntGrad MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Interpolation Guided Gradual MT", "link_suffix": "/forum?id=SmxM4POTBk", "link": "https://openreview.net/forum?id=SmxM4POTBk", "pdf_link": "https://openreview.net/pdf?id=SmxM4POTBk", "keywords": "Machine Translation, Self-domonstration, Sentence Interpolation, Low Resource Language", "abstract": "Recent Large Language Models (LLMs) have demonstrated strong performance in translation without needing to be finetuned on additional parallel corpora. However, they still underperform for low-resource language pairs. Previous works have focused on mitigating this issue by leveraging relevant few-shot examples or external resources such as dictionaries or grammar books, making models heavily reliant on these nonparametric sources of information. In this paper, we propose a novel method named IntGrad MT that focuses on fully exploiting an LLM\u2019s inherent translation capability. IntGrad MT achieves this by constructing a chain of few-shot examples, each consisting of a source sentence and the model\u2019s own translation, that rise incrementally in difficulty. IntGrad MT employs two techniques: Sentence Interpolation, which generates a sequence of sentences that gradually change from an easy sentence to translate to a difficult one, and Gradual MT, which sequentially translates this chain using translations of earlier sentences as few-shot examples for the translation of subsequent ones. With this approach, we observe a substantial enhancement in the xCOMET scores of various LLMs for multiple languages, especially in low-resource languages such as Hindi(8.26), Swahili(7.10), Bengali(6.97) and Marathi(13.03). Our approach presents a practical way of enhancing LLMs' performance without extra training.", "title_embedding_index": 4907, "title_abs_embedding_index": 4932}, {"title": "LLM Bandit: Cost-Efficient LLM Generation via Preference-Conditioned Dynamic Routing", "link_suffix": "/forum?id=rEqETC88RY", "link": "https://openreview.net/forum?id=rEqETC88RY", "pdf_link": "https://openreview.net/pdf?id=rEqETC88RY", "keywords": "LLM Routing, Reinforcement Learning, Multi-objective Optimization", "abstract": "The rapid advancement in large language models (LLMs) has brought forth a diverse range of models with varying capabilities that excel in different tasks and domains. However, selecting the optimal LLM for user queries often involves a challenging trade-off between accuracy and cost, a problem exacerbated by the diverse demands of individual queries. In this work, we present a novel framework that formulates the LLM selection process as a multi-armed bandit problem, enabling dynamic and intelligent routing of queries to the most appropriate model. Our approach incorporates a preference-conditioned dynamic routing mechanism, allowing users to specify their preferences at inference time, thereby offering a customizable balance between performance and cost. Additionally, our selection policy is designed to generalize to unseen LLMs, ensuring adaptability to new models as they emerge. Experimental results demonstrate that our method achieves significant improvements in both accuracy and cost-effectiveness across various LLM platforms, showcasing the potential of our framework to adaptively optimize LLM selection in real-world scenarios.", "title_embedding_index": 4908, "title_abs_embedding_index": 4933}, {"title": "A Provably Robust Algorithm for Differentially Private Clustered Federated Learning", "link_suffix": "/forum?id=rBAnJed1iY", "link": "https://openreview.net/forum?id=rBAnJed1iY", "pdf_link": "https://openreview.net/pdf?id=rBAnJed1iY", "keywords": "Federated Learning, Clustered Federated Learning, Differential Privacy, Clustering", "abstract": "Federated Learning (FL), which is a decentralized machine learning (ML) approach, often incorporates differential privacy (DP) to enhance privacy guarantees for clients' data. However, differentially private federated learning (DPFL) introduces performance disparities across clients, particularly affecting minority groups. Some recent works have attempted to address performance fairness under data heterogeneity in vanilla FL settings through clustering clients, but these existing methods remain sensitive and prone to errors, which are further exacerbated by the DP noise in DPFL. This shortcoming makes the existing methods inappropriate for DPFL settings. To fill this gap, we propose a robust algorithm for differentially private clustered FL, designed to be robust to the DP noise in the system and to identify clients' clusters correctly. To this end, we propose to cluster clients on the server based on both their model updates and training loss values. Furthermore, when clustering clients' model updates, our proposed approach addresses the server's uncertainties by employing large batch sizes along with Gaussian Mixture Model (GMM) to reduce the impact of DP and stochastic noise and avoid potential clustering errors. This idea is efficient especially in privacy-sensitive scenarios, where more DP noise is used. We provide theoretical analysis justifying our proposed approach, and evaluate it extensively across diverse data distributions and privacy budgets. Our experimental results show its effectiveness in mitigating the disparate impact of DP in FL settings with a small computational cost, while enjoying DP privacy guarantees.", "title_embedding_index": 4909, "title_abs_embedding_index": 4934}, {"title": "Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness", "link_suffix": "/forum?id=ZjOXuAfS6l", "link": "https://openreview.net/forum?id=ZjOXuAfS6l", "pdf_link": "https://openreview.net/pdf?id=ZjOXuAfS6l", "keywords": "optimization, nonconvex optimization, stochastic optimization, adaptive optimization, relaxed smoothness, lower bounds, adagrad", "abstract": "Recent results in non-convex stochastic optimization demonstrate the convergence of popular adaptive algorithms (e.g., AdaGrad) under the $(L_0, L_1)$-smoothness condition, but the rate of convergence is a higher-order polynomial in terms of problem parameters like the smoothness constants. The complexity guaranteed by such algorithms to find an $\\epsilon$-stationary point may be significantly larger than the optimal complexity of $\\Theta \\left( \\Delta L \\sigma^2 \\epsilon^{-4} \\right)$ achieved by SGD in the $L$-smooth setting, where $\\Delta$ is the initial optimality gap, $\\sigma^2$ is the variance of stochastic gradient. However, it is currently not known whether these higher-order dependencies can be tightened. To answer this question, we investigate complexity lower bounds for several adaptive optimization algorithms in the $(L_0, L_1)$-smooth setting, with a focus on the dependence in terms of problem parameters $\\Delta, L_0, L_1$. We provide complexity bounds for three variations of AdaGrad, which show at least a quadratic dependence on problem parameters $\\Delta, L_0, L_1$. Notably, we show that the decorrelated variant of AdaGrad-Norm requires at least $\\Omega \\left( \\Delta^2 L_1^2 \\sigma^2 \\epsilon^{-4} \\right)$ stochastic gradient queries to find an $\\epsilon$-stationary point. We also provide a lower bound for SGD with a broad class of adaptive stepsizes. Our results show that, for certain adaptive algorithms, the $(L_0, L_1)$-smooth setting is fundamentally more difficult than the standard smooth setting, in terms of the initial optimality gap and the smoothness constants.", "title_embedding_index": 4910, "title_abs_embedding_index": 4935}, {"title": "Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks", "link_suffix": "/forum?id=zCxGCdzreM", "link": "https://openreview.net/forum?id=zCxGCdzreM", "pdf_link": "https://openreview.net/pdf?id=zCxGCdzreM", "keywords": "reinforcement learning, open-endedness, unsupervised environment design, automatic curriculum learning, benchmark", "abstract": "While large models trained with self-supervised learning on offline datasets have shown remarkable capabilities in text and image domains, achieving the same generalisation for agents that act in sequential decision problems remains an open challenge.\nIn this work, we take a step towards this goal by procedurally generating tens of millions of 2D physics-based tasks and using these to train a general reinforcement learning (RL) agent for physical control.\nTo this end, we introduce Kinetix: an open-ended space of physics-based RL environments that can represent tasks ranging from robotic locomotion and grasping to video games and classic RL environments, all within a unified framework.\nKinetix makes use of our novel hardware-accelerated physics engine Jax2D that allows us to cheaply simulate billions of environment steps during training.\nOur trained agent exhibits strong physical reasoning capabilities, being able to zero-shot solve unseen human-designed environments.  Furthermore, fine-tuning this general agent on tasks of interest shows significantly stronger performance than training an RL agenttabula rasa.  This includes solving some environments that standard RL training completely fails at.\nWe believe this demonstrates the feasibility of large scale, mixed-quality pre-training for online RL and we hope that Kinetix will serve as a useful framework to investigate this further.\nWe open-source Jax2D, Kinetix, and our final model weights.", "title_embedding_index": 4911, "title_abs_embedding_index": 4936}, {"title": "IS TRANSFORMER A STOCHASTIC PARROT? A CASE STUDY IN SIMPLE ARITHMETIC TASK", "link_suffix": "/forum?id=tYVmxoRps3", "link": "https://openreview.net/forum?id=tYVmxoRps3", "pdf_link": "https://openreview.net/pdf?id=tYVmxoRps3", "keywords": "large language model, interpretability", "abstract": "Large pre-trained language models have demonstrated impressive capabilities, but \nthere is still much to learn about how they operate. In this study, we\n conduct a investigation of the autoregressive transformer\u2019s ability to\n perform basic addition operations. \n Specifically, by using causal analysis we found that a few different attention heads in the middle layers control the addition carry, with each head processing carries of different lengths. Due to the lack of globality in these attention heads, the model struggles to handle long-sequence addition tasks. By performing inference intervention on mistral-7B, partial task performance can be restored, with the accuracy on 20-digit long-sequence additions from 2% to 38%. Moreover, through fine-tuning, we discovered that the model still struggles to generalize carry chains beyond the training sequence length and the formation of the attention heads is crucial to the length generalization. Our research reveals how the model performs addition, and further provides insights into the debate on whether these models are merely statistical.", "title_embedding_index": 4912, "title_abs_embedding_index": 4937}, {"title": "Promptus: Representing Real-World Video as Stable Diffusion Prompts for Video Streaming", "link_suffix": "/forum?id=BnYJdouhkp", "link": "https://openreview.net/forum?id=BnYJdouhkp", "pdf_link": "https://openreview.net/pdf?id=BnYJdouhkp", "keywords": "Video Streaming, Stable Diffusion, AIGC, Prompt", "abstract": "With the exponential growth of video traffic, traditional video streaming systems are approaching their limits in compression efficiency and communication capacity. To further reduce bitrate while maintaining quality, we propose Promptus, a disruptive novel system that streaming prompts instead of video content, which represents real-world video frames with a series of \"prompts\" for delivery and employs Stable Diffusion to generate videos at the receiver. To ensure that the prompt representation is pixel-aligned with the original video, a gradient descent-based prompt fitting framework is proposed. Further, a low-rank decomposition-based bitrate control algorithm is introduced to achieve adaptive bitrate. For inter-frame compression, a temporal smoothing-based prompt interpolation algorithm is proposed. Evaluations across various video genres demonstrate that, compared to H.265, Promptus can achieve more than a 4x bandwidth reduction while preserving the same perceptual quality. On the other hand, at extremely low bitrates, Promptus can enhance the perceptual quality by 0.139 and 0.118 (in LPIPS) compared to VAE and H.265, respectively, and decreases the ratio of severely distorted frames by 89.3% and 91.7%. Our work opens up a new paradigm for efficient video communication. Promptus will be open-sourced after publication.", "title_embedding_index": 4913, "title_abs_embedding_index": 4938}, {"title": "Scaling Transformers for Low-Bitrate High-Quality Speech Coding", "link_suffix": "/forum?id=4YpMrGfldX", "link": "https://openreview.net/forum?id=4YpMrGfldX", "pdf_link": "https://openreview.net/pdf?id=4YpMrGfldX", "keywords": "Audio coding, neural audio codecs, transformers", "abstract": "The tokenization of audio with neural audio codec models is a vital part of modern AI pipelines for the generation or understanding of speech, alone or in a multimodal context. Traditionally such tokenization models have concentrated on low parameter-count architectures using only components with strong inductive biases. In this work we show that by applying a transformer architecture with large parameter count to this problem, and applying a flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to reach state-of-the-art speech quality at extremely low bit-rates of $400$ or $700$ bits-per-second. The trained models strongly out-perform existing baselines in both objective and subjective tests.", "title_embedding_index": 4914, "title_abs_embedding_index": 4939}, {"title": "Riemannian Optimization for Hyperbolic Prototypical Networks", "link_suffix": "/forum?id=jzneu6AO2x", "link": "https://openreview.net/forum?id=jzneu6AO2x", "pdf_link": "https://openreview.net/pdf?id=jzneu6AO2x", "keywords": "Hyperbolic Representation Learning, Prototype Learning, Image Classification", "abstract": "This paper addresses the utilization of hyperbolic geometry within a Prototype Learning framework. Specifically, we introduce Riemannian optimization for Hyperbolic Prototypical Networks (RHPN), a novel approach that leverages Prototype Learning on Riemannian manifolds applied to the Poincare' ball. RHPN capitalizes on the efficiency and effectiveness of updating prototypes during training, coupled with a regularization term crucial to boost the performances. We set up an extensive experimentation that shows that RHPN is able to outperform the state-of-the-art in Prototype Learning, both in low and high dimensions, extending the impact of hyperbolic spaces to a wider range of scenarios.", "title_embedding_index": 4915, "title_abs_embedding_index": 4940}, {"title": "PREDICTING 3D STRUCTURE BY LATENT POSTERIOR SAMPLING", "link_suffix": "/forum?id=lMcoxeMYYw", "link": "https://openreview.net/forum?id=lMcoxeMYYw", "pdf_link": "https://openreview.net/pdf?id=lMcoxeMYYw", "keywords": "NeRF, diffusion model, probabilistic reasoning, reconstruction, latent representation.", "abstract": "The remarkable achievements of both generative models of 2D images and neural field representations for 3D scenes present a compelling opportunity to integrate the strengths of both approaches.\nIn this work, we propose a methodology that combines a NeRF-based representation of 3D scenes with probabilistic modeling and reasoning using diffusion models.\nWe view 3D reconstruction as a perception problem with inherent uncertainty that can thereby benefit from probabilistic inference methods.The core idea is to represent the 3D scene as a stochastic latent variable for which we can learn a prior and use it to perform posterior inference given a set of observations. \nWe formulate posterior sampling using the score-based inference method of diffusion models in conjunction with a likelihood term computed from a reconstruction model that includes volumetric rendering. \nWe train the model using a two-stage process: first we train the reconstruction model while auto-decoding the latent representations for a dataset of 3D scenes, and then we train the prior over the latents using a diffusion model.\nBy using the model to generate samples from the posterior we demonstrate that various 3D reconstruction tasks can be performed, differing by the type of observation used as inputs. \nWe showcase reconstruction from single-view, multi-view, noisy images, sparse pixels, and sparse depth data. \nThese observations vary in the amount of information they provide for the scene and we show that our method can model the varying levels of inherent uncertainty associated with each task.\nOur experiments illustrate that this approach yields a comprehensive method capable of accurately predicting 3D structure from diverse types of observations.", "title_embedding_index": 4916, "title_abs_embedding_index": 4941}, {"title": "Insight Over Sight? Exploring the Vision-Knowledge Conflicts in Multimodal LLMs", "link_suffix": "/forum?id=3YQYo1O01W", "link": "https://openreview.net/forum?id=3YQYo1O01W", "pdf_link": "https://openreview.net/pdf?id=3YQYo1O01W", "keywords": "Multimodal Large Language Models, Knowledge Conflict, Diagnostic benchmark, Commonsense Knowledge", "abstract": "This paper explores the problem of commonsense-level vision-knowledge conflict in Multimodal Large Language Models (MLLMs), where visual information contradicts model's internal commonsense knowledge (see Figure 1). To study this issue, we introduce an automated pipeline, augmented with human-in-the-loop quality control, to establish a benchmark aimed at simulating and assessing the conflicts in MLLMs. Utilizing this pipeline, we have crafted a diagnostic benchmark comprising 374 original images and 1,122 high-quality question-answer (QA) pairs. This benchmark covers two types of conflict targets and three question difficulty levels, providing a thorough assessment tool. Through this benchmark, we evaluate the conflict-resolution capabilities of nine representative MLLMs across various model families and find a noticeable over-reliance on textual queries. Drawing on these findings, we propose a novel prompting strategy, \"Focus-on-Vision\" (FoV), which markedly enhances MLLMs' ability to favor visual data over conflicting textual knowledge. Our detailed analysis and the newly proposed strategy significantly advance the understanding and mitigating of vision-knowledge conflicts in MLLMs.\nThe data and code will be released.", "title_embedding_index": 4917, "title_abs_embedding_index": 4942}, {"title": "On Understanding of the Dynamics of Model Capacity in Continual Learning", "link_suffix": "/forum?id=O0z4mkjl7i", "link": "https://openreview.net/forum?id=O0z4mkjl7i", "pdf_link": "https://openreview.net/pdf?id=O0z4mkjl7i", "keywords": "Continual Learning, dynamic pogramming, theory, CL challenges.", "abstract": "The core issue in continual learning (CL) is balancing catastrophic forgetting of prior knowledge with generalization to new tasks, otherwise, known as the stability-plasticity dilemma. We argue that the dilemma is akin to the capacity(the networks' ability to represent tasks) of the neural network(NN) in the CL setting. Within this context, this work introduces ``CL\u2019s effective model capacity (CLEMC)\" to understand the dynamical behavior of stability-plasticity balance point in the CL setting. We define CLEMC as a function of the NN, the task data, and the optimization procedure. Leveraging CLEMC, we demonstrate that the capacity is non-stationary and regardless of the NN architecture and optimization method, the network\u2019s ability to represent new tasks diminishes if the incoming tasks\u2019 data distributions differ from previous ones. We  formulate these results using dynamical systems' theory and conduct extensive experiments to complement the findings. Our analysis extends from a small feed-forward(FNN) and convolutional networks(CNN) to medium sized graph neural networks(GNN) to transformer-based large language models(LLM) with millions of parameters.", "title_embedding_index": 4918, "title_abs_embedding_index": 4943}, {"title": "SENSEI: Semantic Exploration Guided by Foundation Models to Learn Versatile World Models", "link_suffix": "/forum?id=6DkpewPCcO", "link": "https://openreview.net/forum?id=6DkpewPCcO", "pdf_link": "https://openreview.net/pdf?id=6DkpewPCcO", "keywords": "intrinsic motivation, exploration, foundation models, model-based RL", "abstract": "Exploring useful behavior is a keystone of reinforcement learning (RL). Intrinsic motivation attempts to decouple exploration from external, task-based rewards. However, existing approaches to intrinsic motivation that follow general principles such as information gain, mostly uncover low-level interactions. In contrast, children\u2019s play suggests that they engage in meaningful high-level behavior by imitating or interacting with their caregivers. Recent work has focused on using foundation models to inject these semantic biases into exploration. However, these methods often rely on unrealistic assumptions, such as environments already embedded in language or access to high-level actions. To bridge this gap, we propose SEmaNtically Sensible ExploratIon (SENSEI), a framework to equip model- based RL agents with intrinsic motivation for semantically meaningful behavior. To do so, we distill an intrinsic reward signal of interestingness from Vision Language Model (VLM) annotations. The agent learns to predict and maximize these intrinsic rewards using a world model learned directly from intrinsic rewards, image observations, and low-level actions. We show that in both robotic and video game-like simulations SENSEI manages to discover a variety of meaningful behaviors. We believe SENSEI provides a general tool for integrating feedback from foundation models into autonomous agents, a crucial research direction, as openly available VLMs become more powerful.", "title_embedding_index": 4919, "title_abs_embedding_index": 4944}, {"title": "The Convergence of Second-Order Sampling Methods for Diffusion Models", "link_suffix": "/forum?id=yhmVrA8W0v", "link": "https://openreview.net/forum?id=yhmVrA8W0v", "pdf_link": "https://openreview.net/pdf?id=yhmVrA8W0v", "keywords": "diffusion models, reserve SDE", "abstract": "Diffusion models have achieved great success in generating samples from complex distributions, notably in the domains of images and videos. Beyond the experimental success, theoretical insights into their performance have been illuminated, particularly concerning the convergence of diffusion models when applied with discretization methods such as Euler-Maruyama (EM) and Exponential Integrator (EI). This paper embarks on analyzing the convergence of the higher-order discretization method (SDE-DPM-2) under $L^2$-accurate score estimate. Our findings reveal that to attain $\\tilde{O}(\\epsilon_0^2)$ Kullback-Leibler (KL) divergence between the target and the sampled distributions, the sampling complexity - or the required number of discretization steps - for SDE-DPM-2 is $\\tilde{O}(1/\\epsilon_0)$, which is better than the currently known sample complexity of EI given by $\\tilde{O}(1/\\epsilon_0^2)$. We further extend our analysis to the Runge-Kutta-2 (RK-2) method, which demands a sampling complexity of $\\tilde{O}(1/\\epsilon_0^2)$, indicating that SDE-DPM-2 is more efficient than RK-2. Our study also demonstrates that the convergence of SDE-DPM-2 under Variance Exploding (VE) SDEs aligns with that of Variance Preserving (VP) SDEs, highlighting the adaptability of SDE-DPM-2 across various diffusion models frameworks.", "title_embedding_index": 4920, "title_abs_embedding_index": 4945}, {"title": "VT-PLUG: Integrating Visual Task Plugins with Unified Instruction Tuning", "link_suffix": "/forum?id=a4PBF1YInZ", "link": "https://openreview.net/forum?id=a4PBF1YInZ", "pdf_link": "https://openreview.net/pdf?id=a4PBF1YInZ", "keywords": "Multimodal Large Language Model, Multi-task Learning, Object Detection, Segmentation, Keypoint Detection", "abstract": "Multimodal Large Language Models (MLLMs) demonstrate robust zero-shot capabilities across diverse vision-language tasks after training on mega-scale datasets. However, dense prediction tasks, such as semantic segmentation and keypoint detection, pose significant challenges for MLLMs when represented solely as text outputs. These challenges often necessitate task-specific visual decoders, leading to the underutilization of MLLMs' multi-task potential. In this work, we propose VT-PLUG, a novel framework that leverages modular visual components as scalable plugins for a variety of visual applications. During the joint training of vision-language tasks with varying prediction densities, we propose a Visual Decoding Chain-of-Thought (VD-CoT) mechanism to prevent task conflicts. VD-CoT requires the model to predict the current task's recognition entities, decoding unit type, and other specific details, while also providing learnable queries for precise decoding. Additionally, we construct VT-Instruct, a large-scale multi-task dataset containing over 100 million multimodal dialogue samples across 25 task types. Beyond text inputs and outputs, VT-Instruct incorporates various visual prompts such as point, box, scribble, and mask, and generates outputs composed of text and visual units like point, box, keypoint, and mask. The combination of different visual prompts and visual units generates a wide variety of task types, expanding the applicability of VT-PLUG significantly. The source code, dataset and demo will be released athttps://anonymous.4open.science/r/VT-PLUG.", "title_embedding_index": 4921, "title_abs_embedding_index": 4946}, {"title": "Eidetic Learning: an Efficient and Provable Solution to Catastrophic Forgetting", "link_suffix": "/forum?id=6E8GCcCgxl", "link": "https://openreview.net/forum?id=6E8GCcCgxl", "pdf_link": "https://openreview.net/pdf?id=6E8GCcCgxl", "keywords": "catastrophic forgetting, continual learning", "abstract": "Catastrophic forgetting -- the phenomenon of a neural network learning a task and losing the ability to perform it after being trained on some other task -- is a long-standing problem for neural networks \\citep{mccloskey1989catastrophic}. We introduce Eidetic Learning and prove that it guarantees networks do not forget. When training an EideticNet, accuracy on previous tasks is preserved because the neurons important for them are fixed and, most importantly, the hidden states that those neurons operate on are guaranteed to be unchanged by \\textit{any} subsequent tasks for \\textit{any} input sample. EideticNets are easy to implement, their complexity in time and space is linear in the number of parameters, and their guarantees hold for normalization layers during pre-training and fine-tuning. We show empirically with a variety of network architectures and sets of tasks that EideticNets are immune to forgetting. While the practical benefits of EideticNets are substantial, we believe they can be of benefit to practitioners and theorists alike. They have the potential to open new directions of exploration for lifelong and continual learning. We will release the code repository containing the EideticNet PyTorch framework upon publication.", "title_embedding_index": 4922, "title_abs_embedding_index": 4947}, {"title": "Safety-Advanced Autonomous Driving for Urgent Hazardous Situations using Q-Compared Soft Actor-Critic", "link_suffix": "/forum?id=MJWJoICJQh", "link": "https://openreview.net/forum?id=MJWJoICJQh", "pdf_link": "https://openreview.net/pdf?id=MJWJoICJQh", "keywords": "Autonomous Driving, Reinforcement Learning, Imitation Learning, Hybrid Learning", "abstract": "Autonomous vehicles must be capable of safe driving under all conditions to ensure passenger safety. This includes urgent hazardous situations (UHS), such as skidding on slippery roads or tire grip saturation during high-speed driving, which are not only difficult even for expert human drivers but also challenging to develop autonomous driving technologies that surpass human capabilities. Even though the recent advancements in machine learning including imitation learning (IL), reinforcement learning (RL), and hybrid learning (HL) have enabled the safe navigation of autonomous vehicles in various complex scenarios, they have fundamental limitations in UHS. Driving policies trained via IL degrade in novel situations where expert demonstration data is scarce or of poor quality, and RL struggles to develop optimal driving policies in UHS, which have broad state and action spaces and high transition variance. HL techniques combining IL and RL also fall short, as they require nearly optimal demonstration data, which is nearly impossible to obtain in UHS due to the difficulty for human drivers to react appropriately.\nTo address these limitations, we propose a novel HL technique, Q-Compared Soft Actor-Critic (QC-SAC), which effectively utilizes immature demonstration data to develop optimal driving policies and adapt quickly to novel situations in UHS. QC-SAC evaluates the quality of demonstration data based on action value Q to prioritize beneficial data and disregard detrimental ones. Furthermore, QC-SAC improves the performance of the Q-network by leveraging demonstration data and enhances learning by rapidly incorporating new successful experiences from ongoing interactions, enabling fast adaptation to new situations. We test QC-SAC for two extreme UHS scenarios: oversteer control with collision avoidance (OCCA) and time-trial race (TTR). In OCCA, QC-SAC achieves a success rate 2.36 times higher than existing techniques, and in TTR, it reduces lap time by more than 13.6% while completing 300 test runs without a single failure. By proposing an innovative HL technique capable of training superior driving policies with immature demonstration data, we provide a solution for autonomous driving technologies that can handle UHS and introduce the world-first safe-advanced autonomous driving technology capable of controlling a vehicle oversteer safely and avoiding obstacles ahead.", "title_embedding_index": 4923, "title_abs_embedding_index": 4948}, {"title": "EEEC: Emotion-Experiencer-Event-Cause multi-step chain reasoning for Emotion-Cause Pair Extraction", "link_suffix": "/forum?id=EJTeOf8iG0", "link": "https://openreview.net/forum?id=EJTeOf8iG0", "pdf_link": "https://openreview.net/pdf?id=EJTeOf8iG0", "keywords": "Experiencer; Event; Multi-step chain reasoning; Emotion-Cause Pair Extraction", "abstract": "Emotion-cause pair extraction (ECPE) aims to identify all emotion and cause clauses in documents, forming the ECPs. Although existing methods have achieved some success, they face issues such as overlooking the impact of emotion experiencers, failing to leverage specific domain knowledge, and tending to spurious correlations. To address these issues, we transform the ECPE task into a multi-step reasoning problem and propose the Emotion-Experience-Event-Cause (EEEC) framework. We introduce an experiencer identification task to understand the source of emotions and enhance the association between emotion and cause clauses. In addition, by combining both prior knowledge and induced reasoning, EEEC guides a large-scale language model (LLM) to perform the emotion-reason pair extraction task efficiently. Experimental results demonstrate that EEEC achieves performance close to current state-of-the-art supervised fine-tuning methods. The data and code are released athttps://anonymous.4open.science/r/EEEC-EB80/.", "title_embedding_index": 4924, "title_abs_embedding_index": 4949}]