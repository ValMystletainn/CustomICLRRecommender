[
    {
        "title": "Ada-K Routing: Boosting the Efficiency of MoE-based LLMs",
        "link_suffix": "/forum?id=9CqkpQExe2",
        "link": "https://openreview.net/forum?id=9CqkpQExe2",
        "pdf_link": "https://openreview.net/pdf?id=9CqkpQExe2",
        "keywords": "Large Language Models, Mixture-of-Experts, Reinforcement Learning",
        "abstract": "In the era of Large Language Models (LLMs), Mixture-of-Experts (MoE) architectures offer a promising approach to managing computational costs while scaling up model parameters. Conventional MoE-based LLMs typically employ static Top-K routing, which activates a fixed and equal number of experts for each token regardless of their significance within the context. In this paper, we propose a novel Ada-K routing strategy that dynamically adjusts the number of activated experts for each token, thereby improving the balance between computational efficiency and model performance. Specifically, our strategy incorporates learnable and lightweight allocator modules that decide customized expert resource allocation tailored to the contextual needs for each token. These allocators are designed to be fully pluggable, making it broadly applicable across all mainstream MoE-based LLMs. We leverage the Proximal Policy Optimization (PPO) algorithm to facilitate an end-to-end learning process for this non-differentiable decision-making framework. Extensive evaluations on four popular baseline models demonstrate that our Ada-K routing method significantly outperforms conventional Top-K routing. Compared to Top-K, our method achieves over 25% reduction in FLOPs and more than 20% inference speedup while still improving performance across various benchmarks. Moreover, the training of Ada-K is highly efficient. Even for Mixtral-8x22B, a MoE-based LLM with more than 140B parameters, the training time is limited to 8 hours. Detailed analysis shows that harder tasks, middle layers, and content words tend to activate more experts, providing valuable insights for future adaptive MoE system designs. Both the training code and model checkpoints will be publicly available."
    },
    {
        "title": "Towards Constraint-aware Learning for Resource Allocation in NFV-enabled Networks",
        "link_suffix": "/forum?id=EBBeSbmAyh",
        "link": "https://openreview.net/forum?id=EBBeSbmAyh",
        "pdf_link": "https://openreview.net/pdf?id=EBBeSbmAyh",
        "keywords": "Network Resource Allocation; Combinatorial Optimization; Reinforcement Learning; Graph Neural Network",
        "abstract": "Virtual Network Embedding (VNE) is a challenging combinatorial optimization problem that refers to resource allocation associated with hard and multifaceted constraints in network function virtualization (NFV). Existing works for VNE struggle to handle such complex constraints, leading to compromised system performance and stability. In this paper, we propose a \\textbf{CON}straint-\\textbf{A}ware \\textbf{L}earning framework for VNE, named \\textbf{CONAL}, to achieve efficient constraint management. Concretely, we formulate the VNE problem as a constrained Markov decision process with violation tolerance. This modeling approach aims to improve both resource utilization and solution feasibility by precisely evaluating solution quality and the degree of constraint violation. We also propose a reachability-guided optimization with an adaptive reachability budget method that dynamically assigns budget values. This method achieves persistent zero violation to guarantee the feasibility of VNE solutions and more stable policy optimization by handling instances without any feasible solution. Furthermore, we propose a constraint-aware graph representation method to efficiently learn cross-graph relations and constrained path connectivity in VNE. Finally, extensive experimental results demonstrate the superiority of our proposed method over state-of-the-art baselines. Our code is available at \\href{https://anonymous.4open.science/r/iclr25-conal}{https://anonymous.4open.science/r/iclr25-conal}."
    },
    {
        "title": "Communication-Efficient Federated Learning via Model Update Distillation",
        "link_suffix": "/forum?id=Zh9gz3CaWm",
        "link": "https://openreview.net/forum?id=Zh9gz3CaWm",
        "pdf_link": "https://openreview.net/pdf?id=Zh9gz3CaWm",
        "keywords": "Federated Learning, Edge Computing, Communication-Efficient, Knowledge Distillation",
        "abstract": "Federated learning (FL) is a popular distributed machine learning framework for edge computing. However, it faces a significant challenge: the communication overhead caused by frequent model updates between clients and the central server. Previous studies have overlooked a crucial piece of information: the central server already knows the initial model on each client before local training begins in every round. This oversight leads to significant redundancy in communication, as full model information are transmitted unnecessarily. To address this, we propose a novel framework called \\textit{model update distillation} (MUD), which leverages this prior knowledge to decouple model parameters from the network architecture. Instead of transmitting raw parameter updates, our method synthesizes and transmits compact tensor sequences that encode only the essential information for synchronization. This dramatically reduces communication overhead while still allowing recipients to accurately reconstruct the intended model updates. Extensive experimental results demonstrate that FedMUD achieves substantial improvements in communication efficiency, making it a highly effective solution for federated learning in bandwidth-constrained environments. The PyTorch-like core code can be found in \\ref{alg: pytorch}."
    },
    {
        "title": "AuthFace: Towards Authentic Blind Face Restoration with Face-oriented Generative Diffusion Prior",
        "link_suffix": "/forum?id=Z28efFRE9r",
        "link": "https://openreview.net/forum?id=Z28efFRE9r",
        "pdf_link": "https://openreview.net/pdf?id=Z28efFRE9r",
        "keywords": "Blind Face Restoration, Diffusion Models",
        "abstract": "Blind face restoration (BFR) is a challenging and fundamental problem in computer vision. To faithfully restore high-quality (HQ) photos from poor-quality ones, recent research endeavors predominantly rely on facial image priors from the powerful pretrained text-to-image (T2I) diffusion models. However, such priors often lead to the incorrect generation of non-facial features and insufficient facial details, thus rendering them less practical for real-world applications. In this paper, we propose a novel framework, namely AuthFace that achieves highly authentic face restoration results by exploring a face-oriented generative diffusion prior. To learn such a prior, we first collect a dataset of 1.5K high-quality images, with resolutions exceeding 8K, captured by professional photographers. Based on the dataset, we then introduce a novel face-oriented restoration-tuning pipeline that fine-tunes a pretrained T2I model. Identifying key criteria of quality-first and photography-guided annotation, we involve the retouching and reviewing process under the guidance of photographers for these high-quality images that show rich facial features. The photography-guided annotation system fully explores the potential of these high-quality photographic images. In this way, the potent natural image priors from pretrained T2I diffusion models can be subtly harnessed, specifically enhancing their capability in facial detail restoration. Moreover, to minimize artifacts in critical facial areas, such as eyes and mouth, we propose a time-aware latent facial feature loss to learn the authentic face restoration process. Extensive experiments on the synthetic and real-world BFR datasets demonstrate the superiority of our approach. Codes and datasets will be available upon acceptance."
    },
    {
        "title": "Deep Clustering with Associative Memories",
        "link_suffix": "/forum?id=Awsb8jhEx3",
        "link": "https://openreview.net/forum?id=Awsb8jhEx3",
        "pdf_link": "https://openreview.net/pdf?id=Awsb8jhEx3",
        "keywords": "deep clustering, associative memories, representation learning, Hopfield networks",
        "abstract": "Deep clustering -- joint representation learning and latent space clustering -- is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. Recently, Associative Memories were utilized in the end-to-end differentiable\n$\\texttt{ClAM}$ clustering scheme (Saha et al. 2023). In this work, we show how Associative Memories enable a novel take on deep clustering, $\\texttt{DClAM}$, simplifying the whole pipeline and tying together the representation learning and clustering more intricately. Our experiments showcase the advantage of $\\texttt{DClAM}$, producing improved clustering quality regardless of the architecture choice (convolutional, residual or fully-connected) or data modality (images or text)."
    },
    {
        "title": "SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios",
        "link_suffix": "/forum?id=Hx2ADQLi8M",
        "link": "https://openreview.net/forum?id=Hx2ADQLi8M",
        "pdf_link": "https://openreview.net/pdf?id=Hx2ADQLi8M",
        "keywords": "Moving audio toolkit, moving audio dataset, speech separation, speech enhancement",
        "abstract": "The systematic evaluation of speech separation and enhancement models under moving sound source conditions typically requires extensive data comprising diverse scenarios. However, real-world datasets often contain insufficient data to meet the training and evaluation requirements of models. Although synthetic datasets offer a larger volume of data, their acoustic simulations lack realism. Consequently, neither real-world nor synthetic datasets effectively fulfill practical needs. To address these issues, we introduce SonicSim, a synthetic toolkit de- designed to generate highly customizable data for moving sound sources. SonicSim is developed based on the embodied AI simulation platform, Habitat-sim, supporting multi-level adjustments, including scene-level, microphone-level, and source-level, thereby generating more diverse synthetic data. Leveraging SonicSim, we constructed a moving sound source benchmark dataset, SonicSet, using the Librispeech, the Freesound Dataset 50k (FSD50K) and Free Music Archive (FMA), and 90 scenes from the Matterport3D to evaluate speech separation and enhancement models. Additionally, to validate the differences between synthetic data and real-world data, we randomly selected 5 hours of raw data without reverberation from the SonicSet validation set to record a real-world speech separation dataset, which was then compared with the corresponding synthetic datasets. Similarly, we utilized the real-world speech enhancement dataset RealMAN to validate the acoustic gap between other synthetic datasets and the SonicSet dataset for speech enhancement. The results indicate that the synthetic data generated by SonicSim can effectively generalize to real-world scenarios. Code is publicly available athttps//:anonymous.4open.science/w/SonicSim-ICLR2025/."
    },
    {
        "title": "Live2Diff: Live Stream Translation via Uni-directional Attention in Video Diffusion Models",
        "link_suffix": "/forum?id=YA1Ur2eGFl",
        "link": "https://openreview.net/forum?id=YA1Ur2eGFl",
        "pdf_link": "https://openreview.net/pdf?id=YA1Ur2eGFl",
        "keywords": "Generative Model, Video Generation, Video Translation",
        "abstract": "Large Language Models have shown remarkable efficacy in generating streaming data such as text and audio, thanks to their temporally uni-directional attention mechanism, which models correlations between the current token andprevioustokens.\nHowever, video streaming remains much less explored, despite a growing need for live video processing.\nState-of-the-art video diffusion models leveragebi-directional temporal attention to model the correlations between the current frame and all thesurrounding(i.e. includingfuture) frames, which hinders them from processing streaming videos.\nTo address this problem, we presentLive2Diff, the first attempt at designing a video diffusion model with uni-directional temporal attention, specifically targeting live streaming video translation.\nCompared to previous works, our approach ensures temporal consistency and smoothness by correlating the current frame with its predecessors and a few initial warmup frames, without any future frames.\nAdditionally, we use a highly efficient denoising scheme featuring akv-cachemechanism and pipelining, to facilitate streaming video translation at interactive framerates.\nExtensive experiments demonstrate the effectiveness of the proposed attention mechanism and pipeline, outperforming previous methods in terms of temporal smoothness and/or efficiency."
    },
    {
        "title": "Transfering Knowledge into Efficient Tiny Models for Object Detection with Dual Prompt Distillation",
        "link_suffix": "/forum?id=yhKNCvYlCr",
        "link": "https://openreview.net/forum?id=yhKNCvYlCr",
        "pdf_link": "https://openreview.net/pdf?id=yhKNCvYlCr",
        "keywords": "knowledge distillation, object detection",
        "abstract": "Knowledge Distillation (KD) has demonstrated significant benefits for learning compact models for object detection. Most current work focuses on general distillation settings, where student models are relatively large and learnable, then compete with the distillation performance. However, due to the model scale and inference speed, these models are seldom deployed in real-world applications. In this paper, we dive into a challenging but more applicable setting: how to distill rich teacher knowledge into tiny, faster models for object detection? We first show that simply applying previous KD strategies under such settings cannot achieve satisfying results, due to the extremely large model capacity gap between the teacher-student pairs. To this end, we propose a simple prompt-based object detection distillation framework, namely DualPromptKD, which aims to improve knowledge transfer efficiency from both teacher and student perspectives. Specifically, by distilling teacher representations into compact external prompts, we enable the student model to fully leverage proficient teacher knowledge even at inference time. In terms of the limited learning ability of the student model, we introduce lightweight internal prompts tailored to bolster the feature imitation capability for the target model. Extensive experimental results on the COCO benchmarks validate the effectiveness and generalization of our approach, including different image backbones and detector types. Notably, our DualPromptKD surpasses the previous best distillation strategies by more than 2.0 mAP under various experimental settings. The code will be available."
    },
    {
        "title": "Information-Theoretical Principled Trade-off between Jailbreakability and Stealthiness on Vision Language Models",
        "link_suffix": "/forum?id=j7ZWfqCYCY",
        "link": "https://openreview.net/forum?id=j7ZWfqCYCY",
        "pdf_link": "https://openreview.net/pdf?id=j7ZWfqCYCY",
        "keywords": "Jailbreak, Vision-Language Models, Security, Information Theory",
        "abstract": "In recent years, Vision-Language Models (VLMs) have demonstrated significant advancements in artificial intelligence, transforming tasks across various domains. Despite their capabilities, these models are susceptible to jailbreak attacks, which can compromise their safety and reliability. This paper explores the trade-off between jailbreakability and stealthiness in VLMs, presenting a novel algorithm to detect non-stealthy jailbreak attacks and enhance model robustness. We introduce a stealthiness-aware jailbreak attack using diffusion models, highlighting the challenge of detecting AI-generated content. Our approach leverages Fano\u2019s inequality to elucidate the relationship between attack success rates and stealthiness scores, providing an explainable framework for evaluating these threats. Our contributions aim to fortify AI systems against sophisticated attacks, ensuring their outputs remain aligned with ethical standards and user expectations."
    },
    {
        "title": "DLPO: Diffusion Model Loss-Guided Reinforcement Learning for Fine-Tuning Text-to-Speech Diffusion Models",
        "link_suffix": "/forum?id=WzrkZeDxrM",
        "link": "https://openreview.net/forum?id=WzrkZeDxrM",
        "pdf_link": "https://openreview.net/pdf?id=WzrkZeDxrM",
        "keywords": "Reinforcement Learning, Speech Synthesis, Diffusion Models",
        "abstract": "Recent advancements in generative models have sparked a significant interest within the machine learning community. Particularly, diffusion models have demonstrated remarkable capabilities in synthesizing images and speech. Studies such as those by Lee et al. (2023), Black et al. (2023), Wang et al. (2023), and Fan et al. (2024) illustrate that Reinforcement Learning with Human Feedback (RLHF) can enhance diffusion models for image synthesis. However, due to architectural differences between these models and those employed in speech synthesis, it remains uncertain whether RLHF could similarly benefit speech synthesis models. In this paper, we explore the practical application of RLHF to diffusion-based text-to-speech synthesis, leveraging the mean opinion score (MOS) as predicted by UTokyo-SaruLab MOS prediction system (Saeki et al., 2022) as a proxy loss. We introduce diffusion model loss-guided RL policy optimization (DLPO) and compare it against other RLHF approaches, employing the NISQA speech quality and naturalness assessment model (Mittag et al., 2021) and human preference experiments for further evaluation. Our results show that RLHF can enhance diffusion-based text-to-speech synthesis models, and, moreover, DLPO can better improve diffusion models in generating natural and high quality speech audios."
    },
    {
        "title": "Exploring Source View Capability: Improve Generalizable 3D Reconstruction with Multi-view Context from Source Views",
        "link_suffix": "/forum?id=6lMkx3rq6z",
        "link": "https://openreview.net/forum?id=6lMkx3rq6z",
        "pdf_link": "https://openreview.net/pdf?id=6lMkx3rq6z",
        "keywords": "Generalizable 3D Reconstruction, Novel View Synthesis, NeRF, 3DGS",
        "abstract": "Recent generalizable 3D reconstruction methods have been facing challenges in constructing geometry-consistent 3D features. \nThis is primarily due to the source views conveying redundant information to the sampled 3D points that they do not observe, resulting in the samples struggling to distinguish the correct observations of them. \nWe attribute this issue to that canonical supervision methods focus solely on the rendered target view from a single viewpoint, overlooking source views that capture the scene from different perspectives.\nWith this insight, we pioneer a supervision method for source views, which can be applied alongside existing target view supervision in each iteration. \nSpecifically, we define the Learned Geometry of the Scene (LGS) as source-view depth distributions, which are derived from the weights of source views for each sampled 3D point. \nTo regularize the LGS to better model the real-world geometry, we introduce a novel unsupervised learning objective, which mitigates the optimization bias in existing objectives and ensures the LGS is more concentrated near the real-world geometry surface. \nRegularizing the LGS effectively helps filter out irrelevant source views for each sampled 3D point, and thus noticeably improves the performance of backbones.\nMathematical proof is provided to validate the proposed objective, and extensive experiments demonstrate that our supervision method significantly improves both NeRF- and 3DGS-based backbones with negligible computation overhead."
    },
    {
        "title": "Brain-like Functional Organization within Large Language Models",
        "link_suffix": "/forum?id=mtyYWBx2ZF",
        "link": "https://openreview.net/forum?id=mtyYWBx2ZF",
        "pdf_link": "https://openreview.net/pdf?id=mtyYWBx2ZF",
        "keywords": "large language models, network organization, functional brain networks",
        "abstract": "The human brain has long inspired the pursuit of artificial intelligence (AI). Recently, neuroimaging studies provide compelling evidence of alignment between the computational representation of artificial neural networks (ANNs) and the neural responses of the human brain to external stimuli, suggesting that ANNs may employ brain-like information processing strategies. While such alignment has been observed across sensory modalities\u2014visual, auditory, and linguistic\u2014much of the focus has been on the behaviors of artificial neurons (ANs) at the population level, leaving the functional organization of individual ANs that facilitates such brain-like processes largely unexplored. In this study, we bridge this gap by directly coupling sub-groups of artificial neurons with functional brain networks (FBNs), the foundational organizational structure of the human brain. Specifically, we extract  representative patterns from temporal responses of ANs in large language models (LLMs), and use them as fixed regressors to construct voxel-wise encoding models to predict brain activity recorded by functional magnetic resonance imaging (fMRI). This framework effectively links the AN sub-groups to FBNs, enabling the delineation of brain-like functional organization within LLMs. Our findings reveal that LLMs (BERT and Llama 1\u20133) exhibit brain-like functional architecture, with sub-groups of artificial neurons mirroring the organizational patterns of well-established FBNs. Notably, the brain-like functional organization of LLMs evolves with the increased sophistication and capability, achieving an improved balance between the diversity of computational behaviors and the consistency of functional specializations. This research represents the first exploration of brain-like functional organization within LLMs, offering novel insights to inform the development of artificial general intelligence (AGI) with human brain principles."
    },
    {
        "title": "TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for Efficient Speech Separation",
        "link_suffix": "/forum?id=rzx3vcvlzj",
        "link": "https://openreview.net/forum?id=rzx3vcvlzj",
        "pdf_link": "https://openreview.net/pdf?id=rzx3vcvlzj",
        "keywords": "Speech separation, lightweight model, time-frequency domain",
        "abstract": "In recent years, much speech separation research has focused primarily on improving model performance. However, for low-latency speech processing systems, high efficiency is equally important. Therefore, we propose a speech separation model with significantly reduced parameters and computational costs:Time-frequency Interleaved Gain Extraction and Reconstruction network (TIGER). TIGER leverages prior knowledge to divide frequency bands and compresses frequency information. We employ a multi-scale selective attention module to extract contextual features, while introducing a full-frequency-frame attention module to capture both temporal and frequency contextual information. Additionally, to more realistically evaluate the performance of speech separation models in complex acoustic environments, we introduce a dataset calledEchoSet. This dataset includes noise and more realistic reverberation (e.g., considering object occlusions and material properties), with speech from two speakers overlapping at random proportions. Experimental results showed that models trained on EchoSet had better generalization ability than those trained on other datasets to the data collected in the physical world, which validated the practical value of the EchoSet. On EchoSet and real-world data, TIGER significantly reduces the number of parameters by94.3% and the MACs by95.3% while achieving performance surpassing state-of-the-art (SOTA) model TF-GridNet. This is the first speech separation model with fewer than1 million parametersthat achieves performance comparable to the SOTA model."
    },
    {
        "title": "Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models",
        "link_suffix": "/forum?id=88JJjsLtqr",
        "link": "https://openreview.net/forum?id=88JJjsLtqr",
        "pdf_link": "https://openreview.net/pdf?id=88JJjsLtqr",
        "keywords": "Text-to-Image Diffusion Models, Style Transfer, Content Leakage",
        "abstract": "Given a style-reference image as the additional image condition, text-to-image diffusion models have demonstrated impressive capabilities in generating images that possess the content of text prompts while adopting the visual style of the reference image. However, current state-of-the-art methods often struggle to disentangle content and style from style-reference images, leading to issues such as content leakages. To address this issue, we propose a masking-based method that efficiently decouples content from style without the need of tuning any model parameters. By simply masking specific elements in the style reference's image features, we uncover a critical yet under-explored principle: guiding with appropriately-selected fewer conditions (e.g., dropping several image feature elements) can efficiently avoid unwanted content flowing into the diffusion models, enhancing the style transfer performances of text-to-image diffusion models. In this paper, we validate this finding both theoretically and experimentally. Extensive experiments across various styles demonstrate the effectiveness of our masking-based method and support our theoretical results."
    },
    {
        "title": "AnO(klog\u2061n)Time Fourier Set Query Algorithm",
        "link_suffix": "/forum?id=QQvhOyIldg",
        "link": "https://openreview.net/forum?id=QQvhOyIldg",
        "pdf_link": "https://openreview.net/pdf?id=QQvhOyIldg",
        "keywords": "Fourier transform, set query",
        "abstract": "Fourier transformation is an extensively studied problem in many research fields. It has many applications in machine learning, signal processing, compressed sensing, and so on. In many real-world applications, approximated Fourier transformation is sufficient and we only need to do the Fourier transform on a subset of coordinates. \nGiven a vector $x \\in \\mathbb{C}^{n}$, approximation parameters $\\epsilon, \\delta \\in (0, 0.1)$, and a query set $S \\subset [n]$ of size $k$, we propose an algorithm to compute an approximate Fourier transform result $x'$ which uses $O(\\epsilon^{-1} k \\log(n/\\delta))$ Fourier measurements and runs in $O(\\epsilon^{-1} k \\log(n/\\delta))$ time. For $\\hat{x}$ being the Fourier transformation result, our algorithm can output a vector $x'$ such that $\\| ( x' - \\widehat{x} )_S  \\|_2^2 \\leq \\epsilon \\| \\widehat{x}_{\\overline{S}} \\|_2^2 + \\delta \\| \\widehat{x} \\|_1^2 $ holds with probability of at least $9/10$, where $\\overline{S}$ denotes the complement of the set $S$, i.e., $\\overline{S} := [n] \\setminus S$."
    },
    {
        "title": "Mixture-of-Instructions: Aligning Large Language Models via Mixture Prompting",
        "link_suffix": "/forum?id=9P5I9zTUAd",
        "link": "https://openreview.net/forum?id=9P5I9zTUAd",
        "pdf_link": "https://openreview.net/pdf?id=9P5I9zTUAd",
        "keywords": "language model, alignment, supervised fine-tuning",
        "abstract": "With the proliferation of large language models (LLMs), the comprehensive alignment of such models across multiple tasks has emerged as a critical area of research. Existing alignment methodologies primarily address single task, such as multi-turn dialogue, coding, mathematical problem-solving, and tool usage. However, AI-driven products that leverage language models usually necessitate a fusion of these abilities to function effectively in real-world scenarios. Moreover, the considerable computational resources required for proper alignment of LLMs underscore the need for a more robust, efficient, and encompassing approach to multi-task alignment, ensuring improved generative performance. In response to these challenges, we introduce a novel technique termed Mixture-of-Instructions (MoI), which employs a strategy of instruction packing combined with diverse system prompts to boost the alignment efficiency of language models. We have also compiled a diverse set of seven benchmark datasets to rigorously evaluate the alignment efficacy of the MoI-enhanced language model. Our methodology was applied to the open-source Qwen-7B-chat model, culminating in the development of Qwen-SFT-MoI. This enhanced model demonstrates significant advancements in generative capabilities across coding, mathematics, and tool use tasks."
    },
    {
        "title": "The Impact of Element Ordering on LM Agent Performance",
        "link_suffix": "/forum?id=MHP4jGMN2E",
        "link": "https://openreview.net/forum?id=MHP4jGMN2E",
        "pdf_link": "https://openreview.net/pdf?id=MHP4jGMN2E",
        "keywords": "Agents, AI Agents, LLM Agents, LM Agents",
        "abstract": "There has been a surge of interest in language model agents that can navigate virtual environments such as the web or desktop. To navigate such environments, agents benefit from information on the various elements (e.g., buttons, text, or images) present. However, it remains unclear which element attributes have the greatest impact on agent performance, especially in environments that only provide a graphical representation (i.e., pixels). Here we find that the ordering in which elements are presented to the language model is surprisingly impactful\u2014randomizing element ordering in webpages compromises average agent performance to a degree comparable to removing all visible text from webpages. While web agents benefit from the semantic hierarchical ordering of elements available via the browser, agents that parse elements directly from pixels do not have access to any such ordering. Here we endeavor to derive effective orderings and investigate the impact of various element ordering methods in web and desktop environments. We find that dimensionality reduction provides a viable ordering for pixel-only environments. We train a UI element detection model to derive elements from pixels and apply our findings to an agent benchmark\u2014OmniACT\u2014where we only have access to pixels. Our method completes more than two times as many tasks on average relative to the previous state-of-the-art."
    },
    {
        "title": "DiSciPLE: Learning Interpretable Programs for Scientific Discovery",
        "link_suffix": "/forum?id=dhoCfPPjeZ",
        "link": "https://openreview.net/forum?id=dhoCfPPjeZ",
        "pdf_link": "https://openreview.net/pdf?id=dhoCfPPjeZ",
        "keywords": "AI for Science, Large Language Model, Evolutionary Algorithm, Interpretabilty",
        "abstract": "Creating hypotheses for new observations is a key step in the scientific process of understanding a problem in any domain. A good hypothesis that is interpretable, reliable (good at predicting unseen observations), and data-efficient; is useful for scientists aiming to make novel discoveries. This paper introduces an automatic way of learning such interpretable and reliable hypotheses in a data-efficient manner. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution), an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create hypotheses as Python programs. Additionally, we propose two improvements: a program critic and a program simplifier to further improve our method to produce good hypotheses. We evaluate our method on four different real-world tasks in two scientific domains and show significantly better results. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation"
    },
    {
        "title": "MAD: Multi-Alignment MEG-to-Text Decoding",
        "link_suffix": "/forum?id=dM4yZd6ic9",
        "link": "https://openreview.net/forum?id=dM4yZd6ic9",
        "pdf_link": "https://openreview.net/pdf?id=dM4yZd6ic9",
        "keywords": "MEG, text, speech, transfer learning",
        "abstract": "Deciphering language from brain activity is a crucial task in brain-computer interface (BCI) research. Non-invasive cerebral signaling techniques including electroencephalography (EEG) and magnetoencephalography (MEG) are becoming increasingly popular due to their safety and practicality, avoiding invasive electrode implantation. However, current works under-investigated three points: 1) a predominant focus on EEG with limited exploration of MEG, which provides superior signal quality; 2) poor performance on unseen text, indicating the need for models that can better generalize to diverse linguistic contexts; 3) insufficient integration of information from other modalities, which could potentially constrain our capacity to comprehensively understand the intricate dynamics of brain activity.This study presents a novel approach for translating MEG signals into text using a speech-decoding framework with multiple alignments. Our method is the first to introduce an end-to-end multi-alignment framework for totally unseen text generation directly from MEG signals. We achieve an impressive BLEU-1 score on the $\\textit{GWilliams}$ dataset, significantly outperforming the baseline from 5.49 to 10.44 on the BLEU-1 metric. This improvement demonstrates the advancement of our model towards real-world applications and underscores its potential in advancing BCI research."
    },
    {
        "title": "MixEval-X: Any-to-any Evaluations from Real-world Data Mixture",
        "link_suffix": "/forum?id=hpCfPEvBsr",
        "link": "https://openreview.net/forum?id=hpCfPEvBsr",
        "pdf_link": "https://openreview.net/pdf?id=hpCfPEvBsr",
        "keywords": "Evaluation, Multi-modal Evaluation, Benchmark, Multi-modal Benchmark, Any-to-any, MixEval, Real-world, Data Mixture, Artificial General Intelligence, AGI",
        "abstract": "Various input and output capabilities are essential for artificial intelligence (AI) models to effectively learn from and engage with diverse real-world signals. Thus, reliable evaluations are desired to guide their development. We identify two key issues in related evaluations: (1) they have inconsistent standards, often designed by different communities with varying levels of maturity, protocols, and principles; (2) they often show strong query, grading, and generalization bias. To address these, we introduce MixEval-X, the first any-to-any real-world benchmark that optimizes and standardizes evaluation across different input and output modalities. We propose multi-modal benchmark mixture and adaptation-rectification pipelines to reconstruct real-world task distributions, ensuring that evaluation tasks generalize more effectively to real-world use cases. Extensive meta-evaluations demonstrate that our reconstruction strategy accurately aligns benchmark samples with real-world task distributions and achieves strong correlations with crowd-sourced real-world evaluation results (up to 0.98). We also provide comprehensive model evaluation results to rerank the models and organizations in the field. We present detailed insights to enhance the community's understanding of multi-modal evaluations and inform future research directions."
    },
    {
        "title": "Prompting the Unseen: Detecting Hidden Backdoors in Black-Box Models",
        "link_suffix": "/forum?id=6Nnni5GtK3",
        "link": "https://openreview.net/forum?id=6Nnni5GtK3",
        "pdf_link": "https://openreview.net/pdf?id=6Nnni5GtK3",
        "keywords": "visual prompting, model reprogramming, backdoor detection, poisoning",
        "abstract": "Visual prompting (VP) is a new technique that adapts well-trained frozen models for source domain tasks to target domain tasks. This study examines VP's benefits for black-box model-level backdoor detection. The visual prompt in VP maps class subspaces between source and target domains. We identify a misalignment, termed class subspace inconsistency, between clean and poisoned datasets. Based on this, we introduce BProm, a black-box model-level detection method to identify backdoors in suspicious models, if any. BProm leverages the low classification accuracy of prompted models when backdoors are present. Extensive experiments confirm BProm's effectiveness."
    },
    {
        "title": "Neural Exploratory Landscape Analysis",
        "link_suffix": "/forum?id=EEI5R89Cmv",
        "link": "https://openreview.net/forum?id=EEI5R89Cmv",
        "pdf_link": "https://openreview.net/pdf?id=EEI5R89Cmv",
        "keywords": "Landscape Analysis, Black-Box Optimization, Meta-Black-Box Optimization, Learning to Optimize",
        "abstract": "Recent research in Meta-Black-Box Optimization (MetaBBO) have shown that meta-trained neural networks can effectively guide the design of black-box optimizers, significantly reducing the need for expert tuning and delivering robust performance across complex problem distributions. Despite their success, a paradox remains: MetaBBO still rely on human-crafted Exploratory Landscape Analysis features to inform the meta-level agent about the low-level optimization progress. To address the gap, this paper proposes Neural Exploratory Landscape Analysis (NeurELA), a novel framework that dynamically profiles landscape features through a two-stage, attention-based neural network, executed in an entirely end-to-end fashion. NeurELA is pre-trained over a variety of MetaBBO algorithms using a multi-task neuroevolution strategy. Extensive experiments show that NeurELA achieves consistently superior performance when integrated into different and even unseen MetaBBO tasks and can be efficiently fine-tuned for further performance boost. This advancement marks a pivotal step in making MetaBBO algorithms more autonomous and broadly applicable. The source code of NeurELA can be accessed athttps://anonymous.4open.science/r/Neur-ELA-303C."
    },
    {
        "title": "ACTIVE: Offline Reinforcement Learning via Adaptive Imitation and In-sampleV-Ensemble",
        "link_suffix": "/forum?id=qiluFujVc8",
        "link": "https://openreview.net/forum?id=qiluFujVc8",
        "pdf_link": "https://openreview.net/pdf?id=qiluFujVc8",
        "keywords": "Offline Reinforcement Learning, In-sample Learning, Ensemble, Uncertainty Quantification, Dual Gradient Descent",
        "abstract": "Offline reinforcement learning (RL) aims to learn from static datasets and thus faces the challenge of value estimation errors for out-of-distribution actions. The in-sample learning scheme addresses this issue by performing implicit TD backups that does not query the values of unseen actions. However, pre-existing in-sample value learning and policy extraction methods suffer from over-regularization, limiting their performance on suboptimal or compositional datasets. In this paper, we analyze key factors in in-sample learning that might potentially hinder the use of a milder constraint. We propose Actor-Critic with Temperature adjustment and In-sample Value Ensemble (ACTIVE), a novel in-sample offline RL algorithm that leverages an ensemble of $V$-functions for critic training and adaptively adjusts the constraint level using dual gradient descent. We theoretically show that the $V$-ensemble suppresses the accumulation of initial value errors, thereby mitigating overestimation. Our experiments on the D4RL benchmarks demonstrate that ACTIVE alleviates overfitting of value functions and outperforms existing in-sample methods in terms of learning stability and policy optimality."
    },
    {
        "title": "Stochastic Steepest Descent with Acceleration for\u2113p-Smooth Non-Convex Optimization",
        "link_suffix": "/forum?id=I9aemDuy5b",
        "link": "https://openreview.net/forum?id=I9aemDuy5b",
        "pdf_link": "https://openreview.net/pdf?id=I9aemDuy5b",
        "keywords": "Non-convex Optimization, Non-Euclidean Acceleration, Stochastic Steepest Descent",
        "abstract": "In this work, we analyze stochastic $\\ell_p$ steepest descent for non-convex problems. Specifically, for $p > 2$, we establish $\\epsilon$-approximate stationarity (in expectation) with respect to the dual norm $\\Vert\\cdot\\Vert_{p^*}^{p^*}$ at a rate of $O(\\epsilon^{-4})$, thereby generalizing the previous guarantees for signSGD ($p=\\infty$). In addition, inspired by techniques for the convex setting, we present a new accelerated $\\ell_p$ descent method, called Stacey, based on interpolated primal-dual iterate sequences that are designed for non-Euclidean smooth optimization settings. We compare our algorithm against popular methods such as SGD, Adam, AdamW, and Lion on image classification and pretraining language modeling tasks, and our results demonstrate the potential for both faster convergence and achieving higher accuracy. We further evaluate our algorithm for different values of $p$ across various models and datasets, highlighting the importance and efficiency of non-Euclidean methods as compared to standard Euclidean-based approaches."
    },
    {
        "title": "HeteGraph-Mamba: Heterogeneous Graph Learning via Selective State Space Model",
        "link_suffix": "/forum?id=mn61GWpEiK",
        "link": "https://openreview.net/forum?id=mn61GWpEiK",
        "pdf_link": "https://openreview.net/pdf?id=mn61GWpEiK",
        "keywords": "graph neural network, state space model",
        "abstract": "We propose a heterogeneous graph mamba network (HGMN) as the first exploration in leveraging the selective state space models (SSSMs) for heterogeneous graph learning. Compared with the literature, our HGMN overcomes two major challenges: (i) capturing long-range dependencies among heterogeneous nodes and (ii) adapting SSSMs to heterogeneous graph data. Our key contribution is a general graph architecture that can solve heterogeneous nodes in real-world scenarios, followed an efficient flow. Methodologically, we introduce a two-level efficient tokenization approach that first captures long-range dependencies within identical node types, and subsequently across all node types. Empirically, we conduct comparisons between our framework and 19 state-of-the-art methods on the heterogeneous benchmarks. The extensive comparisons demonstrate that our framework outperforms other methods in both the accuracy and efficiency dimensions."
    }
]