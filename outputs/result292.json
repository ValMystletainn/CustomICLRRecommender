[
    {
        "title": "On the expressiveness and spectral bias of KANs",
        "link_suffix": "/forum?id=ydlDRUuGm9",
        "link": "https://openreview.net/forum?id=ydlDRUuGm9",
        "pdf_link": "https://openreview.net/pdf?id=ydlDRUuGm9",
        "keywords": "Kolmogorov-Arnold Network, Spectral Bias, Approximation Theory",
        "abstract": "Kolmogorov-Arnold Networks (KAN) \\cite{liu2024kan} were very recently proposed as a potential alternative to the prevalent architectural backbone of many deep learning models, the multi-layer perceptron (MLP). KANs have seen success in various tasks of AI for science, with their empirical efficiency and accuracy demostrated in function regression, PDE solving, and many more scientific problems.In this article, we revisit the comparison of KANs and MLPs, with emphasis on a theoretical perspective. On the one hand, we compare the representation and approximation capabilities of KANs and MLPs. We establish that MLPs can be represented using KANs of a comparable size. This shows that the approximation and representation capabilities of KANs are at least as good as MLPs. Conversely, we show that KANs can be represented using MLPs, but that in this representation the number of parameters increases by a factor of the KAN grid size. This suggests that KANs with a large grid size may be more efficient than MLPs at approximating certain functions. On the other hand, from the perspective of learning and optimization, we study the spectral bias of KANs compared with MLPs. We demonstrate that KANs are less biased toward low frequencies than MLPs. We highlight that the multi-level learning feature specific to KANs, i.e. grid extension of splines, improves the learning process for high-frequency components.  Detailed comparisons with different choices of depth, width, and grid sizes of KANs are made, shedding some light on how to choose the hyperparameters in practice."
    },
    {
        "title": "Neural Probabilistic Logic Learning for Knowledge Graph Reasoning",
        "link_suffix": "/forum?id=EGxgZzDODh",
        "link": "https://openreview.net/forum?id=EGxgZzDODh",
        "pdf_link": "https://openreview.net/pdf?id=EGxgZzDODh",
        "keywords": "Knowledge graph reasoning, embedding, rule-based, variational inference",
        "abstract": "Knowledge graph (KG) reasoning is a task that aims to predict unknown facts based on known factual samples. Reasoning methods can be divided into two categories: rule-based methods and KG-embedding based methods. The former possesses precise reasoning capabilities but finds it challenging to reason efficiently over large-scale knowledge graphs. While gaining the ability to reason over large-scale knowledge graphs, the latter sacrifices reasoning accuracy. This paper aims to design a reasoning framework called Neural Probabilistic Logic Learning(NPLL) that achieves accurate reasoning on knowledge graphs. Our approach introduces a scoring module that effectively enhances the expressive power of embedding networks. We strike a balance between model simplicity and reasoning capabilities by incorporating a Markov Logic Network based on variational inference. We empirically evaluate our approach on several benchmark datasets, and the experimental results validate that our method substantially enhances the accuracy and quality of the reasoning results."
    },
    {
        "title": "MPC-Minimized Secure LLM Inference",
        "link_suffix": "/forum?id=beAlX6RjsW",
        "link": "https://openreview.net/forum?id=beAlX6RjsW",
        "pdf_link": "https://openreview.net/pdf?id=beAlX6RjsW",
        "keywords": "secure inference, secure multi-party computation (MPC), transformer, large language model (LLM), open-source foundational model, fine-tuning, LoRA, head-merging",
        "abstract": "Many inference services based on large language models (LLMs) pose a privacy concern, either revealing user prompts to the service or the proprietary weights to the user. Secure inference offers a solution to this problem through secure multi-party computation (MPC), however, it is still impractical for modern LLM workload due to the large overhead imposed by MPC. To address this overhead, we propose\nMARILL, a framework that adapts LLM fine-tuning to minimize MPC usage during secure inference. MARILL introduces high-level architectural changes during fine-tuning that significantly reduce the number of expensive operations needed within MPC during inference, by removing some and relocating others outside MPC without compromising security. As a result, MARILL-generated models are more efficient across all secure inference protocols and our approach complements MPC-friendly approximations for such operations. Compared to standard fine-tuning, MARILL results in $2.2\u221211.3\\times$ better runtime and $2.4\u22126.9\\times$ better communication during secure inference across various MPC settings, while typically preserving over $90$% performance across downstream tasks. Anonymous code is available athttps://anonymous.4open.science/r/MPC-auto-B100."
    },
    {
        "title": "Predicting Observation after Action in a Hierarchical Energy-based Model with Memory",
        "link_suffix": "/forum?id=DP3BwwTKbL",
        "link": "https://openreview.net/forum?id=DP3BwwTKbL",
        "pdf_link": "https://openreview.net/pdf?id=DP3BwwTKbL",
        "keywords": "Neuroscience, Prediction, Energy-based models, Sampling-based inference, Local learning rules, Attractor neural networks",
        "abstract": "Understanding the mechanisms of brain function is greatly advanced by predictive models. Recent advancements in machine learning further underscore the potency of prediction for learning optimal representation. However, there remains a gap in creating a biologically plausible model that explains how the neural system achieves prediction. In this paper, we introduce a framework employing an energy-based model (EBM) to capture the nuanced processes of predicting observation after action within the neural system, encompassing prediction, learning, and inference. We implement the EBM with a hierarchical structure and integrate a continuous attractor neural network for memory, constructing a biologically plausible model. In experimental evaluations, our model demonstrates efficacy across diverse scenarios. The range of actions includes eye movement, motion in environments, head turning, and static observation while the environment changes. Our model not only makes accurate predictions for environments it was trained on, but also provides reasonable predictions for unseen environments, matching the performances of machine learning methods in multiple tasks. We hope that this study contributes to a deep understanding of how the neural system performs prediction."
    },
    {
        "title": "Learning Differential Pyramid Representation for Tone Mapping",
        "link_suffix": "/forum?id=bztzb1fyhv",
        "link": "https://openreview.net/forum?id=bztzb1fyhv",
        "pdf_link": "https://openreview.net/pdf?id=bztzb1fyhv",
        "keywords": "Tone Mapping, Differential Pyramid, Image Signal Processor, High Dynamic Range, Image Retouching",
        "abstract": "To display high dynamic range (HDR) images on low dynamic range (LDR) screens, tone mapping operations (TMO) are required to compress the dynamic range. Recently, the deep learning-based TMO methods with 3D Look-Up Table (LUT) have shown promising performance. However, these methods often fail to deliver satisfactory results in local areas, and generating image-level TMO on down-sampled low-resolution images leads to loss of details. To overcome this problem, we propose to construct a learnable differential pyramid representation network, termed DPRNet, for joint global and local tone mapping. Specifically, we construct multi-layer perceptrons to globally modulate the tones in pixel-level. Then, we propose a local 3D LUT, which generates the TMO coefficients in patch-level. To further enhance the details, we propose a learnable differential pyramid to capture multi-scale high-frequency components, coupled with an iterative mask learning strategy to refine high-frequency details. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art methods, improving PSNR by 2.58 dB in the HDR+ dataset and 3.31 dB in the HDRI Haven dataset respectively compared with the second best method. In addition, our method has the best generalization ability in unsupervised video TMO. We provide an anonymous online demo athttps://xxxxxx2024.github.io/DPRNet/."
    },
    {
        "title": "OccSora: 4D Occupancy Generation Models as World Simulators for Autonomous Driving",
        "link_suffix": "/forum?id=XIFnghzusY",
        "link": "https://openreview.net/forum?id=XIFnghzusY",
        "pdf_link": "https://openreview.net/pdf?id=XIFnghzusY",
        "keywords": "Autonomous driving, world model",
        "abstract": "Understanding the evolution of 3D scenes is important for effective autonomous driving. While conventional methods model the scene development with the motion of individual instances, world models emerge as a generative framework to describe the general scene dynamics. However, most existing methods adopt an autoregressive framework to perform next-token prediction, which suffer from inefficiency to model long-term temporal evolutions.To address this, we propose a diffusion-based 4D occupancy generation model, OccSora, to simulate the development of the 3D world for autonomous driving. We employ a 4D scene tokenizer to obtain compact discrete spatial-temporal representations for 4D occupancy input and achieve high-quality reconstruction for long-sequence occupancy videos. We then learn a diffusion transformer on the spatial-temporal representations and generate 4D occupancy conditioned on a trajectory prompt. We conduct extensive experiments on the widely used nuScenes dataset with Occ3D occupancy annotations. OccSora can generate 16s videos with authentic 3D layout and temporal consistency, demonstrating its ability to understand the spatial and temporal distributions of driving scenes. With trajectory-aware 4D generation, OccSora has the potential to serve as a world simulator for the decision-making of autonomous driving."
    },
    {
        "title": "Towards Effective Updating of Pretrained Symbolic Music Models for Fine-Grained Bar-Level Control",
        "link_suffix": "/forum?id=IeZpJNc3uy",
        "link": "https://openreview.net/forum?id=IeZpJNc3uy",
        "pdf_link": "https://openreview.net/pdf?id=IeZpJNc3uy",
        "keywords": "Symbolic Music Model, Fine-Grained Bar-Level Control",
        "abstract": "Automatically generating symbolic music scores tailored to specific user needs offers significant benefits for musicians and enthusiasts alike. Pretrained symbolic music autoregressive models have demonstrated promising results, thanks to large datasets and advanced transformer architectures. However, in practice, the control provided by such models is often limited, particularly when fine-grained controls are needed at the level of individual bars. While fine-tuning the model with newly introduced control tokens may seem like a straightforward solution, our research reveals challenges in this approach, as the model frequently struggles to respond effectively to these precise bar-level control signals. To overcome this issue, we propose two novel strategies. First, we introduce a pre-training task that explicitly links control signals with their corresponding musical tokens, enabling a more effective initialization for fine-tuning. Second, we develop a unique counterfactual loss function that enhances alignment between the generated music and the specified control prompts. These combined methods substantially improve bar-level control, yielding a 13.06% improvement over the fine-tuning baseline. Importantly, subjective evaluations confirm that this increased control does not compromise the musical quality produced by the original pretrained model."
    },
    {
        "title": "Entropy-driven Data Knowledge Distillation in Digraph Representation Learning",
        "link_suffix": "/forum?id=gLaEjxiRc3",
        "link": "https://openreview.net/forum?id=gLaEjxiRc3",
        "pdf_link": "https://openreview.net/pdf?id=gLaEjxiRc3",
        "keywords": "Data Knowledge Distillation, Graph Neural Network, Directed Graph Learning",
        "abstract": "The directed graph (digraph), as a generalization of undirected graphs, exhibits superior representation capability in modeling complex topology systems and has garnered considerable attention in recent years. Despite the notable efforts made by existing DiGraph Neural Networks (DiGNNs) to leverage directed edges, they still fail to comprehensively delve into the abundant data knowledge concealed in the digraphs. This limitation results in sub-optimal performance and underscores the necessity of further exploring the potential correlations between the directed topology and node profiles from a data-centric perspective, thereby empowering model-centric neural networks with stronger encoding capabilities. In this paper, we propose \\textbf{E}ntropy-driven \\textbf{D}igraph knowl\\textbf{E}dge distillatio\\textbf{N} (EDEN), which can serve as a new data-centric digraph learning paradigm or a model-agnostic hot-and-plug data online knowledge distillation module for most existing DiGNNs to fully leverage informative digraphs. Specifically, EDEN first utilizes directed structural measurements from a topological perspective to construct a knowledge tree, guided by the hierarchical encoding theory. Subsequently, EDEN quantifies the mutual information of nodes from a feature perspective to further refine the knowledge flow, facilitating tree layer-wise knowledge distillation. As a general framework, EDEN also can naturally extend to undirected scenarios and demonstrate satisfactory performance. In our experiments, EDEN has been widely evaluated on 14 (di)graph datasets and across 4 downstream tasks. The results demonstrate that EDEN attains SOTA performance and exhibits strong improvement for prevalent (Di)GNNs."
    },
    {
        "title": "High dimensional Bayesian Optimization via Condensing-Expansion Projection",
        "link_suffix": "/forum?id=M8XUdsjxQM",
        "link": "https://openreview.net/forum?id=M8XUdsjxQM",
        "pdf_link": "https://openreview.net/pdf?id=M8XUdsjxQM",
        "keywords": "Bayesian optimization, random projection, high-dimension, Gaussian Process",
        "abstract": "In high-dimensional settings, Bayesian optimization (BO) can be expensive and infeasible. The random embedding Bayesian optimization algorithm is commonly used to address high-dimensional BO challenges. However, this method relies on the effective subspace assumption on the optimization problem's objective function, which limits its applicability. \nIn this paper, we introduce Condensing-Expansion Projection Bayesian optimization (CEPBO), a novel random projection-based approach for high-dimensional BO that does not reply on the effective subspace assumption. The approach is both simple to implement and highly practical. We present two algorithms based on different random projection matrices: the Gaussian projection matrix and the hashing projection matrix.  Experimental results demonstrate that both algorithms outperform existing random embedding-based algorithms in most cases, achieving superior performance on high-dimensional BO problems.\nThe code is available in \\url{https://anonymous.4open.science/r/CEPBO-14429}."
    },
    {
        "title": "Does SGD really happen in tiny subspaces?",
        "link_suffix": "/forum?id=v6iLQBoIJw",
        "link": "https://openreview.net/forum?id=v6iLQBoIJw",
        "pdf_link": "https://openreview.net/pdf?id=v6iLQBoIJw",
        "keywords": "optimization for deep networks, training dynamics, SGD, Hessian, low-rank subspace",
        "abstract": "Understanding the training dynamics of deep neural networks is challenging due to their high-dimensional nature and intricate loss landscapes. Recent studies have revealed that, along the training trajectory, the gradient approximately aligns with a low-rank top eigenspace of the training loss Hessian, referred to as the dominant subspace. Given this alignment, this paper explores whether neural networks can be trained within the dominant subspace, which, if feasible, could lead to more efficient training methods. Our primary observation is that when the SGD update is projected onto the dominant subspace, the training loss does not decrease further. This suggests that the observed alignment between the gradient and the dominant subspace is spurious. Surprisingly, projecting out the dominant subspace proves to be just as effective as the original update, despite removing the majority of the original update component. We observe similar behavior across practical setups, including the large learning rate regime (also known as Edge of Stability), Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the main causes and implications of this spurious alignment, shedding light on the dynamics of neural network training."
    },
    {
        "title": "MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL",
        "link_suffix": "/forum?id=6RtRsg8ZV1",
        "link": "https://openreview.net/forum?id=6RtRsg8ZV1",
        "pdf_link": "https://openreview.net/pdf?id=6RtRsg8ZV1",
        "keywords": "reinforcement learning, model based reinforcement learning, data augmentation, high update ratios",
        "abstract": "Building deep reinforcement learning (RL) agents that find a good policy with few samples has proven notoriously challenging. To achieve sample efficiency, recent work has explored updating neural networks with large numbers of gradient steps for every new sample. While such high update-to-data (UTD) ratios have shown strong empirical performance, they also introduce instability to the training process.  Previous approaches need to rely on periodic neural network parameter resets to address this instability, but restarting the training process is infeasible in many real-world applications and requires tuning the resetting interval. In this paper, we focus on one of the core difficulties of stable training with limited samples: the inability of learned value functions to generalize to unobserved on-policy actions. We mitigate this issue directly by augmenting the off-policy RL training process with a small amount of data generated from a learned world model. Our method, Model-Augmented Data for TD Learning (MAD-TD) uses small amounts of generated data to stabilize high UTD training and achieve competitive performance on the most challenging tasks in the DeepMind control suite. Our experiments further highlight the importance of employing a good model to generate data, MAD-TD's ability to combat value overestimation, and its practical stability gains for continued learning."
    },
    {
        "title": "A Study of Necessity & Sufficiency of Linear Transformations in the Attention Mechanism",
        "link_suffix": "/forum?id=PWtx9fJqM5",
        "link": "https://openreview.net/forum?id=PWtx9fJqM5",
        "pdf_link": "https://openreview.net/pdf?id=PWtx9fJqM5",
        "keywords": "Transformers, Attention, Self-Attention",
        "abstract": "Scaled Dot Product Attention (SDPA) is the backbone of many modern\n  deep-learning models. It is so versatile that it has been used in\n  natural language, vision, and multi-modal domains with very little\n  change compared to its original formulation. This paper studies the linear transformations used in SDPA. To this end, we introduce three variants of the attention mechanism by removing consecutive linear transformations or adding an extra one. We name these variants Optimized ($W^V$ removed),\n  Efficient ($W^V$ and $W^K$ removed), and Super Attention ($W^V$ and $W^K$ removed and $W^A$ introduced) to simplify comparison when referring to them. In addition to providing the mathematical intuition behind these choices, we evaluate these variants on several datasets of varying size and complexity in vision and text modalities for predictive and generative tasks. Optimized and\n  Efficient variants have one and two matrix multiplications fewer\n  per head, respectively, and 25% and 50% fewer parameters,\n  respectively, than standard SDPA. However, the performance change compared to difference in parameter count is small. Super Attention introduces a new linear transformation\n  on the values, transforming them from the left. It outperforms\n  standard SPDA in both modalities by up to 10%\n  while having one fewer matrix multiplication per head and 25% fewer\n  parameters than standard SPDA. Consequently, it is also faster than standard SDPA."
    },
    {
        "title": "Finite Sample Analyses for Continuous-time Linear Systems: System Identification and Online Control",
        "link_suffix": "/forum?id=pBQs8kQm63",
        "link": "https://openreview.net/forum?id=pBQs8kQm63",
        "pdf_link": "https://openreview.net/pdf?id=pBQs8kQm63",
        "keywords": "online control, system identification, continuous-time linear system",
        "abstract": "Real world evolves in continuous time but computations are done from finite samples. Therefore, we study algorithms using finite observations in continuous-time linear dynamical systems. \nWe first study the system identification problem, and propose a first non-asymptotic error analysis with finite observations. Our algorithm identifies system parameters without needing integrated observations over certain time intervals, making it more practical for real-world applications. Further we propose a lower bound result that shows our estimator is provably optimal up to constant factors.\nMoreover, we apply the above algorithm to online control regret analysis for continuous-time linear system. Our system identification method allows us explore more efficiently, enabling the swift detection of ineffective policies. We achieve a regret of $\\mathcal{O}(\\sqrt{T})$ over a single $T$-time horizon in a controllable system, requiring only $\\mathcal{O}(T)$ observations of the system."
    },
    {
        "title": "DROSIA: Decoupled Representation on Sequential Information Aggregation for Time Series Forecasting",
        "link_suffix": "/forum?id=6SxOzYVuy6",
        "link": "https://openreview.net/forum?id=6SxOzYVuy6",
        "pdf_link": "https://openreview.net/pdf?id=6SxOzYVuy6",
        "keywords": "decoupled representation, sequence modeling, time series forecasting, representation learning",
        "abstract": "Time series forecasting is crucial in various fields, including finance, energy consumption, weather, transportation, and network traffic. It necessitates effective and efficient sequence modeling to encapsulate intricate temporal relationships. However, conventional methods often aggregate sequential information into representations of each time point by considering other points in the sequence, thereby ignoring the intra-individual information and suffering from inefficiency. To address these challenges, we introduce a novel approach, DROSIA: Decoupled Representation On Sequential Information Aggregation, which only integrates temporal relationships once as an additional representation for each point, achieving sequential information aggregation in a decoupled fashion. Thus balancing between individual and sequential information, along with a reduction in computational complexity. We select several widely used time series forecasting datasets, and previously top-performing models and baselines, for a comprehensive comparison. The experimental results validate the effectiveness and efficiency of DROSIA, which achieves state-of-the-art performance with only linear complexity. When provided with sufficiently long input data, the channel-independent DROSIA even outperforms the current best channel-dependent model, highlighting its proficiency in sequence modeling and capturing long-distance dependencies. Our code will be made open-source in the subsequent version of this paper."
    },
    {
        "title": "DiT-LSTM-SVAR Model For Portfolios",
        "link_suffix": "/forum?id=MeOi6u9E23",
        "link": "https://openreview.net/forum?id=MeOi6u9E23",
        "pdf_link": "https://openreview.net/pdf?id=MeOi6u9E23",
        "keywords": "DiT-LSTM, SVAR, Portfolios",
        "abstract": "This paper proposes a novel combined model named DiT-LSTM-SVAR, which successfully integrates time series and the Efficient Markets Hypothesis. This is the first to combine the microstructure of financial markets with deep learning networks to improve the performance of portfolios. We employ the DiT model to predict the upside and downside movements and an information decomposition model based on the SVAR model to identify random walk stocks. The DiT module significantly improves the Matthews correlation coefficient by almost 3%. The annual return of the portfolio is improved by almost 20%. The SVAR module greatly improves the Matthews correlation coefficient by almost 4%. Portfolios constructed using the DiT-LSTM-SVAR module based on market and public information outperformed those created with the DiT-LSTM model. The annual cumulative return of the portfolio is 266.60% and a Sharpe ratio of 1.8."
    },
    {
        "title": "Understanding Nonlinear Implicit Bias via Region Counts in Input Space",
        "link_suffix": "/forum?id=9UxC2J7Pup",
        "link": "https://openreview.net/forum?id=9UxC2J7Pup",
        "pdf_link": "https://openreview.net/pdf?id=9UxC2J7Pup",
        "keywords": "implicit bias, region counts, non-linear neural network, generalization gap",
        "abstract": "One explanation for the strong generalization ability of neural networks is implicit bias. Yet, the definition and mechanism of implicit bias in non-linear contexts remains little understood. In this work, we propose to characterize implicit bias by the count of connected regions in the input space with the same predicted label. Compared with parameter-dependent metrics (e.g., norm or normalized margin), region count can be better adapted to nonlinear, overparameterized models, because it is determined by the function mapping and is invariant to reparametrization. Empirically, we found that small region counts align with geometrically simple decision boundaries and correlate well with good generalization performance. We also observe that good hyper-parameter choices such as larger learning rates and smaller batch sizes can induce small region counts. We further establish the theoretical connections and explain how larger learning rate can induce small region counts in neural networks."
    },
    {
        "title": "Differentiable Cluster Graph Neural Network",
        "link_suffix": "/forum?id=P4pkLckzt8",
        "link": "https://openreview.net/forum?id=P4pkLckzt8",
        "pdf_link": "https://openreview.net/pdf?id=P4pkLckzt8",
        "keywords": "Graph Neural Networks, Graph Representation Learning, Node Classification",
        "abstract": "Graph Neural Networks often struggle with long-range information propagation and \nlocal heterophilous neighborhood aggregation. Inspired by the observation that cluster patterns manifest at  global and local levels, we propose to tackle both challenges with a unified framework that incorporates a clustering inductive bias into the message passing mechanism, using additional cluster-nodes.\nCentral to our approach is the formulation of an optimal transport based clustering objective. \nHowever, optimizing this objective in a differentiable way is non-trivial.\nTo navigate this, we adopt an iterative process, alternating between solving for the cluster assignments and updating the node/cluster-node embeddings. \nNotably, our derived optimization steps are themselves simple yet elegant message passing steps operating seamlessly on a bipartite graph of nodes and cluster-nodes.\nOur clustering-based approach can effectively capture both local and global information,demonstrated by extensive experiments on  heterophilous and homophilous datasets."
    },
    {
        "title": "BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics",
        "link_suffix": "/forum?id=JXKFPJe0NU",
        "link": "https://openreview.net/forum?id=JXKFPJe0NU",
        "pdf_link": "https://openreview.net/pdf?id=JXKFPJe0NU",
        "keywords": "Robotic Manipulation, Model-Based Planning, Neural Dynamics, Branch-and-Bound Method",
        "abstract": "Neural-network-based dynamics models learned from observational data have shown strong predictive capabilities for scene dynamics in robotic manipulation tasks. However, their inherent non-linearity presents significant challenges for effective planning. Current planning methods, often dependent on extensive sampling or local gradient descent, struggle with long-horizon motion planning tasks involving complex contact events.\nIn this paper, we present a GPU-accelerated branch-and-bound (BaB) framework for motion planning in manipulation tasks that require trajectory optimization over neural dynamics models. Our approach employs a specialized branching heuristic to divide the search space into sub-domains and applies a modified bound propagation method, inspired by the state-of-the-art neural network verifier $\\alpha,\\beta$-CROWN, to efficiently estimate objective bounds within these sub-domains. The branching process guides planning effectively, while the bounding process strategically reduces the search space.\nOur framework achieves superior planning performance, generating high-quality state-action trajectories and surpassing existing methods in challenging, contact-rich manipulation tasks such as non-prehensile planar pushing with obstacles, object sorting, and rope routing in both simulated and real-world settings. Furthermore, our framework supports various neural network architectures, ranging from simple multilayer perceptrons to advanced graph neural dynamics models, and scales efficiently with different model sizes."
    },
    {
        "title": "Dynamic Discriminative Operations for Efficient Generative Inference of LLMs",
        "link_suffix": "/forum?id=HzBfoUdjHt",
        "link": "https://openreview.net/forum?id=HzBfoUdjHt",
        "pdf_link": "https://openreview.net/pdf?id=HzBfoUdjHt",
        "keywords": "Efficient inference of LLMs, Long context",
        "abstract": "Efficient generative inference in Large Language Models (LLMs) is impeded by the growing memory demands of Key-Value (KV) cache, especially for longer sequences. Traditional KV Cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, leading to issues such as context loss or hallucinations. To address this, we introduceDynamicDiscriminativeOperations ($\\mathbf{D_2 O}$), a novel method that optimizes KV cache size dynamically and discriminatively at two levels without fine-tuning, while preserving essential context. Atlayer-level, by observing the varying densities of attention weights between shallow and deep layers, we dynamically determine which layers should avoid excessive eviction via our proposeddynamic allocation strategyto minimize information loss. Attoken-level, for the eviction strategy in each layer, $\\mathbf{D_2 O}$ innovatively incorporates acompensation mechanismthat maintains a similarity threshold to re-discriminate the importance of currently discarded tokens, determining whether they should be recalled and merged with similar tokens. Extensive experiments on various benchmarks and LLM architectures have shown that $\\mathbf{D_2 O}$ not only achieves significant memory savings and enhances inference throughput by more than 3$\\times$ but also maintains high-quality long-text generation."
    },
    {
        "title": "Towards Black-Box Membership Inference Attack for Diffusion Models",
        "link_suffix": "/forum?id=LRSspInlN5",
        "link": "https://openreview.net/forum?id=LRSspInlN5",
        "pdf_link": "https://openreview.net/pdf?id=LRSspInlN5",
        "keywords": "diffusion model, membership inference attack",
        "abstract": "Given the rising popularity of AI-generated art and the associated copyright con-\ncerns, identifying whether an artwork was used to train a diffusion model is an\nimportant research topic. The work approaches this problem from the membership\ninference attack (MIA) perspective. We first identify the limitation of applying\nexisting MIA methods for proprietary diffusion models: the required access of\ninternal U-nets. To address the above problem, we introduce a novel member-\nship inference attack method that uses only the image-to-image variation API and\noperates without access to the model\u2019s internal U-net. We validate our method\nusing DDIM and Stable Diffusion setups and further extend both our approach and\nexisting algorithms to the Diffusion Transformer architecture. Our experimental\nresults consistently outperform previous methods."
    },
    {
        "title": "Scaling Laws for Downstream Task Performance in Machine Translation",
        "link_suffix": "/forum?id=vPOMTkmSiu",
        "link": "https://openreview.net/forum?id=vPOMTkmSiu",
        "pdf_link": "https://openreview.net/pdf?id=vPOMTkmSiu",
        "keywords": "scaling laws, transfer learning, machine translation, large language models, data valuation",
        "abstract": "Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the \\emph{pretraining} data and its size affect downstream performance (translation quality) as judged by: downstream cross-entropy and translation quality metrics such as BLEU and COMET scores. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and translation quality scores improve monotonically with more pretraining data. In such cases, we show that it is possible to predict the downstream translation quality metrics with good accuracy using a log-law. However, there are cases where moderate misalignment causes the downstream translation scores to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By analyzing these, we provide new practical insights for choosing appropriate pretraining data."
    },
    {
        "title": "Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models",
        "link_suffix": "/forum?id=vtT09dYPGI",
        "link": "https://openreview.net/forum?id=vtT09dYPGI",
        "pdf_link": "https://openreview.net/pdf?id=vtT09dYPGI",
        "keywords": "multimodal large language model, dynamic routing",
        "abstract": "Recently, mixture of experts (MoE) has become a popular paradigm for achieving the trade-off between modal capacity and efficiency of multimodal large language models (MLLMs). Different from previous efforts, we are dedicated to exploring the dynamic experts in existing MLLMs and showing that a standard MLLM can also be a mixture of experts. However, achieving this target is still notoriously challenging. The well-trained MLLMs are more accustomed to the fixed pathway and a drastic change in its inference manner also greatly impedes its performance. To address these issues, we propose a novel dynamic expert routing method for existing MLLMs, termed Routing Experts (RoE), which can achieve example-dependent optimal path routing without obvious structure tweaks. Meanwhile, a new structure sparsity regularization is also introduced to force the well-trained MLLMs to learn more short-cut pathways. In addition, we also address the alignment of the training and inference of MLLMs in terms of network routing. To validate RoE, we apply it to a set of existing MLLMs, including LLaVA-1.5, LLaVA-HR and VILA, and conduct extensive experiments on a bunch of VL benchmarks. The experiment results not only show the effectiveness of our RoE in improving MLLMs' efficiency, but also yield obvious advantages over MoE-LLaVA in both performance and speed, e.g.,  an average performance gain of 3.3% on 5 benchmarks while being 1.61 times faster. Our code is anonymously released athttps://anonymous.4open.science/r/AnonymousRoE-6FE6"
    },
    {
        "title": "Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator",
        "link_suffix": "/forum?id=VD4PFpecG2",
        "link": "https://openreview.net/forum?id=VD4PFpecG2",
        "pdf_link": "https://openreview.net/pdf?id=VD4PFpecG2",
        "keywords": "reinforcement Learning, regret, LEQR, finite horizon, episodic setting",
        "abstract": "Risk-sensitive linear quadratic regulator is one of the most fundamental problems in risk-sensitive optimal control. \nIn this paper, we study online adaptive control of risk-sensitive linear quadratic regulator in the finite horizon episodic setting. \nWe propose a simple least-squares greedy algorithm and show that it achieves $\\widetilde{\\mathcal{O}}(\\log N)$ regret under a specific identifiability assumption, where $N$ is the total number of episodes. If the identifiability assumption is not satisfied, we propose incorporating exploration noise into the least-squares-based algorithm, resulting in an algorithm with $\\widetilde{\\mathcal{O}}(\\sqrt{N})$ regret. \nTo our best knowledge, this is the first set of regret bounds for episodic risk-sensitive linear quadratic regulator. \nOur proof relies on perturbation analysis of less-standard Riccati equations for risk-sensitive linear quadratic control, and a delicate analysis of the loss in the risk-sensitive performance criterion due to applying the suboptimal controller in the online learning process."
    },
    {
        "title": "SinPoint: A Novel Topological Consistent Augmentation for Point Cloud Understanding",
        "link_suffix": "/forum?id=jMtsvoOU3G",
        "link": "https://openreview.net/forum?id=jMtsvoOU3G",
        "pdf_link": "https://openreview.net/pdf?id=jMtsvoOU3G",
        "keywords": "Topological consistent, homeomorphism, Point clouds augmentation",
        "abstract": "Data augmentation is a highly effective method for addressing the issue of data scarcity in machine learning and computer vision tasks. It involves diversifying the original data through a series of transformations to improve the robustness and generalization ability of the model. However, due to the disorder and irregularity of point clouds, existing methods struggle to enrich geometric diversity and maintain topological consistency, leading to imprecise point cloud understanding. In this paper, we propose SinPoint, a novel method designed to preserve the topological structure of the original point cloud through a homeomorphism. Additionally, it utilizes the Sine function to generate smooth displacements. This simulates object deformations, thereby producing a rich diversity of samples. Our extensive experiments demonstrate that SinPoint consistently outperforms existing Mixup and Deformation methods on various benchmark point cloud datasets, improving performance for shape classification and part segmentation tasks. Specifically, when used with PointNet++ and DGCNN, SinPoint achieves a state-of-the-art accuracy of 90.2 on shape classification with the real-world ScanObjectNN dataset. Furthermore, our method is highly versatile and scalable, and it can adapt to different scenarios and requirements for point cloud tasks."
    },
    {
        "title": "SteBen: Steiner Tree Problem Benchmark for Neural Combinatorial Optimization on Graphs",
        "link_suffix": "/forum?id=tKif2rXQ6V",
        "link": "https://openreview.net/forum?id=tKif2rXQ6V",
        "pdf_link": "https://openreview.net/pdf?id=tKif2rXQ6V",
        "keywords": "Dataset & benchmark, Neural Combinatorial Optimization, Steiner Tree Problem",
        "abstract": "The Steiner Tree Problem (STP) is an NP-hard combinatorial optimization problem with applications in areas like network design and facility location. Despite its importance, learning-based solvers for STP have been hindered by the lack of large-scale, diverse datasets necessary to train and evaluate advanced neural models. To address this limitation, we introduce a standardized dataset comprising over a million high-quality instances with optimal solutions, spanning various problem sizes and graph structures. Our dataset enables benchmarking of neural combinatorial optimization methods across both supervised and reinforcement learning paradigms, encompassing autoregressive and non-autoregressive inference approaches. Our experiments show that supervised learning excels in in-distribution settings, while reinforcement learning generalizes better to unseen problem sizes, highlighting a trade-off between solution quality and generalization. We compare NCO methods across different STP scales and graph types, and demonstrate that solvers trained on our datasets generalize well to real-world instances without fine-tuning, proving its practical utility. We hope this benchmark promotes further STP research and advances NCO techniques for broader combinatorial optimization challenges."
    }
]