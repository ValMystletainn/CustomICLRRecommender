[
    {
        "title": "Efficient Time Series Forecasting via Hyper-Complex Models and Frequency Aggregation",
        "link_suffix": "/forum?id=WFlLqUmb9v",
        "link": "https://openreview.net/forum?id=WFlLqUmb9v",
        "pdf_link": "https://openreview.net/pdf?id=WFlLqUmb9v",
        "keywords": "time-series forecasting, frequency models, hyper-complex machine learning, short-time Fourier transform",
        "abstract": "Time series forecasting is a long-standing problem in statistics and machine learning. One of the key challenges is processing sequences with long-range dependencies. To that end, a recent line of work applied the short-time Fourier transform (STFT), which partitions the sequence into multiple subsequences and applies a Fourier transform to each separately. Current models do not leverage information that is shared between neighboring windows, despite being highly correlated.  We propose the Frequency Information Aggregation (FIA)-Net, which is based on a novel complex-valued MLP architecture that aggregates adjacent window information in the frequency domain. To further increase the receptive field of the FIA-Net, we treat the set of windows as hyper-complex (HC) valued vectors and employ HC algebra to efficiently combine information from all STFT windows altogether. Using the HC-MLP backbone allows for improved handling of sequences with long-term dependence. Furthermore, due to the nature of HC operations, the HC-MLP uses up to three times fewer parameters than the equivalent standard window aggregation method. We evaluate the FIA-Net on various time-series benchmarks and show that the proposed methodologies outperform existing state of the art methods in terms of both accuracy and efficiency. Our code is publicly available on \\url{https://anonymous.4open.science/r/research-1803/}."
    },
    {
        "title": "A New Look at Low-Rank Recurrent Neural Networks",
        "link_suffix": "/forum?id=fWXYD0ZCdd",
        "link": "https://openreview.net/forum?id=fWXYD0ZCdd",
        "pdf_link": "https://openreview.net/pdf?id=fWXYD0ZCdd",
        "keywords": "low-rank rnn, computational neuroscience, dynamical systems, neural dynamics",
        "abstract": "Low-rank recurrent neural networks (RNNs) have recently gained prominence as a framework for understanding how neural systems solve complex cognitive tasks. However, training and interpreting these networks remains an important open problem. Here we address these challenges by adopting a view of low-rank RNNs as parametrizing a low-dimensional ordinary differential equation (ODE) using a set of nonlinear basis functions. This perspective, which arises from an approach known as the ``neural engineering framework'', reveals that low-rank RNNs are equivalent to neural ODEs with a single hidden layer. We show that training a low-rank RNN to implement a particular dynamical system can thus be formalized as least-squares regression in a random basis. This allows us to propose a new method for finding the smallest RNN capable of implementing a dynamical system using a variant of orthogonal matching pursuit. More generally, our perspective clarifies limits on the expressivity of low-rank RNNs, such as the fact that without inputs, a low-rank RNN with sigmoidal nonlinearity can only implement odd-symmetric functions. We delve further into the role of inputs in shaping network dynamics and show that RNNs can produce identical trajectories using  a wide variety of static or time-varying dynamics; this highlights the importance of perturbations for inferring dynamics from observed neural  trajectories. Finally, we highlight the usefulness of our  framework by comparing to RNNs trained using backprop-through-time on neuroscience-inspired tasks, showcasing that our method achieves faster and more accurate learning with smaller networks than gradient-based training."
    },
    {
        "title": "Are Language Model Logits Calibrated?",
        "link_suffix": "/forum?id=x418ZpazsR",
        "link": "https://openreview.net/forum?id=x418ZpazsR",
        "pdf_link": "https://openreview.net/pdf?id=x418ZpazsR",
        "keywords": "language modeling, calibration, model understanding",
        "abstract": "Some information is factual (e.g., \"Paris is in France\"), whereas other information is probabilistic (e.g., \"the coin flip will be a [Heads/Tails].\").  We believe that good Language Models (LMs) should understand and reflect this nuance. Our work investigates this by testing if LMs' output probabilities arecalibratedto their textual contexts. We define model \"calibration\" as the degree to which the output probabilities of candidate tokens are aligned with the relative likelihood that should be inferred from the given context. For example, if the context concerns two equally likely options (e.g., heads or tails for a fair coin), the output probabilities should reflect this. Likewise, context that concerns non-uniformly likely events (e.g., rolling a six with a die) should also be appropriately captured with proportionate output probabilities. We find that even in simple settings the best LMs (1) are poorly calibrated, and (2) have systematic biases (e.g., preferred colors and sensitivities to word orderings). For example, gpt-4o-mini often picks the first of two options presented in the prompt regardless of the options' implied likelihood, whereas Llama-3.1-8B picks the second. Our other consistent finding is mode-collapse: Instruction-tuned models often over-allocate probability mass on a single option. These systematic biases introduce non-intuitive model behavior, making models harder for users to understand."
    },
    {
        "title": "Optimal Hyperdimensional Representation for Learning and Cognitive Computation",
        "link_suffix": "/forum?id=NYPJz0CL5X",
        "link": "https://openreview.net/forum?id=NYPJz0CL5X",
        "pdf_link": "https://openreview.net/pdf?id=NYPJz0CL5X",
        "keywords": "hyperdimensional computing; vector symbolic architecture; decoding;",
        "abstract": "Hyperdimensional Computing (HDC), as a novel neurally-inspired computing methodology, uses lightweight and high-dimensional operations to realize major brain functionalities. Recent HDC works mainly focus on two aspects: brain-like learning and cognitive computation. However, it lacks differentiation between these functions and their requirements for HDC algorithms. We address this gap by proposing an adaptable hyperdimensional kernel-based encoding method. We explore how encoding settings impact HDC performance for both tasks, highlighting the distinction between learning patterns and retrieving information. We provide detailed guidance on kernel design, optimizing data points for accurate decoding or correlated learning. Experimental results with our proposed encoder significantly boost image classification accuracy from 65% to 95% by considering pixel correlations and increase decoding accuracy from 85% to 100% by maximizing pixel vector separation. Factorization tasks are shown to require highly exclusive representation to enable accurate convergence."
    },
    {
        "title": "Investigating Grokking phenomena below the Critical Data Regime",
        "link_suffix": "/forum?id=9spNhEw6qf",
        "link": "https://openreview.net/forum?id=9spNhEw6qf",
        "pdf_link": "https://openreview.net/pdf?id=9spNhEw6qf",
        "keywords": "Grokking",
        "abstract": "In this paper, we explore the practical utility of grokking, a phenomenon where\nmodels generalize long after overfitting the training data. This offers a promising\navenue for training on changing distributions, especially in data-scarce environ-\nments. We investigate a scenario where a model grokked on a distribution p1 is\nutilized to grok another model on a different distribution p2, particularly in a data\ncrunch situation on the p2 distribution. We further explore distilling multiple small\nmodels grokked on different distributions to generalize a larger model. This ap-\nproach is crucial where data is scarcely available for these different distributions,\nthus saving computational resources. Finally, we present a setup for continually\npretraining a grokked model from distribution p1 to p2. Our experiments reveal\nthat distilling from a grokked model provides quick generalization over the cur-\nrent task while simultaneously alleviating the forgetting of previous knowledge.\nWe analyze these scenarios over various algorithmic tasks such as addition, sub-\ntraction, and multiplication. Our results provide a framework for efficient model\ntraining in dynamic and data-limited scenarios, enabling the development of more\nrobust, adaptable systems."
    },
    {
        "title": "Can Decoding by Contrasting Layers Really Improve Factuality in Large Language Models?",
        "link_suffix": "/forum?id=cb4PoT7ePW",
        "link": "https://openreview.net/forum?id=cb4PoT7ePW",
        "pdf_link": "https://openreview.net/pdf?id=cb4PoT7ePW",
        "keywords": "Factuality, Contrastive Decoding, Parametric Memory",
        "abstract": "Large language models (LLMs) have made notable advancements across diverse applications,  but their susceptibility to  hallucinations remains a critical challenge. That is, they could produce outputs  divergent from real-world evidence or user-provided inputs. Recent studies have explored a contrastive decoding strategy known as DoLa, which mitigates output inaccuracy by contrasting the outputs from the final layer against those from the previous layers.  Nevertheless, such strategy has its limitation, as LLMs, which already have internalized extensive parametric knowledge through comprehensive pre-training and fine-tuning phases, may generate errors due to incorrect or obsolete information within their parameters.  As an alternative,  trusted external knowledge could be included in the prompt context for querying,  but the constrained context window of LLMs poses a significant barrier restricting the amount of information that can be provided. \nTo address the above issues,  we propose to integrate the contrasive decoding strategy with a long-context encoder that effectively condenses extensive initial contexts into a more concise format.  Extensive experiments have demonstrated that, \nour proposed methodology enhances the factual accuracy of the produced content, \nwhen applied to various datasets. For instance, it has improved the performance of LLaMA2-7B models on the Quality dataset by 61.61%, compared to the DoLa decoding method, showcasing its effectiveness in enhancing the reliability of LLMs in generating truthful information."
    },
    {
        "title": "How Does Data Diversity Shape The Weight Landscape of Neural Networks?",
        "link_suffix": "/forum?id=wCIkU0XR4f",
        "link": "https://openreview.net/forum?id=wCIkU0XR4f",
        "pdf_link": "https://openreview.net/pdf?id=wCIkU0XR4f",
        "keywords": "data diversity, regularization, data augmentation, synthetic data, transfer learning",
        "abstract": "To enhance the generalization of machine learning models to unseen data, techniques such as dropout, weight decay (L2 regularization), and noise augmentation\nare commonly employed. While regularization methods (i.e., dropout and weight\ndecay) are geared toward adjusting model parameters to prevent overfitting, data\naugmentation increases the diversity of the input training set, a method purported\nto improve accuracy and calibration error. In this paper, we investigate the impact of each of these techniques on the parameter space of neural networks, with\nthe goal of understanding how they alter the weight landscape in transfer learning\nscenarios. To accomplish this, we employ Random Matrix Theory to analyze the\neigenvalue distributions of pre-trained models, fine-tuned using these techniques\nbut using different levels of data diversity, for the same downstream tasks. We\nobserve that diverse data influences the weight landscape in a similar fashion as\ndropout. Additionally, we compare commonly used data augmentation methods\nwith synthetic data created by generative models. We conclude that synthetic data\ncan bring more diversity into real input data, resulting in a better performance on\nout-of-distribution test instances."
    },
    {
        "title": "Generalization through variance: how noise shapes inductive biases in diffusion models",
        "link_suffix": "/forum?id=7lUdo8Vuqa",
        "link": "https://openreview.net/forum?id=7lUdo8Vuqa",
        "pdf_link": "https://openreview.net/pdf?id=7lUdo8Vuqa",
        "keywords": "diffusion models, generalization, inductive biases, theory, infinite-width neural networks, generative models, path integral",
        "abstract": "How diffusion models generalize beyond their training set is not known, and is somewhat mysterious given two facts: the optimum of the denoising score matching (DSM) objective usually used to train diffusion models is the score function of the training distribution; and the networks usually used to learn the score function are expressive enough to learn this score to high accuracy. We claim that a certain feature of the DSM objective—the fact that its target is not the training distribution's score, but a noisy quantity only equal to it in expectation—strongly impacts whether and to what extent diffusion models generalize. In this paper, we develop a mathematical theory that partly explains this 'generalization through variance' phenomenon. Our theoretical analysis exploits a physics-inspired path integral approach to compute the distributions typically learned by a few paradigmatic under- and overparameterized diffusion models. We find that the distributions diffusion models effectively learn to sample from resemble their training distributions, but with `gaps' filled in, and that this inductive bias is due to the covariance structure of the noisy target used during training. We also characterize how this inductive bias interacts with feature-related inductive biases."
    },
    {
        "title": "Learning to Explore and Exploit with GNNs for Unsupervised Combinatorial Optimization",
        "link_suffix": "/forum?id=vaJ4FObpXN",
        "link": "https://openreview.net/forum?id=vaJ4FObpXN",
        "pdf_link": "https://openreview.net/pdf?id=vaJ4FObpXN",
        "keywords": "combinatorial optimization, unsupervised learning, graph neural networks",
        "abstract": "Combinatorial optimization (CO) problems are pervasive\nacross various domains, but their NP-hard nature often necessitates problem-specific\nheuristic algorithms. Recent advancements in deep learning have led to the development of learning-based heuristics, yet these approaches often struggle with limited search capabilities.\nWe introduce  Explore-and-Exploit GNN ($X^2$GNN, pronounced x-squared GNN), \na novel unsupervised neural framework that combines exploration and exploitation for combinatorial search optimization:\ni) Exploration - $X^2$GNN generates multiple \nsolutions simultaneously, promoting diversity in the search space; \n(ii) Exploitation - $X^2$GNN  employs neural stochastic iterative refinement, where sampled partial solutions guide the search toward promising regions and help escape local optima.\n$X^2$GNN  employs neural stochastic iterative refinement to exploit partial existing solutions, guiding the search toward promising regions and helping escape local optima. By balancing exploration and exploitation $X^2$GNN achieves superior performance and generalization on several graph CO problems including Max Cut, Max Independent Set, and Max Clique.   Notably, for large Max Clique problems, $X^2$GNN consistently generates solutions within 1.2% of optimality, while other state-of-the-art learning-based approaches struggle to reach within 22% of optimal. Moreover, $X^2$GNN consistently generates better solutions than Gurobi on large graphs for all three problems under reasonable time budgets. Furthermore, $X^2$GNN exhibits exceptional generalization capabilities. For the Maximum Independent Set problem, $X^2$GNN outperforms state-of-the-art methods even when trained on smaller or out-of-distribution graphs compared to the test set."
    },
    {
        "title": "Structure-aware Attention based on Vector Symbolic Architectures",
        "link_suffix": "/forum?id=zET0Zg71WT",
        "link": "https://openreview.net/forum?id=zET0Zg71WT",
        "pdf_link": "https://openreview.net/pdf?id=zET0Zg71WT",
        "keywords": "transformers, attention, vector symbolic architectures, neurosymbolic ai, hyperdimensional computing",
        "abstract": "The introduction of the Transformer has brought about a revolution in AI. Central to the success of the Transformer architecture is the self-attention mechanism, enabling context dependence and long-range dependencies between tokens. Recent work has drawn an equivalence between Hopfield networks, a kind of associative memory model, and Transformers. In this work, we leverage this bridge, using Vector Symbolic Architectures (VSA), a brain-inspired computational paradigm capable of representing and implementing data structures, including associative memory models, to define a broad class of attention mechanisms catered for complex data types. In particular, we use Generalized Holographic Reduced Representations (GHRR), an implementation of a VSA, as the foundation for our proposed class of attention mechanisms. We show that GHRR is capable of implementing attention and design a GHRR Transformer encoder architecture based on the demonstrated mathematical equivalence. We propose a new kind of binding-based positional encoding based on methods used in VSAs for encoding sequential information. We extend the attention mechanism in our architecture to support graphs, inspired by techniques used in VSAs to encode graph representations. We evaluate the GHRR Transformer on language modeling, vertex classification, and graph classification tasks. Results suggest that our approach provides benefits in language modeling and graph classification tasks compared to baseline models."
    },
    {
        "title": "Performance Heterogeneity in Message-Passing and Transformer-based Graph Neural Networks",
        "link_suffix": "/forum?id=5B6eSE6l4M",
        "link": "https://openreview.net/forum?id=5B6eSE6l4M",
        "pdf_link": "https://openreview.net/pdf?id=5B6eSE6l4M",
        "keywords": "Graph Neural Networks, Transformers, Rewiring, Example Hardness, Generalization",
        "abstract": "Graph Neural Networks have emerged as the most popular architecture for graph-level learning, including graph classification and regression tasks, which frequently arise in areas such as biochemistry and drug discovery. Achieving good performance in practice requires careful model design. Due to gaps in our understanding of the relationship between model and data characteristics, this often requires manual architecture and hyperparameter tuning. This is particularly pronounced in graph-level tasks, due to much higher variation in the input data than in node-level tasks. To work towards closing these gaps, we begin with a systematic analysis of individual performance in graph-level tasks. Our results establish significant performance heterogeneity in both message-passing and transformer-based architectures. We then investigate the interplay of model and data characteristics as drivers of the observed heterogeneity. Our results suggest that graph topology alone cannot explain heterogeneity. Using the Tree Mover’s Distance, which jointly evaluates topological and feature information, we establish a link between class-distance ratios and performance heterogeneity in graph classification. These insights motivate model and data preprocessing choices that account for heterogeneity between graphs. We propose a selective rewiring approach, which only targets graphs whose individual performance benefits from rewiring. We further show that the optimal network depth depends on the graph’s spectrum, which motivates a heuristic for choosing the number of GNN layers. Our experiments  demonstrate the utility of both design choices in practice."
    },
    {
        "title": "Voronoi Tessellation-based Confidence Decision Boundary Visualization to Enhance Understanding of Active Learning",
        "link_suffix": "/forum?id=13G5KXm98a",
        "link": "https://openreview.net/forum?id=13G5KXm98a",
        "pdf_link": "https://openreview.net/pdf?id=13G5KXm98a",
        "keywords": "Decision Boundary, Visualization, Active Learning",
        "abstract": "The current visualizations used in active learning are quite basic, making it difficult for researchers to effectively observe and analyze the practical performance of different sampling strategies. To address this issue, we introduce a more informative visual evaluation approach observation metric, the confidence decision boundary, which is generated through Voronoi tessellation and evaluated using ridge confidence, a newly proposed measure. This approach enhances the information content in boundary regions where data distribution is sparse. Based on the confidence decision boundary, we conducted a series of visualizations to evaluate various active learning query strategies. These visualizations are able to capture nuanced variations regarding how models based on different strategies perform sampling, the characteristics of points selected by various methods, and the impact of newly sampled points on the model. This enables a much deeper understanding of the underlying mechanisms of existing query strategies."
    },
    {
        "title": "SEPAL: Scalable Feature Learning on Huge Knowledge Graphs",
        "link_suffix": "/forum?id=oVwTvCI9Us",
        "link": "https://openreview.net/forum?id=oVwTvCI9Us",
        "pdf_link": "https://openreview.net/pdf?id=oVwTvCI9Us",
        "keywords": "Knowledge graph, Scalable, Feature learning",
        "abstract": "Knowledge graphs accumulate information about more and more entities of the world. Much research is conducted to improve embedding models that capture this information and give useful node features in many downstream applications. However, most current methods are hard to scale to large knowledge graphs, partly because GPU memory is too small to hold the embeddings of many entities --YAGO4 has 67M entities. To scale existing embedding models on modest hardware, we introduce SEPAL: Scalable Embedding Propagation Algorithm for Large knowledge graphs.\nThe key idea of SEPAL to reduce compute is to only optimize embeddings on a core subset of entities, those that come with much more information than others. Then SEPAL propagates these embeddings to the rest of the graph with message passing, but no explicit optimization.\nTo enable efficient message passing, we break down large graphs into well-connected subgraphs that fit in GPU memory using a new algorithm called BLOCS: Balanced Local Overlapping Connected Subgraphs.\nWe evaluate SEPAL on five different knowledge graphs for four downstream regression tasks. We show that SEPAL outperforms alternative on downstream tasks, while providing a $43\\times$ speedup to its base embedding algorithm.\nMoreover, outside the core subgraph, embeddings obtained by message passing are not degraded compared to traditional methods, demonstrating the validity of SEPAL's propagation."
    },
    {
        "title": "Universal Black-Box Reward Poisoning Attack against Offline Reinforcement Learning",
        "link_suffix": "/forum?id=TCpJXzMnnp",
        "link": "https://openreview.net/forum?id=TCpJXzMnnp",
        "pdf_link": "https://openreview.net/pdf?id=TCpJXzMnnp",
        "keywords": "Offline Reinforcement Learning, Reward Poisoning Attack, Universal Black Box Attack",
        "abstract": "We study the problem of universal black-boxed reward poisoning attacks against general offline reinforcement learning with deep neural networks. We consider a black-box threat model where the attacker is entirely oblivious to the learning algorithm, and its budget is limited by constraining the amount of corruption at each data point and the total perturbation. We require the attack to be universally efficient against any efficient algorithms that might be used by the agent. We propose an attack strategy called the `policy contrast attack.' The idea is to find low- and high-performing policies covered by the dataset and make them appear to be high- and low-performing to the agent, respectively. To the best of our knowledge, we propose the first universal black-box reward poisoning attack in the general offline RL setting. We provide theoretical insights on the attack design and empirically show that our attack is efficient against current state-of-the-art offline RL algorithms in different learning datasets."
    },
    {
        "title": "Tropical Expressivity of Neural Networks",
        "link_suffix": "/forum?id=aYx7JR20sI",
        "link": "https://openreview.net/forum?id=aYx7JR20sI",
        "pdf_link": "https://openreview.net/pdf?id=aYx7JR20sI",
        "keywords": "fundamental domain, linear regions, sampling, tropical geometry, symbolic computing",
        "abstract": "We propose an algebraic geometric framework to study the expressivity of linear activation neural networks.  A particular quantity of neural networks that has been actively studied is the number of linear regions, which gives a quantification of the information capacity of the architecture.  To study and evaluate information capacity and expressivity, we work in the setting of tropical geometry---a combinatorial and polyhedral variant of algebraic geometry---where there are known connections between tropical rational maps and feedforward neural networks. Our work builds on and expands this connection to capitalize on the rich theory of tropical geometry to characterize and study various architectural aspects of neural networks. Our contributions are threefold: we provide a novel tropical geometric approach to selecting sampling domains among linear regions; an algebraic result allowing for a guided restriction of the sampling domain for network architectures with symmetries; and a new open source OSCAR library to analyze neural networks symbolically using their tropical representations, where we present a new algorithm that computes the exact number of their linear regions. We provide a comprehensive set of proof-of-concept numerical experiments demonstrating the breadth of neural network architectures to which tropical geometric theory can be applied to reveal insights on expressivity characteristics of a network.  Our work provides the foundations for the adaptation of both theory and existing software from computational tropical geometry and symbolic computation to neural networks and deep learning."
    },
    {
        "title": "RLHF with Inconsistent Multi-Agent Feedback Under General Function Approximation: A Theoretical Perspective",
        "link_suffix": "/forum?id=GqGoa44obw",
        "link": "https://openreview.net/forum?id=GqGoa44obw",
        "pdf_link": "https://openreview.net/pdf?id=GqGoa44obw",
        "keywords": "RLHF theory, inconsistent multi-agent feedback, regret analysis",
        "abstract": "Reinforcement learning from human feedback (RLHF) has been widely studied, as a method for leveraging feedback from human evaluators to guide the learning process. However, existing theoretical analyses typically assume that the human feedback is generated by the ground-truth reward function. This may not be true in practice, because the reward functions in human minds for providing feedback are usually different from the ground-truth reward function, e.g., due to diverse personal experiences and inherent biases. Such inconsistencies could lead to undesirable outcomes when applying existing algorithms, particularly when considering feedback from heterogeneous agents. Therefore, in this paper, we make the first effort to investigate a more practical and general setting of RLHF, where feedback could be generated by multiple agents with reward functions differing from the ground truth. To address this challenge, we develop a new algorithm with novel ideas for handling inconsistent multi-agent feedback, including a Steiner-Point-based confidence set to exploit the benefits ofmulti-agentfeedback and a new weighted importance sampling method to manage complexity issues arising frominconsistency. Our theoretical analysis develops new methods to demonstrate the optimality of our algorithm. This result is the first of its kind to demonstrate the fundamental impact and potential of inconsistent multi-agent feedback in RLHF."
    },
    {
        "title": "Theoretical Analyses of Hyperparameter Selection in Graph-Based Semi-Supervised Learning",
        "link_suffix": "/forum?id=gqC0egRfWq",
        "link": "https://openreview.net/forum?id=gqC0egRfWq",
        "pdf_link": "https://openreview.net/pdf?id=gqC0egRfWq",
        "keywords": "graph-based semi-supervised learning, hyperparameter selection, sample complexity, pseudo-dimension, Rademacher complexity",
        "abstract": "Graph-based semi-supervised learning (SSL) is a powerful paradigm in machine learning for modeling and exploiting the underlying graph structure that captures the relationship between labeled and unlabeled data. A large number of classical as well as modern deep learning based algorithms have been proposed for this problem, often having tunable hyperparameters. We initiate a formal study of hyperparameter tuning from parameterized algorithm families for this problem. We obtain novel $O(\\log n)$ pseudo-dimension upper bounds for hyperparameter selection in three classical label propagation based algorithm families, where $n$ is the number of nodes, implying bounds on the amount of data needed for learning provably good parameters. We further provide matching $\\Omega(\\log n)$ pseudo-dimension lower bounds, thus asymptotically characterizing the learning-theoretic complexity of the parameter tuning problem. We extend our study to hyperparameter selection in modern graph neural networks. We bound the Rademacher complexity for tuning the self-loop weighting in recently proposed Simplified Graph Convolution (SGC) networks. We further propose a novel tunable architecture that interpolates graph convolutional neural networks (GCN) and graph attention networks (GAT) in every layer, and provide Rademacher complexity bounds for tuning the interpolation coefficient."
    },
    {
        "title": "Code-of-thought prompting: Probing AI Safety with Code",
        "link_suffix": "/forum?id=lUyYX9VFgA",
        "link": "https://openreview.net/forum?id=lUyYX9VFgA",
        "pdf_link": "https://openreview.net/pdf?id=lUyYX9VFgA",
        "keywords": "Toxicity analysis, AI Safety, LLMs",
        "abstract": "Large Language Models (LLMs) have rapidly advanced in multiple capabilities, such as text and code understanding, leading to their widespread use in a wide range of applications,\nsuch as healthcare, education, and\nsearch. Due to the critical nature of these applications, there has been a heightened emphasis on aligning these models to human values and preferences to improve safety and reliability. In this paper, we demonstrate that contemporary efforts fall severely short of the ultimate goal of AI safety and fail to ensure safe, non-toxic outputs. We systematically evaluate the safety of LLMs through a novel model interaction paradigm dubbed Code of Thought (CoDoT) prompting that transforms natural language (NL)\nprompts into pseudo-code.\nCoDoT represents NL inputs in a precise,\nstructured, and concise form, allowing us to utilize its programmatic interface to test several facets of AI safety. Under the CoDoT prompting paradigm, we show that a wide range of large language models emit highly toxic outputs with the potential to cause great harm. CoDoT leads to a staggering 16.5× increase in toxicity on GPT-4 TURBO and a massive 4.6 x increase on average, across multiple models and languages. Notably, we find that state-of-the-art mixture-of-experts (MoE) models are approximately 3x more susceptible to toxicity than standard architectures. Our findings raise a troubling concern that recent safety and alignment efforts have regressed LLMs and inadvertently introduced safety backdoors and blind spots. Our work calls for an urgent need to rigorously evaluate the design choices of safety efforts from first principles, given the rapid adoption of LLMs."
    },
    {
        "title": "Fitting Networks with a Cancellation Trick",
        "link_suffix": "/forum?id=C06kww3Qky",
        "link": "https://openreview.net/forum?id=C06kww3Qky",
        "pdf_link": "https://openreview.net/pdf?id=C06kww3Qky",
        "keywords": "Network analysis, DCBM, logit-DCBM, community detection, SCORE",
        "abstract": "The degree-corrected block model (DCBM), latent space model (LSM), and $\\beta$-model are all popular network models. We combine their modeling ideas and propose the logit-DCBM as a new model. Similar as the $\\beta$-model and LSM, the logit-DCBM contains nonlinear factors, where fitting the parameters is a challenging open problem. We resolve this problem by introducing a cancellation trick. We also propose R-SCORE as a recursive community detection algorithm, where in each iteration, we first use the idea above to update our parameter estimation, and then use the results to remove the nonlinear factors in the logit-DCBM so the renormalized model approximately satisfies a low-rank model, just like the DCBM. Our numerical study suggests that R-SCORE significantly improves over existing spectral approaches in many cases. Also, theoretically, we show that  the Hamming error rate of R-SCORE is faster than that of SCORE in a specific sparse region, and is at least as fast outside this region."
    },
    {
        "title": "Do LLM Agents  Have Regret? A Case Study in Online Learning and Games",
        "link_suffix": "/forum?id=qn9tBYQHGi",
        "link": "https://openreview.net/forum?id=qn9tBYQHGi",
        "pdf_link": "https://openreview.net/pdf?id=qn9tBYQHGi",
        "keywords": "LLM agents, online learning, repeated games",
        "abstract": "Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of regret. We first empirically study the no-regret behaviors of LLMs in canonical non-stochastic online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behaviors of LLM agents, under certain assumptions on the supervised pre-training and the rationality model of human decision-makers who generate the data. Notably, we also identify (simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To further promote the no-regret behaviors, we propose a novel unsupervised training loss of regret-loss, which, in contrast to the supervised pre-training loss, does not require the labels of (optimal) actions. Finally, we establish the statistical guarantee of generalization bound for regret-loss minimization, and more importantly, the optimization guarantee that minimizing such a loss may automatically lead to known no-regret learning algorithms, when single-layer self-attention models are used. Our further experiments demonstrate the effectiveness of our regret-loss, especially in addressing the above “regrettable” cases."
    },
    {
        "title": "Conflict-Aware Adversarial Training",
        "link_suffix": "/forum?id=HaXlWs1LX8",
        "link": "https://openreview.net/forum?id=HaXlWs1LX8",
        "pdf_link": "https://openreview.net/pdf?id=HaXlWs1LX8",
        "keywords": "Adversarial Training, Robustness",
        "abstract": "Adversarial training is the most effective method to obtain adversarial robustness for deep neural networks by directly involving adversarial samples in the training procedure. To obtain an accurate and robust model, the weighted-average method is applied to optimize standard loss and adversarial loss simultaneously. In this paper, we argue that the weighted-average method does not provide the best tradeoff for the standard performance and adversarial robustness. We argue that the failure of the weighted-average method is due to the conflict between the gradients derived from standard and adversarial loss, and further demonstrate such a conflict increases with attack budget theoretically and practically. To alleviate this problem, we propose a new trade-off paradigm for adversarial training with a conflict-aware factor for the convex combination of standard and adversarial loss, named \\textbf{Conflict-Aware Adversarial Training~(CA-AT)}. Comprehensive experimental results show that CA-AT consistently offers a superior trade-off between standard performance and adversarial robustness under the settings of adversarial training from scratch and parameter-efficient finetuning."
    },
    {
        "title": "Seq-VCR: Preventing  Collapse in Intermediate Transformer Representations for Enhanced Reasoning",
        "link_suffix": "/forum?id=30oIfmrcFO",
        "link": "https://openreview.net/forum?id=30oIfmrcFO",
        "pdf_link": "https://openreview.net/pdf?id=30oIfmrcFO",
        "keywords": "LLMs, Representation Learning, Reasoning",
        "abstract": "Decoder-only Transformers often struggle with complex reasoning tasks, particularly arithmetic reasoning requiring multiple sequential operations. In this work, we identify representation collapse in the model’s intermediate layers as a key factor limiting their reasoning capabilities. To address this, we propose Sequential Variance-Covariance Regularization (Seq-VCR), which enhances the entropy of intermediate representations and prevents collapse. Combined with dummy pause tokens as substitutes for chain-of-thought (CoT) tokens, our method significantly improves performance in arithmetic reasoning problems. In the challenging 5 × 5 integer multiplication task, our approach achieves 99.5% exact match accuracy, outperforming models of the same size (which yield 0% accuracy) and GPT-4 with five-shot CoT prompting (44%). We also demonstrate superior results on arithmetic expression and longest increasing subsequence (LIS) datasets. Our findings highlight the importance of preventing intermediate layer representation collapse to enhance the reasoning capabilities of Transformers and show that Seq-VCR offers an effective solution without requiring explicit CoT supervision."
    },
    {
        "title": "Underdamped Diffusion Bridges with Applications to Sampling",
        "link_suffix": "/forum?id=Q1QTxFm0Is",
        "link": "https://openreview.net/forum?id=Q1QTxFm0Is",
        "pdf_link": "https://openreview.net/pdf?id=Q1QTxFm0Is",
        "keywords": "Variational Inference, Sampling, Diffusion Models",
        "abstract": "We provide a general framework for learning diffusion bridges that transport prior to target distributions. It includes existing diffusion models for generative modeling, but also underdamped versions with degenerate diffusion matrices, where the noise only acts in certain dimensions. Extending previous findings, our framework allows to rigorously show that score-matching in the underdamped case is indeed equivalent to maximizing a lower bound on the likelihood. Motivated by superior convergence properties and compatibility with sophisticated numerical integration schemes of underdamped stochastic processes, we proposeunderdamped diffusion bridges, where a general density evolution is learned rather than prescribed by a fixed noising process. We apply our method to the challenging task of sampling from unnormalized densities without access to samples from the target distribution. Across a diverse range of sampling problems, our approach demonstrates state-of-the-art performance, notably outperforming alternative methods, while requiring significantly fewer discretization steps and almost no hyperparameter tuning."
    },
    {
        "title": "CodeBenchGen: Creating Scalable Execution-based Code Generation Benchmarks",
        "link_suffix": "/forum?id=XXVRkPB1tg",
        "link": "https://openreview.net/forum?id=XXVRkPB1tg",
        "pdf_link": "https://openreview.net/pdf?id=XXVRkPB1tg",
        "keywords": "code generation, code generation benchmark, evaluation for code generation, synthetic data generation",
        "abstract": "To adequately test modern code generation systems, evaluation benchmarks must execute and test the code generated by the system. However, these execution and testing requirements have largely limited benchmarks to settings where code is easily executable or has human-written tests. To facilitate evaluation of code generation systems across diverse scenarios, we present CodeBenchGen, a framework to create scalable execution-based benchmarks from naturally occurring code sources. Specifically, we leverage a large language model (LLM) to sandbox arbitrary pieces of code into evaluation examples, including test cases for execution-based evaluation. We illustrate the usefulness of our framework by creating a dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries converted from code in 367 GitHub repositories taken from the Code- SearchNet dataset. To demonstrate the solvability of examples in Exec-CSN, we present a human study demonstrating that 81.3% of the examples can be solved by humans and 61% are rated as “requires effort to solve”. We conduct code generation experiments on open-source and proprietary models and analyze the performance of both humans and models. We provide code and data at:https://github.com/CodeBenchGen/CodeBenchGen."
    },
    {
        "title": "A Scalable Communication Protocol for Networks of Large Language Models",
        "link_suffix": "/forum?id=Q47jVPzJ3G",
        "link": "https://openreview.net/forum?id=Q47jVPzJ3G",
        "pdf_link": "https://openreview.net/pdf?id=Q47jVPzJ3G",
        "keywords": "llm, multi-agent systems, protocol, negotiation, collaboration",
        "abstract": "Communication is a prerequisite for collaboration.\nWhen scaling networks of AI-powered agents, communication must be versatile, efficient, and portable.\nThese requisites, which we refer to as the Agent Communication Trilemma, are hard to achieve in large networks of agents.\nWe introduce Agora, a meta protocol that leverages existing communication standards to make LLM-powered agents solve complex problems efficiently.\nIn Agora, agents typically use standardised routines for frequent communications, natural language for rare communications, and LLM-written routines for everything in between. \nAgora sidesteps the Agent Communication Trilemma and robustly handles changes in interfaces and members, allowing unprecedented scalability with full decentralisation and minimal involvement of human beings. \nOn large Agora networks, we observe the emergence of self-organising, fully automated protocols that achieve complex goals without human intervention."
    }
]