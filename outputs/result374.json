[
    {
        "title": "Going Beyond Feature Similarity: Effective Dataset distillation based on Class-aware Conditional Mutual Information",
        "link_suffix": "/forum?id=0no1Wp2R2j",
        "link": "https://openreview.net/forum?id=0no1Wp2R2j",
        "pdf_link": "https://openreview.net/pdf?id=0no1Wp2R2j",
        "keywords": "dataset distillation, conditional mutual information",
        "abstract": "Dataset distillation (DD) aims to minimize the time and memory consumption needed for training deep neural networks on large datasets, by creating a smaller synthetic dataset that has similar performance to that of the full real dataset. However, current dataset distillation methods often result in synthetic datasets that are excessively difficult for networks to learn from, due to the compression of a substantial amount of information from the original data through metrics measuring feature similarity, e,g., distribution matching (DM). In this work, we introduce conditional mutual information (CMI) to assess the class-aware complexity of a dataset and propose a novel method by minimizing CMI. Specifically, we minimize the distillation loss while constraining the class-aware complexity of the synthetic dataset by minimizing its empirical CMI from the feature space of pre-trained networks, simultaneously. Conducting on a thorough set of experiments, we show that our method can serve as a general regularization method to existing DD methods and improve the performance and training efficiency."
    },
    {
        "title": "Learning Structure-Dynamics-aware Representations for Efficient and Robust 3D Pose Estimation",
        "link_suffix": "/forum?id=XHvdM04T0l",
        "link": "https://openreview.net/forum?id=XHvdM04T0l",
        "pdf_link": "https://openreview.net/pdf?id=XHvdM04T0l",
        "keywords": "3D pose estimation, Part-based Adaptive GNN, Frameset-based Skipped Transformer, efficient and robust",
        "abstract": "Recent works in 2D-to-3D pose uplifting for monocular 3D Human Pose Estimation (HPE) have shown significant progress. However, two key challenges persist in real-world applications: vulnerability to joint noise and high computational costs. These issues arise from the dense joint-frame connections and iterative correlations typically employed by mainstream GNN-based and Transformer-based methods. To address these challenges, we propose a novel approach that leverages human physical structure and long-range dynamics to learn spatial part- and temporal frameset-based representations. This method is inherently robust to missing or erroneous joints while also reducing model parameters. Specifically, in the Spatial Encoding stage, coarse-grained body parts are used to construct structural correlations with a fully adaptive graph topology. This spatial correlation representation is integrated with muti-granularity pose attributes to generate a comprehensive pose representation for each frame. In Temporal Encoding and Decoding stages, Skipped Self-Attention is performed in framesets to establish long-term temporal dependencies from multiple perspectives of movement. On this basis, a compact Graph and Skipped Transformer (G-SFormer) is proposed, which realises efficient and robust 3D HEP in both experimental and practical scenarios. Extensive experiments on Human3.6M, MPI-INF-3DHP and Human-Eva benchmarks demonstrate that G-SFormer series models can compete and outperform the state-of-the-arts but takes only a fraction of parameters and around 1% computational cost. It also exhibits outstanding robustness to inaccurately detected 2D poses. The source code will be available athttps://sites.google.com/view/g-sformer."
    },
    {
        "title": "Uncertainty-aware Reward Model: Teaching Reward Models to Know What is Unknown",
        "link_suffix": "/forum?id=bPO4jLOTGG",
        "link": "https://openreview.net/forum?id=bPO4jLOTGG",
        "pdf_link": "https://openreview.net/pdf?id=bPO4jLOTGG",
        "keywords": "large language models, reward models, ensemble, uncertainty-aware",
        "abstract": "Reward models (RM) play a critical role in aligning generations of large language models (LLM) to human expectations. However, prevailing RMs fail to capture the stochasticity within human preferences and cannot effectively evaluate the reliability of reward predictions. To address these issues, we propose Uncertain-aware RM (URM) and Uncertain-aware RM Ensemble (URME) to incorporate and manage uncertainty in reward modeling. URM can model the distribution of disentangled attributes within human preferences, while URME quantifies uncertainty through discrepancies in the ensemble, thereby identifying potential lack of knowledge during reward evaluation. Experiment results indicate that the proposed URM achieves state-of-the-art performance compared to models with the same size, demonstrating the effectiveness of modeling uncertainty within human preferences. Furthermore, empirical results show that through uncertainty quantification, URM and URME can identify unreliable predictions to improve the quality of reward evaluations."
    },
    {
        "title": "Towards Natural Image Matting in the Wild via Real-Scenario Prior",
        "link_suffix": "/forum?id=tSmkYZ8vU7",
        "link": "https://openreview.net/forum?id=tSmkYZ8vU7",
        "pdf_link": "https://openreview.net/pdf?id=tSmkYZ8vU7",
        "keywords": "Image Matting, Interactive Matting",
        "abstract": "Recent approaches attempt to adapt powerful interactive segmentation models, such as SAM, to interactive matting and fine-tune the models based on synthetic matting datasets. However, models trained on synthetic data fail to generalize to complex and occlusion scenes. We address this challenge by proposing a new matting dataset based on the COCO dataset, namely COCO-Matting. Specifically, the construction of our COCO-Matting includes accessory fusion and mask-to-matte, which selects real-world complex images from COCO and converts semantic segmentation masks to matting labels. The built COCO-Matting comprises an extensive collection of 38,251 human instance-level alpha mattes in complex natural scenarios. Furthermore, existing SAM-based matting methods extract intermediate features and masks from a frozen SAM and only train a lightweight matting decoder by end-to-end matting losses, which do not fully exploit the potential of the pre-trained SAM. Thus, we propose SEMat which revamps the network architecture and training objectives. For network architecture, the proposed feature-aligned transformer learns to extract fine-grained edge and transparency features. The proposed matte-aligned decoder aims to segment matting-specific objects and convert coarse masks into high-precision mattes. For training objectives, the proposed regularization and trimap loss aim to retain the prior from the pre-trained model and push the matting logits extracted from the mask decoder to contain trimap-based semantic information. Extensive experiments across seven diverse datasets demonstrate the superior performance of our method, proving its efficacy in interactive natural image matting. Code is available in the supplementary file."
    },
    {
        "title": "Prompt Diffusion Robustifies Any-Modality Prompt Learning",
        "link_suffix": "/forum?id=2x1U8a3s7G",
        "link": "https://openreview.net/forum?id=2x1U8a3s7G",
        "pdf_link": "https://openreview.net/pdf?id=2x1U8a3s7G",
        "keywords": "Prompt learning, Diffusion model, Vision-language models",
        "abstract": "Foundation models enable prompt-based classifiers for zero-shot and few-shot learning. Nonetheless, the conventional method of employing fixed prompts suffers from distributional shifts that negatively impact generalizability to unseen samples. This paper introduces prompt diffusion, which uses a diffusion model to gradually refine prompts to obtain a customized prompt for each sample. \nSpecifically, we first optimize a collection of prompts to obtain over-fitted prompts per sample. Then, we propose a prompt diffusion model within the prompt space, enabling the training of a generative transition process from a random prompt to its overfitted prompt. As we cannot access the label of a test image during inference, our model gradually generates customized prompts solely from random prompts using our trained, prompt diffusion. Our prompt diffusion is generic, \ufb02exible, and modality-agnostic, making it a simple plug-and-play module seamlessly embedded into existing prompt learning methods for textual, visual, or multi-modal prompt learning.\nOur diffusion model uses a fast ODE-based sampling strategy to optimize test sample prompts in just five steps, offering a good trade-off between performance improvement and computational efficiency.\nFor all prompt learning methods tested, adding prompt diffusion yields more robust results for base-to-new generalization, cross-dataset generalization, and domain generalization in classification tasks tested over 15 diverse datasets."
    },
    {
        "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging",
        "link_suffix": "/forum?id=dwQIVcW1du",
        "link": "https://openreview.net/forum?id=dwQIVcW1du",
        "pdf_link": "https://openreview.net/pdf?id=dwQIVcW1du",
        "keywords": "program synthesis, code generation, large language models, machine learning for code, self-debugging",
        "abstract": "While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness."
    },
    {
        "title": "YOLO-RD: Introducing Relevant and Compact Explicit Knowledge to YOLO by Retriever-Dictionary",
        "link_suffix": "/forum?id=KXDOmD7DM7",
        "link": "https://openreview.net/forum?id=KXDOmD7DM7",
        "pdf_link": "https://openreview.net/pdf?id=KXDOmD7DM7",
        "keywords": "Real-time Object Detection, Object Detection, Dictionary Learning, Retrieval-Augmented Generation, Representation Learning",
        "abstract": "Identifying and localizing objects within images is a fundamental challenge, and numerous efforts have been made to enhance model accuracy by experimenting with diverse architectures and refining training strategies. Nevertheless, a prevalent limitation in existing models is overemphasizing the current input while ignoring the information from the entire dataset. We introduce an innovative $\\textbf{R}etriever-\\textbf{D}ictionary$ (RD) module to address this issue. This architecture enables YOLO-based models to efficiently retrieve features from a Dictionary that contains the insight of the dataset, which is built by the knowledge from  Visual Models (VM), Large Language Models (LLM), or Visual Language Models (VLM). The flexible RD enables the model to incorporate such explicit knowledge that enhances the ability to benefit multiple tasks, specifically, segmentation, detection, and classification, from pixel to image level. The experiments show that using the RD significantly improves model performance, achieving more than a 3% increase in mean Average Precision for object detection with less than a 1% increase in model parameters. Beyond 1-stage object detection models, the RD module improves the effectiveness of 2-stage models and DETR-based architectures, such as Faster R-CNN and Deformable DETR."
    },
    {
        "title": "Vividportraits: Face Parsing Guided Portrait Animation",
        "link_suffix": "/forum?id=Tp9c7s3ZmN",
        "link": "https://openreview.net/forum?id=Tp9c7s3ZmN",
        "pdf_link": "https://openreview.net/pdf?id=Tp9c7s3ZmN",
        "keywords": "face parsing; portrait animation; diffusion model",
        "abstract": "Portrait animation aims to transfer the facial expressions and movements of a target character onto a reference character. This task presents two main challenges: accurately transferring motion and expressions while fully preserving the identity features of the reference portrait. We introduce Vividportraits, a diffusion-based model designed to effectively meet these objectives. In contrast to existing methods that rely on sparse representations such as facial landmarks, our approach leverages facial parsing maps for motion guidance, enabling a more precise conveyance of subtle expressions. A random scaling technique is applied during training to prevent the model from internalizing identity-specific features from the driving images. Furthermore, we perform foreground-background segmentation on the reference portrait to reduce data redundancy. The long-video generation process is refined to improve consistency across sequences. Our model, exclusively trained on public datasets, demonstrates superior performance relative to current state-of-the-art methods, achieving a notable 8% improvement in expression metric. More visual results are available on the anonymous websitehttps://www.vividportraits.cn."
    },
    {
        "title": "Enhancing Performance of Multilayer Perceptrons by Knot-Gathering Initialization",
        "link_suffix": "/forum?id=Tnd3dZxyEv",
        "link": "https://openreview.net/forum?id=Tnd3dZxyEv",
        "pdf_link": "https://openreview.net/pdf?id=Tnd3dZxyEv",
        "keywords": "Multilayer perceptron, expressiveness, linear region, knot density, initialization",
        "abstract": "Multilayer perceptrons (MLPs) with ReLU-like activation functions form a high-dimensional, piecewise linear function space, characterized by \"knots\"\u2014points of non-differentiability. The density of such knots within a given input domain measures the MLP's capacity for function approximation. Despite the simplicity of this concept, knots remain underexploited to enhance the practical performance of MLPs. This paper introduces Knot Gathering Initialization (KGI), a novel method that amplifies the local expressiveness of MLPs by increasing the knot density within the input domain prior to training. As an initialization technique, KGI is lightweight, data-independent, and hyperparameter-insensitive. The concept of knots, and hence KGI, can be directly generalized to smooth activation functions from different angles, including geometry, information transmission, and spectral analysis. We demonstrate the effectiveness of KGI across diverse tasks, including curve and surface fitting, image classification, time series regression, physics-informed operator learning, representation disentanglement, and large language model pretraining. These experiments unexceptionally show that KGI improves both accuracy and convergence speed of MLPs, whether used standalone or as components of larger architectures. Promising future directions include: 1) the natural extension of KGI to convolutional and graph convolutional layers, as well as Low-Rank Adaptation (LoRA) for finetuning; and 2) applying knot gathering throughout training, rather than just at initialization."
    },
    {
        "title": "Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning",
        "link_suffix": "/forum?id=25kAzqzTrz",
        "link": "https://openreview.net/forum?id=25kAzqzTrz",
        "pdf_link": "https://openreview.net/pdf?id=25kAzqzTrz",
        "keywords": "deep semi-supervised learning, generalization error, feature learning",
        "abstract": "Semi-supervised learning (SSL), exemplified by FixMatch (Sohn et al., 2020), has shown significant generalization advantages over supervised learning (SL), particularly in the context of deep neural networks (DNNs). However, it is still unclear, from a theoretical standpoint, why FixMatch-like SSL algorithms generalize  better than SL on DNNs. In this work, we present the first theoretical justification for the enhanced test accuracy observed in  FixMatch-like SSL applied to DNNs by taking  convolutional neural networks (CNNs) on classification tasks as an example. Our theoretical analysis reveals that the semantic feature learning processes in FixMatch and SL are rather different. In particular, FixMatch learns all the discriminative features of each semantic class, while SL only randomly captures a subset of features due to the well-known lottery ticket hypothesis. Furthermore, we show that our analysis framework can be applied to other FixMatch-like SSL methods, e.g., FlexMatch, FreeMatch, Dash, and SoftMatch. Inspired by our theoretical analysis, we develop an improved variant of FixMatch, termed Semantic-Aware FixMatch (SA-FixMatch). Experimental results corroborate our theoretical findings and the enhanced generalization capability of SA-FixMatch."
    },
    {
        "title": "The Role of Deductive and Inductive Reasoning in Large Language Models",
        "link_suffix": "/forum?id=kcAejITM7C",
        "link": "https://openreview.net/forum?id=kcAejITM7C",
        "pdf_link": "https://openreview.net/pdf?id=kcAejITM7C",
        "keywords": "Large Language Models; Prompt Methods",
        "abstract": "Large Language Models (LLMs) have achieved substantial progress in artificial intelligence, particularly in reasoning tasks. However, their reliance on static prompt structures, coupled with limited dynamic reasoning capabilities, often constrains their adaptability to complex and evolving problem spaces. In this paper, we propose the Deductive and InDuctive (DID) method, which enhances LLM reasoning by dynamically integrating both deductive and inductive reasoning within the prompt construction process. Drawing inspiration from cognitive science, the DID approach mirrors human adaptive reasoning mechanisms, offering a flexible framework that allows the model to adjust its reasoning pathways based on task context and performance. We empirically validate the efficacy of DID on established datasets such as AIW and MR-GSM8K, as well as on our custom dataset, Holiday Puzzle, which presents tasks about different holiday date calculating challenges. By leveraging DID\u2019s hybrid prompt strategy, we demonstrate significant improvements in both solution accuracy and reasoning quality, achieved without imposing substantial computational overhead. Our findings suggest that DID provides a more robust and cognitively aligned framework for reasoning in LLMs, contributing to the development of advanced LLM-driven problem-solving strategies informed by cognitive science models."
    },
    {
        "title": "Unleashing the Information Flow: Graph Neural Networks are Noisy Communication Channels",
        "link_suffix": "/forum?id=S3zKrEQpRr",
        "link": "https://openreview.net/forum?id=S3zKrEQpRr",
        "pdf_link": "https://openreview.net/pdf?id=S3zKrEQpRr",
        "keywords": "graph neural networks, entropy, channel capacity, model dimensionality estimation",
        "abstract": "Existing message-passing graph neural networks often rely on carefully designed information propagation methods to perform reasonably in graph-related mining tasks. However, this invokes the problem of whether the dimensions of learnable matrices and the depths of the networks are properly estimated. While this challenge has been attempted by others, it remains an open problem. Using the principle of maximum entropy and Shannon's theorem, we demonstrate that message-passing graph neural networks function similarly to noisy communication channels. The optimal information transmission state of graph neural networks can be reached when Shannon's theorem is satisfied, which is determined by their entropy and channel capacity. In addition, we illustrate that the widths of trainable matrices should be sufficiently large to avoid the shrinkage of model channel capacity and the increase of the channel capacity diminishes as the depth of the networks increases. The proposed approach is empirically verified through extensive experiments on five public semi-supervised node classification datasets."
    },
    {
        "title": "Implicit Bias in Matrix Factorization and its Explicit Realization in a new Architecture",
        "link_suffix": "/forum?id=ZTvUT49JjL",
        "link": "https://openreview.net/forum?id=ZTvUT49JjL",
        "pdf_link": "https://openreview.net/pdf?id=ZTvUT49JjL",
        "keywords": "Implicit Regularization, Matrix Factorization, Neural Networks",
        "abstract": "Gradient descent for matrix factorization is known to exhibit an implicit bias toward approximately low-rank solutions. While existing theories often assume the boundedness of iterates, empirically the bias persists even with unbounded sequences. We thus hypothesize that implicit bias is driven by divergent dynamics markedly different from the convergent dynamics for data fitting. Using this perspective, we introduce a new factorization model: $X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while $D$ is a diagonal factor allowing the model to span the entire search space. Our experiments reveal that this model exhibits a strong implicit bias regardless of initialization and step size, yielding truly (rather than approximately) low-rank solutions. Furthermore, drawing parallels between matrix factorization and neural networks, we propose a novel neural network model featuring constrained layers and diagonal components. This model achieves strong performance across various regression and classification tasks while finding low-rank solutions, resulting in efficient and lightweight networks."
    },
    {
        "title": "Active In-Context Learning: Enhancing the Generalization of Large Multimodal Models",
        "link_suffix": "/forum?id=OcXsdBo6vK",
        "link": "https://openreview.net/forum?id=OcXsdBo6vK",
        "pdf_link": "https://openreview.net/pdf?id=OcXsdBo6vK",
        "keywords": "Large Multimodal Models, In-context Learning, Active Learning",
        "abstract": "The performance of Large Multimodal Models (LMMs) on downstream tasks improves substantially when examples of visual-text relationships are incorporated as context, with performance gains increasing as the number of examples and the context window size grow. However, collecting high-quality training sets for In-Context Learning (ICL) to retrieve multimodal examples is not trivial, particularly in specialized domains like healthcare, remote sensing, finance, and scientific research, due to the significant costs of manual labeling and strict privacy regulations. In this paper, we introduce Active In-Context Learning (AICL), a novel paradigm that eliminates the need for traditional training sets in multimodal ICL. AICL dynamically selects and annotates a small, highly informative set of samples in real-time during the query phase of LMMs. This active set evolves throughout querying, with the most relevant examples being continuously retrieved from it to optimize LMM performance on new data, without relying on pre-existing training sets. To construct an optimal active set, we propose Spectral-based Representative Sampling, which applies spectral clustering in the early query phase to select samples that are early, class-balanced, and representative, ensuring the active set captures key features of the data distribution and reduces data bias. To fully leverage the active set, we propose Similarity-enhanced TopK Prompt Construction, which retrieves the most relevant multimodal examples using a TopK similarity strategy and integrates the visual similarities between the multimodal examples and the query samples directly into the text prompts. By incorporating this similarity information, LMMs can better grasp the relationships, leading to more accurate and context-aware predictions. Experimental results on 10 specialized datasets and four LMMs show that our method significantly enhances LMMs\u2019 generalization performance. For example, in medical diagnosis tasks, our method, using only 10 annotated samples in the active set, outperforms existing ICL methods that rely on 2,000 annotated training samples."
    },
    {
        "title": "Group Diffusion Transformers are Unsupervised Multitask Learners",
        "link_suffix": "/forum?id=uOxoje4Sa9",
        "link": "https://openreview.net/forum?id=uOxoje4Sa9",
        "pdf_link": "https://openreview.net/pdf?id=uOxoje4Sa9",
        "keywords": "diffusion transformers, unsupervised pretraining, group generation, task-agnostic",
        "abstract": "While large language models (LLMs) have revolutionized natural language processing with their task-agnostic capabilities, visual generation tasks such as image translation, style transfer, and character customization still rely heavily on supervised, task-specific datasets. In this work, we introduce \\textbf{Group Diffusion Transformers (GDTs)}, a novel framework that unifies diverse visual generation tasks by redefining them as a \\textbf{group generation} problem. In this approach, a set of related images is generated simultaneously, optionally conditioned on a subset of the group. GDTs build upon diffusion transformers with minimal architectural modifications by concatenating self-attention tokens across images. This allows the model to implicitly capture cross-image relationships (\\textit{e.g.}, identities, styles, layouts, surroundings, textures, and color schemes) through caption-based correlations. Our design enables scalable, unsupervised, and task-agnostic pretraining using extensive collections of image groups sourced from multimodal internet articles, image galleries, and video frames. We evaluate GDTs on a comprehensive benchmark featuring over 200 instructions across 30 distinct visual generation tasks, including picture book creation, font design, style transfer, sketching, colorization, drawing sequence generation, and character customization. Our models achieve competitive \\textbf{zero-shot} performance without any additional fine-tuning or gradient updates. Furthermore, ablation studies confirm the effectiveness of key components such as data scaling, group size, and model design. These results demonstrate the potential of GDTs as scalable, general-purpose visual generation systems. We will release the code and models to support further research."
    },
    {
        "title": "Representation learning for financial time series forecasting",
        "link_suffix": "/forum?id=qU1GtrDDst",
        "link": "https://openreview.net/forum?id=qU1GtrDDst",
        "pdf_link": "https://openreview.net/pdf?id=qU1GtrDDst",
        "keywords": "representation learning, contrastive predictive coding, cpc",
        "abstract": "The accurate forecasting of financial time series remains a significant challenge due to the stochastic nature of the underlying data. To improve prediction accuracy, feature engineering has become a vital aspect of forecasting financial assets. However, engineering features manually often requires domain expertise. We propose to utilise an automated feature generation architecture, Contrastive Predictive Coding (CPC), to generate embeddings as input to improve the performance of downstream financial time series forecasting models. To benchmark the effectiveness of our approach, we evaluate forecasting models on predicting the next day's log return on various foreign exchange markets with and without embeddings. Finally, we assess our CPC architecture by employing the same trained encoder on different currency pairs and calculating the Sharpe ratio of our strategies."
    },
    {
        "title": "Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models",
        "link_suffix": "/forum?id=0Lpz2o6NDE",
        "link": "https://openreview.net/forum?id=0Lpz2o6NDE",
        "pdf_link": "https://openreview.net/pdf?id=0Lpz2o6NDE",
        "keywords": "4D texture synthesis, consistent video generation, zero-shot",
        "abstract": "3D meshes are widely used in computer vision and graphics because of their efficiency in animation and minimal memory footprint. They are extensively employed in movies, games, AR, and VR, leading to the creation of a vast number of mesh sequences. However, creating temporally consistent and realistic textures for these mesh sequences remains labor-intensive for professional artists. On the other hand, video diffusion models have demonstrated remarkable capabilities in text-driven video generation, enabling users to create countless video clips based solely on their imagination. Despite their strengths, these models often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D mesh sequences. In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures. Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space. To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results. We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue. Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing. Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences."
    },
    {
        "title": "Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining",
        "link_suffix": "/forum?id=RXeP5ajK2S",
        "link": "https://openreview.net/forum?id=RXeP5ajK2S",
        "pdf_link": "https://openreview.net/pdf?id=RXeP5ajK2S",
        "keywords": "Autoregressive Image Generation, Multi-modality, LLM",
        "abstract": "We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general multimodal capabilities. Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT), transforming Lumina-mGPT into a foundation model that seamlessly achieves omnipotent task unification. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like text-to-image/multiview generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multi-turn visual question answering. Additionally, we conduct extensive analysis to illustrate the inference-time mechanisms of Lumina-mGPT and its emergent zero-shot capabilities. We release all code and checkpoints, hoping to facilitate the progress toward building artificial general intelligence."
    },
    {
        "title": "Mitigating Generative Privacy Risks of Diffusion Models via Mixed Self-Synthesized Data Fine-tuning",
        "link_suffix": "/forum?id=coE6XbziUR",
        "link": "https://openreview.net/forum?id=coE6XbziUR",
        "pdf_link": "https://openreview.net/pdf?id=coE6XbziUR",
        "keywords": "Diffusion Models, AI privacy, Membership Inference Attack, AI safety",
        "abstract": "Diffusion models (DMs) have demonstrated exceptional performance across various generative tasks, yet they also face significant security and privacy concerns, such as Membership Inference Attacks (MIAs), where adversaries attempt to determine whether specific images were part of the DM's training set. These threats present serious risks, particularly as pre-trained DMs are increasingly accessible online. To address these privacy concerns, we begin by investigating how fine-tuning DMs on a manipulated self-synthesized dataset affects their generative privacy risks, and have the following observations: (1) DMs fine-tuned solely on self-synthesized clean images are more vulnerable to privacy attacks (2) DMs fine-tuned on perturbed self-synthesized images become more robust against privacy attacks but exhibit degraded image generation quality.\nBased on the observations, we propose MixSyn, a simple and effective framework designed to mitigate privacy risks by fine-tuning DMs on a mixed self-synthesized dataset, which is a mixture of clean and perturbed synthetic images. Extensive experimental results demonstrate that our method significantly mitigates the generative privacy risks of DMs while preserving their original image generation quality."
    },
    {
        "title": "Understanding Scale Shift in Domain Generalization for Crowd Localization",
        "link_suffix": "/forum?id=4VfPLTqdrq",
        "link": "https://openreview.net/forum?id=4VfPLTqdrq",
        "pdf_link": "https://openreview.net/pdf?id=4VfPLTqdrq",
        "keywords": "Crowd Localization, Domain Generalization, Scale Shift",
        "abstract": "Crowd localization plays a crucial role in visual scene understanding towards predicting each pedestrian location in a crowd, thus being applicable to various downstream tasks.\nHowever, existing approaches suffer from significant performance degradation due to differences in head scale distributions (scale shift) between training and testing data, a challenge known as domain generalization (DG). This paper aims to comprehend the nature of scale shift within the context of domain generalization for crowd localization models.\nTo this end, we address three key questions: (i) how to quantify the scale shift influence on DG task, (ii) why does this influence occur, (iii) how to mitigate the influence.\nSpecifically, we first establish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms, to quantify the influence. \nThrough extensive experiments, we demonstrate the limitations of existing algorithms and highlight the under-explored nature of this issue.\nTo further understand its behind reason, we provide a rigorous theoretical analysis on scale shift. \nBuilding on this analysis, we further propose a simple yet effective algorithm called Semantic Hook to mitigate the influence of scale shift on DG, which also serves as a case study revealing three significant insights for future research. Our results emphasize the importance of this novel and applicable research direction, which we term $\\textit{Scale Shift Domain Generalization}$."
    },
    {
        "title": "Block-Attention for Efficient RAG",
        "link_suffix": "/forum?id=7zNYY1E2fq",
        "link": "https://openreview.net/forum?id=7zNYY1E2fq",
        "pdf_link": "https://openreview.net/pdf?id=7zNYY1E2fq",
        "keywords": "LLM, RAG, efficient language model",
        "abstract": "We introduce Block-Attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. \nTraditional approaches often encode the entire context.\nInstead, Block-Attention divides retrieved documents into discrete blocks, with each block independently calculating key-value (KV) states except for the final block.\nIn RAG scenarios, by defining each passage as a block, Block-Attention enables us to reuse the KV states of passages that have been seen before, thereby significantly reducing the latency and the computation overhead during inference.\nThe implementation of Block-Attention involves block segmentation, position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention mechanism. \nExperiments on four RAG benchmarks demonstrate that after block fine-tuning, the Block-Attention model achieves performance comparable to self-attention models (68.4% vs 67.9% on Llama3) or even superior performance (62.8% vs 59.6% on Mistral).\nNotably, Block-Attention significantly reduces the time to first token (TTFT) and floating point operations (FLOPs) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared to the self-attention models, the time consumption and corresponding FLOPs are reduced by 98.7% and 99.8%, respectively."
    },
    {
        "title": "Uncertainty modeling for fine-tuned implicit functions",
        "link_suffix": "/forum?id=iZl0VqEdxa",
        "link": "https://openreview.net/forum?id=iZl0VqEdxa",
        "pdf_link": "https://openreview.net/pdf?id=iZl0VqEdxa",
        "keywords": "uncertainty, implicit functions, 3D reconstruction, occupancy networks",
        "abstract": "Implicit functions such as Neural Radiance Fields (NeRFs), occupancy networks, and signed distance functions (SDFs) have become pivotal in computer vision for reconstructing detailed object shapes from sparse views. Achieving optimal performance with these models can be challenging due to the extreme sparsity of inputs and distribution shifts induced by data corruptions. To this end, large, noise-free synthetic datasets can serve as shape priors to help models fill in gaps, but the resulting reconstructions must be approached with caution. Uncertainty estimation is crucial for assessing the quality of these reconstructions, particularly in identifying areas where the model is uncertain about the parts it has inferred from the prior. In this paper, we introduce Dropsembles, a novel method for uncertainty estimation in tuned implicit functions. We demonstrate the efficacy of our approach through a series of experiments, starting with toy examples and progressing to a real-world scenario. Specifically, we train a Convolutional Occupancy Network on synthetic anatomical data and test it on low-resolution MRI segmentations of the lumbar spine. Our results show that Dropsembles achieve the accuracy and calibration levels of deep ensembles but with significantly less computational cost."
    },
    {
        "title": "Learning Mask Invariant Mutual Information for Masked Image Modeling",
        "link_suffix": "/forum?id=NoiaAT0eec",
        "link": "https://openreview.net/forum?id=NoiaAT0eec",
        "pdf_link": "https://openreview.net/pdf?id=NoiaAT0eec",
        "keywords": "Masked image modeling, Self-supervised learning, Visual pretraining",
        "abstract": "Masked autoencoders (MAEs) represent a prominent self-supervised learning paradigm in computer vision. Despite their empirical success, the underlying mechanisms of MAEs remain insufficiently understood. Recent studies have attempted to elucidate the functioning of MAEs through contrastive learning and feature representation analysis, yet these approaches often provide only implicit insights. In this paper, we propose a new perspective for understanding MAEs by leveraging the information bottleneck principle in information theory. Our theoretical analyses reveal that optimizing the latent features to balance relevant and irrelevant information is key to improving MAE performance. Building upon our proofs, we introduce MI-MAE, a novel method that optimizes MAEs through mutual information maximization and minimization. By enhancing latent features to retain maximal relevant information between them and the output, and minimizing irrelevant information between them and the input, our approach achieves better performance. Extensive experiments on standard benchmarks show that MI-MAE significantly outperforms MAE models in tasks such as image classification, object detection, and semantic segmentation. Our findings validate the theoretical framework and highlight the practical advantages of applying the information bottleneck principle to MAEs, offering deeper insights for developing more powerful self-supervised learning models."
    },
    {
        "title": "COOL: Efficient and Reliable Chain-Oriented Objective Logic with Neural Networks Feedback Control for Program Synthesis",
        "link_suffix": "/forum?id=Pjkes5MdKI",
        "link": "https://openreview.net/forum?id=Pjkes5MdKI",
        "pdf_link": "https://openreview.net/pdf?id=Pjkes5MdKI",
        "keywords": "program synthesis, Chain-of-Logic, neural network feedback control",
        "abstract": "Program synthesis methods, whether formal or neural-based, lack fine-grained control and flexible modularity, which limits their adaptation to complex software development. These limitations stem from rigid Domain-Specific Language (DSL) frameworks and neural network incorrect predictions. To this end, we propose the \\textbf{Chain of Logic (CoL)}, which organizes synthesis stages into a chain and provides precise heuristic control to guide the synthesis process. Furthermore, by integrating neural networks with libraries and introducing a \\textbf{Neural Network Feedback Control (NNFC)} mechanism, our approach modularizes synthesis and mitigates the impact of neural network mispredictions. Experiments on relational and symbolic synthesis tasks show that CoL significantly enhances the efficiency and reliability of DSL program synthesis across multiple metrics. Specifically, CoL improves accuracy by 70% while reducing tree operations by 91% and time by 95%. Additionally, NNFC further boosts accuracy by 6%, with a 64% reduction in tree operations under challenging conditions such as insufficient training\ndata, increased difficulty, and multidomain synthesis. These improvements confirm COOL as a highly efficient and reliable program synthesis framework."
    },
    {
        "title": "How to Build a Pre-trained Multimodal model for Simultaneously Chatting and Decision-making?",
        "link_suffix": "/forum?id=07cehZ97Xb",
        "link": "https://openreview.net/forum?id=07cehZ97Xb",
        "pdf_link": "https://openreview.net/pdf?id=07cehZ97Xb",
        "keywords": "vision language action model; decision making; autonomous driving; multimodal",
        "abstract": "Existing large pre-trained models typically map text input to text output in an end-to-end manner, such as ChatGPT, or map a segment of text input to a hierarchy of action decisions, such as OpenVLA. However, humans can simultaneously generate text and actions when receiving specific input signals. For example, a driver can make precise driving decisions while conversing with a friend in the passenger seat. Motivated by this observation, we consider the following question in this work: is it possible to construct a pre-trained model that can provide both language interaction and precise decision-making capabilities in dynamic open scenarios. We provide a definitive answer to this question by developing a new model architecture termed Visual Language Action model for Chatting and Decision Making (VLA4CD), and further demonstrating its performance in challenging automonous driving tasks. We build VLA4CD on the basis of transformer-based LLM architecture. Specifically, we leverage LoRA to fine-tune a pre-trained LLM with data of multiple modalities covering language, visual, and action. Unlike the existing LoRA operations used for LLM fine-tuning, we have designed new computational modules and training cost functions for VLA4CD. These designs enable VLA4CD to provide continuous-valued action decisions while outputting text responses. In contrast, existing LLMs can only output text responses, and current VLA models can only output action decisions. Moreover, these VLA models handle action data by discretizing and then tokenizing the discretized actions, a method unsuitable for complex decision-making tasks involving high-dimensional continuous-valued action vectors, such as autonomous driving. The extensive experimental results on the closed-loop autonomous driving platform CARLA validate that: (1) the model construction method we proposed is effective; (2) compared to the state-of-the-art VLA model, VLA4CD can provide more accurate real-time decision-making while retaining the text interaction capability inherent to LLMs."
    }
]