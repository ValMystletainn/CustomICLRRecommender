[{"title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates", "link_suffix": "/forum?id=ecRyUAPshY", "link": "https://openreview.net/forum?id=ecRyUAPshY", "pdf_link": "https://openreview.net/pdf?id=ecRyUAPshY", "keywords": "knowledge editing, code large language models, program synthesis", "abstract": "Large language models (LLMs) are increasingly being used to synthesize and reason about source code. The libraries and API functions they invoke are continuously evolving, with functionality being added or changing. Yet, no prior work has studied how an LLM's knowledge about code API functions can be updated. To fill this gap, we presentCodeUpdateArena, a benchmark for knowledge editing in the code domain. An instance in our benchmark consists of a synthetic API function update paired with a program synthesis example that uses the updated functionality; our goal is to update an LLM to be able to solve this program synthesis example without providing documentation of the update at inference time. Compared to knowledge editing for facts, success here is more challenging: a code LLM must reason about the semantics of the modified function rather than just reproduce its syntax. Our dataset is constructed by first prompting GPT-$4$ to generate atomic and executable function updates. Then, for each update, we generate program synthesis examples whose code solutions are prone to use the update. Our benchmark covers updates of various types to 54 functions from seven diverse Python packages, with a total of 670 program synthesis examples. Our experiments show that fine-tuning open-source code LLMs (i.e., DeepSeek, CodeLlama) on documentation of a new update does not allow them to incorporate changes for problem-solving. However, prepending the same information does help, establishing that the information is present, and careful fine-tuning on examples demonstrating the update shows improvement, paving the way for better knowledge editing techniques for code.", "title_embedding_index": 19800, "title_abs_embedding_index": 19825}, {"title": "Scale-aware Recognition in Satellite Images under Resource Constraints", "link_suffix": "/forum?id=QIxFo9mFwR", "link": "https://openreview.net/forum?id=QIxFo9mFwR", "pdf_link": "https://openreview.net/pdf?id=QIxFo9mFwR", "keywords": "Satellite Imagery, Resolution", "abstract": "Recognition of features in satellite imagery (forests, swimming pools, etc.) depends strongly on the spatial scale of the concept and therefore the resolution of the images. This poses two challenges:\nWhich resolution is best suited for recognizing a given concept, and where and when should the costlier higher-resolution (HR) imagery be acquired? \nWe present a novel scheme to address these challenges by introducing three components: (1) A technique to  distill knowledge from models trained on HR imagery to recognition models that operate on imagery of lower resolution (LR), (2) a sampling strategy for HR imagery based on model disagreement, and (3) an LLM-based approach for inferring concept \"scale\". With these components we present a system to efficiently perform scale-aware recognition in satellite imagery, improving accuracy over single-scale inference while following budget constraints.Our novel approach offers up to a 26.3% improvement over entirely HR baselines, using 76.3 % fewer HR images.", "title_embedding_index": 19801, "title_abs_embedding_index": 19826}, {"title": "Watermarking for User Identification in Large Language Models", "link_suffix": "/forum?id=ZACAKudvKW", "link": "https://openreview.net/forum?id=ZACAKudvKW", "pdf_link": "https://openreview.net/pdf?id=ZACAKudvKW", "keywords": "Large Language model; Watermark", "abstract": "We identify a new task for watermarking -- namely the simultaneous identification of text as being automatically generated alongside the identification of the LLM user. \nWe show that a na\u00efve approach that treats a text as artificially generated if a user is correctly identified is prone to problems of false positives arising from multiple hypothesis comparison. \nWe propose a novel approach (Our code is submitted with the supplementary material. We will also open it on Github after the anonymity period.) that retains almost similar rates as the number of users increase. We derive theoretical bounds that support our experimental approach.", "title_embedding_index": 19802, "title_abs_embedding_index": 19827}, {"title": "IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation", "link_suffix": "/forum?id=4w99NAikOE", "link": "https://openreview.net/forum?id=4w99NAikOE", "pdf_link": "https://openreview.net/pdf?id=4w99NAikOE", "keywords": "Compositional text-to-image generation, Feedback learning for diffusion model", "abstract": "Advanced diffusion models like Stable Diffusion 3, Omost, and FLUX have made notable strides in compositional text-to-image generation. However, these methods typically exhibit distinct strengths for compositional generation, with some excelling in handling attribute binding and others in spatial relationships. This disparity highlights the need for an approach that can leverage the complementary strengths of various models to comprehensively improve the composition capability. To this end, we introduce IterComp, a novel framework that aggregates composition-aware model preferences from multiple models and employs an iterative feedback learning approach to enhance compositional generation. Specifically, we curate a gallery of six powerful open-source diffusion models and evaluate their three key compositional metrics: attribute binding, spatial relationships, and non-spatial relationships. Based on these metrics, we develop a composition-aware model preference dataset comprising numerous image-rank pairs to train composition-aware reward models. Then, we propose an iterative feedback learning method to enhance compositionality in a closed-loop manner, enabling the progressive self-refinement of both the base diffusion model and reward models over multiple iterations. Detailed theoretical proof demonstrates the effectiveness of this method. Extensive experiments demonstrate our significant superiority over previous methods, particularly in multi-category object composition and complex semantic alignment. IterComp opens new research avenues in reward feedback learning for diffusion models and compositional generation.", "title_embedding_index": 19803, "title_abs_embedding_index": 19828}, {"title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models", "link_suffix": "/forum?id=m2nmp8P5in", "link": "https://openreview.net/forum?id=m2nmp8P5in", "pdf_link": "https://openreview.net/pdf?id=m2nmp8P5in", "keywords": "Symbolic Regression, Equation Discovery, Large Language Models, Evolutionary Search", "abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely large combinatorial hypothesis spaces. Current methods of equation discovery, commonly known as symbolic regression techniques, largely focus on extracting equations from data alone, often neglecting the domain-specific prior knowledge that scientists typically depend on. They also employ limited representations such as expression trees, constraining the search space and expressiveness of equations. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeleton hypotheses, drawing from its domain knowledge, which are then optimized against data to estimate parameters. We evaluate LLM-SR on four benchmark problems across diverse scientific domains (e.g., physics, biology), which we carefully designed to simulate the discovery process and prevent LLM recitation. Our results demonstrate that LLM-SR discovers physically accurate equations that significantly outperform state-of-the-art symbolic regression baselines, particularly in out-of-domain test settings. We also show that LLM-SR's incorporation of scientific priors enables more efficient equation space exploration than the baselines.", "title_embedding_index": 19804, "title_abs_embedding_index": 19829}, {"title": "GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks", "link_suffix": "/forum?id=qgsXsqahMq", "link": "https://openreview.net/forum?id=qgsXsqahMq", "pdf_link": "https://openreview.net/pdf?id=qgsXsqahMq", "keywords": "Uncertainty Quantification; Graph Neural Networks; Ensemble Learning; Mixture of Experts", "abstract": "Graph Neural Networks (GNNs) deliver strong classification results but often suffer from poor calibration performance, leading to overconfidence or underconfidence. This is particularly problematic in high-stakes applications where accurate uncertainty estimates are essential. Existing post-hoc methods, such as temperature scaling, fail to effectively utilize graph structures, while current GNN calibration methods often overlook the potential of leveraging diverse input information and model ensembles jointly. In the paper, we propose Graph Ensemble Temperature Scaling (GETS), a novel calibration framework that combines input and model ensemble strategies within a Graph Mixture-of-Experts (MoE) architecture. GETS integrates diverse inputs, including logits, node features, and degree embeddings, and adaptively selects the most relevant experts for each node\u2019s calibration procedure. Our method outperforms state-of-the-art calibration techniques, reducing expected calibration error (ECE) by $\\geq$ 25% across 10 GNN benchmark datasets. Additionally, GETS is computationally efficient, scalable, and capable of selecting effective input combinations for improved calibration performance.", "title_embedding_index": 19805, "title_abs_embedding_index": 19830}, {"title": "C3T: Cross-modal Transfer Through Time for Human Action Recognition", "link_suffix": "/forum?id=e8c7XDRJcg", "link": "https://openreview.net/forum?id=e8c7XDRJcg", "pdf_link": "https://openreview.net/pdf?id=e8c7XDRJcg", "keywords": "cross-modal transfer, multimodal learning, human action recognition, human activity reocognition, IMU", "abstract": "In order to unlock the potential of diverse sensors, we investigate a method to transfer knowledge between modalities using the structure of a unified multimodal representation space for Human Action Recognition (HAR). \nWe formalize and explore an understudied cross-modal transfer setting we term Unsupervised Modality Adaptation (UMA), where the modality used in testing is not used in supervised training, i.e. zero labeled instances of the test modality are available during training. \nWe develop three methods to perform UMA: Student-Teacher (ST), Contrastive Alignment (CA), and Cross-modal Transfer Through Time (C3T).\nOur extensive experiments on various camera+IMU datasets compare these methods to each other in the UMA setting, and to their empirical upper bound in the supervised setting.\nThe results indicate C3T is the most robust and highest performing by at least a margin of 8%, and nears the supervised setting performance even in the presence of temporal noise.\nThis method introduces a novel mechanism for aligning signals across time-varying latent vectors, extracted from the receptive field of temporal convolutions.\nOur findings suggest that C3T has significant potential for developing generalizable models for time-series sensor data, opening new avenues for multi-modal learning in various applications.", "title_embedding_index": 19806, "title_abs_embedding_index": 19831}, {"title": "Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning", "link_suffix": "/forum?id=vXG7d2VlHU", "link": "https://openreview.net/forum?id=vXG7d2VlHU", "pdf_link": "https://openreview.net/pdf?id=vXG7d2VlHU", "keywords": "spatial reasoning, vision language models, multimodal large language models", "abstract": "Vision language models (VLMs) have demonstrated impressive performance across a wide range of downstream tasks. However, their proficiency in spatial reasoning remains limited, despite its crucial role in tasks involving navigation and interaction with physical environments. \nSpecifically, much of the spatial reasoning in these tasks occurs in two-dimensional (2D) environments, and our evaluation reveals that state-of-the-art VLMs frequently generate implausible and incorrect responses to composite spatial reasoning problems, including simple pathfinding tasks that humans can solve effortlessly at a glance. \nTo address this, we explore an effective approach to enhance 2D spatial reasoning within VLMs by training the model on basic spatial capabilities.\nWe begin by disentangling the key components of 2D spatial reasoning: direction comprehension, distance estimation, and localization.\nOur central hypothesis is that mastering these basic spatial capabilities can significantly enhance a model's performance on composite spatial tasks requiring advanced spatial understanding and combinatorial problem-solving.\nTo investigate this hypothesis, we introduce Sparkle,\na framework that fine-tunes VLMs on these three basic spatial capabilities by synthetic data generation and targeted supervision to form an instruction dataset for each capability.\nOur experiments demonstrate that VLMs fine-tuned with Sparkle achieve significant performance gains, not only in the basic tasks themselves but also in generalizing to composite and out-of-distribution spatial reasoning tasks (e.g., improving from 13.5% to 40.0% on the shortest path problem). These findings underscore the effectiveness of mastering basic spatial capabilities in enhancing composite spatial problem-solving, offering insights into systematic strategies for improving VLMs' spatial reasoning capabilities.", "title_embedding_index": 19807, "title_abs_embedding_index": 19832}, {"title": "MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods", "link_suffix": "/forum?id=KI45uDnmzv", "link": "https://openreview.net/forum?id=KI45uDnmzv", "pdf_link": "https://openreview.net/pdf?id=KI45uDnmzv", "keywords": "Mamba, Quantization", "abstract": "Mamba is an efficient sequence model that rivals Transformers and demonstrates significant potential as a foundational architecture for various tasks. Quantization is commonly used in neural networks to reduce model size and computational latency. However, applying quantization to Mamba remains underexplored, and existing quantization methods, which have been effective for CNN and Transformer models, appear inadequate for Mamba models (e.g., Quarot suffers a 21% accuracy drop on Vim-T\u2020 even under W8A8). We have pioneered the exploration of this issue and identified several key challenges. First, significant outliers arepresent in gate projections, output projections, and matrix multiplications. Second, Mamba\u2019s unique parallel scan further amplifies these outliers, leading to uneven and heavy-tailed data distributions. Third, even with the application of the Hadamard transform, the variance across channels in weights and activations still remains inconsistent. To these ends, we propose MambaQuant, a post-training quantization (PTQ) framework consisting of: 1) Karhunen-Lo`eve Transformation (KLT) enhanced rotation, rendering the rotation matrix adaptable to diverse channel distributions. 2) Smooth-Fused rotation, which equalizes channel variances and can merge additional parameters into model weights. Experiments show that MambaQuant can quantize both weights and activations into 8-bit with less than 1% accuracy loss for Mamba-based vision and language tasks. To our knowledge, MambaQuant is the first comprehensive PTQ design for the Mamba family, paving the way for further advancements in its application.", "title_embedding_index": 19808, "title_abs_embedding_index": 19833}, {"title": "Enhancing Graph Invariant Learning from a Negative Inference Perspective", "link_suffix": "/forum?id=dl4nnpJssi", "link": "https://openreview.net/forum?id=dl4nnpJssi", "pdf_link": "https://openreview.net/pdf?id=dl4nnpJssi", "keywords": "Graph learning, out-of-distribution generalization, environment awareness, negative inference, prompt learning.", "abstract": "The out-of-distribution (OOD) generalization challenge is a longstanding problem  in graph learning. Through studying the fundamental cause of data distribution shift, i.e., the changes of environments, significant progress has been achieved in addressing this issue. However, we observe that existing works still fail to effectively address complex environment shifts. Previous practices place excessive attention on extracting causal subgraphs, inevitably treating spurious subgraphs as environment variables. While spurious subgraphs are controlled by environments, the space of environment changes encompass more than the scale of spurious subgraphs. Therefore, existing efforts have a limited inference space for environments,  leading to failure under severe environment changes. To tackle this issue, we propose a negative inference graph OOD framework (NeGo)  to broaden the inference space for environment factors. Inspired by the successful practice of prompt learning in capturing underlying semantics and causal associations in large language models, we design a negative prompt environment inference to extract underlying environment information. We further introduce the environment-enhanced invariant subgraph learning method to effectively exploit inferred environment embedding, ensuring the robust extraction of causal subgraph in the environment shifts. Lastly, we conduct a comprehensive evaluation of NeGo on real-world datasets and synthetic datasets across domains. NeGo outperforms baselines on nearly all datasets, which verify the effectiveness of our framework. Our source code is available at \\url{https://anonymous.4open.science/r/NeGo-E4C1}.", "title_embedding_index": 19809, "title_abs_embedding_index": 19834}, {"title": "Backtracking Improves Generation Safety", "link_suffix": "/forum?id=Bo62NeU6VF", "link": "https://openreview.net/forum?id=Bo62NeU6VF", "pdf_link": "https://openreview.net/pdf?id=Bo62NeU6VF", "keywords": "AI safety, Generation algorithm, Backtracking", "abstract": "Text generation has a fundamental limitation almost by definition: there is no taking back tokens that have been generated, even when they are clearly problematic.\nIn the context of language model safety, when a partial unsafe generation is produced, language models by their nature tend to happily keep on generating similarly unsafe additional text.\nThis is in fact how safety alignment of frontier models gets circumvented in the wild, despite great efforts in improving their safety.\nDeviating from the paradigm of approaching safety alignment as prevention (decreasing the probability of harmful responses), we propose backtracking, a technique that allows language models to \"undo\" and recover from their own unsafe generation through the introduction of a special [RESET] token.\nOur method can be incorporated into either SFT or DPO training to optimize helpfulness and harmlessness.\nWe show that models trained to backtrack are consistently safer than baseline models: backtracking Llama-3-8B is four times more safe than the baseline model (6.1% $\\to$ 1.5%) in our evaluations without regression in helpfulness.\nOur method additionally provides protection against four adversarial attacks including an adaptive attack, despite not being trained to do so.", "title_embedding_index": 19810, "title_abs_embedding_index": 19835}, {"title": "ST-GCond: Self-supervised and Transferable Graph Dataset Condensation", "link_suffix": "/forum?id=wYWJFLQov9", "link": "https://openreview.net/forum?id=wYWJFLQov9", "pdf_link": "https://openreview.net/pdf?id=wYWJFLQov9", "keywords": "Graph Neural Network; Graph Dataset Condensation", "abstract": "The increasing scale of graph datasets significantly enhances deep learning models but also presents substantial training challenges. Graph dataset condensation has emerged to condense large datasets into smaller yet informative ones that maintain similar test performance. However, these methods require downstream usage to match the original dataset and task, which is impractical in real-world scenarios. Our empirical studies show that existing methods fail in \"cross-task\" and \"cross-dataset\" scenarios, often performing worse than training from scratch. To address these challenges, we propose a novel method: Self-supervised and Transferable Graph dataset Condensation (ST-GCond). For cross-task transferability, we propose a task-disentangled meta optimization strategy to adaptively update the condensed graph according to the task relevance, encouraging information preservation for various tasks. For cross-dataset transferability, we propose a multi-teacher self-supervised optimization strategy to incorporate auxiliary self-supervised tasks to inject universal knowledge into the condensed graph. Additionally, we incorporate mutual information guided joint condensation mitigating the potential conflicts and ensure the condensing stability. Experiments on both node-level and graph-level datasets show that ST-GCond outperforms existing methods by 2.5% to 18.7% in all cross-task and cross-dataset scenarios, and also achieves state-of-the-art performance on 5 out of 6 datasets in the single dataset and task scenario.", "title_embedding_index": 19811, "title_abs_embedding_index": 19836}, {"title": "PPT: Patch Order Do Matters In Time Series Pretext Task", "link_suffix": "/forum?id=7zwIEbSTDy", "link": "https://openreview.net/forum?id=7zwIEbSTDy", "pdf_link": "https://openreview.net/pdf?id=7zwIEbSTDy", "keywords": "Time Series Classification, Self-Supervised Learning, Pretext Task", "abstract": "Recently, patch-based models have been widely discussed in time series analysis. However, existing pretext tasks for patch-based learning, such as masking, may not capture essential time and channel-wise patch interdependencies in time series data, presumed to result in subpar model performance. In this work, we introducePatch order-aware Pretext Task (PPT), a new self-supervised patch order learning pretext task for time series classification. PPT exploits the intrinsic sequential order information among patches across time and channel dimensions of time series data, where model training is aided by channel-wise patch permutations. The permutation disrupts patch order consistency across time and channel dimensions with controlled intensity to provide supervisory signals for learning time series order characteristics. To this end, we propose two patch order-aware learning methods: patch order consistency learning, which quantifies patch order correctness, and contrastive learning, which distinguishes weakly permuted patch sequences from strongly permuted ones. With patch order learning, we observe enhanced model performance, e.g., improving up to 7% accuracy for the supervised cardiogram task and outperforming mask-based learning by 5% in the self-supervised human activity recognition task. We also propose ACF-CoS, an evaluation metric that measures theimportance of ordernessfor time series datasets, which enables pre-examination of the efficacy of PPT in model training.", "title_embedding_index": 19812, "title_abs_embedding_index": 19837}, {"title": "Rethinking Graph Neural Networks From A Geometric Perspective Of Node Features", "link_suffix": "/forum?id=lBMRmw59Lk", "link": "https://openreview.net/forum?id=lBMRmw59Lk", "pdf_link": "https://openreview.net/pdf?id=lBMRmw59Lk", "keywords": "Graph neural networks, node classification, feature centroid simplex, coarse geometry", "abstract": "Many works on graph neural networks (GNNs) focus on graph topologies and analyze graph-related operations to enhance performance on tasks such as node classification. In this paper, we propose to understand GNNs based on a feature-centric approach. Our main idea is to treat the features of nodes from each label class as a whole, from which we can identify the centroid. The convex hull of these centroids forms a simplex called the feature centroid simplex, where a simplex is a high-dimensional generalization of a triangle. We borrow ideas from coarse geometry to analyze the geometric properties of the feature centroid simplex by comparing them with basic geometric models, such as regular simplexes and degenerate simplexes. Such a simplex provides a simple platform to understand graph-based feature aggregation, including phenomena such as heterophily, oversmoothing, and feature re-shuffling. Based on the theory, we also identify simple and useful tricks for the node classification task.", "title_embedding_index": 19813, "title_abs_embedding_index": 19838}, {"title": "Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies", "link_suffix": "/forum?id=9UGfOJBuL8", "link": "https://openreview.net/forum?id=9UGfOJBuL8", "pdf_link": "https://openreview.net/pdf?id=9UGfOJBuL8", "keywords": "neurodegenerative disease, conditional diffusion model, longitudinal data analysis", "abstract": "Modeling the progression of neurodegenerative diseases such as Alzheimer\u2019s disease (AD) is crucial for early detection and prevention given their irreversible nature. However, the scarcity of longitudinal data and complex disease dynamics make the analysis highly challenging. Moreover, longitudinal samples often contain irregular and large intervals between subject visits, which underscore the necessity for advanced data generation techniques that can accurately simulate disease progression over time. In this regime, we propose a novel conditional generative model for synthesizing longitudinal sequences and present its application to neurodegenerative disease data generation conditioned on multiple time-dependent ordinal factors, such as age and disease severity. Our method sequentially generates continuous data by bridging gaps between sparse data points with a diffusion model, ensuring a realistic representation of disease progression. The synthetic data are curated to integrate both cohort-level and individual-specific characteristics, where the cohort-level representations are modeled with an ordinal regression to capture longitudinally monotonic behavior. Extensive experiments on four AD biomarkers validate the superiority of our method over nine baseline approaches, highlighting its potential to be applied to a variety of longitudinal data generation.", "title_embedding_index": 19814, "title_abs_embedding_index": 19839}, {"title": "Realistic-Gesture: Co-Speech Gesture Video Generation through Semantic-aware Gesture Representation", "link_suffix": "/forum?id=EXsiGFkwV6", "link": "https://openreview.net/forum?id=EXsiGFkwV6", "pdf_link": "https://openreview.net/pdf?id=EXsiGFkwV6", "keywords": "gesture generation; motion representation; video generation", "abstract": "Co-speech gesture generation is crucial for creating lifelike avatars and enhancing human-computer interactions by synchronizing gestures with speech in computer vision. Despite recent advancements, existing methods often struggle with accurately aligning gesture motions with speech signals and achieving pixel-level realism. To address these challenges, we introduce Realistic-Gesture, a groundbreaking framework that transforms co-speech gesture video generation through three innovative components: (1) a speech-aware gesture tokenization that incorporate speech context into motion pattern representation, (2) a mask gesture generator that learns to map audio signals to gestures by predicting masked motion tokens, enabling bidirectional contextually relevant gesture synthesis and editing, and (3) a structure-aware refinement module that employs differentiable edge connection to link gesture keypoints to improve video generation. Our extensive experiments demonstrate that Realistic-Gesture not only produces highly realistic and speech-aligned gesture videos but also supports long-sequence generation and video gesture editing applications.", "title_embedding_index": 19815, "title_abs_embedding_index": 19840}, {"title": "CORTEX: Concept-Oriented Token Explanation in Vector-Quantized Generative Model", "link_suffix": "/forum?id=oW7jUu4gVg", "link": "https://openreview.net/forum?id=oW7jUu4gVg", "pdf_link": "https://openreview.net/pdf?id=oW7jUu4gVg", "keywords": "Vector-Quantized Generative Model, Explainability, Information Bottleneck", "abstract": "Vector-Quantized Generative Models (VQGMs) have emerged as powerful tools for image generation. However, the key component of VQGMs---the codebook of discrete tokens---is still not well understood, e.g., which tokens are critical to generate an image of a certain concept? This paper introduces Concept-Oriented Token Explanation (CORTEX), a novel approach for interpreting VQGMs by identifying concept-specific token combinations. Our framework employs two methods: (1) a saliency-based method that analyzes token saliency value in individual images, and (2) an optimization-based method that explores the entire codebook to find globally relevant tokens. Experimental results demonstrate CORTEX's efficacy in providing clear explanations of token usage in the generative process, outperforming baselines across multiple pretrained VQGMs. CORTEX not only improves VQGM transparency but also enables tasks such as targeted image editing, offering valuable insights into the model's internal representations.", "title_embedding_index": 19816, "title_abs_embedding_index": 19841}, {"title": "Cost-Effective Synthetic Data Generation for Post-Training using QWICK", "link_suffix": "/forum?id=tp1QiTH2aa", "link": "https://openreview.net/forum?id=tp1QiTH2aa", "pdf_link": "https://openreview.net/pdf?id=tp1QiTH2aa", "keywords": "synthetic data generation, post-training", "abstract": "Large language models (LLMs) are showing expert-level ability in various fields (e.g., programming and math). However, this progress heavily relies on the generation of high-quality synthetic data to improve the models\u2019 capabilities during post-training. Generating such data in a cost-effective manner presents a significant challenge. Specifically, stronger models tend to generate higher-quality data but come with a substantial computational cost, while weaker models are cheaper to run but may produce weaker outputs. In this paper, we introduce Question-Wise model pICK (QWICK) to address this challenge. By tracking the empirical reward, cost, and number of trials for each model, QWICK strikes a balance between exploitation and exploration, ultimately converging on a cost-effective model for each specific question. Specifically, QWICK achieves a 50% cost reduction on a programming dataset and a 40% cost reduction on a mathematics dataset, without compromising data quality. Furthermore, compared to baseline methods, our approach can produce up to 2.1 times more valid synthetic data at the same cost. Our anonymized code is available athttps://anonymous.4open.science/r/QWICK-17C3", "title_embedding_index": 19817, "title_abs_embedding_index": 19842}, {"title": "KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models", "link_suffix": "/forum?id=vNATZfmY6R", "link": "https://openreview.net/forum?id=vNATZfmY6R", "pdf_link": "https://openreview.net/pdf?id=vNATZfmY6R", "keywords": "large multimodal models, analogical reasoning, cognition, developmental psychology", "abstract": "This paper investigates visual analogical reasoning in large multimodal models (LMMs) compared to human adults and children. A \u201cvisual analogy\u201d is an abstract rule inferred from one image and applied to another. \nWhile benchmarks exist for testing visual reasoning in LMMs, they require advanced skills and omit basic visual analogies that even young children can make. Inspired by developmental psychology, we propose a new benchmark of 1,400 visual transformations of everyday objects to test LMMs on visual analogical reasoning and compare them to children and adults. We structure the evaluation into three stages: identifying what changed (e.g., color, number, etc.), how it changed (e.g., added one object), and applying the rule to new scenarios. Our findings show that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \u201cwhat\u201d effectively, they struggle with quantifying the \u201chow\u201d and extrapolating this rule to new objects. In contrast, children and adults exhibit much stronger analogical reasoning at all three stages. Additionally, the strongest tested model, GPT-4V, performs better in tasks involving simple surface-level visual attributes like color and size, correlating with quicker human adult response times. Conversely, more complex tasks such as number, rotation, and reflection, which necessitate extensive cognitive processing and understanding of extrinsic spatial properties in the physical world, present more significant challenges. Altogether, these findings highlight the limitations of training models on data that primarily consists of 2D images and text.", "title_embedding_index": 19818, "title_abs_embedding_index": 19843}, {"title": "Learning from weak labelers as constraints", "link_suffix": "/forum?id=2BtFKEeMGo", "link": "https://openreview.net/forum?id=2BtFKEeMGo", "pdf_link": "https://openreview.net/pdf?id=2BtFKEeMGo", "keywords": "unsupervised learning, weak supervision, learning theory", "abstract": "We study programmatic weak supervision, where in contrast to labeled data, we have access to \\emph{weak labelers}, each of which either abstains or provides noisy labels corresponding to any input. Most previous approaches typically employ latent generative models that model the joint distribution of the weak labels and the latent ``true'' label. The caveats are that this relies on assumptions that may not always hold in practice such as conditional independence assumptions over the joint distribution of the weak labelers and the latent true label, and more general implicit inductive biases in the latent generative models. In this work, we consider a more explicit form of side-information that can be leveraged to denoise the weak labeler, namely the bounds on the average error of the weak labelers. We then propose a novel but natural weak supervision objective that minimizes a regularization functional subject to satisfying these bounds. This turns out to be a difficult constrained optimization problem due to discontinuous accuracy bound constraints. We provide a continuous optimization formulation for this objective through an alternating minimization algorithm that iteratively computes soft pseudo labels on the unlabeled data satisfying the constraints while being close to the model, and then updates the model on these labels until all the constraints are satisfied. We follow this with a theoretical analysis of this approach and provide insights into its denoising effects in training discriminative models given multiple weak labelers. Finally, we demonstrate the superior performance and robustness of our method on a popular weak supervision benchmark.", "title_embedding_index": 19819, "title_abs_embedding_index": 19844}, {"title": "Weakly Supervised Understanding of Skilled Human Activity in Videos", "link_suffix": "/forum?id=HCoSsULNxG", "link": "https://openreview.net/forum?id=HCoSsULNxG", "pdf_link": "https://openreview.net/pdf?id=HCoSsULNxG", "keywords": "skilled human activity understanding, weakly supervised learning, action quality assessment, long-form video understanding", "abstract": "Understanding skilled human activity is crucial in fields such as sports analytics, medical training, and professional development, where assessing proficiency can directly influence performance and outcomes. However, many existing approaches rely on human-annotated numerical scores or rankings, which are not only time-consuming but also introduce subjectivity. Conversely, categorizing proficiency as either high or low, though providing less detailed information, is easier to collect and can often be derived from group characteristics such as the distinction between novices and experts in surgical training. This new setting challenges models to uncover intrinsic patterns that reflect proficiency based solely on these weak labels. To achieve this, we introduce Sparse Skill Extractor, a multi-scale contrastive learning framework. It enforces both local and global feature comparisons between groups while pruning irrelevant video segments to highlight key moments of skilled or unskilled performance.  Our results demonstrate that Sparse Skill Extractor not only delivers strong performance in predicting demonstrator proficiency but also enhances interpretability by facilitating the detection of non-proficient timestamps for low proficiency demonstrations.", "title_embedding_index": 19820, "title_abs_embedding_index": 19845}, {"title": "BISIMULATION METRIC FOR MODEL PREDICTIVE CONTROL", "link_suffix": "/forum?id=F07ic7huE3", "link": "https://openreview.net/forum?id=F07ic7huE3", "pdf_link": "https://openreview.net/pdf?id=F07ic7huE3", "keywords": "Reinforcement Learning, Model-based reinforcement learning, optimal control, MPC", "abstract": "Model-based reinforcement learning (MBRL) has shown promise for improving sample efficiency and decision-making in complex environments. However, existing methods face challenges in training stability, robustness to noise, and computational efficiency. In this paper, we propose Bisimulation Metric for Model Predictive Control (BS-MPC), a novel approach that incorporates bisimulation metric loss in its objective function to directly optimize the encoder. This optimization enables the learned encoder to extract intrinsic information from the original state space while discarding irrelevant details. BS-MPC improves training stability, robustness against input noise, and computational efficiency by reducing training time. We evaluate BS-MPC on both continuous control and image-based tasks from the DeepMind Control Suite, demonstrating superior performance and robustness compared to state-of-the-art baseline methods.", "title_embedding_index": 19821, "title_abs_embedding_index": 19846}, {"title": "InfiniBench: A Comprehensive Benchmark for Large Multimodal Models in Very Long Video Understanding", "link_suffix": "/forum?id=2D0uXQbntW", "link": "https://openreview.net/forum?id=2D0uXQbntW", "pdf_link": "https://openreview.net/pdf?id=2D0uXQbntW", "keywords": "video understanding, benchmark, long video benchmark, long video understanding", "abstract": "Understanding long videos, ranging from tens of minutes to several hours, presents unique challenges in video comprehension. Despite the increasing importance of long-form video content, existing benchmarks primarily focus on shorter clips. To address this gap, we introduce InfiniBench a comprehensive benchmark for very long video understanding which presents 1)The longest video duration, averaging 52.59 minutes per video; 2) The largest number of question-answer pairs, 108.2K; 3) Diversity in questions that examine nine different skills and include both multiplechoice questions and open-ended questions; 4) Human-centric, as the video sources come from movies and daily TV shows, with specific human-level question designs such as Movie Spoiler Questions that require critical thinking and comprehensive understanding. Using InfiniBench, we comprehensively evaluate existing Large Multi-Modality Models (LMMs) on each skill, including the commercial models such as GPT-4o and Gemini 1.5 Flash and the open-source models. The evaluation shows significant challenges in our benchmark. Our findings reveal that even leading AI models like GPT-4o and Gemini 1.5 Flash face challenges in achieving high performance in long video understanding, with average accuracies of just 49.16% and 42.72%, and average scores of 3.22 and 2.71 out of 5, respectively. We hope this benchmark will stimulate the LMMs community towards long video and human-level understanding. Our benchmark can be accessed at (https://infinibench.github.io/Infinibench-website/) and will be made publicly available.", "title_embedding_index": 19822, "title_abs_embedding_index": 19847}, {"title": "Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling", "link_suffix": "/forum?id=EgJhwYR2tB", "link": "https://openreview.net/forum?id=EgJhwYR2tB", "pdf_link": "https://openreview.net/pdf?id=EgJhwYR2tB", "keywords": "LLM, Knowledge Distillation, On-policy", "abstract": "Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.", "title_embedding_index": 19823, "title_abs_embedding_index": 19848}, {"title": "NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval", "link_suffix": "/forum?id=MYw74B77KQ", "link": "https://openreview.net/forum?id=MYw74B77KQ", "pdf_link": "https://openreview.net/pdf?id=MYw74B77KQ", "keywords": "semantic similarity search, pre-trained embedding model, fine-tuning", "abstract": "$k$-Nearest Neighbor search on dense vector embeddings ($k$-NN retrieval) from pre-trained embedding models is the predominant retrieval method for text and images, as well as Retrieval-Augmented Generation (RAG) pipelines. In practice, application developers often fine-tune the embeddings to improve their accuracy on the dataset and query workload in hand. Existing approaches either fine-tune the pre-trained model itself or, more efficiently, but at the cost of accuracy, train adaptor models to transform the output of the pre-trained model. We present NUDGE, a family of novelnon-parametricembedding fine-tuning approaches that are significantly more accurate and efficient than both sets of existing approaches. NUDGE directly modifies the embeddings of data records to maximize the accuracy of $k$-NN retrieval. We present a thorough theoretical and experimental study of NUDGE's non-parametric approach. We show that even though the underlying problem is NP-Hard, constrained variations can be solved efficiently. These constraints additionally ensure that the changes to the embeddings are modest, avoiding large distortions to the semantics learned during pre-training. In experiments across five pre-trained models and nine standard text and image retrieval datasets,NUDGE runs in minutes and often improves NDCG@10 by more than 10% over existing fine-tuning methods. On average, NUDGE provides 3.3$\\times$ and 4.3$\\times$ higher increase in accuracy and runs 200$\\times$ and 3$\\times$ faster, respectively, over fine-tuning the pre-trained model and training adaptors.", "title_embedding_index": 19824, "title_abs_embedding_index": 19849}]