[{"title": "Gamified crowd-sourcing of high-quality data for visual fine-tuning", "link_suffix": "/forum?id=LCk3umTAXx", "link": "https://openreview.net/forum?id=LCk3umTAXx", "pdf_link": "https://openreview.net/pdf?id=LCk3umTAXx", "keywords": "Large Multimodal Models, Visual Question Answering, Visual Instruction Tuning, Gamification, Supervised Learning, Data Generation", "abstract": "This paper introduces gamified adversarial prompting (GAP), a framework that\ncrowd-sources high-quality data for visual instruction tuning of large multimodal\nmodels. GAP transforms the data collection process into an engaging game, in-\ncentivizing players to provide fine-grained, challenging questions and answers\nthat target gaps in the model\u2019s knowledge. Our contributions include (1) an ap-\nproach to capture question-answer pairs from humans that directly address weak-\nnesses in a model\u2019s knowledge, (2) a method for evaluating and rewarding players\nthat successfully incentivizes them to provide high-quality submissions, and (3) a\nscalable, gamified platform that succeeds in collecting this data from over 50,000\nparticipants in just a few weeks. Our implementation of GAP has significantly im-\nproved the accuracy of a small multimodal model, namely MiniCPM-Llama3-V-\n2.5-8B, increasing its GPT score from 0.147 to 0.477 on our dataset, approaching\nthe benchmark set by the much larger GPT-4V. Moreover, we demonstrate that\nthe data generated using MiniCPM-Llama3-V-2.5-8B also enhances its perfor-\nmance across other benchmarks, and exhibits cross-model benefits. Specifically,\nthe same data improves the performance of QWEN2-VL-2B and QWEN2-VL-7B\non the same multiple benchmarks.", "title_embedding_index": 3750, "title_abs_embedding_index": 3775}, {"title": "GE-PEFT: Gated Expandable Parameter-Efficient Fine-Tuning for Continual Learning", "link_suffix": "/forum?id=NmiFwEP8K5", "link": "https://openreview.net/forum?id=NmiFwEP8K5", "pdf_link": "https://openreview.net/pdf?id=NmiFwEP8K5", "keywords": "PEFT, Continual Learning, Knowledge Transfer, Language Models", "abstract": "Continual learning (CL) is a research field focused on continuously adapting foundation models such as large language models (LMs) to newly emerging information sources and tasks. While aspects such as parameter efficiency, knowledge transfer, and managing model capacity have recently received attention, the main research focus in CL remains on preventing catastrophic forgetting. Specifically, there is a lack of solutions that address all these aspects simultaneously. We bridge this gap by introducing Gated Expandable Parameter-Efficient Fine-Tuning (GE-PEFT). Our approach shares knowledge of previous tasks through leveraging a single, dynamically expanding PEFT module within LMs while selectively gating irrelevant previous tasks. Our experiments across multiple task-incremental CL benchmarks demonstrate that GE-PEFT outperforms existing state-of-the-art CL approaches in both full CL and few-shot settings. Our ablation and parameter sensitivity studies highlight the benefit of each proposed component, demonstrating that GE-PEFT offers a more efficient and adaptive solution for CL in LMs.", "title_embedding_index": 3751, "title_abs_embedding_index": 3776}, {"title": "Score-based free-form architectures for high-dimensional Fokker-Planck equations", "link_suffix": "/forum?id=5qg6JPSgCj", "link": "https://openreview.net/forum?id=5qg6JPSgCj", "pdf_link": "https://openreview.net/pdf?id=5qg6JPSgCj", "keywords": "Fokker-Planck Equations, Normalization Condition, Score Model, Physical Constraints.", "abstract": "Deep learning methods, which incorporate the PDE residual as loss function, have recently emerged to solve the Fokker-Planck equations. In the implement, proper normalization condition is required to avoid a trivial solution. However, specific network architectures may limit representation capacity, and soft regularization term requires careful balancing of multi-objective loss function. In this paper, we propose a novel framework: Fokker-Planck neural network (FPNN) that adopt a score PDE loss to decouple the score learning and the density normalization into two stages. Our method is mesh-free and causality-free, allowing for free-form network architectures to strictly satisfy normalization constraints by computing the partition function only once. We demonstrate the effectiveness on various high-dimensional steady-state Fokker-Planck (SFP) equations, achieving superior accuracy and over a 20$\\times$ speedup compared to state-of-the-art methods. Without any labeled data, FPNN achieve average relative errors of 11.36%, 13.87% and 12.72% for 4D Ring, 6D Unimodal and 6D Multi-modal problems respectively, requiring only 256, 200, and 980 parameters. Experimental results highlights the potential as a universal fast solver for handling more than 20-dimensional SFP equations, with great gains in efficiency, accuracy, memory and computational resource usage.", "title_embedding_index": 3752, "title_abs_embedding_index": 3777}, {"title": "Window-Based Hierarchical Dynamic Attention for Learned Image Compression", "link_suffix": "/forum?id=6j0GH40mFt", "link": "https://openreview.net/forum?id=6j0GH40mFt", "pdf_link": "https://openreview.net/pdf?id=6j0GH40mFt", "keywords": "Dynamic attention, learned image compression, adaptive entropy model.", "abstract": "Transformers have been successfully applied to learned image compression (LIC). In fact, dense self-attention is difficult to ignore contextual information that degrades the entropy estimations. To overcome this challenging problem, we incorporate dynamic attention in LIC for the first time.  The window-based dynamic attention (WDA) module is proposed to adaptively tune attention based on entropy distribution by sparsifying the attention matrix. Additionally, the WDA module is embedded into encoder and decoder transformation layers to refine attention in multi-scales, hierarchically extracting compact latent representations. Similarly, we propose the dynamic-reference entropy model (DREM) to adaptively select context information. This decreases the difficulty of entropy estimation by leveraging the relevant subset of decoded symbols, achieving an accurate entropy model. To the best of our knowledge, this is the first work employing dynamic attention for LIC and extensive experiments demonstrate the proposed method outperforms the state-of-the-art LIC methods.", "title_embedding_index": 3753, "title_abs_embedding_index": 3778}, {"title": "ReGen: Generative Robot Simulation via Inverse Design", "link_suffix": "/forum?id=EbCUbPZjM1", "link": "https://openreview.net/forum?id=EbCUbPZjM1", "pdf_link": "https://openreview.net/pdf?id=EbCUbPZjM1", "keywords": "generative simulation, robot, autonomous driving, large language model, inverse design", "abstract": "Simulation plays a key role in scaling robot learning and validating policies, but constructing simulations remains labor-intensive. In this paper, we introduce ReGen, a generative simulation framework that automates this process using inverse design. Given an agent's behavior (such as a motion trajectory or objective function) and its textual description, we infer the underlying scenarios and environments that could have caused the behavior.\nOur approach leverages large language models to construct and expand a graph that captures cause-and-effect relationships and relevant entities with properties in the environment, which is then processed to configure a robot simulation environment. Our approach supports (i) augmenting simulations based on ego-agent behaviors, (ii) controllable, counterfactual scenario generation, (iii) reasoning about agent cognition and mental states, and (iv) reasoning with distinct sensing modalities, such as braking due to faulty GPS signals. \nWe demonstrate our method in autonomous driving and robot manipulation tasks, generating more diverse, complex simulated environments compared to existing simulations with high success rates, and enabling controllable generation for corner cases. This approach enhances the validation of robot policies and supports data or simulation augmentation, advancing scalable robot learning for improved generalization and robustness.", "title_embedding_index": 3754, "title_abs_embedding_index": 3779}, {"title": "Flow-based Variational Mutual Information: Fast and Flexible Approximations", "link_suffix": "/forum?id=spDUv05cEq", "link": "https://openreview.net/forum?id=spDUv05cEq", "pdf_link": "https://openreview.net/pdf?id=spDUv05cEq", "keywords": "Mutual Information, Variational Methods, Normalizing Flows, Bayesian Optimal Experimental Design", "abstract": "Mutual Information (MI) is a fundamental measure of dependence between random variables, but its practical application is limited because it is difficult to calculate in many circumstances. Variational methods offer one approach by introducing an approximate distribution to create various bounds on MI, which in turn is an easier optimization problem to solve. In practice, the variational distribution chosen is often a Gaussian, which is convenient but lacks flexibility in modeling complicated distributions. In this paper, we introduce two new classes of variational estimators that extend the previous Gaussian-based variational estimators with Normalizing Flows. Our new estimators maintain many of the same theoretical guarantees while simultaneously enhancing the expressivity of the variational distribution. We experimentally verify that our new methods are effective on large MI problems where critic-based estimators, such as MINE and InfoNCE, are fundamentally limited. Furthermore, we compare against a diverse set of benchmarking tests to show that the flow-based estimators often perform as well, if not better, than the critic-based counterparts. Finally, we demonstrate how these estimators can be effectively utilized in the Bayesian Optimal Experimental Design setting for online sequential decision making.", "title_embedding_index": 3755, "title_abs_embedding_index": 3780}, {"title": "Uncertainty-Regularized Diffusional Subgoals for Hierarchical Reinforcement Learning", "link_suffix": "/forum?id=JNsac6zbg2", "link": "https://openreview.net/forum?id=JNsac6zbg2", "pdf_link": "https://openreview.net/pdf?id=JNsac6zbg2", "keywords": "Hierarchical Reinforcement Learning", "abstract": "Hierarchical reinforcement learning (HRL) aims to solve complex tasks by making decisions across multiple levels of temporal abstraction. However, off-policy training of hierarchical policies faces non-stationarity issues because the low-level policy is constantly changing, which makes it difficult for the high-level policy that generates subgoals to adapt. In this paper, we propose a conditional diffusion model-based approach for subgoal generation to mitigate these non-stationarity challenges. Specifically, we employ a Gaussian Process (GP) prior on subgoal generation as a surrogate distribution to regularize the diffusion policy and inform the diffusion process about uncertain areas in the action space. We introduce adaptive inducing states to facilitate sparse GP-based subgoal generation, enhancing sample efficiency and promoting better exploration in critical regions of the state space. Building on this framework, we develop an exploration strategy that identifies promising subgoals based on the learned predictive distribution of the diffusional subgoals. Experimental results demonstrate significant improvements in both sample efficiency and performance on challenging continuous control benchmarks compared to prior HRL methods.", "title_embedding_index": 3756, "title_abs_embedding_index": 3781}, {"title": "Adversarial Robustness of Graph Transformers", "link_suffix": "/forum?id=leFBpvYaPx", "link": "https://openreview.net/forum?id=leFBpvYaPx", "pdf_link": "https://openreview.net/pdf?id=leFBpvYaPx", "keywords": "Adversarial Robustness, Graph Transformer", "abstract": "Existing studies have shown that Message-Passing Graph Neural Networks (MPNNs) are highly susceptible to adversarial attacks. In contrast, despite the increasing importance of Graph Transformers (GTs), their robustness properties are unexplored. Thus, for the purpose of robustness evaluation, we design the first adaptive attacks for GTs. We provide general design principles for strong gradient-based attacks on GTs w.r.t. structure perturbations and instantiate our attack framework for five representative and popular GT architectures. Specifically, we study GTs with specialized attention mechanisms and Positional Encodings (PEs) based on random walks, pair-wise shortest paths, and the Laplacian spectrum. We evaluate our attacks on multiple tasks and threat models, including structure perturbations on node and graph classification and node injection for graph classification. Our results reveal that GTs can be catastrophically fragile in many cases. \nConsequently, we show how to leverage our adaptive attacks for adversarial training, substantially improving robustness.", "title_embedding_index": 3757, "title_abs_embedding_index": 3782}, {"title": "Discovering High-Quality Chess Puzzles Through One Billion Plays with Offline Reinforcement Learning", "link_suffix": "/forum?id=YKW98Icu1X", "link": "https://openreview.net/forum?id=YKW98Icu1X", "pdf_link": "https://openreview.net/pdf?id=YKW98Icu1X", "keywords": "Offline RL, education, chess, puzzle recommendation", "abstract": "Learning and skill mastery requires extensive and deliberate practice. In many learning settings, producing high-quality pedagogical materials can require a high level of domain expertise and be very time-consuming. Pedagogical materials often need to train students to engage in different thinking patterns. In some domains, such as chess, puzzles are used to help students practice their skills in calculating the next moves and recognizing known patterns on a board. Giving students a practice set of puzzles to help them learn different modes of thinking is challenging because the teacher needs to carefully balance between different motifs and how many look-ahead steps a student needs to perform. Popular online platforms like Chess.com and Lichess offer players millions of puzzles. Unlike chess tactics puzzles procured by human experts, where chess beginners can learn valuable insights, these puzzles are automatically generated and often regarded as having low pedagogical values. These platforms also rely on a heuristic to recommend puzzles to users for practice.\nUsing the user history data over an entire year, a total of 1.6 billion puzzle-solving histories, we learn the pedagogical value of a puzzle and how to automatically choose a set of puzzles to better support chess learners in a completely unstructured way using insights from offline reinforcement learning. We validate the quality of the puzzles discovered by our model by collecting annotation ratings from titled chess players. The success of our pipeline shows promise for a future where we can understand the pedagogical values of practice items in other domains like math or coding problems.", "title_embedding_index": 3758, "title_abs_embedding_index": 3783}, {"title": "Multi-agent cooperation through learning-aware policy gradients", "link_suffix": "/forum?id=GkWA6NjePN", "link": "https://openreview.net/forum?id=GkWA6NjePN", "pdf_link": "https://openreview.net/pdf?id=GkWA6NjePN", "keywords": "multi-agent learning, reinforcement learning, decentralized training, social dilemmas, cooperation, iterated prisoner's dilemma, melting pot", "abstract": "Self-interested individuals often fail to cooperate, posing a fundamental challenge for multi-agent learning. How can we achieve cooperation among self-interested, independent learning agents? Promising recent work has shown that in certain tasks cooperation can be established between `learning-aware' agents who model the learning dynamics of each other. Here, we present the first unbiased, higher-derivative-free policy gradient algorithm for learning-aware reinforcement learning, which takes into account that other agents are themselves learning through trial and error based on multiple noisy trials. We then leverage efficient sequence models to condition behavior on long observation histories that contain traces of the learning dynamics of other agents. Training long-context policies with our algorithm leads to cooperative behavior and high returns on standard social dilemmas, including a challenging environment where temporally-extended action coordination is required. Finally, we derive from the iterated prisoner's dilemma a novel explanation for how and when cooperation arises among self-interested learning-aware agents.", "title_embedding_index": 3759, "title_abs_embedding_index": 3784}, {"title": "Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement", "link_suffix": "/forum?id=2ET561DyPe", "link": "https://openreview.net/forum?id=2ET561DyPe", "pdf_link": "https://openreview.net/pdf?id=2ET561DyPe", "keywords": "Few-Class, lightweight, small neural network, benchmark, scaling law, image similarity, convolutional neural network, CNN, transformer", "abstract": "We propose Few-Class Arena (FCA), as a unified benchmark with focus on testing efficient image classification models for few classes. A wide variety of benchmark datasets with many classes (80-1000) have been created to assist Computer Vision architectural evolution. An increasing number of vision models are evaluated with these many-class datasets. However, real-world applications often involve substantially fewer classes of interest (2-10). This gap between many and few classes makes it difficult to predict performance of the few-class applications using models trained on the available many-class datasets. To date, little has been offered to evaluate models in this Few-Class Regime. We conduct a systematic evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes, and test a wide spectrum of Convolutional Neural Networks and Transformer architectures over ten datasets by using our newly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset difficulty and a more efficient selection of models, we incorporate a difficulty measure as a function of class similarity. FCA offers a new tool for efficient machine learning in the Few-Class Regime, with goals ranging from a new efficient class similarity proposal, to lightweight model architecture design, to a new scaling law. FCA is user-friendly and can be easily extended to new models and datasets, facilitating future research work. Our benchmark is available athttps://github.com/fewclassarena/fca.", "title_embedding_index": 3760, "title_abs_embedding_index": 3785}, {"title": "Sequence-to-sequence modeling for Temporal Reconstruction of Cellular Events", "link_suffix": "/forum?id=PHESUVacAw", "link": "https://openreview.net/forum?id=PHESUVacAw", "pdf_link": "https://openreview.net/pdf?id=PHESUVacAw", "keywords": "self-supervised learning, single cell RNAseq, generative modeling, temporal sequence modeling", "abstract": "Single-cell omics technologies capture molecular snapshots of cells, while most\nbiological processes unfold over time. Accurately predicting single-cell gene ex-\npression at unmeasured time points enhances our understanding of these processes,\nreducing costs and experimental effort by enabling the interpolation and extrap-\nolation of observed data. This helps study continuous development, response to\nperturbations, and disease progression. To address this problem, we propose an\nencoder-decoder transformer architecture for Temporal Reconstruction of Cellular\nEvents (TRACE). TRACE models gene expression generation as a sequence-to-\nsequence generation task by learning to transform a sequence of genes from a\nsource condition (e.g., previous time) into a sequence of genes in a target condition\n(e.g., next time point). TRACE decoder learns to generate gene tokens of the target\ncondition by iteratively unmaking tokens in the target sequence, overcoming the dis-\ncordance between autoregressive modeling and the non-sequential nature of gene\nexpression data. We evaluate TRACE both quantitatively and qualitatively on three\ndatasets, covering a range of tasks and biological scenarios. TRACE outperforms\nexisting models in generalizing across in-distribution and out-of-distribution tasks\nfor temporal prediction. Furthermore, we demonstrate the biological relevance of\nthe cell embeddings learned by TRACE by delineating activation-dependent cell\nstages in immune cells, measured across multiple time points. Our findings suggest\nthat TRACE can enhance in silico hypothesis generation, improving our under-\nstanding and prediction of cellular changes over time. This ultimately facilitates\ndisease understanding and supports the design of cost-effective experiments for\nbiological discovery.", "title_embedding_index": 3761, "title_abs_embedding_index": 3786}, {"title": "BLIPEE:  Fast and Robust BLIP with Adversarially Trained Early Exits", "link_suffix": "/forum?id=ERcGlGIM2D", "link": "https://openreview.net/forum?id=ERcGlGIM2D", "pdf_link": "https://openreview.net/pdf?id=ERcGlGIM2D", "keywords": "Early Exits; Multimodal model", "abstract": "In recent years, Vision-Language Models (VLMs) have shown remarkable performance improvements in vision-language tasks. However, their large size poses challenges for real-world applications where inference latency is a concern. To tackle this issue, we propose employing Early Exit (EE) strategies in VLM. However, training exit classifiers in VLMs is challenging, particularly with limited labeled training data. To address this, we introduce BLIPEE, an adversarial training approach within a GAN-based framework. Here, each exit consists of a transformer layer and a classifier, and the transformer layer is adversarially trained to produce feature representations similar to the final layer, while a feature classifier serves as the discriminator. Our method focuses on performing input-adaptive inference that mitigates the overthinking issue and increases inference speed. Experimental results demonstrate the effectiveness of our approach in enhancing accuracy and model robustness by mitigating overthinking and the phenomenon of mid-crisis that we highlight. The anonymized source code is available athttps://anonymous.4open.science/status/BLIPEE-3ED3.", "title_embedding_index": 3762, "title_abs_embedding_index": 3787}, {"title": "Query-Efficient Planning with Language Models", "link_suffix": "/forum?id=OPdmIxdkPb", "link": "https://openreview.net/forum?id=OPdmIxdkPb", "pdf_link": "https://openreview.net/pdf?id=OPdmIxdkPb", "keywords": "llm, planning, adaptive, query-efficient", "abstract": "Planning in complex environments requires an agent to efficiently query a world model to find a feasible sequence of actions from start to goal.\nRecent work has shown that Large Language Models (LLMs), with their rich prior knowledge and reasoning capabilities, can potentially help with planning by searching over promising states and adapting to feedback from the world. \nIn this paper, we propose and study two fundamentally competing frameworks that leverage LLMs for query-efficient planning. \nThe first uses LLMs as a heuristic within a search-based planner to select promising nodes to expand and propose promising actions. \nThe second uses LLMs as a generative planner to propose an entire sequence of actions, query the world model, and adapt based on feedback.\nWe show that while both approaches improve upon comparable baselines, using an LLM as a generative planner results in significantly fewer interactions. Our key finding is that the LLM as a planner can more rapidly adapt its planning strategies based on immediate feedback than LLM as a heuristic. We present evaluations and ablations on Robotouille and PDDL planning benchmarks and discuss connections to existing theory on query-efficient planning algorithms.", "title_embedding_index": 3763, "title_abs_embedding_index": 3788}, {"title": "Transformers Struggle to Learn to Search Without In-context Exploration", "link_suffix": "/forum?id=9cQB1Hwrtw", "link": "https://openreview.net/forum?id=9cQB1Hwrtw", "pdf_link": "https://openreview.net/pdf?id=9cQB1Hwrtw", "keywords": "search, reasoning, transformers, scaling laws, mechanistic interpretability, circuit analysis", "abstract": "Search is an ability fundamental in many important tasks, and recent studies have shown that large-language models (LLMs) struggle to perform search robustly. It is unknown whether this inability is due to a lack of data, insufficient model parameters, or fundamental limitations of the transformer architecture. In this work, we use graph connectivity as a testbed to generate effectively limitless high-coverage data to train small transformers and test whether they can learn to perform search. We find that, under specific conditions on the training distribution, the transformer is able to learn to search.We analyze the algorithm that the transformer has learned through a novel mechanistic interpretability technique that enables us to extract the computation graph from the trained model. We find that for each vertex in the input graph, transformers compute the set of vertices reachable from that vertex. Each layer then progressively expands these sets, allowing the model to search over a number of vertices exponential in the number of layers.However, we find that as the input graph size increases, the transformer has greater difficulty in learning the task. This difficulty is not resolved even as the number of parameters is increased, suggesting that simply increasing the scale of LLMs will not lead to robust search abilities.Finally, we show that by loosening the task to allow the model toexplorethe graphin-context, allowing the model to visit vertices that do not necessarily lead to the goal and backtrack, the transformer is able to more easily learn to search robustly.", "title_embedding_index": 3764, "title_abs_embedding_index": 3789}, {"title": "Self-Conditioned Diffusion Model for Consistent Human Image and Video Synthesis", "link_suffix": "/forum?id=1fC4ytCAgb", "link": "https://openreview.net/forum?id=1fC4ytCAgb", "pdf_link": "https://openreview.net/pdf?id=1fC4ytCAgb", "keywords": "Diffusion model, human image generation", "abstract": "Consistent human-centric image and video synthesis aims to generate images or videos with new poses while preserving appearance consistency with a given reference image, which is crucial for low-cost visual content creation. Recent advancements based on diffusion models typically rely on separate networks for reference appearance feature extraction and target visual generation, leading to inconsistent domain gaps between references and targets. In this paper, we frame the task as a spatially-conditioned inpainting problem, where the target image is inpainted to maintain appearance consistency with the reference. This approach enables the reference features to guide the generation of pose-compliant targets within a unified denoising network, thereby mitigating domain gaps. Additionally, to better maintain the reference appearance information, we impose a causal feature interaction framework, in which reference features can only query from themselves, while target features can query appearance information from both the reference and the target.\nTo further enhance computational efficiency and flexibility, in practical implementation, we decompose the spatially-conditioned generation process into two stages: reference appearance extraction and conditioned target generation. Both stages share a single denoising network, with interactions restricted to self-attention layers. This proposed method ensures flexible control over the appearance of generated human images and videos. By fine-tuning existing base diffusion models on human video data, our method demonstrates strong generalization to unseen human identities and poses without requiring additional per-instance fine-tuning. Experimental results validate the effectiveness of our approach, showing competitive performance compared to existing methods for consistent human image and video synthesis.", "title_embedding_index": 3765, "title_abs_embedding_index": 3790}, {"title": "Imbalance-Regularized LoRA: A Plug-and-Play Method for Improving Fine-Tuning of Foundation Models", "link_suffix": "/forum?id=XIcR6JTe9D", "link": "https://openreview.net/forum?id=XIcR6JTe9D", "pdf_link": "https://openreview.net/pdf?id=XIcR6JTe9D", "keywords": "Low-Rank Adaptation, Fine-Tuning", "abstract": "Low-Rank Adaptation (LoRA) is an effective fine-tuning algorithm for large models, enabling efficient adaptation with fewer trainable parameters. Despite its success, there remains significant potential for improving LoRA's performance. In this paper, we introduce iLoRA (Imbalance-Regularized LoRA), which enhances LoRA by incorporating a regularization term to address the imbalance in forward propagation. This regularization maintains an imbalance between matrices $\\mathbf{A}$ and $\\mathbf{B}$, ensuring stable activation variance independent of dimension. Specifically, we first analyze forward dynamics, observe this imbalance in stable training, and introduce imbalanced regularization. Further, by combining this with preconditioning techniques (Zhang and Pilanci, 2024), we propose $\\pi$LoRA (Preconditioned iLoRA), which improves the backpropagation process. Our method is a plug-and-play algorithm that requires only minor modifications to the existing code and incurs negligible additional computational overhead. Finally, experiments on large language models and text-to-image models demonstrate that iLoRA and $\\pi$LoRA significantly outperform existing LoRA and preconditioned LoRA methods.", "title_embedding_index": 3766, "title_abs_embedding_index": 3791}, {"title": "Is uniform expressivity too restrictive? Towards efficient expressivity of GNNs", "link_suffix": "/forum?id=lsvGqR6OTf", "link": "https://openreview.net/forum?id=lsvGqR6OTf", "pdf_link": "https://openreview.net/pdf?id=lsvGqR6OTf", "keywords": "Graph Neural Networks, Expressivity, Efficiency, Activation Function, Queries, Logic", "abstract": "Uniform expressivity  guarantees that a Graph Neural Network (GNN) can express a query without the parameters depending on the size of the input graphs. This property is desirable in applications in order to  have number of trainable parameters  that is independent of the size of the input graphs. Uniform expressivity of the two variable guarded fragment (GC2) of first order logic is a well-celebrated result for Rectified Linear Unit (ReLU) GNNs [Barcelo &. Al, 2020]. In this article, we prove that uniform expressivity of GC2 queries is not possible for GNNs with a wide class of Pfaffian activation functions (including the sigmoid and $\\tanh$), answering a question formulated by [Grohe, 2021]. We also show that despite these limitations, many of those GNNs can still efficiently express GC2 queries in a way that the number of parameters remains logarithmic on the maximal degree of the input graphs. Furthermore, we demonstrate that a log-log dependency on the degree is achievable for a certain choice of activation function. This shows that uniform expressivity can be successfully relaxed by covering large graphs appearing in practical applications. Our experiments illustrates that our theoretical estimates hold in practice.", "title_embedding_index": 3767, "title_abs_embedding_index": 3792}, {"title": "Training Neural Networks as Recognizers of Formal Languages", "link_suffix": "/forum?id=aWLQTbfFgV", "link": "https://openreview.net/forum?id=aWLQTbfFgV", "pdf_link": "https://openreview.net/pdf?id=aWLQTbfFgV", "keywords": "neural network, formal language theory, transformer, rnn, lstm", "abstract": "Characterizing the computational power of neural network architectures in terms of formal language theory remains a crucial line of research, as it describes lower and upper bounds on the reasoning capabilities of modern AI. This paper addresses the fact that when empirically testing these bounds, there is often a disconnect between experiments and the formal claims they are meant to support. The problem is that formal language theory pertains specifically to recognizers: machines that receive a string as input and classify whether it belongs to a language. However, many experiments do not test neural networks as recognizers, but use proxy tasks that are similar in only an informal sense, such as language modeling or sequence-to-sequence transduction. We correct this discrepancy by training and evaluating neural networks directly as binary classifiers of strings, using a general method that can be applied to a wide variety of formal languages. We propose a new algorithm for length-controlled sampling of strings from regular languages that is asymptotically much more efficient than previous methods. We provide results on a variety of formal languages for three neural architectures: a simple RNN, an LSTM, and a causally-masked transformer. We find that the RNN and LSTM often outperform the transformer, and that auxiliary training objectives such as language modeling do not consistently affect accuracy positively or negatively. Our contributions will facilitate future work on empirically testing theoretical language recognition claims.", "title_embedding_index": 3768, "title_abs_embedding_index": 3793}, {"title": "MLPs for NLP: Towards Discovering Inductive Bias From Scratch", "link_suffix": "/forum?id=dDLGZTKZYZ", "link": "https://openreview.net/forum?id=dDLGZTKZYZ", "pdf_link": "https://openreview.net/pdf?id=dDLGZTKZYZ", "keywords": "Language Models, Architecture Design, MLP, Inductive Bias, Scaling Laws", "abstract": "The recent rise of large language models has been fueled by scale. More data, more compute, and bigger models have consistently led to better performance. This scaling paradigm has been applied most notably to the transformer architecture, which is especially conducive to training parallelization and sequence modeling. In this work, we ask what happens if we apply the power of scale to the simplest possible class of neural network: the multi-layer perceptron (MLP). Specifically, we train MLPs to perform next-token prediction on billions of tokens of text. Indeed, their performance consistently improves with scale, though vanilla MLPs are still clearly inferior to transformers for this task, especially because their parameter count grows with the length of the input sequences. We then perform a mechanistic analysis of the trained models, and identify a consistent emergent structure: most neurons in the first hidden layer either perform arbitrary linear functions over a small look-back window, or low-frequency functions over the entire context. These neuron types recall $n$-gram and bag-of-words techniques from classical statistical language modeling. Using the discrete cosine transform, we define a unified way of reparameterizing these neuron types such that the number of parameters per neuron does not depend on the sequence length.", "title_embedding_index": 3769, "title_abs_embedding_index": 3794}, {"title": "Loss2Net: Loss Meta-Learning for Regression with A-priori Unknown Metrics", "link_suffix": "/forum?id=zhxATDLAmJ", "link": "https://openreview.net/forum?id=zhxATDLAmJ", "pdf_link": "https://openreview.net/pdf?id=zhxATDLAmJ", "keywords": "loss meta-learning, loss-metric mismatch, system management, unknown metric, transfer learning", "abstract": "There exist many practical applications where regression tasks must cope with a generally overseen problem: the output variable to be computed, which is often a decision variable, impacts the  performance metric to optimize in a manner that is not known a priori. This challenge translates into a loss-metric mismatch, which makes standard loss functions such as Mean Square Error (MSE) not suitable because they significantly hinder the final performance. While this problem is of crucial importance in, e.g., many engineering and economic applications, the literature in meta-learning of loss functions has focused on other problems, such as classification or few-shot learning tasks. In this work, we aim at closing this research gap by proposing a model that can handle common situations in real systems where the unknown prediction-metric relationship is time-correlated, non-differentiable, or depends on multiple intertwined predictions. We present a novel loss meta-learning architecture for regression, named Loss2Net, which is able to (i) jointly learn the actual regressor and the loss function that it should minimize, directly from system responses; (ii) it does so without any assumption on the loss function structure; (iii) it provides a manner to learn non-differentiable and multi-dimensional loss functions from entangled performance metrics. Detailed experiments for power grid and telecommunications infrastructure optimization, grounded on real-world measurement data, demonstrate how Loss2Net can effectively learn unidentified loss functions.", "title_embedding_index": 3770, "title_abs_embedding_index": 3795}, {"title": "Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping", "link_suffix": "/forum?id=NKotdPUc3L", "link": "https://openreview.net/forum?id=NKotdPUc3L", "pdf_link": "https://openreview.net/pdf?id=NKotdPUc3L", "keywords": "Stochastic Optimization, Heavy-Tailed Noises", "abstract": "Recently, the study of heavy-tailed noises in first-order nonconvex stochastic optimization has gotten a lot of attention since it was recognized as a more realistic condition as suggested by many empirical observations. Specifically, the stochastic noise, that is the difference between the stochastic and true gradient, is considered only to have a finite $\\mathfrak{p}$-th moment where $\\mathfrak{p}\\in\\left(1,2\\right]$ instead of assuming it always satisfies the classical finite variance assumption. To deal with this more challenging setting, people have proposed different new algorithms and proved them to converge at an optimal $\\mathcal{O}(T^{\\frac{1-\\mathfrak{p}}{3\\mathfrak{p}-2}})$ rate for smooth objectives. Notably, all these new-designed algorithms are based on the same technique \u2013 gradient clipping. Naturally, one may want to know whether the clipping method is a necessary ingredient and the only way to guarantee convergence under heavy-tailed noises. In this work, by revisiting the existing Normalized Stochastic Gradient Descent with Momentum (NSGDM) algorithm, we provide the first convergence result under heavy-tailed noises but without gradient clipping. Concretely, under careful analysis, we prove that NSGDM can achieve the optimal $\\mathcal{O}(T^{\\frac{1-\\mathfrak{p}}{3\\mathfrak{p}-2}})$ rate even under the relaxed smooth condition. More interestingly, we also establish the first $\\mathcal{O}(T^{\\frac{1-\\mathfrak{p}}{2\\mathfrak{p}}})$ convergence rate in the case where the tail index $\\mathfrak{p}$ is unknown in advance, which is arguably the common scenario in practice.", "title_embedding_index": 3771, "title_abs_embedding_index": 3796}, {"title": "StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text", "link_suffix": "/forum?id=26oSbRRpEY", "link": "https://openreview.net/forum?id=26oSbRRpEY", "pdf_link": "https://openreview.net/pdf?id=26oSbRRpEY", "keywords": "Text-To-Video; Diffusion Models; Long Video; Autoregressive", "abstract": "Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, simplifying the process of producing diverse and individual content.\n    Current methods excel in generating short videos (up to 16s), but produce hard-cuts when naively extended to long video synthesis.\n    To overcome these limitations, we present $\\textit{StreamingT2V}$, an autoregressive method that generates long videos of \\textbf{up to 2 minutes or longer} with seamless transitions.\n    The key components are:\n    (i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the preceding chunk via an attentional mechanism, leading to consistent chunk transitions, \n    (ii) a long-term memory block called appearance preservation module (APM), which extracts high-level scene and object features from the first video chunk to prevent the model from forgetting the initial scene,  and (iii) a randomized blending approach that allows for the autoregressive application of a video enhancer on videos of indefinite length, ensuring consistency across chunks. \n    Experiments show that StreamingT2V produces high motion amount, while competing methods suffer from video stagnation when applied naively in an autoregressive fashion.\n    Thus, we propose with StreamingT2V a high-quality seamless text-to-long video generator, surpassing competitors in both consistency and motion.", "title_embedding_index": 3772, "title_abs_embedding_index": 3797}, {"title": "Learnable Stability-Aware Unstructured Grid Coarsening Using Graph Neural Networks for Accelerated Physics Simulations", "link_suffix": "/forum?id=TSTgP4W3ga", "link": "https://openreview.net/forum?id=TSTgP4W3ga", "pdf_link": "https://openreview.net/pdf?id=TSTgP4W3ga", "keywords": "Graph Neural Networks, differentiable solvers, numerical modelling, grid coarsening, upscaling", "abstract": "Efficient simulations of complex physical systems described by partial differential equations (PDE) require computational methods that can reduce the resource demands without sacrificing the accuracy. Traditionally, this is achieved by ``upscaling'' the simulation grids or by aggregating cells based on a priori information. Here, we introduce a novel framework based on graph neural networks (GNN) for learnable self-supervised differentiable coarsening of unstructured computational grids. We leverage graph-based representation of the physical system and offer a graph coarsening method which preserves the underlying physical properties together with the stability of the chosen numerical scheme. This is achieved by minimizing the error between the output of the simulations using coarsened and original graph. We demonstrate the approach on several example differential equations, modeling sub-surface flow and wave propagation. We demonstrate that the model exhibits ability to maintain high fidelity in simulation outputs even after 95% reduction on the nodes, significantly reducing computational overhead. We also show that the model exhibits generalizability to unseen scenarios, thereby outperforming the baselines. Thus, the developed approach demonstrates the ability to accelerate simulation without comprising accuracy and hence has potential for accelerating physical simulations in various domains.", "title_embedding_index": 3773, "title_abs_embedding_index": 3798}, {"title": "Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems", "link_suffix": "/forum?id=S1Bv3068Xt", "link": "https://openreview.net/forum?id=S1Bv3068Xt", "pdf_link": "https://openreview.net/pdf?id=S1Bv3068Xt", "keywords": "Backdoor attacks, Large language models, Autonomous agents, Robotics", "abstract": "Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework forBackdoorAttacks againstLLM-basedDecision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms:word injection,scenario manipulation, andknowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.", "title_embedding_index": 3774, "title_abs_embedding_index": 3799}]