[{"title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs", "link_suffix": "/forum?id=lOTfiKt4Gc", "link": "https://openreview.net/forum?id=lOTfiKt4Gc", "pdf_link": "https://openreview.net/pdf?id=lOTfiKt4Gc", "keywords": "Large Language Models, Jailbreak, Red-teaming, Safety", "abstract": "As Large Language Models (LLMs) become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments, including the European Union, have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for model developers and testers. \nThere remains a gap in translating these broad requirements into actionable testing questions to verify LLM compliance.To address this challenge, we introduce GUARD (\\textbf{G}uideline \\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and Jailbreak \\textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD assigns LLMs to play different roles, enabling the collaborative and automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'', creating scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a comprehensive compliance report, delineating the extent of adherence and highlighting any violations.We have empirically validated the effectiveness of GUARD on five LLMs, including Vicuna-13B, LongChat-7B, Llama-2-7B, GPT-3.5, and GPT-4, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD transfers these diagnostics to vision-language models (MiniGPT-v2 and Gemini Vision Pro), demonstrating its versatility and providing valuable insights for the development of safer, more reliable LLM-based applications across diverse modalities.", "title_embedding_index": 21450, "title_abs_embedding_index": 21475}, {"title": "Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation", "link_suffix": "/forum?id=ykD8a9gJvy", "link": "https://openreview.net/forum?id=ykD8a9gJvy", "pdf_link": "https://openreview.net/pdf?id=ykD8a9gJvy", "keywords": "generative keyframe interpolation, image-to-video diffusion models", "abstract": "We present a method for generating video sequences with coherent motion between a pair of input keyframes. We adapt a pretrained large-scale image-to-video diffusion model (originally trained to generate videos moving forward in time from a single input image) for keyframe interpolation, i.e., to produce a video between two input frames. We accomplish this adaptation through a lightweight fine-tuning technique that produces a version of the model that instead predicts videos moving backwards in time from a single input image. This model (along with the original forward-moving model) is subsequently used in a dual-directional diffusion sampling process that combines the overlapping model estimates starting from each of the two keyframes. Our experiments shows that our method outperforms both existing diffusion-based methods and traditional frame interpolation techniques.", "title_embedding_index": 21451, "title_abs_embedding_index": 21476}, {"title": "Safety Alignment Shouldn't Be Complicated", "link_suffix": "/forum?id=9H91juqfgb", "link": "https://openreview.net/forum?id=9H91juqfgb", "pdf_link": "https://openreview.net/pdf?id=9H91juqfgb", "keywords": "Safety Alignment, Alignment Tax, Safety-critical Neurons, Large Language Models (LLMs)", "abstract": "As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe and aligned responses is a pressing need. Previous research on alignment has largely focused on general instruction-following but has often overlooked the unique properties and challenges of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment should teach an otherwise unsafe model to choose the correct reasoning direction - interpreted as a specialized binary classification task - and incorporate a refusal mechanism with multiple reserved fallback options. Furthermore, through SSAH, we hypothesize that safety guardrails in LLMs can be established by just a small number of essential components. To verify this, we conduct an ablation study and successfully identify four types of attribute-critical components in safety-aligned LLMs: Exclusive Safety Unit (ESU), Exclusive Utility Unit (EUU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components \\textbf{(7.5%)} during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Additionally, we show that leveraging redundant units \\textbf{(20%)} in the pre-trained model as an ``alignment budget'' can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated. We believe this work contributes to the foundation of efficient and scalable safety alignment for future LLMs.", "title_embedding_index": 21452, "title_abs_embedding_index": 21477}, {"title": "Discretized Quadratic Integrate-and-Fire Neuron Model for Direct Training of Spiking Neural Networks", "link_suffix": "/forum?id=mJ4mgYjDru", "link": "https://openreview.net/forum?id=mJ4mgYjDru", "pdf_link": "https://openreview.net/pdf?id=mJ4mgYjDru", "keywords": "Machine Learning, Neuromorphic Computing, Deep Learning, Computer Vision", "abstract": "Spiking Neural Networks (SNNs) present a promising alternative to traditional Artificial Neural Networks for their potential energy savings. Conventional approaches that use SNNs typically employ a discretized neuron model, which loses the complex, non-linear dynamics found in biological neurons. This can potentially lead to a decrease in model performance and an increase in energy consumption through extraneous spiking activity. To address this issue, we introduce a discretized Quadratic Integrate-and-Fire (QIF) neuron model. The QIF model maintains complex non-linear dynamics, addressing limitations in the widely adopted discretization of the Leaky Integrate-and-Fire (LIF) neurons, which utilize linear dynamics. Our model is evaluated against state-of-the-art approaches on static datasets (CIFAR-10, CIFAR-100) and neuromorphic datasets (CIFAR-10 DVS, N-Caltech-101, N-Cars, DVS128-Gesture), demonstrating competitive performance and improved accuracy. The QIF model achieves a significant energy reduction of up to $123$% compared to the same model using the LIF neuron model. Additionally, we observe smoother loss landscapes and larger local minima with the QIF model, leading to quicker training convergence. Our findings suggest that the QIF neuron model offers a promising alternative to the widely adopted LIF neuron model.", "title_embedding_index": 21453, "title_abs_embedding_index": 21478}, {"title": "Orient Anything", "link_suffix": "/forum?id=bIf1YXztnD", "link": "https://openreview.net/forum?id=bIf1YXztnD", "pdf_link": "https://openreview.net/pdf?id=bIf1YXztnD", "keywords": "3d orientation, shape analysis, 3d deep learning, geometric deep learning", "abstract": "Orientation estimation is a fundamental task in 3D shape analysis which consists of estimating a shape's orientation axes: its side-, up-, and front-axes. Using this data, one can rotate a shape into canonical orientation, where its orientation axes are aligned with the coordinate axes. Developing an orientation algorithm that reliably estimates complete orientations of general shapes remains an open problem. We introduce a two-stage orientation pipeline that achieves state of the art performance on up-axis estimation and further demonstrate its efficacy on full-orientation estimation, where one seeks all three orientation axes. Unlike previous work, we train and evaluate our method on all of Shapenet rather than a subset of classes. We motivate our engineering contributions by theory describing fundamental obstacles to orientation estimation for rotationally-symmetric shapes, and show how our method avoids these obstacles.", "title_embedding_index": 21454, "title_abs_embedding_index": 21479}, {"title": "M3C: a Multi-Domain Multi-Objective, Mixed-Modality Framework for Cost-Effective, Industry Scale Recommendation", "link_suffix": "/forum?id=VCZ1o8gFny", "link": "https://openreview.net/forum?id=VCZ1o8gFny", "pdf_link": "https://openreview.net/pdf?id=VCZ1o8gFny", "keywords": "Recommendation, efficiency, data consolidation", "abstract": "The ever-expanding landscape of products, surfaces, policies, and regulations poses\nsignificant challenges for recommendation systems, leading to data fragmentation\nand prohibitive hikes in infrastructure costs. To address these challenges, we\npropose M3C, a holistic co-design of model, data and efficiency strategies. M3C\n(1) partitions the recommendation space to allow better representation learning\nand encourage knowledge sharing within a subspace; (2) covers each partition\nusing a hierarchy of foundational and vertical networks tailored to handle multi-\ndomain, multi-objective tasks with mixed-modal inputs; (3) forms a unified data\nrepresentation that utilizes heterogeneous signals across domains, objectives and\noptimization goals to alleviate data fragmentation, label sparsity, and to enhance\nknowledge sharing; (4) improves execution efficiency and lowers costs with a suite\nof stability and throughput optimizations. We show that across a diverse set of tasks\non public and industry datasets, M3C delivers up to 1% lower LogLoss compared\nto 10 state-of-the-art baselines, while improving system efficiency by up to 20%.\nFurthermore, in a large-scale industry setting our deployment of M3C has resulted\nin 7% top-line metrics improvement in online tests with 10% capacity savings.", "title_embedding_index": 21455, "title_abs_embedding_index": 21480}, {"title": "Asking Specifically Instead of Ambiguously to Your GPT Improves Image Caption", "link_suffix": "/forum?id=vwENIgfZdQ", "link": "https://openreview.net/forum?id=vwENIgfZdQ", "pdf_link": "https://openreview.net/pdf?id=vwENIgfZdQ", "keywords": "vision-language models, image captioning", "abstract": "The advances in large vision-language models (VLMs) have sparked a growing interest in generating accurate, complete, and user-friendly image captions to enhance downstream multi-modality tasks such as text-to-image generation, text-driven object detection, and grounding. However, current VLM-based image captioning methods often miss important details, recognize incorrect objects or relationships, and deliver suboptimal captions for downstream applications. One primary reason for this issue is the ambiguous prompts typically used, such as \"describe this image in detail,\" which fail to guide the VLM's focus on specific elements within the image. To address this, we extensively explore the difference between using ambiguous prompts and decomposing them into a series of specific questions. We find that asking a series of targeted element-specific questions significantly enhances the attention of VLMs to important objects, the consistency of the answers under repeated questions, and the alignment with their training data distribution. Building on this insight, we introduce ASSIST, a method that systematically decomposes image caption prompts into a sequence of focused questions corresponding to distinct image elements.We annotated 100k images using GPT-4V with this approach and fine-tuned a LLAVA model, resulting in a captioner that greatly improves caption accuracy and quality. Our fine-tuned model recognizes $\\times 1.5$ more correct objects and achieves $\\times1.5$ higher precision in describing them on the COCO benchmark compared to vague prompting methods. Additionally, our method produces element-specific answers that can be efficiently organized into graph structures, benefiting tasks like open-vocabulary object detection and image generation. This leads to significant improvements in the accuracy, precision, and mIoU of state-of-the-art detection models, with precision scores increasing by $\\times 1.7$ over previous methods. Experiments across diverse scenarios and benchmarks validate the effectiveness of ASSIST. All code, datasets, and models will be made publicly available.", "title_embedding_index": 21456, "title_abs_embedding_index": 21481}, {"title": "Decentralized Federated Learning Over Noisy Labels: A Majority Voting Method", "link_suffix": "/forum?id=t8hMqAn8ZG", "link": "https://openreview.net/forum?id=t8hMqAn8ZG", "pdf_link": "https://openreview.net/pdf?id=t8hMqAn8ZG", "keywords": "decentralized federated learning, distributed learning, federated learning, majority voting, label-noise learning", "abstract": "Contrary to centralized federated learning (CFL), decentralized federated learning (DFL) allows clients to cooperate in training their local models without relying on a central parameter server. As different clients have varying annotation skills and preferences, noisy labels are inevitable in decentralized data ownership. In centralized learning (CL) and CFL settings, learning from noisy labels has been extensively explored; however, such methods cannot be directly applied in DFL settings due to limited computational resources or privacy requirements. This paper introduces DFLMV \\textit{(majority voting based decentralized federated learning)}, a general DFL framework for learning from noisy data without relying on any assumptions about local client noise models while maintaining data privacy for all clients. Specifically, (1) Clients first use traditional DFL to train their local models until they become stable. (2) Clients use each of their neighbors' models to make a prediction of every data point in their training datasets, then correct the labels based on majority voting. (3) Clients further fine-tune their models based on their updated training dataset. A theoretical analysis of DFLMV is also provided. Extensive experiments conducted on MNIST, Fashion-MNIST, CIFA-10, CIFAR-10N, CIFAR-100N, Clothing1M, and ANIMAL-10N validate the effectiveness of our proposed approach at various noise levels and different data settings in mitigating the adverse effects of noisy labels.", "title_embedding_index": 21457, "title_abs_embedding_index": 21482}, {"title": "Auction-Based Regulation for Artificial Intelligence", "link_suffix": "/forum?id=06GH83hDIv", "link": "https://openreview.net/forum?id=06GH83hDIv", "pdf_link": "https://openreview.net/pdf?id=06GH83hDIv", "keywords": "Regulation, Mechanisms, Auctions, Artificial Intelligence", "abstract": "In an era of \"moving fast and breaking things\", regulators have moved slowly to pick up the safety, bias, and legal pieces left in the wake of broken Artificial Intelligence (AI) deployment. Since AI models, such as large language models, are able to push misinformation and stoke division within our society, it is imperative for regulators to employ a framework that mitigates these dangers and ensures user safety. While there is much-warranted discussion about how to address the safety, bias, and legal woes of state-of-the-art AI models, the number of rigorous and realistic mathematical frameworks to regulate AI safety is lacking. We take on this challenge, proposing an auction-based regulatory mechanism that provably incentivizes model-building agents (i) to deploy safer models and (ii) to participate in the regulation process. We provably guarantee, via derived Nash Equilibria, that each participating agent's best strategy is to submit a model safer than a prescribed minimum-safety threshold. Empirical results show that our regulatory auction boosts safety and participation rates by 20% and 15% respectively, outperforming simple regulatory frameworks that merely enforce minimum safety standards.", "title_embedding_index": 21458, "title_abs_embedding_index": 21483}, {"title": "Redundant Queries in DETR-Based 3D Detection Methods: Unnecessary and Prunable", "link_suffix": "/forum?id=Na0OnR5WxD", "link": "https://openreview.net/forum?id=Na0OnR5WxD", "pdf_link": "https://openreview.net/pdf?id=Na0OnR5WxD", "keywords": "DETR, 3D object detection, Query pruning", "abstract": "Query-based models are extensively used in 3D object detection tasks, with a wide range of pre-trained checkpoints readily available online. However, despite their popularity, these models often require an excessive number of object queries, far surpassing the actual number of objects to detect. The redundant queries result in unnecessary computational and memory costs. In this paper, we find that not all queries contribute equally --- a significant portion of queries have a much smaller impact compared to others. Based on this observation, we propose an embarrassingly simple approach that Gradually Prunes Queries (GPQ) according to classification scores that queries generated. Compared to existing pruning methods, our method introduces no additional learnable parameters. GPQ is easy to implement to any query-based method by integrating it in after-training fine-tune using an existing checkpoint. By using our method, one can easily generate several different models with fewer queries using an checkpoint has exicessive queries. Experiments on various advanced 3D detectors show that GPQ effectively reduces redundant queries while maintaining performance, and it achieves at most a 67.86% reduction in FLOPs with a 76.38% decrease in inference time after deployment. The code will be available soon.", "title_embedding_index": 21459, "title_abs_embedding_index": 21484}, {"title": "Towards Accurate Deep Learning Model Selection: A Calibrated Metric Approach", "link_suffix": "/forum?id=lvHHWDJCcr", "link": "https://openreview.net/forum?id=lvHHWDJCcr", "pdf_link": "https://openreview.net/pdf?id=lvHHWDJCcr", "keywords": "Model Selection, Deep Learning, Calibration, Metric, Neural Networks, Deep Click-Through Rate Prediction Models, Stock Return Prediction Models", "abstract": "The adoption of deep learning across various fields has been extensive, yet the methods for reliably evaluating the performance of deep learning pipelines remain underdeveloped. Typically, with the increased use of large datasets and complex models, the training process is run only once and the new modeling result is compared to previous benchmarks. This practice can lead to imprecise comparisons due to the variance in deep learning pipelines, which stems from the inherent randomness in the training process. Traditional solutions often require running the training process multiple times and are often infeasible in Deep Learning due to computational constraints. In this paper, we introduce a calibrated metric approach, designed to address this issue by reducing the variance present in its conventional counterpart. Consequently, this new metric improves the accuracy in detecting effective modeling improvements in the model selection stage. The efficacy of the new approach has been justified both theoretically and empirically.", "title_embedding_index": 21460, "title_abs_embedding_index": 21485}, {"title": "Building Vision Models upon Heat Conduction", "link_suffix": "/forum?id=bTi6usR2hF", "link": "https://openreview.net/forum?id=bTi6usR2hF", "pdf_link": "https://openreview.net/pdf?id=bTi6usR2hF", "keywords": "Vision Models, Representation Learning, Heat Conduction", "abstract": "Visual representation models leveraging attention mechanisms are challenged by significant computational overhead, particularly when pursuing large receptive fields. In this study, we aim to mitigate this challenge by introducing the Heat Conduction Operator (HCO) built upon the physical heat conduction principle. HCO conceptualizes image patches as heat sources and models their correlations through adaptive thermal energy diffusion, enabling robust visual representations. HCO enjoys a computational complexity of O(N^1.5), as it can be implemented using discrete cosine transformation (DCT) operations. HCO is plug-and-play, combining with deep learning backbones produces visual representation models (termed vHeat) with global receptive fields. Experiments across vision tasks demonstrate that, beyond the stronger performance, vHeat achieves up to a 3x throughput, 80% less GPU memory allocation and 35% fewer computational FLOPs compared to the Swin-Transformer.", "title_embedding_index": 21461, "title_abs_embedding_index": 21486}, {"title": "Uncertainty-Aware Decoding with Minimum Bayes' Risk", "link_suffix": "/forum?id=hPpyUv1XyQ", "link": "https://openreview.net/forum?id=hPpyUv1XyQ", "pdf_link": "https://openreview.net/pdf?id=hPpyUv1XyQ", "keywords": "mbr, uncertainty, llms, decoding, machine translation, language generation, variational learning", "abstract": "Despite their outstanding performance in the majority of scenarios, contemporary\nlanguage models still occasionally produce undesirable outputs, for example, hallucinated text. While such behaviors have previously been linked to uncertainty,\nthere is a notable lack of methods that actively consider uncertainty during text\ngeneration. In this work, we show how Minimum Bayes\u2019 Risk (MBR) decoding, a\nmethod that was originally designed to account for the imperfect nature of probabilistic language models, can be generalized into a principled uncertainty-aware\ndecoding method. In short, we account for model uncertainty during decoding\nby incorporating a posterior over model parameters into MBR\u2019s computation of\nexpected risk. We show that this modified expected risk is useful for both choosing\noutputs and deciding when to abstain from generation. We benchmark different\nmethods for learning posteriors and show that performance correlates with the\ndiversity of the combined set of models\u2019 predictions.", "title_embedding_index": 21462, "title_abs_embedding_index": 21487}, {"title": "SPA: Enhancing 3D Multimodal LLMs with Mask-based Streamlining Preference Alignment", "link_suffix": "/forum?id=j80J5cyyqP", "link": "https://openreview.net/forum?id=j80J5cyyqP", "pdf_link": "https://openreview.net/pdf?id=j80J5cyyqP", "keywords": "LLMs, Reperentation learning, MLLMs, 3D visual abilities", "abstract": "Integrating 3D features into Large Language Models (LLMs) is a rapidly evolving field, with models like 3D-LLM, Point-Bind LLM, and PointLLM making notable strides. PointLLM, pre-trained and fine-tuned on the Objaverse dataset, enhances understanding by optimizing the projector, boosting resource efficiency and consistency. However, we observed a persistent bottleneck: increasing the LLM backbone size doesn't consistently improve performance. Preliminary experiments showed that enhancing the 3D encoder or extending fine-tuning alone failed to resolve this. While post-training partially addressed the issue, it required two stages and additional text sample generation, making it inefficient. To overcome this, we propose \\textbf{S}treamlining \\textbf{P}reference \\textbf{A}lignment \\textbf{(SPA)}, a post-training stage for MLLMs with 3D encoders.  SPA leverages the 3D encoder\u2019s inductive bias through 3D-masking, ensuring robust output while preserving consistent differences. Unlike traditional post-training, SPA maximizes the encoder's spatial reasoning by increasing the probability gap between positive and negative logits. This approach eliminates redundant text generation, greatly enhancing resource efficiency and improving the overall alignment process. In addition, we identified evaluation issues in the existing benchmarks and conducted a re-benchmark, resulting in a more robust evaluation approach. The model combined with the SPA method as post-training stage successfully overcame the performance bottleneck and achieved better results across various evaluations on current scene-level and object-level benchmarks. Code is available at~\\url{https://anonymous.4open.science/r/3dmllm-dap-5A50}.", "title_embedding_index": 21463, "title_abs_embedding_index": 21488}, {"title": "Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources", "link_suffix": "/forum?id=M5LGyR71yS", "link": "https://openreview.net/forum?id=M5LGyR71yS", "pdf_link": "https://openreview.net/pdf?id=M5LGyR71yS", "keywords": "large language models, llms, synthetic-generation, dataset-generation, real-world data", "abstract": "Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage.  In this paper, we propose Source2Synth, a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources.\nSource2Synth improves the dataset quality by discarding low-quality generations based on their answerability.\nWe demonstrate the generality of this approach by applying it to two challenging domains:  we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA).\nOur method improves  performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.", "title_embedding_index": 21464, "title_abs_embedding_index": 21489}, {"title": "ToolBridge: An Open-Source Dataset to Equip LLMs with External Tool Capabilities", "link_suffix": "/forum?id=gRbWCGCFBz", "link": "https://openreview.net/forum?id=gRbWCGCFBz", "pdf_link": "https://openreview.net/pdf?id=gRbWCGCFBz", "keywords": "LLM, datasets, tool use, open source", "abstract": "Through the integration of external tools, large language models (LLMs) such as GPT-4o and Llama 3.1 significantly expand their functional capabilities, evolving from elementary conversational agents to general-purpose assistants. We argue that the primary drivers of these advancements are the quality and diversity of the training data. However, the existing LLMs with external tool integration provide only limited transparency regarding their datasets and data collection methods. This lack of transparency has led to the initiation of this research. Specifically, in this project, we aim to reveal the process of constructing datasets that empower LLMs to effectively learn how to utilize external tools and make this information available to the public through the introduction of ToolBridge. ToolBridge proposes to employ a collection of general open-access datasets as its raw dataset pool and applies a series of strategies to identify appropriate data entries for tool API insertions.", "title_embedding_index": 21465, "title_abs_embedding_index": 21490}, {"title": "FreeTraj: Tuning-Free Trajectory Control via Noise Guided Video Diffusion", "link_suffix": "/forum?id=CU7QfWJ6nC", "link": "https://openreview.net/forum?id=CU7QfWJ6nC", "pdf_link": "https://openreview.net/pdf?id=CU7QfWJ6nC", "keywords": "Diffusion Model, Video Diffusion, Trajectory Control, Motion Control", "abstract": "Diffusion model has demonstrated remarkable capability in video generation, which further sparks interest in introducing trajectory control into the generation process. While existing works mainly focus on training-based methods (e.g., conditional adapter), we argue that diffusion model itself allows decent control over the generated content without requiring any training. In this study, we introduce a tuning-free framework to achieve trajectory-controllable video generation, by imposing guidance on both noise construction and attention computation. Specifically, 1) we first show several instructive phenomena and analyze how initial noises influence the motion trajectory of generated content. 2) Subsequently, we propose FreeTraj, a tuning-free approach that enables trajectory control by modifying noise sampling and attention mechanisms. 3) Furthermore, we extend FreeTraj to facilitate longer and larger video generation with controllable trajectories. Equipped with these designs, users have the flexibility to provide trajectories manually or opt for trajectories automatically generated by the LLM trajectory planner. Extensive experiments validate the efficacy of our approach in enhancing the trajectory controllability of video diffusion models. Generated video samples are available at the anonymous website:https://FreeTraj.github.io.", "title_embedding_index": 21466, "title_abs_embedding_index": 21491}, {"title": "Improving Equivariant Networks with Probabilistic Symmetry Breaking", "link_suffix": "/forum?id=ZE6lrLvATd", "link": "https://openreview.net/forum?id=ZE6lrLvATd", "pdf_link": "https://openreview.net/pdf?id=ZE6lrLvATd", "keywords": "equivariance, symmetry, symmetry-breaking, canonicalization, graphs, GNNs", "abstract": "Equivariance encodes known symmetries into neural networks, often enhancing generalization. However, equivariant networks cannotbreaksymmetries: the output of an equivariant network must, by definition, have at least the same self-symmetries as its input. This poses an important problem, both (1) for prediction tasks on domains where self-symmetries are common, and (2) for generative models, which must break symmetries in order to reconstruct from highly symmetric latent spaces. This fundamental limitation can in fact be addressed by consideringequivariant conditional distributions, instead of equivariant functions. We therefore present novel theoretical results that establish necessary and sufficient conditions for representing such distributions. Concretely, this representation provides a practical framework for breaking symmetries in any equivariant network via randomized canonicalization. Our method, SymPE (Symmetry-breaking Positional Encodings), admits a simple interpretation in terms of positional encodings. This approach expands the representational power of equivariant networks while retaining the inductive bias of symmetry, which we justify through generalization bounds. Experimental results demonstrate that SymPE significantly improves performance of group-equivariant and graph neural networks across diffusion models for graphs, graph autoencoders, and lattice spin system modeling.", "title_embedding_index": 21467, "title_abs_embedding_index": 21492}, {"title": "CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair", "link_suffix": "/forum?id=8KQzoD5XAr", "link": "https://openreview.net/forum?id=8KQzoD5XAr", "pdf_link": "https://openreview.net/pdf?id=8KQzoD5XAr", "keywords": "Verilog Code Generation, Synthetic Data Generation, Large Language Models", "abstract": "Despite the significant progress made in code generation with large language models, challenges persist, especially with hardware description languages such as Verilog. This paper first presents an analysis of fine-tuned LLMs on Verilog coding, with synthetic data from prior methods. We identify two main issues: difficulties in handling non-textual representations (Karnaugh maps, state-transition diagrams and waveforms) and significant variability during training with models randomly making ''minor'' mistakes. To address these limitations, we enhance data curation by creating correct-by-construction data targeting non-textual representations. Additionally, we introduce an automated framework that generates error reports from various model checkpoints and injects these errors into open-source code to create targeted code repair data. Our fine-tuned Starcoder2-15B outperforms prior state-of-the-art results by 3.8%, 10.9%, 6.6% for pass@1 on VerilogEval-Machine, VerilogEval-Human, and RTLLM.", "title_embedding_index": 21468, "title_abs_embedding_index": 21493}, {"title": "A Differential Equation Approach for Wasserstein GANs and Beyond", "link_suffix": "/forum?id=7oaWthT9EO", "link": "https://openreview.net/forum?id=7oaWthT9EO", "pdf_link": "https://openreview.net/pdf?id=7oaWthT9EO", "keywords": "Generative modelling, finite elements, gradient flow, persistent training", "abstract": "We propose a new theoretical lens to view Wasserstein generative adversarial\nnetworks (WGANs). In our framework, we define a discretization inspired by a\ndistribution-dependent ordinary differential equation (ODE). We show that such\na discretization is convergent and propose a viable class of adversarial training\nmethods to implement this discretization, which we call W1 Forward Euler (W1-\nFE). In particular, the ODE framework allows us to implement persistent training,\na novel training technique that cannot be applied to typical WGAN algorithms\nwithout the ODE interpretation. Remarkably, when we do not implement persistent\ntraining, we prove that our algorithms simplify to existing WGAN algorithms; when\nwe increase the level of persistent training appropriately, our algorithms outperform\nexisting WGAN algorithms in both low- and high-dimensional examples.", "title_embedding_index": 21469, "title_abs_embedding_index": 21494}, {"title": "Reset Method based on the Theory of Manifold Optimization on Real Manifolds", "link_suffix": "/forum?id=xVw8YNEtH3", "link": "https://openreview.net/forum?id=xVw8YNEtH3", "pdf_link": "https://openreview.net/pdf?id=xVw8YNEtH3", "keywords": "Manifold Optimization, Real Manifolds, Method, Deep Learning.", "abstract": "Manifold optimization is prominent in the fields of applied mathematics, statistics, machine learning, and in particular, deep learning. By leveraging the intrinsic geometric properties of manifolds, constrained optimization problems can be transformed into unconstrained optimization problems on certain manifolds.  An innovative method, Reset Method, is introduced that combines manifold optimization and standard methods (SGD, Adam and AdamW), aiming to enhance the improvement of precision. The efficacy of our proposed method is corroborated by extensive deep learning experiments, providing visible higher precision.", "title_embedding_index": 21470, "title_abs_embedding_index": 21495}, {"title": "On inherent limitations of GPT/LLM\\Architecture", "link_suffix": "/forum?id=JNZ3Om6NPS", "link": "https://openreview.net/forum?id=JNZ3Om6NPS", "pdf_link": "https://openreview.net/pdf?id=JNZ3Om6NPS", "keywords": "0-1 laws, first-order logic, probabilistic spaces, finite graphs", "abstract": "This paper shows that reasoning/proving issues of $GPT/LLM$ are an inherent logical consequence of the architecture. Namely, they are due to a schema of its prediction mechanism of the next token in a sequence, and randomization involved in the process.After the natural formalization of the problem into a domain of finite graphs,  $G({\\omega})$, we prove the following general theorem:For almost all proofs, any learning algorithm of inference, that uses randomization in $G({\\omega})$, and necessitates veracity of inference, is almost surely a literal learning.In the context, \"literal learning\" stands for one which is either vacuous, i.e. $\\forall x~[P(x) \\implies Q(x)]$ where $P(x)$ is false for every $x$, or create a random inference from a false assumption (hallucination), or it essentially memorizes the inferences from training/synthetic data.A few corollaries follow. For instance, if its formulation is somewhat original, it is easy to notice the issue of solving mathematical problems with $LLMs$ in the case of even low-complexity tasks.  Since its solution is unlikely to be found in a holistic form in a training dataset, a correct proof is not to be expected.It is because, in a rigorous context, $GPT$ has exponentially decreasing odds of finding a valid proof of the result unless it simply \u201crepeats\u201d a known proof, perhaps with trivial modifications. Another observation is that the degradation has an exponential rate by the length of a proof. In other words, an attempt to prove a complex enough statement virtually has no chance to be \nsuccessful.In a novel rigorous context (i.e., when $GPT$-based architecture is looking to prove a new result, for instance, a hypothesis), that is virtually impossible even for a long enough fragment. The probability of success becomes infinitesimal quickly for either a fragment of possible proof or a weaker non-trivial statement. That also was empirically shown for data mixtures and confirmed experimentally.", "title_embedding_index": 21471, "title_abs_embedding_index": 21496}, {"title": "IgBleng: Unifying 3D structures and sequences in antibody language models", "link_suffix": "/forum?id=jHKqr1sDDM", "link": "https://openreview.net/forum?id=jHKqr1sDDM", "pdf_link": "https://openreview.net/pdf?id=jHKqr1sDDM", "keywords": "Antibodies, LLM, structure, multi-modal", "abstract": "Large language models (LLMs) trained on antibody sequences have shown significant potential in the rapidly advancing field of machine learning-assisted antibody engineering and drug discovery. However, current state-of-the-art antibody LLMs often overlook structural information, which could enable the model to more effectively learn the functional properties of antibodies by providing richer, more informative data. In response to this limitation, we introduce IgBlend, which integrates both the 3D coordinates of backbone atoms (C-alpha, N, and C) and antibody sequences. Our model is trained on a diverse dataset containing over 4 million unique structures and more than 200 million unique sequences, including heavy and light chains as well as nanobodies. We rigorously evaluate IgBlend using established benchmarks such as sequence recovery, complementarity-determining region (CDR) editing and inverse folding and demonstrate that IgBlend consistently outperforms current state-of-the-art models across all benchmarks. Furthermore, experimental validation shows that the model's log probabilities correlate well with measured binding affinities.", "title_embedding_index": 21472, "title_abs_embedding_index": 21497}, {"title": "FreSh: Frequency Shifting for Accelerated Neural Representation Learning", "link_suffix": "/forum?id=zMjjzXxS64", "link": "https://openreview.net/forum?id=zMjjzXxS64", "pdf_link": "https://openreview.net/pdf?id=zMjjzXxS64", "keywords": "spectral bias, automatic hyperparameter selection, implicit neural representation, discrete fourier transform", "abstract": "Implicit Neural Representations (INRs) have recently gained attention as a powerful approach for continuously representing signals such as images, videos, and 3D shapes using multilayer perceptrons (MLPs). However, MLPs are known to exhibit a low-frequency bias, limiting their ability to capture high-frequency details accurately. This limitation is typically addressed by incorporating high-frequency input embeddings or specialized activation layers. In this work, we demonstrate that these embeddings and activations are often configured with hyperparameters that perform well on average but are suboptimal for specific input signals under consideration, necessitating a costly grid search to identify optimal settings. Our key observation is that the initial frequency spectrum of an untrained model's output correlates strongly with the model's eventual performance on a given target signal. Leveraging this insight, we propose frequency shifting (or FreSh), a method that selects embedding hyperparameters to align the frequency spectrum of the model\u2019s initial output with that of the target signal. We show that this simple initialization technique improves performance across various neural representation methods and tasks, achieving results comparable to extensive hyperparameter sweeps but with only marginal computational overhead compared to training a single model with default hyperparameters.", "title_embedding_index": 21473, "title_abs_embedding_index": 21498}, {"title": "Exploiting Hierarchical Taxonomies in Pretrained Continual Learning", "link_suffix": "/forum?id=mLTbDVzHVh", "link": "https://openreview.net/forum?id=mLTbDVzHVh", "pdf_link": "https://openreview.net/pdf?id=mLTbDVzHVh", "keywords": "pretrained continual learning", "abstract": "Drawing inspiration from human learning behaviors, this work proposes a novel approach to mitigate catastrophic forgetting in Prompt-based Continual Learning (PCL) models by exploiting the relationships between continuously emerging class data. We find that applying human habits of organizing and connecting information can serve as an efficient strategy when training deep learning models.\nSpecifically, by building a hierarchical tree structure based on the expanding set of labels, we gain fresh insights into the data, identifying groups of similar classes could easily cause confusion. Additionally, we delve deeper into the hidden connections between classes by exploring the original pretrained model\u2019s behavior through an optimal transport-based approach. From these insights, we propose\na novel regularization loss function that encourages models to focus more on challenging knowledge areas, thereby enhancing overall performance. Experimentally, our method demonstrated significant superiority over current state-of-the-arts on various benchmarks.", "title_embedding_index": 21474, "title_abs_embedding_index": 21499}]