[{"title": "LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs", "link_suffix": "/forum?id=3A71qNKWAS", "link": "https://openreview.net/forum?id=3A71qNKWAS", "pdf_link": "https://openreview.net/pdf?id=3A71qNKWAS", "keywords": "Long context LLMs; Long-form generation; Benchmark", "abstract": "Current benchmarks like ``$\\textit{Needle-in-a-Haystack}$'' ($\\textit{NIAH}$), $\\textit{Ruler}$, and $\\textit{Needlebench}$ focus on models' ability to understand long-context input sequences but fail to capture a critical dimension: the generation of high-quality long-form text. Applications such as design proposals, technical documentation, and creative writing rely on coherent, instruction-following outputs over extended sequences\u2014a challenge that existing benchmarks do not adequately address. To fill this gap, we introduce $\\textit{LongGenBench}$, a novel benchmark designed to rigorously evaluate large language models' (LLMs) ability to generate long text while adhering to complex instructions. Through tasks requiring specific events or constraints within generated text, $\\textit{LongGenBench}$ evaluates model performance across four distinct scenarios, three instruction types, and two generation-lengths (16K and 32K tokens). Our evaluation of ten state-of-the-art LLMs reveals that, despite strong results on $\\textit{Ruler}$, all models struggled with long text generation on $\\textit{LongGenBench}$, particularly as text length increased. This suggests that current LLMs are not yet equipped to meet the demands of real-world, long-form text generation. We open-source $\\textit{LongGenBench}$ to promote comprehensive evaluation and improvement in this critical area, with code and data available at ${anonymousurl}$.", "title_embedding_index": 17650, "title_abs_embedding_index": 17675}, {"title": "Disentangling the Roles of Representation and Selection in Data Pruning (for Fine-Tuning)", "link_suffix": "/forum?id=EOPLy80bBm", "link": "https://openreview.net/forum?id=EOPLy80bBm", "pdf_link": "https://openreview.net/pdf?id=EOPLy80bBm", "keywords": "data pruning, fine-tuning", "abstract": "Data pruning, the process of carefully selecting a small subset of training data, has been shown to improve both training efficiency and performance. It typically involves two steps: (1) obtaining a representation for each instance, and (2) applying a selection algorithm using these representations. However, the distinct roles of these two steps, as well as their interactions, remain unclear. To address this, we conduct a systematic study of data pruning, focusing on NLP fine-tuning. Our theoretical and empirical findings reveal that data representation often plays a more fundamental role than the selection algorithm: gradients, despite being computationally expensive, provide stronger pruning signals than other representations, making gradient-based methods consistently outperform cheaper alternatives. We also demonstrate that different selection algorithms excel in specific scenarios but are heavily influenced by the chosen representation. These insights provide clear guidelines for future research and practical applications.", "title_embedding_index": 17651, "title_abs_embedding_index": 17676}, {"title": "To FP8 and Back Again: Quantifying Reduced Precision Effects on LLM Training Stability", "link_suffix": "/forum?id=pNgyXuGcx4", "link": "https://openreview.net/forum?id=pNgyXuGcx4", "pdf_link": "https://openreview.net/pdf?id=pNgyXuGcx4", "keywords": "LLM, FP8, Quantization, Training", "abstract": "The massive computational costs associated with large language model (LLM) pretraining have spurred great interest in reduced-precision floating-point representations to accelerate the process. As a result, the BrainFloat16 (BF16) precision has become the de facto standard for LLM training, with hardware support included in recent accelerators. This trend has gone even further in the latest processors, where FP8 has recently been introduced. However, prior experience with FP16, which was found to be less stable than BF16, raises concerns as to whether FP8, with even fewer bits than FP16, can be a cost-effective option for LLM training. We argue that reduced-precision training schemes must have similar training stability and hyperparameter sensitivities to their higher-precision counterparts in order to be cost-effective. However, we find that currently available methods for FP8 training are not robust enough to allow their use as economical replacements. This prompts us to investigate the stability of reduced-precision LLM training in terms of robustness across random seeds and learning rates. To this end, we propose new evaluation techniques and a new metric for quantifying loss landscape sharpness in autoregressive language models. By simulating incremental bit reductions in floating-point representations, we analyze the relationship between representational power and training stability with the intent of aiding future research into the field.", "title_embedding_index": 17652, "title_abs_embedding_index": 17677}, {"title": "Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval", "link_suffix": "/forum?id=zKFUNRH0hN", "link": "https://openreview.net/forum?id=zKFUNRH0hN", "pdf_link": "https://openreview.net/pdf?id=zKFUNRH0hN", "keywords": "Video Understanding", "abstract": "Video retrieval requires aligning visual content with corresponding natural language descriptions. In this paper, we introduce Modality Auxiliary Concepts for Video Retrieval (MAC-VR), a novel approach that leverages modality-specific tags---automatically extracted from foundation models---to enhance video retrieval.\nPrevious works have proposed to emulate human reasoning by introducing latent concepts derived from the features of a video and its corresponding caption. Building on these efforts to align latent concepts across both modalities, we propose learning auxiliary concepts from modality-specific tags. \nWe introduce these auxiliary concepts to improve the alignment of visual and textual latent concepts, and so be able to distinguish each concept from the other.\nTo strengthen the alignment between visual and textual latent concepts\u2014where a set of visual concepts matches a corresponding set of textual concepts\u2014we introduce an Alignment Loss. This loss aligns the proposed auxiliary concepts with the modalities' latent concepts, enhancing the model's ability to accurately match videos with their appropriate captions. \nWe conduct extensive experiments on three diverse datasets: MSR-VTT, DiDeMo, and ActivityNet Captions. The experimental results consistently demonstrate that modality-specific tags significantly improve cross-modal alignment, achieving performance comparable to current state-of-the-art methods.", "title_embedding_index": 17653, "title_abs_embedding_index": 17678}, {"title": "The Devil is in the Word: Video-Conditioned Text Representation Refinement for Text-to-Video Retrieval", "link_suffix": "/forum?id=jOVJhKzc3Y", "link": "https://openreview.net/forum?id=jOVJhKzc3Y", "pdf_link": "https://openreview.net/pdf?id=jOVJhKzc3Y", "keywords": "Text-to-Video Retrieval, Video-conditioned Text Representation Enhancement", "abstract": "Pre-trained vision-language models (VLMs), such as CLIP, have shown remarkable success in the text-video retrieval task due to their strong vision-language\nrepresentations learned from large-scale paired image-text samples. However,\ncompared to videos, text is often short and concise, making it difficult to fully\ncapture the rich and redundant semantics present in a video with thousands of\nframes. Recent advances have focused on utilizing text features to extract key information from these redundant video frames. However, text representation generated without considering video information can suffer from bias and lack the\nexpressiveness needed to capture key words that could enhance retrieval performance. In this study, we first conduct preliminary experiments to demonstrate\nthe importance of enhancing text representations. These experiments reveal that\ntext representation only generated from text input often misinterpret critical information. To address this, we propose a simple yet efficient method, VICTER, i.e.,\nvideo-conditioned text representation refinement, to enrich text representation using a versatile module. Specifically, we introduce a video abstraction module that\nextracts representative features from multiple video frames. This is followed by\na video-conditioned text enhancement module that refines the original text features by reassessing individual word features and extracting key words using the\ngenerated video features. Empirical evidence shows that VICTER not only effectively captures relevant key words from the input text but also complements\nvarious existing frameworks. Our experimental results demonstrate a significant\nimprovement of VICTER over several baseline frameworks (with 0.4% \u223c 1.0%\nimprovements on R@1). Furthermore, VICTER achieves state-of-the-art performance on three benchmark datasets, including MSRVTT, DiDeMo, and LSMDC. Code will be made available.", "title_embedding_index": 17654, "title_abs_embedding_index": 17679}, {"title": "Hi-Mamba: Hierarchical Mamba for Efficient Image Super-Resolution", "link_suffix": "/forum?id=iPYwddLhXR", "link": "https://openreview.net/forum?id=iPYwddLhXR", "pdf_link": "https://openreview.net/pdf?id=iPYwddLhXR", "keywords": "Image Super-Resolution, Mamba, Structured State Space Models", "abstract": "State Space Models (SSM), such as Mamba, have shown strong representation ability in modeling long-range dependency with linear complexity, achieving successful applications from high-level to low-level vision tasks. However, SSM's sequential nature necessitates multiple scans in different directions to compensate for the loss of spatial dependency when unfolding the image into a 1D sequence. This multi-direction scanning strategy significantly increases the computation overhead and is unbearable for high-resolution image processing. To address this problem, we propose a novel Hierarchical Mamba network, namely, Hi-Mamba, for image super-resolution (SR).  Hi-Mamba consists of two key designs: (1) The Hierarchical Mamba Block (HMB) assembled by a Local SSM (L-SSM) and a Region SSM (R-SSM) both with the single-direction scanning, aggregates multi-scale representations to enhance the context modeling ability. (2) The Direction Alternation  Hierarchical Mamba Group (DA-HMG) allocates the isomeric single-direction scanning into cascading HMBs to enrich the spatial relationship modeling.\nExtensive experiments demonstrate the superiority of Hi-Mamba across five benchmark datasets for efficient SR. For example, Hi-Mamba achieves a significant PSNR improvement of 0.29 dB on Manga109 for $\\times3$ SR, compared to the strong lightweight MambaIR.", "title_embedding_index": 17655, "title_abs_embedding_index": 17680}, {"title": "Groundwater Seepage Modeling in a River-Canal System based on Physics-Informed Neural Networks", "link_suffix": "/forum?id=GeMWhBIzrk", "link": "https://openreview.net/forum?id=GeMWhBIzrk", "pdf_link": "https://openreview.net/pdf?id=GeMWhBIzrk", "keywords": "Physics-Informed Neural Networks, Groundwater Prediction, Definite condition, Hard constraints, Self-Supervised", "abstract": "Neural networks, especially deep learning, have achieved revolutionary advances in several domains, including image and speech recognition, with excellent results. However, their reliance on labeled data, lack of interpretability, and inconsistency with physical principles limit their applicability in groundwater seepage prediction and other scientific disciplines. Physics-Informed Neural Networks (PINNs) significantly improve these issues by integrating physical knowledge with neural networks. This study focuses on modeling the groundwater flow field and proposes a physics-informed river-canal groundwater seepage model (PI-RGSM). This model enables self-supervised learning by incorporating hard constraints of boundary and initial conditions, utilizing hydrogeological parameters and boundary conditions as direct inputs, thus diminishing dependence on observable data. Compared to the baseline PINNs, the PI-RGSM adapts to and accurately predicts diverse seepage situations with just one training session, achieving a mean coefficient of determination of 0.978. To further enhance applicability in complex dynamic groundwater seepage situations, we propose PI-RGSM-K, which builds upon PI-RGSM. This model simulates heterogeneous groundwater seepage fields and improves performance in complex seepage environments through parameterized hydraulic conductivity field $K(x,y)$ and fine-adjusted model architecture, attaining a mean coefficient of determination of 0.982. The physics-informed neural network models proposed in this study demonstrate exceptional efficacy in precisely forecasting groundwater seepage behavior.", "title_embedding_index": 17656, "title_abs_embedding_index": 17681}, {"title": "Emergent properties with repeated examples", "link_suffix": "/forum?id=xrXci5YGm7", "link": "https://openreview.net/forum?id=xrXci5YGm7", "pdf_link": "https://openreview.net/pdf?id=xrXci5YGm7", "keywords": "transformers, learning on repeated examples, emergence", "abstract": "We study the performance of transformers as a function of the number of repetitions of training examples with algorithmically generated datasets. On three problems of mathematics: the greatest common divisor, modular multiplication, and matrix eigenvalues, we show that for a fixed number of training steps, models trained on smaller sets of repeated examples outperform models trained on larger sets of single-use examples. We also demonstrate that {\\em two-set training} - repeated use of a small random subset of examples, along normal sampling on the rest of the training set - provides for faster learning and better performance. This highlights that the benefits of repetition can outweigh those of data diversity. These datasets and problems provide a controlled setting to shed light on the still poorly understood interplay between generalization and memorization in deep learning.", "title_embedding_index": 17657, "title_abs_embedding_index": 17682}, {"title": "Anti-Reference: Universal  and Immediate Defense Against Reference-Based Generation", "link_suffix": "/forum?id=qvuC22BT2q", "link": "https://openreview.net/forum?id=qvuC22BT2q", "pdf_link": "https://openreview.net/pdf?id=qvuC22BT2q", "keywords": "Diffusion model, Customized generation, Adversarial Attack, Image generation", "abstract": "Diffusion models have completely transformed the field of generative models, demonstrating unparalleled capabilities in generating high-fidelity images. However, when misused, such a powerful and convenient tool could create fake news or disturbing content targeted at individual victims, causing severe negative social impacts. In this paper, we introduce Anti-Reference, a novel method that protects images from the threats posed by reference-based generation techniques by adding imperceptible adversarial noise to the images. We propose a unified loss function that enables joint attacks on fine-tuning-based customization methods, non-fine-tuning customization methods, and human-centric driving methods. Based on this loss, we train a Noise Encoder with a DiT architecture to predict the noise or directly optimize the noise using the PGD (Projected Gradient Descent) method. Our method demonstrates strong black-box transferability, being equally effective against black-box models and some commercial APIs such as Animate Anyone, and EMO. Extensive experiments validate the performance of Anti-Reference, establishing a new benchmark in image security.", "title_embedding_index": 17658, "title_abs_embedding_index": 17683}, {"title": "Cross-Modal Alignment via Variational Copula Modelling", "link_suffix": "/forum?id=PnQJ24n1qq", "link": "https://openreview.net/forum?id=PnQJ24n1qq", "pdf_link": "https://openreview.net/pdf?id=PnQJ24n1qq", "keywords": "Copula, Multimodal learning, Missing modality", "abstract": "Various data modalities are common in real-world applications. In healthcare, for example, electronic health records, medical images, and clinical notes provide comprehensive information for diagnosis and treatment.\n    Thus, it is essential to develop multimodal learning methods that aggregate information from multiple modalities to generate meaningful representations for downstream tasks.\n    The key challenge here is how to appropriately align the representations of the respective modalities and fuse them into a joint distribution.\n    Existing methods mainly focus on fusing the representations via concatenation or the Kronecker product, which oversimplifies the interaction structure between modalities, prompting the need to model more complex interactions.\n    Moreover, the notion of joint distribution of the latent representation that incorporates higher-order interactions between modalities is also underexplored.\n    Copula is a powerful statistical structure in modelling the interactions between variables, as it bridges the joint distribution and marginal distributions of multiple variables.\n    In this paper, we propose a novel copula modelling-driven multimodal learning framework, which focuses on learning the joint distribution of various modalities to capture the complex interaction among them.\n    The key idea is interpreting the copula model as a tool to align the marginal distributions of the modalities efficiently. \n    By assuming a Gaussian mixture distribution for each modality and a copula model on the joint distribution, our model can also generate accurate representations for missing modalities.\n    Extensive experiments on public MIMIC datasets demonstrate the superior performance of our model over other competitors.\n    Ablation studies also validate the effectiveness of the copula alignment strategy and the robustness of our model over different choices of the copula family. \n    Code is anonymously available athttps://anonymous.4open.science/r/CM2-C1FD/README.md.", "title_embedding_index": 17659, "title_abs_embedding_index": 17684}, {"title": "Enhancing Multimodal LLM for Detailed and Accurate Video Captioning using Multi-Round Preference Optimization", "link_suffix": "/forum?id=ufi0WPTgWp", "link": "https://openreview.net/forum?id=ufi0WPTgWp", "pdf_link": "https://openreview.net/pdf?id=ufi0WPTgWp", "keywords": "Multi-modal large language models, video captioning, multi-round DPO, rebirth tuning", "abstract": "Videos contain a wealth of information, and generating detailed and accurate descriptions in natural language is a key aspect of video understanding. In this paper, we present video-SALMONN 2, an advanced audio-visual large language model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with paired audio) captioning through directed preference optimization (DPO). We propose new metrics to evaluate the completeness and accuracy of video descriptions, which are optimized using DPO. To further improve training, we introduce a novel multi-round DPO (mrDPO) approach, which involves periodically updating the DPO reference model, merging and re-initializing the LoRA module as a proxy for parameter updates after each training round (1,000 steps), and incorporating guidance from ground-truth video captions to stabilize the process. To address potential catastrophic forgetting of non-captioning abilities due to mrDPO, we propose rebirth tuning, which finetunes the pre-DPO LLM by using the captions generated by the mrDPO-trained model as supervised labels. Experiments show that mrDPO significantly enhances video-SALMONN 2's captioning accuracy, reducing global and local error rates by 40% and 20%, respectively, while decreasing the repetition rate by 35%. The final video-SALMONN 2 model, with just 7 billion parameters, surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning tasks, while maintaining competitive performance to the state-of-the-art on widely used video question-answering benchmark among models of similar size. Upon acceptance, we will release the code, model checkpoints, and training and test data. Demos are available athttps://video-salmonn-2.github.io.", "title_embedding_index": 17660, "title_abs_embedding_index": 17685}, {"title": "GeneBench: Systematic Evaluation of Genomic Foundation Models and Beyond", "link_suffix": "/forum?id=0bswm093Yl", "link": "https://openreview.net/forum?id=0bswm093Yl", "pdf_link": "https://openreview.net/pdf?id=0bswm093Yl", "keywords": "genetic foundation model, benchmark, hybrid model", "abstract": "The Genomic Foundation Model (GFM) paradigm is expected to facilitate the extraction of generalizable representations from massive genomic data, thereby enabling their application across a spectrum of downstream applications. Despite advancements, a lack of evaluation framework makes it difficult to ensure equitable assessment due to experimental settings, model intricacy, benchmark datasets, and reproducibility challenges. In the absence of standardization, comparative analyses risk becoming biased and unreliable. To surmount this impasse, we introduce GeneBench, a comprehensive benchmarking suite specifically tailored for evaluating the efficacy of Genomic Foundation Models. GeneBench offers a modular and expandable framework that encapsulates a variety of state-of-the-art methodologies. Through systematic evaluations of datasets spanning diverse biological domains with a particular emphasis on both short-range and long-range genomic tasks, firstly including the three most important DNA tasks covering Coding Region, Non-Coding Region, Genome Structure, etc. Our results on GenBench has led to an interesting discovery: regardless of the number of parameters, the noticeable variation in preference between attention-based and convolution-based models for short- and long-range tasks could offer valuable insights for the future development of GFM. As a result, we propose a straightforward modified model called Genhybrid, which is an effective and efficient convolution-attention hybrid model suitable for all tasks.", "title_embedding_index": 17661, "title_abs_embedding_index": 17686}, {"title": "Action Typicality and Uniqueness Learning for Zero-Shot Video Anomaly Detection", "link_suffix": "/forum?id=KYxxSgwRxf", "link": "https://openreview.net/forum?id=KYxxSgwRxf", "pdf_link": "https://openreview.net/pdf?id=KYxxSgwRxf", "keywords": "video anomaly detection, zero-shot, skeleton-based", "abstract": "Zero-Shot Video Anomaly Detection (ZS-VAD) is an urgent task in scenarios where the target video domain lacks training data due to various concerns, \\emph{e.g.}, data privacy. The skeleton-based approach is a promising way to achieve ZS-VAD as it eliminates domain disparities in both background and human appearance. However, existing methods only learn low-level skeleton representation and rely on the domain-specific normality boundary, which cannot generalize well to new scenes with different normal and abnormal behavior patterns. In this paper, we propose a novel skeleton-based zero-shot video anomaly detection framework, which captures both scene-generic typical anomalies and scene-adaptive unique anomalies. Firstly, we introduce a language-guided typicality modeling module that projects skeleton snippets into action semantic space and learns generalizable typical distributions of normal and abnormal behavior. Secondly, we propose a test-time context uniqueness analysis module to finely analyze the spatio-temporal differences between skeleton snippets and then derive scene-adaptive boundaries. Without using any training samples from the target domain, our method achieves state-of-the-art results on four large-scale VAD datasets: ShanghaiTech, UBnormal, NWPU, and UCF-Crime. The Code will be publicly available.", "title_embedding_index": 17662, "title_abs_embedding_index": 17687}, {"title": "EVLM: An Efficient Vision-Language Model for Visual Understanding", "link_suffix": "/forum?id=S7M1iqFLVm", "link": "https://openreview.net/forum?id=S7M1iqFLVm", "pdf_link": "https://openreview.net/pdf?id=S7M1iqFLVm", "keywords": "multi-modal language models, cross-attention, moe", "abstract": "In the field of multi-modal language models, the majority of methods are built on an archi-\ntecture similar to LLaVA. These models use a single-layer ViT feature as a visual prompt,\ndirectly feeding it into the language models alongside textual tokens. However, when dealing\nwith long sequences of visual signals or inputs such as videos, the self-attention mechanism\nof language models can lead to significant computational overhead. Additionally, using\nsingle-layer ViT features makes it challenging for large language models to perceive visual\nsignals fully. This paper proposes an efficient multi-modal language model to minimize\ncomputational costs while enabling the model to perceive visual signals as comprehensively\nas possible. Our method primarily includes: (1) employing cross-attention to image-text\ninteraction similar to Flamingo. (2) utilize hierarchical ViT features. (3) introduce the\nMixture of Experts (MoE) mechanism to enhance model effectiveness. Our model achieves\ncompetitive scores on public multi-modal benchmarks and performs well in tasks such as\nimage captioning and video captioning.", "title_embedding_index": 17663, "title_abs_embedding_index": 17688}, {"title": "Accelerating Error Correction Code Transformers", "link_suffix": "/forum?id=lAXlDAdan5", "link": "https://openreview.net/forum?id=lAXlDAdan5", "pdf_link": "https://openreview.net/pdf?id=lAXlDAdan5", "keywords": "Error Correction Code, quantization, attention optimization", "abstract": "Error correction codes (ECC) are crucial for ensuring reliable information transmission in communication systems. Choukroun & Wolf (2022b) recently introduced the Error Correction Code Transformer (ECCT), which has demonstrated promising performance across various transmission channels and families of codes. However, its high computational and memory demands limit its practical applications compared to traditional decoding algorithms. Achieving effective quantization of the ECCT presents significant challenges due to its inherently small architecture, since existing, very low-precision quantization techniques often lead to performance degradation in compact neural networks. In this paper, we introduce a novel acceleration method for transformer-based decoders. We first propose a ternary weight quantization method specifically designed for the ECCT, inducing a decoder with multiplication-free linear layers. We present an\noptimized self-attention mechanism to reduce computational complexity via codeaware multi-heads processing. Finally, we provide positional encoding via the Tanner graph eigendecomposition, enabling a richer representation of the graph connectivity. The approach not only matches or surpasses ECCT\u2019s performance but also significantly reduces energy consumption, memory footprint, and computational complexity. Our method brings transformer-based error correction closer to practical implementation in resource-constrained environments, achieving a 90% compression ratio and reducing arithmetic operation energy consumption by at least 224 times on modern hardware.", "title_embedding_index": 17664, "title_abs_embedding_index": 17689}, {"title": "On-Device Collaborative Language Modeling via a Mixture of Generalists and Specialists", "link_suffix": "/forum?id=rf0ZoDASnS", "link": "https://openreview.net/forum?id=rf0ZoDASnS", "pdf_link": "https://openreview.net/pdf?id=rf0ZoDASnS", "keywords": "Collaborative Learning, Federated Learning, On-device language models, Mixture of Experts", "abstract": "On-device LLMs have gained increasing attention for their ability to enhance privacy and provide a personalized user experience. \nTo facilitate learning with private and scarce local data, federated learning has become a standard approach, though it introduces challenges related to system and data heterogeneity among end users. As a solution, we propose a novel $\\textbf{Co}$llaborative learning approach with a $\\textbf{Mi}$xture of $\\textbf{G}$eneralists and $\\textbf{S}$pecialists (CoMiGS), being the first to effectively address both. Our approach distinguishes generalists and specialists by aggregating certain experts across end users while keeping others localized to specialize in user-specific datasets. A key innovation of our method is the bi-level optimization formulation of the Mixture-of-Experts learning objective, where the router is updated using a separate validation set that represents the target distribution. CoMiGS effectively balances collaboration and personalization, as demonstrated by its superior performance in scenarios with high data heterogeneity across multiple datasets. By design, our approach accommodates users' varying computational resources through different numbers of specialists. By decoupling resource abundance from data quantity, CoMiGS remains robust against overfitting\u2014due to the generalists' regularizing effect\u2014while adapting to local data through specialist expertise.", "title_embedding_index": 17665, "title_abs_embedding_index": 17690}, {"title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models", "link_suffix": "/forum?id=9H1uctBWgF", "link": "https://openreview.net/forum?id=9H1uctBWgF", "pdf_link": "https://openreview.net/pdf?id=9H1uctBWgF", "keywords": "Large Language Models, Federated Full-Parameter Tuning, Scalability, Theoretical Guarantees", "abstract": "Large Language Models (LLMs) have become indispensable in numerous real-world applications. Unfortunately, fine-tuning these models at scale, especially in federated settings where data privacy and communication efficiency are critical, presents significant challenges. Existing methods often resort to parameter-efficient fine-tuning (PEFT) to mitigate communication overhead, but this typically comes at the cost of model accuracy. To address these limitations, we propose federated full-parameter tuning at scale for LLMs (Ferret), the first first-order method with shared randomness to enable scalable full-parameter tuning of LLMs across decentralized data sources while maintaining competitive model accuracy. Ferret accomplishes this through three aspects: (1) it employs widely applied first-order methods for efficient local updates; (2) it projects these updates into a low-dimensional space to considerably reduce communication overhead; and (3) it reconstructs local updates from this low-dimensional space with shared randomness to facilitate effective full-parameter global aggregation, ensuring fast convergence and competitive final performance. Our rigorous theoretical analyses and insights along with extensive experiments, show that Ferret significantly enhances the scalability of existing federated full-parameter tuning approaches by achieving high computational efficiency, reduced communication overhead, and fast convergence, all while maintaining competitive model accuracy.", "title_embedding_index": 17666, "title_abs_embedding_index": 17691}, {"title": "Solving Diverse Combinatorial Optimization Problems with a Unified Model", "link_suffix": "/forum?id=Kc3yoIL5oR", "link": "https://openreview.net/forum?id=Kc3yoIL5oR", "pdf_link": "https://openreview.net/pdf?id=Kc3yoIL5oR", "keywords": "Large Model, Combinatorial Optimization, Transformers", "abstract": "Combinatorial Optimization (CO) covers a wide range of problems that exist in many real-world scenarios, while solving them using learning based methods has drawn great attention. Developing a unified deep model to solve diverse CO problems has many benefits, including a reduction in the need for hand-crafted designs for individual problems and enhanced flexibility for few-shot learning in unseen problem types. Meanwhile, a unified model with a single architecture and parameter set for diverse CO problems remains absent. To the best of our knowledge, we are the first to formally investigate and develop such a unified model. Motivated by the success of the next-token-prediction concept, we formulate each solution into an Markov Decision Process, and train the model with transformer backbone using tokenized data collected from problem solution trajectories. However, directly training the unified model is challenging due to the long token length of the trajectories, which arises from the complex observation space of CO problems, resulting from their NP-hard nature. Furthermore, using the same model to simultaneously predict observations and actions\u2014distinct types of elements within a trajectory\u2014further increases training difficulty. To address these challenges, we introduce two key designs. First, to reduce token length, we implement a CO-prefix design that aggregates the static features of the problems. Second, to account for the heterogeneity of state and action tokens within the MDP, we adopt a two-stage self-supervised learning scheme. In the first stage, a dynamic prediction model is learned, which then serves as a pre-trained model for subsequent policy generation. Experiments across a set of nine problems demonstrate the robust problem-solving capabilities of our unified model, along with its few-shot and even zero-shot generalization abilities. We believe our framework provides a valuable complement to existing neural CO methods that focus on achieving optimal performance for individual CO problems.", "title_embedding_index": 17667, "title_abs_embedding_index": 17692}, {"title": "Characterizing Massive Activations of Attention Mechanism in Graph Neural Networks", "link_suffix": "/forum?id=poKOBfJrKf", "link": "https://openreview.net/forum?id=poKOBfJrKf", "pdf_link": "https://openreview.net/pdf?id=poKOBfJrKf", "keywords": "GraphML, Transformers, GraphNN, Adversarial", "abstract": "Graph Neural Networks (GNNs) have become increasingly popular for effectively modeling data with graph structures. Recently, attention mechanisms have been integrated into GNNs to improve their ability to capture complex patterns. This paper presents the first comprehensive study revealing a critical, unexplored consequence of this integration: the emergence of Massive Activations (MAs) within attention layers. We introduce a novel method for detecting and analyzing MAs, focusing on edge features in different graph transformer architectures. Our study assesses various GNN models using benchmark datasets, including ZINC, TOX21, and PROTEINS. Key contributions include (1) establishing the direct link between attention mechanisms and MAs generation in GNNs, (2) developing a robust definition and detection method for MAs based on activation ratio distributions, (3) introducing the Explicit Bias Term (EBT) as a potential countermeasure and exploring it as an adversarial framework to assess models robustness based on the presence or absence of MAs. Our findings highlight the prevalence and impact of attention-induced MAs across different architectures, such as GraphTransformer, GraphiT, and SAN. The study reveals the complex interplay between attention mechanisms, model architecture, dataset characteristics, and MAs emergence, providing crucial insights for developing more robust and reliable graph models.", "title_embedding_index": 17668, "title_abs_embedding_index": 17693}, {"title": "InfiniteMesh: View Interpolation using Multi-view Diffusion for 3D Mesh Reconstruction", "link_suffix": "/forum?id=pw0fKwi0Pj", "link": "https://openreview.net/forum?id=pw0fKwi0Pj", "pdf_link": "https://openreview.net/pdf?id=pw0fKwi0Pj", "keywords": "View Interpolation, Multi-view Diffusion, 3D Mesh Reconstruction", "abstract": "We present InfiniteMesh, a feed-forward framework for efficient high-quality image-to-3D generation with view interpolation. Recent advancements in Large Reconstruction Model (LRM) have demonstrated significant potential in extracting 3D content from multi-view images produced by 2D diffusion models. Nevertheless, challenges remain as 2D diffusion models often struggle to generate dense images with strong multi-view consistency, and LRMs often exacerbate this multi-view inconsistency during 3D reconstruction. To address these issues, we propose a novel framework based on LRM that employs 2D diffusion-based view interpolation to enhance the quality of the generated mesh. Leveraging multi-view images produced by a 2D diffusion model, our approach introduces an Infinite View Interpolation module to generate interpolated images from main views. Subsequently, we employ a tri-plane-based mesh reconstruction strategy to extract robust tokens from these multiple generated images and produce the final mesh. Extensive experiments indicate that our method generates high-quality 3D content in terms of both texture and geometry, surpassing previous state-of-the-art methods.", "title_embedding_index": 17669, "title_abs_embedding_index": 17694}, {"title": "Composing Unbalanced Flows for Flexible Docking and Relaxation", "link_suffix": "/forum?id=gHLWTzKiZV", "link": "https://openreview.net/forum?id=gHLWTzKiZV", "pdf_link": "https://openreview.net/pdf?id=gHLWTzKiZV", "keywords": "molecular docking, flow matching, structure relaxation, unbalanced transport", "abstract": "Diffusion models have emerged as a successful approach for molecular docking, but they often cannot model protein flexibility or generate nonphysical poses. We argue that both these challenges can be tackled by framing the problem as a transport between distributions. Still, existing paradigms lack the flexibility to define effective maps between such complex distributions. To address this limitation we propose Unbalanced Flow Matching, a generalization of Flow Matching (FM) that allows trading off sample efficiency with approximation accuracy and enables more accurate transport. Empirically, we apply Unbalanced FM on flexible docking and structure relaxation, demonstrating our ability to model protein flexibility and generate energetically favorable poses. On the PDBBind docking benchmark, our method FlexDock improves the docking performance while increasing the proportion of energetically favorable poses from 30% to 73%.", "title_embedding_index": 17670, "title_abs_embedding_index": 17695}, {"title": "Temporal Reasoning Transfer from Text to Video", "link_suffix": "/forum?id=sHAvMp5J4R", "link": "https://openreview.net/forum?id=sHAvMp5J4R", "pdf_link": "https://openreview.net/pdf?id=sHAvMp5J4R", "keywords": "Video Large Language Models, Temporal Reasoning", "abstract": "Video Large Language Models (Video LLMs) have shown promising capabilities in video comprehension, yet they struggle with tracking temporal changes and reasoning about temporal relationships.\nWhile previous research attributed this limitation to the ineffective temporal encoding of visual inputs, our diagnostic study reveals that video representations contain sufficient information for even small probing classifiers to achieve perfect accuracy.\nSurprisingly, we find that the key bottleneck in Video LLMs' temporal reasoning capability stems from the underlying LLM's inherent difficulty with temporal concepts, as evidenced by poor performance on textual temporal question-answering tasks.\nBuilding on this discovery, we introduce the \\textbf{T}extual \\textbf{T}emporal reasoning \\textbf{T}ransfer (\\textbf{T3}). \nT3 synthesizes diverse temporal reasoning tasks in pure text format from existing image-text datasets, addressing the scarcity of video samples with complex temporal scenarios. \nRemarkably, \\emph{without using any video data}, T3 enhances LongVA-7B's temporal understanding, yielding a 5.3 absolute accuracy improvement on the challenging TempCompass benchmark, which enables our model to outperform ShareGPT4Video-8B trained on 28,000 video samples.\nAdditionally, the enhanced LongVA-7B model achieves competitive performance on comprehensive video benchmarks. For example, it achieves a 49.7 accuracy on the Temporal Reasoning task of Video-MME, surpassing powerful large-scale models such as InternVL-Chat-V1.5-20B and VILA1.5-40B. \nFurther analysis reveals a strong correlation between textual and video temporal task performance, validating the efficacy of transferring temporal reasoning abilities from text to video domains.", "title_embedding_index": 17671, "title_abs_embedding_index": 17696}, {"title": "Interpreting Language Reward Models via Contrastive Explanations", "link_suffix": "/forum?id=i8IwcQBi74", "link": "https://openreview.net/forum?id=i8IwcQBi74", "pdf_link": "https://openreview.net/pdf?id=i8IwcQBi74", "keywords": "Explainable AI, Contrastive Explanations, Counterfactual Explanations, Alignment, Large Language Model", "abstract": "Reward models (RMs) are a crucial component in the alignment of large language models\u2019 (LLMs) outputs with human values. RMs approximate human preferences over possible LLM responses to the same prompt by predicting and comparing reward scores. However, as they are typically modified versions of LLMs with scalar output heads, RMs are large black boxes whose predictions are not explainable. More transparent RMs would enable improved trust in the alignment of LLMs. In this work, we propose to use contrastive explanations to explain any binary response comparison made by an RM. Specifically, we generate a diverse set of new comparisons similar to the original one to characterise the RM\u2019s local behaviour. The perturbed responses forming the new comparisons are generated to explicitly modify manually specified high-level evaluation attributes, on which analyses of RM behaviour are grounded. In quantitative experiments, we validate the effectiveness of our method for finding high-quality contrastive explanations. We then showcase the qualitative usefulness of our method for investigating global sensitivity of RMs to each evaluation attribute, and demonstrate how representative examples can be automatically extracted to explain and compare behaviours of different RMs. We see our method as a flexible framework for RM explanation, providing a basis for more interpretable and trustworthy LLM alignment.", "title_embedding_index": 17672, "title_abs_embedding_index": 17697}, {"title": "Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning", "link_suffix": "/forum?id=tErHYBGlWc", "link": "https://openreview.net/forum?id=tErHYBGlWc", "pdf_link": "https://openreview.net/pdf?id=tErHYBGlWc", "keywords": "Reinforcement Learning, Representation Learning, Transfer Learning, Information Theory", "abstract": "Extracting relevant information from a stream of high-dimensional observations is a central challenge for deep reinforcement learning agents. Actor-critic algorithms add further complexity to this challenge, as it is often unclear whether the same information will be relevant to both the actor and the critic. To this end, we here explore the principles that underlie effective representations for an actor and for a critic. We focus our study on understanding whether an actor and a critic will benefit from a decoupled, rather than shared, representation. \nOur primary finding is that when decoupled, the representations for the actor and critic systematically specialise in extracting different types of information from the environment---the actor's representation tends to focus on action-relevant information, while the critic's representation specialises in encoding value and dynamics information. Finally, we demonstrate how these insights help select representation learning objectives that play into the actor's and critic's respective knowledge specialisations, and improve performance in terms of agent returns.", "title_embedding_index": 17673, "title_abs_embedding_index": 17698}, {"title": "IV-mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis", "link_suffix": "/forum?id=ImpeMDJfVL", "link": "https://openreview.net/forum?id=ImpeMDJfVL", "pdf_link": "https://openreview.net/pdf?id=ImpeMDJfVL", "keywords": "IV-mixed Sampler, Video Synthesis, Inference-heavy Algorithms, Training-free", "abstract": "The multi-step sampling mechanism, a key feature of visual diffusion models, has significant potential to replicate the success of OpenAI's Strawberry in enhancing performance by increasing the inference computational cost. Sufficient prior studies have demonstrated that correctly scaling up computation in the sampling process can successfully lead to improved generation quality, enhanced image editing, and compositional generalization. While there have been rapid advancements in developing inference-heavy algorithms for improved image generation, relatively little work has explored inference scaling laws in video diffusion models (VDMs). Furthermore, existing research shows only minimal performance gains that are perceptible to the naked eye. To address this, we design a novel training-free algorithmIV-Mixed Samplerthat leverages the strengths of image diffusion models (IDMs) to assist VDMs surpass their current capabilities. The core ofIV-Mixed Sampleris to use IDMs to significantly enhance the quality of each video frame and VDMs ensure the temporal coherence of the video during the sampling process. Our experiments have demonstrated thatIV-Mixed Samplerachieves state-of-the-art performance on 4 benchmarks including UCF-101-FVD, MSR-VTT-FVD, Chronomagic-Bench-150, and Chronomagic-Bench-1649. For example, the open-source Animatediff withIV-Mixed Samplerreduces the UMT-FVD score from 275.2 to 228.6, closing to 223.1 from the closed-source Pika-2.0.", "title_embedding_index": 17674, "title_abs_embedding_index": 17699}]