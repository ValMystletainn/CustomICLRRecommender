[
    {
        "title": "The Quest for Winning Tickets in Low-Rank Adapters",
        "link_suffix": "/forum?id=PxYhHF6FNH",
        "link": "https://openreview.net/forum?id=PxYhHF6FNH",
        "pdf_link": "https://openreview.net/pdf?id=PxYhHF6FNH",
        "keywords": "Parameter-efficient fine-tuning, Low-Rank Adaptation, Lottery Ticket Hypothesis, Pruning",
        "abstract": "Low-Rank Adaptation (LoRA), a prominent parameter-efficient fine-tuning (PEFT) method, offers an effective strategy for adapting large pre-trained models to specific tasks with minimal computational overhead. LoRA achieves this by introducing low-rank parameter matrices to the frozen pre-trained models. However, despite their efficiency, LoRA and its variants modify all elements of a parameter block, which is unnecessary as LoRA primarily aims to adjust a small set of subspaces that capture task-specific knowledge. Drawing inspiration from the Lottery Ticket Hypothesis (LTH), which posits that dense neural networks contain sparse subnetworks capable of performing similarly to fully-parameterized models, we investigate whether similar sparse subnetworks exist for low-rank adapters. We demonstrate that such subnetworks, often referred to as \"winning tickets\" in the context of LTH, indeed exist for low-rank adapters. We introduce a method to identify this sparse subset of weights for each layer by relating the top subspaces of the pretrained parameter block to the elements of the corresponding weight matrix. This subset is then fine-tuned using LoRA. We show that this sparse subset is not necessarily unique; as long as sparsity is kept within a certain bound defined by the task, random subnetworks with similar sparsity can act as winning tickets. Building on this discovery, we propose a novel approach called Partial-LoRA, which adds sparse low-rank parameters to pre-trained models. Through extensive experiments on 8 vision and 4 language tasks, we demonstrate that Partial-LoRA can reduce trainable parameters by up to 87% while maintaining or even improving model performance in some cases. Our work thus reduces memory needs and theoretically grounds sparse LoRAs."
    },
    {
        "title": "ADIFF: Explaining audio difference using natural language",
        "link_suffix": "/forum?id=l4fMj4Vnly",
        "link": "https://openreview.net/forum?id=l4fMj4Vnly",
        "pdf_link": "https://openreview.net/pdf?id=l4fMj4Vnly",
        "keywords": "audio processing; audio-language; multimodal learning",
        "abstract": "Understanding and explaining differences between audio recordings is crucial for fields like audio forensics, quality assessment, and audio generation. This involves identifying and describing audio events, acoustic scenes, signal characteristics, and their emotional impact on listeners. This paper stands out as the first work to comprehensively study the task of explaining audio differences and then propose benchmark, baselines for the task. First, we present two new datasets for audio difference explanation derived from the AudioCaps and Clotho audio captioning datasets. Using Large Language Models (LLMs), we generate three levels of difference explanations: (1) concise descriptions of audio events and objects, (2) brief sentences about audio events, acoustic scenes, and signal properties, and (3) comprehensive explanations that include semantics and listener emotions. For the baseline, we use prefix tuning where audio embeddings from two audio files are used to prompt a frozen language model. Our empirical analysis and ablation studies reveal that the naive baseline struggles to distinguish perceptually similar sounds and generate detailed tier 3 explanations. To address these limitations, we propose ADIFF, which introduces a cross-projection module, position captioning, and a three-step training process to enhance the model\u2019s ability to produce detailed explanations. We evaluate our model using objective metrics and human evaluation and show our model enhancements lead to significant improvements in performance over naive baseline and SoTA Audio-Language Model (ALM) Qwen Audio. Lastly, we conduct multiple ablation studies to study the effects of cross-projection, language model parameters, position captioning, third stage fine-tuning, and present our findings. Our benchmarks, findings, and strong baseline pave the way for nuanced and human-like explanations of audio differences."
    },
    {
        "title": "Improving Data Efficiency via Curating LLM-Driven Rating Systems",
        "link_suffix": "/forum?id=DKkQtRMowq",
        "link": "https://openreview.net/forum?id=DKkQtRMowq",
        "pdf_link": "https://openreview.net/pdf?id=DKkQtRMowq",
        "keywords": "Data Selection, LLM, Instruction Tuning",
        "abstract": "Instruction tuning is critical for adapting large language models (LLMs) to downstream tasks, and recent studies have demonstrated that small amounts of human-curated data can outperform larger datasets, challenging traditional data scaling laws. While LLM-based data quality rating systems offer a cost-effective alternative to human annotation, they often suffer from inaccuracies and biases, even in powerful models like GPT-4. In this work, we introduce $DS^2$, aDiversity-awareScore curation method forDataSelection. By systematically modeling error patterns through a score transition matrix, $DS^2$ corrects LLM-based scores and promotes diversity in the selected data samples. Our approach shows that a curated subset (just 3.3% of the original dataset) outperforms full-scale datasets (300k samples) across various machine-alignment benchmarks, and matches or surpasses human-aligned datasets such as LIMA with the same sample size (1k samples). These findings challenge conventional data scaling assumptions, highlighting that redundant, low-quality samples can degrade performance and reaffirming that ``more can be less''."
    },
    {
        "title": "Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision",
        "link_suffix": "/forum?id=UFKC0lMTdK",
        "link": "https://openreview.net/forum?id=UFKC0lMTdK",
        "pdf_link": "https://openreview.net/pdf?id=UFKC0lMTdK",
        "keywords": "Large Multimodal Model, Foundation Model, Visual Grounding, Weakly Supervised Learning",
        "abstract": "Current large multimodal models (LMMs) face challenges in grounding, which requires the model to relate language components to visual entities. Contrary to the common practice that fine-tunes LMMs with additional grounding supervision, we find that the grounding ability can in fact emerge in LMMs trained without explicit grounding supervision. To reveal this emerging grounding, we introduce an \u201cattend-and-segment\u201d method which leverages attention maps from standard LMMs to perform pixel-level segmentation. Furthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM utilizing a diffusion-based visual encoder, as opposed to the standard CLIP visual encoder, and trained with the same weak supervision. Without being constrained by the biases and limited scale of grounding-specific supervision data, our approach is more generalizable and scalable. We achieve competitive performance on both grounding-specific and general visual question answering benchmarks, compared with grounding LMMs and generalist LMMs, respectively. Notably, we achieve a 44.2 grounding mask recall on grounded conversation generation without any grounding supervision, outperforming the extensively supervised model GLaMM."
    },
    {
        "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
        "link_suffix": "/forum?id=cpGPPLLYYx",
        "link": "https://openreview.net/forum?id=cpGPPLLYYx",
        "pdf_link": "https://openreview.net/pdf?id=cpGPPLLYYx",
        "keywords": "Vision-Language Models, In-Context Learning, LLMs",
        "abstract": "Large language models (LLMs) famously exhibit emergent in-context learning (ICL) - the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into multimodal ICL have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}. We evaluate the abilities of state-of-the-art VLLMs against this benchmark suite, revealing their diverse strengths and weaknesses, and showing that even the most advanced models, such as GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks, and the associated strengths and limitations of existing models, we hope that our dataset will inspire future work on enhancing the in-context learning capabilities of VLLMs, as well as inspire new applications that leverage VLLM ICL."
    },
    {
        "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
        "link_suffix": "/forum?id=vHO9mU87dc",
        "link": "https://openreview.net/forum?id=vHO9mU87dc",
        "pdf_link": "https://openreview.net/pdf?id=vHO9mU87dc",
        "keywords": "Long-Context LLM Inference, KV Cache Optimization",
        "abstract": "With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch sizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory."
    },
    {
        "title": "Camera Pose Estimation Emerging In Video Diffusion Transformer",
        "link_suffix": "/forum?id=lgf2LW7fOJ",
        "link": "https://openreview.net/forum?id=lgf2LW7fOJ",
        "pdf_link": "https://openreview.net/pdf?id=lgf2LW7fOJ",
        "keywords": "video generation, video diffusion model, DiT, camera tracking",
        "abstract": "Diffusion-based video generators are now a reality. Being trained on a large corpus of real videos, such models can generate diverse yet realistic videos (Brooks et al., 2024; Zheng et al., 2024). Given that the videos appear visually coherent across camera changes, we ask, do the underlying generators implicitly learn camera registrations? Hence, we propose a novel adaptation to repurpose the intermediate features of the generator for camera pose estimation by linking them to the SoTA camera calibration decoder of DUSt3R (Wang et al., 2024a). This effectively unifies the video generation and camera estimation into a single framework. On top of unifying two different networks into one, our architecture can directly be trained on real video and simultaneously produces correspondence, with respect to the first frame, for all the video frames. Our final model, named JOG3R can be used in text-to-video mode, and additionally it produces camera pose estimates at a quality on par with the SoTA model DUSt3R, which was trained exclusively for camera pose estimation. We report that the synergy between video generation and 3D camera reconstruction tasks leads to around 25% better FVD scores with JOG3R against pretrained OpenSora."
    },
    {
        "title": "U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models",
        "link_suffix": "/forum?id=sy1lbQxj9J",
        "link": "https://openreview.net/forum?id=sy1lbQxj9J",
        "pdf_link": "https://openreview.net/pdf?id=sy1lbQxj9J",
        "keywords": "diffusion model, denoising, belief propagation, generative hierarchical models",
        "abstract": "U-Nets are among the most widely used architectures in computer vision, renowned for their exceptional performance in applications such as image segmentation, denoising, and diffusion modeling. However, a theoretical explanation of the U-Net architecture design has not yet been fully established.This paper introduces a novel interpretation of the U-Net architecture by studying certain generative hierarchical models, which are tree-structured graphical models extensively utilized in both language and image domains. With their encoder-decoder structure, long skip connections, and pooling and up-sampling layers, we demonstrate how U-Nets can naturally implement the belief propagation denoising algorithm in such generative hierarchical models, thereby efficiently approximating the denoising functions. This leads to an efficient sample complexity bound for learning the denoising function using U-Nets within these models. Additionally, we discuss the broader implications of these findings for diffusion models in generative hierarchical models. We also demonstrate that the conventional architecture of convolutional neural networks (ConvNets) is ideally suited for classification tasks within these models. This offers a unified view of the roles of ConvNets and U-Nets, highlighting the versatility of generative hierarchical models in modeling complex data distributions."
    },
    {
        "title": "Encoder-only Next Token Prediction",
        "link_suffix": "/forum?id=6ApaDkSMtX",
        "link": "https://openreview.net/forum?id=6ApaDkSMtX",
        "pdf_link": "https://openreview.net/pdf?id=6ApaDkSMtX",
        "keywords": "LLM, Transformer",
        "abstract": "Next-token prediction models have predominantly relied on decoder-only Transformers with causal attention, driven by the common belief that causal attention is essential to prevent \"cheating\" by masking future tokens. We challenge this widely accepted notion and argue that this design choice is about efficiency rather than necessity. While decoder-only Transformers are still a good choice for practical reasons, they are not the only viable option. In this work, we introduce Encoder-only Next Token Prediction (ENTP). We explore the differences between ENTP and decoder-only Transformers in expressive power and complexity, highlighting potential advantages of ENTP. We introduce the Triplet-Counting task and show, both theoretically and experimentally, that while ENTP can perform this task easily, a decoder-only Transformer cannot. Finally, we empirically demonstrate ENTP\u2019s superior performance across various realistic tasks, such as length generalization and in-context learning."
    },
    {
        "title": "Controlled LLM Decoding via Discrete Auto-regressive Biasing",
        "link_suffix": "/forum?id=Duuerhutvq",
        "link": "https://openreview.net/forum?id=Duuerhutvq",
        "pdf_link": "https://openreview.net/pdf?id=Duuerhutvq",
        "keywords": "LLMs, controlled decoding, MCMC",
        "abstract": "Controlled text generation allows for enforcing user-defined constraints on large language model outputs, an increasingly important field as LLMs become more prevalent in everyday life. One common approach uses energy-based decoding, which defines a target distribution through an energy function that combines multiple constraints into a weighted average. However, these methods often struggle to balance fluency with constraint satisfaction, even with extensive tuning of the energy function's coefficients. In this paper, we identify that this suboptimal balance arises from sampling in continuous space rather than the natural discrete space of text tokens. To address this, we propose \\emph{Discrete Auto-regressive Biasing}, a controlled decoding algorithm that leverages gradients while operating entirely in the discrete text domain.\nSpecifically, we introduce a new formulation for controlled text generation by defining a joint distribution over the generated sequence and an auxiliary bias sequence. To efficiently sample from this joint distribution, we propose a Langevin-within-Gibbs sampling algorithm using gradient-based discrete MCMC. Our method significantly improves constraint satisfaction while maintaining comparable or better fluency, all with lower computational costs. We demonstrate the advantages of our controlled decoding method on sentiment control, language detoxification, and keyword-guided generation."
    },
    {
        "title": "Language Models are Graph Learners",
        "link_suffix": "/forum?id=GURRWHkPtx",
        "link": "https://openreview.net/forum?id=GURRWHkPtx",
        "pdf_link": "https://openreview.net/pdf?id=GURRWHkPtx",
        "keywords": "Language model, Data augmentation, Node classification",
        "abstract": "Language Models (LMs) are increasingly challenging the dominance of domain-specific models, including Graph Neural Networks (GNNs) and Graph Transformers (GTs), in graph learning tasks. Following this trend, we propose a novel approach that empowers off-the-shelf LMs to achieve performance comparable to state-of-the-art GNNs on node classification tasks, without requiring any architectural modification. By preserving the LM's original architecture, our approach retains a key benefit of LM instruction tuning: the ability to jointly train on diverse datasets, fostering greater flexibility and efficiency. To achieve this, we introduce two key augmentation strategies: (1) Enriching LMs' input using topological and semantic retrieval methods, which provide richer contextual information, and (2) guiding the LMs' classification process through a lightweight GNN classifier that effectively prunes class candidates. Our experiments on real-world datasets show that backbone Flan-T5 models equipped with these augmentation strategies outperform state-of-the-art text-output node classifiers and are comparable to top-performing vector-output node classifiers. By bridging the gap between specialized task-specific node classifiers and general LMs, this work paves the way for more versatile and widely applicable graph learning models. We will open-source the code upon publication."
    },
    {
        "title": "A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals",
        "link_suffix": "/forum?id=xCkgX4Xfu0",
        "link": "https://openreview.net/forum?id=xCkgX4Xfu0",
        "pdf_link": "https://openreview.net/pdf?id=xCkgX4Xfu0",
        "keywords": "exploration, emergent skills, contrastive reinforcement learning, open-ended learning",
        "abstract": "In this paper, we present empirical evidence of skills and directed exploration emerging from a simple RL algorithm long before any successful trials are observed. For example, in a manipulation task, the agent is given a single observation of the goal state (see Fig. 1) and learns skills, first for moving its end-effector, then for pushing the block, and finally for picking up and placing the block. These skills emerge before the agent has ever successfully placed the block at the goal location and without the aid of any reward functions, demonstrations, or manually-specified distance metrics. Once the agent has learned to reach the goal state reliably, exploration is reduced. Implementing our method involves a simple modification of prior work and does not require density estimates, ensembles, or any additional hyperparameters. Intuitively, the proposed method seems like it should be terrible at exploration, and we lack a clear theoretical understanding of why it works so effectively, though our experiments provide some hints."
    },
    {
        "title": "Efficient Dictionary Learning with Switch Sparse Autoencoders",
        "link_suffix": "/forum?id=k2ZVAzVeMP",
        "link": "https://openreview.net/forum?id=k2ZVAzVeMP",
        "pdf_link": "https://openreview.net/pdf?id=k2ZVAzVeMP",
        "keywords": "interpretability, mixture of experts, sparse autoencoders",
        "abstract": "Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational challenge. In this work, we introduce $\\textbf{Switch Sparse Autoencoders}$, a novel SAE architecture aimed at reducing the compute cost of training SAEs. Inspired by sparse mixture of experts models, Switch SAEs route activation vectors between smaller \"expert\" SAEs, enabling SAEs to efficiently scale to many more features. We present experiments comparing Switch SAEs with other SAE architectures, and find that Switch SAEs deliver a substantial Pareto improvement in the reconstruction vs. sparsity frontier for a given fixed training compute budget. We also study the geometry of features across experts, analyze features duplicated across experts, and verify that Switch SAE features are as interpretable as features found by other SAE architectures."
    },
    {
        "title": "Scaling Wearable Foundation Models",
        "link_suffix": "/forum?id=yb4QE6b22f",
        "link": "https://openreview.net/forum?id=yb4QE6b22f",
        "pdf_link": "https://openreview.net/pdf?id=yb4QE6b22f",
        "keywords": "Health, Foundation Model, Scaling",
        "abstract": "Wearable sensors have become ubiquitous thanks to a variety of health tracking features. The resulting continuous and longitudinal measurements from everyday life generate large volumes of data; however, making sense of these observations for scientific and actionable insights is non-trivial. Inspired by the empirical success of generative modeling, where large neural networks learn powerful representations from vast amounts of text, image, video, or audio data, we investigate the scaling properties of sensor foundation models across compute, data, and model size. Using a dataset of up to 40 million hours of in-situ heart rate, heart rate variability, electrodermal activity, accelerometer, skin temperature, and altimeter per-minute data from over 165,000 people, we create LSM, a multimodal foundation model built on the largest wearable-signals dataset with the most extensive range of sensor modalities to date. Our results establish the scaling laws of LSMs for tasks such as imputation, interpolation and extrapolation, both across time and sensor modalities. Moreover, we highlight how LSMs enables sample-efficient downstream learning for tasks like exercise and activity recognition."
    },
    {
        "title": "Offline Reinforcement Learning with Closed-loop Policy Evaluation and Diffusion World-Model Adaptation",
        "link_suffix": "/forum?id=1zuJZ1jGvT",
        "link": "https://openreview.net/forum?id=1zuJZ1jGvT",
        "pdf_link": "https://openreview.net/pdf?id=1zuJZ1jGvT",
        "keywords": "reinforcement learning, offline reinforcement learning, model-based reinforcement learning, diffusion model",
        "abstract": "Generative models, particularly diffusion models, have been utilized as world models in offline reinforcement learning (RL) to generate synthetic data, enhancing policy learning efficiency. Current approaches either train diffusion models once before policy learning begins or rely on online interactions for alignment. In this paper, we propose a novel offline RL algorithm, Adaptive Diffusion World Model for Policy Evaluation (ADEPT), which integrates closed-loop policy evaluation with world model adaptation. It employs an uncertainty-penalized diffusion model to iteratively interact with the target policy for evaluation. The uncertainty of the world model is estimated by comparing the output generated with different noises, which is then used to constrain out-of-distribution actions. During policy training, the diffusion model performs importance-sampled updates to progressively align with the evolving policy. We analyze the performance of the proposed method and provide an upper bound on the return gap between our method and the real environment under an optimal policy. The results shed light on various key factors affecting learning performance. Evaluations on the D4RL benchmark demonstrate significant improvement over state-of-the-art baselines, especially when only sub-optimal demonstrations are available -- thus requiring improved alignment between the world model and offline policy evaluation."
    },
    {
        "title": "Zero-Shot Learning of Causal Models",
        "link_suffix": "/forum?id=x3F8oPxKV2",
        "link": "https://openreview.net/forum?id=x3F8oPxKV2",
        "pdf_link": "https://openreview.net/pdf?id=x3F8oPxKV2",
        "keywords": "Causality, Transformers, Generative Models",
        "abstract": "With the increasing acquisition of datasets over time, we now have access to precise and varied descriptions of the world, capturing all sorts of phenomena.\nThese datasets can be seen as empirical observations of unknown causal generative processes, or Structural Causal Models (SCMs).\nRecovering these causal generative processes from observations poses formidable challenges, and often require to learn a specific generative model for each dataset.\nIn this work, we propose to learn a \\emph{single} model capable of inferring in a zero-shot manner the causal generative processes of datasets. \nRather than learning a specific SCM for each dataset, we enable FiP, the architecture proposed in~\\cite{scetbon2024fip}, to infer the generative SCMs conditionally on their empirical representations.\nMore specifically, we propose to amortize the learning of a conditional version of FiP to infer directly the generative SCMs from observations and causal structures on synthetically generated datasets.\nWe show that our model is capable of predicting in zero-shot the true generative SCMs, and as a by-product, of (i) generating new dataset samples, and (ii) inferring intervened ones.\nOur experiments demonstrate that our amortized procedure achieves performances on par with SoTA methods trained specifically for each dataset on both in and out-of-distribution problems. \nTo the best of our knowledge, this is the first time that SCMs are inferred in a zero-shot manner from observations, paving the way for a paradigmatic shift towards the assimilation of causal knowledge across datasets."
    },
    {
        "title": "Hierarchical Information Flow for Generalized Efficient Image Restoration",
        "link_suffix": "/forum?id=C0Ubo0XBPn",
        "link": "https://openreview.net/forum?id=C0Ubo0XBPn",
        "pdf_link": "https://openreview.net/pdf?id=C0Ubo0XBPn",
        "keywords": "Hierarchical information flow, image restoration, tree structure",
        "abstract": "While vision transformers show promise in numerous image restoration (IR) tasks, the challenge remains in efficiently generalizing and scaling up a model for multiple IR tasks. To strike a balance between efficiency and model capacity for a generalized transformer-based IR method, we propose a hierarchical information flow mechanism for image restoration, dubbed Hi-IR, which progressively propagates information propagation among pixels in a bottom-up manner. Hi-IR constructs a hierarchical information tree representing the degraded image across three levels. Each level encapsulates different types of information, with higher levels encompassing broader objects and concepts and lower levels focusing on local details. Moreover, the hierarchical tree architecture removes long-range self-attention, improves the computational efficiency and memory utilization, thus preparing it for effective model scaling. Based on that, we explore model scaling to improve our method's capabilities, which is expected to positively impact IR in large-scale training settings. Extensive experimental results show that Hi-IR achieves state-of-the-art performance in seven common image restoration tasks, affirming its effectiveness and generalizability."
    },
    {
        "title": "Tabby: Tabular Adaptation for Language Models",
        "link_suffix": "/forum?id=2RNGX3iTr6",
        "link": "https://openreview.net/forum?id=2RNGX3iTr6",
        "pdf_link": "https://openreview.net/pdf?id=2RNGX3iTr6",
        "keywords": "tabular, generative, llm, mixture-of-experts, synthesis, transformer",
        "abstract": "While advances in large language models (LLMs) have greatly improved the quality of synthetic text data in recent years, synthesizing tabular data has received far less attention. Many of the top-performing approaches to this problem rely on techniques that adapt models originally developed for other modalities, potentially leaving generative performance on the table. We address these disparities in attention and performance for tabular data by introducing Tabby, a simple but powerful post-training modification to the standard Transformer-based language model architecture that enables its use for tabular dataset synthesis. Tabby relies on Gated Mixture-of-Experts layers, allowing each data column to be modeled by a dedicated set of parameters within the transformer multi-layer perceptrons or language modeling heads.  Applying Tabby to Distilled-GPT2 improves synthetic data quality up to 7% compared to previous tabular dataset synthesis methods, achieving performance near or equal to that of real data."
    },
    {
        "title": "Guiding Skill Discovery with Foundation Models",
        "link_suffix": "/forum?id=nZBUtzJhf8",
        "link": "https://openreview.net/forum?id=nZBUtzJhf8",
        "pdf_link": "https://openreview.net/pdf?id=nZBUtzJhf8",
        "keywords": "reinforcement learning, unsupervised skill discovery, foundation models",
        "abstract": "Learning diverse skills without hand-crafted reward functions could potentially accelerate reinforcement learning in downstream tasks. However, existing skill discovery methods focus solely on maximizing the diversity of skills without considering human preferences, which leads to undesirable behaviors and possibly dangerous skills. For instance, a cheetah robot trained using previous methods learns to roll in all directions to maximize skill diversity, whereas we would prefer it to run without flipping or entering hazardous areas. In this work, we propose aFoundation modelGuided (FoG) skill discovery method, which incorporates human intentions into skill discovery through foundation models. Specifically, FoG extracts a score function from foundation models to evaluate states based on human intentions, assigning higher values to desirable states and lower to undesirable ones. These scores are then used to re-weight the rewards of skill discovery algorithms. By optimizing the re-weighted skill discovery rewards, FoG successfully learns to eliminate undesirable behaviors, such as flipping or rolling, and to avoid hazardous areas in both state-based and pixel-based tasks. Interestingly, we show that FoG can discover skills involving behaviors that are difficult to define. Interactive visualisations are available from:https://sites.google.com/view/iclr-fog"
    },
    {
        "title": "Asynchronous Federated Reinforcement Learning with Policy Gradient Updates: Algorithm Design and Convergence Analysis",
        "link_suffix": "/forum?id=5DUekOKWcS",
        "link": "https://openreview.net/forum?id=5DUekOKWcS",
        "pdf_link": "https://openreview.net/pdf?id=5DUekOKWcS",
        "keywords": "Federated Learning, Reinforcement Learning, Asynchronous",
        "abstract": "To improve the efficiency of reinforcement learning (RL), we propose a novel asynchronous federated reinforcement learning (FedRL) framework termed AFedPG, which constructs a global model through collaboration among $N$ agents using policy gradient (PG) updates. To address the challenge of lagged policies in asynchronous settings, we design a delay-adaptive lookahead technique \\textit{specifically for FedRL} that can effectively handle heterogeneous arrival times of policy gradients. We analyze the theoretical global convergence bound of AFedPG, and characterize the advantage of the proposed algorithm in terms of both the sample complexity and time complexity. Specifically, our AFedPG method achieves $\\mathcal{O}(\\frac{{\\epsilon}^{-2.5}}{N})$ sample complexity for global convergence at each agent on average. Compared to the single agent setting with $\\mathcal{O}(\\epsilon^{-2.5})$ sample complexity, it enjoys a linear speedup with respect to the number of agents. Moreover, compared to synchronous FedPG, AFedPG improves the time complexity from $\\mathcal{O}(\\frac{t_{\\max}}{N})$ to $\\mathcal{O}({\\sum_{i=1}^{N} \\frac{1}{t_{i}}})^{-1}$, where $t_{i}$ denotes the time consumption in each iteration at agent $i$, and $t_{\\max}$ is the largest one. The latter complexity $\\mathcal{O}({\\sum_{i=1}^{N} \\frac{1}{t_{i}}})^{-1}$ is always smaller than the former one, and this improvement becomes significant in large-scale federated settings with heterogeneous computing powers ($t_{\\max}\\gg t_{\\min}$). Finally, we empirically verify the improved performance of AFedPG in four widely-used MuJoCo environments with varying numbers of agents. We also demonstrate the advantages of AFedPG in various computing heterogeneity scenarios."
    },
    {
        "title": "Almost Sure Reasoning: Generating Verified Formalizations with Language Models and Logical Solvers",
        "link_suffix": "/forum?id=aNf8VCQE0h",
        "link": "https://openreview.net/forum?id=aNf8VCQE0h",
        "pdf_link": "https://openreview.net/pdf?id=aNf8VCQE0h",
        "keywords": "Automated Reasoning, SAT/SMT Solvers, Formal Methods, Large Language Models",
        "abstract": "Robustness of reasoning remains a challenging problem for large language models, and addressing it is crucial for advancing the reliability and practical application of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately translate the reasoning problem from natural language to the formal language of the solver. SSV produces strong abstract formalizations of problems  by verifying and refining them against concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has $\\textit{near-perfect}$ precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such $\\textit{almost sure reasoning}$ as a new approach that can reduce the need for manual human verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems."
    },
    {
        "title": "Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds",
        "link_suffix": "/forum?id=5BSlakturs",
        "link": "https://openreview.net/forum?id=5BSlakturs",
        "pdf_link": "https://openreview.net/pdf?id=5BSlakturs",
        "keywords": "Diffusion models, text-to-image generation",
        "abstract": "Text-to-image diffusion models have demonstrated remarkable capability in generating realistic images from arbitrary text prompts. However, they often produce inconsistent results for compositional prompts such as \"two dogs\" or \"a penguin on the right of a bowl\". Understanding these inconsistencies is crucial for reliable image generation. In this paper, we highlight the significant role of initial noise in these inconsistencies, where certain noise patterns are more reliable for compositional prompts than others. Our analyses reveal that different initial random seeds tend to guide the model to place objects in distinct image areas, potentially adhering to specific patterns of camera angles and image composition associated with the seed. To improve the model's compositional ability, we propose a method for mining these reliable cases, resulting in a curated training set of generated images without requiring any manual annotation. \nBy fine-tuning text-to-image models on these images, we significantly enhance their compositional capabilities. For numerical composition, we observe relative increases of 29.3% and 19.5% for Stable Diffusion and PixArt-$\\alpha$, respectively. Spatial composition sees even larger gains, with 60.7% for Stable Diffusion and 21.1% for PixArt-$\\alpha$."
    },
    {
        "title": "Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment",
        "link_suffix": "/forum?id=6kPBThI6ZJ",
        "link": "https://openreview.net/forum?id=6kPBThI6ZJ",
        "pdf_link": "https://openreview.net/pdf?id=6kPBThI6ZJ",
        "keywords": "multimodal, diffusion model, image generation, lora, mllm, stable diffusion, mme, hoi, tta",
        "abstract": "While diffusion models are powerful in generating high-quality, diverse synthetic data for object-centric tasks, existing methods struggle with scene-aware tasks such as Visual Question Answering (VQA) and Human-Object Interaction (HOI) Reasoning, where it is critical to preserve scene attributes in generated images consistent with a multimodal context, i.e. a reference image with accompanying text guidance query. To address this, we introduce $\\textbf{Hummingbird}$, the first diffusion-based image generator which, given a multimodal context, generates highly diverse images w.r.t. to the reference image while ensuring high fidelity by accurately preserving scene attributes, such as object interactions and spatial relationships from the text guidance. Hummingbird employs a novel Multimodal Context Evaluator that simultaneously optimizes our formulated Global Semantic and Fine-grained Consistency Rewards to ensure generated images preserve the scene attributes of reference images in relation to the text guidance while maintaining diversity. As the first model to address the task of maintaining both diversity and fidelity given multimodal context, we introduce a new benchmark formulation incorporating MME Perception and Bongard HOI datasets. Benchmark experiments show that Hummingbird outperforms all existing methods by achieving superior fidelity while maintaining diversity, validating Hummingbird's potential as a robust multimodal context-aligned image generator in complex visual tasks."
    },
    {
        "title": "An Expressive Quantum-Driven Graph Learning Approach with Application to Mixed-integer Linear Programming",
        "link_suffix": "/forum?id=IQi8JOqLuv",
        "link": "https://openreview.net/forum?id=IQi8JOqLuv",
        "pdf_link": "https://openreview.net/pdf?id=IQi8JOqLuv",
        "keywords": "Quantum machine learning, Mixed-integer linear programming, graph learning",
        "abstract": "Graph-structure data is ubiquitous, and graph learning models have recently been further extended to address complex problems like mixed-integer linear programming (MILP). However, recent literature has shown that classic graph neural networks (GNNs) suffer fundamental limitations in learning MILP graph representation, i.e., GNNs may map two different MILP graphs to the same representation. To overcome the limitations of classical GNNs, we introduce an expressive quantum-driven graph learning approach, leveraging quantum machine learning (QML) to recognize patterns that are difficult for classical methods to learn. Specifically, the proposed versatile quantum graph learning architecture, composed of a node feature layer, a graph message interaction layer, and an optional auxiliary layer. Its versatility is reflected in effectively encoding features of nodes and edges while ensuring node permutation equivariance and flexibly creating different circuit structures for various expressive requirements and downstream tasks. VQGLA is well suited for learning complex graph tasks like MILP representation. Experimental results highlight the effectiveness of VQGLA in capturing and learning representations for MILPs. In comparison to traditional GNNs, VQGLA exhibits superior discriminative capabilities and demonstrates enhanced generalization across various problem instances, making it a more promising solution for complex optimization tasks."
    },
    {
        "title": "Personalized Visual Instruction Tuning",
        "link_suffix": "/forum?id=sAxdIJ4l6z",
        "link": "https://openreview.net/forum?id=sAxdIJ4l6z",
        "pdf_link": "https://openreview.net/pdf?id=sAxdIJ4l6z",
        "keywords": "large language model, multimodal large language model",
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress; however, these models exhibit a notable limitation, which we refer to as \"face blindness.\" Specifically, they can engage in general conversations but fail to conduct personalized dialogues targeting at specific individuals. This deficiency hinders the application of MLLMs in personalized settings, such as tailored visual assistants on mobile devices, or domestic robots that need to recognize members of the family. In this paper, we introduce Personalized Visual Instruction Tuning (PVIT), a novel data curation and training framework designed to enable MLLMs to identify target individuals within an image and engage in personalized and coherent dialogues. Our approach involves the development of a sophisticated pipeline that autonomously generates training data containing personalized conversations. This pipeline leverages the capabilities of various visual experts, image generation models, and (multi-modal) large language models. To evaluate the personalized potential of MLLMs, we present a benchmark called P-Bench, which encompasses various question types with different levels of difficulty. The experiments demonstrate a substantial personalized performance enhancement after fine-tuning with our curated dataset."
    }
]