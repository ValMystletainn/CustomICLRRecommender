[{"title": "What's Wrong With Non-Autoregressive Graph Neural Networks in Neural Combinatorial Optimization", "link_suffix": "/forum?id=WszeEzjcq2", "link": "https://openreview.net/forum?id=WszeEzjcq2", "pdf_link": "https://openreview.net/pdf?id=WszeEzjcq2", "keywords": "graph neural network, combinatorial optimization, supervised learning", "abstract": "Neural combinatorial optimization (NCO) leverages machine learning models to tackle complex combinatorial problems by learning heuristics or direct solution construction. Graph Neural Networks (GNNs) are particularly effective for NCO due to their ability to capture the relational structure inherent in many such problems. In this work, we examine the supervised non-autoregressive (NAR) solution construction framework, revealing a misalignment between training objective and solution quality. Specifically, through experiments on six GNN architectures across three problems\u2014Traveling Salesperson Problem (TSP), Maximum Independent Set (MIS), and Minimum Vertex Cover (MVC)\u2014we show that lower training loss does not correlate with lower optimality gap. To address this, we propose a supervised autoregressive (AR) framework that leverages the conditional dependencies between variables by training to complete partial solutions. Empirical results show that the proposed AR framework does not exhibit the same misalignment and consistently improves performance. We further compare the proposed AR framework against existing supervised GNN-based methods and achieve superior performance, especially in terms of generalizing to larger problem instances.", "title_embedding_index": 10000, "title_abs_embedding_index": 10025}, {"title": "Learn from Known Unknowns: A Unified Empirical Bayesian Framework for Improving Group Robustness", "link_suffix": "/forum?id=VGRiMWRRCs", "link": "https://openreview.net/forum?id=VGRiMWRRCs", "pdf_link": "https://openreview.net/pdf?id=VGRiMWRRCs", "keywords": "Group Robustness, Spurious Correlations, Shortcut Learning", "abstract": "The lack of group robustness has emerged as a critical concern in machine learning, as conventional methods like Empirical Risk Minimization (ERM) can achieve high overall accuracy while yielding low worst-group accuracy in minority groups. This issue often stems from spurious correlations\u2014non-essential features that models exploit as shortcuts\u2014which can compromise deep learning models in high-stakes applications. Previous works have found that simply retraining classifiers with reweighted datasets or rebalanced samples could significantly improve robustness. However, existing methods lack a unified framework, as they often exhibit inconsistent performance across datasets, and sometimes rely heavily on hyperparameter tuning, making them impractical for real-world datasets. In this work, we first argue that existing methods can be unified as one Empirical Bayesian framework, where a priori of group information is not specified. We then propose our method \\textit{Learn from Known Unknowns} under this framework by quantifying the epistemic uncertainty of biased ERM models and introducing a selective reweighting technique for retraining. Our empirical results demonstrate that this approach improves group robustness across diverse datasets and reduces reliance on hyperparameter tuning, offering a more efficient and scalable solution to spurious correlations.", "title_embedding_index": 10001, "title_abs_embedding_index": 10026}, {"title": "ACTIVE TEST TIME PROMPT LEARNING IN VISION- LANGUAGE MODELS", "link_suffix": "/forum?id=pdzHpQbGrn", "link": "https://openreview.net/forum?id=pdzHpQbGrn", "pdf_link": "https://openreview.net/pdf?id=pdzHpQbGrn", "keywords": "Vision-Language Models, Prompt Learning, Active Learning, Test-Time Adaptation", "abstract": "Test Time Optimisation is a setting where a model is made to learn new parameters on-the-fly during inference with the help of those very samples it is supposed to be tested on. Learning prompts at test time to improve the performance of Vision Language Models(VLMs) in downstream tasks has become a popular setting in recent times. In this paper, we propose a new framework for the Test Time Prompt Tuning in Pre-trained VLMs which incorporates actively sampled labels in the learning process to improve the performance of the model in downstream test-time settings. Our problem setting is underexplored yet well-motivated by considerations such as performance, efficiency and real-life applicability. Active Learning can be especially beneficial in the test-time setting in providing the option to query the true label when the model is uncertain in a real-life scenario and Prompt Tuning provides the advantage due to parameter efficiency. Our method is guided by these two principles and successfully combines the two to come up with a test-time optimisation scheme that is evaluated to be an improvement over existing methods under a fair evaluation protocol.  We conduct experiments across 10 cross-dataset transfer datasets and 4 domain-generalisation datasets to show consistent improvement over the state-of-the-art.", "title_embedding_index": 10002, "title_abs_embedding_index": 10027}, {"title": "Comparing and Contrasting Deep Learning Weather Prediction Backbones on Navier-Stokes and Atmospheric Dynamics", "link_suffix": "/forum?id=7dmsy2Vd5h", "link": "https://openreview.net/forum?id=7dmsy2Vd5h", "pdf_link": "https://openreview.net/pdf?id=7dmsy2Vd5h", "keywords": "deep learning weather prediction, benchmark, navier-stokes, weatherbench, controlled experiment", "abstract": "Remarkable progress in the development of Deep Learning Weather Prediction (DLWP) models positions  them  to become competitive with traditional numerical weather prediction (NWP) models. Indeed, a wide number of DLWP architectures---based on various backbones, including U-Net, Transformer, Graph Neural Network (GNN), and Fourier Neural Operator (FNO)---have demonstrated their potential at forecasting atmospheric states. However, due to differences in training protocols, forecast horizons, and data choices, it remains unclear which (if any) of these methods and architectures are most suitable for weather forecasting and for future model development. Here, we step back and provide a detailed empirical analysis, under controlled conditions, comparing and contrasting the most prominent DLWP models, along with their backbones. We accomplish this by predicting  synthetic two-dimensional incompressible Navier-Stokes and real-world global weather dynamics. In terms of accuracy, memory consumption, and runtime, our results illustrate various tradeoffs. For example, on synthetic data, we observe favorable performance of FNO; and on the real-world WeatherBench dataset, our results demonstrate the suitability of ConvLSTM and SwinTransformer for short-to-mid-ranged forecasts. For long-ranged weather rollouts of up to 365 days, we observe superior stability and physical soundness in architectures that formulate a spherical data representation, i.e., GraphCast and Spherical FNO. In addition, we observe that all of these model backbones ``saturate,'' i.e., none of them exhibit so-called neural scaling, which highlights an important direction for future work on these and related models. The code is available at \\url{https://anonymous.4open.science/r/dlwp-benchmark-F88C}.", "title_embedding_index": 10003, "title_abs_embedding_index": 10028}, {"title": "Finetuning CLIP to Reason about Pairwise Differences", "link_suffix": "/forum?id=UyBMzsFThf", "link": "https://openreview.net/forum?id=UyBMzsFThf", "pdf_link": "https://openreview.net/pdf?id=UyBMzsFThf", "keywords": "CLIP, finetuning, vision language models", "abstract": "Vision-language models (VLMs) such as CLIP are trained via contrastive learning between text and image pairs, resulting in aligned image and text embeddings that are useful for many downstream tasks. A notable drawback of CLIP, however, is that the resulting embedding space seems to lack some of the structure of their purely text-based alternatives. For instance, while text embeddings have been long noted to satisfy analogies in embedding space using vector arithmetic, CLIP has no such property. In this paper, we propose an approach to natively train CLIP in a contrastive manner to reason about differences in embedding space. We finetune CLIP so that the differences in image embedding space correspond to text descriptions of the image differences, which we synthetically generate with large language models on image-caption paired datasets. We first demonstrate that our approach yields significantly improved capabilities in ranking images by a certain attribute (e.g., elephants are larger than cats), which is useful in retrieval or constructing attribute-based classifiers, and improved zeroshot classification performance on many downstream image classification tasks. In addition, our approach enables a new mechanism for inference that we refer to as comparative prompting, where we leverage prior knowledge of text descriptions of differences between classes of interest, achieving even larger performance gains in classification. Finally, we illustrate that the resulting embeddings obey a larger degree of geometric properties in embedding space, such as in text-to-image generation.", "title_embedding_index": 10004, "title_abs_embedding_index": 10029}, {"title": "A simple connection from loss flatness to compressed representations in neural networks", "link_suffix": "/forum?id=j7yeq2sOj3", "link": "https://openreview.net/forum?id=j7yeq2sOj3", "pdf_link": "https://openreview.net/pdf?id=j7yeq2sOj3", "keywords": "Sharpness, Flatness, compression, dimensionality", "abstract": "The generalization capacity of deep neural networks has been studied in a variety of ways, including at least two distinct categories of approaches: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). Although these two approaches are related, they are rarely studied together explicitly. Here, we present an analysis that bridges this gap. We show that in the final phase of learning in deep neural networks, the compression of the manifold of neural representations correlates with the flatness of the loss around the minima explored by SGD. This correlation is predicted by a relatively simple mathematical relationship: a flatter loss corresponds to a lower upper bound on the compression metrics of neural representations. Our work builds upon the linear stability insight by Ma and Ying, deriving inequalities between various compression metrics and quantities involving sharpness. Empirically, our derived inequality predicts a consistently positive correlation between representation compression and loss sharpness in multiple experimental settings. Overall, we advance a dual perspective on generalization in neural networks in both parameter and feature space.", "title_embedding_index": 10005, "title_abs_embedding_index": 10030}, {"title": "Bayes-Nash Generative Privacy Protection Against Membership Inference Attacks", "link_suffix": "/forum?id=o4X6UM18rI", "link": "https://openreview.net/forum?id=o4X6UM18rI", "pdf_link": "https://openreview.net/pdf?id=o4X6UM18rI", "keywords": "privacy, game theory, membership inference attack, Bayes Nash equilibrium", "abstract": "An ability to share data, even in aggregated form, is critical to advancing both conventional and data science. However, insofar as such datasets are comprised of individuals, their membership in these datasets is often viewed as sensitive, with membership inference attacks (MIAs) threatening to violate their privacy. We propose a Bayesian game model for privacy-preserving publishing of data-sharing mechanism outputs (for example, summary statistics for sharing genomic data). In this game, the defender minimizes a combination of expected utility and privacy loss, with the latter being maximized by a Bayes-rational attacker. We propose a GAN-style algorithm to approximate a Bayes-Nash equilibrium of this game, and introduce the notions of Bayes-Nash generative privacy (BNGP) and Bayes generative privacy (BGP) risk that aims to optimally balance the defender's privacy and utility in a way that is robust to the attacker's heterogeneous preferences with respect to true and false positives. We demonstrate the properties of composition and post-processing for BGP risk and establish conditions under which BNGP and pure differential privacy (PDP) are equivalent. We apply our method to sharing genomic summary statistics, where MIAs can re-identify individuals even from aggregated data. Theoretical analysis and empirical results demonstrate that our Bayesian game-theoretic method outperforms state-of-the-art approaches for privacy-preserving sharing of summary statistics.", "title_embedding_index": 10006, "title_abs_embedding_index": 10031}, {"title": "Neural Deconstruction Search for Vehicle Routing Problems", "link_suffix": "/forum?id=SrnTGdJKYG", "link": "https://openreview.net/forum?id=SrnTGdJKYG", "pdf_link": "https://openreview.net/pdf?id=SrnTGdJKYG", "keywords": "neural combinatorial optimization; vehicle routing problem; reinforcement learning; neural deconstruction", "abstract": "Autoregressive construction approaches generate solutions to vehicle routing problems in a step-by-step fashion, leading to high-quality solutions that are nearing the performance achieved by handcrafted, operations research techniques.\nIn this work, we challenge the conventional paradigm of sequential solution construction and introduce an iterative search framework where solutions are instead deconstructed by a neural policy. Throughout the search, the neural policy collaborates with a simple greedy insertion algorithm to rebuild the deconstructed solutions. Our approach surpasses the performance of state-of-the-art operations research methods across three challenging vehicle routing problems of various problem sizes.", "title_embedding_index": 10007, "title_abs_embedding_index": 10032}, {"title": "Securing Multimodal Large Language Models: Defending Against Jailbreak Attacks with Adversarial Tuning", "link_suffix": "/forum?id=BHTgbGSCXu", "link": "https://openreview.net/forum?id=BHTgbGSCXu", "pdf_link": "https://openreview.net/pdf?id=BHTgbGSCXu", "keywords": "multimodal large language models, jailbreak, defense", "abstract": "While multimodal large language models (MLLMs) have achieved remarkable success in recent advancements, their susceptibility to jailbreak attacks has come to light. In such attacks, adversaries exploit carefully crafted prompts to coerce models into generating harmful or undesirable content. Existing defense mechanisms often rely on external inference steps or safety alignment training, both of which are less effective and impractical when facing sophisticated adversarial perturbations in white-box scenarios. To address these challenges and bolster MLLM robustness, we introduce SafeMLLM, a novel adversarial tuning framework. SafeMLLM operates in two stages during each training iteration: (1) generating adversarial perturbations through a newly proposed contrastive embedding attack (CoE-Attack), which optimizes token embeddings under a contrastive objective, and (2) updating model parameters to neutralize the perturbation effects while preserving model utility on benign inputs. We evaluate SafeMLLM across six MLLMs and six jailbreak methods spanning multiple modalities. Experimental results show that SafeMLLM effectively defends against diverse attacks, maintaining robust performance without compromising normal interactions with users.", "title_embedding_index": 10008, "title_abs_embedding_index": 10033}, {"title": "Learning Actionable Counterfactual Explanations in Large State Spaces", "link_suffix": "/forum?id=byIsedbVo5", "link": "https://openreview.net/forum?id=byIsedbVo5", "pdf_link": "https://openreview.net/pdf?id=byIsedbVo5", "keywords": "counterfactual explanations, recourse, data-driven algorithms, fairness", "abstract": "An increasing number of high-stakes domains rely on machine learning to make decisions that have significant consequences for individuals, such as in loan approvals and college admissions.  The black-box nature of these processes has led to a growing demand for solutions that make individuals aware of potential ways they could improve their qualifications.\nCounterfactual explanations (CFEs) are one form of feedback commonly used to provide insight into decision-making systems. Specifically, contemporary CFE generators provide explanations in the form of low-level CFEs whose constituent actions precisely describe how much a negatively classified individual should add or subtract from their input features to achieve the desired positive classification.\nHowever, the low-level CFE generators have several shortcomings: they are hard to scale, often misaligned with real-world conditions, constrained by information access (e.g., they can not query the classifier), and make inadequate use of available historical data. \nTo address these challenges, we propose three data-driven CFE generators that create generalizable CFEs with desirable characteristics for individuals and decision-makers.\nThrough extensive empirical experiments, we compare the proposed CFE generators with a low-level CFE generator on four real-world (BRFSS, Foods, and two NHANES datasets), five semi-synthetic, and five variants of fully-synthetic datasets. \nOur problem can also be seen as learning an optimal policy in a family of large but deterministic Markov decision processes.", "title_embedding_index": 10009, "title_abs_embedding_index": 10034}, {"title": "The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation", "link_suffix": "/forum?id=Ij9ilPh36h", "link": "https://openreview.net/forum?id=Ij9ilPh36h", "pdf_link": "https://openreview.net/pdf?id=Ij9ilPh36h", "keywords": "LLM, NLP, fine-tuning, open-ended text generation, Hyperfitting, Phenomenon, Neural Networks, Early-stopping, Overfitting", "abstract": "This paper introduces the counter-intuitive generalization results of overfitting pre-trained large language models (LLMs) on very small datasets. In the setting of open-ended text generation, it is well-documented that LLMs tend to generate repetitive and dull sequences, a phenomenon that is especially apparent when generating using greedy decoding. This issue persists even with state-of-the-art LLMs containing billions of parameters, trained via next-token prediction on large datasets. We find that by further fine-tuning these models to achieve a near-zero training loss on a small set of samples -- a process we refer to as hyperfitting -- the long-sequence generative capabilities are greatly enhanced. This phenomenon extends to LLMs of various sizes, different domains, and even autoregressive image generation. We further find this phenomena to be distinctly different from that of Grokking and double descent. Surprisingly, our experiments indicate that hyperfitted models rarely fall into repeating sequences they were trained on, and even explicitly blocking these sequences results in high-quality output. All hyperfitted models produce extremely low-entropy predictions, often allocating nearly all probability to a single token. Interestingly, investigations into the hyperfitting data show that the top candidates emerging from these predictions are not deterministically set by the content of the samples.", "title_embedding_index": 10010, "title_abs_embedding_index": 10035}, {"title": "Learning a Fast Mixing Exogenous Block MDP using a Single Trajectory", "link_suffix": "/forum?id=41WIgfdd5o", "link": "https://openreview.net/forum?id=41WIgfdd5o", "pdf_link": "https://openreview.net/pdf?id=41WIgfdd5o", "keywords": "Reinforcement Learning, Reinforcement Learning Theory, Controllable Representations, Representation Learning, Exogenous Noise, Controllable Latent State, Unsupervised Reinforcement Learning", "abstract": "In order to train agents that can quickly adapt to new objectives or reward functions, efficient unsupervised representation learning in sequential decision-making environments can be important. Frameworks such as the Exogenous Block Markov Decision Process (Ex-BMDP) have been proposed to formalize this representation-learning problem (Efroni et al., 2022b). In the Ex-BMDP framework, the agent's high-dimensional observations of the environment have two latent factors: a controllable factor, which evolves deterministically within a small state space according to the agent's actions, and an exogenous factor, which represents time-correlated noise, and can be highly complex. The goal of the representation learning problem is to learn an encoder that maps from observations into the controllable latent space, as well as the dynamics of this space. Efroni et al. (2022b) has shown that this is possible with a sample complexity that depends only on the size of the controllable latent space, and not on the size of the noise factor. However, this prior work has focused on the episodic setting, where the controllable latent state resets to a specific start state after a finite horizon.By contrast, if the agent can only interact with the environment in a single continuous trajectory, prior works have not established sample-complexity bounds. We propose STEEL, the first provably sample-efficient algorithm for learning the controllable dynamics of an Ex-BMDP from a single trajectory, in the function approximation setting. STEEL has a sample complexity that depends only on the sizes of the controllable latent space and the encoder function class, and (at worst linearly) on the mixing time of the exogenous noise factor. We prove that STEEL is correct and sample-efficient, and demonstrate STEEL on two toy problems.", "title_embedding_index": 10011, "title_abs_embedding_index": 10036}, {"title": "DataGen: Unified Synthetic Dataset Generation via Large Language Models", "link_suffix": "/forum?id=F5R0lG74Tu", "link": "https://openreview.net/forum?id=F5R0lG74Tu", "pdf_link": "https://openreview.net/pdf?id=F5R0lG74Tu", "keywords": "large language model, evaluation, synthetic data", "abstract": "Large Language Models (LLMs) such as GPT-4 and Llama3 have significantly impacted various fields by enabling high-quality synthetic data generation and reducing dependence on expensive human-generated datasets. \nDespite this, challenges remain in the areas of generalization, controllability, diversity, and truthfulness within the existing generative frameworks. To address these challenges, this paper presents DataGen, a comprehensive LLM-powered framework designed to produce diverse, accurate, and highly controllable datasets. DataGen is adaptable, supporting all types of text datasets and enhancing the generative process through innovative mechanisms. To augment data diversity, DataGen incorporates an attribute-guided generation module and a group checking feature. For accuracy, it employs a code-based mathematical assessment for label verification alongside a retrieval-augmented generation technique for factual validation. The framework also allows for user-specified constraints, enabling customization of the data generation process to suit particular requirements. Extensive experiments demonstrate the superior quality of data generated by DataGen, and each module within DataGen plays a critical role in this enhancement. Additionally, DataGen is applied in two practical scenarios: benchmarking LLMs and data augmentation. The results indicate that DataGen effectively supports dynamic and evolving benchmarking and that data augmentation improves LLM capabilities in various domains, including agent-oriented abilities and reasoning skills.", "title_embedding_index": 10012, "title_abs_embedding_index": 10037}, {"title": "Enhancing Deception Detection with Cognitive Load Features: An Audio-Visual Approach", "link_suffix": "/forum?id=WNZNsyzcaB", "link": "https://openreview.net/forum?id=WNZNsyzcaB", "pdf_link": "https://openreview.net/pdf?id=WNZNsyzcaB", "keywords": "deception detection, cognitive load, audio visual", "abstract": "Deception ranges from minor mischief to serious fraud, often leading to significant psychological and financial harm. Effective deception detection is crucial to mitigate these risks and preserve societal trust. Cognitive load is a useful indicator for detecting deception, as lying causes individuals to experience greater mental strain. \nWhile prior research leveraged cognitive load features, typically measured through physiological signals such as pupil dilation, these methods often require specialized equipment and can be subject to human bias. \nThese limitations hinder the scalability and automation of deception detection systems. \nThus, we propose a novel deception detection framework that automatically extracts cognitive load features from audio-visual data, eliminating the need for specialized hardware or subjective human input. \nOur approach integrates these features into the deception detection pipeline, enhancing its robustness. \nMoreover, we introduce a focal loss to address the inherent complexity of deception detection. \nThis objective function enables the model to focus on harder-to-detect instances of deception, thereby improving the performance. \nOur approach achieves state-of-the-art results on benchmark audio-visual datasets, demonstrating significant improvements in automated deception detection. \nExtensive experiments validate the effectiveness of both our cognitive load feature extraction and the proposed objective function in advancing the field.", "title_embedding_index": 10013, "title_abs_embedding_index": 10038}, {"title": "Differentiable Distance Between Hierarchically-Structured Data", "link_suffix": "/forum?id=yDlvteYBbF", "link": "https://openreview.net/forum?id=yDlvteYBbF", "pdf_link": "https://openreview.net/pdf?id=yDlvteYBbF", "keywords": "Distance, Distance function, Tree-structured data, Heterogenous Graphs, JSONs, Multiple Instance Learning", "abstract": "Many machine learning algorithms solving various problems are available for\nmetric spaces. While there are plenty of distances for vector spaces, much\nless exists for structured data (rooted heterogeneous trees) stored in popular\nformats like JSON, XML, ProtoBuffer, MessagePack, etc. This paper\nintroduces the Hierarchically-structured Tree Distance (HTD) designed\nespecially for these data. The HTD distance is modular with differentiable\nparameters weighting the importance of different sub-spaces. This allows\nthe distance to be tailored to a given dataset and task, such as classification,\nclustering, and anomaly detection. The extensive experimental comparison\nshows that distance-based algorithms with the proposed HTD distance\nare competitive to state-of-the-art methods based on neural networks with\norders of magnitude more parameters. Furthermore, we show that HTD is\nmore suited to analyze heterogeneous Graph Neural Networks than Tree\nMover\u2019s Distance.", "title_embedding_index": 10014, "title_abs_embedding_index": 10039}, {"title": "How vulnerable is my learned policy? Adversarial attacks on modern behavioral cloning policies", "link_suffix": "/forum?id=Ju7zj6tUm6", "link": "https://openreview.net/forum?id=Ju7zj6tUm6", "pdf_link": "https://openreview.net/pdf?id=Ju7zj6tUm6", "keywords": "Adversarial Attacks, Learning from Demonstrations, Behavior Cloning", "abstract": "Learning from Demonstration (LfD) algorithms have shown promising results in robotic manipulation tasks, but their vulnerability to adversarial attacks remains underexplored. This paper presents a comprehensive study of adversarial attacks on both classic and recently proposed algorithms, including Behavior Cloning (BC), LSTM-GMM, Implicit Behavior Cloning (IBC), Diffusion Policy (DP), and VQ-Behavior Transformer (VQ-BET). We study the vulnerability of these methods to untargeted, targeted and universal adversarial perturbations. While explicit policies, such as BC, LSTM-GMM and VQ-BET can be attacked in the same manner as standard computer vision models, we find that attacks for implicit and denoising policy models are nuaced and require developing novel attack methods. Our experiments on several simulated robotic manipulation tasks reveal that most of the current methods are highly vulnerable to adversarial perturbations. We also investigate the transferability of attacks across algorithms and architectures, providing insights into the generalizability of adversarial perturbations in LfD. We find that, the success rate of the transfer attacks is highly dependent on the task, raising necessity for more fine-grained metrics that capture intricate details of adversarial weakness of the state distribution. In summary, our findings highlight the vulnerabilities of modern BC algorithms, paving way for future work in addressing such limitations.", "title_embedding_index": 10015, "title_abs_embedding_index": 10040}, {"title": "Language-Image Models with 3D Understanding", "link_suffix": "/forum?id=yaQbTAD2JJ", "link": "https://openreview.net/forum?id=yaQbTAD2JJ", "pdf_link": "https://openreview.net/pdf?id=yaQbTAD2JJ", "keywords": "Multi-modal Large Language Model with 3D Understanding; 3D Image Grounding from Image", "abstract": "Multi-modal large language models (MLLMs) have shown incredible capabilities in a variety of 2D vision and language tasks. We extend MLLMs\u2019 perceptual capabilities to ground and reason about images in 3-dimensional space. To that end, we first develop a large-scale pretraining dataset for 2D and 3D called LV3D by combining multiple existing 2D and 3D recognition datasets under a common task formulation: as multi-turn question-answering. Next, we introduce a new MLLM named CUBE-LLM and pre-train it on LV3D. We show that pure data scaling makes a strong 3D perception capability without 3D specific architectural design or training objective. CUBE-LLM exhibits intriguing properties similar to LLMs: (1) CUBE-LLM can apply chain-of-thought prompting to improve 3D understanding from 2D context information. (2) CUBE-LLM can follow complex and diverse instructions and adapt to versatile input and output formats. (3) CUBE-LLM can be visually prompted such as 2D box or a set of candidate 3D boxes from specialists. Our experiments on outdoor benchmarks demonstrate that CUBE-LLM significantly outperforms existing baselines by 21.3 points of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7 points on the DriveLM dataset for complex reasoning about driving scenarios, respectively. CUBE-LLM also shows competitive results in general MLLM benchmarks such as refCOCO for 2D grounding with (87.0) average score, as well as visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning.", "title_embedding_index": 10016, "title_abs_embedding_index": 10041}, {"title": "Preference Optimization with Multi-Sample Comparisons", "link_suffix": "/forum?id=Ozfu2uBH55", "link": "https://openreview.net/forum?id=Ozfu2uBH55", "pdf_link": "https://openreview.net/pdf?id=Ozfu2uBH55", "keywords": "RLHF, Preference Optimization", "abstract": "Recent advancements in generative models, particularly large language models (LLMs) and diffusion models, have been driven by extensive pretraining on large datasets followed by post-training. However, current post-training methods such as reinforcement learning from human feedback (RLHF) and direct alignment from preference methods (DAP) primarily utilize single-sample comparisons. These approaches often fail to capture critical characteristics such as generative diversity and bias, which are more accurately assessed through multiple samples. To address these limitations, we introduce a novel approach that extends post-training to include multi-sample comparisons. To achieve this, we propose Multi-sample Direct Preference Optimization (mDPO) and Multi-sample Identity Preference Optimization (mIPO). These methods improve traditional DAP methods by focusing on group-wise characteristics. Empirically, we demonstrate that multi-sample comparison is more effective in optimizing collective characteristics~(e.g., diversity and bias) for generative models than single-sample comparison. Additionally, our findings suggest that multi-sample comparisons provide a more robust optimization framework, particularly for dataset with label noise.", "title_embedding_index": 10017, "title_abs_embedding_index": 10042}, {"title": "Differentially Private Federated Learning with Time-Adaptive Privacy Spending", "link_suffix": "/forum?id=W0nydevOlG", "link": "https://openreview.net/forum?id=W0nydevOlG", "pdf_link": "https://openreview.net/pdf?id=W0nydevOlG", "keywords": "Differential Privacy, Federated Learning, Time Adaptive Privacy Spending, Individualized Privacy Constraints", "abstract": "Federated learning (FL) with differential privacy (DP) provides a framework for collaborative machine learning, enabling clients to train a shared model while adhering to strict privacy constraints. The framework allows each client to have an individual privacy guarantee, e.g., by adding different amounts of noise to each client\u2019s model updates. One underlying assumption is that all clients spend their privacy budgets uniformly over time (learning rounds). However, it has been shown in the literature that learning in early rounds typically focuses on more coarse-grained features that can be learned at lower signal-to-noise ratios while later rounds learn fine-grained features that benefit from higher signal-to-noise ratios. Building on this intuition, we propose a time-adaptive DP-FL framework that expends the privacy budget non-uniformly across both time and clients. Our framework enables each client to save privacy budget in early rounds so as to be\nable to spend more in later rounds when additional accuracy is beneficial in learning more fine-grained features. We theoretically prove utility improvements in the case that clients with stricter privacy budgets spend budgets unevenly across rounds, compared to clients with more relaxed budgets, who have sufficient budgets to distribute their spend more evenly. Our practical experiments on standard benchmark datasets support our theoretical results and show that, in practice, our algorithms improve the privacy-utility trade-offs compared to baseline schemes.", "title_embedding_index": 10018, "title_abs_embedding_index": 10043}, {"title": "Divide, Reweight, and Conquer: A Logit Arithmetic Approach for In-Context Learning", "link_suffix": "/forum?id=jqAqZhEMsk", "link": "https://openreview.net/forum?id=jqAqZhEMsk", "pdf_link": "https://openreview.net/pdf?id=jqAqZhEMsk", "keywords": "Efficient Inference, In-context Learning, Non-gradient Optimization, Large Language Models", "abstract": "In-Context Learning (ICL) emerges as a key feature for Large Language Models (LLMs), allowing them to adapt to new tasks by leverageing task-specific examples without updating model parameters. However, ICL faces challenges with increasing numbers of examples due to performance degradation and quadratic computational costs. In this paper, we propose Logit Arithmetic Reweighting\nApproach (LARA), a novel framework that enhances ICL by using logit-based ensembling of multiple demonstrations. Our approach divides long input demonstrations into parallelizable shorter inputs to significantly reduce memory requirements, and then effectively aggregate the information by reweighting logits of each group via a non-gradient optimization approach. We further introduce Bi-\nnary LARA (B-LARA), a variant that constrains weights to binary values to simplify the search space and reduces memory usage by filtering out less informative demonstration groups. Experiments on BBH and MMLU demonstrate that LARA and B-LARA outperform all baseline methods in both accuracy and memory efficiency. We also conduct extensive analysis to show that LARA generalizes well to scenarios of varying numbers of examples from limited to many-shot demonstrations. Our codes can be found inhttps://anonymous.4open.science/r/LARA-F55B.", "title_embedding_index": 10019, "title_abs_embedding_index": 10044}, {"title": "VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for MLLMs", "link_suffix": "/forum?id=5zDU4pFxkg", "link": "https://openreview.net/forum?id=5zDU4pFxkg", "pdf_link": "https://openreview.net/pdf?id=5zDU4pFxkg", "keywords": "multimodal LLM, spatial planning", "abstract": "With the recent introduction of vision understanding capabilities in large language models, multimodal LLMs (MLLMs) have inherited and advanced a series of intriguing capabilities from classical LLMs. Among these capabilities, visual spatial planning - the ability to comprehend the spatial arrangements of objects and devise action plans to achieve specific desired outcomes - remains under-explored in MLLMs. In our study, we introduce VSP, a benchmark specifically designed to 1) evaluate the spatial planning capability in these models in general, and 2) break down the visual planning task into finer-grained sub-tasks, including perception and reasoning, and measure their capabilities in these sub-tasks. Contrary to expectations that MLLMs should naturally process scene images and reason effectively, evaluation on the benchmark shows that both open-source and private MLLMs fail to generate effective plans for even simple spatial planning tasks. The fine-grained analysis further reveals that while MLLMs have flaws in both perception and reasoning, the deficiency in the former capabilities is significantly worse. Evaluations on these tasks reveal fundamental deficiencies in the models\u2019 visual perception and reasoning abilities, explaining their worse performance in the general spatial planning tasks. Our work illuminates future directions for improving multimodal LLMs' abilities in spatial planning.", "title_embedding_index": 10020, "title_abs_embedding_index": 10045}, {"title": "Bilinear MLPs enable weight-based mechanistic interpretability", "link_suffix": "/forum?id=gI0kPklUKS", "link": "https://openreview.net/forum?id=gI0kPklUKS", "pdf_link": "https://openreview.net/pdf?id=gI0kPklUKS", "keywords": "interpretability, mechanistic interpretability, bilinear, feature extraction, weight-based, eigenvector, eigendecomposition, tensor network", "abstract": "A mechanistic understanding of how MLPs do computation in deep neural networks remains elusive. Current interpretability work can extract features from hidden activations over an input dataset but generally cannot explain how MLP weights construct features. One challenge is that element-wise nonlinearities introduce higher-order interactions and make it difficult to trace computations through the MLP layer. In this paper, we analyze bilinear MLPs, a type of Gated Linear Unit (GLU) without any element-wise nonlinearity that nevertheless achieves competitive performance. Bilinear MLPs can be fully expressed in terms of linear operations using a third-order tensor, allowing flexible analysis of the weights. Analyzing the spectra of bilinear MLP weights using eigendecomposition reveals interpretable low-rank structure across toy tasks, image classification, and language modeling. We use this understanding to craft adversarial examples, uncover overfitting, and identify small language model circuits directly from the weights alone. Our results demonstrate that bilinear layers serve as an interpretable drop-in replacement for current activation functions and that weight-based interpretability is viable for understanding deep-learning models.", "title_embedding_index": 10021, "title_abs_embedding_index": 10046}, {"title": "Expressivity of Neural Networks with Random Weights and Learned Biases", "link_suffix": "/forum?id=5xwx1Myosu", "link": "https://openreview.net/forum?id=5xwx1Myosu", "pdf_link": "https://openreview.net/pdf?id=5xwx1Myosu", "keywords": "random neural networks, recurrent neural networks, plasticity, deep learning, neuroscience, multi-task learning", "abstract": "Landmark universal function approximation results for neural networks with trained weights and biases provided impetus for the ubiquitous use of neural networks as learning models in neuroscience and Artificial Intelligence (AI). Recent work has pushed the bounds of universal approximation by showing that arbitrary functions can similarly be learned by tuning smaller subsets of parameters, for example the output weights, within randomly initialized networks. Despite the role of biases in shaping a neural network's response to changes in context, as demonstrated by a wealth of neuroscience literature and in-context learning in AI, it is an open question whether universal approximation results can be shown when only biases are learned. The current paper answers this question. We provide theoretical and numerical evidence demonstrating that feedforward neural networks with fixed random weights can approximate any continuous function on compact sets. We further show an analogous result for the approximation of dynamical systems with recurrent neural networks. Our results are relevant to neuroscience, where they demonstrate the potential for behaviourally relevant changes in dynamics without modifying synaptic weights, as well as for AI, where they shed light on multi-task methods, like bias fine-tuning, and in-context learning.", "title_embedding_index": 10022, "title_abs_embedding_index": 10047}, {"title": "PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer", "link_suffix": "/forum?id=YFxfcQMLWX", "link": "https://openreview.net/forum?id=YFxfcQMLWX", "pdf_link": "https://openreview.net/pdf?id=YFxfcQMLWX", "keywords": "Efficient Vision Transformer, Attention Approximation, Computer Vision, Deep Learning", "abstract": "We present Polynomial Attention Drop-in Replacement (PADRe), a novel and unifying framework designed to replace the conventional self-attention mechanism in transformer models. Notably, several recent alternative attention mechanisms, including Hyena, Mamba, SimA, Conv2Former, and Castling-ViT, can be viewed as specific instances of our PADRe framework.  PADRe leverages polynomial functions and draws upon established results from approximation theory, enhancing computational efficiency without compromising accuracy.  PADRe's key components include multiplicative nonlinearities, which we implement using straightforward, hardware-friendly operations such as Hadamard products, incurring only linear computational and memory costs. PADRe further avoids the need for using complex functions such as Softmax, yet it maintains comparable or superior accuracy compared to traditional self-attention. We assess the effectiveness of PADRe as a drop-in replacement for self-attention across diverse computer vision tasks. These tasks include image classification, image-based 2D object detection, and 3D point cloud object detection. Empirical results demonstrate that PADRe runs significantly faster than the conventional self-attention (11x~43x faster on server GPU and mobile NPU) while maintaining similar accuracy when substituting self-attention in the transformer models.", "title_embedding_index": 10023, "title_abs_embedding_index": 10048}, {"title": "Ladder: Language Driven Slice Discovery and Error Rectification", "link_suffix": "/forum?id=9y8N9D1nMr", "link": "https://openreview.net/forum?id=9y8N9D1nMr", "pdf_link": "https://openreview.net/pdf?id=9y8N9D1nMr", "keywords": "robustness, subgroup analysis, error analysis, error mitigation, multimodal, slice discovery", "abstract": "Error slice discovery is crucial to diagnose and mitigate model errors. Current clustering or discrete attribute-based slice discovery methods face key limitations: 1) clustering results in incoherent slices, while assigning discrete attributes to slices leads to incomplete coverage of error patterns due to missing or insufficient attributes; 2) these methods lack complex reasoning, preventing them from fully explaining model biases; 3) they fail to integrate \\textit{domain knowledge}, limiting their usage in specialized fields \\eg radiology. We propose\\ladder (\\underline{La}nguage-\\underline{D}riven \\underline{D}iscovery and \\underline{E}rror \\underline{R}ectification), to address the limitations by: (1) leveraging the flexibility of natural language to address incompleteness, (2) employing LLM's latent \\textit{domain knowledge} and advanced reasoning to analyze sentences and derive testable hypotheses directly, identifying biased attributes, and form coherent error slices without clustering. Existing mitigation methods typically address only the worst-performing group, often amplifying errors in other subgroups. In contrast,\\ladder generates pseudo attributes from the discovered hypotheses to mitigate errors across all biases without explicit attribute annotations or prior knowledge of bias. Rigorous evaluations on 6 datasets spanning natural and medical images -- comparing 200+ classifiers with diverse architectures, pretraining strategies, and LLMs -- show that\\ladder consistently outperforms existing baselines in discovering and mitigating biases. The code is available\\footnote{\\url{https://github.com/AI-annonymous/ICLR-submission}}.", "title_embedding_index": 10024, "title_abs_embedding_index": 10049}]