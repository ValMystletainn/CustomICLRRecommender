[{"title": "Many-Objective Multi-Solution Transport", "link_suffix": "/forum?id=Neb17mimVH", "link": "https://openreview.net/forum?id=Neb17mimVH", "pdf_link": "https://openreview.net/pdf?id=Neb17mimVH", "keywords": "Multi-Objective Optimization, Mixture of Experts", "abstract": "Optimizing the performance of many objectives (instantiated by tasks or clients) jointly with a few Pareto stationary solutions (models) is critical in machine learning. However, previous multi-objective optimization methods often focus on a few objectives and cannot scale to many objectives that outnumber the solutions, leading to either subpar performance or ignored objectives. We introduce ''Many-objective multi-solution Transport (MosT)'', a framework that finds multiple diverse solutions in the Pareto front of many objectives. Our insight is to seek multiple solutions, each performing as a domain expert and focusing on a specific subset of objectives while collectively covering all of them. MosT formulates the problem as a bi-level optimization of weighted objectives for each solution, where the weights are defined by an optimal transport between objectives and solutions. Our algorithm ensures convergence to Pareto stationary solutions for complementary subsets of objectives. On a range of applications in federated learning, multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly outperforms strong baselines, delivering high-quality, diverse solutions that profile the entire Pareto frontier, thus ensuring balanced trade-offs across many objectives.", "title_embedding_index": 2700, "title_abs_embedding_index": 2725}, {"title": "An Auditing Test to Detect Behavioral Shift in Language Models", "link_suffix": "/forum?id=h0jdAboh0o", "link": "https://openreview.net/forum?id=h0jdAboh0o", "pdf_link": "https://openreview.net/pdf?id=h0jdAboh0o", "keywords": "AI alignment, model auditing, model evaluations, red teaming, sequential hypothesis testing", "abstract": "As language models (LMs) approach human-level performance, comprehensive understanding of their behavior becomes crucial. This includes evaluating capabilities, biases, task performance, and alignment with societal values. Extensive initial evaluations, including red teaming and diverse benchmarking, can establish a model\u2019s behavioral profile. However, subsequent fine-tuning or deployment changes may alter these behaviors in both intended and unintended ways. We present an efficient method for continual Behavioral Shift Auditing (BSA) in LMs. Building on anytime-valid hypothesis testing, our auditing test detects behavioral shifts solely through model generations. It compares outputs from a baseline model to those of the model under scrutiny, providing theoretical guarantees for change detection while controlling false positives. The test features a configurable tolerance parameter, allowing adjustment of sensitivity to behavioral changes for different use cases. We evaluate our approach using two case studies: monitoring changes in (a) toxicity and (b) translation performance. We find that the test is able to detect meaningful changes in behavior distributions using just hundreds of examples. We hope to contribute a valuable tool for AI practitioners, enabling rapid detection of behavioral shifts in deployed LMs, with implications for safety monitoring, quality assurance, and responsible AI development.", "title_embedding_index": 2701, "title_abs_embedding_index": 2726}, {"title": "Enhancing Hallucination Detection with Noise Injection", "link_suffix": "/forum?id=2bWf4M5tRo", "link": "https://openreview.net/forum?id=2bWf4M5tRo", "pdf_link": "https://openreview.net/pdf?id=2bWf4M5tRo", "keywords": "Hallucination Detection; Robustness", "abstract": "Large Language Models (LLMs) are observed to generate plausible yet incorrect responses, known as hallucinations. Effectively detecting such hallucination instances is crucial for the safe deployment of LLMs. Recent research has linked hallucination to model uncertainty, suggesting to detect hallucinations by measuring dispersion over answer distributions obtained from a set of samples drawn from the model.\nWhile using the model's next token probabilities used during training is a natural way to obtain samples, in this work, we argue that for the purpose of hallucination detection, it is overly restrictive and hence sub-optimal. Motivated by this viewpoint, we perform an extensive empirical analysis showing that an alternative way to measure uncertainty - by perturbing hidden unit activations in intermediate layers of the model - is complementary to sampling, and can significantly improve detection accuracy over mere sampling.", "title_embedding_index": 2702, "title_abs_embedding_index": 2727}, {"title": "Blind Unlearning: Unlearning Without a Forget Set", "link_suffix": "/forum?id=KEeTRb8GLf", "link": "https://openreview.net/forum?id=KEeTRb8GLf", "pdf_link": "https://openreview.net/pdf?id=KEeTRb8GLf", "keywords": "machine unlearning, data privacy", "abstract": "Machine unlearning is the study of methods to efficiently remove the influence\nof some subset of the training data from the parameters of a previously-trained\nmodel. Existing methods typically require direct access to the \u201cforget set\u201d \u2013 the\nsubset of training data to be forgotten by the model. This limitation impedes privacy, as organizations need to retain user data for the sake of unlearning when a\nrequest for deletion is made, rather than being able to delete it immediately. We\nfirst introduce the setting of blind unlearning \u2013 unlearning without explicit access\nto the forget set. Then, we propose a method for approximate unlearning called\nRELOAD, that leverages ideas from gradient-based unlearning and neural network\nsparsity to achieve blind unlearning. The method serially applies an ascent step\nwith targeted parameter re-initialization and fine-tuning, and on empirical unlearning tasks, RELOAD often approximates the behaviour of a from-scratch retrained\nmodel better than approaches that leverage the forget set. Finally, we extend the\nblind unlearning setting to blind remedial learning, the task of efficiently updating\na previously-trained model to an amended dataset.", "title_embedding_index": 2703, "title_abs_embedding_index": 2728}, {"title": "All You Need Is A Reference: Cross-modality Referring Segmentation for Abdominal MRI", "link_suffix": "/forum?id=Sz2Ar6EqD5", "link": "https://openreview.net/forum?id=Sz2Ar6EqD5", "pdf_link": "https://openreview.net/pdf?id=Sz2Ar6EqD5", "keywords": "promptable segmentation model, cross-modality, referring segmentation", "abstract": "Multi-modality MRI scans can provide comprehensive diagnoses of abdominal disease but this also introduces new segmentation burdens to derive quantitative imaging biomarkers. In this work, we propose a referring segmentation task where users only need to draw simple scribbles on one modality, called reference modality, to guide the segmentation of both the unseen target modalities and the reference modality. To benchmark the multi-modality segmentation task, we provide a new dataset with 3,277 organs from 534 MRI scans, covering five commonly used MRI modalities. Furthermore, we present a referring segmentation model, CrossMR, to simultaneously segment multiple modalities based on scribbles on reference modality. Experiments demonstrate that our method can achieve comparable performance to the state of the art on one in-distribution reference modality and significantly better generalization ability on four out-of-distribution modalities. This opens a door for efficiently segmenting targets across multiple modalities. The new dataset, code, and trained model weights will be publicly available athttps://ref-seg-mr.github.io/.", "title_embedding_index": 2704, "title_abs_embedding_index": 2729}, {"title": "Visual Prompting with Iterative Refinement for Design Critique Generation", "link_suffix": "/forum?id=mXZ98iNFw2", "link": "https://openreview.net/forum?id=mXZ98iNFw2", "pdf_link": "https://openreview.net/pdf?id=mXZ98iNFw2", "keywords": "User Interface Design Critique, Multimodal LLM, Visual Grounding, Prompting Techniques", "abstract": "Feedback is crucial for every design process, such as user interface (UI) design, and automating design critiques can significantly improve the efficiency of the design workflow. Although existing multimodal large language models (LLMs) excel in many tasks, they often struggle with generating high-quality design critiques---a complex task that requires producing detailed design comments that are visually grounded in a given design's image. Building on recent advancements in iterative refinement of text output and visual prompting methods, we propose an iterative visual prompting approach for UI critique that takes an input UI screenshot and design guidelines and generates a list of design comments, along with corresponding bounding boxes that map each comment to a specific region in the screenshot. The entire process is driven completely by LLMs, which iteratively refine both the text output and bounding boxes using few-shot samples tailored for each step. We evaluated our approach using Gemini-1.5-pro and GPT-4o, and found that human experts generally preferred the design critiques generated by our pipeline over those by the baseline. To assess the generalizability of our approach to other multimodal tasks, we applied our pipeline to open-vocabulary object and attribute detection, and experiments showed that our method also outperformed the baseline, with improvements of up to 82%.", "title_embedding_index": 2705, "title_abs_embedding_index": 2730}, {"title": "Task Vectors are Cross-Modal", "link_suffix": "/forum?id=McqeEcMSzy", "link": "https://openreview.net/forum?id=McqeEcMSzy", "pdf_link": "https://openreview.net/pdf?id=McqeEcMSzy", "keywords": "in-context learning, interpretability, multimodal models, vision and language models, task vectors, function vectors", "abstract": "We investigate the internal representations of vision-and-language models (VLMs) and how they encode task representations. We consider tasks specified through examples or instructions, using either text or image inputs. Surprisingly, we find that conceptually similar tasks are mapped to similar task vector representations, regardless of how they are specified. Our findings suggest that to output answers, tokens in VLMs undergo three distinct phases: input, task, and answer, a process which is consistent across different modalities and specifications. The task vectors we identify in VLMs are general enough to be derived in one modality (e.g., text) and transferred to another (e.g., image). Additionally, we find that ensembling exemplar and instruction based task vectors produce better task representations. Taken together, these insights shed light on the underlying mechanisms of VLMs, particularly their ability to represent tasks in a shared manner across different modalities and task specifications.", "title_embedding_index": 2706, "title_abs_embedding_index": 2731}, {"title": "Gradients protection in federated learning for Biometric authentication", "link_suffix": "/forum?id=uW3tNSx7PZ", "link": "https://openreview.net/forum?id=uW3tNSx7PZ", "pdf_link": "https://openreview.net/pdf?id=uW3tNSx7PZ", "keywords": "federated learning, security, safety, facial authentication", "abstract": "In the context of face recognition models, different facial features contribute unevenly to a model's ability to correctly identify individuals, making some features more critical and, therefore, more susceptible to attacks.\nDeep Gradient Leakage (DGL) is a highly effective attack that recovers private training images from gradient vectors, posing significant privacy challenges in distributed learning systems where clients share gradients. Data augmentation, a technique for artificially manipulating the training set by creating modified copies of existing data, plays a crucial role in improving the accuracy of deep learning models. \nIn this paper, we explore various data augmentation methods to protect original training images, in test time thereby enhancing security in distributed learning systems as well as increasing accuracy during training. Our experiments demonstrate that augmentation methods improve model performance during training on augmented images, and we can use the  same methods during testing as perturbation methods to preserve some features of the image and have safety against DGL.This project has four primary objectives: first, to develop a vision transformer face validation model that trains on distributed devices to ensure privacy; second, to utilize augmentation methods to perturb private images and increase neural network safety; and third, to provide protection against attacks, ensuring that reconstructing attacks cannot extract sensitive information from gradients at any point in the system.\nand lastly we introduce a new novel perturbation method for a multi biometric authentication, system which offers accuracy for identification and guarantees safety and anonymity of entities.", "title_embedding_index": 2707, "title_abs_embedding_index": 2732}, {"title": "Actions Speak Louder Than States: Going Beyond Bayesian Inference in In-Context Reinforcement Learning", "link_suffix": "/forum?id=b5MCteb3w7", "link": "https://openreview.net/forum?id=b5MCteb3w7", "pdf_link": "https://openreview.net/pdf?id=b5MCteb3w7", "keywords": "meta-reinforcement learning, in-context learning, decision-making", "abstract": "In this paper, we investigate in-context learning (ICL) for reinforcement learning (RL), particularly extending beyond Bayesian inference to more advanced and richer learning paradigms in transformers. Transformers have shown promise for few-shot and zero-shot learning, but their capabilities for ICL in RL environments are not well explored. Our work studies the role of task diversity in RL environments on the downstream ICL capabilities of transformers. To do so, we introduce a novel RL benchmark, developed to provide a rich variety of tasks, essential for this exploration. Through this environment, we not only demonstrate the critical role of task diversity in facilitating advanced learning algorithms like transformers but also investigate the effects of model architecture, regularization, and other factors on the learning process. This study marks a pivotal advance in understanding the dynamics of ICL in RL, showcasing how diverse tasks can drive transformer models to surpass traditional learning methods.", "title_embedding_index": 2708, "title_abs_embedding_index": 2733}, {"title": "SWGA: A Distributed Hyperparameter Search Method for Time Series Prediction Models", "link_suffix": "/forum?id=xTrAA3UKPa", "link": "https://openreview.net/forum?id=xTrAA3UKPa", "pdf_link": "https://openreview.net/pdf?id=xTrAA3UKPa", "keywords": "Machine Learning, Deep Learning, Time Series Prediction, Hyperparameter Search, Genetic Algorithms", "abstract": "We propose a distributed hyperparameter search method for time series prediction models named SWGA (Sliding Window Genetic Algorithm). Compared to current genetic algorithms for hyperparameter search, our method has three major advantages: (i) It adopts a configurable sliding window mechanism to effectively combat overfitting from distribution shifts inherent in time series data. (ii) It introduces a warm-up stage using Bayesian optimization-based methods to generate a good initial population. (iii) It supports distributed hyperparameter search across multi-node computing clusters, enhancing both scalability and efficiency. To demonstrate SWGA's efficacy, we conduct hyperparameter search experiments on time series datasets from various domains. The experiment results show that our method consistently finds a hyperparameter configuration that achieves better performance on out-of-sample time series data compared to the traditional genetic algorithm. On average, it reduces the out-of-sample loss by about 56.1%.", "title_embedding_index": 2709, "title_abs_embedding_index": 2734}, {"title": "Customized Procedure Planning in Instructional Videos", "link_suffix": "/forum?id=gnWk0ZF22j", "link": "https://openreview.net/forum?id=gnWk0ZF22j", "pdf_link": "https://openreview.net/pdf?id=gnWk0ZF22j", "keywords": "Customized Procedure Planning, multi-modal models, vision-language models", "abstract": "Generating customized procedures for task planning in instructional videos poses a unique challenge for vision-language models. In this paper, we introduce Customized Procedure Planning in Instructional Videos, a novel task that focuses on generating a sequence of detailed action steps for task completion based on user requirements and the task's initial visual state. Existing methods often neglect customization and user directions, limiting their real-world applicability. The absence of instructional video datasets with step-level state and video-specific action plan annotations has hindered progress in this domain. To address these challenges, we introduce the Customized Procedure Planner (CPP) framework, a causal, open-vocabulary model that leverages a LlaVA-based approach to predict procedural plans based on a task's initial visual state and user directions. To overcome the data limitation, we employ a weakly-supervised approach, using the strong vision-language model GEMINI and the large language model (LLM) GPT-4 to create detailed  video-specific action plans from the benchmark instructional video datasets (COIN, CrossTask), producing pseudo-labels for training. Discussing the limitations of the existing procedure planning evaluation metrics in an open-vocabulary setting, we propose novel automatic LLM-based metrics with few-shot in-context learning to evaluate the customization and planning capabilities of our model, setting a strong baseline. Additionally, we implement an LLM-based objective function to enhance model training for improved customization. Extensive experiments, including human evaluations, demonstrate the effectiveness of our approach, establishing a strong baseline for future research in customized procedure planning.", "title_embedding_index": 2710, "title_abs_embedding_index": 2735}, {"title": "Assessing Episodic Memory in LLMs with Sequence Order Recall Tasks", "link_suffix": "/forum?id=LLtUtzSOL5", "link": "https://openreview.net/forum?id=LLtUtzSOL5", "pdf_link": "https://openreview.net/pdf?id=LLtUtzSOL5", "keywords": "Large Language Models (LLMs), Episodic Memory, Long-Term Memory, Working Memory, Long-context, Memory Evaluation, Memory Benchmark, Memory-augmentation, Retrieval-augmented generation, long-document evaluation, neuroscience, cognitive science, NeuroAI", "abstract": "Current LLM benchmarks focus on evaluating models' memory of facts and semantic relations, primarily assessing semantic aspects of long-term memory. However, in humans, long-term memory also includes episodic memory, which links memories to their contexts, such as the time and place they occurred. The ability to contextualize memories is crucial for many cognitive tasks and everyday functions. This form of memory has not been evaluated in LLMs with existing benchmarks. To address the gap in evaluating memory in LLMs, we introduce Sequence Order Recall Tasks (SORT), which we adapt from tasks used to study episodic memory in cognitive psychology. SORT requires LLMs to recall the correct order of text segments, and provides a general framework that is both easily extendable and does not require any additional annotations. We present an initial evaluation dataset, Book-SORT, comprising 36k pairs of segments extracted from 9 books recently added to the public domain. Based on a human experiment with 155 participants, we show that humans can recall sequence order based on long-term memory of a book. We find that models can perform the task with high accuracy when relevant text is given in-context during the SORT evaluation. However, when presented with the book text only during training, LLMs' performance on SORT falls short. By allowing to evaluate more aspects of memory, we believe that SORT will aid in the emerging development of memory-augmented models.", "title_embedding_index": 2711, "title_abs_embedding_index": 2736}, {"title": "Radar: Fast Long-Context Decoding for Any Transformer", "link_suffix": "/forum?id=ZTpWOwMrzQ", "link": "https://openreview.net/forum?id=ZTpWOwMrzQ", "pdf_link": "https://openreview.net/pdf?id=ZTpWOwMrzQ", "keywords": "Long-context decoding, Large language models, Inference acceleration, Random features", "abstract": "Transformer models have demonstrated exceptional performance across a wide range of applications. Though forming the foundation of Transformer models, the dot-product attention does not scale well to long-context data since its time requirement grows quadratically with context length. In this work, we propose Radar, a training-free approach that accelerates inference by dynamically searching for the most important context tokens. For any pre-trained Transformer, Radar can reduce the decoding time complexity without training or heuristically evicting tokens. Moreover, we provide theoretical justification for our approach, demonstrating that Radar can reliably identify the most important tokens with high probability. We conduct extensive comparisons with the previous methods on a wide range of tasks. The results demonstrate that Radar achieves the state-of-the-art performance across different architectures with reduced time complexity, offering a practical solution for efficient long-context processing of Transformers.", "title_embedding_index": 2712, "title_abs_embedding_index": 2737}, {"title": "Bypassing Skip-Gram Negative Sampling: Dimension Regularization as a More Efficient Alternative for Graph Embeddings", "link_suffix": "/forum?id=SPViZd7rvi", "link": "https://openreview.net/forum?id=SPViZd7rvi", "pdf_link": "https://openreview.net/pdf?id=SPViZd7rvi", "keywords": "graph embeddings, negative sampling, skip gram, dimension regularization", "abstract": "A wide range of graph embedding objectives decompose into two components: one that attracts the embeddings of nodes that are perceived as similar, and another that repels embeddings of nodes that are perceived as dissimilar. Without repulsion, the embeddings would collapse into trivial solutions. Skip-Gram Negative Sampling (SGNS) is a popular and efficient repulsion approach that prevents collapse by repelling each node from a sample of dissimilar nodes. In this work, we show that when repulsion is most needed and the embeddings approach collapse, SGNS node-wise repulsion is, in the aggregate, an approximate re-centering of the node embedding dimensions. Such dimension operations are much more scalable than node operations and yield a simpler geometric interpretation of the repulsion. Our result extends findings from self-supervised learning to the skip-gram model, establishing a connection between skip-gram node contrast and dimension regularization. We use this observation to propose a flexible algorithm augmentation framework that improves the scalability of any existing algorithm using SGNS. The framework prioritizes node attraction and replaces SGNS with dimension regularization. We instantiate this generic framework for LINE and node2vec and show that the augmented algorithms preserve downstream link-prediction performance while reducing GPU memory usage by up to $33.3$% and training time by $22.1$%. Further, for graphs that are globally sparse but locally dense, we show that removing repulsion altogether can improve performance, but, when repulsion is otherwise needed, dimension regularization provides an effective and efficient alternative to SGNS.", "title_embedding_index": 2713, "title_abs_embedding_index": 2738}, {"title": "Transformers meet Neural Algorithmic Reasoners", "link_suffix": "/forum?id=fk4czNKXPC", "link": "https://openreview.net/forum?id=fk4czNKXPC", "pdf_link": "https://openreview.net/pdf?id=fk4czNKXPC", "keywords": "Graph Neural Network, Transformer, Neural Algorithmic Reasoning, Length Generalization", "abstract": "Transformers have revolutionized machine learning with their simple yet effective architecture. Pre-training Transformers on massive text datasets from the Internet has led to unmatched generalization for natural language understanding (NLU) tasks. However, such language models remain fragile when tasked with algorithmic forms of reasoning, where computations must be precise and robust. To address this limitation, we propose a novel approach that combines the Transformer's language understanding with the robustness of graph neural network (GNN)-based neural algorithmic reasoners (NARs). Such NARs proved effective as generic solvers for algorithmic tasks, when specified in graph form. To make their embeddings accessible to a Transformer, we propose a hybrid architecture with a two-phase training procedure, allowing the tokens in the language model to cross-attend to the node embeddings from the NAR. We evaluate our resulting TransNAR model on CLRS-Text, the text-based version of the CLRS-30 benchmark, and demonstrate significant gains over Transformer-only models for algorithmic reasoning, both in and out of distribution. Finally, we empirically show that Transformer-only models distilled from TransNAR models also exhibit improved out-of-distribution generalization capabilities.", "title_embedding_index": 2714, "title_abs_embedding_index": 2739}, {"title": "NETS: A Non-Equilibrium Transport Sampler", "link_suffix": "/forum?id=8NiTKmEzJV", "link": "https://openreview.net/forum?id=8NiTKmEzJV", "pdf_link": "https://openreview.net/pdf?id=8NiTKmEzJV", "keywords": "sampling, measure transport, statistical physics", "abstract": "We propose an algorithm, termed the Non-Equilibrium Transport Sampler (NETS), to sample from unnormalized probability distributions. NETS can be viewed as a variant of annealed importance sampling (AIS) based on Jarzynski's equality, in which the stochastic differential equation used to perform the non-equilibrium sampling is augmented with an additional learned drift term that lowers the impact of the unbiasing weights used in AIS. We show that this drift is the minimizer of a variety of objective functions, which can all be estimated in an unbiased fashion without backpropagating through solutions of the stochastic differential equations governing the sampling. We also prove that some these objectives control the Kullback-Leibler divergence of the estimated distribution from its target. NETS is shown to be unbiased and, in addition, has a tunable diffusion coefficient which can be adjusted post-training to maximize the effective sample size. We demonstrate the efficacy of the method on standard benchmarks, high-dimensional Gaussian mixture distributions, and a model from statistical lattice field theory, for which it surpasses the performances of related work and existing baselines.", "title_embedding_index": 2715, "title_abs_embedding_index": 2740}, {"title": "Mixing It Up:  The Cocktail Effect of Multi-Task Fine-Tuning on LLM Performance - A Case Study in Finance", "link_suffix": "/forum?id=VoHJTTA2MB", "link": "https://openreview.net/forum?id=VoHJTTA2MB", "pdf_link": "https://openreview.net/pdf?id=VoHJTTA2MB", "keywords": "LLM, Finetuning, Domain Adaptation, Finance, Benchmarks", "abstract": "The application of large language models (LLMs) in domain-specific contexts, including finance, has expanded rapidly. Domain-specific LLMs are typically evaluated based on their performance in various downstream tasks relevant to the domain. In this work, we present a detailed analysis of fine-tuning LLMs for such tasks. Somewhat counterintuitively, we find that in domain-specific cases, fine-tuning exclusively on the target task is not always the most effective strategy. Instead, multi-task fine-tuning - where models are trained on a cocktail of related tasks - can significantly enhance performance. We demonstrate how this approach enables a small model, such as Phi-3-Mini, to achieve state-of-the-art results, even surpassing the much larger GPT-4-o model on financial benchmarks. Our study involves a large-scale experiment, training over 200 models using several widely adopted LLMs as baselines, and empirically confirms the benefits of multi-task fine-tuning. Additionally, we explore the use of general instruction data as a form of regularization, suggesting that it helps minimize performance degradation. We also investigate the inclusion of mathematical data, finding improvements in numerical reasoning that transfer effectively to financial tasks. Finally, we note that while fine-tuning for downstream tasks leads to targeted improvements in task performance, it does not necessarily result in broader gains in domain knowledge or complex domain reasoning abilities.", "title_embedding_index": 2716, "title_abs_embedding_index": 2741}, {"title": "Can Knowledge Editing Really Correct Hallucinations?", "link_suffix": "/forum?id=hmDt068MoZ", "link": "https://openreview.net/forum?id=hmDt068MoZ", "pdf_link": "https://openreview.net/pdf?id=hmDt068MoZ", "keywords": "LLMs, Knowledge Editing, Hallucination, Benchmark", "abstract": "Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across different tasks. Meanwhile, knowledge editing has become a burgeoning paradigm that is designed to correct the erroneous factual knowledge encoded in LLMs for its advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? Then, we proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire more future improvements and facilitate the progress in the field of knowledge editing. Data and code are available at here.", "title_embedding_index": 2717, "title_abs_embedding_index": 2742}, {"title": "INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge", "link_suffix": "/forum?id=k3gCieTXeY", "link": "https://openreview.net/forum?id=k3gCieTXeY", "pdf_link": "https://openreview.net/pdf?id=k3gCieTXeY", "keywords": "evaluation, multilinguality, large language models", "abstract": "The performance differential of large language models (LLM) between languages hinders their effective deployment in many regions, inhibiting the potential economic and societal value of generative AI tools in many communities. However, the development of functional LLMs in many languages (i.e., multilingual LLMs) is bottlenecked by the lack of high-quality evaluation resources in languages other than English. Moreover, current practices in multilingual benchmark construction often translate English resources, ignoring the regional and cultural knowledge of the environments in which multilingual systems would be used. In this work, we construct an evaluation suite of 197,243 QA pairs from local exam sources to measure the capabilities of multilingual LLMs in a variety of regional contexts.\nOur novel resource, INCLUDE, is a comprehensive knowledge- and reasoning-centric benchmark across 44 written languages that evaluates multilingual LLMs for performance in the actual language environments where they would be deployed.", "title_embedding_index": 2718, "title_abs_embedding_index": 2743}, {"title": "SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems", "link_suffix": "/forum?id=1R5BcYS8EC", "link": "https://openreview.net/forum?id=1R5BcYS8EC", "pdf_link": "https://openreview.net/pdf?id=1R5BcYS8EC", "keywords": "surrogate models, multimodal text and timeseries models, language-interfaced regression", "abstract": "Surrogate models are used to predict the behavior of complex energy systems that are too expensive to simulate with traditional numerical methods. \nOur work introduces the use of language descriptions, which we call \"system captions\" or SysCaps, to interface with such surrogates. \nWe argue that interacting with surrogates through text, particularly natural language, makes these models more accessible for both experts and non-experts.\nWe introduce a lightweight multimodal text and timeseries regression model and a training pipeline that uses large language models (LLMs) to synthesize high-quality captions from simulation metadata. \nOur experiments on two real-world simulators of buildings and wind farms show that our SysCaps-augmented surrogates have better accuracy on held-out systems than traditional methods while enjoying new generalization abilities, such as handling semantically related descriptions of the same test system.\nAdditional experiments also highlight the potential of SysCaps to unlock language-driven design space exploration and to regularize training through prompt augmentation.", "title_embedding_index": 2719, "title_abs_embedding_index": 2744}, {"title": "Robust Thompson Sampling Algorithms Against Reward Poisoning Attacks", "link_suffix": "/forum?id=EpmbH6DpJI", "link": "https://openreview.net/forum?id=EpmbH6DpJI", "pdf_link": "https://openreview.net/pdf?id=EpmbH6DpJI", "keywords": "Robust Bandit Algorithm, Thompson Sampling", "abstract": "Thompson sampling is one of the most popular learning algorithms for online sequential decision-making problems and has rich real-world applications. However, current Thompson sampling algorithms are limited by the assumption that the rewards received are uncorrupted, which may not be true in real-world applications where adversarial reward poisoning exists. To make Thompson sampling more reliable, we want to make it robust against adversarial reward poisoning. The main challenge is that one can no longer compute the actual posteriors for the true reward, as the agent can only observe the rewards after corruption. In this work, we solve this problem by computing pseudo-posteriors that are less likely to be manipulated by the attack. We propose robust algorithms based on Thompson sampling for the popular stochastic and contextual linear bandit settings in both cases where the agent is aware or unaware of the budget of the attacker. We theoretically show that our algorithms guarantee near-optimal regret under any attack strategy.", "title_embedding_index": 2720, "title_abs_embedding_index": 2745}, {"title": "Generative Classifiers Avoid Shortcut Solutions", "link_suffix": "/forum?id=oCUYc7BzXQ", "link": "https://openreview.net/forum?id=oCUYc7BzXQ", "pdf_link": "https://openreview.net/pdf?id=oCUYc7BzXQ", "keywords": "distribution shift, shortcut, generative models, robustness", "abstract": "Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on standard image and text distribution shift benchmarks and reduce the impact of spurious correlations present in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that affect when generative classifiers outperform discriminative ones.", "title_embedding_index": 2721, "title_abs_embedding_index": 2746}, {"title": "Compute-Constrained Data Selection", "link_suffix": "/forum?id=4es2oO9tw1", "link": "https://openreview.net/forum?id=4es2oO9tw1", "pdf_link": "https://openreview.net/pdf?id=4es2oO9tw1", "keywords": "Data Selection, Compute-constrained, Scaling Laws.", "abstract": "Data selection can reduce the amount of training data needed to fine-tune LLMs; however, the efficacy of data selection scales directly with its compute. Motivated by the practical challenge of compute-constrained fine-tuning, we consider the setting in which both the cost of selecting data and training are budgeted for. We first formalize the problem of data selection with a cost-aware utility function, and model the data selection problem as trading off initial selection cost for training gain. We run a comprehensive sweep of experiments across multiple tasks, varying compute budget by scaling fine-tuning tokens, model sizes, and data selection compute. These experiments support the proposed model of data selection as a balancing problem between the benefits to training speed and the additional cost of selection. Interestingly we find that many powerful data selection methods are almost never compute-optimal, and that cheaper data selection alternatives dominate both from a theoretical and empirical perspective.", "title_embedding_index": 2722, "title_abs_embedding_index": 2747}, {"title": "softmax is not enough (for sharp out-of-distribution)", "link_suffix": "/forum?id=wMj6PgKVuJ", "link": "https://openreview.net/forum?id=wMj6PgKVuJ", "pdf_link": "https://openreview.net/pdf?id=wMj6PgKVuJ", "keywords": "softmax, transformers, out-of-distribution, sharpness, entropy", "abstract": "A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from \"circuits\" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. In this paper, we dispel this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. We attribute this to a fundamental limitation of the softmax function to robustly approximate sharp functions, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.", "title_embedding_index": 2723, "title_abs_embedding_index": 2748}, {"title": "Beyond the Boundaries of Proximal Policy Optimization", "link_suffix": "/forum?id=9soA8GWQ9g", "link": "https://openreview.net/forum?id=9soA8GWQ9g", "pdf_link": "https://openreview.net/pdf?id=9soA8GWQ9g", "keywords": "Reinforcement learning, optimization, proximal policy optimization", "abstract": "Proximal policy optimization (PPO) is a widely-used algorithm for on-policy reinforcement learning. This work offers an alternative perspective of PPO, in which it is decomposed into the inner-loop estimation of update vectors, and the outer-loop application of updates using gradient ascent with unity learning rate. Using this insight we propose outer proximal policy optimization (outer-PPO); a framework wherein these update vectors are applied using an arbitrary gradient-based optimizer. The decoupling of update estimation and update application enabled by outer-PPO highlights several implicit design choices in PPO that we challenge through empirical investigation. In particular we consider non-unity learning rates and momentum applied to the outer loop, and a momentum-bias applied to the inner estimation loop. Methods are evaluated against an aggressively tuned PPO baseline on Brax, Jumaji and MinAtar environments; non-unity learning rates and momentum both achieve statistically significant improvement on Brax and Jumaji, given the same hyperparameter tuning budget.", "title_embedding_index": 2724, "title_abs_embedding_index": 2749}]