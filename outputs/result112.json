[
    {
        "title": "Scalable and Enhanced Hallucination Detection in LLMs using Semantic Clustering",
        "link_suffix": "/forum?id=GXzwq6waYb",
        "link": "https://openreview.net/forum?id=GXzwq6waYb",
        "pdf_link": "https://openreview.net/pdf?id=GXzwq6waYb",
        "keywords": "Hallucination, Semantic entropy, LLMs, Semantic clustering",
        "abstract": "Large language models (LLMs) are increasingly being adopted across various domains, driven by their ability to generate general-purpose and domain-specific text. However, LLMs can also produce responses that seem plausible but are factually incorrectâ€”a phenomenon commonly referred to as \"hallucination\". This issue limits the potential and trustworthiness of LLMs, especially in critical fields such as medicine and law. Among the strategies proposed to address this problem uncertainty-based methods stand out due to their ease of implementation, independence from external data sources, and compatibility with standard LLMs. In this paper, we present an optimized semantic clustering framework for automated hallucination detection in LLMs, using sentence embeddings and hierarchical clustering. Our proposed method enhances both scalability and performance compared to existing approaches across different LLM models. This results in more homogeneous clusters, improved entropy scores, and a more accurate reflection of detected hallucinations. Our approach significantly boosts accuracy on widely used open and closed-book question-answering datasets such as TriviaQA, NQ, SQuAD, and BioASQ, achieving AUROC score improvements of up to 9.3% over the current state-of-the-art semantic entropy method. Further ablation studies highlight the effectiveness of different components of our approach."
    },
    {
        "title": "The Foundations of Tokenization: Statistical and Computational Concerns",
        "link_suffix": "/forum?id=B5iOSxM2I0",
        "link": "https://openreview.net/forum?id=B5iOSxM2I0",
        "pdf_link": "https://openreview.net/pdf?id=B5iOSxM2I0",
        "keywords": "Tokenization, Language Models, Consistency, NLP, Theoretical Foundations, Stochastic Maps, Category Theory",
        "abstract": "Tokenization--the practice of converting strings of characters from an alphabet into sequences of tokens over a vocabulary--is a critical step in the NLP pipeline. The use of token representations is widely credited with increased model performance but is also the source of many undesirable behaviors, such as spurious ambiguity or inconsistency. Despite its recognized importance as a standard representation method in NLP, the theoretical underpinnings of tokenization are not yet fully understood. In particular, the impact of tokenization on statistical estimation has been investigated mostly through empirical means. The present paper contributes to addressing this theoretical gap by proposing a unified formal framework for representing and analyzing tokenizer models. Based on the category of stochastic maps, this framework enables us to establish general conditions for a principled use of tokenizers, and most importantly, the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. Additionally, we discuss statistical and computational concerns crucial for designing and implementing tokenizer models, such as inconsistency, ambiguity, tractability, and boundedness. The framework and results advanced in this paper contribute to building robust theoretical foundations for representations in neural language modeling that can inform future empirical research."
    },
    {
        "title": "AskChart: Universal Chart Understanding through Textual Enhancement",
        "link_suffix": "/forum?id=SFkIb7pb8x",
        "link": "https://openreview.net/forum?id=SFkIb7pb8x",
        "pdf_link": "https://openreview.net/pdf?id=SFkIb7pb8x",
        "keywords": "Chart Understanding, Multimodal Learning, Vision-Language Models, Visual-Textual Representation, Instruction Tuning",
        "abstract": "Chart understanding tasks such as ChartQA and Chart-to-Text involve automatically extracting and interpreting key information from charts, enabling users to query or convert visual data into structured formats. State-of-the-art approaches primarily focus on visual cues from chart images, failing toexplicitlyincorporate rich textual information (e.g., data labels and axis labels) embedded within the charts. This textual information is vital for intuitive human comprehension and interpretation of charts. Moreover, existing models are often large and computationally intensive, limiting their practical applicability. In this paper, we introduce AskChart, a universal model thatexplicitlyintegrates bothtextualandvisualcues from charts using a sparse Mixture of Experts (MoE) architecture. AskChart facilitates the learning of enhanced visual-textual representations of charts for effectively handling multiple chart understanding tasks, while maintaining a smaller model size. To capture the synergy between visual and textual modalities, we curate a large-scale dataset named ChartBase with about 7.5M data samples, which helps align textual and visual information and facilitates the extraction of visual entities and text. To effectively train AskChart, we design a three-stage training strategy to align visual and textual modalities embedded within charts for learning robust visual-textual representations and optimizing the learning of the MoE layer for advancing chart understanding. Extensive experiments across five datasets demonstrate the significant performance gains of AskChart in four chart understanding tasks. Remarkably, AskChart with 4.6B parameters outperforms state-of-the-art models with 13B parameters by 68.3% in Open-ended ChartQA and 49.2% in Chart-to-Text tasks, while achieving comparable performance in ChartQA and Chart-to-Table tasks."
    },
    {
        "title": "On Representing Convex Quadratically Constrained Quadratic Programs via Graph Neural Networks",
        "link_suffix": "/forum?id=68J0pJFCi3",
        "link": "https://openreview.net/forum?id=68J0pJFCi3",
        "pdf_link": "https://openreview.net/pdf?id=68J0pJFCi3",
        "keywords": "Quadratically Constrained Quadratic Programs, Graph Neural Networks, Tripartite Graph Representation",
        "abstract": "Convex quadratically constrained quadratic programs (QCQPs) involve finding a solution within a convex feasible region defined by quadratic constraints while minimizing a convex quadratic objective function. These problems arise in various industrial applications, including power systems and signal processing. Traditional methods for solving convex QCQPs primarily rely on matrix factorization, which quickly becomes computationally prohibitive as the problem size increases. Recently, graph neural networks (GNNs) have gained attention for their potential in representing and solving various optimization problems such as linear programs and linearly constrained quadratic programs. In this work, we are the first to investigate the representation power of GNNs in the context of QCQP tasks. Specifically, we propose a new tripartite graph representation for general convex QCQPs and properly associate it with message-passing GNNs. We demonstrate that there exist GNNs capable of reliably representing key properties of convex QCQPs, including feasibility, optimal value, and optimal solution. Our result deepens the understanding of the connection between QCQPs and GNNs, paving the way for future machine learning approaches to efficiently solve QCQPs."
    },
    {
        "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering System",
        "link_suffix": "/forum?id=KQsR2JrCwx",
        "link": "https://openreview.net/forum?id=KQsR2JrCwx",
        "pdf_link": "https://openreview.net/pdf?id=KQsR2JrCwx",
        "keywords": "Question Answering System; Retrieval Augmented Generation; Document Query Gap; Conical Distribution Hypothesis; Document Fingerprint",
        "abstract": "Modern QA systems entail retrieval-augmented generation (RAG) for accurate and trustworthy responses. However, the inherent gap between user queries and relevant documents hinders precise matching. Motivated by our conical distribution hypothesis, which posits that potential queries and documents form a cone-like structure in the embedding space, we introduce QAEncoder, a training-free approach to bridge this gap. Specifically, QAEncoder estimates the expectation of potential queries in the embedding space as a robust surrogate for the document embedding, and attaches document fingerprints to effectively distinguish these embeddings. Extensive experiments on fourteen embedding models across six languages and eight datasets validate QAEncoder's alignment capability, which offers a plug-and-play solution that seamlessly integrates with existing RAG architectures and training-based methods."
    },
    {
        "title": "Variational Inference for Self-Supervised Speech Models Fine-tuning on Downstream Tasks",
        "link_suffix": "/forum?id=UT5B7fktaw",
        "link": "https://openreview.net/forum?id=UT5B7fktaw",
        "pdf_link": "https://openreview.net/pdf?id=UT5B7fktaw",
        "keywords": "SSL models, Fine-tuning, Variational Inference, SER, ASR, SV",
        "abstract": "Despite the growing interest in self-supervised speech models, recent research has primarily focused on modifying upstream model architectures and pretraining techniques, with less attention given to how features from self-supervised models are used. In this paper, we explore the use of variational inference to enhance the performance of self-supervised audio models in downstream tasks. We hypothesize that adaptively reweighting the outputs of the model layers is crucial to improving performance on these tasks. We extensively evaluate our method alongside widely used baselines, demonstrating that understanding sample-specific information is essential for improved performance on several tasks. Our proposed method surpasses existing approaches and generalizes to various speech tasks, including automatic speech recognition, speaker verification, and emotion recognition. Finally, we analyze our method to provide deeper insight into the importance of our modifications."
    },
    {
        "title": "LLMs for Generalizable Language-Conditioned Policy Learning under Minimal Data Requirements",
        "link_suffix": "/forum?id=zAzzMOaisF",
        "link": "https://openreview.net/forum?id=zAzzMOaisF",
        "pdf_link": "https://openreview.net/pdf?id=zAzzMOaisF",
        "keywords": "Large Language Models, Language-conditioned policy, Offline policy learning, Decison Making Agent, Goals generalization, Domain generalization",
        "abstract": "To develop autonomous agents capable of executing complex, multi-step decision-making tasks as specified by humans in natural language, existing reinforcement learning approaches typically require expensive labeled datasets or access to real-time experimentation. Moreover, conventional methods often face difficulties in generalizing to unseen goals and states, thereby limiting their practical applicability. This paper presents TEDUO, a novel training pipeline for offline language-conditioned policy learning. TEDUO operates on easy-to-obtain, unlabeled datasets and is suited for the so-called in-the-wild evaluation, wherein the agent encounters previously unseen goals and states. To address the challenges posed by such data and evaluation settings, our method leverages the prior knowledge and instruction-following capabilities of large language models (LLMs) to enhance the fidelity of pre-collected offline data and enable flexible generalization to new goals and states. Empirical results demonstrate that the dual role of LLMs in our frameworkâ€”as data enhancers and generalizersâ€”facilitates both effective and data-efficient learning of generalizable language-conditioned policies."
    },
    {
        "title": "Implicit degree bias in the link prediction task",
        "link_suffix": "/forum?id=K9zedJlybd",
        "link": "https://openreview.net/forum?id=K9zedJlybd",
        "pdf_link": "https://openreview.net/pdf?id=K9zedJlybd",
        "keywords": "link prediction, graph machine learning, sampling bias, benchmark",
        "abstract": "Link prediction---a task of distinguishing actual hidden edges from random unconnected node pairs---is one of the quintessential tasks in graph machine learning. Despite being widely accepted as a universal benchmark and a downstream task for representation learning, the link prediction benchmark's validity has rarely been questioned. Here, we show that the common edge sampling procedure in the link prediction task has an implicit bias toward high-degree nodes. This produces a highly skewed evaluation that favors methods overly dependent on node degree. In fact  a ``null'' link prediction method based solely on node degree can yield nearly optimal performance in this setting. We propose a degree-corrected link prediction benchmark that offers a more reasonable assessment and better aligns with the performance on the recommendation task. Finally, we demonstrate that the degree-corrected benchmark can more effectively train graph machine-learning models by reducing overfitting to node degrees and facilitating the learning of relevant structures in graphs."
    },
    {
        "title": "HyResPINNs: Adaptive Hybrid Residual Networks for Learning Optimal Combinations of Neural and RBF Components for Physics-Informed Modeling",
        "link_suffix": "/forum?id=5rfj85bHCy",
        "link": "https://openreview.net/forum?id=5rfj85bHCy",
        "pdf_link": "https://openreview.net/pdf?id=5rfj85bHCy",
        "keywords": "physics-informed neural networks, residual networks, partial differential equations, radial basis function networks",
        "abstract": "Physics-informed neural networks (PINNs) are an increasingly popular class of techniques for the numerical solution of partial differential equations (PDEs), where neural networks are trained using loss functions regularized by relevant PDE terms to enforce physical constraints. We present a new class of PINNs called HyResPINNs, which augment traditional PINNs with adaptive hybrid residual blocks that combine the outputs of a standard neural network and a radial basis function (RBF) network. A key feature of our method is the inclusion of adaptive combination parameters within each residual block, which dynamically learn to weigh the contributions of the neural network and RBF network outputs. Additionally, adaptive connections between residual blocks allow for flexible information flow throughout the network. We show that HyResPINNs are more robust to training point locations and neural network architectures than traditional PINNs. Moreover, HyResPINNs offer orders of magnitude greater accuracy than competing methods on certain problems, with only modest increases in training costs. We demonstrate the strengths of our approach on challenging PDEs, including the Allen-Cahn equation and the Darcy-Flow equation. Our results suggest that HyResPINNs effectively bridge the gap between traditional numerical methods and modern machine learning-based solvers."
    },
    {
        "title": "Complete and Lipschitz continuous invariants of graphs under geometric isomorphism in R^n",
        "link_suffix": "/forum?id=JXd1QUREJb",
        "link": "https://openreview.net/forum?id=JXd1QUREJb",
        "pdf_link": "https://openreview.net/pdf?id=JXd1QUREJb",
        "keywords": "Euclidean graph, geometric isomorphism, complete SE(n)-invariant, Lipschitz continuous metric, molecule",
        "abstract": "Euclidean graphs embedded in R^n with unordered vertices and straight-line edges represent important real objects such as molecules whose atoms are connected by chemical bonds. Many real objects preserve their properties under any rigid motion from the special Euclidean group SE(n).Embedded graphs were previously distinguished under such rigid motion or geometric isomorphism in R^n.Experimental noise motivates new Lipschitz continuous invariants so that perturbations of all vertices up to epsilon change the invariants up to a constant multiple of epsilon in a suitable metric, whose running time should polynomially depend on the number of unordered vertices.We developed new complete invariants that are stable under noise, form a natural hierarchy, and distinguish all chemically different graphs in the QM9 database of 130K+ molecules within a few hours on a modest desktop."
    },
    {
        "title": "Strategy-centric Synthesis: Connecting Billions of Image-Text Pairs to High-Quality Visual Instruction Data",
        "link_suffix": "/forum?id=M4J8OtcqT0",
        "link": "https://openreview.net/forum?id=M4J8OtcqT0",
        "pdf_link": "https://openreview.net/pdf?id=M4J8OtcqT0",
        "keywords": "Visual Instruction Tuning, Strategy, Synthetic Data, Complex, Diverse, Scalable",
        "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable generalization across tasks by aligning visual and linguistic representations. High-quality visual instruction data is critical for enhancing the performance of Vision-Language Models. However, current visual instruction tuning datasets, which are primarily derived from past visual tasks, have several limitations. For instance, the range of question types is often restricted and closely tied to the original visual tasks. Furthermore, image diversity is limited, as images collected for various specialized vision tasks clearly fail to adequately represent real-world user queries. Additionally, previous instruction datasets tend to lack complexity, focusing on single tasks like captioning or OCR, which makes it challenging to train models for more complex, multi-skill scenarios. To address these limitations, we propose a novel paradigms called strategy-centric synthesis: automatically synthesizing high-quality instruction data from large-scale image-text pairs. First, we employ an efficient heuristic method to select high-quality, complex images from DataComp-1B image-text pairs. Carefully crafted prompts and these images are fed to VLMs to extract high-quality query strategies and generate corresponding image descriptions. These descriptions are subsequently used to retrieve images aligned with specific questioning strategies. Finally, the retrieved images and their matching strategies are used to synthesize high-quality instructional data. Our experiments indicate that with continued instruction fine-tuning via LoRA on only 3,000 newly synthesized data samples, 0.45% of the LLAVA-1.5 instruction tuning dataset, the model significantly outperforms the original LLAVA-1.5-7B across multiple benchmarks, thereby demonstrating the effectiveness of our approach."
    },
    {
        "title": "Flow Graph Neural Networks",
        "link_suffix": "/forum?id=iKI7wT6fCP",
        "link": "https://openreview.net/forum?id=iKI7wT6fCP",
        "pdf_link": "https://openreview.net/pdf?id=iKI7wT6fCP",
        "keywords": "Graph Neural Networks, Graph Attention Networks, Directed Acyclic Graphs, Power Grids, Electronic Circuits",
        "abstract": "Graph Neural Networks (GNNs) have become essential for learning from graph-structured data. However, existing GNNs do not consider the conservation law inherent in graphs associated with a flow of physical resources, such as electrical current in power grids or traffic in transportation networks. To address this limitation and enhance the performance on tasks where accurate modeling of resource flows is crucial, we propose Flow Graph Neural Networks (FlowGNNs). This novel GNN framework adapts existing graph attention mechanisms to reflect the conservation of resources by distributing a node's message among its outgoing edges instead of allowing arbitrary duplication of the node's information. We further extend this framework to directed acyclic graphs (DAGs), enabling discrimination between non-isomorphic flow graphs that would otherwise be indistinguishable for standard GNNs tailored to DAGs. We validate our approach through extensive experiments on two different flow graph domainsâ€”electronic circuits and power gridsâ€”and demonstrate that the proposed framework enhances the performance of traditional GNN architectures on both graph-level classification and regression tasks."
    },
    {
        "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM Inference",
        "link_suffix": "/forum?id=QliOktBcy3",
        "link": "https://openreview.net/forum?id=QliOktBcy3",
        "pdf_link": "https://openreview.net/pdf?id=QliOktBcy3",
        "keywords": "Large Language Model, KV Cache Compression, Private Inference, Multi-party Computation",
        "abstract": "Private LLM inference based on multi-party computation (MPC) offers cryptographically-secure protection for both user prompt and proprietary model weights. However, it suffers from large latency overhead for long input sequences. While key-value (KV) cache eviction algorithms have been proposed to reduce the computation and memory cost for plaintext inference, they are not designed for MPC and may even introduce more overhead. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further choose a small subset of tokens for attention computation. As existing dynamic selection algorithms incur too much latency, we propose a series of optimizations to drastically reduce the KV cache selection overhead, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and layer-wise index sharing strategy. With extensive experiments, we demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different LLM generation tasks and achieves 1.8 âˆ¼ 2.01Ã— and 3.39 âˆ¼ 8.37Ã— decoding latency and communication reduction on different sequence lengths, respectively."
    },
    {
        "title": "Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models",
        "link_suffix": "/forum?id=P9VdRQOyqu",
        "link": "https://openreview.net/forum?id=P9VdRQOyqu",
        "pdf_link": "https://openreview.net/pdf?id=P9VdRQOyqu",
        "keywords": "Multimodal Large Language Model, Alignment",
        "abstract": "In the broader context of deep learning, Multimodal Large Language Models have achieved significant breakthroughs by leveraging powerful Large Language Models as a backbone to align different modalities into the language space. A prime exemplification is the development of Video Large Language Models (Video-LLMs). While numerous advancements have been proposed to enhance the video understanding capabilities of these models, they are predominantly trained on questions generated directly from video content. However, in real-world scenarios, users often pose questions that extend beyond the informational scope of the video, highlighting the need for Video-LLMs to assess the relevance of the question. We demonstrate that even the best-performing Video-LLMs fail to reject unfit questions-not necessarily due to a lack of video understanding, but because they have not been trained to identify and refuse such questions. To address this limitation, we propose alignment for answerability, a framework that equips Video-LLMs with the ability to evaluate the relevance of a question based on the input video and appropriately decline to answer when the question exceeds the scope of the video, as well as an evaluation framework with a comprehensive set of metrics designed to measure model behavior before and after alignment. Furthermore, we present a pipeline for creating a dataset specifically tailored for alignment for answerability, leveraging existing video-description paired datasets. The code and the dataset will be publicly available."
    },
    {
        "title": "Conformal Prediction Sets Can Cause Disparate Impact",
        "link_suffix": "/forum?id=fZK6AQXlUU",
        "link": "https://openreview.net/forum?id=fZK6AQXlUU",
        "pdf_link": "https://openreview.net/pdf?id=fZK6AQXlUU",
        "keywords": "Conformal Prediction, Fairness, Uncertainty Quantification, Trustworthy ML, Human Subject Experiments",
        "abstract": "Although conformal prediction is a promising method for quantifying the uncertainty of machine learning models, the prediction sets it outputs are not inherently actionable. Many applications require a single output to act on, not several. To overcome this, prediction sets can be provided to a human who then makes an informed decision. In any such system it is crucial to ensure the fairness of outcomes across protected groups, and researchers have proposed that Equalized Coverage be used as the standard for fairness. By conducting experiments with human participants, we demonstrate that providing prediction sets can increase the unfairness of their decisions. Disquietingly, we find that providing sets that satisfy Equalized Coverage actually increases unfairness compared to marginal coverage. Instead of equalizing coverage, we propose to equalize set sizes across groups which empirically leads to more fair outcomes."
    },
    {
        "title": "GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning",
        "link_suffix": "/forum?id=EVuANndPlX",
        "link": "https://openreview.net/forum?id=EVuANndPlX",
        "pdf_link": "https://openreview.net/pdf?id=EVuANndPlX",
        "keywords": "Knowledge Graph, Large Language Models, Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) in Knowledge Graph Question Answering (KGQA) enriches the context of Large Language Models (LLMs) with retrieved KG information based on the question. However, KGs contain complex graph information and existing KG retrieval methods are challenged  when questions require multi-hop information. To improve RAG in complex KGQA, we introduce the GNN-RAG framework, which leverages Graph Neural Networks (GNNs) for effective graph reasoning and retrieval. GNN-RAG consists of a graph neural phase, where the GNN retriever learns to identify useful graph information for KGQA, e.g., when tackling complex questions. At inference time, the GNN scores answer candidates for the given question and the shortest paths in the KG  that connect question entities and answer candidates are retrieved to represent KG reasoning paths. The paths are verbalized and given as context to the downstream LLM for ultimate KGQA; GNN-RAG can be seamlessly integrated with different LLMs for RAG. Experimental results show that GNN-RAG  achieves state-of-the-art performance in two widely used KGQA benchmarks (WebQSP and CWQ), outperforming or matching GPT-4 performance with a 7B tuned LLM. In addition,GNN-RAG excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5% points at answer F1. Furthermore, we show the effectiveness of GNN-RAG in retrieval augmentation, which further boosts KGQA performance."
    },
    {
        "title": "Generation Network for Echocardiographic Sectional Positioning and Shape Completion",
        "link_suffix": "/forum?id=C9DazhfVZR",
        "link": "https://openreview.net/forum?id=C9DazhfVZR",
        "pdf_link": "https://openreview.net/pdf?id=C9DazhfVZR",
        "keywords": "Echocardiography; 3D Cardiac Modeling; Weakly Supervised Learning; Point Cloud Generation; AI-assisted Echocardiographic Analysis",
        "abstract": "The precise localization of 2D echocardiography planes in relation to a dynamic heart necessitates specialized expertise, as existing automated algorithms prmarily classify standard views while lacking the capability for comprehensive 3D structural perception. Traditional measurement techniques have evolved to infer 3D heart geometry, yet recent advancements in artificial intelligence, though demonstrating spatial awareness, still fall short in providing explicit 3D modeling. CTA-based digital twins, while promising, are hindered by cost and radiation concerns. Echocardiography, being cost-effective and radiation-free, remains limited in its ability to provide 3D perception. To address this gap, we introduce a novel point cloud-based weakly supervised 3D generation network specifically tailored for echocardiograms. This network automates 3D heart inference, and biomarker modeling, based on 2D echocardiography, slice tracking. To further enhance accuracy, we integrated a self-supervised learning branch into our framework, introducing multi-structure reconstruction loss and an overall reconstruction loss specifically designed for cardiac structure completion. Additionally, we constructed a comparative branch that serves to bolster the network's precision in inferring cardiac structures, thereby refining our approach and elevating the fidelity of the generated 3D models. Our approach enables real-time, robust 3D heart modeling, independent of paired data requirements, thereby facilitating research advancements in echocardiographic digital twins."
    },
    {
        "title": "Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs",
        "link_suffix": "/forum?id=NzEIjnIIzv",
        "link": "https://openreview.net/forum?id=NzEIjnIIzv",
        "pdf_link": "https://openreview.net/pdf?id=NzEIjnIIzv",
        "keywords": "Instruction tuning, Parameter-efficient fine-tuning, Transformer, PEFT, LLM",
        "abstract": "Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning."
    },
    {
        "title": "Intrinsic Dimension Correlation: uncovering nonlinear connections in multimodal representations",
        "link_suffix": "/forum?id=Qj1KwBZaEI",
        "link": "https://openreview.net/forum?id=Qj1KwBZaEI",
        "pdf_link": "https://openreview.net/pdf?id=Qj1KwBZaEI",
        "keywords": "intrinsic dimension, nonlinear correlation, multimodal representations, representation similarity",
        "abstract": "To gain insight into the mechanisms behind machine learning methods, it is crucial to establish connections among the features describing data points. However, these correlations often exhibit a high-dimensional and strongly nonlinear nature, which makes them challenging to detect using standard methods. This paper exploits the entanglement between intrinsic dimensionality and correlation to propose a metric that quantifies the (potentially nonlinear) correlation between high-dimensional manifolds. We first validate our method on synthetic data in controlled environments, showcasing its advantages and drawbacks compared to existing techniques. Subsequently, we extend our analysis to large-scale applications in neural network representations. Specifically, we focus on latent representations of multimodal data, uncovering clear correlations between paired visual and textual embeddings, whereas existing methods struggle significantly in detecting similarity. Our results indicate the presence of highly nonlinear correlation patterns between latent manifolds."
    },
    {
        "title": "Regret-Optimal List Replicable Bandit Learning: Matching Upper and Lower Bounds",
        "link_suffix": "/forum?id=0T49QbSOho",
        "link": "https://openreview.net/forum?id=0T49QbSOho",
        "pdf_link": "https://openreview.net/pdf?id=0T49QbSOho",
        "keywords": "Replicability, Regret Bound, Bandit",
        "abstract": "This paper investigateslist replicability[Dixon et al., 2023] in the context of multi-armed (also linear) bandits (MAB). We define an algorithm $A$ for MAB to be $(\\ell,\\delta)$-list replicable if with probability at least $1-\\delta$, $A$ has at most $\\ell$ traces in independent executions even with different random bits, where a trace means sequence of arms played during an execution. For $k$-armed bandits, although the total number of traces can be $\\Omega(k^T)$ for a time horizon $T$, we present several surprising upper bounds that either independent of or logarithmic of $T$: (1) a $(2^{k},\\delta)$-list replicable algorithm with near-optimal regret, $\\widetilde{O}{\\sqrt{kT}}$, (2) a $(O(k/\\delta),\\delta)$-list replicable algorithm with regret $\\widetilde{O}\\left(\\frac{k}{\\delta}\\sqrt{kT}\\right)$, (3) a $((k+1)^{B-1}, \\delta)$-list replicable algorithm with regret $\\widetilde{O}(k^{\\frac{3}{2}}T^{{\\frac{1}{2}}+2^{-\\Omega(B)}})$ for any integer $B>1$. We show that result (3) is nearly tight by establishing there are no $(k-1,\\delta)$-list replicable algorithm with $o(T)$-regret, almost exactly matching $k$-list replicable upper bound for $B=2$. We further show that for linear bandits with $d$-dimensional features, there is a $\\widetilde{O}(d^2T^{1/2+2^{-\\Omega(B)}})$-regret algorithm with $((2d+1)^{B-1},\\delta)$-list replicability, for $B>1$, even when the number of possible arms can be infinite."
    },
    {
        "title": "Jump-teaching: Ultra Robust and Efficient Learning with Noisy Labels",
        "link_suffix": "/forum?id=DfOYQZOilp",
        "link": "https://openreview.net/forum?id=DfOYQZOilp",
        "pdf_link": "https://openreview.net/pdf?id=DfOYQZOilp",
        "keywords": "learning with noisy labels, machine learning, classification",
        "abstract": "Sample selection is the most straightforward technique to combat noisy labels, aiming to prevent mislabeled samples from degrading the robustness of neural networks. However, compounding selection bias and redundant selection operations have always remained challenging in robustness and efficiency. To mitigate selection bias, existing methods utilize disagreement in partner networks or additional forward propagation in a single network. For selection operations, they involve dataset-wise modeling or batch-wise ranking. Any of the above methods yields sub-optimal performance. In this work, we propose $\\textit{Jump-teaching}$, a novel framework for optimizing the typical workflow of sample selection.  Firstly, Jump-teaching is the $\\textit{first}$ work to discover significant disagreements within a single network between different training iterations. Based on this discovery, we propose a jump-manner strategy for model updating to bridge the disagreements. We further illustrate its effectiveness from the perspective of error flow. \nSecondly, Jump-teaching designs a lightweight plugin to simplify selection operations. It creates a detailed yet simple loss distribution on an auxiliary encoding space, which helps select clean samples more effectively. In the experiments, Jump-teaching not only outperforms state-of-the-art works in terms of robustness, but also reduces peak memory usage by $0.46\\times$ and boosts training speed by up to $2.53\\times$. Notably, existing methods can also benefit from the integration with our framework."
    },
    {
        "title": "FlatQuant: Flatness Matters for LLM Quantization",
        "link_suffix": "/forum?id=pxGucWt9vM",
        "link": "https://openreview.net/forum?id=pxGucWt9vM",
        "pdf_link": "https://openreview.net/pdf?id=pxGucWt9vM",
        "keywords": "flatness, post-training quantization, LLM, pre-quantization transformation",
        "abstract": "Recently, quantization has been widely used for the compression and acceleration of large language models~(LLMs). \nDue to the outliers in LLMs, it is crucial to flatten weights and activations to minimize quantization error with the equally spaced quantization points. Prior research explores various pre-quantization transformations to suppress outliers, such as per-channel scaling and Hadamard transformation. However, we observe that these transformed weights and activations can still remain steep and outspread. In this paper, we propose FlatQuant (Fast and Learnable Affine Transformation), a new post-training quantization approach to enhance flatness of weights and activations. Our approach identifies optimal affine transformations tailored to each linear layer, calibrated in hours via a lightweight objective. To reduce runtime overhead, we apply Kronecker decomposition to the transformation matrices, and fuse all operations in FlatQuant into a single kernel. Extensive experiments show that FlatQuant sets up a new state-of-the-art quantization benchmark. For instance, it achieves less than 1% accuracy drop for W4A4 quantization on the LLaMA-3-70B model, surpassing SpinQuant by 7.5%. For inference latency, FlatQuant reduces the slowdown induced by pre-quantization transformation from 0.26x of QuaRot to merely 0.07x, bringing up to 2.3x speedup for prefill and 1.7x speedup for decoding, respectively. Code will be released upon acceptance."
    },
    {
        "title": "Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models",
        "link_suffix": "/forum?id=odvSjn416y",
        "link": "https://openreview.net/forum?id=odvSjn416y",
        "pdf_link": "https://openreview.net/pdf?id=odvSjn416y",
        "keywords": "retrieval, instructions, search, prompts",
        "abstract": "Instruction-tuned language models (LM) are able to respond to imperative commands, providing a more natural user interface compared to their base counterparts. In this work, we present Promptriever, the first retrieval model able to be prompted like an LM. To train Promptriever, we curate and release a new instance-level instruction training set from MS MARCO, spanning nearly 500k instances. Promptriever not only achieves strong performance on standard retrieval tasks, but also follows instructions. We observe: (1) large gains (reaching SoTA) on following detailed relevance instructions (+14.3 p-MRR / +3.1 nDCG on FollowIR), (2) significantly increased robustness to lexical choices/phrasing in the query+instruction (+12.9 Robustness@10 on InstructIR), and (3) the ability to perform hyper-parameter search via prompting to reliably improve retrieval performance (+1.4 average increase on BEIR). Promptriever demonstrates that retrieval models can be controlled with prompts on a per-query basis, setting the stage for future work aligning LM prompting techniques with information retrieval."
    },
    {
        "title": "OmniBench: Towards The Future of  Universal Omni-Language Models",
        "link_suffix": "/forum?id=Rc8z5wLzBF",
        "link": "https://openreview.net/forum?id=Rc8z5wLzBF",
        "pdf_link": "https://openreview.net/pdf?id=Rc8z5wLzBF",
        "keywords": "Multimodal Reasoning, MLLM Benchmark, Text, Audio, Image",
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have aimed to integrate and interpret data across diverse modalities. However, the capacity of these models to concurrently process and reason about multiple modalities remains inadequately explored, partly due to the lack of comprehensive modality-wise benchmarks. \nWe introduceOmniBench, a novel benchmark designed to rigorously evaluate models' ability to recognize, interpret, and reason acrossvisual,acoustic, andtextualinputs simultaneously. We define models capable of such tri-modal processing as omni-language models (OLMs).\nOmniBench is distinguished by high-quality human annotations, ensuring that accurate responses require integrated understanding and reasoning across all three modalities. Our main findings reveal that:i)open-source OLMs exhibit critical limitations in instruction-following and reasoning capabilities within tri-modal contexts; andii)most baselines models perform poorly (below 50% accuracy) even when provided with alternative textual representations of images or/and audio.\nThese results suggest that the ability to construct a consistent context from text, image, and audio is often overlooked in existing MLLM training paradigms. \nTo address this gap, we curate an instruction tuning dataset of 84.5K training samples,OmniInstruct, for training OLMs to adapt to multimodal contexts.\nWe advocate for future research to focus on developing more robust tri-modal integration techniques and training strategies to enhance OLM performance across diverse modalities.\nCodes and datasets are uploaded atour repository."
    },
    {
        "title": "A Model of Place Field Reorganization During Reward Maximization",
        "link_suffix": "/forum?id=QcvwVUqnCg",
        "link": "https://openreview.net/forum?id=QcvwVUqnCg",
        "pdf_link": "https://openreview.net/pdf?id=QcvwVUqnCg",
        "keywords": "Place field, Reinforcement Learning, Successor representation, Representation drift",
        "abstract": "When rodents learn to navigate in a novel environment, a high density of place fields emerge at reward locations, fields elongate against the trajectory, and individual fields change spatial selectivity while demonstrating stable behavior. Why place fields demonstrate these characteristic phenomena during learning remains elusive. We develop a normative framework using a reward maximization objective, whereby the temporal difference (TD) error drives place field reorganization to improve policy learning. Place fields are modelled using Gaussian radial basis functions to represent states in an environment, and directly synapse to an actor-critic for policy learning. Each field's amplitude, center and width, as well as downstream weights, are updated online at each time step to maximize cumulative reward. We demonstrate that this framework unifies the three disparate phenomena observed in navigation experiments. Furthermore, we show that these place field phenomena improves policy convergence when learning to navigate to a single target and relearning multiple new targets. To conclude, we develop a normative model that recapitulates several aspects of hippocampal place field learning dynamics and unifies mechanisms to offer testable predictions for future experiments."
    }
]