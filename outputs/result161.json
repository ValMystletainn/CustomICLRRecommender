[
    {
        "title": "SPikE-SSM: A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequences Learning",
        "link_suffix": "/forum?id=4ILqqOJFkS",
        "link": "https://openreview.net/forum?id=4ILqqOJFkS",
        "pdf_link": "https://openreview.net/pdf?id=4ILqqOJFkS",
        "keywords": "state space models, spiking neural network, long sequence modeling, language modeling",
        "abstract": "Spiking neural networks (SNNs) provide a low-power, energy-efficient solution by utilizing the spike-based and sparse nature of biological systems. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on long sequential tasks, until the recent emergence of state space models (SSMs), which offer superior computational efficiency and modeling capability. However, applying the highly capable SSMs to SNNs for long sequences learning poses three major challenges: \u2776  The membrane potential is determined by the past spiking history of the neuron, leading to reduced efficiency for sequence modeling in parallel computing scenarios. \u2777 Complex dynamics of biological spiking neurons are crucial for functionality but challenging to simulate and exploit effectively in large networks. \u2778 It is arduous to maintain high sparsity while achieving high accuracy for spiking neurons without resorting to dense computing, as utilized in artificial neuron-based SSMs. To address these challenges, we propose a sparse, precise and efficient spiking SSM framework, termed SPikE-SSM. For \u2776, we propose a boundary compression strategy (PMBC) to accelerate the inference of the spiking neuron model, enabling parallel processing for long sequence learning. For \u2777, we propose a novel and concise neuron model incorporating reset-refractory mechanism to leverage the inherent temporal dimension for dynamic computing with biological interpretability. For \u2778, we hierarchically integrate the proposed neuron model to the original SSM block, and enhance the dynamics of SPikE-SSM by incorporating trainable thresholds and refractory magnitudes to balance accuracy and sparsity. Extensive experiments illustrate the effectiveness and robustness of SPikE-SSM on the long range arena benchmarks and large language dataset WikiText-103, showing the potential of dynamic spiking neurons in efficient long sequence learning. The code will be publicly available."
    },
    {
        "title": "Fair Clustering in the Sliding Window Model",
        "link_suffix": "/forum?id=VGQugiuCQs",
        "link": "https://openreview.net/forum?id=VGQugiuCQs",
        "pdf_link": "https://openreview.net/pdf?id=VGQugiuCQs",
        "keywords": "fair clustering, sliding window model",
        "abstract": "We study streaming algorithms for proportionally fair clustering (a notion originally suggested by Chierichetti et al. (2017) in the sliding window model. We show that although there exist efficient streaming algorithms exist in the insertion-only model, surprisingly no algorithm can achieve finite ratio without violating the fairness constraint in sliding window. Hence, the problem of fair clustering is a rare separation between the insertion-only streaming model and the sliding window model. On the other hand, we show that if the fairness constraint by a multiplicative $\\varepsilon$ factor, there exists a $(1 + \\varepsilon)$-approximate sliding window algorithm that uses $\\text{poly}(k\\varepsilon^{-1}\\log n)$ space. This achieves essentially the best parameters (up to degree in the polynomial) provided the aforementioned lower bound. We also implement a number of empirical evaluations on real datasets to complement our theoretical results."
    },
    {
        "title": "FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking",
        "link_suffix": "/forum?id=6RjQ54M1rM",
        "link": "https://openreview.net/forum?id=6RjQ54M1rM",
        "pdf_link": "https://openreview.net/pdf?id=6RjQ54M1rM",
        "keywords": "Federated Learning, Model aggregation, Deep neural networks, Machine learning",
        "abstract": "In Federated Learning (FL), weighted aggregation of local models is conducted to generate a new global model, and the aggregation weights are typically normalized to 1. A recent study identifies the global weight shrinking effect in FL, indicating an enhancement in the global model\u2019s generalization when the sum of weights (i.e., the shrinking factor) is smaller than 1, where how to learn the shrinking factor becomes crucial. However, principled approaches to this solution have not been carefully studied from the adequate consideration of privacy concerns and layer-wise distinctions. To this end, we propose a novel model aggregation strategy, Federated Learning with Adaptive Layer-wise Weight Shrinking (FedLWS), which adaptively designs the shrinking factor in a layer-wise manner and avoids optimizing the shrinking factors on a proxy dataset. We initially explored the factors affecting the shrinking factor during the training process. Then we calculate the layer-wise shrinking factors by considering the distinctions among each layer of the global model. FedLWS can be easily incorporated with various existing methods due to its flexibility. Extensive experiments under diverse scenarios demonstrate the superiority of our method over several state-of-the-art approaches, providing a promising tool for enhancing the global model in FL. We include the source code of FedLWS in the supplementary material."
    },
    {
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models",
        "link_suffix": "/forum?id=lgsyLSsDRe",
        "link": "https://openreview.net/forum?id=lgsyLSsDRe",
        "pdf_link": "https://openreview.net/pdf?id=lgsyLSsDRe",
        "keywords": "LLM, embedding model, retriever",
        "abstract": "Decoder-only large language model (LLM)-based embedding models are beginning to outperform BERT or T5-based embedding models in general-purpose text embedding tasks, including dense vector-based retrieval. In this work, we introduce the NV-Embed model, incorporating architectural designs, training procedures, and curated datasets to significantly enhance the performance of LLM as a versatile embedding model, while maintaining its simplicity and reproducibility.For model architecture, we propose a latent attention layer to obtain pooled embeddings, which consistently improves retrieval and downstream task accuracy compared to mean pooling or using the last <EOS> token embedding from LLMs. To enhance representation learning, we remove the causal attention mask of LLMs during contrastive training. For training algorithm, we introduce a two-stage contrastive instruction-tuning method. It first applies contrastive training with instructions on retrieval datasets, utilizing in-batch negatives and curated hard negative examples. At stage-2, it blends various non-retrieval into instruction tuning, which not only enhances non-retrieval task accuracy but also improves retrieval performance. For training data, we utilize the hard-negative mining, synthetic data generation and existing public available datasets to boost the performance of embedding model. By combining these techniques, our NV-Embed- v1 model secured the No.1 position on the Massive Text Embedding Benchmark (MTEB) (as of May 24, 2024), across 56 embedding tasks. NV-Embed-v2 has reclaimed and maintained the top spot on MTEB since August 30, 2024, demonstrating the sustained effectiveness of the proposed methods over time. Additionally, it achieved the highest scores in the Long Doc section and the second-highest scores in the QA section of the AIR Benchmark, which covers a range of out-of-domain information retrieval topics beyond those in MTEB."
    },
    {
        "title": "Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking",
        "link_suffix": "/forum?id=I8af9JdQTy",
        "link": "https://openreview.net/forum?id=I8af9JdQTy",
        "pdf_link": "https://openreview.net/pdf?id=I8af9JdQTy",
        "keywords": "RLHF, Preference Optimization, Reinforcement Learning, Alignment, Large Language Models",
        "abstract": "Aligning AI systems with human preferences typically suffers from the infamousreward hackingproblem, where optimization of an imperfect reward model leads to undesired behaviors. In this paper, we investigate reward hacking in offline preference optimization (PO), which aims to improve an initial model using a preference dataset. We identify two types of reward hacking stemming from statistical fluctuations in the dataset: Type I Reward Hacking due to subpar choices appearing more favorable, and Type II Reward Hacking due to decent choices appearing less desirable. We prove that many (mainstream or theoretical) PO methods suffer from both types of reward hacking. To address Type I Reward Hacking, we propose POWER, a new PO method that combines Guiaus's Weighted Entropy with a Robust Reward maximization objective. POWER enjoys finite-sample guarantees under general function approximation, competing with the best covered policy in the data. To address Type II Reward Hacking, we analyze the learning dynamics of POWER and combine it with a novel technique that dynamically updates preference labels (POWER-DL) toward certain \"stationary labels\", resulting in diminishing gradients for untrustworthy samples. Empirically, POWER-DL consistently outperforms state-of-the-art methods on alignment benchmarks, achieving improvements of up to13.0points on AlpacaEval 2 and11.5points on Arena Hard over DPO. Strong theoretical guarantees and empirical performance demonstrate the promise of POWER-DL in mitigating reward hacking."
    },
    {
        "title": "Horizon-Length Prediction: Advancing Fill-in-the-Middle Capabilities for Code Generation with Lookahead Planning",
        "link_suffix": "/forum?id=tDANkt6X3D",
        "link": "https://openreview.net/forum?id=tDANkt6X3D",
        "pdf_link": "https://openreview.net/pdf?id=tDANkt6X3D",
        "keywords": "Language Models, Fill-in-the-Middle, Planning, Training",
        "abstract": "Fill-in-the-Middle (FIM) has become integral to code language models, enabling generation of missing code given both left and right contexts. However, the current FIM training paradigm, which reorders original training sequences and then performs regular next-token prediction (NTP), often leads to models struggling to generate content that aligns smoothly with the surrounding context. Crucially, while existing works rely on rule-based post-processing to circumvent this weakness, such methods are not practically usable in open-domain code completion tasks as they depend on restrictive, dataset-specific assumptions (e.g., generating the same number of lines as in the ground truth). Moreover, model performance on FIM tasks deteriorates significantly without these unrealistic assumptions.We hypothesize that NTP alone is insufficient for models to learn effective planning conditioned on the distant right context, a critical factor for successful code infilling. To overcome this, we propose Horizon-Length Prediction (HLP), a novel training objective that teaches models to predict the number of remaining middle tokens (i.e., horizon length) at each step. HLP advances FIM with lookahead planning, enabling models to inherently learn infilling boundaries for arbitrary left and right contexts without relying on dataset-specific post-processing. Our evaluation across different models and sizes shows that HLP significantly improves FIM performance by up to 24% relatively on diverse benchmarks, across file-level and repository-level, and without resorting to unrealistic post-processing methods. Furthermore, the enhanced planning capability gained through HLP boosts model performance on code reasoning. Importantly, HLP only incurs negligible training overhead and no additional inference cost, ensuring its practicality for real-world scenarios."
    },
    {
        "title": "MA2E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder",
        "link_suffix": "/forum?id=klpdEThT8q",
        "link": "https://openreview.net/forum?id=klpdEThT8q",
        "pdf_link": "https://openreview.net/pdf?id=klpdEThT8q",
        "keywords": "multi-agent reinforcement learning, partial observability, masked autoencoder",
        "abstract": "Centralized Training and Decentralized Execution (CTDE) is a widely adopted paradigm to solve cooperative multi-agent reinforcement learning (MARL) problems. Despite the successes achieved with CTDE, partial observability still limits cooperation among agents. While previous studies have attempted to overcome this challenge through communication, direct information exchanges could be restricted and introduce additional constraints. Alternatively, if an agent can infer the global information solely from local observations, it can obtain a global view without the need for communication. To this end, we propose the Multi-Agent Masked Auto-Encoder (MA$^2$E), which utilizes the masked auto-encoder architecture to infer the information of other agents from partial observations. By employing masking to learn to reconstruct global information, MA$^2$E serves as an inference module for individual agents within the CTDE framework. MA$^2$E can be easily integrated into existing MARL algorithms and has been experimentally proven to be effective across a wide range of environments and algorithms."
    },
    {
        "title": "Grounding Robot Policies with Visuomotor Language Guidance",
        "link_suffix": "/forum?id=Afjf6izLvJ",
        "link": "https://openreview.net/forum?id=Afjf6izLvJ",
        "pdf_link": "https://openreview.net/pdf?id=Afjf6izLvJ",
        "keywords": "Foundation Models for Robotics, Policy-adaptation, Self-guidance",
        "abstract": "Recent advances in the fields of natural language processing and computer vision have shown great potential in understanding the underlying dynamics of the world from large-scale internet data. However, translating this knowledge into robotic systems remains an open challenge, given the scarcity of human-robot interactions and the lack of large-scale datasets of real-world robotic data. Previous robot learning approaches such as behavior cloning and reinforcement learning have shown great capabilities in learning robotic skills from human demonstrations or from scratch in specific environments. However, these approaches often require task-specific demonstrations or designing complex simulation environments, which limits the development of generalizable and robust policies for new settings. Aiming to address these limitations, we propose an agent-based framework for grounding robot policies to the current context, considering the constraints of a current robot and its environment using visuomotor-grounded language guidance. The proposed framework is composed of a set of conversational agents designed for specific roles\u2014namely, high-level advisor, visual grounding, monitoring, and robotic agents. Given a base policy, the agents collectively generate guidance at run time to shift the action distribution of the base policy towards more desirable future states. We demonstrate that our approach can effectively guide manipulation policies to achieve significantly higher success rates both in simulation and in real-world experiments without the need for additional human demonstrations or extensive exploration. Project videos athttps://sites.google.com/view/motorcortex/home."
    },
    {
        "title": "Training Free Exponential Context Extension via Cascading KV Cache",
        "link_suffix": "/forum?id=dSneEp59yX",
        "link": "https://openreview.net/forum?id=dSneEp59yX",
        "pdf_link": "https://openreview.net/pdf?id=dSneEp59yX",
        "keywords": "transformer, efficiency, linear attention",
        "abstract": "The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency."
    },
    {
        "title": "Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens",
        "link_suffix": "/forum?id=jQP5o1VAVc",
        "link": "https://openreview.net/forum?id=jQP5o1VAVc",
        "pdf_link": "https://openreview.net/pdf?id=jQP5o1VAVc",
        "keywords": "generative model, scaling, text-to-image, autoregressive",
        "abstract": "Scaling up autoregressive models in vision has not proven as beneficial as in large language models.\nIn this work, we investigate this scaling problem in the context of text-to-image generation, focusing on two critical factors: whether models use discrete or continuous tokens, and whether tokens are generated in a random or fixed raster order using BERT- or GPT-like transformer architectures. Our empirical results show that, while all models scale effectively in terms of validation loss, their evaluation performance -- measured by FID, GenEval score, and visual quality -- follows different trends. Models based on continuous tokens achieves significantly better visual quality than those using discrete tokens. Furthermore, the generation order and attention mechanisms significantly affect the GenEval score: random-order models achieve notably better GenEval scores compared to raster-order models.\nInspired by these findings, we train Fluid, a random-order autoregressive model on continuous tokens. Fluid 10.5B model achieves a new state-of-the-art zeor-shot FID of 6.16 on MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our findings and results will encourage future efforts to further bridge the scaling gap between vision and language models."
    },
    {
        "title": "Low-bit Quantization for Seeing in the Dark",
        "link_suffix": "/forum?id=TOVBglQvhB",
        "link": "https://openreview.net/forum?id=TOVBglQvhB",
        "pdf_link": "https://openreview.net/pdf?id=TOVBglQvhB",
        "keywords": "low-light image enhancement, network quantization",
        "abstract": "Several properties of raw data exhibit significant potential for enhancing images under extremely low-light conditions. Recently, many deep-learning methods for raw-based low-light image enhancement (LLIE) have demonstrated excellent performance. However, deploying them on resource-limited devices is restricted by high computational and storage demands. In this work, we propose a novel low-bit quantization method for raw-based LLIE model to improve their efficiency. Nevertheless, directly adopting existing quantizers for LLIE networks leads to an obvious performance drop due to two main reasons. i) The U-Net model, commonly employed in LLIE, faces challenges in identifying a suitable quantization range due to disparities in distribution between the encoder and decoder features. ii) Low-bit quantized LLIE networks struggle to restore clear details in low-light images because their features have a constraint capacity. We address these issues by introducing a novel low-bit quantization method, the Distribution-Separative Asymmetric Quantizer (DSAQ), designed specifically for U-Net architectures used in LLIE. In order to accurately determine the quantization intervals, DSAQ separates the distribution of encoder and decoder features before they are concatenated by the skip connection. We also make the quantizer asymmetric with trainable scale and offset parameters to suit skewed activation ranges caused by non-linear functions. To further enhance performance, we propose a uniform feature distillation technique, which allows the low-bit student model to effectively assimilate knowledge from the full-precision teacher model, bridging the gap in representation capability. Extensive experiments show that our approach not only greatly reduces the memory and computational requirements of raw-based LLIE models but also has a promising performance. Our 4-bit quantized model can achieve comparable or superior results to full-precision counterparts."
    },
    {
        "title": "The AdEMAMix Optimizer: Better, Faster, Older",
        "link_suffix": "/forum?id=jj7b3p5kLY",
        "link": "https://openreview.net/forum?id=jj7b3p5kLY",
        "pdf_link": "https://openreview.net/pdf?id=jj7b3p5kLY",
        "keywords": "Optimization, LLM, Deep Learning, Momentum",
        "abstract": "Momentum based optimizers are central to a wide range of machine learning applications. These typically rely on an Exponential Moving Average (EMA) of gradients, which decays exponentially the present contribution of older gradients. This accounts for gradients being local linear approximations which lose their relevance as the iterate moves along the loss landscape. This work questions the use of a single EMA to accumulate past gradients and empirically demonstrates how this choice can be sub-optimal: a single EMA cannot simultaneously give a high weight to the immediate past, and a non-negligible weight to older gradients. Building on this observation, we propose AdEMAMix, a simple modification of the Adam optimizer with a mixture of two EMAs to better take advantage of past gradients. Our experiments on language modeling and image classification show---quite surprisingly---that gradients can stay relevant for tens of thousands of steps. They help to converge faster, and often to lower minima: e.g., a $1.3$B parameter AdEMAMix LLM trained on $101$B tokens performs comparably to an AdamW model trained on $197$B tokens ($+95%$). Moreover, our method significantly slows-down model forgetting during training. Our work motivates further exploration of different types of functions to leverage past gradients, beyond EMAs."
    },
    {
        "title": "DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversational Agents",
        "link_suffix": "/forum?id=W1x77vRucB",
        "link": "https://openreview.net/forum?id=W1x77vRucB",
        "pdf_link": "https://openreview.net/pdf?id=W1x77vRucB",
        "keywords": "Conversational Agents Evaluation, Long-Term Multi-Party Dialogue Understanding, Real-Time Evaluation",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of conversational agents, making them applicable to various fields (e.g., education). \n  Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as real-time interactions, multi-party dialogues, and extended contextual dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue simulator.\n  In this simulator, an agent is assigned the role of a character from popular TV shows, requiring it to respond to spontaneous questions using past dialogue information and to distinguish between known and unknown information. \n  Key features of DialSim include evaluating the agent\u2019s ability to respond within a reasonable time limit, handling long-term multi-party dialogues, and testing the agent's performance under randomized questioning with a diverse and high-quality question-answer dataset. We utilized this simulator to evaluate the latest conversational agents and analyze their limitations. Our experiments highlight both the strengths and weaknesses of these agents, providing valuable insights for future improvements in the field of conversational AI. DialSim is available athttps://anonymous.4open.science/r/Simulator-A861."
    },
    {
        "title": "Understanding the Benefits of SimCLR Pre-Training in Two-Layer Convolutional Neural Networks",
        "link_suffix": "/forum?id=VxIetsMu3G",
        "link": "https://openreview.net/forum?id=VxIetsMu3G",
        "pdf_link": "https://openreview.net/pdf?id=VxIetsMu3G",
        "keywords": "SimCLR, convolutional neural network",
        "abstract": "SimCLR is one of the most popular contrastive learning methods for vision tasks. It pre-trains deep neural networks based on a large amount of unlabeled data by teaching the model to distinguish between positive and negative pairs of augmented images. It is believed that SimCLR can pre-train a deep neural network to learn efficient representations that can lead to a better performance of future supervised fine-tuning. Despite its effectiveness, our theoretical understanding of the underlying mechanisms of SimCLR is still limited. In this paper, we theoretically introduce a case study of the SimCLR method. Specifically, we consider training a two-layer convolutional neural network (CNN) to learn a toy image data model. We show that, under certain conditions on the number of labeled data, SimCLR pre-training combined with supervised fine-tuning achieves almost optimal test loss. Notably, the label complexity for SimCLR pre-training is far less demanding compared to direct training on supervised data. Our analysis sheds light on the benefits of SimCLR in learning with fewer labels."
    },
    {
        "title": "FedOne: Query-Efficient Federated Learning for Black-box Discrete Prompt Learning",
        "link_suffix": "/forum?id=NJqu7xwXZk",
        "link": "https://openreview.net/forum?id=NJqu7xwXZk",
        "pdf_link": "https://openreview.net/pdf?id=NJqu7xwXZk",
        "keywords": "Federated Learning, Black-box Prompt Learning, Large Language Model",
        "abstract": "Black-Box Discrete Prompt Learning (BDPL) is a prompt-tuning method that optimizes discrete prompts without accessing model parameters or gradients, making the prompt tuning on a cloud-based Large Language Model (LLM) feasible.\nAdapting Federated Learning (FL) to BDPL could further enhance prompt tuning performance by leveraging data from diverse sources. \nHowever, all previous research on federated black-box prompt tuning had neglected the substantial query cost associated with the cloud-based LLM service. \nTo address this gap, we conducted a theoretical analysis of query efficiency within the context of federated black-box prompt tuning. Our findings revealed that degrading FedAvg to activate only one client per round, a strategy we calledFedOne, enabled optimal query efficiency in federated black-box prompt learning. \nBuilding on this insight, we proposed the FedOne framework, a federated black-box discrete prompt learning method designed to maximize query efficiency when interacting with cloud-based LLMs.\nWe conducted numerical experiments on various aspects of our framework, demonstrating a significant improvement in query efficiency, which aligns with our theoretical results."
    },
    {
        "title": "Adaptive Circuit Behavior and Generalization in Mechanistic Interpretability",
        "link_suffix": "/forum?id=FbZSZEIkEU",
        "link": "https://openreview.net/forum?id=FbZSZEIkEU",
        "pdf_link": "https://openreview.net/pdf?id=FbZSZEIkEU",
        "keywords": "Mechanistic Interpretability, Circuit Generalization, Science of ML",
        "abstract": "Mechanistic interpretability aims to understand the inner workings of large neural networks by identifying \\textit{circuits}, or minimal subgraphs within the model that implement algorithms that are responsible for performing specific tasks. Theindirect object identification (IOI)task serves as a key benchmark for circuit discovery, largely due to the existence of a manually derived circuit in GPT-2 that appears to implement a clear and interpretable algorithm. This circuit was identified using a particular format for the input prompt, though the circuit itself appears to implement a far more general algorithm that should apply to a much larger set of prompts. In this paper, we investigate how the IOI circuit generalizes to prompt variants specifically designed to evaluate the validity of the algorithm implied by the circuit. Surprisingly, we find that the IOI circuit maintains high performance on variants of the IOI prompt where the implied algorithm should fail. This raises a critical question:Does the IOI circuit truly implement such a simple algorithm, or do underlying mechanisms modulate its behavior in order to generalize to different prompts?Our analysis reveals that the functionalities of individual attention heads in the IOI circuit remain largely intact even on prompt variants for which the proposed IOI algorithm would fail. We focus on one such variant named DoubleIO and discover a circuit that reuses all heads and paths in the IOI circuit while adding only 9 additional edges. Our findings indicate that circuits within LLMs may be more flexible and general than previously recognized, underscoring the importance of testing specific aspects of their behavior to unveil the full extent of their capabilities."
    },
    {
        "title": "Unified Multi-Modal Interleaved Document Representation for Information Retrieval",
        "link_suffix": "/forum?id=DakTqQu161",
        "link": "https://openreview.net/forum?id=DakTqQu161",
        "pdf_link": "https://openreview.net/pdf?id=DakTqQu161",
        "keywords": "Information Retrieval, Multi-Modal Information Retrieval, Multi-Modal Representation Learning",
        "abstract": "Information Retrieval (IR) methods aim to identify relevant documents in response to a given query, which have gained remarkable attention due to their successful application in various natural language tasks. However, existing approaches typically consider only the textual information within the documents, which overlooks the fact that documents can contain multiple modalities, including texts, images, and tables. Further, they often segment each long document into multiple discrete passages for embedding, preventing them from capturing the overall document context and interactions between paragraphs. We argue that these two limitations lead to suboptimal document representations for retrieval. In this work, to address them, we aim to produce more comprehensive and nuanced document representations by holistically embedding documents interleaved with different modalities. Specifically, we achieve this by leveraging the capability of recent vision-language models that enable the processing and integration of text, images, and tables into a unified format and representation. Moreover, to mitigate the information loss from segmenting documents into passages, instead of representing and retrieving passages individually, we further merge the representations of segmented passages into one single document representation, while we additionally introduce a reranking strategy to decouple and identify the relevant passage within the document if necessary. Then, through extensive experiments on diverse information retrieval scenarios considering both the textual and multi-modal queries, we show that our approach substantially outperforms relevant baselines, thanks to the consideration of the multi-modal information interleaved within the documents in a unified way."
    },
    {
        "title": "FIG: Flow with Interpolant Guidance for Linear Inverse Problems",
        "link_suffix": "/forum?id=fs2Z2z3GRx",
        "link": "https://openreview.net/forum?id=fs2Z2z3GRx",
        "pdf_link": "https://openreview.net/pdf?id=fs2Z2z3GRx",
        "keywords": "Flow Matching, Diffusion Model, Linear Inverse Problems",
        "abstract": "Diffusion and flow matching models have been recently used to solve various linear inverse problems such as image restoration. Using a pre-trained diffusion or flow-matching model as a prior, most existing methods modify the reverse-time sampling process by incorporating the likelihood information from the measurement. However, they struggle in challenging scenarios, e.g., in case of high measurement noise or severe ill-posedness. In this paper, we propose `Flow with Interpolant Guidance' (FIG), an algorithm  where the reverse-time sampling is efficiently guided with measurement interpolants through theoretically justified schemes. Experimentally, we demonstrate that FIG efficiently produce highly competitive results  on a variety of linear image reconstruction tasks on natural image datasets. We improve upon state-of-the-art baseline algorithms, especially for challenging tasks. Code will be released."
    },
    {
        "title": "Scaling Long Context Training Data by Long-Distance Referrals",
        "link_suffix": "/forum?id=tePFpDgyqg",
        "link": "https://openreview.net/forum?id=tePFpDgyqg",
        "pdf_link": "https://openreview.net/pdf?id=tePFpDgyqg",
        "keywords": "long context training; continue training; dataset;",
        "abstract": "Training large language models for long context understanding faces the challenge of data shortage.\nPrevious data engineering approaches mechanically concatenate short documents, which may create many pseudo long documents but raise concerns about data quality.\nIn this paper, we study the core attribute of high quality data for long context training, and provide a data pipeline, LongPack, to scale\nsuch data.\nWe found that long distance referrals, which occur in natural long documents, are crucial for long-context training.\nHowever, simply concatenating short documents does not reliably generate these relations.\nWe further show that the density of long-distance referrals, which is higher in longer documents, has a key role in training efficiency, making previous upsampling methods suboptimal.\nTo enrich long documents, we propose LongPack, a data pipeline that constructs long documents by packing shorter ones based on referral relationships.\nSpecifically, for web pages, which are the primary source for language model training, we found hyper-link a native signal for such a relation.\nBy packing web pages through their hyper-link connection, we can create longer, high-quality documents.\nOur experiments demonstrate that LongPackis highly scalable, generating a corpus of long documents equivalent in size to an entire pretraining dataset using just 0.5% root documents.\nFurthermore, the constructed documents have a \u2018near-natural\u2019 quality as innate long documents for long context training, reaching a 32.7% higher score than previous state-of-the-art methods."
    },
    {
        "title": "Benchmark on Drug Target Interaction Modeling from a Structure Perspective",
        "link_suffix": "/forum?id=gB2ZeqDpl6",
        "link": "https://openreview.net/forum?id=gB2ZeqDpl6",
        "pdf_link": "https://openreview.net/pdf?id=gB2ZeqDpl6",
        "keywords": "drug-target interaction, benchmark, transformer, GNN",
        "abstract": "The prediction modeling of drug-target interactions is crucial to drug discovery and design, which has seen rapid advancements owing to deep learning technologies. Recently developed methods, such as those based on graph neural networks (GNNs) and Transformers, demonstrate exceptional performance across various datasets by effectively extracting structural information. However, the benchmarking of these novel methods often varies significantly in terms of hyperparameter settings and datasets, which limits algorithmic progress. In view of these, we conduct a comprehensive survey and benchmark for drug-target interaction modeling from a structure perspective, via integrating tens of explicit (i.e., GNN-based) and implicit (i.e., Transformer-based) structure learning algorithms. To this end, we first unify the hyperparameter setting within each class of structure learning methods. Moreover, we conduct a macroscopical comparison between these two classes of encoding strategies as well as the different featurization techniques that inform molecules' chemical and physical properties. We then carry out the microscopical comparison between all the integrated models across the six datasets, via comprehensively benchmarking their effectiveness and efficiency. Remarkably, the summarized insights from the benchmark studies lead to the design of model combos. We demonstrate that our combos can achieve new state-of-the-art performance on various datasets associated with cost-effective memory and computation."
    },
    {
        "title": "DOMAIN GENERALIZATION VIA PARETO OPTIMAL GRADIENT MATCHING",
        "link_suffix": "/forum?id=VfvxZLXYgd",
        "link": "https://openreview.net/forum?id=VfvxZLXYgd",
        "pdf_link": "https://openreview.net/pdf?id=VfvxZLXYgd",
        "keywords": "Domain Generalization",
        "abstract": "In this study, we address the gradient-based domain generalization problem, where predictors aim for consistent gradient directions across different domains. Existing methods have two main challenges. First, minimization of gradient empirical distance or gradient inner products (GIP) leads to gradient fluctuations and magnitude elimination among domains, thereby hindering straightforward learning. Second, the direct application of gradient learning to joint loss function can incur high computation overheads due to second-order derivative approximation. To tackle these challenges, we propose a new Pareto Optimality Gradient Matching (POGM) method. In contrast to existing methods that add gradient matching as regularization, we leverage gradient trajectories as collected data and apply independent training at the meta-learner. In the meta-update, we maximize GIP while limiting the learned gradient from deviating too far from the empirical risk minimization gradient trajectory. By doing so, the aggregate gradient can incorporate knowledge from all domains without suffering gradient magnitude elimination or fluctuation\ntowards any particular domain. Experimental evaluations on datasets from DomainBed demonstrate competitive results yielded by POGM against other baselines while achieving computational efficiency."
    },
    {
        "title": "DreamMakeup: Face Makeup Customization using Latent Diffusion Models",
        "link_suffix": "/forum?id=WUibctXLT7",
        "link": "https://openreview.net/forum?id=WUibctXLT7",
        "pdf_link": "https://openreview.net/pdf?id=WUibctXLT7",
        "keywords": "Diffusion model, Makeup customization, Image editing",
        "abstract": "The exponential growth of the global makeup market has paralleled advancements in virtual makeup simulation technology. Despite the progress led by GANs, their application still encounters significant challenges, including training instability and limited customization capabilities. Addressing these challenges, this paper introduces DreamMakup: a novel Diffusion model based Makeup Customization, leveraging the inherent advantages of diffusion models for superior controllability and precise real-image editing. DreamMakeup employs early-stopped DDIM inversion to preserve the facial structure and identity while enabling extensive customization through various conditioning inputs such as reference images, specific RGB colors, and textual descriptions. Our model demonstrates notable improvements over existing GAN-based frameworks, improved customization, color-matching capabilities, and compatibility with textual descriptions or LLMs with affordable computational costs. Project page is available athttps://dreammakeup.github.io/"
    },
    {
        "title": "tBen: Benchmarking and Testing the Rule-Based Temporal Logic Reasoning Ability of Large Language Models with DatalogMTL",
        "link_suffix": "/forum?id=q3MYZQ3es8",
        "link": "https://openreview.net/forum?id=q3MYZQ3es8",
        "pdf_link": "https://openreview.net/pdf?id=q3MYZQ3es8",
        "keywords": "Temporal Logic Reasoning, Large Language Models, DatalogMTL",
        "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks, including multi-hop question answering, knowledge probing, and symbolic commonsense reasoning. While LLMs have advanced the state-of-the-art in these areas, their ability to explicitly solve rule-based temporal logic reasoning problems\u2014a complex cognitive process involving the understanding, representation, and manipulation of temporal information such as events, their durations, and relationships\u2014remains unexplored. To enhance understanding of LLM performance in this common task widely explored in the traditional symbolic AI field, we have developed a new set of synthetic benchmarks for rule-based temporal logic reasoning tBen. Our tBen benchmarks are built within the context of DatalogMTL, a powerful knowledge representation language for reasoning about the properties of systems that evolve over time, in which we provide flexible configurations for customizing temporal rules and task complexity.We evaluated the close-sourced GPT-4o and the open-sourced Llama-3 using three common prompting settings\u2014$\\textit{zero-shot}$, $\\textit{few-shot}$, and $\\textit{zero-shot-CoT}$\u2014on our synthetic benchmarks. Our key findings are as follows: (i) Without generating the reasoning process (chain-of-thought), even  advanced LLMs like GPT-4o exhibited nearly random performance on these rule-based temporal logic reasoning tasks. However, with chain-of-thought prompting, LLMs demonstrated preliminary temporal logical reasoning abilities; (ii) Both GPT-4o and Llama-3 were unable to solve temporal logical reasoning problems involving recursion, indicating a lack of advanced complex reasoning capabilities in understanding symbolic representations involving time; (iii) There is significant room for improvement in leveraging large language models to address problems widely explored in the traditional logic-based AI domain. Prompts and datasets are available in the appendix, and a datasheet for tBen is also provided."
    },
    {
        "title": "SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning",
        "link_suffix": "/forum?id=fv2hL5n2mh",
        "link": "https://openreview.net/forum?id=fv2hL5n2mh",
        "pdf_link": "https://openreview.net/pdf?id=fv2hL5n2mh",
        "keywords": "AutoML, AutoDS, LLM, Agents, Tree Search, Planning",
        "abstract": "Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-$\\textbf{S}$earch $\\textbf{E}$nhanced $\\textbf{L}$LM $\\textbf{A}$gents ($\\textbf{SELA}$), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space.\nThis novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges. The code will be open-sourced upon publication."
    },
    {
        "title": "Collective Model Intelligence Requires Compatible Specialization",
        "link_suffix": "/forum?id=XVHXVdoV11",
        "link": "https://openreview.net/forum?id=XVHXVdoV11",
        "pdf_link": "https://openreview.net/pdf?id=XVHXVdoV11",
        "keywords": "model merging, collective intelligence, routing, position, analysis",
        "abstract": "In this work, we explore the limitations of combining models by averaging intermediate features, referred to as $\\textit{model merging}$, and propose a new direction for achieving collective model intelligence through what we call $\\textit{compatible specialization}$. Current methods for model merging, such as parameter and feature averaging, struggle to effectively combine specialized models due to representational divergence during fine-tuning. As models specialize to their individual domains, their internal feature representations become increasingly incompatible, leading to poor performance when attempting to merge them for new tasks. We analyze this phenomenon using centered kernel alignment (CKA) and show that as models specialize, the similarity in their feature space structure diminishes, hindering their capacity for collective use. To address these challenges, we investigate routing-based merging strategies, which offer more flexible methods for combining specialized models by dynamically routing across different layers. This allows us to improve on existing methods by combining features from multiple layers rather than relying on fixed, layer-wise combinations. However, we find that these approaches still face limitations when layers within models are representationally incompatible. Our findings highlight the importance of designing new approaches for model merging that operate on well-defined input and output spaces, similar to how humans communicate through language rather than intermediate neural activations."
    }
]