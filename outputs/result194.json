[
    {
        "title": "Systems with Switching Causal Relations: A Meta-Causal Perspective",
        "link_suffix": "/forum?id=J9VogDTa1W",
        "link": "https://openreview.net/forum?id=J9VogDTa1W",
        "pdf_link": "https://openreview.net/pdf?id=J9VogDTa1W",
        "keywords": "Meta-Causality, Meta-Causal Reasoning, Agent Behavior, System Dynamics",
        "abstract": "Most works on causality in machine learning assume that causal relationships are governed by a constant underlying process. However, the flexibility of agents' actions or tipping point behavior in the environmental process can change the qualitative dynamics of the system. As a result, new causal relationships may emerge, while existing ones change or disappear, resulting in an altered causal graph. To analyze these qualitative changes on the causal graph, we propose the concept ofmeta-causal states, which groups classical causal models into clusters based on equivalent qualitative behavior and consolidates specific mechanism parameterizations. We demonstrate how meta-causal states can be inferred from observed agent behavior, and discuss potential methods for disentangling these states from unlabeled data. Finally, we direct our analysis toward the application of a dynamical system, demonstrating that meta-causal states can also emerge from inherent system dynamics, and thus constitute more than a context-dependent framework in which mechanisms emerge merely as a result of external factors."
    },
    {
        "title": "pMixFed: Mixing up model coefficients for Efficient Personalized Federated Learning",
        "link_suffix": "/forum?id=lX8DahcJbL",
        "link": "https://openreview.net/forum?id=lX8DahcJbL",
        "pdf_link": "https://openreview.net/pdf?id=lX8DahcJbL",
        "keywords": "Personalized federated learning, Distributed computing",
        "abstract": "Federated Learning  enables decentralized collaborative learning of machine learning models which presents challenges such as data privacy  and client drift for heterogeneous data. Traditional FL methods offer strong generalization but lack personalized solutions for non-IID data. Personalized federated learning (PFL) addresses   data heterogeneity by tackling these issues through balancing generalization and personalization level. It, however, still faces challenges such as optimal model partitioning and catastrophic forgetting that reduce quality and accuracy of both local and global models. To address these challenges,  we propose ``pMixFed'', a dynamic, layer-wise PFL approach integrating mixup between shared global and personalized local models. We develop adaptive partitioning between shared and personalized layers of the model,  gradual transition of personalization to allow seamless adaptation of local clients, improved generalization across clients, and mitigation of catastrophic forgetting. We provide theoretical analysis of pMixFed. Further, we conduct extensive experiments to demonstrate   its superior performance compared with the existing PFL methods. Empirical results hows faster training, increased robustness, and improved handling of  heterogeneity when using pMixFed as compared with the state-of-the-art PFL models."
    },
    {
        "title": "FairlyUncertain: A Comprehensive Benchmark of Uncertainty in Algorithmic Fairness",
        "link_suffix": "/forum?id=C1Wp4ubvXZ",
        "link": "https://openreview.net/forum?id=C1Wp4ubvXZ",
        "pdf_link": "https://openreview.net/pdf?id=C1Wp4ubvXZ",
        "keywords": "Heteroscedastic, Uncertainty, Fairness, Benchmark",
        "abstract": "Fair predictive algorithms hinge on both equality and trust, yet inherent uncertainty in real-world data challenges our ability to make consistent, fair, and calibrated decisions. While fairly managing predictive error has been extensively explored, some recent work has begun to address the challenge of fairly accounting for irreducible prediction uncertainty. However, a clear taxonomy and well-specified objectives for integrating uncertainty into fairness remains undefined. We address this gap by introducing FairlyUncertain, an axiomatic benchmark for evaluating uncertainty estimates in fairness. Our benchmark posits that fair predictive uncertainty estimates should be consistent across learning pipelines and calibrated to observed randomness. Through extensive experiments on 10 popular fairness datasets, our evaluation reveals: (1) A theoretically justified and simple method for estimating uncertainty in binary settings is more consistent and calibrated than prior work; (2) Abstaining from binary predictions, even with improved uncertainty estimates, reduces error but does not alleviate outcome imbalances between demographic groups; (3) Incorporating consistent and calibrated uncertainty estimates in regression tasks improves fairness without any explicit fairness interventions. Our benchmark package is designed to be extensible and open-source. By providing a standardized framework for assessing the interplay between uncertainty and fairness, FairlyUncertain paves the way for more equitable and trustworthy machine learning practices."
    },
    {
        "title": "When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers",
        "link_suffix": "/forum?id=vRvVVb0NAz",
        "link": "https://openreview.net/forum?id=vRvVVb0NAz",
        "pdf_link": "https://openreview.net/pdf?id=vRvVVb0NAz",
        "keywords": "Task arithmetic, generalization, nonlinear Transformers, deep learning theory, machine unlearning",
        "abstract": "weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B)."
    },
    {
        "title": "Provably Efficient Linear Bandits with Instantaneous Constraints in Non-Convex Feature Spaces",
        "link_suffix": "/forum?id=yOrtDi6IXs",
        "link": "https://openreview.net/forum?id=yOrtDi6IXs",
        "pdf_link": "https://openreview.net/pdf?id=yOrtDi6IXs",
        "keywords": "Linear Bandits, Non-convex feature spaces, Instantaneous hard constraints, Safety, UCB",
        "abstract": "In linear stochastic bandits, tasks with instantaneous hard constraints present significant challenges, particularly when the feature space is non-convex or discrete. This is especially relevant in applications such as financial management, recommendation systems, and medical treatment selection, where safety constraints appear in non-convex forms or where decisions must often be made within non-convex and discrete sets. In these systems, bandit methods rely on the ability of feature functions to extract critical features. However, in contrast to the star-convexity assumption commonly discussed in the literature, these feature functions often lead to non-convex and more complex feature spaces. In this paper, we investigate linear bandits and introduce a method that operates effectively in a non-convex feature space while satisfying instantaneous hard constraints at each time step. We demonstrate that our method, with high probability, achieves a regret of $\\tilde{\\mathcal{O}}\\big( d (1+\\frac{\\tau}{\\epsilon \\iota}) \\sqrt{T}\\big)$ and meets the instantaneous hard constraints, where $d$ represents the feature space dimension, $T$ the total number of rounds, and $\\tau$ a safety related parameter. The constant parameters $\\epsilon$ and $\\iota$ are related to our localized assumptions around the origin and the optimal point. In contrast, standard safe linear bandit algorithms that rely on the star-convexity assumption often result in linear regret. Furthermore, our approach handles discrete action spaces while maintaining a comparable regret bound. Moreover, we establish an information-theoretic lower bound on the regret of $\\Omega \\left( \\max{ \\sqrt{(d-1)T}, \\frac{1}{\\epsilon \\iota^2} } \\right)$ for $T \\geq \\max (d-1, \\frac{32 e}{\\epsilon \\iota^2})$, emphasizing the critical role of $\\epsilon$ and $\\iota$ in the regret upper bound. Lastly, we provide numerical results to validate our theoretical findings."
    },
    {
        "title": "Generalization and Knowledge Transfer in Abstract Visual Reasoning Models",
        "link_suffix": "/forum?id=ddOxvs4NAq",
        "link": "https://openreview.net/forum?id=ddOxvs4NAq",
        "pdf_link": "https://openreview.net/pdf?id=ddOxvs4NAq",
        "keywords": "Abstract Visual Reasoning, Deep Learning, Generalization, Transfer Learning, Raven's Progressive Matrices",
        "abstract": "We study generalization and knowledge reuse capabilities of deep neural networks in the domain of abstract visual reasoning (AVR), employing Raven's Progressive Matrices (RPMs), a recognized benchmark task for assessing AVR abilities. Two knowledge transfer scenarios referring to the I-RAVEN dataset are investigated. Firstly, inspired by generalization assessment capabilities of the PGM dataset and popularity of I-RAVEN, we introduce Attributeless-I-RAVEN, a benchmark with $10$ generalization regimes that allow to test generalization of abstract rules applied to held-out attributes. Secondly, we construct I-RAVEN-Mesh, a dataset that enriches RPMs with a novel component structure comprising line-based patterns, facilitating assessment of progressive knowledge acquisition in transfer learning setting. The developed benchmarks reveal shortcomings of the contemporary deep learning models, which we partly address with Pathways of Normalized Group Convolution (PoNG) model, a novel neural architecture for solving AVR tasks. PoNG excels in both presented challenges, as well as the standard I-RAVEN and PGM setups. Encouraged by these promising results, we further evaluate PoNG in another AVR task, visual analogy problem with both synthetic and real-world images, demonstrating its strength beyond PRMs."
    },
    {
        "title": "Bandits with Anytime Knapsacks",
        "link_suffix": "/forum?id=qlzxeNESWI",
        "link": "https://openreview.net/forum?id=qlzxeNESWI",
        "pdf_link": "https://openreview.net/pdf?id=qlzxeNESWI",
        "keywords": "multi-armed bandits, knapsack problem, online learning",
        "abstract": "We consider bandits with anytime knapsacks (BwAK), a novel version of the BwK problem where there is an anytime cost constraint instead of a total cost budget. This problem setting introduces additional complexities as it mandates adherence to the constraint throughout the decision-making process. We propose SUAK, a novel algorithm that utilizes upper confidence bounds to identify the optimal mixture of arms while maintaining a balance between exploration and exploitation. SUAK is an adaptive algorithm that strategically utilizes the available budget in each round in the decision-making process and skips a round when it is possible to violate the anytime cost constraint. In particular, SUAK slightly under-utilizes the available cost budget to reduce the need for skipping rounds. We show that SUAK attains the same problem-dependent regret upper bound of $O(K \\log T)$ established in prior work under the simpler BwK framework. Finally, we provide simulations to verify the utility of SUAK in practical settings."
    },
    {
        "title": "Physics-Informed Autoencoder for Enhancing Data Quality to Improve the Forecasting Reliability of Carbon Dioxide Emissions from Agricultural Fields",
        "link_suffix": "/forum?id=381rZinzJE",
        "link": "https://openreview.net/forum?id=381rZinzJE",
        "pdf_link": "https://openreview.net/pdf?id=381rZinzJE",
        "keywords": "physics-informed machine learning, autoencoders, gap-fillling, net ecosystem exchange, noise, stochastic differential equation",
        "abstract": "Missing values in measurements for carbon dioxide emissions on drained peatlands remains an open challenge for training forecasting techniques to achieve net zero. At the field scale, existing methods struggle to model $\\ce{CO_2}$ emissions to fill gaps, especially in nighttime measurements. We propose robust Physics-Informed Autoencoders (PIAEs), which combine the generative capabilities of Autoencoders with the reliability of physical models of Net Ecosystem Exchange (NEE) that quantify $\\ce{CO_2}$ exchanges between the atmosphere and major carbon pools. Our method integrates equations describing the physical processes and associated uncertainties to fill gaps in NEE measurements from eddy covariance (EC) flux towers. In the PIAE, various sensor measurements are encoded into the latent space, and a set of decoders is then used to approximate the ecosystem parameters and the optimal NEE forecast, directed by dynamics described by a stochastic differential equation. These decoders utilize nighttime and daytime NEE models that describe carbon transfer as a Wiener process. Finally, we use a two-phased training routine with two loss functions describing each phase: Mean Squared Error (MSE) and Maximum Mean Discrepancy (MMD) between the measurements and the reconstructed samples. PIAE outperforms the current state-of-the-art Random Forest Robust on the prediction of nighttime NEE measurements on various distribution-based and data-fitting metrics. We present significant improvement in capturing temporal trends in the NEE at daily, weekly, monthly and quarterly scales."
    },
    {
        "title": "An Efficient Quantum Classifier Based on Hamiltonian Representations",
        "link_suffix": "/forum?id=3HPOtZxs5s",
        "link": "https://openreview.net/forum?id=3HPOtZxs5s",
        "pdf_link": "https://openreview.net/pdf?id=3HPOtZxs5s",
        "keywords": "quantum computing, quantum machine learning, variational quantum circuits, quantum encoding",
        "abstract": "Quantum computing shows great potential for expanding the range of efficiently solvable problems. This promise arises from the advantageous resource and runtime scaling of certain quantum algorithms over classical ones. Quantum machine learning (QML) seeks to extend these advantages to data-driven methods. Initial evidence suggests quantum-based models can outperform classical ones in terms of scaling, runtime and generalization capabilities. However, critics have pointed out that many works rely on extensive feature reduction or use toy datasets to draw their conclusions, raising concerns about their applicability to larger problems. Scaling up these results is challenging due to hardware limitations and the high costs generally associated with encoding dense vector representations on quantum devices. To address these challenges, we propose an efficient approach called Hamiltonian classifier inspired by ground-state energy optimization in quantum chemistry. This method circumvents the costs associated with data encoding by mapping inputs to a finite set of Pauli strings and computing predictions as their expectation values. In addition, we introduce two variants with different scaling in terms of parameters and sample complexity. We evaluate our approach on text and image classification tasks, comparing it to well-established classical and quantum models. Our results show the Hamiltonian classifier delivers performance comparable to or better than these methods. Notably, our method achieves logarithmic complexity in both qubits and quantum gates, making it well-suited for large-scale, real-world applications."
    },
    {
        "title": "CompassDock: Comprehensive Accurate Assessment Approach for Deep Learning-Based Molecular Docking in Inference and Fine-Tuning",
        "link_suffix": "/forum?id=nWO75tVjfp",
        "link": "https://openreview.net/forum?id=nWO75tVjfp",
        "pdf_link": "https://openreview.net/pdf?id=nWO75tVjfp",
        "keywords": "Compass, DL-based Molecular Docking, LAN-MSE, Favorable Physico-chemical & Bioactivity Features",
        "abstract": "Datasets used for molecular docking, such as PDBBind, contain technical variability - they are noisy. Although the origins of the noise have been discussed , a comprehensive analysis of physical, chemical, and bioactivity characteristics of the datasets is still lacking. To address this gap, we introduce the Compass. Compass integrates two key components: PoseCheck, which examines ligand strain energy, protein-ligand steric clashes, and interactions, and AA-Score, a new empirical scoring function for calculating binding affinity energy. Together, these form a unified workflow that assesses both the physical/chemical properties and bioactivity favorability of ligands and protein-ligand interactions. Our analysis of the PDBBind dataset using Compass reveals substantial noise in the ground truth data. Additionally, we propose CompassDock, which incorporates the Compass module with DiffDock, the state-of-the-art deep learning-based molecular docking method, to enable accurate assessment of docked ligands during inference. Finally, we present a new paradigm for enhancing molecular docking model performance by fine-tuning with Compass Scores, which encompass binding affinity energy, strain energy, and the number of steric clashes identified by Compass. Our results show that, while fine-tuning without Compass improves the percentage of docked poses with RMSD < 2\u00c5, it leads to a decrease in physical/chemical and bioactivity favorability. In contrast, fine-tuning with Compass shows a limited improvement in RMSD < 2\u00c5 but enhances the physical/chemical and bioactivity favorability of the ligand conformation. The source code is available athttps://github.com/anonym8171iclr2025/iclr_2025_paperid_8171."
    },
    {
        "title": "Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models",
        "link_suffix": "/forum?id=HyN9POiYhN",
        "link": "https://openreview.net/forum?id=HyN9POiYhN",
        "pdf_link": "https://openreview.net/pdf?id=HyN9POiYhN",
        "keywords": "fairness, bias transfer hypothesis, prompt adaptation, large language models, coreference resolution",
        "abstract": "Large language models (LLMs) are increasingly being adapted to new tasks and deployed in real-world decision systems. Several previous works have investigated the bias transfer hypothesis (BTH) and find that fairness of pre-trained masked language models has limited effect on the fairness of these models when adapted using fine-tuning. In this work, we expand the study of BTH to causal models under prompt adaptations, as prompting is an accessible, and compute-efficient way to deploy models in real-world systems. In contrast to previous work,  we establish that intrinsic biases in pre-trained Mistral, Falcon and Llama models are strongly correlated (rho >= 0.94) with biases when the same models are zero- and few-shot prompted, using a pronoun co-reference resolution task. Further, we find that biases remain strongly correlated even when LLMs are specifically pre-prompted to exhibit fair or biased behavior (rho >= 0.92), and also when varying few shot composition parameters such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). Our findings highlight the importance of ensuring fairness in pre-trained LLMs, especially when they are later used to perform downstream tasks via prompt adaptation."
    },
    {
        "title": "Can we talk models into seeing the world differently?",
        "link_suffix": "/forum?id=iVMcYxTiVM",
        "link": "https://openreview.net/forum?id=iVMcYxTiVM",
        "pdf_link": "https://openreview.net/pdf?id=iVMcYxTiVM",
        "keywords": "vision language models, vision biases, shape/texture bias",
        "abstract": "Unlike traditional vision-only models, vision language models (VLMs) offer an intuitive way to access visual content through language prompting by combining a large language model (LLM) with a vision encoder. However, both the LLM and the vision encoder come with their own set of biases, cue preferences, and shortcuts, which have been rigorously studied in uni-modal models. A timely question is how such (potentially misaligned) biases and cue preferences behave under multi-modal fusion in VLMs. \nAs a first step towards a better understanding, we investigate a particularly well-studied vision-only bias - the texture vs. shape bias and the dominance of local over global information. \nAs expected, we find that VLMs inherit this bias to some extent from their vision encoders. Surprisingly, the multi-modality alone proves to have important effects on the model behavior, i.e., the joint training and the language querying change the way visual cues are processed. \nWhile this direct impact of language-informed training on a model's visual perception is intriguing, it raises further questions on our ability to actively steer a model's output so that its prediction is based on particular visual cues of the user's choice. \nInterestingly, VLMs have an inherent tendency to recognize objects based on shape information, which is different from what a plain vision encoder would do. Further active steering towards shape-based classifications through language prompts is however limited. In contrast, active VLM steering towards texture-based decisions through simple natural language prompts is often more successful."
    },
    {
        "title": "Vision models trained to estimate spatial latents learned similar ventral-stream-aligned representations",
        "link_suffix": "/forum?id=emMMa4q0qw",
        "link": "https://openreview.net/forum?id=emMMa4q0qw",
        "pdf_link": "https://openreview.net/pdf?id=emMMa4q0qw",
        "keywords": "Vision, Convolutional Neural Networks, Representation Learning, Neural Data Alignment, Ventral Visual Stream, Computational Neuroscience",
        "abstract": "Studies of the functional role of the primate ventral visual stream have traditionally focused on object categorization, often ignoring -- despite much prior evidence -- its role in estimating \"spatial\" latents such as object position and pose. Most leading ventral stream models are derived by optimizing networks for object categorization, which seems to imply that the ventral stream is also derived under such an objective. Here, we explore an alternative hypothesis: Might the ventral stream be optimized for estimating spatial latents? And a closely related question: How different -- if at all -- are representations learned from spatial latent estimation compared to categorization? To ask these questions, we leveraged synthetic image datasets generated by a 3D graphic engine and trained convolutional neural networks (CNNs) to estimate different combinations of spatial and category latents. We found that models trained to estimate just a few spatial latents achieve neural alignment scores comparable to those trained on hundreds of categories, and the spatial latent performance of models strongly correlates with their neural alignment. Spatial latent and category-trained models have very similar -- but not identical -- internal representations, especially in their early and middle layers. We provide evidence that this convergence is partly driven by non-target latent variability in the training data, which facilitates the implicit learning of representations of those non-target latents. Taken together, these results suggest that many training objectives, such as spatial latents, can lead to similar models aligned neurally with the ventral stream. Thus, one should not assume that the ventral stream is optimized for object categorization only. As a field, we need to continue to sharpen our measures of comparing models to brains to better understand the functional roles of the ventral stream."
    },
    {
        "title": "Temporal Graph Rewiring with Expander Graphs",
        "link_suffix": "/forum?id=T8fCTYPWBr",
        "link": "https://openreview.net/forum?id=T8fCTYPWBr",
        "pdf_link": "https://openreview.net/pdf?id=T8fCTYPWBr",
        "keywords": "Temporal Graphs, Dynamic Graphs, Graph Neural Network, Graph Representation Learning",
        "abstract": "Evolving relations in real-world networks are often modelled by temporal graphs. Temporal Graph Neural Networks (TGNNs) emerged to model evolutionary behaviour of such graphs by leveraging the message passing primitive at the core of Graph Neural Networks (GNNs). It is well-known that GNNs are vulnerable to several issues directly related to the input graph topology, such as under-reaching and over-squashing---we argue that these issues can often get exacerbated in temporal graphs, particularly as the result of stale nodes and edges. While graph rewiring techniques have seen frequent usage in GNNs to make the graph topology more favourable for message passing, they have not seen any mainstream usage on TGNNs. In this work, we propose Temporal Graph Rewiring (TGR), the first approach for graph rewiring on temporal graphs, to the best of our knowledge. TGR constructs message passing highways between temporally distant nodes in a continuous-time dynamic graph by utilizing expander graph propagation, a prominent framework used for graph rewiring on static graphs which makes minimal assumptions on the underlying graph structure. On the challenging TGB benchmark, TGR achieves state-of-the-art results on tgbl-review, tgbl-coin, tgbl-comment and tgbl-flight datasets at the time of writing. For tgbl-review, TGR has 50.5% improvement in MRR over the base TGN model and 22.2% improvement over the base TNCN model. The significant improvement over base models demonstrates clear benefits of temporal graph rewiring."
    },
    {
        "title": "Discretization of continuous input spaces in the hippocampal autoencoder",
        "link_suffix": "/forum?id=sOQmgO0PTv",
        "link": "https://openreview.net/forum?id=sOQmgO0PTv",
        "pdf_link": "https://openreview.net/pdf?id=sOQmgO0PTv",
        "keywords": "NeuroAI, Memory, Sparse autoencoders, Hippocampus",
        "abstract": "Understanding the encoding mechanisms of hippocampal place cells remains a significant challenge in neuroscience. Although sparse autoencoders have been shown to exhibit place cell-like activity, the underlying processes are not fully understood. In this study, we compare spatial representations learned by dense and sparse autoencoders trained on images of 3D environments and find that only sparse autoencoders with orthonormal activity regularization in latent space produce place cells. We then show that this regularization promotes similar images to map onto the same neurons, acting as a locality-sensitive hash function. Notably, we demonstrate that these neurons are visually interpretable through activity clamping and decoding, suggesting the formation of detailed episodic memories at the single-neuron level. We then introduce a novel metric to quantify how neurons discretize the image space into disjoint receptive fields, revealing that sparse autoencoders tile input spaces with minimal overlap. Furthermore, we observe that whereas dense autoencoders generate population codes resembling visual cortex activity near criticality, sparse autoencoders produce higher-dimensional codes, thus suggesting a similar coding strategy in the hippocampus. Extending our approach to the auditory domain, we also replicate the emergence of \"frequency place cells\" by training sparse autoencoders on audio snippets sampled from a frequency-varying signal, and show that population representations retain the statistical structure of the sample distribution. Lastly, we demonstrate that reinforcement learning agents can leverage these high-dimensional image representations to solve complex spatial-cognitive tasks, despite their inherent brittleness. Overall, our findings elucidate how sparse input compression in autoencoders can give rise to discrete, interpretable memories, establishing an explicit link between episodic memory formation and spatial representations in the hippocampus."
    },
    {
        "title": "Representation Confusion: Towards Representation Backdoor on CLIP via Concept Activation",
        "link_suffix": "/forum?id=RZ3m2LMYze",
        "link": "https://openreview.net/forum?id=RZ3m2LMYze",
        "pdf_link": "https://openreview.net/pdf?id=RZ3m2LMYze",
        "keywords": "concept, backdoor, CLIP",
        "abstract": "Backdoor attacks pose a significant threat to deep learning models, allowing attackers to stealthily embed hidden triggers that can be exploited during inference. Traditional backdoor attacks typically rely on inserting external patches or perturbations into input data as triggers. However, two key challenges remain, i.e., how to evade detection by defense mechanisms and reduce the computational cost of trigger insertion. To address these challenges and design more advanced backdoor techniques, we first explore the underlying mechanisms of backdoor attacks through the lens of cognitive neuroscience, drawing parallels between model decision-making and human cognitive processes. We conceptualize the decision process elicited by the backdoor-triggering as movement between representation spaces (i.e., learned concepts). Thus, existing methods can be seen as implicit manipulations of these stored concepts. This raises a key question: \\textit{Why not manipulate the concept explicitly? Could the inherent concepts in the model's reasoning serve as an ``internal trigger'' for the backdoor?} Motivated by this, we propose a novel backdoor attack framework, namely Representation Confusion (RepConfAttack), which explicitly manipulates concepts within the model's representation spaces. This approach eliminates the need for backdoor triggers and enhances stealthness by making the attack harder to detect with traditional defenses. Experimental results demonstrate the effectiveness of our method, achieving high attack success rates even against robust defense mechanisms."
    },
    {
        "title": "When and how are modular networks better?",
        "link_suffix": "/forum?id=Olb8JwUGZ3",
        "link": "https://openreview.net/forum?id=Olb8JwUGZ3",
        "pdf_link": "https://openreview.net/pdf?id=Olb8JwUGZ3",
        "keywords": "Neural networks, hierarchical modularity, sparsity, generalization, training efficiency",
        "abstract": "Many real-world learning tasks have an underlying hierarchical modular structure, composed of smaller sub-functions. Traditional neural networks (NNs), however, often ignore this structure, leading to inefficiencies in learning and generalization. Leveraging known structural information can enhance performance by aligning the network architecture with the task\u2019s inherent modularity. In this work, we investigate how modular NNs can outperform traditional dense networks by systematically varying the degree of structural knowledge incorporated. We compare architectures ranging from monolithic dense NNs, which assume no prior knowledge, to hierarchically modular NNs with shared modules, which leverage sparsity, modularity, and module reusability. Our experiments demonstrate that incorporating structural knowledge, particularly through module reuse and fixed connectivity, significantly improves learning efficiency and generalization. Hierarchically modular NNs excel in data-scarce scenarios by promoting functional specialization within the modules and reducing redundancy. These findings suggest that task-specific architectural biases can lead to more efficient, interpretable, and effective learning systems."
    },
    {
        "title": "Enhancing Multi-Modal Reasoning Over Time-Series and Natural Language Data",
        "link_suffix": "/forum?id=Pik26bc4Jx",
        "link": "https://openreview.net/forum?id=Pik26bc4Jx",
        "pdf_link": "https://openreview.net/pdf?id=Pik26bc4Jx",
        "keywords": "Time-Series, Natural Language Processing, Multi-Modal",
        "abstract": "Time-series analysis is critical in many industries such as healthcare, finance and energy sectors, where understanding time-series trends alongside contextual information is essential for informed decision making. However, current time-series models are limited in their ability to perform reasoning that involves both time-series data and textual information. In this work we address this gap by introducing Chat-TS, a large language model (LLM)  designed specifically for reasoning over time-series and textual data. Unlike traditional time-series models Chat-TS integrates time-series tokens into the LLM vocabulary, enhancing its reasoning ability over both text and time-series modalities without compromising its core natural language capabilities.\nTo support the development and validation of Chat-TS we contribute three new datasets: the TS Instruct Training Dataset which pairs diverse time-series data with relevant text instructions and responses for instruction tuning, the TS Instruct question and answer (QA) benchmark, a set of nearly 4000 multiple-choice questions designed to evaluate multi-modal reasoning and the TS Instruct Qualitative Benchmark which provides a smaller subset of QA, math and decision making questions for LLM evaluation. Our training strategy preserves the inherent reasoning capabilities of the LLM while augmenting it with time-series reasoning capabilities. Evaluation results show that Chat-TS achieves state-of-the-art performance in multi-modal reasoning tasks, maintaining strong natural language proficiency while advancing time-series reasoning. All models, datasets, and code will be made publicly available [link]."
    },
    {
        "title": "Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive Speech Recognition",
        "link_suffix": "/forum?id=LrmPGtnros",
        "link": "https://openreview.net/forum?id=LrmPGtnros",
        "pdf_link": "https://openreview.net/pdf?id=LrmPGtnros",
        "keywords": "speech recognition, sequence modeling, non-autoregressive models, RNN-T, Transducers",
        "abstract": "We present Hybrid-Autoregressive Inference Transducers (HAI-T), a novel architecture for speech recognition that extends the Token-and-Duration Transducer (TDT) model. Trained with randomly masked predictor network outputs, HAI-T supports both autoregressive inference with all network components and non-autoregressive inference without the predictor. Additionally, we propose a novel semi-autoregressive inference paradigm that first generates an initial hypothesis using non-autoregressive inference, followed by refinement steps where each token prediction is regenerated using parallelized autoregression on the initial hypothesis. Experiments on multiple datasets across different languages demonstrate that HAI-T achieves efficiency parity with CTC in non-autoregressive mode and with TDT in autoregressive mode. In terms of accuracy, autoregressive HAI-T outperforms TDT and RNN-T, while non-autoregressive HAI-T significantly outperforms CTC. Semi-autoregressive inference further enhances the model's accuracy with minimal computational overhead, and even outperforms TDT results in some cases. These results highlight HAI-T's flexibility in balancing accuracy and speed, positioning it as a strong candidate for real-world speech recognition applications."
    },
    {
        "title": "Theory of LLM sampling: part descriptive and part prescriptive",
        "link_suffix": "/forum?id=ejvf3JrZuC",
        "link": "https://openreview.net/forum?id=ejvf3JrZuC",
        "pdf_link": "https://openreview.net/pdf?id=ejvf3JrZuC",
        "keywords": "decision making, sampling, prescriptive norm, heuristics",
        "abstract": "Large Language Models (LLMs) are increasingly utilized in autonomous decision-making systems, where they sample options from an action space. However, the underlying heuristics guiding the sampling of LLMs remain under-explored. We examine LLM response sampling and propose a theory that the sample of an LLM is driven by a descriptive component  (the notion of statistical average) and  a prescriptive component (notion of an ideal represented in the LLM). In a controlled experimental setting, we demonstrate that LLM outputs deviate from statistically probable outcome in the direction of a presciptive component. We further show this deviation towards prescriptive component consistently appears across diverse real-world domains, including social, public health, and scientific contexts. Using this theory, we show that concept prototypes in LLMs are affected by prescriptive norms, similar to concept of normality in humans. Through case studies, we illustrate that in real-world applications, the shift toward an ideal value in LLM outputs can result in significantly biased decision-making, raising ethical concerns."
    },
    {
        "title": "Efficient Distributed Principal Component Analysis with Parallel Deflation",
        "link_suffix": "/forum?id=X0ytIvgvxR",
        "link": "https://openreview.net/forum?id=X0ytIvgvxR",
        "pdf_link": "https://openreview.net/pdf?id=X0ytIvgvxR",
        "keywords": "Principal Component Analysis, Distributed Learning",
        "abstract": "We study a distributed Principal Component Analysis (PCA) framework where each worker targets a distinct eigenvector and refines its solution by updating from intermediate solutions provided by peers deemed as \"superior\". Drawing intuition from the delation methods, which is traditionally used in centralized eigenvalue problems, our method breaks the sequential dependency in between the deflation steps and allows asynchronous updates of workers while incurring only a small communication cost. To our knowledge, a critical gap in the literature --the theoretical underpinning of such distributed, dynamic interactions among workers-- has remained unaddressed until now. This paper offers the first theoretical analysis explaining why, how, and when these intermediate, hierarchical updates lead to practical and provable convergence in distributed environments. Our theoretical contributions demonstrate that such a distributed PCA algorithm not only converges effectively but does so in a manner that is favorably scalable. We also demonstrate through experiments that our proposed framework offers comparable performance to EigenGame-$\\mu$, the state-of-the-art model-parallel PCA solver."
    },
    {
        "title": "On Information-Theoretic Measures of Predictive Uncertainty",
        "link_suffix": "/forum?id=MNGMpHxi1I",
        "link": "https://openreview.net/forum?id=MNGMpHxi1I",
        "pdf_link": "https://openreview.net/pdf?id=MNGMpHxi1I",
        "keywords": "Uncertainty quantification, Predictive Uncertainty, Information-Theory, Bayesian methods",
        "abstract": "Reliable estimation of predictive uncertainty is crucial for machine learning applications, particularly in high-stakes scenarios where hedging against risks is essential. Despite its significance, a consensus on the correct measurement of predictive uncertainty remains elusive. In this work, we return to first principles to develop a fundamental framework of information-theoretic predictive uncertainty measures. Our proposed framework categorizes predictive uncertainty measures according to two factors: (I) The predicting model (II) The approximation of the true predictive distribution. Examining all possible combinations of these two factors, we derive a set of predictive uncertainty measures that includes both known and newly introduced ones. We empirically evaluate these measures in typical uncertainty estimation settings, such as misclassification detection, selective prediction, and out-of-distribution detection. The results show that no single measure is universal, but the effectiveness depends on the specific setting. Thus, our work provides clarity about the suitability of predictive uncertainty measures by clarifying their implicit assumptions and relationships."
    },
    {
        "title": "Model Developmental Safety: A Safety-Centric Method and Applications in Vision-Language Models",
        "link_suffix": "/forum?id=BRDqmYU8A0",
        "link": "https://openreview.net/forum?id=BRDqmYU8A0",
        "pdf_link": "https://openreview.net/pdf?id=BRDqmYU8A0",
        "keywords": "Model Developmental Safety, Continual Learning, Vision-Language Models, Constrained Optimization",
        "abstract": "In the real world, a learning-enabled system usually undergoes multiple cycles of model development to enhance the system's ability to handle difficult or emerging tasks, which involve collecting new data, training a new model and validating the model.  This continual model development process raises a significant issue that the model development for acquiring new or improving existing capabilities may inadvertently lose capabilities of the old model, also known as catastrophic forgetting. Existing continual learning studies focus on mitigating catastrophic forgetting by trading off performance on previous tasks and new tasks to ensure good average performance.  However, they are inadequate for many applications especially in safety-critical domains, as failure to preserve the performance of the old model not only introduces safety risks and uncertainties but also imposes substantial expenses in the re-improving and re-validation of existing properties. To address this issue, we introducemodel developmental safety as a guaranteeof a learning system such that in the model development process the new model should strictly preserve the existing protected capabilities of the old model while improving its performance on target tasks. \nTo ensure the model developmental safety, we present a safety-centric framework by formulating the model developmental safety as data-dependent constraints. Under this framework, we study how to develop a pretrained vision-language model (aka the CLIP model) for acquiring new capabilities or improving existing capabilities of image classification. We propose an efficient constrained optimization algorithm with theoretical guarantee and use its insights to finetune a CLIP model with task-dependent heads for promoting the model developmental safety. Our experiments on improving vision perception capabilities in autonomous driving dataset and scene recognition dataset demonstrate the efficacy of the proposed approach."
    },
    {
        "title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
        "link_suffix": "/forum?id=6VhDQP7WGX",
        "link": "https://openreview.net/forum?id=6VhDQP7WGX",
        "pdf_link": "https://openreview.net/pdf?id=6VhDQP7WGX",
        "keywords": "vision language model, inference scaling, visual token compression",
        "abstract": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks, driven by incorporating image representations into the token inputs of Large Language Models (LLMs). However, their real-world deployment is often constrained by high latency during inference due to the substantial compute required by the LLM to process the large number of input tokens, predominantly arising from the image. To reduce inference costs, one can either downsize the LLM or reduce the number of input tokens needed to represent the image, the latter of which has been the focus of many recent efforts around token compression. However, it is unclear what the optimal trade-off is given a fixed inference budget. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs is achieved by using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5-10x), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take the first steps toward designing token compression algorithms tailored for high-compression settings, utilizing prompt-based compression of tokens. Our work underscores the performance and efficiency benefits of operating in low visual token regimes and the importance of developing tailored token reduction algorithms for such conditions."
    },
    {
        "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
        "link_suffix": "/forum?id=EkfLaCJ7bk",
        "link": "https://openreview.net/forum?id=EkfLaCJ7bk",
        "pdf_link": "https://openreview.net/pdf?id=EkfLaCJ7bk",
        "keywords": "efficient transformer serving, sparse attention decoding",
        "abstract": "Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to $2.1\\times$."
    }
]