[{"title": "Activations Aren't Cheap in LoRA, Weights Are", "link_suffix": "/forum?id=3ylNuZXtMg", "link": "https://openreview.net/forum?id=3ylNuZXtMg", "pdf_link": "https://openreview.net/pdf?id=3ylNuZXtMg", "keywords": "PEFT, LoRA, finetuning, LLM, memory efficiency, diffusion", "abstract": "LoRA has become the prevailing technique for finetuning large neural networks with limited computational resources. Historically, activations have been regarded as small and computationally inexpensive to manipulate\u2014a view reflected by LoRA, which leverages this assumption and adds a low-rank term to intermediate activations. However, in the era of modern large language models (LLMs) and diffusion models, this notion has been challenged by the desire for increasing context lengths and smaller models, a trend which inevitably leads activations to consume more memory than the model weights themselves. Surprisingly, when finetuning a 1B model with a context length greater than 2048, we find that LoRA finetuning uses more memory than full-parameter finetuning. This study finds that manipulating additional model weights within the computation graph in parameter-efficient finetuning techniques can often be more memory-efficient than operating on the activations. We provide a semantically-equivalent computation graph reformulation for LoRA, and other popular PeFT techniques, which saves memory and trains faster, advancing the Pareto-frontier for finetuning tasks that can be achieved on consumer hardware. Under practical conditions, this reformulation provides up to a 1.4x reduction in max memory usage and latency for LoRA finetuning across various language and diffusion transformers.", "title_embedding_index": 7450, "title_abs_embedding_index": 7475}, {"title": "Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models", "link_suffix": "/forum?id=diXvBHiRyE", "link": "https://openreview.net/forum?id=diXvBHiRyE", "pdf_link": "https://openreview.net/pdf?id=diXvBHiRyE", "keywords": "Code Generation, Multidimension, Benchmark, LLM Evaluation", "abstract": "In recent years, researchers have proposed numerous benchmarks to evaluate the impressive coding capabilities of large language models (LLMs). However, current benchmarks primarily assess the accuracy of LLM-generated code, while neglecting other critical dimensions that also significantly impact code quality in real-world development. Moreover, relying exclusively on correctness as the guiding metric renders LLMs vulnerable to data contamination. Therefore, this paper proposes theRACEbenchmark, which comprehensively evaluates the quality of code generated by LLMs across 4 dimensions: Readability, mAintainability, Correctness, and Efficiency. Specifically, considering the demand-dependent nature of dimensions beyond correctness, we design various types of user requirements for each dimension to assess the model's ability to generate correct code that also meets user demands. We analyze 28 representative LLMs based on RACE and find that: 1) current correctness-centric benchmarks fail to capture the multifaceted requirements of code in real-world scenarios, while RACE provides a comprehensive evaluation that reveals the defects of LLMs across multiple dimensions; 2) the RACE benchmark serves as an effective tool for resisting the risk of data contamination; 3) even the most advanced code LLMs still encounter significant challenges in customized requirements involving complex instructions; 4) most LLMs exhibit an inherent preference for specific coding style. These findings highlight the need for a multidimensional evaluation of code LLMs, emphasizing metrics beyond correctness for real-world applications. Future efforts should aim to develop novel learning algorithms to enhance code generation under varied constraints and improve coverage and usability for diverse user needs.", "title_embedding_index": 7451, "title_abs_embedding_index": 7476}, {"title": "Neuron based Personality Trait Induction in Large Language Models", "link_suffix": "/forum?id=LYHEY783Np", "link": "https://openreview.net/forum?id=LYHEY783Np", "pdf_link": "https://openreview.net/pdf?id=LYHEY783Np", "keywords": "Neuron, Personality, Large Language models", "abstract": "Large language models (LLMs) have become increasingly proficient at simulating various personality traits, an important capability for supporting related applications (e.g., role-playing). To further improve this capacity, in this paper, we present a neuron based approach for personality trait induction in LLMs, with three major technical contributions. First, we construct PERSONALITYBENCH, a large-scale dataset for identifying and evaluating personality traits in LLMs. This dataset is grounded in the Big Five personality traits from psychology and designed to assess the generative capabilities of LLMs towards specific personality traits. Second, by leveraging PERSONALITYBENCH, we propose an efficient method for identifying personality-related neurons within LLMs by examining the opposite aspects of a given trait. Third, we develop a simple yet effective induction method that manipulates the values of these identified personality-related neurons, which enables fine-grained control over the traits exhibited by LLMs without training and modifying model parameters. Extensive experiments validates the efficacy of our neuron identification and trait induction methods. Notably, our approach achieves comparable performance as fine-tuned models, offering a more efficient and flexible solution for personality trait induction in LLMs.", "title_embedding_index": 7452, "title_abs_embedding_index": 7477}, {"title": "Regularized Optimal Transport for Single-Cell Temporal Trajectory Analysis", "link_suffix": "/forum?id=5JXvgNCQUq", "link": "https://openreview.net/forum?id=5JXvgNCQUq", "pdf_link": "https://openreview.net/pdf?id=5JXvgNCQUq", "keywords": "single-cell transcriptomics, temporal trajectory analysis, optimal transport", "abstract": "The temporal relationship between different cellular states and lineages is only partially understood and has major significance for cell differentiation and cancer progression. However, two pain points persist and limit learning-based solutions: ($a$) lack of real datasets and standardized benchmark for early cell developments; ($b$) the complicated transcriptional data fail classic temporal analyses. We integrate $\\texttt{Mouse-RGC}$, a large-scale mouse retinal ganglion cell dataset with annotations for $9$ time stages and $30,000$ gene expressions. Existing approaches show a limited generalization of our datasets. To tackle the modeling bottleneck, we then translate this fundamental biology problem into a machine learning formulation, $\\textit{i.e.}$, $\\textit{temporal trajectory analysis}$. An innovative regularized optimal transport algorithm, $\\texttt{TAROT}$, is proposed to fill in the research gap, consisting of ($1$) customized masked autoencoder to extract high-quality cell representations; ($2$) cost function regularization through biology priors for distribution transports; ($3$) continuous temporal trajectory optimization based on discrete matched time stages. Extensive empirical investigations demonstrate that our framework produces superior cell lineages and pseudotime, compared to existing approaches on $\\texttt{Mouse-RGC}$ and another two public benchmarks. Moreover, $\\texttt{TAROT}$ is capable of identifying biologically meaningful gene sets along with the developmental trajectory, and its simulated gene knockout results echo the findings in physical wet lab validation.", "title_embedding_index": 7453, "title_abs_embedding_index": 7478}, {"title": "A Causal Theoretical Framework for Open Set Domain Adaptation", "link_suffix": "/forum?id=Gp6VU0oJX3", "link": "https://openreview.net/forum?id=Gp6VU0oJX3", "pdf_link": "https://openreview.net/pdf?id=Gp6VU0oJX3", "keywords": "Causal theory, open set domain adaptation, domain adaptation, empirical risk minimization", "abstract": "Open Set Domain Adaptation (OSDA) faces two critical challenges: the emergence\nof unknown classes in the target domain and changes in observed distributions\nacross domains. Although numerous studies have proposed advanced algorithms,\nrecent experimental results demonstrate that the classical Empirical Risk Mini\u0002mization (ERM) approach still delivers state-of-the-art performance. However,\nfew theories can effectively explain this disputed phenomenon. To address the\ntheoretical gap, we focus on constructing a causal theoretical framework for OSDA.\nWe formulate the novel concepts of the Fully Informative Causal Invariance Model\n(FICIM) and the Partially Informative Causal Invariance Model (PICIM). Subse\u0002quently, We derive an OSDA theoretical bound to prove that the ERM performs\nwell when the source domain follows FICIM, while it performs poorly when the\nsource domain follows PICIM. The different results may be attributed to the vary\u0002ing amounts of available information when bounding the target domain\u2019s stable\nexpected risk. Finally, across different datasets, we conduct extensive experiments\non the FICIM and PICIM source domains to validate the effectiveness of our\ntheoretical results.", "title_embedding_index": 7454, "title_abs_embedding_index": 7479}, {"title": "DEALING WITH OUT OF DISTRIBUTION IN PREDICTION PROBLEM", "link_suffix": "/forum?id=i28ZjVxl81", "link": "https://openreview.net/forum?id=i28ZjVxl81", "pdf_link": "https://openreview.net/pdf?id=i28ZjVxl81", "keywords": "representation learning, tabular data, out of distribution", "abstract": "Open world assumption in model development means that a model may not have enough information to effectively handle data that is completely different or out of distribution (OOD). When a model encounters OOD data, it may suffer a significant decrease in performance. Addressing OOD data requires extensive fine-tuning and experimental trials, which in turn require substantial computational resources. Deep learning has been suggested as a solution and has shown significant improvements, but it often requires high-specification hardware, particularly GPUs, which may not always be readily available to general users. Additionally, there is a lack of clear guidance for common users on how to select and evaluate OOD data.\nThis study delves into detection, evaluation, and prediction tasks within the context of OOD on tabular datasets. It demonstrates how common users can identify OOD data from real datasets and provides guidance on evaluating the OOD selection through experiments and visualizations. Furthermore, the study introduces tabular contrast learning (TCL), an enhanced technique specifically designed for tabular prediction tasks. TCL is more efficient compared to other baseline models, making it useful for general machine learning user  with computational limitation on dealing with OOD problems. The study also includes a comprehensive comparison with existing approaches, focusing on both accuracy and computational efficiency.", "title_embedding_index": 7455, "title_abs_embedding_index": 7480}, {"title": "Enhancing Diffusion Posterior Sampling for Inverse Problems by Integrating Crafted Measurements", "link_suffix": "/forum?id=V2x5ZTHMae", "link": "https://openreview.net/forum?id=V2x5ZTHMae", "pdf_link": "https://openreview.net/pdf?id=V2x5ZTHMae", "keywords": "Diffusion models, Inverse problems, Diffusion posterior sampling", "abstract": "Diffusion models have emerged as a powerful foundation model for visual generation. With an appropriate sampling process, it can effectively serve as a generative prior to solve general inverse problems. Current posterior sampling based methods take the measurement (i.e., degraded image sample) into the posterior sampling to infer the distribution of the target data (i.e., clean image sample). However, in this manner, we show that high-frequency information can be prematurely introduced during the early stages, which could induce larger posterior estimate errors during the restoration sampling. To address this issue, we first reveal that forming the log posterior gradient with the noisy measurement ( i.e., samples from a diffusion forward process) instead of the clean one can benefit the reverse process. Consequently, we propose a novel diffusion posterior sampling method DPS-CM, which incorporates a Crafted Measurement (i.e., samples generated by a reverse denoising process, compared to random sampling with noise in standard methods) to form the posterior estimate. This integration aims to mitigate the misalignment with the diffusion prior caused by cumulative posterior estimate errors. Experimental results demonstrate that our approach significantly improves the overall capacity to solve general and noisy inverse problems, such as Gaussian deblurring, super-resolution, inpainting, nonlinear deblurring, and tasks with Poisson noise, relative to existing approaches.", "title_embedding_index": 7456, "title_abs_embedding_index": 7481}, {"title": "Diffusion-based Neural Network Weights Generation", "link_suffix": "/forum?id=j8WHjM9aMm", "link": "https://openreview.net/forum?id=j8WHjM9aMm", "pdf_link": "https://openreview.net/pdf?id=j8WHjM9aMm", "keywords": "generative hyper-representation learning, diffusion model, neural network weights generation, parameters generation, hypernetworks", "abstract": "Transfer learning has gained significant attention in recent deep learning research due to its ability to accelerate convergence and enhance performance on new tasks. However, its success is often contingent on the similarity between source and target data, and training on numerous datasets can be costly, leading to blind selection of pretrained models with limited insight into their effectiveness. To address these challenges, we introduce \\textit{D2NWG}, a diffusion-based neural network weights generation technique that efficiently produces high-performing weights for transfer learning, conditioned on the target dataset. Our method extends generative hyper-representation learning to recast the latent diffusion paradigm for neural network weights generation, learning the weight distributions of models pretrained on various datasets. This allows for automatic generation of weights that generalize well across both seen and unseen tasks, outperforming state-of-the-art meta-learning methods and pretrained models. \nMoreover, our approach is scalable to large architectures such as large language models (LLMs), overcoming the limitations of current parameter generation techniques that rely on task-specific model collections or access to original training data. By modeling the parameter distribution of LLMs, D2NWG enables task-specific parameter generation without requiring additional fine-tuning or large collections of model variants. Extensive experiments show that our method consistently enhances the performance of diverse base models, regardless of their size or complexity, positioning it as a robust solution for scalable transfer learning.", "title_embedding_index": 7457, "title_abs_embedding_index": 7482}, {"title": "Improving Neural Network Accuracy by Concurrently Training with a Twin Network", "link_suffix": "/forum?id=TEmE9PSC65", "link": "https://openreview.net/forum?id=TEmE9PSC65", "pdf_link": "https://openreview.net/pdf?id=TEmE9PSC65", "keywords": "Knowledge Distillation, Representation Learning, Regularization", "abstract": "Recently within Spiking Neural Networks, a method called Twin Network Augmentation (TNA) has been introduced. This technique claims to improve the validation accuracy of a Spiking Neural Network simply by training two networks in conjunction and matching the logits via the Mean Squared Error loss. In this paper, we validate the viability of this method on a wide range of popular Convolutional Neural Network (CNN) benchmarks and compare this approach to existing Knowledge Distillation schemes. Next, we conduct a in-depth study of the different components that make up TNA and determine that its effectiveness is not solely situated in an increase of trainable parameters, but rather the effect of the training methodology. Finally, we analyse the representations learned by networks trained with TNA and highlight their superiority in a number of tasks, thus proving empirically the applicability of Twin Network Augmentation on CNN models.", "title_embedding_index": 7458, "title_abs_embedding_index": 7483}, {"title": "Sylber: Syllabic Embedding Representation of Speech from Raw Audio", "link_suffix": "/forum?id=FyMjfDQ9RO", "link": "https://openreview.net/forum?id=FyMjfDQ9RO", "pdf_link": "https://openreview.net/pdf?id=FyMjfDQ9RO", "keywords": "Self-supervised learning, speech representation, spoken language model, syllable discovery, speech segmentation, speech tokenization", "abstract": "Syllables are compositional units of spoken language that play a crucial role in human speech perception and production. However, current neural speech representations lack structure, resulting in dense token sequences that are costly to process. To bridge this gap, we propose a new model, Sylber, that produces speech representations with clean and robust syllabic structure. Specifically, we propose a self-supervised model that regresses features on syllabic segments distilled from a teacher model which is an exponential moving average of the model in training. This results in a highly structured representation of speech features, offering three key benefits: 1) a fast, linear-time syllable segmentation algorithm, 2) efficient syllabic tokenization with an average of 4.27 tokens per second, and 3) syllabic units better suited for lexical and syntactic understanding. We also train token-to-speech generative models with our syllabic units and show that fully intelligible speech can be reconstructed from these tokens. Lastly, we observe that categorical perception, a linguistic phenomenon of speech perception, emerges naturally in our model, making the embedding space more categorical and sparse than previous self-supervised learning approaches. Together, we present a novel self-supervised approach for representing speech as syllables, with significant potential for efficient speech tokenization and spoken language modeling.", "title_embedding_index": 7459, "title_abs_embedding_index": 7484}, {"title": "How to Verify Any (Reasonable) Distribution Property: Computationally Sound Argument Systems for Distributions", "link_suffix": "/forum?id=GfXMTAJaxZ", "link": "https://openreview.net/forum?id=GfXMTAJaxZ", "pdf_link": "https://openreview.net/pdf?id=GfXMTAJaxZ", "keywords": "property testing, distribution testing, interactive proofs, data science, verification", "abstract": "As statistical analyses become more central to science, industry and society, there is a growing need to ensure correctness of their results. Approximate correctness can be verified by replicating the entire analysis, but can we verify without replication? We focus on distribution testing problems: verifying that an unknown distribution is close to having a  claimed property. Our main contribution is an interactive protocol between a verifier and an untrusted prover, which can be used to verify any distribution property that can be decided in polynomial time given a full and explicit description of the distribution. If the distribution is at statistical distance $\\varepsilon$ from having the property, then the verifier rejects with high probability. This soundness property holds against any polynomial-time  strategy that a cheating prover might follow, assuming the existence of collision-resistant hash functions (a standard assumption in cryptography). For distributions over a domain of size $N$, the protocol consists of $4$ messages and the communication complexity and verifier runtime are roughly $\\widetilde{O}\\left(\\sqrt{N} / \\varepsilon^2 \\right)$. The verifier's sample complexity is $\\widetilde{O}\\left(\\sqrt{N} / \\varepsilon^2 \\right)$, and this is optimal up to $\\text{polylog}(N)$ factors (for any protocol, regardless of its communication complexity). Even for simple properties, approximately deciding whether an unknown distribution has the property can require quasi-linear sample complexity and running time. For any such property, our protocol provides a quadratic speedup over replicating the analysis.", "title_embedding_index": 7460, "title_abs_embedding_index": 7485}, {"title": "Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search", "link_suffix": "/forum?id=8DBTq09LgN", "link": "https://openreview.net/forum?id=8DBTq09LgN", "pdf_link": "https://openreview.net/pdf?id=8DBTq09LgN", "keywords": "Large Language Model, Programmatic Reinforcement Learning", "abstract": "Programmatic reinforcement learning (PRL) has been explored for representing policies through programs as a means to achieve interpretability and generalization. Despite promising outcomes, current state-of-the-art PRL methods are hindered by sample inefficiency, necessitating tens of millions of program-environment interactions. To tackle this challenge, we introduce a novel LLM-guided search framework (LLM-GS). Our key insight is to leverage the programming expertise and common sense reasoning of LLMs to enhance the efficiency of assumption-free, random-guessing search methods. We address the challenge of LLMs' inability to generate precise and grammatically correct programs in domain-specific languages (DSLs) by proposing a Pythonic-DSL strategy \u2014 an LLM is instructed to initially generate Python codes and then convert them into DSL programs. To further optimize the LLM-generated programs, we develop a search algorithm named Scheduled Hill Climbing, designed to efficiently explore the programmatic search space to improve the programs consistently. Experimental results in the Karel domain demonstrate our LLM-GS framework's superior effectiveness and efficiency. Extensive ablation studies further verify the critical role of our Pythonic-DSL strategy and Scheduled Hill Climbing algorithm. Moreover, we conduct experiments with two novel tasks, showing that LLM-GS enables users without programming skills and knowledge of the domain or DSL to describe the tasks in natural language to obtain performant programs.", "title_embedding_index": 7461, "title_abs_embedding_index": 7486}, {"title": "TaKF+: A versatile and parameter-efficient tuning for EEG foundation model", "link_suffix": "/forum?id=2og3oWsC5n", "link": "https://openreview.net/forum?id=2og3oWsC5n", "pdf_link": "https://openreview.net/pdf?id=2og3oWsC5n", "keywords": "EEG, Foundation model, Parameter-efficient fine-tuning, Additive fine-tuning", "abstract": "Electroencephalogram (EEG) data, widely used in brain-computer interfaces (BCIs), pose challenges for reusing deep learning models trained on specific datasets due to variations in recording configurations and domain gaps. While foundation models pre-trained on large-scale EEG datasets have emerged as a promising solution, the challenge of effectively adapting them to downstream tasks has yet to be fully explored. To address this, we propose a novel tuning method, TaKF$^{+}$, which consists of the Task-Adaptive Key-Feature Extractor (TaKF) and adapter modules. TaKF$^{+}$ is designed to efficiently extract task-relevant features from EEG foundation models for downstream tasks while preserving the model\u2019s parameters and significantly reducing computational overhead. We evaluate TaKF$^{+}$ across a diverse range of tasks, including motor imagery, emotion recognition, and seizure detection, and demonstrate its superior performance and adaptability compared to existing methods over publicly available datasets. Our research paves the way for more efficient and versatile applications of EEG foundation models across various domains.", "title_embedding_index": 7462, "title_abs_embedding_index": 7487}, {"title": "NuwaTS: a Foundation Model Mending Every Incomplete Time Series", "link_suffix": "/forum?id=jC6E2iTgfr", "link": "https://openreview.net/forum?id=jC6E2iTgfr", "pdf_link": "https://openreview.net/pdf?id=jC6E2iTgfr", "keywords": "Incomplete Time Series, Missing Data Imputation, Cross-domain adaptation", "abstract": "Time series imputation is critical for many real-world applications and has been widely studied. However, existing models often require specialized designs tailored to specific missing patterns, variables, or domains which limits their generalizability. In addition, current evaluation frameworks primarily focus on domain-specific tasks and often rely on time-wise train/validation/test data splits, which fail to rigorously assess a model\u2019s ability to generalize across unseen variables or domains. In this paper, we present \\textbf{NuwaTS}, a novel framework that repurposes Pre-trained Language Models (PLMs) for general time series imputation. Once trained, NuwaTS can be applied to impute missing data across any domain. We introduce specialized embeddings for each sub-series patch, capturing information about the patch, its missing data patterns, and its statistical characteristics. By combining contrastive learning with the imputation task, we train PLMs to create a versatile, one-for-all imputation model. Additionally, we employ a plug-and-play fine-tuning approach, enabling efficient adaptation to domain-specific tasks with minimal adjustments. To evaluate cross-variable and cross-domain generalization, we propose a new benchmarking protocol that partitions the datasets along the variable dimension. Experimental results on over seventeen million time series from diverse domains demonstrate that NuwaTS outperforms state-of-the-art domain-specific models across various datasets under the proposed benchmarking protocol. Furthermore, we show that NuwaTS generalizes to other time series tasks, such as forecasting.", "title_embedding_index": 7463, "title_abs_embedding_index": 7488}, {"title": "Lean-STaR: Learning to Interleave Thinking and Proving", "link_suffix": "/forum?id=SOWZ59UyNc", "link": "https://openreview.net/forum?id=SOWZ59UyNc", "pdf_link": "https://openreview.net/pdf?id=SOWZ59UyNc", "keywords": "Automated Theorem Proving, AI for Math, Chain-of-Thought Reasoning", "abstract": "Traditional language model-based theorem proving assumes that by training on a sufficient amount of formal proof data, a model will learn to prove theorems. Our key observation is that a wealth of informal information that is not present in formal proofs can be useful for learning to prove theorems. For instance, humans think through steps of a proof, but this thought process is not visible in the resulting code. We present Lean-STaR, a framework for training language models to produce informal thoughts prior to each step of a proof, thereby boosting the model's theorem-proving capabilities. Lean-STaR uses retrospective ground-truth tactics to generate synthetic thoughts for training the language model. At inference time, the trained model directly generates the thoughts prior to the prediction of the tactics in each proof step. Building on the self-taught reasoner framework, we then apply expert iteration to further fine-tune the model on the correct proofs it samples and verifies using the Lean solver. Lean-STaR significantly outperform base models (43.4% \u2192 46.3%, Pass@64). We also analyze the impact of the augmented thoughts on various aspects of the theorem proving process, providing insights into their effectiveness.", "title_embedding_index": 7464, "title_abs_embedding_index": 7489}, {"title": "TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis", "link_suffix": "/forum?id=1CLzLXSFNn", "link": "https://openreview.net/forum?id=1CLzLXSFNn", "pdf_link": "https://openreview.net/pdf?id=1CLzLXSFNn", "keywords": "time series, pattern machine, predictive analysis", "abstract": "Time series analysis plays a critical role in numerous applications, supporting tasks such as forecasting, classification, anomaly detection, and imputation. In this work, we present the time series pattern machine (TSPM), a model designed to excel in a broad range of time series tasks through powerful representation and pattern extraction capabilities. Traditional time series models often struggle to capture universal patterns, limiting their effectiveness across diverse tasks. To address this, we define multiple scales in the time domain and various resolutions in the frequency domain, employing various mixing strategies to extract intricate, task-adaptive time series patterns. Specifically, we introduce \\method, a general-purpose TSPM that processes multi-scale time series using (1) multi-resolution time imaging (MRTI), (2) time image decomposition (TID), (3) multi-scale mixing (MCM), and (4) multi-resolution mixing (MRM) to extract comprehensive temporal patterns. MRTI transforms multi-scale time series into multi-resolution time images, capturing patterns across both temporal and frequency domains. TID leverages dual-axis attention to extract seasonal and trend patterns, while MCM hierarchically aggregates these patterns across scales. MRM adaptively integrates all representations across resolutions. TimeMixer++ achieves state-of-the-art performance across 8 time series analytical tasks, consistently surpassing both general-purpose and task-specific models. Our work marks a promising step toward the next generation of TSPMs, paving the way for further advancements in time series analysis.", "title_embedding_index": 7465, "title_abs_embedding_index": 7490}, {"title": "LLaPA: Harnessing Language Models for Protein Enzyme Function", "link_suffix": "/forum?id=AK9uRqzLjt", "link": "https://openreview.net/forum?id=AK9uRqzLjt", "pdf_link": "https://openreview.net/pdf?id=AK9uRqzLjt", "keywords": "Protein Enzyme Funtion; Large Language Model; Retrieval Augmented Generation", "abstract": "Identifying protein enzyme functions, crucial for numerous applications, is challenging due to the rapid growth in protein sequences. Current methods either struggle with false positives or fail to generalize to lesser-known proteins and those with uncharacterized functions. To tackle these challenges, we propose $\\texttt{LLaPA}$: a Protein-centric $\\underline{L}$arge $\\underline{L}$anguage and $\\underline{P}$rotein $\\underline{A}$ssistant for Enzyme Commission (EC) number prediction. $\\texttt{LLaPA}$ uses a large multi-modal model to accurately predict EC numbers by reformulating the EC number format within the LLM self-regression framework. We introduce a dual-level protein-centric retrieval: the $\\textit{protein-level}$ retrieves protein sequences with similar regions, and the $\\textit{chemical-level}$ retrieves corresponding molecules with relevant reaction information. By inputting the original protein along with the retrieved protein and molecule into the LLM, $\\texttt{LLaPA}$ achieves improved prediction accuracy, with enhanced generalizability to lesser-known proteins. Evaluations on three public benchmarks show accuracy improvements of $\\textbf{17.03\\%}$, $\\textbf{9.32\\%}$, and $\\textbf{38.64\\%}$. These results highlight $\\texttt{LLaPA}$'s ability to generalize to novel protein sequences and functionalities. Codes are provided in the supplement.", "title_embedding_index": 7466, "title_abs_embedding_index": 7491}, {"title": "Searching Strengthens Large Language Models in Finding Bugs of Deep Learning Libraries", "link_suffix": "/forum?id=JwNQP2dNhD", "link": "https://openreview.net/forum?id=JwNQP2dNhD", "pdf_link": "https://openreview.net/pdf?id=JwNQP2dNhD", "keywords": "Large Language Model, Code Generation, Fuzz, Software Engineering, Optimize, Search", "abstract": "Ensuring the quality of deep learning libraries is crucial, as bugs can have significant consequences for downstream software. Fuzzing, a powerful testing method, generates random programs to test software. Generally, effective fuzzing requires generated programs to meet three key criteria: rarity, validity, and variety, among which rarity is most critical for bug detection, as it determines the algorithm's ability to detect bugs. However, current large language model (LLM) based fuzzing approaches struggle to effectively explore the program generation space which results in insufficient rarity and the lack of post-processing leads to a large number of invalid programs and inadequate validity. This paper proposes EvAFuzz, a novel approach that combines Evolutionary Algorithms with LLMs to Fuzz DL libraries. For rarity, EvAFuzz uses a search algorithm to guide LLMs in efficiently exploring the program generation space, iteratively generating increasingly rare programs. For validity, EvAFuzz incorporates a feedback scheme, enabling LLMs to correct invalid programs and achieve high validity. For variety, EvAFuzz constructs a large parent selection space, enriching the diversity of selected parents, and thereby enhancing the variety of generated programs. Our experiments show that EvAFuzz outperforms the previous state-of-the-art (SOTA) in several key metrics. First, in the same version of PyTorch, EvAFuzz detects nine unique crashes, surpassing the SOTA's seven. Next, our method achieves a valid rate of 38.80%, significantly higher than the SOTA's 27.69%. Last, EvAFuzz achieves API coverage rates of 99.49% on PyTorch and 85.76% on TensorFlow, outperforming the SOTA's rates of 86.44% on PyTorch and 69.63% on TensorFlow. These results indicate that our method generates programs with higher rarity, validity, and variety, respectively.", "title_embedding_index": 7467, "title_abs_embedding_index": 7492}, {"title": "DynFrs: An Efficient Framework for Machine Unlearning in Random Forest", "link_suffix": "/forum?id=nsCOeCLR8e", "link": "https://openreview.net/forum?id=nsCOeCLR8e", "pdf_link": "https://openreview.net/pdf?id=nsCOeCLR8e", "keywords": "Machine Unlearning, Random Forest", "abstract": "Random Forests are widely recognized for establishing efficacy in classification and regression tasks, standing out in various domains such as medical diagnosis, finance, and personalized recommendations. These domains, however, are inherently sensitive to privacy concerns, as personal and confidential data are involved. With increasing demand for the right to be forgotten, particularly under regulations such as GDPR and CCPA, the ability to perform machine unlearning has become crucial for Random Forests. However, insufficient attention was paid to this topic, and existing approaches face difficulties in being applied to real-world scenarios. Addressing this gap, we propose the DynFrs framework designed to enable efficient machine unlearning in Random Forests while preserving predictive accuracy. Dynfrs leverages subsampling method Occ(q) and a lazy tag strategy Lzy, and is still adaptable to any Random Forest variant. In essence, Occ(q) ensures that each sample in the training set occurs only in a proportion of trees so that the impact of deleting samples is limited, and Lzy delays the reconstruction of a tree node until necessary, thereby avoiding unnecessary modifications on tree structures. In experiments, applying Dynfrs on Extremely Randomized Trees yields substantial improvements, achieving orders of magnitude faster unlearning performance and better predictive accuracy than existing machine unlearning methods for Random Forests.", "title_embedding_index": 7468, "title_abs_embedding_index": 7493}, {"title": "SoftSignSGD(S3): An Enhanced Optimize for Practical DNN Training and Loss Spikes Minimization Beyond Adam", "link_suffix": "/forum?id=TBJCtWTvXJ", "link": "https://openreview.net/forum?id=TBJCtWTvXJ", "pdf_link": "https://openreview.net/pdf?id=TBJCtWTvXJ", "keywords": "Optimizer, Adam, Loss Spikes", "abstract": "Adam  has been widely successful in training deep neural networks (DNNs), yet the factors contributing to its practical effectiveness and ineffectiveness remain largely underexplored. In this study, we  reveal that the effectiveness of Adam in training complicated DNNs stems primarily from its similarity to SignSGD in managing significant gradient variations. Moreover, we uncover that  Adam is prone to encountering loss spikes  due to potential excessively large updates. Leveraging these insights, we propose a novel optimizer,  SignSoftSGD (S3), which incorporates a more generalized sign-like formulation with a flexible $p$-th order ($p\\ge 1$) momentum in the denominator of the update, rather than being limited to a fixed $2$-order momentum like Adam. We also integrate the a memory-efficient Nesterov's accelerated gradient technique into  S3, further speeding up its convergence at no cost of extra memory. To minimizing the risk of loss spikes, we utilize the same coefficient for the momentums in both the numerator and the denominator of the update, which also practically streamlines the tuning overload  We conduct a theoretical analysis for S3 on a general nonconvex stochastic problem, demonstrating that  S3 can achieve the optimal convergence rate under weak assumptions. Extensive experimentation across various vision and language tasks demonstrates that  S3 not only achieves rapid training convergence and improved inference performance but also mitigates training instability and rarely encounters loss spikes, even with a ${10\\times}$ larger learning rate. Specifically,  S3 showcases comparable or superior performance to AdamW with ${2\\times}$ the number of training steps.", "title_embedding_index": 7469, "title_abs_embedding_index": 7494}, {"title": "Challenging the Counterintuitive: Revisiting Simple Likelihood Tests with Normalizing Flows for Tabular Data Anomaly Detection", "link_suffix": "/forum?id=CX0Z5c0LbN", "link": "https://openreview.net/forum?id=CX0Z5c0LbN", "pdf_link": "https://openreview.net/pdf?id=CX0Z5c0LbN", "keywords": "anomaly detection, tabular data, self-supervised learning, generative model", "abstract": "In this study, we propose a novel approach to anomaly detection in the tabular domain using normalizing flows, leveraging a simple likelihood test to achieve state-of-the-art performance in unsupervised learning. Although simple likelihood tests have been shown to fail in anomaly detection for image data, we redefine the counterintuitive phenomenon and demonstrate, both theoretically and empirically, why this method succeeds in the tabular domain. Our approach outperforms traditional anomaly detection methods by offering more consistent results. Furthermore, we question the practice of fine-tuning parameters for each dataset individually, ensuring fair and unbiased comparisons by adopting uniform hyperparameters across all datasets. Through extensive experimentation, we validate the robustness and scalability of our method, highlighting its practical effectiveness in real-world settings.", "title_embedding_index": 7470, "title_abs_embedding_index": 7495}, {"title": "GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling", "link_suffix": "/forum?id=1p6xFLBU4J", "link": "https://openreview.net/forum?id=1p6xFLBU4J", "pdf_link": "https://openreview.net/pdf?id=1p6xFLBU4J", "keywords": "speech enhancement, language model, semantic information", "abstract": "Semantic information refers to the meaning conveyed through words, phrases, and contextual relationships within a given linguistic structure. Humans can leverage semantic information, such as familiar linguistic patterns and contextual cues, to reconstruct incomplete or masked speech signals in noisy environments. However, existing speech enhancement (SE) approaches often overlook the rich semantic information embedded in speech, which is crucial for improving intelligibility, speaker consistency, and overall quality of enhanced speech signals. To enrich the SE model with semantic information, we employ language models as an efficient semantic learner and propose a comprehensive framework tailored for language model-based speech enhancement, called GenSE. Specifically, we approach SE as a conditional language modeling task rather than a continuous signal regression problem defined in existing works. This is achieved by tokenizing speech signals into semantic tokens using a pre-trained self-supervised model and into acoustic tokens using a custom-designed single-quantizer neural codec model. To improve the stability of language model predictions, we propose a hierarchical modeling method that decouples the generation of clean semantic tokens and clean acoustic tokens into two distinct stages. Moreover, we introduce a token chain prompting mechanism during the acoustic token generation stage to ensure timbre consistency throughout the speech enhancement process. Experimental results on benchmark datasets demonstrate that our proposed approach outperforms state-of-the-art SE systems in terms of speech quality and generalization capability. Codes and demos are publicly available athttps://anonymous.4open.science/w/gen-se-7F52/.", "title_embedding_index": 7471, "title_abs_embedding_index": 7496}, {"title": "Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization", "link_suffix": "/forum?id=vTRWu9zaWo", "link": "https://openreview.net/forum?id=vTRWu9zaWo", "pdf_link": "https://openreview.net/pdf?id=vTRWu9zaWo", "keywords": "deep learning theory, degree of smoothing, generalizability, graduated optimization, SGD, sharpness, smoothing property, stochastic noise", "abstract": "The graduated optimization approach is a heuristic method for finding global optimal solutions for nonconvex functions by using a function smoothing operation with stochastic noise. We show that stochastic noise in stochastic gradient descent (SGD) has the effect of smoothing the objective function, the degree of which is determined by the learning rate, batch size, and variance of the stochastic gradient. Using this finding, we propose and analyze a new graduated optimization algorithm that varies the degree of smoothing by varying the learning rate and batch size, and provide experimental results on image classification tasks with ResNets that support our theoretical findings. We further show that there is an interesting correlation between the degree of smoothing by SGD's stochastic noise, the well-studied ``sharpness'' indicator, and the generalization performance of the model.", "title_embedding_index": 7472, "title_abs_embedding_index": 7497}, {"title": "Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models", "link_suffix": "/forum?id=sYNWqQYJhz", "link": "https://openreview.net/forum?id=sYNWqQYJhz", "pdf_link": "https://openreview.net/pdf?id=sYNWqQYJhz", "keywords": "Federated Learning, Large Language Models, Safety Alignment, Attack, Defense", "abstract": "Federated learning (FL) enables multiple parties to collaboratively fine-tune an large language model (LLM) without the need of direct data sharing. Ideally, by training on decentralized data that is aligned with human preferences and safety principles, federated instruction tuning can result in an LLM that could behave in a helpful and safe manner. In this paper, we for the first time reveal the vulnerability of safety alignment in FedIT by proposing a simple, stealthy, yet effective safety attack method. Specifically, the malicious clients could automatically generate attack data without involving manual efforts and attack the FedIT system by training their local LLMs on such attack data. Unfortunately, this proposed safety attack not only can compromise the safety alignment of LLM trained via FedIT, but also can not be effectively defended against by many existing FL defense methods. Targeting this, we further propose a post-hoc defense method, which could rely on an fully automated pipeline: generation of defense data and further fine-tuning of the LLM. Extensive experiments show that our safety attack method can significantly compromise the LLM's safety alignment (e.g., reduce safety rate by 70%), which can not be effectively defended by existing defense methods (at most 4% absolute improvement), while our safety defense method can significantly enhance the attacked LLM's safety alignment (at most 69% absolute improvement).", "title_embedding_index": 7473, "title_abs_embedding_index": 7498}, {"title": "FEDNET: FREQUENCY ENHANCED DECOMPOSED NETWORK FOR OUT-OF-DISTRIBUTION TIME SERIES CLASSIFICATION", "link_suffix": "/forum?id=OVu9DsOjgH", "link": "https://openreview.net/forum?id=OVu9DsOjgH", "pdf_link": "https://openreview.net/pdf?id=OVu9DsOjgH", "keywords": "out-of-distribution, time series classification, frequency", "abstract": "Time series classification is a crucial task with widespread applications in vari\u0002ous fields such as medicine and energy. Due to the non-stationary property of time series, its data distribution will change over time, which makes it challenging for models to generalize to the out-of-distribution (OOD) environment. How\u0002ever, limitations persist in the current research on OOD time series classification, particularly the absence of a unified consideration addressing both domain dis\u0002tribution shift and temporal distribution shift. To this end, we view the time series distribution shift from the frequency perspective and propose a novel method called Frequency Enhanced Decomposed Network (FEDNet) for OOD time series classification. FEDNet utilizes frequency domain information to guide the decomposition of time series and further eliminates domain shift and temporal shift, it then obtains domain-invariant features for adapting to OOD data. Finally,we provide theoretical insights of FEDNet to validate its superiority for OOD time series classification. Comprehensive results on synthetic and real-world datasets demonstrate that FEDNet achieves state-of-the-art performance in OOD time series classification tasks, surpassing previous methods by up to 7%.", "title_embedding_index": 7474, "title_abs_embedding_index": 7499}]