[{"title": "Neural Wave Equation for Irregularly Sampled Sequence Data", "link_suffix": "/forum?id=kbeX97jExm", "link": "https://openreview.net/forum?id=kbeX97jExm", "pdf_link": "https://openreview.net/pdf?id=kbeX97jExm", "keywords": "Wave Equation, Neural ODE, Sequence Labelling", "abstract": "Sequence labeling problems arise in several real-world applications such as healthcare and robotics. In many such applications, sequence data are irregularly sampled and are of varying complexities. Recently, efforts have been made to develop neural ODE-based architectures to model the evolution of hidden states continuously in time, to address irregularly sampled sequence data. However, they assume a fixed architectural depth and limit their flexibility to adapt to data sets with varying complexities. We propose the neural wave equation, a novel deep learning method inspired by the wave equation, to address this through continuous modeling of depth. Neural Wave Equation models the evolution of hidden states continuously across time as well as depth by using a non-homogeneous wave equation parameterized by a neural network.  Through d'Alembert's analytical solution of the wave equation, we also show that the neural wave equation provides denser connections across the hidden states, allowing for better modeling capability.  We conduct experiments on several sequence labeling problems involving irregularly sampled sequence data and demonstrate the superior performance of the proposed neural wave equation model.", "title_embedding_index": 250, "title_abs_embedding_index": 275}, {"title": "BBOPlace-Bench: Benchmarking Black-Box Optimization for Chip Placement", "link_suffix": "/forum?id=izETL3emSv", "link": "https://openreview.net/forum?id=izETL3emSv", "pdf_link": "https://openreview.net/pdf?id=izETL3emSv", "keywords": "Black-box optimization, Bayesian optimization, Evolutionary algorithm, Chip placement, EDA", "abstract": "Chip placement is a crucial step in modern chip design, because it significantly impacts the subsequent process and the overall quality of the final chip. The application of black-box optimization (BBO) for chip placement has a history of several decades. Nevertheless, early attempts were hampered by immature problem modeling and inefficient algorithm design, resulting in suboptimal placement efficiency and quality compared to the more prevalent analytical methods. Recent advancements in problem modeling and BBO algorithm design have highlighted the effectiveness and efficiency of BBO, demonstrating its potential to achieve state-of-the-art results in chip placement. Despite these advancements, the field lacks a unified benchmark for thoroughly assessing various problem models and BBO algorithms. To address this gap, we propose BBOPlace-Bench, the first benchmark designed for evaluating and developing BBO algorithms specifically for chip placement tasks. BBOPlace-Bench first collects several popular tasks and standardizing their formats, thereby providing uniform and comprehensive information for optimization.  Additionally, BBOPlace-Bench includes a wide range of existing BBO algorithms, including simulated annealing, evolutionary algorithms, evolution strategy, and Bayesian optimization, and evaluates their performance across different problem modelings (i.e., permutation, discrete, and mixed search spaces) using various metrics. Furthermore, BBOPlace-Bench offers a flexible framework that allows users to easily implement and test their unique algorithms. BBOPlace-Bench not only provides efficient solutions for chip placement but also expands the practical application scenarios for various BBO algorithms. The code for BBOPlace-Bench is available in the supplementary file.", "title_embedding_index": 251, "title_abs_embedding_index": 276}, {"title": "ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization", "link_suffix": "/forum?id=5o9JJJPPm6", "link": "https://openreview.net/forum?id=5o9JJJPPm6", "pdf_link": "https://openreview.net/pdf?id=5o9JJJPPm6", "keywords": "Offline Reinforcement Learning, Multi-Agent Reinforcement Learning, Stationary Distribution Correction Estimation", "abstract": "Offline reinforcement learning (RL) has garnered significant attention for its ability to learn effective policies from pre-collected datasets without the need for further environmental interactions. While promising results have been demonstrated in single-agent settings, offline multi-agent reinforcement learning (MARL) presents additional challenges due to the large joint state-action space and the complexity of multi-agent behaviors. A key issue in offline RL is the distributional shift, which arises when the target policy being optimized deviates from the behavior policy that generated the data. This problem is exacerbated in MARL due to the interdependence between agents' local policies and the expansive joint state-action space. Prior approaches have primarily addressed this challenge by incorporating regularization in the space of either Q-functions or policies. In this work, we propose a novel type of regularizer in the space of stationary distributions to address the distributional shift more effectively. Our algorithm, ComaDICE, provides a principled framework for offline cooperative MARL to correct the stationary distribution of the global policy, which is then leveraged to derive local policies for individual agents. Through extensive experiments on the offline multi-agent MuJoCo and StarCraft II benchmarks, we demonstrate that ComaDICE achieves superior performance compared to state-of-the-art offline MARL methods across nearly all tasks.", "title_embedding_index": 252, "title_abs_embedding_index": 277}, {"title": "The Advancement in Stochastic Zeroth-Order Optimization: Mechanism of Accelerated Convergence of Gaussian Direction on Objectives with Skewed Hessian Eigenvalues", "link_suffix": "/forum?id=Bdhro9gxuF", "link": "https://openreview.net/forum?id=Bdhro9gxuF", "pdf_link": "https://openreview.net/pdf?id=Bdhro9gxuF", "keywords": "stochastic zeroth-order optimization, quadratic regularity, gaussian direction, skewed Hessian eigenvalues", "abstract": "This paper primarily investigates large-scale finite-sum optimization problems, which are particularly prevalent in the big data era. \nIn the field of zeroth-order optimization, stochastic optimization methods have become essential tools. \nNatural zeroth-order stochastic optimization methods are primarily based on stochastic gradient descent ($\\texttt{SGD}$).\nThe method of preprocessing the stochastic gradient with Gaussian vector is referred to as $\\texttt{ZO-SGD-Gauss}$ ($\\texttt{ZSG}$), while estimating partial derivatives along coordinate directions to compute the stochastic gradient is known as $\\texttt{ZO-SGD-Coordinate}$ ($\\texttt{ZSC}$).\nCompared to $\\texttt{ZSC}$, $\\texttt{ZSG}$ often demonstrates superior performance in practice.\nHowever, the underlying mechanisms behind this phenomenon remain unclear in the academic community.\nTo the best of our knowledge, our work is the first to theoretically analyze the potential advantages of $\\texttt{ZSG}$ compared to $\\texttt{ZSC}$.\nUnlike the fundamental assumptions applied in general stochastic optimization analyses, the quadratic regularity assumption is proposed to generalize the smoothness and strong convexity to the Hessian matrix. \nThis assumption allows us to incorporate Hessian information into the complexity analysis.\nWhen the objective function is quadratic, the quadratic regularity assumption reduces to the second-order Taylor expansion of the function, and we focus on analyzing and proving the significant improvement of $\\texttt{ZSG}$. \nFor other objective function classes, we also demonstrate the convergence of $\\texttt{ZSG}$ and its potentially better query complexity than that of $\\texttt{ZSC}$. \nFinally, experimental results on both synthetic and real-world datasets substantiate the effectiveness of our theoretical analysis.", "title_embedding_index": 253, "title_abs_embedding_index": 278}, {"title": "Imagine to Ensure Safety in Hierarchical Reinforcement Learning", "link_suffix": "/forum?id=TdIx7u2ECv", "link": "https://openreview.net/forum?id=TdIx7u2ECv", "pdf_link": "https://openreview.net/pdf?id=TdIx7u2ECv", "keywords": "safe reinforcement learning, machine learning, model based reinforcement learning, hierarchical reinforcement learning", "abstract": "This work investigates the safety exploration problem, where an agent must maximize performance while satisfying safety constraints. To address this problem, we propose a method that includes a learnable world model and two policies, a high-level policy and a low-level policy, that ensure safety at both levels. The high-level policy generates safe subgoals for the low-level policy, which progressively guide the agent towards the final goal. Through trajectory imagination, the low-level policy learns to safely reach these subgoals. The proposed method was evaluated on the standard benchmark, SafetyGym, and demonstrated superior performance quality while maintaining comparable safety violations compared to state-of-the-art approaches. In addition, we investigated an alternative implementation of safety in hierarchical reinforcement learning (HRL) algorithms using Lagrange multipliers, and demonstrated in the custom long-horizon environment SafeAntMaze that our approach achieves comparable performance while more effectively satisfying safety constraints, while the flat safe policy fails to accomplish this task.", "title_embedding_index": 254, "title_abs_embedding_index": 279}, {"title": "Probabilistic Conformal Prediction with Approximate Conditional Validity", "link_suffix": "/forum?id=Nfd7z9d6Bb", "link": "https://openreview.net/forum?id=Nfd7z9d6Bb", "pdf_link": "https://openreview.net/pdf?id=Nfd7z9d6Bb", "keywords": "Conformal Prediction, Conditional coverage, Probabilistic method, Uncertainty Quantification", "abstract": "We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $\\textup{P}_{Y \\mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide only a marginal coverage guarantee. In contrast, our approach extends these frameworks to achieve approximately conditional coverage, which is crucial for many practical applications. Our prediction sets adapt to the behavior of the predictive distribution, making them effective even under high heteroscedasticity. While exact conditional guarantees are infeasible without assumptions on the underlying data distribution, we derive non-asymptotic bounds that depend on the total variation distance of the conditional distribution and its estimate. Using extensive simulations, we show that our method consistently outperforms existing approaches in terms of conditional coverage, leading to more reliable statistical inference in a variety of applications.", "title_embedding_index": 255, "title_abs_embedding_index": 280}, {"title": "Path Selection Makes BERT-family Good Generators", "link_suffix": "/forum?id=7jDv1RrNQX", "link": "https://openreview.net/forum?id=7jDv1RrNQX", "pdf_link": "https://openreview.net/pdf?id=7jDv1RrNQX", "keywords": "BERT-family, path selection, natural language generation", "abstract": "The Mask-Predict decoding algorithm has been widely used to enhance the generation capacity of traditional non-autoregressive (NAR) models and provide a good recipe for adapting the pre-trained BERT-like masked language models (MLMs) to NAR generation scenarios.\nHowever, these models, which we denote as NAR-MLMs, are still regarded as inferior to competitive autoregressive (AR) models in terms of performance.\nIn this paper, we further explore the core problems leading to the performance gap of NAR-MLMs and delve into effective solutions for technological innovation.\nSpecifically, most related works neglect the impact of the training sequence decomposition format, i.e., \nUnlike the AR models which can naturally decompose the text sequence in a left-to-right manner for training and inference, NAR-MLMs are trained with a random decomposition but aim to find a determined optimal composition (denoted as decoding paths) during inference.\nTo alleviate this mismatching, we propose decoding path selection to increase the search space for finding a better \ncomposition, and path optimization methods to enable the model decoding path preference during the training process. \nResults on various zero-shot common sense reasoning and reading comprehension tasks and several task-specific generation tasks demonstrate that our NAR-MLM achieves significant performance improvements on common benchmarks with the methods mentioned above, reaching performance levels comparable to even outperforming AR pre-trained models. Our model and code will be available at Github.", "title_embedding_index": 256, "title_abs_embedding_index": 281}, {"title": "Improving CNN training by Riemannian optimization on the generalized Stiefel manifold combined with a gradient-based manifold search", "link_suffix": "/forum?id=6w9qffvXkq", "link": "https://openreview.net/forum?id=6w9qffvXkq", "pdf_link": "https://openreview.net/pdf?id=6w9qffvXkq", "keywords": "Riemannian optimization, Convolutional neural networks, gradient-based optimization, deep neural networks, generalized Stiefel manifold", "abstract": "Enforcing orthonormality constraints in deep learning has been shown to provide significant benefits. Although hard restrictions can be applied by constraining parameter matrices to the Stiefel manifold, this approach limits the solution space to that specific manifold. We show that a generalized Stiefel constraint $X^TSX=\\mathbb{I}$ for Riemannian optimization can lead to even faster convergence than in previous work on CNNs, which enforced orthonormality. The gained flexibility comes from a larger search space. In this paper, we therefore propose a novel approach that retains the advantages of compact restrictions while using a gradient-based formulation to adapt the solution space defined by $S$. This approach results in overall faster convergence rates and improved test performance across CIFAR10, CIFAR100, SVHN, and Tiny ImageNet32 datasets on GPU hardware.", "title_embedding_index": 257, "title_abs_embedding_index": 282}, {"title": "Physics-Informed Neural Networks with Message-Passing Weights", "link_suffix": "/forum?id=GF1sRSBiwY", "link": "https://openreview.net/forum?id=GF1sRSBiwY", "pdf_link": "https://openreview.net/pdf?id=GF1sRSBiwY", "keywords": "Physics-informed Neural Networks, Adaptive Loss-Balancing Algorithms, causal PINN, Belief Propagation", "abstract": "Adaptive loss balancing algorithms play a crucial role in improving the performance of Physics-Informed Neural Networks (PINNs) by effectively managing the weights assigned to different loss components. Most notably,  Wang et al. (2022) introduced Causal Physics-Informed Neural Networks (Causal PINNs), which achieve superior performance by simply reformulating the loss function based on the causal structure that emerges from time dependency. However, despite their empirical success, a solid theoretical analysis for the effectiveness of Causal PINNs has not received adequate attention. This paper addresses this gap by providing a theoretical rationale for Causal PINNs through the Belief Propagation (BP) algorithm, which is commonly used for causal inference. In addition, motivated by this analysis, we propose a Message Passing PINNs (MP-PINNs), a novel adaptive weighting algorithm. Through extensive numerical experiments, we demonstrate that the proposed MP-PINNs significantly outperform existing adaptive weighting methods, exhibiting superior performance in solving complex PDEs. Our findings highlight the potential of MP-PINNs as a powerful tool to enhance both the accuracy \nand efficiency of PINNs.", "title_embedding_index": 258, "title_abs_embedding_index": 283}, {"title": "Prompt-Independent Safe Decoding to Restrain Unsafe Image Generation for Text-to-Image Models against White-Box Adversary", "link_suffix": "/forum?id=bzB7OIbITu", "link": "https://openreview.net/forum?id=bzB7OIbITu", "pdf_link": "https://openreview.net/pdf?id=bzB7OIbITu", "keywords": "Text-to-image generation, AI security, Model compliance", "abstract": "Text-to-image (T2I) models, developed through extensive training, are capable of generating realistic images from textual inputs, profoundly influencing various facets of our lives. Nevertheless, they can be exploited by adversaries who input malicious prompts, leading to the creation of unsafe content and posing serious ethical concerns. Current defense mechanisms primarily rely on external moderation or model modification, but they are inherently fragile against white-box adversaries who have access to the model's weights and can adjust them accordingly.To address this issue, we propose \\sys, a novel defense framework that governs both the diffusion and the decoder module of the text-to-image pipeline, enabling them to reject generating unsafe content and resist malicious fine-tuning attempts. Concretely, we first fine-tune the diffusion and the decoder module with the denial-of-service samples: 1) for the diffusion module, the inputs are unsafe image-caption pairs, the ground truth is zero predicted noise, and 2) for the decoder module, the inputs are unsafe generations from the diffusion, the ground truth is zero decoding. Then, we employ adversarial training to ensure this denial-of-service behavior for unsafe queries remains effective even after the adversary's fine-tuning with unsafe data. Specifically, we continuously simulate potential fine-tuning processes that the adversary might adopt and expose them to the model, enabling it to learn how to resist.Extensive experiments validate that \\sys effectively prevents the generation of unsafe content without compromising the model\u2019s normal performance. Furthermore, our method demonstrates robust resistance to malicious fine-tuning by white-box adversaries, rendering it resource-intensive to corrupt our protected model, thus significantly deterring the misuse of our model for nefarious purposes.", "title_embedding_index": 259, "title_abs_embedding_index": 284}, {"title": "Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding", "link_suffix": "/forum?id=GM7cmQfk2F", "link": "https://openreview.net/forum?id=GM7cmQfk2F", "pdf_link": "https://openreview.net/pdf?id=GM7cmQfk2F", "keywords": "Neural Multi-Objective Combinatorial Optimization, Weight Embedding, Conditional Attention", "abstract": "Recent decomposition-based neural multi-objective combinatorial optimization (MOCO) methods struggle to achieve desirable performance. Even equipped with complex learning techniques, they often suffer from significant optimality gaps in weight-specific subproblems. To address this challenge, we propose a neat weight embedding method to learn weight-specific representations, which captures weight-instance interaction for the subproblems and was overlooked by most current methods. We demonstrate the potentials of our method in two instantiations. First, we introduce a succinct addition model to learn weight-specific node embeddings, which surpassed most existing neural methods. Second, we design an enhanced conditional attention model to simultaneously learn the weight embedding and node embeddings, which yielded new state-of-the-art performance. Experimental results on classic MOCO problems verified the superiority of our method. Remarkably, our method also exhibits favorable generalization performance across problem sizes, even outperforming the neural method specialized for boosting size generalization.", "title_embedding_index": 260, "title_abs_embedding_index": 285}, {"title": "AutoKaggle: A Multi-Agent Framework for Autonomous Data Science Competitions", "link_suffix": "/forum?id=09LEjbLcZW", "link": "https://openreview.net/forum?id=09LEjbLcZW", "pdf_link": "https://openreview.net/pdf?id=09LEjbLcZW", "keywords": "large language models, language agents, multi-agent", "abstract": "Data science competitions on Kaggle, which represent real-world programming challenges, require sophisticated problem-solving approaches. While LLM-based agents demonstrate potential in various fields, their application to data science tasks often falls short due to difficulties in adapting to data changes in multi-stage reasoning and the need for precise reasoning. To address this, we propose AutoKaggle, a robust and user-centric framework that solves Kaggle problems through a collaborative multi-agent cooperative system. AutoKaggle implements an iterative development process that combines code interpretation, debugging, and comprehensive unit testing covering over 30 tests, ensuring code correctness and quality through LLM-based evaluation. It prioritizes user experience by generating detailed reports that elucidate feature engineering processes, data transformations, model selection criteria, and the reasoning behind each decision. It offers customizable workflows, allowing users to intervene and modify each stage of the process, thus combining the advantages of automated intelligence with human expertise. Additionally, we build a universal data science tool library, including carefully verified functions for data cleaning, feature engineering, and modeling, which form the foundation of this solution. We evaluate the framework on 8 carefully selected Kaggle competitions, achieve 83.8% in average completion rate and 42.8% average rank in Kaggle.", "title_embedding_index": 261, "title_abs_embedding_index": 286}, {"title": "Mixture-of-Diffusers: Dual-Stage Diffusion Model for Improved Time Series Generation", "link_suffix": "/forum?id=lcmd2Qdrsv", "link": "https://openreview.net/forum?id=lcmd2Qdrsv", "pdf_link": "https://openreview.net/pdf?id=lcmd2Qdrsv", "keywords": "Time Series Generation; Time Series Analysis; Diffusion Models; Mixture-of-Experts", "abstract": "Synthetic Time Series Generation (TSG) is a crucial task for data augmentation and various downstream applications. While TSG has advanced, its effectiveness often relies on the availability of extensive training datasets, posing challenges in data-scarce scenarios. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) have shown promise, but they frequently struggle to capture the complex temporal dynamics and interdependencies inherent in time series data. To address these limitations, we propose a novel generative framework, Mixture-of-Diffusers (MoD). This approach decomposes the diffusion process into a collection of specialized diffusers, each designed to model specific patterns at distinct noise levels. Early-stage diffusers focus on capturing overarching global and coarse patterns, while late-stage diffusers specialize in capturing fine-grained details as the noise level diminishes. This hierarchical decomposition empowers MoD to learn robust representations and generate realistic time series samples. The model is trained using a combination of multi-objective loss functions, ensuring both temporal consistency and alignment with the true data distribution. Extensive experiments on a diverse range of real-world and simulated time series datasets demonstrate the superior performance of MoD compared to state-of-the-art TSG generative models. Furthermore, rigorous evaluations incorporating both qualitative and quantitative metrics, coupled with assessments of downstream task performance on long-term generation and scarce time series data (see Figure 1), collectively validate the efficacy of our proposed approach.", "title_embedding_index": 262, "title_abs_embedding_index": 287}, {"title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference", "link_suffix": "/forum?id=8sglLco8Ti", "link": "https://openreview.net/forum?id=8sglLco8Ti", "pdf_link": "https://openreview.net/pdf?id=8sglLco8Ti", "keywords": "LLM, KV cache, compression, long-context", "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive contexts, but this ability comes with significant GPU memory costs, particularly in the key-value (KV) cache. Although recent KV cache compression methods show strong performance, all use discrete tokens to maintain the KV cache, leading to a loss of chunk semantic information. We introduce ChunkKV, a novel KV cache compression method that retains the most informative semantic chunks while discarding the less important ones. ChunkKV preserves semantic information by grouping related tokens. Furthermore, ChunkKV exhibits a higher similarity in the indices of the retained KV cache across different layers, so we also propose a layer-wise index reuse technique to further reduce computational overhead. This technique not only improves compression efficiency, but also provides insight into the similarities between layers within LLMs. We evaluated ChunkKV on long-context benchmarks including LongBench and Needle-In-A-HayStack, as well as the GSM8K in-context learning benchmark. Our experiments, conducted with models LLaMA-3-8B-Instruct, Mistral-7B-Instruct, and Qwen2-7B-Instruct, demonstrate that ChunkKV outperforms other KV cache compression methods in performance, even surpassing the full KV cache under the same conditions. With a compression ratio of 10%, ChunkKV achieves state-of-the-art performance on various tasks, indicating its effectiveness in semantic preservation and model performance for long-context and in-context LLM inference.", "title_embedding_index": 263, "title_abs_embedding_index": 288}, {"title": "Diffusion Auto-regressive Transformer for Effective Self-supervised Time Series Forecasting", "link_suffix": "/forum?id=yGv5GzlBwr", "link": "https://openreview.net/forum?id=yGv5GzlBwr", "pdf_link": "https://openreview.net/pdf?id=yGv5GzlBwr", "keywords": "Self-supervised Learning, Diffusion Model, Time Series Forecasting", "abstract": "Self-supervised learning has become an essential and popular approach for enhancing time series forecasting, enabling models to learn universal representations from unlabeled data. However, effectively capturing both the global sequence dependence and local detail features within time series data remains challenging. To address this, we propose a novel generative self-supervised method called TimeDART, denoting Diffusion Auto-regressive Transformer for Time series forecasting. In TimeDART, we treat time series patches as basic modeling units. For one thing, we employ an self-attention based Transformer encoder to model the dependencies of inter-patches. For another, we introduce diffusion and denoising mechanisms to capture the locality features of intra-patch. Notably, we design a cross-attention-based denoising decoder that allows for adjustable optimization difficulty in the self-supervised task, facilitating more effective self-supervised pre-training. Extensive experiments demonstrate that TimeDART achieves state-of-the-art fine-tuning performance compared to the most advanced competitive methods in forecasting tasks. Our code is publicly available athttps://anonymous.4open.science/r/TimeDART-2024.", "title_embedding_index": 264, "title_abs_embedding_index": 289}, {"title": "AutoAdvExBench: Benchmarking Autonomous Exploitation of Adversarial Example Defenses", "link_suffix": "/forum?id=leSbzBtofH", "link": "https://openreview.net/forum?id=leSbzBtofH", "pdf_link": "https://openreview.net/pdf?id=leSbzBtofH", "keywords": "security, benchmark, large language models, agents, adversarial examples", "abstract": "We introduce AutoAdvExBench, a benchmark to evaluate if large language models (LLMs)\ncan autonomously exploit defenses to adversarial examples.\nWe believe our benchmark will be valuable to several distinct audiences. \nFirst, it measures if models can match the abilities of expert adversarial machine learning researchers.\nSecond, it serves as a challenging evaluation for reasoning capabilities that\ncan measure LLMs' ability to understand and interact with sophisticated codebases. \nAnd third, \nsince many adversarial examples defenses have been broken in the past,\nthis benchmark allows for evaluating the ability of LLMs to reproduce\nprior research results automatically.\nWe then benchmark the ability of current LLMs to solve this benchmark,\nand find most are unable to succeed.\nOur strongest agent, with a human-guided prompt,\nis only able to successfully generate adversarial examples on 6 of the 51 defenses in our benchmark.\nThis benchmark is publicly accessible at redacted for review.", "title_embedding_index": 265, "title_abs_embedding_index": 290}, {"title": "Robust Root Cause Diagnosis using In-Distribution Interventions", "link_suffix": "/forum?id=l11DZY5Nxu", "link": "https://openreview.net/forum?id=l11DZY5Nxu", "pdf_link": "https://openreview.net/pdf?id=l11DZY5Nxu", "keywords": "Root Cause Diagnosis, Causal Inference, Interventional RCD", "abstract": "Diagnosing the root cause of an anomaly in a complex interconnected system is a pressing problem in today's cloud services and industrial operations. Effective root cause diagnosis calls for identifying nodes whose disrupted local mechanismscauseanomalous behavior at a target node. We propose IDI, a novel algorithm that predicts root cause as nodes that meet two criteria: 1)Anomaly:root cause nodes should take on anomalous values; 2)Fix:had the root cause nodes assumed usual values, the target node would not have been anomalous. Prior methods of assessing the fix condition rely on counterfactuals inferred from a Structural Causal Model (SCM) trained on historical data. But since anomalies are rare and fall outside the training distribution, the fitted SCMs  yield unreliable counterfactual estimates. IDI overcomes this by relying on interventional estimates obtained by solely probing the fitted SCM at in-distribution inputs. Our theoretical analysis demonstrates that IDI's in-distribution intervention approach outperforms other counterfactual estimation methods under mild assumptions about the data-generating process. Experiments on both synthetic and Petshop RCD benchmark datasets demonstrate that IDI consistently identifies true root causes more accurately and robustly than nine existing state-of-the-art RCD baselines. We release the anonymized code athttps://anonymous.4open.science/r/petshop-BB8A/.", "title_embedding_index": 266, "title_abs_embedding_index": 291}, {"title": "ManiBox: Enhancing Spatial Grasping Generalization via Scalable Simulation Data Generation", "link_suffix": "/forum?id=VEdeDd13gx", "link": "https://openreview.net/forum?id=VEdeDd13gx", "pdf_link": "https://openreview.net/pdf?id=VEdeDd13gx", "keywords": "Robot Learning, Reinforcement Learning, Sim2Real, Embodied AI", "abstract": "Learning a precise robotic grasping policy is crucial for embodied agents operating in complex real-world manipulation tasks. Despite significant advancements, most models still struggle with accurate spatial positioning of objects to be grasped. We first show that this spatial generalization challenge stems primarily from the extensive data requirements for adequate spatial understanding. However, collecting such data with real robots is prohibitively expensive, and relying on simulation data often leads to visual generalization gaps upon deployment. \nTo overcome these challenges, we then focus on state-based policy generalization and present ManiBox, a novel bounding-box-guided manipulation method built on a simulation-based teacher-student framework. The teacher policy efficiently generates scalable simulation data using bounding boxes, which are proven to uniquely determine the objects' spatial positions. The student policy then utilizes these low-dimensional spatial states to enable zero-shot transfer to real robots. \nThrough comprehensive evaluations in simulated and real-world environments, ManiBox demonstrates a marked improvement in spatial grasping generalization and adaptability to diverse objects and backgrounds.\nFurther, our empirical study into scaling laws for policy performance indicates that spatial volume generalization scales positively with data volume. For a certain level of spatial volume, the success rate of grasping empirically follows Michaelis-Menten kinetics relative to data volume, showing a saturation effect as data increases. Our data and code are available in the supplementary material.", "title_embedding_index": 267, "title_abs_embedding_index": 292}, {"title": "Q* Agent: Optimizing Language Agents with Q-Guided Exploration", "link_suffix": "/forum?id=rxUz2DaulF", "link": "https://openreview.net/forum?id=rxUz2DaulF", "pdf_link": "https://openreview.net/pdf?id=rxUz2DaulF", "keywords": "agent, large language model, q-learning, self-training", "abstract": "Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose Q*Agent, leveraging an estimated Q value to generate intermediate annotations for open language agents. \nBy introducing a reasoning tree and performing process reward modeling, Q*Agent provides effective intermediate guidance for each step. This guidance aims to automatically annotate data in a step-wise manner.\nBesides, we propose a Q-guided exploration strategy that can significantly boost model performance by providing process guidance during inference.\nNotably, even with almost half the annotated data, Q*Agent retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that Q*Agent can lead to more accurate decision making through qualitative analysis.", "title_embedding_index": 268, "title_abs_embedding_index": 293}, {"title": "QuantFormer: Learning to quantize for neural activity forecasting in mouse visual cortex", "link_suffix": "/forum?id=BBldjKEBlJ", "link": "https://openreview.net/forum?id=BBldjKEBlJ", "pdf_link": "https://openreview.net/pdf?id=BBldjKEBlJ", "keywords": "Neural Circuit Dynamics, Neural Activity Forecasting, Vector Quantization", "abstract": "Understanding complex animal behaviors hinges on deciphering the intricate neural activities within specific brain circuits. Two-photon imaging emerges as a powerful tool, offering significant insights into the dynamics of neuronal ensembles. In this context, forecasting neural activities is crucial for neuroscientists to create mathematical models of brain dynamics. Existing transformer-based methods, while effective in many domains, struggle to capture the distinctiveness of neural signals characterized by spatiotemporal sparsity and intricate dependencies.\nThis paper introducesQuantFormer, a novel transformer-based model designed for forecasting neural activity in two-photon calcium imaging data. Unlike traditional regression-based approaches,QuantFormerreframes the forecasting task as a classification problem through dynamic signal quantization, enabling better learning of sparse activity patterns. Additionally,QuantFormeraddresses the challenge of analyzing multivariate signals with an arbitrary number of neurons by using specialized neuron prompts. \nLeveraging unsupervised quantization training  on the Allen dataset, the largest publicly available dataset of two-photon calcium imaging,QuantFormerestablishes a new benchmark in mouse neural forecasting. It provides robustness and generalization across individuals and stimuli variations, thus defining the route towards a robust foundation model of the mouse visual cortex.", "title_embedding_index": 269, "title_abs_embedding_index": 294}, {"title": "Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction", "link_suffix": "/forum?id=dQpZolwXiH", "link": "https://openreview.net/forum?id=dQpZolwXiH", "pdf_link": "https://openreview.net/pdf?id=dQpZolwXiH", "keywords": "intrinsic interpretability, explainable AI, multimodal learning, Transformers, shapley values, crop yield prediction, remote sensing", "abstract": "Multimodal learning enables various machine learning tasks to benefit from diverse data sources, effectively mimicking the interplay of different factors in real life events. While the heterogeneous nature of these modalities may necessitate the design of complex architectures, their interpretability is often overlooked. In this study, we leverage the intrinsic explainability of Transformer-based models to explain multimodal learning frameworks. We utilize the self-attention mechanism alongside model-specific feature attribution techniques, comparing these against post-hoc methods. Our detailed analysis focuses on the challenging task of crop yield prediction, exploiting the characteristics of the modalities and the data to aggregate local explanations at multiple levels. Our findings indicate that Transformers significantly outperform other architectures in yield prediction, making them well-suited for further intrinsic interpretability analysis. Among the modalities, satellite data emerged as the most influential but requires deeper layers for effective feature extraction due to its complex structure. Additionally, we observed that the Attention Rollout method is more robust than Generic Attention, aligns more closely with Shapley-based attributions and shows reduced sensitivity to minor input variations.", "title_embedding_index": 270, "title_abs_embedding_index": 295}, {"title": "Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems", "link_suffix": "/forum?id=TbTJJNjumY", "link": "https://openreview.net/forum?id=TbTJJNjumY", "pdf_link": "https://openreview.net/pdf?id=TbTJJNjumY", "keywords": "Neural Combinatorial Optimization, Large-Scale Vehicle Routing Problem", "abstract": "Neural Combinatorial Optimization (NCO) methods have exhibited promising performance in solving Vehicle Routing Problems (VRPs). However, most NCO methods rely on the conventional self-attention mechanism that induces excessive computational complexity, thereby struggling to contend with large-scale VRPs and hindering their practical applicability. In this paper, we propose a lightweight cross-attention mechanism with linear complexity, by which a Transformer network is developed to learn efficient and favorable solutions for large-scale VRPs. We also propose a Self-Improved Training (SIT) algorithm that enables direct model training on large-scale VRP instances, bypassing extensive computational overhead for attaining labels. By iterating solution reconstruction, the Transformer network itself can generate improved partial solutions as pseudo-labels to guide the model training. Experimental results on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes indicate that our method consistently achieves superior performance for synthetic and real-world benchmarks, significantly boosting the scalability of NCO methods.", "title_embedding_index": 271, "title_abs_embedding_index": 296}, {"title": "Vision-Based Pseudo-Tactile Information Extraction and Localization for Dexterous Grasping", "link_suffix": "/forum?id=xcHIiZr3DT", "link": "https://openreview.net/forum?id=xcHIiZr3DT", "pdf_link": "https://openreview.net/pdf?id=xcHIiZr3DT", "keywords": "Pseudo-Tactile Information, Dexterous Grasping, Vision-Based Perception, Robotic Localization", "abstract": "This study addresses the challenges of tactile perception in robotic dexterous hand grasping by focusing on two main tasks: 1) Acquiring tactile information from everyday objects using vision, termed \"pseudo-tactile\" information, and 2) Building a Dexterous Hand (RH8D) model in Isaac Sim for real-time fingertip contact localization. Utilizing Isaac Sim enables safe, cost-effective experimentation and high-precision simulations that facilitate data collection for model validation. The research establishes a scientific connection between simulated 3D coordinates, actual 3D coordinates, and pseudo-tactile information derived from point clouds, quantified through normal vectors and grayscale variance analysis. Results demonstrate the ability to extract clear object surface textures, accurately locate fingertip contact points in real-time (with precision up to $0.001 m$), and provide tactile information at contact points. This framework enhances robotic grasping capabilities and offers low-cost sensory data. The source code and dataset are publicly available now.", "title_embedding_index": 272, "title_abs_embedding_index": 297}, {"title": "Toward a Sheaf-Theoretic Understanding of Compositionality in Large Language Models", "link_suffix": "/forum?id=srOVvTzgPo", "link": "https://openreview.net/forum?id=srOVvTzgPo", "pdf_link": "https://openreview.net/pdf?id=srOVvTzgPo", "keywords": "cognition, compositionality, sheaf-theory, language model evaluation", "abstract": "Compositionality has long been considered a central component of human cognition, enabling us to adeptly learn, manipulate and generate natural language. But what does this concept mean for Large Language Models (LLMs) that strive to imitate our linguistic knowledge? How can we evaluate it in LLMs? In this study, we introduce a novel sheaf-theoretic framework to redefine compositionality for LLMs, outlining four distinct conditions that encapsulate this complex phenomenon. Additionally, we propose targeted tasks to assess these conditions and evaluate them in a diverse array of LLMs. Our preliminary findings offer insights into the behaviour of LLMs, potentially identifying causal factors that influence their overall performance trends. This research emphasizes the potential utility of our sheaf-theoretic framework for compositionality -- both as a tool for understanding LLM  behaviour and for providing a set of metrics for analysing model performance and evaluation.", "title_embedding_index": 273, "title_abs_embedding_index": 298}, {"title": "Sensitivity Verification for Decision Tree Ensembles", "link_suffix": "/forum?id=h0vC0fm1q7", "link": "https://openreview.net/forum?id=h0vC0fm1q7", "pdf_link": "https://openreview.net/pdf?id=h0vC0fm1q7", "keywords": "Robustness verification, Sensitivity analysis, SAT solvers, efficient encodings, NP-hardness, fairness", "abstract": "Tree ensemble models, such as Gradient Boosted Decision trees (GBDTs) and random forests, are widely popular models for a variety of machine learning tasks. The power of these models comes from the ensemble of decision trees, which makes analysis of such models significantly harder than for single trees. As a result, recent work has focussed on developing exact and approximate techniques for questions such as robustness verification, fairness and explainability, for such models of tree ensembles.In this paper, we focus on a specific problem of feature sensitivity of decision tree ensembles and build a formal verification framework for it. We start by showing theoretical (NP-)hardness of the problem and explain how it relates to other verification problems. Next, we provide a novel encoding of the problem using pseudo-Boolean constraints. Based on this encoding, we develop a tunable algorithm to perform sensitivity analysis, which can trade off precision for running time. We implement our algorithm and study its performance on a suite of GBDT benchmarks from the literature. Our experiments show the practical utility of our approach and its improved performance compared to existing approaches.", "title_embedding_index": 274, "title_abs_embedding_index": 299}]