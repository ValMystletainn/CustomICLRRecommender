[
    {
        "title": "Towards Federated RLHF with Aggregated Client Preference for LLMs",
        "link_suffix": "/forum?id=mqNKiEB6pd",
        "link": "https://openreview.net/forum?id=mqNKiEB6pd",
        "pdf_link": "https://openreview.net/pdf?id=mqNKiEB6pd",
        "keywords": "Federated learning, RLHF, LLM",
        "abstract": "Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using user preference data, enabling it to generate content aligned with human preferences. However, due to privacy concerns, users may be reluctant to share sensitive preference data. To address this, we propose utilizing Federated Learning (FL) techniques, allowing large-scale preference collection from diverse real-world users without requiring them to transmit data to a central server. Our federated RLHF methods (i.e., FedBis and FedBiscuit) encode each client’s preferences into binary selectors and aggregate them to capture common preferences. In particular, FedBiscuit overcomes key challenges, such as preference heterogeneity and reward hacking, through innovative solutions like grouping clients with similar preferences to reduce heterogeneity and using multiple binary selectors to enhance LLM output quality. To evaluate the performance of the proposed methods, we establish the first federated RLHF benchmark with a heterogeneous human preference dataset. Experimental results show that by integrating the LLM with aggregated client preferences, FedBis and FedBiscuit significantly enhance the professionalism and readability of the generated content."
    },
    {
        "title": "SIG: Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs",
        "link_suffix": "/forum?id=j0KjevdhkH",
        "link": "https://openreview.net/forum?id=j0KjevdhkH",
        "pdf_link": "https://openreview.net/pdf?id=j0KjevdhkH",
        "keywords": "Self-Interpretable, Graph neural network, Continuous-time Dynamic Graph, Causal inference",
        "abstract": "While graph neural networks have demonstrated potential across various applications, explaining their predictions on dynamic graphs remains largely under-explored. This paper introduces a new research task: self-interpretable GNNs for continuous-time dynamic graphs (CTDGs). We aim to predict future links within dynamic graphs while simultaneously providing causal explanations for these predictions. There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations. To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM).  ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency. Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to OOD data. Our code and datasets are anonymously released athttps://github.com/2024SIG/SIG."
    },
    {
        "title": "Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control",
        "link_suffix": "/forum?id=w3iM4WLuvy",
        "link": "https://openreview.net/forum?id=w3iM4WLuvy",
        "pdf_link": "https://openreview.net/pdf?id=w3iM4WLuvy",
        "keywords": "Decision Frequency, Action Sequence Generation, Model-Based Training, Model-Free Control, Efficient Learning, Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) is rapidly reaching and surpassing human-level control capabilities. However, state-of-the-art RL algorithms often require timesteps and reaction times significantly faster than human capabilities, which is impractical in real-world settings and typically necessitates specialized hardware. Such speeds are difficult to achieve in the real world and often requires specialized hardware. We introduce Sequence Reinforcement Learning (SRL), an RL algorithm designed to produce a sequence of actions for a given input state, enabling effective control at lower decision frequencies. SRL addresses the challenges of learning action sequences by employing both a model and an actor-critic architecture operating at different temporal scales. We propose a \"temporal recall\" mechanism, where the critic uses the model to estimate intermediate states between primitive actions, providing a learning signal for each individual action within the sequence. Once training is complete, the actor can generate action sequences independently of the model, achieving model-free control at a slower frequency. We evaluate SRL on a suite of continuous control tasks, demonstrating that it achieves performance comparable to state-of-the-art algorithms while significantly reducing actor sample complexity. To better assess performance across varying decision frequencies, we introduce the Frequency-Averaged Score (FAS) metric. Our results show that SRL significantly outperforms traditional RL algorithms in terms of FAS, making it particularly suitable for applications requiring variable decision frequencies. Additionally, we compare SRL with model-based online planning, showing that SRL achieves superior FAS while leveraging the same model during training that online planners use for planning. Lastly, we highlight the biological relevance of SRL, showing that it replicates the \"action chunking\" behavior observed in the basal ganglia, offering insights into brain-inspired control mechanisms."
    },
    {
        "title": "CAD-Editor: Text-based CAD Editing through Adapting Large Language Models with Synthetic Data",
        "link_suffix": "/forum?id=Jrb9yXZJKG",
        "link": "https://openreview.net/forum?id=Jrb9yXZJKG",
        "pdf_link": "https://openreview.net/pdf?id=Jrb9yXZJKG",
        "keywords": "Computer Aided Design, Generative Models, Text-based Editing, Large Language Models",
        "abstract": "Computer Aided Design (CAD) is indispensable across various industries. \n\\emph{Text-based CAD editing}, which automatically modifies CAD models following textual instructions, is important yet not extensively studied. \nExisting work explores design variation generation, which randomly alters specific parts of a CAD model, offering no control over the final appearance.\nThis work introduces \\emph{CAD-Editor} for text-based editing.\nWe leverage Large Language Models (LLMs) as the backbone to take the concatenation of textual instruction and original CAD sequence as input and predict the edited CAD sequence, where the sequence representation of a CAD model is designed for easier processing by LLMs.\nMoreover, we propose fine-tuning LLMs by using a synthetic dataset followed by a selective dataset.\nThe synthetic data is produced by leveraging powerful existing models, including design variation generation models for producing paired CAD models and multi-modal models for capturing textual differences between these pairs.\nThe selective data is created by choosing top examples from outputs of the initially fine-tuned LLMs based on human feedback or metrics.\nIn this way, a large-scale synthetic dataset offers basic capability while a selective dataset that is less noisy and better aligned with human intentions boosts performance further.\nExtensive experiments demonstrate the advantage of CAD-Editor both quantitatively and qualitatively."
    },
    {
        "title": "DATASEA - AN AUTOMATIC FRAMEWORK FOR COMPREHENSIVE DATASET PROCESSING USING LARGE LANGUAGE MODELS",
        "link_suffix": "/forum?id=zEPYCDaJae",
        "link": "https://openreview.net/forum?id=zEPYCDaJae",
        "pdf_link": "https://openreview.net/pdf?id=zEPYCDaJae",
        "keywords": "Automated Data Processing, LLM, Data Pipeline Automation, NLP, Data Mining",
        "abstract": "In the era of data-driven decision-making, efficiently acquiring and analyzing\ndiverse datasets is critical for accelerating research and innovation. Yet, traditional manual approaches to dataset discovery, preparation, and exploration\nremain inefficient and cumbersome, especially as the scale and complexity of\ndatasets continue to expand. These challenges create major roadblocks, slowing down the pace of progress and reducing the capacity for data-driven breakthroughs. To address these challenges, we introduce DataSEA (Search, Evaluate, Analyze), a fully automated system for comprehensive dataset processing, leveraging large language models (LLMs) to streamline the data handling\npipeline. DataSEA autonomously searches for dataset sources, retrieves and organizes evaluation metadata, and generates custom scripts to load and analyze\ndata based on user input. Users can provide just a dataset name, and DataSEA\nwill handle the entire preparation process. While fully automated, minimal user\ninteraction can further enhance system accuracy and dataset handling specificity.\nWe evaluated DataSEA on datasets from distinct fields, demonstrating its robustness and efficiency in reducing the time and effort required for data preparation\nand exploration. By automating these foundational tasks, DataSEA empowers\nresearchers to allocate more time to in-depth analysis and hypothesis generation, ultimately accelerating the pace of innovation. The code is available athttps://github.com/SingleView11/DataSEA."
    },
    {
        "title": "Knowledge Manipulation in Language Models",
        "link_suffix": "/forum?id=oDbiL9CLoS",
        "link": "https://openreview.net/forum?id=oDbiL9CLoS",
        "pdf_link": "https://openreview.net/pdf?id=oDbiL9CLoS",
        "keywords": "knowledge manipulation, language models, generative models, reversal curse",
        "abstract": "Language models can store vast factual knowledge, yet their ability to flexibly use this knowledge for downstream tasks (e.g., via instruction finetuning) remains questionable. This paper investigates four fundamental knowledge manipulation tasks: \\textbf{retrieval} (e.g., \"What is person A's attribute X?\"), \\textbf{classification} (e.g., \"Is A's attribute X even or odd?\"), \\textbf{comparison} (e.g., \"Is A greater than B in attribute X?\"), and \\textbf{inverse search} (e.g., \"Which person's attribute X equals T?\").We show that language models excel in knowledge retrieval but struggle even in the simplest classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. Moreover, their performance in inverse knowledge search is virtually 0%, regardless of the prompts.\nOur primary contribution is a \\emph{controlled, synthetic experiment} that confirms these weaknesses are \\emph{inherent} to language models: they cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored in the models, despite adequate training and sufficient model size. Our findings also apply to modern pretrained language models such as GPT-4, thus giving rise to many Turing tests to distinguish Humans from contemporary AIs."
    },
    {
        "title": "Logic-Logit: A Logic-Based Approach to Choice Modeling",
        "link_suffix": "/forum?id=vJgJSrYPe1",
        "link": "https://openreview.net/forum?id=vJgJSrYPe1",
        "pdf_link": "https://openreview.net/pdf?id=vJgJSrYPe1",
        "keywords": "Choice Model, Preference Learning, Interpretability, Rule Learning",
        "abstract": "In this study, we propose a novel rule-based interpretable choice model, {\\bf Logic-Logit}, designed to effectively learn and explain human choices. Choice models have been widely applied across various domains—such as commercial demand forecasting, recommendation systems, and consumer behavior analysis—typically categorized as parametric, nonparametric, or deep network-based. While recent innovations have favored neural network approaches for their computational power, these flexible models often involve large parameter sets and lack interpretability, limiting their effectiveness in contexts where transparency is essential.Previous empirical evidence shows that individuals usually use {\\it heuristic decision rules} to form their consideration sets, from which they then choose. These rules are often represented as {\\it disjunctions of conjunctions} (i.e., OR-of-ANDs). These rules-driven, {\\it consider-then-choose} decision processes enable people to quickly screen numerous alternatives while reducing cognitive and search costs. Motivated by this insight, our approach leverages logic rules to elucidate human choices, providing a fresh perspective on preference modeling. We introduce a unique combination of column generation techniques and the Frank-Wolfe algorithm to facilitate efficient rule extraction for preference modeling—a process recognized as NP-hard. Our empirical evaluation, conducted on both synthetic datasets and real-world data from commercial and healthcare domains, demonstrates that Logic-Logit significantly outperforms baseline models in terms of interpretability and accuracy."
    },
    {
        "title": "A Neural Architecture Dataset for Adversarial Robustness",
        "link_suffix": "/forum?id=AZVvTBxTdZ",
        "link": "https://openreview.net/forum?id=AZVvTBxTdZ",
        "pdf_link": "https://openreview.net/pdf?id=AZVvTBxTdZ",
        "keywords": "Adversarial robustness, neural architecture design",
        "abstract": "Robustness to adversarial attacks is critical for practical deployments of deep neural networks. However, pursuing adversarial robustness from the network architecture perspective demands tremendous computational resources, thereby hampering progress in understanding and designing robust architectures. In this work, we aim to lower this barrier-to-entry for researchers without access to large-scale computation by introducing the first comprehensive neural architecture dataset under adversarial training, dubbed NARes, for adversarial robustness. NARes comprises 15,625 WRN-style unique architectures adversarially trained and evaluated against four adversarial attacks (including AutoAttack). With NARes, researchers can query the adversarial robustness of various models immediately, along with more detailed information, such as fine-grained training statistics, empirical Lipschitz constant, stable accuracy, etc. In addition, four checkpoints are provided for each architecture to facilitate further fine-tuning or analysis. For the first time, the dataset provides a high-resolution architecture landscape for adversarial robustness, enabling quick verifications of theoretical or empirical ideas. Through NARes, we offered some new insight and identified some contradictions in statements of prior studies. We believe NARes can serve as a valuable resource for the community to advance the understanding and design of robust neural architectures."
    },
    {
        "title": "Interpretability of Language Models for Learning Hierarchical Structures",
        "link_suffix": "/forum?id=J6qrIjTzoM",
        "link": "https://openreview.net/forum?id=J6qrIjTzoM",
        "pdf_link": "https://openreview.net/pdf?id=J6qrIjTzoM",
        "keywords": "generative language models, interpretability, induction head, inner workings",
        "abstract": "Transformer-based language models are effective but complex, and understanding their inner workings is a significant challenge. Previous research has primarily explored how these models handle simple tasks like name copying or selection, and we extend this by investigating how these models grasp complex, recursive language structures defined by context-free grammars (CFGs). We introduce a family of synthetic CFGs that produce hierarchical rules, capable of generating lengthy sentences (e.g., hundreds of tokens) that are locally ambiguous and require dynamic programming to parse. Despite this complexity, we demonstrate that generative models like GPT can accurately learn this CFG language and generate sentences based on it. We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm."
    },
    {
        "title": "Inference of Evolving Mental States from Irregular Action Events to Understand Human Behaviors",
        "link_suffix": "/forum?id=YSA0QeYnDd",
        "link": "https://openreview.net/forum?id=YSA0QeYnDd",
        "pdf_link": "https://openreview.net/pdf?id=YSA0QeYnDd",
        "keywords": "temporal point process, logic rule, human-AI collaboration",
        "abstract": "Inference of latent human mental processes, such as belief, intention, or desire, is crucial for developing AI with human-like intelligence, enabling more effective and timely collaboration. In this paper, we introduce a versatile encoder-decoder model designed to infer  evolving mental processes based on irregularly observed action events and predict future occurrences. The primary challenges arise from two factors: both actions and mental processes are irregular events, and the observed action data is often limited. To address the irregularity of these events, we leverage a temporal point process model within the encoder-decoder framework, effectively capturing the dynamics of both action and mental events. Additionally, we implement a backtracking mechanism in the decoder to enhance the accuracy of predicting future actions and evolving mental states. To tackle the issue of limited data, our model incorporates logic rules as priors, enabling accurate inferences from just a few observed samples. These logic rules can be refined and updated as needed, providing flexibility to the model. Overall, our approach enhances the understanding of human behavior by predicting when actions will occur and how mental processes evolve. Experiments on both synthetic and real-world datasets demonstrate the strong performance of our model in inferring mental states and predicting future actions, contributing to the development of more human-centric AI systems."
    },
    {
        "title": "On the Local Complexity of Linear Regions in Deep ReLU Networks",
        "link_suffix": "/forum?id=IQdlPvj4dX",
        "link": "https://openreview.net/forum?id=IQdlPvj4dX",
        "pdf_link": "https://openreview.net/pdf?id=IQdlPvj4dX",
        "keywords": "ReLU networks, Linear Regions, Representation Learning, Low-rank Bias, Robustness, Implicit Regularization",
        "abstract": "We define the $\\textit{local complexity}$ of a neural network with continuous piecewise linear activations as a measure of the density of linear regions over an input data distribution. We show theoretically that ReLU networks that learn low-dimensional feature representations have a lower local complexity. This allows us to connect recent empirical observations on feature learning at the level of the weight matrices with concrete properties of the learned functions. In particular, we show that the local complexity serves as an upper bound on the total variation of the function over the input data distribution and thus that feature learning can be related to adversarial robustness. Lastly, we consider how optimization drives ReLU networks towards solutions with lower local complexity. Overall, this work contributes a theoretical framework towards relating geometric properties of ReLU networks to different aspects of learning such as feature learning and representation cost."
    },
    {
        "title": "Task Facet Learning: A Structured Approach to Prompt Optimization",
        "link_suffix": "/forum?id=ViRDmDAfjg",
        "link": "https://openreview.net/forum?id=ViRDmDAfjg",
        "pdf_link": "https://openreview.net/pdf?id=ViRDmDAfjg",
        "keywords": "prompt optimization, large language model",
        "abstract": "Given a task in the form of a basic description and its training examples, prompt optimization is the problem of synthesizing the given information into a text prompt for a large language model. Humans solve this problem by also considering the different facets that define a task (e.g., counter-examples, explanations, analogies) and including them in the prompt. However, it is unclear whether existing algorithmic approaches, based on iteratively editing a given prompt or automatically selecting a few in-context examples, can cover the multiple facets required to solve a complex task.  In this work, we view prompt optimization as that of learning multiple facets of a task from a set of training examples. We exploit structure in the prompt optimization problem and break down a prompt into loosely coupled semantic sections. The proposed algorithm, UniPrompt, (1) clusters the input space and uses clustered batches so that each batch likely corresponds to a different facet of the task, and (2) utilizes a feedback mechanism to propose adding, editing or deleting a section, which in turn is aggregated over a batch to capture generalizable facets. Empirical evaluation on multiple datasets and a real-world task shows that prompts generated using UniPrompt obtain higher accuracy than human-tuned prompts and those from state-of-the-art methods. In particular, our algorithm can generate long, complex prompts that existing methods are unable to generate."
    },
    {
        "title": "Knowledge Capacity Scaling Laws for Language Models",
        "link_suffix": "/forum?id=FxNNiUgtfa",
        "link": "https://openreview.net/forum?id=FxNNiUgtfa",
        "pdf_link": "https://openreview.net/pdf?id=FxNNiUgtfa",
        "keywords": "scaling laws, knowledge capacity, language models",
        "abstract": "Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate information-theoretically the number of knowledge \\emph{bits} a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store \\emph{2 bits of knowledge per parameter, even when quantized to int8}, and such knowledge can be flexibly extracted for downstream applications.More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity."
    },
    {
        "title": "On Calibration of LLM-based Guard Models for Reliable Content Moderation",
        "link_suffix": "/forum?id=wUbum0nd9N",
        "link": "https://openreview.net/forum?id=wUbum0nd9N",
        "pdf_link": "https://openreview.net/pdf?id=wUbum0nd9N",
        "keywords": "Content Moderation, LLM-based Guard Models, Calibration, Safety",
        "abstract": "Large language models (LLMs) are exposed to significant risks due to their potential for malicious use. Existing studies have developed LLM-based guard models designed to moderate the input and output of threat LLMs, ensuring adherence to safety policies by blocking content that violates these protocols upon deployment. However, limited attention has been given to the reliability and calibration of such guard models. In this work, we empirically conduct comprehensive investigations of confidence calibration for 9 existing LLM-based guard models on 12 benchmarks in both user input and model output classification. Our findings reveal that current LLM-based guard models tend to 1) produce overconfident predictions, 2) exhibit significant miscalibration when subjected to jailbreak attacks, and 3) demonstrate limited robustness to the outputs generated by different types of response models. Additionally, we assess the effectiveness of post-hoc calibration methods to mitigate miscalibration. We demonstrate the efficacy of temperature scaling and, for the first time, highlight the benefits of contextual calibration for confidence calibration of guard models, particularly in the absence of validation sets. Our analysis and experiments underscore the limitations of current LLM-based guard models and provide valuable insights for the future development of well-calibrated guard models toward more reliable content moderation. We also advocate for incorporating reliability evaluation of confidence calibration when releasing future LLM-based guard models."
    },
    {
        "title": "Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers",
        "link_suffix": "/forum?id=HrdVqFSn1e",
        "link": "https://openreview.net/forum?id=HrdVqFSn1e",
        "pdf_link": "https://openreview.net/pdf?id=HrdVqFSn1e",
        "keywords": "Diffusion Models, Probability Flow ODEs, Unified Framework, Deterministic Samplers",
        "abstract": "Score-based diffusion models have emerged as powerful techniques for generating samples from high-dimensional data distributions. These models involve a two-phase process: first, injecting noise to transform the data distribution into a known prior distribution, and second, sampling to recover the original data distribution from noises. Among the various sampling methods, deterministic samplers stand out for their enhanced efficiency. However, analyzing these deterministic samplers presents unique challenges, as they preclude the use of established techniques such as Girsanov's theorem, which are only applicable to stochastic samplers. Furthermore, existing analysis for deterministic samplers usually focuses on some specific examples, lacking a generalized approach for general forward processes and various deterministic samplers. Our paper addresses these limitations by introducing a unified convergence analysis framework. To demonstrate the power of our framework, we analyze the variance-preserving (VP) forward process with the exponential integrator (EI) scheme, and achieved iteration complexity of $\\tilde{O}(d^2/\\epsilon)$. Additionally, we provide a detailed analysis of DDIM-type samplers, which have been underexplored in previous research, achieving polynomial iteration complexity."
    },
    {
        "title": "From Global Assessment to Local Selection: Efficiently Solving Traveling Salesman Problems of All Sizes",
        "link_suffix": "/forum?id=SBbjwfMuik",
        "link": "https://openreview.net/forum?id=SBbjwfMuik",
        "pdf_link": "https://openreview.net/pdf?id=SBbjwfMuik",
        "keywords": "Traveling salesman problem, neural combinatorial optimization, size generalization",
        "abstract": "The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem with broad real-world applications. Recent advancements in neural network-based TSP solvers have shown promising results. Nonetheless, these models often struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility. To address this issue, we introduce a novel neural TSP solver named GELD, built upon our proposed broad global assessment and refined local selection framework. Specifically, GELD integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich embedding representation while accelerating the decision-making process. Moreover, GE incorporates a novel low-complexity attention mechanism, allowing GELD to achieve low inference latency and scalability to larger-scale TSPs. Additionally, we propose a two-stage training strategy that utilizes training instances of different sizes to bolster GELD's generalization ability. Extensive experiments conducted on both synthetic and real-world datasets demonstrate that GELD outperforms seven state-of-the-art models considering both solution quality and inference speed. Furthermore, GELD can be employed as a post-processing method to exchange affordable computing time for significantly improved solution quality, capable of solving TSPs with up to 744,710 nodes without relying on divide-and-conquer strategies."
    },
    {
        "title": "Optimizing Neural Network Representations of Boolean Networks",
        "link_suffix": "/forum?id=1H90Gb9rJ9",
        "link": "https://openreview.net/forum?id=1H90Gb9rJ9",
        "pdf_link": "https://openreview.net/pdf?id=1H90Gb9rJ9",
        "keywords": "Neural Networks, Boolean Networks, Lossless Optimization, Integer Linear Programming, NPN Classification",
        "abstract": "Neural networks are known to be universal computers for Boolean functions. Recent advancements in hardware have significantly reduced matrix multiplication times, making neural network simulation both fast and efficient. Consequently, functions defined by complex Boolean networks are increasingly viable candidates for simulation through their neural network representation. Prior research has introduced a general method for deriving neural network representations of Boolean networks. However, the resulting neural networks are often suboptimal in terms of the number of neurons and connections, leading to slower simulation performance. Optimizing them while preserving functional equivalence --lossless optimization-- is an NP-hard problem, and current methods only provide lossy solutions. In this paper, we present an algorithm to optimize such neural networks in terms of neurons and connections while preserving functional equivalence. Moreover, to accelerate the compression of the neural network, we introduce an objective-aware algorithm that exploits representations that are shared among subproblems of the overall optimization. We demonstrate experimentally that we are able to reduce connections and neurons by up to 70% and 60%, respectively, in comparison to state-of-the-art. We also find that our objective-aware algorithm results in consistent speedups in optimization time, achieving up to 34.3x and 5.9x speedup relative to naive and caching solutions, respectively. Our methods are of practical relevance to applications such as high-throughput circuit simulation and placing neurosymbolic systems on the same hardware architecture."
    },
    {
        "title": "Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference",
        "link_suffix": "/forum?id=9juyeCqL0u",
        "link": "https://openreview.net/forum?id=9juyeCqL0u",
        "pdf_link": "https://openreview.net/pdf?id=9juyeCqL0u",
        "keywords": "Causal Order, Imperfect Experts, Causal Inference, LLMs",
        "abstract": "Large Language Models (LLMs) have recently been used as experts to infer causal graphs, often by repeatedly applying a pairwise prompt that asks about the causal relationship of each variable pair. However, such experts, including human domain experts, cannot distinguish between direct and indirect effects given a pairwise prompt. Therefore, instead of the graph, we propose that causal order be used as a more stable output interface for utilizing expert knowledge. When querying a perfect expert with a pairwise prompt, we show that the inferred graph can have significant errors whereas the causal order is always correct. In practice, however, LLMs are imperfect experts and we find that pairwise prompts lead to multiple cycles and do not yield a valid order. Hence, we propose a prompting strategy that introduces an auxiliary variable for every variable pair and instructs the LLM to avoid cycles within this triplet. We show, both theoretically and empirically, that such a triplet prompt leads to fewer cycles than the pairwise prompt. Across multiple real-world graphs, the triplet prompt yields a more accurate order using both LLMs and human annotators as experts. By querying the expert with different auxiliary variables for the same variable pair, it also increases robustness---triplet method with much smaller models such as Phi-3 and Llama-3 8B outperforms a pairwise prompt with GPT-4. For practical usage, we show how the estimated causal order from the triplet method  can be used to reduce error in downstream discovery and effect inference tasks."
    },
    {
        "title": "Knowledge Retention in Continual Model-Based Reinforcement Learning",
        "link_suffix": "/forum?id=UNHU7uO2qM",
        "link": "https://openreview.net/forum?id=UNHU7uO2qM",
        "pdf_link": "https://openreview.net/pdf?id=UNHU7uO2qM",
        "keywords": "Continual learning, Model-based reinforcement learning, World model, Catastrophic forgetting",
        "abstract": "We propose DRAGO, a novel approach for continual model-based reinforcement learning aimed at improving the incremental development of world models across a sequence of tasks that differ in their reward functions but not the state space or dynamics. DRAGO comprises two key components: $\\textit{Synthetic Experience Rehearsal}$, which leverages generative models to create synthetic experiences from past tasks, allowing the agent to reinforce previously learned dynamics without storing data, and $\\textit{Regaining Memories Through Exploration}$, which introduces an intrinsic reward mechanism to guide the agent toward revisiting relevant states from prior tasks. Together, these components enable the agent to maintain a comprehensive and continually developing world model, facilitating more effective learning and adaptation across diverse environments. Empirical evaluations demonstrate that DRAGO is able to preserve knowledge across tasks, achieving superior performance in various continual learning scenarios."
    },
    {
        "title": "How Can Language Models Learn from Mistakes on Grade-School Math Problems",
        "link_suffix": "/forum?id=zpDGwcmMV4",
        "link": "https://openreview.net/forum?id=zpDGwcmMV4",
        "pdf_link": "https://openreview.net/pdf?id=zpDGwcmMV4",
        "keywords": "pretraining, language model, error correction, error detection",
        "abstract": "Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to \"self-correct'' their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating ``error-correction'' data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others."
    },
    {
        "title": "ChemThinker: Thinking Like a Chemist with Multi-Agent LLMs for Deep Molecular Insights",
        "link_suffix": "/forum?id=zlAUnwhE2v",
        "link": "https://openreview.net/forum?id=zlAUnwhE2v",
        "pdf_link": "https://openreview.net/pdf?id=zlAUnwhE2v",
        "keywords": "Molecular Property Prediction, Molecular Representation Learning, Multi-Agent LLMs",
        "abstract": "Molecular property prediction is vital in drug discovery and cheminformatics, yet many current models lack interpretability, making it difficult for experts to understand the rationale behind predictions. To address this, we introduce ChemThinker, a novel large language models (LLMs) multi-agent framework designed to effectively control the internal representations of concepts and functions within LLMs. ChemThinker emulates the way chemists approach molecular analysis by integrating insights from three perspectives: general molecular properties, data-driven analysis, and task-specific factors. Each perspective uses an agentic approach to stimulate the LLM's internal representations, enabling more targeted and interpretable outputs based on the problem at hand, akin to how stimuli trigger the brain's cognitive processes. By feeding representations from these three perspectives into a simple multi-layer perceptron (MLP), ChemThinker achieves superior performance, significantly outperforming existing baselines across multiple benchmarks. Furthermore, our framework provides interpretable insights into the molecular mechanisms driving the predictions, making it a practical tool for drug discovery and other cheminformatics applications."
    },
    {
        "title": "Enriching Knowledge Distillation with Intra-Class Contrastive Learning",
        "link_suffix": "/forum?id=7L8sZYMlya",
        "link": "https://openreview.net/forum?id=7L8sZYMlya",
        "pdf_link": "https://openreview.net/pdf?id=7L8sZYMlya",
        "keywords": "Knowledge distillation; Computer vision; Contrastive learning.",
        "abstract": "Since the advent of knowledge distillation, much research has focused on how the soft labels generated by the teacher model can be utilized effectively. A study points out that the implicit knowledge within soft labels originates from the multi-view structure present in the data. Feature variations within samples of the same class allow the student model to generalize better by learning diverse representations. However, in existing distillation methods, teacher models predominantly adhere to ground-truth labels as targets, without considering the diverse representations within the same class. Therefore, we propose incorporating an intra-class contrastive loss during teacher training to enrich the intra-class information contained in soft labels. In practice, we find that intra-class loss causes instability in training and slows convergence. To mitigate these issues, margin loss is integrated into intra-class contrastive learning to improve the training stability and convergence speed. Simultaneously, we theoretically analyze the impact of this loss on the intra-class distances and inter-class distances. It has been proved that the intra-class contrastive loss can enrich the intra-class diversity. Experimental results demonstrate the effectiveness of the proposed method."
    },
    {
        "title": "MoIN: Mixture of Introvert Experts to Upcycle an LLM",
        "link_suffix": "/forum?id=L0PciKdHsP",
        "link": "https://openreview.net/forum?id=L0PciKdHsP",
        "pdf_link": "https://openreview.net/pdf?id=L0PciKdHsP",
        "keywords": "llm, lora, moe",
        "abstract": "The goal of this paper is to improve (upcycle) an existing large language model without the prohibitive requirements of continued pre-training of the full-model. The idea is to split the pre-training data into semantically relevant groups and train an expert on each subset. An expert takes the form of a lightweight adapter added on the top of a frozen base model. During inference, an incoming query is first routed to the most relevant expert which is then loaded onto the base model for the forward pass. Unlike typical Mixture of Experts (MoE) models, the experts in our method do not work with other experts for a single query. Hence, we dub them ``introvert'' experts. Freezing the base model and keeping the experts as lightweight adapters allows extreme parallelism during training and inference. Training of all experts can be done in parallel without any communication channels between them. Similarly, the inference can also be heavily parallelized by distributing experts on different GPUs and routing each request to the GPU containing its relevant expert. We implement a proof-of-concept version of this method and show the validity of our approach."
    },
    {
        "title": "REPANA: Reasoning Path Navigated Program Induction for Universally Reasoning over Heterogeneous Knowledge Bases",
        "link_suffix": "/forum?id=Vx3o2tUErQ",
        "link": "https://openreview.net/forum?id=Vx3o2tUErQ",
        "pdf_link": "https://openreview.net/pdf?id=Vx3o2tUErQ",
        "keywords": "Knowledge Base QA; Low Resource Reasoning; Multi-hop QA; Reasoning Interpretability;",
        "abstract": "Program induction is a typical approach that helps Large Language Models (LLMs) in complex knowledge-intensive question answering over knowledge bases (KBs) to alleviate the hallucination of LLMs. However, the accurate program induction usually requires a large number of high-quality parallel data of a specific KB, which is difficult to acquire for many low-resource KBs. Additionally, due to heterogeneity of questions and KB schemas, the transferability of a model trained on a single dataset is poor. To this end, we propose REPANA, a reasoning path navigated program induction framework that enables LLMs to reason over heterogeneous KBs. We decouple the program generation capability into perceiving the KB and mapping questions to program sketches. Accordingly, our framework consists of two main components. The first is an LLM-based navigator, which retrieves reasoning paths of the input question from the given KB. The second is a KB-agnostic parser trained on data from multiple heterogeneous datasets, taking the navigator's retrieved paths and the question as input and generating the corresponding program. Experiments show that REPANA exhibits strong generalization and transferability. It can directly perform inference on datasets not seen during training, outperforming other SoTA low-resource methods and even approaching the performance of supervised methods."
    },
    {
        "title": "DefNTaxS: The Inevitable Need for More Structured Description in Zero-Shot Classification",
        "link_suffix": "/forum?id=B2ChNpcEzZ",
        "link": "https://openreview.net/forum?id=B2ChNpcEzZ",
        "pdf_link": "https://openreview.net/pdf?id=B2ChNpcEzZ",
        "keywords": "zero shot, classification, CLIP, VLM, DCLIP, WaffleCLIP, open vocabulary, pretrained",
        "abstract": "Existing approaches leveraging large pretrained vision-language models (VLMs) like CLIP for zero-shot text-image classification often focus on generating fine-grained class-specific descriptors, leaving higher-order semantic relations between classes underutilised.\nWe address this gap by proposing Defined Taxonomic Stratification (DefNTaxS), a novel and malleable framework that supplements per-class descriptors with inter-class taxonomies to enrich semantic resolution in zero-shot classification tasks.\nUsing large language models (LLMs), DefNTaxS automatically generates subcategories that group similar classes and appends context-specific prompt elements for each dataset/subcategory, reducing inter-class competition and providing deeper semantic insight.\nThis process is fully automated, requiring no manual modifications or further training for any of the models involved.\nWe demonstrate that DefNTaxS yields consistent performance gains across a number of datasets often used to benchmark frameworks of this type, enhancing accuracy and semantic interpretability in zero-shot classification tasks of varying scale, granularity, and type."
    }
]