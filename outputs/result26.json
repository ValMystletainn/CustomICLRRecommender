[{"title": "Common Pitfalls of Margin-based Preference Optimization in Language Model Alignment", "link_suffix": "/forum?id=YaBiGjuDiC", "link": "https://openreview.net/forum?id=YaBiGjuDiC", "pdf_link": "https://openreview.net/pdf?id=YaBiGjuDiC", "keywords": "Alignment, Preference Optimization, Large Language Model", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become the predominant approach for aligning language models (LMs) to be more helpful and less harmful. At its core, RLHF uses a margin-based loss for preference optimization, which specifies the ideal LM behavior only in terms of the difference between preferred and dispreferred responses. \nThis under-specification of ideal behavior for each response individually leads to two unintended consequences as the margin increases:\n(1) The probability of dispreferred (e.g., unsafe) responses may increase, resulting in potential safety alignment failures.\n(2) When the probability of dispreferred responses is reduced, this often coincides with a decrease in the probability of preferred responses, even when these responses are ideal.\nIn this paper, we identify the fundamental issue: margin-based preference optimization loss under-specifies ideal LM behaviors. \nWe derive key conditions under which the probabilities of both preferred and dispreferred responses increase or decrease together. \nThese conditions occur when the inner products between the gradients of the log-probabilities of preferred and dispreferred responses are large. \nWe theoretically analyze when such inner products are large and empirically validate our findings. \nOur framework also reveals important differences in the training dynamics of various preference optimization algorithms and suggests new directions for developing better algorithms for language model alignment.", "title_embedding_index": 1250, "title_abs_embedding_index": 1275}, {"title": "FROM LOW TO HIGH-VALUE DESIGNS: OFFLINE OPTIMIZATION VIA GENERALIZED DIFFUSION", "link_suffix": "/forum?id=K9Elg2JrvY", "link": "https://openreview.net/forum?id=K9Elg2JrvY", "pdf_link": "https://openreview.net/pdf?id=K9Elg2JrvY", "keywords": "Diffusion Model, Probabilistic Method", "abstract": "This paper presents a new perspective on offline optimization. Instead of viewing it as a surrogate or inverse modeling task -- mapping either from an input design to its corresponding performance or from a desired performance to potential input candidates -- we approach offline optimization as a distributional translation task that transforms an implicit distribution of low-value inputs (i.e., the offline data) into a (better) distribution of high-value inputs (i.e., the solution candidates). This avoids explicitly modeling the target function, which is ultimately constrained by the limited amount of offline data. In contrast, our view of offline optimization as a distributional translation task is substantiated through a generalized Brownian bridge diffusion process mapping between two implicit data distributions, which can be more reliably learned using additional low- and high-value inputs drawn from synthetic functions similar to the target function. This is enabled by fitting multiple Gaussian processes with different parameterizations to the offline data and using them as functional posterior to generate artificial functions similar to the target function. Our experiments show that this approach is consistently more effective than previous methods, establishing a new state-of-the-art performance.", "title_embedding_index": 1251, "title_abs_embedding_index": 1276}, {"title": "Incidental Polysemanticity: A New Obstacle for Mechanistic Interpretability", "link_suffix": "/forum?id=OeHSkJ58TG", "link": "https://openreview.net/forum?id=OeHSkJ58TG", "pdf_link": "https://openreview.net/pdf?id=OeHSkJ58TG", "keywords": "polysemanticity, mechanistic interpretability, AI safety, deep learning, science of deep learning, neural computation, interpretability", "abstract": "Polysemantic neurons \u2014 neurons that activate for a set of unrelated features \u2014 have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more \"features\" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand networks' internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, a phenomenon we term incidental polysemanticity. Using a combination of theory and experiments, we show that incidental polysemanticity can arise due to multiple reasons including regularization and neural noise; this incidental polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Our paper concludes by calling for further research quantifying the performance-polysemanticity tradeoff in task-optimized deep neural networks to better understand to what extent polysemanticity is avoidable.", "title_embedding_index": 1252, "title_abs_embedding_index": 1277}, {"title": "Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity", "link_suffix": "/forum?id=lOi6FtIwR8", "link": "https://openreview.net/forum?id=lOi6FtIwR8", "pdf_link": "https://openreview.net/pdf?id=lOi6FtIwR8", "keywords": "model editing, mechanistic interpretability, ai safety, alignment, toxicity, llms", "abstract": "Recent alignment algorithms such as direct preference optimization (DPO) have been developed to improve the safety of large language models (LLMs) by training these models to match human behaviors exemplified by preference data. However, these methods are \nboth computationally intensive and lacking in controllability and transparency, inhibiting their widespread use. Furthermore, these tuning-based methods require large-scale preference data for training and are susceptible to noisy preference data. In this paper, we introduce a tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and demonstrate its effectiveness under the use case of toxicity reduction. Grounded on theory from factor analysis, ProFS is a sample-efficient model editing approach that identifies a toxic subspace in the model parameter space and reduces model toxicity by projecting away the detected subspace. The toxic subspace is identified by extracting preference data embeddings from the language model, and removing non-toxic information from these embeddings. We show that ProFS is more sample-efficient than DPO, further showcasing greater robustness to noisy data. Finally, we attempt to connect tuning based alignment with editing, by establishing both theoretical and empirical connections between ProFS and DPO, showing that ProFS can be interpreted as a denoised version of a single DPO step.", "title_embedding_index": 1253, "title_abs_embedding_index": 1278}, {"title": "Should VLMs be Pre-trained with Image Data?", "link_suffix": "/forum?id=Pj4Aid3XqL", "link": "https://openreview.net/forum?id=Pj4Aid3XqL", "pdf_link": "https://openreview.net/pdf?id=Pj4Aid3XqL", "keywords": "vision language models, pre-training, fine-tuning", "abstract": "Pre-trained LLMs that are further trained with image data perform well on vision-language tasks. \nWhile adding images during a second training phase effectively unlocks this capability, it is unclear how much of a gain or loss this two-step pipeline gives over VLMs which integrate images earlier into the training process. \nTo investigate this, we train models spanning various datasets, scales, image-text ratios, and amount of pre-training done before introducing vision tokens.\nWe then fine-tune these models and evaluate their downstream performance on a suite of vision-language and text-only tasks.\nWe find that pre-training with a mixture of image and text data allows models to perform better on vision-language tasks while maintaining strong performance on text-only evaluations.\nSpecifically, we find that for a 1B model, introducing visual tokens 80% of the way through pre-training results in a 2% average improvement on visual question answering (VQA) over introducing visual tokens to a fully pre-trained model. \nIt does this while retaining performance within 2% of our text-only baseline on downstream text evaluations.", "title_embedding_index": 1254, "title_abs_embedding_index": 1279}, {"title": "DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models", "link_suffix": "/forum?id=VOAMTA8jKu", "link": "https://openreview.net/forum?id=VOAMTA8jKu", "pdf_link": "https://openreview.net/pdf?id=VOAMTA8jKu", "keywords": "Visual Mathematical Benchmark, Vision Language Models", "abstract": "The rapid advancements in Vision-Language Models (VLMs) have shown great potential in tackling mathematical reasoning tasks that involve visual context. Unlike humans who can reliably apply solution steps to similar problems with minor modifications, we found that state-of-the-art VLMs like GPT-4o can consistently fail in these scenarios, revealing limitations in their mathematical reasoning capabilities. In this paper, we investigate themathematical reasoning robustnessin VLMs and evaluate how well these models perform under different variants of the same question, such as changes in visual numerical values or function graphs.\nWhile several vision-based math benchmarks have been developed to assess VLMs' problem-solving capabilities, these benchmarks contain only static sets of problems and cannot easily evaluate mathematical reasoning robustness.\nTo fill this gap, we introduceDynaMath, a dynamic visual math benchmark designed for in-depth assessment of VLMs.DynaMathincludes 501 high-quality, multi-topicseedquestions,each represented as a Python program. Those programs are carefully designed and annotated to enable the automatic generation of a much larger set ofconcretequestions, including many different types of visual and textual variations.DynaMathallows us to evaluate the generalization ability of VLMs, by assessing their performance under varying input conditions of a seed question. We evaluated 14 state-of-the-art VLMs with 5,010 generated concrete questions (10 per seed question). Our results show that the worst-case model accuracy, defined as the percentage of correctly answered seed questions in all 10 variants, is significantly lower than the average-case accuracy. In addition, many models show high consistency in answering these questions -- the incorrectness of a certain variant of a seed question is not only due to inherent randomness. Our analysis emphasizes the need to study the robustness of VLMs' reasoning abilities, andDynaMathprovides valuable insights to guide the development of more reliable models for mathematical reasoning.", "title_embedding_index": 1255, "title_abs_embedding_index": 1280}, {"title": "MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis", "link_suffix": "/forum?id=Mv3GAYJGcW", "link": "https://openreview.net/forum?id=Mv3GAYJGcW", "pdf_link": "https://openreview.net/pdf?id=Mv3GAYJGcW", "keywords": "MetaDesigner", "abstract": "MetaDesigner revolutionizes artistic typography synthesis by leveraging the strengths of Large Language Models (LLMs) to drive a design paradigm centered around user engagement. At the core of this framework lies a multi-agent system comprising the Pipeline, Glyph, and Texture agents, which collectively enable the creation of customized WordArt, ranging from semantic enhancements to the imposition of complex textures. MetaDesigner incorporates a comprehensive feedback mechanism that harnesses insights from multimodal models and user evaluations to refine and enhance the design process iteratively. Through this feedback loop, the system adeptly tunes hyperparameters to align with user-defined stylistic and thematic preferences, generating WordArt that not only meets but exceeds user expectations of visual appeal and contextual relevance. Empirical validations highlight MetaDesigner's capability to effectively serve diverse WordArt applications, consistently producing aesthetically appealing and context-sensitive results.", "title_embedding_index": 1256, "title_abs_embedding_index": 1281}, {"title": "Scalable Back-Propagation-Free Training of Optical Physics-Informed Neural Networks", "link_suffix": "/forum?id=XLDaepymR5", "link": "https://openreview.net/forum?id=XLDaepymR5", "pdf_link": "https://openreview.net/pdf?id=XLDaepymR5", "keywords": "Optical Neural Networks, Physics-Informed Neural Networks, On-Chip Learning, Scalability, Hardware-Software Co-Design", "abstract": "Physics-informed neural networks (PINNs) have shown promise in solving partial differential equations (PDEs), with growing interest in their energy-efficient, real-time training on edge devices. Photonic computing offers a potential solution due to its high operation speed.\nHowever, the lack of photonic memory and the large footprint of current photonic devices prevent training realistic-size PINNs on photonic chips. This paper proposes a completely back-propagation-free (BP-free) and highly salable framework to enable training real-size PINNs on silicon photonics platforms. Our approach involves three key innovations: (1) a sparse-grid Stein derivative estimator to avoid the BP in the loss evaluation of a PINN, (2) a dimension-reduced zeroth-order optimization via tensor-train decomposition to achieve better scalability and convergence in BP-free training, and (3) a scalable on-chip photonic PINN training accelerator design using photonic tensor cores. We validate the performance of our numerical methods in both low- and high-dimensional PDE benchmarks. Through circuit simulation based on real device parameters, we further demonstrate the significant performance benefit (e.g., real-time training, huge chip area reduction) of our photonic accelerator. Our framework addresses the fundamental challenges of photonic AI and will enable real-time training of real-size PINNs on photonic chips.", "title_embedding_index": 1257, "title_abs_embedding_index": 1282}, {"title": "Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters", "link_suffix": "/forum?id=uAtDga3q0r", "link": "https://openreview.net/forum?id=uAtDga3q0r", "pdf_link": "https://openreview.net/pdf?id=uAtDga3q0r", "keywords": "Large Language Models, Adaptive compute, Rank adapters, Neuron adapters", "abstract": "Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in  modern Transformer architectures.", "title_embedding_index": 1258, "title_abs_embedding_index": 1283}, {"title": "EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING", "link_suffix": "/forum?id=jCDF7G3LpF", "link": "https://openreview.net/forum?id=jCDF7G3LpF", "pdf_link": "https://openreview.net/pdf?id=jCDF7G3LpF", "keywords": "JailBreak, AI Security, LLM Vunlnerability", "abstract": "Content warning: This paper contains examples of harmful language and content. As the capabilities of large language models (LLMs) continue to expand,\nthe risk of these models being manipulated or \u201cjailbroken\u201d by malicious users\nincreases significantly. Traditional AI safety measures primarily focus on algorithmic defenses, but there is a growing need to explore more sophisticated attack\nstrategies that consider the dynamic interactions between human users and LLMs.\nThis paper introduces a novel approach to jailbreaking LLMs through the use of\n\u201cSequence of Contexts\u201d (SoC) attacks, wherein sequences of context-switching queries (CSQs) are leveraged to gradually alter the context remembered by the\nmodel and steer it towards generating harmful responses. We employ a multi armed bandit (MAB) framework to automate the SoC attack by balancing exploration and exploitation of different CSQs to maximize the likelihood of a successful jailbreak. We achieve an Attack Success Rate (ASR) of over 95%, with our\nASRs growing with the increase in the attack sequence lengths. Furthermore, this\nresearch provides rigorous theoretical foundations for the proposed method by\nderiving key bounds on the expected sequence length until the optimal CSQ category that successfully jailbreaks the LLM is identified. This paper also presents a\ntheoretical analysis of total reward convergence in jailbreaking LLMs using CSQ\ncategories. The key contributions of this paper are: (i) the creation of a dataset of\nCSQs, (ii) the proposition of a novel strategy to automate SoC-based jailbreaking\nattacks on LLMs, utilizing the MAB framework, and (iii) an in-depth theoretical analysis of the upper bounds on the expected sequence length for identifying\noptimal attack strategies and the convergence of the total reward.", "title_embedding_index": 1259, "title_abs_embedding_index": 1284}, {"title": "A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts", "link_suffix": "/forum?id=TrKRpaOk8y", "link": "https://openreview.net/forum?id=TrKRpaOk8y", "pdf_link": "https://openreview.net/pdf?id=TrKRpaOk8y", "keywords": "Long-Context LLM, Efficient LLM, Context Extension, KV Cache Reduction", "abstract": "Training and serving long-context large language models (LLMs) incurs substantial overhead. \nTo address this, two critical steps are often required: a pretrained LLM typically undergoes a separate stage for context length extension by training on long-context data, followed by architectural modifications to reduce the overhead of KV cache during serving. \nThis paper argues that integrating length extension with a GPU-friendly KV cache reduction architecture not only reduces training overhead during length extension, but also achieves better long-context performance. \nThis leads to our proposed LongGen, which finetunes a pretrained LLM into an efficient architecture during length extension. \nLongGen builds on three key insights: \n(1) Sparse attention patterns, such as window attention (attending to recent tokens), attention sink (initial ones), and blockwise sparse attention (strided token blocks) are well-suited for building efficient long-context models, primarily due to their GPU-friendly memory access patterns, enabling efficiency gains not just theoretically but in practice as well. \n(2) It is essential for the model to have direct access to all tokens. \nA hybrid architecture with 1/3 full attention layers and 2/3 efficient ones achieves a balanced trade-off between efficiency and long-context performance.\n(3) Lightweight training on 5B long-context data is sufficient to extend the hybrid model's context length from 4K to 128K.We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its effectiveness across different scales. \nDuring training with 128K-long contexts, LongGen achieves 1.55x training speedup and reduces wall-clock time by 36%, compared to a full-attention baseline. \nDuring inference, LongGen reduces KV cache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding speedup.\nCompared to baselines that apply KV-cache reduction techniques to full-attention long-context LLMs, LongGen achieves substantially stronger performance not only on the Needle-in-a-Haystack retrieval task, but also on more challenging long-context reasoning tasks, including BABILong and RULER.", "title_embedding_index": 1260, "title_abs_embedding_index": 1285}, {"title": "Denoising Diffusion Causal Discovery", "link_suffix": "/forum?id=Z756zcjNcC", "link": "https://openreview.net/forum?id=Z756zcjNcC", "pdf_link": "https://openreview.net/pdf?id=Z756zcjNcC", "keywords": "Denoising Diffusion Models, Causal Discovery, Causal Reasoning, Diffusion Models, Graph Neural Networks", "abstract": "A common theme across multiple disciplines of science is to understand the underlying dependencies between variables from observational data. Such dependencies are often modeled as Bayesian Network (BNs), which by definition are Directed Acyclic Graphs (DAGs). Recent advancements, such as NOTEARS and DAG-GNN, have focused on formulating continuous DAG constraints and learning DAGs via continuous optimization. However, these methods often have scalability issues and face challenges when applied to real world data. In this paper, we propose Denoising Diffusion Causal Discovery (DDCD), a new learning framework that leverages Denoising Diffusion Probabilistic Models (DDPMs) for causal structural learning. Using the denoising objective, our method allows the model to explore a wider range of noise in the data and effectively captures both linear and nonlinear dependencies. It also has reduced complexity and is more suitable for inference of larger networks. To accommodate potential feedback loops in biological networks, we propose a k-hop DAG constraint. Additionally, we suggest using fixed-size bootstrap sampling to ensure similar training performance across varying dataset sizes. Our experiments on synthetic data demonstrate that DDCD achieves consistent competitive performance compared to existing methods while noticeably reducing computation time.  We also show that DDCD can generate trustworthy networks from real-world datasets.", "title_embedding_index": 1261, "title_abs_embedding_index": 1286}, {"title": "What Secrets Do Your Manifolds Hold? Understanding the Local Geometry of Generative Models", "link_suffix": "/forum?id=etif9j1CnG", "link": "https://openreview.net/forum?id=etif9j1CnG", "pdf_link": "https://openreview.net/pdf?id=etif9j1CnG", "keywords": "Geometry, Diffusion models, VAE, Generative Models, Guidance, Memorization, Out-of-Distribution Detection", "abstract": "Deep Generative Models are frequently used to learn continuous representations of complex data distributions using a finite number of samples. For any generative model, including pre-trained foundation models with GAN, Transformer or Diffusion architectures, generation performance can vary significantly based on which part of the learned data manifold is sampled. In this paper we study the post-training local geometry of the learned manifold and its relationship to generation outcomes for models ranging from toy settings to the latent decoder of the near state-of-the-art Stable Diffusion 1.4 Text-to-Image model. Building on the theory of continuous piecewise-linear (CPWL) generators, we characterize the local geometry in terms of three geometric descriptors - scaling ($\\psi$), rank ($\\nu$), and complexity ($\\delta$). We provide quantitative and qualitative evidence showing that for a given latent, the local descriptors are indicative of generation aesthetics, artifacts, diversity, and memorization. Finally we demonstrate that training a reward model using the local geometry allows us to control the log-likelihood of a generated sample under the learned distribution, and improve the qualitative aspects of an image.", "title_embedding_index": 1262, "title_abs_embedding_index": 1287}, {"title": "L-WISE: Boosting human image category learning through model-based image selection and enhancement", "link_suffix": "/forum?id=AoIKgHu9Si", "link": "https://openreview.net/forum?id=AoIKgHu9Si", "pdf_link": "https://openreview.net/pdf?id=AoIKgHu9Si", "keywords": "Human-aligned models, robust neural networks, visual perception, perceptual learning, medical machine learning", "abstract": "The currently leading artificial neural network (ANN) models of the visual ventral stream -- which are derived from a combination of performance optimization and robustification methods -\u2013 have demonstrated a remarkable degree of behavioral alignment with humans on visual categorization tasks. Extending upon previous work, we show that not only can these models guide image perturbations that change the induced human category percepts, but they also can enhance human ability to accurately report the original ground truth. Furthermore, we find that the same models can also be used out-of-the-box to predict the proportion of correct human responses to individual images, providing a simple, human-aligned estimator of the relative difficulty of each image. Motivated by these observations, we propose to augment visual learning in humans in a way that improves human categorization accuracy at test time. Our learning augmentation approach consists of (i) selecting images based on their model-estimated recognition difficulty, and (ii) using image perturbations that aid recognition for novice learners. We find that combining these model-based strategies gives rise to test-time categorization accuracy gains of 33-72% relative to control subjects without these interventions, despite using the same number of training feedback trials. Surprisingly, beyond the accuracy gain, the training time for the augmented learning group was also shorter by 20-23%. We demonstrate the efficacy of our approach in a fine-grained categorization task with natural images, as well as tasks in two clinically relevant image domains -- histology and dermoscopy -- where visual learning is notoriously challenging. To the best of our knowledge, this is the first application of ANNs to successfully increase visual learning performance in humans, and especially robustly across varied image domains.", "title_embedding_index": 1263, "title_abs_embedding_index": 1288}, {"title": "Hard-Constrained Neural Networks with Universal Approximation Theorem", "link_suffix": "/forum?id=3Imf21Jvwh", "link": "https://openreview.net/forum?id=3Imf21Jvwh", "pdf_link": "https://openreview.net/pdf?id=3Imf21Jvwh", "keywords": "constrained optimization, universal approximation, surrogate models", "abstract": "Incorporating prior knowledge of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and ensures trustworthy predictions. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction---a critical requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Specifically, we encode affine and convex hard constraints, dependent on both inputs and outputs, by appending a differentiable projection layer to the network\u2019s output. This architecture allows unconstrained optimization of the network parameters using standard algorithms while ensuring constraint satisfaction by construction. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: fitting functions under constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems.", "title_embedding_index": 1264, "title_abs_embedding_index": 1289}, {"title": "Predictive Uncertainty Quantification for Bird's Eye View Segmentation: A Benchmark and Novel Loss Function", "link_suffix": "/forum?id=k3y0oyK7sn", "link": "https://openreview.net/forum?id=k3y0oyK7sn", "pdf_link": "https://openreview.net/pdf?id=k3y0oyK7sn", "keywords": "Uncertainty Quantification, Evidential Deep Learning, Bird's Eye View (BEV) Segmentation", "abstract": "The fusion of raw sensor data to create a Bird's Eye View (BEV) representation is critical for autonomous vehicle planning and control. Despite the growing interest in using deep learning models for BEV semantic segmentation, anticipating segmentation errors and enhancing the explainability of these models remain under-explored. This paper introduces a comprehensive benchmark for predictive uncertainty quantification in BEV segmentation, evaluating multiple approaches across three popular datasets and two representative backbones. Our study focuses on the effectiveness of the quantified uncertainty in detecting misclassified and out-of-distribution (OOD) pixels, while also improving model calibration. Through empirical analysis, we uncover challenges in existing uncertainty quantification methods and demonstrate the potential of evidential deep learning techniques, which capture both aleatoric and epistemic uncertainty. To address these challenges, we propose a novel loss function, Uncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly imbalanced data, along with a simple uncertainty-scaling regularization term that improve both uncertainty quantification and model calibration for BEV segmentation.", "title_embedding_index": 1265, "title_abs_embedding_index": 1290}, {"title": "DiscoveryBench: Towards Data-Driven Discovery with Large Language Models", "link_suffix": "/forum?id=vyflgpwfJW", "link": "https://openreview.net/forum?id=vyflgpwfJW", "pdf_link": "https://openreview.net/pdf?id=vyflgpwfJW", "keywords": "data-driven discovery, data analysis, large language models, hypothesis generation, hypothesis verification", "abstract": "Can the rapid advances in code generation, function calling, and data analysis using large language models (LLMs) help automate the search and verification of hypotheses purely from a set of provided datasets? To evaluate this question, we present DiscoveryBench, the first comprehensive benchmark that formalizes the multi-step process of data-driven discovery. The benchmark is designed to systematically assess current model capabilities in discovery tasks and provide a useful resource for improving them. Our benchmark contains 264 tasks collected across 6 diverse domains, such as sociology and engineering, by manually deriving discovery workflows from published papers to approximate the real-world challenges faced by researchers, where each task is defined by a dataset, its metadata, and a discovery goal in natural language. We additionally provide 903 synthetic tasks to conduct controlled evaluations on data-driven workflows that are not covered in the manually collected split. Furthermore, our structured formalism of data-driven discovery enables a facet-based evaluation that provides useful insights into different failure modes. We evaluate several popular LLM-based reasoning frameworks using both open and closed LLMs as baselines on DiscoveryBench and find that even the best system scores only 25%. Our benchmark, thus, illustrates the challenges in autonomous data-driven discovery and serves as a valuable resource for the community to make progress.", "title_embedding_index": 1266, "title_abs_embedding_index": 1291}, {"title": "Maximum Likelihood Estimation for Flow Matching by Direct Second-order Trace Objective", "link_suffix": "/forum?id=7ZUUNMjM9T", "link": "https://openreview.net/forum?id=7ZUUNMjM9T", "pdf_link": "https://openreview.net/pdf?id=7ZUUNMjM9T", "keywords": "flow matching, generative models", "abstract": "Flow matching, one of the attractive deep generative models, has recently been used in wide modality.\nDespite the remarkable success, the flow matching objective of the vector field is insufficient for maximum likelihood estimation. \nPrevious works show that adding the vector field's high-order gradient objectives further improves likelihood.\nHowever, their method only minimizes the upper bound of the high-order objectives, hence it is not guaranteed that the objectives themselves are indeed minimized, resulting in likelihood maximization becoming less effective.\nIn this paper, we propose a method to directly minimize the high-order objective.\nSince our method guarantees that the objective is indeed minimized, our method is expected to improve likelihood compared to previous works.\nWe verify that our proposed method achieves better likelihood in practice through experiments on 2D synthetic datasets and high-dimensional image datasets.", "title_embedding_index": 1267, "title_abs_embedding_index": 1292}, {"title": "Algorithmic Phases of In-Context Learning", "link_suffix": "/forum?id=XgH1wfHSX8", "link": "https://openreview.net/forum?id=XgH1wfHSX8", "pdf_link": "https://openreview.net/pdf?id=XgH1wfHSX8", "keywords": "In-Context Learning, Circuit Competition, Markov Chains, Training Dynamics", "abstract": "In-Context Learning (ICL) has significantly expanded the general-purpose nature of large language models, allowing them to adapt to new tasks using merely the input context. While a series of papers analyzing synthetic domains have established a rich phenomenology of ICL, the use of relatively distinct setups makes it unclear how general the reported insights are. To address this, we propose a synthetic sequence modeling task defined by a finite set of Markov chains that simultaneously captures most well-known results on ICL, e.g., the task retrieval vs. learning dichotomy and emergence of induction heads, hence enabling a unified framework for studying the concept. As we show, the proposed task offers several new insights, such as an explanation for ICL's transient nature, and demonstrates subtleties in ICL's known phenomenology. For example, we find varying experimental conditions (e.g., data diversity) drives transitions between distinct algorithmic solutions, such as unigram vs. bigram models and Bayesian vs. non-Bayesian approaches, implying ICL is best thought of as a mixture of different algorithms, each with its own peculiarities, instead of a monolithic capability.", "title_embedding_index": 1268, "title_abs_embedding_index": 1293}, {"title": "Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents", "link_suffix": "/forum?id=ZMtq9pYw5e", "link": "https://openreview.net/forum?id=ZMtq9pYw5e", "pdf_link": "https://openreview.net/pdf?id=ZMtq9pYw5e", "keywords": "Large Language Models; Graph Reasoning; Multi-Agents", "abstract": "Recent research has explored the use of Large Language Models (LLMs) for tackling complex graph reasoning tasks. However, due to the intricacies of graph structures and the inherent limitations of LLMs in handling long text, current approaches often fail to deliver satisfactory accuracy, even on small-scale graphs and simple tasks. To address these challenges, we introduce GraphAgent-Reasoner, a fine-tuning-free framework that utilizes a multi-agent collaboration strategy for explicit and precise graph reasoning. Inspired by distributed graph computation theory, our framework decomposes graph problems into smaller, node-centric tasks that are distributed among multiple agents. The agents collaborate to solve the overall problem, significantly reducing the amount of information and complexity handled by a single LLM, thus enhancing the accuracy of graph reasoning. By simply increasing the number of agents, GraphAgent-Reasoner can efficiently scale to accommodate larger graphs with over 1,000 nodes. Evaluated on the GraphInstruct dataset, our framework demonstrates near-perfect accuracy on polynomial-time graph reasoning tasks, significantly outperforming the best available models, both closed-source and fine-tuned open-source variants. Our framework also demonstrates the capability to handle real-world graph reasoning applications such as webpage importance analysis.", "title_embedding_index": 1269, "title_abs_embedding_index": 1294}, {"title": "Optimizing(L0,L1)-Smooth Functions by Gradient Methods", "link_suffix": "/forum?id=GQ1Tc3vHbt", "link": "https://openreview.net/forum?id=GQ1Tc3vHbt", "pdf_link": "https://openreview.net/pdf?id=GQ1Tc3vHbt", "keywords": "$(L_0, L_1)$-smoothness, gradient methods, convex optimization, worst-case complexity bounds, acceleration, Polyak stepsizes, nonconvex optimization", "abstract": "We study gradient methods for solving an optimization problem with an $(L_0, L_1)$-smooth objective function. This problem class generalizes that of Lipschitz-smooth problems and has gained interest recently, as it captures a broader range of machine learning applications.\nWe provide novel insights on the properties of this function class and develop a general framework for analyzing optimization methods \n for $(L_0, L_1)$-smooth function in a principled manner.\nWhile our convergence rate estimates recover existing results for minimizing the gradient norm for nonconvex problems,\nour approach allows us to significantly improve the current state-of-the-art complexity results in the case of convex problems. \nWe show that both the gradient method with Polyak stepsizes and the normalized gradient method, without any knowledge of the parameters $L_0$ and $L_1$, achieve the same complexity bounds as the method with the knowledge of these constants. \nIn addition to that, we show that a carefully chosen accelerated gradient method can be applied to $(L_0, L_1)$-smooth functions, further improving previously known results. \nIn all cases, the efficiency bounds we establish do not have an exponential dependency on $L_0$ or $L_1$, and do not depend on the initial gradient norm.", "title_embedding_index": 1270, "title_abs_embedding_index": 1295}, {"title": "Larger Language Models Provably Generalize Better", "link_suffix": "/forum?id=MF7ljU8xcf", "link": "https://openreview.net/forum?id=MF7ljU8xcf", "pdf_link": "https://openreview.net/pdf?id=MF7ljU8xcf", "keywords": "generalization bounds, language models, scaling laws", "abstract": "Why do larger language models generalize better? To explore this question, we develop generalization bounds on the pretraining objective of large language models (LLMs) in the compute-optimal regime, as described by the Chinchilla scaling laws. We introduce a novel, fully empirical Freedman-type martingale concentration inequality that tightens existing bounds by accounting for the variance of the loss function. The generalization bound can be broken into three contributions: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate. As language models are scaled up, the number of parameters per data point stays constant; however, both the loss variance and the quantization error decrease, implying that larger models should have \\emph{smaller} generalization gaps. We examine why larger models tend to be more quantizable from an information theoretic perspective, showing that the rate at which they can integrate new information grows slower than their capacity on the compute optimal frontier. From these findings we produce a scaling law for the generalization gap, showing that our bounds decrease in a predictable way.", "title_embedding_index": 1271, "title_abs_embedding_index": 1296}, {"title": "One Training Fits All: Generalized Data Condensation via Mixture-of-Information Bottleneck Guidance", "link_suffix": "/forum?id=XeRvg7GQH4", "link": "https://openreview.net/forum?id=XeRvg7GQH4", "pdf_link": "https://openreview.net/pdf?id=XeRvg7GQH4", "keywords": "Dataset Condensation", "abstract": "Data condensation (DC) technologies are widely used in buffer-constrained scenarios to reduce the memory demand of training samples and maintain  DNN training performance. However, due to the storage constraint of deployment devices and the high energy costs of condensation procedure, synthetic datasets generated by DC often have inferior performance in terms of training efficiency and scalability, which greatly limits its practical application on various edge devices. \nThis dilemma arises due to two reasons: i) existing state-of-the-art (SoTA) data condensation approaches that update synthetic datasets by intuitively matching intermediate training outputs (e.g.,  gradients, features and distributions) between real datasets and synthetic datasets without improving their representational information capabilities from the perspective of the useful information contained. ii) DC lacks sufficient consideration for the heterogeneity of storage constraints among various edge devices, which will result in large training overheads (i.e., consumption or storage). \nTo tackle the above issue, We propose a novel method named Mixture-of-Information Bottleneck Dataset Condensation (MIBDC), which employs information bottlenecks from synthetic datasets with various Image Per Class (IPC) numbers to improve the overall DC generalization and scalability. \nSpecifically, in this paper, the following two phenomena are found: i) The quality of synthetic datasets improves with increased synthetic dataset quantity. ii) The smaller the number of synthetic datasets, the earlier they can reach the convergence peak.\nBased on the above two findings, this paper proposes that i) large synthetic datasets can guide the better convergence of smaller ones. ii)  information contained in  synthetic datasets with different IPC numbers can play a collaborative role in the guidance of dataset condensation generalization.\nComprehensive experimental results on three well-known datasets show that, compared with state-of-the-art dataset condensation methods, MIBDC can not only enhance the generalization performance of trained models but also achieve superior scalability.", "title_embedding_index": 1272, "title_abs_embedding_index": 1297}, {"title": "EDU-RAG: A RAG Benchmark with Web-enhanced Content in Education Domain. Can RAG Help AI Tutor?", "link_suffix": "/forum?id=a2rSx6t4EV", "link": "https://openreview.net/forum?id=a2rSx6t4EV", "pdf_link": "https://openreview.net/pdf?id=a2rSx6t4EV", "keywords": "Large Language Models (LLM), Retrieval Augmented Generation (RAG), Search", "abstract": "Hallucination has been a notable concern in leveraging Large Language Models (LLMs). Retrieval-Augmented Generation (RAG) is a popular method in addressing that and in maintaining context and coherence in generated outputs and including customized knowledge. In this paper, we aim to create an updated benchmark dataset using middle-school science-domain-ed textbook-question-answering, and web search results, and to evaluate the performances of different LLMs incl. GPT-4o, Llama2-7b, Llama3-8b, with and without RAG. Basically, we want to evaluate whether RAG would improve hallucination problem caused by pre-trained biased LLM model knowledge, or drawing from irrelevant retrieved knowledge despite of access to relevant information. The dataset and method can be used to evaluate different RAG methods.", "title_embedding_index": 1273, "title_abs_embedding_index": 1298}, {"title": "Entering Real Social World! Benchmarking the Theory of Mind and Socialization Capabilities of LLMs from a First-person Perspective", "link_suffix": "/forum?id=b1vVm6Ldrd", "link": "https://openreview.net/forum?id=b1vVm6Ldrd", "pdf_link": "https://openreview.net/pdf?id=b1vVm6Ldrd", "keywords": "Theory of Mind, Socialization, First-person Perspective", "abstract": "In the social world, humans possess the capability to infer and reason about others' mental states (such as emotions, beliefs, and intentions), known as Theory of Mind (ToM). Simultaneously, humans' own mental states evolve in response to social situations, a capability we refer to as \\textit{socialization}. Together, these capabilities form the foundation of human social interaction. In the era of artificial intelligence (AI), especially with the development of large language models (LLMs), we raise intriguing questions: How do LLMs perform in terms of ToM and \\textit{socialization} capabilities? And more broadly, can these AI models truly enter and navigate the real social world? Existing research evaluating LLMs' ToM and \\textit{socialization} capabilities by positioning LLMs as passive observers from a third-person perspective, rather than as active participants. However, compared to the third-person perspective, observing and understanding the world from an ego-centric first-person perspective is a natural approach for both humans and AI agents.  The ToM and \\textit{socialization} capabilities of LLMs from a first-person perspective, a crucial attribute for advancing embodied AI agents, remain unexplored. To answer the aforementioned questions and bridge the research gap, we introduce \\textit{EgoSocialArena}, a novel framework designed to evaluate and investigate the ToM and \\textit{socialization} capabilities of LLMs from a first-person perspective. It encompasses two evaluation environments: static environment and interactive environment, with seven scenarios: Daily Life, Counterfactual, New World, Blackjack, Number Guessing, and Limit Texas Hold\u2019em, totaling 2,195 data entries. With \\textit{EgoSocialArena}, we have conducted a comprehensive evaluation of nine advanced LLMs and observed some key insights regarding the future development of LLMs as well as the capabilities levels of the most advanced LLMs currently available.", "title_embedding_index": 1274, "title_abs_embedding_index": 1299}]