[{"title": "Zer0-Jack: A memory-efficient gradient-based jailbreaking method for black box Multi-modal Large Language Models", "link_suffix": "/forum?id=2yqAzFPT4F", "link": "https://openreview.net/forum?id=2yqAzFPT4F", "pdf_link": "https://openreview.net/pdf?id=2yqAzFPT4F", "keywords": "Jailbreaking attacks, Black-box MLLMs, Zeroth-order optimization", "abstract": "Jailbreaking methods, which induce Multi-modal Large Language Models (MLLMs) to output harmful responses, raise significant safety concerns. Among these methods, gradient-based approaches, which use gradients to generate malicious prompts, have been widely studied due to their high success rates in white-box settings, where full access to the model is available. However, these methods have notable limitations: they require white-box access, which is not always feasible, and involve high memory usage. To address scenarios where white-box access is unavailable, attackers often resort to transfer attacks. In transfer attacks, malicious inputs generated using white-box models are applied to black-box models, but this typically results in reduced attack performance.\nTo overcome these challenges, we propose Zer0-Jack, a method that bypasses the need for white-box access by leveraging zeroth-order optimization. We propose patch coordinate descent to efficiently generate malicious image inputs to directly attack black-box MLLMs, which significantly reduces memory usage further. Through extensive experiments, Zer0-Jack achieves a high attack success rate across various models, surpassing previous transfer-based methods and performing comparably with existing white-box jailbreak techniques. Notably, Zer0-Jack achieves a 95% attack success rate on MiniGPT-4 with the Harmful Behaviors Multi-modal Dataset, demonstrating its effectiveness. Additionally, we show that Zer0-Jack can directly attack commercial MLLMs such as GPT-4o. Codes are provided in the supplement.", "title_embedding_index": 8450, "title_abs_embedding_index": 8475}, {"title": "Needle In A Video Haystack: A Scalable  Synthetic Evaluator for Video MLLMs", "link_suffix": "/forum?id=ZJo6Radbqq", "link": "https://openreview.net/forum?id=ZJo6Radbqq", "pdf_link": "https://openreview.net/pdf?id=ZJo6Radbqq", "keywords": "video MLLM", "abstract": "Video understanding is a crucial next step for multimodal large language models (MLLMs).\nVarious benchmarks are introduced for better evaluating the MLLMs.\nNevertheless, current video benchmarks are still inefficient for evaluating video models during iterative development due to the high cost of constructing datasets and the difficulty in isolating specific skills.\nIn this paper, we propose VideoNIAH (Video Needle in A Haystack), a benchmark construction framework through synthetic video generation. \nVideoNIAH decouples video content from their query-responses by inserting unrelated visual 'needles' into original videos. \nThe framework automates the generation of query-response pairs using predefined rules, minimizing manual labor.  The queries focus on specific aspects of video understanding, enabling more skill-specific evaluations. The separation between video content and the queries also allow for increased video variety and evaluations across different lengths.\nUtilizing VideoNIAH, we compile a video benchmark, VNBench, which includes tasks such as retrieval, ordering, and counting to evaluate three key aspects of video understanding: temporal perception, chronological ordering, and spatio-temporal coherence. We conduct a comprehensive evaluation of both proprietary and open-source models, uncovering significant differences in their video understanding capabilities across various tasks. Additionally, we perform an in-depth analysis of the test results and model configurations. Based on these findings, we provide some advice for improving video MLLM training, offering valuable insights to guide future research and model development.", "title_embedding_index": 8451, "title_abs_embedding_index": 8476}, {"title": "TPI-LLM: Serving 70B-scale LLMs Efficiently on Low-resource Edge Devices", "link_suffix": "/forum?id=0cadcLKbt7", "link": "https://openreview.net/forum?id=0cadcLKbt7", "pdf_link": "https://openreview.net/pdf?id=0cadcLKbt7", "keywords": "DML Systems, Edge LLM Serving, Tensor Parallelism, Memory Scheduling", "abstract": "Large model inference is shifting from cloud to edge due to concerns about the privacy of user interaction data. However, edge devices often struggle with limited computing power, memory, and bandwidth, requiring collaboration across multiple devices to run and speed up LLM inference. Pipeline parallelism, the mainstream solution, is inefficient for single-user scenarios, while tensor parallelism struggles with frequent communications. In this paper, we argue that tensor parallelism can be more effective than pipeline on low-resource devices, and present a compute- and memory-efficient tensor parallel inference system, named TPI-LLM, to serve 70B-scale models. TPI-LLM keeps sensitive raw data local in the users' devices and introduces a sliding window memory scheduler to dynamically manage layer weights during inference, with disk I/O latency overlapped with the computation and communication. This allows larger models to run smoothly on memory-limited devices. We analyze the communication bottleneck and find that link latency, not bandwidth, emerges as the main issue, so a star-based allreduce algorithm is implemented. Through extensive experiments on both emulated and real testbeds, TPI-LLM demonstrated over 80% less time-to-first-token and token latency compared to Accelerate, and over 90% compared to Transformers and Galaxy, while cutting the peak memory footprint of Llama 2-70B by 90%, requiring only 3.1 GB of memory for 70B-scale models.", "title_embedding_index": 8452, "title_abs_embedding_index": 8477}, {"title": "Lean-ing on Quality: How High-Quality Data Beats Diverse Multilingual Data in AutoFormalization", "link_suffix": "/forum?id=Qdp7hlenr6", "link": "https://openreview.net/forum?id=Qdp7hlenr6", "pdf_link": "https://openreview.net/pdf?id=Qdp7hlenr6", "keywords": "llm, large language models, autoformalization, lean", "abstract": "Autoformalization, the process of transforming informal mathematical language into formal specifications and proofs remains a difficult task for state-of-the-art (large) language models. Existing works point to competing explanations for the performance gap. On one hand, large language models exhibit exceptional performance on translation tasks, suggesting their significant potential for autoformalization.  On the other hand, the quantitative reasoning capabilities of standard language models remain limited, leading to suboptimal performance on autoformalization and the subsequent task of formal theorem proving. To this end, we introduce a novel methodology that leverages backtranslation with hand-curated prompts to enhance the mathematical capabilities of language models, particularly addressing the challenge posed by the scarcity of labeled data.  Specifically, we evaluate three primary variations of this strategy: (1) on-the-fly (online) backtranslation, (2) distilled (offline) backtranslation with few-shot amplification, and (3) line-by-line proof analysis integrated with proof state information. Each variant is designed to optimize data quality over quantity, focusing on the high fidelity of generated proofs rather than sheer data scale. Our findings provide evidence that employing our proposed approaches to generate synthetic data, which prioritizes quality over volume, improves the autoformalization performance of LLMs as measured by standard benchmarks such as ProofNet. Crucially, our approach outperforms pretrained models using a minimal number of tokens. We also show, through strategic prompting and backtranslation, that our approaches surpass the performance of finetuning with extensive multilingual datasets such as MMA on ProofNet with only 1/150th of the tokens. Taken together, our methods show a promising new approach to significantly reduce the resources required to formalize proofs, thereby accelerating AI for math.", "title_embedding_index": 8453, "title_abs_embedding_index": 8478}, {"title": "Open Vocabulary Panoptic Segmentation With Retrieval Augmentation", "link_suffix": "/forum?id=0jUeqlQxMi", "link": "https://openreview.net/forum?id=0jUeqlQxMi", "pdf_link": "https://openreview.net/pdf?id=0jUeqlQxMi", "keywords": "Panoptic Segmentation, Open Vocabulary, Retrieval Augmentation", "abstract": "Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.", "title_embedding_index": 8454, "title_abs_embedding_index": 8479}, {"title": "Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions?", "link_suffix": "/forum?id=M8gXSFGkn2", "link": "https://openreview.net/forum?id=M8gXSFGkn2", "pdf_link": "https://openreview.net/pdf?id=M8gXSFGkn2", "keywords": "egocentric, hand-object interaction, video-language model, contrastive learning, multimodal representation learning", "abstract": "Egocentric video-language pretraining is a crucial step in advancing the understanding of hand-object interactions in first-person scenarios. Despite successes on existing testbeds, we find that current EgoVLMs can be easily misled by simple modifications, such as changing the verbs or nouns in interaction descriptions, with models struggling to distinguish between these changes. This raises the question: Do EgoVLMs truly understand hand-object interactions?'' To address this question, we introduce a benchmark called $\\textbf{EgoHOIBench}$, revealing the performance limitation of current egocentric models when confronted with such challenges. We attribute this performance gap to insufficient fine-grained supervision and the greater difficulty EgoVLMs experience in recognizing verbs compared to nouns. To tackle these issues, we propose a novel asymmetric contrastive objective named $\\textbf{EgoNCE++}$. For the video-to-text objective, we enhance text supervision by generating negative captions using large language models or leveraging pretrained vocabulary for HOI-related word substitutions.  For the text-to-video objective, we focus on preserving an object-centric feature space that clusters video representations based on shared nouns. Extensive experiments demonstrate that EgoNCE++ significantly enhances EgoHOI understanding, leading to improved performance across various EgoVLMs in a range of tasks such as multi-instance retrieval, action recognition, and temporal understanding. Our code is available athttps://anonymous.4open.science/r/EgoNCEpp", "title_embedding_index": 8455, "title_abs_embedding_index": 8480}, {"title": "GFlowNets Need Automorphism Correction for Unbiased Graph Generation", "link_suffix": "/forum?id=BkR4QG4azn", "link": "https://openreview.net/forum?id=BkR4QG4azn", "pdf_link": "https://openreview.net/pdf?id=BkR4QG4azn", "keywords": "GFlowNet, graph generation, molecule optimization", "abstract": "Generative Flow Networks (GFlowNets) are generative models capable of producing graphs. While GFlowNet theory guarantees that a fully trained model samples from an unnormalized target distribution, computing state transition probabilities remains challenging due to the presence of equivalent actions that lead to the same state. In this paper, we analyze the properties of equivalent actions in the context of graph generation tasks and propose efficient solutions to address this problem. Our theoretical analysis reveals that naive implementations, which ignore equivalent actions, introduce systematic bias in the sampling distribution for both atom-based and fragment-based graph generation. This bias is directly related to the number of symmetries in a graph, a factor that is particularly critical in applications such as drug discovery, where symmetry plays a key role in molecular structure and function. Experimental results demonstrate that a simple reward-scaling technique not only enables the generation of graphs that closely match the target distribution but also facilitates the sampling of diverse and high-reward samples.", "title_embedding_index": 8456, "title_abs_embedding_index": 8481}, {"title": "ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement", "link_suffix": "/forum?id=YUYJsHOf3c", "link": "https://openreview.net/forum?id=YUYJsHOf3c", "pdf_link": "https://openreview.net/pdf?id=YUYJsHOf3c", "keywords": "LLM, reasoning, generalization, self-improvement", "abstract": "Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we proposeReasoning Generalist via Self-Improvement (ReGenesis), a method toself-synthesize reasoning paths as post-training data by progressing from abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct an in-depth analysis of our framework and show ReGenesis is effective across various language models and design choices.", "title_embedding_index": 8457, "title_abs_embedding_index": 8482}, {"title": "Boosting Methods for Interval-censored Data with Regression and Classification", "link_suffix": "/forum?id=DzbUL4AJPP", "link": "https://openreview.net/forum?id=DzbUL4AJPP", "pdf_link": "https://openreview.net/pdf?id=DzbUL4AJPP", "keywords": "Boosting, Functional gradient descent, Interval-censored data, Minimax error rate, Nonparametric classification, Nonparametric regression, Smoothing spline", "abstract": "Boosting has garnered significant interest across both machine learning and statistical communities. Traditional boosting algorithms, designed for fully observed random samples, often struggle with real-world problems, particularly with interval-censored data. This type of data is common in survival analysis and time-to-event studies where exact event times are unobserved but fall within known intervals. Effective handling of such data is crucial in fields like medical research, reliability engineering, and social sciences. In this work, we introduce novel nonparametric boosting methods for regression and classification tasks with interval-censored data. Our approaches leverages censoring unbiased transformations to adjust loss functions and impute transformed responses while maintaining model accuracy. Implemented via functional gradient descent, these methods ensure scalability and adaptability. We rigorously establish their theoretical properties, including optimality and mean squared error trade-offs, offering solid guarantees. Our proposed methods not only offer a robust framework for enhancing predictive accuracy in domains where interval-censored data are common but also complement existing work, expanding the applicability of boosting techniques. Empirical studies demonstrate robust performance across various finite-sample scenarios, highlighting the practical utility of our approaches.", "title_embedding_index": 8458, "title_abs_embedding_index": 8483}, {"title": "MSSC-BiMamba: Multimodal Sleep Stage Classification with Bidirectional Mamba", "link_suffix": "/forum?id=b1bTe1xyzS", "link": "https://openreview.net/forum?id=b1bTe1xyzS", "pdf_link": "https://openreview.net/pdf?id=b1bTe1xyzS", "keywords": "sleep stage classification, Mamba, efficient channel attention, bidirectional state space model", "abstract": "Monitoring sleep states is essential for evaluating sleep quality and diagnosing sleep disorders. Traditional manual staging is time-consuming and prone to subjective bias, often resulting in inconsistent outcomes. Here, we developed an automated model for sleep staging to enhance diagnostic accuracy and efficiency. Considering the characteristics of polysomnography (PSG) multi-lead sleep monitoring, we designed a multimodal sleep state classification model,  MSSC-BiMamba, that combines an Efficient  Channel Attention (ECA) mechanism with a Bidirectional State Space Model (BSSM). The ECA module allows for weighting data from different sensor channels, thereby amplifying the influence of diverse sensor inputs. Additionally, the implementation of bidirectional Mamba (BiMamba) enables the model to effectively capture the multi-dimensional features and long-range dependencies of PSG data. The developed model demonstrated impressive performance on sleep stage classification tasks on the ISRUC-S3 and ISRUC-S1 datasets, respectively, including healthy and unhealthy sleep patterns. Our model, which can effectively handle diverse sleep conditions, is the first to apply BiMamba to sleep staging with multimodal PSG data, showing substantial gains in computational and memory efficiency over traditional Transformer-style models. This method enhances sleep health management by making monitoring more accessible and extending advanced healthcare through innovative technology.", "title_embedding_index": 8459, "title_abs_embedding_index": 8484}, {"title": "Extreme composite compression of large language models through joint optimization", "link_suffix": "/forum?id=zno7tZVG8T", "link": "https://openreview.net/forum?id=zno7tZVG8T", "pdf_link": "https://openreview.net/pdf?id=zno7tZVG8T", "keywords": "model quantization, model compression, sparsification, joint optimization", "abstract": "Post-Training Quantization (PTQ) and Sparsification (PTS) are dominant methods in the compression of Large Language Models (LLMs) due to their minimal resource usage and generalizability. It is a natural idea to integrate quantization and sparsification in a unified framework, which however, often results in substantial accuracy losses. Here we argue that, the key lies in optimization. This paper introduces a novel joint optimization strategy that concurrently mitigates errors induced by both sparsification and quantization. \nUnlike sequential approaches, our method employs learnable transformation matrices to simultaneously optimize errors across both dimensions, preventing the typical misalignments associated with sequential optimizations. Furthermore, we present a reordering mechanism within the learnable mask sparsification process to maintain consistent sparsity ratios. This mechanism ensures the prioritization of the least important weights during each update iteration, thus enhancing the stability of the compression process. \nOur approach demonstrates considerable performance enhancements across diverse models and datasets, with the most notable gains observed under conditions of extremely low-bit quantization and high sparsity ratios. For example, in the LLaMA2-13b model with weight quantization at 2 bit and a 75% sparsity configuration, our method surpasses the state-of-the-art (SOTA) by 9.03% in average accuracy across five zero-shot tasks. Meanwhile, in the newest LLaMA3-8b model, with weight quantization at 3 bit and a 50% sparsity configuration, our method outperforms the SOTA by 4.58% (56.86% vs 52.28%) in zero-shot tasks and achieves a perplexity reduction of 4.45 on the WikiText2 dataset (10.78 vs 15.23).", "title_embedding_index": 8460, "title_abs_embedding_index": 8485}, {"title": "ZETA: LeveragingZ-order Curves for Efficient Top-kAttention", "link_suffix": "/forum?id=j9VVzueEbG", "link": "https://openreview.net/forum?id=j9VVzueEbG", "pdf_link": "https://openreview.net/pdf?id=j9VVzueEbG", "keywords": "Transformer, In-context learning, Long Context, long-range Transformer, Efficient Transformer", "abstract": "Over recent years, the Transformer has become a fundamental building block for sequence modeling architectures. Yet at its core is the use of self-attention, whose memory and computational cost grow quadratically with the sequence length $N$, rendering it prohibitively expensive for long sequences. A promising approach is top-$k$ attention, which selects only the $k$ most relevant tokens and achieves performance comparable to vanilla self-attention while significantly reducing space and computational demands. However, causal masks require the current query token to only attend to past tokens, preventing existing top-$k$ attention methods from efficiently searching for the most relevant tokens in parallel, thereby limiting training efficiency. In this work, we propose ZETA, leveraging Z-Order Curves for Efficient Top-k Attention, to enable parallel querying of past tokens for entire sequences. We first theoretically show that the choice of key and query dimensions involves a trade-off between the curse of dimensionality and the preservation of relative distances after projection. In light of this insight, we propose reducing the dimensionality of keys and queries in contrast to values and further leveraging Z-order curves to map low-dimensional keys and queries into one-dimensional space, which permits parallel sorting, thereby largely improving the efficiency for top-$k$ token selection. Experimental results demonstrate that ZETA~matches the performance of standard attention on synthetic tasks Associative Recall and outperforms attention and its variants on Long-Range Arena and WikiText-103 language modeling.", "title_embedding_index": 8461, "title_abs_embedding_index": 8486}, {"title": "Model-diff: A Tool for Comparative Study of Language Models in the Input Space", "link_suffix": "/forum?id=F3Migaak2i", "link": "https://openreview.net/forum?id=F3Migaak2i", "pdf_link": "https://openreview.net/pdf?id=F3Migaak2i", "keywords": "prediction difference; input space;", "abstract": "Comparing two (large) language models (LMs) side-by-side and pinpointing their prediction similarities and differences on the same set of inputs are crucial in many real-world scenarios, e.g., one can test if a licensed model was potentially plagiarized by another.\nTraditional analysis compares the LMs' outputs on some benchmark datasets, which only cover a limited number of inputs of designed perspectives for the intended applications.\nThe benchmark datasets cannot prepare data to cover the test cases from unforeseen perspectives which can help us understand differences between models unbiasedly. \nIn this paper, we propose a new model comparative analysis setting that considers a large input space where brute-force enumeration would be infeasible. \nThe input space can be simply defined as all token sequences that a LM would produce low perplexity on --- we follow this definition in the paper as it would produce the most human-readable inputs. \nWe propose a novel framework Model-diff that uses text generation by sampling and deweights the histogram of sampling statistics to estimate prediction differences between two LMs in this input space efficiently and unbiasedly.\nModel-diff achieves this by drawing and counting the inputs at each prediction difference value in negative log-likelihood.\nExperiments reveal for the first time the quantitative prediction differences between LMs in a large input space, potentially facilitating the model analysis for applications such as model plagiarism.", "title_embedding_index": 8462, "title_abs_embedding_index": 8487}, {"title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation", "link_suffix": "/forum?id=i55KTmMb4I", "link": "https://openreview.net/forum?id=i55KTmMb4I", "pdf_link": "https://openreview.net/pdf?id=i55KTmMb4I", "keywords": "Large Language Models, Code Completion, Benchmark", "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.", "title_embedding_index": 8463, "title_abs_embedding_index": 8488}, {"title": "When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust \"APIs'' for Human-AI Interaction", "link_suffix": "/forum?id=SqoL14HDm0", "link": "https://openreview.net/forum?id=SqoL14HDm0", "pdf_link": "https://openreview.net/pdf?id=SqoL14HDm0", "keywords": "large language model, human-LLM interaction, API, prompt engineering, software engineering, controlled natural language", "abstract": "As large language models (LLMs) gain increasing capabilities, they are being widely applied in areas such as intelligent customer service, code generation, and knowledge management. Prompts written in natural language (NL) serve as the \"APIs'' for human-LLM interaction. To enhance prompt quality, best practices for prompt engineering (PE) have been established, along with writing guidelines and templates. However, due to the inherent ambiguity of natural language, even prompts that strictly follow these guidelines often fail to trigger LLM to consistently output high quality responses, particularly for complex tasks. To address this issue, this paper proposes a Controlled Natural Language for Prompt (CNL-P) which incorporates best practices in PE. To overcome the NL's ambiguity, CNL-P introduces precise grammar structures and strict semantic norms, enabling a declarative but structured and accurate representation of user intent. This helps LLMs better understand and execute CNL-P, leading to higher quality responses. To lower the learning curve of CNL-P, we introduce an automatic NL2CNL-P conversion agent based on LLMs, which allow users to describe prompts in NL from which the NL2CNL-P agent generates CNL-P compliant prompts guided by CNL-P grammar. We further develop a linting tool for CNL-P, including syntactic and semantic checks, making static analysis techniques applicable to natural language for the first time. CNL-P\u2019s design not only integrates best practices in PE but also adopts key principles from software engineering (SE). Extensive experiments show that CNL-P can improve the quality of LLM's responses through the novel and organic synergy of PE and SE. We envision that CNL-P has the potential to bridge the gap between emergent PE and traditional SE, paving the way for a new natural language centric programming paradigm.", "title_embedding_index": 8464, "title_abs_embedding_index": 8489}, {"title": "Proactive Agents for Multi-Turn Text-to-Image Generation Under Uncertainty", "link_suffix": "/forum?id=xsmlrhoQzC", "link": "https://openreview.net/forum?id=xsmlrhoQzC", "pdf_link": "https://openreview.net/pdf?id=xsmlrhoQzC", "keywords": "Interpretable belief state, uncertainty estimation, information gathering, intelligent agents, question-asking under uncertainty", "abstract": "User prompts for generative AI models are often underspecified or open-ended, which may lead to sub-optimal responses. This prompt underspecification problem is particularly evident in text-to-image (T2I) generation, where users commonly struggle to articulate their precise intent. This disconnect between the user\u2019s vision and the model\u2019s interpretation often forces users to keep refining their prompts.To address this, we propose a design for building proactive T2I agents equipped with an interface to actively ask clarification questions when uncertain, and present their understanding of user intent as an interpretable belief state that a user can edit.  We build simple prototypes for such agents and verify their effectiveness through both human studies and automated evaluation. We observed that at least90% of human subjects found these agents and their interpretable belief states helpful for their T2I workflow. Moreover, we use a scalable automated evaluation approach using two agents, one with a ground truth image and the other tries to ask as few questions as possible to align with the ground truth. On both the COCO dataset (Lin et al., 2014) and DesignBench, a more photorealistic benchmark we created, we observed that these T2I agents were able to ask informative questions and elicit crucial information to achieve successful alignment with at least 2 times higher VQAScore (Lin et al., 2024) than the standard single-turn T2I generation.", "title_embedding_index": 8465, "title_abs_embedding_index": 8490}, {"title": "Safe Bayesian Optimization for Complex Control Systems via Additive Gaussian Processes", "link_suffix": "/forum?id=57iQSl2G2Q", "link": "https://openreview.net/forum?id=57iQSl2G2Q", "pdf_link": "https://openreview.net/pdf?id=57iQSl2G2Q", "keywords": "Safe Bayesian Optimization, Complex Control Optimization, Additive Gaussian Processes", "abstract": "Controller tuning and optimization have been among the most fundamental problems in robotics and mechatronic systems. The traditional methodology is usually model-based, but its performance heavily relies on an accurate mathematical system model. In control applications with complex dynamics, obtaining a precise model is often challenging, leading us towards a data-driven approach. While various researchers have explored the optimization of a single controller, it remains a challenge to obtain the optimal controller parameters safely and efficiently when multiple controllers are involved. In this paper, we propose SafeCtrlBO to optimize multiple controllers simultaneously and safely. We simplify the exploration process in safe Bayesian optimization, reducing computational effort without sacrificing expansion capability. Additionally, we use additive kernels to enhance the efficiency of Gaussian process updates for unknown functions. Hardware experimental results on a permanent magnet synchronous motor (PMSM) demonstrate that compared to existing safe Bayesian optimization algorithms, SafeCtrlBO can obtain optimal parameters more efficiently while ensuring safety.", "title_embedding_index": 8466, "title_abs_embedding_index": 8491}, {"title": "Long-context Protein Language Model", "link_suffix": "/forum?id=Et0SIGDpP5", "link": "https://openreview.net/forum?id=Et0SIGDpP5", "pdf_link": "https://openreview.net/pdf?id=Et0SIGDpP5", "keywords": "protein language model", "abstract": "Self-supervised training of language models (LMs) has seen great success for protein sequences in learning meaningful representations and for generative drug design. \nMost protein LMs are based on the Transformer architecture trained on individual proteins with short context lengths. Such protein LMs cannot extrapolate to longer proteins and protein complexes well. They also fail to account for the underlying biological mechanisms carried out by biomolecular interactions and dynamics i.e., proteins often interact with other proteins, molecules, and pathways in complex biological systems. \nIn this work, we propose LC-PLM based on an alternative protein LM architecture, BiMamba-S, built off selective structured state-space models, to learn high-quality universal protein representations at the amino acid token level using masked language modeling. We also introduce its graph-contextual variant, LC-PLM-G, which contextualizes protein-protein interaction (PPI) graphs for a second stage of training. LC-PLM demonstrates favorable neural scaling laws, better length extrapolation capability, and a 7% to 34% improvement on protein downstream tasks than Transformer-based ESM-2. LC-PLM-G further trained within the context of PPI graphs shows promising results on protein structure and function prediction tasks. Our study demonstrates the benefit of increasing the context size with computationally efficient LM architecture (e.g. structured SSMs) in learning universal protein representations and incorporating molecular interaction contexts contained in biological graphs.", "title_embedding_index": 8467, "title_abs_embedding_index": 8492}, {"title": "DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation", "link_suffix": "/forum?id=4GSOESJrk6", "link": "https://openreview.net/forum?id=4GSOESJrk6", "pdf_link": "https://openreview.net/pdf?id=4GSOESJrk6", "keywords": "personalized image generation, subject-driven image generation, personalization, image generation, benchmarking, human-aligned evaluation", "abstract": "Personalized image generation holds great promise in assisting humans in everyday work and life due to its impressive function in creatively generating personalized content. However, current evaluations either are automated but misalign with humans or require human evaluations that are time-consuming and expensive. In this work, we present DreamBench++, a human-aligned benchmark that advanced multimodal GPT models automate. Specifically, we systematically design the prompts to let GPT be both human-aligned and self-aligned, empowered with task reinforcement. Further, we construct a comprehensive dataset comprising diverse images and prompts. By benchmarking 7 modern generative models, we demonstrate that \\dreambench results in significantly more human-aligned evaluation, helping boost the community with innovative findings.", "title_embedding_index": 8468, "title_abs_embedding_index": 8493}, {"title": "Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons", "link_suffix": "/forum?id=1NkrxqY4jK", "link": "https://openreview.net/forum?id=1NkrxqY4jK", "pdf_link": "https://openreview.net/pdf?id=1NkrxqY4jK", "keywords": "Large Language Models, Mechanistic Interpretability, Safety Alignment, Neuron", "abstract": "Large language models (LLMs) excel in various capabilities but pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment through the lens of mechanistic interpretability, focusing on identifying and analyzingsafety neuronswithin LLMs that are responsible for safety behaviors. We proposeinference-time activation contrastingto locate these neurons anddynamic activation patchingto evaluate their causal effects on model safety. Experiments on multiple prevalent LLMs demonstrate that we can consistently identify about $5$% safety neurons, and by only patching their activations we can restore over $90$% of the safety performance across various red-teaming benchmarks without influencing general ability. The finding of safety neurons also helps explain the ''alignment tax'' phenomenon by revealing that the key neurons for model safety and helpfulness significantly overlap, yet they require different activation patterns for the same neurons. Furthermore, we demonstrate an application of our findings in safeguarding LLMs by detecting unsafe outputs before generation.", "title_embedding_index": 8469, "title_abs_embedding_index": 8494}, {"title": "Data-centric Prediction Explanation via Kernelized Stein Discrepancy", "link_suffix": "/forum?id=KlV5CkNQkl", "link": "https://openreview.net/forum?id=KlV5CkNQkl", "pdf_link": "https://openreview.net/pdf?id=KlV5CkNQkl", "keywords": "Prediction Explanation, Kernelized Stein Discrepency", "abstract": "Existing example-based prediction explanation methods often bridge test and training data points through the model\u2019s parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain) prediction explanation method that exploits properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperforms existing methods from various aspects, including 1) preciseness (fine-grained explanation), 2) consistency, and 3) computation efficiency, leading to a surprisingly simple, effective, and robust prediction explanation solution.", "title_embedding_index": 8470, "title_abs_embedding_index": 8495}, {"title": "Accurate and Scalable Graph Neural Networks via Message Invariance", "link_suffix": "/forum?id=UqrFPhcmFp", "link": "https://openreview.net/forum?id=UqrFPhcmFp", "pdf_link": "https://openreview.net/pdf?id=UqrFPhcmFp", "keywords": "Subgraph Sampling, Graph Neural Networks, Topological Compensation, Message Symmetries", "abstract": "Message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. The whole message passing for a mini-batch of target nodes consists of two parts: message passing between the in-batch nodes (MP-IB) and message passing from their out-of-batch neighbors to the in-batch nodes (MP-OB). However, MP-OB recursively relies on higher-order out-of-batch neighbors, leading to an exponentially growing computational cost with respect to the number of layers. Due to the neighbor explosion, the whole message passing stores most nodes on the GPU such that many GNNs are infeasible to large-scale graphs. To address this challenge, we propose an accurate and fast mini-batch approach, namely topological compensation (TOP), which obtains the outputs of the whole message passing solely through MP-IB, without the costly MP-OB. The major pillar of TOP is a novel concept of message invariance, which defines message-invariant transformations to convert costly MP-OB into fast MP-IB. This ensures that the modified MP-IB has the same output as the whole message passing. Extensive experiments demonstrate that TOP is significantly faster than existing mini-batch methods by order of magnitude on vast graphs (millions of nodes and billions of edges) without accuracy degradation.", "title_embedding_index": 8471, "title_abs_embedding_index": 8496}, {"title": "AVSS: a new benchmark for airport video semantic segmentation", "link_suffix": "/forum?id=3nkIRKh3Sk", "link": "https://openreview.net/forum?id=3nkIRKh3Sk", "pdf_link": "https://openreview.net/pdf?id=3nkIRKh3Sk", "keywords": "Airport Ground, Semantic Segmentation, Video Surveillance", "abstract": "Airport video semantic segmentation is fundamental to airport surveillance applications, yet there currently lacks a specialized benchmark and algorithms for this task. In this paper, we introduce the first large-scale Airport Video Semantic Segmentation dataset (AVSS) for airport surveillance. AVSS comprises 18 common semantic categories at airports, and 250 videos, totaling over 140,000 frames with accurate manual annotations. AVSS covers a wide range of challenges for airport video surveillance, such as extreme multi-scale, intra-class diversity, inter-class similarity, etc. We analyze statistical information and evaluate 17 state-of-the-art (SOTA) semantic segmentation algorithms on AVSS. The significant performance degradation indicates that current models are far from practical application. Furthermore, we discuss how to develop video semantic segmentation algorithms for airport surveillance and the generalizability of AVSS to other tasks and datasets. AVSS serves as a research resource for airport semantic segmentation and a robustness evaluation tool for segmentation algorithms in practical applications. AVSS is available atwww.agvs-caac.com/avss/avss.html.", "title_embedding_index": 8472, "title_abs_embedding_index": 8497}, {"title": "AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment", "link_suffix": "/forum?id=Mgf7qdUbX5", "link": "https://openreview.net/forum?id=Mgf7qdUbX5", "pdf_link": "https://openreview.net/pdf?id=Mgf7qdUbX5", "keywords": "RLHF poisoning, Backdoor, LLM Alignment", "abstract": "With the growing adoption of reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), the risk of backdoor installation during alignment has increased, leading to unintended and harmful behaviors. Existing backdoor triggers are typically limited to fixed word patterns, making them detectable during data cleaning and easily removable post-poisoning. In this work, we explore the use of prompt-specific paraphrases as backdoor triggers, enhancing their stealth and resistance to removal during LLM alignment. We propose AdvBDGen, an adversarially fortified generative fine-tuning framework that automatically generates prompt-specific backdoors that are effective, stealthy, and transferable across models. AdvBDGen employs a generator-detector pair, fortified by an adversary, to ensure the installability and stealthiness of backdoors. It enables the crafting of complex triggers using as little as 3% of the fine-tuning data. Once installed, these backdoors can jailbreak LLMs during inference, demonstrate improved stability against perturbations compared to traditional constant triggers, and are harder to remove. These properties highlight the greater risks posed by such an adversarially crafted backdoors to LLM alignment.", "title_embedding_index": 8473, "title_abs_embedding_index": 8498}, {"title": "Single-View 3D Representations for Reinforcement Learning by Cross-View Neural Radiance Fields", "link_suffix": "/forum?id=Crsl3zbfvW", "link": "https://openreview.net/forum?id=Crsl3zbfvW", "pdf_link": "https://openreview.net/pdf?id=Crsl3zbfvW", "keywords": "3D scene representation, Single-view inference, NeRF, Reinforcement Learning", "abstract": "Reinforcement learning (RL) has enabled robots to develop complex skills, but its success in image-based tasks often depends on effective representation learning. Prior works have primarily focused on 2D representations, often overlooking the inherent 3D geometric structure of the world, or have attempted to learn 3D representations that require extensive resources such as synchronized multi-view images even during deployment. To address these issues, we propose a novel RL framework that extracts 3D-aware representations from single-view RGB input, without requiring camera calibration information or synchronized multi-view images during the downstream RL. Our method employs an autoencoder architecture, using a masked ViT as the encoder and a latent-conditioned NeRF as the decoder, trained with cross-view completion to capture fine-grained, 3D geometry-aware representations. Additionally, we utilize a time contrastive loss that further regularizes the learned representation for consistency across different viewpoints. Our method significantly enhances the RL agent\u2019s performance in complex tasks, demonstrating superior effectiveness compared to prior 3D representation-based methods, even when using only a single, uncalibrated camera during deployment.", "title_embedding_index": 8474, "title_abs_embedding_index": 8499}]