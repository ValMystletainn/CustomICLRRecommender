[
    {
        "title": "3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery",
        "link_suffix": "/forum?id=RgE1qiO2ek",
        "link": "https://openreview.net/forum?id=RgE1qiO2ek",
        "pdf_link": "https://openreview.net/pdf?id=RgE1qiO2ek",
        "keywords": "Structure-based Drug Discovery, Protein-ligand Docking, 3D Molecule Generation, Transformer",
        "abstract": "Structure-based drug discovery, encompassing the tasks of protein-ligand docking and pocket-aware 3D drug design, represents a core challenge in drug discovery. However, no existing work can deal with both tasks to effectively leverage the duality between them, and current methods for each task are hindered by challenges in modeling 3D information and the limitations of available data. To address these issues, we propose 3DMolFormer, a unified dual-channel transformer-based framework applicable to both docking and 3D drug design tasks, which exploits their duality by utilizing docking functionalities within the drug design process. Specifically, we represent 3D pocket-ligand complexes using parallel sequences of discrete tokens and continuous numbers, and we design a corresponding dual-channel transformer model to handle this format, thereby overcoming the challenges of 3D information modeling. Additionally, we alleviate data limitations through large-scale pre-training on a mixed dataset, followed by supervised and reinforcement learning fine-tuning techniques respectively tailored for the two tasks. Experimental results demonstrate that 3DMolFormer outperforms previous approaches in both protein-ligand docking and pocket-aware 3D drug design, highlighting its promising application in structure-based drug discovery."
    },
    {
        "title": "Towards a Unified Framework of Clustering-based Anomaly Detection",
        "link_suffix": "/forum?id=SYI409tbsv",
        "link": "https://openreview.net/forum?id=SYI409tbsv",
        "pdf_link": "https://openreview.net/pdf?id=SYI409tbsv",
        "keywords": "Anomaly Detection, Clustering",
        "abstract": "Unsupervised Anomaly Detection (UAD) plays a crucial role in identifying abnormal patterns within data without labeled examples, holding significant practical implications across various domains. Although the individual contributions of representation learning and clustering to anomaly detection are well-established, their interdependencies remain under-explored due to the absence of a unified theoretical framework. Consequently, their collective potential to enhance anomaly detection performance remains largely untapped. To bridge this gap, in this paper, we propose a novel probabilistic mixture model for anomaly detection to establish a theoretical connection among representation learning, clustering, and anomaly detection. By maximizing a novel anomaly-aware data likelihood, representation learning and clustering can effectively reduce the adverse impact of anomalous data and collaboratively benefit anomaly detection. Meanwhile, a theoretically substantiated anomaly score is naturally derived from this framework. Lastly, drawing inspiration from gravitational analysis in physics, we have devised an improved anomaly score that more effectively harnesses the combined power of representation learning and clustering. Extensive experiments, involving 17 baseline methods across 30 diverse datasets, validate the effectiveness and generalization capability of the proposed method, surpassing state-of-the-art methods."
    },
    {
        "title": "Morse: Fast Sampling for Accelerating Diffusion Models Universally",
        "link_suffix": "/forum?id=UlsI4z3QQP",
        "link": "https://openreview.net/forum?id=UlsI4z3QQP",
        "pdf_link": "https://openreview.net/pdf?id=UlsI4z3QQP",
        "keywords": "Diffusion models, image generation, text-to-image generation, model acceleration",
        "abstract": "In this paper, we present Morse, a simple and universal framework for accelerating diffusion models. The key insight of Morse is to reformulate the iterative generation (from noise to data) process via taking advantage of fast jump sampling and adaptive residual feedback strategies. Specifically, Morse involves two models called Dash and Dot that interact with each other. The Dash model is just the pre-trained diffusion model of any type, but operates in a jump sampling regime, creating sufficient space for sampling efficiency improvement. The Dot model is significantly faster than the Dash model, which is learnt to generate residual feedback conditioned on the observations at the current jump sampling point on the trajectory of the Dash model, lifting the noise estimate to easily match the next-step estimate of the Dash model without jump sampling. By chaining the outputs of the Dash and Dot models run in a time-interleaved fashion, Morse exhibits the merit of flexibly attaining desired image generation performance while improving overall runtime efficiency. With our proposed weight sharing strategy between the Dash and Dot models, Morse is efficient for training and inference. We validate the efficacy of our method under a variety of experimental setups. Our method shows an average speedup of 1.78\u00d7 to 3.31\u00d7 over a wide range of sampling step budgets relative to baseline diffusion models. Furthermore, we show that our method can be also generalized to improve the Latent Consistency Model (LCM-SDXL, which is already accelerated with consistency distillation technique) tailored for few-step text-to-image synthesis. The code will be made publicly available."
    },
    {
        "title": "Evidential Learning-based Certainty Estimation for Robust Dense Feature Matching",
        "link_suffix": "/forum?id=4NWtrQciRH",
        "link": "https://openreview.net/forum?id=4NWtrQciRH",
        "pdf_link": "https://openreview.net/pdf?id=4NWtrQciRH",
        "keywords": "Evidential Deep Learning, Dense Feature Matching, Pose Estimation",
        "abstract": "Dense feature matching methods aim to estimate a dense correspondence field between images. Inaccurate correspondence can occur due to the presence of unmatchable region, highlighting the need for certainty measurement. This is typically addressed by training a binary classifier to decide whether each predicted correspondence is reliable. However, deep neural network-based classifiers can be vulnerable to image corruptions or perturbations, making it difficult to obtain reliable matching pairs in corrupted scenario. In this work, we propose an evidiential deep learning framework to enhance the robustness of dense matching against corruptions. We modify the certainty prediction branch in dense matching models to generate appropriate belief masses and compute the certainty score by taking expectation over the resulting Dirichlet distribution. We evaluate our method on a wide range of benchmarks and show that our method leads to improved robustness against common corruptions and adversarial attacks, achieving up to 10.1% improvement under severe corruptions."
    },
    {
        "title": "Fair Class-Incremental Learning using Sample Weighting",
        "link_suffix": "/forum?id=kQNlIHlM6m",
        "link": "https://openreview.net/forum?id=kQNlIHlM6m",
        "pdf_link": "https://openreview.net/pdf?id=kQNlIHlM6m",
        "keywords": "trustworthy ai, model fairness, continual learning, class-incremental learning",
        "abstract": "Model fairness is becoming important in class-incremental learning for Trustworthy AI. While accuracy has been a central focus in class-incremental learning, fairness has been relatively understudied. However, naively using all the samples of the current task for training results in unfair catastrophic forgetting for certain sensitive groups including classes. We theoretically analyze that forgetting occurs if the average gradient vector of the current task data is in an \"opposite direction\" compared to the average gradient vector of a sensitive group, which means their inner products are negative. We then propose a fair class-incremental learning framework that adjusts the training weights of current task samples to change the direction of the average gradient vector and thus reduce the forgetting of underperforming groups and achieve fairness. For various group fairness measures, we formulate optimization problems to minimize the overall losses of sensitive groups while minimizing the disparities among them. We also show the problems can be solved with linear programming and propose an efficient Fairness-aware Sample Weighting (FSW) algorithm. Experiments show that FSW achieves better accuracy-fairness tradeoff results than state-of-the-art approaches on real datasets."
    },
    {
        "title": "Do LLMs Have Political Correctness? Analyzing Ethical Biases and Jailbreak Vulnerabilities in AI Systems",
        "link_suffix": "/forum?id=zf53vmj6k4",
        "link": "https://openreview.net/forum?id=zf53vmj6k4",
        "pdf_link": "https://openreview.net/pdf?id=zf53vmj6k4",
        "keywords": "LLM, safety, jailbreak",
        "abstract": "Although large language models (LLMs) demonstrate impressive proficiency in various tasks, they present potential safety risks, such as \n'jailbreaks', where malicious inputs can coerce LLMs into generating harmful content. To address these issues, many LLM developers have implemented various safety measures to align these models. This alignment involves several techniques, including data filtering during pre-training, supervised fine-tuning, reinforcement learning from human feedback, and red-teaming exercises. These methods often introduce deliberate and intentional biases similar to Political Correctness (PC) to ensure the ethical behavior of LLMs. In this paper, we delve into the intentional biases injected into LLMs for safety purposes and examine methods to circumvent these safety alignment techniques. Notably, these intentional biases result in a jailbreaking success rate in GPT-4o models that differs by 20% between non-binary and cisgender keywords and by 16% between white and black keywords, even when the other parts of the prompts are identical. We introduce the concept ofPCJailbreak, highlighting the inherent risks posed by these safety-induced biases. Additionally, we propose an efficient defense methodPCDefense, which prevents jailbreak attempts by injecting defense prompts prior to generation.PCDefensestands as an appealing alternative to Guard Models, such as Llama-Guard, that require additional inference cost after text generation. Our findings emphasize the urgent need for LLM developers to adopt a more responsible approach when designing and implementing safety measures. To enable further research and improvements, we open-source ourcode and artifactsof PCJailbreak, providing the community with tools to better understand and mitigate safety-induced biases in LLMs."
    },
    {
        "title": "SPARC: Continual learning beyond experience rehearsal and model surrogates",
        "link_suffix": "/forum?id=QLOGfFSB50",
        "link": "https://openreview.net/forum?id=QLOGfFSB50",
        "pdf_link": "https://openreview.net/pdf?id=QLOGfFSB50",
        "keywords": "Continual learning, lifelong learning, computer vision, experience rehearsal, parameter isolation",
        "abstract": "Continual learning (CL) has become increasingly important as deep neural networks (DNNs) are required to adapt to the continuous influx of data without retraining from scratch. However, a significant challenge in CL is catastrophic forgetting (CF), where learning new tasks erases previously acquired knowledge, either partially or completely. Existing solutions often rely on experience rehearsal or full model surrogates to mitigate CF. While effective, these approaches introduce substantial memory and computational overhead, limiting their scalability and applicability in real-world scenarios where efficiency is critical. To address this, we propose SPARC, a scalable CL approach that eliminates the need for both experience rehearsal and full model surrogates. SPARC employs parameter-efficient task-specific working memories to capture information relevant to each task and task-agnostic semantic memory for cross-task knowledge consolidation. Additionally, SPARC introduces weight re-normalization in the classification layer to reduce recency bias toward newly learned tasks. SPARC is lightweight, requiring only 6% of the parameters used by full-model surrogates, yet it delivers superior performance on Seq-TinyImageNet and matches the results of rehearsal-based methods on various CL benchmarks. This makes SPARC a practical solution for continual learning where computational efficiency and scalability are paramount."
    },
    {
        "title": "Language models scale reliably with over-training and on downstream tasks",
        "link_suffix": "/forum?id=iZeQBqJamf",
        "link": "https://openreview.net/forum?id=iZeQBqJamf",
        "pdf_link": "https://openreview.net/pdf?id=iZeQBqJamf",
        "keywords": "large language models, scaling laws, over-training, task prediction",
        "abstract": "Scaling laws are useful guides for derisking expensive training runs, as they predict performance of large models using cheaper, small-scale experiments. However, there remain gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., \"Chinchilla optimal\" regime). In contrast, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but models are usually compared on downstream task performance. To address both shortcomings, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we fit scaling laws that extrapolate in both the amount of over-training and the number of model parameters. This enables us to predict the validation loss of a 1.4B parameter, 900B token run (i.e., 32$\\times$ over-trained) and a 6.9B parameter, 138B token run (i.e., a compute-optimal run)\u2013\u2013each from experiments that take 300$\\times$ less compute. Second, we relate the perplexity of a language model to its downstream task performance by proposing a power law. We use this law to predict top-1 error averaged over downstream tasks for the two aforementioned models, using experiments that take 20$\\times$ less compute."
    },
    {
        "title": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL",
        "link_suffix": "/forum?id=BAglD6NGy0",
        "link": "https://openreview.net/forum?id=BAglD6NGy0",
        "pdf_link": "https://openreview.net/pdf?id=BAglD6NGy0",
        "keywords": "Text-to-SQL, LLMs",
        "abstract": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by large language models (LLMs), the latest state-of-the-art techniques are still trapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which limits their applicability in open scenarios. \nTo address this challenge, we propose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to improve the comprehensive capabilities of open-source LLMs for Text2SQL, thereby providing a more practical solution.  Our approach begins with multi-task supervised fine-tuning (SFT) using various synthetic training data related to SQL generation.  Unlike existing SFT-based  Text2SQL methods, we introduced several additional SFT tasks, including schema linking, noise correction, and continuation writing.  Engaging in a variety of SQL generation tasks enhances the model's understanding of SQL syntax and improves its ability to generate high-quality SQL queries. Additionally, inspired by the collaborative modes of LLM agents, we introduce a Multitask Collaboration Prompting (MCP) strategy.  This strategy leverages collaboration across several SQL-related tasks to reduce hallucinations during SQL generation, thereby maximizing the potential of enhancing Text2SQL performance through explicit multitask capabilities. Extensive experiments and in-depth analyses have been performed on eight open-source LLMs and five widely-used benchmarks. The results demonstrate that our proposal outperforms the latest Text2SQL methods and yields leading performance."
    },
    {
        "title": "Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective",
        "link_suffix": "/forum?id=UxkznlcnHf",
        "link": "https://openreview.net/forum?id=UxkznlcnHf",
        "pdf_link": "https://openreview.net/pdf?id=UxkznlcnHf",
        "keywords": "large language models; synthetic data; information bottleneck",
        "abstract": "Synthetic data has become a pivotal resource in post-training tasks for large language models (LLMs) due to the scarcity of high-quality, specific data. While various methods have been developed to generate synthetic data, there remains a discernible gap between the practical effects of synthetic data and our theoretical comprehension. To address this challenge, we commence by presenting a detailed modeling of the prevalent synthetic data generation process. Building upon this modeling, we demonstrate that the generalization capability of the post-trained model is critically determined by the information gain derived from the generative model, as analyzed from a novel reverse-bottleneck perspective. Moreover, we introduce the concept of Generalization Gain via Mutual Information (GGMI) and elucidate the relationship between generalization gain and information gain. This analysis serves as a theoretical foundation for synthetic data generation and further highlights its connection with the generalization capability of post-trained models, offering an understanding about the design of synthetic data generation techniques and the optimization of the post-training process. We open source our code through an anonymous GitHub repository athttps://anonymous.4open.science/r/Understanding-Synthetic."
    },
    {
        "title": "StyleGuide: Crafting visual style prompting with negative visual query guidance",
        "link_suffix": "/forum?id=618qfjvSt9",
        "link": "https://openreview.net/forum?id=618qfjvSt9",
        "pdf_link": "https://openreview.net/pdf?id=618qfjvSt9",
        "keywords": "Style transfer, Generative models, Diffusion models, Visual prompting, Visual instruction, Computer vision, Content creation, Image synthesis",
        "abstract": "In the domain of text-to-image generation, diffusion models have emerged as powerful tools. Recently,  studies on visual prompting, where images are used as prompts, have enabled more precise control over style and content. However, existing methods often suffer from content leakage, where undesired elements from the visual style prompt are transferred along with the intended style (content leakage). To address this issue, we 1) extends classifier-free guidance (CFG) to utilize swapping self-attention and propose 2)negative visual query guidance (NVQG) to reduce the transfer of unwanted contents. NVQG employ negative score by intentionally simulating content leakage scenarios which swaps queries instead of key and values of self-attention layers from visual style prompts. This simple yet effective method significantly reduces content leakage. Furthermore, we provide careful solutions for using a real image as a visual style prompts and for image-to-image (I2I) tasks. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, reflecting the style of the references and ensuring that resulting images match the text prompts."
    },
    {
        "title": "Text Boosts Generalization: A Plug-and-Play Captioner for Real-World Image Restoration",
        "link_suffix": "/forum?id=RjwWClPZtV",
        "link": "https://openreview.net/forum?id=RjwWClPZtV",
        "pdf_link": "https://openreview.net/pdf?id=RjwWClPZtV",
        "keywords": "Image Restoration, Generative models, Diffusion Model, Multimodel Large Language Model",
        "abstract": "Generalization has long been a central challenge in real-world image restoration. While recent diffusion-based restoration methods, which leverage generative priors from text-to-image models, have made progress in recovering more realistic details, they still encounter \"generative capability inactivation\" when applied to out-of-distribution data. To address this, we propose using text as an auxiliary invariant representation to reactivate the generative capabilities of these models. We begin by identifying two key properties of text input in diffusion-based restoration: richness and relevance, and examine their respective influence on model performance. Building on these insights, we introduce Res-Captioner, a module that generates enhanced textual descriptions tailored to image content and degradation levels, effectively mitigating response failures. Additionally, we present RealIR, a new benchmark designed to capture diverse real-world scenarios. Extensive experiments demonstrate that Res-Captioner significantly boosts the generalization ability of diffusion-based restoration models, while remaining fully plug-and-play."
    },
    {
        "title": "VADER: Video Diffusion Alignment via Reward Gradients",
        "link_suffix": "/forum?id=B9dYUFfzl3",
        "link": "https://openreview.net/forum?id=B9dYUFfzl3",
        "pdf_link": "https://openreview.net/pdf?id=B9dYUFfzl3",
        "keywords": "Diffusion Model, Text-to-Video Generation, Generative Models",
        "abstract": "We have made significant progress towards building foundational video diffusion models. As these models are trained using large-scale unsupervised data, it has become crucial to adapt these models to specific downstream tasks. Adapting these models via supervised fine tuning requires collecting target datasets of videos, which is challenging and tedious. In this work, we utilize pre-trained reward models that are learned via preferences on top of powerful vision discriminative models to adapt video diffusion models. These models contain dense gradient information with respect to generated RGB pixels, which is critical to efficient learning in complex search spaces, such as videos. We show that backpropagating gradients from these reward models to a video diffusion model can allow for compute and sample efficient alignment. We show results across a variety of reward models and video diffusion models, demonstrating that our approach can learn much more efficiently in terms of reward queries and computation than prior gradient-free approaches."
    },
    {
        "title": "Symmetric Space Learning for Combinatorial Generalization",
        "link_suffix": "/forum?id=l4qMrcMp2i",
        "link": "https://openreview.net/forum?id=l4qMrcMp2i",
        "pdf_link": "https://openreview.net/pdf?id=l4qMrcMp2i",
        "keywords": "Generative Model, Generalization, Combinatorial Generalization, Machine Learning, Manifold Learning, Representation Learning",
        "abstract": "Symmetries on representations within generative models have shown essential roles in predicting unobserved combinations of semantic changes, known as combinatorial generalization tasks. However, these efforts have primarily focused on learning symmetries from only training data, and thus, the extension of trained symmetries to unseen samples remains uncontrolled. A potential approach for generalizing the symmetries is leveraging geometric information on manifolds that contain functional semantic structures for unseen data, but it still falls short of supporting symmetry learning. In this paper, we address this $\\textit{symmetry generalization}$ by forcing $\\textit{symmetric space}$ on latent space for utilizing semantic structures from symmetry and manifold perspectives. We clarify an equivariance-based constraint that restricts symmetry generalization, and prove that: 1) enforcing the homogeneous space property of symmetric space onto the data manifold eliminates this constraint, 2) a homogeneous latent manifold induces the data manifold through diffeomorphic data-to-latent mapping, and 3) the isometry property of symmetric space extends neighbor symmetries of a point to another within the space. For practical implementation, we propose a method to align sampled points from symmetric space with their explicitly trained geodesic. We verify the method in a detailed analysis on a toy dataset and enhance combinatorial generalization on common benchmarks. This work represents the first effective effort to align symmetries with manifolds for combinatorial generalization."
    },
    {
        "title": "HiddenGuard: Fine-Grained Safe Generation with Specialized Representation Router",
        "link_suffix": "/forum?id=NgCNMlTXx9",
        "link": "https://openreview.net/forum?id=NgCNMlTXx9",
        "pdf_link": "https://openreview.net/pdf?id=NgCNMlTXx9",
        "keywords": "Large Language Models, AI Safety, Context-Aware Moderation, Representation Router",
        "abstract": "As Large Language Models (LLMs) grow increasingly powerful, ensuring their safety and alignment with human values remains a critical challenge. Current alignment approaches predominantly rely on refusal alignment, such as training models to refuse harmful prompts or implementing filters at various stages to block certain responses. These methods are designed toward a binary outcome: either denying to answer the question entirely or answering with full access to the model's parametric knowledge.\nThe binary nature of current alignment approaches presents significant limitations. These methods often fail to balance safety and utility, resulting in either overly cautious responses or overlooking subtle harmful content. They also prevent users from accessing benign information when it's mixed with harmful content. For instance, a model might refuse to provide basic, public information about a medication's composition due to misuse concerns.\nFurthermore, these approaches struggle with context-dependent sensitivity, potentially over-censoring harmless content or missing nuanced harmful outputs. Ideally, LLMs should offer informative responses while avoiding the disclosure of harmful and sensitive information.\nTo address these challenges, we introduce HiddenGuard, a novel framework for fine-grained safe generation in LLMs. Our method incorporates PRISM (rePresentation Router for In-Stream Moderation), a specialized moudule that operates alongside the LLM architecture. By leveraging intermediate hidden states, HiddenGuard enables real-time, token-level harmfulness detection and redaction, without loss in capability. This approach captures deeper semantic information, allowing for more nuanced and context-aware content control compared to traditional filtering techniques. Consequently, the model can generate informative responses while selectively redacting or replacing sensitive information, rather than refusing to answer outright.\nWe also contribute a comprehensive dataset with token-level fine-grained annotations of potentially harmful information across diverse contexts. Our experiments demonstrate that HiddenGuard achieves over 90% in F1 score for detecting and redacting harmful content while preserving the overall utility and informativeness of the model's responses."
    },
    {
        "title": "Towards Debiased Source-Free Domain Adaptation",
        "link_suffix": "/forum?id=mLztw5kEQ9",
        "link": "https://openreview.net/forum?id=mLztw5kEQ9",
        "pdf_link": "https://openreview.net/pdf?id=mLztw5kEQ9",
        "keywords": "source-free domain adaptation, sfda, domain adaptation, contrastive learning, spurious correlation, debiasing, debiased sfda",
        "abstract": "Source-Free Domain Adaptation (SFDA) aims to adapt a model trained in an inaccessible source domain $S$ to a different, unlabelled target domain $T$. The conventional approach generates pseudo-labels for the $T$ samples with the source-trained model, which are then used for model adaptation. However, we show that the adapted model is biased to the spurious correlations in $S$, consequently leading to catastrophic failure on $T$ samples that are dissimilar to $S$. Unfortunately, without any prior knowledge about spurious correlations, the current SFDA setting has no mechanism to circumvent this bias. We introduce a practical setting to address this gap -- Debiased SFDA, where the model receives additional supervision from a pre-trained, frozen reference model. This setting stays in line with the essence of SFDA, which accommodates proprietary source-domain training, while also offering prior knowledge that is unaffected by source-domain training to facilitate debiasing. Under this setting, we propose 1) a simple contrastive objective that debiases the source-trained model from spurious correlations inconsistent with the reference model; 2) a diagnostic metric that evaluates the degree to which an adapted model is biased towards $S$. Our objective can be easily plugged into different baselines for debiasing, and through extensive evaluations, we demonstrate that it engenders consistent improvements across standard benchmarks. Code is supplied under supplementary material."
    },
    {
        "title": "Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head",
        "link_suffix": "/forum?id=m7Nd3K0iru",
        "link": "https://openreview.net/forum?id=m7Nd3K0iru",
        "pdf_link": "https://openreview.net/pdf?id=m7Nd3K0iru",
        "keywords": "knowledge distillation",
        "abstract": "Traditional knowledge distillation focuses on aligning the student's predicted probabilities with both ground-truth labels and the teacher's predicted probabilities. However, the transition to predicted probabilities from logits would obscure certain indispensable information. To address this issue, it is intuitive to additionally introduce a logit-level loss function as a supplement to the widely used probability-level loss function, for exploiting the latent information of logits. Unfortunately, we empirically find that the amalgamation of the newly introduced logit-level loss and the previous probability-level loss will lead to performance degeneration, even trailing behind the performance of employing either loss in isolation. We attribute this phenomenon to the collapse of the classification head, which is verified by our theoretical analysis based on the neural collapse theory. Specifically, the gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone. Drawing from the theoretical analysis, we propose a novel method called dual-head knowledge distillation, which partitions the linear classifier into two classification heads responsible for different losses, thereby preserving the beneficial effects of both losses on the backbone while eliminating adverse influences on the classification head. Extensive experiments validate that our method can effectively exploit the information inside the logits and achieve superior performance against state-of-the-art counterparts."
    },
    {
        "title": "Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets",
        "link_suffix": "/forum?id=1auB9yeB9a",
        "link": "https://openreview.net/forum?id=1auB9yeB9a",
        "pdf_link": "https://openreview.net/pdf?id=1auB9yeB9a",
        "keywords": "landscape analysis, modular addition; gradient dynamics; reasoning; symmetry; representation learning",
        "abstract": "We prove rich algebraic structures of the solution space for 2-layer neural networks with quadratic activation and $L_2$ loss, trained on reasoning tasks in Abelian group (e.g., modular addition). Such a rich structure enables \\emph{analytical} construction of global optimal solutions from partial solutions that only satisfy part of the loss, despite its high nonlinearity. We coin the framework as \\ours{} (\\emph{\\underline{Co}mposing \\underline{G}lobal \\underline{O}ptimizers}). Specifically, we show that the weight space over different numbers of hidden nodes of the 2-layer network is equipped with a semi-ring algebraic structure, and the loss function to be optimized consists of \\emph{monomial potentials}, which are ring homomorphism, allowing partial solutions to be composed into global ones by ring addition and multiplication. Our experiments show that around $95%$ of the solutions obtained by gradient descent match exactly our theoretical constructions. Although the global optimizers constructed only required a small number of hidden nodes, our analysis on gradient dynamics shows that overparameterization asymptotically decouples training dynamics and is beneficial. We further show that training dynamics favors simpler solutions under weight decay, and thus high-order global optimizers such as perfect memorization are unfavorable."
    },
    {
        "title": "Anchors Aweigh! Sail for Optimal Unified Multi-Modal Representations",
        "link_suffix": "/forum?id=U2K4bQVWez",
        "link": "https://openreview.net/forum?id=U2K4bQVWez",
        "pdf_link": "https://openreview.net/pdf?id=U2K4bQVWez",
        "keywords": "multimodal learning, representation learning, unified representation, ImageBind, shared embedding space, CentroBind",
        "abstract": "Multimodal learning plays a crucial role in enabling machine learning models to fuse and utilize diverse data sources, such as text, images, and audio, to support a variety of downstream tasks. A unified representation across various modalities is particularly important for improving efficiency and performance. Recent binding methods, such as ImageBind (Girdhar et al., 2023), typically use a fixed anchor modality to align multimodal data in the anchor modal embedding space. In this paper, we mathematically analyze the fixed anchor binding methods and uncover notable limitations: (1) over-reliance on the choice of the anchor modality, (2) failure to capture intra-modal information, and (3) failure to account for inter-modal correlation among non-anchored modalities. To address these limitations, we propose CentroBind, a simple yet powerful approach that eliminates the need for a fixed anchor; instead, it employs dynamically adjustable centroid-based anchors generated from all available modalities, resulting in a balanced and rich representation space.\nWe theoretically demonstrate that our method captures three crucial properties of multimodal learning: intra-modal learning, inter-modal learning, and multimodal alignment, while also constructing a robust unified representation across all modalities. Our experiments on both synthetic and real-world datasets demonstrate the superiority of the proposed method, showing that dynamic anchor methods outperform all fixed anchor binding methods as the former captures more nuanced multimodal interactions."
    },
    {
        "title": "I2AM: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps",
        "link_suffix": "/forum?id=bBNUiErs26",
        "link": "https://openreview.net/forum?id=bBNUiErs26",
        "pdf_link": "https://openreview.net/pdf?id=bBNUiErs26",
        "keywords": "Image-to-Image Diffusion Model, Interpretable AI, Explainable AI, Image Attribution",
        "abstract": "Large-scale diffusion models have made significant advances in image generation, particularly through cross-attention mechanisms. While cross-attention has been well-studied in text-to-image tasks, their interpretability in image-to-image (I2I) diffusion models remains underexplored. This paper introduces Image-to-Image Attribution Maps $(\\textbf{I}^2\\textbf{AM})$, a method that enhances the interpretability of I2I models by visualizing bidirectional attribution maps, from the reference image to the generated image and vice versa. $\\text{I}^2\\text{AM}$ aggregates cross-attention scores across time steps, attention heads, and layers, offering insights into how critical features are transferred between images. We demonstrate the effectiveness of $\\text{I}^2\\text{AM}$ across object detection, inpainting, and super-resolution tasks. Our results demonstrate that $\\text{I}^2\\text{AM}$ successfully identifies key regions responsible for generating the output, even in complex scenes. Additionally, we introduce the Inpainting Mask Attention Consistency Score (IMACS) as a novel evaluation metric to assess the alignment between attribution maps and inpainting masks, which correlates strongly with existing performance metrics. Through extensive experiments, we show that $\\text{I}^2\\text{AM}$ enables model debugging and refinement, providing practical tools for improving I2I model's performance and interpretability."
    },
    {
        "title": "Critic-CoT: Boosting the reasoning abilities of large language model via Chain-of-Thought Critic",
        "link_suffix": "/forum?id=JEehcb48Vp",
        "link": "https://openreview.net/forum?id=JEehcb48Vp",
        "pdf_link": "https://openreview.net/pdf?id=JEehcb48Vp",
        "keywords": "Self-critic, Large language model, Chain-of-thought",
        "abstract": "Self-critic has become a crucial mechanism for enhancing the reasoning performance of LLMs. However, current approaches mainly involve basic prompts for intuitive instance-level feedback, which resembles System-1 processes and limits the reasoning capabilities. Moreover, there is a lack of in-depth investigations into the relationship between LLM's ability to criticize and its task-solving performance. To address these issues, we propose Critic-CoT, a novel framework that pushes LLMs toward System-2-like critic capability. Through a step-wise CoT reasoning paradigm and the automatic construction of distant-supervision data without human annotation, Critic-CoT enables LLMs to engage in slow, analytic self-critique and refinement, thereby improving their reasoning abilities. Experiments on GSM8K and MATH demonstrate that our enhanced model significantly boosts task-solving performance by filtering out invalid solutions or iterative refinement. Furthermore, we investigate the intrinsic correlation between critique and task-solving abilities within LLMs, discovering that these abilities can mutually reinforce each other rather than conflict."
    },
    {
        "title": "Revisiting Adversarial Examples from the Perspective of Asymptotic Equipartition Property",
        "link_suffix": "/forum?id=RG806nMtQr",
        "link": "https://openreview.net/forum?id=RG806nMtQr",
        "pdf_link": "https://openreview.net/pdf?id=RG806nMtQr",
        "keywords": "Adversarial examples, Adversarial robustness, Asymptotic equipartition property",
        "abstract": "Adversarial examples, which can mislead neural networks through subtle perturbations, continue to challenge our understanding, raising more questions than answers. This paper presents a novel perspective on interpreting adversarial examples through the Asymptotic Equipartition Property (AEP). Our theoretical analysis examines the noise within these examples, revealing that while normal noise aligns with AEP, adversarial noise does not. This insight allows us to classify samples in high-dimensional space as belonging to either the typical or non-typical set, corresponding to normal and adversarial examples, respectively. \nOur analyses and experiments show adversarial examples arise from AEP in high-dimensional space and derive some key properties regarding their quantity, probability, and information capacity. These findings enhance our understanding of adversarial examples and clarify their counterintuitive phenomena, such as adversarial transferability, the trade-off between robustness and accuracy, and robust overfitting."
    },
    {
        "title": "Rethinking the Bias of Foundation Model under Long-tailed Distribution",
        "link_suffix": "/forum?id=l6K688mhDT",
        "link": "https://openreview.net/forum?id=l6K688mhDT",
        "pdf_link": "https://openreview.net/pdf?id=l6K688mhDT",
        "keywords": "foundation model, long-tailed learning",
        "abstract": "Long-tailed learning has garnered increasing attention due to its practical significance. Among the various approaches, the fine-tuning paradigm has gained considerable interest with the advent of foundation models. However, most existing methods primarily focus on leveraging knowledge from these models, overlooking the inherent biases introduced by the imbalanced training data they rely on. In this paper, we examine how such imbalances affect long-tailed downstream tasks. Specifically, we refer to the biases in foundation models and downstream tasks as parameter imbalance and data imbalance, respectively. Through fine-tuning, we observe that parameter imbalance plays a more critical role, while data imbalance can be mitigated using existing re-balancing strategies. Moreover, we find that parameter imbalance cannot be effectively addressed by current re-balancing techniques, such as adjusting the logits, during training, unlike data imbalance. To tackle both imbalances simultaneously, we constitute a causal structure graph and view the partial semantic factor as the confounder, which brings spurious correlations between input samples and labels. To resolve the negative effects of this, we propose a novel backdoor adjustment method that learns the true causal effect between input samples and labels, rather than merely fitting the correlations in the data. Experimental results validate the effectiveness of our method."
    },
    {
        "title": "ChinaTravel: A Real-World Benchmark for Language Agents in Chinese Travel Planning",
        "link_suffix": "/forum?id=9dfRC2dq0R",
        "link": "https://openreview.net/forum?id=9dfRC2dq0R",
        "pdf_link": "https://openreview.net/pdf?id=9dfRC2dq0R",
        "keywords": "Language Agents, Evaluation, Travel Planning, Neural-Symbolic Learning",
        "abstract": "Recent advances in Large Language Models (LLMs), particularly in language reasoning and tool-use capabilities have sparked the rapid development of \\emph{Language Agents} to assist humans across various real-world applications. Among these, travel planning stands out as a significant domain, presenting both academic challenges and practical value due to its inherent complexity and real-world relevance. However, existing travel plan benchmarks do not test language agents with human users or their ability to follow customized requirements, both of which are vital for deploying them in real-world applications. In this paper, we propose ChinaTravel, a new benchmark tailored to authentic Chinese travel requirements, aiming to provide a more realistic evaluation framework for future language agents. We collect the travel requirements through questionnaires and employ an efficient and faithful evaluation process with 46 metrics covering feasibility, constraint satisfaction, and preference comparison. Moreover, we identify three challenges in the real-world deployments of travel planning, including \\emph{constraint recognition}, \\emph{concept openness}, and \\emph{customized preference}. The empirical studies show that even state-of-the-art neural-symbolic agents succeed in 51.3% constraint validation of human queries. Our findings point to the need for methods that can improve the ability of agents to understand diverse intentions or keep track of constraints with emerging concepts from human requirements."
    },
    {
        "title": "PersonaEval: Benchmarking LLMs on Role-Playing Evaluation Tasks",
        "link_suffix": "/forum?id=wZbkQStAXj",
        "link": "https://openreview.net/forum?id=wZbkQStAXj",
        "pdf_link": "https://openreview.net/pdf?id=wZbkQStAXj",
        "keywords": "Role-playing, evaluating evaluators",
        "abstract": "Role-playing in large language models (LLMs) has become a crucial area of research, enabling models to simulate diverse personas and tailor responses, significantly impacting natural language understanding and human-computer interaction. However, while advanced LLMs like GPT-4 are used to evaluate role-playing methods, their reliability in providing accurate assessments remains uncertain, especially in distinguishing nuanced role-playing characteristics. In this paper, we introduce PersonaEval, a benchmark designed to assess the effectiveness of LLMs in role-playing evaluation tasks. We frame the problem as a classification task to determine whether an LLM evaluator can distinguish between sentences from different levels of expertise based solely on linguistic cues. Using real-world data from the Wired 5 Levels video series\u2014where experts explain concepts to five distinct audiences: a child, a teenager, a college student, a graduate student, and another expert\u2014we design three evaluation settings that correspond to commonly used LLM evaluation approaches: five-level classification, pairwise role comparison, and few-shot learning. These settings aim to capture various aspects of how effectively LLMs evaluate role-playing performance. Our study highlights the limitations of current LLMs in persona evaluation tasks and underscores the need for further research to enhance their evaluation capabilities. We provide a foundation for future work aimed at improving the accuracy and professionalism of LLM evaluators in role-playing contexts."
    }
]