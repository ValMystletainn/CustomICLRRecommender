[{"title": "Language Guided Representation Learning", "link_suffix": "/forum?id=6RmZ0V8Vwk", "link": "https://openreview.net/forum?id=6RmZ0V8Vwk", "pdf_link": "https://openreview.net/pdf?id=6RmZ0V8Vwk", "keywords": "representation learning, generalization, natural language, shortcut learning, continual learning, language guidance", "abstract": "Deep neural networks have achieved notable success; however, they still encounter significant challenges compared to humans, particularly in areas such as shortcut learning, texture bias, susceptibility to noise, and catastrophic forgetting, all of which hinder their ability to generalize and adapt. Humans excel in learning high-level abstractions, attributed to various mechanisms in the brain, including reasoning, explanation, and the ability to share concepts verbally\u2014largely facilitated by natural language as a tool for abstraction and systematic generalization. Inspired by this, we investigate how language can be leveraged to guide representation learning. To this end, we explore two approaches to language guidance: Explicit Language Guidance, which introduces direct and verbalizable insights into the model, and Implicit Language Guidance, which provides more intuitive and indirect cues. Our extensive empirical analysis shows that, despite being trained exclusively on text, these methods provide supervision to vision encoders, resulting in improvements in generalization, robustness, and task adaptability in continual learning. These findings underscore the potential of language-guided learning to develop AI systems that can benefit from abstract, high-level concepts, similar to human cognitive abilities.", "title_embedding_index": 3550, "title_abs_embedding_index": 3575}, {"title": "Revisiting and Extending Similarity-based Metrics in Summary Factual Consistency Detection", "link_suffix": "/forum?id=ESM2ixIp3X", "link": "https://openreview.net/forum?id=ESM2ixIp3X", "pdf_link": "https://openreview.net/pdf?id=ESM2ixIp3X", "keywords": "Factual Consistency, Summarisation", "abstract": "Cutting-edge abstractive summarisers generate fluent summaries, but the factuality of the generated text is not guaranteed. \nEarly summary factuality evaluation metrics are usually based on n-gram overlap and embedding similarity, but are reported fail to align with human annotations.\nTherefore, many techniques for detecting factual inconsistencies build pipelines around natural language inference (NLI) or question-answering (QA) models with additional supervised learning steps. \nIn this paper, we revisit similarity-based metrics,\nshowing that this failure stems from the use of reference texts for comparison and the granularity of the comparison. \nWe propose a new zero-shot factuality evaluation metric,\nSentence-BERT Score (SBERTScore), which compares sentences between the summary and the source document. \nIt outperforms widely-used word-word metrics including BERTScore and can compete with existing NLI and QA-based factuality metrics on the benchmark without needing any fine-tuning.\nOur experiments indicate that each technique has different strengths, with SBERTScore particularly effective at identifying correct summaries.\nAdditionally, we demonstrate how a combination of techniques is more effective at detecting various types of error.", "title_embedding_index": 3551, "title_abs_embedding_index": 3576}, {"title": "A2Perf: Real-World Autonomous Agents Benchmark", "link_suffix": "/forum?id=ga1IraEqTE", "link": "https://openreview.net/forum?id=ga1IraEqTE", "pdf_link": "https://openreview.net/pdf?id=ga1IraEqTE", "keywords": "benchmark, reinforcement learning, autonomous agents, agents, benchmarking", "abstract": "Autonomous agents and systems cover a number of application areas, from robotics and digital assistants to combinatorial optimization, all sharing common, unresolved research challenges. It is not sufficient for agents to merely solve a given task; they must generalize to out-of-distribution tasks, perform reliably, and use hardware resources efficiently during training and on-device deployment, among other requirements. Several major classes of methods, such as reinforcement learning and imitation learning, are commonly used to tackle these problems, each with different trade-offs. However, there is currently no benchmarking suite that defines the environments, datasets, and metrics which can be used to develop reference implementations and seed leaderboards with baselines, providing a meaningful way for the community to compare progress. We introduce A2Perf---a benchmarking suite including three environments that closely resemble real-world domains: computer chip floorplanning, web navigation, and quadruped locomotion. A2Perf provides metrics that track task performance, generalization, system resource efficiency, and reliability, which are all critical to real-world applications. In addition, we propose a data cost metric to account for the cost incurred acquiring offline data for imitation learning, reinforcement learning, and hybrid algorithms, which allows us to better compare these approaches. A2Perf also contains baseline implementations of standard algorithms, enabling apples-to-apples comparisons across methods and facilitating progress in real-world autonomy. As an open-source and extendable benchmark, A2Perf is designed to remain accessible, documented, up-to-date, and useful to the research community over the long term.", "title_embedding_index": 3552, "title_abs_embedding_index": 3577}, {"title": "MatText: Do Language Models Need More than Text & Scale for Materials Modeling?", "link_suffix": "/forum?id=ihwRfc4RNw", "link": "https://openreview.net/forum?id=ihwRfc4RNw", "pdf_link": "https://openreview.net/pdf?id=ihwRfc4RNw", "keywords": "Large language models, AI for science, Material representations, LLMS for Materials, Datasets and Benchmark", "abstract": "Effectively representing materials as text has the potential to leverage the vast advancements of large language models (LLMs) for discovering new materials. While LLMs have shown remarkable success in various domains, their application to materials science remains underexplored. A fundamental challenge is the lack of understanding of how to best utilize text-based representations for materials modeling. This challenge is further compounded by the absence of a comprehensive benchmark to rigorously evaluate the capabilities and limitations of these textual representations in capturing the complexity of material systems. To address this gap, we propose MatText, a suite of benchmarking tools and datasets designed to systematically evaluate the performance of language models in modeling materials. MatText encompasses nine distinct text-based representations for material systems, including several novel representations. Each representation incorporates unique inductive biases that capture relevant information and integrate prior physical knowledge about materials. Additionally, MatText provides essential tools for training and benchmarking the performance of language models in the context of materials science. These tools include standardized dataset splits for each representation across a range of dataset sizes, probes for evaluating sensitivity to geometric factors, and tools for seamlessly converting crystal structures into text. Using MatText, we conduct an extensive analysis of the capabilities of language models in modeling materials with different representations and dataset scales. Our findings reveal that current language models consistently struggle to capture the geometric information crucial for materials modeling across all representations. Instead, these models tend to leverage local information, which is emphasized in some of our novel representations. Our analysis underscores MatText's ability to reveal shortcomings of text-based methods for materials design.", "title_embedding_index": 3553, "title_abs_embedding_index": 3578}, {"title": "Towards Unifying Interpretability and Control: Evaluation via Intervention", "link_suffix": "/forum?id=uOrfve3prk", "link": "https://openreview.net/forum?id=uOrfve3prk", "pdf_link": "https://openreview.net/pdf?id=uOrfve3prk", "keywords": "machine learning interpretability, mechanistic interpretability", "abstract": "With the growing complexity and capability of large language models (LLMs), a need to understand model reasoning has emerged, often motivated by an underlying goal of controlling and aligning models. While numerous interpretability and steering methods have been proposed as solutions, they are typically designed either for understanding or for control, seldom addressing both, with the connection between interpretation and control more broadly remaining tenuous. Additionally, the lack of standardized applications, motivations, and evaluation metrics makes it difficult to assess these methods' practical utility and efficacy. To address the aforementioned issues, we propose intervention as a fundamental goal of interpretability and introduce success criteria to evaluate how well methods are able to control model behavior through interventions. We unify and extend four popular interpretability methods\u2014sparse autoencoders, logit lens, tuned lens, and probing\u2014into an abstract encoder-decoder framework. This framework maps intermediate latent representations to human-interpretable feature spaces, enabling interventions on these interpretable features, which can then be mapped back to latent representations to control model outputs. We introduce two new evaluation metrics: intervention success rate and the coherence-intervention tradeoff, designed to measure the accuracy of explanations and their utility in controlling model behavior. Our findings reveal that (1) although current methods allow for intervention, they are inconsistent across various models and features, (2) lens-based methods outperform others in achieving simple, concrete interventions, and (3) interventions often compromise model performance and coherence, underperforming simpler alternatives, such as prompting, for steering model behavior and highlighting a critical shortcoming of current interpretability approaches in real-world applications requiring control. Code is made available for replicability.", "title_embedding_index": 3554, "title_abs_embedding_index": 3579}, {"title": "The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD", "link_suffix": "/forum?id=DwqoBkj2Mw", "link": "https://openreview.net/forum?id=DwqoBkj2Mw", "pdf_link": "https://openreview.net/pdf?id=DwqoBkj2Mw", "keywords": "differential privacy, heuristics, privacy auditing", "abstract": "We propose a simple heuristic privacy analysis of noisy clipped stochastic gradient descent (DP-SGD) in the setting where only the last iterate is released and the intermediate iterates remain hidden. Namely, our heuristic assumes a linear structure for the model.We show experimentally that our heuristic is predictive of the outcome of privacy auditing applied to various training procedures. Thus it can be used prior to training as a rough estimate of the final privacy leakage. We also probe the limitations of our heuristic by providing some artificial counterexamples where it underestimates the privacy leakage.The standard composition-based privacy analysis of DP-SGD effectively assumes that the adversary has access to all intermediate iterates, which is often unrealistic. However, this analysis remains the state of the art in practice. While our heuristic does not replace a rigorous privacy analysis, it illustrates the large gap between the best theoretical upper bounds and the privacy auditing lower bounds and sets a target for further work to improve the theoretical privacy analyses.", "title_embedding_index": 3555, "title_abs_embedding_index": 3580}, {"title": "Bayesian Optimization via Continual Variational Last Layer Training", "link_suffix": "/forum?id=1jcnvghayD", "link": "https://openreview.net/forum?id=1jcnvghayD", "pdf_link": "https://openreview.net/pdf?id=1jcnvghayD", "keywords": "Bayesian deep learning, bayesian optimization, uncertainty", "abstract": "Gaussian Processes (GPs) are widely seen as the state-of-the-art surrogate models for Bayesian optimization (BO) due to their ability to model uncertainty and their performance on tasks where correlations are easily captured (such as those defined by Euclidean metrics) and their ability to be efficiently updated online. However, the performance of GPs depends on the choice of kernel, and kernel selection for complex correlation structures is often difficult or must be made bespoke. While Bayesian neural networks are a promising direction for higher capacity surrogate models, they have so far seen limited use due to a combination of cost of use and poor performance. In this paper, we propose an approach which offers the strengths of both methods. We build on variational Bayesian last layers (VBLLs), which provide a simple and computationally lightweight approach to Bayesian uncertainty quantification in neural networks. We connect training of these models to exact conditioning in GPs, and propose an efficient online training algorithm that interleaves conditioning and optimization. Our findings suggest that VBLL networks significantly outperform GPs and other BNN architectures on tasks with complex input correlations, and match the performance of well-tuned GPs on established benchmark tasks.", "title_embedding_index": 3556, "title_abs_embedding_index": 3581}, {"title": "Towards Efficient Vision-Language Tuning: More Information Density, More Generalizability", "link_suffix": "/forum?id=SBZiZFp560", "link": "https://openreview.net/forum?id=SBZiZFp560", "pdf_link": "https://openreview.net/pdf?id=SBZiZFp560", "keywords": "Vision-Language Models, Parameter-Efficient Tuning, Information Density, Generalizability", "abstract": "With the advancement of large pre-trained vision-language models, effectively transferring the knowledge embedded within these foundational models to downstream tasks has become a pivotal topic, particularly in data-scarce environments. Recently, parameter-efficient fine-tuning approaches, especially prompt tuning, have garnered considerable attention. To better understand the nature of prompt tuning, we propose the concept of ``Information Density'' (ID) to indicate whether a matrix strongly belongs to certain feature spaces rather than being evenly distributed across various feature spaces. We suppose a higher ID with strong bias across some feature spaces naturally leads to excellent robustness and stability. Our research, inspired by the observation that generalizability is closely linked to the information density of the prompt embedding, introduces the Dense Information Prompt (DIP). DIP aims to enhance information density to improve generalization. Several alternative algorithms to increase ID are proposed and verified effective. With further help of proper initialization and regularization, comprehensive experiments substantiate the superiority of DIP. Notably, DIP surpasses the latest state-of-the-art methods by a substantial margin with an exceptionally small parameter count and no extra inference overhead. Across a range of tasks spanning 11 datasets, DIP improves the average downstream accuracy of classic prompt tuning by up to 5.76%.", "title_embedding_index": 3557, "title_abs_embedding_index": 3582}, {"title": "Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences", "link_suffix": "/forum?id=E48QvQppIN", "link": "https://openreview.net/forum?id=E48QvQppIN", "pdf_link": "https://openreview.net/pdf?id=E48QvQppIN", "keywords": "Bayesian optimization, generative model, antibody, biological sequence", "abstract": "To build effective therapeutics, biologists iteratively mutate antibody sequences to improve binding and stability. Proposed mutations can be informed by previous measurements or by learning from large antibody databases to predict only typical antibodies. Unfortunately, the space of typical antibodies is enormous to search, and experiments often fail to find suitable antibodies on a budget. Here we introduce Clone-informed Bayesian Optimization (CloneBO), a Bayesian optimization procedure that efficiently optimizes antibodies in the lab by teaching a generative model how our immune system optimizes antibodies in our bodies. Our immune system makes antibodies by iteratively evolving specific portions of their sequences to bind their target strongly and stably, resulting in a set of related, evolving sequences known as aclonal family. We train a large language model, CloneLM, on hundreds of thousands of clonal families and use it to design sequences with mutations that are most likely to optimize an antibody in our bodies. We guide our designs to fit previous measurements using a twisted sequential Monte Carlo procedure. We show that CloneBO optimizes antibodies substantially more efficiently than previous methods in realisticin silicoexperiments and designs stronger and more stable binders inin vitrowet lab experiments.", "title_embedding_index": 3558, "title_abs_embedding_index": 3583}, {"title": "Learning and Interpreting Multiple Representations of Semantics in a Neurobiological System", "link_suffix": "/forum?id=hbon6Jbp9Q", "link": "https://openreview.net/forum?id=hbon6Jbp9Q", "pdf_link": "https://openreview.net/pdf?id=hbon6Jbp9Q", "keywords": "Pruning, probing, representation, explainableAI, semantics, brain", "abstract": "A defining feature of computation in the human brain is that different regions can manifest different representations of the same object set. Here we introduce a novel method to learn and interpret multiple neural representations of lexical objects within specific, topographically-defined brain areas. Our approach fine-tunes a pre-trained language model (LM) for each brain region of interest, resulting in better alignment of the LM\u2019s representational space with that of the corresponding brain area. This alignment is achieved through supervised structural pruning of LM features, which selects a subset of features most relevant to the target brain region. We then interpret these retained features using a linear probing task to identify the semantic information they encode. Both the pruning and probing steps are validated through out-of-sample testing, with pruning significantly improving the prediction of brain representations. This method advances on existing approaches by $i$) eliminating the reliance on hand-crafted encoders, reducing potential biases; $ii$) optimizing the alignment process via data-driven learning; and $iii$) providing interpretability of the semantic features in a black-box LM. From a neurobiological perspective, we find that brain regions encoding social and cognitive aspects of lexical items consistently also represent their sensory-motor features, though the reverse does not hold.", "title_embedding_index": 3559, "title_abs_embedding_index": 3584}, {"title": "Mixed-curvature decision trees and random forests", "link_suffix": "/forum?id=HgSIfXTpBE", "link": "https://openreview.net/forum?id=HgSIfXTpBE", "pdf_link": "https://openreview.net/pdf?id=HgSIfXTpBE", "keywords": "representation learning, non-euclidean geometry, decision trees, random forests, hyperbolic geometry, hyperspherical geometry", "abstract": "Decision trees (DTs) and their random forest (RF) extensions are workhorses of classification and regression in Euclidean spaces. However, algorithms for learning in non-Euclidean spaces are still limited. We extend DT and RF algorithms to product manifolds: Cartesian products of several hyperbolic, hyperspherical, or Euclidean components. Such manifolds handle heterogeneous curvature while still factorizing neatly into simpler components, making them compelling embedding spaces for complex datasets. Our novel angular reformulation of DTs respects the geometry of the product manifold, yielding splits that are geodesically convex, maximum-margin, and composable. In the special cases of single-component manifolds, our method simplifies to its Euclidean or hyperbolic counterparts, or introduces hyperspherical DT algorithms, depending on the curvature. We benchmark our method on various classification, regression, and link prediction tasks on synthetic data, graph embeddings, mixed-curvature variational autoencoder latent spaces, and empirical data. Compared to six other classifiers, product DTs and RFs ranked first on 21 of 22 single-manifold benchmarks and 18 of 35 product manifold benchmarks, and placed in the top 2 on 53 of 57 benchmarks overall. This highlights the value of product DTs and RFs as straightforward yet powerful new tools for data analysis in product manifolds.", "title_embedding_index": 3560, "title_abs_embedding_index": 3585}, {"title": "G-AlignNet: Geometry-Driven Quality Alignment for Robust Dynamical Systems Modeling", "link_suffix": "/forum?id=CgXAophhEb", "link": "https://openreview.net/forum?id=CgXAophhEb", "pdf_link": "https://openreview.net/pdf?id=CgXAophhEb", "keywords": "adaptive physical systems, dynamical modeling, data quality alignment, geometric representation learning, geometric optimization", "abstract": "The Neural ODE family has shown promise in modeling complex systems but often assumes consistent data quality, making them less effective in real-world applications with irregularly sampled, incomplete, or multi-resolution data. Current methods, such as ODE-RNN, aim to address these issues but lack formal performance guarantees and can struggle with highly evolving dynamical systems. To tackle this, we propose a novel approach that leverages parameter manifolds to improve robustness in system dynamical modeling. Our method utilizes the orthogonal group as the underlying structure for the parameter manifold, facilitating both quality alignment and dynamical learning in a unified framework. Unlike previous methods, which primarily focus on empirical performance, our approach offers stronger theoretical guarantees of error convergence thanks to the novel architecture and well-posed optimization with orthogonality. Numerical experiments demonstrate significant improvements in interpolation and prediction tasks, particularly in scenarios involving high- and low-resolution data, irregular sampling intervals, etc. Our framework provides a step toward more reliable dynamics learning in changing environments where data quality cannot be assumed.", "title_embedding_index": 3561, "title_abs_embedding_index": 3586}, {"title": "Effects of Random Edge-Dropping on Over-Squashing in Graph Neural Networks", "link_suffix": "/forum?id=ZZwP9zljas", "link": "https://openreview.net/forum?id=ZZwP9zljas", "pdf_link": "https://openreview.net/pdf?id=ZZwP9zljas", "keywords": "Graph Neural Networks, Over-squashing, DropEdge", "abstract": "Message Passing Neural Networks (MPNNs) are a class of Graph Neural Networks (GNNs) that leverage the graph topology to propagate messages across increasingly larger neighborhoods. The message-passing scheme leads to two distinct challenges: over-smoothing and over-squashing. While several algorithms, e.g. DropEdge and its variants \u2013 DropNode, DropAgg and DropGNN \u2013 have successfully addressed the over-smoothing problem, their impact on over-squashing remains largely unexplored. This represents a critical gap in the literature as failure to mitigate over-squashing would make these methods unsuitable for long-range tasks. In this work, we take the first step towards closing this gap by studying the aforementioned algorithms in the context of over-squashing. We present novel theoretical results that characterize the negative effects of DropEdge on sensitivity between distant nodes, suggesting its unsuitability for long-range tasks. Our findings are easily extended to its variants, allowing us to build a comprehensive understanding of how they affect over squashing. We evaluate these methods using real-world datasets, demonstrating their detrimental effects. Specifically, we show that while DropEdge-variants improve test-time performance in short-range tasks, they deteriorate performance in long-range ones. Our theory explains these results as follows: random edge-dropping lowers the effective receptive field of GNNs, which although beneficial for short-range tasks, misaligns the models on long-range ones. This forces the models to overfit to short-range artefacts in the training set, resulting in poor generalization. Our conclusions highlight the need to re-evaluate various methods designed for training deep GNNs, with a renewed focus on modelling long-range interactions.", "title_embedding_index": 3562, "title_abs_embedding_index": 3587}, {"title": "Factorized Implicit Global Convolution for Automotive Computational Fluid Dynamics Prediction", "link_suffix": "/forum?id=YNQF003Ad3", "link": "https://openreview.net/forum?id=YNQF003Ad3", "pdf_link": "https://openreview.net/pdf?id=YNQF003Ad3", "keywords": "Computational Fluid Dynamics, Reparametrized Convolution", "abstract": "Computational Fluid Dynamics (CFD) is crucial for automotive design, requiring the analysis of large 3D point clouds to study how vehicle geometry affects pressure fields and drag forces. However, existing deep learning approaches for CFD struggle with the computational complexity of processing high-resolution 3D data.\n    We propose Factorized Implicit Global Convolution (FIGConv), a novel architecture that efficiently solves CFD problems for very large 3D meshes with arbitrary input and output geometries. FIGConv achieves quadratic complexity $O(N^2)$, a significant improvement over existing 3D neural CFD models that require cubic complexity $O(N^3)$. Our approach combines Factorized Implicit Grids to approximate high-resolution domains, efficient global convolutions through 2D reparameterization, and a U-shaped architecture for effective information gathering and integration.\n    We validate our approach on the industry-standard Ahmed body dataset and the large-scale DrivAerNet dataset. On DrivAerNet, our model achieves an $R^2$ value of 0.95 for drag prediction, outperforming the previous state-of-the-art by a significant margin. This represents a 40% improvement in relative mean squared error and a 70% improvement in absolute mean squared error over prior methods.", "title_embedding_index": 3563, "title_abs_embedding_index": 3588}, {"title": "Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups", "link_suffix": "/forum?id=DTatjJTDl1", "link": "https://openreview.net/forum?id=DTatjJTDl1", "pdf_link": "https://openreview.net/pdf?id=DTatjJTDl1", "keywords": "non-Euclidean generative modeling, denoising diffusion, Lie group", "abstract": "The generative modeling of data on manifold is an important task, for which diffusion models in flat spaces typically need nontrivial adaptations. This article demonstrates how a technique called `trivialization' can transfer the effectiveness of diffusion models in Euclidean spaces to Lie groups. In particular, an auxiliary momentum variable was algorithmically introduced to help transport the position variable between data distribution and a fixed, easy-to-sample distribution. Normally, this would incur further difficulty for manifold data because momentum lives in a space that changes with the position. However, our trivialization technique creates a new momentum variable that stays in a simple $\\textbf{fixed vector space}$. This design, together with a manifold preserving integrator, simplifies implementation and avoids inaccuracies created by approximations such as projections to tangent space and manifold, which were typically used in prior work, hence facilitating generation with high-fidelity and efficiency. The resulting method achieves state-of-the-art performance on protein and RNA torsion angle generation and sophisticated torus datasets. We also, arguably for the first time, tackle the generation of data on high-dimensional Special Orthogonal and Unitary groups, the latter essential for quantum problems.", "title_embedding_index": 3564, "title_abs_embedding_index": 3589}, {"title": "UPT++: Latent Point Set Neural Operators for Modeling System State Transitions", "link_suffix": "/forum?id=qXEmoWllKW", "link": "https://openreview.net/forum?id=qXEmoWllKW", "pdf_link": "https://openreview.net/pdf?id=qXEmoWllKW", "keywords": "neural operator, navier-stokes, molecular dynamics, latent space, particle simulations", "abstract": "Particle methods comprise a wide spectrum of numerical algorithms, ranging from computational fluid dynamics governed by the Navier-Stokes equations to molecular dynamics governed by the many-body Schr\"odinger equation. At its core, these methods represent the continuum as a collection of discrete particles, on which the respective PDE is solved. We introduce UPT++, a latent point set neural operator for modeling the dynamics of such particle systems by mapping a particle set back to a continuous (latent) representation, instead of operating on the particles directly. We argue via what we call thediscretization paradoxthat continuous modeling is advantageous even if the reference numerical discretization scheme comprises particles. Algorithmically, UPT++ extends Universal Physics Transformers -- a framework for efficiently scaling neural operators -- by novel importance-based encoding and decoding. Furthermore, our encoding and decoding enable outputs that remain consistent across varying input sampling resolutions, i.e., UPT++ is a neural operator. We discuss two types of UPT++ operators: (i) time-evolution operator for fluid dynamics, and (ii) sampling operator for molecular dynamics tasks. Experimentally, we demonstrate that our method reliably models complex physics phenomena of fluid dynamics and exhibits beneficial scaling properties, tested on simulations of up to 200k particles. Furthermore, we showcase on molecular dynamics simulations that UPT++ can effectively explore the metastable conformation states of unseen peptide molecules.", "title_embedding_index": 3565, "title_abs_embedding_index": 3590}, {"title": "HAL: Harmonic Learning in High-Dimensional MDPs", "link_suffix": "/forum?id=ZQzGrkihVq", "link": "https://openreview.net/forum?id=ZQzGrkihVq", "pdf_link": "https://openreview.net/pdf?id=ZQzGrkihVq", "keywords": "harmonic learning, harmonic analytic basis training", "abstract": "Since the initial successes of deep reinforcement learning on learning policies purely by interacting with complex high-dimensional state representations and a decade of extensive research, deep neural policies have been applied to a striking variety of fields ranging from pharmaceuticals to foundation models. Yet, one of the strongest assumptions of reinforcement learning is to expect to receive a reward signal from the MDP. While this assumption comes in handy in certain fields, i.e. automated financial markets, it does not naturally fit in many others where the computational complexity of providing such a signal for the task at hand is larger than in fact learning one. Thus, in this paper we focus on learning policies in MDPs without this assumption, and study sequential decision making without having access to information on rewards provided by the MDP. We introduce We introduce harmonic learning, a training method in high-dimensional MDPs, and provide a theoretically well-founded algorithm that significantly improves the sample complexity of deep neural policies. The theoretical and empirical analysis reported in our paper demonstrates that harmonic learning achieves substantial improvements in sample efficient training while constructing more stable and resilient policies that can generalize to uncertain environments.", "title_embedding_index": 3566, "title_abs_embedding_index": 3591}, {"title": "Synthetic Theorem Generation in Lean", "link_suffix": "/forum?id=EeDSMy5Ruj", "link": "https://openreview.net/forum?id=EeDSMy5Ruj", "pdf_link": "https://openreview.net/pdf?id=EeDSMy5Ruj", "keywords": "theorem proving, large language model, synthetic data generation, Lean", "abstract": "The application of large language models (LLMs) to theorem proving presents a promising avenue for advancing formal mathematics. Interactive theorem provers, such as Lean, offer a rigorous framework within which these models can assist in or automate proof discovery, grounding their reasoning capabilities in a sound, verifiable formal system. However, the potential of LLMs in this domain is constrained by the limited availability of formal proof corpora for training. To address this limitation, we introduce a synthetic theorem generator capable of producing novel Lean theorems and their corresponding proofs. Our approach employs forward reasoning to synthesize new propositions from premises drawn from existing Lean libraries. We explore candidate reasoning steps using a search strategy that optimizes for diversity of output, apply them in a linear fashion that avoids irrelevant proof steps, and assess their effect by meta-programmatically executing corresponding Lean tactics. These methods enable the generation of an arbitrary number of new theorems and proofs across various mathematical domains, using common Lean proof tactics while ensuring the correctness of generated theorems by construction.  We demonstrate the efficacy of the generated theorems and training data by fine-tuning models on synthetic theorems and evaluating them on the miniF2F-test benchmark. Our results show improvements in theorem-proving capabilities, with accuracy increasing from 37.3% to 38.5% for the Falcon2-11B model trained solely on Mathlib, and from 38.1% to 39.3% for the same model trained on a mix of rich datasets. These improvements highlight the value of our diverse synthetic data in augmenting limited existing corpora of formal proofs, providing complementary information that enhances LLMs' performance on theorem-proving tasks even when combined with other datasets.", "title_embedding_index": 3567, "title_abs_embedding_index": 3592}, {"title": "Improving Discrete Diffusion with Schedule-Conditioning", "link_suffix": "/forum?id=wQk6yaRGOi", "link": "https://openreview.net/forum?id=wQk6yaRGOi", "pdf_link": "https://openreview.net/pdf?id=wQk6yaRGOi", "keywords": "discrete diffusion, image generation, language model", "abstract": "In research on discrete diffusion generative models, one long-standing mystery is the dominance of the masking state corruption process.\nIn masking diffusion, all data points collapse to a sequence of mask tokens without any transitions between non-mask tokens, ruling out small edits from one unmasked token to another. By contrast, in image modeling, the dominant corruption process is Gaussian noise, which encourages gradual movements in pixel space. In this paper, we propose that masking diffusion dominates due to knowledge of when corruptions occurred. When it makes predictions, it does so conditional on the schedule of previous corruptions; this allows it to devote less capacity to inferring whether a corruption has occurred and more capacity to modeling relationships between tokens. \nWe use this insight to build knowledge of corruptions into other discrete diffusion models; we call our method schedule-conditioned diffusion (SCUD). We show that SCUD generalizes classical discrete diffusion and masking diffusion.\nWe show that applying SCUD to models with different corruption processes leads to improved perplexities on images, text, and protein sequences; Finally, by applying SCUD to models with corruption processes with ``gradual'' structure, we build diffusion models that outperform masking.", "title_embedding_index": 3568, "title_abs_embedding_index": 3593}, {"title": "GeoLoRA: Geometric integration for parameter efficient fine-tuning", "link_suffix": "/forum?id=bsFWJ0Kget", "link": "https://openreview.net/forum?id=bsFWJ0Kget", "pdf_link": "https://openreview.net/pdf?id=bsFWJ0Kget", "keywords": "Low Rank, Finetuninge, Robustness, Rank Adaptive", "abstract": "Low-Rank Adaptation (LoRA) has become a widely used method for parameter-efficient fine-tuning of large-scale, pre-trained neural networks. However, LoRA and its extensions face several challenges, including the need for rank adaptivity, robustness, and computational efficiency during the fine-tuning process. We introduce GeoLoRA, a novel approach that addresses these limitations by leveraging dynamical low-rank approximation theory. GeoLoRA requires only a single backpropagation pass over the small-rank adapters, significantly reducing computational cost as compared to similar dynamical low-rank training methods and making it faster than popular baselines such as AdaLoRA. This allows GeoLoRA to efficiently adapt the allocated parameter budget across the model, achieving smaller low-rank adapters compared to heuristic methods like AdaLoRA and LoRA, while maintaining critical convergence, descent, and error-bound theoretical guarantees. The resulting method is not only more efficient but also more robust to varying hyperparameter settings. We demonstrate the effectiveness of GeoLoRA on several state-of-the-art benchmarks, showing that it outperforms existing methods in both\naccuracy and computational efficiency", "title_embedding_index": 3569, "title_abs_embedding_index": 3594}, {"title": "FedPCE: Federated Personalized Client Embeddings", "link_suffix": "/forum?id=HwwT4HuRSD", "link": "https://openreview.net/forum?id=HwwT4HuRSD", "pdf_link": "https://openreview.net/pdf?id=HwwT4HuRSD", "keywords": "federated learning, deep learning, computer vision, transfer learning, model personalization, image classification, domain adaptation", "abstract": "Despite recent efforts, federated learning (FL) still faces performance challenges due to non-IID data distributions among clients. \nThis distribution shift complicates the addition of new clients and the transfer of federally learned models to unseen data. \nInspired by the adaptation ability of normalization layer parameters, we first demonstrate the effectiveness of models trained using FedBN when being adapted to so far unseen data.\nSpecifically, we extend the adaptation method based on a visual analysis of the normalization layer feature vectors. \nWe introduce Federated Personalized Client Embeddings (FedPCE), which utilizes local embeddings to capture the underlying structure of the normalization feature vectors and, by extension, the dataset.\nOur results show that FedPCE performs comparably to other common FL algorithms during both training and adaptation. \nNotably, FedPCE achieves this performance using only a fraction of the parameters during fine-tuning (32 parameters in our experiments) compared to other methods.", "title_embedding_index": 3570, "title_abs_embedding_index": 3595}, {"title": "LFPS: Learned Farthest Point Sampling", "link_suffix": "/forum?id=6xCgMOm9oM", "link": "https://openreview.net/forum?id=6xCgMOm9oM", "pdf_link": "https://openreview.net/pdf?id=6xCgMOm9oM", "keywords": "Point Clouds, Farthest Point Sampling, Learned Sampling, Loss Function", "abstract": "The processing of point clouds with deep neural networks is relevant for many applications, including remote sensing and autonomous driving with LiDAR sensors. To ensure the computational feasibility of point cloud processing, it is crucial to reduce the cloud's resolution, i.e., its number of points. This downsampling of point clouds requires a deep learning model to abstract information, enabling it to process points within a more holistic context. A traditional technique for reducing the resolution of a point cloud is Farthest Point Sampling (FPS). It achieves a uniform point distribution but does not adapt to the network's learning process. In contrast, learned sampling methods are adaptive to the network but cannot be seamlessly incorporated into diverse network architectures and do not guarantee uniformity. Thus, they can miss informative regions of the point cloud, reducing their effectiveness for large-scale point cloud applications.To address these limitations and bridge the gap between algorithmic and learned sampling methods, we introduce Learned Farthest Point Sampling (LFPS), an innovative approach that combines the advantages of both algorithmic and learned techniques. Our method relies on a novel loss function designed to enforce a uniform point distribution. We show by theoretical proof that its minima guarantee a uniformity comparable to FPS. Furthermore, we extend the loss function to include information about key points, enabling the network to adaptively influence point selection while preserving uniform distribution in relevant as well as less relevant regions. In experimental studies, we evaluate the performance of LFPS both independently and within existing network architectures. Our results (a) show that LFPS serves as a plug-in alternative for algorithmic sampling methods, particularly as a faster alternative to FPS for large-scale point clouds, and (b) confirm the enhanced performance of LFPS across various tasks, emphasizing its versatility and effectiveness.", "title_embedding_index": 3571, "title_abs_embedding_index": 3596}, {"title": "Model Tells Itself Where to Attend: Steerable Prompting for Reliable Reading Comprehension of LLM", "link_suffix": "/forum?id=AT64R0ivUO", "link": "https://openreview.net/forum?id=AT64R0ivUO", "pdf_link": "https://openreview.net/pdf?id=AT64R0ivUO", "keywords": "Reading Comprehension, Steerable Prompting, Automatic Attention Steering", "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various real-world tasks. However, they often struggle to fully comprehend and effectively utilize their input contexts, resulting in responses that are hallucinated. This difficulty increases for contexts that are long or contain distracting information, which can divert LLMs from fully capturing essential evidence. To address this issue, many works use prompting to help LLMs comprehend contextual information more reliably. For instance, iterative prompting highlights key information in two steps that first ask the LLM to identify important pieces of context and then derive answers accordingly. However, textual prompting methods are constrained to highlighting key information implicitly in token space, which is often insufficient to fully steer the model's attention. To improve model reading comprehension, we propose SteerPrompt, a method that automatically identifies key contextual information and explicitly highlights it by steering an LLM's attention scores. Like prompting, SteerPrompt is applied at inference time and does not require changing any model parameters. Our experiments on open-book QA demonstrate that SteerPrompt effectively enables models to grasp essential contextual information, leading to substantially improved problem-solving performance, e.g., an average improvement of 7.95% for LLAMA3-70B-Instruct. Code will be publicly available.", "title_embedding_index": 3572, "title_abs_embedding_index": 3597}, {"title": "OneFit: Unified Neural Garment Simulation using Function-based Representation and Learning", "link_suffix": "/forum?id=86HwTRg0qh", "link": "https://openreview.net/forum?id=86HwTRg0qh", "pdf_link": "https://openreview.net/pdf?id=86HwTRg0qh", "keywords": "Garment draping, Unsupervised learning, Neural simulation, Virtual try-on", "abstract": "The digital garment modeling using self-supervised learning has significantly evolved in terms of the speed and visual quality of garment deformation simulations. Recent advances  have incorporated size-awareness which allows to drape garments realistically, by stretching only to avoid collisions with the human body. It allows their deployment into virtual try-on systems where the goal is to observe garment fitting. However, a major-shortcoming is that they learn mesh-specific models which requires a distinct model to be trained for each mesh representations of a given garment.In this paper, we introduce a novel self-supervised garment simulation approach to learn garment deformations using only functions. \nFirst, our PolyFit module converts the garment mesh patches into functions which allows a compact yet detail-preserving representation.\nThen, OneFit learns the deformations of these patches by restricting the space of the PolyFit function transformations conditioned on different body poses, in a physics-guided and an intrinsic geometry-aware manner. It not only extends to various mesh-representations of a given garment but also to diverse representations of a garment type. Hence, a model trained on single garment can generalise across several garment types. Thanks to its compact representation, it is computationally superior to its counterparts, in terms of both training and inference and scales well to unseen garments. Thus, by training OneFit on a set of garments, a mesh-agnostic, garment-agnostic deformation model can be learnt which can be finetuned (or postprocessed) to accommodate unseen garment types.", "title_embedding_index": 3573, "title_abs_embedding_index": 3598}, {"title": "Continuity-Driven Pose Estimation for Videos", "link_suffix": "/forum?id=b9qIPrOfCw", "link": "https://openreview.net/forum?id=b9qIPrOfCw", "pdf_link": "https://openreview.net/pdf?id=b9qIPrOfCw", "keywords": "video pose estiamtion", "abstract": "Video-based pose estimation plays a critical role in understanding human actions and enabling effective human-computer interaction. By exploiting temporal information from video frames, it enhances the localization of human keypoints. Previous feature-fusion methods often rely on a frozen single-frame backbone trained on individual frames, followed by a network to learn temporal information from video sequences. Consequently, these approaches fail to capture the temporal continuity between frames at the backbone network level, thereby restricting the network's capacity to effectively learn and leverage sequential information. In this paper, we introduce a novel approach to supervise continuity in the whole video pose estimation model from two perspectives: semantic continuity and pixel-wise keypoint distribution continuity. To this end, we propose a Semantic Alignment Space, where a semantic alignment encodes feature maps from different frames into this space, ensuring continuous supervision of the encoded representations. To further maintain pixel-wise keypoint distribution continuity, we introduce the Trajectory Probability Difference Integration method, which minimizes the trajectory difference expectation across frames. Additionally, to better capture temporal dependencies, we present a Multi-frame Heatmap Fusion structure that aggregates heatmaps from adjacent frames for a more refined output. Extensive experiments on the PoseTrack17, PoseTrack18, and PoseTrack21 datasets demonstrate the effectiveness of our approach, consistently achieving state-of-the-art results.", "title_embedding_index": 3574, "title_abs_embedding_index": 3599}]