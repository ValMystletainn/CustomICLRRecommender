[
    {
        "title": "LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement",
        "link_suffix": "/forum?id=YLIsIzC74j",
        "link": "https://openreview.net/forum?id=YLIsIzC74j",
        "pdf_link": "https://openreview.net/pdf?id=YLIsIzC74j",
        "keywords": "Macro placement, Chip design, EDA",
        "abstract": "Machine learning techniques have shown great potential in enhancing macro placement, a critical stage in modern chip design.\nHowever, existing methods primarily focus ononlineoptimization ofintermediate surrogate metricsthat are available at the current placement stage, rather than directly targeting thecross-stage metrics---such as the timing performance---that measure the final chip quality.\nThis is mainly because of the high computational costs associated with performing post-placement stages for evaluating such metrics, making theonlineoptimization impractical.\nConsequently, these optimizations struggle to align with actual performance improvements and can even lead to severe manufacturing issues.\nTo bridge this gap, we proposeLaMPlace, whichLearnsaMask for optimizing cross-stage metrics in macro placement.\nSpecifically, LaMPlace trains a predictor onofflinedata to estimate thesecross-stage metricsand then leverages the predictor to quickly generate a mask, i.e., a pixel-level feature map that quantifies the impact of placing a macro in each chip grid location on the design metrics.\nThis mask essentially acts as a fast evaluator, enabling placement decisions based oncross-stage metricsrather thanintermediate surrogate metrics.\nExperiments on commonly used benchmarks demonstrate that LaMPlace significantly improves the chip quality across several key design metrics, achieving an average improvement of 9.6%, notably 43.0% and 30.4% in terms of WNS and TNS, respectively, which are two crucial cross-stage metrics that reflect the final chip quality in terms of the timing performance."
    },
    {
        "title": "Computing Circuits Optimization via Model-Based Circuit Genetic Evolution",
        "link_suffix": "/forum?id=KWH4UIoQKS",
        "link": "https://openreview.net/forum?id=KWH4UIoQKS",
        "pdf_link": "https://openreview.net/pdf?id=KWH4UIoQKS",
        "keywords": "AI Chips Design, Computing Circuits Optimization, Evolutionary Algorithm, Reinforcement Learning",
        "abstract": "Optimizing computing circuits such as multipliers and adders is a fundamental challenge in modern integrated circuit design. Recent efforts propose formulating this optimization problem as a reinforcement learning (RL) proxy task, offering a promising approach to search high-speed and area-efficient circuit design solutions. However, we show that the RL-based formulation (proxy task) converges to a local optimal design solution (original task) due to the deceptive reward signals and incrementally localized actions in the RL-based formulation. To address this challenge, we propose a novel model-based circuit genetic evolution (MUTE) framework, which reformulates the problem as a genetic evolution process by proposing a grid-based genetic representation of design solutions. This novel formulation avoids misleading rewards by evaluating and improving generated solutions using the true objective value rather than proxy rewards. To promote globally diverse exploration, MUTE proposes a multi-granularity genetic crossover operator that recombines design substructures at varying column ranges between two grid-based genetic solutions. To the best of our knowledge, MUTE is the first to reformulate the problem as a circuit genetic evolution process, which enables effectively searching for global optimal design solutions. We evaluate MUTE on several fundamental computing circuits, including multipliers, adders, and multiply-accumulate circuits. Experiments on these circuits demonstrate that MUTE significantly Pareto-dominates state-of-the-art approaches in terms of both area and delay. Moreover, experiments demonstrate that circuits designed by MUTE well generalize to large-scale computation-intensive circuits as well."
    },
    {
        "title": "FSL-MIC: An Attentional Few-Shot Learning Framework for EEG Motor Imagery Classification",
        "link_suffix": "/forum?id=PcE0yAGAGW",
        "link": "https://openreview.net/forum?id=PcE0yAGAGW",
        "pdf_link": "https://openreview.net/pdf?id=PcE0yAGAGW",
        "keywords": "Few-shot learning, Data Augmentation, EEG, motor imagery, BCI, Transformer, CNN",
        "abstract": "Electroencephalography (EEG) is a key non-invasive technique used to investigate brain activity, particularly in motor imagery (MI) research. Traditional methods for classifying EEG signals often rely on handcrafted features and heuristic parameters, which can limit generalization across tasks and subjects. Recent advances in deep learning, particularly few-shot learning (FSL), offer promising alternatives to improve classification accuracy in scenarios with limited training data. This study explores the effectiveness of FSL algorithms, including Relation Networks, to enhance MI classification. It also examines how transfer learning and data augmentation techniques contribute to improving classification performance.We propose a novel framework with three core modules\u2014feature embedding, attention, and relation\u2014that facilitates the classification of unseen subject categories using only a few labeled samples. The attention mechanism identifies key features related to the query data, while the relation module predicts query labels by modeling relationships between support and query data across subjects. Our experimental results demonstrate the effectiveness of our approach on two benchmark datasets, BCI 2a and BCI 2b, as well as our experimental dataset. The proposed FSL framework significantly outperforms traditional methods, offering promising applications in real-time Brain-Computer Interface (BCI) systems across various EEG setups. This research advances the understanding of machine learning in EEG applications and highlights the potential of FSL techniques in overcoming the challenges of limited training data in MI classification."
    },
    {
        "title": "Learning to Watermark LLM-generated Text via Reinforcement Learning",
        "link_suffix": "/forum?id=r6aX67YhD9",
        "link": "https://openreview.net/forum?id=r6aX67YhD9",
        "pdf_link": "https://openreview.net/pdf?id=r6aX67YhD9",
        "keywords": "LLM Watermark",
        "abstract": "We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks) with no generation overhead. It also allows watermarked model open-sourcing. In addition, if used together with alignment, the extra overhead introduced is low -- only training an extra reward model (i.e. our detector). We hope our work can bring more effort into studying a broader watermark design that is not limited to working with a fixed LLM."
    },
    {
        "title": "CAN - CONTINUOUSLY ADAPTING NETWORKS",
        "link_suffix": "/forum?id=SI6zocV2SS",
        "link": "https://openreview.net/forum?id=SI6zocV2SS",
        "pdf_link": "https://openreview.net/pdf?id=SI6zocV2SS",
        "keywords": "Continual Learning, Catastrophic Forgetting, Synaptic Plasticity, Hebbian Learning, Adaptive Neural Networks",
        "abstract": "Catastrophic forgetting is a fundamental challenge in neural networks that prevents continuous learning, which is one of the properties essential for achieving true general artificial intelligence. When trained sequentially on multiple tasks, conventional neural networks overwrite previously learned knowledge, hindering their ability to retain and apply past experiences. However, people and other animals can learn new things continuously without forgetting them. To overcome this problem, we devised an architecture that preserves significant task-specific connections by combining selective neuron freezing with Hebbian learning principles. Hebbian learning enables the network to adaptively strengthen synaptic connections depending on parameter activation. It is inspired by the synaptic plasticity seen in brains. By preserving the most important neurons using selective neuron freezing, new tasks can be trained without changing them. Experiments conducted on standard datasets show that our model significantly reduces the risk of catastrophic forgetting, allowing the network to learn continually."
    },
    {
        "title": "Geometric Representation Condition Improves Equivariant Molecule Generation",
        "link_suffix": "/forum?id=vFVjJsy3PG",
        "link": "https://openreview.net/forum?id=vFVjJsy3PG",
        "pdf_link": "https://openreview.net/pdf?id=vFVjJsy3PG",
        "keywords": "molecule generation, equivariant generative models, representation, geometric deep learning, diffusion models",
        "abstract": "Recent advancements in molecular generative models have demonstrated substantial potential in accelerating scientific discovery, particularly in drug design. However, these models often face challenges in generating high-quality molecules, especially in conditional scenarios where specific molecular properties must be satisfied. In this work, we introduce GeoRCG, a general framework to enhance the performance of molecular generative models by integrating geometric representation conditions. We decompose the molecule generation process into two stages: first, generating an informative geometric representation; second, generating a molecule conditioned on the representation. Compared to directly generating a molecule, the relatively easy-to-generate representation in the first-stage guides the second-stage generation to reach a high-quality molecule in a more goal-oriented and much faster way. Leveraging EDM as the base generator, we observe significant quality improvements in unconditional molecule generation on the widely-used QM9 and GEOM-DRUG datasets. More notably, in the challenging conditional molecular generation task, our framework achieves an average 31% performance improvement over state-of-the-art approaches, highlighting the superiority of conditioning on semantically rich geometric representations over conditioning on individual property values as in previous approaches. Furthermore, we show that, with such representation guidance, the number of diffusion steps can be reduced to as small as 100 while maintaining superior generation quality than that achieved with 1,000 steps, thereby significantly accelerating the generation process."
    },
    {
        "title": "The Perils of Optimizing Learned Reward Functions: Low Training Error Does Not Guarantee Low Regret",
        "link_suffix": "/forum?id=OmFlDvsvc3",
        "link": "https://openreview.net/forum?id=OmFlDvsvc3",
        "pdf_link": "https://openreview.net/pdf?id=OmFlDvsvc3",
        "keywords": "Reward learning, RLHF, RL, Safety, Distributional shift, Generalization, Learning Theory",
        "abstract": "In reinforcement learning, specifying reward functions that capture the intended task can be very challenging. Reward learning aims to address this issue by learning the reward function. However, a learned reward model may have a low loss on the training distribution, and yet subsequently produce a policy with large regret. We say that such a reward model has an error-regret mismatch. The main source of an error-regret mismatch is the distribution shift that commonly occurs during policy optimization. In this paper, we mathematically show that a sufficiently low expected test error of the reward model guarantees low worst-case regret, but that for any fixed expected test error, there exist realistic data distributions that allow for error-regret mismatch to occur. We then show that similar problems persist even when using policy regularization techniques, commonly employed in methods such as RLHF. Our theoretical results highlight the importance of developing new ways to measure the quality of learned reward models."
    },
    {
        "title": "Using Interleaved Ensemble Unlearning to Keep Backdoors at Bay for Finetuning Vision Transformers",
        "link_suffix": "/forum?id=fr7cLDfNNU",
        "link": "https://openreview.net/forum?id=fr7cLDfNNU",
        "pdf_link": "https://openreview.net/pdf?id=fr7cLDfNNU",
        "keywords": "Vision Transformer, Backdoor Defence",
        "abstract": "Vision Transformers (ViTs) have become popular in computer vision tasks. Backdoor attacks, which trigger undesirable behaviours in models during inference, threaten ViTs' performance, particularly in security-sensitive tasks. Although backdoor defences have been developed for Convolutional Neural Networks (CNNs), they are less effective for ViTs, and defences tailored to ViTs are scarce. To address this, we present Interleaved Ensemble Unlearning (IEU), a method for finetuning clean ViTs on backdoored datasets. In stage 1, a shallow ViT is finetuned to have high confidence on backdoored data and low confidence on clean data. In stage 2, the shallow ViT acts as a \"gate\" to block potentially poisoned data from the defended ViT. This data is added to an unlearn set and asynchronously unlearned via gradient ascent. We demonstrate IEU's effectiveness on three datasets against 11 state-of-the-art backdoor attacks and show its versatility by applying it to different model architectures."
    },
    {
        "title": "Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization",
        "link_suffix": "/forum?id=1poUSIGSCI",
        "link": "https://openreview.net/forum?id=1poUSIGSCI",
        "pdf_link": "https://openreview.net/pdf?id=1poUSIGSCI",
        "keywords": "Interpretability, Interpretable Latent Space, Interpretable Directions, Space-Filling Vector Quantization",
        "abstract": "Generative adversarial networks (GANs) learn a latent space whose samples can be mapped to real-world images. Such latent spaces are difficult to interpret. Some earlier supervised methods aim to create an interpretable latent space or discover interpretable directions that require exploiting data labels or annotated synthesized samples for training. However, we propose using a modification of vector quantization called space-filling vector quantization (SFVQ), which quantizes the data on a piece-wise linear curve. SFVQ can capture the underlying morphological structure of the latent space and thus make it interpretable. We apply this technique to model the latent space of pretrained StyleGAN2 and BigGAN networks on various datasets. Our experiments show that the SFVQ curve yields a general interpretable model of the latent space that determines which part of the latent space corresponds to what specific generative factors. Furthermore, we demonstrate that each line of SFVQ's curve can potentially refer to an interpretable direction for applying intelligible image transformations. We also showed that the points located on an SFVQ line can be used for controllable data augmentation."
    },
    {
        "title": "VideoPhy: Evaluating Physical Commonsense for Video Generation",
        "link_suffix": "/forum?id=9D2QvO1uWj",
        "link": "https://openreview.net/forum?id=9D2QvO1uWj",
        "pdf_link": "https://openreview.net/pdf?id=9D2QvO1uWj",
        "keywords": "text-to-video generation, physical commonsense, video-text alignment, generative modeling, video evaluation",
        "abstract": "Recent advances in internet-scale video data pretraining have led to the development of text-to-video generative models that can create high-quality videos across a broad range of visual concepts, synthesize realistic motions and render complex objects. Hence, these generative models have the potential to become general-purpose simulators of the physical world. However, it is unclear how far we are from this goal with the existing text-to-video generative models. To this end, we present VideoPhy, a benchmark designed to assess whether the generated videos follow physical commonsense for real-world activities (e.g. marbles will roll down when placed on a slanted surface). Specifically, we curate diverse prompts that involve interactions between various material types in the physical world (e.g., solid-solid, solid-fluid, fluid-fluid). We then generate videos conditioned on these captions from diverse state-of-the-art text-to-video generative models, including open models (e.g., CogVideoX) and closed models (e.g., Lumiere, Dream Machine). Our human evaluation reveals that the existing models severely lack the ability to generate videos adhering to the given text prompts, while also lack physical commonsense. Specifically, the best performing model, CogVideoX-5B, generates videos that adhere to the caption and physical laws for 39.6% of the instances. VideoPhy thus highlights that the video generative models are far from accurately simulating the physical world. Finally, we propose an auto-evaluator, VideoCon-Physics, to assess the performance reliably for the newly released models. We will release the data and model in the camera-ready version."
    },
    {
        "title": "SSR: Alignment-Aware Modality Connector for Speech Language Models",
        "link_suffix": "/forum?id=tZDhrhUOcs",
        "link": "https://openreview.net/forum?id=tZDhrhUOcs",
        "pdf_link": "https://openreview.net/pdf?id=tZDhrhUOcs",
        "keywords": "modality fusion, speech language model, spoken language understanding",
        "abstract": "Fusing speech into pre-trained language model (SpeechLM) usually suffers from inefficient encoding of long-form speech and catastrophic forgetting of pre-trained text modality. We propose SSR-connector (Segmented Speech Representation Connector) for better modality fusion. Leveraging speech-text alignments, our approach segments and compresses speech features to match the granularity of text embeddings. Additionally, we introduce a two-stage training pipeline that includes the distillation and fine-tuning phases to mitigate catastrophic forgetting. SSR-connector outperforms existing mechanism for speech-text modality fusion, consistently achieving better speech understanding (e.g., $+10$ accuracy on StoryCloze and $+20$ on Speech-MMLU) while preserving pre-trained text ability."
    },
    {
        "title": "Enforcing Latent Euclidean Geometry in VAEs for Statistical Manifold Interpolation",
        "link_suffix": "/forum?id=a72vorQK8v",
        "link": "https://openreview.net/forum?id=a72vorQK8v",
        "pdf_link": "https://openreview.net/pdf?id=a72vorQK8v",
        "keywords": "scRNA-seq, Riemannian geometry, representation learning, trajectory inference, VAEs, statistical manifolds",
        "abstract": "Latent linear interpolations are a powerful tool for navigating the representation space of deep generative models. This aspect is particularly relevant in applied settings, where meaningful latent traversals can be learnt to represent the evolution of a system's trajectory and mapped back to the often complex and high-dimensional data space. However, when data lies on a manifold with complex geometry, linear interpolations of the representation space do not directly correspond to geodesic paths along the manifold unless enforced. An example of such a setting is scRNA-seq, where high-dimensional and discrete cellular data is assumed to lie on a negative binomial statistical manifold modelled by the decoder of a variational autoencoder. We introduce FlatVI, a novel training framework enforcing Euclidean geometry in the latent space of discrete-likelihood variational autoencoders modelling count data. In our regularisation setting, straight lines in the latent domain correspond to geodesic interpolations in the decoded space, improving the combination of our model with methods assuming Euclidean latent geometry. Results on simulated data empirically support our claims, while experiments on temporally resolved biological datasets show improvements in the reconstruction of cellular trajectories and the learning of biologically meaningful velocity fields."
    },
    {
        "title": "Scaleable Quantum Control via Physics Constrained Reinforcement Learning",
        "link_suffix": "/forum?id=YPvI7SofeZ",
        "link": "https://openreview.net/forum?id=YPvI7SofeZ",
        "pdf_link": "https://openreview.net/pdf?id=YPvI7SofeZ",
        "keywords": "reinforcement learning, quantum computing, quantum control, quantum dynamics, control theory",
        "abstract": "Quantum optimal control is concerned with the realisation of desired dynamics in quantum systems, serving as a linchpin for advancing quantum technologies and fundamental research. \nAnalytic approaches and standard optimisation algorithms do not yield satisfactory solutions for large quantum systems, and especially not for real world quantum systems which are open and noisy. \nWe devise a physics-informed Reinforcement Learning (RL) algorithm that restricts the space of possible solutions.\nWe incorporate priors about the desired time scales of the quantum state dynamics -- as well as realistic control signal limitations -- as constraints to the RL algorithm. \nThese physics-informed constraints additionally improve computational scalability by facilitating parallel optimisation. \nWe evaluate our method on three broadly relevant quantum systems (multi-level $\\Lambda$ system, Rydberg atom and superconducting transmon) and incorporate real-world complications, arising from dissipation and control signal perturbations. \nWe achieve both higher fidelities -- which exceed 0.99 across all systems --  and better robustness to time-dependent perturbations and experimental imperfections than previous methods. \nLastly, we demonstrate that incorporating multi-step feedback can yield solutions robust even to strong perturbations."
    },
    {
        "title": "Multi-environment Topic Models",
        "link_suffix": "/forum?id=Q3RoP5IhHy",
        "link": "https://openreview.net/forum?id=Q3RoP5IhHy",
        "pdf_link": "https://openreview.net/pdf?id=Q3RoP5IhHy",
        "keywords": "Topic Models, ML for Social Science, Out-of-distribution Generalization, Multi-environment learning",
        "abstract": "Probabilistic topic models are a powerful tool for extracting latent themes from large text datasets. In many text datasets, we also observe per-document covariates (e.g., source, style, political affiliation) that act as environments that modulate a \"global\" (environment-agnostic) topic representation. Accurately learning these representations is important for prediction on new documents in unseen environments and for estimating the causal effect of topics on real-world outcomes. To this end, we introduce the Multi-environment Topic Model (MTM), an unsupervised probabilistic model that separates global and environment-specific terms. Through experimentation on various political content, from ads to tweets and speeches, we show that the MTM produces interpretable global topics with distinct environment-specific words. On multi-environment data, the MTM outperforms strong baselines in and out-of-distribution. It also enables the discovery of accurate causal effects."
    },
    {
        "title": "ViVa: Video-Trained Value Functions for Guiding Online RL from Diverse Data",
        "link_suffix": "/forum?id=uN2kkAvarI",
        "link": "https://openreview.net/forum?id=uN2kkAvarI",
        "pdf_link": "https://openreview.net/pdf?id=uN2kkAvarI",
        "keywords": "Reinforcement Learning, Unsupervised Pretraining, Robotics",
        "abstract": "Online reinforcement learning (RL) with sparse rewards poses a challenge partly because of the lack of feedback on states leading to the goal. Furthermore, expert offline data with reward signal is rarely available to provide this feedback and bootstrap online learning. How can we guide online agents to the right solution without this on-task data? Reward shaping offers a solution by providing fine-grained signal to nudge the policy towards the optimal solution. However, reward shaping often requires domain knowledge to hand-engineer heuristics for a specific goal. To enable more general and inexpensive guidance, we propose and analyze a data-driven methodology that automatically guides RL by learning from widely available video data such as Internet recordings, off-task demonstrations, task failures, and undirected environment interaction. By learning a model of optimal goal-conditioned value from diverse passive data, we open the floor to scaling up and using a wide variety of data sources to model general goal-reaching behaviors relevant to guiding online RL. Specifically, we use intent-conditioned value functions to learn from diverse video and incorporate these goal-conditioned values into the reward. Our experiments show that video-trained value functions work well with a variety of data sources, exhibit positive transfer from human video pre-training, can generalize to unseen goals, and scale with dataset size."
    },
    {
        "title": "UncertaintyRAG: Span Uncertainty Enhanced Long-Context Modeling for Retrieval-Augmented Generation",
        "link_suffix": "/forum?id=SR8LFpmVun",
        "link": "https://openreview.net/forum?id=SR8LFpmVun",
        "pdf_link": "https://openreview.net/pdf?id=SR8LFpmVun",
        "keywords": "RAG, Long-contex, Distribution Shift, Signal-to-noise Ratio, Unsupervised Learning",
        "abstract": "We introduce $UncertaintyRAG$, a novel method for long-context Retrieval-Augmented Generation (RAG) that leverages Signal-to-Noise Ratio (SNR)-based span uncertainty to estimate similarity between text chunks. This span uncertainty improves the calibration of model predictions, enhancing robustness and addressing semantic inconsistencies caused by random chunking. Utilizing this, we develop an efficient unsupervised learning technique for training the retrieval model and design an effective data sampling and scaling strategy. $UncertaintyRAG$ achieves a 2.03% improvement over baselines on LLaMA-2-7B, reaching state-of-the-art performance while using only 4% of the training data compared to other powerful open-source retrieval models under distribution shift settings. Our method demonstrates strong calibration through span uncertainty, resulting in better generalization and robustness in long-context RAG tasks. Moreover, $UncertaintyRAG$ offers a lightweight retrieval model that can be seamlessly integrated into any large language model with varying context window lengths without the need for fine-tuning, highlighting the versatility of our approach."
    },
    {
        "title": "On the Completeness of Invariant Geometric Deep Learning Models",
        "link_suffix": "/forum?id=52x04chyQs",
        "link": "https://openreview.net/forum?id=52x04chyQs",
        "pdf_link": "https://openreview.net/pdf?id=52x04chyQs",
        "keywords": "geometric deep learning, invariant models, completeness, expressiveness, graph neural network, subgraph graph neural network",
        "abstract": "Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features in point clouds. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of a wide range of invariant models. We first rigorously characterize the expressiveness of the most classic invariant model, message-passing neural networks incorporating distance (DisGNN), restricting its unidentifiable cases to be only highly symmetric point clouds. We then prove that GeoNGNN, the geometric counterpart of one of the simplest subgraph graph neural networks, can effectively break these corner cases' symmetry and thus achieve E(3)-completeness. By leveraging GeoNGNN as a theoretical tool, we further prove that: 1) most subgraph GNNs developed in traditional graph learning can be seamlessly extended to geometric scenarios with E(3)-completeness; 2) DimeNet, GemNet and SphereNet, three well-established invariant models, are also all capable of achieving E(3)-completeness. Our theoretical results fill the gap in the expressive power of invariant models, contributing to a rigorous and comprehensive understanding of their capabilities."
    },
    {
        "title": "On Pre-training of Multimodal Language Models Customized for Chart Understanding",
        "link_suffix": "/forum?id=94LyPGDi0Y",
        "link": "https://openreview.net/forum?id=94LyPGDi0Y",
        "pdf_link": "https://openreview.net/pdf?id=94LyPGDi0Y",
        "keywords": "Multimodal LLM, Chart Understanding",
        "abstract": "Recent studies customizing Multimodal Large Language Models (MLLMs) for domain-specific tasks have yielded promising results, especially in the field of scientific chart comprehension. These studies generally utilize visual instruction tuning with specialized datasets to enhance question and answer (QA) accuracy within the chart domain. However, they often neglect the fundamental discrepancy between natural image-caption pre-training data and digital chart image-QA data, particularly in the models' capacity to extract underlying numeric values from charts. This paper tackles this oversight by exploring the training processes necessary to improve MLLMs' comprehension of charts. We present three key findings: (1) Incorporating raw data values in alignment pre-training markedly improves comprehension of chart data. (2) Replacing images with their textual representation randomly during end-to-end fine-tuning transfer the language reasoning capability to chart interpretation skills. (3) Requiring the model to first extract the underlying chart data and then answer the question in the fine-tuning can further improve the accuracy. Consequently, we introduce CHOPINLLM, an MLLM tailored for in-depth chart comprehension. CHOPINLLM effectively interprets various types of charts, including unannotated ones, while maintaining robust reasoning abilities. Furthermore, we establish a new benchmark to evaluate MLLMs' understanding of different chart types across various comprehension levels. Experimental results show that CHOPINLLM exhibits strong performance in understanding both annotated and unannotated charts across a wide range of types."
    },
    {
        "title": "Generating Multi-Modal and Multi-Attribute Single-Cell Counts with CFGen",
        "link_suffix": "/forum?id=3MnMGLctKb",
        "link": "https://openreview.net/forum?id=3MnMGLctKb",
        "pdf_link": "https://openreview.net/pdf?id=3MnMGLctKb",
        "keywords": "scRNA-seq, Flow Matching, generative modeling, multiomics",
        "abstract": "Generative modeling of single-cell RNA-seq data has proven instrumental for tasks like trajectory inference, batch effect removal, and gene expression generation. However, the most recent deep generative models simulating synthetic single cells from noise operate on pre-processed continuous gene expression approximations, overlooking the discrete nature of single-cell data, which limits their effectiveness and hinders the incorporation of robust noise models. Additionally, aspects like controllable multi-modal and multi-label generation of cellular data are underexplored. This work introduces Cell Flow for Generation (CFGen), a flow-based conditional generative model that accounts for the discrete nature of single-cell data. CFGen generates whole-genome multimodal single-cell counts reliably, improving the recovery of crucial biological data characteristics while tackling relevant generative tasks such as rare cell type augmentation and batch correction. We also introduce a novel framework for compositional data generation using Flow Matching. By showcasing CFGen on a diverse set of biological datasets and settings, we provide evidence of its value to the fields of computational biology and deep generative models."
    },
    {
        "title": "Powering Neural Architecture Search with Robust Masked Autoencoders",
        "link_suffix": "/forum?id=l5EYUpoTrZ",
        "link": "https://openreview.net/forum?id=l5EYUpoTrZ",
        "pdf_link": "https://openreview.net/pdf?id=l5EYUpoTrZ",
        "keywords": "Neural Architecture Search, Masked Autoencoder",
        "abstract": "Neural Architecture Search (NAS) relies heavily on labeled data, which is labor-intensive and time-consuming to obtain. In this paper, we propose a novel NAS method based on an unsupervised paradigm, specifically Masked Autoencoders (MAE), thereby eliminating the need for labeled data during the searching process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) in the unsupervised setting by designing a hierarchical decoder. Through extensive experiments conducted across various search spaces and datasets, we demonstrate the effectiveness and robustness of our method, offering empirical evidence of its superiority over baseline approaches."
    },
    {
        "title": "Learning Orthogonal Multi-Index Models: A Fine-Grained Information Exponent Analysis",
        "link_suffix": "/forum?id=QY52D9BeJo",
        "link": "https://openreview.net/forum?id=QY52D9BeJo",
        "pdf_link": "https://openreview.net/pdf?id=QY52D9BeJo",
        "keywords": "multi-index model, information exponent, sample complexity, stochastic gradient descent",
        "abstract": "The information exponent (Ben Arous et al. (2021)) --- which is equivalent to the lowest degree in the Hermite\n  expansion of the link function for Gaussian single-index models --- has played an important role in predicting the \n  sample complexity of online stochastic gradient descent (SGD) in various learning tasks. In this work, we \n  demonstrate that, for multi-index models, focusing solely on the lowest degree can miss key structural details \n  and result in suboptimal rates.Specifically, we consider the task of learning target functions of form $f_*(\\mathbf{x}) = \\sum_{k=1}^{P} \\phi({\\mathbf{v_k}^*} \\cdot \\mathbf{x})$, \n  where $P \\ll d$, the ground-truth directions $\\{ v_k^* \\}_{k=1}^P$ are orthonormal, and only the second and \n  $2L$-th Hermite coefficients of the link function $\\phi$ can be nonzero.Based on the theory of information exponent, when the lowest degree is $2L$, recovering the directions requires \n  $d^{2L-1}\\mathrm{poly}(P)$ samples, and when the lowest degree is $2$, only the relevant subspace (not the exact \n  directions) can be recovered due to the rotational invariance of the second-order terms. In contrast, we show that \n  by considering both second- and higher-order terms, we can first learn the relevant space via the second-order\n  terms, and then the exact directions using the higher-order terms, and the overall sample complexity of online \n  SGD is $d \\mathrm{poly}(P)$."
    },
    {
        "title": "Physics-constrained Graph Symbolic Regression",
        "link_suffix": "/forum?id=Ia17iAtr0P",
        "link": "https://openreview.net/forum?id=Ia17iAtr0P",
        "pdf_link": "https://openreview.net/pdf?id=Ia17iAtr0P",
        "keywords": "Symbolic Regression, Physics-constrained, Graph Neural Network, Reinforcement Learning, Monte-Carlo Tree Search, Expression Tree, Automated Feature Engineering, Symbolic Graph",
        "abstract": "As data-driven scientific discovery increasingly demands explainable over \u2018black-box\u2019 machine learning (ML) methods, Symbolic Regression (SR) that derives analytical expressions can help identify key functional dependencies in complex systems. However, traditional SR methods often suffer from (a) inefficient exploration due to their inability to compress the search space of equivalent expressions, and (b) non-physical solutions that violate fundamental physics constraints. We here introduce a symmetric invariant representation of candidate analytical expressions using a Symbolic Graph(SG), on which the Symbolic Graph Neural Network (SGNN) encodes operators, symmetries, and $a\\ priori$ known physics constraints. We further develop reinforcement learning (RL) algorithms with Monte-Carlo Tree Search (MCTS) on our SGNN for SR. Such a physics-constrained graph symbolic regression (PCGSR) method effectively compresses the search space for efficient SR. Experiments on synthetic and real-world scientific datasets demonstrate the efficiency and accuracy of our PCGSR in discovering underlying expressions that adhere to physical laws, yielding physically meaningful solutions."
    },
    {
        "title": "Streaming Algorithms For\u2113pFlows and\u2113pRegression",
        "link_suffix": "/forum?id=Kpjvm2mB0K",
        "link": "https://openreview.net/forum?id=Kpjvm2mB0K",
        "pdf_link": "https://openreview.net/pdf?id=Kpjvm2mB0K",
        "keywords": "Regression, Streaming, Online algorithms, Flows",
        "abstract": "We initiate the study of one-pass streaming algorithms for underdetermined $\\ell_p$ linear regression problems of the form\n  $$\n      \\min_{\\mathbf A\\mathbf x = \\mathbf b} \\lVert\\mathbf x\\rVert_p ,, \\qquad \n      \\text{where } \\mathbf A \\in \\mathbb R^{n \\times d} \\text{ with } n \\ll d ,,\n  $$\n  which generalizes basis pursuit ($p = 1$) and least squares solutions to\n  underdetermined linear systems ($p = 2$). We study the column-arrival\n  streaming model, in which the columns of $\\mathbf A$ are presented one by one in a\n  stream. When $\\mathbf A$ is the incidence matrix of a graph, this corresponds to an\n  edge insertion graph stream, and the regression problem captures $\\ell_p$\n  flows which includes transshipment ($p = 1$), electrical flows ($p = 2$), and\n  max flow ($p = \\infty$) on undirected graphs as special cases. Our goal is to\n  design algorithms which use space much less than the entire stream, which has\n  a length of $d$.For the task of estimating the cost of the $\\ell_p$ regression problem for\n  $p\\in[2,\\infty]$, we show a streaming algorithm which constructs a sparse\n  instance supported on $\\tilde O(\\varepsilon^{-2}n)$ columns of $\\mathbf A$\n  which approximates the cost up to a $(1\\pm\\varepsilon)$ factor, which\n  corresponds to $\\tilde O(\\varepsilon^{-2}n^2)$ bits of space in general and\n  an $\\tilde O(\\varepsilon^{-2}n)$ space semi-streaming algorithm for\n  constructing $\\ell_p$ flow sparsifiers on graphs. This extends to $p\\in(1,\n  2)$ with $\\tilde O(\\varepsilon^{2}n^{q/2})$ columns, where $q$ is the H\"older\n  conjugate exponent of $p$. For $p = 2$, we show that $\\Omega(n^2)$ bits of\n  space are required in general even for outputting a constant factor\n  solution. For $p = 1$, we show that the cost cannot be estimated even to an\n  $o(\\sqrt n)$ factor in $\\mathrm{poly}(n)$ space.On the other hand, if we are interested in outputting a solution $\\mathbf\n  x$, then we show that $(1+\\varepsilon)$-approximations require $\\Omega(d)$\n  space for $p > 1$, and in general, $\\kappa$-approximations require\n  $\\tilde\\Omega(d/\\kappa^{2q})$ space for $p > 1$. We complement these lower\n  bounds with the first sublinear space upper bounds for this problem, showing\n  that we can output a $\\kappa$-approximation using space only\n  $\\mathrm{poly}(n) \\cdot \\tilde O(d/\\kappa^q)$ for $p > 1$, as well as a\n  $\\sqrt n$-approximation using $\\mathrm{poly}(n, \\log d)$ space for $p = 1$."
    },
    {
        "title": "Continual Learning After Model Deployment",
        "link_suffix": "/forum?id=BrqFB8Nl7e",
        "link": "https://openreview.net/forum?id=BrqFB8Nl7e",
        "pdf_link": "https://openreview.net/pdf?id=BrqFB8Nl7e",
        "keywords": "Open-World, Continual Learning",
        "abstract": "This paper studies continual learning after model deployment. A real-world application environment is often an open world filled with novel or out-of-distribution (OOD) objects that have not been seen before. We can call continual learning in such an environmentopen-world continual learning(OWCL). OWCL incrementally performs two main tasks: (1) detecting OOD objects, and (2) continually learning the OOD or new objects on the fly. Although OOD detection and continual learning have been extensively studied separately, their combination for OWCL has barely been attempted. This is perhaps because in addition to the existing challenges of OOD detection and continual learning such ascatastrophic forgetting(CF), OWCL also faces the challenge of data scarcity. As novel objects appear sporadically, when an object from a new/novel class is detected, it is difficult to learn it from one or a few samples to give good accuracy. This paper proposes a novel method called OpenLD to deal with these problems based onlinear discriminant analysis(LDA) and a pre-trained model. This method enables OOD detection and incremental learning of the detected samples on the fly with no CF. Experimental evaluation demonstrates the effectiveness of OpenLD."
    },
    {
        "title": "Is Memorization Actually Necessary for Generalization?",
        "link_suffix": "/forum?id=GbEmJmnQCz",
        "link": "https://openreview.net/forum?id=GbEmJmnQCz",
        "pdf_link": "https://openreview.net/pdf?id=GbEmJmnQCz",
        "keywords": "Memorization",
        "abstract": "Memorization is the ability of deep models to associate training data with seemingly random labels. Even though memorization may not align with a model's ability to generalize, recent work by~\\citet{feldman2020longtail} has demonstrated that memorization is in fact \\textit{necessary} for generalization. However, upon closer inspection, we find that their  methodology has three limitations. First, the definition of memorization is imprecise, leading to contradictory results. Second, their proposed algorithm used for \\textit{approximating} the leave-one-out test (the gold standard for calculating memorization scores) suffers from a high approximation error. Three, the authors induce a distribution shift when calculating marginal utility, leading to flawed results. Having accounted for these errors, we re-evaluate the role of memorization on generalization. We show that most memorization thresholds (the value that dictates whether a point is memorized or not) do not have a statistically significant impact on model accuracy, contrary to what was previously reported. In light of these findings, future researchers are encouraged to design techniques that can accurately approximate memorization scores."
    }
]