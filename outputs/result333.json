[{"title": "eRAM-V: From Interaction to Integration in Efficient Multimodal Large Language Models", "link_suffix": "/forum?id=GtlV6o1yUy", "link": "https://openreview.net/forum?id=GtlV6o1yUy", "pdf_link": "https://openreview.net/pdf?id=GtlV6o1yUy", "keywords": "Multimodal Large Language Model; Interpretability of MLLM", "abstract": "Multimodal large language models (MLLMs) have made significant progress in recent years, yet the interaction between vision and language representations remains underexplored. Prior work has primarily relied on empirical heuristics to guide architecture design. While effective, this approach can lead to sub-optimal designs and computational redundancy. In this work, we examine the fusion process between visual and textual data. Our findings indicate that in auto-regressive MLLMs, fine-grained interactions between visual and text tokens primarily occur in the middle layers. This leads to redundancy in the shallow and deep layers, where modeling only selected visual representations is sufficient. Based on these insights, we introduce eRAM-V, an MLLM that balances computational efficiency and performance. eRAM-V models selected visual features across all layers and integrates fine-grained visual features at specific layers, as needed. Extensive experiments show that eRAM-V outperforms baseline models with equivalent computational budgets, achieving superior results across various benchmarks.", "title_embedding_index": 16600, "title_abs_embedding_index": 16625}, {"title": "REFINE: Inversion-Free Backdoor Defense via Model Reprogramming", "link_suffix": "/forum?id=4IYdCws9fc", "link": "https://openreview.net/forum?id=4IYdCws9fc", "pdf_link": "https://openreview.net/pdf?id=4IYdCws9fc", "keywords": "Backdoor Defense, Model Reprogramming, Backdoor Attack, AI Security", "abstract": "Backdoor attacks on deep neural networks (DNNs) have emerged as a significant security threat, allowing adversaries to implant hidden malicious behaviors during the model training phase. Pre-processing-based defense, which is one of the most important defense paradigms, typically focuses on input transformations or backdoor trigger inversion (BTI) to deactivate or eliminate embedded backdoor triggers during the inference process. However, these methods suffer from inherent limitations: transformation-based defenses often struggle to balance the intensity of transformations with preserving the model's accuracy, while BTI-based defenses require accurate reconstruction of the trigger patterns, which is rarely achievable without prior knowledge. In this paper, we propose REFINE, an inversion-free backdoor defense method based on model reprogramming. REFINE consists of two key components: (1) an input transformation module that disrupts both benign and backdoor patterns, generating new benign features; and (2) an output remapping module that redefines the model's output domain to guide the input transformations effectively. By further integrating supervised contrastive loss, REFINE enhances the defense capabilities while maintaining model utility. Extensive experiments on various benchmark datasets demonstrate the effectiveness of our REFINE and its resistance to potential adaptive attacks.", "title_embedding_index": 16601, "title_abs_embedding_index": 16626}, {"title": "Uncoupled and Convergent Learning in Monotone Games under Bandit Feedback", "link_suffix": "/forum?id=zDJNUDprhW", "link": "https://openreview.net/forum?id=zDJNUDprhW", "pdf_link": "https://openreview.net/pdf?id=zDJNUDprhW", "keywords": "online learning, game", "abstract": "We study the problem of no-regret learning algorithms for general monotone and smooth games and their last-iterate convergence properties. Specifically, we investigate the problem under bandit feedback and strongly uncoupled dynamics, which allows modular development of the multi-player system that applies to a wide range of real applications. We propose a mirror-descent-based algorithm, which converges in $O(T^{-1/4})$ and is also no-regret. The result is achieved by a dedicated use of two regularizations and the analysis of the fixed point thereof. The convergence rate is further improved to $O(T^{-1/2})$ in the case of strongly monotone games.\nMotivated by practical tasks where the game evolves over time, the algorithm is extended to time-varying monotone games. We provide the first non-asymptotic result in converging monotone games and give improved results for equilibrium tracking games.", "title_embedding_index": 16602, "title_abs_embedding_index": 16627}, {"title": "Generalization by Specialization: Unveiling Specialized Subnetworks in Large Language Models", "link_suffix": "/forum?id=9uZGq8P2QM", "link": "https://openreview.net/forum?id=9uZGq8P2QM", "pdf_link": "https://openreview.net/pdf?id=9uZGq8P2QM", "keywords": "LLM; Subnetworks; Generalization", "abstract": "In recent years, large language models (LLMs) have exhibited remarkable generalization capabilities. Previous studies have largely focused on examining the generalization mechanisms in smaller models to draw inferences about similar mechanisms in larger language models. However, these smaller models typically possess limited generalization capacity. In this study, we explore the generalization mechanisms of billion-parameter language models, with a particular attention on publicly available models such as LLaMA and Gemma. Our findings reveal that weight activations exhibit task-specific behavior, indicating that not all weights are necessary for task performance. Building on this insight, we introduce a parameter probing method to identify subnetworks optimized for specific tasks without extensive fine-tuning. This method involves sorting and grouping weight activations followed by the pruning of less significant groups based on a small validation set.\nFurthermore, our results show that subnetworks specialized for domain-specific tasks achieve improved performance and generalization within their respective domains, but their performance deteriorates across different domains. \nThis study presents a novel perspective on generalization of LLMs where the strength of large language models lies in their multiplicity of domain-specific subnetworks, allowing them to excel in various in-domain tasks.", "title_embedding_index": 16603, "title_abs_embedding_index": 16628}, {"title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?", "link_suffix": "/forum?id=lR7rqLtsXZ", "link": "https://openreview.net/forum?id=lR7rqLtsXZ", "pdf_link": "https://openreview.net/pdf?id=lR7rqLtsXZ", "keywords": "Large Language Model, Memory Efficient Training, Full-rank Training", "abstract": "Low-rank training has emerged as a promising approach for reducing memory usage in training Large Language Models (LLMs). Previous methods either rely on decomposing weight matrices (e.g., LoRA), or seek to decompose gradient matrices (e.g., GaLore) to ensure reduced memory consumption. However, both of them constrain the training in a low-rank subspace, thus inevitably leading to sub-optimal performance. This raises a question: whether it is possible to consistently preserve the low-rank constraint for memory efficiency, while achieving full-rank training (i.e., training with full-rank gradients of full-rank weights) to avoid inferior outcomes?\nIn this paper, we propose a new plug-and-play training framework for LLMs called Fira, as the first attempt to achieve this goal. First, we observe an interesting phenomenon during LLM training: the scaling impact of adaptive optimizers (e.g., Adam) on the gradient norm remains similar from low-rank to full-rank training. Based on this observation, we propose a norm-based scaling method, which utilizes the scaling impact of low-rank optimizers as substitutes for that of original full-rank optimizers to enable full-rank training. In this way, we can preserve the low-rank constraint in the optimizer while achieving full-rank training for better performance. Moreover, we find that there are sudden gradient rises during the optimization process, potentially causing loss spikes. To address this, we further put forward a norm-growth limiter to smooth the gradient via regulating the relative increase of gradient norms.\nExtensive experiments on the pre-training and fine-tuning of LLMs show that Fira outperforms both LoRA and GaLore, achieving performance that is comparable to or even better than full-rank training. For instance, our Fira can reduce the memory usage of optimizer states by 61.1%, while achieving improved performance for pre-training on the LLaMA 1B architecture. Notably, for pre-training on the LLaMA 7B architecture, our method uses an $8\\times$ smaller rank than GaLore, yet outperforms it by a large margin.", "title_embedding_index": 16604, "title_abs_embedding_index": 16629}, {"title": "VARIATIONAL DIFFUSION CHANNEL DECODING: A ULTRA-LOW-COST NEURAL CHANNEL DECODER", "link_suffix": "/forum?id=YHDY5uXOSN", "link": "https://openreview.net/forum?id=YHDY5uXOSN", "pdf_link": "https://openreview.net/pdf?id=YHDY5uXOSN", "keywords": "diffusion; channel coding", "abstract": "Neural channel decoder, as a data-driven channel decoding strategy, has shown very promising improvement on error-correcting capability over the classical methods. However, the success of those deep learning-based decoder comes at the cost of drastically increased model storage and computational complexity, hindering their practical adoptions in real-world time-sensitive resource-sensitive communication and storage systems. To address this challenge, we propose an efficient variational diffusion model-based channel decoder, which effectively integrates the domain-specific belief propagation process to the modern diffusion model. By reaping the low-cost benefits of belief propagation and strong learning capability of diffusion model, our proposed neural decoder simultaneously achieves very low cost and high error-correcting performance. Experimental results show that, compared with the state-of-the-art neural channel decoders, our\nmodel provides a feasible solution for practical deployment via achieving the best decoding performance with order-of-magnitude (1000\u00d7 and up) savings in computational cost and model size.", "title_embedding_index": 16605, "title_abs_embedding_index": 16630}, {"title": "Policy Transfer via Latent Graph Planning", "link_suffix": "/forum?id=l5HEECYJ3i", "link": "https://openreview.net/forum?id=l5HEECYJ3i", "pdf_link": "https://openreview.net/pdf?id=l5HEECYJ3i", "keywords": "Reinforcement Learning, Transfer Learning", "abstract": "We introduce a transfer learning framework for deep reinforcement learning that integrates graph-based planning with self-supervised representation learning to efficiently transfer knowledge across tasks. While standard reinforcement learning aims to learn policies capable of solving long-horizon tasks, the resulting policies often fail to generalize to novel tasks and environments. Our approach addresses this limitation by decomposing long-horizon tasks into sequences of transferable short-horizon tasks modeled by goal-conditioned policies. We utilize a planning graph to generate fine-grained sub-goals that guide these short-horizon policies to solve novel long-horizon tasks. Experimental results show that our method improves sample efficiency and demonstrates an improved ability to solve sparse-reward and long-horizon tasks compared to baseline methods in challenging single-agent and multi-agent scenarios. In particular, compared to the state-of-the-art, our method achieves the same or better expected policy reward while requiring fewer training samples when learning novel tasks.", "title_embedding_index": 16606, "title_abs_embedding_index": 16631}, {"title": "BEATS: Optimizing LLM Mathematical Capabilities with BackVerify and Adaptive Disambiguate based Efficient Tree Search", "link_suffix": "/forum?id=03u7pbpyeN", "link": "https://openreview.net/forum?id=03u7pbpyeN", "pdf_link": "https://openreview.net/pdf?id=03u7pbpyeN", "keywords": "Large Language Models, Tree Search, Back Verification", "abstract": "Large Language Models (LLMs) have exhibited exceptional performance across a broad range of tasks and domains. However, they still encounter difficulties in solving mathematical problems due to the rigorous and logical nature of mathematics. Previous studies have employed techniques such as supervised fine-tuning (SFT), prompt engineering, and search-based methods to improve the mathematical problem-solving abilities of LLMs. Despite these efforts, their performance remains suboptimal and demands substantial computational resources. To address this issue, we propose a novel approach, BEATS, to enhance mathematical problem-solving abilities. Our method leverages newly designed prompts that guide the model to iteratively rewrite, advance by one step, and generate answers based on previous steps. Additionally, we introduce a new back-verification technique that uses LLMs to validate the correctness of the generated answers. Furthermore, we employ a pruning tree search to optimize search time while achieving state-of-the-art (SOTA) performance. Notably, our method improves Qwen2-7b-Instruct's score from 36.94 to 61.52 (outperforming GPT-4\u2019s 42.5) on the MATH benchmark.", "title_embedding_index": 16607, "title_abs_embedding_index": 16632}, {"title": "Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models", "link_suffix": "/forum?id=9WYMDgxDac", "link": "https://openreview.net/forum?id=9WYMDgxDac", "pdf_link": "https://openreview.net/pdf?id=9WYMDgxDac", "keywords": "generative models, calibration/uncertainty, inference methods", "abstract": "Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduceTRON, atwo-step framework forrisk control and assessment, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios.TRONcomprises two main components: (1) a novel conformal score tosampleresponse sets of minimum size, and (2) a nonconformity score toidentifyhigh-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show thatTRONachieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.", "title_embedding_index": 16608, "title_abs_embedding_index": 16633}, {"title": "Mind Control through Causal Inference: Predicting Clean Images from Poisoned Data", "link_suffix": "/forum?id=ho4mNiwr2n", "link": "https://openreview.net/forum?id=ho4mNiwr2n", "pdf_link": "https://openreview.net/pdf?id=ho4mNiwr2n", "keywords": "Causal Inference, Backdoor Attacks", "abstract": "Anti-backdoor learning, aiming to train clean models directly from poisoned datasets, serves as an important defense method for backdoor attack. However, existing methods usually fail to recover backdoored samples to their original, correct labels and suffer from poor generalization to large pre-trained models due to its non end-to end training, making them unsuitable for protecting the increasingly prevalent large pre-trained models. To bridge the gap, we first revisit the anti-backdoor learning problem from a causal perspective. Our theoretical causal analysis reveals that incorporating \\emph{\\textbf{both}} images and the associated attack indicators preserves the model's integrity. Building on the theoretical analysis, we introduce an end-to-end method, Mind Control through Causal Inference (MCCI), to train clean models directly from poisoned datasets. This approach leverages both the image and the attack indicator to train the model. Based on this training paradigm, the model\u2019s perception of whether an input is clean or backdoored can be controlled. Typically, by introducing fake non-attack indicators, the model perceives all inputs as clean and makes correct predictions, even for poisoned samples. Extensive experiments demonstrate that our method achieves state-of-the-art performance, efficiently recovering the original correct predictions for poisoned samples and enhancing accuracy on clean samples.", "title_embedding_index": 16609, "title_abs_embedding_index": 16634}, {"title": "Segment Any 3D Object with Language", "link_suffix": "/forum?id=ENv1CeTwxc", "link": "https://openreview.net/forum?id=ENv1CeTwxc", "pdf_link": "https://openreview.net/pdf?id=ENv1CeTwxc", "keywords": "Open-set, 3D Instance Segmentation, Multimodal", "abstract": "In this paper, we investigate Open-Vocabulary 3D Instance Segmentation (OV-3DIS) with free-form language instructions. Earlier works mainly rely on annotated base categories for training which leads to limited generalization to unseen novel categories. To mitigate the poor generalizability to novel categories, recent works generate class-agnostic masks or projecting generalized masks from 2D to 3D, subsequently classifying them with the assistance of 2D foundation model. However, these works often disregard semantic information in the mask generation, leading to sub-optimal performance. Instead, generating generalizable but semantic-aware masks directly from 3D point clouds would result in superior outcomes. To the end, we introduce Segment any 3D Object with LanguagE ($\\textbf{SOLE}$), which is a semantic and geometric-aware visual-language learning framework with strong generalizability by generating semantic-related masks directly from 3D point clouds. Specifically, we propose a multimodal fusion network to incorporate multimodal semantics in both backbone and decoder. In addition, to align the 3D segmentation model with various language instructions and enhance the mask quality, we introduce three types of multimodal associations as supervision. Our SOLE outperforms previous methods by a large margin on ScanNetv2, ScanNet200, and Replica benchmarks, and the results are even closed to the fully-supervised counterpart despite the absence of class annotations in the training. Furthermore, extensive qualitative results demonstrate the versatility of our SOLE to language instructions. The code will be made publicly available.", "title_embedding_index": 16610, "title_abs_embedding_index": 16635}, {"title": "Let the Rule Speak: Enhancing In-context Learning Debiasing with Interpretability", "link_suffix": "/forum?id=YXewbZ8FgU", "link": "https://openreview.net/forum?id=YXewbZ8FgU", "pdf_link": "https://openreview.net/pdf?id=YXewbZ8FgU", "keywords": "In-context learning, debiasing, fuzzy rule, interpretability, multi-objective optimization", "abstract": "In-context learning, which allows large language models to perform diverse tasks with a few demonstrations, is found to have imbalanced per-class prediction accuracy on multi-class text classification. Although notable output correction methods have been developed to tackle the issue and simultaneously improve downstream prediction accuracy, they may fail to answer the core interpretability challenges: why and which certain classes need corrections, and more importantly, to have an easy-to-understand transformation for correcting each class. To address such interpretability gaps, we first find that the imbalance arises from certain classes consistently receiving high ICL output probabilities, whereas others receiving lower or mixed ranges, so the former is more frequently chosen, resulting in higher accuracy; more crucially, we find that these ranges have significantly varying degrees of influence on the accuracy bias, highlighting the need for precise, interpretable probability corrections by range. Motivated by this, we propose FuRud, a fuzzy rule optimization based debiasing method, that (1) detects which classes need corrections, and (2) for each correction-needed class, detects its probability ranges and applies asymmetric amplifications or reductions to correct them interpretably. Notably, across seven benchmark datasets, FuRud reduces the pairwise class accuracy bias (COBias) by more than half (56%), while achieving a relative increase of 21% in accuracy, outperforming state-of-the-art debiasing methods. Moreover, FuRud can optimize a downstream task in a few-shot manner, with as few as 10 optimization examples. Furthermore, FuRud can work for prompt formats that lead to highly skewed predictions. For example, FuRud greatly improves ICL outputs which use letter options, with 44% relative accuracy increase and 54% relative COBias reduction.", "title_embedding_index": 16611, "title_abs_embedding_index": 16636}, {"title": "OpenMU: Your Swiss Army Knife for Music Understanding", "link_suffix": "/forum?id=burz7mU0YD", "link": "https://openreview.net/forum?id=burz7mU0YD", "pdf_link": "https://openreview.net/pdf?id=burz7mU0YD", "keywords": "Music understnading, Multimodal Large Language Model", "abstract": "We present OpenMU-Bench, a large-scale benchmark suite for addressing the data scarcity issue in training multimodal language models to understand music. To construct  OpenMU-Bench, we leveraged existing datasets and bootstrapped new annotations.\nOpenMU-Bench also broadens the scope of music understanding by including lyrics understanding and music tool usage. Using  OpenMU-Bench, we trained our music understanding model, OpenMU, with extensive ablations, demonstrating that OpenMU outperforms baseline models such as MU-Llama. Both OpenMU and OpenMU-Bench are open-sourced to facilitate future research in music understanding and to enhance creative music production efficiency\\footnote{We will release the code, datasets, and model checkpoints upon acceptance.}.", "title_embedding_index": 16612, "title_abs_embedding_index": 16637}, {"title": "UrbanMLLM: Joint Learning of Cross-view Imagery for Urban Understanding", "link_suffix": "/forum?id=YBht9Vp5vC", "link": "https://openreview.net/forum?id=YBht9Vp5vC", "pdf_link": "https://openreview.net/pdf?id=YBht9Vp5vC", "keywords": "Multi-modal large language model, Cross-view learning, Urban understanding", "abstract": "Multimodal large language models (MLLMs) have exhibited remarkable capabilities for performing complex vision-language tasks in various domains. Currently, MLLMs in urban studies are only developed focusing on remote sensing imagery. However, except for the macroscopic information from remote sensing imagery, effective urban understanding also requires detailed appearance information of urban zones from street-view imagery,  which is largely overlooked by existing MLLMs.  The primary challenges of developing such a versatile urban MLLM are twofold. Firstly, it needs a large-scale corpus with well-organized, cross-view urban imagery paired with corresponding text for cross-modal training. Secondly, traditional MLLMs typically learn image-text pairs independently, hard to support joint modeling of cross-view urban imagery.  To address these challenges, in this work, we propose UrbanMLLM, a novel MLLM that jointly learns from remote sensing and street-view imagery to harness their complementary information. We first collect a large-scale dataset containing satellite-view and street-view imagery along with their geotags and annotated texts.   Technically,  we propose a brand MLLM architecture with a cross-view perceiver to explicitly connect visual information of cross-view urban imagery.  We also introduce a novel pre-training paradigm based on structural interleaved urban image-text documents integrating satellite-view, street-view imagery and related textual descriptions. This approach encourages the model to implicitly learn the relationships between different types of urban imagery, enhancing the understanding in each domain.  We evaluate our model on a comprehensive benchmark including 13 types of urban understanding tasks across satellite-view, street-view and cross-view domains. Extensive experiments demonstrate that UrbanMLLM achieves an average of 27.3% and 25.5% performance improvement compared with the best open-sourced and closed-sourcedMLLMs, respectively. Moreover, we thoroughly study the impact of different pre-training data choices and model scales on performance, offering practical insights for effective MLLM design. The proposed UrbanMLLM offers a scalable and versatile solution for understanding urban environments.", "title_embedding_index": 16613, "title_abs_embedding_index": 16638}, {"title": "SPARQ: Outlier-free SpeechLM with Fast Adaptation and Robust Quantization", "link_suffix": "/forum?id=Z2uhdwOrn0", "link": "https://openreview.net/forum?id=Z2uhdwOrn0", "pdf_link": "https://openreview.net/pdf?id=Z2uhdwOrn0", "keywords": "Multi-modal foundation model; Speech Language Model; Low-rank adaptation; Post-training quantization", "abstract": "We propose SpARQ (outlier-free SpeechLM for Fast Adaptation and Robust Quantization) to address the outlier problem in Speech and Language multi-modal Models (SpeechLMs). Our primary observation is that outliers stemming from cross-modal (speech and text) low-rank adaptation and post-training quantization stages affect the performance of the current SpeechLMs. Methodologically, SpARQ leverages a pretrained language model as its foundation, substituting the traditional attention layer with a novel stabilized outlier-free layer. This modification eliminates outliers typically arising during cross-modal low-rank adaptation and post-training quantization. The model is then fine-tuned on multi-modal data using this outlier-free architecture, allowing it to handle textLM, speechLM, ASR, and TTS tasks through a unified interface while maintaining compatibility with parameters adapted from standard pretrained LLMs. Consequently, on the OPT-1.3b model, the proposed framework achieves relative performance improvements: 41% in cross-modal low-rank adaptation and 45% in post-training quantization, along with a 1.33x training speedup. We benchmark it against state-of-the-art low-rank adaptation and post-training quantization methods.", "title_embedding_index": 16614, "title_abs_embedding_index": 16639}, {"title": "Adaptive Camera Sensor for Vision Models", "link_suffix": "/forum?id=He2FGdmsas", "link": "https://openreview.net/forum?id=He2FGdmsas", "pdf_link": "https://openreview.net/pdf?id=He2FGdmsas", "keywords": "domain-adaptation, sensor-control, domain-shift, Out-of-Distribution, data-centric", "abstract": "Domain shift remains a persistent challenge in deep-learning-based computer vision, often requiring extensive model modifications or large labeled datasets to address. Inspired by human visual perception, which adjusts input quality through corrective lenses rather than over-training the brain, we propose Lens, a novel camera sensor control method that enhances model performance by capturing high-quality images from the model's perspective, rather than relying on traditional human-centric sensor control. Lens is lightweight and adapts sensor parameters to specific models and scenes in real-time.  At its core, Lens utilizes VisiT, a training-free, model-specific quality indicator that evaluates individual unlabeled samples at test time using confidence scores, without additional adaptation costs. To validate Lens, we introduce ImageNet-ES Diverse, a new benchmark dataset capturing natural perturbations from varying sensor and lighting conditions. Extensive experiments on both ImageNet-ES and our new ImageNet-ES Diverse show that Lens significantly improves model accuracy across various baseline schemes for sensor control and model modification, while maintaining low latency in image captures. Lens effectively compensates for large model size differences and integrates synergistically with model improvement techniques.", "title_embedding_index": 16615, "title_abs_embedding_index": 16640}, {"title": "MathScape: Evaluating MLLMs in Multi-modal Math Scenarios through a Hierarchical Benchmark", "link_suffix": "/forum?id=3jvgm61l9S", "link": "https://openreview.net/forum?id=3jvgm61l9S", "pdf_link": "https://openreview.net/pdf?id=3jvgm61l9S", "keywords": "Multimodal Large Language Models, Math Ability, Benchmark", "abstract": "With the development of Multimodal Large Language Models (MLLMs), the evaluation of multimodal models in the context of mathematical problems has become a valuable research field. Multimodal visual-textual mathematical reasoning serves as a critical \nindicator for evaluating the comprehension and complex multi-step quantitative reasoning abilities of MLLMs. However, previous multimodal math benchmarks have not sufficiently integrated visual and textual information. To address this gap, we proposed MathScape, a new benchmark that emphasizes the understanding and application of combined visual and textual information. MathScape is designed to evaluate photo-based math problem scenarios, assessing the theoretical understanding and application ability of MLLMs through a categorical hierarchical approach. We conduct a multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark is challenging even for the most sophisticated models. By analyzing the evaluation results, we identify the limitations of MLLMs, offering valuable insights for enhancing model performance.", "title_embedding_index": 16616, "title_abs_embedding_index": 16641}, {"title": "Truthfulness Without Supervision: Model Evaluation Using Peer Prediction", "link_suffix": "/forum?id=EW62GvCzP9", "link": "https://openreview.net/forum?id=EW62GvCzP9", "pdf_link": "https://openreview.net/pdf?id=EW62GvCzP9", "keywords": "Language Model Evaluation, AI Alignment, AI Truthfulness and Deception, Large Language Models", "abstract": "Current evaluation methods for language models rely on supervision, but trusted supervision for difficult tasks is often unavailable, especially for superhuman models. In such cases, evaluation schemes based on imperfect supervision can be exploited, leading to deceptive results. \nHowever, underutilized in the context of model evaluation, a wealth of research from the mechanism design literature focuses on game-theoreticincentive compatibility- eliciting honest and informative answers without trusted supervision. \nDrawing from this literature, we introduce the peer prediction method for model evaluation. It tells apart honest and informative answers from deceptive and uninformative ones, using a metric based on mutual predictability and without requiring ground truth labels. \nWe demonstrate the method's effectiveness and resistance to deception, with both theoretical guarantees and comprehensive empirical validation on up to 405B-parameter models.\nIn contrast to LLM-as-a-Judge methods which require strong and trusted judges, we discover an inverse scaling property in peer prediction, where, surprisingly, resistance to deception isstrengthenedas the capability gap between the jury and participantswidens, enabling reliable evaluation of superhuman models without trusted supervision.\nIn particular, LLM-as-a-Judge evaluations become worse than random guesses when facing deceptive models 5-20$\\times$ its size, while peer prediction thrives when such gaps are large, including in cases with over 100$\\times$ size difference.\nLooking forward, we view this work as a step towards game-theoretic resistance to model deception in alignment and evaluation.", "title_embedding_index": 16617, "title_abs_embedding_index": 16642}, {"title": "Symmetric Kernels with Non-Symmetric Data: A Data-Agnostic Learnability Bound", "link_suffix": "/forum?id=LjQDYcFWmN", "link": "https://openreview.net/forum?id=LjQDYcFWmN", "pdf_link": "https://openreview.net/pdf?id=LjQDYcFWmN", "keywords": "Kernel Ridge Regression, Gaussian Process, NTK, NNGP, Deep Learning Theory, Symmetry, Sample Complexity, Learnability, Bayesian Inference", "abstract": "Kernel ridge regression (KRR) and Gaussian processes (GPs) are fundamental tools in statistics and machine learning, with recent applications to highly over-parameterized deep neural networks. The ability of these tools to learn a target function is directly related to the eigenvalues of their kernel sampled on the input data. Targets having support on higher eigenvalues are more learnable. While kernels are often highly symmetric objects, the data is often not. \nThus, kernel symmetry seems to have little to no bearing on the above eigenvalues or learnability, making spectral analysis on real-world data challenging.\nHere, we show that contrary to this common lure, one may use eigenvalues and eigenfunctions associated with highly idealized data measures to bound learnability on realistic data. As a demonstration, we give a theoretical lower bound on the sample complexity of copying heads for kernels associated with generic transformers acting on natural language.", "title_embedding_index": 16618, "title_abs_embedding_index": 16643}, {"title": "Group rank for encrypted data", "link_suffix": "/forum?id=k7xH8V3prW", "link": "https://openreview.net/forum?id=k7xH8V3prW", "pdf_link": "https://openreview.net/pdf?id=k7xH8V3prW", "keywords": "privacy preserving; encrypted data; rank correlation", "abstract": "Recently, there has been an increasing demand for privacy-preserving techniques in numerous machine learning algorithms, elevating it to a critical concern. One promising solution involves the application of homomorphic encryption (HE). This study focuses on obtaining statistics based on the ranks of HE-encrypted data as a vital tool for robust data analysis. However, computing ranks in HE comes with significant computational costs due to the necessity of comparison operations, and there is currently no efficient method available.\nTo address this gap, we propose an approximate rank method that exploits pairwise comparisons of data to derive ranks for encrypted information. This method effectively measures the association between two-dimensional ranks. Specifically, by utilizing approximate ranks of two variables, we estimate Spearman rank correlation without relying on perfect sorting and introduce a technique to reduce the number of required comparisons.\nNumerical experiments have been conducted to validate our approach, demonstrating that the disparity in values between rank correlation and approximate rank correlation is not substantial. Notably, the processing of one block comprising 32,768 ciphertexts took approximately one minute, exhibiting observed linear complexity dependent on the number of blocks.", "title_embedding_index": 16619, "title_abs_embedding_index": 16644}, {"title": "Retrieval-guided Cross-view Image Synthesis", "link_suffix": "/forum?id=axyvTIt4bU", "link": "https://openreview.net/forum?id=axyvTIt4bU", "pdf_link": "https://openreview.net/pdf?id=axyvTIt4bU", "keywords": "Cross-view Image Synthesis; Domain Gap; Semantic Segmentation Maps; Retrieval", "abstract": "Cross-view image synthesis task aims to synthesize a photo-realistic ground-view image in correspondence with the aerial image in another view or vice versa.  However, the following limitations exist:  1) existing works require extra semantic segmentation maps or preprocessing modules to bridge the domain gap. 2) the current models focus only on shared semantics in the view transformation and ignore exclusive semantics, thus performing poorly in terms of image quality and realism. 3) cross-view image synthesis for urban areas is more difficult and challenging than that of existing datasets due to the complex surroundings and building textures,the two existing datasets,however, are primarily rural and suburban scenarios. With these challenges in mind, the findings of this study can be summarized as follows: 1) a novel retrieval-guided framework, which adopts a retrieval network as the embedder to reduce the domain gap. 2) a new generator, which enhances the semantic consistency and the diversity of exclusive semantics in the target view. 3) a new dataset (named VIGOR-GEN), which offers more practical cross-view image pairs in urban areas and enriches the cross-view datasets.  Extensive experiments on CVUSA, CVACT and VIGOR-GEN benchmarks verify the effectiveness of our proposed method to synthesize the photo-realistic images from the given single image in another view, outperforming the existing state-of-the-art methods.", "title_embedding_index": 16620, "title_abs_embedding_index": 16645}, {"title": "Vision Language Model Based Caption Evaluation Method Leveraging Visual Context Extraction", "link_suffix": "/forum?id=2iPvFbjVc3", "link": "https://openreview.net/forum?id=2iPvFbjVc3", "pdf_link": "https://openreview.net/pdf?id=2iPvFbjVc3", "keywords": "Image Captioning, Evaluation, Vision and Language, LLM as a judge", "abstract": "Given the accelerating progress of vision and language modeling, accurate evaluation of machine-generated image captions remains critical. In order to evaluate captions more closely to human preferences, metrics need to discriminate between captions of varying quality and content. However, conventional metrics fall short of comparing beyond superficial matches of words or embedding similarities; thus, they still need improvement. This paper presents VisCE2, a vision language model-based caption evaluation method. Our method focuses on visual context, which refers to the detailed content of images, including objects, attributes, and relationships. By extracting and organizing them into a structured format, we replace the human-written references with visual contexts and help VLMs better understand the image, enhancing evaluation performance. Through meta-evaluation on multiple datasets, we validated that VisCE2 outperforms the conventional pre-trained metrics in capturing caption quality and demonstrates superior consistency with human judgment.", "title_embedding_index": 16621, "title_abs_embedding_index": 16646}, {"title": "UrbanWorld: An Urban World Model for 3D City Generation", "link_suffix": "/forum?id=4W1wTg7q9o", "link": "https://openreview.net/forum?id=4W1wTg7q9o", "pdf_link": "https://openreview.net/pdf?id=4W1wTg7q9o", "keywords": "Urban world model, 3D city generation", "abstract": "Cities, as the essential environment of human life, encompass diverse physical elements such as buildings, roads and vegetation, which continuously interact with dynamic entities like people and vehicles. Crafting realistic, interactive 3D urban environments is essential for nurturing AGI systems and constructing AI agents capable of perceiving, decision-making, and acting like humans in real-world environments. However, creating high-fidelity 3D urban environments usually entails extensive manual labor from designers, involving intricate detailing and representation of complex urban elements. Therefore, accomplishing this automatically remains a longstanding challenge. Toward this problem, we propose UrbanWorld, the first generative urban world model that can automatically create a customized, realistic and interactive 3D urban world with flexible control conditions. Specifically, we design a progressive diffusion-based rendering method to produce 3D urban assets with high-quality textures. Moreover, we propose a specialized urban multimodal large language model (Urban MLLM) trained on realistic street-view image-text corpus to supervise and guide the generation process. UrbanWorld incorporates four key stages in the generation pipeline: flexible 3D layout generation from OSM data or urban layout with semantic and height maps, urban scene design with Urban MLLM, controllable urban asset rendering via progressive 3D diffusion, and MLLM-assisted scene refinement. We conduct extensive quantitative analysis on five visual metrics, demonstrating that UrbanWorld achieves state-of-the-art generation realism. Next, we provide qualitative results about the controllable generation capabilities of UrbanWorld using both textual and image-based prompts. Lastly, we verify the interactive nature of these environments by showcasing the agent perception and navigation within the created environments. We contribute UrbanWorld as an open-source tool available athttps://github.com/Urban-World/UrbanWorld.", "title_embedding_index": 16622, "title_abs_embedding_index": 16647}, {"title": "Diffusion Trajectory-guided Policy: A Novel Framework for Long-Horizon Robot Manipulation", "link_suffix": "/forum?id=VaoeAi5CW8", "link": "https://openreview.net/forum?id=VaoeAi5CW8", "pdf_link": "https://openreview.net/pdf?id=VaoeAi5CW8", "keywords": "Robotics, Imitation Learning, Generative Model, Vision-Language Action", "abstract": "Recently, Vision-Language Models (VLMs) have made substantial progress in robot imitation learning, benefiting from increased amounts of demonstration data. However, the high cost of data collection remains a significant bottleneck, and the scarcity of demonstrations often result in poor generalization of the imitation policy, especially in long-horizon robotic manipulation tasks. To address these challenges, we propose the Diffusion Trajectory-guided Policy (DTP) framework, which generates task-relevant trajectories through a diffusion model to guide policy learning for long-horizon tasks. Furthermore, we demonstrate that our DTP method offers a useful interface for prompt engineering, providing a novel way to connect robot manipulation skills with interactions involving LLMs or humans. Our approach employs a two-stage training process: initially, we train a generative vision-language model to create diffusion task-relevant trajectories, then refine the imitation policy using these trajectories. We validate that the DTP method achieves substantial performance improvements in extensive experiments on the CALVIN simulation benchmark, starting from scratch without any external pretraining. Our approach outperforms state-of-the-art baselines by an average of 25% in success rate across various settings.", "title_embedding_index": 16623, "title_abs_embedding_index": 16648}, {"title": "RESOLVE: Relational Reasoning with Symbolic and Object-Level Features Using Vector Symbolic Processing", "link_suffix": "/forum?id=92AFW5nq8M", "link": "https://openreview.net/forum?id=92AFW5nq8M", "pdf_link": "https://openreview.net/pdf?id=92AFW5nq8M", "keywords": "Abstract Reasoning, Neuro Vector Symbolic Architectures, Self-Attention", "abstract": "Modern transformer-based encoder-decoder architectures struggle with reasoning tasks due to their inability to effectively extract relational information between input objects (data/tokens). Recent work introduced the $\\textit{Abstractor}$ module, embedded between transformer layers, to address this gap. However, the Abstractor layer while excelling at capturing relational information (pure relational reasoning), faces challenges in tasks that require both object and relational-level reasoning (partial relational reasoning). To address this, we propose $\\texttt{RESOLVE}$, a neuro-vector symbolic architecture that combines object-level features with relational representations in high-dimensional spaces, using fast and efficient operations such as bundling (summation) and binding (Hadamard product) allowing both object-level features and relational representations to coexist within the same structure without interfering with one another. $\\texttt{RESOLVE}$ is driven by a novel attention mechanism that operates in a bipolar high dimensional space, allowing fast attention score computation compared to the state-of-the-art. By leveraging this design, the model achieves both low compute latency and memory efficiency. $\\texttt{RESOLVE}$ also  offers better generalizability while achieving higher accuracy in purely relational reasoning tasks such as sorting as well as partial relational reasoning tasks such as math problem-solving compared to state-of-the-art methods.", "title_embedding_index": 16624, "title_abs_embedding_index": 16649}]