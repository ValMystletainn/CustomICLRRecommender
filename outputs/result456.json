[
    {
        "title": "Polybasic Speculative Decoding Under a Theoretical Perspective",
        "link_suffix": "/forum?id=n7iwmPacDt",
        "link": "https://openreview.net/forum?id=n7iwmPacDt",
        "pdf_link": "https://openreview.net/pdf?id=n7iwmPacDt",
        "keywords": "Speculative Decoding; LLM Inference",
        "abstract": "Speculative decoding has emerged as a critical technique for accelerating inference in large language models, achieving significant speedups while ensuring consistency with the outputs of the original models.\nHowever, there is currently a lack of theoretical guidance in speculative decoding. \nAs a result, most existing works are dualistic target-draft model paradigm, which significantly restricts the hinders potential application scenarios.\nIn this paper, we propose a polybasic speculative decoding framework supported by a solid theoretical foundation.\nWe first deduce a theorem to control the ideal inference time of speculative decoding systems which is then serve as a design criterion that effectively expands the original dualistic speculative decoding into a more efficient polybasic speculative decoding. \nWe further theoretically analyze the sampling process, identifying variables that can be optimized to enhance inference efficiency in multi-model systems.\nWe demonstrate, both theoretically and empirically, that this system accelerates inference for the target model, and that our approach is orthogonal to the majority of existing speculative methods, allowing for independent application or combination with other techniques. \nExperimentally, we conducted comprehensive evaluations across a wide range of models, including those from the Vicuna, LLaMA2-Chat, and LLaMA3 families. \nOur method achieved remarkable latency speedup ratios of $\\textbf{3.31$\\times$-4.01$\\times$}$ for LLaMA2-Chat 7B, up to $\\textbf{3.87$\\times$}$ for LLaMA3-8B, and up to $\\textbf{4.43$\\times$}$ for Vicuna-7B, while maintaining the distribution of the generated text. Code is available in supplementary materials."
    },
    {
        "title": "DuaRot: Dual Rotation for Advanced Outlier Mitigation in Rotated LLMs",
        "link_suffix": "/forum?id=oHBS7R6JcP",
        "link": "https://openreview.net/forum?id=oHBS7R6JcP",
        "pdf_link": "https://openreview.net/pdf?id=oHBS7R6JcP",
        "keywords": "large language model, Walsh\u2013Hadamard transform, reparameterization, hardware-aware, rotational invariance",
        "abstract": "By employing rotation, outliers in activations can be effectively mitigated without altering the output, thereby facilitating the quantization of large language models (LLMs). However, existing rotation-based methods only consider global activation distributions, leaving the finer-grained distributions underexplored. Additionally, these methods predominantly rely on the Walsh\u2013Hadamard transform (WHT) to accelerate online rotation operations, while not fully considering performance between matrix multiplication~(Matmul) and WHT in actual runtime. These limitations hinder the rotation's ability to effectively reduce quantization errors and decrease inference speed. Therefore, improvements are needed in their performance regarding both accuracy and speed. In this paper, we propose a dual rotation method for rotation matrices, dubbed DuaRot, based on reparameterization. During training, DuaRot sequentially refines global and local features to achieve effective outlier mitigation. During inference, global and local rotations can be merged, which maintains rotational invariance without introducing additional computational overhead. Meanwhile, we propose a hardware-aware matrix configuration strategy, which determines whether the online Hadamard matrix should be expanded into a trainable parameter space by taking the runtime of the WHT and Matmul into account. This approach further enhances the reduction of quantization errors in online rotation operations without compromising inference speed. Extensive experiments demonstrate that DuaRot outperforms existing methods across various models and quantization configurations. For instance, when applied to LLaMA3-8B, DuaRot achieves WikiText-2 perplexities of 7.49 and 7.41 under W4A4KV4 and W4A4KV16 configurations with Round-to-Nearest (RTN), improving by 0.51 and 0.41 over the state-of-the-art, respectively. The code will be publicly available soon."
    },
    {
        "title": "Efficient Learning in Neural Networks without Gradient Backpropagation",
        "link_suffix": "/forum?id=R0YGjmqiwB",
        "link": "https://openreview.net/forum?id=R0YGjmqiwB",
        "pdf_link": "https://openreview.net/pdf?id=R0YGjmqiwB",
        "keywords": "Brain-inspired, Learning algorithm, Learning efficiency",
        "abstract": "The brain possesses highly efficient learning algorithms that have not been fully understood. The gradient backpropagation (BP) serves as a powerful tool for training artificial neural networks, but it diverges from the known anatomical and physiological constraints of the brain. Conversely, biologically plausible learning algorithms have efficiency limitations in training deep neural networks. To bridge this gap, we introduce a perturbation-based approach called low-rank cluster orthogonal (LOCO) weight modification. Unlike traditional brain-inspired algorithms, LOCO offers mathematical convergence guarantees. Theoretical analysis shows that LOCO provides an unbiased estimate of the BP gradient, with low variance. As a result, LOCO is highly efficient. It can train the deepest spiking neural networks to date without gradient backpropagation, achieving state-of-the-art performance on several benchmark datasets and exhibiting the ability to overcome catastrophic forgetting. These findings suggest that biologically feasible learning methods can be substantially more efficient than previously believed. Furthermore, avoiding gradient backpropagation allows LOCO to achieve O(1) time complexity for weight updates. This opens a promising avenue for developing distributed computing systems that are more efficient than BP-based counterparts."
    },
    {
        "title": "BiDRN: Binarized 3D Whole-body Human Mesh Recovery",
        "link_suffix": "/forum?id=bxqfyKl4lQ",
        "link": "https://openreview.net/forum?id=bxqfyKl4lQ",
        "pdf_link": "https://openreview.net/pdf?id=bxqfyKl4lQ",
        "keywords": "3D whole-body human mesh recovery, Binarization",
        "abstract": "3D whole-body human mesh recovery aims to reconstruct the 3D human body, face, and hands from a single image. Although powerful deep learning models have achieved accurate estimation in this task, they require enormous memory and computational resources. Consequently, these methods can hardly be deployed on resource-limited edge devices. In this work, we propose a Binarized Dual Residual Network (BiDRN), a novel quantization method designed to estimate the 3D human body, face, and hands parameters efficiently. Specifically, we design a basic unit Binarized Dual Residual Block (BiDRB) composed of Local Convolution Residual (LCR) and Block Residual (BR), which can preserve as much full-precision information as possible. For LCR, we further generalize it to four kinds of convolutional modules so that full-precision information can be propagated even across mismatched dimensions when reshaping features. Additionally, we also binarize the face and hands box-prediction network as Binarized BoxNet, which further reduces the model redundancy. Comprehensive quantitative and qualitative experiments demonstrate the effectiveness of BiDRN, which has a significant improvement over state-of-the-art binarization algorithms. Moreover, our BiDRN achieves comparable performance with the full-precision method Hand4Whole while using only22.1%parameters and14.8%operations. We will release all the code and pretrained models."
    },
    {
        "title": "Automated Fine-Grained Mixture-of-Experts Quantization",
        "link_suffix": "/forum?id=etxbRucurT",
        "link": "https://openreview.net/forum?id=etxbRucurT",
        "pdf_link": "https://openreview.net/pdf?id=etxbRucurT",
        "keywords": "quantization",
        "abstract": "Mixture of Experts (MoE) enables efficient parameter scaling in large language models by dynamically activating relevant parameter subsets per input token. \nCompressing MoE models presents unique challenges due to their inherent sparsity. Traditional quantization techniques, which are typically effective for dense models, prove inadequate when applied to MoE architectures.\nThis paper proposes an efficient MoE quantization algorithm.\nWe propose a fine-grained, adaptive quantization approach coupled with an efficient method for determining optimal configurations.\nSpecifically, we construct a mixed-precision quantization search space encompassing different granularities from expert-level to channel-level. \nThis approach facilitates precise bit-width resource allocation across model components based on their significance and activation frequency. \nAnd then, we leverage evolutionary algorithms to efficiently navigate this search space, autonomously identifying optimal quantization configurations. \nThe synergy between adaptive granularity and automated search effectively mitigates the distinctive quantization challenges inherent to MoE models, culminating in a fully automated framework for efficient MoE quantization.\nExperimental results indicate that our method achieves significant performance improvements across multiple evaluation tasks, with particularly notable results in low-bit quantization scenarios.\nWhen applied to the Mixtral-8x7b-v0.1 model, our approach outperforms the current state-of-the-art by $9.24$% , setting a new benchmark in MoE quantization. Code is available in supplementary materials."
    },
    {
        "title": "Assessing Open-world Forgetting in Generative Image Model Customization",
        "link_suffix": "/forum?id=Sw7c4fwpSC",
        "link": "https://openreview.net/forum?id=Sw7c4fwpSC",
        "pdf_link": "https://openreview.net/pdf?id=Sw7c4fwpSC",
        "keywords": "generative models, diffusion, open-world forgetting, semantic drift, appearance drift",
        "abstract": "Recent advances in diffusion models have significantly enhanced image generation capabilities. However, customizing these models with new classes often leads to unintended consequences that compromise their reliability. We introduce the concept ofopen-world forgettingto emphasize the vast scope of these unintended alterations, contrasting it with the well-studiedclosed-world forgetting, which is measurable by evaluating performance on a limited set of classes or skills.\nOur research presents the first comprehensive investigation into open-world forgetting in diffusion models, focusing on semantic and appearance drift of representations. We utilize zero-shot classification to analyze semantic drift, revealing that even minor model adaptations lead to unpredictable shifts affecting areas far beyond newly introduced concepts, with dramatic drops in zero-shot classification of up to 60%. Additionally, we observe significant changes in texture and color of generated content when analyzing appearance drift.\nTo address these issues, we propose a mitigation strategy based on functional regularization, designed to preserve original capabilities while accommodating new concepts. Our study aims to raise awareness of unintended changes due to model customization and advocates for the analysis of open-world forgetting in future research on model customization and finetuning methods. Furthermore, we provide insights for developing more robust adaptation methodologies."
    },
    {
        "title": "GenDataAgent: On-the-fly Dataset Augmentation with Synthetic Data",
        "link_suffix": "/forum?id=WoGnnggVCZ",
        "link": "https://openreview.net/forum?id=WoGnnggVCZ",
        "pdf_link": "https://openreview.net/pdf?id=WoGnnggVCZ",
        "keywords": "supervised learning, classification, computer vision, synthetic data, generative AI, responsible AI, fairness",
        "abstract": "Synthetic data is increasingly employed for training dataset augmentation in computer vision. However, prior works typically perform a uniform search across the entire category space, overlooking the interaction between synthetic data generation and downstream task training. Furthermore, balancing the diversity of synthetic data while ensuring it remains within the same distribution as real data (i.e., avoiding outliers) remains a significant challenge.\nIn this work, we propose a generative agent to augment target training datasets with synthetic data for model fine-tuning. Our agent iteratively generates relevant data on-the-fly, aligning with the target training dataset distribution. It prioritizes sampling diverse synthetic data that complements marginal training samples, with a focus on synthetic data that exhibit higher variance in gradient updates. Evaluations across diverse supervised image classification tasks demonstrate the effectiveness of our approach."
    },
    {
        "title": "The Super Weight in Large Language Models",
        "link_suffix": "/forum?id=0Ag8FQ5Rr3",
        "link": "https://openreview.net/forum?id=0Ag8FQ5Rr3",
        "pdf_link": "https://openreview.net/pdf?id=0Ag8FQ5Rr3",
        "keywords": "natural language processing",
        "abstract": "Recent works have shown a surprising result: a small fraction of Large Language Model (LLM) parameter outliers are disproportionately important to the quality of the model. LLMs contain billions of parameters, so these small fractions, such as 0.01%, translate to hundreds of thousands of parameters. In this work, we present an even more surprising finding: pruning as few as a single parameter can destroy an LLM\u2019s ability to generate text\u2014resulting in an increase in perplexity by three orders of magnitude and reducing zero-shot accuracy to guessing. We propose a data-free method for identifying such parameters, termed super weights, using a single forward pass through the model. Additionally, we find that these super weights induce correspondingly rare and large activation outliers, termed super activations. When preserved with high precision, super activations can enhance simple round-to-nearest quantization, making it competitive with state-of-the-art methods. For weight quantization, we similarly find that by preserving the super weight and clipping other weight outliers, round-to-nearest quantization can scale to much larger block sizes than previously considered. To facilitate further research into super weights, we provide an index of super weight coordinates for common, openly available LLMs."
    },
    {
        "title": "ST-Modulator: Modulating Space-Time Attention for Multi-Grained Video Editing",
        "link_suffix": "/forum?id=SSslAtcPB6",
        "link": "https://openreview.net/forum?id=SSslAtcPB6",
        "pdf_link": "https://openreview.net/pdf?id=SSslAtcPB6",
        "keywords": "diffusion model, video editing",
        "abstract": "Recent advancements in diffusion models have significantly improved video generation and editing capabilities. However, multi-grained video editing, which encompasses class-level, instance-level, and part-level modifications, remains a formidable challenge. The major difficulties in multi-grained editing include semantic misalignment of text-to-region control and feature coupling within the diffusion model. To address these difficulties, we present ST-Modulator, a zero-shot approach that modulates space-time (cross- and self-) attention mechanisms to achieve fine-grained control over video content. We enhance text-to-region control by amplifying each local prompt's attention to its corresponding spatial-disentangled region while minimizing interactions with irrelevant areas in cross-attention. Additionally, we improve feature separation by increasing intra-region awareness and reducing inter-region interference in self-attention. Extensive experiments demonstrate our method achieves state-of-the-art performance in real-world scenarios. More details are available on the project page."
    },
    {
        "title": "Let the Code LLM Edit Itself When You Edit the Code",
        "link_suffix": "/forum?id=zqzsZ5cXbB",
        "link": "https://openreview.net/forum?id=zqzsZ5cXbB",
        "pdf_link": "https://openreview.net/pdf?id=zqzsZ5cXbB",
        "keywords": "code generation, efficiency, large language model, code assistant",
        "abstract": "In this work, we investigate a typical scenario in code generation where a developer edits existing code in real time and requests a code assistant, e.g., a large language model, to re-predict the next token or next line on the fly. Naively, the LLM needs to re-encode the entire KV cache to provide an accurate prediction. However, this process is computationally expensive, especially when the sequence length is long. Simply encoding the edited subsequence and integrating it to the original KV cache meets the temporal confusion problem, leading to significantly worse performance. We address this efficiency and accuracy trade-off by introducing $\\underline{\\textbf{P}\\text{ositional}\\  \\textbf{I}\\text{ntegrity}\\  \\textbf{E}\\text{ncoding}}$ (PIE). Building upon the rotary positional encoding, PIE first removes the rotary matrices in the Key cache that introduce temporal confusion and then reapplies the correct rotary matrices. This process ensures that positional relationships between tokens are correct and requires only a single round of matrix multiplication. We validate the effectiveness of PIE through extensive experiments on the RepoBench-C-8k dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters. Our evaluation includes three real-world coding tasks: code insertion, code deletion, and multi-place code editing. Results demonstrate that PIE reduces computational overhead by over 85%  compared to the standard full recomputation approach across all model sizes and tasks while well approximating the model performance."
    },
    {
        "title": "CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes",
        "link_suffix": "/forum?id=a3ptUbuzbW",
        "link": "https://openreview.net/forum?id=a3ptUbuzbW",
        "pdf_link": "https://openreview.net/pdf?id=a3ptUbuzbW",
        "keywords": "neural rendering, novel view synthesis, large-scale scene, radiance field, surfel splatting, surfel reconstruction",
        "abstract": "Recently, 3D Gaussian Splatting (3DGS) has revolutionized radiance field reconstruction, manifesting efficient and high-fidelity novel view synthesis. However, accurately representing surfaces, especially in large and complex scenarios, remains a significant challenge due to the unstructured nature of 3DGS. In this paper, we present CityGaussianV2, a novel approach for large-scale scene reconstruction that addresses critical challenges related to geometric accuracy and efficiency. Building on the favorable generalization capabilities of 2D Gaussian Splatting (2DGS), we address its convergence and scalability issues. Specifically, we implement a decomposed-gradient-based densification and depth regression technique to eliminate blurry artifacts and accelerate convergence. To scale up, we introduce an elongation filter that mitigates Gaussian count explosion caused by 2DGS degeneration. Furthermore, we optimize the CityGaussian pipeline for parallel training, achieving up to 10$\\times$ compression, at least 25% savings in training time, and a 50% decrease in memory usage. We also established standard geometry benchmarks under large-scale scenes. Experimental results demonstrate that our method strikes a promising balance between visual quality, geometric accuracy, as well as storage and training costs."
    },
    {
        "title": "Soft-TransFormers for Continual Learning",
        "link_suffix": "/forum?id=o9icFTlKi4",
        "link": "https://openreview.net/forum?id=o9icFTlKi4",
        "pdf_link": "https://openreview.net/pdf?id=o9icFTlKi4",
        "keywords": "Soft-Transformers, Continual Learning, Well-initialized Lottery Ticket Hypothesis (WLTH)",
        "abstract": "Inspired by Well-initialized Lottery Ticket Hypothesis (WLTH), which provides suboptimal fine-tuning solutions, we propose a novel fully fine-tuned continual learning (CL) method referred to as Soft-TransFormers (Soft-TF). Soft-TF sequentially learns and selects an optimal soft-network or subnetwork for each task. During sequential training in CL, Soft-TF jointly optimizes the weights of sparse layers to obtain task-adaptive soft (real-valued) networks or subnetworks (binary masks), while keeping the well-pre-trained layer parameters frozen. In inference, the identified task-adaptive network of Soft-TF masks the parameters of the pre-trained network, mapping to an optimal solution for each task and minimizing Catastrophic Forgetting (CF) - the soft-masking preserves the knowledge of the pre-trained network. Extensive experiments on Vision Transformer (ViT) and CLIP demonstrate the effectiveness of Soft-TF, achieving state-of-the-art performance across various CL scenarios, including Class-Incremental Learning (CIL) and Task-Incremental Learning (TIL), supported by convergence theory."
    },
    {
        "title": "Debiasing Mini-Batch Quadratics for Applications in Deep Learning",
        "link_suffix": "/forum?id=Q0TEVKV2cp",
        "link": "https://openreview.net/forum?id=Q0TEVKV2cp",
        "pdf_link": "https://openreview.net/pdf?id=Q0TEVKV2cp",
        "keywords": "quadratic Taylor approximation, mini-batching, second-order optimizers, conjugate gradients, uncertainty quantification, Laplace approximation, stochastic curvature, GGN, KFAC",
        "abstract": "Quadratic approximations form a fundamental building block of machine learning methods. E.g., second-order optimizers try to find the Newton step into the minimum of a local quadratic proxy to the objective function; and the second-order approximation of a network's loss function can be used to quantify the uncertainty of its outputs via the Laplace approximation. When computations on the entire training set are intractable - typical for deep learning - the relevant quantities are computed on mini-batches. This, however, distorts and biases the shape of the associatedstochasticquadratic approximations in an intricate way with detrimental effects on applications. In this paper, we (i) show that this bias introduces a systematic error, (ii) provide a theoretical explanation for it, (iii) explain its relevance for second-order optimization and uncertainty quantification via the Laplace approximation in deep learning, and (iv) develop and evaluate debiasing strategies."
    },
    {
        "title": "Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration",
        "link_suffix": "/forum?id=jsBhmOCKYs",
        "link": "https://openreview.net/forum?id=jsBhmOCKYs",
        "pdf_link": "https://openreview.net/pdf?id=jsBhmOCKYs",
        "keywords": "Image Restoration, Domain Adaptation, Diffusion Loss",
        "abstract": "Although learning-based image restoration methods have made significant progress, they still struggle with limited generalization to real-world scenarios due to the substantial domain gap caused by training on synthetic data. Existing methods address this issue by improving data synthesis pipelines, estimating degradation kernels, employing deep internal learning, and performing domain adaptation and regularization. Previous domain adaptation methods have sought to bridge the domain gap by learning domain-invariant knowledge in either feature or pixel space. However, these techniques often struggle to extend to low-level vision tasks within a stable and compact framework. In this paper, we show that it is possible to perform domain adaptation via the noise space using diffusion models. In particular, by leveraging the unique property of how auxiliary conditional inputs influence the multi-step denoising process, we derive a meaningful diffusion loss that guides the restoration model in progressively aligning both restored synthetic and real-world outputs with a target clean distribution. We refer to this method as denoising as adaptation.  To prevent shortcuts during joint training, we present crucial strategies such as channel-shuffling layer and residual-swapping contrastive learning in the diffusion model. They implicitly blur the boundaries between conditioned synthetic and real data and prevent the reliance of the model on easily distinguishable features. Experimental results on three classical image restoration tasks, namely denoising, deblurring, and deraining, demonstrate the effectiveness of the proposed method. Code and model will be made publicly available."
    },
    {
        "title": "LLM-Assisted Fast and Customized Model Generation: A Preliminary Exploration",
        "link_suffix": "/forum?id=vYO7owSSHZ",
        "link": "https://openreview.net/forum?id=vYO7owSSHZ",
        "pdf_link": "https://openreview.net/pdf?id=vYO7owSSHZ",
        "keywords": "Customized Model Generation, Hypernetworks, Large Language Models",
        "abstract": "The rapid advancement of AI models has significantly impacted daily life, with Large Language Models (LLMs) playing a pivotal role in automating tasks and providing all-in-one solutions via API services. Meanwhile, there is a growing demand for private, resource-constrained, customizable, and high-performance models tailored to specific user needs. However, many users struggle to deploy these models due to limited resources or technical expertise. In this work, we try to address these challenges by focusing on two primary objectives: (1) to meet the specific needs of a broad range of users, and (2) to lower the barriers to AI model usage (\\textit{e.g.}, resource constraints, technical expertise) for most users. In our preliminary exploration, we introduce FLAME, a framework that determines and generates AI models based on data or task descriptions provided by users. While existing solutions rely on pre-built models or extensive finetuning, FLAME leverages LLMs (\\textit{e.g.}, GPT4-turbo) to capture data patterns and task features from user input, converting them into user requirements and structured metadata (\\textit{e.g.}, task type, model architecture, and classifier dimension). Then, FLAME uses them as guidance to generate customized models by hypernetworks. This approach significantly improves efficiency, achieving up to 270x faster model production compared to finetuning-based paradigms (e.g., all-parameter and LoRA fine-tuning) while maintaining comparable performance across various tasks. We validate the effectiveness of FLAME through comprehensive experiments on Natural Language Processing (NLP), Computer Vision (CV), and tabular datasets, demonstrating its ability to quickly deliver high-quality, customized models."
    },
    {
        "title": "VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors",
        "link_suffix": "/forum?id=IReyEK7Sst",
        "link": "https://openreview.net/forum?id=IReyEK7Sst",
        "pdf_link": "https://openreview.net/pdf?id=IReyEK7Sst",
        "keywords": "Video Editing, Video Diffusion Models, DiT, 3D Transformation",
        "abstract": "Generative methods for image and video editing use generative models as priors to perform edits despite incomplete information, such as changing the composition of 3D objects shown in a single image. Recent methods have shown promising composition editing results in the image setting, but in the video setting, editing methods have focused on editing object appearance, object motion, or camera motion, and as a result, methods to edit object composition in videos are still missing. We propose VideoHandles as a method for editing 3D object compositions in videos of static scenes with camera motion. Our approach allows editing the 3D position of a 3D object across all frames of a video in a temporally consistent manner. This is achieved by lifting intermediate features of a generative model to a 3D reconstruction that is shared between all frames, editing the reconstruction, and projecting the features on the edited reconstruction back to each frame. To the best of our knowledge, this is the first generative approach to edit object compositions in videos. Our approach is simple and training-free, while outperforming state-of-the-art image editing baselines."
    },
    {
        "title": "Generalization  Bounds for  Neural Ordinary Differential Equations and Residual Neural Networks",
        "link_suffix": "/forum?id=B8qoU7kgSF",
        "link": "https://openreview.net/forum?id=B8qoU7kgSF",
        "pdf_link": "https://openreview.net/pdf?id=B8qoU7kgSF",
        "keywords": "NEURAL ORDINARY DIFFERENTIAL EQUATIONS, GENERALIZATION  BOUNDS, BOUNDED VARIATION FUNCTIONS, MACHINE LEARNING",
        "abstract": "Neural ordinary differential equations (neural ODEs) represent a widely-used\nclass of deep learning models characterized by continuous depth. Understand-\ning the generalization error bound is important to evaluate how well a model is\nexpected to perform on new, unseen data. Earlier works in this direction involved\nconsidering the linear case on the dynamics function (a function that models the\nevolution of state variables) of Neural ODE Marion (2024). Other related work\nis on bound for Neural Controlled ODE Bleistein & Guilloux (2023) that de-\npends on the sampling gap. We consider a class of neural ordinary differential\nequations (ODEs) with a general nonlinear function for time-dependent and time-\nindependent cases which is Lipschitz with respect to state variables. We observed\nthat the solution of the neural ODEs would be of bound variations if we assume\nthat the dynamics function of Neural ODEs is Lipschitz continuous with respect\nto the hidden state. We derive a generalization bound for the time-dependent\nand time-independent Neural ODEs.Using the fact that Neural ODEs are limiting\ncases of time-dependent Neural ODEs we obtained a bound for the residual neural\nnetworks. We showed the effect of overparameterization and domain bound in the\ngeneralization error bound. This is the first time, the generalization bound for the\nNeural ODE with a more general non-linear function has been found."
    },
    {
        "title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular Videos",
        "link_suffix": "/forum?id=LuGHbK8qTa",
        "link": "https://openreview.net/forum?id=LuGHbK8qTa",
        "pdf_link": "https://openreview.net/pdf?id=LuGHbK8qTa",
        "keywords": "Dynamic Reconstrction; Video Reconstruction; 3D Gaussian Splatting",
        "abstract": "Modern 3D engines and graphics pipelines require mesh as a memory-efficient representation, which allows efficient rendering, geometry processing, texture editing, and many other downstream operations. However, it is still highly difficult to obtain high-quality mesh in terms of structure and detail from monocular visual observations. The problem becomes even more challenging for dynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh given a single monocular video. Our work leverages the recent advancement in 3D Gaussian Splatting to construct the mesh sequence with temporal consistency from a video. Building on top of this representation, DG-Mesh recovers high-quality meshes from the Gaussian points and can track the mesh vertices over time, which enables applications such as texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly distributed Gaussians, resulting better mesh reconstruction through mesh-guided densification and pruning on the deformed Gaussians. By applying cycle-consistent deformation between the canonical and the deformed space, we can project the anchored Gaussian back to the canonical space and optimize Gaussians across all time frames. During the evaluation on different datasets, DG-Mesh provides significantly better mesh reconstruction and rendering than baselines."
    },
    {
        "title": "DocLayout-YOLO: Enhancing Document Layout Analysis through Diverse Synthetic Data and Global-to-Local Adaptive Perception",
        "link_suffix": "/forum?id=k0X4m9GAQV",
        "link": "https://openreview.net/forum?id=k0X4m9GAQV",
        "pdf_link": "https://openreview.net/pdf?id=k0X4m9GAQV",
        "keywords": "Document Layout Analysis, Document Pretraining, Document Understanding",
        "abstract": "Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models will be made publicly available."
    },
    {
        "title": "NaVILA: Legged Robot Vision-Language-Action Model for Navigation",
        "link_suffix": "/forum?id=gkDRrvqeWF",
        "link": "https://openreview.net/forum?id=gkDRrvqeWF",
        "pdf_link": "https://openreview.net/pdf?id=gkDRrvqeWF",
        "keywords": "Vision Language Action, Legged Robots, Vision Language Navigation",
        "abstract": "This paper proposes to solve the problem of Vision-and-Language Navigation with legged robots, which not only provides a flexible way for humans to command but also allows the robot to navigate through more challenging and cluttered scenes. However, it is non-trivial to translate human language instructions all the way to low-level leg joint actions. We propose NaVILA, a 2-level framework that unifies a Vision-Language-Action model (VLA) with locomotion skills. Instead of directly predicting low-level actions from VLA, NaVILA first generates mid-level actions with spatial information in the form of language, (e.g., \"moving forward 75cm\"), which serves as an input for a visual locomotion RL policy for execution. NaVILA substantially improves previous approaches on existing benchmarks. The same advantages are demonstrated in our newly developed benchmarks with IsaacLab, featuring more realistic scenes, low-level controls, and real-world robot experiments."
    },
    {
        "title": "Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances",
        "link_suffix": "/forum?id=16O8GCm8Wn",
        "link": "https://openreview.net/forum?id=16O8GCm8Wn",
        "pdf_link": "https://openreview.net/pdf?id=16O8GCm8Wn",
        "keywords": "AI Security, Watermark, Diffusion Model, Image Editing",
        "abstract": "Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Our model and benchmark will be publicly available."
    },
    {
        "title": "Space-Correlated Transformer: Jointly Explore the Matching and Motion Clues in 3D Single Object Tracking",
        "link_suffix": "/forum?id=Sl1kRAATbw",
        "link": "https://openreview.net/forum?id=Sl1kRAATbw",
        "pdf_link": "https://openreview.net/pdf?id=Sl1kRAATbw",
        "keywords": "3D Single Object Tracking; Point Cloud; Transformer; Space-Correlation",
        "abstract": "3D Single Object Tracking (3D SOT) in LiDAR point clouds plays a crucial role in autonomous driving. Current approaches mostly follow two paradigms, i.e., Siamese matching-based and motion-centric. However, LiDAR point clouds lack enough appearance information, while the motion-centric trackers suffer from complex model structures. To address these issues, we present a novel and conceptually simple tracking framework dubbed SCtrack, which jointly explores the matching and motion clues in point clouds. Specifically, SCtrack embeds point clouds into spatially structured features and conducts space correlation along the aligned spatial region. The target relative motion is directly inferred from the correlated features. In contrast to prevalent PointNet-based features, our spatially structured representation inherently models motion clues among the consecutive frames of point clouds, thereby being complementary to appearance matching. To better utilize the aligned structured features, we employ a strategy of varied-size space regions that adapt to different target shapes and locations during space correlation. Without bells and whistles, SCtrack achieves leading performance, with 89.1%, 71.5%, and 62.7% precision on KITTI, NuScenes, and Waymo Open Dataset, and runs at a considerably high speed of 60 Fps on a single RTX3090 GPU. Extensive studies validate the effectiveness of our SCtrack framework. The code will be released."
    },
    {
        "title": "On-Policy Policy Gradient Reinforcement Learning Without On-Policy Sampling",
        "link_suffix": "/forum?id=zJfOyS1YLW",
        "link": "https://openreview.net/forum?id=zJfOyS1YLW",
        "pdf_link": "https://openreview.net/pdf?id=zJfOyS1YLW",
        "keywords": "reinforcement learning, on-policy, policy gradient, data collection",
        "abstract": "On-policy reinforcement learning RL algorithms perform policy updates using i.i.d. trajectories collected by the current policy. However, after observing only a finite number of trajectories, on-policy sampling may produce data that fails to match the expected on-policy data distribution. This sampling error leads to noisy updates and data inefficient on-policy learning. Recent work in the policy evaluation setting has shown that non-i.i.d., off-policy sampling can produce data with lower sampling error than on-policy sampling can produce~\\citep{zhong2022robust}. Motivated by this observation, we introduce an adaptive, off-policy sampling method to improve the data efficiency of on-policy policy gradient algorithms. Our method, Proximal Robust On-Policy Sampling (PROPS) reduces sampling error by collecting data with a behavior policy that increases the probability of sampling actions that are under-sampled with respect to the current policy. Rather than discarding data from old policies -- as is commonly done in on-policy algorithms -- PROPS uses data collection to adjust the distribution of previously collected data to be approximately on-policy. We empirically evaluate PROPS on both continuous-action MuJoCo benchmark tasks as well discrete-action tasks and demonstrate that (1) PROPS decreases sampling error throughout training and (2) improves the data efficiency of on-policy policy gradient algorithms. Our work improves the RL community\u2019s understanding of a nuance in the on-policy vs off-policy dichotomy: on-policy learning requires on-policy data, not on-policy sampling."
    },
    {
        "title": "Revisiting In-context Learning Inference Circuit in Large Language Models",
        "link_suffix": "/forum?id=xizpnYNvQq",
        "link": "https://openreview.net/forum?id=xizpnYNvQq",
        "pdf_link": "https://openreview.net/pdf?id=xizpnYNvQq",
        "keywords": "In-context Learning; Induction Circuit; Mechanistic Interpretability",
        "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm on Language Models (LMs) with inner mechanisms un-explored. There are already existing works describing the inner processing of ICL, while they struggle to capture all the inference phenomena in large language models. Therefore, this paper proposes a comprehensive circuit to model the inference dynamics and try to explain the observed phenomena of ICL. In detail, we divide ICL inference into 3 major operations: (1) Summarize: LMs encode every input text (demonstrations and queries) into linear representation in the hidden states with sufficient information to solve ICL tasks. (2) Semantics Merge: LMs merge the encoded representations of demonstrations with their corresponding label tokens to produce joint representations of labels and demonstrations. (3) Feature Retrieval and Copy: LMs search the joint representations similar to the query representation on a task subspace, and copy the searched representations into the query. Then, language model heads capture these copied label representations to a certain extent and decode them into predicted labels. The proposed inference circuit successfully captured many phenomena observed during the ICL process, making it a comprehensive and practical explanation of the ICL inference process. Moreover, ablation analysis by disabling the proposed steps seriously damages the ICL performance, suggesting the proposed inference circuit is a dominating mechanism. Additionally, we confirm and list some bypass mechanisms that solve ICL tasks in parallel with the proposed circuit."
    },
    {
        "title": "FreeVS: Generative View Synthesis on Free Driving Trajectory",
        "link_suffix": "/forum?id=dTGH9vUVdf",
        "link": "https://openreview.net/forum?id=dTGH9vUVdf",
        "pdf_link": "https://openreview.net/pdf?id=dTGH9vUVdf",
        "keywords": "Novel View Synthesis, Driving Scene, Free Trajectory, Image Generation",
        "abstract": "Existing reconstruction-based novel view synthesis methods for driving scenes focus on synthesizing camera views along the recorded trajectory of the ego vehicle. \nTheir image rendering performance will severely degrade on viewpoints falling out of the recorded trajectory, where camera rays are untrained.\nWe propose FreeVS, a novel fully generative approach that can synthesize camera views on free new trajectories in real driving scenes. \nTo control the generation results to be 3D consistent with the real scenes and accurate in viewpoint pose, we propose the pseudo-image representation of view priors to control the generation process.\nViewpoint translation simulation is applied on pseudo-images to simulate camera movement in each direction.\nOnce trained, FreeVS can be applied to any validation sequences without reconstruction process and synthesis views on novel trajectories.\nMoreover, we propose two new challenging benchmarks tailored to driving scenes, which are novel camera synthesis and novel trajectory synthesis, emphasizing the freedom of viewpoints.\nGiven that no ground truth images are available on novel trajectories, we also propose to evaluate the consistency of images synthesized on novel trajectories with 3D perception models.\nExperiments on the Waymo Open Dataset show that FreeVS has a strong image synthesis performance on both the recorded trajectories and novel trajectories. \nThe code will be released."
    }
]