[{"title": "DualContrast: Unsupervised Disentangling of Content and Transformations with Implicit Parameterization", "link_suffix": "/forum?id=EMKZyZSl70", "link": "https://openreview.net/forum?id=EMKZyZSl70", "pdf_link": "https://openreview.net/pdf?id=EMKZyZSl70", "keywords": "Unsupervised Learning, Shape Analysis, Identifiability in Representation Learning, Disentangled Representation Learning, ML in Biology", "abstract": "Unsupervised disentanglement of content and transformation is significantly important for analyzing shape-focused real-world scientific image datasets, given their efficacy in solving downstream image-based shape-analysis tasks. The existing relevant works address the problem by explicitly parameterizing the transformation factors, significantly reducing their expressiveness. Moreover, they are not applicable in cases where transformations can not be readily parametrized. An alternative to such explicit approaches is contrastive methods with data augmentation, which implicitly disentangles transformations and content. However, the existing contrastive strategies are insufficient to this end. Therefore, we developed a novel contrastive method with generative modeling, DualContrast, specifically for unsupervised disentanglement of content and transformations in shape-focused image datasets. DualContrast works by creating positive and negative pairs for both content and transformation factors from data and latent spaces. Our extensive experiments showcase the superiority of DualContrast over existing self-supervised and explicit parameterization approaches. We leveraged DualContrast to disentangle protein identities and protein conformations in cellular 3D protein images. Moreover, we also disentangled transformations in MNIST, viewpoint in the Linemod, and human deformation in the Starmen dataset as transformations using DualContrast in an unsupervised manner.", "title_embedding_index": 11100, "title_abs_embedding_index": 11125}, {"title": "WASH: Train your Ensemble with Communication-Efficient Weight Shuffling, then Average", "link_suffix": "/forum?id=fhJeqL1rRg", "link": "https://openreview.net/forum?id=fhJeqL1rRg", "pdf_link": "https://openreview.net/pdf?id=fhJeqL1rRg", "keywords": "weight averaging, model averaging, model merging, permutation, communication, distributed, parallel, ensembling", "abstract": "The performance of deep neural networks is enhanced by ensemble methods, which average the output of several models. However, this comes at an increased cost at inference. Weight averaging methods aim to balance the generalization of ensembling and the inference speed of a single model by averaging the parameters of an ensemble of models. Yet, naive averaging results in poor performance as models converge to different loss basins, and aligning the models to improve the performance of the average is challenging. Alternatively, inspired by distributed training, methods like DART and PAPA have been proposed to train several models in parallel such that they will end up in the same basin, resulting in good averaging accuracy. However, these methods either compromise ensembling accuracy or demand significant communication between models during training. In this paper, we introduce WASH, a novel distributed method for training model ensembles for weight averaging that achieves state-of-the-art image classification accuracy. WASH maintains models within the same basin by randomly shuffling a small percentage of weights during training, resulting in diverse models and lower communication costs compared to standard parameter averaging methods.", "title_embedding_index": 11101, "title_abs_embedding_index": 11126}, {"title": "FRAPPE: Fast RAG-Inspired Prompt Evaporator", "link_suffix": "/forum?id=MjR5LcAGXJ", "link": "https://openreview.net/forum?id=MjR5LcAGXJ", "pdf_link": "https://openreview.net/pdf?id=MjR5LcAGXJ", "keywords": "Compression, Prompt engineering, Efficient LLM Inference, Toxicity reduction, Task-agnostic, Summarization", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in various tasks such as multi-document QA, summarization, and text classification. This has been achieved in part by recent advancements in prompt engineering and in-context learning (ICL), enabling LLMs to consume tens of thousands of input tokens as the supported context for the given query. However, this creates higher computational costs, longer latency, and potential performance degradation. To address these issues, we propose a task-agnostic and efficient approach called \u201cFast RAG Inspired Prompt Evaporator\u201d, or FRAPPE, to significantly reduce LLMs\u2019 latency, memory requirement, and computation by compressing input tokens. Unlike many other proposed approaches for prompt compression, our method does not rely on any large model for computing conditional probabilities, and data preparation is fast with negligible memory requirements. In particular, our approach first pre-processes the input data, categorizes and ranks phrases based on their informativeness, and finally selects the highest-ranked phrases to generate highly compressed and extractive input. We show the efficacy of our approach through a comprehensive set of experiments on public datasets and benchmarks. For instance, on the summarization task of the MeetingBank dataset, at a compression rate of 70%, our proposed approach achieves performance similar to the full context while performing compression up to 4 times faster than the contemporary state of the art compression algorithms.  We extend FRAPPE to create the Context-Aware FRAPPE algorithm, which incorporates task-specific information when ranking phrases, which further improves performance of downstream tasks using compressed text.  Additionally, we demonstrate that the use of FRAPPE can reduce toxicity by close to 50% relative to the original text by removing extraneous vitriolic phrases, in contrast to other compression methods, which often increase toxicity.", "title_embedding_index": 11102, "title_abs_embedding_index": 11127}, {"title": "The Challenging Growth: Evaluating the Scalability of Causal Models", "link_suffix": "/forum?id=HBf6HFnpmH", "link": "https://openreview.net/forum?id=HBf6HFnpmH", "pdf_link": "https://openreview.net/pdf?id=HBf6HFnpmH", "keywords": "Causality, benchmark, causal discovery, causal inference", "abstract": "One of the pillars of causality is the study of causal models and understanding\nunder which hypotheses we can guarantee their ability to grasp causal information\nand to leverage it for making inferences. Real causal phenomena, however,\nmay involve drastically different settings such as high dimensionality, causal insufficiency,\nand nonlinearities, which can be in stark contrast with the initial assumptions\nmade by most models. Additionally, providing fair benchmarks under\nsuch conditions presents challenges due to the lack of realistic data where the\ntrue data generating process is known. Consequently, most analyses converge\ntowards either small and synthetic toy examples or theoretical analyses, while\nempirical evidence is limited. In this work, we present in-depth experimental\nresults on two large datasets modeling a real manufacturing scenario, which we\nrelease. We show the nontrivial behavior of a well-known manufacturing process,\nsimulated using a physics-based simulator built and validated by domain experts.\nWe demonstrate the inadequacy of many state-of-the-art models and analyze the\nwide differences in their performance and tractability, both in terms of runtime\nand memory complexity. We observe that a wide range of causal models are computationally\nprohibitive for certain tasks, whereas others do not suffer from those\nburdens by design, but require to pay a price in terms of expressiveness. Upon\npublication, all artefacts will be released to serve as reference for future research\non real world applications of causality, including a general web-page and a leaderboard\nfor benchmarking.", "title_embedding_index": 11103, "title_abs_embedding_index": 11128}, {"title": "HOGT: High-Order Graph Transformers", "link_suffix": "/forum?id=BapOwAzicb", "link": "https://openreview.net/forum?id=BapOwAzicb", "pdf_link": "https://openreview.net/pdf?id=BapOwAzicb", "keywords": "Graph representation learning, Graph Transformer", "abstract": "Inspired by the success of transformers on natural language processing (NLP) and computer vision (CV) tasks, graph transformers (GTs) have recently been proposed to boost the performance of graph learning. \nHowever, the attention mechanisms used in existing GTs face certain limitations in capturing crucial topological information or scaling to large graphs, due to their quadratic complexity. \nTo address these limitations, in this paper, we propose a high-order information propagation strategy within the transformer architecture to simultaneously learn the local, long-range, and higher-order relationships of the graph. \n\\textcolor{blue}{We first propose a flexible sampling method to extract communities from the graph, and create new community nodes and in particular a learnable community sampling method with reinforcement learning.} We then propose a three-step message-passing strategy dubbed \\emph{HOGT} to capture the local and higher-order information in the communities and propagate long-range dependency information between the community nodes to finally obtain comprehensive node representations. Note that as structural information has been flexibly integrated into our designed community-based message-passing scheme, HOGT discards the positional encoding which was thought to be important for GT.", "title_embedding_index": 11104, "title_abs_embedding_index": 11129}, {"title": "Can Data be Myopic? Outlier Detection in High-Dimensional Tabular Data via Subspaces", "link_suffix": "/forum?id=UM6yage1H0", "link": "https://openreview.net/forum?id=UM6yage1H0", "pdf_link": "https://openreview.net/pdf?id=UM6yage1H0", "keywords": "One-class classification, Tabular Data, Generative methods, Deep Learning, Generative Adversarial Active Learning, Subspace Outlier Detection", "abstract": "Outlier detection in high-dimensional tabular data is an important task in data mining, essential for many downstream tasks and applications. Existing unsupervised outlier detection algorithms face one or more problems, including inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV). To address these issues, we introduce Generative Subspace Adversarial Active Learning (GSAAL), a novel approach that uses a Generative Adversarial Network with multiple adversaries. These adversaries learn the marginal class probability functions over different data subspaces, while a single generator in the full space models the entire distribution of the inlier class. GSAAL is specifically designed to address the MV limitation while also handling the IA and CD, being the only method to do so. We provide a mathematical formulation of MV, convergence guarantees for the discriminators, and scalability results for GSAAL. Our extensive experiments demonstrate the effectiveness and scalability of GSAAL, highlighting its superior performance compared to other popular OD methods, especially in MV scenarios.", "title_embedding_index": 11105, "title_abs_embedding_index": 11130}, {"title": "SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs", "link_suffix": "/forum?id=lqHv6dxBkj", "link": "https://openreview.net/forum?id=lqHv6dxBkj", "pdf_link": "https://openreview.net/pdf?id=lqHv6dxBkj", "keywords": "sparse training, low rank adapter, LLM, optimization", "abstract": "We propose SLoPe, a Double-PrunedSparse PlusLazy Low-rank AdapterPretraining method for LLMs that improves the accuracy of sparse LLMs while accelerating their pretraining and inference and reducing their memory footprint. Sparse pretraining of LLMs reduces the accuracy of the model, to overcome this, prior work uses dense models during fine-tuning. SLoPe improves the accuracy of sparsely pretrained models by adding low-rank adapters in the final 1% iterations of pretraining without adding significant overheads to the model pretraining and inference. In addition, SLoPe uses a double-pruned backward pass formulation that prunes the transposed weight matrix using N:M sparsity structures to enable an accelerated sparse backward pass. SLoPe accelerates the training and inference of models with billions of parameters up to 1.25\u00d7 and 1.54\u00d7 respectively (OPT-33B and OPT-66B) while reducing their memory usage by up to 0.63\u00d7 and 0.61\u00d7 for training and inference respectively.", "title_embedding_index": 11106, "title_abs_embedding_index": 11131}, {"title": "DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?", "link_suffix": "/forum?id=DSsSPr0RZJ", "link": "https://openreview.net/forum?id=DSsSPr0RZJ", "pdf_link": "https://openreview.net/pdf?id=DSsSPr0RZJ", "keywords": "data science, agent, benchmark, llm", "abstract": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12% of data analysis tasks and achieving a 34.74% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.", "title_embedding_index": 11107, "title_abs_embedding_index": 11132}, {"title": "Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder", "link_suffix": "/forum?id=LTDtjrv02Y", "link": "https://openreview.net/forum?id=LTDtjrv02Y", "pdf_link": "https://openreview.net/pdf?id=LTDtjrv02Y", "keywords": "Latent NeRF, NeRF, Autoencoder, Inverse Graphics, Nerfstudio, 3D", "abstract": "While pre-trained image autoencoders are increasingly utilized in computer vision, the application of inverse graphics in 2D latent spaces has been under-explored. \nYet, besides reducing the training and rendering complexity, applying inverse graphics in the latent space enables a valuable interoperability with other latent-based 2D methods.\nThe major challenge is that inverse graphics cannot be directly applied to such image latent spaces because they lack an underlying 3D geometry. \nIn this paper, we propose an Inverse Graphics Autoencoder (IG-AE) that specifically addresses this issue.\nTo this end, we regularize an image autoencoder with 3D-geometry by aligning its latent space with jointly trained latent 3D scenes. \nWe utilize the trained IG-AE to bring NeRFs to the latent space with a latent NeRF training pipeline, which we implement in an open-source extension of the Nerfstudio framework, thereby unlocking latent scene learning for its supported methods. \nWe experimentally confirm that Latent NeRFs trained with IG-AE present an improved quality compared to a standard autoencoder, all while exhibiting training and rendering accelerations with respect to NeRFs trained in the image space.", "title_embedding_index": 11108, "title_abs_embedding_index": 11133}, {"title": "KnowData: Knowledge-Enabled Data Generation for Improving Multimodal Models", "link_suffix": "/forum?id=FqWtMGw8tt", "link": "https://openreview.net/forum?id=FqWtMGw8tt", "pdf_link": "https://openreview.net/pdf?id=FqWtMGw8tt", "keywords": "Multimodal learning; Knowledge integration; Synthetic datasets; Zero-shot classification", "abstract": "In this paper, we introduce a framework to enhance the quality of synthetic image-text pairs for multimodal models such as CLIP. Our approach, named KnowData, integrates real-world knowledge explicitly into the generation of text descriptions. It combines structured knowledge from knowledge graphs like ConceptNet and unstructured knowledge extracted from Wikipedia, to ensure that the generated text descriptions are both contextually rich and accurately reflective of real-world knowledge. Additionally, we leverage Large Language Models for the expansion, summarization, and refinement of the text descriptions to ensure their coherence. These enriched texts are subsequently used to generate images through advanced text-to-image models like Stable Diffusion and DALLE-3. CLIP models are then fine-tuned with these synthetic image-text pairs for zero-shot classification tasks. Our experiments across 9 datasets demonstrate that CLIP models fine-tuned with our knowledge-guided synthetic datasets outperform state-of-the-art (SOTA) zero-shot CLIP methods (e.g., +11.23% on DTD and +4% on EuroSAT based on ViT-B/16 model; +11.47% on CIFAR-100 and +7.99% on DTD based on ResNet-50 model). These results showcase the improved out-of-distribution robustness and adaptability of our approach across a diverse set of data domains. We further substantiate the design of KnowData through ablation studies, revealing that the integration of knowledge not only enhances zero-shot performance but also contributes to the reliability, diversity, and detail-orientation of the generated synthetic images, thereby offering better data scaling laws for model performance.", "title_embedding_index": 11109, "title_abs_embedding_index": 11134}, {"title": "What Are Good Positional Encodings for Directed Graphs?", "link_suffix": "/forum?id=s4Wm71LFK4", "link": "https://openreview.net/forum?id=s4Wm71LFK4", "pdf_link": "https://openreview.net/pdf?id=s4Wm71LFK4", "keywords": "Directed graphs, positional encodings, graph neural networks", "abstract": "Positional encodings (PEs) are essential for building powerful and expressive graph neural networks and graph transformers, as they effectively capture the relative spatial relationships between nodes. Although extensive research has been devoted to PEs in undirected graphs, PEs for directed graphs remain relatively unexplored. This work seeks to address this gap. We first introduce the notion ofWalk Profile, a generalization of walk-counting sequences for directed graphs. A walk profile encompasses numerous structural features crucial for directed graph-relevant applications, such as program analysis and circuit performance prediction. We identify the limitations of existing PE methods in representing walk profiles and propose a novelMulti-q Magnetic Laplacian PE, which extends the Magnetic Laplacian eigenvector-based PE by incorporating multiple potential factors. The new PE can provably express walk profiles. Furthermore, we generalize prior basis-invariant neural networks to enable the stable use of the new PE in the complex domain. Our numerical experiments validate the expressiveness of the proposed PEs and demonstrate their effectiveness in solving sorting network satisfiability and performing well on general circuit benchmarks.", "title_embedding_index": 11110, "title_abs_embedding_index": 11135}, {"title": "ASTrA: Adversarial Self-supervised Training with Adaptive-Attacks", "link_suffix": "/forum?id=ZbkqhKbggH", "link": "https://openreview.net/forum?id=ZbkqhKbggH", "pdf_link": "https://openreview.net/pdf?id=ZbkqhKbggH", "keywords": "Self-supervised Adversarial Training, Robustness", "abstract": "Existing self-supervised adversarial training (self-AT) methods rely on handcrafted\nadversarial attack strategies for PGD attacks, which fail to adapt to\nthe evolving learning dynamics of the model and do not account for instance specific\ncharacteristics of images. This results in sub-optimal adversarial robustness\nand limits the alignment between clean and adversarial data distributions.\nTo address this, we propose ASTrA (Adversarial Self-supervised Training with\nAdaptive-Attacks), a novel framework introducing a learnable, self-supervised\nattack strategy network that autonomously discovers optimal attack parameters\nthrough exploration-exploitation in a single training episode. ASTrA leverages a\nreward mechanism based on contrastive loss, optimized with REINFORCE, enabling\nadaptive attack strategies without labeled data or additional hyperparameters.\nWe further introduce a mixed contrastive objective to align the distribution\nof clean and adversarial examples in representation space. ASTrA achieves\nstate-of-the-art results on CIFAR-10, CIFAR-100, and STL-10 while integrating\nseamlessly as a plug-and-play module for other self-AT methods. ASTrA shows\nscalability to larger datasets, demonstrates strong semi-supervised performance,\nand is resilient to robust overfitting, backed by explainability analysis on optimal\nattack strategies.", "title_embedding_index": 11111, "title_abs_embedding_index": 11136}, {"title": "GUI-World: A GUI-oriented Dataset for Multimodal LLM-based Agents", "link_suffix": "/forum?id=QarKTT5brZ", "link": "https://openreview.net/forum?id=QarKTT5brZ", "pdf_link": "https://openreview.net/pdf?id=QarKTT5brZ", "keywords": "GUI, GUI Agent, Multimodal Large Language Model, Benchmark, Dataset, Video LLM, Instruction Tuning", "abstract": "Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-oriented questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-oriented tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding.", "title_embedding_index": 11112, "title_abs_embedding_index": 11137}, {"title": "A Pontryagin Perspective on Reinforcement Learning", "link_suffix": "/forum?id=t3rdi80xCz", "link": "https://openreview.net/forum?id=t3rdi80xCz", "pdf_link": "https://openreview.net/pdf?id=t3rdi80xCz", "keywords": "reinforcement learning, control theory", "abstract": "Reinforcement learning has traditionally focused on learning state-dependent policies to solve optimal control problems in aclosed-loopfashion. In this work, we introduce the paradigm ofopen-loop reinforcement learningwhere a fixed action sequence is learned instead. We present three new algorithms: one robust model-based method and two sample-efficient model-free methods. Rather than basing our algorithms on Bellman's equation from dynamic programming, our work builds onPontryagin's principlefrom the theory of open-loop optimal control. We provide convergence guarantees and evaluate all methods empirically on a pendulum swing-up task, as well as on two high-dimensional MuJoCo tasks, demonstrating remarkable performance compared to existing baselines.", "title_embedding_index": 11113, "title_abs_embedding_index": 11138}, {"title": "Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2", "link_suffix": "/forum?id=TZa84ZkOLM", "link": "https://openreview.net/forum?id=TZa84ZkOLM", "pdf_link": "https://openreview.net/pdf?id=TZa84ZkOLM", "keywords": "protein design, diffusion model, motif scaffolding, protein structure, generative model, biology, bioinformatics, structural biology", "abstract": "Protein diffusion models have emerged as a promising approach for protein design. One such pioneering model is Genie, a method that asymmetrically represents protein structures during the forward and backward processes, using simple Gaussian noising for the former and expressive SE(3)-equivariant attention for the latter. In this work we introduce Genie 2, extending Genie to capture a larger and more diverse protein structure space through architectural innovations and massive data augmentation. Genie 2 adds motif scaffolding capabilities via a novel multi-motif framework that designs co-occurring motifs with unspecified inter-motif positions and orientations. This makes possible complex protein designs that engage multiple interaction partners and perform multiple functions. On both unconditional and conditional generation, Genie 2 achieves state-of-the-art performance, outperforming all known methods on key design metrics including designability, diversity, and novelty. Genie 2 also solves more motif scaffolding problems than other methods and does so with more unique and varied solutions. Taken together, these advances set a new standard for structure-based protein design.", "title_embedding_index": 11114, "title_abs_embedding_index": 11139}, {"title": "Understanding When and Why Graph Attention Mechanisms Work via Node Classification", "link_suffix": "/forum?id=PfZekmXRjN", "link": "https://openreview.net/forum?id=PfZekmXRjN", "pdf_link": "https://openreview.net/pdf?id=PfZekmXRjN", "keywords": "Graph attention mechanisms, node classification, contextual stochastic block model, over-smoothing", "abstract": "Despite the growing popularity of graph attention mechanisms, their theoretical understanding remains limited. This paper aims to explore the conditions under which these mechanisms are effective in node classification tasks through the lens of Contextual Stochastic Block Models (CSBMs). Our theoretical analysis reveals that incorporating graph attention mechanisms isnot universally beneficial. Specifically, by appropriately definingstructural noiseandfeature noisein graphs, we show that graph attention mechanisms can enhance classification performance when structural noise exceeds feature noise. Conversely, when feature noise predominates, simpler graph convolution operations are more effective. Furthermore, we examine the over-smoothing phenomenon and show that, in the high signal-to-noise ratio (SNR) regime, graph convolutional networks suffer from over-smoothing, whereas graph attention mechanisms can effectively resolve this issue. Building on these insights, we propose a novel multi-layer Graph Attention Network (GAT) architecture that significantly outperforms single-layer GATs in achievingperfect node classificationin CSBMs, relaxing the SNR requirement from $ \\omega(\\sqrt{\\log n}) $ to $ \\omega(\\sqrt{\\log n} / \\sqrt[3]{n}) $. To our knowledge, this is the first study to delineate the conditions for perfect node classification using multi-layer GATs. Our theoretical contributions are corroborated by extensive experiments on both synthetic and real-world datasets, highlighting the practical implications of our findings.", "title_embedding_index": 11115, "title_abs_embedding_index": 11140}, {"title": "Beyond the Lazy versus Rich Dichotomy: Geometry Insights in Feature Learning from Task-Relevant Manifold Untangling", "link_suffix": "/forum?id=4IRYGvyevW", "link": "https://openreview.net/forum?id=4IRYGvyevW", "pdf_link": "https://openreview.net/pdf?id=4IRYGvyevW", "keywords": "Computational neuroscience, storage capacity, neural manifolds, representational geometry, rich and lazy learning, training dynamics, feature learning", "abstract": "The ability to integrate task-relevant information into neural representations is a fundamental aspect of both human and machine intelligence. Recent studies have explored the transition of neural networks from thelazytraining regime (where the trained network is equivalent to a linear model of initial random features) to therichfeature learning regime (where the network learns task-relevant features). However, most approaches focus on weight matrices or neural tangent kernels, limiting their relevance for neuroscience due to the lack of representation-based methods to study feature learning. Furthermore, the simple lazy-versus-rich dichotomy overlooks the potential for richer subtypes of feature learning driven by variations in learning algorithms, network architectures, and data properties.In this work, we present a framework based on representational geometry to study feature learning. The key idea is to use the untangling of task-relevant neural manifolds as a signature of rich learning. We employ manifold capacity\u2014a representation-based measure\u2014to quantify this untangling, along with geometric metrics to uncover structural differences in feature learning. Our contributions are threefold: First, we show both theoretically and empirically that task-relevant manifolds untangle during rich learning, and that manifold capacity quantifies the degree of richness. Second, we use manifold geometric measures to reveal distinct learning stages and strategies driven by network and data properties, demonstrating that feature learning is richer than the lazy-versus-rich dichotomy. Finally, we apply our method to problems in neuroscience and machine learning, providing geometric insights into structural inductive biases and out-of-distribution generalization. Our work introduces a novel perspective for understanding and quantifying feature learning through the lens of representational geometry.", "title_embedding_index": 11116, "title_abs_embedding_index": 11141}, {"title": "Integrating State Space Model and Transformer for Global-Local Processing in Super-Resolution Networks", "link_suffix": "/forum?id=1YZw3RK2kg", "link": "https://openreview.net/forum?id=1YZw3RK2kg", "pdf_link": "https://openreview.net/pdf?id=1YZw3RK2kg", "keywords": "Computer Vision and Pattern Recognition, image super-resolution", "abstract": "Single image super-resolution aims to recover high-quality images from low-resolution inputs and is a key topic in computer vision. While Convolutional Neural Networks (CNNs) and Transformer models have shown great success in SISR, they have notable limitations: CNNs struggle with non-local information, and Transformers face quadratic complexity in global attention. To address these issues, Mamba models introduce a State Space Model (SSM) with linear complexity. However, recent research shows that Mamba models underperform in capturing local dependencies in 2D images. In this paper, we propose a novel approach that integrates Mamba SSM blocks with Transformer self-attention layers, combining their strengths. We also introduce register tokens and a new SE-Scaling attention mechanism to improve performance while reducing computational costs. The resulting super-resolution network, SST (State Space Transformer), achieves state-of-the-art results on both classical and lightweight tasks.", "title_embedding_index": 11117, "title_abs_embedding_index": 11142}, {"title": "Mycroft: Towards Effective and Efficient External Data Augmentation", "link_suffix": "/forum?id=GULx8rzzjC", "link": "https://openreview.net/forum?id=GULx8rzzjC", "pdf_link": "https://openreview.net/pdf?id=GULx8rzzjC", "keywords": "data scarcity; supervised learning; data augmentation", "abstract": "Machine learning (ML) models often require large amounts of data to perform well. \n   When the available data is limited, model trainers may need to acquire more data from external sources.\n   Often, useful data is held by private entities who are hesitant to share their data due to propriety and privacy concerns. \n   This makes it challenging and expensive for model trainers to acquire the data they need to improve model performance.\n   To address this challenge, we propose $\\texttt{Mycroft}$, a \n   data-efficient method that enables model trainers to evaluate the relative \n   utility of different data sources while working with a constrained data-sharing \n   budget. By leveraging feature space distances and gradient matching, $\\texttt{Mycroft}$ \n   identifies small but informative data subsets from each owner, allowing model \n   trainers to maximize performance with minimal data exposure. Experimental \n   results across four tasks in two domains show that $\\texttt{Mycroft}$ converges rapidly \n   to the performance of the full-information baseline, where all data is shared.\n   Moreover, $\\texttt{Mycroft}$ is robust to noise and can effectively rank data owners by \n   utility.  $\\texttt{Mycroft}$ can pave the way for democratized training of high performance ML models.", "title_embedding_index": 11118, "title_abs_embedding_index": 11143}, {"title": "Learning grid cells by predictive coding", "link_suffix": "/forum?id=2ofVtMvRil", "link": "https://openreview.net/forum?id=2ofVtMvRil", "pdf_link": "https://openreview.net/pdf?id=2ofVtMvRil", "keywords": "grid cells, predictive coding, computational neuroscience", "abstract": "Grid cells in the medial entorhinal cortex (MEC) of the mammalian brain exhibit a strikingly regular hexagonal firing field over space. These cells are learned after birth and are thought to support spatial navigation but also more abstract computations. Although various computational models, including those based on artificial neural networks, have been proposed to explain the formation of grid cells, the process through which the MEC circuit ${\\it learns}$ to develop grid cells remains unclear. In this study, we argue that predictive coding, a biologically plausible plasticity rule known to learn visual representations, can also train neural networks to develop hexagonal grid representations from spatial inputs. We demonstrate that grid cells emerge robustly through predictive coding in both static and dynamic environments, and we develop an understanding of this grid cell learning capability by analytically comparing predictive coding with existing models. Our work therefore offers a novel and biologically plausible perspective on the learning mechanisms underlying grid cells. Moreover, it extends the predictive coding theory to the hippocampal formation, suggesting a unified learning algorithm for diverse cortical representations.", "title_embedding_index": 11119, "title_abs_embedding_index": 11144}, {"title": "How does Your RL Agent Explore? An Optimal Transport Analysis of Occupancy Measure Trajectories", "link_suffix": "/forum?id=LOiYxBcGA9", "link": "https://openreview.net/forum?id=LOiYxBcGA9", "pdf_link": "https://openreview.net/pdf?id=LOiYxBcGA9", "keywords": "reinforcement learning, wasserstein distance, occupancy measure, exploration-exploitation, effort of learning", "abstract": "The rising successes of RL are propelled by combining smart algorithmic strategies and deep architectures to optimize the distribution of returns and visitations over the state-action space. A quantitative framework to compare the learning processes of these eclectic RL algorithms is currently absent but desired in practice. We address this gap by representing the learning process of an RL algorithm as a sequence of policies generated during training, and then studying the policy trajectory induced in the manifold of occupancy measures. \nUsing an optimal transport-based metric, we measure the length of the paths induced by the policy sequence yielded by an RL algorithm between an initial policy and a final optimal policy. Hence, we first define theEffort of Sequential Learning(ESL). ESL quantifies the relative distance that an RL algorithm travels compared to the shortest path from the initial to the optimal policy. Further, we connect the dynamics of policies in the occupancy measure space and regret, another metric to understand the suboptimality of an RL algorithm, by defining theOptimal Movement Ratio(OMR). OMR assesses the fraction of movements in the occupancy measure space that effectively reduce an analogue of regret. Finally, we derive approximation guarantees to estimate ESL and OMR with finite number of samples and without access to an optimal policy. Through empirical analyses across various environments and algorithms, we demonstrate that ESL and OMR provide insights into the exploration processes of RL algorithms and hardness of different tasks in discrete and continuous MDPs.", "title_embedding_index": 11120, "title_abs_embedding_index": 11145}, {"title": "Retrieval-based Zero-shot Crowd Counting", "link_suffix": "/forum?id=YeOxaKHE9b", "link": "https://openreview.net/forum?id=YeOxaKHE9b", "pdf_link": "https://openreview.net/pdf?id=YeOxaKHE9b", "keywords": "Crowd-counting, Annotator free, Zero-shot, Vision-language models", "abstract": "Existing crowd-counting methods rely on the manual localization of each person in the image. While recent efforts have attempted to circumvent the annotation burden through vision-language models or crowd image generation, these approaches rely on pseudo-labels to perform crowd-counting. Simulated datasets provide an alternative to the annotation cost associated with real datasets. However, the use of large-scale simulated data often results in a distribution gap between real and simulated domains. To address the latter, we introduce knowledge retrieval inspired by knowledge-enhanced models in natural language processing. With knowledge retrieval, we extract simulated crowd images and their text descriptions to augment the image embeddings of real crowd images to improve zero-shot crowd-counting. Knowledge retrieval allows one to use a vast amount of non-parameterized knowledge during testing, enhancing a model's inference capability. Our work is the first to actively incorporate text information to regress the crowd count in any supervised manner. Moreover, to address the domain gap, we propose a pre-training and retrieval mechanism that uses unlabeled real crowd images along with simulated data. We report state-of-the-art results for zero-shot counting on five public datasets, surpassing existing multi-model crowd-counting methods. The code will be made publicly available after the review process.", "title_embedding_index": 11121, "title_abs_embedding_index": 11146}, {"title": "Can Large Language Models Effectively Modify Graphs?", "link_suffix": "/forum?id=WRKVA3TgSv", "link": "https://openreview.net/forum?id=WRKVA3TgSv", "pdf_link": "https://openreview.net/pdf?id=WRKVA3TgSv", "keywords": "LLMs, graphs", "abstract": "Graphs are essential tools for modeling complex relationships. While prior research with earlier generations of large language models (LLMs) showed them to struggle with basic graph primitives, we find that the situation has changed with modern state-of-the-art (SOTA) LLMs, which excel at these tasks. Given these advances, we propose a more challenging evaluation problem: graph modification, a foundational, interpretable, and non-trivial problem in which an LLM must determine the outcome of adding or deleting a given sequence of nodes or edges, and potentially then compute on the resulting modified graph. We introduce GraphModQA, a novel benchmark dataset comprising graph modification question-answer pairs designed to rigorously test LLMs\u2019 abilities in graph manipulation and dynamic reasoning. Our results show that while SOTA LLMs perform well on static graph property tasks, their accuracy degrades on graph modification tasks; their performance is particularly low as the number of modifications increases, and when the adjacency matrix is used to represent the graph --- an essential encoding not explored in previous work. We provide new techniques for improving performance on graph modification tasks, and we introduce Modify and Print (MAP) prompting, which asks models to output the intermediate adjacency matrices at each step, and which markedly improves the models' performance. Our findings highlight a critical gap in current LLM capabilities regarding dynamic graph reasoning tasks and underscore the potential of techniques like MAP prompting to mitigate these challenges.", "title_embedding_index": 11122, "title_abs_embedding_index": 11147}, {"title": "ESQA: Event Sequences Question Answering", "link_suffix": "/forum?id=q7aROKohBZ", "link": "https://openreview.net/forum?id=q7aROKohBZ", "pdf_link": "https://openreview.net/pdf?id=q7aROKohBZ", "keywords": "event sequences, large language models, multi-modality", "abstract": "Event sequences (ESs) arise in many practical domains including finance, retail, social networks, and healthcare. In the context of machine learning, event sequences can be seen as a special type of tabular data with annotated timestamps. Despite the importance of ESs modeling and analysis, little effort was made in adapting large language models (LLMs) to the ESs domain. In this paper, we highlight the common difficulties of ESs processing and propose a novel solution capable of solving multiple downstream tasks with little or no finetuning. In particular, we solve the problem of working with long sequences and improve time and numeric features processing. The resulting method, called ESQA, effectively utilizes the power of LLMs and, according to extensive experiments, achieves state-of-the-art results in the ESs domain.", "title_embedding_index": 11123, "title_abs_embedding_index": 11148}, {"title": "LLM4Solver: Large Language Model for Efficient Algorithm Design of Combinatorial Optimization Solver", "link_suffix": "/forum?id=XTxdDEFR6D", "link": "https://openreview.net/forum?id=XTxdDEFR6D", "pdf_link": "https://openreview.net/pdf?id=XTxdDEFR6D", "keywords": "Combinatorial Optimization Solver, Large Language Models, Evolutionary Search", "abstract": "The optimization of algorithms in exact combinatorial optimization (CO) solver plays a fundamental role in operations research.\nHowever, due to the extensive requirements on domain knowledge and the large search space for algorithm design, the refinement on these algorithms remains highly challenging for both manual and learning-based paradigms. \nTo tackle this problem, we propose a novel machine learning framework---large language model for exact combinatorial optimization solver (LLM4Solver)---to $\\textit{efficiently}$ design high-quality algorithms of the CO solvers. \nThe core idea is that, instead of searching in the high-dimensional and discrete symbolic space from scratch, we can utilize the prior knowledge learned from large language models to directly search in the space of programming languages.\nSpecifically, we first use a pre-trained LLM as the generator for high-quality algorithms. Then, to efficiently explore the discrete and non-gradient algorithm space, we employ a derivative-free evolutionary framework as the algorithm optimizer.\nExperiments on extensive benchmarks show that the algorithms learned by LLM4Solver $\\textit{significantly}$ outperform all the state-of-the-art (SOTA) human-designed and learning-based policies (on GPU) in terms of the solution quality, the solving efficiency, and the cross-benchmark generalization ability. \nThe appealing features of LLM4Solver include 1) the high training efficiency to outperform SOTA methods within ten iterations, and 2) the high cross-benchmark generalization ability on heterogeneous MIPLIB 2017.\nLLM4Solver shows the encouraging potential to efficiently design algorithms for the next generation of modern CO solvers.", "title_embedding_index": 11124, "title_abs_embedding_index": 11149}]