[{"title": "FairLoRA: Unpacking Bias Mitigation in Vision Models with Fairness-Regularized Low-Rank Adaptation", "link_suffix": "/forum?id=pB3KeBCnQs", "link": "https://openreview.net/forum?id=pB3KeBCnQs", "pdf_link": "https://openreview.net/pdf?id=pB3KeBCnQs", "keywords": "LoRA, ViTs, Fairness, Vision Models, Bias mitigation", "abstract": "Recent advances in parameter-efficient fine-tuning methods, such as Low Rank Adaptation (LoRA), have gained significant attention for their ability to efficiently adapt large foundational models to various downstream tasks. These methods are appreciated for achieving performance comparable to full fine-tuning on aggregate-level metrics, while significantly reducing computational costs. To systematically address fairness in LLMs previous studies fine-tune on fairness specific data using a larger LoRA rank than typically used. In this paper, we introduce FairLoRA, a novel fairness-specific regularizer for LoRA aimed at reducing performance disparities across data subgroups by minimizing per-class variance in loss. To the best of our knowledge, we are the first to introduce a fairness based finetuning through LoRA. Our results demonstrate that the need for higher ranks to mitigate bias is not universal; it depends on factors such as the pre-trained model, dataset, and task. More importantly, we systematically evaluate FairLoRA across various vision models, including ViT, DiNO, and CLIP, in scenarios involving distribution shifts. We further emphasize the necessity of using multiple fairness metrics to obtain a holistic assessment of fairness, rather than relying solely on the metric optimized during training.", "title_embedding_index": 3450, "title_abs_embedding_index": 3475}, {"title": "Wait, That's Not an Option: LLM Robustness with Incorrect Multiple-Choice Options", "link_suffix": "/forum?id=lbfjL60JdC", "link": "https://openreview.net/forum?id=lbfjL60JdC", "pdf_link": "https://openreview.net/pdf?id=lbfjL60JdC", "keywords": "LLMs, Reflective Judgment, Alignment, Instruction Following, Misleading Instructions", "abstract": "Decision-making under full alignment requires balancing between reasoning and faithfulness - a challenge for large language models (LLMs). This study explores whether LLMs prioritize following instructions over reasoning and truth when given \"misleading\" instructions, such as \"Respond solely with A or B\", even when neither option is correct. We introduce a new metric called \"reflective judgment\", which sheds new light on the relationship between the pre-training and post-training alignment schemes. In tasks ranging from basic arithmetic to domain-specific assessments, models like GPT-4o, o1-mini, or Claude 3 Opus adhered to instructions correctly but failed to reflect on the validity of the provided options. Contrary, models from the Llama 3.1 family (8B, 70B, 405B) or base Qwen2.5 (7B, 14B, 32B) families exhibit improved refusal rates with size, indicating a scaling effect.\nWe also observed that alignment techniques, though intended to enhance reasoning, sometimes weakened the models' ability to reject incorrect instructions, leading them to follow flawed prompts uncritically. Finally, we have also conducted a parallel human study revealing similar patterns in human behavior and annotations. We highlight how popular RLHF datasets might disrupt either training or evaluation due to annotations exhibiting poor reflective judgement.", "title_embedding_index": 3451, "title_abs_embedding_index": 3476}, {"title": "Root Cause Analysis of Failure with Observational Causal Discovery", "link_suffix": "/forum?id=2pEqXce0um", "link": "https://openreview.net/forum?id=2pEqXce0um", "pdf_link": "https://openreview.net/pdf?id=2pEqXce0um", "keywords": "causal discovery, root cause analysis", "abstract": "Finding the root cause of failures is a prominent problem in many complex networks. Causal inference provides us with tools to address this problem algorithmically to automate this process and solve it efficiently. The existing methods either use a known causal structure to identify root cause via backtracking the changes, or ignore the causal structure but rely on invariance tests to identify the changing causal mechanisms after the failure. We first establish a connection between root cause analysis and the \\textit{Interactive Graph Search (IGS)} problem. This mapping highlights the importance of causal knowledge: we demonstrate that any algorithm relying solely on marginal invariance tests to identify root causes must perform at least $\\Omega(\\log_{2}(n) + d\\log_{1+d}n)$ many tests, where $n$ represents the number of components and $d$ denotes the maximum out-degree of the graph. We then present an optimal algorithm that achieves this bound by reducing the root cause identification problem as an instance of IGS. Moreover, we show that even if the causal graph is partially known in the form of a Markov equivalence class, we can identify the root-cause with linear number of invariance tests. Our experiments on a production-level application demonstrate that, even in the absence of complete causal information, our approach accurately identifies the root cause of failures.", "title_embedding_index": 3452, "title_abs_embedding_index": 3477}, {"title": "No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs", "link_suffix": "/forum?id=kbm6tsICar", "link": "https://openreview.net/forum?id=kbm6tsICar", "pdf_link": "https://openreview.net/pdf?id=kbm6tsICar", "keywords": "ordinary differential equations, ODE discovery, dynamical systems", "abstract": "Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model\u2019s behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug development, may allow us to both verify its biological plausibility (e.g., the drug concentration curve is non-negative and decays to zero in the long term) and to design dosing guidelines (e.g., by looking at the peak concentration and its timing). Discovery of closed-form ordinary differential equations (ODEs) can be employed to obtain such insights by finding a compact mathematical equation and then analyzing it (a two-step approach). However, its widespread use (in pharmacology and other domains) is currently hindered because the analysis process may be time-consuming, requiring substantial mathematical expertise, or even impossible if the equation is too complex. Moreover, if the found equation's behavior does not satisfy the requirements, editing it or influencing the discovery algorithms to rectify it is challenging as the link between the symbolic form of an ODE and its behavior can be elusive. This paper proposes a conceptual shift to modeling low-dimensional dynamical systems by departing from the traditional two-step modeling process. Instead of first discovering a closed-form equation and then analyzing it, our approach, direct semantic modeling, predicts the semantic representation of the dynamical system (i.e., description of its behavior) directly from data, bypassing the need for complex post-hoc analysis. This direct approach also allows the incorporation of intuitive inductive biases into the optimization algorithm and editing the model's behavior directly, ensuring that the model meets the desired specifications. Our approach not only simplifies the modeling pipeline but also enhances the transparency and flexibility of the resulting models compared to traditional closed-form ODEs. We validate the effectiveness of this method through extensive experiments, demonstrating its advantages in terms of both performance and practical usability.", "title_embedding_index": 3453, "title_abs_embedding_index": 3478}, {"title": "Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models", "link_suffix": "/forum?id=vQhn4wrQ6j", "link": "https://openreview.net/forum?id=vQhn4wrQ6j", "pdf_link": "https://openreview.net/pdf?id=vQhn4wrQ6j", "keywords": "model souping, model merging, cross-lingual transfer, multilingual, math, mathematical reasoning, LLM, SFT", "abstract": "Model merging, such as model souping, is the practice of combining different models with the same architecture together without further training. In this work, we present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models (LLMs) for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities. Starting from the same pretrained model, we fine-tune separate \"experts\" on math instruction data in English and on generic instruction data in the target language. We then replace the top and bottom transformer layers of the math expert directly with layers from the language expert, which consequently enhances math performance in the target language. The resulting merged models outperform the individual experts and other merging methods on the math benchmark, MGSM, by 10% across four major languages where math instruction data is scarce. In addition, this layer swapping is simple, inexpensive, and intuitive, as it is based on an interpretative analysis of the most important parameter changes during the fine-tuning of each expert. The ability to successfully re-compose LLMs for cross-lingual transfer in this manner opens up future possibilities to combine model expertise, create modular solutions, and transfer reasoning capabilities across languages all post hoc.", "title_embedding_index": 3454, "title_abs_embedding_index": 3479}, {"title": "Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning", "link_suffix": "/forum?id=Pujt3ADZgI", "link": "https://openreview.net/forum?id=Pujt3ADZgI", "pdf_link": "https://openreview.net/pdf?id=Pujt3ADZgI", "keywords": "RLHF Theory, LLM Alignment", "abstract": "Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent\nRLHF approaches are reward-based, following the Bradley-Terry (BT) model assumption, which may not fully capture the complexity of human preferences. In\nthis paper, we explore RLHF under a general preference framework and approach\nit from a game-theoretic perspective. Specifically, we formulate the problem as\na two-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via no-\nregret learning, thereby approximating the Nash policy. Unlike previous methods,\nINPO bypasses the need for estimating the expected win rate for individual responses, which typically incurs high computational or annotation costs. Instead,\nwe introduce a new loss objective that is directly minimized over a preference\ndataset. We provide theoretical analysis for our approach and demonstrate its\neffectiveness through experiments on various representative benchmarks. With an\nLLaMA-3-8B-based SFT model, INPO achieves a 42.6% length-controlled win\nrate on AlpacaEval 2.0 and a 37.8% win rate on Arena-Hard, showing substantial\nimprovement over the state-of-the-art online RLHF algorithms.", "title_embedding_index": 3455, "title_abs_embedding_index": 3480}, {"title": "FlexTSF: A universal forecasting model for time series with variable regularities", "link_suffix": "/forum?id=LuLzcBsp5c", "link": "https://openreview.net/forum?id=LuLzcBsp5c", "pdf_link": "https://openreview.net/pdf?id=LuLzcBsp5c", "keywords": "Time series forecasting, Universal forecasting model, Irregular time series, Regular time series, Self-supervised pre-training, Zero-shot learning", "abstract": "Developing a foundation model for time series forecasting across diverse domains has attracted significant attention in recent years. Existing works typically assume regularly sampled, well-structured data, limiting their applicability to more generalized scenarios where time series often contain missing values, unequal sequence lengths, and irregular time intervals between measurements. To cover diverse domains and handle variable regularities, we propose FlexTSF, a universal time series forecasting model that possesses better generalization and natively support both regular and irregular time series. FlexTSF produces forecasts in an autoregressive manner and incorporates three novel designs: VT-Norm, a normalization strategy to ablate data domain barriers, IVP Patcher, a patching module to learn representations from flexibly structured time series, and LED attention, an attention mechanism seamlessly integrating these two and propagate forecasts with awareness of domain and time information, enabling effective time series forecasting across varying regularities. Experiments on $12$ datasets show that FlexTSF outperforms state-of-the-art forecasting models respectively designed for regular and irregular time series. Furthermore, after self-supervised pre-training, FlexTSF shows exceptional performance in both zero-shot and few-show settings for time series forecasting.", "title_embedding_index": 3456, "title_abs_embedding_index": 3481}, {"title": "An undetectable watermark for generative image models", "link_suffix": "/forum?id=jlhBFm7T2J", "link": "https://openreview.net/forum?id=jlhBFm7T2J", "pdf_link": "https://openreview.net/pdf?id=jlhBFm7T2J", "keywords": "Watermarking, AI Safety, Diffusion Models, Generative AI", "abstract": "We present the first undetectable watermarking scheme for generative image models.Undetectabilityensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries.\nIn particular, an undetectable watermark does not degrade image quality under any efficiently computable metric.\nOur scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness.\nWe experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1.\nOur experiments verify that, in contrast toevery prior schemewe tested, our watermark does not degrade image quality.\nOur experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images.\nFinally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Code is provided in the supplementary materials.", "title_embedding_index": 3457, "title_abs_embedding_index": 3482}, {"title": "Teaching LLMs How To Learn with Contextual Fine-Tuning", "link_suffix": "/forum?id=FS2nukC2jv", "link": "https://openreview.net/forum?id=FS2nukC2jv", "pdf_link": "https://openreview.net/pdf?id=FS2nukC2jv", "keywords": "Large Language Models", "abstract": "Prompting Large Language Models (LLMs), or providing context on the expected model of operation, is an effective way to steer the outputs of such models to satisfy human desiderata after they have been trained. But in rapidly evolving domains, there is often need to fine-tune LLMs to improve either the kind of knowledge in their memory or their abilities to perform open ended reasoning in new domains. When human's learn new concepts, we often do so by linking the new material that we are studying to concepts we have already learned before. To that end, we ask, \"can prompting help us teach LLMs how to learn\". In this work, we study a novel generalization of instruction tuning, called contextual fine-tuning, to fine-tune LLMs. Our method leverages instructional prompts designed to mimic human cognitive strategies in learning and problem-solving to guide the learning process during training, aiming to improve the model\u2019s interpretation and understanding of domain-specific knowledge. We empirically demonstrate that this simple yet effective modification improves the ability of LLMs to be fine-tuned rapidly on new datasets both within the medical and financial domains.", "title_embedding_index": 3458, "title_abs_embedding_index": 3483}, {"title": "Mamba-based Chemical Foundational Model for Fast Inference", "link_suffix": "/forum?id=7dsC1w4yzP", "link": "https://openreview.net/forum?id=7dsC1w4yzP", "pdf_link": "https://openreview.net/pdf?id=7dsC1w4yzP", "keywords": "Mamba, foundation model, molecular property prediction, classification, molecular reconstruction, synthesis yield prediction", "abstract": "We present a novel approach to chemical foundation models, leveraging structured state space sequence models (SSMs) to overcome the limitations of traditional Transformer-based architectures. While Transformers have achieved state-of-the-art results in chemical tasks such as property prediction and molecule generation, their self-attention mechanism is constrained by its inability to model data outside of a finite context window and its quadratic scaling with respect to window length. In contrast, SSMs offer a promising alternative for sequence modeling, enabling the capture of complex patterns and dependencies in molecular structures. Our Mamba architecture, a simplified end-to-end SSM-based neural network, eliminates the need for attention and MLP blocks, allowing for faster inference. We pre-train Mamba on a large, curated dataset of 91 million SMILES samples (equivalent to 4 billion molecular tokens) sourced from PubChem, and evaluate its performance on various benchmark datasets. Our experiments demonstrate the SSM's capacity to provide state-of-the-art results while maintaining fast inference, supporting complex tasks such as molecular property prediction, classification, molecular reconstruction, and synthesis yield prediction. This work advances the state-of-the-art in AI methodology in chemical sciences, offering a promising direction for future research in molecular modeling and discovery.", "title_embedding_index": 3459, "title_abs_embedding_index": 3484}, {"title": "Modeling dynamic social vision highlights gaps between deep learning and humans", "link_suffix": "/forum?id=wAXsx2MYgV", "link": "https://openreview.net/forum?id=wAXsx2MYgV", "pdf_link": "https://openreview.net/pdf?id=wAXsx2MYgV", "keywords": "NeuroAI, vision, fMRI, deep learning, social perception", "abstract": "Deep learning models trained on computer vision tasks are widely considered the most successful models of human vision to date. The majority of work that supports this idea evaluates how accurately these models predict behavior and brain responses to static images of objects and scenes. Real-world vision, however, is highly dynamic, and far less work has evaluated deep learning models on human responses to moving stimuli, especially those that involve more complicated, higher-order phenomena like social interactions. Here, we extend a dataset of natural videos depicting complex multi-agent interactions by collecting human-annotated sentence captions for each video, and we benchmark 350+ image, video, and language models on behavior and neural responses to the videos. As in prior work, we find that many vision models reach the noise ceiling in predicting visual scene features and responses along the ventral visual stream (often considered the primary neural substrate of object and scene recognition). In contrast, vision models poorly predict human action and social interaction ratings and neural responses in the lateral stream (a neural pathway theorized to specialize in dynamic, social vision), though video models show a striking advantage in predicting mid-level lateral stream regions. Language models (given human sentence captions of the videos) predict action and social ratings better than image and video models, but perform poorly at predicting neural responses in the lateral stream. Together, these results identify a major gap in AI's ability to match human social vision and provide insights to guide future model development for dynamic, natural contexts.", "title_embedding_index": 3460, "title_abs_embedding_index": 3485}, {"title": "Single Domain Generalization for Rare Event Detection in Medical Imaging", "link_suffix": "/forum?id=omM5m7mRy5", "link": "https://openreview.net/forum?id=omM5m7mRy5", "pdf_link": "https://openreview.net/pdf?id=omM5m7mRy5", "keywords": "Deep Learning, Knowledge, Rare Event Detection, Out-of-distribution detection", "abstract": "Single Domain Generalization (SDG) addresses the challenge of training a model on a single domain to ensure it generalizes well to unseen target domains. Although extensively studied in image classification, there is a lack of prior work on SDG for rare event or image classification in imbalanced dataset. In the medical diagnosis and disease detection domain, where data is often limited and events of interest are rare, deep learning (DL) models frequently exhibit suboptimal performance, leading to poor generalization across datasets. In multi-center studies, disparate data sources, differences in scanners and imaging protocols introduce domain shifts that exacerbate variability in rare event characteristics. This paper addresses this challenge by first leveraging a pre-trained large vision model to rank classes based on their similarity to the rare event class, allowing focused handling of the most similar class, and then integrates domain-invariant knowledge on rare event with DL to accurately classify the rare event class. By carefully incorporating expert knowledge with data-driven DL, our technique effectively regularizes the model, enhancing robustness and performance even with limited data availability. We present a case study on seizure onset zone detection using fMRI data, demonstrating that our approach significantly outperforms state-of-the-art vision transformers, large vision models, and knowledge-based systems, achieving an average F1 score of 90.2% while maintaining an overall F1 score of 85.0% across multi-center datasets.", "title_embedding_index": 3461, "title_abs_embedding_index": 3486}, {"title": "EgoQR: Efficient QR Code Reading in Egocentric Settings", "link_suffix": "/forum?id=AfZH9EEuRR", "link": "https://openreview.net/forum?id=AfZH9EEuRR", "pdf_link": "https://openreview.net/pdf?id=AfZH9EEuRR", "keywords": "QR Code Reading, Egocentric vision, Smart Wearable Devices, Resource-Constrained Computing, QR Code Detection, QR Code Decoding, Super Resolution", "abstract": "QR codes have become ubiquitous in daily life, enabling rapid information exchange. With the increasing adoption of smart wearable devices, there is a need for efficient, and friction-less QR code reading capabilities from Egocentric point-of-views. However, adapting existing phone-based QR code readers to egocentric images poses significant challenges.\nCode reading from egocentric images bring unique challenges such as wide field-of-view, code distortion and lack of visual feedback as compared to phones where users can adjust the position and framing. Furthermore, wearable devices impose constraints on resources like compute, power and memory.\nTo address these challenges, we present EgoQR, a novel system for reading QR codes from egocentric images, and is well suited for deployment on wearable devices. Our approach consists of two primary components: detection and decoding, designed to operate on high-resolution images on the device with minimal power consumption and added latency. The detection component efficiently locates potential QR codes within the image, while our enhanced decoding component extracts and interprets the encoded information. We incorporate innovative techniques to handle the specific challenges of egocentric imagery, such as varying perspectives, wider field of view, and motion blur.\nWe evaluate our approach on a dataset of egocentric images, demonstrating 34% improvement in reading the code compared to an existing state of the art QR code readers.", "title_embedding_index": 3462, "title_abs_embedding_index": 3487}, {"title": "Truthful Aggregation of LLMs with an Application to Online Advertising", "link_suffix": "/forum?id=yCEf1cJDGh", "link": "https://openreview.net/forum?id=yCEf1cJDGh", "pdf_link": "https://openreview.net/pdf?id=yCEf1cJDGh", "keywords": "mechanism design, llm, auction, online advertising", "abstract": "The next frontier of online advertising is revenue generation from LLM-generated content. We consider a setting where advertisers aim to influence the responses of an LLM to align with their interests, while platforms seek to maximize advertiser value and ensure user satisfaction. The challenge is that advertisers may misreport their preferences. To address this, we introduce MOSAIC, an auction mechanism that ensures that truthful reporting is a dominant strategy for advertisers, and which aligns each advertiser\u2019s utility with their contribution to social welfare. Importantly, the mechanism operates without LLM fine-tuning or access to model weights and provably converges to the output of the optimally fine-tuned LLM for the platform\u2019s objective as computational resources increase. Additionally, it can incorporate contextual information about the advertisers, accelerating convergence. Via experiments with a publicly available LLM, we show that MOSAIC significantly boosts advertiser value and platform revenue with low computational overhead. While our motivating application is online advertising, our mechanism can be applied in any setting with monetary transfers, making it a general-purpose solution for truthfully aggregating the preferences of self-interested agents over LLM-generated replies.", "title_embedding_index": 3463, "title_abs_embedding_index": 3488}, {"title": "Longitudinal Ensemble Integration for sequential classification with multimodal data", "link_suffix": "/forum?id=j7OAzA9DQd", "link": "https://openreview.net/forum?id=j7OAzA9DQd", "pdf_link": "https://openreview.net/pdf?id=j7OAzA9DQd", "keywords": "longitudinal multimodal data, sequential classification, deep learning, LSTM, heterogeneous ensembles, dementia diagnosis", "abstract": "Effectively modeling multimodal longitudinal data is a pressing need in various application areas, especially biomedicine. Despite this, few approaches exist in the literature for this problem, with most not adequately taking into account the multimodality of the data. In this study, we developed multiple configurations of a novel multimodal and longitudinal learning framework, Longitudinal Ensemble Integration (LEI), for sequential classification. We evaluated LEI\u2019s performance, and compared it against existing approaches, for the early detection of dementia, which is among the most studied multimodal sequential classification tasks. LEI outperformed these approaches due to its use of intermediate base predictions arising from the individual data modalities, which enabled their better integration over time. LEI\u2019s design also enabled the identification of features that were consistently important across time for the effective prediction of dementia-related diagnoses. Overall, our work demonstrates the potential of LEI for sequential classification from longitudinal multimodal data.", "title_embedding_index": 3464, "title_abs_embedding_index": 3489}, {"title": "Sample Efficient Multiple-policy Evaluation in Reinforcement Learning", "link_suffix": "/forum?id=GrRo9uV3OH", "link": "https://openreview.net/forum?id=GrRo9uV3OH", "pdf_link": "https://openreview.net/pdf?id=GrRo9uV3OH", "keywords": "Theory, Reinforcement learning, Policy evaluation, Sample complexity", "abstract": "We study the multiple-policy evaluation problem where we are given a set of $K$ policies and the goal is to evaluate their performance (expected total reward over a fixed horizon) to an accuracy $\\epsilon$ with probability at least $1-\\delta$. We propose a sample-efficient algorithm named \\CAESAR for this problem. Our approach is based on computing an approximate optimal offline sampling distribution and using the data sampled from it to perform the simultaneous estimation of the policy values. \\CAESAR has two phases. In the first we produce coarse estimates of the visitation distributions of the target policies at a low order sample complexity rate that scales with $\\tilde{O}(\\frac{1}{\\epsilon})$. In the second phase, we approximate the optimal offline sampling distribution and compute the importance weighting ratios for all target policies by minimizing a step-wise quadratic loss function inspired by the DualDICE \\citep{nachum2019dualdice} objective. Up to low order and logarithmic terms \\CAESAR achieves a sample complexity $\\tilde{O}\\left(\\frac{H^4}{\\epsilon^2}\\sum_{h=1}^H\\min_{\\mu_h}\\max_{k\\in[K]}\\sum_{s,a}\\frac{(d_h^{\\pi^k}(s,a))^2}{\\mu_h(s,a)}\\right)$, where $d^{\\pi}$ is the visitation distribution of policy $\\pi$, $\\mu$ is the sampling distribution, and $H$ is the horizon.", "title_embedding_index": 3465, "title_abs_embedding_index": 3490}, {"title": "Learning Extrapolative Sequence Transformations from Markov Chains", "link_suffix": "/forum?id=DQfHkEcUqV", "link": "https://openreview.net/forum?id=DQfHkEcUqV", "pdf_link": "https://openreview.net/pdf?id=DQfHkEcUqV", "keywords": "Large language model, Markov chain Monte Carlo", "abstract": "Generative sequence-level models are appealing in settings where the desired outputs must adhere to global constraints. In these settings,  autoregressive sampling can struggle to explore the solution space sufficiently to find the optimal solution, especially when optimal solutions involve extrapolating beyond the training data. However, searching the solution space through approximate inference methods such as Markov chain Monte Carlo (MCMC) is computationally expensive. To address this computational burden, we propose to train a separate inference network based on selected states from Markov chains. The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization. We find that the learned inference network confers many of the same generalization benefits as the slow sampling process, but with the additional benefit of high sample efficiency. This is particularly true in cases where the model must extrapolate beyond the range of values seen in the training data, but our approach demonstrates success even on the anonymization task, which relies solely on interpolation. Finally, we analyze the effects of various strategies to select states from the search space.", "title_embedding_index": 3466, "title_abs_embedding_index": 3491}, {"title": "Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs", "link_suffix": "/forum?id=Qja5s0K3VX", "link": "https://openreview.net/forum?id=Qja5s0K3VX", "pdf_link": "https://openreview.net/pdf?id=Qja5s0K3VX", "keywords": "Partially Observable Markov Decision Process; Offline Policy Evaluation; Reinforcement Learning Theory", "abstract": "We investigate off-policy evaluation (OPE), a central and fundamental problem\nin reinforcement learning (RL), in the challenging setting of Partially Observable\nMarkov Decision Processes (POMDPs) with large observation spaces. Recent\nworks of Uehara et al. (2023a); Zhang & Jiang (2024) developed a model-free\nframework and identified important coverage assumptions (called belief and outcome coverage) that enable accurate OPE of memoryless policies with polynomial sample complexities, but handling more general target policies that depend on\nthe entire observable history remained an open problem. In this work, we prove\ninformation-theoretic hardness for model-free OPE of history-dependent policies in\nseveral settings, characterized by additional assumptions imposed on the behavior\npolicy (memoryless vs. history-dependent) and/or the state-revealing property of\nthe POMDP (single-step vs. multi-step revealing). We further show that some hardness can be circumvented by a natural model-based algorithm\u2014whose analysis has surprisingly eluded the literature despite the algorithm\u2019s simplicity\u2014demonstrating\nprovable separation between model-free and model-based OPE in POMDPs.", "title_embedding_index": 3467, "title_abs_embedding_index": 3492}, {"title": "Large Language Models can be Strong Self-Detoxifiers", "link_suffix": "/forum?id=jY5oml9fe9", "link": "https://openreview.net/forum?id=jY5oml9fe9", "pdf_link": "https://openreview.net/pdf?id=jY5oml9fe9", "keywords": "detoxification; LLM", "abstract": "Reducing the likelihood of generating harmful and toxic output is an essential task when aligning large language models (LLMs). Existing methods mainly rely on training an external reward model (i.e., another language model) or fine-tuning the LLM using self-generated data to influence the outcome. In this paper, we show that LLMs have the capability of self-detoxification without the use of an additional reward model or re-training. We propose \\textit{Self-disciplined Autoregressive Sampling (SASA)}, a lightweight controlled decoding algorithm for toxicity reduction of LLMs. SASA leverages the contextual representations from an LLM to learn linear subspaces characterizing toxic v.s. non-toxic output in analytical forms. When auto-completing a response token-by-token, SASA dynamically tracks the margin of the current output to steer the generation away from the toxic subspace, by adjusting the autoregressive sampling strategy. Evaluated on LLMs of different scale and nature, namely Llama-3.1-Instruct (8B), Llama-2 (7B), and GPT2-L models with the RealToxicityPrompts, BOLD, and AttaQ benchmarks, SASA markedly enhances the quality of the generated sentences relative to the original models and attains comparable performance to state-of-the-art detoxification techniques, significantly reducing the toxicity level by only using the LLM's internal representations.", "title_embedding_index": 3468, "title_abs_embedding_index": 3493}, {"title": "STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning", "link_suffix": "/forum?id=4VHiptx7xe", "link": "https://openreview.net/forum?id=4VHiptx7xe", "pdf_link": "https://openreview.net/pdf?id=4VHiptx7xe", "keywords": "dynamic time warping, few-shot imitation learning, retrieval, foundation models", "abstract": "Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a multi-task, generalist policy by training broadly across them. Notably, while these generalist policies can improve the average performance across many tasks, the performance of generalist policies on any one task is often suboptimal due to negative transfer between partitions of the data, compared to task-specific specialist policies. In this work, we argue for the paradigm of training policies during deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant data at test time.  Furthermore, we show that many robotics tasks share considerable amounts of low-level behaviors and that retrieval at the \"sub\"-trajectory granularity enables significantly improved data utilization, generalization, and robustness in adapting policies to novel problems. In contrast, existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared cross-task content. This work proposes STRAP, a technique for leveraging pre-trained vision foundation models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in a robust fashion. STRAP outperforms both prior retrieval algorithms and multi-task learning methods in simulated and real experiments, showing the ability to scale to much larger offline datasets in the real world as well as the ability to learn robust control policies with just a handful of real-world demonstrations.", "title_embedding_index": 3469, "title_abs_embedding_index": 3494}, {"title": "A Theoretical Study of Neural Network Expressive Power via Manifold Topology", "link_suffix": "/forum?id=L7gyAKWpiM", "link": "https://openreview.net/forum?id=L7gyAKWpiM", "pdf_link": "https://openreview.net/pdf?id=L7gyAKWpiM", "keywords": "Topology, Manifold, Homology", "abstract": "A prevalent assumption regarding real-world data is that it lies on or close to a low-dimensional manifold. When deploying a neural network on data manifolds, the required size, i.e., the number of neurons of the network, heavily depends on the intricacy of the underlying latent manifold. While significant advancements have been made in understanding the geometric attributes of manifolds, it's essential to recognize that topology, too, is a fundamental characteristic of manifolds. In this study, we investigate network expressive power in terms of the latent data manifold. Integrating both topological and geometric facets of the data manifold, we present a size upper bound of ReLU neural networks.", "title_embedding_index": 3470, "title_abs_embedding_index": 3495}, {"title": "Evaluating Model Robustness Against Unforeseen Adversarial Attacks", "link_suffix": "/forum?id=PsaRfjbfnv", "link": "https://openreview.net/forum?id=PsaRfjbfnv", "pdf_link": "https://openreview.net/pdf?id=PsaRfjbfnv", "keywords": "AI Safety, ML safety, adversarial robustness, distribution shift, unforeseen adversaries", "abstract": "When considering real-world adversarial settings, defenders are unlikely to have access to the full range of deployment-time adversaries, and adversaries are likely to use realistic adversarial distortions that will not be limited to small $L_p$-constrained perturbations. To narrow in on this discrepancy between research and reality we introduce ImageNet-UA, a new benchmark for evaluating model robustness against a wide range of unforeseen adversaries. We make use of our benchmark to identify holes in current popular adversarial defense techniques, highlighting a rich space of techniques which can improve unforeseen robustness. We hope the greater variety and realism of ImageNet-UA will make it a useful tool for those working on real-world worst-case robustness, enabling development of more robust defenses which can generalize beyond attacks seen during training.", "title_embedding_index": 3471, "title_abs_embedding_index": 3496}, {"title": "Benchmarking the Fidelity and Utility of Synthetic Relational Data", "link_suffix": "/forum?id=PUXy7vQ5M3", "link": "https://openreview.net/forum?id=PUXy7vQ5M3", "pdf_link": "https://openreview.net/pdf?id=PUXy7vQ5M3", "keywords": "data generation, literature review, empirical comparison, discriminative detection, data quality", "abstract": "Synthesizing relational data has started to receive more attention from researchers, practitioners, and industry. The task is more difficult than synthesizing a single table due to the added complexity of relationships between tables. For the same reason, benchmarking methods for synthesizing relational data introduces new challenges. Our work is motivated by a lack of an empirical evaluation of state-of-the-art methods and by gaps in the understanding of how such an evaluation should be done. We review related work on relational data synthesis, common benchmarking datasets, and approaches to measuring the fidelity and utility of synthetic data. We combine the best practices and a novel robust detection approach into a benchmarking tool and use it to compare six methods, including two commercial tools. While some methods are better than others, no method is able to synthesize a dataset that is indistinguishable from original data. For utility, we typically observe moderate correlation between real and synthetic data for both model predictive performance and feature importance.", "title_embedding_index": 3472, "title_abs_embedding_index": 3497}, {"title": "Can LLMs Enhance Performance Prediction for Deep Learning Models?", "link_suffix": "/forum?id=Txxz9fBPcJ", "link": "https://openreview.net/forum?id=Txxz9fBPcJ", "pdf_link": "https://openreview.net/pdf?id=Txxz9fBPcJ", "keywords": "Graph Neural Networks, Graph Tokens, Large Language Models", "abstract": "Accurate performance prediction of Deep Learning (DL) models is essential for efficient resource allocation and optimizations in various stages of the DL system stack. While existing approaches can achieve high prediction accuracy, they lack ability to quickly adapt to new hardware environments or emerging workloads. \nThis paper leverages both Graph Neural Networks (GNNs) and Large Language Models (LLMs) to enhance the accuracy and adaptability of DL performance prediction. Our intuition is that GNNs are adept at capturing the structural information of DL models, naturally represented as graphs, while LLMs provide generalization and the ability to quickly adapt to various tasks thanks to extensive pre-training data.\nWe empirically demonstrate that using GNN-derived graph embeddings as inputs to an LLM outperforms traditional representations, including high-level text summary and lossless semi-structured text (e.g., JSON), for this task. Furthermore, we propose a structured pre-training strategy to enable model adaptation to new hardware environments, significantly reducing the need for extensive retraining. Our experiments validate the effectiveness of this approach, showing an 8.8 percentage-point improvement in accuracy over a state-of-the-art GNN baseline. Notably, when adapted to new hardware with few samples, our method achieves a remarkable 30--70 percentage-point increase in accuracy compared to the GNN baseline.", "title_embedding_index": 3473, "title_abs_embedding_index": 3498}, {"title": "PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches", "link_suffix": "/forum?id=gyHoR6uFhU", "link": "https://openreview.net/forum?id=gyHoR6uFhU", "pdf_link": "https://openreview.net/pdf?id=gyHoR6uFhU", "keywords": "Large Language Models, NLP, Efficiency, Fine-tuning, Efficient Fine-tuning, Portability", "abstract": "As large language models (LLMs) increasingly shape the AI landscape, fine-tuning pretrained models has become more popular than in the pre-LLM erafor achieving optimal performance in domain-specific tasks. However, pretrained LLMs such as ChatGPT are periodically evolved (i.e., model parameters are frequently updated), making it challenging for downstream users with limited resources to keep up with fine-tuning the newest LLMs for their domain application. Even though fine-tuning costs have nowadays been reduced thanks to the innovations of parameter-efficient fine-tuning such as LoRA, not all downstream users have adequate computing for frequent personalization. Moreover, access to fine-tuning datasets, particularly in sensitive domains such as healthcare, could be time-restrictive, making it crucial to retain the knowledge encoded in earlier fine-tuned rounds for future adaptation. In this paper, we present PORTLLM, a training-free framework that (i) creates an initial lightweight model update patch to capture domain-specific knowledge, and (ii) allows a subsequent seamless plugging for the continual personalization of evolved LLM at minimal cost. Our extensive experiments cover seven representative datasets, from easier question-answering tasks {BoolQ, SST2} to harder reasoning tasks {WinoGrande, GSM8K}, and models including {Mistral-7B,Llama2, Llama3.1, and Gemma2}, validating the portability of our designed model patches and showcasing the effectiveness of our proposed framework. For instance, PORTLLM achieves comparable performance to LoRA fine-tuning with reductions of up to 12.2\u00d7 in GPU memory usage. Finally, we provide theoretical justifications to understand the portability of our model update patches, which offers new insights into the theoretical dimension of LLMs\u2019 personalization.", "title_embedding_index": 3474, "title_abs_embedding_index": 3499}]