[{"title": "Risk Quadrangle and Robust Optimization Based on\u03c6-Divergence", "link_suffix": "/forum?id=7BDUTI6aS7", "link": "https://openreview.net/forum?id=7BDUTI6aS7", "pdf_link": "https://openreview.net/pdf?id=7BDUTI6aS7", "keywords": "robust optimization, distributionally robust optimization, convex optimization, regression, classification, risk quadrangle, risk measure, $\\varphi$-divergence", "abstract": "The Fundamental Risk Quadrangle (FRQ) is a unified framework linking risk management, statistical estimation, and optimization. Distributionally robust optimization (DRO) based on $\\varphi$-divergence minimizes the maximal expected loss, where the maximum is over a $\\varphi$-divergence uncertainty set. This paper introduces the \\emph{extended} $\\varphi$-divergence and the extended $\\varphi$-divergence quadrangle, which integrates DRO into the FRQ framework. We derive the primal and dual representations of the quadrangle elements (risk, deviation, regret, error, and statistic). The dual representation provides an interpretation of classification, portfolio optimization, and regression as robust optimization based on the extended $\\varphi$-divergence. The primal representation offers tractable formulations of these robust optimizations as convex optimization. We provide illustrative examples showing that many common problems, such as least-squares regression, quantile regression, support vector machines, and CVaR optimization, fall within this framework. Additionally, we conduct a case study to visualize the optimal solution of the inner maximization in robust optimization.", "title_embedding_index": 950, "title_abs_embedding_index": 975}, {"title": "Projected Neural Differential Equations for Learning Constrained Dynamics", "link_suffix": "/forum?id=2AWZTv6kgV", "link": "https://openreview.net/forum?id=2AWZTv6kgV", "pdf_link": "https://openreview.net/pdf?id=2AWZTv6kgV", "keywords": "neural differential equations, neural ordinary differential equations, constraints, dynamics, scientific machine learning, ai for science", "abstract": "Neural differential equations offer a powerful approach for learning dynamics from data.\n  However, they do not impose known constraints that should be obeyed by the learned model.\n  It is well-known that enforcing constraints in surrogate models can enhance their generalizability and numerical stability.\n  In this paper, we introduce projected neural differential equations (PNDEs), a new method for constraining neural differential equations based on projection of the learned vector field to the tangent space of the constraint manifold.\n  In tests on several challenging examples, including chaotic dynamical systems and state-of-the-art power grid models, PNDEs outperform existing methods while requiring fewer hyperparameters.\n  The proposed approach demonstrates significant potential for enhancing the modeling of constrained dynamical systems, particularly in complex domains where accuracy and reliability are essential.", "title_embedding_index": 951, "title_abs_embedding_index": 976}, {"title": "Hierarchical Object-Oriented POMDP Planning for Object Rearrangement", "link_suffix": "/forum?id=BgcapX9ers", "link": "https://openreview.net/forum?id=BgcapX9ers", "pdf_link": "https://openreview.net/pdf?id=BgcapX9ers", "keywords": "rearrangement, POMDP, planning, reinforcement learning, object search", "abstract": "We present an online planning framework for solving multi-object rearrangement problems in partially observable, multi-room environments. Current object rearrangement solutions, primarily based on Reinforcement Learning or hand-coded planning methods, often lack adaptability to diverse challenges. To address this limitation, we introduce a novel Hierarchical Object-Oriented Partially Observed Markov Decision Process (HOO-POMDP) planning approach. This approach comprises of (a) an object-oriented POMDP planner generating sub-goals, (b) a set of low-level policies for sub-goal achievement, and (c) an abstraction system converting the continuous low-level world into a representation suitable for abstract planning. We evaluate our system on varying numbers of objects, rooms, and problem types in AI2-THOR simulated environments with promising results.", "title_embedding_index": 952, "title_abs_embedding_index": 977}, {"title": "FLARE: Fine-tuned Long-context Acceleration with ReLU-enhanced FIRE", "link_suffix": "/forum?id=LlE61BEYpB", "link": "https://openreview.net/forum?id=LlE61BEYpB", "pdf_link": "https://openreview.net/pdf?id=LlE61BEYpB", "keywords": "FIRE, Functional Interpolation for Relative Position Encoding, fine-tune, fine-tuning, ReLU, Softmax, Softplus, Softmax alternatives, long context, transformer, large language model, edge device, Flash Attention", "abstract": "Deploying transformer models on battery-constrained edge devices is challenging, which becomes particularly problematic for handling long-context LLM applications. The limited computational resources of edge devices, especially those using specialized custom accelerators, often make it infeasible to utilize latency-optimization techniques for Softmax, such as FlashAttention, leading to significant performance bottlenecks. Due to limited memory bandwidth, reduced parallelism, and hardware constraints, edge devices struggle to efficiently implement these techniques, which are designed to hide the computational delays associated with Softmax. As a result, the Softmax operation becomes a critical bottleneck for many edge accelerator implementations involving long contexts. Moreover, techniques aimed at improving model efficiency often require pre-training models from scratch to fully utilize these optimizations or else lead to accuracy losses when substituting more hardware-efficient approximations. In this paper, we explore direct fine-tuning of long-context efficient techniques that can support lightweight models and show that similar perplexities can be reached to those of the original models on language modeling datasets. We fine-tune the element-wise ReLU function in place of Softmax, remove absolute position encodings, and substitute and fine-tune in Functional Interpolation for Relative Position Encoding (FIRE). We show that fine-tuning this combination into a pre-trained LLM model yields a compounding effect: simultaneously increasing the max context length and inference efficiency for longer contexts. We illustrate this in hardware with performance, power, and area (PPA) analysis, showing that ReLU has 8 times higher frequency, uses 0.1% of power, and 0.11% energy-per-cycle compared to Softmax. We propose to fine-tune long-context with ReLU-enhanced FIRE (FLARE), demonstrating how FIRE and ReLU can be fused into an equivalent algorithm that allows bypassing, on average, 98.9% of FIRE\u2019s original operations during inference. Finally, we show how this technique can extend inference speed and efficiency gains to training, contributing a ReLU-augmented FlashAttention forward-pass implementation in CUDA, which demonstrates a 3.8x speedup over conventional Softmax-based FlashAttention for 4096 context length and greater savings projected for larger contexts.", "title_embedding_index": 953, "title_abs_embedding_index": 978}, {"title": "Diverse Preference Learning for Capabilities and Alignment", "link_suffix": "/forum?id=pOq9vDIYev", "link": "https://openreview.net/forum?id=pOq9vDIYev", "pdf_link": "https://openreview.net/pdf?id=pOq9vDIYev", "keywords": "alignment, diversity, natural language processing, reinforcement learning", "abstract": "As LLMs increasingly impact society, their ability to represent diverse perspectives is critical.  However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to overweight majority opinions and sacrifice diversity in exchange for optimal reward. To address this, we propose Diverse Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty \u2014 allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Diverse Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Diverse Preference Learning resembles, but is a Pareto improvement over standard temperature scaling.", "title_embedding_index": 954, "title_abs_embedding_index": 979}, {"title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval", "link_suffix": "/forum?id=uMLeOlzlZ2", "link": "https://openreview.net/forum?id=uMLeOlzlZ2", "pdf_link": "https://openreview.net/pdf?id=uMLeOlzlZ2", "keywords": "information retrieval, language model, database, materials informatics", "abstract": "Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and often biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data from the Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool-usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP\u2019s capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning interatomic potentials. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics and paves the way for knowledge distillation and fine-tuning of future language models.", "title_embedding_index": 955, "title_abs_embedding_index": 980}, {"title": "Beyond Layers: A Global Message-Passing Mechanism for Heterophilic Graphs", "link_suffix": "/forum?id=W4q7cwRCwg", "link": "https://openreview.net/forum?id=W4q7cwRCwg", "pdf_link": "https://openreview.net/pdf?id=W4q7cwRCwg", "keywords": "Message Passing Mechanism; Graph Neural Network; Heterophilic Graph", "abstract": "The effectiveness of most graph neural networks is largely attributed to the message-passing mechanism.\nDespite the significant success in homophilic graphs (i.e., similar nodes are connected by edges), message-passing mechanism in heterophilic graphs (i.e., dissimilar nodes are connected by edges) is still challenging.\nDue to the existence of low-order but dissimilar neighbor nodes in a path, messages from similar but high-order neighbor nodes are often weakened. \nIn this paper, firstly, we conduct both theoretical and empirical analysis of the layer-by-layer local nature of the message-passing mechanism.\nThen, we propose a novel GloMP-GNN for heterophilic graphs by comprehensively introducing global insights into the message-passing mechanism.1) During the message propagation phase, the global insight is introduced from the perspective of graph structure. \nWe design a structure-based global propagation strategy, where messages can be effectively propagated with the bridge of virtual edges between a global virtual node and graph nodes.\nMoreover, a global edge adaption approach is included to aggregate messages with adaptive edge weight adjustment.\n2) During the feature updating phase, the global insight is introduced with a feature-augmented compensatory updating method.\nThrough a multi-view feature updating mechanism, the node feature representation can be effectively augmented by compensating the weakened message from different views.\nFinally, we conduct extensive experimental evaluations on eight datasets, which demonstrate the superiority of our proposed GloMP-GNN. As broader impacts, GloMP-GNN consistently performs well across multiple layers and also effectively prevents the over-smoothing problem.\nCodes are available on Github withhttps://github.com/Anonymous-GloMP-GNN/GloMP-GNN.", "title_embedding_index": 956, "title_abs_embedding_index": 981}, {"title": "Deferred Backdoor Functionality Attacks on Deep Learning Models", "link_suffix": "/forum?id=S5JCqTJyKj", "link": "https://openreview.net/forum?id=S5JCqTJyKj", "pdf_link": "https://openreview.net/pdf?id=S5JCqTJyKj", "keywords": "AI security, Backdoor Attack, Stealthy Attack", "abstract": "Deep learning models are vulnerable to backdoor attacks, where adversaries inject malicious functionality during training that activates on trigger inputs at inference time. Extensive research has focused on developing stealthy backdoor attacks to evade detection and defense mechanisms. \nHowever, these approaches still have limitations that leave the door open for detection and mitigation due to their inherent design to cause malicious behavior in the presence of a trigger.\nTo address this limitation, we introduce Defferred Activated Backdoor Functionality (DABF), a new paradigm in backdoor attacks. Unlike conventional attacks, DABF initially conceals its backdoor, producing benign outputs even when triggered. This stealthy behavior allows DABF to bypass multiple detection and defense methods, remaining undetected during initial inspections.\nThe backdoor functionality is strategically activated only after the model undergoes subsequent updates, such as retraining on benign data. DABF attacks exploit the common practice in the life cycle of machine learning models to perform model updates and fine-tuning after initial deployment. To implement DABF attacks, we approach the problem by making the unlearning of the backdoor fragile, allowing it to be easily cancelled and subsequently reactivate the backdoor functionality. To achieve this, we propose a novel two-stage training scheme, called $\\texttt{DeferBad}$. Our extensive experiments across various fine-tuning scenarios, backdoor attack types, datasets, and model architectures demonstrate the effectiveness and stealthiness of $\\texttt{DeferBad}$.", "title_embedding_index": 957, "title_abs_embedding_index": 982}, {"title": "Efficient Diffusion Posterior Sampling for Dose Reduced CT Reconstruction", "link_suffix": "/forum?id=aZVRFIDhYL", "link": "https://openreview.net/forum?id=aZVRFIDhYL", "pdf_link": "https://openreview.net/pdf?id=aZVRFIDhYL", "keywords": "CT reconstruction, Diffusion model, Posterior sampling", "abstract": "The clinical efficacy of Computed Tomography (CT) is well-established, yet concerns regarding its radiation exposure persist. To mitigate this risk, a reduction in X-ray photon count or projection views is typically pursued, albeit at the expense of image quality. In this study, we introduce an innovative diffusion posterior sampling approach for CT image reconstruction at reduced radiation doses. This method initiates with a predictive step, leveraging data enhancement on the posterior approximation derived from a pre-trained diffusion model and the measurement data. Subsequently, a forward sampling phase ensues, which maps the output to a noisy timestep, followed by a diffusion estimation process. Additionally, we propose an acceleration strategy that employs superior initialization to significantly curtail the sampling steps required. Our experimental findings indicate that this method not only enhances the quality of reconstructed images by an average of 3.5 db but also accelerates the process to over ten times faster than existing diffusion-based techniques. These outcomes underscore the method's potential in clinical settings.", "title_embedding_index": 958, "title_abs_embedding_index": 983}, {"title": "Large Language Model Evaluation via Matrix Nuclear-Norm", "link_suffix": "/forum?id=PDnM7mSO7M", "link": "https://openreview.net/forum?id=PDnM7mSO7M", "pdf_link": "https://openreview.net/pdf?id=PDnM7mSO7M", "keywords": "Large language model, evaluation, matrix entropy, nuclear norm", "abstract": "As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their $( O(n^3) )$ time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the $( L_{1,2}\\text{-norm} )$ to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to $( O(n^2) )$ and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency.", "title_embedding_index": 959, "title_abs_embedding_index": 984}, {"title": "Efficient Reinforcement Learning with Large Language Model Priors", "link_suffix": "/forum?id=e2NRNQ0sZe", "link": "https://openreview.net/forum?id=e2NRNQ0sZe", "pdf_link": "https://openreview.net/pdf?id=e2NRNQ0sZe", "keywords": "Reinforcement Learning; Probabilistic Inference; Language Prior;", "abstract": "In sequential decision-making (SDM) tasks, methods like reinforcement learning (RL) and heuristic search have made notable advances in specific cases. However, they often require extensive exploration and face challenges in generalizing across diverse environments due to their limited grasp of the underlying decision dynamics. In contrast, large language models (LLMs) have recently emerged as powerful general-purpose tools, due to their capacity to maintain vast amounts of domain-specific knowledge. To harness this rich prior knowledge for efficiently solving complex SDM tasks, we propose treating LLMs as prior action distributions and integrating them into RL frameworks through Bayesian inference methods, making use of variational inference and direct posterior sampling. The proposed approaches facilitate the seamless incorporation of fixed LLM priors into both policy-based and value-based RL frameworks. Our experiments show that incorporating LLM-based action priors significantly reduces exploration and optimization complexity, substantially improving sample efficiency compared to traditional RL techniques, e.g., using LLM priors decreases the number of required samples by over 90% in offline learning scenarios.", "title_embedding_index": 960, "title_abs_embedding_index": 985}, {"title": "Benchmarking DNA Sequence Models for Causal Variant Prediction in Human Genetics", "link_suffix": "/forum?id=GlLXMjW7oF", "link": "https://openreview.net/forum?id=GlLXMjW7oF", "pdf_link": "https://openreview.net/pdf?id=GlLXMjW7oF", "keywords": "genomics, genetics, variant effect prediction, DNA, language models", "abstract": "Machine learning holds immense promise in biology, particularly for the challenging task of identifying causal variants for Mendelian and complex traits.  Two primary approaches have emerged for this task: supervised sequence-to-function models trained on functional genomics experimental data and self-supervised DNA language models that learn evolutionary constraints on sequences.  However, the field currently lacks consistently curated datasets with accurate labels, especially for non-coding variants, that are necessary to comprehensively benchmark these models and advance the field.  In this work, we present TraitGym, a curated dataset of genetic variants that are either known to be causal or are strong candidates across 113 Mendelian and 83 complex traits, along with carefully constructed control variants.  We frame the causal variant prediction task as a binary classification problem and benchmark various models, including functional-genomics-supervised models, self-supervised models, models that combine machine learning predictions with curated annotation features, and ensembles of these.  Our results provide insights into the capabilities and limitations of different approaches for predicting the functional consequences of genetic variants.  We find that alignment-based models CADD and GPN-MSA compare favorably for Mendelian traits and complex disease traits, while functional-genomics-supervised models Enformer and Borzoi perform better for complex non-disease traits.  All curated benchmark data, together with training and benchmarking scripts, will be made publicly available upon publication.", "title_embedding_index": 961, "title_abs_embedding_index": 986}, {"title": "Uncertainty-Based Abstention in LLMs Improves Safety and Reduces Hallucinations", "link_suffix": "/forum?id=1DIdt2YOPw", "link": "https://openreview.net/forum?id=1DIdt2YOPw", "pdf_link": "https://openreview.net/pdf?id=1DIdt2YOPw", "keywords": "LLMs, uncertainty, abstention, correctness, hallucinations, safety", "abstract": "A major barrier to the practical deployment of large language models (LLMs) is their lack of reliability. Three situations where this is particularly apparent are correctness, hallucinations when given unanswerable questions, and safety where responses are harmful or offensive. In all three cases, models should ideally abstain from responding---much like humans refrain from answering questions when uncertain. Inspired by analogous approaches in classification, this study explores the feasibility and efficacy of LLMs abstaining when uncertain in the domain of question-answering. We investigate two kinds of uncertainties, statistical uncertainty metrics and a distinct verbalized measure, termed as In Dialogue Uncertainty (InDU), measuring hedge words such as `I don't know' in responses. Using these uncertainty measures combined with models with and without reinforcement learning with human feedback (RLHF), we show in all three situations, abstention based on the right kind of uncertainty measure can boost the reliability of LLMs. By abstaining for a few highly uncertain samples we improve correctness by up to 8%, avoid 50% of hallucinations by correctly identifying unanswerable questions, and in particular increase safety by 70-99% with almost no additional computational overhead.", "title_embedding_index": 962, "title_abs_embedding_index": 987}, {"title": "On last-iterate convergence of distributed Stochastic Gradient Descent algorithm with momentum", "link_suffix": "/forum?id=BiymAD5ETK", "link": "https://openreview.net/forum?id=BiymAD5ETK", "pdf_link": "https://openreview.net/pdf?id=BiymAD5ETK", "keywords": "stochastic optimization, convergence analyse, distributed, momentum", "abstract": "Distributed Stochastic Gradient optimization algorithms are studied extensively to address challenges in centralized approaches, such as data privacy, communication load, and computational efficiency, especially when dealing with large datasets. However, convergence theory research for these algorithms has been limited, particularly for distributed momentum-based SGD (mSGD) algorithms.\nCurrent theoretical work on distributed mSGD algorithms primarily focuses on establishing time-average convergence theory, whereas last-iterate convergence\u2014considered a stronger and more practical definition than time-average convergence\u2014has yet to be thoroughly explored. In this paper, we aim to establish the last-iterate convergence theory for a class of distributed mSGD algorithms with a decaying learning rate. First, we propose a general framework for distributed mSGD algorithms. Within this framework and under general conditions, we have proven the last-iterate convergence of the gradient of the loss function for a class of distributed mSGD algorithms. \nFurthermore, we have estimated the corresponding last-iterate convergence rate under supplementary conditions.  Moreover, we theoretically prove that in the early stage, the adding of a momentum term can make the iterations converge more rapidly to a neighborhood of the stationary point. Some experiments are provided to illustrate the theoretical findings.", "title_embedding_index": 963, "title_abs_embedding_index": 988}, {"title": "Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits", "link_suffix": "/forum?id=Oeb0I3JcVc", "link": "https://openreview.net/forum?id=Oeb0I3JcVc", "pdf_link": "https://openreview.net/pdf?id=Oeb0I3JcVc", "keywords": "Linear bandit, Thompson sampling, Greedy, Data-driven exploration", "abstract": "This paper is motivated by recent research in the $d$-dimensional stochastic linear bandit literature, which has revealed an unsettling discrepancy: algorithms like Thompson sampling and Greedy demonstrate promising empirical performance, yet this contrasts with their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometric properties of the uncertainty ellipsoid around the main problem parameter. This methodology enables us to formulate an instance-dependent frequentist regret bound, which incorporates the geometric information, for a broad class of base algorithms, including Greedy, OFUL, and Thompson sampling. This result allows us to identify and ``course-correct\" problem instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ for a $T$-period decision-making scenario, effectively maintaining the desirable attributes of the base algorithms, including their empirical efficacy. We present simulation results to validate our findings using synthetic and real data.", "title_embedding_index": 964, "title_abs_embedding_index": 989}, {"title": "Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning", "link_suffix": "/forum?id=FvQsk3la17", "link": "https://openreview.net/forum?id=FvQsk3la17", "pdf_link": "https://openreview.net/pdf?id=FvQsk3la17", "keywords": "Actor-Critic, Exploration, Reinforcement Learning, Thompson Sampling, Langevin Monte Carlo, Deep Reinforcement learning, Continuous Control", "abstract": "Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them. Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a novel model-free RL algorithm, \\emph{Langevin Soft Actor Critic} (LSAC), which prioritizes enhancing critic learning through uncertainty estimation over policy optimization. LSAC employs three key innovations: approximate Thompson sampling through distributional Langevin $Q$ updates, parallel tempering for exploring multiple modes of the posterior of the $Q$ function and diffusion synthesized state-action samples regularized with $Q$ action gradients. Our extensive experiments demonstrate that LSAC outperforms or matches the performance of mainstream model-free RL algorithms for continuous control tasks.\nNotably, LSAC marks the first successful application of a Langevin Monte Carlo (LMC) based Thompson sampling in continuous control tasks with continuous action spaces, setting a new benchmark for future research in the field.", "title_embedding_index": 965, "title_abs_embedding_index": 990}, {"title": "FGW-CLIP: Enhancing Enzyme Screening via Fused Gromov-Wasserstein Contrastive Learning", "link_suffix": "/forum?id=JUu0tsd0Zk", "link": "https://openreview.net/forum?id=JUu0tsd0Zk", "pdf_link": "https://openreview.net/pdf?id=JUu0tsd0Zk", "keywords": "Contrastive Learning, Enzyme Screening, 3D Structure, Hierarchical Classifications", "abstract": "Enzymes are crucial catalysts for biochemical reactions, underpinning numerous biological processes. The efficient identification of specific enzymes from extensive protein libraries is essential for understanding and harnessing these biological reactions. While traditional computational methods for enzyme screening are time-consuming and resource-intensive, recent contrastive learning approaches have shown promise. However, these methods often overlook the inherent hierarchical classifications within enzymes and reactions, as well as the significance of molecular structure in catalysis. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. This approach incorporates multiple alignments, including representation alignment between reactions and enzymes, and internal alignment within enzyme and reaction representations. By introducing a regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, enhancing information exchange within these domains. FGW-CLIP demonstrates superior performance on the widely-used EnzymeMap benchmark, significantly outperforming existing methods in enzyme virtual screening tasks. Notably, it achieves state-of-the-art results in both BEDROC and EF metrics, indicating its efficacy in identifying relevant enzymes for given reactions. These results highlight the potential of our method to advance virtual enzyme screening, offering a powerful tool for enzyme discovery and characterization.", "title_embedding_index": 966, "title_abs_embedding_index": 991}, {"title": "The Persian Rug: solving toy models of superposition using large-scale symmetries", "link_suffix": "/forum?id=rapXZIfwbX", "link": "https://openreview.net/forum?id=rapXZIfwbX", "pdf_link": "https://openreview.net/pdf?id=rapXZIfwbX", "keywords": "mechanistic interpretability, autoencoder, sparse data, superposition", "abstract": "We present a complete mechanistic description of the algorithm learned by a minimal non-linear sparse data autoencoder in the limit of large input dimension. The model, originally presented in \\cite{elhage2022superposition}, compresses sparse data vectors through a linear layer and decompresses using another linear layer followed by a ReLU activation. We notice that when the data is permutation symmetric (no input feature is privileged) large models reliably learn an algorithm that is sensitive to individual weights only through their large-scale statistics. For these models, the loss function becomes analytically tractable. Using this understanding, we give explicit upper bounds on the loss, which show that the model is near-optimal among recently proposed architectures. In particular, changes to the elementwise activation function or the addition of gating can at best improve its performance by a constant factor. Finally, we forward-engineer a model with the requisite symmetries and show that its loss precisely matches that of the trained models. Unlike the trained model weights, the minimal randomness in the artificial weights results in miraculous fractal structures resembling a Persian rug, to which the algorithm is oblivious. Our work contributes to neural network interpretability by introducing techniques for understanding the structure of autoencoders.", "title_embedding_index": 967, "title_abs_embedding_index": 992}, {"title": "Online Preference Alignment for Language Models via Count-based Exploration", "link_suffix": "/forum?id=cfKZ5VrhXt", "link": "https://openreview.net/forum?id=cfKZ5VrhXt", "pdf_link": "https://openreview.net/pdf?id=cfKZ5VrhXt", "keywords": "Reinforcement Learning from Human Feedback, RLHF, Preference Alignment, Exploration, LLMs", "abstract": "Reinforcement Learning from Human Feedback (RLHF) has shown great potential in fine-tuning Large Language Models (LLMs) to align with human preferences. Existing methods perform preference alignment from a fixed dataset, which can be limited in data coverage and the resulting reward model is hard to generalize in out-of-distribution responses. Thus, online RLHF is more desirable to empower the LLM to explore outside the support of the initial dataset by iteratively collecting the prompt-response pairs. In this paper, we study the fundamental problem in online RLHF, i.e., how to explore for LLM. We give a theoretical motivation in linear reward assumption to show that an optimistic reward with an upper confidence bound (UCB) term leads to a provably efficient RLHF policy. Then, we reformulate our objective to direct preference optimization with an exploration term, where the UCB-term can be converted to a count-based exploration bonus. We further propose a practical algorithm, named Count-based Online Preference Optimization (COPO), which leverages a simple coin-flip counting module to estimate the pseudo-count of a prompt-response pair in previously collected data. COPO encourages LLMs to balance exploration and preference optimization in an iterative manner, which enlarges the exploration space and the entire data coverage of iterative LLM policies. We conduct online RLHF experiments on Zephyr and Llama-3 models. The results on instruction-following and standard academic benchmarks show that COPO significantly increases performance.", "title_embedding_index": 968, "title_abs_embedding_index": 993}, {"title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive Latency Reduction", "link_suffix": "/forum?id=ulGwcj1egv", "link": "https://openreview.net/forum?id=ulGwcj1egv", "pdf_link": "https://openreview.net/pdf?id=ulGwcj1egv", "keywords": "Input-Adaptive Layer Selection; Resource-Constrained Environments; Latency Reduction; Finetuning", "abstract": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across domanins such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit   2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences.   To address both limitations, we propose \\textsc{FiRST}, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during prefill stage) decides which layers will be skipped during decoding.  \\textsc{FiRST} preserves compatibility with KV caching enabling faster inference while being quality-aware. \\textsc{FiRST} is model-agnostic and can be easily enabled on any pre-trained LLM. We further improve performance by incorporating LoRA adapters for fine-tuning on external datasets, enhancing task-specific accuracy while maintaining latency benefits. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on task. Extensive experiments  show that \\textsc{FiRST} significantly reduces latency while retaining competitive performance (as compared to baselines), making our approach an efficient solution for LLM deployment in low-resource environments.", "title_embedding_index": 969, "title_abs_embedding_index": 994}, {"title": "ClusterGen: Token Generation in Sublinear Time and Memory with Clustering KV Cache", "link_suffix": "/forum?id=M922KJFO7O", "link": "https://openreview.net/forum?id=M922KJFO7O", "pdf_link": "https://openreview.net/pdf?id=M922KJFO7O", "keywords": "KV cache, large language models, clustering", "abstract": "Despite the significant success of large language models (LLMs), their extensive memory requirements pose challenges for deploying them in long-context token generation. The substantial memory footprint of LLM decoders arises from the necessity to store all previous tokens in the attention module, a requirement imposed by key-value (KV) caching. In this work, our focus is on developing an efficient compression technique for the KV cache. Empirical evidence indicates a significant clustering tendency within key embeddings in the attention module. Building on this key insight, we have devised a novel caching method with sublinear complexity, employing online clustering on key tokens and online \n sampling on values. The result is a provably accurate and efficient attention decoding algorithm, termed ClusterGen. Not only does this algorithm ensure a sublinear memory footprint and sublinear time complexity, but we also establish a tight error bound for our approach. Empirical evaluations on long-context question-answering tasks demonstrate that ClusterGen significantly outperforms existing and state-of-the-art KV cache compression methods in terms of performance and efficiency.", "title_embedding_index": 970, "title_abs_embedding_index": 995}, {"title": "Outward Odyssey: Improving Reward Models with Proximal Policy Exploration for Preference-Based Reinforcement Learning", "link_suffix": "/forum?id=gXV84CnMUm", "link": "https://openreview.net/forum?id=gXV84CnMUm", "pdf_link": "https://openreview.net/pdf?id=gXV84CnMUm", "keywords": "Preference-based Reinforcement Learning; Reinforcement Learning; Human Feedback", "abstract": "Reinforcement learning (RL) heavily depends on well-designed reward functions, which can be challenging to create and may introduce biases, especially for complex behaviors. Preference-based RL (PbRL) addresses this by using human feedback to construct a reward model that reflects human preferences, yet requiring considerable human involvement. To alleviate this, several PbRL methods aim to select queries that need minimal feedback. However, these methods do not directly enhance the data coverage within the preference buffer. In this paper, to emphasize the critical role of preference buffer coverage in determining the quality of the reward model, we first investigate and find that a reward model's evaluative accuracy is the highest for trajectories within the preference buffer's distribution and significantly decreases for out-of-distribution trajectories. Against this phenomenon, we introduce theProximal Policy Exploration (PPE)algorithm, which consists of aproximal-policy extensionmethod and amixture distribution querymethod.\nTo achieve higher preference buffer coverage, theproximal-policy extensionmethod encourages active exploration of data within near-policy regions that fall outside the preference buffer's distribution. To balance the inclusion of in-distribution and out-of-distribution data, themixture distribution querymethod proactively selects a mix of data from both outside and within the preference buffer's distribution for querying. PPE not only expands the preference buffer's coverage but also ensures the reward model's evaluative capability for in-distribution data. Our comprehensive experiments demonstrate that PPE achieves significant improvement in both human feedback efficiency and RL sample efficiency, underscoring the importance of preference buffer coverage in PbRL tasks.", "title_embedding_index": 971, "title_abs_embedding_index": 996}, {"title": "Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System", "link_suffix": "/forum?id=c4w1TqcSi0", "link": "https://openreview.net/forum?id=c4w1TqcSi0", "pdf_link": "https://openreview.net/pdf?id=c4w1TqcSi0", "keywords": "llm agent, multi-agent, inference scaling law", "abstract": "Large Language Model (LLM) based multi-agent systems (MAS) show remarkable potential in collaborative problem-solving, yet they still face critical challenges: low communication efficiency, poor scalability, and a lack of effective parameter-updating optimization methods for multi-agent collaboration. We present Optima, a novel framework that addresses these issues by significantly enhancing both communication efficiency and task effectiveness in LLM-based MAS through LLM training. At its core, Optima employs an iterative generate, rank, select, and train paradigm, incorporating a reward function that balances task performance, token efficiency, and communication readability. We explore various RL algorithms, including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid approaches, providing insights into their effectiveness-efficiency trade-offs for iterative LLM-based MAS training. Additionally, we integrate Monte Carlo Tree Search-inspired techniques for DPO data generation, conceptualizing conversation turns as tree nodes to explore diverse interaction trajectories. We evaluate Optima on common multi-agent tasks, including information-asymmetric question answering and complex reasoning. Our method demonstrates consistent and substantial improvements over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up to 2.8x performance gain with less than 10% tokens on tasks requiring heavy multi-agent information exchange. Moreover, Optima's efficiency gains open new possibilities for leveraging inference-compute more effectively, potentially leading to improved inference-time scaling laws. By addressing fundamental challenges in multi-agent collaboration and providing a novel optimization framework, Optima shows the potential towards scalable, efficient, and effective LLM-based MAS.", "title_embedding_index": 972, "title_abs_embedding_index": 997}, {"title": "Searching for Optimal Solutions with LLMs via Bayesian Optimization", "link_suffix": "/forum?id=aVfDrl7xDV", "link": "https://openreview.net/forum?id=aVfDrl7xDV", "pdf_link": "https://openreview.net/pdf?id=aVfDrl7xDV", "keywords": "search, optimization, LLMs, test-time compute, bayesian optimization, reasoning", "abstract": "Efficient scaling of test-time compute to search for optimal solutions is an important step towards building generally-capable language models that can reason. Recent work, however, shows that tasks of varying complexity require distinct search strategies to solve optimally, thus making it challenging to design a one-size-fits-all approach. Prior solutions either attempt to predict task difficulty to select the optimal search strategy, often infeasible in practice, or use a static, pre-defined strategy, e.g., repeated parallel sampling or greedy sequential search, which is sub-optimal. In this work, we argue for an alternative view that dynamically adapts the search strategy to changing estimates of the uncertainty in the search space with each round of generation via the probabilistic framework of Bayesian optimization (BO). To this end, we introduce Bayesian-OPRO (BOPRO)---a generalization of a recent method for in-context optimization, which iteratively samples from new proposal distributions by prompting the LLM with a subset of its previous generations selected to explore different parts of the search space. We evaluate our method on a word-search task called Semantle and the joint task of hypothesis search cum program synthesis using a one-dimensional version of the challenging Abstraction and Reasoning Corpus (1D-ARC) to find that BOPRO trails a strong greedy baseline in aggregate. Our analysis of the behaviors exhibited by each method reveals, nonetheless, that BOPRO demonstrates desirable properties essential for building a general solution for search, and we conclude by identifying key areas for future research to address its current limitations.", "title_embedding_index": 973, "title_abs_embedding_index": 998}, {"title": "CoS: Enhancing Personalization and Mitigating Bias with Context Steering", "link_suffix": "/forum?id=xQCXInDq0m", "link": "https://openreview.net/forum?id=xQCXInDq0m", "pdf_link": "https://openreview.net/pdf?id=xQCXInDq0m", "keywords": "personalization, context, large language model, inference, controllable generation", "abstract": "To deliver high-quality, personalized responses, large language models (LLMs) must effectively incorporate \\textit{context} \u2014 personal, demographic, and cultural information specific to an end-user. For example, asking the model to explain Newton's second law with the context \\textit{I am a toddler''} should produce a response different from when the context is \\textit{I am a physics professor''}. However, leveraging the context in practice is a nuanced and challenging task, and is often dependent on the specific situation or user base. The model must strike a balance between providing specific, personalized responses and maintaining general applicability. Current solutions, such as prompt-engineering and fine-tuning require collection of contextually appropriate responses as examples, making them time-consuming and less flexible to use across different contexts. In this work, we introduce \\textbf{Context Steering (CoS)} \u2014a simple, training-free decoding approach that amplifies the influence of the \\textit{context} in next token predictions. CoS computes contextual influence by comparing the output probabilities from two LLM forward passes: one that includes the context and one that does not. By linearly scaling the contextual influence, CoS allows practitioners to flexibly control the degree of personalization for different use cases. We show that CoS can be applied to autoregressive LLMs, and demonstrates strong performance in personalized recommendations. Additionally, we show that CoS can function as a Bayesian Generative model to infer and quantify correlations between open-ended texts, broadening its potential applications.", "title_embedding_index": 974, "title_abs_embedding_index": 999}]