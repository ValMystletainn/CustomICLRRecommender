[
    {
        "title": "Grounding Continuous Representations in Geometry: Equivariant Neural Fields",
        "link_suffix": "/forum?id=A4eCzSohhx",
        "link": "https://openreview.net/forum?id=A4eCzSohhx",
        "pdf_link": "https://openreview.net/pdf?id=A4eCzSohhx",
        "keywords": "Geometric Deep Learning, Neural Fields, Equivariance, Representation Learning, Latent Point Clouds",
        "abstract": "Conditional Neural Fields (CNFs) are increasingly being leveraged as continuous signal representations, by associating each data-sample with a latent variable that conditions a shared backbone Neural Field (NeF) to reconstruct the sample. However, existing CNF architectures face limitations when using this latent downstream in tasks requiring fine-grained geometric reasoning, such as classification and segmentation. We posit that this results from lack of explicit modelling of geometric information (e.g. locality in the signal or the orientation of a feature) in the latent space of CNFs. As such, we propose Equivariant Neural Fields (ENFs), a novel CNF architecture which uses a geometry-informed cross-attention to condition the NeF on a geometric variable\u2014a latent point cloud of features\u2014that enables an equivariant decoding from latent to field. We show that this approach induces a steerability property by which both field and latent are grounded in geometry and amenable to transformation laws: if the field transforms, the latent representation transforms accordingly\u2014and vice versa. Crucially, this equivariance relation ensures that the latent is capable of (1) representing geometric patterns faitfhully, allowing for geometric reasoning in latent space, (2) weight-sharing over similar local patterns, allowing for efficient learning of datasets of fields. We validate these main properties in a range of tasks including classification, segmentation, forecasting and reconstruction, showing clear improvement over baselines with a geometry-free latent space."
    },
    {
        "title": "FOSP: Fine-tuning Offline Safe Policy through World Models",
        "link_suffix": "/forum?id=dbuFJg7eaw",
        "link": "https://openreview.net/forum?id=dbuFJg7eaw",
        "pdf_link": "https://openreview.net/pdf?id=dbuFJg7eaw",
        "keywords": "Safe RL, Offline-to-online RL, Robot Learning",
        "abstract": "Offline Safe Reinforcement Learning (RL) seeks to address safety constraints by learning from static datasets and restricting exploration. However, these approaches heavily rely on the dataset and struggle to generalize to unseen scenarios safely. In this paper, we aim to improve safety during the deployment of vision-based robotic tasks through online fine-tuning an offline pretrained policy. To facilitate effective fine-tuning, we introduce model-based RL, which is known for its data efficiency. Specifically, our method employs in-sample optimization to improve offline training efficiency while incorporating reachability guidance to ensure safety. After obtaining an offline safe policy, safe policy expansion approach is leveraged for online fine-tuning. The performance of our method is validated on simulation benchmarks with five vision-only tasks and through real-world robot deployment using limited data. It demonstrates that our approach significantly improves the generalization of offline policies to unseen safety-constrained scenarios. To the best of our knowledge, this is the first work to explore offline-to-online RL for safe generalization tasks. The videos are available athttps://sites.google.com/view/safefinetune/home."
    },
    {
        "title": "Length-Induced Embedding Collapse in Transformer-based Models",
        "link_suffix": "/forum?id=jgISC1wdYy",
        "link": "https://openreview.net/forum?id=jgISC1wdYy",
        "pdf_link": "https://openreview.net/pdf?id=jgISC1wdYy",
        "keywords": "Embedding Models, Length Collapse, Mechanistic Interpretability",
        "abstract": "Text embeddings enable various applications, but their performance deteriorates on longer texts. In this paper, we find that the performance degradation is due to a phenomenon called \\textbf{Length Collapse}, where longer text embeddings collapse into a narrow space. This collapse results in a distributional inconsistency between embeddings of different text lengths, ultimately hurting the performance of downstream tasks. Theoretically, by considering the self-attention mechanism inherently functions as a low-pass filter, we prove that long sequences increase the attenuation rate of the low-pass filter effect of the self-attention mechanism. With layers going deeper, excessive low-pass filtering causes the token signals to retain only their Direct-Current (DC) component, which means the input token feature maps will collapse into a narrow space, especially in long texts. Based on the above analysis, we propose to mitigate the undesirable length collapse limitation by introducing a temperature in $\\softmax(\\cdot)$, which achieves a higher low-filter attenuation rate. The tuning-free method, called \\textbf{TempScale}, can be plugged into multiple transformer-based embedding models. Empirically, we demonstrate that TempScale can improve existing embedding models especially on long text inputs, bringing up to \\textbf{0.53%} performance gains on 40 datasets from  Massive Text Embedding Benchmark (MTEB) and \\textbf{0.82%} performance gains on 4 datasets from LongEmbed, which specifically focuses on long context retrieval. The source code is available at \\textcolor{blue}{\\url{https://anonymous.4open.science/r/Length_Collapse-22D2}}."
    },
    {
        "title": "Gradual Learning: Optimizing Fine-Tuning with Partially Mastered Knowledge in Large Language Models",
        "link_suffix": "/forum?id=EukID7GvBy",
        "link": "https://openreview.net/forum?id=EukID7GvBy",
        "pdf_link": "https://openreview.net/pdf?id=EukID7GvBy",
        "keywords": "Large Language Model; DCAI ; Fine-tuning",
        "abstract": "During the pretraining phase, large language models (LLMs) acquire vast amounts of knowledge from extensive text corpora. Nevertheless, in later stages such as fine-tuning and inference, the model may encounter knowledge not covered in the initial training, which can lead to hallucinations and degraded performance. This issue has a profound impact on the model's capabilities, as it will inevitably face out-of-scope knowledge after pretraining. Furthermore, fine-tuning is often required to adapt LLMs to domain-specific tasks, necessitating the acquisition of new knowledge. However, this phenomenon limits the model\u2019s ability to learn and integrate new information during fine-tuning. The effectiveness of fine-tuning largely depends on the type of knowledge involved. Existing research suggests that fine-tuning the model on partially mastered knowledge\u2014for instance, question-answer pairs where the model has a chance of providing correct responses under non-greedy decoding\u2014can enable the model to acquire new knowledge while mitigating the forgetting of previously learned information. Notably, this approach can still lead to the forgetting of fully mastered knowledge, constraining the fine-tuning dataset to a narrower range and limiting the model's overall potential for improvement. Given the model\u2019s intrinsic reasoning abilities and the interconnectedness of different knowledge areas, it is likely that as the model\u2019s capacity to utilize existing knowledge improves during fine-tuning, previously unmastered knowledge may become more understandable. To explore this hypothesis, we conducted experiments and, based on the results, proposed a two-stage fine-tuning strategy. This approach not only improves the model's overall test accuracy and knowledge retention but also preserves its accuracy on previously mastered content. When fine-tuning on the WikiQA dataset, our method increases the amount of knowledge acquired by the model in this stage by 24%."
    },
    {
        "title": "Autocorrelation Matters: Understanding the Role of Initialization Schemes for State Space Models",
        "link_suffix": "/forum?id=sZJNkorXMk",
        "link": "https://openreview.net/forum?id=sZJNkorXMk",
        "pdf_link": "https://openreview.net/pdf?id=sZJNkorXMk",
        "keywords": "State Space Model; Optimization;",
        "abstract": "Current methods for initializing state space model (SSM) parameters primarily rely on the HiPPO framework \\citep{gu2023how}, which is based on online function approximation with the SSM kernel basis. \nHowever, the HiPPO framework does not explicitly account for the effects of the temporal structures of input sequences on the optimization of SSMs.\nIn this paper, we take a further step to investigate the roles of SSM initialization schemes by considering the autocorrelation of input sequences. \nSpecifically, we: (1) rigorously characterize the dependency of the SSM timescale on sequence length based on sequence autocorrelation; (2) find that with a proper timescale, allowing a zero real part for the eigenvalues of the SSM state matrix mitigates the curse of memory while still maintaining stability at initialization; (3) show that the imaginary part of the eigenvalues of the SSM state matrix determines the conditioning of SSM optimization problems, and uncover an approximation-estimation tradeoff when training SSMs with a specific class of target functions."
    },
    {
        "title": "LagEncoder: A Non-Parametric Method for Representation Learning",
        "link_suffix": "/forum?id=vxrtEHc97c",
        "link": "https://openreview.net/forum?id=vxrtEHc97c",
        "pdf_link": "https://openreview.net/pdf?id=vxrtEHc97c",
        "keywords": "Non-parametric encoder, Finite element method, Interpretable model, Universal architecture, Scaling law, ImageNet, ResNet, ViT",
        "abstract": "Non-parametric encoders offer advantages in interpretability and generalizability. However, they often perform significantly worse than deep neural networks on many challenging recognition tasks, and it remains unclear how to apply these techniques effectively to such tasks.\nIn this work, we introduce LagEncoder, a non-parametric, training-free feature extraction method based on Finite Element Basis Functions. Our encoder has a universal architecture that can be applied to various types of raw data and recognition tasks. We found that LagEncoder overcomes the limitations of neural networks in regression problems, where they struggle to fit multi-frequency functions. LagEncoder can be used independently to build models, similar to the principles of transfer learning, where only the head is trained\u2014this makes the model converge quickly and requires low training costs.\nAdditionally, LagEncoder serves as an efficient parameter-efficient fine-tuning (PEFT) approach. Our experiments on the ImageNet dataset show that pre-trained models using LagEncoder achieve performance improvements within just one training epoch. Moreover, it does not require adjustments to the original training recipe, and the model's total parameters remain nearly unchanged. Our evaluation of the scaling law for model performance shows that using LagEncoder is more cost-effective than simply increasing model size."
    },
    {
        "title": "DuRND: Rewarding from Novelty to Contribution for Reinforcement Learning via Dual Random Networks Distillation",
        "link_suffix": "/forum?id=7P7FsPL05D",
        "link": "https://openreview.net/forum?id=7P7FsPL05D",
        "pdf_link": "https://openreview.net/pdf?id=7P7FsPL05D",
        "keywords": "Reinforcement Learning, Exploration-Exploitation Trade-off, Random Network Distillation, Auxiliary Rewards",
        "abstract": "Existing reward shaping techniques for sparse-reward tasks in reinforcement learning generally fall into two categories: novelty-based exploration bonuses and value-based rewards. The former encourages agents to explore less visited areas but can divert them from their main objectives, while the latter promotes stable late-stage convergence but often lacks sufficient early exploration. To combine the benefits of both, we propose Dual Random Networks Distillation (DuRND), a novel framework integrating two lightweight random network modules. These modules jointly generate two rewards: a novelty reward to drive exploration and a contribution reward to evaluate progress toward desired behaviors, achieving an efficient balance between exploration and exploitation. With low computational overhead, DuRND excels in high-dimensional environments like Atari, VizDoom, and MiniWorld, outperforming several benchmarks."
    },
    {
        "title": "d-Linear Generation Error Bound for Distributed Diffusion Models",
        "link_suffix": "/forum?id=f3hIphjjY8",
        "link": "https://openreview.net/forum?id=f3hIphjjY8",
        "pdf_link": "https://openreview.net/pdf?id=f3hIphjjY8",
        "keywords": "Distributed diffusion models, Generation error bound",
        "abstract": "The recent rise of distributed diffusion models has been driven by the explosive growth of data and the increasing demand for data generation. However, distributed diffusion models face unique challenges in resource-constrained environments. Existing approaches lack theoretical support, particularly with respect to generation error in such settings. In this paper, we are the first to derive the generation error bound for distributed diffusion models with arbitrary pruning, not assuming perfect score approximation. By analyzing the convergence of the score estimation model trained with arbitrary pruning in a distributed manner, we highlight the impact of complex factors such as model evolution dynamics and arbitrary pruning on the generation performance. This theoretical generation error bound is linear in the data dimension $d$, aligning with state-of-the-art results in the single-worker paradigm."
    },
    {
        "title": "SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents",
        "link_suffix": "/forum?id=xKDZAW0He3",
        "link": "https://openreview.net/forum?id=xKDZAW0He3",
        "pdf_link": "https://openreview.net/pdf?id=xKDZAW0He3",
        "keywords": "memory management, conversational agent, RAG, text segmentation, prompt compression",
        "abstract": "To deliver coherent and personalized experiences in long-term conversations, existing approaches typically perform retrieval augmented response generation by constructing memory banks from conversation history at either the turn-level, session-level, or through summarization techniques.\nIn this paper, we explore the impact of different memory granularities and present two key findings: (1) Both turn-level and session-level memory units are suboptimal, affecting not only the quality of final responses, but also the accuracy of the retrieval process.\n(2) The redundancy in natural language introduces noise, hindering precise retrieval. We demonstrate thatLLMLingua-2, originally designed for prompt compression to accelerate LLM inference, can serve as an effective denoising method to enhance memory retrieval accuracy.Building on these insights, we proposeSeCom, a method that constructs a memory bank with topical segments by introducing a conversationSegmentation model, while performing memory retrieval based onCompressed memory units.\nExperimental results show thatSeComoutperforms turn-level, session-level, and several summarization-based methods on long-term conversation benchmarks such asLOCOMOandLong-MT-Bench+. Additionally, the proposed conversation segmentation method demonstrates superior performance on dialogue segmentation datasets such asDialSeg711,TIAGE, andSuperDialSeg."
    },
    {
        "title": "Document-Level In-Context Few-Shot Relation Extraction via Pre-Trained Language Models",
        "link_suffix": "/forum?id=9hpcTgztk8",
        "link": "https://openreview.net/forum?id=9hpcTgztk8",
        "pdf_link": "https://openreview.net/pdf?id=9hpcTgztk8",
        "keywords": "relation extraction, document, in-context few-shot learning, knowledge base, large language models, natural language processing",
        "abstract": "Document-level relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods for this task use pre-trained language models (LMs) via fine-tuning, yet fine-tuning is computationally expensive and cannot adapt to new relation types or new LMs. As a remedy, we leverage the generalization capabilities of pre-trained LMs and present a novel framework for document-level in-context few-shot relation extraction. Our framework has three strengths: it eliminates the need (1) for named entity recognition and (2) for human annotations of documents, and (3) it can be updated to new LMs without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extraction, and demonstrate that our framework achieves state-of-the-art performance. We further show that our framework actually performs much better than the original labels from the development set of DocRED. Finally, we conduct an extensive benchmark demonstrating the effectiveness of our framework, achieving state-of-the-art results across six relation extraction datasets and outperforming more than 30 baseline methods. Unlike our framework, the baseline methods have large computational overhead (e.g., from fine-tuning). To the best of our knowledge, we are the first to reformulate the document-level relation extraction task as a tailored in-context few-shot learning paradigm."
    },
    {
        "title": "SpaceSet: A Large-scale Realistic Space-based Image Dataset for Space Situational Awareness",
        "link_suffix": "/forum?id=z2VBHpRT14",
        "link": "https://openreview.net/forum?id=z2VBHpRT14",
        "pdf_link": "https://openreview.net/pdf?id=z2VBHpRT14",
        "keywords": "space situational awareness, object detection and tracking, space image dataset, high resolution image",
        "abstract": "Space situational awareness (SSA) plays an imperative role in maintaining safe space operations, especially given the increasingly congested space traffic around Earth. Space-based SSA offers a flexible and lightweight solution compared to traditional ground-based SSA. With advanced machine learning approaches, space-based SSA can extract features from high-resolution images in space to detect and track resident space objects (RSOs). However, existing spacecraft image datasets, such as SPARK, fall short of providing realistic camera observations, rendering the derived algorithms unsuitable for real SSA systems. In this research, we introduce SpaceSet, a large-scale realistic space-based image dataset for SSA. We consider accurate space orbit dynamics and a physical camera model with various noise distributions, generating images at the photon level. To extend the available observation window, four overlapping cameras are simulated with a fixed rotation angle. SpaceSet includes images of RSOs observed from $19 km$ to $63,000 km$, captured by a tracker operating in LEO, MEO, and GEO orbits over a period of $5,000$ seconds. Each image has a resolution of $4418 \\times 4418$ pixels, providing detailed features for developing advanced SSA approaches. We split the dataset into three subsets: SpaceSet-100, SpaceSet-5000, and SpaceSet-full, catering to various image processing applications. The SpaceSet-full corpus includes a comprehensive data-loader with $781.5GB$ of images and $25.9MB$ of ground truth labels. We also benchmark detection and tracking algorithms on the SpaceSet-100 dataset using a specified splitting method to accelerate the training process."
    },
    {
        "title": "Response Tuning: Aligning Large Language Models without Instruction",
        "link_suffix": "/forum?id=rnRBGMNYa2",
        "link": "https://openreview.net/forum?id=rnRBGMNYa2",
        "pdf_link": "https://openreview.net/pdf?id=rnRBGMNYa2",
        "keywords": "Large Language Models, Instruction Tuning, LLM Safety",
        "abstract": "Instruction tuning\u2014supervised fine-tuning using instruction-response pairs\u2014is a foundational step in transitioning pre-trained Large Language Models (LLMs) into helpful and safe chat assistants. Our hypothesis is that establishing an adequate output space can enable such a transition given the capabilities inherent in pre-trained LLMs. To verify this, we propose Response Tuning (RT), which eliminates the instruction-conditioning step in instruction tuning and solely focuses on response space supervision. Our experiments demonstrate that RT models, trained only using responses, can effectively respond to a wide range of instructions and exhibit helpfulness comparable to that of their instruction-tuned counterparts. Furthermore, we observe that controlling the training response distribution can significantly improve their user preference or elicit target behaviors such as refusing assistance for unsafe queries. Our findings illuminate the role of establishing an adequate output space in alignment, highlighting the potential of the extensive inherent capabilities of pre-trained LLMs."
    },
    {
        "title": "Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining",
        "link_suffix": "/forum?id=XdRIno98gG",
        "link": "https://openreview.net/forum?id=XdRIno98gG",
        "pdf_link": "https://openreview.net/pdf?id=XdRIno98gG",
        "keywords": "Self-supervised Monocular Depth Estimation, Deep Metric Learning, Knowledge Distillation",
        "abstract": "Self-supervised monocular depth estimation (SSMDE) aims to predict the dense depth map of a monocular image, by learning depth from RGB image sequences, eliminating the need for ground-truth depth labels.\nAlthough this approach simplifies data acquisition compared to supervised methods, it struggles with reflective surfaces, as they violate the assumptions of Lambertian reflectance, leading to inaccurate training on such surfaces.\nTo tackle this problem, we propose a novel training strategy for an SSMDE by leveraging triplet mining to pinpoint reflective regions at the pixel level, guided by the camera geometry between different viewpoints.\nThe proposed reflection-aware triplet mining loss specifically penalizes the inappropriate photometric error minimization on the localized reflective regions while preserving depth accuracy on non-reflective areas.\nWe also incorporate a reflection-aware knowledge distillation method that enables a student model to selectively learn the pixel-level knowledge from reflective and non-reflective regions. This results in robust depth estimation across areas.\nEvaluation results on multiple datasets demonstrate that our method effectively enhances depth quality on reflective surfaces and outperforms state-of-the-art SSMDE baselines."
    },
    {
        "title": "An Effective Theory of Bias Amplification",
        "link_suffix": "/forum?id=VoI4d6uhdr",
        "link": "https://openreview.net/forum?id=VoI4d6uhdr",
        "pdf_link": "https://openreview.net/pdf?id=VoI4d6uhdr",
        "keywords": "fairness, algorithmic bias, machine learning theory, random matrix theory",
        "abstract": "Machine learning models may capture and amplify biases present in data, leading to disparate test performance across social groups. To better understand, evaluate, and mitigate these possible biases, a deeper theoretical understanding of how model design choices and data distribution properties could contribute to bias is needed. In this work, we contribute a precise analytical theory in the context of ridge regression, both with and without random projections, where the former models neural networks in a simplified regime. Our theory offers a unified and rigorous explanation of machine learning bias, providing insights into phenomena such as bias amplification and minority-group bias in various feature and parameter regimes. For example, we demonstrate that there may be an optimal regularization penalty or training time to avoid bias amplification, and there can be fundamental differences in test error between groups that do not vanish with increased parameterization. Importantly, our theoretical predictions align with several empirical observations reported in the literature. We extensively empirically validate our theory on diverse synthetic and semi-synthetic datasets."
    },
    {
        "title": "SpikeZIP: Compressing Spiking Neural Network with Paths-Ensemble Training for Optimized Pareto-front Performance",
        "link_suffix": "/forum?id=u438df0Uce",
        "link": "https://openreview.net/forum?id=u438df0Uce",
        "pdf_link": "https://openreview.net/pdf?id=u438df0Uce",
        "keywords": "Spiking Neural Networks, Quantization, ANN-SNN Conversion",
        "abstract": "Spiking neural network (SNN) has attracted great attention due to its great energy efficiency on neuromorphic hardware. \nBy transferring the parameters of a pretrained artificial neural network (ANN) and utilizing the ANN quantization, recent works of ANN-SNN conversion can produce SNNs with close-to-ANN accuracy and low inference latency (known as the number of time-steps).\nNevertheless, existing works fail at providing theoretic equivalence between Quantized-ANN (QANN) and its converted SNN, while the SNN accuracy at small time-step (i.e. Pareto-frontier) can be further improved.\nTo solve the problems, this paper proposes a novel conversion framework called SpikeZIP. The SpikeZIP utilizes the ANN-Quantized ANN(QANN)-SNN two-step conversion to obtain SNN which improves the Pareto frontier of accuracy versus inference time-steps. SpikeZIP integrates two novel algorithms: 1) a paths-ensemble training algorithm that considers the SNN temporal information when fine-tuning QANN; 2) a mathematically equivalent conversion algorithm between the whole QANN and SNN. In the experiment, SpikeZIP can achieve 73.92% accuracy on ImageNet with VGG-16 within 9 time-steps and 74.21% accuracy on ImageNet with ResNet-34 within 11 time-steps which are better than SOTA works."
    },
    {
        "title": "CC-VFed: Client Contribution Detects Byzantine Attacks in Vertical Federated Learning",
        "link_suffix": "/forum?id=E3qIInyTgL",
        "link": "https://openreview.net/forum?id=E3qIInyTgL",
        "pdf_link": "https://openreview.net/pdf?id=E3qIInyTgL",
        "keywords": "Vertical Federated Learning, Byzantine Attacks",
        "abstract": "Vertical federated learning (VFL) is a type of federated learning where the collection of different features is shared among multiple clients, and it is attracting attention as a training method that takes into account the privacy and security of training data. On the other hand, in federated learning, there is a threat of Byzantine attacks, where some malicious clients disrupt the training of the model and output an trained model that does not exhibit the behavior that should be obtained. Thus far, numerous defense methods against Byzantine attacks on horizontal federated learning have been proposed, most of which focus on the similarity of the models generated across clients having the similar features and mitigate the attacks by excluding outliers. However, in VFL, the feature sets assigned by each client are inherently different, making similar methods inapplicable, and there is little existing research in this area. In light of the above, this paper organizes and classifies feasible Byzantine attacks and proposes a new defense method CC-VFed against these attack methods. Firstly, this paper organizes and classifies attack methods that contaminate training data, demonstrating that sign-flipping attacks pose a threat to VFL. Subsequently, in order to capture the differences in client features, this paper proposes a method for detecting and neutralizing malicious clients based on their contribution to output labels, demonstrating that it is indeed possible to defend Byzantine attacks in VFL."
    },
    {
        "title": "Reflection on Knowledge Graph for Large Language Models Reasoning",
        "link_suffix": "/forum?id=6f7RoeQ7Go",
        "link": "https://openreview.net/forum?id=6f7RoeQ7Go",
        "pdf_link": "https://openreview.net/pdf?id=6f7RoeQ7Go",
        "keywords": "Large Language Models, Knowledge Graph Question Answering, Knowledge-Intensive Tasks, Multi-Task Tuning",
        "abstract": "Recent studies have highlighted the potential benefits of supplementing Large Language Models (LLMs) with information retrieved from knowledge graphs to enhance their performance. However, current approaches often introduce additional noise in the pipeline process of knowledge retrieval and reasoning, leading to the accumulation of errors, impeding LLMs from effectively combining the external knowledge in answering complex multi-hop questions. To this end, we introduce RefKG, an innovative framework specifically crafted to enhance the reasoning capabilities of LLMs through reflective engagement with knowledge graphs. In particular, RefKG autonomously conduct retrieval and reflection on knowledge graphs. Its reasoning process includes four steps: decomposing complex queries, retrieving and pruning evidence subgraphs, generating textual evidence, and evidence-enhanced reasoning. To enhance the alignment of LLMs with external knowledge, we have developed a multi-task tuning strategy that not only infuses knowledge to LLMs but also teaches them how to utilize the knowledge in answering questions, thereby significantly improving their ability to handle knowledge-intensive tasks. Experimental results on fact verification and knowledge graph question answering tasks demonstrate that RefKG outperforms previous state-of-the-art models."
    },
    {
        "title": "Choose Your Anchor Wisely: Effective Unlearning Diffusion Models via Concept Reconditioning",
        "link_suffix": "/forum?id=4aWzNhmq4K",
        "link": "https://openreview.net/forum?id=4aWzNhmq4K",
        "pdf_link": "https://openreview.net/pdf?id=4aWzNhmq4K",
        "keywords": "Machine Unlearning, Diffusion Models.",
        "abstract": "Large-scale conditional diffusion models (DMs) have demonstrated exceptional ability in generating high-quality images from textual descriptions, gaining widespread use across various domains. However, these models also carry the risk of producing harmful, sensitive, or copyrighted content, creating a pressing need to remove such information from their generation capabilities. While retraining from scratch is prohibitively expensive, machine unlearning provides a more efficient solution by selectively removing undesirable knowledge while preserving utility. In this paper, we introduce \\textbf{COncept REconditioning (CORE)}, a simple yet effective approach for unlearning diffusion models. Similar to some existing approaches, CORE guides the noise predictor conditioned on forget concepts towards an anchor generated from alternative concepts. However, CORE introduces key differences in the choice of anchor and retain loss, which contribute to its enhanced performance. We evaluate the unlearning effectiveness and retainability of CORE on UnlearnCanvas. Extensive experiments demonstrate that CORE surpasses state-of-the-art methods including its close variants and achieves near-perfect performance, especially when we aim to forget multiple concepts. More ablation studies show that CORE's careful selection of the anchor and retain loss is critical to its superior performance."
    },
    {
        "title": "Learning to Discretize Denoising Diffusion ODEs",
        "link_suffix": "/forum?id=xDrFWUmCne",
        "link": "https://openreview.net/forum?id=xDrFWUmCne",
        "pdf_link": "https://openreview.net/pdf?id=xDrFWUmCne",
        "keywords": "Diffusion models, Efficient Sampling, Ordinary Differentiable Equations",
        "abstract": "Diffusion Probabilistic Models (DPMs) are generative models showing competitive performance in various domains, including image synthesis and 3D point cloud generation. Sampling from pre-trained DPMs involves multiple neural function evaluations (NFE) to transform Gaussian noise samples into images, resulting in higher computational costs compared to single-step generative models such as GANs or VAEs. Therefore, reducing the number of NFEs while preserving generation quality is crucial. To address this, we propose LD3, a lightweight framework designed to learn the optimal time discretization for sampling. LD3 can be combined with various samplers and consistently improves generation quality without having to retrain resource-intensive neural networks. We demonstrate analytically and empirically that LD3 improves sampling efficiency much less computational overhead. We evaluate our method with extensive experiments on 7 pre-trained models, covering unconditional and conditional sampling in both pixel-space and latent-space DPMs. We achieve FIDs of 2.38 (10 NFE), and 2.27 (10 NFE) on unconditional CIFAR10 and AFHQv2 in 5-10 minutes of training. LD3 offers an efficient approach to sampling from pre-trained diffusion models."
    },
    {
        "title": "Automated Zonal level implant loosening detection from Hip X-ray using a multi-staged approach",
        "link_suffix": "/forum?id=IFOgfaX2Fj",
        "link": "https://openreview.net/forum?id=IFOgfaX2Fj",
        "pdf_link": "https://openreview.net/pdf?id=IFOgfaX2Fj",
        "keywords": "Radiolucency, Implant loosening, Gruen zones, Charnley zones",
        "abstract": "Hip arthroplasty is a surgical procedure that involves the replacement of a patient\u2019s hip joint with a prosthetic implant. While these implants are initially effective, they may eventually fail and necessitate revision surgery. It is important to identify the 3 Charnley and 7 Gruen zones around the implant and then identify the zone-wise radiolucency which indicates loosening for effective pre and post-operative planning. Despite the importance of zones, there is a lack of automation attempts in this field. In this work, we have proposed a 3-stage algorithm that detects the sanity of the image for diagnosis, segments into the zones, and then identifies radiolucency within the zones. We have demonstrated a 94% accuracy for Fit/Not Fit segregation, a 0.95 dice score for our zonal segmentation, and a 98% overall loosening accuracy. Obtaining an average dice score of 0.92 in the segmentation of zones and 0.93 accuracy on loosening detection on a blind dataset indicates the robustness of the proposed algorithm. This work will contribute to the development of more efficient and accurate models to detect implant loosening."
    },
    {
        "title": "Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization",
        "link_suffix": "/forum?id=ySJSGZxN7M",
        "link": "https://openreview.net/forum?id=ySJSGZxN7M",
        "pdf_link": "https://openreview.net/pdf?id=ySJSGZxN7M",
        "keywords": "Nearest Neighbor Search, Optimization",
        "abstract": "The Hierarchical Navigable Small World (HNSW) algorithm is widely used for approximate nearest neighbor (ANN) search, leveraging the principles of navigable small-world graphs. However, it faces some limitations. The first is the local optima problem, which arises from the algorithm's greedy search strategy, selecting neighbors based solely on proximity at each step. This often leads to cluster disconnections. The second limitation is that HNSW frequently fails to achieve logarithmic complexity, particularly in high-dimensional datasets, due to the exhaustive traversal through each layer. To address these limitations, we propose a novel algorithm that mitigates local optima and cluster disconnections while improving inference speed. The first component is a dual-branch HNSW structure with LID-based insertion mechanisms, enabling traversal from multiple directions. This improves outlier node capture, enhances cluster connectivity, and reduces the risk of local minima. The second component introduces a bridge-building technique that adds shortcuts between layers, enabling direct jumps and speeding up inference. Experiments on various benchmarks and datasets showed that our algorithm outperforms the original HNSW in both accuracy and speed. We evaluated six datasets across Computer Vision (CV), deep learning (DL), and Natural Language Processing (NLP), showing improvements of 2.5% in NLP, 15% in DL, and up to 35% in CV tasks. Inference speed is also improved by 12% across all datasets. Ablation studies revealed that LID-based insertion had the greatest impact on performance, followed by the dual-branch structure and bridge-building components."
    },
    {
        "title": "Tool Decoding: A Plug-and-Play Approach to Enhancing Language Models for Tool Usage",
        "link_suffix": "/forum?id=5bUy4F59mk",
        "link": "https://openreview.net/forum?id=5bUy4F59mk",
        "pdf_link": "https://openreview.net/pdf?id=5bUy4F59mk",
        "keywords": "large language models, tool usage, decoding method",
        "abstract": "Despite the significant advancements in large language models (LLMs), their tool-use capabilities remain limited. This limitation stems from the fact that existing approaches often merely adapt strategies designed for basic natural language tasks, overlooking the specific challenges inherent in tool usage, such as precise tool selection, strict predefined formats, and accurate parameter assignment.\nTo bridge this gap, we conduct a fine-grained analysis of the tool usage process, breaking it down into three critical stages: tool awareness, tool selection, and tool call. Our analysis reveals that most failures stem from selection errors, format violations, and parameter mis-assignments.\nBuilding on these insights, we propose \\textbf{Tool Decoding}, a novel, training-free approach that directly incorporates tool-specific information into the decoding process. Tool Decoding employs constrained decoding to ensure format correctness and eliminate hallucinations, while leveraging order consistency to improve parameter accuracy through structured sampling and a majority-voting mechanism. This approach effectively addresses many common tool-use errors in a plug-and-play manner, allowing for seamless generalization to new tools as long as they are accompanied by well-structured documentation to guide the decoding process. \nExperimental evaluations on benchmarks like API-Bank and BFCL V2 \u2022 Live show that Tool Decoding leads to significant improvements across a diverse set of more than 10 models, including both generalist and tool-finetuned models. Almost all models demonstrate performance gains exceeding 70% on both benchmarks. Among the 7B-level models, five outperform GPT-3.5 on key tasks, with two even surpassing GPT-4."
    },
    {
        "title": "Retrieval-Augmented Decision Transformer: External Memory for In-context RL",
        "link_suffix": "/forum?id=PIHPmNNp7w",
        "link": "https://openreview.net/forum?id=PIHPmNNp7w",
        "pdf_link": "https://openreview.net/pdf?id=PIHPmNNp7w",
        "keywords": "Reinforcement Learning, Transformer, Multi-task Learning, In-context Learning, Retrieval-augmentation",
        "abstract": "In-context learning (ICL) is the ability of a model to learn a new task by observing a few exemplars in its context. While prevalent in NLP, this capability has recently also been observed in Reinforcement Learning (RL) settings. Prior in-context RL methods, however, require entire episodes in the agent's context. Given that complex environments typically lead to long episodes with sparse rewards, these methods are constrained to simple environments with short episodes. To address these challenges, we introduce Retrieval-Augmented Decision Transformer (RA-DT). RA-DT employs an external memory mechanism to store past experiences from which it retrieves only sub-trajectories relevant for the current situation. The retrieval component in RA-DT does not require training and can be entirely domain-agnostic. We evaluate the capabilities of RA-DT on grid-world environments, robotics simulations, and procedurally-generated video games. On grid worlds, RA-DT outperforms baselines, while using only a fraction of their context length. Furthermore, we illuminate the limitations of current in-context RL methods on complex environments and discuss future directions. To facilitate future research, we release datasets for four of the considered environments."
    },
    {
        "title": "Hyper-multi-step: The Truth Behind Difficult Long-context Tasks",
        "link_suffix": "/forum?id=LRPzo4jixx",
        "link": "https://openreview.net/forum?id=LRPzo4jixx",
        "pdf_link": "https://openreview.net/pdf?id=LRPzo4jixx",
        "keywords": "LLM, long context, reasoning, multi-step, benchmark, CoT, attention mechanism, linear probing",
        "abstract": "Long-context language models (LCLM), characterized by their extensive context window, is becoming increasingly popular. Meanwhile, many long-context benchmarks present challenging tasks that even the most advanced LCLMs struggle to complete. However, the underlying sources of various challenging long-context tasks have seldom been studied. To bridge this gap, we conduct experiments to indicate their difficulty stems primarily from two basic issues: \"multi-matching retrieval,\" which requires the simultaneous retrieval of multiple items, and \"logic-based retrieval,\" which necessitates logical judgment within retrieval criteria. These two problems, while seemingly straightforward, actually exceed the capabilities of LCLMs because they are proven to be hyper-multi-step (demanding numerous steps to solve) in nature. This finding could explain why LLMs struggle with more advanced long-context tasks, providing a more accurate perspective for rethinking solutions for them."
    },
    {
        "title": "IgSeek: Fast and Accurate Antibody Design via Structure Retrieval",
        "link_suffix": "/forum?id=KgiMUvJcwm",
        "link": "https://openreview.net/forum?id=KgiMUvJcwm",
        "pdf_link": "https://openreview.net/pdf?id=KgiMUvJcwm",
        "keywords": "Antibody Design, Structure Retrieval, Equivariant",
        "abstract": "Recent advancements in protein design have leveraged diffusion models to generate structural scaffolds, followed by a process known as protein inverse folding, which involves sequence inference on these scaffolds. However, these methodologies face significant challenges when applied to hyper-variable structures such as antibody Complementarity-Determining Regions (CDRs), where sequence inference frequently results in non-functional sequences due to hallucinations. Distinguished from prevailing protein inverse folding approaches, this paper introduces IgSeek, a novel structure-retrieval framework that infers CDR sequences by retrieving similar structures from a natural antibody database. Specifically, IgSeek employs a simple yet effective multi-channel equivariant graph neural network to generate high-quality geometric representations of CDR backbone structures. Subsequently, it aligns sequences of structurally similar CDRs and utilizes structurally conserved sequence motifs to enhance inference accuracy. Our experiments demonstrate that IgSeek not only proves to be highly efficient in structural retrieval but also outperforms state-of-the-art approaches in sequence recovery for both antibodies and T-Cell Receptors, offering a new retrieval-based perspective for therapeutic protein design."
    }
]