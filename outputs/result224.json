[
    {
        "title": "Link Prediction on Text Attributed Graphs: A New Benchmark and Efficient LM-nested GNN Design",
        "link_suffix": "/forum?id=fpTh0UxcmQ",
        "link": "https://openreview.net/forum?id=fpTh0UxcmQ",
        "pdf_link": "https://openreview.net/pdf?id=fpTh0UxcmQ",
        "keywords": "Link Prediction, Graph Convolutional Network, Pretrain Language Model",
        "abstract": "Textual and topological information is significant for link prediction (LP) in textattributed graphs (TAGs). Recent link prediction methods have focused on improving the performance of capturing structural features by Graph Convolutional Networks (GCNs), the importance of enhancing text embeddings, powered by the powerful Pretrain Language Model (PLM), has been underestimated. We collect and introduce eight graphs with rich textual information. We further benchmarked current competitive link prediction methods and PLM-based methods in a unified experimental setting, systematically investigating the representation power of the text encoders in the link prediction task. Based on our investigation, we introduce LMGJOINT \u2014 a memory-efficient fine-tuning method. The key design features include: residual connection of textual proximity, a combination of structural and textual embeddings, and a cache embedding training strategy. Our empirical analysis shows that these design elements improve MRR by up to 19.75% over previous state-of-the-art methods and achieve competitive performance across a wide range of models and datasets."
    },
    {
        "title": "Augmented Conditioning is Enough for Effective Training Image Generation",
        "link_suffix": "/forum?id=9aIlDR7hjq",
        "link": "https://openreview.net/forum?id=9aIlDR7hjq",
        "pdf_link": "https://openreview.net/pdf?id=9aIlDR7hjq",
        "keywords": "Synthetic Training Datasets, Image Generation, Generative Models, Diffusion",
        "abstract": "Image generation abilities of text-to-image diffusion models have significantly\nadvanced, yielding highly photo-realistic images from descriptive text and\nincreasing the viability of leveraging synthetic images to train computer vision\nmodels. To serve as effective training data, generated images must be highly\nrealistic while also sufficiently diverse within the support of the target data\ndistribution. Yet, state-of-the-art conditional image generation models have been\nprimarily optimized for creative applications, prioritizing image realism and\nprompt adherence over conditional diversity. In this paper, we investigate how\nto improve the diversity of generated images with the goal of increasing their\neffectiveness to train downstream image classification models, without fine-tuning\nthe image generation model. We find that conditioning the generation process\non an augmented real image and text prompt produces generations that serve as\neffective synthetic datasets for downstream training. Conditioning on real training\nimages contextualizes the generation process to produce images that are in-domain\nwith the real image distribution, while data augmentations introduce visual\ndiversity that improves the performance of the downstream classifier. We validate\naugmentation-conditioning on a total of five established long-tail and few-shot im-\nage classification benchmarks and show that leveraging augmentations to condition\nthe generation process results in consistent improvements over the state-of-the-art\non the long-tailed benchmark and remarkable gains in extreme few-shot regimes of\nthe remaining four benchmarks. These results constitute an important step towards\neffectively leveraging synthetic data for downstream training."
    },
    {
        "title": "HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models",
        "link_suffix": "/forum?id=ZYwLfi50GI",
        "link": "https://openreview.net/forum?id=ZYwLfi50GI",
        "pdf_link": "https://openreview.net/pdf?id=ZYwLfi50GI",
        "keywords": "human motion generation, human-object interaction",
        "abstract": "We address the problem of generating realistic 3D human object interactions (HOIs) driven by textual prompts. To this end, we take a modular design and decompose the complex task into simpler subtasks. We first develop a dual-branch diffusion model (DBDM) to generate both human and object motions conditioned on the input text, and encourage coherent motions by a cross-attention communication module between the human and object motion generation branches. We also develop an affordance prediction diffusion model (APDM) to predict the contacting area between the human and object during the interactions driven by the textual prompt. The APDM is independent of the results by the DBDM and thus can correct potential errors by the latter. Moreover, it stochastically generates the contacting points to diversify the generated motions. Finally, we incorporate the estimated contacting points into the classifier-guidance to achieve accurate and close contact between humans and objects. To train and evaluate our approach, we annotate BEHAVE dataset with text descriptions. Experimental results on BEHAVE and OMOMO demonstrate that our approach produces realistic HOIs with various interactions and different types of objects."
    },
    {
        "title": "Learning on Graphs with Large Language Models (LLMs): A Deep Dive into Model Robustness",
        "link_suffix": "/forum?id=FhhH14jso4",
        "link": "https://openreview.net/forum?id=FhhH14jso4",
        "pdf_link": "https://openreview.net/pdf?id=FhhH14jso4",
        "keywords": "Large Language Models, Learning on Graphs, Robustness",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various natural language processing tasks. Recently, several LLMs-based pipelines have been developed to enhance learning on graphs with text attributes, showcasing promising performance. However, graphs are well-known to be susceptible to adversarial attacks and it remains unclear whether LLMs exhibit robustness in learning on graphs. To address this gap, our work aims to explore the potential of LLMs in the context of adversarial attacks on graphs. Specifically, we investigate the robustness against graph structural and textual perturbations in terms of two dimensions: LLMs-as-Enhancers and LLMs-as-Predictors. Through extensive experiments, we find that, compared to shallow models, both LLMs-as-Enhancers and LLMs-as-Predictors offer superior robustness against structural and textual attacks. Based on these findings, we carried out additional analyses to investigate the underlying causes. Furthermore, we have made our benchmark library openly available to facilitate quick and fair evaluations, and to encourage ongoing innovative research in this field."
    },
    {
        "title": "Effective and Efficient Time-Varying Counterfactual Prediction with State-Space Models",
        "link_suffix": "/forum?id=yheQRc5xWB",
        "link": "https://openreview.net/forum?id=yheQRc5xWB",
        "pdf_link": "https://openreview.net/pdf?id=yheQRc5xWB",
        "keywords": "Time Series; State-space Models; Treatment Effect Estimation",
        "abstract": "Time-varying counterfactual prediction (TCP) from observational data supports the answer of when and how to assign multiple sequential treatments, yielding importance in various applications. Despite the progress achieved by recent advances, e.g., LSTM or Transformer based causal approaches, their capability of capturing interactions in long sequences remains to be improved in both prediction performance and running efficiency. In parallel with the development of TCP, the success of the state-space models (SSMs) has achieved remarkable progress toward long-sequence modeling with saved running time. Consequently, studying how Mamba simultaneously benefits the effectiveness and efficiency of TCP  becomes a compelling research direction. In this paper, we propose to exploit advantages of the SSMs to tackle the TCP task, by introducing a counterfactual Mamba model with Covariate-based Decorrelation towards Selective Parameters (Mamba-CDSP). Motivated by the over-balancing problem in TCP of the direct covariate balancing methods, we propose to de-correlate between the current treatment and the representation of historical covariates, treatments, and outcomes, which can mitigate the confounding bias while preserve more covariate information. In addition, we show that the overall de-correlation in TCP is equivalent to regularizing the selective parameters of Mamba over each time step, which leads our approach to be effective and lightweight. We conducted extensive experiments on both synthetic and real-world datasets, demonstrating that Mamba-CDSP not only outperforms baselines by a large margin, but also exhibits prominent running efficiency."
    },
    {
        "title": "Number Cookbook: Number Understanding of Language Models and How to Improve It",
        "link_suffix": "/forum?id=BWS5gVjgeY",
        "link": "https://openreview.net/forum?id=BWS5gVjgeY",
        "pdf_link": "https://openreview.net/pdf?id=BWS5gVjgeY",
        "keywords": "number understanding, large language model, reasoning",
        "abstract": "Large language models (LLMs) can solve an increasing number of complex tasks while making surprising mistakes in basic numerical understanding and processing (such as $9.11 > 9.9$). The latter ability is essential for tackling complex arithmetic and mathematical problems and serves as a foundation for most reasoning tasks, but previous work paid less attention to it or only discussed several restricted tasks (like integer addition). In this paper, we comprehensively investigate thenumerical understanding and processing ability(NUPA) of LLMs. Firstly, we introduce a test suite covering four common numerical representations and 17 distinct numerical tasks in four major categories, resulting in 41 meaningful combinations in total. These tasks are derived from primary and secondary education curricula, encompassing nearly all everyday numerical understanding and processing scenarios, and the rules of these tasks are very simple and clear.\nWe find that current LLMs exhibit a considerable probability of error in many of them. To address this, we analyze and evaluate techniques designed to enhance NUPA, identifying three key factors that influence NUPA of LLMs. We also perform finetuning of practical-scale LLMs on our defined NUPA tasks and find that a naive finetuning improves the performance but these tricks cannot be directly used to finetune an already-trained model. We further explore the impact of chain-of-thought techniques on NUPA. Our work takes an initial step towards understanding and improving NUPA of LLMs."
    },
    {
        "title": "Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows",
        "link_suffix": "/forum?id=MKP1g8wU0P",
        "link": "https://openreview.net/forum?id=MKP1g8wU0P",
        "pdf_link": "https://openreview.net/pdf?id=MKP1g8wU0P",
        "keywords": "operator learning, neural operators, Navier-Stokes, PDE, partial differential equations, computational fluid dynamics",
        "abstract": "Recent advancements in operator-type neural networks have shown promising results in approximating the solutions of spatiotemporal Partial Differential Equations (PDEs). However, these neural networks often entail considerable training expenses, and may not always achieve the desired accuracy required in many scientific and engineering disciplines. \nIn this paper, we propose a new learning framework to address these issues. A new spatiotemporal adaptation is proposed to generalize any Fourier Neural Operator (FNO) variant to learn maps between Bochner spaces, which is capable of performing an arbitrary-lengthed temporal super-resolution for the first time. \nTo better exploit this capacity, a new paradigm is proposed to refine the commonly adopted end-to-end neural operator training and evaluations with the help from the wisdom from traditional numerical PDE theory and techniques. \nSpecifically, in the learning problems for the turbulent flow modeling by the Navier-Stokes Equations (NSE), the proposed paradigm trains an FNO only for a few epochs. Then, only the newly proposed spatiotemporal spectral convolution layer is fine-tuned without the frequency truncation. The fine-tuning loss function uses a negative Sobolev norm for the first time in operator learning, defined through a reliable functional-type a posteriori error estimator whose evaluation is exact thanks to the Parseval identity. Moreover, unlike the difficult nonconvex optimization problems in the end-to-end training, this fine-tuning loss is convex. \nNumerical experiments on commonly used NSE benchmarks demonstrate significant improvements in both computational efficiency and accuracy, compared to end-to-end evaluation and traditional numerical PDE solvers."
    },
    {
        "title": "Fully Quanvolutional Networks for Time Series Classification",
        "link_suffix": "/forum?id=UG62fwmOxZ",
        "link": "https://openreview.net/forum?id=UG62fwmOxZ",
        "pdf_link": "https://openreview.net/pdf?id=UG62fwmOxZ",
        "keywords": "quanvolutional neural networks, quanvolution, time series classification",
        "abstract": "Quanvolutional neural networks have shown promise in areas like computer vision and time series analysis. However, their applicability to multi-dimensional and diverse data types remains underexplored in the literature. Existing models are predominantly hybrid, with minimal quantum involvement and a heavy reliance on classical layers. To address previous shortcomings related to scalability and data encoding inefficiencies, we introduce a new quanvolution algorithm. Focusing on time series data, we propose the Quanv1D layer, which is trainable, modular, and capable of handling time series of arbitrary dimensions. To minimize qubit usage, we employ amplitude embedding (which requires only $log_2n$ qubits) instead of linear mapping methods like angle embedding, which demands an impractically large number of qubits even for simulations. In addition to this new layer, we present a new architecture called Fully Quanvolutional Networks (FQN), composed entirely of Quanv1D layers. We tested this lightweight model on 12 UEA time series classification datasets and compared it against both quantum and classical models, including the current state-of-the-art ModernTCN. On most datasets, FQN achieved near state-of-the-art accuracy and even outperformed the compared models on some, all while using a fraction of parameters. Moreover, we observed that FQN exhibits a self-regularizing property, leading to improved training performance."
    },
    {
        "title": "PrivacyRestore: Privacy-Preserving Inference in Large Language Models via Privacy Removal and Restoration",
        "link_suffix": "/forum?id=MyRcW53CCC",
        "link": "https://openreview.net/forum?id=MyRcW53CCC",
        "pdf_link": "https://openreview.net/pdf?id=MyRcW53CCC",
        "keywords": "Privacy Protection, Activation Steering, LLM Inference",
        "abstract": "The widespread usage of online Large Language Models (LLMs) inference services has raised significant privacy concerns about the potential exposure of private information in user inputs to malicious eavesdroppers. Existing privacy protection methods for LLMs suffer from either insufficient privacy protection, performance degradation, or large inference time overhead. To address these limitations, we propose PrivacyRestore, a plug-and-play method to protect the privacy of user inputs during LLM inference. The server first trains restoration vectors for each privacy span and then release to clients. Privacy span is defined as a contiguous sequence of tokens within a text that contain private information. The client then aggregate restoration vectors of all privacy spans in the input into a single meta restoration vector which is later sent to the server side along with the input without privacy spans.The private information is restored via activation steering during inference. Furthermore, we prove that PrivacyRestore inherently prevents the linear growth of the privacy budget.We create three datasets, covering medical and legal domains, to evaluate the effectiveness of privacy preserving methods. The experimental results show that PrivacyRestore effectively protects private information and maintain acceptable levels of performance and inference overhead."
    },
    {
        "title": "TAGA: Text-Attributed Graph Self-Supervised Learning by Synergizing Graph and Text Mutual Transformations",
        "link_suffix": "/forum?id=OuxdVB6g1F",
        "link": "https://openreview.net/forum?id=OuxdVB6g1F",
        "pdf_link": "https://openreview.net/pdf?id=OuxdVB6g1F",
        "keywords": "Text-attributed graphs, self-supervised learning, representation learning",
        "abstract": "Text-Attributed Graphs (TAGs) enhance graph structures with natural language descriptions, enabling detailed representation of data and their relationships across a broad spectrum of real-world scenarios. Despite the potential for deeper insights, existing TAG representation learning primarily omit the semantic relationship among node texts, and mostly relies on supervised methods, necessitating extensive labeled data and limiting applicability across diverse contexts. This paper introduces a new self-supervised learning framework, Text-And-Graph Multi-View Alignment (TAGA), which overcomes these constraints by integrating TAGs' structural and semantic dimensions. TAGA constructs two complementary views: Text-of-Graph view, which organizes node texts into structured documents based on graph topology, and the Graph-of-Text view, which converts textual nodes and connections into graph data. By aligning representations from both views, TAGA captures joint textual and structural information. In addition, a novel structure-preserving random walk algorithm is proposed for efficient training on large-sized TAGs. Our framework demonstrates strong performance in zero-shot and few-shot scenarios across eight real-world datasets."
    },
    {
        "title": "Learning a Compact, Parcel-independent Representation of the fMRI Functional Connectivity",
        "link_suffix": "/forum?id=9ppkh7L4eQ",
        "link": "https://openreview.net/forum?id=9ppkh7L4eQ",
        "pdf_link": "https://openreview.net/pdf?id=9ppkh7L4eQ",
        "keywords": "dimensionality reduction, fMRI, variational autoencoder, performance evaluation, application",
        "abstract": "Functional connectivity in functional magnetic resonance imaging (fMRI) data is often calculated at the level of area parcels. Given the data's low-dimensional nature, we posit a substantial degree of redundancy in these representations. Moreover, establishing correspondence across different individuals poses a significant challenge. We hypothesize that it is possible to learn a compact representation of the functional connectivity data to enhance computational efficiency without losing the essential structure of the original data. Furthermore, comparable functional connectivity across individuals with differing area parcel definitions would be more straightforward when projected to a common low-dimensional latent space. Our analysis, based on various performance benchmarks, indicates that the low-dimensional latent space learned from the functional connectivity of one dataset generalizes well to another dataset. Notably, the latent space learned using a variational autoencoder represents the data more effectively than linear methods at lower dimensions. However, at higher dimensions, the differences between linear and nonlinear dimensionality reduction methods diminish, rendering them comparable to the parcel representation with 333 dimensions. Our findings highlight the potential of employing an established transformation to obtain a low-dimensional latent representation in future functional connectivity research, thereby promoting reproducibility and supporting open science objectives."
    },
    {
        "title": "Latent Diffusion with LLMs for Reasoning",
        "link_suffix": "/forum?id=Xe6UmKMInx",
        "link": "https://openreview.net/forum?id=Xe6UmKMInx",
        "pdf_link": "https://openreview.net/pdf?id=Xe6UmKMInx",
        "keywords": "Large Language Models, Latent Diffusion, Reasoning",
        "abstract": "Despite the widespread adoption of large language models with hundreds of billions of parameters, these models still struggle on complex reasoning benchmarks. In this paper, we argue that the autoregressive nature of current language models are not suited for reasoning due to fundamental limitations, and that reasoning requires slow accumulation of knowledge through time. We show that combining latent diffusion models with an encoder-decoder transformer architecture provides a scalable way to address some of the fundamental shortcomings posed by autoregressive models. Diffusion models can arrive at predictions through many forward passes in latent space, and their reasoning is not handicapped by the order of the tokens in the dataset. Through our experiments, we show that latent diffusion language models is a feasible approach towards scalable language models that have general complex reasoning abilities."
    },
    {
        "title": "Hi-Gaussian: Hierarchical Gaussians under Normalized Spherical Projection for Single-View 3D Reconstruction",
        "link_suffix": "/forum?id=L3WnnnBRdu",
        "link": "https://openreview.net/forum?id=L3WnnnBRdu",
        "pdf_link": "https://openreview.net/pdf?id=L3WnnnBRdu",
        "keywords": "Hierarchical Gaussian Sampling, Normalized Spherical Projection, Single-View 3D Reconstruction, Gaussian Splatting",
        "abstract": "Single-view 3D reconstruction is a fundamental problem in computer vision, having a significant impact on downstream tasks such as Autonomous Driving, Virtual Reality and Augment Reality. However, existing single-view reconstruction methods are unable to reconstruct the regions outside the input field-of-view or the areas occluded by visible parts. In this paper, we propose Hi-Gaussian, which employs feed-forward 3D Gaussians for efficient and generalizable single-view 3D reconstruction. A Normalized Spherical Projection module is introduced following an Encoder-Decoder network in our model, assigning a larger range to the transformed spherical coordinates, which can enlarge the field of view during scene reconstruction. Besides, to reconstruct occluded regions behind the visible part, we introduce a novel Hierarchical Gaussian Sampling strategy, utilizing two layers of Gaussians to hierarchically represent 3D scenes. We first use a pre-trained monocular depth estimation model to provide depth initialization for $leader$ Gaussians, and then leverage the $leader$ Gaussians to estimate the distribution followed by $follower$ Gaussians. Extensive experiments show that our method outperforms other methods for scene reconstruction and novel view synthesis, on both outdoor and indoor datasets."
    },
    {
        "title": "NN-ResDMD: Learning Koopman Representations for Complex Dynamics with Spectral Residuals",
        "link_suffix": "/forum?id=53xxT3LwJB",
        "link": "https://openreview.net/forum?id=53xxT3LwJB",
        "pdf_link": "https://openreview.net/pdf?id=53xxT3LwJB",
        "keywords": "Koopman operator, data driven dynamical system, dynamic mode decomposition",
        "abstract": "Analyzing long-term behaviors in high-dimensional nonlinear dynamical systems remains a significant challenge. The Koopman operator framework has emerged as a powerful tool to address this issue by providing a globally linear perspective on nonlinear dynamics. However, existing methods for approximating the Koopman operator and its spectral components, particularly in large-scale systems, often lack robust theoretical guarantees.\nResidual Dynamic Mode Decomposition (ResDMD) introduces a spectral residual measure to assess the convergence of the estimated Koopman spectrum, which helps filter out spurious spectral components. \nNevertheless, it depends on pre-computed spectra, thereby inheriting their inaccuracies. \nTo overcome its limitations, we introduce the Neural Network-ResDMD (NN-ResDMD), a method that directly estimates Koopman spectral components by minimizing the spectral residual. By leveraging neural networks, NN-ResDMD automatically identifies the optimal basis functions of the Koopman invariant subspace, eliminating the need for manual selection and improving the reliability of the analysis.\nExperiments on physical and biological systems demonstrate that NN-ResDMD significantly improves both accuracy and scalability, making it an effective tool for analyzing complex dynamical systems."
    },
    {
        "title": "Adaptive backtracking for fast optimization",
        "link_suffix": "/forum?id=SrGP0RQbYH",
        "link": "https://openreview.net/forum?id=SrGP0RQbYH",
        "pdf_link": "https://openreview.net/pdf?id=SrGP0RQbYH",
        "keywords": "Optimization, backtracking line search, Armijo condition, descent lemma, adaptive optimization methods",
        "abstract": "Backtracking line search is foundational in numerical optimization. \nThe basic idea is to adjust the step size of an algorithm by a {\\em constant} factor until some chosen criterion (e.g. Armijo, Goldstein, Descent Lemma) is satisfied. \nWe propose a new way for adjusting step sizes, replacing the constant factor used in regular backtracking with one that takes into account the degree to which the chosen criterion is violated, without additional computational burden. \nWe perform a variety of experiments on over fifteen real world datasets, which confirm that adaptive backtracking often leads to significantly faster optimization.\nFor convex problems, we prove adaptive backtracking requires fewer adjustments to produce a feasible step size than regular backtracking does for two popular line search criteria: the Armijo condition and the descent lemma.\nFor nonconvex smooth problems, we prove adaptive backtracking enjoys the same guarantees of regular backtracking."
    },
    {
        "title": "On Memorization of Large Language Models in Logical Reasoning",
        "link_suffix": "/forum?id=5sQiK2qTGa",
        "link": "https://openreview.net/forum?id=5sQiK2qTGa",
        "pdf_link": "https://openreview.net/pdf?id=5sQiK2qTGa",
        "keywords": "LLM, memorization, logical reasoning, perturbation, knights and knaves",
        "abstract": "Large language models (LLMs) achieve good performance on challenging reasoning benchmarks, yet could also make basic reasoning mistakes. This contrasting behavior is puzzling when it comes to understanding the mechanisms behind LLMs' reasoning capabilities. \nOne hypothesis is that the increasingly high and nearly saturated performance on common reasoning benchmarks could be due to the memorization of similar problems.\nIn this paper, we systematically investigate this hypothesis with a quantitative measurement of memorization in reasoning tasks, using a dynamically generated logical reasoning benchmark based on Knights and Knaves (K&K) puzzles. We found that LLMs could interpolate the training puzzles (achieving near-perfect accuracy) after fine-tuning, yet fail when those puzzles are slightly perturbed, suggesting that the models heavily rely on memorization to solve those training puzzles. \nOn the other hand, we show that while fine-tuning leads to heavy memorization, it also consistently improves generalization performance. In-depth analyses with perturbation tests, cross difficulty-level transferability, probing model internals, and fine-tuning with wrong answers suggest that the LLMs learn to reason on K&K puzzles despite training data memorization. This phenomenon indicates that LLMs exhibit a complex interplay between memorization and genuine reasoning abilities.\nFinally, our analysis with per-sample memorization score sheds light on how LLMs switch between reasoning and memorization in solving logical puzzles."
    },
    {
        "title": "Scalable Approximate Message Passing for Bayesian Neural Networks",
        "link_suffix": "/forum?id=EIwGR0w8VG",
        "link": "https://openreview.net/forum?id=EIwGR0w8VG",
        "pdf_link": "https://openreview.net/pdf?id=EIwGR0w8VG",
        "keywords": "Message Passing, Bayesian Neural Networks, Uncertainty Estimation, Factor Graphs",
        "abstract": "Bayesian neural networks (BNNs) offer the potential for reliable uncertainty quantification, data efficiency, and interpretability, which are critical for trustworthy AI in high-stakes domains. However, existing methods often struggle with issues such as overconfidence, hyperparameter sensitivity, and posterior collapse, leaving room for alternative approaches. In this work, we advance message passing (MP) for BNNs and present a novel framework that models the predictive posterior as a factor graph. To the best of our knowledge, our framework is the first MP method that handles convolutional neural networks and avoids double-counting training data, a limitation of previous MP methods that causes overconfidence. We evaluate our approach on MNIST; compared to stochastic gradient descent (SGD), our method achieves superior relative calibration, better out-of-distribution recognition, and similar or better accuracy. Our approach particularly excels in data-constrained settings: The LeNet-5 architecture trained with MP achieves 94.62% accuracy on MNIST with only 640 samples, compared to 22.15% for SGD. On synthetic data, we validate the uncertainty estimates and observe a strong correlation (0.9) between posterior credible intervals and its probability of covering the true data-generating function outside the training range. While our method scales to an MLP with 5.6 million parameters, further improvements are necessary to match the scale and performance of state-of-the-art variational inference methods."
    },
    {
        "title": "Breach By A Thousand Leaks: Unsafe Information Leakage in 'Safe' AI Responses",
        "link_suffix": "/forum?id=8Rov0fjpOL",
        "link": "https://openreview.net/forum?id=8Rov0fjpOL",
        "pdf_link": "https://openreview.net/pdf?id=8Rov0fjpOL",
        "keywords": "AI Safety, Information Theory",
        "abstract": "Vulnerability of Frontier language models to misuse and jailbreaks has prompted the development of safety measures like filters and alignment training in an effort to ensure safety through robustness to adversarially crafted prompts. We assert that robustness is fundamentally insufficient for ensuring safety goals, and current defenses and evaluation methods fail to account for risks of dual-intent queries and their composition for malicious goals. To quantify these risks, we introduce a new safety evaluation framework based on \\textit{impermissible information leakage} of model outputs and demonstrate how our proposed question-decomposition attack can extract dangerous knowledge from a censored LLM more effectively than traditional jailbreaking. Underlying our proposed evaluation method is a novel information-theoretic threat model of \\textit{inferential adversaries}, distinguished from \\textit{security adversaries}, such as jailbreaks, in that success is measured by inferring impermissible knowledge from victim outputs as opposed to forcing explicitly impermissible outputs from the victim. Through our information-theoretic framework, we show that to ensure safety against inferential adversaries, defense mechanisms must ensure \\textit{information censorship}, bounding the leakage of impermissible information. However, we prove that such defenses inevitably incur a safety-utility trade-off."
    },
    {
        "title": "Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models",
        "link_suffix": "/forum?id=UL95EpgrlS",
        "link": "https://openreview.net/forum?id=UL95EpgrlS",
        "pdf_link": "https://openreview.net/pdf?id=UL95EpgrlS",
        "keywords": "Large Image-Language Models, Evaluation Benchmark, Instruction Tuning Methods, Object Hallucinations, Visual Grounding",
        "abstract": "Large Vision and Language Models have enabled significant advances in fully supervised and zero-shot vision tasks. These large pre-trained architectures serve as the baseline to what is currently known as Instruction Tuning Large Vision and Language models (IT-LVLMs). IT-LVLMs are general-purpose multi-modal assistants whose responses are modulated by natural language instructions and arbitrary visual data. Despite this versatility, IT-LVLM effectiveness in fundamental computer vision problems remains unclear, primarily due to the absence of a standardized evaluation benchmark. This paper introduces a Multi-modal Evaluation Benchmark named MERLIM, a scalable test-bed to assess the performance of IT-LVLMs on fundamental computer vision tasks. MERLIM contains over 300K image-question pairs and has a strong focus on detecting cross-modal \u201challucination\u201d events in IT-LVLMs. Our results show that state-of-the-art IT-LVMLs are still limited at identifying fine-grained visual concepts, object hallucinations are common across tasks, and their results are strongly biased by small variations in the input query, even if the queries have the very same semantics. Our findings also suggest that these models lack direct visual groundings, but can still make adequate guesses from global visual patterns or textual biases contained in the LLM component."
    },
    {
        "title": "Efficient Training Framework for Realistic Sensory-Motor Integration in a Biologically Constrained Barrel Cortex Model",
        "link_suffix": "/forum?id=UvfI4grcM7",
        "link": "https://openreview.net/forum?id=UvfI4grcM7",
        "pdf_link": "https://openreview.net/pdf?id=UvfI4grcM7",
        "keywords": "Barrel cortex, biophysical modeling, sensory-motor integration, recurrent spiking neural networks, efficient training",
        "abstract": "The brain's ability to transform sensory inputs into motor functions is central to neuroscience and crucial for the development of embodied intelligence. Sensory-motor integration involves complex neural circuits, diverse neuronal types, and intricate intercellular connections. Bridging the gap between biological realism and behavioral functionality presents a formidable challenge. In this study, we focus on the columnar structure of the superficial layers of mouse barrel cortex as a model system. We constructed a model comprising 4,218 neurons across 13 neuronal subtypes, with neural distribution and connection strengths constrained by anatomical experimental findings. A key innovation of our work is the development of an efficient training algorithm tailored for this biologically constrained model. Additionally, we converted an existing simulated whisker sweep dataset into a spiking-based format, enabling our network to be trained and tested on neural signals that more closely mimic those observed in biological systems. The results of object discrimination utilizing whisker signals demonstrate that our barrel cortex model, grounded in biological realism, achieves a classification accuracy exceeds classical convolutional neural networks (CNNs), recurrent neural networks (RNNs), and long short-term memory networks (LSTMs), by an average of 8.6%, and is on par with recent spiking neural networks (SNNs) in performance. Interestingly, a whisker deprivation experiment, designed in accordance with neuroscience practices, further validates the perceptual capabilities of our model in behavioral tasks. Critically, it offers significant biological interpretability: post-training analysis reveals that neurons within our model exhibit firing characteristics and distribution patterns similar to those observed in the actual neuronal systems of the barrel cortex. This study advances our understanding of neural processing in the barrel cortex and exemplifies how integrating detailed biological structures into neural network models can enhance both scientific inquiry and artificial intelligence applications."
    },
    {
        "title": "Late Chunking: Contextual Chunk Embeddings Using Long-Context Embedding Models",
        "link_suffix": "/forum?id=74QmBTV0Zf",
        "link": "https://openreview.net/forum?id=74QmBTV0Zf",
        "pdf_link": "https://openreview.net/pdf?id=74QmBTV0Zf",
        "keywords": "text embedding, information retrieval, chunking, contrastive learning",
        "abstract": "Many use cases require retrieving smaller portions of text, and dense vector-based retrieval systems often perform better with shorter text segments, as the semantics are less likely to be \"over-compressed\" in the embeddings. Consequently, practitioners often split text documents into smaller chunks and encode them separately. However, chunk embeddings created in this way can lose contextual information from surrounding chunks, resulting in sub-optimal representations. In this paper, we introduce a novel method called \"late chunking, which leverages long context embedding models to first embed all tokens of the long text, with chunking applied after the transformer model and just before mean pooling - hence the term \"late\" in its naming. The resulting chunk embeddings capture the full contextual information, leading to superior results across various retrieval tasks.  The method is generic enough to be applied to a wide range of long-context embedding models and works without additional training. To further increase the effectiveness of late chunking, we propose a dedicated fine-tuning approach for embedding models."
    },
    {
        "title": "Concealing Backdoors in Federated Learning by Trigger-Optimized Data Poisoning",
        "link_suffix": "/forum?id=b4b6rERW4G",
        "link": "https://openreview.net/forum?id=b4b6rERW4G",
        "pdf_link": "https://openreview.net/pdf?id=b4b6rERW4G",
        "keywords": "Data Poisoning, Backdoor Attack, Federated Learning",
        "abstract": "Federated Learning (FL) is a decentralized machine learning method that enables participants to collaboratively train a model without  sharing their private data. Despite its privacy and scalability benefits, FL is susceptible to backdoor attacks, where adversaries poison the local training data of a subset of clients using backdoor triggers, aiming to make the aggregated model produce malicious results when the same backdoor conditions are met by an inference-time input. Existing backdoor attacks in FL suffer from common deficiencies: fixed trigger patterns and reliance on the assistance of model poisoning. State-of-the-art defenses based on analyzing clients' model updates exhibit a good defense performance on these attacks because of the significant divergence between malicious and benign client model updates. To effectively conceal malicious model updates among benign ones, we propose DPOT, a backdoor attack strategy in FL that dynamically constructs backdoor objectives by optimizing a backdoor trigger, making backdoor data have minimal effect on model updates. We provide theoretical justifications for DPOT's attacking principle and display experimental results showing that DPOT, via only a data-poisoning attack, effectively undermines state-of-the-art defenses and outperforms existing backdoor attack techniques on various datasets."
    },
    {
        "title": "The Complexity of Two-Team Polymatrix Games with Independent Adversaries",
        "link_suffix": "/forum?id=9VGTk2NYjF",
        "link": "https://openreview.net/forum?id=9VGTk2NYjF",
        "pdf_link": "https://openreview.net/pdf?id=9VGTk2NYjF",
        "keywords": "algorithmic game theory, Nash equilibrium, minmax optimization",
        "abstract": "Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the computation of Nash equilibria in such games where each pair of players plays either a zero-sum or a coordination game. We are particularly interested in the setting where players can be grouped into a small number of teams of identical interest. While the three-team version of the problem is known to be PPAD-complete, the complexity for two teams has remained open. Our main contribution is to prove that the two-team version remains hard, namely it is CLS-hard. Furthermore, we show that this lower bound is tight for the setting where one of the teams consists of multiple independent adversaries. On the way to obtaining our main result, we prove hardness of finding any stationary point in the simplest type of non-convex-concave min-max constrained optimization problem, namely for a class of bilinear polynomial objective functions."
    },
    {
        "title": "ATM: Improving Model Merging by Alternating Tuning and Merging",
        "link_suffix": "/forum?id=lNtio1tdbL",
        "link": "https://openreview.net/forum?id=lNtio1tdbL",
        "pdf_link": "https://openreview.net/pdf?id=lNtio1tdbL",
        "keywords": "model merging, task arithmetic, multi-task learning, task vectors",
        "abstract": "Model merging has recently emerged as a cost-efficient paradigm for Multi-task Learning (MTL). Among merging solutions, Task Arithmetic \\citep{task-vectors} stands out for its simplicity and effectiveness. In this paper, we start by motivating the effectiveness of task vectors with their relation to multi-task gradients. We show that in the single epoch scenario, task vectors are exactly equivalent to \n gradients obtained by performing gradient descent in a multi-task setting, and still approximate the latter with further epochs. We further strengthen the explanation by showing that task vectors work best when equality is maintained and motivate their effectiveness in the general case by showing that most of the contribution in the total update is determined by the gradient of the first epoch. Guided by this parallel, we propose viewing model merging as a single step in an iterative process that Alternates between Tuning and Merging (ATM). Acting as a midpoint between model merging and multi-task gradient descent, ATM obtains state-of-the-art results with the same data and computing requirements. We first extensively evaluate our approach under diverse settings, demonstrating state-of-the-art performance, leading by an accuracy of up to 19% in computer vision and 20% in NLP over the best baselines. We then motivate its effectiveness empirically, showing increased orthogonality between task vectors and, theoretically, proving it to minimize an upper bound to the loss obtained by finetuning jointly on all tasks."
    },
    {
        "title": "RotPruner: Large Language Model Pruning in Rotated Space",
        "link_suffix": "/forum?id=wV9iMiyQcc",
        "link": "https://openreview.net/forum?id=wV9iMiyQcc",
        "pdf_link": "https://openreview.net/pdf?id=wV9iMiyQcc",
        "keywords": "network pruning, sparsity, Large Language Model",
        "abstract": "Network pruning is a crucial technique for compressing large language models with billions of parameters, aiming to reduce memory and computational costs with minimal performance degradation. However, existing pruning methods for LLMs often focus on heuristic metrics or layer-wise reconstruction losses, neglecting the impact on the overall model output, which can lead to suboptimal result. Additionally, these methods operate directly on the original weight and activation spaces, which may not be ideal for pruning. In this paper, we propose that the original parameter space is not optimal for pruning and present a novel training-based pruning framework called RotPruner. RotPruner rotates the spaces of weight matrices and activations in linear layers, and applies existing pruning methods in a rotated space that is more suitable for pruning. We introduce an efficient algorithm to identify an appropriate rotation that preserves the performance of pruned LLMs. RotPruner is capable of integrating with other pruning methods and supporting unstructured, semi-structured, and structured pruning. We evaluate RotPruner on several large language models, including OPT, LLaMA-2, and LLaMA-3, and demonstrate state-of-the-art performance on both language modeling and zero-shot tasks."
    }
]