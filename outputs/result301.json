[
    {
        "title": "Semantic Equitable Clustering: You Only Iterate Once to Cluster Vision Tokens",
        "link_suffix": "/forum?id=oW7T3p5wE1",
        "link": "https://openreview.net/forum?id=oW7T3p5wE1",
        "pdf_link": "https://openreview.net/pdf?id=oW7T3p5wE1",
        "keywords": "Equi-partition Clustering, Efficient Vision Transformer, multimodal large language models (MLLM)",
        "abstract": "The Vision Transformer (ViT) has gained prominence for its superior relational modeling prowess. However, its global attention mechanism's quadratic complexity poses substantial computational burdens. A common remedy spatially groups tokens for self-attention, reducing computational requirements. Nonetheless, this strategy neglects semantic information in tokens, possibly scattering semantically-linked tokens across distinct groups, thus compromising the efficacy of self-attention intended for modeling inter-token dependencies. Motivated by these insights,  we introduce a fast and balanced clustering method, namedSemanticEquitableClustering (SEC).  SEC clusters tokens based on their global semantic relevance in an efficient, straightforward manner.  In contrast to traditional clustering methods requiring multiple iterations, our method achieves token clustering in a single pass. Additionally, SEC regulates the number of tokens per cluster, ensuring a balanced distribution for effective parallel processing on  current computational platforms without necessitating further optimization. Capitalizing on SEC, we propose a versatile vision backbone, SECViT. Comprehensive experiments in image classification, object detection, instance segmentation, and semantic segmentation validate to the effectiveness of SECViT. Remarkably, SECViT attains an impressive84.3%image classification accuracy with only27Mparameters and4.6GFLOPs, without the need for for additional supervision or data. Moreover, SEC can be conveniently and swiftly applied to multimodal large language models (MLLM), such as LLaVA, to serve as a vision language connector,\neffectively accelerating the model\u2019s efficiency while maintaining unchanged or better performance."
    },
    {
        "title": "Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models",
        "link_suffix": "/forum?id=c9z65sDx6M",
        "link": "https://openreview.net/forum?id=c9z65sDx6M",
        "pdf_link": "https://openreview.net/pdf?id=c9z65sDx6M",
        "keywords": "Diffusion Models, PIC Simulations, Synthetic Data Generation",
        "abstract": "The rapid development of AI highlights the pressing need for sustainable energy, a critical global challenge for decades. Nuclear fusion, generally seen as an ultimate solution, has been the focus of intensive research for nearly a century, with investments reaching hundreds of billions of dollars. Recent advancements in Inertial Confinement Fusion have drawn significant attention to fusion research, in which Laser-Plasma Interaction (LPI) is critical for ensuring fusion stability and efficiency. However, the complexity of LPI upon fusion ignition makes analytical approaches impractical, leaving researchers depending on extremely computation-demanding Particle-in-Cell (PIC) simulations to generate data, presenting a significant bottleneck to advancing fusion research. In response, this work introduces Diff-PIC, a novel framework that leverages conditional diffusion models as a computationally efficient alternative to PIC simulations for generating high-fidelity scientific LPI data. In this work, physical patterns captured by PIC simulations are distilled into diffusion models associated with two tailored enhancements: (1) To effectively capture the complex relationships between physical parameters and corresponding outcomes, the parameters are encoded in a physically-informed manner. (2) To further enhance efficiency while maintaining high fidelity and physical validity, the rectified flow technique is employed to transform our model into a one-step conditional diffusion model. Experimental results show that Diff-PIC achieves 16,200 times speedup compared to traditional PIC on a 100 picosecond simulation, with an average reduction in MAE / RMSE / FID of 59.21% / 57.15% / 39.46% with respect to two other SOTA data generation approaches."
    },
    {
        "title": "NRGBoost: Energy-Based Generative Boosted Trees",
        "link_suffix": "/forum?id=wQHyjIZ1SH",
        "link": "https://openreview.net/forum?id=wQHyjIZ1SH",
        "pdf_link": "https://openreview.net/pdf?id=wQHyjIZ1SH",
        "keywords": "Energy-Based Models, Generative Models, Gradient Boosting, Tabular Data",
        "abstract": "Despite the rise to dominance of deep learning in unstructured data domains, tree-based methods such as Random Forests (RF) and Gradient Boosted Decision Trees (GBDT) are still the workhorses for handling discriminative tasks on tabular data. We explore generative extensions of these popular algorithms with a focus on explicitly modeling the data density (up to a normalization constant), thus enabling other applications besides sampling. \nAs our main contribution we propose an energy-based generative boosting algorithm that is analogous to the second order boosting implemented in popular packages like XGBoost. We show that, despite producing a generative model capable of handling inference tasks over any input variable, our proposed algorithm can achieve similar discriminative performance to GBDT on a number of real world tabular datasets, outperforming alternative generative approaches. At the same time, we show that it is also competitive with neural network based models for sampling."
    },
    {
        "title": "Customizing Reinforcement Learning Agent with Multi-Objective Preference Control",
        "link_suffix": "/forum?id=j46zZVzVVQ",
        "link": "https://openreview.net/forum?id=j46zZVzVVQ",
        "pdf_link": "https://openreview.net/pdf?id=j46zZVzVVQ",
        "keywords": "reinforcement learning, multi-objective optimization, deep reinforcement learning",
        "abstract": "Practical reinforcement learning (RL) usually requires agents to be optimized for multiple potentially conflicting criteria, e.g. speed vs. safety. \nAlthough Multi-Objective RL (MORL) algorithms have been studied in previous works, their trained agents often lack precise controllability of the delicate trade-off among multiple objectives. Hence, the resulting agent is not versatile in aligning with customized requests from different users. \nTo bridge the gap, we develop ``Preference control (PC) RL'', which aims to train a meta-policy that takes user preference as input controlling the generation of a trajectory on the Pareto frontier adhering to the preference. To this end, we train a preference-conditioned meta-policy by our proposed preference-regularized MORL algorithm. The achieved meta-policy performs as a multi-objective optimizer that can produce user-desired solutions on the Pareto frontier. The proposed algorithm is analyzed and its convergence and controllability are theoretically justified. \nExperiments from discrete toy examples to higher-dimension robotic control tasks and experiments with more than two objectives are conducted to show its performance.  In these experiments, PCRL-trained policies show significantly better controllability than existing approaches and can generate Pareto optimal solutions with better diversity and utilities."
    },
    {
        "title": "Mini-batch Submodular Maximization",
        "link_suffix": "/forum?id=1DEEVAl5QX",
        "link": "https://openreview.net/forum?id=1DEEVAl5QX",
        "pdf_link": "https://openreview.net/pdf?id=1DEEVAl5QX",
        "keywords": "smoothed analysis, submodular maximization",
        "abstract": "We present the firstmini-batchalgorithm for maximizing a non-negative monotonedecomposablesubmodular function, $F=\\sum_{i=1}^N f^i$, under a set of constraints. \nWe consider two sampling approaches: uniform and weighted. We show that mini-batch with weighted sampling improves over the state of the art sparsifier based approach both in theory and in practice. Surprisingly, we experimentally observe that uniform sampling achieves superior results to weighted sampling. However, it isimpossibleto explain this using worst-case analysis. Our main contribution is usingsmoothed analysisto provide a theoretical foundation for our experimental results. We show that, undervery mildassumptions, uniform sampling is superior for both the mini-batch and the sparsifier approaches. We empirically verify that these assumptions hold for our datasets. Uniform sampling is simple to implement and has complexity independent of $N$, making it the perfect candidate to tackle massive real-world datasets."
    },
    {
        "title": "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching",
        "link_suffix": "/forum?id=45FzVIdA3T",
        "link": "https://openreview.net/forum?id=45FzVIdA3T",
        "pdf_link": "https://openreview.net/pdf?id=45FzVIdA3T",
        "keywords": "omnidirectional image, image matching, feature matching, dense matching",
        "abstract": "We introduce the first learning-based dense matching algorithm, termed Equirectangular Projection-Oriented Dense Kernelized Feature Matching (EDM), specifically designed for omnidirectional images. Equirectangular projection (ERP) images, with their large fields of view, are particularly suited for dense matching techniques that aim to establish comprehensive correspondences across images. However, ERP images are subject to significant distortions, which we address by leveraging the spherical camera model and geodesic flow refinement in the dense matching method. To further mitigate these distortions, we propose spherical positional embeddings based on 3D Cartesian coordinates of the feature grid. Additionally, our method incorporates bidirectional transformations between spherical and Cartesian coordinate systems during refinement, utilizing a unit sphere to improve matching performance. We demonstrate that our proposed method achieves notable performance enhancements, with improvements of +26.72 and +42.62 in AUC@5\u00b0 on the Matterport3D and Stanford2D3D datasets, respectively."
    },
    {
        "title": "Adaptive Caching for Faster Video Generation with Diffusion Transformers",
        "link_suffix": "/forum?id=DyyLUUVXJ5",
        "link": "https://openreview.net/forum?id=DyyLUUVXJ5",
        "pdf_link": "https://openreview.net/pdf?id=DyyLUUVXJ5",
        "keywords": "Diffusion Transformers, Caching, Content-adaptive Generation",
        "abstract": "Generating temporally-consistent high-fidelity videos can be computationally expensive, especially over longer temporal spans. More-recent Diffusion Transformers (DiTs)--- despite making significant headway in this context--- have only heightened such challenges as they rely on larger models and heavier attention mechanisms, resulting in slower inference speeds. In this paper, we introduce a $\\textit{training-free}$ method to accelerate video DiTs, termed Adaptive Caching ($\\textit{AdaCache}$), which is motivated by the fact that $\\textit{``not all videos are created equal''}$: meaning, some videos require fewer denoising steps to attain a reasonable quality than others. Building on this, we not only cache computations through the diffusion process, but also devise a caching schedule tailored to each video generation, maximizing the quality-latency trade-off. We further introduce a Motion Regularization ($\\textit{MoReg}$) scheme to utilize video information within AdaCache, essentially controlling the compute allocation based on motion content. Altogether, our plug-and-play contributions grant significant inference speedups (e.g. up to 4.7x on Open-Sora 720p - 2s video generation) without sacrificing the generation quality, across multiple video DiT baselines. Our code will be made publicly-available."
    },
    {
        "title": "Scalable Universal T-Cell Receptor Embeddings from Adaptive Immune Repertoires",
        "link_suffix": "/forum?id=wyF5vNIsO7",
        "link": "https://openreview.net/forum?id=wyF5vNIsO7",
        "pdf_link": "https://openreview.net/pdf?id=wyF5vNIsO7",
        "keywords": "Immunomics, T-cell Receptor Embeddings, GloVe, Random Projection Theory, Scaling, Unsupervised Representation Learning",
        "abstract": "T cells are a key component of the adaptive immune system, targeting infections, cancers, and allergens with specificity encoded by their T cell receptors (TCRs), and retaining a memory of their targets. High-throughput TCR repertoire sequencing captures a cross-section of TCRs that encode the immune history of any subject, though the data are heterogeneous, high dimensional, sparse, and mostly unlabeled. \nSets of TCRs responding to the same antigen,i.e., a protein fragment, co-occur in subjects sharing immune genetics and exposure history. Here, we leverage TCR co-occurrence across a large set of TCR repertoires and employ the GloVe (Pennington et al., 2014)  algorithm to derive low-dimensional, dense vector representations (embeddings) of TCRs. We then aggregate these TCR embeddings to generate subject-level embeddings based on observedsubject-specificTCR subsets. Further, we leverage random projection theory to improve GloVe's computational efficiency in terms of memory usage and training time. Extensive experimental results show that TCR embeddings targeting the same pathogen have high cosine similarity, and subject-level embeddings encode both immune genetics and pathogenic exposure history."
    },
    {
        "title": "SINAI: Selective Injection of Noise for Adversarial Robustness with Improved Efficiency",
        "link_suffix": "/forum?id=BvlaNTMl7P",
        "link": "https://openreview.net/forum?id=BvlaNTMl7P",
        "pdf_link": "https://openreview.net/pdf?id=BvlaNTMl7P",
        "keywords": "Adversarial Robustness, Efficient Neural Networks, Hardware and Software Co-design",
        "abstract": "Deep Neural Networks (DNNs) have revolutionized a wide range of industries, from healthcare and finance to automotive, by offering unparalleled capabilities in data analysis and decision-making. Despite their transforming impact, DNNs face two critical challenges: the vulnerability to adversarial attacks and the increasing computational costs associated with more complex and larger models. In this paper, we introduce an effective method designed to simultaneously enhance adversarial robustness and execution efficiency. Unlike prior studies that enhance robustness via uniformly injecting noise, we introduce a non-uniform noise injection algorithm, strategically applied at each DNN layer to disrupt adversarial perturbations introduced in attacks. By employing approximation techniques, our approach identifies and protects essential neurons while strategically introducing noise into non-essential neurons. Our experimental results demonstrate that our method successfully enhances both robustness and efficiency across several attack scenarios, model architectures, and datasets."
    },
    {
        "title": "Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance",
        "link_suffix": "/forum?id=nuX2yPejiL",
        "link": "https://openreview.net/forum?id=nuX2yPejiL",
        "pdf_link": "https://openreview.net/pdf?id=nuX2yPejiL",
        "keywords": "Stochastic Polyak Step-size, SGD, Stochastic Heavy Ball, Convergence Analysis, Convex Optimization, Momentum",
        "abstract": "Stochastic gradient descent with momentum, also known as Stochastic Heavy Ball method (SHB), is one of the most popular algorithms for solving large-scale stochastic optimization problems in various machine learning tasks. In practical scenarios, tuning the step-size and momentum parameters of the method is a prohibitively expensive and time-consuming process. In this work, inspired by the recent advantages of stochastic Polyak step-size in the performance of stochastic gradient descent (SGD), we propose and explore new Polyak-type variants suitable for the update rule of the SHB method. In particular, using the Iterate Moving Average (IMA) viewpoint of SHB, we propose and analyze three novel step-size selections: MomSPSmax, MomDecSPS, and MomAdaSPS. For MomSPSmax, we provide convergence guarantees for SHB to a neighborhood of the solution for convex and smooth problems (without assuming interpolation). If interpolation is also satisfied, then using MomSPSmax, SHB converges to the true solution at a fast rate matching the deterministic HB. The other two variants, MomDecSPS and MomAdaSPS, are the first adaptive step-size for SHB that guarantee convergence to the exact minimizer - without a priori knowledge of the problem parameters and without assuming interpolation. Our convergence analysis of SHB is tight and obtains the convergence guarantees of stochastic Polyak step-size for SGD as a special case. We supplement our analysis with experiments validating our theory and demonstrating the effectiveness and robustness of our algorithms."
    },
    {
        "title": "InstaTrain: Adaptive Training via Ultra-Fast Natural Annealing within Dynamical Systems",
        "link_suffix": "/forum?id=QhhShUQIpJ",
        "link": "https://openreview.net/forum?id=QhhShUQIpJ",
        "pdf_link": "https://openreview.net/pdf?id=QhhShUQIpJ",
        "keywords": "Time-Series Prediction, Online Learning, Nature-Powered Computing",
        "abstract": "Time-series modeling is broadly adopted to capture underlying patterns and trends present in historical data, allowing for prediction of future values. However, one crucial aspect in such modeling is often overlooked: in highly dynamic environments, data distributions can shift drastically within a second or less. Under this circumstance, traditional predictive models, even online learning methods struggle to adapt to the ultra-fast and complex distribution shift present in highly dynamic scenarios. To address this, we propose InstaTrain, a novel learning paradigm that enables frequent model updates with microsecond-level intervals for real-world prediction tasks, allowing it to keep pace with rapidly evolving data distributions. In this work, (1) We transform the slow and expensive model training process into an ultra-fast natural annealing process that can be carried out on a dynamical system. (2) Leveraging a recently proposed electronic dynamical system, we augment the system with a parameter update module, extending its capabilities to encompass both rapid training and inference. Experimental results across highly dynamic datasets demonstrate that our method delivers on average, a remarkable 4,135$\\times$ training speedup, $10^5\\times$ reduction in training energy costs, and 69.1% / 45.8% lower test MAE over the best result of SOTA methods running on GPUs with / without the online learning mechanism."
    },
    {
        "title": "ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts",
        "link_suffix": "/forum?id=6BoStmXGBf",
        "link": "https://openreview.net/forum?id=6BoStmXGBf",
        "pdf_link": "https://openreview.net/pdf?id=6BoStmXGBf",
        "keywords": "lora, PEFT, parameter-efficient finetuning, parameter-efficient pre-training, vision transformer, ViT, domain adaptation, domain generalization, satellite images, foundation models",
        "abstract": "Parameter-efficient fine-tuning (PEFT) techniques such as low-rank adaptation (LoRA) can effectively adapt large pre-trained foundation models to downstream tasks using only a small fraction (0.1%-10%) of the original trainable weights. \nAn under-explored question of PEFT is in extending the pre-training phase without supervised labels; that is, can we adapt a pre-trained foundation model to a new domain via efficient self-supervised pre-training on this new domain? \nIn this work, we introduce ExPLoRA, a highly effective technique to improve transfer learning of pre-trained vision transformers (ViTs) under domain shifts.\nInitializing a ViT with pre-trained weights on large, natural-image datasets such as from DinoV2 or MAE, ExPLoRA continues the unsupervised pre-training objective on a new domain, unfreezing 1-2 pre-trained ViT blocks and tuning all other layers with LoRA.\nWe then fine-tune the resulting model only with LoRA on this new domain for supervised learning. \nOur experiments demonstrate state-of-the-art results on satellite imagery, even outperforming fully pre-training and fine-tuning ViTs. \nUsing the DinoV2 training objective, we demonstrate up to 7.5% improvement in linear probing top-1 accuracy on downstream tasks while using <10% of the number of parameters that are used in prior fully-tuned state-of-the art approaches. \nOur ablation studies confirm the efficacy of our approach over other baselines, including PEFT and unfreezing more ViT blocks."
    },
    {
        "title": "A Likelihood Based Approach to Distribution Regression Using Conditional Deep Generative Models",
        "link_suffix": "/forum?id=V6hhhXoTSq",
        "link": "https://openreview.net/forum?id=V6hhhXoTSq",
        "pdf_link": "https://openreview.net/pdf?id=V6hhhXoTSq",
        "keywords": "Deep Generative Model, Conditional Distribution, Smoothness Disparity",
        "abstract": "In this work, we explore the theoretical properties of conditional deep generative models under the statistical framework of distribution regression where the response variable lies in a high-dimensional ambient space but concentrates around a potentially lower-dimensional manifold. More specifically, we study the large-sample properties of a likelihood-based approach for estimating these models. Our results lead to the convergence rate of a sieve maximum likelihood estimator (MLE) for estimating the conditional distribution (and its devolved counterpart) of the response given predictors in the Hellinger (Wasserstein) metric. Particularly, our rate depends solely on the intrinsic dimension and smoothness of the true conditional distribution. These findings provide an explanation of why conditional deep generative models can circumvent the curse of dimensionality from the perspective of statistical foundations and demonstrate that they can learn a broader class of nearly singular conditional distributions. Our analysis also emphasizes the importance of introducing a small noise perturbation to the data when they are supported sufficiently close to a manifold. Finally, in our numerical studies, we demonstrate the effective implementation of the proposed approach using both synthetic and real-world datasets, which also provide complementary validation to our theoretical findings."
    },
    {
        "title": "Agree to Disagree: Demystifying Homogeneous Deep Ensembles through Distributional Equivalence",
        "link_suffix": "/forum?id=XYRPm8rAGM",
        "link": "https://openreview.net/forum?id=XYRPm8rAGM",
        "pdf_link": "https://openreview.net/pdf?id=XYRPm8rAGM",
        "keywords": "deep ensemble, trustworthiness, distributional equivalence",
        "abstract": "Deep ensembles improve the performance of the models by taking the average predictions of a group of ensemble members. However, the origin of these capabilities remains a mystery and deep ensembles are used as a reliable \u201cblack box\u201d to improve the performance. Existing studies typically attribute such improvement to Jensen gaps of the deep ensemble method, where the loss of the mean does not exceed the mean of the loss for any convex loss metric. In this work, we demonstrate that Jensen\u2019s inequality is not responsible for the effectiveness of deep ensembles, and convexity is not a necessary condition. Instead, Jensen Gap focuses on the \u201caverage loss\u201d of individual models, which provides no practical meaning. Thus it fails to explain the core phenomena of deep ensembles such as the superiority to any single ensemble member, the decreasing loss with the number of ensemble members, etc. Regarding this mystery, we provide theoretical analysis and comprehensive empirical results from a statistical perspective that reveal the true mechanism of deep ensembles. Our results highlight that deep ensembles originate from the homogeneous output distribution across all ensemble members. Specifically, the predictions of homogeneous models (Abe et al., 2022b) have the distributional equivalence property \u2013 Although the predictions of independent ensemble members are point-wise different, they form an identical distribution. Such agreement and disagreement contribute to deep ensembles\u2019 \u201cmagical power\u201d. Based on this discovery, we provide rigorous proof of the effectiveness of deep ensembles and analytically quantify the extent to which ensembles improve performance. The derivations not only theoretically quantify the effectiveness of deep ensembles for the first time, but also enable estimation schemes that foresee the performance of ensembles with different capacities. Furthermore, different from existing studies, our results also point out that deep ensembles work in a different mechanism from model scaling a single model, even though significant correlations between them have been observed."
    },
    {
        "title": "REDO: Execution-Free Runtime Error Detection for Coding Agents",
        "link_suffix": "/forum?id=THkF3VWSNv",
        "link": "https://openreview.net/forum?id=THkF3VWSNv",
        "pdf_link": "https://openreview.net/pdf?id=THkF3VWSNv",
        "keywords": "error detection, coding agents, runtime, execution-free, large language models",
        "abstract": "As LLM-based agents exhibit exceptional capabilities in addressing complex problems, there is a growing focus on developing coding agents to tackle increasingly sophisticated tasks. Despite their promising performance, these coding agents often produce programs or modifications that contain runtime errors, which can cause code failures and are difficult for static analysis tools to detect. Enhancing the ability of coding agents to statically identify such errors could significantly improve their overall performance. In this work, we introduce Execution-free Runtime Error Detection for COding Agents (REDO), a method that integrates LLMs with static analysis tools to detect runtime errors for coding agents, without code execution. Additionally, we propose a benchmark task, SWE-Bench-Error-Detection (SWEDE), based on SWE-Bench (lite), to evaluate error detection in repository-level problems with complex external dependencies. Finally, through both quantitative and qualitative analyses across various error detection tasks, we demonstrate that REDO outperforms current state-of-the-art methods by achieving a 11.0% higher accuracy and 9.1% higher weighted F1 score; and provide insights into the advantages of incorporating LLMs for error detection."
    },
    {
        "title": "LR0.FM: LOW-RESOLUTION ZERO-SHOT CLASSIFICATION BENCHMARK FOR FOUNDATION MODELS",
        "link_suffix": "/forum?id=AsFxRSLtqR",
        "link": "https://openreview.net/forum?id=AsFxRSLtqR",
        "pdf_link": "https://openreview.net/pdf?id=AsFxRSLtqR",
        "keywords": "Benchmark, Zeroshot, Foundation model, low resolution",
        "abstract": "Visual-language foundation Models (FMs) exhibit remarkable zero-shot generalization across diverse tasks, largely attributed to extensive pre-training on largescale datasets. However, their robustness on low-resolution/pixelated (LR) images, a common challenge in real-world scenarios, remains underexplored. We introduce LR0.FM, a comprehensive benchmark evaluating the impact of low resolution on the zero-shot classification performance of 10 FM(s) across 66 backbones and 15 datasets. We propose a novel metric, Weighted Aggregated Robustness, to address the limitations of existing metrics and better evaluate model performance across resolutions and datasets. Our key findings show that: (i) model size positively correlates with robustness to resolution degradation, (ii) pre-training dataset quality is more important than its size, and (iii) fine-tuned and higher resolution models are less robust against LR. Our analysis further reveals that model makes semantically reasonable predictions at LR, and the lack of fine-grained details in input adversely impacts the model\u2019s initial layers more than the deeper layers. We use these insights and introduce a simple strategy, LRTK0, to enhance robustness of models without compromising their pre-trained weights. We demonstrate the effectiveness of LR-TK0 for robustness against lowresolution across several datasets and its generalization capability across backbones and other approaches. Code will be publicly released."
    },
    {
        "title": "Reinforcement learning with combinatorial actions for coupled restless bandits",
        "link_suffix": "/forum?id=DhH3LbA6F6",
        "link": "https://openreview.net/forum?id=DhH3LbA6F6",
        "pdf_link": "https://openreview.net/pdf?id=DhH3LbA6F6",
        "keywords": "reinforcement learning, combinatorial optimization, restless bandits, mixed-integer programming",
        "abstract": "Reinforcement learning (RL) has increasingly been applied to solve real-world planning problems, with progress in handling large state spaces and time horizons. However, a key bottleneck in many domains is that RL methods cannot accommodate large, combinatorially structured action spaces. In such settings, even representing the set of feasible actions at a single step may require a complex discrete optimization formulation. We leverage recent advances in embedding trained neural networks into optimization problems to propose SEQUOIA, an RL algorithm that directly optimizes for long-term reward over the feasible action space. Our approach embeds a Q-network into a mixed-integer program to select a combinatorial action in each timestep. Here, we focus on planning over restless bandits, a class of planning problems which capture many real-world examples of sequential decision making. We introduce coRMAB, a broader class of restless bandits with combinatorial actions that cannot be decoupled across the arms of the restless bandit, requiring direct solving over the joint, exponentially large action space. We empirically validate SEQUOIA on four novel restless bandit problems with combinatorial constraints: multiple interventions, path constraints, bipartite matching, and capacity constraints. Our approach significantly outperforms existing methods\u2014which cannot address sequential planning and combinatorial selection simultaneously\u2014by an average of 28.3% on these difficult instances."
    },
    {
        "title": "Language Repository for Long Video Understanding",
        "link_suffix": "/forum?id=O8FkMqNF1M",
        "link": "https://openreview.net/forum?id=O8FkMqNF1M",
        "pdf_link": "https://openreview.net/pdf?id=O8FkMqNF1M",
        "keywords": "Large-language models, Long-video understanding, Visual question-answering",
        "abstract": "Language has become a prominent modality in computer vision with the rise of LLMs. Despite supporting long context-lengths, their effectiveness in handling long-term information gradually declines with input length. This becomes critical, especially in applications such as long-form video understanding. In this paper, we introduce a Language Repository (LangRepo) for LLMs, that maintains concise and structured information as an interpretable (i.e., all-textual) representation. Our repository is updated iteratively based on multi-scale video chunks. We introduce write and read operations that focus on pruning redundancies in text, and extracting information at various temporal scales. The proposed framework is evaluated on zero-shot visual question-answering benchmarks including EgoSchema, NExT-QA, IntentQA and NExT-GQA, showing state-of-the-art performance at its scale. Our code will be made publicly available."
    },
    {
        "title": "One Step Diffusion via Shortcut Models",
        "link_suffix": "/forum?id=OlzB6LnXcS",
        "link": "https://openreview.net/forum?id=OlzB6LnXcS",
        "pdf_link": "https://openreview.net/pdf?id=OlzB6LnXcS",
        "keywords": "diffusion, flow-matching, fast inference, distillation",
        "abstract": "Diffusion models and flow matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce Shortcut Models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time."
    },
    {
        "title": "Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment",
        "link_suffix": "/forum?id=mtSSFiqW6y",
        "link": "https://openreview.net/forum?id=mtSSFiqW6y",
        "pdf_link": "https://openreview.net/pdf?id=mtSSFiqW6y",
        "keywords": "LLM inference, speculative decoding",
        "abstract": "The performance of large language models (LLMs) is closely linked to their underlying size, leading to ever-growing networks and hence slower inference. Speculative decoding has been proposed as a technique to accelerate autoregressive generation, leveraging a fast draft model to propose candidate tokens, which are then verified in parallel based on their likelihood under the target model. While this approach guarantees to reproduce the target output, it incurs a substantial penalty: many high-quality draft tokens are rejected, even when they represent objectively valid continuations. Indeed, we show that even powerful draft models such as GPT-4o, as well as human text cannot achieve high acceptance rates under the standard verification scheme. This severely limits the speedup potential of current speculative decoding methods, as an early rejection becomes overwhelmingly likely when solely relying on alignment of draft and target.\nWe thus ask the following question: Can we adapt verification to recognize correct, but non-aligned replies? To this end, we draw inspiration from the LLM-as-a-judge framework, which demonstrated that LLMs are able to rate answers in a versatile way. We carefully design a dataset coined TokenCourt to elicit the same capability in the target model by training a compact module on top of the embeddings to produce ``judgements\" of the current continuation. We showcase our strategy on the Llama-3.1 family, where our 8B/405B-Judge achieves a speedup of $9\\times$ over Llama-405B, while maintaining its quality on a large range of benchmarks. These benefits remain present even in optimized inference frameworks, where our method reaches up to $141$ tokens/s for 8B/70B-Judge and $129$ tokens/s for 8B/405B on $2$ and $8$ H100s respectively."
    },
    {
        "title": "Scale-Free Graph-Language Models",
        "link_suffix": "/forum?id=nFcgay1Yo9",
        "link": "https://openreview.net/forum?id=nFcgay1Yo9",
        "pdf_link": "https://openreview.net/pdf?id=nFcgay1Yo9",
        "keywords": "scale-free property, language models, k nearest neighbor graph",
        "abstract": "Graph-language models (GLMs) have shown great potential in graph-based semi-supervised learning. In GLMs, graph generation and text embedding are two key stages, which are typically achieved by inferring a latent graph and finetuning a language model, respectively. However, the former relies on artificial assumptions about the underlying edge distribution, while the latter requires sufficient data annotations. This paper introduces a novel GLM that addresses these two challenges sequentially by leveraging a well-grounded structural prior. Specifically, we explore an inherent nature of real edge distribution\u2014the scale-free property\u2014for graph generation. We unexpectedly reveal that this natural characteristic can be closely approximated by a simple k-nearest neighbor graph. By using this scale-free graph, we subsequently develop a graph-based pseudo-labeler to generate complementary supervision for text embedding. Extensive experiments validate our findings and highlight the potential of GLMs built on scale-free structures."
    },
    {
        "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
        "link_suffix": "/forum?id=cFu7ze7xUm",
        "link": "https://openreview.net/forum?id=cFu7ze7xUm",
        "pdf_link": "https://openreview.net/pdf?id=cFu7ze7xUm",
        "keywords": "Large Language Models; Long Context; Efficiency;",
        "abstract": "Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges.\nCaching all Key and Value (KV) states across all attention heads consumes substantial memory.\nExisting KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements.\nIn this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens.\nIn contrast, all other heads, which primarily focus on recent tokens and attention sinks\u2014referred to as Streaming Heads\u2014do not require full attention.\nBased on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities.\nDuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately.\nOur method significantly reduces long-context inference memory by up to 2.55$\\times$ for MHA and 1.67$\\times$ for GQA models while speeding up decoding by up to 2.18$\\times$ and 1.50$\\times$ and accelerating pre-filling by up to 1.73$\\times$ and 1.63$\\times$ for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention.\nNotably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.33 million context length measured on a single A100 GPU. Code and dataset will be released upon publication."
    },
    {
        "title": "Private Stochastic Convex Optimization with Tysbakov Noise Condition and Large Lipschitz Constant",
        "link_suffix": "/forum?id=YnJnY7O1PT",
        "link": "https://openreview.net/forum?id=YnJnY7O1PT",
        "pdf_link": "https://openreview.net/pdf?id=YnJnY7O1PT",
        "keywords": "Stochastic Convex Optimization, Differential Privacy",
        "abstract": "We study Stochastic Convex Optimization in Differential Privacy model (DP-SCO). Unlike previous studies, here we assume the population risk function satisfies\nthe Tysbakov Noise Condition (TNC) with some parameter $\\theta>1$, where the Lipschitz constant of the loss could be extremely large or even unbounded, but the $\\ell_2$-norm gradient of the loss has bounded $k$-th moment with $k\\geq 2$. \nFor the Lipschitz case with $\\theta\\geq 2$, we first propose an $(\\epsilon, \\delta)$-DP algorithms whose utility bound is $\\tilde{O}\\left(\\left(\\tilde{r}_{2k}(\\frac{1}{\\sqrt{n}}+(\\frac{\\sqrt{d}}{n\\epsilon}))^\\frac{k-1}{k}\\right)^\\frac{\\theta}{\\theta-1}\\right)$in high probability, where $n$ is the sample size, $d$ is the model dimension, and $\\tilde{r}_{2k}$ is a term that only depends on the $2k$-th moment of the gradient.It is notable that such an upper bound is independent of the Lipschitz constant. We then extend to the case where \n $\\theta\\geq \\bar{\\theta}> 1$ for some known constant $\\bar{\\theta}$. Moreover, when the privacy budget $\\epsilon$ is small enough, we show an upper bound of $\\tilde{O}\\left(\\left(\\tilde{r}_{k}(\\frac{1}{\\sqrt{n}}+(\\frac{\\sqrt{d}}{n\\epsilon}))^\\frac{k-1}{k}\\right)^\\frac{\\theta}{\\theta-1}\\right)$even if the loss function is not Lipschitz. For the lower bound, we show that for any $\\theta\\geq 2$, the private minimax rate for $\\rho$-zero Concentrated Differential Privacy is lower bounded by $\\Omega\\left(\\left(\\tilde{r}_{k}(\\frac{1}{\\sqrt{n}}+(\\frac{\\sqrt{d}}{n\\sqrt{\\rho}}))^\\frac{k-1}{k}\\right)^\\frac{\\theta}{\\theta-1}\\right)$."
    },
    {
        "title": "Feature Matching Intervention: Leveraging Observational Data for Causal Representation Learning",
        "link_suffix": "/forum?id=8GhwePP7vA",
        "link": "https://openreview.net/forum?id=8GhwePP7vA",
        "pdf_link": "https://openreview.net/pdf?id=8GhwePP7vA",
        "keywords": "Causal representation learning, Observational data, Out-of-distribution generalization",
        "abstract": "A major challenge in causal inference from observational data is the absence of perfect interventions, making it difficult to distinguish causal features from spurious ones. We propose an innovative approach, Feature Matching Intervention (FMI), which uses a matching procedure to mimic perfect interventions. We define causal latent graphs, extending structural causal models to latent feature space, providing a framework that connects FMI with causal graph learning. Our feature matching procedure emulates perfect interventions within these causal latent graphs. Theoretical results demonstrate that FMI exhibits strong out-of-distribution (OOD) generalizability. Experiments further highlight FMI's superior performance in effectively identifying causal features solely from observational data."
    },
    {
        "title": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning",
        "link_suffix": "/forum?id=iMnd6c5bAa",
        "link": "https://openreview.net/forum?id=iMnd6c5bAa",
        "pdf_link": "https://openreview.net/pdf?id=iMnd6c5bAa",
        "keywords": "Bayesian optimization, Density ratio estimation-based Bayesian optimization, Bayesian optimization with semi-supervised learning",
        "abstract": "Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of efficiently finding a global optimum of an expensive-to-evaluate black-box function. In general, a probabilistic regression model is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based methods, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, supervised classifiers are employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy are prone to be overconfident for known knowledge on global solution candidates. Supposing that we have access to unlabeled points, e.g., predefined fixed-size pools, we propose density ratio estimation-based Bayesian optimization with semi-supervised learning to solve this challenge. Finally, we show the empirical results of our methods and several baseline methods in two distinct scenarios with unlabeled point sampling and a fixed-size pool and analyze the validity of our proposed methods in diverse experiments."
    }
]