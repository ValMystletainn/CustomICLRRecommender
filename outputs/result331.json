[
    {
        "title": "MetaTool: Facilitating Large Language Models to Master Tools with Meta-task Augmentation",
        "link_suffix": "/forum?id=6AUzsrsNUx",
        "link": "https://openreview.net/forum?id=6AUzsrsNUx",
        "pdf_link": "https://openreview.net/pdf?id=6AUzsrsNUx",
        "keywords": "large language models, tool learning, function calling, tool understanding, instruction tuning",
        "abstract": "Utilizing tools with Large Language Models (LLMs) is essential for grounding AI agents in real-world applications. The prevailing approach involves few-shot prompting with demonstrations or fine-tuning with expert annotations. However, mere in-context demonstrations may fail to cover sufficient knowledge for complex tools and tasks. Training on solution paths is also hindered by the high cost of expert annotations and generalizing to new tools. A core challenge of generalizable tool use lies in understanding the \"meta'', or fundamental natures of tools that are transferable across tasks, such as causality and constraints. In this paper, we present MetaTool, a novel tool learning methodology designed to generalize across any reusable toolset. Our approach incorporates a self-supervised augmentation technique derived from a series of meta-tasks. This involves predicting masked elements in the tool execution process. The self-supervised procedure enables scalable generation of high-quality QA data, which is handful for supervising tool understanding. By incorporating meta-task data into task-oriented training, our method significantly enhances the performance of open-source LLMs, achieving results comparable to ChatGPT in both tool-based planning and chatting scenarios. Through large-scale instruction tuning, the MetaTool model demonstrates impressive zero-shot generalizability on new tasks."
    },
    {
        "title": "Crack in the Armor: Universal Stability Measurement for Large Language Models",
        "link_suffix": "/forum?id=1CRu6bGx25",
        "link": "https://openreview.net/forum?id=1CRu6bGx25",
        "pdf_link": "https://openreview.net/pdf?id=1CRu6bGx25",
        "keywords": "Large Language Models, sensitivity analysis, local influence measure",
        "abstract": "Large Language Models (LLMs) and Vision Language Models (VLMs) have become essential to general artificial intelligence, demonstrating impressive capabilities in task understanding and problem-solving. The real-world functionality of these large models critically depends on their stability. However, there is still a lack of rigorous studies examining the stability of LLMs when subjected to various perturbations. \nIn this paper, we aim to address this gap by proposing a novel influence measure for LLMs. This measure is inspired by statistical methods grounded in information geometry, offering desirable invariance properties. Using this framework, we analyze the sensitivity of LLMs in response to parameter or input perturbations. \nTo evaluate the effectiveness of our approach, we conduct extensive experiments on models of varying sizes, from 1.5B to 13B parameters. The results clearly demonstrate the efficacy of our measure in identifying salient parameters and pinpointing vulnerable areas of input images that dominate model outcomes. Our research not only enhances the understanding of LLM sensitivity but also highlights the broad potential of our influence measure in optimizing models for tasks such as model quantization and model merging."
    },
    {
        "title": "Motion Control of High-Dimensional Musculoskeletal System with Hierarchical Model-Based Planning",
        "link_suffix": "/forum?id=MWHIIWrWWu",
        "link": "https://openreview.net/forum?id=MWHIIWrWWu",
        "pdf_link": "https://openreview.net/pdf?id=MWHIIWrWWu",
        "keywords": "Model predictive control, High-dimensional embodied system",
        "abstract": "Controlling high-dimensional nonlinear systems presents significant challenges in biological and robotic applications due to the large state and action spaces. While deep reinforcement learning has emerged as the leading approach, it suffers from computationally-intensive and time-consuming, and are not scalable to wide varieties of tasks that each require significant manual tuning. This paper introduces Model Predictive Control with Morphology-aware Proportional Control (MPC$^2$), a novel hierarchical model-based algorithm that addresses these challenges. By integrating a sampling-based model predictive controller for target posture planning with a morphology-aware proportional controller for actuator coordination, our algorithm achieves stable movement control of a 700-actuator musculoskeletal model without training. We show that MPC$^2$ enables zero-shot high-dimensional motion control across diverse movement tasks, such as standing, walking on varying terrains, and sports motion imitation. It can be incorporated into optimal cost function design to automatically optimize the objective, reducing the reliance on traditional reward engineering methods. This work presents a major advancement in (near) real-time control for complex dynamical systems."
    },
    {
        "title": "Multimodal Situational Safety",
        "link_suffix": "/forum?id=I9bEi6LNgt",
        "link": "https://openreview.net/forum?id=I9bEi6LNgt",
        "pdf_link": "https://openreview.net/pdf?id=I9bEi6LNgt",
        "keywords": "Multimodal situational safety, safety benchmark and evaluation",
        "abstract": "Multimodal Large Language Models (MLLMs) are rapidly evolving, demonstrating impressive capabilities as multimodal assistants that interact with both humans and their environments. However, this increased sophistication introduces significant safety concerns. In this paper, we present the first evaluation and analysis of a novel safety challenge termed Multimodal Situational Safety, which explores how safety considerations vary based on the specific situation in which the user or agent is engaged. \nWe argue that for an MLLM to respond safely\u2014whether through language or action\u2014it often needs to assess the safety implications of a language query within its corresponding visual context.\nTo evaluate this capability, we develop the Multimodal Situational Safety benchmark (MSSBench) to assess the situational safety performance of current MLLMs. \nThe dataset comprises 1,820 language query-image pairs, half of which the image context is safe, and the other half is unsafe. \nWe also develop an evaluation framework that analyzes key safety aspects, including explicit safety reasoning, visual understanding, and, crucially, situational safety reasoning. \nOur findings reveal that current MLLMs struggle with this nuanced safety problem in the instruction-following setting and struggle to tackle these situational safety challenges all at once, highlighting a key area for future research. \nFurthermore, we develop multi-agent pipelines to coordinately solve safety challenges, which shows consistent improvement in safety over the original MLLM response."
    },
    {
        "title": "PaCA: Partial Connection Adaptation for Efficient Fine-Tuning",
        "link_suffix": "/forum?id=iYkhxre0In",
        "link": "https://openreview.net/forum?id=iYkhxre0In",
        "pdf_link": "https://openreview.net/pdf?id=iYkhxre0In",
        "keywords": "Parameter-Efficient Fine-Tuning (PEFT), Large Language Models (LLM), Memory efficient training, Accelerating training",
        "abstract": "Prior parameter-efficient fine-tuning (PEFT) algorithms reduce memory usage and computational costs of fine-tuning large neural network models by training only a few additional adapter parameters, rather than the entire model. However, the reduction in computational costs due to PEFT does not necessarily translate to a reduction in training time; although the computational costs of the adapter layers are much smaller than the pretrained layers, it is well known that those two types of layers are processed sequentially on GPUs, resulting in significant latency overhead. LoRA and its variants avoid this latency overhead by merging the low-rank adapter matrices with the pretrained weights during inference. However, those layers cannot be merged during training since the pretrained weights must remain frozen while the low-rank adapter matrices are updated continuously over the course of training. Furthermore, LoRA and its variants do not reduce activation memory, as the first low-rank adapter matrix still requires the input activations to the pretrained weights to compute weight gradients. To mitigate this issue, we proposePartialConnectionAdaptation (PaCA), which fine-tunes randomly selected partial connections within the pretrained weights instead of introducing adapter layers in the model. PaCA not only enhances training speed by eliminating the time overhead due to the sequential processing of the adapter and pretrained layers but also reduces activation memory since only partial activations, rather than full activations, need to be stored for gradient computation. Compared to LoRA, PaCA reduces training time by 22% and total memory usage by 16%, while maintaining comparable accuracy across various fine-tuning scenarios, such as fine-tuning on the MMLU dataset and instruction tuning on the Oasst1 dataset. PaCA can also be combined with quantization, enabling the fine-tuning of large models such as LLaMA3.1-70B. In addition, PaCA enables training on 23% longer sequence data and improves throughput by 16% on both NVIDIA A100 and INTEL Gaudi 2 GPUs compared to LoRA. The code is available athttps://anonymous.4open.science/r/paca-366F."
    },
    {
        "title": "Physics-Informed Self-Guided Diffusion Model for High-Fidelity Simulations",
        "link_suffix": "/forum?id=EaiU4F5pwn",
        "link": "https://openreview.net/forum?id=EaiU4F5pwn",
        "pdf_link": "https://openreview.net/pdf?id=EaiU4F5pwn",
        "keywords": "Physics-informed Neural Networks, Computational Fluid Dynamics",
        "abstract": "Machine learning (ML) models are increasingly explored in fluid dynamics as a promising way to generate high-fidelity computational fluid dynamics data more efficiently. A common strategy is to use low-fidelity data as computational-efficient inputs, and employ ML techniques to reconstruct high-fidelity flow fields. However, existing work typically assumes that low-fidelity data is artificially downsampled from high-fidelity sources, which limits model performance. In real-world applications, low-fidelity data is generated directly by numerical solvers with a lower initial state resolution, resulting in large deviations from high-fidelity data. To address this gap, we propose PG-Diff, a novel diffusion model for reconstructing high-fidelity flow fields, where both low- and high-fidelity data are generated from numerical solvers. Our experiments reveal that state-of-the-art models struggle to recover fine-grained high-fidelity details when using solver-generated low-fidelity inputs, due to distribution shift. To overcome this challenge, we introduce an \\textit{Importance Weight} strategy during training as self-guidance and a training-free \\textit{Residual Correction} method during inference as physical inductive bias, guiding the diffusion model toward higher-quality reconstructions. Experiments on four 2D turbulent flow datasets demonstrate the effectiveness of our proposed method."
    },
    {
        "title": "Text-to-Image Rectified Flow as Plug-and-Play Priors",
        "link_suffix": "/forum?id=SzPZK856iI",
        "link": "https://openreview.net/forum?id=SzPZK856iI",
        "pdf_link": "https://openreview.net/pdf?id=SzPZK856iI",
        "keywords": "3D Generation, Rectified Flow, Flow Matching",
        "abstract": "Large-scale diffusion models have achieved remarkable performance in generative tasks. Beyond their initial training applications, these models have proven their ability to function as versatile plug-and-play priors. For instance, 2D diffusion models can serve as loss functions to optimize 3D implicit models. Rectified Flow, a novel class of generative models, has demonstrated superior performance across various domains. Compared to diffusion-based methods, rectified flow approaches surpass them in terms of generation quality and efficiency. In this work, we present theoretical and experimental evidence demonstrating that rectified flow based methods offer similar functionalities to diffusion models \u2014 they can also serve as effective priors. Besides the generative capabilities of diffusion priors, motivated by the unique time-symmetry properties of rectified flow models, a variant of our method can additionally perform image inversion. Experimentally, our rectified flow based priors outperform their diffusion counterparts \u2014 the SDS and VSD losses \u2014 in text-to-3D generation. Our method also displays competitive performance in image inversion and editing."
    },
    {
        "title": "Graph Assisted Offline-Online Deep Reinforcement Learning for Dynamic Workflow Scheduling",
        "link_suffix": "/forum?id=4PlbIfmX9o",
        "link": "https://openreview.net/forum?id=4PlbIfmX9o",
        "pdf_link": "https://openreview.net/pdf?id=4PlbIfmX9o",
        "keywords": "workflow scheduling, graph attention neural network, reinforcement learning, online learning",
        "abstract": "Dynamic workflow scheduling (DWS) in cloud computing presents substantial challenges due to heterogeneous machine configurations, unpredictable workflow arrivals/patterns, and constantly evolving environments. However, existing research often assumes homogeneous setups and static conditions, limiting flexibility and adaptability in real-world scenarios. In this paper, we propose a novelGraph assisted Offline-Online Deep Reinforcement Learning(GOODRL) approach to building an effective and efficient scheduling agent for DWS. Our approach features three key innovations: (1) atask-specificgraph representation and aGraph Attention Actor Networkthat enable the agent to dynamically assign focused tasks to heterogeneous machines while explicitly considering the future impact of each machine on these tasks; (2) asystem-orientedgraph representation and aGraph Attention Critic Networkthat facilitate efficient processing of new information and understanding its impact on the current state, crucial for managing unpredictable workflow arrivals/patterns in real-time; and (3) anoffline-onlinemethod that utilizes imitation learning for effective offline training and applies gradient control and decoupled high-frequency critic training techniques during online learning to sustain the agent\u2019s robust performance in rapidly changing environments. Experimental results demonstrate that GOODRL significantly outperforms several state-of-the-art algorithms, achieving substantially lower mean flowtime and high adaptability in various online and offline scenarios."
    },
    {
        "title": "MP-Mat: A 3D-and-Instance-Aware Matting Framework with Multiplane Representation",
        "link_suffix": "/forum?id=Xw0fCEMFss",
        "link": "https://openreview.net/forum?id=Xw0fCEMFss",
        "pdf_link": "https://openreview.net/pdf?id=Xw0fCEMFss",
        "keywords": "image matting, image editing",
        "abstract": "Human instance matting aims to estimate an alpha matte for each human instance in an image, which is challenging as it easily fails in complex cases requiring disentangling mingled pixels belonging to multiple instances along hairy and thin boundary structures. In this work, we address this by introducing a novel 3D-and-instance-aware matting framework with multiplane representation, where the multiplane concept is designed from two different perspectives: scene geometry level and instance level. Specifically, we first build feature-level multiplane representations to split the scene into multiple planes based on depth differences. This approach makes the scene representation 3D-aware, and can serve as an effective clue for splitting instances in different 3D positions, thereby improving interpretability and boundary handling ability especially in occlusion areas. Then, we introduce another multiplane representation that splits the scene in an instance-level perspective, and represents each instance with both matte and color. We also treat background as a special instance, which is often overlooked by existing methods. Such an instance-level representation facilitates both foreground and background content awareness, and is useful for other down-stream tasks like image editing. Once built, the representation can be reused to realize controllable instance-level image editing with high efficiency. Extensive experiments validate the clear advantage of MP-Mat in matting task. We also demonstrate its superiority in image editing tasks, an area under-explored by existing matting-focused methods, where our approach under zero-shot inference even outperforms trained specialized image editing techniques by large margins. Code will be released to inspire relevant fields."
    },
    {
        "title": "Pay Attention to What Matters",
        "link_suffix": "/forum?id=iN64nSYt0z",
        "link": "https://openreview.net/forum?id=iN64nSYt0z",
        "pdf_link": "https://openreview.net/pdf?id=iN64nSYt0z",
        "keywords": "Large Language Models, Mechanistic Interventions, Alignment, Transformer Interpretability",
        "abstract": "Despite the remarkable success of Large Language Models (LLMs), they still exhibit a limited capability to align their outputs to the user instructions. In this work, we introduce a simple and effective method, which we name as GUIDE, that mechanistically increases attention scores in instruction tokens. To support this operation, we present Influence, a novel metric that highlights how the user's instructions propagate with transformer layers and impact the LLM output. Our results show that GUIDE improves the accuracy of following certain instructions 29.4% to 60.4 %, outperforming natural prompting alternatives."
    },
    {
        "title": "Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning",
        "link_suffix": "/forum?id=v9CDpLpjiE",
        "link": "https://openreview.net/forum?id=v9CDpLpjiE",
        "pdf_link": "https://openreview.net/pdf?id=v9CDpLpjiE",
        "keywords": "Understanding ambiguous instructions, large multimodal model, chain-of-thoughts, multimodal",
        "abstract": "As large-scale models evolve, language instructions are increasingly utilized in multi-modal tasks. Due to human language habits, these instructions often contain ambiguities in real-world scenarios, necessitating the integration of visual context or common sense for accurate interpretation. However, even highly intelligent large models exhibit significant performance limitations on ambiguous instructions, where weak reasoning abilities of disambiguation can lead to catastrophic errors. To address this issue, this paper proposes Visual-O1, a multi-modal multi-turn chain-of-thought reasoning framework. It simulates human multi-modal multi-turn reasoning, providing instantial experience for highly intelligent models or empirical experience for generally intelligent models to understand ambiguous instructions. Unlike traditional methods that require models to possess high intelligence to understand long texts or perform lengthy complex reasoning, our framework does not significantly increase computational overhead and is more general and effective, even for generally intelligent models. Experiments show that our method not only significantly enhances the performance of models of different intelligence levels on ambiguous instructions but also improves their performance on general datasets. Our work highlights the potential of artificial intelligence to work like humans in real-world scenarios with uncertainty and ambiguity. We will release our data and code."
    },
    {
        "title": "TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram",
        "link_suffix": "/forum?id=5sU32OCxgZ",
        "link": "https://openreview.net/forum?id=5sU32OCxgZ",
        "pdf_link": "https://openreview.net/pdf?id=5sU32OCxgZ",
        "keywords": "Test-time adaptation, domain adaptation, computational geometry",
        "abstract": "Deep learning models often struggle with generalization when deploying on real-world data, due to the common distributional shift to the training data. Test-time adaptation (TTA) is an emerging scheme used at inference time to address this issue. In TTA, models are adapted online at the same time when making predictions to test data. Neighbor-based approaches have gained attention recently, where prototype embeddings provide location information to alleviate the feature shift between training and testing data. However, due to their inherit limitation of simplicity, they often struggle to learn useful patterns and encounter performance degradation. To confront this challenge, we study the TTA problem from a geometric point of view. We first reveal that the underlying structure of neighbor-based methods aligns with the Voronoi Diagram, a classical computational geometry model for space partitioning. Building on this observation, we propose the Test-Time adjustment by Voronoi Diagram guidance (TTVD), a novel framework that leverages the benefits of this geometric property. Specifically, we explore two key structures: 1) Cluster-induced Voronoi Diagram (CIVD): This integrates the joint contribution of self-supervision and entropy-based methods to provide richer information. 2) Power Diagram (PD): A generalized version of the Voronoi Diagram that refines partitions by assigning weights to each Voronoi cell. Our experiments under rigid, peer-reviewed settings on CIFAR-10-C, CIFAR-100-C, ImageNet-C, and ImageNet-R shows that TTVD achieves remarkable improvements compared to state-of-the-art methods. Moreover, extensive experimental results also explore the effects of batch size and class imbalance, which are two scenarios commonly encountered in real-world applications. These analyses further validate the robustness and adaptability of our proposed framework."
    },
    {
        "title": "Revisiting the Design Choices in Max-Return Sequence Modeling",
        "link_suffix": "/forum?id=IUwqJ8VT4F",
        "link": "https://openreview.net/forum?id=IUwqJ8VT4F",
        "pdf_link": "https://openreview.net/pdf?id=IUwqJ8VT4F",
        "keywords": "Sequence Modeling in Offline Reinforcement Learning",
        "abstract": "Decision Transformer (DT), free from optimal value functions fitting and policy gradient computation, attempts to solve offline reinforcement learning (RL) via supervised sequence modeling. During inference, sequence modeling requires an initial target returns assigned with expert knowledge, which blocks comprehensive evaluation on more diverse datasets. As a result, existing sequence modeling only focuses on limited evaluation on Gym datasets and some understanding is severely biased. In this paper, we aim to revisit the design choices, including architecture and context length, in sequence modeling on more diverse datasets. We utilize the max-return sequence modeling that replaces the manual target returns with maximized returns predicted by itself. We systematically investigate the impact of 1) architectural choices and 2) context lengths in max-return sequence modeling on nine datasets with varying data distributions. Abundant experiments and thorough analyses reveal that design choices are highly influenced by the dataset characteristics, which further underscores the significance of more diverse evaluation."
    },
    {
        "title": "Spurious Forgetting in Continual Learning of Language Models",
        "link_suffix": "/forum?id=ScI7IlKGdI",
        "link": "https://openreview.net/forum?id=ScI7IlKGdI",
        "pdf_link": "https://openreview.net/pdf?id=ScI7IlKGdI",
        "keywords": "Continual Learning, Language Models",
        "abstract": "Recent advancements in large language models (LLMs) reveal a perplexing phenomenon in continual learning: despite extensive training, models experience significant performance declines, raising questions about task alignment and underlying knowledge retention. This study first explores the concept of \"spurious forgetting\", proposing that such performance drops often reflect a decline in task alignment rather than true knowledge loss. Through controlled experiments with a synthesized dataset, we investigate the dynamics of model performance during the initial training phases of new tasks, discovering that early optimization steps can disrupt previously established task alignments. Our theoretical analysis connects these shifts to orthogonal updates in model weights, providing a robust framework for understanding this behavior. Ultimately, we introduce a Freezing strategy that fix the bottom layers of the model, leading to substantial improvements in four continual learning scenarios. Our findings underscore the critical distinction between task alignment and knowledge retention, paving the way for more effective strategies in continual learning."
    },
    {
        "title": "From Mismatch to Harmony: Resolving Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation",
        "link_suffix": "/forum?id=JeJ2uTQrF1",
        "link": "https://openreview.net/forum?id=JeJ2uTQrF1",
        "pdf_link": "https://openreview.net/pdf?id=JeJ2uTQrF1",
        "keywords": "Personalized Federated Learning, Data Heterogeneity, Feature-Classifier Mismatch, Prompt-Driven Feature Transformation",
        "abstract": "In conventional Federated Learning approaches like FedAvg, training a global model becomes challenging in the presence of data heterogeneity. To address this, Personalized Federated Learning (PFL) has emerged as a leading solution, enabling clients to train personalized models that are tailored to local data distributions. Surprisingly, our linear probe experiments reveal that FedAvg\u2019s feature extractor outperforms most PFL methods on local client data. Even more intriguingly, applying a simple linear transformation to align local features from FedAvg\u2019s extractor with the classifier enables FedAvg to surpass most PFL methods. These findings suggest that in data heterogeneity scenarios, FedAvg\u2019s weaker performance is not due to inadequate global model training but rather a mismatch between local features and the classifier. This observation motivates us to develop a new framework to address this mismatch problem. A straightforward solution would be to insert the personalized linear transformation layer mentioned above between the global feature extractor and the global classifier. However, this approach can easily overfit the limited local training data due to the large number of personalized parameters, and it is insufficient for handling complex datasets. In this paper, we introduce FedPFT, which leverages personalized prompts to resolve the mismatch problem. These prompts, along with local features, are fed into a shared self-attention-based module, where features are transformed via the attention mechanism to align with the global classifier. These prompts consist of minimal trainable parameters, reducing the risk of overfitting to local data. Additionally, this prompt-driven approach offers strong flexibility, allowing for task-specific prompts to integrate additional training objectives (e.g., contrastive learning) to further enhance performance. Our experiments demonstrate that FedPFT outperforms state-of-the-art methods by up to 5.07%, with additional improvements of up to 7.08% when collaborative contrastive learning is introduced."
    },
    {
        "title": "Optimal Non-Asymptotic Rates of Value Iteration for Average-Reward Markov Decision Processes",
        "link_suffix": "/forum?id=WuTczPV8WC",
        "link": "https://openreview.net/forum?id=WuTczPV8WC",
        "pdf_link": "https://openreview.net/pdf?id=WuTczPV8WC",
        "keywords": "Average-reward Markov decision process, Average-reward MDP, Value iteration, Reinforcement learning theory, RL theroy, Dynamic programming, Convergence analysis, Anchoring mechanism",
        "abstract": "While there is an extensive body of research on the analysis of Value Iteration (VI) for discounted cumulative-reward MDPs, prior work on analyzing VI for (undiscounted) average-reward MDPs has been limited, and most prior results focus on asymptotic rates in terms of Bellman error. In this work, we conduct refined non-asymptotic analyses of average-reward MDPs, obtaining a collection of convergence results advancing our understanding of the setup. Among our new results, most notable are the $\\mathcal{O}(1/k)$-rates of Anchored Value Iteration on the Bellman error under the multichain setup and the span-based complexity lower bound that matches the $\\mathcal{O}(1/k)$ upper bound up to a constant factor of $8$ in the weakly communicating and unichain setups."
    },
    {
        "title": "Student-Informed Teacher Training",
        "link_suffix": "/forum?id=Dzh0hQPpuf",
        "link": "https://openreview.net/forum?id=Dzh0hQPpuf",
        "pdf_link": "https://openreview.net/pdf?id=Dzh0hQPpuf",
        "keywords": "Reinforcement Learning, Imitation Learning, Robotics",
        "abstract": "Imitation learning with a privileged teacher has proven effective for learning complex control behaviors from high-dimensional inputs, such as images. In this framework, a teacher is trained with privileged task information, while a student tries to predict the actions of the teacher with more limited observations, e.g., in a robot navigation task, the teacher might have access to distances to nearby obstacles, while the student only receives visual observations of the scene. However, privileged imitation learning faces a key challenge: the student might be unable to imitate the teacher's behavior due to partial observability. This problem arises because the teacher is trained without considering if the student is capable of imitating the learned behavior. To address this teacher-student asymmetry, we propose a framework for joint training of the teacher and student policies, encouraging the teacher to learn behaviors that can be imitated by the student despite the latters' limited access to information and its partial observability. Based on the performance bound in imitation learning, we add (i) the approximated action difference between teacher and student as a penalty term to the reward function of the teacher, and (ii) a supervised teacher-student alignment step. We motivate our method with a maze navigation task and demonstrate its effectiveness on complex vision-based quadrotor flight and manipulation tasks."
    },
    {
        "title": "When Graph Neural Networks Meet Dynamic Mode Decomposition",
        "link_suffix": "/forum?id=duGygkA3QR",
        "link": "https://openreview.net/forum?id=duGygkA3QR",
        "pdf_link": "https://openreview.net/pdf?id=duGygkA3QR",
        "keywords": "Dynamic Mode Decomposition, Graph Neural Networks",
        "abstract": "Graph Neural Networks (GNNs) have emerged as fundamental tools for a wide range of prediction tasks on graph-structured data. Recent studies have drawn analogies between GNN feature propagation and diffusion processes, which can be interpreted as dynamical systems. In this paper, we delve deeper into this perspective by connecting the dynamics in GNNs to modern Koopman theory and its numerical method, Dynamic Mode Decomposition (DMD). We illustrate how DMD can estimate a low-rank, finite-dimensional linear operator based on multiple states of the system, effectively approximating potential nonlinear interactions between nodes in the graph. This approach allows us to capture complex dynamics within the graph accurately and efficiently. We theoretically establish a connection between the DMD-estimated operator and the original dynamic operator between system states. Building upon this foundation, we introduce a family of DMD-GNN models that effectively leverage the low-rank eigenfunctions provided by the DMD algorithm. We further discuss the potential of enhancing our approach by incorporating domain-specific constraints such as symmetry into the DMD computation, allowing the corresponding GNN models to respect known physical properties of the underlying system. Our work paves the path for applying advanced dynamical system analysis tools via GNNs. We validate our approach through extensive experiments on various learning tasks, including directed graphs, large-scale graphs, long-range interactions, and spatial-temporal graphs. We also empirically verify that our proposed models can serve as powerful encoders for link prediction tasks. The results demonstrate that our DMD-enhanced GNNs achieve state-of-the-art performance, highlighting the effectiveness of integrating DMD into GNN frameworks."
    },
    {
        "title": "Proposer-Agent-Evaluator (PAE): Autonomous Skill Discovery For Foundation Model Internet Agents",
        "link_suffix": "/forum?id=v2D1ASk5MT",
        "link": "https://openreview.net/forum?id=v2D1ASk5MT",
        "pdf_link": "https://openreview.net/pdf?id=v2D1ASk5MT",
        "keywords": "VLM Agent, Web/GUI Agent, VLM, Reinforcement Learning, Skill Discovery",
        "abstract": "The vision of a broadly capable and goal-directed agent, such as an Internet-browsing agent in the digital world and a household humanoid in the physical world, has rapidly advanced, thanks to the generalization capability of foundation models. Such a generalist agent needs to have a large and diverse skill repertoire, such as finding directions between two travel locations and buying specific items from the Internet. If each skill needs to be specified manually through a fixed set of human-annotated instructions, the agent's skill repertoire will necessarily be limited due to the quantity and diversity of human-annotated instructions. In this work, we address this challenge by introducing Proposer-Agent-Evaluator (PAE), a novel framework that enables foundation model agents to autonomously discover and practice skills in the wild. At the heart of PAE is a context-aware task proposer that autonomously proposes tasks for the agent to practice with context information of the websites such as user demos or even just the name of the website itself. Then, the agent policy attempts those tasks in the real world with resulting trajectories evaluated by an autonomous model-based success evaluator. The success evaluation serves as the reward signal for the agent to refine its policies through RL. We validate PAE on challenging vision-based web navigation, using both real-world and self-hosted websites from WebVoyager and WebArena. Our results show that PAE significantly improves the zero-shot generalization capability of VLM Internet agents (more than 30% relative improvement) to both unseen tasks and websites. Our model also achieves an absolute advantage of over 10% (from 22.6% to 33.0%) comparing to other state-of-the-art open source VLM agents including Qwen2VL-72B. To the best of our knowledge, this work represents the first attempt to apply autonomous task proposal with RL for agents, achieving SOTA performance among open-source models. We plan to release our models and code to facilitate further research."
    },
    {
        "title": "Efficient Personalized Federated Learning via Adaptive Weight Clustering Pruning",
        "link_suffix": "/forum?id=W5yncAUfSG",
        "link": "https://openreview.net/forum?id=W5yncAUfSG",
        "pdf_link": "https://openreview.net/pdf?id=W5yncAUfSG",
        "keywords": "Peronalized Federated Learning, Communication Efficiency, Data Heterogeneity, Bandwidth Heterogeneity",
        "abstract": "This paper introduces a novel personalized federated learning approach, Adaptive Federated Weight Clustering Pruning (AdFedWCP), specifically designed to optimize communication efficiency in heterogeneous network environments. AdFedWCP innovatively combines adaptive weight clustering pruning techniques, effectively addressing data and bandwidth heterogeneity. By dynamically adjusting clustering centroids based on layer importance and client-specific data characteristics, it significantly reduces communication overhead. Experimental results show that AdFedWCP achieves a reduction in communication volume ranging from 87.54% to 87.82% in communication volume, surpassing the state-of-the-art work on reducing communication overhead in personalized federated learning. AdFedWCP also surpasses existing methods in terms of accuracy across multiple datasets, with improvements ranging from 9.13% to 21.79% over the baselines on EMNIST, CIFAR-10, and CIFAR-100. These results highlight AdFedWCP\u2019s advantages in balancing communication efficiency and model accuracy, making it an ideal choice for resource-constrained federated learning environments."
    },
    {
        "title": "Memory-Driven Multimodal Chain of Thought for Embodied Long-Horizon Task Planning",
        "link_suffix": "/forum?id=Z1Va3Ue4GF",
        "link": "https://openreview.net/forum?id=Z1Va3Ue4GF",
        "pdf_link": "https://openreview.net/pdf?id=Z1Va3Ue4GF",
        "keywords": "Task Planning; Multimodal Large Language Model; Reasoning",
        "abstract": "Existing methods excel in short-horizon tasks but struggle with complex, long-horizon planning in dynamic environments. To address these limitations, we propose the Memory-Driven Multimodal Chain of Thought (MCoT-Memory), a framework designed to enhance task planning through two key innovations: 1) Evolving Scene Graph-Driven Chain of Thought with CoT Memory Retrieval, which enables the agent to continuously update a scene graph with visual information captured along its trajectory, providing a structured and dynamic representation of the environment that informs real-time decision-making, and uniquely incorporates CoT memory retrieval to allow the agent to leverage past experiences in its reasoning process; 2) Stepwise Confidence-Driven Memory Retention, which employs an expert model to evaluate reasoning across multiple dimensions of accuracy, ensuring that only high-confidence experiences are retained in memory for future retrieval, thus enabling the agent to build on valuable insights and improve performance in long-horizon tasks.\nTo advance long-horizon task planning, we present ExtendaBench, a comprehensive benchmark encompassing 1,198 tasks across two simulators, VirtualHome and Habitat 2.0. The tasks are categorized into ultra-short, short, median, and long tasks. Extensive experiments demonstrate that prior methods struggle with long-horizon tasks, while MCoT-Memory significantly improves performance, marking it as a promising approach for embodied task planning."
    },
    {
        "title": "Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models",
        "link_suffix": "/forum?id=9QYJu1cGfE",
        "link": "https://openreview.net/forum?id=9QYJu1cGfE",
        "pdf_link": "https://openreview.net/pdf?id=9QYJu1cGfE",
        "keywords": "human motion generation, large motion model, large language model",
        "abstract": "Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted towards the development of large motion models. \nDespite some progress, current state-of-the-art works remain far from achieving truly generalist models, largely due to the lack of large-scale, high-quality motion data. \nTo address this, we present MotionBase, the first million-level motion benchmark, offering 15 times the data volume of the previous largest dataset and featuring multimodal data with hierarchically detailed descriptions.\nBy leveraging this vast dataset, our large motion model demonstrates strong performance across a broad range of motions, including unseen ones.\nThrough systematic investigation, we underscore the importance of scaling both data and model size, with synthetic data and pseudo labels playing a crucial role in mitigating data acquisition costs.\nMoreover, our research reveals the limitations of existing evaluation metrics, particularly in handling out-of-domain text instructions --- an issue that has long been overlooked.\nIn addition to these, we introduce a novel 2D lookup-free tokenizer for motion quantization, which preserves motion information and expands codebook capacity, further enhancing the representative ability of large motion models.\nThe release of MotionBase and the insights gained from this study are expected to pave the way for the development of more powerful and versatile motion generation models.\nOur code and database will be released at \\url{https://anonymous.4open.science/r/MotionBase}."
    },
    {
        "title": "OS-ATLAS: Foundation Action Model for Generalist GUI Agents",
        "link_suffix": "/forum?id=n9PDaFNi8t",
        "link": "https://openreview.net/forum?id=n9PDaFNi8t",
        "pdf_link": "https://openreview.net/pdf?id=n9PDaFNi8t",
        "keywords": "GUI agent, language agent, GUI grounding, executable language grounding",
        "abstract": "Existing efforts in building GUI agents heavily rely on the availability of robust commercial Vision-Language Models (VLMs) such as GPT-4o and GeminiPro\u0002Vision. Practitioners are often reluctant to use open-source VLMs due to their significant performance lag compared to their closed-source counterparts, particularly in GUI grounding and Out-Of-Distribution (OOD) scenarios. To facilitate future research in this area, we developed OS-Atlas \u2014a foundational GUI action model that excels at GUI grounding and OOD agentic tasks through innovations in both data and modeling. We have invested substantial engineering effort into developing a toolkit for synthesizing multi-platform GUI grounding data. Lever\u0002aging this toolkit, we are releasing the largest open-source cross-platform GUI grounding corpus to date, which contains over 13 million GUI elements. This dataset, combined with innovations in model training, provides a solid foundation for OS-Atlas to understand GUI screenshots and generalize to unseen interfaces. Through extensive evaluation across six benchmarks spanning three different platforms (mobile, desktop, and web), OS-Atlas demonstrates significant performance improvements over previous state-of-the-art models. Our evaluation also uncovers valuable insights into continuously improving and scaling the agentic capabilities of open-source VLMs. All our data, code, and models will be made publicly available."
    },
    {
        "title": "Zero-shot Concept Bottleneck Models via Sparse Regression of Retrieved Concepts",
        "link_suffix": "/forum?id=5Aem9XFZ0t",
        "link": "https://openreview.net/forum?id=5Aem9XFZ0t",
        "pdf_link": "https://openreview.net/pdf?id=5Aem9XFZ0t",
        "keywords": "concept bottleneck models, interpretability, retrieving, sparse linear regression, vision-language models",
        "abstract": "Concept bottleneck models (CBMs) are inherently interpretable neural network models, which explain their final label prediction by high-level semantic \\textit{concepts} predicted in the intermediate layers. Previous works of CBMs have succeeded in achieving high-accuracy concept/label predictions without manually collected concept labels by incorporating large language models (LLMs) and vision-language models (VLMs). However, they still require training on the target dataset to learn input-to-concept and concept-to-label correspondences, incurring target dataset collections and training resource requirements. In this paper, we present \\textit{zero-shot concept bottleneck models} (Z-CBMs), which are interpretable models predicting labels and concepts in a fully zero-shot manner without training neural networks. Z-CBMs utilize a large-scale concept bank, which is composed of millions of noun phrases extracted from caption datasets, to describe arbitrary input in various domains. To infer the input-to-concept correspondence, we introduce \\textit{concept retrieval}, which dynamically searches input-related concepts from the concept bank on the multi-modal feature space of pre-trained VLMs. This enables Z-CBMs to handle the millions of concepts and extract appropriate concepts for each input image. In the concept-to-label inference stage, we apply \\textit{concept regression} to select important concepts from the retrieved concept candidates containing noisy concepts related to each other. To this end, concept regression estimates the importance weight of concepts with sparse linear regression approximating the input image feature vectors by the weighted sum of concept feature vectors. Through extensive experiments, we confirm that our Z-CBMs achieve both high target task performance and interpretability without any additional training."
    },
    {
        "title": "Adversarial Suffixes May Be Features Too!",
        "link_suffix": "/forum?id=eyBkAAeSP0",
        "link": "https://openreview.net/forum?id=eyBkAAeSP0",
        "pdf_link": "https://openreview.net/pdf?id=eyBkAAeSP0",
        "keywords": "LLM, adversarial attacks, safety alignment, robustness",
        "abstract": "Despite significant ongoing efforts in safety alignment, large language models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks that can induce harmful behaviors, including those triggered by adversarial suffixes. Building on prior research, we hypothesize that these adversarial suffixes are not mere bugs but may represent features that can dominate the LLM's behavior. To evaluate this hypothesis, we conduct several experiments. First, we demonstrate that benign features can be effectively made to function as adversarial suffixes, i.e., we develop a feature extraction method to extract sample-agnostic features from benign dataset in the form of suffixes and show that these suffixes may effectively compromise safety alignment. Second, we show that adversarial suffixes generated from jailbreak attacks may contain meaningful features, i.e., appending the same suffix to different prompts results in responses exhibiting specific characteristics. Third, we show that such benign-yet-safety-compromising features can be easily introduced through fine-tuning using only benign datasets, i.e., even in the absence of harmful content. This highlights the critical risk posed by dominating benign features in the training data and calls for further research to reinforce LLM safety alignment. Our code and data is available at \\url{https://github.com/anonymous}."
    }
]