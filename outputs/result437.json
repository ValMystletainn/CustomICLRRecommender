[{"title": "Navigating Concept Drift and Temporal Shift: Distribution Shift Generalized Time-Series Forecasting", "link_suffix": "/forum?id=Klx0Rq9vbC", "link": "https://openreview.net/forum?id=Klx0Rq9vbC", "pdf_link": "https://openreview.net/pdf?id=Klx0Rq9vbC", "keywords": "Time-Series Forecasting, Distribution Shift Generalization", "abstract": "Time-series forecasting finds broad applications in real-world scenarios. Due to the dynamic nature of time series data, it is crucial for time-series forecasting models to produce robust predictions under potential distribution shifts. In this paper, we initially identify two types of distribution shifts in time series: concept drift and temporal shift. We acknowledge that while existing studies primarily focus on addressing temporal shift issues in time series, designing proper concept drift methods for time series data received comparatively less attention.Motivated by the need to mitigate potential concept drift issues in time-series forecasting, this work proposes a novel soft attention mechanism that effectively leverages and ensemble information from the horizon time series. Furthermore, recognizing that both concept drift and temporal shift could occur concurrently in time-series forecasting scenarios while an integrated solution remains missing, this paper introduces ShifTS, a model-agnostic framework seamlessly addressing both concept drift and temporal shift issues in time-series forecasting. Extensive experiments demonstrate the efficacy of ShifTS in consistently enhancing the forecasting accuracy of agnostic models across multiple datasets, and consistently outperforming existing concept drift, temporal shift, and combined baselines.", "title_embedding_index": 21800, "title_abs_embedding_index": 21825}, {"title": "Remove Symmetries to Control Model Expressivity", "link_suffix": "/forum?id=Gv0TOAigIY", "link": "https://openreview.net/forum?id=Gv0TOAigIY", "pdf_link": "https://openreview.net/pdf?id=Gv0TOAigIY", "keywords": "model capacity, symmetry", "abstract": "When symmetry is present in the loss function, the model is likely to be trapped in a low-capacity state that is sometimes known as a ``collapse.\" Being trapped in these low-capacity states can be a major obstacle to training across many scenarios where deep learning technology is applied. We first prove two concrete mechanisms through which symmetries lead to reduced capacities and ignored features during training. We then propose a simple and theoretically justified algorithm, \\textit{syre}, to remove almost all symmetry-induced low-capacity states in neural networks. The proposed method is shown to improve the training of neural networks in scenarios when this type of entrapment is especially a concern. A remarkable merit of the proposed method is that it is model-agnostic and does not require any knowledge of the symmetry.", "title_embedding_index": 21801, "title_abs_embedding_index": 21826}, {"title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models", "link_suffix": "/forum?id=HnhNRrLPwm", "link": "https://openreview.net/forum?id=HnhNRrLPwm", "pdf_link": "https://openreview.net/pdf?id=HnhNRrLPwm", "keywords": "large vision-language model, interleaved text-and-image evaluation", "abstract": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs.", "title_embedding_index": 21802, "title_abs_embedding_index": 21827}, {"title": "Modumer: Modulating Transformer for Image Restoration", "link_suffix": "/forum?id=RJG7fCVkhQ", "link": "https://openreview.net/forum?id=RJG7fCVkhQ", "pdf_link": "https://openreview.net/pdf?id=RJG7fCVkhQ", "keywords": "Image restoration, Transformer block, Modulation design", "abstract": "Image restoration aims to recover clean images from degraded versions. While Transformer-based approaches have achieved significant advancements in this field, they are limited by high complexity and their inability to capture omni-range dependencies, hindering their overall performance. In this work, we develop Modumer for effective and efficient image restoration by revisiting the Transformer block and Modulation design, which processes input through a convolutional block and projection layers, and fuses features via element-wise multiplication. Specifically, within each unit of Modumer, we integrate the cascaded Modulation design with the downsampled Transformer block to build the attention layers, enabling omni-kernel modulation and mapping inputs into high-dimensional feature spaces. Moreover, we introduce a bioinspired parameter-sharing mechanism to attention layers, which not only enhances efficiency but also improves performance. Additionally, a dual-domain feed-forward network strengthens the representational power of the model. Extensive experiments demonstrate that the proposed Modumer achieves state-of-the-art performance on ten different datasets for five image restoration tasks: image motion deblurring, image deraining, image dehazing, image desnowing, and low-light image enhancement. Furthermore, our model yields promising performance on all-in-one image restoration tasks.", "title_embedding_index": 21803, "title_abs_embedding_index": 21828}, {"title": "Effective Interplay between Sparsity and Quantization: From Theory to Practice", "link_suffix": "/forum?id=wJv4AIt4sK", "link": "https://openreview.net/forum?id=wJv4AIt4sK", "pdf_link": "https://openreview.net/pdf?id=wJv4AIt4sK", "keywords": "theory of compression, model compression, quantization, max-scaled numerical encoding, sparsity, unstructured sparsity, structured sparsity, N:M sparsity, large language models, magnitude pruning, post-training quantization, efficient inference", "abstract": "The increasing size of deep neural networks (DNNs) necessitates effective model compression to reduce their computational and memory footprints. Sparsity and quantization are two prominent compression methods that have been shown to reduce DNNs' computational and memory footprints significantly while preserving model accuracy. However, how these two methods interact when combined together remains a key question for developers, as many tacitly assume that they are orthogonal, meaning that their combined use does not introduce additional errors beyond those introduced by each method independently. In this paper, we provide the first mathematical proof that sparsity and quantization are non-orthogonal. We corroborate these results with experiments spanning a range of large language models, including the OPT and LLaMA model families (with 125M to 8B parameters), and vision models like ViT and ResNet. We show that the order in which we apply these methods matters because applying quantization before sparsity may disrupt the relative importance of tensor elements, which may inadvertently remove significant elements from a tensor. More importantly, we show that even if applied in the correct order, the compounded errors from sparsity and quantization can significantly harm accuracy. Our findings extend to the efficient deployment of large models in resource-constrained compute platforms to reduce serving cost, offering insights into best practices for applying these compression methods to maximize hardware resource efficiency without compromising accuracy.", "title_embedding_index": 21804, "title_abs_embedding_index": 21829}, {"title": "DiffusionTrend: A Minimalist Approach to Virtual Fashion Try-On", "link_suffix": "/forum?id=w5Q3r8Jq3v", "link": "https://openreview.net/forum?id=w5Q3r8Jq3v", "pdf_link": "https://openreview.net/pdf?id=w5Q3r8Jq3v", "keywords": "Virtual Try-on; Diffusion Model; Image Editing", "abstract": "In this paper, we introduce DiffusionTrend, a pioneering approach for virtual fashion try-on that forgoes the need for training diffusion models, thereby offering simple, conventional pose virtual try-on services with significantly reduced computational overhead. By leveraging advanced diffusion models, DiffusionTrend harnesses latents rich with prior information to capture the nuances of garment details. Throughout the diffusion denoising process, these details are seamlessly integrated into the model image generation, expertly directed by a precise garment mask crafted by a lightweight and compact CNN. Although our DiffusionTrend model initially demonstrates suboptimal metric performance, our exploratory approach offers several significant advantages: (1) It circumvents the need for resource-intensive training of diffusion models on large datasets. (2) It eliminates the necessity for various complex and user-unfriendly model inputs. (3) It delivers a visually compelling virtual try-on experience, underscoring the potential of training-free diffusion models for future research within the community. Overall, this initial foray into the application of untrained diffusion models in virtual try-on technology paves the way for further exploration and refinement in this innovative field.", "title_embedding_index": 21805, "title_abs_embedding_index": 21830}, {"title": "Concept Bottleneck Models under Label Noise", "link_suffix": "/forum?id=a8wjeqTZ9C", "link": "https://openreview.net/forum?id=a8wjeqTZ9C", "pdf_link": "https://openreview.net/pdf?id=a8wjeqTZ9C", "keywords": "Concept bottleneck models, Label noise, Sharpness-aware minimization", "abstract": "Concept bottleneck models (CBMs) are a class of interpretable neural network models that make the final predictions based on intermediate representations known as concepts. With these concepts being human-interpretable, CBMs enable one to better understand the decisions made by neural networks. Despite this advantage, we find that CBMs face a critical limitation: they require additional labeling efforts for concept annotation, which can easily increase the risk of mislabeling, i.e., CBMs need to be trained with noisy labels. In this work, we systematically investigate the impact of label noise on CBMs, demonstrating that it can significantly compromise both model performance and interpretability. Specifically, we measure the impact of varying levels of label noise across different training schemes, through diverse lenses including extensive numerical evaluations, feature visualizations, and in-depth analysis of individual concepts, identifying key factors contributing to the breakdowns and establishing a better understanding of underlying challenges. To mitigate these issues, we propose leveraging a robust optimization technique called sharpness-aware minimization (SAM). By improving the quality of intermediate concept predictions, SAM enhances both the subsequent concept-level interpretability and final target prediction performance.", "title_embedding_index": 21806, "title_abs_embedding_index": 21831}, {"title": "ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models", "link_suffix": "/forum?id=2OegVbwvY2", "link": "https://openreview.net/forum?id=2OegVbwvY2", "pdf_link": "https://openreview.net/pdf?id=2OegVbwvY2", "keywords": "vision-language models, prompt-tuning, black-box optimization, zeroth-order optimization", "abstract": "Recent research has introduced various approaches for prompt-tuning black-box vision-language models, referred to as black-box prompt-tuning (BBPT). While BBPT has demonstrated considerable potential, it is often found that many existing methods require an excessive number of queries (i.e., function evaluations), which poses a significant challenge in real-world scenarios where the number of allowed queries is limited. To tackle this issue, we propose Zeroth-order Intrinsic-dimensional Prompt-tuning (ZIP), a novel approach that enables efficient and robust prompt optimization in a purely black-box setting. The key idea of ZIP is to reduce the problem dimensionality and the variance of zeroth-order gradient estimates, such that the training is done fast with far less queries. We achieve this by re-parameterizing prompts in low-rank representations and designing intrinsic-dimensional clipping of gradients. We evaluate ZIP on 13+ vision-language tasks in standard benchmarks and show that it achieves an average improvement of approximately 6% in few-shot accuracy and 48% in query efficiency compared to the best-performing alternative BBPT methods, establishing a new state of the art. Our ablation analysis further shows that the proposed clipping mechanism is robust and nearly optimal, without the need to manually select the clipping threshold, matching the result of expensive hyperparameter search.", "title_embedding_index": 21807, "title_abs_embedding_index": 21832}, {"title": "IFORMER: INTEGRATING CONVNET AND TRANSFORMER FOR MOBILE APPLICATION", "link_suffix": "/forum?id=4ytHislqDS", "link": "https://openreview.net/forum?id=4ytHislqDS", "pdf_link": "https://openreview.net/pdf?id=4ytHislqDS", "keywords": "Lightweight Networks, Efficient Networks, Vision Transformers, Classification", "abstract": "We present a new family of mobile hybrid vision networks, called iFormer, with a\nfocus on optimizing latency and accuracy on mobile applications. iFormer effectively\nintegrates the fast local representation capacity of convolution with the efficient\nglobal modeling ability of self-attention. The local interactions are derived\nfrom transforming a standard convolutional network, i.e., ConvNeXt, to design a\nmore lightweight mobile network. Our newly introduced mobile modulation attention\nremoves memory-intensive operations in MHA and employs an efficient\nmodulation mechanism to boost dynamic global representational capacity. We\nconduct comprehensive experiments demonstrating that iFormer outperforms existing\nlightweight networks across various tasks. Notably, iFormer achieves an\nimpressive Top-1 accuracy of 80.4% on ImageNet-1k with a latency of only 1.10\nms on an iPhone 13, surpassing the recently proposed MobileNetV4 under similar\nlatency constraints. Additionally, our method shows significant improvements in\ndownstream tasks, including COCO object detection, instance segmentation, and\nADE20k semantic segmentation, while still maintaining low latency on mobile\ndevices for high-resolution inputs in these scenarios. The source code and trained\nmodels will be available soon.", "title_embedding_index": 21808, "title_abs_embedding_index": 21833}, {"title": "Variational Inference with Unnormalized Priors", "link_suffix": "/forum?id=pu7a7JHW20", "link": "https://openreview.net/forum?id=pu7a7JHW20", "pdf_link": "https://openreview.net/pdf?id=pu7a7JHW20", "keywords": "variational inference, variational autoencoders, generative models, energy-based models", "abstract": "Variational inference typically assumes normalized priors, limiting the expressiveness of generative models like Variational Autoencoders (VAEs). In this work, we propose a novel approach by replacing the prior \ud835\udc5d(\ud835\udc67) with an unnormalized energy-based distribution \nexp(\u2212\ud835\udc38(\ud835\udc67))/\ud835\udc4d, where \ud835\udc38(\ud835\udc67) is the energy function and \ud835\udc4d is the partition function. This leads to a variational lower bound that allows for two key innovations: (1) the incorporation of more powerful, flexible priors into the VAE framework, resulting in improved likelihood estimates and enhanced generative performance, and (2) the ability to train energy-based models (EBMs) without the need for computationally expensive Markov chain sampling, requiring only a small \ud835\udc5b > 1 importance samples from the posterior distribution. Our approach bridges VAEs and EBMs, providing a scalable and efficient framework for leveraging unnormalized priors in probabilistic models.", "title_embedding_index": 21809, "title_abs_embedding_index": 21834}, {"title": "SegFit: Robust SMPL-X Fitting with Body Part Segmentation on Real-World Point Clouds", "link_suffix": "/forum?id=HW8xnOUcBx", "link": "https://openreview.net/forum?id=HW8xnOUcBx", "pdf_link": "https://openreview.net/pdf?id=HW8xnOUcBx", "keywords": "Multi Human Body Part Segmentation, Human Pose Estimation", "abstract": "Fitting parametric human body models to 3D point cloud is crucial for applications such as virtual reality and human-robot interaction but remains challenging due to the lack of contextual guidance, often leading to imprecise results. To address this, we propose a hybrid approach that incorporates body part segmentation into the fitting process, enhancing pose estimation and segmentation accuracy. \nOur method starts with an initial segmentation, assigning each point to a specific body part. This segmentation guides a two-step optimization in fitting an SMPL-X model: first, approximating the initial pose and orientation using body part centroids, and second, refining the model by considering the entire point cloud. After fitting, we reassign body parts to the point cloud through nearest-neighbor matching, resulting in more accurate segmentation. This enhanced segmentation serves as pseudo ground truth to fine-tune the segmentation network in a self-supervised manner, creating a feedback loop where improvements in pose fitting lead to better segmentation and vice versa. We evaluate our approach on four challenging datasets -- PosePrior, EgoBody, BEHAVE, and Hi4D -- demonstrating significant improvements over leading methods, including a tenfold increase in pose modeling accuracy and a 15% enhancement in segmentation accuracy after fine-tuning. Our contributions are twofold: (1) introducing a novel hybrid method that unifies pose fitting and body part segmentation on point clouds, enabling mutual enhancement through iterative refinement; and (2) developing a self-supervised technique for fine-tuning segmentation networks using pseudo ground truths derived from fitted models. This work advances the state of the art in human body fitting to point clouds, facilitating more accurate human representations in complex environments and benefiting applications that require precise human modeling. We will make the source code publicly available.", "title_embedding_index": 21810, "title_abs_embedding_index": 21835}, {"title": "Selecting Influential Samples for Long Context Alignment via Homologous Models\u2019 Guidance and Contextual Awareness Measurement", "link_suffix": "/forum?id=E7ecidOeCE", "link": "https://openreview.net/forum?id=E7ecidOeCE", "pdf_link": "https://openreview.net/pdf?id=E7ecidOeCE", "keywords": "Long context alignment, Large language models, Data selection, Efficient instruction tuning", "abstract": "The expansion of large language models to effectively handle instructions with extremely long contexts has yet to be fully investigated. The primary obstacle lies in constructing a high-quality long instruction-following dataset devised for long context alignment. Existing studies have attempted to scale up the available data volume by synthesizing long instruction-following samples. However, indiscriminately increasing the quantity of data without a well-defined strategy for ensuring data quality may introduce low-quality samples and restrict the final performance. To bridge this gap, we aim to address the unique challenge of long-context alignment, i.e., modeling the long-range dependencies for handling instructions and lengthy input contexts. We propose GATEAU, a novel framework designed to identify the influential and high-quality samples enriched with long-range dependency relations by utilizing crafted Homologous Models' Guidance (HMG) and Contextual Awareness Measurement (CAM). Specifically, HMG attempts to measure the difficulty of generating corresponding responses due to the long-range dependencies, using the perplexity scores of the response from two homologous models with different context windows. Also, the role of CAM is to measure the difficulty of understanding the long input contexts due to long-range dependencies by evaluating whether the model\u2019s attention is focused on important segments. Built upon both proposed methods, we select the most challenging samples as the influential data to effectively frame the long-range dependencies, thereby achieving better performance of LLMs. Comprehensive experiments indicate that GATEAU effectively identifies samples enriched with long-range dependency relations and the model trained on these selected samples exhibits better instruction-following and long-context understanding capabilities.", "title_embedding_index": 21811, "title_abs_embedding_index": 21836}, {"title": "Do as We Do, Not as You Think: the Conformity of Large Language Models", "link_suffix": "/forum?id=st77ShxP1K", "link": "https://openreview.net/forum?id=st77ShxP1K", "pdf_link": "https://openreview.net/pdf?id=st77ShxP1K", "keywords": "Large Language Models, Conformity, Multi-agent System", "abstract": "Recent advancements in large language models (LLMs) revolutionize the field of intelligent agents, enabling collaborative multi-agent systems capable of tackling complex problems across various domains. However, the potential of conformity within these systems, analogous to phenomena like conformity bias and group-think in human group dynamics, remains largely unexplored, raising concerns about their collective problem-solving capabilities and possible ethical implications. This paper presents a comprehensive study on conformity in LLM-driven multi-agent systems, focusing on three aspects: the existence of conformity, the factors influencing conformity, and potential mitigation strategies. In particular, we introduce BENCHFORM, a new conformity-oriented benchmark, featuring reasoning-intensive tasks and five distinct interaction protocols designed to probe LLMs\u2019 behavior in collaborative scenarios. Several representative LLMs are evaluated on BENCHFORM, using metrics such as conformity rate and independence rate to quantify conformity\u2019s impact. Our analysis delves into factors influencing conformity, including interaction time and majority size, and examines how the subject agent rationalize its conforming behavior. Furthermore, we explore two strategies to mitigate conformity effects, i.e., developing enhanced persona and implementing a reflection mechanism. Several interesting findings regarding LLMs\u2019 conformity are derived from empirical results and case studies. We hope that these insights can pave the way for more robust and ethically-aligned collaborative AI systems. Our benchmark and code will be publicly available.", "title_embedding_index": 21812, "title_abs_embedding_index": 21837}, {"title": "Stable Offline Value Function Learning with Bisimulation-based Representations", "link_suffix": "/forum?id=JMe5FW8c3e", "link": "https://openreview.net/forum?id=JMe5FW8c3e", "pdf_link": "https://openreview.net/pdf?id=JMe5FW8c3e", "keywords": "reinforcement learning, representation learning, off-policy, offline policy evaluation, bisimulations, stability, value function learning, abstractions", "abstract": "In reinforcement learning, offline value function learning is the procedure of using an offline dataset to estimate the expected discounted return from each state when taking actions according to a fixed target policy. The stability of this procedure, i.e., whether it converges to its fixed-point, critically depends on the representations of the state-action pairs. Poorly learned representations can make value function learning unstable, or even divergent. Therefore, it is critical to stabilize value function learning by explicitly shaping the state-action representations. Recently, the class of bisimulation-based algorithms have shown promise in shaping representations for control. However, it is still unclear if this class of methods can \\emph{stabilize} value function learning. In this work, we investigate this question and answer it affirmatively. We introduce a bisimulation-based algorithm called kernel representations for offline policy evaluation (\\textsc{krope}). \\textsc{krope} uses a kernel to shape state-action representations such that state-action pairs that have similar immediate rewards and lead to similar next state-action pairs under the target policy also have similar representations. We show that \\textsc{krope}: 1) learns stable representations and 2) leads to lower value error than baselines. Our analysis provides new theoretical insight into the stability properties of bisimulation-based methods and suggests that practitioners can use these methods for stable and accurate evaluation of offline reinforcement learning agents.", "title_embedding_index": 21813, "title_abs_embedding_index": 21838}, {"title": "kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers", "link_suffix": "/forum?id=49v8meXjHS", "link": "https://openreview.net/forum?id=49v8meXjHS", "pdf_link": "https://openreview.net/pdf?id=49v8meXjHS", "keywords": "efficient transformers, self-attention mechanism, sublinear algorithms, sampling, k-nearest neighbors", "abstract": "Despite their power, Transformers \\citep{vaswani2017attention} face challenges with long sequences due to the quadratic complexity of self-attention. To address this limitation, methods like k-Nearest-Neighbor ($k$NN) attention have been introduced \\citep{roy2021efficient}, enabling each token to attend to only its $k$ closest tokens. While $k$NN attention has shown empirical success in making Transformers more efficient, its exact approximation guarantees have not been theoretically analyzed. In this work, we establish a theoretical framework for $k$NN attention, reformulating self-attention as expectations over softmax distributions and leveraging lazy Gumbel sampling \\citep{mussmann2017fast} with $k$NN indices for efficient approximation. Building on this framework, we also propose novel sub-quadratic algorithms that approximate self-attention gradients by leveraging efficient sampling techniques, such as Markov Chain-based estimation. Finally, we demonstrate the practical effectiveness of these algorithms through empirical experiments, showcasing their benefits in both training and inference.", "title_embedding_index": 21814, "title_abs_embedding_index": 21839}, {"title": "LEGO-Compiler: Enhancing Neural Compilation Through Composable Chain of Thought", "link_suffix": "/forum?id=mS7xin7BPK", "link": "https://openreview.net/forum?id=mS7xin7BPK", "pdf_link": "https://openreview.net/pdf?id=mS7xin7BPK", "keywords": "code translation, neural compilation, chain of thought, scalability, in-context learning", "abstract": "Large language models (LLMs) have the potential to revolutionize how we design and implement compilers and code translation tools. However, existing LLMs struggle to handle long and complex programs.  We introduce LEGO-Compiler, a novel neural compilation system that leverages LLMs to translate high-level languages into assembly code. Our approach centers on three key innovations: LEGO translation, which decomposes large programs into manageable blocks; annotation-based Chain-of-Thoughts, guiding LLMs through the compilation process with LLM-annotated context; and a feedback mechanism for self-correction. Supported by formal proofs of code composability, LEGO-Compiler demonstrates high accuracy on multiple datasets including over 99% on ExeBench and 100% on industrial-grade CoreMark, and successfully handles programs far exceeding the length limitations of native LLM translation. This work opens new avenues for applying LLMs to system-level tasks, complementing traditional compiler technologies.", "title_embedding_index": 21815, "title_abs_embedding_index": 21840}, {"title": "Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios", "link_suffix": "/forum?id=SDV7Y6Dhx9", "link": "https://openreview.net/forum?id=SDV7Y6Dhx9", "pdf_link": "https://openreview.net/pdf?id=SDV7Y6Dhx9", "keywords": "dataset distillation", "abstract": "Dataset distillation has demonstrated strong performance on simple datasets like CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in more complex scenarios. \nIn this paper, we propose a novel approach that \\textbf{e}mphasizes the \\textbf{d}iscriminative \\textbf{f}eatures (obtained by Grad-CAM) for dataset distillation, called \\textbf{EDF}.\nOur approach is inspired by a key observation: in simple datasets, high-activation areas typically occupy most of the image, whereas in complex scenarios, the size of these areas is much smaller.\nUnlike previous methods that treat all pixels equally when synthesizing images, EDF uses Grad-CAM activation maps to enhance high-activation areas.\nFrom a supervision perspective, we downplay supervision signals that have lower losses, as they contain common patterns.\nAdditionally, to help the DD community better explore complex scenarios, we build the Complex Dataset Distillation (Comp-DD) benchmark by meticulously selecting sixteen subsets, eight easy and eight hard, from ImageNet-1K.\nNotably, EDF consistently outperforms SOTA results in complex scenarios, such as ImageNet-1K subsets.\nHopefully, more researchers will be inspired and encouraged to enhance the practicality and efficacy of DD. \nOur code and benchmark will be made public.", "title_embedding_index": 21816, "title_abs_embedding_index": 21841}, {"title": "Query Efficient Nonsmooth Stochastic Black-Box Bilevel Optimization with Bregman Distance", "link_suffix": "/forum?id=v2uPdQDwSz", "link": "https://openreview.net/forum?id=v2uPdQDwSz", "pdf_link": "https://openreview.net/pdf?id=v2uPdQDwSz", "keywords": "zeroth-order gradient, bilevel optimization", "abstract": "Bilevel optimization (BO) has recently gained significant attention in various machine learning applications due to its ability to model the hierarchical structures inherent in these problems. Several gradient-free methods have been proposed to address stochastic black-box bilevel optimization problems, where the gradients of both the upper and lower-level objective functions are unavailable. However, these methods suffer from high query complexity and do not accommodate more general bilevel problems involving nonsmooth regularization. In this paper, we present a query-efficient method that effectively leverages Bregman distance to solve nonsmooth stochastic black-box bilevel optimization problems. More importantly, we provide a non-asymptotic convergence analysis, showing that our method requires only $\\mathcal{O}({d_1(d_1+d_2)^2}{\\epsilon^{-2}})$ queries to reach the $\\epsilon$-stationary point. Additionally, we conduct experiments on data hyper-cleaning and hyper-representation learning tasks, demonstrating that our algorithms outperform existing bilevel optimization methods.", "title_embedding_index": 21817, "title_abs_embedding_index": 21842}, {"title": "Optimizing Large Language Models with Automatic Speech Recognition for Medication Corpus in Low-Resource Healthcare Settings.", "link_suffix": "/forum?id=gpKEDj9Dgg", "link": "https://openreview.net/forum?id=gpKEDj9Dgg", "pdf_link": "https://openreview.net/pdf?id=gpKEDj9Dgg", "keywords": "Automatic Speech Recognition, Large Language Models, Healthcare, Low Resource Settings", "abstract": "Automatic Speech Recognition (ASR) systems, while effective in general contexts, often face challenges in low-resource settings, especially in specialized domains such as healthcare. This study investigates the integration of Large Language Models (LLMs) with ASR systems to improve transcription accuracy in such environments. Focusing on medication-related conversations in healthcare, we fine-tuned the Whisper-Large ASR model on a custom dataset, Pharma-Speak, and applied the LLaMA 3 model for second-pass rescoring to correct ASR output errors. To achieve efficient fine-tuning without altering the full LLM parameters, we employed Low-Rank Adaptation (LoRA), which enables re-ranking of the ASR\u2019s N-best hypotheses while retaining the LLM's original knowledge. \nOur results demonstrate a significant reduction in Word Error Rate (WER) across multiple epochs, validating the effectiveness of the LLM-based rescoring method. The integration of LLMs in this framework shows potential for overcoming the limitations posed by conventional ASR models in low-resource settings. While computational constraints and the inherent strength of Whisper-Large presented some limitations, our approach lays the groundwork for further exploration of domain-specific ASR enhancements using LLMs, particularly in healthcare applications.", "title_embedding_index": 21818, "title_abs_embedding_index": 21843}, {"title": "RED: Efficiently Boosting Ensemble Robustness via Random Sampling Inference", "link_suffix": "/forum?id=sNoJSfGh6y", "link": "https://openreview.net/forum?id=sNoJSfGh6y", "pdf_link": "https://openreview.net/pdf?id=sNoJSfGh6y", "keywords": "Adversarial Robustness, Ensemble Defences, Randomness, Hypernetworks, Computer Vision", "abstract": "Despite the remarkable achievements of Deep Neural Networks (DNNs) in handling diverse tasks, these high-performing models remain susceptible to adversarial attacks. Considerable research has focused on bolstering the robustness of individual models and subsequently employing a simple ensemble defense strategy. However, existing ensemble techniques tend to increase the inference latency and the parameter number while achieving suboptimal robustness, which motivates us to reconsider the framework of model ensemble. To address the challenge of suboptimal robustness and inference latency, we introduce a novel ensemble defense approach called Random Ensemble Defense (RED).  Specifically, we expedite inference via random sampling, which also makes it difficult for an attacker to attack a model ensemble. To effectively train a model ensemble, it is crucial to diversify the adversarial vulnerabilities among its members. This can be approached by reducing the adversarial transferability among them. To this end, we propose incorporating gradient similarity and Lipschitz regularizers into the training process. Moreover, to overcome the obstacle of a large number of parameters, we develop a parameter-lean version of RED (PS-RED). Extensive experiments, conducted across popular datasets, demonstrate that the proposed methods not only significantly improve ensemble robustness but also minimize inference delays and optimize storage usage for ensemble models. For example, our models enhance robust accuracy by approximately 15% (RED) and save parameters by approximately 90% (PS-RED) on CIFAR-10 compared with the most recent baselines.", "title_embedding_index": 21819, "title_abs_embedding_index": 21844}, {"title": "ARTIFICIAL KURAMOTO OSCILLATORY NEURONS", "link_suffix": "/forum?id=nwDRD4AMoN", "link": "https://openreview.net/forum?id=nwDRD4AMoN", "pdf_link": "https://openreview.net/pdf?id=nwDRD4AMoN", "keywords": "Oscillatory neurons, Feature binding, Object-centric learning, Reasoning, Adversarial robustness", "abstract": "It has long been known in both neuroscience and AI that ``binding'' between neurons leads to a form of competitive learning where representations are compressed in order to represent more abstract concepts in deeper layers of the network. More recently, it was also hypothesized that dynamic (spatiotemporal) representations play an important role in both neuroscience and AI. Building on these ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamical alternative to threshold units, which can be combined with arbitrary connectivity designs such as fully connected, convolutional, or attentive mechanisms. Our generalized Kuramoto updates bind neurons together through their synchronization dynamics. We show that this idea provides performance improvements across a wide spectrum of tasks such as unsupervised object discovery, adversarial robustness, calibrated uncertainty quantification, and reasoning. We believe that these empirical results show the importance of rethinking our assumptions at the most basic neuronal level of neural representation, and in particular show the importance of dynamical representations.", "title_embedding_index": 21820, "title_abs_embedding_index": 21845}, {"title": "Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions", "link_suffix": "/forum?id=TDuxzV3Efo", "link": "https://openreview.net/forum?id=TDuxzV3Efo", "pdf_link": "https://openreview.net/pdf?id=TDuxzV3Efo", "keywords": "Diffusion Models, Computer Vision", "abstract": "Recent advances in text-to-image (T2I) diffusion models have significantly improved the quality of generated images. However, providing efficient control over individual subjects, particularly the attributes characterizing them, remains a key challenge. While existing methods have introduced mechanisms to modulate attribute expression, they typically provide either detailed, object-specific localization of such a modification or fine-grained, nuanced control of attributes. \nNo current approach offers both simultaneously, resulting in a gap when trying to achieve precise continuous and subject-specific attribute modulation in image generation.\nIn this work, we demonstrate that token-level directions exist within commonly used CLIP text embeddings that enable fine-grained, subject-specific control of high-level attributes in T2I models. We introduce two methods to identify these directions: a simple, optimization-free technique and a learning-based approach that utilizes the T2I model to characterize semantic concepts more specifically. Our methods allow the augmentation of the prompt text input, enabling fine-grained control over multiple attributes of individual subjects simultaneously, without requiring any modifications to the diffusion model itself. This approach offers a unified solution that fills the gap between global and localized control, providing competitive flexibility and precision in text-guided image generation.", "title_embedding_index": 21821, "title_abs_embedding_index": 21846}, {"title": "Sequence Denoising with Self-Augmentation for Knowledge Tracing", "link_suffix": "/forum?id=7dufGaLYF8", "link": "https://openreview.net/forum?id=7dufGaLYF8", "pdf_link": "https://openreview.net/pdf?id=7dufGaLYF8", "keywords": "knowledge tracing\uff0csequence denoising\uff0cdata augmentation\uff0cai for education", "abstract": "Knowledge tracing (KT) aims to predict students' future knowledge levels based on their historical interaction sequences. Most KT methods rely on interaction data between students and questions to assess knowledge states and these approaches typically assume that the interaction data is reliable. In fact, on the one hand, factors such as guessing or slipping could inevitably bring in noise in sequences. On the other hand, students' interaction sequences are often sparse, which could amplify the impact of noise, further affecting the accurate assessment of knowledge states. Although data augmentation which is always adopted in KT could alleviate data sparsity, it also brings noise again during the process. Therefore, denoising strategy is urgent and it should be employed not only on the original sequences but also on the augmented sequences. To achieve this goal, we adopt a plug and play denoising framework in our method. The denoising technique is adopted not only on the  original and the enhanced sequences separately during the data augmentation process, but also we explore the hard noise through the comparison between the two streams. During the denoising process, we employ a novel strategy for selecting data samples to balance the hard and soft noise leveraging Singular Value Decomposition (SVD). This approach optimizes the ratio of explicit to implicit denoising and combines them to improve feature representation. Extensive experiments on four real-world datasets demonstrate that our method not only enhances accuracy but also maintains model interpretability.", "title_embedding_index": 21822, "title_abs_embedding_index": 21847}, {"title": "Diffusion Feedback Helps CLIP See Better", "link_suffix": "/forum?id=tLFWU6izoA", "link": "https://openreview.net/forum?id=tLFWU6izoA", "pdf_link": "https://openreview.net/pdf?id=tLFWU6izoA", "keywords": "CLIP Model, Diffusion Model, Generative Feedback, Representation Learning", "abstract": "Contrastive Language-Image Pre-training (CLIP), which excels at abstracting open-world representations across domains and modalities, has become a foundation for a variety of vision and multimodal tasks. However, recent studies reveal that CLIP has severe visual shortcomings, such as which can hardly distinguish orientation, quantity, color, structure, etc. These visual shortcomings also limit the perception capabilities of multimodal large language models (MLLMs) built on CLIP. The main reason could be that the image-text pairs used to train CLIP are inherently biased, due to the lack of the distinctiveness of the text and the diversity of images. In this work, we present a simple post-training approach for CLIP models, which largely overcomes its visual shortcomings via a self-supervised diffusion process. We introduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP. Specifically, DIVA leverages generative feedback from text-to-image diffusion models to optimize CLIP representations, with only images (without corresponding text). We demonstrate that DIVA improves CLIP's performance on the challenging MMVP-VLM benchmark which assesses fine-grained visual abilities to a large extent (e.g., 3-7%), and enhances the performance of MLLMs and vision models on multimodal understanding and segmentation tasks. Extensive evaluation on 29 image classification and retrieval benchmarks confirms that our framework preserves CLIP's strong zero-shot capabilities. The code will be publicly available soon.", "title_embedding_index": 21823, "title_abs_embedding_index": 21848}, {"title": "Weakly-supervised 3D Referring Expression Segmentation", "link_suffix": "/forum?id=cSAAGL0cn0", "link": "https://openreview.net/forum?id=cSAAGL0cn0", "pdf_link": "https://openreview.net/pdf?id=cSAAGL0cn0", "keywords": "Weakly-supervised learning, 3D Referring Expression Segmentation", "abstract": "3D Referring Expression Segmentation (3D-RES) aims to generate precise segmentation masks for targets based on free-form text descriptions. Despite significant advancements, current methods still rely on costly point-level mask-description pair annotations. In this paper, we introduce the Multi-Expert Network (MEN), a novel weakly supervised framework that utilizes the multimodal alignment of vision-language models across various semantic cues to reveal the relationships between descriptions and 3D instances. The primary challenges lie in effectively extracting and matching visual and textual context, while eliminating potential distractions. To address this, we propose the Multi-Expert Mining (MEM) and Multi-Expert Aggregation (MEA) modules. The MEM module employs multiple experts to extract semantic cues from full-context, attribute, and category dimensions. The MEA module mathematically consolidates the outputs of these experts, automatically assigning greater weight to more accurate ones, thus improving target selection accuracy and robustness. Extensive experiments on the ScanRefer and Multi3DRefer benchmarks demonstrate the effectiveness of our method in addressing the challenges of weakly supervised 3D-RES.", "title_embedding_index": 21824, "title_abs_embedding_index": 21849}]