[
    {
        "title": "Criteria and Bias of Parameterized Linear Regression under Edge of Stability Regime",
        "link_suffix": "/forum?id=0jJ94VVgzi",
        "link": "https://openreview.net/forum?id=0jJ94VVgzi",
        "pdf_link": "https://openreview.net/pdf?id=0jJ94VVgzi",
        "keywords": "Edge of Stability, gradient descent, implicit bias",
        "abstract": "Classical optimization theory requires a small step-size for gradient-based methods to converge. Nevertheless, recent findings (Cohen et al., 2021) challenge the traditional idea by empirically demonstrating Gradient Descent (GD) converges even when the step-size $\\eta$ exceeds the threshold of $2/L$, where $L$ is the global smooth constant. This is usually known as the \\emph{Edge of Stability} (EoS) phenomenon.  A widely held belief suggests that an objective function with subquadratic growth plays an important role in incurring EoS. In this paper, we provide a more comprehensive answer by considering the task of finding linear interpolator $\\beta \\in \\mathbb{R}^{d}$ for regression with loss function $l(\\cdot)$, where $\\beta$ admits parameterization as $\\beta =  w^2_{+} -  w^2_{-}$. Contrary to the previous work that suggests a subquadratic $l$ is necessary for EoS, our novel finding reveals that EoS occurs even when $l$ is quadratic under proper conditions. This argument is made rigorous by both empirical and theoretical evidence, demonstrating the GD trajectory converges to a linear interpolator in a non-asymptotic way. Moreover, the model under quadratic $l$, also known as a depth-$2$ \\emph{diagonal linear network}, remains largely unexplored under the EoS regime. Our analysis then sheds some new light on the implicit bias of diagonal linear networks when a larger step-size is employed, enriching the understanding of EoS on more practical models."
    },
    {
        "title": "Improving Generalization with Flat Hilbert Bayesian Inference",
        "link_suffix": "/forum?id=xZ2lTzfyFv",
        "link": "https://openreview.net/forum?id=xZ2lTzfyFv",
        "pdf_link": "https://openreview.net/pdf?id=xZ2lTzfyFv",
        "keywords": "Bayesian Inference, Sharpness-aware Minimization",
        "abstract": "We introduce Flat Hilbert Bayesian Inference (FHBI), an algorithm designed to enhance generalization in Bayesian inference. Our approach involves an iterative two-step procedure with an adversarial functional perturbation step and a functional descent step within the reproducing kernel Hilbert spaces. This methodology is supported by a theoretical analysis that extends previous findings on generalization ability from finite-dimensional Euclidean spaces to infinite-dimensional functional spaces. To evaluate the effectiveness of FHBI, we conduct comprehensive comparisons against seven baseline methods on the VTAB-1K benchmark, which encompasses 19 diverse datasets across various domains with diverse semantics. Empirical results demonstrate that FHBI consistently outperforms the baselines by notable margins, highlighting its practical efficacy. Our code is available at \\url{https://anonymous.4open.science/r/Flat-Hilbert-Variational-Inference-008F/}."
    },
    {
        "title": "Language Model Preference Evaluation with Multiple Weak Evaluators",
        "link_suffix": "/forum?id=rTM95kwzXM",
        "link": "https://openreview.net/forum?id=rTM95kwzXM",
        "pdf_link": "https://openreview.net/pdf?id=rTM95kwzXM",
        "keywords": "Large Language Models, Weak Evaluators",
        "abstract": "Despite the remarkable success of Large Language Models (LLMs), evaluating their outputs' quality regardingpreferenceremains a critical challenge. Existing works usually leverage a powerful LLM (e.g., GPT4) as the judge for comparing LLMs' output pairwisely, yet such model-based evaluator is vulnerable toconflicting preference, i.e., output A is better than B, B than C, but C than A, causing contradictory evaluation results. To improve model-based preference evaluation, we introduce GED (Preference Graph Ensemble and Denoise), a novel approach that leverages multiple model-based evaluators to construct preference graphs, and then ensemble and denoise these graphs for better, non-contradictory evaluation results. In particular, our method consists of two primary stages: aggregating evaluations into a unified graph and applying a denoising process to eliminate cyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We provide theoretical guarantees for our framework, demonstrating its efficacy in recovering the ground truth preference structure.  Extensive experiments across ten benchmark datasets show that GED outperforms baseline methods in model ranking, response selection, and model alignment tasks. Notably, GED combines weaker evaluators like Llama3-8B, Mistral-7B, and Qwen2-7B to surpass the performance of stronger evaluators like Qwen2-72B, highlighting its ability to enhance evaluation reliability and improve model performance."
    },
    {
        "title": "PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection",
        "link_suffix": "/forum?id=R22JPTQYWV",
        "link": "https://openreview.net/forum?id=R22JPTQYWV",
        "pdf_link": "https://openreview.net/pdf?id=R22JPTQYWV",
        "keywords": "Oriented Object Detection, Point Supervised Object Detection",
        "abstract": "Single point supervised oriented object detection has gained attention and made initial progress within the community. Diverse from those approaches relying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBB has shown promise due to its prior-free feature. In this paper, we propose PointOBB-v2, a simpler, faster, and stronger method to generate pseudo rotated boxes from points without relying on any other prior. Specifically, we first generate a Class Probability Map (CPM) by training the network with non-uniform positive and negative sampling. We show that the CPM is able to learn the approximate object regions and their contours. Then, Principal Component Analysis (PCA) is applied to accurately estimate the orientation and the boundary of objects.  By further incorporating a separation mechanism, we resolve the confusion caused by the overlapping on the CPM, enabling its operation in high-density scenarios.  Extensive comparisons demonstrate that our method achieves a training speed 15.58$\\times$ faster and an accuracy improvement of 11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets compared to the previous state-of-the-art, PointOBB. This significantly advances the cutting edge of single point supervised oriented detection in the modular track. Code and models will be released."
    },
    {
        "title": "Generalizable Non-Line-of-Sight Imaging with Learnable Physical Priors",
        "link_suffix": "/forum?id=HzbPIqvhGg",
        "link": "https://openreview.net/forum?id=HzbPIqvhGg",
        "pdf_link": "https://openreview.net/pdf?id=HzbPIqvhGg",
        "keywords": "Non-Line-of-Sight, Learnable Path Compensation, Adaptive Phasor Field",
        "abstract": "Non-line-of-sight (NLOS) imaging, recovering the hidden volume from indirect reflections, has attracted increasing attention due to its potential applications. Despite promising results, existing NLOS reconstruction approaches are constrained by the reliance on empirical physical priors, e.g., single fixed path compensation. Moreover, these approaches still possess limited generalization ability, particularly when dealing with scenes at a low signal-to-noise ratio (SNR). To overcome the above problems, we introduce a novel learning-based approach, comprising two key designs: Learnable Path Compensation (LPC) and Adaptive Phasor Field (APF). The LPC applies tailored path compensation coefficients to adapt to different objects in the scene, effectively reducing light wave attenuation, especially in distant regions. Meanwhile, the APF learns the precise Gaussian window of the illumination function for the phasor field, dynamically selecting the relevant spectrum band of the transient measurement. Experimental validations demonstrate that our proposed approach, only trained on synthetic data, exhibits the capability to seamlessly generalize across various real-world datasets captured by different imaging systems and characterized by low SNRs."
    },
    {
        "title": "Unlocking the Power of Gradient Guidance for Structure-Based Molecule Optimization",
        "link_suffix": "/forum?id=xt3mCoDks7",
        "link": "https://openreview.net/forum?id=xt3mCoDks7",
        "pdf_link": "https://openreview.net/pdf?id=xt3mCoDks7",
        "keywords": "molecule optimization, structure-based drug design, Bayesian flow network",
        "abstract": "Structure-based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets.\nA promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities.\nTo this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the first gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance.\nWe introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization.\nOur proposed MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3% , Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x \"Me-Better\" Ratio as much as 3D baselines.\nFurthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility and potential."
    },
    {
        "title": "Compositional Generative Inference Using Diffusion-based Optimization",
        "link_suffix": "/forum?id=e0YazAdpzD",
        "link": "https://openreview.net/forum?id=e0YazAdpzD",
        "pdf_link": "https://openreview.net/pdf?id=e0YazAdpzD",
        "keywords": "Diffusion, Composition, Probablistic Models, Energy based models",
        "abstract": "Compositional generative tasks, despite being important and having potential applications, have not been thoroughly addressed due to the unclear formulation and the challenges associated with selecting composition strategies. In this paper, we propose a probabilistic graphical approach to tackle the problem of compositional generative tasks and alleviate these challenges. Our approach formulates the problem as a Bayesian inference problem using a representative bipartite Bayesian network. In this network, one set of random variables represents the generation targets, while the other set represents observable variables with explicit or implicit distribution information. To solve this problem, we employ variational inference on the marginal distribution of observable variables. We approximate this distribution using diffusion models. We view the diffusion models as approximate Markov Chain Monte Carlo (MCMC) samplers for the marginals. Based on this perspective, we introduce a novel MCMC-based inference algorithm that incorporates per-step optimization using aggregated objectives from the diffusion models. We demonstrate the generality of our method and conduct experiments to validate its applicability to various compositional generation tasks."
    },
    {
        "title": "ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Reinforcement Learning",
        "link_suffix": "/forum?id=IOkYP5ZxO5",
        "link": "https://openreview.net/forum?id=IOkYP5ZxO5",
        "pdf_link": "https://openreview.net/pdf?id=IOkYP5ZxO5",
        "keywords": "visuo-tactile representation learning, reinforcement learning, contrastive learning",
        "abstract": "Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches struggle to effectively integrate visual and tactile information, resulting in suboptimal performance. In this paper, we presentViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide an agent's behavior. We introduceSoft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method, to enhance the fusion of these two modalities,  and adopt a CVAE module to utilize complementary information within visuo-tactile representation. We conduct comprehensive experiments, including $\\mathbf{9}$ tasks in simulation environment, across $\\mathbf{5}$ different benchmarks, to compare ViTaS with existing baselines. The results demonstrate that ViTaS achieves state-of-the-art performance, with an average improvement of $\\mathbf{51}$%. Furthermore, our method significantly enhances sample efficiency while maintaining minimal parameters, underscoring the effectiveness of our approach. The code will be released upon acceptance."
    },
    {
        "title": "A Unified Theory of Quantum Neural Network Loss Landscapes",
        "link_suffix": "/forum?id=fv8TTt9srF",
        "link": "https://openreview.net/forum?id=fv8TTt9srF",
        "pdf_link": "https://openreview.net/pdf?id=fv8TTt9srF",
        "keywords": "quantum machine learning, neural tangent kernel, loss landscape, spin glass, Kac\u2013Rice formula",
        "abstract": "Classical neural networks with random initialization famously behave as Gaussian processes in the limit of many neurons, which allows one to completely characterize their training and generalization behavior. No such general understanding exists for quantum neural networks (QNNs), which\u2014outside of certain special cases\u2014are known to not behave as Gaussian processes when randomly initialized. We here prove that QNNs and their first two derivatives instead generally form what we call \"Wishart processes,\" where certain algebraic properties of the network determine the hyperparameters of the process. This Wishart process description allows us to, for the first time: give necessary and sufficient conditions for a QNN architecture to have a Gaussian process limit; calculate the full gradient distribution, generalizing previously known barren plateau results; and calculate the local minima distribution of algebraically constrained QNNs. Our unified framework suggests a certain simple operational definition for the \"trainability\" of a given QNN model using a newly introduced, experimentally accessible quantity we call the \"degrees of freedom\" of the network architecture."
    },
    {
        "title": "A Plug-In Curriculum Scheduler for Improved Deformable Medical Image Registration",
        "link_suffix": "/forum?id=a0JBoEy0af",
        "link": "https://openreview.net/forum?id=a0JBoEy0af",
        "pdf_link": "https://openreview.net/pdf?id=a0JBoEy0af",
        "keywords": "medical image registration; curriculum learning",
        "abstract": "Deformable image registration is a crucial task in medical image analysis, and its complexity has spurred significant research and ongoing progress.\nMuch of the work in this area has concentrated on achieving incremental performance gains by adjusting network architectures or introducing new loss functions. \nHowever, these modifications are often tailored to specific tasks or datasets, which limits their general applicability. \nTo address this limitation, we propose an innovative solution: a plug-in curriculum scheduler that can be seamlessly integrated into existing methods without changing their core architecture.\nOur scheduler, inspired by curriculum learning, progressively increases task difficulty to enhance performance, incorporating sample difficulty and matching accuracy as key criteria. \nSample difficulty is assessed at voxel and volume levels, using Variance of Gradients for voxel complexity and Gaussian blurring for volume evaluation, while matching accuracy involves gradually increasing supervision for improved alignment and accuracy.\nWe empirically demonstrate that this scheduler achieves superior accuracy and visual quality in various tasks and datasets."
    },
    {
        "title": "Enhancing Robust Fairness via Confusional Spectral Regularization",
        "link_suffix": "/forum?id=lW0ZndAimF",
        "link": "https://openreview.net/forum?id=lW0ZndAimF",
        "pdf_link": "https://openreview.net/pdf?id=lW0ZndAimF",
        "keywords": "Adversarial robustness, Worst-class robust accuracy, Robust fairness",
        "abstract": "Recent research has highlighted a critical issue known as ``robust fairness\", where robust accuracy varies significantly across different classes, undermining the reliability of deep neural networks (DNNs). \nA common approach to address this has been to dynamically reweight classes during training, giving more weight to those with lower empirical robust performance. \nHowever, our findings reveal a limitation: the class with the worst robust accuracy in training set does not consistently align with that in testing set, indicating the need for a more principled solution.\nIn this work, we derive a robust generalization bound for the worst-class robust error within the PAC-Bayesian framework, accounting for unknown data distributions. \nOur analysis shows that the worst-class robust error is influenced by two main factors: the spectral norm of the empirical robust confusion matrix and the information embedded in the model and training set. \nWhile the latter has been extensively studied, we propose a novel regularization technique targeting the spectral norm of the robust confusion matrix to improve worst-class robust accuracy and enhance robust fairness.\nWe validate our approach through comprehensive experiments on various datasets and models, demonstrating its effectiveness in enhancing robust fairness."
    },
    {
        "title": "Compositional VQ Sampling for Efficient and Accurate Conditional Image Generation",
        "link_suffix": "/forum?id=gKui6QvvfK",
        "link": "https://openreview.net/forum?id=gKui6QvvfK",
        "pdf_link": "https://openreview.net/pdf?id=gKui6QvvfK",
        "keywords": "image generation, compositional generalization",
        "abstract": "Compositional diffusion and energy-based models have driven progress in controllable image generation, however the challenge of composing discrete generative models has remained open, holding the potential for improvements in efficiency, interpretability and generation quality. To this end, we propose a framework for controllable conditional generation of images. We formulate a process for composing discrete generation processes, enabling generation with an arbitrary number of input conditions without the need for any specialised training objective. We adapt this result for parallel token prediction with masked generative transformers, enabling accurate and efficient conditional sampling from the discrete latent space of VQ models. In particular, our method attains an average error rate of 19.3% across nine experiments spanning three datasets (between one and three input conditions for each dataset), representing an average 63.4% reduction in error rate relative to the previous state-of-the-art. Our method also outperforms the next-best approach (ranked by error rate) in terms of FID in seven out of nine settings, with an average FID of $24.23$, and average improvement of $-9.58$. Furthermore, our method offers a $2.3\\times$ to $12\\times$ speedup over comparable methods. We find that our method can generalise to combinations of input conditions that lie outside the training data (e.g. more objects per image for Positional CLEVR) in addition to offering an interpretable dimension of controllability via concept weighting. Outside of the rigorous quantitative settings, we further demonstrate that our approach can be readily applied to an open pre-trained discrete text-to-image model, demonstrating fine-grained control of text-to-image generation. The accuracy and efficiency of our framework across diverse conditional image generation settings reinforces its theoretical foundations, while opening up practical avenues for future work in controllable and composable image generation."
    },
    {
        "title": "Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents",
        "link_suffix": "/forum?id=U1T6sq12uj",
        "link": "https://openreview.net/forum?id=U1T6sq12uj",
        "pdf_link": "https://openreview.net/pdf?id=U1T6sq12uj",
        "keywords": "Large Language Models, LLM-Generated Content, Infomation Retrieval, Source Bias",
        "abstract": "Previous studies have found that PLM-based retrieval models exhibit a preference for LLM-generated content, assigning higher relevance scores to these documents even when their semantic quality is comparable to human-written ones. This phenomenon, known as source bias, threatens the sustainable development of the information access ecosystem. However, the underlying causes of source bias remain unexplored. In this paper, we explain the process of information retrieval with a causal graph and discover that PLM-based retrievers learn perplexity features for relevance estimation, causing source bias by ranking the documents with low perplexity higher. Theoretical analysis further reveals that the phenomenon stems from the positive correlation between the gradients of the loss functions in language modeling task and retrieval task. Based on the analysis, a causal-inspired inference-time debiasing method is proposed, called $\\textbf{C}$ausal $\\textbf{D}$iagnosis and $\\textbf{C}$orrection (CDC). CDC first diagnoses the bias effect of the perplexity and then separates the bias effect from the overall estimated relevance score. Experimental results across three domains demonstrate the superior debiasing effectiveness of CDC, emphasizing the validity of our proposed explanatory framework. Source codes are available athttps://anonymous.4open.science/r/Perplexity-Trap-D6FE."
    },
    {
        "title": "M2M: LEARNING CONTROLLABLE MULTI OF EXPERTS AND MULTI-SCALE OPERATORS ARE THE PARTIAL DIFFERENTIAL EQUATIONS NEED",
        "link_suffix": "/forum?id=MUL7tKvNei",
        "link": "https://openreview.net/forum?id=MUL7tKvNei",
        "pdf_link": "https://openreview.net/pdf?id=MUL7tKvNei",
        "keywords": "Multi-experts and Multi-scales; Controlled operator learning; machine learing in PDEs",
        "abstract": "Learning the evolutionary dynamics of Partial Differential Equations (PDEs) is critical in understanding dynamic systems, yet current methods insufficiently learn their representations. This is largely due to the multi-scale nature of the solution, where certain regions exhibit rapid oscillations while others evolve more slowly. This paper introduces a framework of multi-scale and multi-expert (M$^2$M) neural operators designed to simulate and learn PDEs efficiently. We employ a divide-and-conquer strategy to train a multi-expert gated network for the dynamic router policy. Our method incorporates a controllable prior gating mechanism that determines the selection rights of experts, enhancing the model's efficiency.  To optimize the learning process, we have implemented a PI (Proportional, Integral) control strategy to adjust the allocation rules precisely. This universal controllable approach allows the model to achieve greater accuracy. We test our approach on benchmark 2D Navier-Stokes equations and provide a custom multi-scale dataset. M$^2$M can achieve higher simulation accuracy and offer improved interpretability compared to baseline methods."
    },
    {
        "title": "GS-VTON: Controllable 3D Virtual Try-on with Gaussian Splatting",
        "link_suffix": "/forum?id=8eenzfwKqU",
        "link": "https://openreview.net/forum?id=8eenzfwKqU",
        "pdf_link": "https://openreview.net/pdf?id=8eenzfwKqU",
        "keywords": "3D virtual try-on, 3D Gaussian Splatting, diffusion model",
        "abstract": "Diffusion-based 2D virtual try-on (VTON) techniques have recently demonstrated strong performance, while the development of 3D VTON has largely lagged behind. Despite recent advances in text-guided 3D scene editing, integrating 2D VTON into these pipelines to achieve vivid 3D VTON remains challenging. The reasons are twofold. First, text prompts cannot provide sufficient details in describing clothing. Second, 2D VTON results generated from different viewpoints of the same 3D scene lack coherence and spatial relationships, hence frequently leading to appearance inconsistencies and geometric distortions. To resolve these problems, we introduce an image-prompted 3D VTON method (dubbed GS-VTON) which, by leveraging 3D Gaussian Splatting (3DGS) as the 3D representation, enables the transfer of pre-trained knowledge from 2D VTON models to 3D while improving cross-view consistency.(1)Specifically, we propose a personalized diffusion model that utilizes low-rank adaptation (LoRA) fine-tuning to incorporate personalized information into pre-trained 2D VTON models. To achieve effective LoRA training, we introduce a reference-driven image editing approach that enables the simultaneous editing of multi-view images while ensuring consistency.(2)Furthermore, we propose a persona-aware 3DGS editing framework to facilitate effective editing while maintaining consistent cross-view appearance and high-quality 3D geometry.(3)Additionally, we have established a new 3D VTON benchmark, 3D-VTONBench, which facilitates comprehensive qualitative and quantitative 3D VTON evaluations. Through extensive experiments and comparative analyses with existing methods, the proposed \\OM has demonstrated superior fidelity and advanced editing capabilities, affirming its effectiveness for 3D VTON."
    },
    {
        "title": "HART: Efficient Visual Generation with Hybrid Autoregressive Transformer",
        "link_suffix": "/forum?id=q5sOv4xQe4",
        "link": "https://openreview.net/forum?id=q5sOv4xQe4",
        "pdf_link": "https://openreview.net/pdf?id=q5sOv4xQe4",
        "keywords": "autoregressive models, image generation, text-to-image",
        "abstract": "We introduce Hybrid Autoregressive Transformer (HART), the first autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components:  discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Code will be released upon publication."
    },
    {
        "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
        "link_suffix": "/forum?id=7eoN0PpKtc",
        "link": "https://openreview.net/forum?id=7eoN0PpKtc",
        "pdf_link": "https://openreview.net/pdf?id=7eoN0PpKtc",
        "keywords": "image morphing, diffusion model, tuning-free method",
        "abstract": "We presentFreeMorph, the first tuning-free method for image morphing that accommodates inputs with varying semantics or layouts. Unlike existing methods, which rely on fine-tuning pre-trained diffusion models and are limited by time constraints and semantic/layout discrepancies, FreeMorph delivers high-fidelity image morphing without extensive training. Despite its efficiency and potential, tuning-free methods still face challenges in maintaining high-quality image morphing due to the non-linear nature of the multi-step denoising process and bias inherited from the pre-trained diffusion model. In this paper, we introduce FreeMorph to address this challenge by integrating two key innovations.1)We first propose aguidance-aware spherical interpolationdesign that incorporates the explicit guidance from the input images by modifying the self-attention modules, addressing identity loss, and ensuring consistent transitions throughout the generated sequences.2)We further introduce astep-oriented motion flowthat blends self-attention modules derived from each input image to achieve controlled and directional transitions that respect both input images. Our extensive evaluations demonstrate that FreeMorph outperforms existing methods with training that is 10X - 50X faster, establishing a new state-of-the-art for image morphing. The code will be released."
    },
    {
        "title": "ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient PINNs learning",
        "link_suffix": "/forum?id=o1IiiNIoaA",
        "link": "https://openreview.net/forum?id=o1IiiNIoaA",
        "pdf_link": "https://openreview.net/pdf?id=o1IiiNIoaA",
        "keywords": "PINNs, SciML, PDEs, Natural Gradient, Neural Tangent Kernel",
        "abstract": "In the recent years, Physics Informed Neural Networks (PINNs) have received strong interest as a method to solve PDE driven systems, in particular for data assimilation purpose. This method is still in its infancy, with many shortcomings and failures that remain not properly understood.\nIn this paper we propose a natural gradient approach to PINNs which contributes to speed-up and improve the accuracy of the training.\nBased on an  in depth analysis of the differential geometric structures of the problem, we come up with two distinct contributions:\n(i) a new natural gradient algorithm that scales as $\\min(P^2S, S^2P)$, where $P$ is the number of parameters, and $S$ the batch size;\n(ii) a mathematically principled reformulation of the PINNs problem that allows the extension of natural gradient to it, with proved connections to Green's function theory."
    },
    {
        "title": "EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation",
        "link_suffix": "/forum?id=81cta3WQVI",
        "link": "https://openreview.net/forum?id=81cta3WQVI",
        "pdf_link": "https://openreview.net/pdf?id=81cta3WQVI",
        "keywords": "3D Generation, Auto-regressive Mesh Generation",
        "abstract": "Current auto-regressive mesh generation methods suffer from issues such as incompleteness, insufficient detail, and poor generalization. \nIn this paper, we propose an Auto-regressive Auto-encoder (ArAE) model capable of generating high-quality 3D meshes with up to 4,000 faces at a spatial resolution of $512^3$.\nWe introduce a novel mesh tokenization algorithm that efficiently compresses triangular meshes into 1D token sequences, significantly enhancing training efficiency. \nFurthermore, our model compresses variable-length triangular meshes into a fixed-length latent space, enabling training latent diffusion models for better generalization. \nExtensive experiments demonstrate the superior quality, diversity, and generalization capabilities of our model in both point cloud and image-conditioned mesh generation tasks."
    },
    {
        "title": "Craftium: Creating Efficient Environments for Open-Ended and Embodied Agents Beyond Gridworlds",
        "link_suffix": "/forum?id=ga1sPJen12",
        "link": "https://openreview.net/forum?id=ga1sPJen12",
        "pdf_link": "https://openreview.net/pdf?id=ga1sPJen12",
        "keywords": "reinforcement learning, environment, embodied, open-ended, continual learning, meta reinforcement learning",
        "abstract": "Advancements in open-ended and embodied AI require highly adaptable and computationally efficient environments. Yet, existing platforms often lack the flexibility, efficiency, or richness necessary to drive progress in these areas. Research in fields related to open-endedness, such as unsupervised environment design and continual reinforcement learning, usually defaults to simplistic 2D grid environments, as more complex alternatives are either too rigid or computationally expensive. Conversely, in embodied AI, the field relies on fully featured video games like Minecraft, which are rich in content but computationally inefficient and offer limited customization for creating new tasks. This paper introduces Craftium, a framework based on the open-source Minetest game engine, providing a highly customizable, easy-to-use, and efficient platform for building rich Minecraft-like 3D environments. We showcase environments of different complexity and nature: from simple reinforcement learning tasks to a vast world with many creatures and biomes, along with a customizable procedural task generator. Conducted benchmarks show that Craftium substantially improves the computational cost of Minecraft-based frameworks, achieving +2K steps per second more."
    },
    {
        "title": "Autoregressive Action Sequence Learning for Robotic Manipulation",
        "link_suffix": "/forum?id=Lr8IIc1rB8",
        "link": "https://openreview.net/forum?id=Lr8IIc1rB8",
        "pdf_link": "https://openreview.net/pdf?id=Lr8IIc1rB8",
        "keywords": "Multi-Task Robot Learning, Manipulation, Autoregressive Model",
        "abstract": "Autoregressive models have demonstrated remarkable success in natural language processing. In this work, we design a simple yet effective autoregressive architecture for robotic manipulation tasks. We propose the Chunking Causal Transformer (CCT), which extends the next-single-token prediction of causal transformers to support multi-token prediction in a single pass. Further, we design a novel attention interleaving strategy that allows CCT to be trained efficiently with teacher-forcing. Based on CCT, we propose the Autoregressive Policy (ARP) model, which learns to generate action sequences autoregressively. We find that action sequence learning enables better leverage of the underlying causal relationships in robotic tasks. We evaluate ARP across diverse robotic manipulation environments, including Push-T, ALOHA, and RLBench, and show that it outperforms  the state-of-the-art methods in all tested environments, while being more efficient in computation and parameter sizes. Video demonstrations, our source code and the models of ARP are all included in the supplementary material."
    },
    {
        "title": "Beyond Random Augmentations: Pretraining with Hard Views",
        "link_suffix": "/forum?id=AK1C55o4r7",
        "link": "https://openreview.net/forum?id=AK1C55o4r7",
        "pdf_link": "https://openreview.net/pdf?id=AK1C55o4r7",
        "keywords": "Self-Supervised Learning, Data Augmentation, Pretraining",
        "abstract": "Self-Supervised Learning (SSL) methods typically rely on random image augmentations, or views, to make models invariant to different transformations. We hypothesize that the efficacy of pretraining pipelines based on conventional random view sampling can be enhanced by explicitly selecting views that benefit the learning progress. A simple yet effective approach is to select hard views that yield a higher loss. In this paper, we propose Hard View Pretraining (HVP), a learning-free strategy that extends random view generation by exposing models to more challenging samples during SSL pretraining. HVP encompasses the following iterative steps: 1) randomly sample multiple views and forward each view through the pretrained model, 2) create pairs of two views and compute their loss, 3) adversarially select the pair yielding the highest loss according to the current model state, and 4) perform a backward pass with the selected pair. In contrast to existing hard view literature, we are the first to demonstrate hard view pretraining's effectiveness at scale, particularly training on the full ImageNet-1k dataset, and evaluating across multiple SSL methods, Convolutional Networks, and Vision Transformers. As a result, HVP sets a new state-of-the-art on DINO ViT-B/16, reaching 78.8% linear evaluation accuracy (a 0.6% improvement) and consistent gains of 1% for both 100 and 300 epoch pretraining, with similar improvements across transfer tasks in DINO, SimSiam, iBOT, and SimCLR."
    },
    {
        "title": "Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding",
        "link_suffix": "/forum?id=LZfjxvqw0N",
        "link": "https://openreview.net/forum?id=LZfjxvqw0N",
        "pdf_link": "https://openreview.net/pdf?id=LZfjxvqw0N",
        "keywords": "auto-regressive image generation, acceleration, training-free, image generation",
        "abstract": "The current large auto-regressive models can generate high-quality, high-resolution images, but these models require hundreds or even thousands of steps of next-token prediction during inference, resulting in substantial time consumption. In existing studies, Jacobi decoding, an iterative parallel decoding algorithm, has been used to accelerate the auto-regressive generation and can be executed without training. However, the Jacobi decoding relies on a deterministic criterion to determine the convergence of iterations. Thus, it works for greedy decoding but is incompatible with sampling-based decoding which is crucial for visual quality and diversity in the current auto-regressive text-to-image generation. In this paper, we propose a training-free probabilistic parallel decoding algorithm, Speculative Jacobi Decoding (SJD), to accelerate auto-regressive text-to-image generation. By introducing a probabilistic convergence criterion, our SJD accelerates the inference of auto-regressive text-to-image generation while maintaining the randomness in sampling-based token decoding and allowing the model to generate diverse images. Specifically, SJD facilitates the model to predict multiple tokens at each step and accepts tokens based on the probabilistic criterion, enabling the model to generate images with fewer steps than the conventional next-token-prediction paradigm. We also investigate the token initialization strategies that leverage the spatial locality of visual data to further improve the acceleration ratio under specific scenarios. We conduct experiments for our proposed SJD on multiple auto-regressive text-to-image generation models, showing the effectiveness of model acceleration without sacrificing the visual quality. Code will be released upon acceptance."
    },
    {
        "title": "Video Active Perception: Efficient Inference-Time Long-Form Video Understanding with Vision-Language Models",
        "link_suffix": "/forum?id=KtqZrNjvjd",
        "link": "https://openreview.net/forum?id=KtqZrNjvjd",
        "pdf_link": "https://openreview.net/pdf?id=KtqZrNjvjd",
        "keywords": "Video question answering, Vision Language Model",
        "abstract": "Large vision-language models (VLMs) have advanced multimodal tasks such as video question answering (QA), yet they struggle with long-form videos due to the computational burden of processing excessive tokens. Inspired by active perception theory, which posits that models gain information by acquiring data that differ from their expectations, we introduce Video Active Perception (VAP), a training-free method to enhance long-form video QA using VLMs. Our approach treats key frame selection as data acquisition in active perception and leverages a lightweight text-conditioned video generation model to represent prior world knowledge. Empirically, VAP achieves state-of-the-art zero-shot results on long-form video QA datasets such as EgoSchema, NExT-QA, ActivityNet-QA and CLEVRER, achieving an increase of up to 5.6 X efficiency by frames per question over standard GPT-4o, Gemini 1.5 Pro, and LLaVA-OV. Moreover, VAP shows stronger reasoning abilities than previous methods and effectively selects key frames relevant to questions. These findings highlight the potential of leveraging active perception to improve efficiency and effectiveness of long-form video QA."
    },
    {
        "title": "More Harmful, Less noticeable: Learning Adversarial Null-Text Embeddings for Inconspicuous Attack",
        "link_suffix": "/forum?id=XjSfcJUcaA",
        "link": "https://openreview.net/forum?id=XjSfcJUcaA",
        "pdf_link": "https://openreview.net/pdf?id=XjSfcJUcaA",
        "keywords": "adversarial attack, diffusion model, image editing",
        "abstract": "Adversarial examples, which are artificially crafted data intended to disrupt the output of deep learning models, present a new round of challenges to the stability and security of artificial intelligence technology. Unrestricted adversarial examples, obtained by modifying the semantic elements of images, have the characteristics of being natural and semantically meaningful. However, previous methods either significantly altered the image's color or content, or blurred visual details (such as text or geometric designs), making the generated adversarial examples easily detectable by the human eye. In this paper, we propose a method to generate highly natural adversarial examples based on stable diffusion. This is achieved by introducing adversarial loss during the image reconstruction process to perturb cross-attention mechanism. To further enhance image quality, we introduce perceptual loss into the adversarial attack process for the first time. Extensive experiments and visualizations demonstrate the effectiveness of our proposed method. Compared to the current state-of-the-art methods, our approach not only improves the adversarial transferability by an average of 12.59-50.3% but also significantly enhances image quality. Code will be publicly available."
    }
]