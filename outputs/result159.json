[
    {
        "title": "MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning",
        "link_suffix": "/forum?id=HVtu26XDAA",
        "link": "https://openreview.net/forum?id=HVtu26XDAA",
        "pdf_link": "https://openreview.net/pdf?id=HVtu26XDAA",
        "keywords": "Multimodal LLM",
        "abstract": "We present MM1.5, a new family of multimodal large language models (MLLMs) designed to enhance capabilities in text-rich image understanding, visual referring and grounding, and multi-image reasoning. Building upon the MM1 architecture, MM1.5 adopts a data-centric approach to model training, systematically exploring the impact of diverse data mixtures across the entire model training lifecycle. This includes high-quality OCR data and synthetic captions for continual pre-training, as well as an optimized visual instruction-tuning data mixture for supervised fine-tuning. Our models range from 1B to 30B parameters, encompassing both dense and mixture-of-experts (MoE) variants, and demonstrate that careful data curation and training strategies can yield strong performance even at small scales (1B and 3B). Additionally, we introduce two specialized variants: MM1.5-Video, designed for video understanding, and MM1.5-UI, tailored for mobile UI understanding. Through extensive empirical studies and ablations, we provide detailed insights into the training processes and decisions that inform our final designs, offering valuable guidance for future research in MLLM development."
    },
    {
        "title": "CityNav: Language-Goal Aerial Navigation Dataset Using Geographic Information",
        "link_suffix": "/forum?id=LjvIJFCa5J",
        "link": "https://openreview.net/forum?id=LjvIJFCa5J",
        "pdf_link": "https://openreview.net/pdf?id=LjvIJFCa5J",
        "keywords": "City-scale 3D Vision, Aerial Navigation, 3D Vision and Language",
        "abstract": "Vision-and-language navigation (VLN) aims to guide autonomous agents through real-world environments by integrating visual and linguistic cues. Despite notable advancements in ground-level navigation, the exploration of aerial navigation using these modalities remains limited. This gap primarily arises from a lack of suitable resources for real-world, city-scale aerial navigation studies. To remedy this gap, we introduce CityNav, a novel dataset explicitly designed for language-guided aerial navigation in photorealistic 3D environments of real cities. CityNav comprises 32k natural language descriptions paired with human demonstration trajectories, collected via a newly developed web-based 3D simulator. Each description identifies a navigation goal, utilizing the names and locations of landmarks within actual cities. As an initial step toward addressing this challenge, we provide baseline models of navigation agents that incorporate an internal 2D spatial map representing landmarks referenced in the descriptions. We have benchmarked the latest aerial navigation methods alongside our proposed baseline model on the CityNav dataset. The findings are revealing: (i) our aerial agent model trained on human demonstration trajectories,  outperform those trained on shortest path trajectories by a large margin; (ii) incorporating 2D spatial map information markedly and robustly enhances navigation performance at a city scale; (iii) despite the use of map information, our challenging CityNav dataset reveals a persistent performance gap between our baseline models and human performance. To foster further research in aerial VLN, we have made the dataset and code available athttps://anonymous.4open.science/w/city-nav-77E3/."
    },
    {
        "title": "An Empirical Study on Normalization in Mamba",
        "link_suffix": "/forum?id=YK8eO7BEkJ",
        "link": "https://openreview.net/forum?id=YK8eO7BEkJ",
        "pdf_link": "https://openreview.net/pdf?id=YK8eO7BEkJ",
        "keywords": "Mamba, long-sequence modeling, normalization, performance, stability",
        "abstract": "Normalization layers are crucial for enhancing both the training efficiency and stability of deep neural network architectures. The recently proposed Mamba network has shown considerable promise in achieving competitive results alongside Transformers. However, ensuring training stability in Mamba, as in many deep architectures, remains a significant challenge, where normalization techniques are crucial for solving this issue. In this paper, we conduct a systematic investigation into the effects of various normalization techniques and their combinations within the Mamba architecture. Our analysis encompasses both long sequence modeling and image classification tasks. For long sequence modeling, we perform extensive experiments to assess the impact of applying normalization layers both before and after S6 Modules. The results show that normalization across layers leads to enhanced training stability and improved model performance. Furthermore, we validate these findings on large-scale V-Mamba models and offer practical suggestions for selecting appropriate normalization techniques. We hope that our insights will contribute to mitigating training instabilities in deep learning and fostering the development of more robust architectures. All code and models used in this study will be open-sourced on GitHub."
    },
    {
        "title": "The Disparate Benefits of Deep Ensembles",
        "link_suffix": "/forum?id=OUhR7Ghg3K",
        "link": "https://openreview.net/forum?id=OUhR7Ghg3K",
        "pdf_link": "https://openreview.net/pdf?id=OUhR7Ghg3K",
        "keywords": "Deep Ensembles, Algorithmic Fairness, Disparate Benefits, Post-Processing",
        "abstract": "Ensembles of Deep Neural Networks, Deep Ensembles, are widely used as a simple way to boost predictive performance. However, their impact on algorithmic fairness is not well understood yet. Algorithmic fairness investigates how a model\u2019s performance varies across different groups, typically defined by protected attributes such as age, gender, or race. In this work, we investigate the interplay between the performance gains from Deep Ensembles and fairness. Our analysis reveals that they unevenly favor different groups in what we refer to as a disparate benefits effect. We empirically investigate this effect with Deep Ensembles applied to popular facial analysis and medical imaging datasets, where protected group attributes are given and find that it occurs for multiple established group fairness metrics, including statistical parity and equal opportunity. Furthermore, we identify the per-group difference in predictive diversity of ensemble members as the potential cause of the disparate benefits effect. Finally, we evaluate different approaches to reduce unfairness due to the disparate benefits effect. Our findings show that post-processing is an effective method to mitigate this unfairness while preserving the improved performance of Deep Ensembles."
    },
    {
        "title": "Seeking the Right Question: Towards High-Quality Visual Instruction Generation",
        "link_suffix": "/forum?id=rLySPkyl3S",
        "link": "https://openreview.net/forum?id=rLySPkyl3S",
        "pdf_link": "https://openreview.net/pdf?id=rLySPkyl3S",
        "keywords": "computer vision, vision language model, visual instruction generation",
        "abstract": "Large language models achieve significant improvements in instruction following through training with synthetic data. The self-instruct method generates instructions based on manually selected tasks, establishing an annotation-free paradigm for synthesizing instructions. However, the experience of synthesizing language instructions for LLMs does not directly transfer to visual instruction generation. Visual instructions encompass both images and questions, and questions generated directly from images often struggle to form high-quality instructions.\nBy analyzing real user queries, we summarize the characteristics of high-quality instructions: they require image perception, reasoning, and answerability. We propose a three-stage visual instruction generation pipeline, named \"Seeking the Right Question\" (SRQ), to produce high-quality instructions. In stage 1, we select 160 instructions that meet high-quality standards as seed questions, categorizing them into eight groups based on multi-modal task types. In stage 2, we introduce capability-driven prompting to generate high-quality questions. In stage 3, we implement an Image Dependency Scoring Mechanism to filter the generated questions. Additionally, we use GPT-4o to directly generate answers, forming $<$image, question, answer$>$ triples for model training.\nTo demonstrate the effectiveness of SRQ, we construct the high-quality instruction dataset Allava-SRQ from 125,000 images sampled from the Allava dataset. Experimental results show that Allava-SRQ significantly improves the performance of multiple baseline models across various benchmarks. We plan to open-source SRQ and the high-quality instruction dataset Allava-SRQ to promote advancements in the field of visual instruction generation."
    },
    {
        "title": "Self-Alignment for Offline Safe Reinforcement Learning",
        "link_suffix": "/forum?id=ZtOnddFVT3",
        "link": "https://openreview.net/forum?id=ZtOnddFVT3",
        "pdf_link": "https://openreview.net/pdf?id=ZtOnddFVT3",
        "keywords": "offline safe reinforcement learning, self alignment, prompt, lyapunov stability",
        "abstract": "Deploying an offline reinforcement learning (RL) agent into a downstream task is challenging and faces unpredictable transitions due to the distribution shift between the offline RL dataset and the real environment. To solve the distribution shift problem, some prior works aiming to learn a well-performing and safer agent have employed conservative or safe RL methods in the offline setting. However, the above methods require a process of retraining from scratch or fine-tuning to satisfy the desired criteria for performance and safety. In this work, we propose a Lyapunov conditioned self-alignment method for a transformer-based world model , which does not require retraining and conducts the test-time adaptation for the desired criteria. We show that a transformer-based world model can be described as a model-based hierarchical RL. As a result, we can combine hierarchical RL and our in-context learning for self-alignment in transformers. The proposed self-alignment framework aims to make the agent safe by self-instructing with the Lyapunov condition. In experiments, we demonstrate that our self-alignment algorithm outperforms safe RL methods in continuous control and safe RL benchmark environments in terms of return, costs, and failure rate."
    },
    {
        "title": "LEVERAGING LEARNING RATE GRADIENTS FOR AUTOMATIC LEARNING RATE SELECTION",
        "link_suffix": "/forum?id=j83R1R3euh",
        "link": "https://openreview.net/forum?id=j83R1R3euh",
        "pdf_link": "https://openreview.net/pdf?id=j83R1R3euh",
        "keywords": "hyperparameter tuning, learning rate gradient, automatic learning rate selection",
        "abstract": "Selecting an optimal learning rate (LR) is crucial for training deep neural networks, significantly affecting both convergence speed and final model performance. Determining this optimal LR typically involves two key challenges: choosing an appropriate initial LR and selecting an LR scheduler for adjusting the LR during training. This paper focuses on the former challenge\u2014selecting the initial LR. Traditionally, this task relies on manual tuning or heuristic methods, often involving extensive trial-and-error or computationally expensive search strategies like grid search or random search. We propose an algorithm, Automatic Learning Rate Selection (ALRS), to find the initial LR without the need for manual intervention. ALRS leverages the gradient of the LR itself \u2014 a less explored approach in the field. ALRS is a computationally lightweight pre-training process that automatically selects the initial LR by iterative refinements using the LR gradient, specifically analyzing its sign information, combined with suitable search algorithms. This approach efficiently converges to the optimal LR in a stable and robust manner across various optimizers and network architectures.We evaluate our technique on standard deep learning benchmarks, including MNIST with a CNN and CIFAR-10 and CIFAR-100 with ResNet-18, using both SGD and Adam optimizers. Our experiments demonstrate that the automatically determined LRs achieve performance comparable to manually tuned LRs and state-of-the-art results."
    },
    {
        "title": "Is the sparsity of high dimensional spaces the reason why VAEs are poor generative models?",
        "link_suffix": "/forum?id=4xEACJ2fFn",
        "link": "https://openreview.net/forum?id=4xEACJ2fFn",
        "pdf_link": "https://openreview.net/pdf?id=4xEACJ2fFn",
        "keywords": "variational autoencoder, generative model, high dimensional statistics, spin glass, latent space, hyperspherical coordinates",
        "abstract": "Variational autoencoders (VAE) encode data into lower dimension latent vectors before decoding those vectors back to data. Once trained, decoding a random latent vector usually does not produce meaningful data, at least when the latent space has more than a dozen dimensions. In this paper, we investigate this issue drawing insight from high dimensional physical systems such as spin-glasses, which exhibit a phase transition from a high entropy random configuration to a lower energy and more organised state when cooled quickly in the presence of a magnetic field. The latent of a standard VAE is by definition close to a uniform distribution on a hypersphere, and thus similar to the high entropy spin-glass state. We propose to formulate the latent variables of a VAE using hyperspherical coordinates, which allows to compress the latent vectors towards an island on the hypersphere, thereby reducing the latent sparsity, analogous to a quenched spin-glass. We show that this is feasible with modest computational increase and that it improves the generation ability of the VAE."
    },
    {
        "title": "Independently-Normalized SGD for Generalized-Smooth Nonconvex Optimization",
        "link_suffix": "/forum?id=O2GBkHujdP",
        "link": "https://openreview.net/forum?id=O2GBkHujdP",
        "pdf_link": "https://openreview.net/pdf?id=O2GBkHujdP",
        "keywords": "Non-convex optimization, Stochastic Algorithm",
        "abstract": "Recent studies have shown that many nonconvex machine learning problems meet a so-called generalized-smooth condition that extends beyond traditional smooth nonconvex optimization. However, the existing algorithms designed for generalized-smooth nonconvex optimization encounter significant limitations in both their design and convergence analysis.\nIn this work, we first study deterministic generalized-smooth nonconvex optimization and analyze the convergence of normalized gradient descent under the generalized Polyak-Lojasiewicz condition. Our results provide a comprehensive understanding of the interplay between gradient normalization and function geometry. Then, for stochastic generalized-smooth nonconvex optimization, we propose an independently-normalized stochastic gradient descent algorithm, which leverages independent sampling, gradient normalization, and clipping to achieve an $\\mathcal{O}(\\epsilon^{-4})$ sample complexity under relaxed assumptions. Experiments demonstrate the fast convergence of our algorithm."
    },
    {
        "title": "Realizing Video Summarization from the Path of Language-based Semantic Understanding",
        "link_suffix": "/forum?id=ujNe7sybJu",
        "link": "https://openreview.net/forum?id=ujNe7sybJu",
        "pdf_link": "https://openreview.net/pdf?id=ujNe7sybJu",
        "keywords": "Visual Language Model, Large Language Model, Video Summarization, Video Understanding, VideoLLM",
        "abstract": "The recent development of Video-based Large Language Models (VideoLLMs), has significantly advanced video summarization by aligning video features\u2014and, in some cases, audio features\u2014with Large Language Models (LLMs). Each of these VideoLLMs possesses unique strengths and weaknesses. Many recent methods have required extensive fine-tuning to overcome the limitations of these models, which can be resource-intensive. In this work, we observe that the strengths of one VideoLLM can complement the weaknesses of another. Leveraging this insight, we propose a novel video summarization framework inspired by the Mixture of Experts (MoE) paradigm, which operates as an inference-time algorithm without requiring any form of fine-tuning. Our approach integrates multiple VideoLLMs to generate comprehensive and coherent textual summaries. It effectively combines visual and audio content, provides detailed background descriptions, and excels at identifying keyframes, which enables more semantically meaningful retrieval compared to traditional computer vision approaches that rely solely on visual information, all without the need for additional fine-tuning. Moreover, the resulting summaries enhance performance in downstream tasks such as summary video generation, either through keyframe selection or in combination with text-to-image models. Our language-driven approach offers a semantically rich alternative to conventional methods and provides flexibility to incorporate newer VideoLLMs, enhancing adaptability and performance in video summarization tasks."
    },
    {
        "title": "Flow matching achieves almost minimax optimal convergence",
        "link_suffix": "/forum?id=2OMyAFjiJJ",
        "link": "https://openreview.net/forum?id=2OMyAFjiJJ",
        "pdf_link": "https://openreview.net/pdf?id=2OMyAFjiJJ",
        "keywords": "flow matching, generative model, convergence rate, optimality",
        "abstract": "Flow matching (FM) has gained significant attention as a simulation-free generative model. Unlike diffusion models, which are based on stochastic differential equations, FM employs a simpler approach by solving an ordinary differential equation with an initial condition from a normal distribution, thus streamlining the sample generation process. This paper discusses the convergence properties of FM in terms of the $p$-Wasserstein distance, a measure of distributional discrepancy. We establish that FM can achieve an almost minimax optimal convergence rate for $1 \\leq p \\leq 2$, presenting the first theoretical evidence that FM can reach convergence rates comparable to those of diffusion models. Our analysis extends existing frameworks by examining a broader class of mean and variance functions for the vector fields and identifies specific conditions necessary to attain these optimal rates."
    },
    {
        "title": "SPLR: A Spiking Neural Network for Long-Range Temporal Dependency Learning",
        "link_suffix": "/forum?id=2Ez4dhU3NG",
        "link": "https://openreview.net/forum?id=2Ez4dhU3NG",
        "pdf_link": "https://openreview.net/pdf?id=2Ez4dhU3NG",
        "keywords": "spiking neural networks, long range dependencies, event data modelling, hippo matrix, state space models",
        "abstract": "Spiking Neural Networks (SNNs) offer an efficient framework for processing event-driven data due to their sparse, spike-based communication, making them ideal for real-time tasks. However, their inability to capture long-range dependencies limits their effectiveness in complex temporal modeling. To address this challenge, we present a Spiking Network for Learning Long-Range Relations (SPLR). SPLR address the limitations of conventional spiking network in two ways. First, we introduce SPLR convolutional layer that leverages state-space dynamics to enhance feature extraction while retaining the efficiency of sparse, event-based processing. Second, we incorporate a Spike-Aware HiPPO (SA-HiPPO) matrix that allows it to effectively maintain long-range memory by adapting the HiPPO framework for discrete, spike-driven inputs. Together, the preceding novel spike-aware state space dynamics, enhance feature extraction while retaining the efficiency of sparse, event-based processing. Experimental results across various event-based datasets demonstrate that SPLR outperforms prior methods for processing event-driven data in tasks requiring both fine-grained temporal dynamics and the retention of long-range dependencies. This unified framework advances the state of event-based learning, providing a scalable and efficient solution for real-time applications such as event-based vision and sensor fusion in neuromorphic computing."
    },
    {
        "title": "Large Scale Video Continual Learning with Bootstrapped Compression",
        "link_suffix": "/forum?id=7L2bpe7lfm",
        "link": "https://openreview.net/forum?id=7L2bpe7lfm",
        "pdf_link": "https://openreview.net/pdf?id=7L2bpe7lfm",
        "keywords": "video, video continual learning, continual learning, compression",
        "abstract": "Continual learning (CL) promises to allow neural networks to learn from continuous streams of inputs, instead of IID (independent and identically distributed) sampling, which requires random access to a full dataset. This would allow for much smaller storage requirements and self-sufficiency of deployed systems that cope with natural distribution shifts, similarly to biological learning. We focus on video CL employing a rehearsal-based approach, which reinforces past samples from a memory buffer. We posit that part of the reason why practical video CL is challenging is the high memory requirements of video, further exacerbated by long-videos and continual streams, which are at odds with the common rehearsal-buffer size constraints. To address this, we propose to use compressed vision, i.e. store video codes (embeddings) instead of raw inputs, and train a video classifier by IID sampling from this rolling buffer. Training a video compressor online (so not depending on any pre-trained networks) means that it is also subject to catastrophic forgetting. We propose a scheme to deal with this forgetting by refreshing video codes, which requires careful decompression with a previous version of the network and recompression with a new one. We expand current video CL benchmarks to large-scale settings, namely EpicKitchens-100 and Kinetics-700, with thousands of relatively long videos, and demonstrate empirically that our video CL method outperforms prior art with a significantly reduced memory footprint."
    },
    {
        "title": "Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection",
        "link_suffix": "/forum?id=mUXdysoxEP",
        "link": "https://openreview.net/forum?id=mUXdysoxEP",
        "pdf_link": "https://openreview.net/pdf?id=mUXdysoxEP",
        "keywords": "Out-of-Distribution Detection, Feature Separation, Neural Collapse",
        "abstract": "In the open world, detecting out-of-distribution (OOD) data, whose labels are disjoint with those of in-distribution (ID) samples, is important for reliable deep neural networks (DNNs). To achieve better detection performance, one type of approach proposes to fine-tune the model with auxiliary OOD datasets to amplify the difference between ID and OOD data through a separation loss defined on model outputs. However, none of these studies consider enlarging the feature disparity, which should be more effective compared to outputs. The main difficulty lies in the diversity of OOD samples, which makes it hard to describe their feature distribution, let alone design losses to separate them from ID features. In this paper, we neatly fence off the problem based on an aggregation property of ID features named Neural Collapse (NC). NC means that the penultimate features of ID samples within a class are nearly identical to the last layer weight of the corresponding class. Based on this property, we propose a simple but effective loss called Separation Loss, which binds the features of OOD data in a subspace orthogonal to the principal subspace of ID features formed by NC. In this way, the features of ID and OOD samples are separated by different dimensions. By optimizing the feature separation loss rather than purely enlarging output differences, our detection achieves SOTA performance on CIFAR10, CIFAR100 and ImageNet benchmarks without any additional data augmentation or sampling, demonstrating the importance of feature separation in OOD detection. The code will be published."
    },
    {
        "title": "MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models",
        "link_suffix": "/forum?id=qIbbBSzH6n",
        "link": "https://openreview.net/forum?id=qIbbBSzH6n",
        "pdf_link": "https://openreview.net/pdf?id=qIbbBSzH6n",
        "keywords": "Trustworthiness, multimodal foundation models, safety, hallucination, OOD generalization, bias/fairness, privacy, adversarial robustness",
        "abstract": "Multimodal foundation models (MMFMs) play a crucial role in various applications, including autonomous driving, healthcare, and virtual assistants. However, several studies have revealed vulnerabilities in these models, such as generating unsafe content by text-to-image models. Existing benchmarks on multimodal models either predominantly assess the helpfulness of these models, or only focus on limited perspectives such as fairness and privacy. In this paper, we present the first unified platform, MMDT (Multimodal DecodingTrust), designed to provide a comprehensive safety and trustworthiness evaluation for MMFMs. Our platform assesses models from multiple perspectives, including safety, hallucination, fairness/bias, privacy, adversarial robustness, and out-of-distribution (OOD) generalization. We have designed various evaluation scenarios and red teaming algorithms under different tasks for each perspective to generate challenging data, forming a high-quality benchmark. We evaluate a range of multimodal models using MMDT, and our findings reveal a series of vulnerabilities and areas for improvement across these perspectives. This work introduces the first comprehensive and unique safety and trustworthiness evaluation platform for MMFMs, paving the way for developing safer and more reliable MMFMs and systems."
    },
    {
        "title": "Ferret-UI One: Mastering Universal User Interface Understanding Across Platforms",
        "link_suffix": "/forum?id=GBfYgjOfSe",
        "link": "https://openreview.net/forum?id=GBfYgjOfSe",
        "pdf_link": "https://openreview.net/pdf?id=GBfYgjOfSe",
        "keywords": "Multimodal LLM, UI Understanding",
        "abstract": "Building a generalist model for user interface (UI) understanding is challenging due to various foundational issues, such as platform diversity, resolution variation, and data limitation. In this paper, we introduce Ferret-UI One, a multimodal large language model (MLLM) designed for universal UI understanding across a wide range of platforms, including iPhone, Android, iPad, Webpage, and AppleTV. \nBuilding on the foundation of Ferret-UI, Ferret-UI One introduces three key innovations: support for multiple platform types, high-resolution perception through adaptive scaling, and advanced task training data generation powered by GPT-4o with set-of-mark visual prompting. These advancements enable Ferret-UI One to perform complex, user-centered interactions, making it highly versatile and adaptable for the expanding diversity of platform ecosystems. Extensive empirical experiments on referring, grounding, user-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE next-action prediction dataset, and GUI-World multi-platform benchmark demonstrate that Ferret-UI One significantly outperforms Ferret-UI, and also shows strong cross-platform transfer capabilities."
    },
    {
        "title": "3CIL: Causality-Inspired Contrastive Conditional Imitation Learning for Autonomous Driving",
        "link_suffix": "/forum?id=4QVgnxXVDB",
        "link": "https://openreview.net/forum?id=4QVgnxXVDB",
        "pdf_link": "https://openreview.net/pdf?id=4QVgnxXVDB",
        "keywords": "Imitation Learning, Autonomous Driving, Causal Reasoning, Causal Confusion",
        "abstract": "Imitation learning (IL) aims to recover an expert's strategy by performing supervised learning on the demonstration datasets. Incorporating IL in safety-crucial tasks like autonomous driving is promising as it requires less interaction with the actual environment than reinforcement learning approaches. However,  the robustness of IL methods is often questioned, as phenomena like causal confusion occur frequently and hinder it from practical use. In this paper, we conduct causal reasoning to investigate the crucial requirements for the ideal imitation generalization performance. With insights derived from modeled causalities, we propose causality-inspired contrastive conditional imitation learning (3CIL), a conditional imitation learning method equipped with contrastive learning and action residual prediction tasks, regularizing the imitator in causal and anti-causal directions. To mitigate the divergence with experts in unfamiliar scenarios, 3CIL introduces a sample-weighting term that transforms the prediction error into an emphasis on critical samples. Extensive experiments in the CARLA simulator show the proposed method significantly improves the driving capabilities of models."
    },
    {
        "title": "Are you SURE? Enhancing Multimodal Pretraining with Missing Modalities through Uncertainty Estimation",
        "link_suffix": "/forum?id=IT7LSnBdtY",
        "link": "https://openreview.net/forum?id=IT7LSnBdtY",
        "pdf_link": "https://openreview.net/pdf?id=IT7LSnBdtY",
        "keywords": "Multimodal learning, Uncertainty estimation",
        "abstract": "Multimodal learning has demonstrated incredible successes by integrating diverse data sources, yet it often relies on the availability of all modalities - an assumption that rarely holds in real-world applications. Pretrained multimodal models, while effective, struggle when confronted with small-scale and incomplete datasets (i.e., missing modalities), limiting their practical applicability. Previous studies on reconstructing missing modalities have overlooked the reconstruction's potential unreliability, which could compromise the quality of the final outputs. We presentSURE(Scalable Uncertainty and Reconstruction Estimation), a novel framework that extends the capabilities of pretrained multimodal models by introducing latent space reconstruction and uncertainty estimation for both reconstructed modalities and downstream tasks.  Our method is architecture-agnostic, reconstructs missing modalities, and delivers reliable uncertainty estimates, improving both interpretability and performance. SURE introduces a unique Pearson Correlation-based loss and applies statistical error propagation in deep networks for the first time, allowing precise quantification of uncertainties from missing data and model predictions. Extensive experiments across tasks such as sentiment analysis, genre classification, and action recognition show that SURE consistently achieves state-of-the-art performance, ensuring robust predictions even in the presence of incomplete data."
    },
    {
        "title": "MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models",
        "link_suffix": "/forum?id=Usklli4gMc",
        "link": "https://openreview.net/forum?id=Usklli4gMc",
        "pdf_link": "https://openreview.net/pdf?id=Usklli4gMc",
        "keywords": "multimodal, rag, vision-language, visual knowledge",
        "abstract": "Existing multimodal retrieval benchmarks primarily focus on evaluating whether models can retrieve and utilize external textual knowledge for question answering. However, there are scenarios where retrieving visual information is either more beneficial or easier to access than textual data. \nIn this paper, we introduce a multimodal retrieval-augmented generation benchmark, MRAG-Bench, in which we systematically identify and categorize scenarios where visually augmented knowledge is better than textual knowledge, for instance, more images from varying viewpoints.\nMRAG-Bench consists of 16,130 images and 1,353 human-annotated multiple-choice questions across 9 distinct scenarios. With MRAG-Bench, we conduct an evaluation of 10 open-source and 4 proprietary large vision-language models (LVLMs). Our results show that all LVLMs exhibit greater improvements when augmented with images compared to textual knowledge, confirming that MRAG-Bench is vision-centric. Additionally, we conduct extensive analysis with MRAG-Bench, which offers valuable insights into retrieval-augmented LVLMs. Notably, the top-performing model, GPT-4o, faces challenges in effectively leveraging retrieved knowledge, achieving only a 5.82% improvement with ground-truth information, in contrast to a 33.16% improvement observed in human participants. These findings highlight the importance of MRAG-Bench in encouraging the community to enhance LVLMs' ability to utilize retrieved visual knowledge more effectively."
    },
    {
        "title": "BRSSD10k :  A SEGMENTATION DATASET\\OF BANGLADESHI ROAD SCENARIO",
        "link_suffix": "/forum?id=1W6oINj8ne",
        "link": "https://openreview.net/forum?id=1W6oINj8ne",
        "pdf_link": "https://openreview.net/pdf?id=1W6oINj8ne",
        "keywords": "Instance Segmentation, Computer Vision, Dataset, Autonomous Driving, Bangadeshi Road",
        "abstract": "In this paper, we present a novel Bangladeshi Road Scenario Segmentation Dataset designed to advance autonomous driving technologies under the challenging and diverse road conditions of Bangladesh. This comprehensive instance segmentation dataset comprised 10,082 high-resolution images captured across nine major cities, including Dhaka, Sylhet, Chittagong, and Rajshahi, addressing the critical need for region-specific computer vision data in developing countries. Unlike existing autonomous driving datasets that primarily focus on western road conditions, BRSSD10k encompasses a wide range of environments unique to Bangladesh, including unstructured urban areas, hilly terrains, village roads, and densely populated city centers. The dataset features instance segmentation annotations with classes specifically tailored to reflect the distinctive elements of Bangladeshi roads, such as rickshaws, CNGs (auto-rickshaws), informal roadside stalls, and various nonstandard vehicles. To demonstrate its utility as a benchmarking tool for autonomous driving systems, we present comparative results from several state-of-the-art instance segmentation models tested on this dataset, achieving an mAP of 0.441. This evaluation not only showcases the dataset's effectiveness in assessing model performance but also underscores the need for adaptive algorithms capable of handling diverse and unpredictable urban environments in the context of autonomous navigation."
    },
    {
        "title": "SOP-Agent: Empower General Purpose AI Agent with Domain-Specific SOPs",
        "link_suffix": "/forum?id=oWm80iR1m9",
        "link": "https://openreview.net/forum?id=oWm80iR1m9",
        "pdf_link": "https://openreview.net/pdf?id=oWm80iR1m9",
        "keywords": "autonomous agent, grounded decision-making",
        "abstract": "Despite significant advancements in general-purpose AI agents, several challenges still hinder their practical application in real-world scenarios. First, the limited planning capabilities of Large Language Models (LLM) restrict AI agents from effectively solving complex tasks that require long-horizon planning (Liu et al. 2023). Second, general-purpose AI agents struggle to efficiently utilize domain-specific knowledge and human expertise. In this paper, we introduce the Standard Operational Procedure-guided Agent (SOP-agent), a novel framework for constructing domain-specific agents through pseudocode-style Standard Operational Procedures (SOPs) written in natural language. Formally, we represent a SOP as a decision graph, which is traversed to guide the agent in completing tasks specified by the SOP. We conduct extensive experiments across tasks in multiple domains, including decision-making, search and reasoning, code generation, data cleaning, and grounded customer service. The SOP-agent demonstrates excellent versatility, achieving performance superior to general-purpose agent frameworks and comparable to domain-specific agent systems. Additionally, we introduce the Grounded Customer Service Benchmark, the first benchmark designed to evaluate the grounded decision-making capabilities of AI agents in customer service scenarios based on SOPs."
    },
    {
        "title": "Large-Scale Dynamic Graph Generation via LLM-based Agent Simulation",
        "link_suffix": "/forum?id=qWLgJCl1Y6",
        "link": "https://openreview.net/forum?id=qWLgJCl1Y6",
        "pdf_link": "https://openreview.net/pdf?id=qWLgJCl1Y6",
        "keywords": "LLM-based Agents, graph generation",
        "abstract": "Graph generation is a fundamental task that has been extensively studied in social, technological, and scientific analysis. \nFor modeling the dynamic graph evolution process, traditional rule-based methods struggle to capture community structures within graphs, while deep learning methods only focus on fitting training graphs. \nThis limits existing graph generators to producing graphs that adhere to predefined rules or closely resemble training datasets, achieving poor performance in dynamic graph generation.\nGiven that graphs are abstract representations arising from pairwise interactions in human activities, a realistic simulation of human-wise interaction could provide deeper insights into the graph evolution mechanism.\nWith the increasing recognition of large language models (LLMs) in simulating human behavior, we introduce GraphAgent-Generator (GAG), a novel simulation-based framework for dynamic graph generation. Without training or fine-tuning process of LLM, our framework effectively replicates seven macro-level structural characteristics in established network science theories while surpassing existing baselines in graph expansion tasks by 31% on specific evaluation metrics. Through node classification task, we validate GAG effectively preserves characteristics of real-world network for node-wise textual features in generated text-rich graph. \nFurthermore, by incorporating parallel acceleration, GAG supports generating graphs with up to nearly 100,000 nodes or 10 million edges through large-scale LLM-based agent simulation, with a minimum speed-up of 90.4%. \nThe source code is available at \\url{https://anonymous.4open.science/r/GraphAgent-2206}."
    },
    {
        "title": "APCtrl: Adding Conditional Control to Diffusion Models by Alternative Projection",
        "link_suffix": "/forum?id=yPxhj1FKhG",
        "link": "https://openreview.net/forum?id=yPxhj1FKhG",
        "pdf_link": "https://openreview.net/pdf?id=yPxhj1FKhG",
        "keywords": "Diffusion Models, Condition Diffusion, Alternative Projection, Control-on-Training, Control-on-Sampling",
        "abstract": "Enhancing the versatility of pretrained diffusion models through advanced conditioning techniques is crucial for improving their applicability. We present APCtrl, a novel conditional image generation approach that formulates the latent ( \\dmrv{z}\\dms{t} ) at timestep ( t ) as the projection ( \\dmrv{z}\\dms{t} = \\text{Proj}{\\bmfrakD\\dms{t}} (\\dmrv{z}{ \\dms{t} + \\dms{1} }) ) onto the denosing set ( \\bmfrakD\\dms{t} ). For conditional control, APCtrl integrates the condition  set ( \\bmfrakC_\\dms{t} ), defined by a latent control network (\\bmcalA_{\\dmv{theta}}(\\cdot, \\cdot)). Our method simplifies conditional sampling to recursive projections ( \\dmrv{z}\\dms{t} = \\text{Proj}{\\bmfrakI_\\dms{t}} \\circ \\text{Proj}{\\bmfrakD\\dms{t}} (\\dmrv{z}_{ \\dms{t} + \\dms{1} }) ), where each projection step integrates both the diffusion and condition priors. By employing Alternative Projection, our approach offers several key advantages: 1. Multi-Condition Generation: easily expandable with additional conditional sets; 2. Model and Sampling Agnosticism: works with any model or sampling method; 3. Unified Control Loss: simplifies the management of diverse control applications; 4. Efficiency: delivers comparable control with reduced training and sampling times. Extensive experiments demonstrate the superior performance of our method."
    },
    {
        "title": "Conflict-Averse Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning",
        "link_suffix": "/forum?id=ogXkmugNZw",
        "link": "https://openreview.net/forum?id=ogXkmugNZw",
        "pdf_link": "https://openreview.net/pdf?id=ogXkmugNZw",
        "keywords": "Multi-Objective Reinforcement Learning, Safe Reinforcement Learning, Gradient Aggregation",
        "abstract": "In real-world applications, a reinforcement learning (RL) agent should consider multiple objectives and adhere to safety guidelines.\nTo address these considerations, we propose a constrained multi-objective RL algorithm named constrained multi-objective gradient aggregator (CoMOGA).\nIn the field of multi-objective optimization, managing conflicts between the gradients of the multiple objectives is crucial to prevent policies from converging to local optima.\nIt is also essential to efficiently handle safety constraints for stable training and constraint satisfaction.\nWe address these challenges straightforwardly by treating the maximization of multiple objectives as a constrained optimization problem (COP), where the constraints are defined to improve the original objectives.\nExisting safety constraints are then integrated into the COP, and the policy is updated by solving the COP, which ensures the avoidance of gradient conflicts.\nDespite its simplicity, CoMOGA guarantees convergence to global optima in a tabular setting.\nThrough various experiments, we have confirmed that preventing gradient conflicts is critical, and the proposed method achieves constraint satisfaction across all tasks."
    },
    {
        "title": "MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer",
        "link_suffix": "/forum?id=d23EVDRJ6g",
        "link": "https://openreview.net/forum?id=d23EVDRJ6g",
        "pdf_link": "https://openreview.net/pdf?id=d23EVDRJ6g",
        "keywords": "motion synthesis, generative masked modelling, vector quantization, single instance learning",
        "abstract": "Generative masked transformer have demonstrated remarkable success across various content generation tasks, primarily due to their ability to effectively model large-scale dataset distributions with high consistency. However, in the animation domain, large datasets are not always available. Applying generative masked modeling to generate diverse instances from a single MoCap reference may lead to overfitting, a challenge that remains unexplored. In this work, we present MotionDreamer, a localized masked modeling paradigm designed to learn motion internal patterns from a given motion with arbitrary topology and duration. By embedding the given motion into quantized tokens with a novel distribution regularization method, MotionDreamer constructs a robust and informative codebook for local motion patterns. Moreover, a sliding window local attention is introduced in our masked transformer, enabling the generation of natural yet diverse animations that closely resemble the reference motion patterns. As demonstrated through comprehensive experiments, MotionDreamer outperforms the state-of-the-art methods that are typically GAN or Diffusion-based in both faithfulness and diversity. Thanks to the consistency and robustness of quantization-based approach, MotionDreamer can also effectively perform downstream tasks such as temporal motion editing, crowd motion synthesis, and beat-aligned dance generation, all using a single reference motion. Our implementation, learned models and results are to be made publicly available upon paper acceptance."
    }
]