[{"title": "Evolving Virtual World with Delta-Engine", "link_suffix": "/forum?id=RVSQpkfsLq", "link": "https://openreview.net/forum?id=RVSQpkfsLq", "pdf_link": "https://openreview.net/pdf?id=RVSQpkfsLq", "keywords": "virtual world, role-playing games, large language model", "abstract": "In this paper, we focus on the \\emph{virtual world}, a cyberspace that people can live in. In a sense, any video game can be regarded as a virtual world. However, the fundamental difference between them is the evolving nature, which means the real world is constantly changed by humans' behavior, while existing virtual worlds are strictly defined by the back-end engine and cannot be changed by users' behavior.\nFor this, we propose a special world engine called \\textbf{\\emph{Delta-Engine}} to drive this virtual world. $\\Delta$ associates the world's evolution with the engine's scaling. It consists of a base engine and a neural proxy. The base engine programs the prototype of the virtual world; given a trigger, the neural proxy generates new snippets on the base engine through \\emph{incremental prediction}.\nThis paper presents a full-stack introduction to the delta-engine. The key feature of the delta-engine is its scalability to user-generated content. Technically, this is supported by dual aspects of algorithm and data. To do this, we leverage the retrieval technique to enhance the connection of the neural proxy and the base engine, and propose the human-LLM collaborative design to align the neural proxy with novel and interesting data.", "title_embedding_index": 16750, "title_abs_embedding_index": 16775}, {"title": "Enhancing Factuality in Detailed Image Captioning with LLM-MLLM Collaboration", "link_suffix": "/forum?id=psIymxANmd", "link": "https://openreview.net/forum?id=psIymxANmd", "pdf_link": "https://openreview.net/pdf?id=psIymxANmd", "keywords": "Image Captioning, Caption Evaluation Metric, Multimodal Large Language Model, Large Language Model", "abstract": "Multimodal large language models (MLLMs) capable of interpreting images can generate highly detailed and extensive captions, owing to their advanced language modeling capabilities. However, the captions they produce frequently contain hallucinations. Furthermore, our empirical analysis reveals that existing hallucination detection methods are less effective in detailed image captioning tasks. We attribute this to the increasing reliance of MLLMs on their own generated text, rather than the input image, as the sequence length grows. To address this issue, we propose a novel corrector-based method that decomposes a given caption into atomic propositions, evaluates the factuality of each unit, and revises the caption accordingly. Our method is training-free and can be applied in a plug-and-play manner to any captioning model. Additionally, we introduce an evaluation framework and a benchmark dataset to facilitate the systematic analysis of detailed captions. Our experiments demonstrate that existing approaches to improve the factuality of MLLM outputs may fall short in detailed image captioning tasks. In contrast, our proposed method significantly enhances the factual accuracy of captions, even improving those generated by GPT-4V. Finally, we highlight a limitation of VQA-centric benchmarking by demonstrating that an MLLM's performance on VQA benchmarks may not correlate with its ability to generate detailed image captions.", "title_embedding_index": 16751, "title_abs_embedding_index": 16776}, {"title": "NoisyTraj: Robust Trajectory Prediction with Noisy Observations", "link_suffix": "/forum?id=7mdi1i1mSd", "link": "https://openreview.net/forum?id=7mdi1i1mSd", "pdf_link": "https://openreview.net/pdf?id=7mdi1i1mSd", "keywords": "Trajectory prediction", "abstract": "Trajectory prediction aims to forecast an agent's future trajectories based on its historical observed trajectories, which is a critical task for various applications such as autonomous driving, robotics, and surveillance systems. Most existing trajectory prediction methods assume that the observed trajectories collected for forecasting are clean. However, in real-world scenarios, noise is inevitably introduced into the observations due to errors from sensors, detection, and tracking processes, resulting in the collapse of the existing approaches. Therefore, it is essential to perform robust trajectory prediction based on noisy observations, which is a more practical scenario. In this paper, we propose NoisyTraj, a noise-agnostic approach capable of tackling the problem of trajectory prediction with arbitrary types of noisy observations. Specifically, we put forward a mutual information-based mechanism to denoise the original noisy observations. This mechanism optimizes the produced trajectories to exhibit a pattern that closely resembles the clean trajectory pattern while deviating from the noisy one.\nConsidering that the trajectory structure may be destroyed through the only optimization of mutual information, we introduce an additional reconstruction loss to preserve the structure information of the produced observed trajectories. Moreover, we further propose a ranking loss based on the intuitive idea that prediction performance using denoised trajectories should surpass that using the original noisy observations, thereby further enhancing performance. \nBecause NoisyTraj does not rely on any specific module tailored to particular noise distributions, it can handle arbitrary types of noise in principle.\nAdditionally, our proposed NoisyTraj can be easily integrated into existing trajectory prediction models. Extensive experiments conducted on the ETH/UCY and Stanford Drone datasets (SDD) demonstrate that NoisyTraj significantly improves the accuracy of trajectory prediction with noisy observations, compared to the baselines.", "title_embedding_index": 16752, "title_abs_embedding_index": 16777}, {"title": "AutoModel: Autonomous Model Development for Image Classification with LLM Agents", "link_suffix": "/forum?id=6ofUPFtqPF", "link": "https://openreview.net/forum?id=6ofUPFtqPF", "pdf_link": "https://openreview.net/pdf?id=6ofUPFtqPF", "keywords": "AI agents, automation, computer vision", "abstract": "Computer vision is a critical component in a wide range of real-world applications, including plant monitoring in agriculture and handwriting classification in digital systems. However, developing high-quality computer vision systems traditionally requires both machine learning (ML) expertise and domain-specific knowledge, making the process labor-intensive, costly, and inaccessible to many. To address these challenges, we introduce AutoModel, an LLM agent framework that autonomously builds and optimizes image classification models. By leveraging the collaboration of specialized LLM agents, AutoModel removes the need for ML practitioners or domain experts for model development, streamlining the process and democratizing image classification. In this work, we evaluate AutoModel across a diverse range of datasets consisting of varying sizes and domains, including standard benchmarks and Kaggle competition datasets, demonstrating that it consistently outperforms zero-shot LLM-generated pipelines and achieves human practitioner-level performance.", "title_embedding_index": 16753, "title_abs_embedding_index": 16778}, {"title": "Decomposition of one-layer neural networks via the infinite sum of reproducing kernel Banach spaces", "link_suffix": "/forum?id=CFMdrcK935", "link": "https://openreview.net/forum?id=CFMdrcK935", "pdf_link": "https://openreview.net/pdf?id=CFMdrcK935", "keywords": "Neural networks, Reproducing kernel Banach spaces, Class of  Integral RKBSs", "abstract": "In this paper, we define the sum of RKBSs using the characterization theorem of RKBSs and show that the sum of RKBSs is compatible with the direct sum of feature spaces. Moreover, we decompose the integral RKBS $\\mathcal{F}{\\sigma}(\\mathcal{X},\\Omega)$ into the sum of $p$-norm RKBSs ${\\mathcal{L}{\\sigma}^{1}(\\mu_{i})}_{i\\in I}$. Finally, we provide some applications to enhance the structural understanding of the integral RKBS class.", "title_embedding_index": 16754, "title_abs_embedding_index": 16779}, {"title": "Position-Query-Based Autoencoders for View Decoupled Cross Point Cloud Reconstruction and a Self-Supervised Learning Framework", "link_suffix": "/forum?id=jZASmAlxp2", "link": "https://openreview.net/forum?id=jZASmAlxp2", "pdf_link": "https://openreview.net/pdf?id=jZASmAlxp2", "keywords": "Representation learning, Self-supervised learning, Point Cloud, Generative Methods", "abstract": "Point cloud learning, especially in a self-supervised way without manual labels, has received emerging attention in both vision and learning communities, with its potential utility in wide areas. Most existing generative approaches for point cloud self-supervised learning focus on recovering masked points from visible ones within a single view. Recognizing that a two-view pre-training paradigm inherently introduces greater diversity and variance, it could thus enable more challenging and informative pre-training. Inspired by this, we explore the potential of two-view learning in this domain. In this paper, we propose Point-PQAE, a cross-reconstruction generative paradigm that first generates two decoupled point clouds/views and then reconstructs one from the other. To achieve this goal, we develop a crop mechanism for point cloud view generation for the first time and further propose a novel positional encoding to represent the 3D relative position between the two decoupled views. The cross-reconstruction significantly increases the difficulty of pre-training compared to self-reconstruction, which enables our method to achieve new state-of-the-art results and surpasses previous single-modal self-reconstruction methods in 3D self-supervised learning by a large margin. Specifically, it outperforms self-reconstruction baseline (Point-MAE) 6.5%, 7.0%, 6.7% in three variants of ScanObjectNN with Mlp-Linear evaluation protocol. Source code will be released.", "title_embedding_index": 16755, "title_abs_embedding_index": 16780}, {"title": "The Role of Task Complexity in Emergent Abilities of Small Language Models", "link_suffix": "/forum?id=OW5Gf4cse1", "link": "https://openreview.net/forum?id=OW5Gf4cse1", "pdf_link": "https://openreview.net/pdf?id=OW5Gf4cse1", "keywords": "LLM, ListOps, Emergent Abilities, Scaling Laws", "abstract": "We investigate the relationship between task complexity and the minimum model\nsize required for learning specific tasks in small transformer models. We focus\non the ListOps dataset, consisting of nested math operations. We define the task\ncomplexity as the Kolmogorov complexity (KC) of the code solving the task, using\na rough proxy for KC. We find a power-law relation between KC and parameters\nrequired to learn, suggesting number of parameters to learn harder task increases\nalmost cubic in KC. On individual math operations, sum mod 10 is hardest to\nlearn. Surprisingly, when combining tasks, we observe that sum is learned earlier\nand with fewer parameters when trained alongside max and median. Analyzing\nthe model, we find strong evidence that models trained on sum alone and models\ntrained jointly converge to different algorithms. Concretely, the sum alone model\ndoesn\u2019t seem to have learned number properties in the embedding layer, likely\nmemorizing the sum table. In contrast, models trained on three tasks (maximum,\nmedian and sum) reveals that joint training results in clear number-like properties.\nFinally, we also find evidence that the sum-only model utilizes its feedforward\nlayer more than the jointly trained model. Conversely, the attention layer in the\njoint model is activated more than the sum model. Our findings suggest there is\nanother dimension to emergent abilities in language models, namely the algorithms\nbeing learned, potentially impacting scaling laws.", "title_embedding_index": 16756, "title_abs_embedding_index": 16781}, {"title": "Sampling-guided Heterogeneous Graph Neural Network with Temporal Smoothing for Scalable Longitudinal Data Imputation", "link_suffix": "/forum?id=kat8uANDlU", "link": "https://openreview.net/forum?id=kat8uANDlU", "pdf_link": "https://openreview.net/pdf?id=kat8uANDlU", "keywords": "Graph Neural Network, Missing data imputation, Longitudinal data, Representative learning", "abstract": "In this paper, we propose a novel framework, the Sampling-guided Heterogeneous Graph Neural Network ($\\text{S\\small{HT-GNN}}$), to effectively tackle the challenge of missing data imputation in longitudinal studies. Unlike traditional methods, which often require extensive preprocessing to handle irregular or inconsistent missing data, our approach accommodates arbitrary missing data patterns while maintaining computational efficiency. $\\text{S\\small{HT-GNN}}$ models both observations and covariates as distinct node types, connecting observation nodes at successive time points through subject-specific longitudinal subnetworks, while covariate-observation interactions are represented by attributed edges within bipartite graphs. By leveraging subject-wise mini-batch sampling and a multi-layer temporal smoothing mechanism, $\\text{S\\small{HT-GNN}}$ efficiently scales to large datasets, while effectively learning node representations and imputing missing data. Extensive experiments on both synthetic and real-world datasets, including the Alzheimer's Disease Neuroimaging Initiative ($\\text{A\\small{DNI}}$) dataset, demonstrate that $\\text{S\\small{HT-GNN}}$ significantly outperforms existing imputation methods, even with high missing data rates (e.g., 80%). The empirical results highlight $\\text{S\\small{HT-GNN}}$\u2019s robust imputation capabilities and superior performance, particularly in the context of complex, large-scale longitudinal data.", "title_embedding_index": 16757, "title_abs_embedding_index": 16782}, {"title": "Stable Diffusion Feature Extraction for Sketching with One Example", "link_suffix": "/forum?id=1vjMuNJ2Ik", "link": "https://openreview.net/forum?id=1vjMuNJ2Ik", "pdf_link": "https://openreview.net/pdf?id=1vjMuNJ2Ik", "keywords": "Diffusion Model, Stable Diffusion, Domain Adaptation, Sketch Extraction, Single Shot", "abstract": "Sketching is both a fundamental artistic expression and a crucial aspect of art. The significance of sketching has increased alongside the development of sketch-based generative and editing models. \nTo enable individuals to use these sketch-based generative models effectively, personalizing sketch extraction is crucial. In response, we introduce $\\text{DiffSketch}$, a novel method capable of generating various geometrically aligned sketches from text or images, using a single manual drawing for training the style. Our method exploits rich information available in features from a pretrained Stable Diffusion model to achieve effective domain adaptation. To further streamline the process of sketch extraction, we further refine our approach by distilling the knowledge from the trained generator into the image-to-sketch network, which is termed as $\\text{DiffSketch}_{distilled}$. Through a series of comparisons, we verify that our method not only outperforms existing state-of-the-art sketch extraction methods but also surpasses diffusion-based stylization methods in the task of extracting sketches.", "title_embedding_index": 16758, "title_abs_embedding_index": 16783}, {"title": "Compositional Entailment Learning for Hyperbolic Vision-Language Models", "link_suffix": "/forum?id=3i13Gev2hV", "link": "https://openreview.net/forum?id=3i13Gev2hV", "pdf_link": "https://openreview.net/pdf?id=3i13Gev2hV", "keywords": "Vision-Language Models, Hyperbolic Geometry, Representation Learning, CLIP", "abstract": "Image-text representation learning forms a cornerstone in vision-language models, where pairs of images and textual descriptions are contrastively aligned in a shared embedding space. Since visual and textual concepts are naturally hierarchical, recent work has shown that hyperbolic space can serve as a high-potential manifold to learn vision-language representation with strong downstream performance. In this work, for the first time we show how to fully leverage the innate hierarchical nature of hyperbolic embeddings by looking beyond individual image-text pairs. We propose Compositional Entailment Learning for hyperbolic vision-language models. The idea is that an image is not only described by a sentence but is itself a composition of multiple object boxes, each with their own textual description. Such information can be obtained freely by extracting nouns from sentences and using openly available localized grounding models. We show how to hierarchically organize images, image boxes, and their textual descriptions through contrastive and entailment-based objectives. Empirical evaluation on a hyperbolic vision-language model trained with millions of image-text pairs shows that the proposed compositional learning approach outperforms conventional Euclidean CLIP learning, as well as recent hyperbolic alternatives, with better zero-shot and retrieval generalization and clearly stronger hierarchical performance.", "title_embedding_index": 16759, "title_abs_embedding_index": 16784}, {"title": "Rule-Based Rating and Selection of LLM Training Data", "link_suffix": "/forum?id=SpTzsQjgxF", "link": "https://openreview.net/forum?id=SpTzsQjgxF", "pdf_link": "https://openreview.net/pdf?id=SpTzsQjgxF", "keywords": "large language models, data selection, data rating", "abstract": "The quality of training data is crucial for the performance of large language models (LLMs). There are recent studies utilizing LLMs to rate and select data based on scores from a small set of human-designed metrics (rules). However, existing rule-based methods often overly rely on human heuristics, lack robust metrics for rule evaluation, and exhibit limited adaptability to new tasks. In our work, we propose a novel rule-based framework that leverages the orthogonality of score vectors corresponding to rules as a unique metric for rule evaluation. Our method employs an automated pipeline that first uses LLMs to generate a diverse set of rules, covering a wide range of rating aspects. It then rates a batch of data according to these rules and applies the determinantal point process (DPP) from random matrix theory to select the most orthogonal score vectors,  effectively isolating a subset of independent rules. Then these rules are applied to rate all data and samples with the highest average scores are selected for further downstream tasks such as LLM training. We validate our method through two experimental setups: 1) comparison against ground truth ratings and 2) benchmarking LLMs trained with the selected data. Our extensive experiments span various settings, including general pre-training and domain-specific fine-tuning in fields such as IMDB, Medical, Math, and Code. The results show that our DPP rule-based rating method consistently outperforms other methods, such as rating without rules, uniform sampling, importance resampling, and QuRating, in terms of both rating accuracy and model performance.", "title_embedding_index": 16760, "title_abs_embedding_index": 16785}, {"title": "Derail Yourself: Multi-turn LLM Jailbreak Attack through self-discovered clues", "link_suffix": "/forum?id=kvvvUPDAPt", "link": "https://openreview.net/forum?id=kvvvUPDAPt", "pdf_link": "https://openreview.net/pdf?id=kvvvUPDAPt", "keywords": "LLM jailbreak, LLM safety alignment, Multi-turn attack", "abstract": "This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets. ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues. In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-o1. We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack. We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks.", "title_embedding_index": 16761, "title_abs_embedding_index": 16786}, {"title": "Rationalizing and Augmenting Dynamic Graph Neural Networks", "link_suffix": "/forum?id=thV5KRQFgQ", "link": "https://openreview.net/forum?id=thV5KRQFgQ", "pdf_link": "https://openreview.net/pdf?id=thV5KRQFgQ", "keywords": "Graph Data Augmentation, Dynamic Graph Neural Networks", "abstract": "Graph data augmentation (GDA) has shown significant promise in enhancing the performance, generalization, and robustness of graph neural networks (GNNs). However, contemporary methodologies are often limited to static graphs, whose applicability on dynamic graphs\u2014more prevalent in real-world applications\u2014remains unexamined. In this paper, we empirically highlight the challenges faced by static GDA methods when applied to dynamic graphs, particularly their inability to maintain temporal consistency. In light of this limitation, we propose a dedicated augmentation framework for dynamic graphs, termed $\\texttt{DyAug}$, which adaptively augments the evolving graph structure with temporal consistency awareness. Specifically, we introduce the paradigm of graph rationalization for dynamic GNNs, progressively distinguishing between causal subgraphs (\\textit{rationale}) and the non-causal complement (\\textit{environment}) across snapshots. We develop three types of environment replacement, including, spatial, temporal, and spatial-temporal, to facilitate data augmentation in the latent representation space, thereby improving the performance, generalization, and robustness of dynamic GNNs. Extensive experiments on six benchmarks and three GNN backbones demonstrate that $\\texttt{DyAug}$ can \\textbf{(I)} improve the performance of dynamic GNNs by $0.89\\%\\sim3.13\\%\\uparrow$; \\textbf{(II)} effectively counter targeted and non-targeted adversarial attacks with $6.2\\%\\sim12.2\\%\\uparrow$ performance boost; \\textbf{(III)} make stable predictions under temporal distribution shifts.", "title_embedding_index": 16762, "title_abs_embedding_index": 16787}, {"title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage", "link_suffix": "/forum?id=0bmGL4q7vJ", "link": "https://openreview.net/forum?id=0bmGL4q7vJ", "pdf_link": "https://openreview.net/pdf?id=0bmGL4q7vJ", "keywords": "Multimodal Agents, Vision-language Model, Tool usage", "abstract": "The advancement of large language models (LLMs) prompts the development of multi-modal agents, providing a feasible way to solve practical tasks by using tools. In this paper, we propose a multi-modal agent tuning method that automatically generates multi-modal tool-usage data and tunes a vision-language model (VLM) as the controller for powerful tool-usage reasoning. To preserve the data quality, we prompt the GPT-4o model to separately generate queries, files, and trajectories, followed by a query-file verifier and trajectory verifier. Based on the data synthesis pipeline, we collect the MM-traj dataset with 20k tasks using 10 tools. Then, we build the T3-agent that uses MiniCPM-V as the controller Trajectory Tuning for Tool usage using MM-Traj.  Evaluations on the GTA and GAIA benchmarks show that the T3-agent has achieved remarkable improvements and outperforms GPT-4 driven agents by 10%, showing the effectiveness of the proposed data synthesis pipeline that leads to better reasoning capabilities in tool usage.", "title_embedding_index": 16763, "title_abs_embedding_index": 16788}, {"title": "Bridging the Gap Beteween SL and TD Learning via Q-conditioned maximization", "link_suffix": "/forum?id=BMWOw3xhUQ", "link": "https://openreview.net/forum?id=BMWOw3xhUQ", "pdf_link": "https://openreview.net/pdf?id=BMWOw3xhUQ", "keywords": "Goal-Conditioned Reinforcement Learning, Data Augmentation, Stitching Property", "abstract": "Recent research highlights the efficacy of supervised learning (SL) as a methodology within reinforcement learning (RL), yielding commendable results. Nonetheless, investigations reveal that SL-based methods lack the stitching capability typically associated with RL approaches such as TD learning, which facilitate the resolution of tasks by stitching diverse trajectory segments. This prompts the question: How can SL methods be endowed with stitching property and bridge the gap with TD learning? This paper addresses this challenge by exploring the maximization of the objective in the goal-conditioned RL. We introduce the concept of Q-conditioned maximization supervised learning, grounded in the assertion that the goal-conditioned RL objective is equivalent to the Q-function, thus embedding Q-function maximization into traditional SL-based methodologies. Building upon this premise, we propose Goal-Conditioned Reinforced Supervised Learning (GCReinSL), which enhances SL-based approaches by incorporating maximize Q-function. GCReinSL emphasizes the maximization of the Q-function during the training phase to estimate the maximum expected return within the distribution, subsequently guiding optimal action selection during the inference process. We demonstrate that GCReinSL enables SL methods to exhibit stitching property, effectively equivalent to applying goal data augmentation to SL methods. Experimental results on offline datasets designed to evaluate stitching capability show that our approach not only effectively selects appropriate goals across diverse trajectories but also outperforms previous works that applied goal data augmentation to SL methods.", "title_embedding_index": 16764, "title_abs_embedding_index": 16789}, {"title": "Monitoring Latent World States in Language Models with Propositional Probes", "link_suffix": "/forum?id=0yvZm2AjUr", "link": "https://openreview.net/forum?id=0yvZm2AjUr", "pdf_link": "https://openreview.net/pdf?id=0yvZm2AjUr", "keywords": "Interpretability, Language models, AI Safety", "abstract": "Language models (LMs) are susceptible to bias, sycophancy, backdoors, and other tendencies that lead to unfaithful responses to the input context. Interpreting internal states of LMs could help monitor and correct unfaithful behavior. We hypothesize that LMs faithfully represent their input contexts in a latent world model, and we seek to extract these latent world states as logical propositions. For example, given the input context ``Greg is a nurse. Laura is a physicist.'', we aim to decode the propositions WorksAs(Greg, nurse) and WorksAs(Laura, physicist) from the model's internal activations. To do so we introducepropositional probes, which compositionally extract lexical concepts from token activations and bind them into propositions. Key to this is identifying abinding subspacein which bound tokens have high similarity (Greg $\\leftrightarrow$ nurse) but unbound ones do not (Greg $\\not\\leftrightarrow$ physicist). Despite only being trained on linguistically simple English templates, we find that propositional probes generalize to inputs written as short stories and translated to Spanish. Moreover, in three settings where LMs respond unfaithfully to the input context---prompt injections, backdoor attacks, and gender bias--- the decoded propositions remain faithful. This suggests that LMs often encode a faithful world model but decode it unfaithfully, which motivates the search for better interpretability tools for monitoring LMs.", "title_embedding_index": 16765, "title_abs_embedding_index": 16790}, {"title": "Human Expertise Really Matters!  Mitigating Unfair Utility Induced by Heterogenous Human Expertise in  AI-assisted Decision-Making", "link_suffix": "/forum?id=MUWkqH6e7d", "link": "https://openreview.net/forum?id=MUWkqH6e7d", "pdf_link": "https://openreview.net/pdf?id=MUWkqH6e7d", "keywords": "AI for good, Human-centric ML, Fairness, Calibration", "abstract": "AI-assisted decision-making often involves an AI model providing calibrated confidence, which helps humans integrate these with their own confidence to make higher-utility final decisions. However, when human decision-makers are heterogeneous in their expertise, existing AI-assisted decision-making may fail to provide fair utility across them. Such unfairness raises concerns about social welfare among diverse humans due to inequities in access to equally effective AI assistance, which may reduce their willingness and trust to engage with AI systems. In this work, we investigate how to calibrate  AI confidence to provide fair utility across diverse human populations with heterogeneous expertise. We first demonstrate that rational decision-makers with heterogeneous expertise are unlikely to obtain fair decision utility from existing AI confidence calibrations. We propose a novel confidence calibration criterion,inter-group alignment, which synergizes with human alignment to jointly determine the upper bound of utility disparity across diverse human populations.  Building on this foundation, we propose a new fairness-aware confidence calibration method,group-level multicalibration, which ensures a sufficient condition for achieving both inter-group and human alignment. We validate our theoretical findings through extensive experiments on four real-world multimodal tasks, where classifiers assist human experts in decision-making. The results indicate that our calibrated AI confidence facilitates fairer utility across human populations, concurrently enhancing overall utility.The implementation code is available athttps://anonymous.4open.science/r/iclr4103.", "title_embedding_index": 16766, "title_abs_embedding_index": 16791}, {"title": "On the Power of Learning-Augmented Search Trees", "link_suffix": "/forum?id=FUwWdUi55e", "link": "https://openreview.net/forum?id=FUwWdUi55e", "pdf_link": "https://openreview.net/pdf?id=FUwWdUi55e", "keywords": "learning-augmented; binary search tree; algorithm with predictions", "abstract": "We study learning-augmented binary search trees (BSTs) via Treaps with carefully designed priorities.\nThe result is a simple search tree in which the depth of each item $x$ is determined by its predicted weight $w_x$.\nSpecifically, each item $x$ is assigned a composite priority of $-\\lfloor\\log\\log(1/w_x)\\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform random variable. By choosing $w_x$ as the relative frequency of $x$, the resulting search trees achieve static optimality.\nThis approach generalizes the recent learning-augmented BSTs [Lin-Luo-Woodruff ICML`22], which only work for Zipfian distributions, by extending them to arbitrary input distributions.\nFurthermore, we demonstrate that our method can be generalized to a B-Tree data structure using the B-Treap approach [Golovin ICALP'09]. Our search trees are also capable of leveraging localities in the access sequence through online self-reorganization, thereby achieving the working-set property. Additionally, they are robust to prediction errors and support dynamic operations, such as insertions, deletions, and prediction updates. We complement our analysis with an empirical study, demonstrating that our method outperforms prior work and classic data structures.", "title_embedding_index": 16767, "title_abs_embedding_index": 16792}, {"title": "Decoding Game: On Minimax Optimality of Heuristic Text Generation Strategies", "link_suffix": "/forum?id=Wfw4ypsgRZ", "link": "https://openreview.net/forum?id=Wfw4ypsgRZ", "pdf_link": "https://openreview.net/pdf?id=Wfw4ypsgRZ", "keywords": "decoding strategy, text generation, sampling, optimality, game theory, robust optimization, implicit regularization, sparsity", "abstract": "Decoding strategies play a pivotal role in text generation for modern language models, yet a puzzling gap divides theory and practice. Surprisingly, strategies that should intuitively be optimal, such as Maximum a Posteriori (MAP), often perform poorly in practice. Meanwhile, popular heuristic approaches like Top-$k$ and Nucleus sampling, which employ truncation and normalization of the conditional next-token probabilities, have achieved great empirical success but lack theoretical justifications. In this paper, we propose Decoding Game, a comprehensive theoretical framework which reimagines text generation as a two-player zero-sum game between Strategist, who seeks to produce text credible in the true distribution, and Nature, who distorts the true distribution adversarially. After discussing the decomposibility of multi-step generation, we derive the optimal strategy in closed form for one-step Decoding Game. It is shown that the adversarial Nature imposes an implicit regularization on likelihood maximization, and truncation-normalization methods are first-order approximations to the optimal strategy under this regularization. Additionally, by generalizing the objective and parameters of Decoding Game, near-optimal strategies encompass diverse methods such as greedy search, temperature scaling, and hybrids thereof. Numerical experiments are conducted to complement our theoretical analysis.", "title_embedding_index": 16768, "title_abs_embedding_index": 16793}, {"title": "Inertial Confinement Fusion Forecasting via Large Language Models", "link_suffix": "/forum?id=JQrBYfD2gg", "link": "https://openreview.net/forum?id=JQrBYfD2gg", "pdf_link": "https://openreview.net/pdf?id=JQrBYfD2gg", "keywords": "AI for Science, Inertial Confinement Fusion", "abstract": "Controlled fusion energy is deemed pivotal for the advancement of human civilization. In this study, we introduce $\\textbf{LPI-LLM}$, a novel integration of Large Language Models (LLMs) with classical reservoir computing paradigms tailored to address a critical challenge, Laser-Plasma Instabilities ($\\texttt{LPI}$), in Inertial Confinement Fusion ($\\texttt{ICF}$). Our approach offers several key contributions: Firstly, we propose the $\\textit{LLM-anchored Reservoir}$, augmented with a $\\textit{Fusion-specific Prompt}$, enabling accurate forecasting of $\\texttt{LPI}$-generated-hot electron dynamics during implosion. Secondly, we develop $\\textit{Signal-Digesting Channels}$ to temporally and spatially describe the driver laser intensity across time, capturing the unique characteristics of $\\texttt{ICF}$ inputs. Lastly, we design the $\\textit{Confidence Scanner}$ to quantify the confidence level in forecasting, providing valuable insights for domain experts to design the $\\texttt{ICF}$ process. Extensive experiments demonstrate the superior performance of our method, achieving 1.90 CAE, 0.14 $\\texttt{top-1}$ MAE, and 0.11 $\\texttt{top-5}$ MAE in predicting Hard X-ray ($\\texttt{HXR}$) energies emitted by the hot electrons in $\\texttt{ICF}$ implosions, which presents state-of-the-art comparisons against concurrent best systems.  Additionally, we present $\\textbf{LPI4AI}$, the first $\\texttt{LPI}$ benchmark based on physical experiments, aimed at fostering novel ideas in $\\texttt{LPI}$ research and enhancing the utility of LLMs in scientific exploration. Overall, our work strives to forge an innovative synergy between AI and $\\texttt{ICF}$ for advancing fusion energy.", "title_embedding_index": 16769, "title_abs_embedding_index": 16794}, {"title": "Autoencoder and Classifier based Joint-Guided Completion for Partial Multi-Modal Hashing", "link_suffix": "/forum?id=oGrGnPndHw", "link": "https://openreview.net/forum?id=oGrGnPndHw", "pdf_link": "https://openreview.net/pdf?id=oGrGnPndHw", "keywords": "partial multi-modal hashing, autoencoder, classifier, Joint-Guided Completion", "abstract": "The Multi-Modal Hashing (MMH) method based on complete modalities cannot effectively handle incomplete multi-modal samples, thus requiring the completion of missing modalities. Existing completion methods typically use complete modality samples with the same label to generate completion information. On one hand, they cannot fully utilize the different information between samples with different labels; on the other hand, they cannot effectively extract the global structural information of multi-modal samples. Therefore, we propose the autoencoder and classifier based joint-guided completion for partial multi-modal hashing (JCPMH) method that integrates autoencoders and classifiers. First, to fully utilize the different information between samples with different labels, we design a multi-modal classification module composed of multiple classifiers to learn different information. Second, we concatenate the multi-modal data into a whole and extract cross-modal global structural information through an autoencoder. Finally, based on the hashing module, multi-modal classification module and autoencoder module, we design a loss function to guide the generator to generate more accurate completion information for learning hash codes. JCPMH can utilize partial multi-modal samples for offline training and handle incomplete multi-modal samples during online retrieval. Additionally, we conducted extensive experiments to demonstrate the effectiveness of this model.", "title_embedding_index": 16770, "title_abs_embedding_index": 16795}, {"title": "PLLaVA: Parameter-efficient LLaVA Extension from Image to Video Understanding", "link_suffix": "/forum?id=Rs8fLyaOer", "link": "https://openreview.net/forum?id=Rs8fLyaOer", "pdf_link": "https://openreview.net/pdf?id=Rs8fLyaOer", "keywords": "Video Understanding;Parameter-efficient;Pooling", "abstract": "Vision-language pre-training has significantly elevated performance across a wide range of image-language applications. Yet, the pre-training process for video-related tasks demands exceptionally large computational and data resources, which hinders the progress of video-language models. This paper investigates a straightforward, highly efficient, and resource-light approach to adapting an existing image-language pre-trained model for dense video understanding. Our preliminary experiments reveal that directly fine-tuning pre-trained image-language models with multiple frames as inputs on video datasets leads to performance saturation or even a drop. Our further investigation reveals that it is largely attributed to the bias of learned high-norm visual features.  Motivated by this finding, we propose a simple but effective pooling strategy to smooth the feature distribution along the temporal dimension and thus reduce the dominant impacts from the extreme features. The new model is termed Pooling LLaVA, or PLLaVA in short.  PLLaVA achieves new state-of-the-art performance on modern benchmark datasets for both video question-answer and captioning tasks. Notably, on the recent popular Video ChatGPT benchmark, PLLaVA achieves a score of 3.25 out of 5 on average of five evaluated dimensions. On the latest multi-choice benchmark MVBench, PLLaVA achieves 58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V (IG-VLM).", "title_embedding_index": 16771, "title_abs_embedding_index": 16796}, {"title": "Orthogonal Deep Neural Networks (ODNN): Uncovering Hidden Physics in Partially Observable Systems", "link_suffix": "/forum?id=ZujMVRn7Md", "link": "https://openreview.net/forum?id=ZujMVRn7Md", "pdf_link": "https://openreview.net/pdf?id=ZujMVRn7Md", "keywords": "representation learning, physics identification, orthogonality", "abstract": "Accurately identifying the underlying physical laws in complex systems is vital for effective control and interpretation. However, many systems are governed by a combination of known physical principles and unobservable or poorly understood components. Traditional model-based methods like Kalman filters and state-space models often rely on oversimplified assumptions, while modern data-driven approaches, such as physics-informed neural networks (PINNs), can suffer from overfitting or lack theoretical guarantees in recovering true physical dynamics. We propose the Orthogonal Deep Neural Network (ODNN) architecture to address these limitations. ODNN disentangles known physical components from unobservable or poorly understood components by imposing orthogonal constraints on the deep neural network. Unlike additive regularization methods, ODNN converts the physical constraints directly into the network structure, ensuring that the DNN focuses on capturing the unknown or complex dynamics without overfitting. This novel approach leverages both explicit orthogonality (e.g., zero inner product) and implicit orthogonality (e.g., contrasting convexity, periodicity, or symmetry) between physical laws and unknown components. Theoretically, we prove that ODNN provides strong guarantees for accurate system identification under mild orthogonality assumptions, building on the universal approximation theorem. Empirically, ODNN is evaluated across eight synthetic and real-world datasets, showcasing its ability to recover governing physical equations with high accuracy and interpretability. Our results demonstrate that ODNN offers significant advantages in terms of generalizability and robustness, making it a valuable framework for physics-based model identification in complex systems.", "title_embedding_index": 16772, "title_abs_embedding_index": 16797}, {"title": "Tailor3D: Customized 3D Assets Editing and Generation with Dual-Side Images", "link_suffix": "/forum?id=SJqoP0rp8w", "link": "https://openreview.net/forum?id=SJqoP0rp8w", "pdf_link": "https://openreview.net/pdf?id=SJqoP0rp8w", "keywords": "3D Object Editing, 3D Generative Models", "abstract": "Recent advances in 3D AIGC have shown promise in directly creating 3D objects from text and images, offering significant cost savings in animation and product design. However, detailed edit and customization of 3D assets remains a long-standing challenge. Specifically, 3D Generation methods lack the ability to follow finely detailed instructions as precisely as their 2D image creation counterparts. Imagine you can get a toy through 3D AIGC but with undesired accessories and dressing. To tackle this challenge, we propose a novel pipeline called Tailor3D, which swiftly creates customized 3D assets from editable dual-side images. We aim to emulate a tailor's ability to locally change objects or perform overall style transfer. Unlike creating 3D assets from multiple views, using dual-side images eliminates conflicts on overlapping areas that occur when editing individual views. Specifically, it begins by editing the front view, then generates the back view of the object through multi-view diffusion. Afterward, it proceeds to edit the back views. Finally, a Dual-sided LRM is proposed to seamlessly stitch together the front and back 3D features, akin to a tailor sewing together the front and back of a garment. The Dual-sided LRM rectifies imperfect consistencies between the front and back views, enhancing editing capabilities and reducing memory burdens while seamlessly integrating them into a unified 3D representation with the LoRA Triplane Transformer. Experimental results demonstrate Tailor3D's effectiveness across various 3D generation and editing tasks, including 3D generative fill and style transfer. It provides a user-friendly, efficient solution for editing 3D assets, with each editing step taking only seconds to complete.", "title_embedding_index": 16773, "title_abs_embedding_index": 16798}, {"title": "Reinforcement Learning with Action Sequence for Data-Efficient Robot Learning", "link_suffix": "/forum?id=FXm7EEFRa8", "link": "https://openreview.net/forum?id=FXm7EEFRa8", "pdf_link": "https://openreview.net/pdf?id=FXm7EEFRa8", "keywords": "Reinforcement Learning, Robot Learning, Robotics, Data-Efficiency", "abstract": "Training reinforcement learning (RL) agents on robotic tasks typically requires a large number of training samples. This is because training data often consists of noisy trajectories, whether from exploration or human-collected demonstrations, making it difficult to learn value functions that understand the effect of taking each action. On the other hand, recent behavior-cloning (BC) approaches have shown that predicting a sequence of actions enables policies to effectively approximate noisy, multi-modal distributions of expert demonstrations. Can we use a similar idea for improving RL on robotic tasks? In this paper, we introduce a novel RL algorithm that learns a critic network that outputs Q-values over a sequence of actions. By explicitly training the value functions to learn the consequence of executing a series of current and future actions, our algorithm allows for learning useful value functions from noisy trajectories. We study our algorithm across various setups with sparse and dense rewards, and with or without demonstrations, spanning mobile bi-manual manipulation, whole-body control, and tabletop manipulation tasks from BiGym, HumanoidBench, and RLBench. We find that, by learning the critic network with action sequences, our algorithm outperforms various RL and BC baselines, in particular on challenging humanoid control tasks.", "title_embedding_index": 16774, "title_abs_embedding_index": 16799}]