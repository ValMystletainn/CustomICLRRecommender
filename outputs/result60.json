[
    {
        "title": "Matrix Product Sketching via Coordinated Sampling",
        "link_suffix": "/forum?id=eHfq8Q3LeD",
        "link": "https://openreview.net/forum?id=eHfq8Q3LeD",
        "pdf_link": "https://openreview.net/pdf?id=eHfq8Q3LeD",
        "keywords": "Sketching Algorithm, Matrix Multiplication, Model Compression, Data Discovery, Efficient resource",
        "abstract": "We revisit the well-studied problem of approximating a matrix product, $\\bv{A}^T\\bv{B}$, based on small space sketches $\\mathcal{S}(\\bv{A})$ and  $\\mathcal{S}(\\bv{B})$ of $\\bv{A} \\in \\R^{n \\times d}$ and $\\bv{B}\\in \\R^{n \\times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when $\\bv{A}$ and $\\bv{B}$ are sparse, methods based on \\emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm error $\\epsilon|\\bv{A}|_F|\\bv{B}|_F$, coordinated sampling requires sketches of size $O(s/\\epsilon^2)$ when $\\bv{A}$ and $\\bv{B}$ have at most $s \\leq d,m$ non-zeros per row. In contrast, linear sketching leads to sketches of size $O(d/\\epsilon^2)$ and $O(m/\\epsilon^2)$ for $\\bv{A}$ and $\\bv{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching."
    },
    {
        "title": "A Phase Transition Induces Catastrophic Overfitting in Adversarial Training",
        "link_suffix": "/forum?id=9dBBq2ehY5",
        "link": "https://openreview.net/forum?id=9dBBq2ehY5",
        "pdf_link": "https://openreview.net/pdf?id=9dBBq2ehY5",
        "keywords": "Adversarial Training, FGSM, Catastrophic Overfitting",
        "abstract": "We derive the implicit bias of Projected Gradient Descent (PGD) Adversarial Training (AT). We show that a phase transition in the loss structure of as a function of the adversarial budget $\\epsilon$ manifests as Catastrophic Overfitting (CO). Below a critical threshold $\\epsilon_c$, single step methods efficiently provide an increase in robustness, while above this critical point, additional PGD steps and/or regularization are needed. We show that high curvature solutions arise in the implicit bias of PGD AT. \nWe provide analytical and empirical evidence for our arguments by appealing to a simple model with one-dimensional inputs and a single trainable parameter, where the CO phenomenon can be replicated. In this model, we show that such high curvature solutions exist for arbitrarily small $\\epsilon$. Additionally, we can compute the critical value $\\epsilon_c$ in single-step AT for bounded parameter norms. We believe our work provides a deeper understanding of CO that aligns with the intuition the community has built around it."
    },
    {
        "title": "On the Robustness of Vision-Language Models Against Distractions",
        "link_suffix": "/forum?id=EGjTCIcSnW",
        "link": "https://openreview.net/forum?id=EGjTCIcSnW",
        "pdf_link": "https://openreview.net/pdf?id=EGjTCIcSnW",
        "keywords": "Model Evaluation, Vision-Language Models, Multimodal, Distraction Robustness",
        "abstract": "Although vision-language models (VLMs) have achieved significant success in various applications such as visual question answering, their resilience to prompt distractions remains as an under-explored area. Understanding how distractions affect VLMs is crucial for improving their real-world applicability, as inputs could be filled with noisy and irrelevant information in many practical scenarios. This paper aims to assess the robustness of VLMs against both visual and textual distractions in the context of science question answering. Built on the \\emph{ScienceQA} dataset, we developed a new benchmark that introduces distractions in both the visual and textual contexts. To evaluate the reasoning capacity of VLMs amidst these distractions, we analyzed the performance of ten state-of-the-art models, including GPT-4o. Our findings reveal that most VLMs are vulnerable to various types of distractions, experiencing noticeable degradation in reasoning capabilities when confronted with distractions. Notably, models such as InternVL2 demonstrates a higher degree of robustness to these distractions. We also found that models exhibit greater sensitivity to textual distractions than visual ones. Additionally, we explored various mitigation strategies, such as prompt engineering, to counteract the impact of distractions. While these strategies improved model resilience, our analysis shows that there remain significant opportunities for further improvement."
    },
    {
        "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze",
        "link_suffix": "/forum?id=Wnu2c6pjs1",
        "link": "https://openreview.net/forum?id=Wnu2c6pjs1",
        "pdf_link": "https://openreview.net/pdf?id=Wnu2c6pjs1",
        "keywords": "LVLM, Eye Gaze, Video, Medicine, Medical Image, Chest X-ray, Chest X-ray Report Generation",
        "abstract": "Large Vision-Language Models (LVLMs) have demonstrated promising performance in chest X-ray (CXR) analysis. To enhance human-computer interaction, several studies have incorporated radiologists' eye gaze, typically through heatmaps or textual prompts. However, these methods often overlook the sequential order of eye movements, which could provide valuable insights by highlighting both the areas of interest and the order in which they are examined. In this work, we propose a novel approach called RadEyeVideo that integrates radiologists’ eye-fixation data as a video sequence, capturing both the temporal and spatial dynamics of their gaze. The video, featuring a red gaze point overlaid on CXR images, emphasizes regions of focused attention during interpretation. We evaluate this method in CXR report generation and disease diagnosis using three general-domain, open-source LVLMs with a video input capabilities. When prompted with eye-gaze videos, model performance improves by up to 25.4% on Impression generation task and on average 7.9% for all tasks using scaled evaluation metrics. Our approach enhanced open-domain LVLM models, when combined with exemplar reports for in-context learning, outperform medical models as well as those specifically trained for CXR report generation on the benchmark dataset. This work highlights that domain expert's knowledge (eye-gaze information in this case), when effectively integrated with LVLMs, can significantly enhance general-domain models' capabilities in clinical tasks, pointing out a new effective approach of utilising LVLMs in healthcare and beyond."
    },
    {
        "title": "DUET: Decentralized Bilevel Optimization without Lower-Level Strong Convexity",
        "link_suffix": "/forum?id=jxMAPMqNr5",
        "link": "https://openreview.net/forum?id=jxMAPMqNr5",
        "pdf_link": "https://openreview.net/pdf?id=jxMAPMqNr5",
        "keywords": "Decentralized optimization, Bilevel optimization, Convex optimization, Heterogeneous data distributions, Multi agent learning",
        "abstract": "Decentralized bilevel optimization (DBO) provides a powerful framework for multi-agent systems to solve local bilevel tasks in a decentralized fashion without the need for a central server. \nHowever, most existing DBO methods rely on lower-level strong convexity (LLSC) to guarantee unique solutions and and a well-defined hypergradient for stationarity measure, hindering their applicability in many practical scenarios not satisfying LLSC. \nTo overcome this limitation, we introduce a new single-loop DBO algorithm called diminishing quadratically-regularized bilevel decentralized optimization (DUET), which eliminates the need for LLSC by introducing a diminishing quadratic regularization to the lower-level (LL) objective. \nWe show that DUET achieves an iteration complexity of  $O(1/T^{1-5p-\\frac{11}{4}\\tau})$ for approximate KKT-stationary point convergence under relaxed assumptions, where $p$ and $\\tau $ are control parameters for LL learning rate and averaging, respectively.\nIn addition, our DUET algorithm incorporates gradient tracking to address data heterogeneity, a key challenge in DBO settings. \nTo the best of our knowledge, this is the first work to tackle DBO without LLSC under decentralized settings with data heterogeneity.\nNumerical experiments validate the theoretical findings and demonstrate the practical effectiveness of our proposed algorithms."
    },
    {
        "title": "Preserving Deep Representations in One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework",
        "link_suffix": "/forum?id=eNQp79A5Oz",
        "link": "https://openreview.net/forum?id=eNQp79A5Oz",
        "pdf_link": "https://openreview.net/pdf?id=eNQp79A5Oz",
        "keywords": "Neural Network Pruning, Structured Pruning, Optimization, Hessian-free Optimization",
        "abstract": "We present SNOWS, a one-shot post-training pruning framework aimed at reducing neural network inference cost without requiring retraining. Current leading one-shot pruning methods minimize layer-wise least squares reconstruction error which does not take into account deeper network representations. We propose to optimize a more global reconstruction objective that accounts for nonlinear activations deep in the network to obtain a better proxy for the network loss. Optimizing this nonlinear objective leads to a more challenging optimization problem---we demonstrate that it can be solved efficiently using a specialized second-order optimization framework. A key innovation of our framework is the use of Hessian-free optimization to compute exact Newton descent steps without needing to compute or store the full Hessian matrix. A distinct advantage of SNOWS is that it can be readily applied on top of any sparse mask derived from prior methods, readjusting their weights to exploit nonlinearities in deep feature representations. SNOWS obtains state-of-the-art results on various one-shot pruning benchmarks including residual networks and Vision Transformers (VIT/B-16 and VIT/L-16, 86m and 304m parameters respectively)."
    },
    {
        "title": "Characterizing Context Influence and Hallucination in Summarization",
        "link_suffix": "/forum?id=IZuwA3hken",
        "link": "https://openreview.net/forum?id=IZuwA3hken",
        "pdf_link": "https://openreview.net/pdf?id=IZuwA3hken",
        "keywords": "Influence, Hallucinations, Language Models",
        "abstract": "Although Large Language Models (LLMs) have achieved remarkable performance in numerous downstream tasks, their ubiquity has raised two significant concerns. One is that LLMs can hallucinate by generating content that contradicts relevant contextual information; the other is that LLMs can inadvertently leak private information due to input regurgitation. Many prior works have extensively studied each concern independently, but none have investigated them simultaneously. Furthermore, auditing the influence of provided context during open-ended generation with a privacy emphasis is understudied. To this end, we comprehensively characterize the influence and hallucination of contextual information during summarization. We introduce a definition for context influence and Context-Influence Decoding (CID), and then we show that amplifying the context (by factoring out prior knowledge) and the context being out of distribution with respect to prior knowledge increases the context's influence on an LLM. Moreover, we show that context influence gives a lower bound of the private information leakage of CID. We corroborate our analytical findings with experimental evaluations that show improving the F1 ROGUE-L score on CNN-DM for LLaMA 3 by $\\textbf{10}$% over regular decoding also leads to $\\textbf{1.5x}$ more influence by the context. Moreover, we empirically evaluate how context influence and hallucination are affected by (1) model capacity, (2) context size, (3) the length of the current response, and (4) different token $n$-grams of the context."
    },
    {
        "title": "Capturing the Temporal Dependence of Training Data Influence",
        "link_suffix": "/forum?id=uHLgDEgiS5",
        "link": "https://openreview.net/forum?id=uHLgDEgiS5",
        "pdf_link": "https://openreview.net/pdf?id=uHLgDEgiS5",
        "keywords": "data valuation",
        "abstract": "Traditional data influence estimation methods, like influence function, assume that learning algorithms are permutation-invariant with respect to training data. However, modern training paradigms—especially for foundation models using stochastic algorithms and non-convergent, multi-stage curricula—are sensitive to data ordering, thus violating this assumption. This mismatch renders influence functions inadequate for answering some critical questions in current machine learning: How can we differentiate the influence of the same data contributing at different stages of training? More generally, how can we capture the dependence of data influence on the optimization trajectory during training? To address this gap, we formalize the concept of \\emph{trajectory-specific leave-one-out (LOO) error}, which quantifies the impact of removing a data point from a specific iteration during training, accounting for the exact sequence of data encountered and the model's optimization trajectory. However, exactly evaluating the trajectory-specific LOO presents a significant computational challenge. To address this, we propose \\emph{data value embedding}, a novel technique enabling efficient approximation of trajectory-specific LOO. Specifically, we compute a training data embedding that encapsulates the cumulative interactions between data and the evolving model parameters. The LOO can then be efficiently approximated through a simple dot-product between the data value embedding and the gradient of the given test data. As data value embedding captures training data ordering, it offers valuable insights into model training dynamics. In particular, we uncover distinct phases of data influence, revealing that data points in the early and late stages of training exert a greater impact on the final model. These insights translate into actionable strategies for managing the computational overhead of data selection by strategically timing the selection process, potentially opening new avenues in data curation research."
    },
    {
        "title": "FEABench: Evaluating Language Models on Real World Physics Reasoning Ability",
        "link_suffix": "/forum?id=hDkLpu1E64",
        "link": "https://openreview.net/forum?id=hDkLpu1E64",
        "pdf_link": "https://openreview.net/pdf?id=hDkLpu1E64",
        "keywords": "numerical analysis, finite element, benchmark, agents",
        "abstract": "Building precise simulations of the real world and invoking numerical solvers to answer quantitative problems is an essential requirement in engineering and science. We present FEABench, a benchmark to evaluate the ability of large language models (LLMs) and LLM agents to simulate and solve physics, mathematics and engineering problems using finite element analysis (FEA). We introduce a multipronged evaluation scheme to investigate the ability of LLMs to solve these problems by reasoning over natural language problem descriptions and operating COMSOL Multiphysics$^\\textregistered$, an FEA software, to compute the answers. In addition to testing state-of-the art-LLMs, we further design a language model agent equipped with the ability to interact with the software through its Application Programming Interface (API), examine its outputs and use tools to improve its solutions over multiple iterations. Our best performing strategy generates executable API calls 88% of the time. However, this benchmark still proves to be challenging enough that the LLMs and agents we tested were not able to completely and correctly solve any problem. LLMs that can successfully interact with and operate FEA software to solve problems such as those in our benchmark would significantly push the frontiers of their utility. Acquiring this capability would augment LLMs' reasoning skills with the precision of numerical solvers and advance the development of autonomous systems that can tackle complex problems in the real world."
    },
    {
        "title": "Learning from Preferences and Mixed Demonstrations in General Settings",
        "link_suffix": "/forum?id=Sfct4aXXcw",
        "link": "https://openreview.net/forum?id=Sfct4aXXcw",
        "pdf_link": "https://openreview.net/pdf?id=Sfct4aXXcw",
        "keywords": "reinforcement learning, rl, human feedback, rlhf, modelling, preferences, demonstrations, rankings, machine learning, reward learning",
        "abstract": "Reinforcement learning is a general method for learning in sequential settings, but it can often be difficult to specify a good reward function when the task is complex. In these cases, preference feedback or expert demonstrations can be used instead. However, existing approaches utilising both together are either ad-hoc or rely on domain-specific properties. Building upon previous work, we develop a novel theoretical framework for learning from human data. Based on this we introduce LEOPARD: Learning Estimated Objectives from Preferences And Ranked Demonstrations. LEOPARD can simultaneously learn from a broad range of data, including negative/failed demonstrations, to effectively learn reward functions in general domains. We find that when a limited amount of human feedback is available, LEOPARD outperforms the current standard practice of pre-training on demonstrations and finetuning on preferences. Furthermore, we show that LEOPARD learns faster when given many types of feedback, rather than just a single one."
    },
    {
        "title": "Wasserstein Distances, Neuronal Entanglement, and Sparsity",
        "link_suffix": "/forum?id=cnKhHxN3xj",
        "link": "https://openreview.net/forum?id=cnKhHxN3xj",
        "pdf_link": "https://openreview.net/pdf?id=cnKhHxN3xj",
        "keywords": "Polysemanticity, Disentanglement, Wasserstein Distance, Sparsity, Large Language Models",
        "abstract": "Disentangling polysemantic neurons is at the core of many current approaches to interpretability of large language models. Here we attempt to study how disentanglement can be used to understand performance, in particular under weight sparsity, one of today's leading post-training optimization techniques. We suggest a novel measure for estimating neuronal entanglement: the Wasserstein distance of a neuron’s output distribution to a Gaussian. Moreover, we show the existence of a small number of highly entangled \"Wasserstein Neurons\" in each linear layer of an LLM, characterized by their highly non-Gaussian output distributions and their significant impact on model accuracy. To study this phenomena, we propose a new experimental framework for disentangling polysemantic neurons. Our framework separates each layer’s inputs to create a mixture of experts where each neuron's output is computed by a mixture of neurons of lower Wasserstein distance, each better at maintaining accuracy when sparsified without retraining. We provide strong evidence that this is because the mixture of sparse experts is effectively disentangling the input-output relationship of every individual neuron, in particular the difficult Wasserstein neurons."
    },
    {
        "title": "Unstable Unlearning: The Hidden Risk of Concept Resurgence in Diffusion Models",
        "link_suffix": "/forum?id=0OB3RVmTXE",
        "link": "https://openreview.net/forum?id=0OB3RVmTXE",
        "pdf_link": "https://openreview.net/pdf?id=0OB3RVmTXE",
        "keywords": "machine unlearning, concept unlearning, evaluation, diffusion models, text to image",
        "abstract": "Text-to-image diffusion models rely on massive, web-scale datasets. Training them from scratch is computationally expensive, and as a result, developers often prefer to make incremental updates to existing models. These updates often compose fine-tuning steps (to learn new concepts or improve model performance) with “unlearning” steps (to “forget” existing concepts, such as copyrighted data or the ability to generate explicit content). In this work, we demonstrate a critical and previously unknown vulnerability that arises in this paradigm: even under benign, non-adversarial conditions, fine-tuning a text-to-image diffusion model on seemingly unrelated images can cause it to “relearn” concepts that were previously “unlearned.” We comprehensively investigate the causes and scope of this phenomenon, which we term concept resurgence, by performing a series of experiments based on fine-tuning Stable Diffusion v1.4 alongside “mass concept erasure”, the current state of the art for unlearning in text-to-image diffusion models (Lu et al., 2024). Our findings underscore the fragility of composing incremental model updates, and raise new serious concerns about current approaches to ensuring the safety and alignment of text-to-image diffusion models."
    },
    {
        "title": "Chemistry-Inspired Diffusion with Non-Differentiable Guidance",
        "link_suffix": "/forum?id=4dAgG8ma3B",
        "link": "https://openreview.net/forum?id=4dAgG8ma3B",
        "pdf_link": "https://openreview.net/pdf?id=4dAgG8ma3B",
        "keywords": "guided diffusion, ai4science, molecule generation",
        "abstract": "Recent advances in diffusion models have shown remarkable potential in the conditional generation of novel molecules. These models can be guided in two ways: (i) explicitly, through additional features representing the condition, or (ii) implicitly, using a property predictor. However, training property predictors in conditional diffusion models requires an abundance of labeled data and is inherently challenging in real-world applications. We propose a novel approach that attenuates the limitations of acquiring large labeled datasets by leveraging domain knowledge from quantum chemistry as a non-differentiable oracle to guide an unconditional diffusion model. Instead of relying on neural networks, the oracle provides accurate guidance in the form of estimated gradients, allowing the diffusion process to sample from a conditional distribution specified by quantum chemistry. We show that this results in more precise conditional generation of novel and stable molecular structures. Our experiments demonstrate that our method: (1) significantly reduces atomic forces, enhancing the validity of generated molecules when used for stability optimization; (2) is compatible with both explicit and implicit guidance in diffusion models, enabling joint optimization of molecular properties and stability; and (3) generalizes effectively to molecular optimization tasks beyond stability optimization."
    },
    {
        "title": "A Simple Baseline for Predicting Future Events with Auto-Regressive Tabular Transformers",
        "link_suffix": "/forum?id=DNjHslZrqu",
        "link": "https://openreview.net/forum?id=DNjHslZrqu",
        "pdf_link": "https://openreview.net/pdf?id=DNjHslZrqu",
        "keywords": "Event Prediction, Tabular Data, Transformers",
        "abstract": "Many real-world applications of tabular data involve using historic events to predict properties of new ones, for example whether a credit card transaction is fraudulent or what rating a customer will assign a product on a retail platform.\nExisting approaches to event prediction include costly, brittle, and application-dependent techniques such as time-aware positional embeddings, learned row and field encodings, and oversampling methods for addressing class imbalance.\nMoreover, these approaches often assume specific use-cases, for example that we know the labels of all historic events or that we only predict a pre-specified label and not the data’s features themselves.\nIn this work, we propose a simple but flexible baseline using standard autoregressive LLM-style transformers with elementary positional embeddings and a causal language modeling objective.\nOur baseline outperforms existing approaches across popular datasets and can be employed for various use-cases.\nWe demonstrate that the same model can predict labels, impute missing values, or model event sequences."
    },
    {
        "title": "On the Convergence of Adam-Type Algorithms for Bilevel Optimization under Unbounded Smoothness",
        "link_suffix": "/forum?id=rIJbFQ1zII",
        "link": "https://openreview.net/forum?id=rIJbFQ1zII",
        "pdf_link": "https://openreview.net/pdf?id=rIJbFQ1zII",
        "keywords": "Bilevel Optimization, Adam, Unbounded Smoothness",
        "abstract": "Adam has become one of the most popular optimizers for training modern deep neural networks, such as transformers. However, its applicability is largely restricted to single-level optimization problems. In this paper, we aim to extend vanilla Adam to tackle bilevel optimization problems, which have important applications in machine learning, such as meta-learning. In particular, we study stochastic bilevel optimization problems where the lower-level function is strongly convex and the upper-level objective is nonconvex with potentially unbounded smoothness. This unbounded smooth objective function covers a broad class of neural networks, including transformers, which may exhibit non-Lipschitz gradients. In this work, we first introduce AdamBO, a single-loop Adam-type method that achieves $\\widetilde{O}(\\epsilon^{-4})$ oracle complexity to find $\\epsilon$-stationary points, where the oracle calls involve stochastic gradient or Hessian/Jacobian-vector product evaluations. The key to our analysis is a novel randomness decoupling lemma that provides refined control over the lower-level variable. Additionally, we propose VR-AdamBO, a variance-reduced version with an improved oracle complexity of $\\widetilde{O}(\\epsilon^{-3})$. The improved analysis is based on a novel stopping time approach and a careful treatment of the lower-level error. We conduct extensive experiments on various machine learning tasks involving bilevel formulations with recurrent neural networks (RNNs) and transformers, demonstrating the effectiveness of our proposed Adam-type algorithms."
    },
    {
        "title": "Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning",
        "link_suffix": "/forum?id=nDmwloEl3N",
        "link": "https://openreview.net/forum?id=nDmwloEl3N",
        "pdf_link": "https://openreview.net/pdf?id=nDmwloEl3N",
        "keywords": "Robotics, Imitation Learning",
        "abstract": "Diffusion Policies have become widely used in Imitation Learning, offering several appealing properties, such as generating multimodal and discontinuous behavior.\nAs models are becoming larger to capture more complex capabilities, their computational demands increase, as shown by recent scaling laws. \nTherefore, continuing with the current architectures will present a computational roadblock. \nTo address this gap, we propose Mixture-of-Denoising Experts (MoDE) as a novel policy for Imitation Learning.\nMoDE surpasses current state-of-the-art Transformer-based Diffusion Policies while enabling parameter-efficient scaling, reducing the inference cost significantly. \nTo achieve this, MoDE uses sparse experts combined with a novel routing strategy that conditions the expert selection on the current noise level of the denoising process.\nThis is combined with a noise-conditioned self-attention mechanism for further improvements.\nMoDE achieves state-of-the-art performance across 134 tasks in four established imitation learning benchmarks (CALVIN and LIBERO).\nIt surpasses both CNN-based and Transformer Diffusion Policies by an average of 20% in all settings, while using 40% fewer FLOPs and fewer active parameters.\nFurthermore, we conduct comprehensive ablations on MoDE's components, providing insights for designing efficient and scalable Transformer architectures for Diffusion Policies."
    },
    {
        "title": "No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data",
        "link_suffix": "/forum?id=hSZaCIznB2",
        "link": "https://openreview.net/forum?id=hSZaCIznB2",
        "pdf_link": "https://openreview.net/pdf?id=hSZaCIznB2",
        "keywords": "implicit neural representations, dataset, fairness in AI, representation learning, geospatial modeling, Earth representation, wavelet, location encoding",
        "abstract": "Implicit neural representations (INRs) exhibit growing promise in addressing Earth representation challenges, ranging from emissions monitoring to climate modeling. However, existing methods disproportionately prioritize global average performance, whereas practitioners require fine-grained insights to understand biases and variations in these models. To bridge this gap, we introduce FAIR-Earth: a first-of-its-kind dataset explicitly crafted to challenge and examine inequities in Earth representations. FAIR-Earth comprises various high-resolution Earth signals, and uniquely aggregates extensive metadata along stratifications like landmass size and population density to assess the fairness of models. Evaluating state-of-the-art INRs across the various modalities of FAIR-Earth, we uncover striking performance disparities. Certain subgroups, especially those associated with high-frequency signals (e.g., islands, coastlines), are consistently poorly modeled by existing methods. In response, we propose spherical wavelet encodings, building on previous spatial encoding research for INRs. Leveraging the multi-resolution analysis capabilities of wavelets, our encodings yield more consistent performance over various scales and locations, offering more accurate and robust representations of the biased subgroups. These open-source contributions represent a crucial step towards facilitating the equitable assessment and deployment of implicit Earth representations."
    },
    {
        "title": "Lie Group-Induced Dynamics in Score-Based Generative Modeling",
        "link_suffix": "/forum?id=IARgA4HqjJ",
        "link": "https://openreview.net/forum?id=IARgA4HqjJ",
        "pdf_link": "https://openreview.net/pdf?id=IARgA4HqjJ",
        "keywords": "Score-matching, Generative modeling, Denoising Diffusion models, Lie groups, Lie algebras, Molecular Conformer Generation, Deep Learning",
        "abstract": "We extend score-based generative modeling by incorporating Lie group actions on the data manifold into the denoising diffusion process. Our approach yields a Langevin dynamics whose infinitesimal transformations decompose as a direct sum of Lie algebra representations, enabling generative processes that align with the underlying symmetry properties of the data. Unlike equivariant models, which restrict the space of learnable functions by quotienting out group orbits, our method incorporates both global and local symmetries and can model any target distribution. Standard score-matching, which minimizes the Fisher divergence, emerges as a special case of our framework when the Lie group is the translation group in Euclidean space. We prove that our generalized generative processes arise as solutions to a new class of reverse-time stochastic differential equations (SDEs), introduced here for the first time. We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as SO(3)-guided molecular conformer generation and modeling ligand-specific global SE(3) transformations for molecular docking. We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions, lifting the requirement of a Gaussian prior. Additionally, we demonstrate the universality of our approach by deriving how it extends to flow matching techniques."
    },
    {
        "title": "InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma",
        "link_suffix": "/forum?id=2TasVD7FXp",
        "link": "https://openreview.net/forum?id=2TasVD7FXp",
        "pdf_link": "https://openreview.net/pdf?id=2TasVD7FXp",
        "keywords": "multi-agent reinforcement learning, climate change, ai for climate",
        "abstract": "InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. The benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs."
    },
    {
        "title": "Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control",
        "link_suffix": "/forum?id=1X1R7P6yzt",
        "link": "https://openreview.net/forum?id=1X1R7P6yzt",
        "pdf_link": "https://openreview.net/pdf?id=1X1R7P6yzt",
        "keywords": "control barrier functions, multi-agent systems, black-box systems, partial observability, reinforcement learning",
        "abstract": "Control policies that can achieve high task performance and satisfy safety constraints are desirable for any system, including multi-agent systems (MAS). One promising technique for ensuring the safety of MAS is distributed control barrier functions (CBF). However, it is difficult to design distributed CBF-based policies for MAS that can tackle unknown discrete-time dynamics, partial observability, changing neighborhoods, and input constraints, especially when a distributed high-performance nominal policy that can achieve the task is unavailable. To tackle these challenges, we proposeDGPPO, a new framework thatsimultaneouslylearns both adiscretegraph CBF which handles neighborhood changes and input constraints, and a distributed high-performance safe policy for MAS with unknown discrete-time dynamics.\nWe empirically validate our claims on a suite of multi-agent tasks spanning three different simulation engines. The results suggest that, compared with existing methods, our DGPPO framework obtains policies that achieve high task performance (matching baselines that ignore the safety constraints), and high safety rates (matching the most conservative baselines), with aconstantset of hyperparameters across all environments."
    },
    {
        "title": "Aligning Language Models with Demonstrated Feedback",
        "link_suffix": "/forum?id=1qGkuxI9UX",
        "link": "https://openreview.net/forum?id=1qGkuxI9UX",
        "pdf_link": "https://openreview.net/pdf?id=1qGkuxI9UX",
        "keywords": "personalization, few-shot learning, human computer interaction, alignment",
        "abstract": "Language models are aligned to emulate the collective voice of many, resulting in outputs that align with no one in particular. Steering LLMs away from generic output is possible through supervised finetuning or RLHF, but requires prohibitively large datasets for new ad-hoc tasks. We argue that it is instead possible to align an LLM to a specific setting by leveraging a very small number ($<10$) of demonstrations as feedback. Our method, Demonstration ITerated Task Optimization (DITTO), directly aligns language model outputs to a user's demonstrated behaviors. Derived using ideas from online imitation learning, DITTO cheaply generates online comparison data by treating users' demonstrations as preferred over output from the LLM and its intermediate checkpoints. We evaluate DITTO's ability to learn fine-grained style and task alignment across domains such as news articles, emails, and blog posts. Additionally, we conduct a user study soliciting a range of demonstrations from participants ($N=16$). Across our benchmarks and user study, we find that win-rates for DITTO outperform few-shot prompting, supervised fine-tuning, and other self-play methods by an average of 19% points. By using demonstrations as feedback directly, DITTO offers a novel method for effective customization of LLMs."
    },
    {
        "title": "Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle",
        "link_suffix": "/forum?id=IGuLzOXTB9",
        "link": "https://openreview.net/forum?id=IGuLzOXTB9",
        "pdf_link": "https://openreview.net/pdf?id=IGuLzOXTB9",
        "keywords": "LLM Forecasting, Continuous Evaluation, Temporal Generalization",
        "abstract": "Existing evaluation benchmarks of Large Language Models (LLMs) can become outdated due to continuous model updates and the evolving information landscape. This presents a significant challenge: How can we effectively evaluate LLMs in a way that remains relevant over time? To address this, we explore the potential of future event prediction as a continuous evaluation for LLMs, assessing their ability to make predictions about real-world events and exhibit temporal generalization. Towards this goal, we propose a continuous LLM evaluation using daily news. We automatically generate question-answer (QA) pairs from daily news, constructing our Daily Oracle dataset, which challenges LLMs to predict \"future\" events based on its pre-training data. Our findings show that as pre-training data becomes outdated, LLMs exhibit performance degradation over time. While the Retrieval Augmented Generation (RAG) technique can enhance prediction accuracy, the performance degradation pattern still exists, underscoring the necessity for ongoing model updates."
    },
    {
        "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression",
        "link_suffix": "/forum?id=cnecLUNs6w",
        "link": "https://openreview.net/forum?id=cnecLUNs6w",
        "pdf_link": "https://openreview.net/pdf?id=cnecLUNs6w",
        "keywords": "in-context learning, transformers, hijacking attacks, linear regression, linear transformers, transfer of adversarial attacks",
        "abstract": "Transformers have demonstrated remarkable in-context learning capabilities across various domains, including statistical learning tasks. While previous work has shown that transformers can implement common learning algorithms, the adversarial robustness of these learned algorithms remains unexplored. This work investigates the vulnerability of in-context learning in transformers tohijacking attacksfocusing on the setting of linear regression tasks. Hijacking attacks are prompt-manipulation attacks in which the adversary's goal is to manipulate the prompt to force the transformer to generate a specific output. We first prove that single-layer linear transformers, known to implement gradient descent in-context, are non-robust and can be manipulated to output arbitrary predictions by perturbing\na single example in the in-context training set. While our experiments show these attacks succeed on linear transformers, we find they do not transfer to more complex transformers with GPT-2 architectures. Nonetheless, we show that these transformers can be hijacked using gradient-based adversarial attacks. We then demonstrate that adversarial training enhances transformers' robustness against hijacking attacks, even when just applied during finetuning.  Additionally, we find that in some settings, adversarial training against a weaker attack model can lead to robustness to a stronger attack model.  Lastly, we find that hijacking attacks against one transformer can only transfer to other transformers when they are small-scale, while attacks against larger transformers do not transfer even against transformers of the same architecture but trained with different random seeds."
    },
    {
        "title": "See It from My Perspective: How Language Affects Cultural Bias in Image Understanding",
        "link_suffix": "/forum?id=Xbl6t6zxZs",
        "link": "https://openreview.net/forum?id=Xbl6t6zxZs",
        "pdf_link": "https://openreview.net/pdf?id=Xbl6t6zxZs",
        "keywords": "vision-language models, multilinguality, cultural bias, vqa, emotion classification, art",
        "abstract": "Vision-language models (VLMs) can respond to queries about images in many languages. However, beyond language, culture affects how we see things.  For example, individuals from Western cultures focus more on the central figure in an image while individuals from East Asian cultures attend more to scene context (Nisbett 2001).  In this work, we characterize the Western bias of VLMs in image understanding and investigate the role that language plays in this disparity. We evaluate VLMs across subjective and objective visual tasks with culturally diverse images and annotations. We find that VLMs perform better on the Western split than on the East Asian split of each task.  Through controlled experimentation, we trace one source of this bias in image understanding to the lack of diversity in language model construction. While inference in a language nearer to a culture can lead to reductions in bias, we show it is much more effective when that language was well-represented during text-only pre-training. Interestingly, this yields bias reductions even when prompting in English. Our work highlights the importance of richer representation of all languages in building equitable VLMs."
    },
    {
        "title": "Catastrophic Cyber Capabilities Benchmark (3CB): Robustly Evaluating LLM Agent Cyber Offense Capabilities",
        "link_suffix": "/forum?id=kMT8ujhYbA",
        "link": "https://openreview.net/forum?id=kMT8ujhYbA",
        "pdf_link": "https://openreview.net/pdf?id=kMT8ujhYbA",
        "keywords": "AI Safety, AI Evaluations",
        "abstract": "LLM agents have the potential to revolutionize defensive cyber operations, but their offensive capabilities are not yet fully understood. To prepare for emerging threats, model developers and governments are evaluating the cyber capabilities of foundation models. However, these assessments often lack transparency and a comprehensive focus on offensive capabilities. In response, we introduce the Catastrophic Cyber Capabilities Benchmark (3CB), a novel framework designed to rigorously assess the real-world offensive capabilities of LLM agents. Our evaluation of modern LLMs on 3CB reveals that frontier models, such as GPT-4o and Claude 3.5 Sonnet, can perform offensive tasks such as reconnaissance and exploitation across domains ranging from binary analysis to web technologies. Conversely, smaller open-source models exhibit limited offensive capabilities. Our software solution and the corresponding benchmark provides a critical tool to reduce the gap between rapidly improving capabilities and robustness of cyber offense evaluations, aiding in the safer deployment and regulation of these powerful technologies."
    }
]