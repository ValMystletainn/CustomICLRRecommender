[{"title": "SCORE NEURAL OPERATOR: A GENERATIVE MODEL FOR LEARNING AND GENERALIZING ACROSS MULTIPLE PROBABILITY DISTRIBUTIONS", "link_suffix": "/forum?id=AlsvUVZFE9", "link": "https://openreview.net/forum?id=AlsvUVZFE9", "pdf_link": "https://openreview.net/pdf?id=AlsvUVZFE9", "keywords": "Score-based generative model, operator learning, few-shot learning", "abstract": "Most existing generative models are limited to learning a single probability distribution from the training data and cannot generalize to novel distributions for unseen data. \nAn architecture that can generate samples from both trained datasets and unseen probability distributions would mark a significant breakthrough. \nRecently, score-based generative models have gained considerable attention for their comprehensive mode coverage and  high-quality image synthesis, as they effectively learn an operator that maps a probability distribution to its corresponding score function. \nIn this work, we introduce the $\\emph{Score Neural Operator}$, which learns the mapping from multiple probability distributions to their score functions within a unified framework. \nWe employ latent space techniques to facilitate the training of score matching, which tends to over-fit in the original image pixel space, thereby enhancing sample generation quality. \nOur trained Score Neural Operator  demonstrates the ability to predict score functions of probability measures beyond the training space and exhibits strong generalization performance in both 2-dimensional Gaussian Mixture Models and 1024-dimensional MNIST double-digit datasets.\nImportantly, our approach offers significant potential for few-shot learning applications, where a single image from a new distribution can be leveraged to generate multiple distinct images from that distribution.", "title_embedding_index": 7600, "title_abs_embedding_index": 7625}, {"title": "CMIMP: Effortlessly Achieving Diverse Population Training for Zero-Shot Coordination", "link_suffix": "/forum?id=xEivccxGEg", "link": "https://openreview.net/forum?id=xEivccxGEg", "pdf_link": "https://openreview.net/pdf?id=xEivccxGEg", "keywords": "reinforcement Learning, zero-shot coordination, population-based training", "abstract": "Zero-shot coordination has recently become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing algorithms exhibit inefficiency, as the training cost scales linearly with the population size. To address this issue, this paper proposes the  Conditional Mutual Information Maximized Population (CMIMP), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of CMIMP, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.", "title_embedding_index": 7601, "title_abs_embedding_index": 7626}, {"title": "Neural Causal Graph for Interpretable and Intervenable Classification", "link_suffix": "/forum?id=nmvmPIi185", "link": "https://openreview.net/forum?id=nmvmPIi185", "pdf_link": "https://openreview.net/pdf?id=nmvmPIi185", "keywords": "Causal Inference, Graph Neural Network, Classification", "abstract": "Advancements in neural networks have significantly enhanced the performance of classification models, achieving remarkable accuracy across diverse datasets. However, these models often lack transparency and do not support interactive reasoning with human users, which are essential attributes for applications that require trust and user engagement. To overcome these limitations, we introduce an innovative framework, Neural Causal Graph (NCG), that integrates causal inference with neural networks to enable interpretable and intervenable reasoning. We then propose an intervention training method to model the intervention probability of the prediction, which can serve as a contextual prompt to facilitate the fine-grained reasoning and human-AI interaction ability of NCG. Our experiments reveal that the proposed framework significantly enhances the performance of classical classification baselines. Furthermore, NCG achieves nearly 95% top-1 accuracy on the ImageNet dataset by employing a test-time intervention method. This capability not only supports sophisticated post-hoc interpretation but also enables dynamic human-AI interactions, significantly improving the model's transparency and applicability in real-world scenarios.", "title_embedding_index": 7602, "title_abs_embedding_index": 7627}, {"title": "CBQ: Cross-Block Quantization for Large Language Models", "link_suffix": "/forum?id=eW4yh6HKz4", "link": "https://openreview.net/forum?id=eW4yh6HKz4", "pdf_link": "https://openreview.net/pdf?id=eW4yh6HKz4", "keywords": "Large Language Model Compression, ultra-low bits precision", "abstract": "Post-training quantization (PTQ) has played a pivotal role in compressing large language models (LLMs) at ultra-low costs. Although current PTQ methods have achieved promising results by addressing outliers and employing layer- or block-wise loss optimization techniques, they still suffer from significant performance degradation at ultra-low bits precision. To dissect this issue, we conducted an in-depth analysis of quantization errors specific to LLMs and surprisingly discovered that, unlike traditional sources of quantization errors, the growing number of model parameters, combined with the reduction in quantization bits, intensifies inter-layer and intra-layer dependencies, which severely impact quantization accuracy. This finding highlights a critical challenge in quantizing LLMs. To address this, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ leverages a cross-block dependency to establish long-range dependencies across multiple blocks and integrates an adaptive LoRA-Rounding technique to manage intra-layer dependencies. To further enhance performance, CBQ incorporates a coarse-to-fine pre-processing mechanism for processing weights and activations. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across various LLMs and datasets. Notably, CBQ only takes 4.3 hours to quantize a weight-only quantization of a 4-bit LLAMA1-65B model, achieving a commendable trade off between performance and efficiency.", "title_embedding_index": 7603, "title_abs_embedding_index": 7628}, {"title": "Exploring and Unleashing the Power of Message Passing on Heterophilous Graphs", "link_suffix": "/forum?id=aZjOk7wmWf", "link": "https://openreview.net/forum?id=aZjOk7wmWf", "pdf_link": "https://openreview.net/pdf?id=aZjOk7wmWf", "keywords": "Heterophily, Message Passing, Graph Neural Networks", "abstract": "Graph Neural Networks (GNNs) have demonstrated strong performance in graph mining tasks due to their message-passing mechanism, which is aligned with the homophily assumption that adjacent nodes exhibit similar behaviors. However, in many real-world graphs, connected nodes may display contrasting behaviors, termed as heterophilous patterns, which has attracted increased interest in heterophilous GNNs (HTGNN).\nAlthough the message-passing mechanism seems unsuitable for heterophilous graphs due to the propagation of class-irrelevant information, it is still widely used in many existing HTGNNs and consistently achieves notable success. \nThis raises the question: why does message passing remain effective on heterophilous graphs?\nTo answer this question, in this paper, we revisit the message-passing mechanisms in heterophilous graph neural networks and reformulate them into a unified heterophilious message-passing (HTMP) mechanism.\nBased on HTMP and empirical analysis, we reveal that the success of message passing in existing HTGNNs is attributed to implicitly enhancing the compatibility matrix among classes.\nMoreover, we argue that the full potential of the compatibility matrix is not completely achieved due to the existence of incomplete and noisy semantic neighborhoods in real-world heterophilous graphs.\nTo bridge this gap, we introduce a new approach named CMGNN, which operates within the HTMP mechanism to explicitly leverage and improve the compatibility matrix.\nA thorough evaluation involving 10 benchmark datasets and comparative analysis against 17 well-established baselines highlights the superior performance of the HTMP mechanism and CMGNN method.", "title_embedding_index": 7604, "title_abs_embedding_index": 7629}, {"title": "Continuous Spiking Graph ODE Networks", "link_suffix": "/forum?id=OXKJtrfH5n", "link": "https://openreview.net/forum?id=OXKJtrfH5n", "pdf_link": "https://openreview.net/pdf?id=OXKJtrfH5n", "keywords": "Spiking graph neural network; graph ODE", "abstract": "Spiking Graph Networks (SGNs), as bio-inspired neural models that address energy consumption challenges for graph classification, have attracted considerable attention from researchers and the industry. \nHowever, SGNs are typically applied in static scenarios with real-valued inputs and cannot be directly utilized for dynamic prediction because of their limited capacity to handle dynamic real-valued features, denoted as architectural inapplicability. Moreover, they suffer from accuracy loss due to the inherently discrete nature of spike-based representations.\nInspired by recent graph ordinary differential equation (ODE) methods, we propose the framework named \\textbf{C}ontinuous \\textbf{S}piking \\textbf{G}raph \\textbf{O}DE Networks (\\method{}), which leverages the advantages of graph ODE to address the architectural inapplicability, and employs high-order structures to solve the problem of information loss.\nSpecifically, \\method{} replaces the high energy-consuming static SGNs with an efficient Graph ODE process by incorporating SGNs with graph ODE into a unified framework, thereby achieving energy efficiency.\nThen, we derive a high-order spike representation capable of preserving more information. By integrating this with a high-order graph ODE, we propose the second-order \\method{} to address the information loss challenge.\nFurthermore, we prove that the second-order \\method{} maintains stability during the dynamic graph learning.\nExtensive experiments validate the superiority of the proposed \\method{} in performance while maintaining low power consumption.", "title_embedding_index": 7605, "title_abs_embedding_index": 7630}, {"title": "FARM: Functional Group-Aware Representations for Small Molecules", "link_suffix": "/forum?id=o2o1XNeI1b", "link": "https://openreview.net/forum?id=o2o1XNeI1b", "pdf_link": "https://openreview.net/pdf?id=o2o1XNeI1b", "keywords": "Molecular representation learning, Masked language modeling, Contrastive learning", "abstract": "We introduce Functional Group-Aware Representations for Small Molecules (FARM), a novel foundation model designed to bridge the gap between SMILES, natural language, and molecular graphs. The key innovation of FARM lies in its functional group-aware tokenization, which incorporates functional group information directly into the representations. This strategic reduction in tokenization granularity in a way that is intentionally interfaced with key drivers of functional properties (i.e., functional groups) enhances the model's understanding of chemical language, expands the chemical lexicon, more effectively bridging SMILES and natural language, and ultimately advances the model\u2019s capacity to predict molecular properties. FARM also represents molecules from two perspectives: by using masked language modeling to capture atom-level features and by employing graph neural networks to encode the whole molecule topology. By leveraging contrastive learning, FARM aligns these two views of representations into a unified molecular embedding. We rigorously evaluate FARM on the MoleculeNet dataset, where it achieves state-of-the-art performance on 10 out of 12 tasks. These results highlight FARM\u2019s potential to improve molecular representation learning, with promising applications in drug discovery and pharmaceutical research.", "title_embedding_index": 7606, "title_abs_embedding_index": 7631}, {"title": "Token Pruning Meets Audio: Investigating Unique Behaviors in Vision Transformer-Based Audio Classification", "link_suffix": "/forum?id=SvCOhZRQqa", "link": "https://openreview.net/forum?id=SvCOhZRQqa", "pdf_link": "https://openreview.net/pdf?id=SvCOhZRQqa", "keywords": "Audio Spectrogram Transformer, Token Pruning", "abstract": "Vision Transformers (ViTs) have achieved state-of-the-art performance across various computer vision tasks. To reduce the high computational cost of ViTs, token pruning has been proposed to selectively remove tokens that are not crucial. While effective in vision tasks by discarding non-object regions, applying this technique to audio tasks presents unique challenges. In audio processing, distinguishing relevant from non-relevant regions is less straightforward. In this study, we applied token pruning to a ViT-based audio classification model using Mel-spectrograms and analyzed the trade-offs between model performance and computational cost. Our proposed MP-EViT model reduces MAC operations by $2\\times$ with less than a 1% decrease in accuracy for both speech command recognition and environmental sound classification. Notably, while many tokens from signal (high-intensity) regions were pruned, tokens from background (low-intensity) regions were frequently retained, indicating the model\u2019s reliance on these regions. Our analysis showed that both signal and background regions significantly contribute to classification outcomes. In the ablation study, forcing the model to focus only on signal (high-intensity) regions led to lower accuracy, suggesting that background (low-intensity) regions contain unique, irreplaceable information.", "title_embedding_index": 7607, "title_abs_embedding_index": 7632}, {"title": "Curse of Instructions: Large Language Models Cannot Follow Multiple Instructions at Once", "link_suffix": "/forum?id=R6q67CDBCH", "link": "https://openreview.net/forum?id=R6q67CDBCH", "pdf_link": "https://openreview.net/pdf?id=R6q67CDBCH", "keywords": "instruction following, large language models", "abstract": "Large language models (LLMs) have demonstrated impressive performance across various natural language processing (NLP) tasks owing to the strong capability of following instructions. To further accelerate the integration of LLMs into our society, it is essential to have LLMs follow many instructions as accurately as humans do. This study reveals that LLMs unexpectedly struggle to follow all instructions simultaneously as the number of instructions increases.  First, to validate our claim, we introduce ManyIFEval, a large-scale benchmark dataset comprising task prompts with up to ten objectively verifiable instructions. Second, we conduct experiments based on ManyIFEval with GPT-4o, Claude-3.5, Gemini-1.5, Gemma2, and Llama3.1, demonstrating that as the instruction count rises, the models' ability to follow individual instruction deteriorates gradually but constantly. As a result, the models' ability to follow all the instructions significantly drops: the success rate of all the instructions is precisely explained by the success rate of individual instructions to the power of total number of instructions. We refer to it as the ``curse of instructions''. Third, to remove the curse without retraining models, we propose an inference-time strategy that enhances performance through iterative self-refinement. We demonstrate that instruction-level chain-of-thought reasoning significantly improves their capability to detect and correct instruction-following errors. Notably, our method has improved the success rate of following ten instructions by GPT-4o from 15% to 31% and Claude 3.5 Sonnet from 44% to 58%. We also show that precision is more important than recall in feedback: just telling LLMs that they are not following all the instructions also improves self-refinement success. Our findings highlight a fundamental limitation of instruction-following ability and suggest a future direction for building trustworthy LLMs that can coexist with human society.", "title_embedding_index": 7608, "title_abs_embedding_index": 7633}, {"title": "PAR-AdvGAN: Improving Adversarial Attack Capability with Progressive Auto-Regression AdvGAN", "link_suffix": "/forum?id=a05PWdPKo0", "link": "https://openreview.net/forum?id=a05PWdPKo0", "pdf_link": "https://openreview.net/pdf?id=a05PWdPKo0", "keywords": "Adversarial attack, transferability, GANs", "abstract": "Deep neural networks have demonstrated remarkable performance across various domains. However, they are vulnerable to adversarial examples, which can lead to erroneous predictions. Generative Adversarial Networks (GANs) can leverage the generators and discriminators model to quickly produce high-quality adversarial examples. Since both modules train in a competitive and simultaneous manner, GAN-based algorithms like AdvGAN can generate adversarial examples with better transferability compared to traditional methods. However, the generation of perturbations is usually limited to a single iteration, preventing these examples from fully exploiting the potential of the methods. To tackle this issue, we introduce a novel approach named Progressive Auto-Regression AdvGAN (PAR-AdvGAN). It incorporates an auto-regressive iteration mechanism within a progressive generation network to craft adversarial examples with enhanced attack capability. We thoroughly evaluate our PAR-AdvGAN method with a large-scale experiment, demonstrating its superior performance over various state-of-the-art black-box adversarial attacks, as well as the original AdvGAN. Moreover, PAR-AdvGAN significantly accelerates the adversarial example generation, i.e., achieving the speeds of up to 335.5 frames per second on Inception-v3 model, outperforming the gradient-based transferable attack algorithms. Our code is available at:https://anonymous.4open.science/r/PAR-01BF/", "title_embedding_index": 7609, "title_abs_embedding_index": 7634}, {"title": "Disentangling data distribution for Federated Learning", "link_suffix": "/forum?id=Nb7Akh3SjN", "link": "https://openreview.net/forum?id=Nb7Akh3SjN", "pdf_link": "https://openreview.net/pdf?id=Nb7Akh3SjN", "keywords": "Disentangled, Federated learning, Communication efficiency, Diffusion model", "abstract": "Federated Learning (FL) facilitates collaborative training of a global model whose performance is boosted by private data owned by distributed clients, without compromising data privacy. Yet the wide applicability of FL is hindered by entanglement of data distributions across different clients. This paper demonstrates for the first time that by disentangling data distributions FL can in principle achieve efficiencies comparable to those of distributed systems, requiring only one round of communication. \nTo this end, we propose a novel FedDistr algorithm, which employs stable diffusion models to decouple and recover data distributions. Empirical results on the CIFAR100 and DomainNet datasets show that FedDistr significantly enhances model utility and efficiency in both disentangled and near-disentangled scenarios while ensuring privacy, outperforming traditional federated learning methods.", "title_embedding_index": 7610, "title_abs_embedding_index": 7635}, {"title": "Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding", "link_suffix": "/forum?id=ogKE7LcvW6", "link": "https://openreview.net/forum?id=ogKE7LcvW6", "pdf_link": "https://openreview.net/pdf?id=ogKE7LcvW6", "keywords": "loss landscape, non-smooth, training dynamics, network embedding, over-parameterization, neural network", "abstract": "In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We deduce the conditions for stationarity that apply to both non-differentiable and differentiable areas of the landscape. Additionally, we show that, if a stationary point does not contain \"escape neurons\", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum.  Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks. By precluding the existence of the saddle escaping types that previous works did not rule out, we advance one step closer to a complete picture of the entire dynamics. Moreover, we are also able to fully discuss how network embedding, which is to instantiate a narrower network within a wider network, reshapes the stationary points.", "title_embedding_index": 7611, "title_abs_embedding_index": 7636}, {"title": "TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees", "link_suffix": "/forum?id=O0sQ9CPzai", "link": "https://openreview.net/forum?id=O0sQ9CPzai", "pdf_link": "https://openreview.net/pdf?id=O0sQ9CPzai", "keywords": "Reinforcement Learning from Human Feedback, Language Models, Preferences Learning", "abstract": "In the domain of complex reasoning tasks, such as mathematical reasoning, recent advancements have proposed the use of Direct Preference Optimization (DPO) to suppress output of dispreferred responses, thereby enhancing the long-chain reasoning capabilities of large language models (LLMs). To this end, these studies employed LLMs to generate preference trees via Tree-of-thoughts (ToT) and sample the paired preference responses required by the DPO algorithm. However, the DPO algorithm based on binary preference optimization is unable to learn multiple responses with varying degrees of preference/dispreference that provided by the preference trees, resulting in incomplete preference learning. In this work, we introduce Tree Preference Optimization (TPO), that does not sample paired preference responses from the preference tree; instead, it directly learns from the entire preference tree during the fine-tuning. Specifically, TPO formulates the language model alignment as a Preference List Ranking problem, where the policy can potentially learn more effectively from a ranked preference list of responses given the prompt.  In addition, to further assist LLMs in identifying discriminative steps within long-chain reasoning and increase the relative reward margin in the preference list, TPO utilizes Adaptive Step Reward to adjust the reward values of each step in trajectory for performing fine-grained preference optimization. We carry out extensive experiments on mathematical reasoning tasks to evaluate TPO. The experimental results indicate that TPO consistently outperforms DPO across three public large language models on four datasets. The code is available onhttps://anonymous.4open.science/r/TPO.", "title_embedding_index": 7612, "title_abs_embedding_index": 7637}, {"title": "Gradient-Free Adversarial Attack on Time Series Regression: Targeting XAI Explanations", "link_suffix": "/forum?id=3i4OShnmnG", "link": "https://openreview.net/forum?id=3i4OShnmnG", "pdf_link": "https://openreview.net/pdf?id=3i4OShnmnG", "keywords": "Adversarial attacks, Explainable artificial intelligence, Time series regression, Robustness", "abstract": "Explainable Artificial Intelligence (XAI) sheds light on the decision-making ground of black-box models by offering explanations.\nThese explanations need to be robust for trustworthy time series regression applications in high-stake areas like medicine or finance, which yet remains largely unexplored.\nFurthermore, most adversarial attack methods currently rely on white-box strategies, which require access to gradient information from both the model and the XAI method. In real-world scenarios, such information is often difficult or impossible to obtain.\nTo address these challenges, we propose a novel gradient-free adversarial attack method specifically designed for time series explanations, targeting non-differentiable XAI techniques.\nTo enhance the effectiveness of our method for time series data, we introduce an attack objective function based on Dynamic Time Warping (DTW).\nAdditionally, we implement an explanation-based local attack strategy, which ensures that the adversarial perturbations remain imperceptible within the time series data.\nIn our experiments, we generate adversarial examples to attack four different XAI methods across three black-box models, using two time series datasets.\nThe results reveal the vulnerability of current non-differentiable XAI methods.\nFurthermore, by comparing our approach with existing attack methods, we demonstrate the superiority of our proposed objective function and local attack strategy.", "title_embedding_index": 7613, "title_abs_embedding_index": 7638}, {"title": "VideoWebArena:  Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks", "link_suffix": "/forum?id=unDQOUah0F", "link": "https://openreview.net/forum?id=unDQOUah0F", "pdf_link": "https://openreview.net/pdf?id=unDQOUah0F", "keywords": "agents, benchmark, video understanding, multimodal agents", "abstract": "Videos are often used to learn or extract the necessary information to complete tasks\nin ways different than what text and static imagery alone can provide. However,\nmany existing agent benchmarks neglect long-context video understanding, instead\nfocusing on text or static image inputs. To bridge this gap, we introduce VideoWe-\nbArena (VideoWA), a benchmark for evaluating the capabilities of long-context\nmultimodal agents for video understanding. VideoWA consists of 2,021 web agent\ntasks based on manually crafted video tutorials, which total almost four hours of\ncontent. For our benchmark, we define a taxonomy of long-context video-based\nagent tasks with two main areas of focus: skill retention and factual retention.\nWhile skill retention tasks evaluate whether an agent can use a given human demon-\nstration to complete a task efficiently, the factual retention task evaluates whether\nan agent can retrieve instruction-relevant information from a video to complete\na task. We find that the best model achieves 13.3% success on factual retention\ntasks and 46.0% on factual retention QA pairs, far below human performance\nat 73.9% and 79.3%, respectively. On skill retention tasks, long-context models\nperform worse with tutorials than without, exhibiting a 5% performance decrease\nin WebArena tasks and a 10.3% decrease in VisualWebArena tasks. Our work\nhighlights the need to improve the agentic abilities of long-context multimodal\nmodels and provides a testbed for future development with long-context video\nagents.", "title_embedding_index": 7614, "title_abs_embedding_index": 7639}, {"title": "Constrained Diffusion Implicit Models", "link_suffix": "/forum?id=8xStV6KJEr", "link": "https://openreview.net/forum?id=8xStV6KJEr", "pdf_link": "https://openreview.net/pdf?id=8xStV6KJEr", "keywords": "Diffusion, Inverse Problems, DDIM, Inpainting", "abstract": "This paper describes an efficient algorithm for solving noisy linear inverse problems using pretrained diffusion models. Extending the paradigm of denoising diffusion implicit models (DDIM), we propose conditional diffusion implicit models (CDIM) that modify the diffusion updates to enforce a constraint upon the final output. For noiseless inverse problems, CDIM exactly satisfies the constraints; in the noisy case, we generalize CDIM to satisfy an exact constraint on the residual distribution of the noise. Experiments across a variety of tasks and metrics show strong performance of CDIM, with analogous inference acceleration to unconditional DDIM: $10$ to $50$ times faster than previous conditional diffusion methods.  We demonstrate the versatility of our approach on many problems including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reconstruction.", "title_embedding_index": 7615, "title_abs_embedding_index": 7640}, {"title": "Preventing Unintended Memorization by Covering with Over-Memorization", "link_suffix": "/forum?id=onvN3zsNMI", "link": "https://openreview.net/forum?id=onvN3zsNMI", "pdf_link": "https://openreview.net/pdf?id=onvN3zsNMI", "keywords": "memorization, deep learning, data privacy, machine learning", "abstract": "From the advances of deep learning, the privacy concerns of deep neural networks are in the limelight. A particular concern is  privacy of the training data, which is often compromised by the model's inherent memorization capabilities. Suppressing such memorization can enhance privacy but introduces two main challenges: 1) removing a memorized instance from the training dataset will result in the model to memorize another instance instead, and 2) the memorization is essential for improving the generalization error. To address these challenges, we propose an over-memorization method that involves training the model with both the standard training set and a set of redundant, non-sensitive instances. Our method leverages the model's limited memorization capacity to focus on irrelevant data, thereby preventing it from memorizing the training data. Our empirical results demonstrate that this method not only enhances protection against membership inference attacks but also minimizes the loss of utility by effectively redirecting the model's generalization efforts towards non-sensitive instances.", "title_embedding_index": 7616, "title_abs_embedding_index": 7641}, {"title": "DiNO-Diffusion: Scaling Medical Diffusion Models via Self-Supervised Pre-Training", "link_suffix": "/forum?id=zkn2tvtt8J", "link": "https://openreview.net/forum?id=zkn2tvtt8J", "pdf_link": "https://openreview.net/pdf?id=zkn2tvtt8J", "keywords": "Diffusion Models, Generative AI, Medical Imaging, Self-Supervision", "abstract": "Diffusion models (DMs) require large annotated datasets for training, limiting their applicability in medical imaging where datasets are typically smaller and sparsely annotated. We introduce DiNO-Diffusion, a self-supervised method for training DMs that conditions the generation process on image embeddings extracted from DiNO, a pretrained vision transformer. By not relying on annotations, our training leverages over 868k unlabelled images from public chest X-Ray (CXR) datasets. DiNO-Diffusion shows comprehensive manifold coverage, with FID scores as low as 4.7, and emerging properties when evaluated in downstream tasks, allowing to generate semantically-diverse synthetic datasets even from small data pools, demonstrating up to 20% AUC increase in classification performance when used for data augmentation. Results suggest that DiNO-Diffusion could facilitate the creation of large datasets for flexible training of downstream AI models from limited amount of real data, while also holding potential for privacy preservation. Additionally, DiNO-Diffusion demonstrates zero-shot segmentation performance of up to 84.4% Dice score when evaluating lung lobe segmentation, evidencing good CXR image-anatomy alignment akin to textual descriptors on vanilla DMs. Finally, DiNO-Diffusion can be easily adapted to other medical imaging modalities or state-of-the-art diffusion models, allowing large-scale, multi-domain image generation pipelines for medical imaging.", "title_embedding_index": 7617, "title_abs_embedding_index": 7642}, {"title": "Beyond Simple Sum of Delayed Rewards: Non-Markovian Reward Modeling for Reinforcement Learning", "link_suffix": "/forum?id=VnNSkUXejc", "link": "https://openreview.net/forum?id=VnNSkUXejc", "pdf_link": "https://openreview.net/pdf?id=VnNSkUXejc", "keywords": "Reinforcement Learning, Delayed Reward", "abstract": "Reinforcement Learning (RL) empowers agents to acquire various skills by learning from reward signals. Unfortunately, designing high-quality instance-level rewards often demands significant effort. An emerging alternative, RL with delayed reward, focuses on learning from rewards presented periodically, which can be obtained from human evaluators assessing the agent's performance over sequences of behaviors. However, traditional methods in this domain assume the existence of underlying Markovian rewards and that the observed delayed reward is simply the sum of instance-level rewards, both of which often do not align well with real-world scenarios. In this paper, we introduce the problem of RL from Composite Delayed Reward (RLCoDe), which generalizes traditional RL from delayed rewards by eliminating the strong assumption. We suggest that the delayed reward may arise from a more complex structure reflecting the overall contribution of the sequence. To address this problem, we present a framework for modeling composite delayed rewards, using a weighted sum of non-Markovian components to capture the different contributions of individual steps. Building on this framework, we propose Composite Delayed Reward Transformer (CoDeTr), which incorporates a specialized in-sequence attention mechanism to effectively model these contributions. We conduct experiments on challenging locomotion tasks where the agent receives delayed rewards computed from composite functions of observable step rewards. The experimental results indicate that CoDeTr consistently outperforms baseline methods across evaluated metrics. Additionally, we demonstrate that it effectively identifies the most significant time steps within the sequence and accurately predicts rewards that closely reflect the environment feedback.", "title_embedding_index": 7618, "title_abs_embedding_index": 7643}, {"title": "A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery", "link_suffix": "/forum?id=zGzs5SIwT8", "link": "https://openreview.net/forum?id=zGzs5SIwT8", "pdf_link": "https://openreview.net/pdf?id=zGzs5SIwT8", "keywords": "Causal Discovery, Heteroscedastic Noise, Score Matching", "abstract": "Real-world data often violates the equal-variance assumption (homoscedasticity), making it essential to account for heteroscedastic noise in causal discovery. In this work, we explore heteroscedastic symmetric noise models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \\sigma(X)N$, with $X$ as the cause and $N$ as independent noise following a symmetric distribution. We introduce a novel criterion for identifying HSNMs based on the skewness of the score (i.e., the gradient of the log density) of the data distribution. This criterion establishes a computationally tractable measurement that is zero in the causal direction but nonzero in the anticausal direction, enabling the causal direction discovery. We extend this skewness-based criterion to the multivariate setting and propose \\texttt{SkewScore}, an algorithm that handles heteroscedastic noise without requiring the extraction of exogenous noise. We also conduct a case study on the robustness of \\texttt{SkewScore} in a bivariate model with a latent confounder, providing theoretical insights into its performance. Empirical studies further validate the effectiveness of the proposed method.", "title_embedding_index": 7619, "title_abs_embedding_index": 7644}, {"title": "QueST: Querying Functional and Structural Niches on Spatial Transcriptomics data via Contrastive Subgraph Embedding", "link_suffix": "/forum?id=V6TD4io8Gu", "link": "https://openreview.net/forum?id=V6TD4io8Gu", "pdf_link": "https://openreview.net/pdf?id=V6TD4io8Gu", "keywords": "Graph neural networks; Subgraph contrastive learning; Spatial niche query; Spatial transcriptomics; Batch removal", "abstract": "The functional or structural spatial regions within tissues, referred to as spatial niches, are elements for illustrating the spatial contexts of multicellular organisms. A key challenge is querying shared niches across diverse tissues, which is crucial for achieving a comprehensive understanding of the organization and phenotypes of cell populations. However, current data analysis methods predominantly focus on creating spatial-aware embeddings for cells, neglecting the development of niche-level representations for effective querying. To address this gap, we introduce QueST, a novel niche representation learning model designed for querying spatial niches across multiple samples. QueST utilizes a novel subgraph contrastive learning approach to explicitly capture niche-level characteristics and incorporates adversarial training to mitigate batch effects. We evaluate QueST on established benchmarks using human and mouse datasets, demonstrating its superiority over state-of-the-art graph representation learning methods in accurate niche queries. Overall, QueST offers a specialized model for spatial niche queries, paving the way for deeper insights into the patterns and mechanisms of cell spatial organization across tissues.", "title_embedding_index": 7620, "title_abs_embedding_index": 7645}, {"title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model", "link_suffix": "/forum?id=Z30Mdbv5jO", "link": "https://openreview.net/forum?id=Z30Mdbv5jO", "pdf_link": "https://openreview.net/pdf?id=Z30Mdbv5jO", "keywords": "Sparse-view Reconstruction, Video Diffusion, Gaussian Splatting", "abstract": "Advancements in 3D scene reconstruction have transformed 2D images from the real world into 3D models, producing realistic 3D results from hundreds of input photos. Despite great success in dense-view reconstruction scenarios, rendering a detailed scene from sparse views is still an ill-posed optimization problem, often resulting in artifacts and distortions in unseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction problem as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. Nevertheless, it is challenging to preserve 3D view consistency when directly generating video frames from pre-trained models. To address this issue, given limited input views, the proposed ReconX first constructs a global point cloud and encodes it into a contextual space as the 3D structure condition. Guided by the condition, the video diffusion model then synthesizes video frames that are detail-preserved and exhibit a high degree of 3D consistency, ensuring the coherence of the scene from various perspectives. Finally, we recover the 3D scene from the generated video through a confidence-aware 3D Gaussian Splatting optimization scheme. Extensive experiments on various real-world datasets show the superiority of ReconX over state-of-the-art methods in terms of quality and generalizability.", "title_embedding_index": 7621, "title_abs_embedding_index": 7646}, {"title": "Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision", "link_suffix": "/forum?id=q5EZ7gKcnW", "link": "https://openreview.net/forum?id=q5EZ7gKcnW", "pdf_link": "https://openreview.net/pdf?id=q5EZ7gKcnW", "keywords": "unreliable human supervision, language model post-training, scalable oversight", "abstract": "Language model (LM) post-training relies on two stages of human supervision: task demonstrations for supervised finetuning (SFT), followed by preference comparisons for reinforcement learning from human feedback (RLHF) via algorithms like proximal preference optimization (PPO) or direct preference optimization (DPO). As LMs become more capable, the tasks they are given become harder to supervise. Will post-training remain effective under unreliable supervision? To test this, we simulate unreliable demonstrations and comparison feedback using small LMs and time-constrained humans. We find that in the presence of unreliable supervision, SFT still retains some effectiveness, but DPO fails to improve the model beyond SFT. To address this, we propose iterative label refinement (ILR) as a replacement for RLHF with unreliable supervision. ILR directly improves the SFT data by using comparison feedback to decide whether human demonstrations should be replaced by model-generated alternatives, then retrains the model via SFT on the updated data. SFT+ILR outperforms SFT+DPO on several tasks with LM-simulated unreliable supervision (math, coding, safe instruction-following), with results further verified by human experiments on instruction-following. Our findings suggest that as LMs take on complex tasks where human supervision is unreliable, RLHF may no longer be the best use of human comparison feedback; instead, it is better to direct feedback towards improving the training data rather than continually training the model.", "title_embedding_index": 7622, "title_abs_embedding_index": 7647}, {"title": "SHARE: Bridging Shape and Ray Estimation for Pose-Free Generalizable Gaussian Splatting", "link_suffix": "/forum?id=EAT5Jpa4ws", "link": "https://openreview.net/forum?id=EAT5Jpa4ws", "pdf_link": "https://openreview.net/pdf?id=EAT5Jpa4ws", "keywords": "3D Gaussian Splatting, Novel View Synthesis, Pose Estimation", "abstract": "While generalizable 3D Gaussian Splatting enables efficient, high-quality rendering of unseen scenes, it heavily depends on precise camera poses for accurate geometry. In real-world scenarios, obtaining accurate poses is challenging, leading to noisy pose estimates and geometric misalignments. To address this, we introduce SHARE, a novel pose-free generalizable Gaussian Splatting framework that overcomes these ambiguities. Our ray-guided multi-view fusion network consolidates multi-view features into a unified pose-aware canonical volume, bridging 3D reconstruction and ray-based pose estimation. In addition, we propose an anchor-aligned Gaussian prediction strategy for fine-grained geometry estimation within a canonical view.\nExtensive experiments on diverse real-world datasets show that SHARE achieves state-of-the-art performance in pose-free generalizable Gaussian splatting.", "title_embedding_index": 7623, "title_abs_embedding_index": 7648}, {"title": "Controllable Text-to-Speech Synthesis with Masked-Autoencoded Style Representation", "link_suffix": "/forum?id=qH5uyYCG2j", "link": "https://openreview.net/forum?id=qH5uyYCG2j", "pdf_link": "https://openreview.net/pdf?id=qH5uyYCG2j", "keywords": "controllable text-to-speech, representation learning", "abstract": "Controllable text-to-speech (TTS) systems aim to manipulate various stylistic attributes of generated speech. Despite considerable research in this area, existing models that use natural language prompts as an interface often lack the ability for fine-grained control and face a scarcity of high-quality data. To address these challenges, we propose a two-stage style-controllable TTS system with language models, utilizing a masked-autoencoded style representation as an intermediary. In our approach, we employ a masked autoencoder to learn a content-disentangled style feature of speech, which is then discretized using a residual vector quantizer. In the first stage, an autoregressive transformer is used for the conditional generation of these style tokens from text and control signals. In the second stage, we generate codec tokens from both text and sampled style tokens. Experiments demonstrate that training the first-stage model on extensive datasets enhances the robustness of the two-stage model in terms of quality and content accuracy. Additionally, our model achieves superior control over attributes such as pitch and emotion. By selectively combining discrete labels and speaker embeddings, we can fully control the speaker\u2019s timbre and other stylistic information, or adjust attributes like pitch and emotion for a specified speaker. Audio samples are available athttps://style-ar-tts.github.io.", "title_embedding_index": 7624, "title_abs_embedding_index": 7649}]