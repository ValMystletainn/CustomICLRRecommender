[
    {
        "title": "Attention layers provably solve single-location regression",
        "link_suffix": "/forum?id=DVlPp7Jd7P",
        "link": "https://openreview.net/forum?id=DVlPp7Jd7P",
        "pdf_link": "https://openreview.net/pdf?id=DVlPp7Jd7P",
        "keywords": "theory of neural networks, attention, Transformer, statistical learning theory, optimization, first-oder optimization, gradient flow",
        "abstract": "Attention-based models, such as Transformer, excel across various tasks but lack a comprehensive theoretical understanding, especially regarding token-wise sparsity and internal linear representations. To address this gap, we introduce the single-location regression task, where only one token in a sequence determines the output, and its position is a latent random variable, retrievable via a linear projection of the input. To solve this task, we propose a dedicated predictor, which turns out to be a simplified version of a non-linear self-attention layer. We study its theoretical properties, by showing its asymptotic Bayes optimality and analyzing its training dynamics. In particular, despite the non-convex nature of the problem, the predictor effectively learns the underlying structure. This work highlights the capacity of attention mechanisms to handle sparse token information and internal linear structures."
    },
    {
        "title": "Efficient Sparse Single-stage 3D Visual Grounding with Text-guided Pruning",
        "link_suffix": "/forum?id=b39J2X4rjT",
        "link": "https://openreview.net/forum?id=b39J2X4rjT",
        "pdf_link": "https://openreview.net/pdf?id=b39J2X4rjT",
        "keywords": "3D Visual Grounding, Efficient Single-stage Architecture, Feature Pruning",
        "abstract": "In this paper, we propose an efficient sparse convolution-based architecture called ESS3D for 3D visual grounding. Conventional 3D visual grounding methods are difficult to meet the requirements of real-time inference due to the two-stage or point-based architecture. Inspired by the success of multi-level fully sparse convolutional architecture in 3D object detection, we aim to build a new 3D visual grounding framework following this technical route. However, as in visual grounding task the 3D scene representation should be deeply interacted with text features, sparse convolution-based architecture is inefficient for this interaction due to the large amount of voxel features. To this end, we propose text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D scene representation and text features in an efficient way by gradual region pruning and target completion. Specifically, TGP iteratively sparsifies the 3D scene representation and thus efficiently interacts the voxel features with text features by cross-attention. To mitigate the affect of pruning on delicate geometric information, CBA adaptively fixes the over-pruned region by voxel completion with negligible computational overhead. Compared with previous single-stage methods, ESS3D achieves top inference speed and surpasses previous fastest method by 100% FPS. ESS3D also achieves state-of-the-art accuracy even compared with two-stage methods, with $+1.13$ lead ofAcc@0.5on ScanRefer, and $+5.4$ and $+5.0$ leads on NR3D and SR3D respectively. The code will be released soon."
    },
    {
        "title": "Training One-Dimensional Graph Neural Networks is NP-Hard",
        "link_suffix": "/forum?id=7BESdFZ7YA",
        "link": "https://openreview.net/forum?id=7BESdFZ7YA",
        "pdf_link": "https://openreview.net/pdf?id=7BESdFZ7YA",
        "keywords": "Computational Complexity, Graph Neural Networks, Training, ReLU",
        "abstract": "We initiate the study of the computational complexity of training graph neural networks (GNNs). While the intractability of training multidimensonal GNNs immediately follows from known lower bounds for training classical neural networks (and holds even for trivial GNNs), one-dimensional GNNs form a crucial case of interest: the computational complexity of training such networks depends on both the graphical structure of the network and the properties of the involved activation and aggregation functions. As our main result, we establish the NP-hardness of training ReLU-activated one-dimensional GNNs via a highly non-trivial reduction. We complement this result with algorithmic upper bounds for the training problem in the ReLU-activated and linearly-activated settings."
    },
    {
        "title": "Grounding Multimodal Large Language Model in GUI World",
        "link_suffix": "/forum?id=M9iky9Ruhx",
        "link": "https://openreview.net/forum?id=M9iky9Ruhx",
        "pdf_link": "https://openreview.net/pdf?id=M9iky9Ruhx",
        "keywords": "Multimodal Large Language Model, GUI Element Grounding",
        "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have accelerated the development of Graphical User Interface (GUI) agents capable of automating complex tasks across digital platforms. However, precise GUI element grounding remains a key challenge for accurate interaction and generalization. In this work, we present an effective GUI grounding framework, which includes an automated data collection engine that gathers extensive GUI screenshots and annotations to ensure broad generalization. We also propose a lightweight and flexible GUI grounding module designed to efficiently localize UI elements by pre-training on the collected data, and introduce a novel method to integrate this module with MLLMs for the effective execution of GUI tasks. Our approach demonstrates superior performance in task accuracy and adaptability, as validated by benchmarks such as ScreenSpot, MiniWob, AITW, and Mind2Web."
    },
    {
        "title": "Intransigent Teachers Guide Better Test-Time Adaptation Students",
        "link_suffix": "/forum?id=Chq4OQ3p18",
        "link": "https://openreview.net/forum?id=Chq4OQ3p18",
        "pdf_link": "https://openreview.net/pdf?id=Chq4OQ3p18",
        "keywords": "test-time adaptation",
        "abstract": "Test-Time Adaptation (TTA) has recently emerged as a promising strategy that allows the adaptation of pre-trained models to changing data distributions at deployment time, without access to any labels. To address the error accumulation problem, various approaches have used the teacher-student framework. In this work, we challenge the common strategy of setting the teacher weights to be an exponential moving average of the student by showing that error accumulation still occurs, but only on longer sequences compared to those commonly utilized. We analyze the stability-plasticity trade-off within the teacher-student framework and propose to use an intransigent teacher instead. We show that not changing any of the weights of the teacher model within existing TTA methods allows them to significantly improve their performance on multiple datasets with longer scenarios and smaller batch sizes. Finally, we show that the proposed changes are applicable to different architectures and are more robust to changes in hyper-parameters."
    },
    {
        "title": "Improved Noise Schedule for Diffusion Training",
        "link_suffix": "/forum?id=j3U6CJLhqw",
        "link": "https://openreview.net/forum?id=j3U6CJLhqw",
        "pdf_link": "https://openreview.net/pdf?id=j3U6CJLhqw",
        "keywords": "diffusion model, effcient training, noise schedule, image generation",
        "abstract": "Diffusion models have emerged as the de facto choice for generating high-quality visual content across multiple domains.\nHowever, training a single model to predict noise at multiple levels presents significant challenges, requiring numerous iterations and resulting in substantial computational costs.\nVarious approaches, such as loss weighting strategy design and architectural refinements, have been introduced to expedite convergence and improve model performance.\nIn this study, we propose a novel approach to design the noise schedule for enhancing the training of diffusion models. Our key insight is that the importance sampling of the logarithm of the Signal-to-Noise ratio ($\\log \\text{SNR}$), theoretically equivalent to a modified noise schedule, is particularly beneficial for training efficiency when increasing the sample frequency around $\\log \\text{SNR}=0$. This strategic sampling allows the model to focus on the critical transition point between signal dominance and noise dominance, potentially leading to more robust and accurate predictions.\nWe empirically demonstrate the superiority of our noise schedule over the standard cosine schedule.\nFurthermore, we highlight the advantages of our noise schedule design on the ImageNet benchmark, showing that the designed schedule consistently benefits different prediction targets.\nOur findings contribute to the ongoing efforts to optimize diffusion models, potentially paving the way for more efficient and effective training paradigms in the field of generative AI."
    },
    {
        "title": "Variational Diffusion Posterior Sampling with Midpoint Guidance",
        "link_suffix": "/forum?id=6EUtjXAvmj",
        "link": "https://openreview.net/forum?id=6EUtjXAvmj",
        "pdf_link": "https://openreview.net/pdf?id=6EUtjXAvmj",
        "keywords": "Diffusion models, Inverse problems, posterior sampling",
        "abstract": "Diffusion models have recently shown considerable potential in solving Bayesian inverse problems when used as priors. However, sampling from the resulting denoising posterior distributions remains a challenge as it involves intractable terms. To tackle this issue, state-of-the-art approaches formulate the problem as that of sampling from a surrogate diffusion model targeting the posterior and decompose its scores into two terms: the prior score and an intractable guidance term. While the former is replaced by the pre-trained score of the considered diffusion model, the guidance term has to be estimated. In this paper, we propose a novel approach that utilises a decomposition of the transitions which, in contrast to previous methods, allows a trade-off between the complexity of the intractable guidance term and that of the prior transitions. We validate the proposed approach through extensive experiments on linear and nonlinear inverse problems, including challenging cases with latent diffusion models as priors, and demonstrate its effectiveness in reconstructing electrocardiogram (ECG) from partial measurements for accurate cardiac diagnosis."
    },
    {
        "title": "Resolving Complex Social Dilemmas by Aligning Preferences with Counterfactual Regret",
        "link_suffix": "/forum?id=CgkGFeSpo0",
        "link": "https://openreview.net/forum?id=CgkGFeSpo0",
        "pdf_link": "https://openreview.net/pdf?id=CgkGFeSpo0",
        "keywords": "Counterfacutla Regret, Sequential Social Dilemma",
        "abstract": "Social dilemmas are situations where gains from cooperation are possible but misaligned incentives make it hard to find and stabilize prosocial joint behavior. In such situations selfish behaviors may harm the social good. In spatiotemporally complex social dilemmas, the barriers to cooperation that emerge from misaligned incentives interact with obstacles that stem from spatiotemporal complexity. In this paper, we propose a multi-agent reinforcement learning algorithm which aims to find cooperative resolutions for such complex social dilemmas. Agents maximize their own interests while also helping others, regardless of the actions their co-players take. This approach disentangles the causes of selfish reward from the causes of prosocial reward. Empirically, our method outperforms multiple baseline methods in several complex social dilemma environments."
    },
    {
        "title": "Geometric Graph Neural Network based track finding",
        "link_suffix": "/forum?id=PgVo0t9rC2",
        "link": "https://openreview.net/forum?id=PgVo0t9rC2",
        "pdf_link": "https://openreview.net/pdf?id=PgVo0t9rC2",
        "keywords": "Tracking, GNN, High Energy Physics",
        "abstract": "An essential component of event reconstruction in particle physics experiments is identifying the trajectory of charged particles in the detector.   Traditional methods for track finding are often complex, and tailored to specific detectors and input  geometries, limiting their adaptability to new detector designs and optimization processes.\nTo overcome these limitations, we present a novel, end-to-end track finding algorithm that is detector-agnostic and can take into account multiple input geometric types. To achieve this, our approach unifies  inputs from multiple sub-detectors and detector types into a single geometric algebra representation, simplifying data handling compared to traditional methods.\nThen, we leverage an equivariant graph neural network, GATr, to perform track finding across all data from an event simultaneously. \nWe validate the effectiveness of our pipeline on various detector concepts with different technologies for the FCC-ee at CERN, specifically the IDEA and CLD  detectors.\nThis work generalizes track finding across  diverse types of input geometric data and  tracking technologies, facilitating the development of innovative detector concepts, accelerating detector development cycles, and enabling comprehensive detector optimization."
    },
    {
        "title": "P-Align: Self-Alignment in Physical Dynamical System Modeling",
        "link_suffix": "/forum?id=AgTSjXh7vl",
        "link": "https://openreview.net/forum?id=AgTSjXh7vl",
        "pdf_link": "https://openreview.net/pdf?id=AgTSjXh7vl",
        "keywords": "dynamic systems modeling, physical consistency.",
        "abstract": "Deep learning has emerged as the new paradigm in modeling complex physical dynamical systems. Nevertheless, data-driven methods learn patterns by optimizing statistical metrics, tend to overlook the adherence to physical laws. Previous work have attempted to incorporate physical constraints into neural networks, but they often face limitations due to lack of flexibility or optimization challenges. In this paper, we propose a novel framework, Physics-aware Self-Alignment (P-Align), to enhance the physical consistency of dynamical systems modeling.  P-Align enables dynamical system models to provides physics-aware rewards, which makes self-alignment of dynamical system models possible. Comprehensive experiments show that \\method{} not only gave an average statistical skill score boost of more than 32% for ten backbones on five datasets, but also significantly enhances physics-aware metrics. All of our source codes will be released via GitHub."
    },
    {
        "title": "Zero-shot Imputation with Foundation Inference Models for Dynamical Systems",
        "link_suffix": "/forum?id=NPSZ7V1CCY",
        "link": "https://openreview.net/forum?id=NPSZ7V1CCY",
        "pdf_link": "https://openreview.net/pdf?id=NPSZ7V1CCY",
        "keywords": "Zero-shot imputation, foundation models, time series imputation, dynamical systems, amortized inference, zero-shot interpolation, foundation models for time series",
        "abstract": "Dynamical systems governed by ordinary differential equations (ODEs) serve as models for a vast number of natural and social phenomena. In this work, we offer a fresh perspective on the classical problem of imputing missing time series data, whose underlying dynamics are assumed to be determined by ODEs. Specifically, we revisit ideas from amortized inference and neural operators, and propose a novel supervised learning framework forzero-shot time series imputation, through parametric functions satisfying some (hidden) ODEs. Our proposal consists of two components. First, a broad probability distribution over the space of ODE solutions, observation times and noise mechanisms, with which we generate a large, synthetic dataset of (hidden) ODE solutions, along with their noisy and sparse observations. Second, a neural recognition model that is trainedoffline, to map the generated time series onto the spaces of initial conditions and time derivatives of the (hidden) ODE solutions, which we then integrate to impute the missing data. We empirically demonstrate thatone and the same(pretrained) recognition model can perform zero-shot imputation across 63 distinct time series with missing values, each sampled from widely different dynamical systems. Likewise, we demonstrate that it can perform zero-shot imputation of missing high-dimensional data in 10 vastly different settings, spanning human motion, air quality, traffic and electricity studies, as well as Navier-Stokes simulations \u2014without requiring any fine-tuning. What is more, our proposal often outperforms state-of-the-art methods, which are trained on the target datasets.Our pretrained model is available with the supplementary material"
    },
    {
        "title": "Prompt Reverse Learning: Enhancing Visual Language Models for Rare Image Recognition",
        "link_suffix": "/forum?id=dsiwLm8yjz",
        "link": "https://openreview.net/forum?id=dsiwLm8yjz",
        "pdf_link": "https://openreview.net/pdf?id=dsiwLm8yjz",
        "keywords": "Prompt Learning",
        "abstract": "Large visual language models like CLIP have demonstrated impressive performance on various downstream tasks involving common data, e.g., natural images, by leveraging prompt learning. However, these models often falter when applied to tasks involving rare data, e.g., medical images. We provide an experimental insight into this phenomenon: CLIP is insensitive to the class names of rare images. For instance, replacing the class name \u201cmedulloblastoma\u201d (a type of brain tumor) with \u201cdog\u201d in prompts has minimal impact on performance, a phenomenon not observed with common images. This highlights the disparity in representation learning between common and rare data. To realign prompt learning with rare image recognition, we propose a novel prompt learning strategy, termed prompt reverse learning (PeLen). Different from the existing methods that adapt CLIP's representations to downstream tasks, PeLen adapts task-specific representations to CLIP's representations. Built upon the insensitivity to the class names of rare images, PeLen designates common images and their class names to represent a specific class of rare images and class names, e.g., allowing the image and text of a dog to correspond to the image and text of medulloblastoma. Consequently, PeLen learns prompts to align the representations between the rare images and the visual and textual representations of common images. Our experiments on three types of rare images demonstrate the efficacy of PeLen for rare image recognition."
    },
    {
        "title": "On Extending Direct Preference Optimization to Accommodate Ties",
        "link_suffix": "/forum?id=h71cSd2loX",
        "link": "https://openreview.net/forum?id=h71cSd2loX",
        "pdf_link": "https://openreview.net/pdf?id=h71cSd2loX",
        "keywords": "Preference Optimization, Ties, Direct Preference Optimization, Langauge Model, Machine Translation, Summarisation.",
        "abstract": "We derive and investigate two DPO variants that explicitly model the possibility of declaring a tie in pair-wise comparisons. We replace the Bradley-Terry model in DPO with two well-known modeling extensions, by Rao and Kupper and by Davidson, that assign probability to ties as alternatives to clear preferences. Our experiments in neural machine translation and summarization show that explicitly labeled ties can be added to the datasets for these DPO variants without  the degradation in task performance that is observed when the same tied pairs are presented to DPO. We find empirically that the inclusion of ties leads to stronger regularization with respect to the reference policy as measured by KL divergence, and we see this even for DPO in its original form. These findings motivate and enable the inclusion of tied pairs in preference optimization as opposed to  simply discarding them."
    },
    {
        "title": "On the (un) interpretability of Ensembles: A Computational Analysis",
        "link_suffix": "/forum?id=6zVElUoc6l",
        "link": "https://openreview.net/forum?id=6zVElUoc6l",
        "pdf_link": "https://openreview.net/pdf?id=6zVElUoc6l",
        "keywords": "explainable AI, XAI, explainability",
        "abstract": "Despite the widespread adoption of ensemble models, it is widely acknowledged within the ML community that they offer limited interpretability. For instance, while a single decision tree is considered interpretable, ensembles of decision trees (e.g., boosted-trees) are usually regarded as black-boxes. Although this reduced interpretability is widely acknowledged, the topic has received only limited attention from a theoretical and mathematical viewpoint.  In this work, we provide an elaborate analysis of the interpretability of ensemble models through the lens ofcomputational complexitytheory. In a nutshell, we explore different forms of explanations, and analyze whether obtaining explanations for ensembles is strictly computationally less tractable than for their constituent base models. We show that this is indeed the case for ensembles that consist of interpretable models, such as decision trees or linear models; but this is not the case for ensembles consisting of more complex models, such as neural networks. Next, we perform a fine-grained analysis using parameterized complexity to measure the impact of different problem parameters on an ensemble's interpretability. Our findings reveal that even if we shrink thesizeof all base models in an ensemble substantially, the ensemble as a whole remains intractable to interpret. However, an analysis of thenumberof base models yields a surprising dynamic --- while ensembles consisting of a limited number of decision trees can be interpreted efficiently, ensembles that consist of a small (evenconstant) number of linear models are computationally intractable to interpret."
    },
    {
        "title": "Towards Pose-Free Dynamic Neural Fields: Leveraging Geometric Foundation Models",
        "link_suffix": "/forum?id=zFfZEQHUiv",
        "link": "https://openreview.net/forum?id=zFfZEQHUiv",
        "pdf_link": "https://openreview.net/pdf?id=zFfZEQHUiv",
        "keywords": "Neural Rendering, Pose-free, Gaussian Splatting, Dynamic View Synthesis",
        "abstract": "Dynamic view synthesis (DVS) from monocular videos has remarkably advanced in recent years, achieving high-fidelity rendering with reduced computational costs. Despite these advancements, the optimization of dynamic neural fields still relies on traditional structure from motion (SfM), requiring that all objects remain stationary during scene capture. To address this limitation, we present \\textbf{SC-4DGS}, a pose-free optimization pipeline for dynamic Gaussian Splatting (GS) from monocular videos, which eliminates the need for SfM through self-calibration. Specifically, we jointly optimize dynamic Gaussian representations and camera poses by utilizing DUSt3R, enabling accurate calibration and rendering.\nFurthermore, we introduce a comprehensive benchmark, \\textbf{Kubric-MRig}, that includes extensive camera and object motions along with simultaneous multi-view captures. \nUnlike previous benchmarks for DVS, where ground truths for camera information are absent due to the difficulty of capturing multiple viewpoints simultaneously, it facilitates evaluating both calibration and rendering quality in dynamic scenes.\nExperimental results demonstrate that the proposed method outperforms previous pose-free dynamic neural fields and achieves competitive performance compared to existing pose-free 3D neural fields."
    },
    {
        "title": "Think Small, Act Big: Primitive-level Skill Prompt Learning for Lifelong Robot Manipulation",
        "link_suffix": "/forum?id=tpUEqmjZiS",
        "link": "https://openreview.net/forum?id=tpUEqmjZiS",
        "pdf_link": "https://openreview.net/pdf?id=tpUEqmjZiS",
        "keywords": "Robotics, Imitation Learning",
        "abstract": "The general-purpose robots need to continuously acquire new skills in lifelong spans without revisiting past experiences, known as Rehearsal-free Lifelong Learning, which remains significantly challenging.  Recent advances learn a separate adapter along pretrained policy for each new skill to address catastrophic forgetting problem, ignoring the shared knowledge between old skills and new ones. To tackle these issues, we propose Primitive-level Skill Prompt Learning (PSPL), to achieve lifelong robot manipulation via reusable and extensible primitives.  Within our two stage learning scheme, we first learn a set of prefix skill prompts to extract shared knowledge through multi-skills pre-training stage, where motion-aware skill prompts are learned to capture semantic and motion shared primitives across different skills.  Secondly, when acquiring new skills in lifelong span, new prefix skill prompts are added and learned via cross-attention between prefix prompts of old skills, boosting the new skills learning via shared knowledge transfer. For evaluation, we construct a large-scale skill dataset and conduct extensive experiments in both simulation and real-world tasks, demonstrating PSPL's superior performance over state-of-the-art methods. Code and dataset will be released upon acceptance."
    },
    {
        "title": "HIERARCHICAL EQUIVARIANT GRAPH GENERATION",
        "link_suffix": "/forum?id=uEqOYXtn7f",
        "link": "https://openreview.net/forum?id=uEqOYXtn7f",
        "pdf_link": "https://openreview.net/pdf?id=uEqOYXtn7f",
        "keywords": "graph, generative model, graph generation, hierarchical, coarsening, pooling, lifting, gnn, mpnn, spanning supergraph",
        "abstract": "Deep learning, and more specifically denoising models, have significantly improved graph generative modeling. However, challenges remain in capturing global graph properties from local interactions, ensuring scalability, and maintaining node permutation equivariance. While existing equivariant models address node permutation issues, they struggle with scalability, often requiring dense graph representations that scale with $\\mathcal{O}(n^2)$.To overcome these challenges, we introduce a novel coarsening-lifting method that generates sparse spanning supergraphs, preserving global graph properties. These supergraphs serve as both conditioning structures and sparse message-passing layouts for generative models. Leveraging this method with discrete diffusion, we model graphs hierarchically, enabling efficient generation of large graphs.Our approach, to the best of our knowledge, is the first hierarchical equivariant generative model for graphs. We demonstrate its performance introducing new evaluation datasets with larger graphs and more instances than traditional benchmarks."
    },
    {
        "title": "Diversity-Enhanced and Classification-Aware Prompt Learning for Few-Shot Learning via Stable Diffusion",
        "link_suffix": "/forum?id=PulKaNibeQ",
        "link": "https://openreview.net/forum?id=PulKaNibeQ",
        "pdf_link": "https://openreview.net/pdf?id=PulKaNibeQ",
        "keywords": "meta-learning, synthetic dataset generation, diffusion model",
        "abstract": "Recent text-to-image generative models have exhibited an impressive ability to generate fairly realistic images from some text prompts. In this work, we explore to leverage off-the-shelf text-to-image generative models to train non-specific downstream few-shot classification model architectures using synthetic dataset to classify real images. Current approaches use hand-crafted or model-generated text prompts of text-to-image generative models to generated desired synthetic images, however, they have limited capability of generating diversity images. \nEspecially, their synthetic datasets has relatively limited relevance to the downstream classification tasks. This makes them \nfairly hard to guarantee training models from synthetic images are efficient in practice. To address this issue, we propose a method capable of adaptively learning proper text prompts for the off-the-shelf diffusion\nmodel to generate diverse and classification-aware synthetic images. Our approach shows notable improvements in various\nclassification datasets, with results comparable to existing prompt designing methods. \nWe find that replacing data generation strategy of existing zero/few-shot methods with proposed method could consistly improves downstream classification performance across different network architectures, demostrating its model-agnostic characteristic for few-shot learning. This makes it possible to train an efficient downstream few-shot learning models from synthetic images generated by proposed method for real problems."
    },
    {
        "title": "Multi-Model Induced Source-free Video Domain Adaptation",
        "link_suffix": "/forum?id=fsmEuS5ZNg",
        "link": "https://openreview.net/forum?id=fsmEuS5ZNg",
        "pdf_link": "https://openreview.net/pdf?id=fsmEuS5ZNg",
        "keywords": "Domain Adaptation, Multi-Task Learning",
        "abstract": "Existing Source-free Video Domain Adaptation (SFVDA) aims to learn a target video model for an unlabeled target domain by transferring knowledge from a labeled source domain using a single pre-trained source video model. In this paper, we explore a new SFVDA setting where multiple source domains exist, each offering a library of source models with different architectures. This setting offers both opportunities and challenges: while the presence of multiple source models enriches the pool of transferable knowledge, it also increases the risk of negative transfer due to inappropriate source knowledge. To tackle these challenges, we introduce the Multiple-Source-Video-Model Aggregation Framework (MSVMA), comprising two key modules. The first module, termed Multi-level Instance Transferability Calibration (MITC), enhances existing uncertainty-based transferability estimation metrics by incorporating scale information from both group and dataset levels. This integration facilitates accurate transferability estimation at the instance level across diverse models. The second module, termed Instance-level Multi Video Model Aggregation (IMVMA), leverages the calculated instance-level transferability to guide a path generation network. This network produces instance-specific weights for unsupervised aggregation of source models. Empirical results from three video domain adaptation datasets demonstrate the state-of-the-art performance of our MSVMA framework."
    },
    {
        "title": "Identifying Drivers of Predictive Aleatoric Uncertainty",
        "link_suffix": "/forum?id=D9JSxF2Xhx",
        "link": "https://openreview.net/forum?id=D9JSxF2Xhx",
        "pdf_link": "https://openreview.net/pdf?id=D9JSxF2Xhx",
        "keywords": "uncertainty, explainability, trustworthy ML, probabilistic methods, transfer learning",
        "abstract": "Explainability and uncertainty quantification are two pillars of trustable artificial intelligence. However, the reasoning behind uncertainty estimates is generally left unexplained. Identifying the drivers of uncertainty complements explanations of point predictions in recognizing model limitations and enhances trust in decisions and their communication. So far, explanations of uncertainties have been rarely studied. The few exceptions rely on Bayesian neural networks or technically intricate approaches, such as auxiliary generative models, thereby hindering their broad adoption. We propose a straightforward approach to explain predictive aleatoric uncertainties. We estimate uncertainty in regression as predictive variance by adapting a neural network with a Gaussian output distribution. Subsequently, we apply out-of-the-box explainers to the model's variance output. This approach can explain uncertainty influences more reliably than more complex published approaches, which we demonstrate in a synthetic setting with a known data-generating process. We further adapt multiple metrics from conventional XAI research to uncertainty explanations. We quantify our findings with a nuanced benchmark analysis that includes real-world datasets. Finally, we apply our approach to an age regression model and discover reasonable drivers of uncertainty. Overall, the proposed straightforward method explains uncertainty estimates with little modifications to the model architecture and decisively outperforms more intricate methods."
    },
    {
        "title": "Physics-Informed Neural Predictor",
        "link_suffix": "/forum?id=vAuodZOQEZ",
        "link": "https://openreview.net/forum?id=vAuodZOQEZ",
        "pdf_link": "https://openreview.net/pdf?id=vAuodZOQEZ",
        "keywords": "Fluid dynamics, Spatiotemporal prediction, Physics-informed learning",
        "abstract": "Accurately predicting fluid dynamics and evolution has been a long-standing challenge in physical sciences. Conventional deep learning methods often rely on the nonlinear modeling capabilities of neural networks to establish mappings between past and future states, overlooking the fluid dynamics, or only modeling the velocity field, neglecting the coupling of multiple physical quantities. In this paper, we propose a new physics-informed learning approach that enables the prediction of coupled physical states, under a partially observed data environment. Central to our method lies in the discretization of physical equations, which are directly integrated into the model architecture and loss function. This integration enables the model to predict future states of observable data while simultaneously inferring and predicting hidden physical quantities (e.g., velocity and pressure) purely from visual observations. By incorporating physical equations, our model demonstrates temporal extrapolation and spatial generalization capabilities. Experimental results show that our approach achieves the state-of-the-art performance in spatiotemporal prediction across both numerical simulations and real-world extreme-precipitation nowcasting benchmarks."
    },
    {
        "title": "Freeze and Cluster: A simple baseline for Rehearsal-Free Continual Category Discovery",
        "link_suffix": "/forum?id=kfFmqu3zQm",
        "link": "https://openreview.net/forum?id=kfFmqu3zQm",
        "pdf_link": "https://openreview.net/pdf?id=kfFmqu3zQm",
        "keywords": "Novel Class Discovery, Continual Learning, Self-supervised Learning, Knowledge Transfer",
        "abstract": "This paper addresses the problem of Rehearsal-Free Continual Category Discovery (RF-CCD), which focuses on continuously identifying novel class by leveraging knowledge from labeled data. Existing methods typically train from scratch, overlooking the potential of base models, and often resort to data storage to prevent forgetting. Moreover, because RF-CCD encompasses both continual learning and novel class discovery, previous approaches have struggled to effectively integrate advanced techniques from these fields, resulting in less convincing comparisons and failing to reveal the unique challenges posed by RF-CCD. To address these challenges, we lead the way in integrating advancements from both domains and conducting extensive experiments and analyses. Our findings demonstrate that this integration can achieve state-of-the-art results, leading to the conclusion that \"in the presence of pre-trained models, the representation does not improve and may even degrade with the introduction of unlabeled data.\u201d To mitigate representation degradation, we propose a straightforward yet highly effective baseline method. This method first utilizes prior knowledge of known categories to estimate the number of novel classes. It then acquires representations using a model specifically trained on the base classes, generates high-quality pseudo-labels through k-means clustering, and trains only the classifier layer. We validate our conclusions and methods by conducting extensive experiments across multiple benchmarks, including the Stanford Cars, CUB, iNat, and Tiny-ImageNet datasets. The results clearly illustrate our findings, demonstrate the effectiveness of our baseline, and pave the way for future advancements in RF-CCD."
    },
    {
        "title": "GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation",
        "link_suffix": "/forum?id=qPzYF2EpXb",
        "link": "https://openreview.net/forum?id=qPzYF2EpXb",
        "pdf_link": "https://openreview.net/pdf?id=qPzYF2EpXb",
        "keywords": "3D Manipulation, imitation learning, foundation models, sub-goals, diffusion models",
        "abstract": "Robots' ability to follow language instructions and execute diverse 3D tasks is vital in robot learning. Traditional imitation learning-based methods perform well on seen tasks but struggle with novel, unseen ones due to variability. Recent approaches leverage large foundation models to assist in understanding novel tasks, thereby mitigating this issue. However, these methods lack a task-specific learning process, which is essential for an accurate understanding of 3D environments, often leading to execution failures. In this paper, we introduce GravMAD, a sub-goal-driven, language-conditioned action diffusion framework that combines the strengths of imitation learning and foundation models. Our approach breaks tasks into sub-goals based on language instructions, allowing auxiliary guidance during both training and inference. During training, we introduce Sub-goal Keypose Discovery to identify key sub-goals from demonstrations. Inference differs from training, as there are no demonstrations available, so we use pre-trained foundation models to bridge the gap and identify sub-goals for the current task. In both phases, GravMaps are generated from sub-goals, providing GravMAD with more flexible 3D spatial guidance compared to fixed 3D positions. Empirical evaluations on RLBench show that GravMAD significantly outperforms state-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36% gain on tasks encountered during training. These results demonstrate GravMAD's strong multi-task learning and generalization in 3D manipulation. Video demonstrations are available at:https://gravmad.github.io."
    },
    {
        "title": "Augmentation-Driven Metric for Balancing Preservation and Modification in Text-Guided Image Editing",
        "link_suffix": "/forum?id=08FCLXDY3S",
        "link": "https://openreview.net/forum?id=08FCLXDY3S",
        "pdf_link": "https://openreview.net/pdf?id=08FCLXDY3S",
        "keywords": "evaluation metric, text-guided image editing, multi-modal representation",
        "abstract": "The development of vision-language and generative models has significantly advanced text-guided image editing, which seeks \\textit{preservation} of core elements in the source image while implementing \\textit{modifications} based on the target text. However, in the absence of evaluation metrics specifically tailored for text-guided image editing, existing metrics are limited in their ability to balance the consideration of both preservation and modification. Especially, our analysis reveals that CLIPScore, the most commonly used metric, tends to favor modification, resulting in inaccurate evaluations.\nTo address this problem, we propose \\texttt{AugCLIP}, a simple yet effective evaluation metric that balances preservation and modification. \n\\texttt{AugCLIP} begins by leveraging a multi-modal large language model (MLLM) to augment detailed descriptions that encapsulate visual attributes from the source image and the target text, enabling the incorporation of richer information. Then, \\texttt{AugCLIP} estimates the modification vector that transforms the source image to align with the target text with minimum alteration as a projection into the hyperplane that separates the source and target attributes. Additionally, we account for the relative importance of each attribute considering the interdependent relationships among visual attributes. Our extensive experiments on five benchmark datasets, encompassing a diverse range of editing scenarios, demonstrate that \\texttt{AugCLIP} aligns remarkably well with human evaluation standards compared to existing metrics. The code for evaluation will be open-sourced to contribute to the community."
    },
    {
        "title": "Diversifying Spurious Subgraphs for Graph Out-of-Distribution Generalization",
        "link_suffix": "/forum?id=XWaI6FLVgi",
        "link": "https://openreview.net/forum?id=XWaI6FLVgi",
        "pdf_link": "https://openreview.net/pdf?id=XWaI6FLVgi",
        "keywords": "OOD generalization, invariant learning, graph neural networks",
        "abstract": "Environment augmentation methods have gained some success in overcoming the out-of-distribution (OOD) generalization challenge in Graph Neural Networks (GNNs). Yet, there exists a challenging trade-off in the augmentation: On one hand, it requires the generated graphs as diverse as possible to extrapolate to unseen environments. On the other hand, it requires the generated graphs to preserve the invariant substructures causally related to the targets. Existing approaches have proposed various environment augmentation strategies to enrich spurious patterns for OOD generalization. However, we argue that these methods remain limited in diversity and precision of the generated environments for two reasons: i) the deterministic nature of the graph composition strategy used for environment augmentation may limit the diversity of the generated environments, and ii) the presence of spurious correlations may lead to the exclusion of invariant subgraphs and reduce the precision of the generated environments. To address this trade-off, we propose a novel paradigm that accurately identifies spurious subgraphs, and an environment augmentation strategy called spurious subgraph diversification, which extrapolates to maximally diversified spurious subgraphs by randomizing the spurious subgraph generation, while preserving the invariant substructures.  Our method is theoretically sound and demonstrates strong empirical performance on both synthetic and real-world datasets, outperforming the second-best method by up to 24.19% across 17 baseline methods, underscoring its superiority in graph OOD generalization."
    }
]