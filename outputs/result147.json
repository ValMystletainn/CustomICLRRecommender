[
    {
        "title": "Efficient Bisection Projection to Ensure NN Solution Feasibility for Optimization over General Set",
        "link_suffix": "/forum?id=7TXdglI1g0",
        "link": "https://openreview.net/forum?id=7TXdglI1g0",
        "pdf_link": "https://openreview.net/pdf?id=7TXdglI1g0",
        "keywords": "Constrained Optimization, Neural Network, Feasibility, Bisection, Learning based Optimization",
        "abstract": "Neural networks (NNs) have shown promise in solving constrained optimization problems in real-time. However, ensuring that NN-generated solutions strictly adhere to constraints is challenging due to NN prediction errors. Recent methods have achieved feasibility guarantees over ball-homeomorphic sets with low complexity and bounded optimality loss, yet extending these guarantees to more general sets remains largely open. \nIn this paper, we developBisection Projection, an efficient approach to ensure NN solution feasibility for optimization over general compact sets with non-empty interiors, irrespective of their ball-homeomorphic properties. \nOur method begins by identifying multiple interior points (IPs) within the constraint set, chosen based on their eccentricity modulated by the NN infeasibility region. \nWe utilize another unsupervised-trained NN (called IPNN) to map inputs to these interior points, thereby reducing the complexity of computing these IPs in run-time.\nFor NN solutions initially deemed infeasible, we apply a bisection procedure that adjusts these solutions towards the identified interior points, ensuring feasibility with minor projection-induced optimality loss. We prove the feasibility guarantee and bound the optimality loss of our approach under mild conditions. \nExtensive simulations, including non-convex optimal power flow problems in large-scale networks, demonstrate that bisection projection outperforms existing methods in solution feasibility and computational efficiency with comparable optimality losses."
    },
    {
        "title": "Unraveling the Shift of Visual Information Flow in MLLMs: From Phased Interaction to Efficient Inference",
        "link_suffix": "/forum?id=0eRJRbVG95",
        "link": "https://openreview.net/forum?id=0eRJRbVG95",
        "pdf_link": "https://openreview.net/pdf?id=0eRJRbVG95",
        "keywords": "Multimodal Large Language Models, Visual Information Flow, Inference Acceleration",
        "abstract": "Multimodal large language models (MLLMs) improve performance on vision-language tasks by integrating visual features from pre-trained vision encoders into large language models (LLMs). However, how MLLMs process and utilize visual information remains unclear. In this paper, a shift in the dominant flow of visual information is uncovered: (1) in shallow layers, strong interactions are observed between image tokens and instruction tokens, where most visual information is injected into instruction tokens to form cross-modal semantic representations; (2) in deeper layers, image tokens primarily interact with each other, aggregating the remaining visual information to optimize semantic representations within the visual modality. Based on these insights, we propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference acceleration method that dynamically prunes image tokens at specific layers, reducing computational costs by approximately 65% without sacrificing performance. Our findings offer a new understanding of visual information processing in MLLMs and provide a state-of-the-art solution for efficient inference. Code is released athttps://anonymous.4open.science/r/HiMAP."
    },
    {
        "title": "Representational Similarity via Interpretable Visual Concepts",
        "link_suffix": "/forum?id=ih3BJmIZbC",
        "link": "https://openreview.net/forum?id=ih3BJmIZbC",
        "pdf_link": "https://openreview.net/pdf?id=ih3BJmIZbC",
        "keywords": "representational alignment, representational similarity, explainability, interpretability, comparison",
        "abstract": "How do two deep neural networks differ in how they arrive at a decision? Measuring deep network similarity has been a long-standing open question. Most existing metrics provide a single number to measure the similarity of two networks at a given layer, but give no insight into what makes them similar or dissimilar. We introduce an interpretable representational similarity method to compare two networks. We use it to compare representations in models that differ in size, architecture, and training paradigm and discover shared and unique concepts between models. Our work offers a new window into how deep networks represent images, as well as a practical tool to discover the potential cause of model failures."
    },
    {
        "title": "Cost-Effective Online Multi-LLM Selection with Versatile Reward Models",
        "link_suffix": "/forum?id=JLDAWbzTUg",
        "link": "https://openreview.net/forum?id=JLDAWbzTUg",
        "pdf_link": "https://openreview.net/pdf?id=JLDAWbzTUg",
        "keywords": "Combinatorial multi-armed bandit, Online learning, Large language model, Long-term cost constraint",
        "abstract": "With the rapid advancement of large language models (LLMs),  the diversity of multi-LLM tasks and the variability in their pricing structures have become increasingly important, as costs can vary greatly between different LLMs. To tackle these challenges, we introduce the \\textit{C2MAB-V}, a \\underline{C}ost-effective \\underline{C}ombinatorial \\underline{M}ulti-armed \\underline{B}andit with \\underline{V}ersatile reward models for optimal LLM selection and usage.  This online model differs from traditional static approaches or those reliant on a single LLM without cost consideration. With multiple LLMs deployed on a scheduling cloud and a local server dedicated to handling user queries, \\textit{C2MAB-V} facilitates the selection of multiple LLMs over a combinatorial search space, specifically tailored for various collaborative task types with different reward models. Based on our designed online feedback mechanism and confidence bound technique, \\textit{C2MAB-V} can effectively address the multi-LLM selection challenge by managing the exploration-exploitation trade-off across different models, while also balancing cost and reward for diverse tasks. The NP-hard integer linear programming problem for selecting multiple LLMs with trade-off dilemmas is addressed by: i) decomposing the integer problem into a relaxed form by the local server, ii) utilizing a discretization rounding scheme that provides optimal LLM combinations by the scheduling cloud, and iii) continual online updates based on feedback. Theoretically, we prove that \\textit{C2MAB-V} offers strict guarantees over versatile reward models,  matching state-of-the-art results for regret and violations in some degenerate cases. Empirically, we show that \\textit{C2MAB-V} effectively balances performance and cost-efficiency with nine LLMs for three application scenarios."
    },
    {
        "title": "Multi-View Graph Neural Networks with Language Models for Mutli-Source Recommender Systems",
        "link_suffix": "/forum?id=kMCRuP2X8t",
        "link": "https://openreview.net/forum?id=kMCRuP2X8t",
        "pdf_link": "https://openreview.net/pdf?id=kMCRuP2X8t",
        "keywords": "Graph Neural Networks, Contrastive Learning, Self-Supervised Learning, Language Models, Social trust, Textual Reviews, Recommender Systems",
        "abstract": "Graph Neural Networks (GNNs) have become increasingly popular in recommender systems due to their ability to model complex user-item relationships. However, current GNN-based approaches face several challenges: They primarily rely on sparse user-item interaction data, which can lead to overfitting and limit generalization performance. Moreover, they often overlook additional valuable information sources, such as social trust and user reviews, which can provide deeper insights into user preferences and enhance recommendation accuracy. To address these limitations, we propose a multi-view GNN framework that integrates diverse information sources using contrastive learning and language models. Our method employs a lightweight Graph Convolutional Network (LightGCN) on user-item interactions to generate initial user and item representations. We use an attention mechanism for the user view to integrate social trust information with user-generated textual reviews, which are transformed into high-dimensional vectors using a pre-trained language model. Similarly, we aggregate all reviews associated with each item and use language models to generate item representations for the item view. We then construct an item graph by applying a meta-path to the user-item interactions. GCNs are applied to both the social trust network and the item graph, generating enriched embeddings for users and items. To align and unify these heterogeneous data sources, we employ a contrastive learning mechanism that ensures consistent and complementary representations across different views. Experimental results on multiple real-world datasets such as Epinions, Yelp, and Ciao demonstrate significant performance improvements over state-of-the-art methods."
    },
    {
        "title": "Retraction-free optimization over the Stiefel manifold with application to the LoRA fine-tuning",
        "link_suffix": "/forum?id=c2OtbtZXFC",
        "link": "https://openreview.net/forum?id=c2OtbtZXFC",
        "pdf_link": "https://openreview.net/pdf?id=c2OtbtZXFC",
        "keywords": "landing, manifold, fine-tuning, LoRA",
        "abstract": "Optimization over the Stiefel manifold has played a significant role in various machine learning tasks. Many existing algorithms either use the retraction operator to keep each iterate staying on the manifold, or solve an unconstrained quadratic penalized problem. The retraction operator in the former corresponds to orthonormalization of matrices and can be computationally costly for large-scale matrices. The latter approach usually equips with an unknown large penalty parameter. To address the above issues, we propose a retraction-free and penalty parameter-free algorithm, which lands on the manifold. Moreover, our convergence theory allows using constant step size, which improve the result of converging to a neighborhood in \\citep{ablin2022fast}.\n  A key component of the analysis is the convex-like property of the quadratic penalty of the Stiefel manifold, which enables us to explicitly characterize the constant penalty parameter. As an application, we introduce a new algorithm, Manifold-LoRA, which employs the landing technique and a carefully designed step size strategy to accelerate low-rank adaptation (LoRA) in fine-tuning large language models. Numerical experiments on the benchmark datasets demonstrate the efficiency of our proposed method."
    },
    {
        "title": "Discrimination for Generation",
        "link_suffix": "/forum?id=XcFJZORJgY",
        "link": "https://openreview.net/forum?id=XcFJZORJgY",
        "pdf_link": "https://openreview.net/pdf?id=XcFJZORJgY",
        "keywords": "Generative model, Discriminative model, Neural Tangent Kernel, Functional Analysis",
        "abstract": "There are two primary approaches to learning from data: discriminative models, which make predictions based on provided data, and generative models, which learn data distributions to create new instances. This paper introduces a novel framework, Discrimination for Generation (DFG), as the first attempt to bridge the gap between discriminative and generative models. Through DFG, discriminative models can function as generative models. We leverage the Neural Tangent Kernel (NTK) to map discriminative models into a connected functional space, enabling the calculation of the distance between the data manifold and a sampled data point.\nOur experimental results demonstrate that the proposed algorithm can generate high-fidelity images and can be applied to various tasks such as Targeted Editing and Inpainting, in addition to both unconditional and conditional image generation.\nThis connection provides a novel perspective for interpreting models. Moreover, our method is algorithm-, architecture-, and dataset-agnostic, offering flexibility and proving to be a robust technique across a wide range of scenarios."
    },
    {
        "title": "Optimizing Activations Beyond Entropy Minimization for Test-Time Adaptation of Graph Neural Networks",
        "link_suffix": "/forum?id=vhazhSm6I0",
        "link": "https://openreview.net/forum?id=vhazhSm6I0",
        "pdf_link": "https://openreview.net/pdf?id=vhazhSm6I0",
        "keywords": "test-time adaptation, batch normalization, graph neural network, energy-based model",
        "abstract": "Test-time adaptation for classification models involves optimizing classifiers through self-supervised learning without labeled training samples. Existing methods often rely on entropy minimization as the optimization objective, which\u00a0indeed\u00a0addresses the model performance connections with prediction confidence or representations amenable to cluster structure.\u00a0However, due to the lack of ground truth in training samples, test-time adaptation, as an effective way to deal with the shifting dataset distributions or domains, can sometimes lead to model collapse. In this paper, we focus on optimizing activations in batch normalization (BN) layers for test-time adaptation of graph neural networks (GNNs). Unlike many entropy minimization methods prone to catastrophic model collapse, our approach leverages pseudo-labels of test samples to mitigate the potential forgetting of training data. \nWe optimize activations in BN by a two-step process.\u00a0First, we identify weights and masks for the empirical batch mean and variance\u00a0of both training and test samples.\u00a0Subsequently, we refine\u00a0BN's\u00a0scale and shift parameters using a reformulated loss function with an energy-based model for improved generalization.\u00a0Empirical evaluation across seven challenging datasets demonstrates the superior performance of our method compared to state-of-the-art test-time adaptation approaches."
    },
    {
        "title": "DRoC: Elevating Large Language Models for Complex Vehicle Routing via Decomposed Retrieval of Constraints",
        "link_suffix": "/forum?id=s9zoyICZ4k",
        "link": "https://openreview.net/forum?id=s9zoyICZ4k",
        "pdf_link": "https://openreview.net/pdf?id=s9zoyICZ4k",
        "keywords": "vehicle routing problems; large language model; retrieval augmented generation",
        "abstract": "This paper proposes Decomposed Retrieval of Constraints (DRoC), a novel framework aimed at enhancing large language models (LLMs) in exploiting solvers to tackle vehicle routing problems (VRPs) with intricate constraints. While LLMs have shown promise in solving simple VRPs, their potential in addressing complex VRP variants is still suppressed, due to the limited embedded internal knowledge that is required to accurately reflect diverse VRP constraints. Our DRoC framework mitigates the issue by integrating external knowledge via a novel retrieval-augmented generation (RAG) approach. More specifically, the DRoC decomposes VRP constraints, externally retrieves information relevant to each constraint, and synergistically combines internal and external knowledge to benefit the program generation for solving VRPs. The DRoC also allows LLMs to dynamically select between RAG and self-debugging mechanisms, thereby optimizing program generation without the need for additional training. Experiments across 48 VRP variants exhibit the superiority of DRoC, with significant improvements in the success rate and optimality gap delivered by the generated programs. The DRoC framework has the potential to elevate LLM performance in complex optimization tasks, fostering the applicability of LLMs in industries such as transportation and logistics."
    },
    {
        "title": "Geo-3DGS: Multi-view Geometry Consistency for 3D Gaussian Splatting and Surface Reconstruction",
        "link_suffix": "/forum?id=c4Nh4A8Xn5",
        "link": "https://openreview.net/forum?id=c4Nh4A8Xn5",
        "pdf_link": "https://openreview.net/pdf?id=c4Nh4A8Xn5",
        "keywords": "Gaussian Splatting, Multi-view geometry consistency, Surface reconstruction, Novel view synthesis",
        "abstract": "Recently, the emergence of 3D Gaussian Splatting (3DGS) has made real-time and high-quality rendering possible. However, it is still challenging for 3DGS to reconstruct accurate geometry surfaces and achieve higher-quality rendering. To address these challenges, we propose to leverage multi-view geometry consistency for 3DGS and surface reconstruction. We reveal that there exists multi-view geometry inconsistency in 3DGS, preventing 3DGS from achieving higher-quality rendering and accurate surface reconstruction. To mitigate the geometry inconsistency, we first develop a multi-view photometric consistency regularization to constrain the rendered depth of 3DGS, which helps establish more stable and consistent 3D Gaussians to facilitate both rendering and surface reconstruction. To reconstruct geometry surfaces from 3DGS, we introduce a neural Signed Distance Function (SDF) field to represent continuous geometries of 3DGS. Then, we propose a geometry consistency-based SDF learning strategy, which leverages multi-view geometry consistency cues from 3DGS to efficiently optimize the SDF field for surface reconstruction. Extensive experiments on various datasets demonstrate that our method achieves both high-quality rendering and accurate surface reconstruction while keeping a good efficiency. Our code will be released upon publication."
    },
    {
        "title": "Text as parameter: interactive prompt optimisation for large language models",
        "link_suffix": "/forum?id=8y7R2pdCl7",
        "link": "https://openreview.net/forum?id=8y7R2pdCl7",
        "pdf_link": "https://openreview.net/pdf?id=8y7R2pdCl7",
        "keywords": "Large language model, prompt optimisation, dialogue",
        "abstract": "Large language models (LLMs) can handle a variety of tasks conditioned on natural language instructions. While fine-tuning improves task-specific performance, adjusting the model weights of LLMs requires a huge amount of computational resources, and it is impractical for real-time updates. Alternatively, prompting allows LLMs to adapt to a broad range of tasks without the need for computationally intensive gradient-based optimisation. \nHowever, crafting effective prompts remains a challenge, to the extent that it is even unclear if expert in-domain knowledge is what is needed or experience in writing prompts or something else. \nApproaches like meta-prompting and self-feedback seek to alleviate this burden, but they rely primarily on a numerical feedback signal, leaving the potential of textual feedback unexplored. \nThese methods also typically require numerous interactions with the environment to gather sufficient context, leading to significant computational overhead.In this work, we propose a novel framework that takes a prompted large language model as an optimiser and treats the text-based prompt itself as a parameter. \nBy interacting with the environment to collect feedback, our proposed method constructs the updated textual prompt. \nOur experimental results demonstrate that this method not only achieves superior performance but also automatically incorporates domain-specific knowledge, establishing a scientifically motivated, practical and efficient approach to prompting for future research."
    },
    {
        "title": "Pan for gold",
        "link_suffix": "/forum?id=1gqR7yEqnP",
        "link": "https://openreview.net/forum?id=1gqR7yEqnP",
        "pdf_link": "https://openreview.net/pdf?id=1gqR7yEqnP",
        "keywords": "Generalization, Overparameterized Network, functional analysis, Domain Adaptation",
        "abstract": "Training a deep model is fundamentally about reducing loss, and we often believe that a ''good model'' is one that trained with a ''good loss.'' This paper investigates that belief. We show that even when learning with unstructured, randomized labels, models can still discover generalized features. We propose that generalization in deep learning is not about learning the structure of data through a well-structured loss, but rather a process akin to ''pan for gold,'' where gradient descent shakes through the function space, naturally stabilizing useful features. To support this, we present quantitative and qualitative experimental evidence, and introduce the Panning through Unstructured Label (PUL) algorithm. We demonstrate its effectiveness across various fields, showing improvements in unsupervised domain adaptation, state-of-the-art performance in object discovery, and its ability to mitigate massive attention issues. Finally, we offer a new interpretation of existing deep learning assumptions, challenging the conventional beliefs in the field."
    },
    {
        "title": "Topological Schr\u00f6dinger Bridge Matching",
        "link_suffix": "/forum?id=WzCEiBILHu",
        "link": "https://openreview.net/forum?id=WzCEiBILHu",
        "pdf_link": "https://openreview.net/pdf?id=WzCEiBILHu",
        "keywords": "Topological Signals, Schr\u00f6dinger Bridge, Gaussian Schr\u00f6dinger Bridge, Bridge Matching",
        "abstract": "Given two boundary distributions, the \\emph{Schr\u00f6dinger Bridge} (SB) problem seeks the \u201cmost likely\u201d random evolution between them with respect to a reference process. \nIt has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. \nWhile these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows.\nIn this work, we propose the \\emph{Topological Schr\u00f6dinger Bridge problem} ($\\mathcal{T}$SBP) for matching signal distributions on a topological domain. \nWe set the reference process to follow some linear tractable \\emph{topology-aware} stochastic dynamics such as topological heat diffusion. \nFor the case of Gaussian boundary distributions, we derive a \\emph{closed-form} topological SB ($\\mathcal{T}$SB) in terms of its time-marginal and stochastic differential. \nIn the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns.\nBuilding on these results, we develop $\\mathcal{T}$SB-based models for matching topological signals by parameterizing the unknowns in the optimal process as \\emph{(topological) neural networks} and learning them through \\emph{likelihood training}. We validate the theoretical results and demonstrate the practical applications of $\\mathcal{T}$SB-based models on both synthetic and real-world networks, emphasizing the role of topology. \nAdditionally, we discuss the connections of $\\mathcal{T}$SB-based models to other emerging models, and outline future directions for topological signal matching."
    },
    {
        "title": "On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning",
        "link_suffix": "/forum?id=eUEMjwh5wK",
        "link": "https://openreview.net/forum?id=eUEMjwh5wK",
        "pdf_link": "https://openreview.net/pdf?id=eUEMjwh5wK",
        "keywords": "Reinforcement learning, robust reinforcement learning, adversarial robustness, partially observable markov decision problems",
        "abstract": "Deep Reinforcement Learning (DRL) policies are critically vulnerable to adversarial noise in observations, posing severe risks in safety-critical scenarios. For example, a self-driving car receiving manipulated sensory inputs about traffic signs could lead to catastrophic outcomes. Existing strategies to fortify RL algorithms against such adversarial perturbations generally fall into two categories: (a) using regularization methods that enhance robustness by incorporating adversarial loss terms into the value objectives, and (b) adopting \"maximin\" principles, which focus on maximizing the minimum value to ensure robustness. While regularization methods reduce the likelihood of successful attacks, their effectiveness drops significantly if an attack does succeed. On the other hand, maximin objectives, although robust, tend to be overly conservative. To address this challenge, we introduce a novel objective called Adversarial Counterfactual Error (ACoE), which naturally balances optimizing value and robustness against adversarial attacks. To optimize ACoE in a scalable manner in model-free settings, we propose a theoretically justified surrogate objective known as Cumulative-ACoE (C-ACoE). The core idea of optimizing C-ACoE is utilizing the belief about the underlying true state given the adversarially perturbed observation. Our empirical evaluations demonstrate that our method outperforms current state-of-the-art approaches for addressing adversarial RL problems across all established benchmarks (MuJoCo, Atari, and Highway) used in the literature."
    },
    {
        "title": "Human-Instruction-Free LLM Self-Alignment with Limited Samples",
        "link_suffix": "/forum?id=ZYUR3HVSAT",
        "link": "https://openreview.net/forum?id=ZYUR3HVSAT",
        "pdf_link": "https://openreview.net/pdf?id=ZYUR3HVSAT",
        "keywords": "LLM, Self-Alignment, ICL",
        "abstract": "Aligning large language models (LLMs) with human values is a vital task for LLM practitioners. Current alignment techniques have several limitations: (1) requiring a large amount of annotated data; (2) demanding heavy human involvement; (3) lacking a systematic mechanism to continuously improve. In this work, we study aligning LLMs to a new domain with limited samples (e.g. < 100). We propose an algorithm that can \\textit{self-align} LLMs \\textit{iteratively} without active human involvement. Unlike existing works, our algorithm relies on neither human-crafted instructions nor labeled rewards, significantly reducing human involvement. In addition, our algorithm can self-improve the alignment continuously. The key idea is to first retrieve high-quality samples related to the target domain and use them as In-context Learning examples to generate more samples. Then we use the \\textit{self-generated} samples to finetune the LLM iteratively. We show that our method can unlock the LLMs' self-generalization ability to perform alignment with near-zero human supervision. We test our algorithm on three benchmarks in safety, truthfulness, and instruction-following, and show good performance in alignment, domain adaptability, and scalability."
    },
    {
        "title": "On the Design and Analysis of LLM-Based Algorithms",
        "link_suffix": "/forum?id=xFezgECSLa",
        "link": "https://openreview.net/forum?id=xFezgECSLa",
        "pdf_link": "https://openreview.net/pdf?id=xFezgECSLa",
        "keywords": "Large Language Models, Compound AI Systems, Algorithm Design and Analysis, Analytical Framework",
        "abstract": "We initiate a formal investigation into the design and analysis of LLM-based algorithms, i.e. algorithms that contain one or multiple calls of large language models (LLMs) as sub-routines and critically rely on the capabilities of LLMs. While LLM-based algorithms, ranging from combinations of basic LLM calls to complicated LLM-powered agent systems and compound AI systems, have achieved remarkable empirical success, the design and optimization of them have oftentimes relied on heuristics and trial-and-errors, which is largely due to a lack of formal and analytical study for these algorithms. To fill this gap, we start by identifying the computational-graph representation, \ntask decomposition as the design principle, and some key abstractions, which then facilitate our formal analysis for the accuracy and efficiency of LLM-based algorithms, despite the black-box nature of LLMs. Through extensive analytical and empirical investigation in a series of case studies, we demonstrate that the proposed framework is broadly applicable to a wide range of scenarios and diverse patterns of LLM-based algorithms, such as parallel, hierarchical and recursive task decomposition. Our proposed framework holds promise for advancing LLM-based algorithms, by revealing the reasons behind curious empirical phenomena, guiding the choices of hyperparameters, predicting the empirical performance of algorithms, and inspiring new algorithm design. To promote further study, we include our source code in the supplementary materials."
    },
    {
        "title": "ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language Models",
        "link_suffix": "/forum?id=J1J5eGJsKZ",
        "link": "https://openreview.net/forum?id=J1J5eGJsKZ",
        "pdf_link": "https://openreview.net/pdf?id=J1J5eGJsKZ",
        "keywords": "Tool-augmented language models",
        "abstract": "Tool-Augmented Language Models (TALMs) leverage external APIs to answer user queries across various domains. However, existing benchmark datasets for TALM research often feature simplistic dialogues that do not reflect real-world scenarios, such as the need for models to ask clarifying questions or proactively call additional APIs when essential information is missing. To address these limitations, we construct and release ToolDial, a dataset comprising 11,111 multi-turn dialogues, with an average of 8.95 turns per dialogue, based on APIs from RapidAPI. ToolDial has two key characteristics. First, the dialogues incorporate 16 user and system actions (e.g., request, clarify, fail inform) to capture the rich dynamics of real-world interactions. Second, we simulate dialogues where the system requests necessary information from the user based on API documentation and seeks additional APIs if the user fails to provide the required information. To facilitate this process, we introduce a method for generating an API graph that represents input and output compatibility between APIs. Using ToolDial, we evaluate a suite of language models on their ability to predict correct actions and extract input parameter values for API calls from the dialogue history. Modern language models achieve accuracy scores below 70%, indicating substantial room for improvement. We provide a detailed analysis of the areas where these models fall short."
    },
    {
        "title": "Leveraging Discrete Structural Information for Molecule-Text Modeling",
        "link_suffix": "/forum?id=eGqQyTAbXC",
        "link": "https://openreview.net/forum?id=eGqQyTAbXC",
        "pdf_link": "https://openreview.net/pdf?id=eGqQyTAbXC",
        "keywords": "molecule-text modeling, 3D molecular tokenization, language model",
        "abstract": "The integration of molecular and natural language representations has emerged as a focal point in molecular science, with recent advancements in Language Models (LMs) demonstrating significant potential for comprehensive modeling of both domains. However, existing approaches face notable limitations, particularly in their neglect of three-dimensional (3D) information, which is crucial for understanding molecular structures and functions. While some efforts have been made to incorporate 3D molecular information into LMs using external structure encoding modules, significant difficulties remain, such as insufficient interaction across modalities in pre-training and challenges in modality alignment. To address the limitations, we propose \\textbf{3D-MolT5}, a unified framework designed to model molecule in both sequence and 3D structure spaces. The key innovation of our approach lies in mapping fine-grained 3D substructure representations into a specialized 3D token vocabulary. This methodology facilitates the seamless integration of sequence and structure representations in a tokenized format, enabling 3D-MolT5 to encode molecular sequences, molecular structures, and text sequences within a unified architecture. Leveraging this tokenized input strategy, we build a foundation model that unifies the sequence and structure data formats. We then conduct joint pre-training with multi-task objectives to enhance the model's comprehension of these diverse modalities within a shared representation space. Thus, our approach significantly improves cross-modal interaction and alignment, addressing key challenges in previous work. Further instruction tuning demonstrated that our 3D-MolT5 has strong generalization ability and surpasses existing methods with superior performance in multiple downstream tasks, such as nearly 70% improvement on molecular property prediction task compared to state-of-the-art methods. Our code is available at \\url{https://anonymous.4open.science/r/3D-MolT5-ICLR2025}."
    },
    {
        "title": "Unlearning Mapping Attack: Exposing Hidden Vulnerabilities in Machine Unlearning",
        "link_suffix": "/forum?id=KvFk356RpR",
        "link": "https://openreview.net/forum?id=KvFk356RpR",
        "pdf_link": "https://openreview.net/pdf?id=KvFk356RpR",
        "keywords": "Machine Unlearning, Deep Learning, Machine Learning Security",
        "abstract": "As machine learning becomes increasingly data-dependent, concerns over privacy and content regulation among data owners have intensified. Machine Unlearning has emerged as a promising solution, allowing for the removal of specific data from pre-trained systems to protect user privacy and regulate information. Existing research on Machine Unlearning has shown considerable success in eliminating the influence of certain data while preserving model performance. However, the resilience of Machine Unlearning to malicious attacks has not been thoroughly examined. In this paper, we investigate the hidden vulnerabilities within current Machine Unlearning techniques. We propose a novel adversarial attack, the Unlearning Mapping Attack (UMA), capable of undermining the unlearning process without altering its procedures. Through experiments on both generative and discriminative tasks, we demonstrate the susceptibility of existing unlearning techniques to UMA. These findings highlight the need to reassess unlearning objectives across various tasks, prompting the introduction of a Robust Unlearning standard that prioritizes protection against adversarial threats. Our extensive studies show the successful adaptation of current unlearning methods to this robust framework. The Python implementation will be made publicly available upon acceptance of the paper."
    },
    {
        "title": "Fair Clustering via Alignment",
        "link_suffix": "/forum?id=QibJggOAnB",
        "link": "https://openreview.net/forum?id=QibJggOAnB",
        "pdf_link": "https://openreview.net/pdf?id=QibJggOAnB",
        "keywords": "Clustering, Fairness, Trustworthy AI",
        "abstract": "Algorithmic fairness in clustering aims to balance the proportions of instances assigned to each cluster with respect to a given sensitive attribute.\nRecently, numerous algorithms have been developed for Fair Clustering (FC), most of which optimize a clustering objective under specifically designed fairness constraints.\nHowever, the inherent complexity or approximation of constrained optimization problems makes it challenging to achieve the optimal trade-off between fairness level and clustering utility in practice.\nFor example, the obtained clustering utility by an existing FC algorithm might be suboptimal, or achieving a certain fairness level could be numerically unstable.\nTo resolve these limitations, we propose a new FC algorithm based on a novel decomposition of the fair $K$-means clustering objective function.\nThe proposed algorithm, called Fair Clustering via Alignment (FCA), operates by (i) finding a joint probability distribution to align the data from different protected groups, and (ii) optimizing cluster centers in the aligned space.\nA key advantage of FCA is that it guarantees optimal clustering utility for any given fairness level while avoiding the need to solve complex constrained optimization problems, thereby obtaining optimal fair clustering in practice.\nExperiments show that FCA offers several empirical benefits over existing methods such as (i) attaining the optimal trade-off between fairness level and clustering utility, and (ii) achieving near-perfect fairness level without numerical instability."
    },
    {
        "title": "Towards Fair RAG: On the Impact of Fair Ranking in Retrieval-Augmented Generation",
        "link_suffix": "/forum?id=7X3fi8aJBL",
        "link": "https://openreview.net/forum?id=7X3fi8aJBL",
        "pdf_link": "https://openreview.net/pdf?id=7X3fi8aJBL",
        "keywords": "Fairness, Ranking, Retrieval, Retrieval-Augmented Generation, RAG",
        "abstract": "Many language models now enhance their responses with retrieval capabilities, leading to the widespread adoption of retrieval-augmented generation (RAG) systems. However, despite retrieval being a core component of RAG, much of the research in this area overlooks the extensive body of work on fair ranking, neglecting the importance of considering all stakeholders involved. This paper presents the first systematic evaluation of RAG systems integrated with fair rankings. We focus specifically on measuring the fair exposure of each relevant item across the rankings utilized by RAG systems (i.e., item-side fairness), aiming to promote equitable growth for relevant item providers. To gain a deep understanding of the relationship between item-fairness, ranking quality, and generation quality in the context of RAG, we analyze nine different RAG systems that incorporate fair rankings across seven distinct datasets. Our findings indicate that RAG systems with fair rankings can maintain a high level of generation quality and, in many cases, even outperform traditional RAG systems, despite the general trend of a tradeoff between ensuring fairness and maintaining system-effectiveness. We believe our insights lay the groundwork for responsible and equitable RAG systems and open new avenues for future research. We publicly release our codebase and dataset."
    },
    {
        "title": "ParFam -- (Neural Guided) Symbolic Regression via Continuous Global Optimization",
        "link_suffix": "/forum?id=8y5Uf6oEiB",
        "link": "https://openreview.net/forum?id=8y5Uf6oEiB",
        "pdf_link": "https://openreview.net/pdf?id=8y5Uf6oEiB",
        "keywords": "symbolic regression, continuous optimization, expressivity, transformers, supervised learning",
        "abstract": "The problem of symbolic regression (SR) arises in many different applications, such as identifying physical laws or deriving mathematical equations describing the behavior of financial markets from given data. Various methods exist to address the problem of SR, often based on genetic programming. However, these methods are usually complicated and involve various hyperparameters. In this paper, we present our new approach ParFam that utilizes parametric families of suitable symbolic functions to translate the discrete symbolic regression problem into a continuous one, resulting in a more straightforward setup compared to current state-of-the-art methods. In combination with a global optimizer, this approach results in a highly effective method to tackle the problem of SR. We theoretically analyze the expressivity of ParFam and demonstrate its performance with extensive numerical experiments based on the common SR benchmark suit SRBench, showing that we achieve state-of-the-art results. Moreover, we present an extension incorporating a pre-trained transformer network (DL-ParFam) to guide ParFam, accelerating the optimization process by up to two magnitudes. Our code and results can be found athttps://anonymous.4open.science/r/parfam-D402."
    },
    {
        "title": "Anomaly Detection by Context Contrasting",
        "link_suffix": "/forum?id=isHiGhFwVV",
        "link": "https://openreview.net/forum?id=isHiGhFwVV",
        "pdf_link": "https://openreview.net/pdf?id=isHiGhFwVV",
        "keywords": "anomaly detection, one class classification, novelty detection, contrastive representation learning, context clustering",
        "abstract": "Anomaly detection focuses on identifying samples that deviate from the norm.\nWhen working with high-dimensional data such as images, a crucial requirement for detecting anomalous patterns is learning lower-dimensional representations that capture concepts of normality. \nRecent advances in self-supervised learning have shown great promise in this regard. \nHowever, many successful self-supervised anomaly detection methods assume prior knowledge about anomalies to create synthetic outliers during training. \nYet, in real-world applications, we often do not know what to expect from unseen data, and we can solely leverage knowledge about normal data. \nIn this work, we propose Con$_2$, which learns representations through context augmentations that allow us to observe samples from\ntwo distinct perspectives while keeping the invariances of normal data. \nCon$_2$ learns rich representations of context-augmented samples by clustering them according to their context while simultaneously \naligning their positions across clusters. \nAt test time, representations of anomalies that do not adhere to the invariances of normal data then deviate from their respective context cluster. \nLearning representations in such a way thus allows us to detect anomalies without making\nassumptions about anomalous data."
    },
    {
        "title": "Taming Transformer Without Using Learning Rate Warmup",
        "link_suffix": "/forum?id=GeUK3zGreN",
        "link": "https://openreview.net/forum?id=GeUK3zGreN",
        "pdf_link": "https://openreview.net/pdf?id=GeUK3zGreN",
        "keywords": "Transformer, Training Dynamics, Model Crash",
        "abstract": "Scaling Transformer to a large scale without using some technical tricks  such as learning rate warump and an obviously lower learning rate, is an extremely challenging task, and is increasingly gaining more attention. In this paper, we provide a theoretical analysis for training Transformer and reveal a key problem behind the model crash phenomenon in the training, \\ie the \\textit{spectral energy concentration} of ${W_q}^{\\top} W_k$, which is the reason for a malignant entropy collapse. To remedy this problem, motivated by \\textit{Weyl's Inequality}, we present a novel optimization strategy---making weight updating in successive steps smooth, that is, if the ratio $\\frac{\\sigma_{1}(\\nabla W_t)}{\\sigma_{1}(W_{t-1})}$ is larger than a threshold, where $\\nabla \\bW_t$ is the updating quantity in step $t$, we will automatically bound the learning rate to a weighted multiply of $\\frac{\\sigma_{1}(W_{t-1})}{\\sigma_{1}(\\nabla W_t)}$. Our optimization strategy is able to prevent the rapid spectral energy concentration to only a few directions, and thus is able to avoid the malignant entropy collapse that will trigger the model crash. We conduct extensive experiments using ViT, Swin-Transformer and GPT, showing that our optimization strategy can effectively and stably train these (Transformer) models without using learning rate warmup."
    },
    {
        "title": "Redefining the task of Bioactivity Prediction",
        "link_suffix": "/forum?id=S8gbnkCgxZ",
        "link": "https://openreview.net/forum?id=S8gbnkCgxZ",
        "pdf_link": "https://openreview.net/pdf?id=S8gbnkCgxZ",
        "keywords": "Bioactivity Prediction, New Dataset",
        "abstract": "Small molecules are vital to modern medicine, and accurately predicting their bioactivity against protein targets is crucial for therapeutic discovery and development. However, current machine learning models often rely on spurious features, leading to biased outcomes. Notably, a simple pocket-only baseline can achieve results comparable to, and sometimes better than, more complex models that incorporate both the protein pockets and the small molecules. Our analysis reveals that this phenomenon arises from insufficient training data and an improper evaluation process, which is typically conducted at the pocket level rather than the small molecule level. To address these issues, we redefine the bioactivity prediction task by introducing a million-scale structural small molecule-protein interaction dataset for unbiased bioactivity prediction task (SIU), which is 50 times larger than the widely used PDBbind. The bioactivity labels in SIU are derived from wet experiments and organized by assay types, ensuring greater accuracy and comparability. The complexes in SIU are constructed using a majority vote from three commonly used docking software programs, enhancing their reliability. Additionally, the structure of SIU allows for multiple small molecules to be associated with each protein pocket, enabling the redefinition of evaluation metrics like Pearson and Spearman correlations across different small molecules targeting the same protein pocket. Experimental results demonstrate that this new task provides a more challenging and meaningful benchmark for training and evaluating bioactivity prediction models, ultimately offering a more robust assessment of model performance."
    }
]