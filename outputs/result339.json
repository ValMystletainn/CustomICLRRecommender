[{"title": "CodePMP: Scalable Preference Model Pretraining for Large Language Model Reasoning", "link_suffix": "/forum?id=tpVQHb4pea", "link": "https://openreview.net/forum?id=tpVQHb4pea", "pdf_link": "https://openreview.net/pdf?id=tpVQHb4pea", "keywords": "Preference Model Pretraining", "abstract": "Large language models (LLMs) have achieved notable advancements in natural language understanding and generation, driven by scalable pretraining and advanced finetuning techniques. However, improving reasoning abilities in LLMs, particularly through reinforcement learning from human feedback (RLHF), remains a challenge due to the scarcity of high-quality preference data, which is often labor-intensive to annotate and essential for reward model (RM) finetuning. To alleviate this issue, we introduce CodePMP, a scalable preference model pretraining (PMP) pipeline that leverages vast amounts of code-preference pairs synthesized from publicly available, high-quality source code. CodePMP improves the sample efficiency of RM finetuning by sufficiently pre-training preference models on synthesized code-preference pairs. In addition to validating CodePMP on widely used mathematical reasoning tasks (GSM8K, MATH), we also demonstrate its effectiveness on logical reasoning benchmarks (ReClor, LogiQA). The results consistently indicate that CodePMP significantly improves the reasoning performance of large language models (LLMs). Furthermore, our findings underscore the critical role of scalable preference model pretraining (PMP) in achieving efficient reward modeling.", "title_embedding_index": 16900, "title_abs_embedding_index": 16925}, {"title": "Enhancing Adversarial Robustness Through Robust Information Quantities", "link_suffix": "/forum?id=b87H1A3sxm", "link": "https://openreview.net/forum?id=b87H1A3sxm", "pdf_link": "https://openreview.net/pdf?id=b87H1A3sxm", "keywords": "Adversarial Robustness, Adversarial Training", "abstract": "It is known that deep neural networks (DNNs) are vulnerable to imperceptible adversarial attacks, and this fact raises concerns about their safety and reliability in real-world applications. In this paper, we aim to boost the robustness of a DNN against white-box adversarial attacks by defining three new information quantities---robust conditional mutual information (CMI), robust separation, and robust normalized CMI (NCMI)---which can serve as robust performance metrics for the DNN. We then utilize these concepts to introduce a novel training method that constrains the robust CMI and increases the robust separation simultaneously. Our experimental results demonstrate that our method consistently enhances model robustness against C&W and AutoAttack on CIFAR and Tiny-ImageNet datasets with and without additional synthetic data. Specifically, it is shown that our approach improves the robust accuracy of a DNN by up to 2.66% on CIFAR datasets and 3.49% on Tiny-ImageNet in the case of PGD attack and 1.70% on CIFAR datasets and 1.63% on Tiny-ImageNet in the case of AutoAttack, in comparison with the state-of-the-art training methods in the literature.", "title_embedding_index": 16901, "title_abs_embedding_index": 16926}, {"title": "Gradient Flow Provably Learns Robust Classifiers for Data from Orthonormal Clusters", "link_suffix": "/forum?id=8CJDYx8GwF", "link": "https://openreview.net/forum?id=8CJDYx8GwF", "pdf_link": "https://openreview.net/pdf?id=8CJDYx8GwF", "keywords": "Orthonormal Clusters, Robust classifier, Two-layer Network, Gradient Flow", "abstract": "Deep learning-based classifiers are known to be vulnerable to adversarial attacks. Existing methods for defending against such attacks require adding a defense mechanism or modifying the learning procedure (e.g., by adding adversarial examples). This paper shows that for certain data distribution one can learn a provably robust classifier using standard learning methods and without adding a defense mechanism. More specifically, this paper addresses the problem of finding a robust classifier for a binary classification problem in which the data comes from a mixture of Gaussian clusters with orthonormal cluster centers. First, we characterize the largest $\\ell_2$-attack any classifier can defend against while maintaining high accuracy, and show the existence of optimal robust classifiers achieving this maximum $\\ell_2$-robustness. Next, we show that given data sampled from the orthonormal cluster model, gradient flow on a two-layer network with a polynomial ReLU activation and without adversarial examples provably finds an optimal robust classifier.", "title_embedding_index": 16902, "title_abs_embedding_index": 16927}, {"title": "Accumulator-Aware Post-Training Quantization for Large Language Models", "link_suffix": "/forum?id=xNgmEWmd9T", "link": "https://openreview.net/forum?id=xNgmEWmd9T", "pdf_link": "https://openreview.net/pdf?id=xNgmEWmd9T", "keywords": "Accumulators, Deep Learning, Inference, Quantization", "abstract": "Several recent studies have investigated low-precision accumulation, reporting improvements in throughput, power, and area across various platforms. However, the accompanying proposals have only considered the quantization-aware training (QAT) paradigm, in which models are fine-tuned or trained from scratch with quantization in the loop. As models continue to grow in size, QAT techniques become increasingly more expensive, which has motivated the recent surge in post-training quantization (PTQ) research. To the best of our knowledge, ours marks the first formal study of accumulator-aware quantization in the PTQ setting. To bridge this gap, we introduce AXE\u2014a practical, low-overhead framework of accumulator-aware extensions designed to endow overflow avoidance guarantees to existing layer-wise PTQ algorithms. We theoretically motivate AXE and demonstrate its flexibility by implementing it on top of two state-of-the-art PTQ algorithms: GPFQ and OPTQ. We further generalize AXE to support multi-stage accumulation for the first time, opening the door for full datapath optimization and scaling to large language models (LLMs). We evaluate AXE across autoregressive language generation models and observe significant improvements in the tradeoff between accumulator bit width and model accuracy over baseline methods.", "title_embedding_index": 16903, "title_abs_embedding_index": 16928}, {"title": "Parallel simulation for sampling under isoperimetry and score-based diffusion models", "link_suffix": "/forum?id=6Gb7VfTKY7", "link": "https://openreview.net/forum?id=6Gb7VfTKY7", "pdf_link": "https://openreview.net/pdf?id=6Gb7VfTKY7", "keywords": "parallel sampling, log-concave sampling, diffusion model, score-based generative modeling, ddpm", "abstract": "In recent years, there has been a surge of interest in proving discretization bounds for sampling under isoperimetry and for diffusion models. As data size grows, reducing the iteration cost becomes an important goal. Inspired by the great success of the parallel simulation of the initial value problem in scientific computation, we propose parallel Picard methods for sampling tasks.  Rigorous theoretical analysis reveals that our algorithm achieves better dependence on dimension $d$ than prior works in iteration complexity  (i.e., reduced from $O(\\mathrm{poly}(\\log d))$ to $O(\\log d)$), which is even optimal for sampling under isoperimetry with specific iteration complexity. Our work highlights the potential advantages of simulation methods in scientific computation for dynamics-based sampling and diffusion models.", "title_embedding_index": 16904, "title_abs_embedding_index": 16929}, {"title": "CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement", "link_suffix": "/forum?id=yf30Al57nu", "link": "https://openreview.net/forum?id=yf30Al57nu", "pdf_link": "https://openreview.net/pdf?id=yf30Al57nu", "keywords": "large language models; preference learning; code generation", "abstract": "Large Language Models (LLMs) have significantly advanced code generation but often require substantial resources and tend to over-generalize, limiting their efficiency for specific tasks. Fine-tuning smaller, open-source LLMs presents a viable alternative; however, it typically lags behind cutting-edge models due to supervised fine-tuning's reliance solely on correct code examples, which restricts the model's ability to learn from its own mistakes and adapt to diverse programming challenges. To bridge this gap, we introduce CodeLutra, a novel framework that enhances low-performing LLMs by leveraging both successful and failed code generation attempts. Unlike conventional fine-tuning, CodeLutra employs an iterative preference learning mechanism to compare correct and incorrect solutions as well as maximize the likelihood of correct codes. Through continuous iterative refinement, CodeLutra enables smaller LLMs to match or surpass GPT-4\u2019s performance in various code generation tasks without relying on vast external datasets or larger auxiliary models. On a challenging data analysis task, using just 500 samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's performance. These results highlight CodeLutra's potential to close the gap between open-source and closed-source models, making it a promising approach in the field of code generation.", "title_embedding_index": 16905, "title_abs_embedding_index": 16930}, {"title": "Spectro-Riemannian Graph Neural Networks", "link_suffix": "/forum?id=2MLvV7fvAz", "link": "https://openreview.net/forum?id=2MLvV7fvAz", "pdf_link": "https://openreview.net/pdf?id=2MLvV7fvAz", "keywords": "Graph representation learning, Spectral graph theory, Riemannian geometry, Non-Euclidean graph neural networks, Geometric deep learning", "abstract": "Can integrating spectral and curvature signals unlock new potential in graph representation learning? Non-Euclidean geometries, particularly Riemannian manifolds such as hyperbolic (negative curvature) and spherical (positive curvature), offer powerful inductive biases for embedding complex graph structures like scale-free, hierarchical, and cyclic patterns. Meanwhile, spectral filtering excels at processing signal variations across graphs, making it effective in homophilic and heterophilic settings. Leveraging both can significantly enhance the learned representations. To this end, we propose Spectro-Riemannian Graph Neural Networks (CUSP) - the first graph representation learning paradigm that unifies both CUrvature (geometric) and SPectral insights. CUSP is a mixed-curvature spectral GNN that learns spectral filters to optimize node embeddings in products of constant curvature manifolds (hyperbolic, spherical, and Euclidean). Specifically, CUSP introduces three novel components: (a) Cusp Laplacian, an extension of the traditional graph Laplacian based on Ollivier-Ricci curvature, designed to capture the curvature signals better; (b) Cusp Filtering, which employs multiple Riemannian graph filters to obtain cues from various bands in the eigenspectrum; and (c) Cusp Pooling, a hierarchical attention mechanism combined with a curvature-based positional encoding to assess the relative importance of differently curved substructures in our graph. Empirical evaluation across eight homophilic and heterophilic datasets demonstrates the superiority of CUSP in node classification and link prediction tasks, with a gain of up to 5.3% over state-of-the-art models.", "title_embedding_index": 16906, "title_abs_embedding_index": 16931}, {"title": "CBMA: Improving Conformal Prediction through Bayesian Model Averaging", "link_suffix": "/forum?id=BKSeNw2HIr", "link": "https://openreview.net/forum?id=BKSeNw2HIr", "pdf_link": "https://openreview.net/pdf?id=BKSeNw2HIr", "keywords": "Bayesian framework, Conformal prediction, Model uncertainty, Uncertainty quantification", "abstract": "Conformal prediction has emerged as a popular technique for facilitating valid predictive inference across a spectrum of machine learning models, under minimal assumption of exchangeability. Recently, Hoff (2023) showed that full conformal Bayes provides the most efficient prediction sets (smallest by expected volume) among all prediction sets that are valid at the  $(1 - \\alpha)$ level if the model is correctly specified. However, a critical issue arises when the Bayesian model itself may be mis-specified, resulting in prediction interval that might be suboptimal, even though it still enjoys the frequentist coverage guarantee. To address this limitation, we propose an innovative solution that combines Bayesian model averaging (BMA) with conformal prediction. This hybrid not only leverages the strengths of Bayesian conformal prediction but also introduces a layer of robustness through model averaging. Theoretically, we prove that the resulting prediction interval will converge to the optimal level of efficiency, if the true model is included among the candidate models. This assurance of optimality, even under potential model uncertainty, provides a significant improvement over existing methods, ensuring more reliable and precise uncertainty quantification.", "title_embedding_index": 16907, "title_abs_embedding_index": 16932}, {"title": "FLDmamba:  Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction", "link_suffix": "/forum?id=9EiWIyJMNi", "link": "https://openreview.net/forum?id=9EiWIyJMNi", "pdf_link": "https://openreview.net/pdf?id=9EiWIyJMNi", "keywords": "Mamba; Time Series Prediction", "abstract": "Time series prediction, a crucial task across various domains, faces significant challenges due to the inherent complexities of time series data, including non-stationarity, multi-scale periodicity, and transient dynamics, particularly when tackling long-term predictions. While Transformer-based architectures have shown promise, their quadratic complexity with sequence length hinders their efficiency for long-term predictions. Recent advancements in State-Space Models, such as Mamba, offer a more efficient alternative for long-term modeling, but they lack the capability to capture multi-scale periodicity and transient dynamics effectively. Meanwhile, they are susceptible to the data noise issue in time series. This paper proposes a novel framework, FLDmamba (Fourier and Laplace Transform Decomposition Mamba), addressing these limitations. FLDmamba leverages the strengths of both Fourier and Laplace transforms to effectively capture both multi-scale periodicity, transient dynamics within time series data, and improve the robustness of the model to the data noise issue. By integrating Fourier analysis into Mamba, FLDmamba enhances its ability to capture global-scale properties, such as multi-scale periodicity patterns, in the frequency domain. Meanwhile, the Fourier Transform aids in isolating underlying patterns or trends from noise in time series data by emphasizing key frequency components, thereby enabling the model to mitigate noise effects. Additionally, incorporating Laplace analysis into Mamba improves its capacity to capture local correlations between neighboring data points, leading to a more accurate representation of transient dynamics. Our extensive experiments demonstrate that FLDmamba achieves superior performance on time series prediction benchmarks, outperforming both Transformer-based and other Mamba-based architectures. This work offers a computationally efficient and effective solution for long-term time series prediction, paving the way for its application in real-world scenarios. To promote the reproducibility of our method, we have made both the code and data accessible via the following URL: \\href{https://anonymous.4open.science/r/FLambas-AD7E/README.md}{https://anonymous.4open.science/r/FLDmamba}", "title_embedding_index": 16908, "title_abs_embedding_index": 16933}, {"title": "ParallelSpec: Parallel Drafter for Efficient Speculative Decoding", "link_suffix": "/forum?id=SXvb8PS4Ud", "link": "https://openreview.net/forum?id=SXvb8PS4Ud", "pdf_link": "https://openreview.net/pdf?id=SXvb8PS4Ud", "keywords": "large language model inference, speculative decoding", "abstract": "Speculative decoding has proven to be an efficient solution to large language model (LLM) inference, where the small drafter predicts future tokens at a low cost, and the target model is leveraged to verify them in parallel. However, most existing works still draft tokens auto-regressively to maintain sequential dependency in language modeling, which we consider a huge computational burden in speculative decoding. We present ParallelSpec, an alternative to auto-regressive drafting strategies in state-of-the-art speculative decoding approaches. In contrast to auto-regressive drafting in the speculative stage, we train a parallel drafter to serve as an efficient speculative model. ParallelSpec learns to efficiently predict multiple future tokens in parallel using a single model, and it can be integrated into any speculative decoding framework that requires aligning the output distributions of the drafter and the target model with minimal training cost. Experimental results show that ParallelSpec accelerates baseline methods in latency up to 62% on text generation benchmarks from different domains, and it achieves 2.84$\\times$ overall speedup on the Llama-2-13B model using third-party evaluation criteria.", "title_embedding_index": 16909, "title_abs_embedding_index": 16934}, {"title": "Point-SAM: Promptable 3D Segmentation Model for Point Clouds", "link_suffix": "/forum?id=yXCTDhZDh6", "link": "https://openreview.net/forum?id=yXCTDhZDh6", "pdf_link": "https://openreview.net/pdf?id=yXCTDhZDh6", "keywords": "3D vision, promptable segmentation, point cloud segmentation", "abstract": "The development of 2D foundation models for image segmentation has been significantly advanced by the Segment Anything Model (SAM). However, achieving similar success in 3D models remains a challenge due to issues such as non-unified data formats, poor model scalability, and the scarcity of labeled data with diverse masks. To this end, we propose a 3D promptable segmentation model Point-SAM, focusing on point clouds. We employ an efficient transformer-based architecture tailored for point clouds, extending SAM to the 3D domain. We then distill the rich knowledge from 2D SAM for Point-SAM training by introducing a data engine to generate part-level and object-level pseudo-labels at scale from 2D SAM. Our model outperforms state-of-the-art 3D segmentation models on several indoor and outdoor benchmarks and demonstrates a variety of applications, such as interactive 3D annotation and zero-shot 3D instance proposal.", "title_embedding_index": 16910, "title_abs_embedding_index": 16935}, {"title": "Causal Discovery via Bayesian Optimization", "link_suffix": "/forum?id=8muemqlnG3", "link": "https://openreview.net/forum?id=8muemqlnG3", "pdf_link": "https://openreview.net/pdf?id=8muemqlnG3", "keywords": "causal discovery, causal structure learning, bayesian optimization", "abstract": "Existing score-based methods for directed acyclic graph (DAG) learning from observational data struggle to recover the causal graph accurately and sample-efficiently. To overcome this, in this study, we propose DrBO (DAG recovery via Bayesian Optimization)\u2014a novel DAG learning framework leveraging Bayesian optimization (BO) to find high-scoring DAGs. We show that, by sophisticatedly choosing the promising DAGs to explore, we can find higher-scoring ones much more efficiently. To address the scalability issues of conventional BO in DAG learning, we replace Gaussian Processes commonly employed in BO with dropout neural networks, trained in a continual manner, which allows for (i) flexibly modeling the DAG scores without overfitting, (ii) incorporation of uncertainty into the estimated scores, and (iii) scaling with the number of evaluations. As a result, DrBO is computationally efficient and can find the accurate DAG in fewer trials and less time than existing state-of-the-art methods. This is demonstrated through an extensive set of empirical evaluations on many challenging settings with both synthetic and real data.", "title_embedding_index": 16911, "title_abs_embedding_index": 16936}, {"title": "Towards Out-of-Modal Generalization without Instance-level Modal Correspondence", "link_suffix": "/forum?id=LuVulfPgZN", "link": "https://openreview.net/forum?id=LuVulfPgZN", "pdf_link": "https://openreview.net/pdf?id=LuVulfPgZN", "keywords": "Multi-Modal Learning; Generalization", "abstract": "The world is understood from various modalities, such as appearance, sound, language, etc. Since each modality only partially represents objects in a certain physical meaning, leveraging additional ones is beneficial in both theory and practice. However, exploiting novel modalities normally requires cross-modal pairs corresponding to the same instance, which is extremely resource-consuming and sometimes even impossible, making knowledge exploration of novel modalities largely restricted. To seek practical multi-modal learning, here we study Out-of-Modal (OOM) Generalization as an initial attempt to generalize to an unknown modality without given instance-level modal correspondence. Specifically, we consider Semi-Supervised and Unsupervised scenarios of OOM Generalization, where the first has scarce correspondences and the second has none, and propose connect & explore (COX) to solve these problems. COX first connects OOM data and known In-Modal (IM) data through a variational information bottleneck framework to extract shared information. Then, COX leverages the shared knowledge to create emergent correspondences, which is theoretically justified from an information-theoretic perspective. As a result, the label information on OOM data emerges along with the correspondences, which help explore the OOM data with unknown knowledge, thus benefiting generalization results. We carefully evaluate the proposed COX method under various OOM generalization scenarios, verifying its effectiveness and extensibility.", "title_embedding_index": 16912, "title_abs_embedding_index": 16937}, {"title": "A grid world agent with favorable inductive biases", "link_suffix": "/forum?id=0spR7wDwBh", "link": "https://openreview.net/forum?id=0spR7wDwBh", "pdf_link": "https://openreview.net/pdf?id=0spR7wDwBh", "keywords": "intrinsic rewards, inductive biases, planning, uncertainty, deep reinforcement learning, reinforcement learning", "abstract": "We present a novel experiential learning agent with causally-informed intrinsic reward that is capable of learning sequential and causal dependencies in a robust and data-efficient way within grid world environments. After reflecting on state-of-the-art Deep Reinforcement Learning algorithms, we provide a relevant discussion of common techniques as well as our own systematic comparison within multiple grid world environments. Additionally, we investigate the conditions and mechanisms leading to data-efficient learning and analyze relevant inductive biases that our agent utilizes to effectively learn causal knowledge and to plan for rewarding future states of greatest expected return.", "title_embedding_index": 16913, "title_abs_embedding_index": 16938}, {"title": "IT3: Idempotent Test-Time Training", "link_suffix": "/forum?id=0hyShAPeBj", "link": "https://openreview.net/forum?id=0hyShAPeBj", "pdf_link": "https://openreview.net/pdf?id=0hyShAPeBj", "keywords": "idempotence;generalization", "abstract": "This paper introduces Idempotent Test-Time Training (IT$^3$),\na novel approach to addressing the challenge of distribution shift.\nWhile supervised-learning methods assume matching train and test distributions, this is rarely the case for machine learning systems deployed in the real world.\nTest-Time Training (TTT) approaches address this by adapting models during inference, but they are limited by a domain specific auxiliary task. IT$^3$ is based on the universal property of idempotence. An idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, namely $f(f(x))=f(x)$. \nAn idempotent operator is one that can be applied sequentially without changing the result beyond the initial application, that is $f(f(X)=f(X)$.\nAt training, the model receives an input $X$ along with another signal that can either be the ground truth label $y$ or a neutral \"don't know\" signal $\\mathbf{0}$. At test time, the additional signal can only be $\\mathbf{0}$. When sequentially applying the model, first predicting $y_0 = f(X, \\mathbf{0})$ and then $y_1 = f(X, y_0)$, the distance between $y_1$ and $y_2$ measures certainty and indicates out-of-distribution input $x$ if high.\n We use this distance, that can be expressed as $||f(X, f(X, \\mathbf{0})) - f(x, \\mathbf{0})||$ as our TTT loss during inference. By carefully optimizing this objective, we effectively train $f(X,\\cdot)$ to be idempotent, projecting the internal representation of the input onto the training distribution.\nWe demonstrate the versatility of our approach across various tasks,\nincluding corrupted image classification, aerodynamic predictions,\ntabular data with missing information, and large-scale aerial photo segmentation. Moreover, these tasks span different architectures such as MLPs, CNNs, and GNNs.", "title_embedding_index": 16914, "title_abs_embedding_index": 16939}, {"title": "Uncertainty Estimation for 3D Object Detection via Evidential Learning", "link_suffix": "/forum?id=eimzz4T1wo", "link": "https://openreview.net/forum?id=eimzz4T1wo", "pdf_link": "https://openreview.net/pdf?id=eimzz4T1wo", "keywords": "3d;detection;uncertainty", "abstract": "3D object detection is an essential task for computer vision applications in autonomous vehicles and robotics. \nHowever, models often struggle to quantify detection reliability, leading to poor performance on unfamiliar scenes.\nWe introduce a framework for quantifying uncertainty in 3D object detection by leveraging an evidential learning loss on Bird's Eye View representations in the 3D detector.\nThese uncertainty estimates require minimal computational overhead and are generalizable across different architectures.\nWe demonstrate both the efficacy and importance of these uncertainty estimates on identifying out-of-distribution scenes, poorly localized objects, and missing (false negative) detections; our framework consistently improves over baselines by 10-20% on average.\nFinally, we integrate this suite of tasks into a system where a 3D object detector auto-labels driving scenes and our uncertainty estimates verify label correctness before the labels are used to train a second model. Here, our uncertainty-driven verification results in a 1% improvement in mAP and a 1-2% improvement in NDS.", "title_embedding_index": 16915, "title_abs_embedding_index": 16940}, {"title": "Hybrid Preference Optimization: Augmenting Direct Preference Optimization with Auxiliary Objectives", "link_suffix": "/forum?id=F5nWSf9etp", "link": "https://openreview.net/forum?id=F5nWSf9etp", "pdf_link": "https://openreview.net/pdf?id=F5nWSf9etp", "keywords": "large language models, alignment, reinforcement learning, direct preference optimization", "abstract": "For aligning large language models (LLMs), prior work has leveraged reinforcement learning via human feedback (RLHF) or variations of direct preference optimization (DPO). While DPO offers a simpler framework based on maximum likelihood estimation, it compromises on the ability to tune language models to easily maximize non-differentiable objectives according to the LLM designer's preferences (e.g., using simpler language or minimizing specific kinds of harmful content). These may neither align with user preferences nor even be able to be captured tractably by binary preference data. To leverage the simplicity and performance of DPO with the generalizability of RL, we propose a hybrid approach between DPO and RLHF. With a simple augmentation to the implicit reward decomposition of DPO, we allow for tuning LLMs to maximize a set of arbitrary auxiliary rewards using offline RL. The proposed method, Hybrid Preference Optimization (HPO), shows the ability to effectively generalize to both user preferences and auxiliary designer objectives, while preserving alignment performance across a range of challenging benchmarks and model sizes.", "title_embedding_index": 16916, "title_abs_embedding_index": 16941}, {"title": "From Unimodal to Multimodal:Scaling up Projectors to Align Modalities", "link_suffix": "/forum?id=jHVJQybLXi", "link": "https://openreview.net/forum?id=jHVJQybLXi", "pdf_link": "https://openreview.net/pdf?id=jHVJQybLXi", "keywords": "vision language models, vision language alignment, unimodal models, representational alignment", "abstract": "Recent contrastive multimodal vision-language models like CLIP have demonstrated robust open-world semantic understanding, becoming the standard image backbones for vision-language applications due to their aligned latent space. However, this practice has left powerful unimodal encoders for both vision and language underutilized in these applications which raises a key question: Is there a plausible way to connect unimodal backbones for zero-shot vision-language tasks? To this end, we propose a novel approach that aligns vision and language modalities using only projection layers on pretrained, frozen unimodal encoders. Our method exploits the high semantic similarity between embedding spaces of well-trained vision and language models. It involves selecting semantically similar encoders in the latent space, curating a concept-rich dataset of image-caption pairs, and training simple MLP projectors. We evaluated our approach on 12 zero-shot classification datasets and 2 image-text retrieval datasets. Our best model, utilizing DINOv2 and All-Roberta-Large text encoder, achieves 76(%) accuracy on ImageNet with a 20-fold reduction in data and 65-fold reduction in compute requirements. The proposed framework enhances the accessibility of model development while enabling flexible adaptation across diverse scenarios, offering an efficient approach to building multimodal models by utilizing existing unimodal architectures. Code and datasets will be released upon acceptance.", "title_embedding_index": 16917, "title_abs_embedding_index": 16942}, {"title": "Data Distillation for extrapolative protein design through exact preference optimization", "link_suffix": "/forum?id=ua5MHdsbck", "link": "https://openreview.net/forum?id=ua5MHdsbck", "pdf_link": "https://openreview.net/pdf?id=ua5MHdsbck", "keywords": "Protein design, Protein Language Models, Preference Learning, Extrapolation, Data distillation", "abstract": "The goal of protein design typically involves increasing fitness (extrapolating) beyond what is seen during training (e.g., towards higher stability, stronger binding affinity, etc.). State-of-the-art methods assume that one can safely steer proteins towards such extrapolated regions by learning from pairs alone. We hypothesize that noisy training pairs are not sufficiently informative to capture the fitness gradient and that models learned from pairs specifically may fail to capture three-way relations important for search, e.g., how two alternatives fair relative to a seed. Building on the success of preference alignment models in large language models, we introduce a progressive search method for extrapolative protein design by directly distilling into the model relevant triplet relations. We evaluated our model's performance in designing AAV and GFP proteins and demonstrated that the proposed framework significantly improves effectiveness in extrapolation tasks.", "title_embedding_index": 16918, "title_abs_embedding_index": 16943}, {"title": "A Realistic Threat Model for Large Language Model Jailbreaks", "link_suffix": "/forum?id=1kMTJnqmyl", "link": "https://openreview.net/forum?id=1kMTJnqmyl", "pdf_link": "https://openreview.net/pdf?id=1kMTJnqmyl", "keywords": "LLM, jailbreaks, threat model, robustness", "abstract": "A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. In their original settings, these methods all largely succeed in coercing the target output, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model combines constraints in perplexity, measuring how far a jailbreak deviates from natural text, and computational budget, in total FLOPs.\nFor the former, we build an N-gram model on 1T tokens, which, in contrast to model-based perplexity, allows for an LLM-agnostic and inherently interpretable evaluation. We adapt popular attacks to this new, realistic threat model, with which we, for the first time, benchmark these attacks on equal footing. After a rigorous comparison, we not only find attack success rates against safety-tuned modern models to be lower than previously presented, but also find that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Further, our threat model is interpretable, thus it allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent N-grams, either selecting N-grams absent from real-world text or rare ones, e.g. specific to code datasets.", "title_embedding_index": 16919, "title_abs_embedding_index": 16944}, {"title": "Radial Basis Operator Networks", "link_suffix": "/forum?id=q6hEuC48Dk", "link": "https://openreview.net/forum?id=q6hEuC48Dk", "pdf_link": "https://openreview.net/pdf?id=q6hEuC48Dk", "keywords": "Operator networks, Radial basis functions, Scientific computing, Partial differential equations", "abstract": "Operator networks are designed to approximate nonlinear operators, which provide mappings between infinite-dimensional spaces such as function spaces. These networks are playing an increasingly important role in machine learning, with their most notable contributions in the field of scientific computing. Their significance stems from their ability to handle the type of data often encountered in scientific applications. For instance, in climate modeling or fluid dynamics, input data typically consists of discretized continuous fields (like temperature distributions or velocity fields). We introduce the radial basis operator network (RBON), which represents a significant advancement as the first operator network capable of learning an operator in both the time domain and frequency domain when adjusted to accept complex-valued inputs. Despite the small, single hidden-layer structure, the RBON boasts small $L^2$ relative test error for both in- and out-of-distribution data (OOD) of less than $1\\times 10^{-7}$ in some benchmark cases. Moreover, the RBON maintains small error on OOD data from entirely different function classes from the training data.", "title_embedding_index": 16920, "title_abs_embedding_index": 16945}, {"title": "Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model", "link_suffix": "/forum?id=4rEI2JdHH6", "link": "https://openreview.net/forum?id=4rEI2JdHH6", "pdf_link": "https://openreview.net/pdf?id=4rEI2JdHH6", "keywords": "Grokking, feature learning, deep learning theory", "abstract": "''Grokking'' is a phenomenon where a neural network first memorizes training data and generalizes poorly, but then suddenly transitions to near-perfect generalization after prolonged training. While intriguing, this delayed generalization phenomenon compromises predictability and efficiency. Ideally, models should generalize directly without delay. To this end, this paper proposes GrokTransfer, a simple and principled method for accelerating grokking in training neural networks, based on the key observation that data embedding plays a\ncrucial role in determining whether generalization is delayed. GrokTransfer first trains a smaller, weaker model to reach a nontrivial (but far from optimal) test performance. Then, the learned input embedding from this weaker model is extracted and used to initialize the embedding in the target, stronger model. We rigorously prove that, on a synthetic XOR task where delayed generalization always\noccurs in normal training, GrokTransfer enables the target model to generalize directly without delay. Moreover, we demonstrate that, across empirical studies of different tasks, GrokTransfer effectively reshapes the training dynamics and eliminates delayed generalization, for both fully-connected neural networks and Transformers.", "title_embedding_index": 16921, "title_abs_embedding_index": 16946}, {"title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling", "link_suffix": "/forum?id=0xUEBQV54B", "link": "https://openreview.net/forum?id=0xUEBQV54B", "pdf_link": "https://openreview.net/pdf?id=0xUEBQV54B", "keywords": "Inference-Time Compute, Large Language Models", "abstract": "Scaling the amount of compute used to train language models has dramatically improved their capabilities. However, when it comes to inference, we often limit the amount of compute to only one attempt per problem. Here, we explore inference compute as another axis for scaling, using the simple technique of repeatedly sampling candidate solutions from a model. Across multiple tasks and models, we observe that coverage \u2013 the fraction of problems that are solved by any generated sample \u2013 scales with the number of samples over four orders of magnitude. Interestingly, the relationship between coverage and the number of samples is often log-linear and can be modelled with an exponentiated power law, suggesting the existence of inference-time scaling laws. In domains like coding and formal proofs, where answers can be automatically verified, these increases in coverage directly translate into improved performance. When we apply repeated sampling to SWE-bench Lite, the fraction of issues solved with DeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250 samples, outperforming the single-sample state-of-the-art of 43%. In domains without automatic verifiers, we find that common methods for picking from a sample collection (majority voting and reward models) plateau beyond several hundred samples and fail to fully scale with the sample budget.", "title_embedding_index": 16922, "title_abs_embedding_index": 16947}, {"title": "Zero-shot Outlier Detection via Synthetically Pretrained Transformers: Model Selection Bygone!", "link_suffix": "/forum?id=gRXLa6LS3J", "link": "https://openreview.net/forum?id=gRXLa6LS3J", "pdf_link": "https://openreview.net/pdf?id=gRXLa6LS3J", "keywords": "zero-shot outlier detection, prior-data fitted networks", "abstract": "Outlier detection (OD) has a vast literature as it finds numerous applications in\nenvironmental monitoring, security, manufacturing, and finance to name a few.\nBeing an inherently unsupervised task, model selection is a key bottleneck for OD\n(both algorithm and hyperparameter selection) without label supervision. There is\na long list of techniques to choose from \u2013 both classical algorithms and deep neural\narchitectures \u2013 and while several studies report their hyperparameter sensitivity, the\nliterature remains quite slim on unsupervised model selection\u2014limiting the effective use of OD in practice. In this paper we present FoMo-0D, for zero/0-shot OD\nexploring a transformative new direction that bypasses the hurdle of model selection\naltogether (!), thus breaking new ground. The fundamental idea behind FoMo-0D is\nthe Prior-data Fitted Networks, recently introduced by M\u00fcller et al. (2022), which\ntrains a Transformer model on a large body of synthetically generated data from a\nprior data distribution. In essence, FoMo-0D is a pretrained Foundation Model\nfor zero/0-shot OD on tabular data, which can directly predict the (outlier/inlier)\nlabel of any test data at inference time, by merely a single forward pass\u2014making\nobsolete the need for choosing an algorithm/architecture and tuning its associated\nhyperparameters, besides requiring no training of model parameters when given a\nnew OD dataset. Extensive experiments on 57 public benchmark datasets against\n26 baseline methods show that FoMo-0D performs statistically no different from the\n2nd top baseline, while significantly outperforming the majority of the baselines,\nwith an average inference time of 7.7 ms per test sample.", "title_embedding_index": 16923, "title_abs_embedding_index": 16948}, {"title": "SALSA: Soup-based Alignment Learning for Stronger Adaptation in RLHF", "link_suffix": "/forum?id=AN6PIiObp0", "link": "https://openreview.net/forum?id=AN6PIiObp0", "pdf_link": "https://openreview.net/pdf?id=AN6PIiObp0", "keywords": "RLHF", "abstract": "In Large Language Model (LLM) development, Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning models with human values and preferences. RLHF traditionally relies on the Kullback-Leibler (KL) divergence between the current policy and a frozen initial policy as a reference, which is added as a penalty in policy optimization algorithms like Proximal Policy Optimization (PPO). While this constraint prevents models from deviating too far from the initial checkpoint, it limits exploration of the reward landscape, reducing the model\u2019s ability to discover higher-quality solutions. As a result, policy optimization is often trapped in a narrow region of the parameter space, leading to suboptimal alignment and performance. This paper presents SALSA (Soup-basedAlignmentLearning forStrongerAdaptation), a novel approach designed to overcome these limitations by creating a more flexible and better located reference model through weight-space averaging of two independent supervised fine-tuned (SFT) models. This model soup allows for larger deviation in KL divergence and exploring a promising region of the solution space without sacrificing stability. By leveraging this more robust reference model, SALSA fosters better exploration, achieving higher rewards and improving model robustness, out-of-distribution generalization, and performance. We validate the effectiveness of SALSA through extensive experiments on popular open models (Llama-7B, Mistral-7B, and Gemma-2B) across various benchmarks (MT-Bench, Arena-Hard, UltraFeedback), where it consistently surpasses PPO by fostering deeper exploration and achieving superior alignment in LLMs.", "title_embedding_index": 16924, "title_abs_embedding_index": 16949}]