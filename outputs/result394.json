[
    {
        "title": "InterIDEAS: An LLM and Expert-Enhanced Dataset for Philosophical Intertextuality",
        "link_suffix": "/forum?id=cA8iQJFioL",
        "link": "https://openreview.net/forum?id=cA8iQJFioL",
        "pdf_link": "https://openreview.net/pdf?id=cA8iQJFioL",
        "keywords": "dataset, natural language processing, digital humanity, large language model, education",
        "abstract": "The formation and circulation of ideas in philosophy have profound implications for pedagogical and scholarly practices. However, traditional analyses often depend on manual reading and subjective interpretation, constrained by human cognitive limits. To address these challenges, we introduce InterIDEAS, a pioneering dataset designed to bridge philosophy and natural language processing (NLP). By merging theories of intertextuality from literary studies with bibliometric techniques and recent LLMs, InterIDEAS enables both quantitative and qualitative analysis of the intellectual, social, and historical relations embedded within these difficult-to-interpret philosophical texts. This dataset not only enhances the study of philosophy but also contributes to the development of language models by providing a training corpus that challenges and enhances their interpretative capacity. InterIDEAS covers over 45,000 pages from key philosophical texts, spanning major thoughts and schools from 1750 to 1950, and features more than 3,150 writers. It manifests the mutual contribution between philosophy and NLP, laying the groundwork for future interdisciplinary research."
    },
    {
        "title": "Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory",
        "link_suffix": "/forum?id=DhdqML3FdM",
        "link": "https://openreview.net/forum?id=DhdqML3FdM",
        "pdf_link": "https://openreview.net/pdf?id=DhdqML3FdM",
        "keywords": "theory, complexity theory, state space models, deep learning architectures, logic in computer science",
        "abstract": "Despite their successes, deep learning models struggle with tasks requiring complex reasoning and function composition. We present a theoretical and empirical investigation into the limitations of Structured State Space Models (SSMs) and Transformers in such tasks. We prove that one-layer SSMs cannot efficiently perform function composition over large domains without impractically large state sizes, and even with Chain-of-Thought prompting, they require a number of steps that scale unfavorably with the complexity of the function composition. Multi-layer SSMs are constrained by log-space computational capacity, limiting their reasoning abilities. Our experiments corroborate these theoretical findings. Evaluating models on tasks including various function composition settings, multi-digit multiplication, dynamic programming, and Einstein's puzzle, we find significant performance degradation even with advanced prompting techniques. Models often resort to shortcuts, leading to compounding errors. These findings highlight fundamental barriers within current deep learning architectures rooted in their computational capacities. We underscore the need for innovative solutions to transcend these constraints and achieve reliable multi-step reasoning and compositional task-solving, which is critical for advancing toward general artificial intelligence."
    },
    {
        "title": "Joint or Disjoint: Mixing Training Regimes for Early-Exit Models",
        "link_suffix": "/forum?id=6nabbltnLp",
        "link": "https://openreview.net/forum?id=6nabbltnLp",
        "pdf_link": "https://openreview.net/pdf?id=6nabbltnLp",
        "keywords": "early-exit, efficient AI, conditional computation",
        "abstract": "Early exits are an important efficiency mechanism integrated into deep neural networks that allows for the termination of the network's forward pass before processing through all its layers. \nEarly exit methods add trainable internal classifiers which leads to different training dynamics. However, there is no consistent verification of the approaches of training of early exit methods and little understanding how training regimes optimize the architecture.  Most early exit methods employ a training strategy that either simultaneously trains the backbone network and the exit heads or trains the exit heads separately. \nWe propose a training approach where the backbone is initially trained on its own, followed by a phase where both the backbone and the exit heads are trained together. Thus, we categorize early-exit training strategies into three distinct categories, and then validate them for their performance and efficiency. \nIn this benchmark, we perform\nboth theoretical and empirical analysis of early-exit training regimes. We study the methods in terms of information flow, loss landscape and numerical rank of activations and gauge the suitability of regimes for various architectures and datasets."
    },
    {
        "title": "ExpertZIP: A Progressive Fusion Framework for Mixture-of-Experts Model Optimization through Huffman Tree Structures",
        "link_suffix": "/forum?id=NwYya2nwf3",
        "link": "https://openreview.net/forum?id=NwYya2nwf3",
        "pdf_link": "https://openreview.net/pdf?id=NwYya2nwf3",
        "keywords": "Large language models, Mixture-of-Experts, Expert Fusion",
        "abstract": "Mixture-of-Experts (MoE) models have gained attention as a novel approach in developing large language models (LLMs), praised for their ability to enhance performance by utilizing multiple  experts. However, while increasing the number of experts in these models can yield performance gains, it also introduces significant trade-offs, such as substantial memory overhead and increased inference time, limiting their scalability and practical deployment. In this work, we conduct a thorough analysis of expert utilization and identify a fundamental inefficiency: many experts are underutilized, leading to suboptimal resource allocation with limited improvement. To address this issue, we propose ExpertZIP, a progressive framework for MoE models that leverages a Huffman tree-based expert fusion technique. This progressive approach systematically merges underutilized experts step by step, ensuring their essential contributions are maintained while drastically reducing memory usage and computational demands. Our approach yields a 17.23x reduction in model size and a 4.84x improvement in inference time, with only a 1.18% decrease in average accuracy compared to the original 64-expert model. Moreover, it demonstrates a 6.47% increase in accuracy relative to the models with an equivalent number of experts. These results demonstrate that our optimized framework provides performance on par with larger models, offering an efficient solution for resource-constrained and real-time applications."
    },
    {
        "title": "AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov\u2013Arnold Networks",
        "link_suffix": "/forum?id=kqdNvAhJrJ",
        "link": "https://openreview.net/forum?id=kqdNvAhJrJ",
        "pdf_link": "https://openreview.net/pdf?id=kqdNvAhJrJ",
        "keywords": "Physics-Informed Neural Networks, Kolmogorov\u2013Arnold Networks, Attention Mechanism, PDEs, Chebyshev Polynomials",
        "abstract": "This paper introduces AC-PKAN, an advanced framework for Physics-Informed Neural Networks (PINNs) that integrates Kolmogorov\u2013Arnold Networks (KANs) with Chebyshev Type-I polynomials and incorporates both internal and external attention mechanisms. Traditional PINNs based on Multilayer Perceptrons (MLPs) encounter challenges when handling complex partial differential equations (PDEs) due to vanishing gradients, limited interpretability, and computational inefficiency. To address these issues, we enhance the model from both external and internal perspectives. Externally, we propose a novel Residual Gradient Attention (RGA) mechanism that dynamically adjusts loss term weights based on gradient norms and residuals, thereby mitigating gradient stiffness and residual imbalance. Internally, AC-PKAN employs point-wise Chebyshev polynomial-based KANs, wavelet-activated MLPs with learnable parameters, and internal attention mechanisms. These integrated components improve both training efficiency and prediction accuracy. We provide mathematical proofs demonstrating that AC-PKAN can theoretically solve any finite-order PDE. Experimental results from five benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems."
    },
    {
        "title": "Deep Denoising Prior: You Only Need a Deep Gaussian Denoiser",
        "link_suffix": "/forum?id=nUp1NvgfOr",
        "link": "https://openreview.net/forum?id=nUp1NvgfOr",
        "pdf_link": "https://openreview.net/pdf?id=nUp1NvgfOr",
        "keywords": "Denoising, test time adaptation, low-level vision",
        "abstract": "Gaussian denoising often serves as the initiation of research in the field of image denoising, owing to its prevalence and intriguing properties. However, deep Gaussian denoiser typically generalizes poorly to other types of noises, such as Poisson noise and real-world noise. In this paper, we reveal that deep Gaussian denoisers have an underlying ability to handle other noises with only ten iterations of self-supervised learning, which is referred to as \\textit{deep denoiser prior}. Specifically, we first pre-train a Gaussian denoising model in a self-supervised manner. Then, for each test image, we construct a pixel bank based on the self-similarity and randomly sample pseudo-instance examples from it to perform test-time adaptation. Finally, we fine-tune the pre-trained Gaussian denoiser using the randomly sampled pseudo-instances. Extensive experiments demonstrate that our test-time adaptation method helps the pre-trained Gaussian denoiser rapidly improve performance in removing both in-distribution and out-of-distribution noise, achieving superior performance compared to existing single-image denoising methods while also significantly reducing computational time."
    },
    {
        "title": "One-shot World Models Using a Transformer Trained on a Synthetic Prior",
        "link_suffix": "/forum?id=VjeT8VFhHo",
        "link": "https://openreview.net/forum?id=VjeT8VFhHo",
        "pdf_link": "https://openreview.net/pdf?id=VjeT8VFhHo",
        "keywords": "World Models, Synthetic Pretraining, Reinforcement Learning, In-Context Learning",
        "abstract": "A World Model is a compressed spatial and temporal representation of a real world environment that allows one to train an agent or execute planning methods. However, world models are typically trained on observations from the real world environment, and they usually do not enable learning policies for other real environments. We propose One-Shot World Model (OSWM), a transformer world model that is learned in an in-context learning fashion from purely synthetic data sampled from a prior distribution. Our prior is composed of multiple randomly initialized neural networks, where each network models the dynamics of each state and reward dimension of a desired target environment. We adopt the supervised learning procedure of Prior-Fitted Networks by masking next-state and reward at random context positions and query OSWM to make probabilistic predictions based on the remaining transition context. During inference time, OSWM is able to quickly adapt to the dynamics of a simple grid world, as well as the CartPole gym and a custom control environment by providing 1k transition steps as context and is then able to successfully train environment-solving agent policies. However, transferring to more complex environments remains a challenge, currently. Despite these limitations, we see this work as an important stepping-stone in the pursuit of learning world models purely from synthetic data."
    },
    {
        "title": "Type-II Saddles and Probabilistic Stability of Stochastic Gradient Descent",
        "link_suffix": "/forum?id=eHf4mj9m9o",
        "link": "https://openreview.net/forum?id=eHf4mj9m9o",
        "pdf_link": "https://openreview.net/pdf?id=eHf4mj9m9o",
        "keywords": "stochastic gradient descent, linear stability, saddle points, deep learning theory",
        "abstract": "Characterizing and understanding the dynamics of stochastic gradient descent (SGD) around saddle points remains an open problem in neural network optimization. We identify two distinct types of saddle points, demonstrating that Type-II saddles pose a significant challenge due to vanishing gradient noise, which makes them particularly difficult for SGD to escape. We show that the dynamics around these saddles can be effectively modeled by a random matrix product process, allowing us to apply concepts from probabilistic stability and Lyapunov exponents. By leveraging ergodic theory, we establish that saddle points can be either attractive or repulsive for SGD, leading to a classification of four distinct dynamic phases based on the gradient's signal-to-noise ratio near the saddle. We apply the theory to the training at the initial stage of neural networks, explaining an intriguing phenomenon that neural networks are prone to be stuck at the initialization point at a larger learning rate. Our results offer a novel theoretical framework for understanding the intricate behavior of SGD around saddle points, with implications for improving optimization strategies in deep learning."
    },
    {
        "title": "It Helps to Take a Second Opinion: Teaching Smaller LLMs To Deliberate Mutually via Selective Rationale Optimisation",
        "link_suffix": "/forum?id=NHxwxc3ql6",
        "link": "https://openreview.net/forum?id=NHxwxc3ql6",
        "pdf_link": "https://openreview.net/pdf?id=NHxwxc3ql6",
        "keywords": "Multi-LLM Deliberation, Smaller LLMs, Rationale Generation, Rationale Refinement, Selective Rationale Optimisation, Trainable, Task-Guided Rationale Selection",
        "abstract": "Very large language models (LLMs) such as GPT-4 have shown the ability to handle complex tasks by generating and self-refining step-by-step rationales. Smaller language models (SLMs), typically with < 13B parameters, have been improved by using the data generated from very-large LMs through knowledge distillation. However, various practical constraints such as API costs, copyright, legal and ethical policies restrict using large (often opaque) models to train smaller models for commercial use. Limited success has been achieved at improving the ability of an SLM to explore the space of possible rationales and evaluate them by itself through self-deliberation. To address this, we propose COALITION, a trainable framework that facilitates interaction between two variants of the same SLM and trains them to generate and refine rationales optimized for the end-task. The variants exhibit different behaviors to produce a set of diverse candidate rationales during the generation and refinement steps. The model is then trained via Selective Rationale Optimization (SRO) to prefer generating rationale candidates that maximize the likelihood of producing the ground-truth answer. During inference, COALITION employs a controller to select the suitable variant for generating and refining the rationales. On five different datasets covering mathematical problems, commonsense reasoning, and natural language inference, COALITION outperforms several baselines by up to 5%. Our ablation studies reveal that cross-communication between the two variants performs better than using the single model to self-refine the rationales. We also demonstrate the applicability of COALITION for LMs of varying scales (4B to 14B parameters) and model families (Mistral, Llama, Qwen, Phi). We release the code for this work here."
    },
    {
        "title": "Denoising Levy Probabilistic Models",
        "link_suffix": "/forum?id=SYmUS6qRub",
        "link": "https://openreview.net/forum?id=SYmUS6qRub",
        "pdf_link": "https://openreview.net/pdf?id=SYmUS6qRub",
        "keywords": "diffusion, generative model, deep learning, machine learning, heavy-tail",
        "abstract": "Investigating noise distributions beyond Gaussian in diffusion generative models remains an open challenge. The Gaussian case has been a large success experimentally and theoretically, admitting a unified stochastic differential equation (SDE) framework, encompassing score-based and denoising formulations. Recent studies have investigated the potential of \\emph{heavy-tailed} noise distributions to mitigate mode collapse and effectively manage datasets exhibiting class imbalance, heavy tails, or prominent outliers. \nVery recently, Yoon et al.\\ (NeurIPS 2023), presented the Levy-Ito model (LIM), directly extending the SDE-based framework to a class of heavy-tailed SDEs, where the injected noise followed an $\\alpha$-stable distribution -- a rich class of heavy-tailed distributions. \nDespite its theoretical elegance and performance improvements, LIM relies on highly involved mathematical techniques, which may limit its accessibility and hinder its broader adoption and further development. \nIn this study, we take a step back, and instead of starting from the SDE formulation, we extend the denoising diffusion probabilistic model (DDPM) by directly replacing the Gaussian noise with $\\alpha$-stable noise. \nBy using only elementary proof techniques, we show that the proposed approach, \\emph{denoising L'{e}vy probabilistic model} (DLPM) algorithmically boils down to running vanilla DDPM with minor modifications, hence allowing the use of existing implementations with minimal changes. \nRemarkably, as opposed to the Gaussian case, DLPM and LIM yield different training algorithms and different backward processes, leading to distinct sampling algorithms. \nThis fundamental difference translates favorably for the performance of DLPM in various aspects: our experiments show that DLPM achieves better coverage of the tails of the data distribution, better generation of unbalanced datasets, and improved computation times requiring significantly smaller number of backward steps."
    },
    {
        "title": "Model Editing for CLIP with Unknown Spurious Correlations in Visual Encoder",
        "link_suffix": "/forum?id=kxALdqWt7r",
        "link": "https://openreview.net/forum?id=kxALdqWt7r",
        "pdf_link": "https://openreview.net/pdf?id=kxALdqWt7r",
        "keywords": "spurious correlation, model editing, model repair, CLIP",
        "abstract": "CLIP, despite its robust zero-shot capabilities, often suffers from spurious correlations that can lead to prediction errors, especially when deployed in environments different from their training data. This paper addresses the challenge of correcting errors in CLIP, particularly when only limited data is available and the underlying biases causing errors are unknown. \nTo tackle this issue, we introduce a novel two-phase model editing framework. In the first phase, we propose to utilize a data-driven approach to identify the spurious features that directly contribute to errors without prior knowledge of the biases and nullify the corresponding components in the model, creating a spurious-feature-ablated model. \nIn the second phase, we edit the original model by aligning the model's outputs with those of the spurious-feature-ablated model for misclassified samples to correct errors, while also aligning with the original model for the remaining data to maintain locality. Our experiments on the synthetic dataset and real-world datasets demonstrate the effectiveness of our method in both identifying the causes of errors and rectifying the model to significantly improve model performance."
    },
    {
        "title": "PrML: Progressive Multi-Task Learning for Monocular 3D Human Pose Estimation",
        "link_suffix": "/forum?id=s4yXbEfZV5",
        "link": "https://openreview.net/forum?id=s4yXbEfZV5",
        "pdf_link": "https://openreview.net/pdf?id=s4yXbEfZV5",
        "keywords": "Monocular 3D Human Pose Estimation; Multi-Task Learning",
        "abstract": "The lifting-based framework has dominated the field of monocular 3D human pose estimation by leveraging the well-detected 2D pose as an intermediate representation. However, it neglects the different initial states between 2D pose and per-joint depth and encodes the well-detected 2D pose feature and unknown per-joint depth feature in an entangled feature space. To address this limitation, we present a novel progressive multi-task learning pose estimation framework named PrML. Firstly, PrML introduces two task branches: one is to refine the well-detected 2D pose feature and the other is to learn the per-joint depth feature. This dual-branch design reduces the explicit influence of uncertain depth features on 2D pose features. Secondly, PrML employs a task aware decoder to supplement the complementary information between the refined 2D pose feature and the well-learned per-joint depth feature. This step establishes the connection between 2D pose and per-joint depth, compensating for the lack of interaction caused by the dual-branch design. We also conduct theoretical analysis from the perspective of mutual information and arrive at a loss to constrain this feature complementary process. Finally, we use two regression heads to regress the 2D pose and per-joint depth respectively, and concatenate them to obtain the final 3D pose. Extensive experiments show that PrML outperforms the conventional lifting-based framework with fewer parameters on two widely used datasets: Human3.6M and MPI-INF-3DHP. Code is available athttps://anonymous.4open.science/r/PrMLand we hope our effort can provide a new framework for monocular 3D human pose estimation."
    },
    {
        "title": "Understanding Model Ensemble in Transferable Adversarial Attack",
        "link_suffix": "/forum?id=28U5Olm32r",
        "link": "https://openreview.net/forum?id=28U5Olm32r",
        "pdf_link": "https://openreview.net/pdf?id=28U5Olm32r",
        "keywords": "adversarial examples, adversarial transferability, model ensemble attack",
        "abstract": "Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack.We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components.Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, contributing to three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks."
    },
    {
        "title": "Diffusion Implicit Policy for Unpaired Scene-aware Motion Synthesis",
        "link_suffix": "/forum?id=rvOpON15JJ",
        "link": "https://openreview.net/forum?id=rvOpON15JJ",
        "pdf_link": "https://openreview.net/pdf?id=rvOpON15JJ",
        "keywords": "Motion Synthesis, Human Animation",
        "abstract": "Human motion generation is a long-standing problem, and scene-aware motion synthesis has been widely researched recently due to its numerous applications. Prevailing methods rely heavily on paired motion-scene data whose quantity is limited. Meanwhile, it is difficult to generalize to diverse scenes when trained only on a few specific ones. Thus, we propose a unified framework, termed Diffusion Implicit Policy (DIP), for scene-aware motion synthesis, where paired motion-scene data are no longer necessary. In this framework, we disentangle human-scene interaction from motion synthesis during training and then introduce an interaction-based implicit policy into motion diffusion during inference. Synthesized motion can be derived through iterative diffusion denoising and implicit policy optimization, thus motion naturalness and interaction plausibility can be maintained simultaneously. The proposed implicit policy optimizes the intermediate noised motion in a GAN Inversion manner to maintain motion continuity and control keyframe poses though the ControlNet branch and motion inpainting. For long-term motion synthesis, we introduce motion blending for stable transitions between multiple sub-tasks, where motions are fused in rotation power space and translation linear space. The proposed method is evaluated on synthesized scenes with ShapeNet furniture, and real scenes from PROX and Replica. Results show that our framework presents better motion naturalness and interaction plausibility than cutting-edge methods. This also indicates the feasibility of utilizing the DIP for motion synthesis in more general tasks and versatile scenes."
    },
    {
        "title": "Towards World Simulator: Crafting Physical Commonsense-Based Benchmark for Video Generation",
        "link_suffix": "/forum?id=6rMHcLWxl4",
        "link": "https://openreview.net/forum?id=6rMHcLWxl4",
        "pdf_link": "https://openreview.net/pdf?id=6rMHcLWxl4",
        "keywords": "World Simulator, Physical Commonsense, Video Generation, Evaluation",
        "abstract": "Text-to-video (T2V) models like Sora have made significant strides in visualizing complex prompts, which is increasingly viewed as a promising path towards constructing the universal world simulator. Cognitive psychologists believe that the foundation for achieving this goal is the ability to understand intuitive physics. However, the capacity of these models to accurately represent intuitive physics remains largely unexplored. To bridge this gap, we introduce PhyGenBench, a comprehensive \\textbf{Phy}sics \\textbf{Gen}eration \\textbf{Ben}chmark designed to evaluate physical commonsense correctness in T2V generation. PhyGenBench comprises 160 carefully crafted prompts across 27 distinct physical laws, spanning four fundamental domains, which could comprehensively assesses models' understanding of physical commonsense. Alongside PhyGenBench, we propose a novel evaluation framework called PhyGenEval. This framework employs a hierarchical evaluation structure utilizing appropriate advanced vision-language models and large language models to assess physical commonsense. Through PhyGenBench and PhyGenEval, we can conduct large-scale automated assessments of T2V models' understanding of physical commonsense, which align closely with human feedback. Our evaluation results and in-depth analysis demonstrate that current models struggle to generate videos that comply with physical commonsense. Moreover, simply scaling up models or employing prompt engineering techniques is insufficient to fully address the challenges presented by PhyGenBench (e.g., dynamic scenarios). We hope this study will inspire the community to prioritize the learning of physical commonsense in these models beyond entertainment applications. We will release the data and codes athttps://github.com/PhyGenBench/PhyGenBench"
    },
    {
        "title": "Advancing Few-shot Continual Learning via Selective Knowledge Transfer",
        "link_suffix": "/forum?id=iN7EIQRUbF",
        "link": "https://openreview.net/forum?id=iN7EIQRUbF",
        "pdf_link": "https://openreview.net/pdf?id=iN7EIQRUbF",
        "keywords": "continual learning, transfer learning",
        "abstract": "Continual learning with large language models (LLMs) is a promising and chal\u0002lenging research that greatly impacts many applications. Existing solutions treat previous tasks equally, making them vulnerable to task interference, lacking scal\u0002ability with a large number of tasks, and oblivious to the intrinsic relationships among tasks. This work presents selective knowledge transfer (SKT), a novel\nand principled framework for continual learning with LLMs. SKT aims to maximize positive knowledge transfer while systematically minimizing the effects of irrelevant information from dissimilar tasks. To this end, SKT first assesses the degree of interference between the current and previous tasks and then selectively aggregates the tasks that maximize knowledge transfer for continual\ntraining. In addition, we integrate SKT into the current state-of-the-art continual language learning algorithm, Progressive Prompts, to introduce Log-evidence Progressive Prompts (LePP), which facilitate knowledge transfer between tasks. Comprehensive evaluations on challenging few-shot continual learning benchmarks demonstrate that LePP can surpass existing baselines for continual learning with LLMs with minimal overhead. Our extensive ablation studies reveal that SKT can discover useful task correlations without any prior knowledge, many of which align with human evaluations. Code will be published upon acceptance."
    },
    {
        "title": "Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation",
        "link_suffix": "/forum?id=9fvnZRCGra",
        "link": "https://openreview.net/forum?id=9fvnZRCGra",
        "pdf_link": "https://openreview.net/pdf?id=9fvnZRCGra",
        "keywords": "Handwritten Text-line Generation;Image Generation;Diffusion Model",
        "abstract": "Existing handwritten text generation methods typically focus on isolated words. However, realistic handwritten texts require attention not only to individual words but also to the relationships between them, such as vertical alignment and horizontal spacing. Therefore, generating entire text line is a more promising task. However, this task poses significant challenges, such as accurately capturing complex style patterns including both intra-word and inter-word patterns, and maintaining content structure across numerous characters. To address these challenges, inspired by human writing priors, we focus on both the vertical style (\\emph{e.g.}, word alignment) and horizontal style (\\emph{e.g.}, word spacing and letter connections) of individual writing samples. Additionally, we decompose text-line content preservation across numerous characters into global context supervision between characters and local supervision of individual character structures. In light of this, we propose DiffBrush, a new diffusion model for text-line generation. DiffBrush employs two complementary proxy objectives to handle vertical and horizontal writing styles, and introduces two-level discriminators to provide content supervision at both the text-line and word levels. Extensive experiments show that DiffBrush excels in generating high-quality text-lines, particularly in style reproduction and content preservation. Our source code will be made publicly available."
    },
    {
        "title": "One Model for All: Multi-Objective Controllable Language Models",
        "link_suffix": "/forum?id=bDPL0ohHBa",
        "link": "https://openreview.net/forum?id=bDPL0ohHBa",
        "pdf_link": "https://openreview.net/pdf?id=bDPL0ohHBa",
        "keywords": "controllable language models, reinforcement learning from human feedback",
        "abstract": "Aligning large language models (LLMs) with human preference is critical to enhancing LLMs' safety, helpfulness, helpfulness, humor, faithfulness, etc. The current reinforcement learning from human feedback (RLHF) mainly focuses on a fixed reward learned from average human ratings, which may weaken the adaptivity and controllability of varying preferences. However, creating personalized LLMs requires aligning LLMs with individual human preferences, which is non-trivial due to the scarce data per user and the diversity of user preferences on multi-objective trade-offs, such as prioritizing humor and empathy in one context, while seeking efficiency and precision in another. Can we train one LLM to produce personalized outputs for different user preferences on the Pareto front? In this paper, we introduce Multi-Objective Control (MOC), which trains an LLM as a meta-policy to directly generate responses in the preference-defined regions of Pareto front. Our approach integrates multi-objective optimization (MOO) principles into Proximal Policy Optimization (PPO) to train an LLM as a preference-conditioned policy network. We improve the computational efficiency of MOC by applying MOO at the policy level, which enables us to finetune an LLM of 7B parameters on a single A6000 GPU. Extensive experiments demonstrate the advantages of MOC over baselines in three aspects: (i) Controllability of LLM outputs w.r.t. user preferences on the trade-off among multiple rewards; (ii) Quality and diversity of LLM outputs, measured by the hyper-volume of multiple solutions achieved; and (iii) Generalization to unseen preferences. These results highlight MOC\u2019s potential for real-world applications requiring scalable and customizable LLMs."
    },
    {
        "title": "MPHIL: Multi-Prototype Hyperspherical Invariant Learning for Graph Out-of-Distribution Generalization",
        "link_suffix": "/forum?id=kiQhuq45hr",
        "link": "https://openreview.net/forum?id=kiQhuq45hr",
        "pdf_link": "https://openreview.net/pdf?id=kiQhuq45hr",
        "keywords": "graph out-of-distribution generalization, invariant learning, hyperspherical space",
        "abstract": "Out-of-distribution (OOD) generalization has emerged as a critical challenge in graph learning, as real-world graph data often exhibit diverse and shifting environments that traditional models fail to generalize across. A promising solution to address this issue is graph invariant learning (GIL), which aims to learn invariant representations by disentangling label-correlated invariant subgraphs from environment-specific subgraphs. However, existing GIL methods face two major challenges: (1) the difficulty of capturing and modeling diverse environments in graph data, and (2) the semantic cliff, where invariant subgraphs from different classes are difficult to distinguish, leading to poor class separability and increased misclassifications. To tackle these challenges, we propose a novel method termed Multi-Prototype Hyperspherical Invariant Learning (MPHIL), which introduces two key innovations: (1) invariant learning in hyperspherical space, enabling robust invariant feature extraction and prototypical learning in a highly discriminative space, and (2) class prototypes as intermediate variables, which eliminate the need for explicit environment modeling in GIL and mitigate the semantic cliff issue through multi-prototype-based classification. Derived from the theoretical framework of GIL, we introduce two novel objective functions: the invariant prototype matching loss to ensure samples are matched to the correct class prototypes, and the prototype separation loss to increase the distinction between prototypes of different classes in the hyperspherical space. Extensive experiments on 11 OOD generalization benchmark datasets demonstrate that MPHIL achieves state-of-the-art performance, significantly outperforming existing methods across graph data from various domains and with different distribution shifts. The source code of MPHIL is available athttps://anonymous.4open.science/r/MPHIL-23C0/."
    },
    {
        "title": "Spectral Group Lasso for Selecting Factors Hidden in Plain Sight",
        "link_suffix": "/forum?id=MVosmEvLSb",
        "link": "https://openreview.net/forum?id=MVosmEvLSb",
        "pdf_link": "https://openreview.net/pdf?id=MVosmEvLSb",
        "keywords": "variable selection, group lasso, factor model",
        "abstract": "This paper introduces the Group Lasso Approach (GL) for variable selection, encompassing two innovative methods: Directed Group Lasso (DGL) and Spectral Group Lasso (SGL). DGL is designed to identify factors that span a linear subspace closely resembling that of all candidate factors. In contrast, SGL begins with Principal Component Analysis (PCA) and extends further by incorporating a secondary estimation stage. The variable selection process in SGL utilizes a loss function with an $\\ell_2$ to $\\ell_\\infty$ norm penalty, similar to the traditional group lasso methodology. We demonstrate the consistency of variable selection for both methods under high-dimensional scaling through rigorous theoretical analysis and empirical validation. Our findings highlight the effectiveness of the Group Lasso Approach in accurately identifying true factors within complex, high-dimensional datasets."
    },
    {
        "title": "Learning Generalizable Environment Models via Discovering Superposed Causal Relationships",
        "link_suffix": "/forum?id=7LmuXey1lH",
        "link": "https://openreview.net/forum?id=7LmuXey1lH",
        "pdf_link": "https://openreview.net/pdf?id=7LmuXey1lH",
        "keywords": "Offline Reinforcement Learning, Dynamics Model Learning",
        "abstract": "In reinforcement learning, a generalizable world  model to mimic the environment is crucial for the assessment of various policy values in downstream tasks such as offline policy optimization and off-policy evaluation. Recently, studies have shown that learning a world model with sparse connections identified by causal discovery techniques can improve generalizability. So far, these studies focus on discovering a single and global causal structure. In this paper, we discuss a more practical setting in which the agent is deployed in an environment mixed with different causal mechanisms, called superposed causal relationships in this article. In this case, global causal discovery techniques will derive a degraded dense causal relationship, which will fail to improve the generalizability of the learned model. To solve the problem, we propose \\textbf{S}uperposed c\\textbf{A}usal \\textbf{M}odel (SAC) learning. SAM learning is an end-to-end framework that learns a transformer-based model which can recognize the causal relationships that the agent is encountering on the fly and then adapts its predictions. The experiments are conducted in two simulated environments, where SAM shows powerful identify abilities in environments with superposed causal relationships. Both the dynamics model and the policies learned by the SAM~generalize well to unseen states."
    },
    {
        "title": "MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models",
        "link_suffix": "/forum?id=WsgEWL8i0K",
        "link": "https://openreview.net/forum?id=WsgEWL8i0K",
        "pdf_link": "https://openreview.net/pdf?id=WsgEWL8i0K",
        "keywords": "Multi-image Understanding, Benchmark, LVLM, Evaluation",
        "abstract": "The capability to process multiple images is crucial for Large Vision-Language Models (LVLMs) to develop a more thorough and nuanced understanding of a scene. Recent multi-image LVLMs have begun to address this need. However, their evaluation has not kept pace with their development. To fill this gap, we introduce the Multimodal Multi-image Understanding (MMIU) benchmark, a comprehensive evaluation suite designed to assess LVLMs across a wide range of multi-image tasks. MMIU encompasses 7 types of multi-image relationships, 52 tasks, 77K images, and 11K meticulously curated multiple-choice questions, making it the most extensive benchmark of its kind. Our evaluation of 24 popular LVLMs, including both open-source and proprietary models, reveals significant challenges in multi-image comprehension, particularly in tasks involving spatial understanding. Even the most advanced models, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through multi-faceted analytical experiments, we identify key performance gaps and limitations, providing valuable insights for future model and data improvements. We aim for MMIU to advance the frontier of LVLM research and development. We release the data and code athttps://github.com/MMIUBenchmark/MMIU."
    },
    {
        "title": "Early Period of Training Impacts Adaptation for Out-of-Distribution Generalization: An Empirical Study",
        "link_suffix": "/forum?id=qDFpNXnuYK",
        "link": "https://openreview.net/forum?id=qDFpNXnuYK",
        "pdf_link": "https://openreview.net/pdf?id=qDFpNXnuYK",
        "keywords": "Early period of training, fine-tuning, parameter freezing, transfer learning",
        "abstract": "Prior research shows that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) data of tasks. Yet, the implications of early learning dynamics on out-of-distribution (OOD) generalization remain poorly understood, primarily due to the complexities and limitations of existing analytical techniques. In this work, we investigate the relationship between learning dynamics, OOD generalization under covariate shift and the early period of neural network training. We utilize the trace of Fisher Information and sharpness, focusing on gradual unfreezing (i.e., progressively unfreezing parameters during training) as our methodology for investigation. Through a series of empirical experiments, we show that 1) changing the number of trainable parameters during the early period of training via gradual unfreezing can significantly improve OOD results; 2) the trace of Fisher Information and sharpness can be used as indicators for the removal of interventions during the early period of training for better OOD generalization. Our experiments on both image and text data show that the early period of training is a general phenomenon that can provide Pareto improvements in ID and OOD performance with minimal complexity. Our work represents a first step towards understanding how early learning dynamics affect neural network OOD generalization and suggests a new avenue to improve and study this problem."
    },
    {
        "title": "From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics",
        "link_suffix": "/forum?id=msD4DHZzFg",
        "link": "https://openreview.net/forum?id=msD4DHZzFg",
        "pdf_link": "https://openreview.net/pdf?id=msD4DHZzFg",
        "keywords": "deep neural network, sequential model, state space model, statistical model",
        "abstract": "The depth of neural networks is a critical factor for their capability, with deeper models often demonstrating superior performance. Motivated by this, significant efforts have been made to enhance layer aggregation - reusing information from previous layers to better extract features at the current layer, to improve the representational power of deep neural networks. However, previous works have primarily addressed this problem from a discrete-state perspective which is not suitable as the number of network layers grows. Therefore, this paper treats the outputs from layers as states of a continuous process and considers leveraging the state space model (SSM) to design the aggregation of layers in very deep neural networks. Inspired by the advancements in Selective State Space Models (S6) for modeling long sequences, we introduce a new module called Selective State Space Model Layer Aggregation (S6LA). This module aims to combine traditional CNN or transformer architectures within a sequential framework, enhancing the representational capabilities of state-of-the-art vision networks. Extensive experiments show that S6LA delivers substantial improvements in both image classification and detection tasks, highlighting the potential of integrating SSMs with contemporary deep learning techniques."
    },
    {
        "title": "VAE-Var: Variational Autoencoder-Enhanced Variational Methods for Data Assimilation in Meteorology",
        "link_suffix": "/forum?id=utz99dx2RN",
        "link": "https://openreview.net/forum?id=utz99dx2RN",
        "pdf_link": "https://openreview.net/pdf?id=utz99dx2RN",
        "keywords": "Data assimilation, Variational Autoencoder, Weather Forecasting",
        "abstract": "Data assimilation (DA) is an essential statistical technique for generating accurate estimates of a physical system's states by combining prior model predictions with observational data, especially in the realm of weather forecasting. Effectively modeling the prior distribution while adapting to diverse observational sources presents significant challenges for both traditional and neural network-based DA algorithms. This paper introduces VAE-Var, a novel neural network-based data assimilation algorithm aimed at 1) enhancing accuracy by capturing the non-Gaussian characteristics of the conditional background distribution $p(\\mathbf{x}|\\mathbf{x}_b)$, and 2) efficiently assimilating real-world observational data. VAE-Var utilizes a variational autoencoder to learn the background error distribution, with its decoder creating a variational cost function to optimize the analysis states. The advantages of VAE-Var include: 1) it maintains the framework of traditional variational assimilation, enabling it to accommodate various observation operators, particularly irregular observations; 2) it lessens the dependence on expert knowledge for constructing the background distribution, allowing for improved modeling of non-Gaussian structures; and 3) experimental results indicate that, when applied to the FengWu weather forecasting model, VAE-Var outperforms DiffDA and two traditional algorithms (interpolation and 3DVar) in terms of assimilation accuracy in sparse observational contexts, and is capable of assimilating real-world GDAS prepbufr observations over a year."
    }
]