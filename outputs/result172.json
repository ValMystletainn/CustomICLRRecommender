[
    {
        "title": "Detecting Hallucination Before Answering: Semantic Compression Through Instruction",
        "link_suffix": "/forum?id=YFOg1LUGG1",
        "link": "https://openreview.net/forum?id=YFOg1LUGG1",
        "pdf_link": "https://openreview.net/pdf?id=YFOg1LUGG1",
        "keywords": "hallucination, hallucination detection, feeling of knowing, LLM, large language model",
        "abstract": "Large language models (LLMs) excel in various tasks but often suffer from hallucinations, providing incorrect information with high confidence. To address this, we focus on detecting when an LLM knows or does not know an answer, a concept referred to as the ``feeling of knowing'' (FoK). We propose a novel approach called Semantic Compression by trying to Answer in One-word (SCAO), which enables efficient FoK detection before generating full sentences, with only minimal computational cost. Additionally, we introduce a method to measure confounding variable effects in benchmarks, an Approximate Misannotation Effect (AME) test. Our experiments demonstrate that the feature fusion model of our SCAO and probing achieves enhanced performance in FoK detection in both short and long-form entity questions. The code and the dataset is available online (https://anonymous.4open.science/r/SCAO-2FF8)."
    },
    {
        "title": "Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation",
        "link_suffix": "/forum?id=R4h5PXzUuU",
        "link": "https://openreview.net/forum?id=R4h5PXzUuU",
        "pdf_link": "https://openreview.net/pdf?id=R4h5PXzUuU",
        "keywords": "Out-of-distribution detection, Trustworthiness, Vision-language models, Foundation models",
        "abstract": "With the recent emergence of foundation models trained on internet-scale data and demonstrating remarkable generalization capabilities, such foundation models have become more widely adopted, leading to an expanding range of application domains. Despite this rapid proliferation, the trustworthiness of foundation models remains underexplored. Specifically, the out-of-distribution detection (OoDD) capabilities of large vision-language models (LVLMs), such as GPT-4o, which are trained on massive multi-modal data, have not been sufficiently addressed. The disparity between their demonstrated potential and practical reliability raises concerns regarding the safe and trustworthy deployment of foundation models. To address this gap, we evaluate and analyze the OoDD capabilities of various proprietary and open-source LVLMs. Our investigation contributes to a better understanding of how these foundation models represent confidence scores through their generated natural language responses. Furthermore, we propose a self-guided prompting approach, termed \\emph{Reflexive Guidance (ReGuide)}, aimed at enhancing the OoDD capability of LVLMs by leveraging self-generated image-adaptive concept suggestions. Experimental results demonstrate that our ReGuide enhances the performance of current LVLMs in both image classification and OoDD tasks."
    },
    {
        "title": "Speculative Streaming: Fast LLM Inference without Auxiliary Models",
        "link_suffix": "/forum?id=jt8wI3ZzXG",
        "link": "https://openreview.net/forum?id=jt8wI3ZzXG",
        "pdf_link": "https://openreview.net/pdf?id=jt8wI3ZzXG",
        "keywords": "Speculative Decoding, Efficient LLM, inference optimization, Multi-stream attention",
        "abstract": "Speculative decoding is a prominent technique to accelerate large language model inference by leveraging predictions from an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, draft models add significant complexity to inference systems. Recently\nseveral single model architectures viz. Medusa have been proposed to speculate tokens in non-autoregressive manner, however, their effectiveness is limited due to lack of dependency between speculated tokens. We introduce a novel speculative decoding method that integrates drafting within the target model by using Multi-stream attention and incorporates future token planning into supervised finetuning objective. To the best of our knowledge, this is the first parameter-efficient approach that scales well with an increasing number of downstream tasks while enhancing downstream metrics and achieving high acceptance rates, attributable to the interdependence among the speculated tokens. Speculative Streaming speeds up decoding by 1.9 - 3X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, while improving generation quality and using \u223c10000X fewer extra parameters than alternative architectures, making it ideal for resource-constrained devices. Our approach can also be effectively deployed in\nlossless settings for generic chatbot applications that do not necessitate supervised fine-tuning. In such setups, we achieve 2.9 - 3.2X speedup while maintaining the integrity of the base model\u2019s output."
    },
    {
        "title": "Data-Driven Creativity: Amplifying Imagination in LLM Writing",
        "link_suffix": "/forum?id=uMxiGoczX1",
        "link": "https://openreview.net/forum?id=uMxiGoczX1",
        "pdf_link": "https://openreview.net/pdf?id=uMxiGoczX1",
        "keywords": "LLM, RLHF",
        "abstract": "During the alignment training of Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF) has proven to be effective in enhancing the model's alignment with human preferences. The RLHF approach requires human annotators to provide data representative of human preferences, aiding the model in advancing towards human-preferred outcomes. In this process, high-quality human preference data is both crucial and challenging to obtain. While many tasks, such as coding and mathematics, can now be more efficiently annotated through Artificial Intelligence Feedback (AIF), numerous tasks still necessitate human input to provide human preference signals. Particularly creative tasks are typical tasks that involving complex human preference. Here, we focus on creative writing tasks and investigate how to collaborate with annotators to acquire high-quality, superior data. We propose an expert-assisted data generation process, named Expert-Objective-Personal-Subjective (EOPS), that can efficiently obtain high-quality ordinal data with minimal human resources. We conduct experiments on three kinds of tasks, and experimental results validat the effectiveness of our method."
    },
    {
        "title": "FTP: Efficient Prefilling for Long-Context LLM Inference via FFN Token Pruning",
        "link_suffix": "/forum?id=fL8Zp8o6RL",
        "link": "https://openreview.net/forum?id=fL8Zp8o6RL",
        "pdf_link": "https://openreview.net/pdf?id=fL8Zp8o6RL",
        "keywords": "Large language model, Inference acceleration, Token pruning, Long-context inference, Natural Language Processing",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance across various NLP tasks, and have extended their capability to long-context scenarios. However, the increasing context length leads to longer inference time in both the prefilling and decoding stages.\nExisting token pruning methods primarily evict tokens to compress the KV cache, and only accelerate the decoding stage.\nRecent studies have extended token pruning to both stages, but they either yield subtle speedup during the prefilling stage or defer a portion of computations to the decoding phase. Critically, these approaches prioritize the attention module, overlooking the significant computations in the Feed-Forward Network (FFN) module.In this work, we focus on the prefilling stage and propose a novel token pruning method named FTP for long-context LLM inference.\nOur approach is based on the observation that the FFN module accounts for over 60% of the inference time. FTP reduces this by pruning non-critical tokens before the inference of FFN. The importance of each token, along with the quantity to be pruned, are dynamically determined by the attention scores in each layer.\nUnlike previous token pruning methods, FTP preserves a substantial amount of information of the pruned tokens through the residual connection, thereby achieving a notable speedup with only a negligible decrease in performance.\nSpecifically, the Qwen2-7B-Instruct model with FTP achieves a speedup of 1.24$\\times$ in the prefilling stage with only a 1.30% performance drop compared to the baseline model. The speedup is further boosted to 1.39$\\times$ on a Qwen1.5-32B-Chat model.\nExtensive experiments on long-context datasets across various tasks demonstrate the potential and effectiveness of FTP."
    },
    {
        "title": "MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models",
        "link_suffix": "/forum?id=6guG2OlXsr",
        "link": "https://openreview.net/forum?id=6guG2OlXsr",
        "pdf_link": "https://openreview.net/pdf?id=6guG2OlXsr",
        "keywords": "Large Language Models, Tool-usage, Benchmark",
        "abstract": "Large Language Models (LLMs) have displayed massive improvements in reason- ing and decision-making skills and can hold natural conversations with users. Recently, many tool-use benchmark datasets have been proposed. However, existing datasets have the following limitations: (1). Insufficient evaluation scenarios (e.g., only cover limited tool-use scenes). (2). Extensive evaluation costs (e.g., GPT API costs). To address these limitations, in this work, we propose a multi-granularity tool-use benchmark for large language models called MTU-Bench. For the \"multi-granularity\" property, our MTU-Bench covers five tool usage scenes (i.e., single-turn and single-tool, single-turn and multiple-tool, multiple-turn and single-tool, multiple-turn and multiple-tool, and out-of-distribution tasks). Besides, all evaluation metrics of our MTU-Bench are based on the prediction results and the ground truth without using any GPT or human evaluation metrics. Moreover, our MTU-Bench is collected by transforming existing high-quality datasets to simulate real-world tool usage scenarios, and we also propose an instruction dataset called MTU-Instruct data to enhance the tool-use abilities of existing LLMs. Comprehensive experimental results demonstrate the effectiveness of our MTU-Bench."
    },
    {
        "title": "ARQ: A Mixed-Precision Quantization Framework for Accurate and Certifiably Robust DNNs",
        "link_suffix": "/forum?id=iXytGy9UOj",
        "link": "https://openreview.net/forum?id=iXytGy9UOj",
        "pdf_link": "https://openreview.net/pdf?id=iXytGy9UOj",
        "keywords": "Quantization, Robustness, Certification, Formal Verification",
        "abstract": "Mixed precision quantization has become an important technique for\nenabling the execution of deep neural networks (DNNs) on limited resource computing platforms.\nTraditional quantization methods have primarily concentrated on maintaining\nneural network accuracy, either ignoring the impact of quantization on the\nrobustness of the network, or using only empirical techniques for improving\nrobustness. In contrast, techniques for robustness certification, which can\nprovide strong guarantees about the robustness of DNNs have not been used\nduring quantization due to their high computation cost and/or scalability\nissues.This paper introduces ARQ, an innovative mixed-precision quantization method that not only\npreserves the clean accuracy of the smoothed classifiers but also maintains\ntheir certified robustness. ARQ uses reinforcement learning to find accurate and robust\nDNN quantization, while efficiently leveraging randomized smoothing,\na popular class of statistical DNN verification algorithms, to guide the search process. \nWe compare ARQ with multiple state-of-the-art quantization techniques on\nseveral DNN architectures commonly used in quantization studies: ResNet-20 on\nCIFAR-10, ResNet-50 on ImageNet, and MobileNetV2 on ImageNet. \nWe demonstrate that ARQ consistently performs better than these baselines\nacross all the benchmarks and the input perturbation levels. In many cases, the performance of ARQ quantized networks can reach that of the original DNN with floating-point weights, but with only 1.5% instructions."
    },
    {
        "title": "The Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise",
        "link_suffix": "/forum?id=GpdO9r73xT",
        "link": "https://openreview.net/forum?id=GpdO9r73xT",
        "pdf_link": "https://openreview.net/pdf?id=GpdO9r73xT",
        "keywords": "Diffusion models, Initial noise",
        "abstract": "Diffusion models have achieved remarkable success in text-to-image generation tasks, yet the influence of initial noise remains largely unexplored. In this study, we identify specific regions within the initial noise image, termed trigger patches, that play a key role in inducing object generation in the resulting images. Notably, these patches areuniversaland can be generalized across various positions, seeds, and prompts. To be specific, extracting these patches from one noise and injecting them into another noise leads to object generation in targeted areas. To identify the trigger patches even before the image has been generated, just like consulting the crystal ball to foresee fate, we first create a dataset consisting of Gaussian noises labeled with bounding boxes corresponding to the objects appearing in the generated images andtrain a detector that identifies these patches from the initial noise.To explain the formation of these patches, we reveal that they areoutliersin Gaussian noise, and follow distinct distributions through two-sample tests. These outliers can take effect when injected into different noises and generalize well across different settings. Finally, we find the misalignment between prompts and the trigger patch patterns can result in unsuccessful image generations. To overcome it, we propose a reject-sampling strategy to obtain optimal noise, aiming to improve prompt adherence and positional diversity in image generation."
    },
    {
        "title": "CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing",
        "link_suffix": "/forum?id=J2FyEVg8HR",
        "link": "https://openreview.net/forum?id=J2FyEVg8HR",
        "pdf_link": "https://openreview.net/pdf?id=J2FyEVg8HR",
        "keywords": "collaborative inference, efficient inference, token-level routing, large language model",
        "abstract": "Large language models (LLMs) have achieved remarkable success in natural language processing tasks but suffer from high computational costs during inference, limiting their deployment in latency-constrained applications. To address this issue, we propose a novel \\textbf{C}ollaborative \\textbf{I}nference with \\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing (CITER) framework that introduces a token-level routing mechanism, enabling efficient collaboration between small and large language models (SLMs & LLMs). Specifically, CITER enables routing non-critical tokens to an SLM to reduce computational overhead, while critical tokens are processed by an LLM to maintain generation quality. We formulate the training of the router as a reinforcement learning task, where the router receives rewards based on both the quality of predictions and the inference cost of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate reward evaluation process, we introduce a shortcut for reward function estimation, significantly reducing the cost of the reward estimation and improving the practicality of our approach. Extensive experiments across four benchmark datasets demonstrate that CITER reduces inference cost while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications."
    },
    {
        "title": "Pacmann: Efficient Private Approximate Nearest Neighbor Search",
        "link_suffix": "/forum?id=yQcFniousM",
        "link": "https://openreview.net/forum?id=yQcFniousM",
        "pdf_link": "https://openreview.net/pdf?id=yQcFniousM",
        "keywords": "Information Retrieval, Privacy",
        "abstract": "We propose a new private Approximate Nearest Neighbor (ANN) search scheme\nnamed Pacmann\nthat allows a client to perform ANN search\nin a vector database \nwithout revealing the query vector to the server.\nUnlike prior constructions that run encrypted search on the server side\nwith computationally-intensive cryptographic techniques,\nPacmann carefully offloads limited computation and storage to the client. \nSpecifically, clients run a graph-based ANN search, where in each hop on the graph, the client privately retrieves local graph information from the server. \nTo make this efficient, we combine two ideas: \n(1) we adapt a leading graph-based ANN search algorithm to be compatible with private information retrieval (PIR) for subgraph retrieval;\n(2) we use a recent class of PIR schemes that trade offline preprocessing for online computational efficiency. \nPacmann achieves significantly better search quality than\nthe state-of-the-art private ANN search schemes,\nshowing up to 2.5$\\times$ better search accuracy on \nreal-world datasets than prior work and\nreaching 90% quality of a state-of-the-art \nnon-private ANN algorithm.\nMoreover on large datasets with up to 100 million vectors,\nPacmann shows better scalability \nthan prior private ANN schemes\nwith up to 2.6$\\times$ reduction in computation time\nand 1.3$\\times$ reduction in overall latency."
    },
    {
        "title": "h4rm3l: A Language for Composable Jailbreak Attack Synthesis",
        "link_suffix": "/forum?id=zZ8fgXHkXi",
        "link": "https://openreview.net/forum?id=zZ8fgXHkXi",
        "pdf_link": "https://openreview.net/pdf?id=zZ8fgXHkXi",
        "keywords": "LLM safety, program synthesis, compositional modeling, jailbreak attacks, red-teaming, domain-specific languages, string transformations, AI safety research, black-box optimization, automated benchmarking",
        "abstract": "Despite their demonstrated valuable capabilities, state-of-the-art (SOTA) widely\ndeployed large language models (LLMs) still cause harm to society due to the inef-\nfectiveness of their safety filters, which can be bypassed by prompt transformations\ncalled jailbreak attacks. Current approaches to LLM safety assessment, which\nemploy datasets of templated prompts and benchmarking pipelines, fail to cover\nsufficiently large and diverse sets of jailbreak attacks, leading to the widespread\ndeployment of unsafe LLMs. Recent research showed that novel jailbreak attacks\ncould be derived by composition, however, a formal composable representation for\njailbreak attacks, which among other benefits could enable the exploration of a\nlarge compositional space of jailbreak attacks through program synthesis methods,\nhas not been previously proposed. We introduce h4rm3l, a novel approach\naddressing this gap with a human-readable domain-specific language (DSL). Our\nframework comprises: (1) The h4rm3l DSL, which formally expresses jailbreak\nattacks as compositions of parameterized string transformation primitives. (2)\nA synthesizer with bandit algorithms that efficiently generates jailbreak attacks\noptimized for a target black box LLM. (3) The h4rm3l red-teaming software\ntoolkit that employs the previous two components and an automated harmful\nLLM behavior classifier that is strongly aligned with human preferences. We\ndemonstrate h4rm3l\u2019s efficacy by synthesizing a dataset of 2656 successful\nnovel jailbreak targeting 6 SOTA open-source and proprietary LLMs (GPT-3.5,\nGPT-4o, Claude-3-sonnet, Claude-3-haiku, Llama3-8b, and Llama3-70b), and\nby benchmarking those models against a subset of the synthesized attacks, and\npreviously published jailbreak attacks which were used as few-shot examples. Our\nresults show that h4rm3l\u2019s synthesized attacks are diverse and more successful\nthan previously reported attacks, with success rates exceeding 90% on SOTA LLMs.\nWarning: This paper and related research artifacts contain offensive and\npotentially disturbing prompts and model-generated content."
    },
    {
        "title": "Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models",
        "link_suffix": "/forum?id=otW0TJOUYF",
        "link": "https://openreview.net/forum?id=otW0TJOUYF",
        "pdf_link": "https://openreview.net/pdf?id=otW0TJOUYF",
        "keywords": "LLM agents, language agents, theory of mind, multi-agent",
        "abstract": "Multi-agent reinforcement learning (MARL) methods struggle with the non-stationarity of multi-agent systems and fail to adaptively learn online when tested with novel agents. Here, we leverage large language models (LLMs) to create an autonomous agent that can handle these challenges. Our agent, Hypothetical Minds, consists of a cognitively-inspired architecture, featuring modular components for perception, memory, and hierarchical planning over two levels of abstraction. We introduce the Theory of Mind module that scaffolds the high-level planning process by generating hypotheses about other agents' strategies in natural language. It then evaluates and iteratively refines these hypotheses by reinforcing hypotheses that make correct predictions about the other agents' behavior. Hypothetical Minds significantly improves performance over previous LLM-agent and RL baselines on a range of competitive, mixed motive, and collaborative domains in the Melting Pot benchmark, including both dyadic and population-based environments. Additionally, comparisons against LLM-agent baselines and ablations reveal the importance of hypothesis evaluation and refinement for succeeding on complex scenarios."
    },
    {
        "title": "A False Sense of Privacy: Evaluating Textual Data Sanitization Beyond Surface-level Privacy Leakage",
        "link_suffix": "/forum?id=04c5uWq9SA",
        "link": "https://openreview.net/forum?id=04c5uWq9SA",
        "pdf_link": "https://openreview.net/pdf?id=04c5uWq9SA",
        "keywords": "Privacy, NLP, Text, Reidentification, Data Release, Sanitization, Anonymization",
        "abstract": "The release of sensitive data often relies on synthetic data generation and Personally Identifiable Information~(PII) removal, with an inherent assumption that these techniques ensure privacy. However, the effectiveness of sanitization methods for text datasets has not been thoroughly evaluated. To address this critical gap, we propose the first privacy evaluation framework for the release of sanitized textual datasets. In our framework, a sparse retriever initially links sanitized records with target individuals based on known auxiliary information. Subsequently, semantic matching quantifies the extent of additional information that can be inferred about these individuals from the matched records. We apply our framework to two datasets: MedQA, containing medical records, and WildChat, comprising individual conversations with ChatGPT. Our results demonstrate that seemingly innocuous auxiliary information, such as specific speech patterns, can be used to deduce personal attributes like age or substance use history from the synthesized dataset.\nWe show that private information can persist in sanitized records at a semantic level, even in synthetic data. Our findings highlight that current data sanitization methods create a false sense of privacy by making only surface-level textual manipulations. This underscores the urgent need for more robust protection methods that address semantic-level information leakage."
    },
    {
        "title": "On Evaluating the Durability of Safeguards for Open-Weight LLMs",
        "link_suffix": "/forum?id=fXJCqdUSVG",
        "link": "https://openreview.net/forum?id=fXJCqdUSVG",
        "pdf_link": "https://openreview.net/pdf?id=fXJCqdUSVG",
        "keywords": "AI Safety, Fine-tuning Attacks, Open-weight LLMs, Adaptive Attacks",
        "abstract": "Many stakeholders---from model developers to policymakers---seek to minimize the risks of large language models (LLMs). Key to this goal is whether technical safeguards can impede the misuse of LLMs, even when models are customizable via fine-tuning or when model weights are openly available. Several recent studies have proposed methods to produce durable LLM safeguards for open-weight LLMs that can withstand adversarial modifications of the model's weights via fine-tuning. This holds the promise of raising adversaries' costs even under strong threat models where adversaries can directly fine-tune parameters.  However, we caution against over-reliance on such methods in their current state. Through several case studies, we demonstrate that even the evaluation of these defenses is exceedingly difficult and can easily mislead audiences into thinking that safeguards are more durable than they really are. We draw lessons from the failure modes that we identify and suggest that future research carefully cabin claims to more constrained, well-defined, and rigorously examined threat models, which can provide useful and candid assessments to stakeholders."
    },
    {
        "title": "Don\u2019t Throw Away Data: Better Sequence Knowledge Distillation",
        "link_suffix": "/forum?id=rpR9fDZw3D",
        "link": "https://openreview.net/forum?id=rpR9fDZw3D",
        "pdf_link": "https://openreview.net/pdf?id=rpR9fDZw3D",
        "keywords": "Knowledge distillation, machine translation, large language model",
        "abstract": "A critical component in knowledge distillation is the means of coupling the teacher and student. The predominant sequence knowledge distillation method involves supervised learning of the student against teacher-decoded outputs, and is exemplified by the current state of the art, which incorporates minimum Bayes risk (MBR) decoding. In this paper we seek to integrate MBR more tightly in distillation training, specifically by using several high scoring MBR translations, rather than a single selected sequence, thus capturing a rich diversity of teacher outputs. Our experiments on English to German and English to Japanese translation show consistent improvements over strong baseline methods for both tasks and with varying model sizes. Additionally, we conduct a detailed analysis focusing on data efficiency and capacity curse aspects to elucidate MBR-n and explore its further potential."
    },
    {
        "title": "Contradiction Retrieval Via Sparse-Aware Sentence Embedding",
        "link_suffix": "/forum?id=c1Vn1RpB64",
        "link": "https://openreview.net/forum?id=c1Vn1RpB64",
        "pdf_link": "https://openreview.net/pdf?id=c1Vn1RpB64",
        "keywords": "contradiction retrieval, sentence embedding",
        "abstract": "Contradiction retrieval refers to identifying and extracting documents that explicitly disagree with or refute the content of a query, which is important to many downstream applications like fact checking and data cleaning. To retrieve contradiction argument to the query from large document corpora, existing methods such as similarity search and crossencoder models exhibit significant limitations. The former struggles to capture the essence of contradiction due to its inherent nature of favoring similarity, while the latter suffers from computational inefficiency, especially when the size of corpora is large. To address these challenges, we introduce a novel approach: SparseCL that leverages specially trained sentence embeddings designed to preserve subtle, contradictory nuances between sentences. Our method utilizes a combined metric of cosine similarity and a sparsity function to efficiently identify and retrieve documents that contradict a given query. This approach dramatically enhances the speed of contradiction detection by reducing the need for exhaustive document comparisons to simple vector calculations. We validate our model using the Arguana dataset, a benchmark dataset specifically geared towards contradiction retrieval, as well as synthetic contradictions generated from the MSMARCO and HotpotQA datasets using GPT-4. Our experiments demonstrate the efficacy of our approach not only in contradiction retrieval with more than 30% accuracy improvements on MSMARCO and HotpotQA across different model architectures but also in applications such as cleaning corrupted corpora to restore high-quality QA retrieval. This paper outlines a promising direction for improving the accuracy and efficiency of contradiction retrieval in large-scale text corpora."
    },
    {
        "title": "QAC:Quantization-Aware Conversion for Mixed-Timestep Spiking Neural Networks",
        "link_suffix": "/forum?id=D4sQzdMvcG",
        "link": "https://openreview.net/forum?id=D4sQzdMvcG",
        "pdf_link": "https://openreview.net/pdf?id=D4sQzdMvcG",
        "keywords": "Spiking Neural Networks, Quantization, ANN-SNN Conversion",
        "abstract": "Spiking Neural Networks (SNNs) have recently garnered widespread attention due to their high computational efficiency and low energy consumption, possessing significant potential for further research. Currently, SNN algorithms are primarily categorized into two types: one involves the direct training of SNNs using surrogate gradients, and the other is based on the mathematical equivalence between ANNs and SNNs for conversion. However, both methods overlook the exploration of mixed-timestep SNNs, where different layers in the network operate with different timesteps. This is because surrogate gradient methods struggle to compute gradients related to timestep, while ANN-to-SNN conversions typically use fixed timesteps, limiting the potential performance improvements of SNNs. In this paper, we propose a Quantization-Aware Conversion (QAC) algorithm that reveals a profound theoretical insight: the power of the quantization bit-width in ANN activations is equivalent to the timesteps in SNNs with soft reset. This finding uncovers the intrinsic nature of SNNs, demonstrating that they act as activation quantizers\u2014transforming multi-bit activation features into single-bit activations distributed over multiple timesteps. Based on this insight, we propose a mixed-precision quantization-based conversion algorithm from ANNs to mixed-timestep SNNs, which significantly reduces the number of timesteps required during inference and improves accuracy. Additionally, we introduce a calibration method for initial membrane potential and thresholds. Experimental results on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method significantly outperforms previous approaches."
    },
    {
        "title": "SlimLLaVA: Automatic Pruning for Large Vision-language Models",
        "link_suffix": "/forum?id=VFhJtV29jZ",
        "link": "https://openreview.net/forum?id=VFhJtV29jZ",
        "pdf_link": "https://openreview.net/pdf?id=VFhJtV29jZ",
        "keywords": "Prune, Large vision lanugage model, Generalization",
        "abstract": "Multimodal large language models achieve satisfying performance in complex reasoning tasks, while still suffers from high model complexity in deployment especially for resource-limited devices. In this paper, we propose an automatic pruning method of large vision-language models for efficient multimodal reasoning. Conventional methods leverage the training data of the original model to select the proper pruning ratio for different network components, while they are infeasible for large vision-language models due to the unbearable search cost caused by web-scale training corpus. On the contrary, we only use a few samples to search the desired pruning policy by maximizing its generalization ability on the unknown training data despite of the model accuracy, so that the optimal accuracy-efficiency trade-off can be obtained for large vision-language models. Specifically, we formulate the generalization gap for the pruning policy based on the structural risk minimization principle. With the task performance and the generalization ability, we iteratively search for the optimal pruning policy in the given search space and optimize the vision projector to evolve the search space with higher upper bound of performance. We conduct extensive experiments on ScienceQA, Vizwiz, MM-vet and LLaVA-Bench datasets for the task of visual question answering. With only 64 samples for pruning policy search, our method achieves 83.05% accuracy on ScienceQA and $\\times$1.47 speedup compared to the dense LLaVA-v1.5-7B model."
    },
    {
        "title": "Learngene Tells You How to Customize: Task-Aware Parameter Prediction at Flexible Scales",
        "link_suffix": "/forum?id=gGpuhyjIlS",
        "link": "https://openreview.net/forum?id=gGpuhyjIlS",
        "pdf_link": "https://openreview.net/pdf?id=gGpuhyjIlS",
        "keywords": "parameter prediction; learngene",
        "abstract": "Reducing serving costs and latency is a fundamental challenge for deploying large-scale models in business applications. To cope with this demand, the Learngene framework encapsulates shareable information from large models into a compact unit called a learngene. This unit serves to initialize downstream models, enabling them to inherit the knowledge from the large model efficiently, hopefully diminishing deployment expenses. However, existing learngene methods are constrained by their strong dependence on the architecture of large model and overlook the features of target tasks, resulting in suboptimal adaptability of downstream models to deployment requirements. In this paper, we present Task-Aware Learngene (TAL), a novel method based on graph hypernetworks that predicts model parameters conditioned on desired model scales and task-specific characteristics. Extensive experiments demonstrate that TAL effectively scales model initialization parameters, selectively utilizes shareable information pertinent to target tasks, and consistently outperforms random initialization and existing parameter prediction methods. Furthermore, TAL exhibits promising transfer learning capabilities for unseen tasks, underscoring its effectiveness in condensing large model knowledge while being aware of downstream requirements."
    },
    {
        "title": "The \"Law'' of the Unconscious Contrastive Learner: Probabilistic Alignment of Unpaired Modalities",
        "link_suffix": "/forum?id=DsIOUoZkVk",
        "link": "https://openreview.net/forum?id=DsIOUoZkVk",
        "pdf_link": "https://openreview.net/pdf?id=DsIOUoZkVk",
        "keywords": "theory, contrastive learning, probabilistic graphical models, multi-modal learning, reinforcement learning",
        "abstract": "While internet-scale data often come in pairs (e.g., audio+image, image+text), we often want to perform inferences over modalities unseen together in the training data (e.g., audio+text). Prior work has addressed this issue by learning multiple contrastive embedding spaces between existing modality pairs, implicitly hoping that unseen modality pairs will end up being aligned. This theoretical paper proves that this hope is well founded, under certain assumptions. Starting with the proper Bayesian approach of integrating out intermediate modalities, we show that directly comparing the representations of data from unpaired modalities can recover the same likelihood ratio. Our analysis builds on prior work on the geometry and probabilistic interpretation of contrastive representations, showing how these representations can answer many of the same inferences as probabilistic graphical models. Our analysis suggests two new ways of using contrastive representations: in settings with pre-trained contrastive models, and for handling language ambiguity in reinforcement learning. Our numerical experiments study the importance of our assumptions and demonstrate these new applications."
    },
    {
        "title": "Graph Neural Preconditioners for Iterative Solutions of Sparse Linear Systems",
        "link_suffix": "/forum?id=Tkkrm3pA35",
        "link": "https://openreview.net/forum?id=Tkkrm3pA35",
        "pdf_link": "https://openreview.net/pdf?id=Tkkrm3pA35",
        "keywords": "General-purpose preconditioner, linear systems, graph neural networks",
        "abstract": "Preconditioning is at the heart of iterative solutions of large, sparse linear systems of equations in scientific disciplines. Several algebraic approaches, which access no information beyond the matrix itself, are widely studied and used, but ill-conditioned matrices remain very challenging. We take a machine learning approach and propose using graph neural networks as a general-purpose preconditioner. They show attractive performance for many problems and can be used when the mainstream preconditioners perform poorly. Empirical evaluation on over 800 matrices suggests that the construction time of these graph neural preconditioners (GNPs) is more predictable and can be much shorter than that of other widely used ones, such as ILU and AMG, while the execution time is faster than using a Krylov method as the preconditioner, such as in inner-outer GMRES. GNPs have a strong potential for solving large-scale, challenging algebraic problems arising from not only partial differential equations, but also economics, statistics, graph, and optimization, to name a few."
    },
    {
        "title": "Flow of Reasoning: Training LLMs for Divergent Problem Solving with Minimal Examples",
        "link_suffix": "/forum?id=HHmnfVQagN",
        "link": "https://openreview.net/forum?id=HHmnfVQagN",
        "pdf_link": "https://openreview.net/pdf?id=HHmnfVQagN",
        "keywords": "Large Language Models, reasoning, diversity, multi-step reasoning, planning, creativity",
        "abstract": "The ability to generate diverse solutions to a given problem is a hallmark of human creativity. This divergent reasoning is also crucial for machines, enhancing their robustness and enabling them to assist humans in many applications such as scientific discovery. However, existing approaches to multi-step reasoning with large language models (LLMs) have mostly focused only on the reasoning accuracy, without further discovering more diverse valid solutions. For example, supervised fine-tuning can improve LLM reasoning quality, but requires extensive supervised data to capture the full range of possible solutions. Reinforcement learning aims to find limited highest-reward solutions while neglecting the solution diversity. To fill this gap, we propose Flow of Reasoning (FoR), an efficient diversity-seeking LLM finetuning method aimed at improving reasoning quality and diversity with minimal data. FoR formulates multi-step LLM reasoning as a Markovian flow on a DAG-structured reasoning graph. This formulation allows us to incorporate and adapt principled GFlowNet approaches, for finetuning LLMs to sample diverse reasoning paths with probabilities proportional to the (unnormalized) reward of target problems. Extensive experiments show that, with limited training examples (e.g., 15 examples), FoR enables the discovery of diverse, creative, high-quality solutions, greatly outperforming a wide range of existing inference and training methods across five challenging puzzle-solving tasks, including BlocksWorld (embodied reasoning), Game24 (math puzzle solving), PrOntoQA (logical reasoning), Rubik's Cube (spatial reasoning), and 1D-ARC (abstraction reasoning)."
    },
    {
        "title": "A Robust Method to Discover Causal or Anticausal Relation",
        "link_suffix": "/forum?id=Q0s6kgrUMr",
        "link": "https://openreview.net/forum?id=Q0s6kgrUMr",
        "pdf_link": "https://openreview.net/pdf?id=Q0s6kgrUMr",
        "keywords": "Trustworthy Machine Learning, Semi-Superivsed Learning, Causality",
        "abstract": "Understanding whether the data generative process follows causal or anticausal relations is important for many applications. Existing causal discovery methods struggle with high-dimensional perceptual data such as images. Moreover, they require well-labeled data, which may not be feasible due to measurement error. In this paper, we propose a robust method to detect whether the data generative process is causal or anticausal. To determine the causal or anticausal relation, we identify an asymmetric property: under the causal relation, the instance distribution does not contain information about the noisy class-posterior distribution. We also propose a practical method to verify this via a noise injection approach. Our method is robust to label errors and is designed to handle both large-scale and high-dimensional datasets effectively. Both theoretical analyses and empirical results on a variety of datasets demonstrate the effectiveness of our proposed method in determining the causal or anticausal direction of the data generative process."
    },
    {
        "title": "Evaluating Ranking Loss Functions in Performance Predictor for NAS",
        "link_suffix": "/forum?id=4o4fDJL6I7",
        "link": "https://openreview.net/forum?id=4o4fDJL6I7",
        "pdf_link": "https://openreview.net/pdf?id=4o4fDJL6I7",
        "keywords": "Neural Architecture Search, Performance Predictor, Loss Function",
        "abstract": "Performance evaluation is a critical but compute-intensive procedure in neural architecture search (NAS). To alleviate evaluation costs, performance predictors have been widely adopted to predict architecture performance directly. Recent studies have introduced ranking loss functions into predictors to focus on the architecture rankings instead of absolute accuracy, thus enhancing the ranking ability of performance predictors. Despite the successful application of ranking loss functions, the lack of comprehensive measure metrics and different experimental configurations make a fair comparison among these loss functions a huge challenge. Additionally, some well-known ranking loss functions have not been thoroughly examined in the context of performance predictors. In this paper, we conduct the first study for 11 ranking loss functions containing the existing and the novel ones by comparing their effectiveness in performance predictors under various settings. We find that: (i) The choice of ranking loss function has a major influence on the performance of predictors; (ii) the quality of the architectures searched by the predictor-based NAS methods is closely correlated with the predictor's performance on top-centered rank metrics, rather than traditional metrics like Kendall Tau. We believe these results and insights can serve as recommendations for the optimal loss function to employ in predictors across various search spaces and experimental conditions."
    },
    {
        "title": "An Empirical Analysis of Uncertainty in Large Language Model Evaluations",
        "link_suffix": "/forum?id=J4xLuCt2kg",
        "link": "https://openreview.net/forum?id=J4xLuCt2kg",
        "pdf_link": "https://openreview.net/pdf?id=J4xLuCt2kg",
        "keywords": "Large Language Model, Model-based LLM Evaluation, LLM-as-a-Judge",
        "abstract": "As LLM-as-a-Judge emerges as a new paradigm for assessing large language models (LLMs), concerns have been raised regarding the alignment, bias, and stability of LLM evaluators. While substantial work has focused on alignment and bias, little research has concentrated on the stability of LLM evaluators. In this paper, we conduct extensive experiments involving 9 widely used LLM evaluators across 2 different evaluation settings to investigate the uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators exhibit varying uncertainty based on model families and sizes. With careful comparative analyses, we find that employing special prompting strategies, whether during inference or post-training, can alleviate evaluation uncertainty to some extent. By utilizing uncertainty to enhance LLM's reliability and detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an uncertainty-aware LLM evaluator named ConfiLM using a human-annotated fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually designed test set sourced from the 2024 Olympics. Experimental results demonstrate that incorporating uncertainty as additional information during the fine-tuning phase can largely improve the model's evaluation performance in OOD scenarios. We hope this work can draw broader research attention to the stability of LLM evaluators."
    }
]