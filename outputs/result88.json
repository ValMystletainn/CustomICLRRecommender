[
    {
        "title": "Beyond Fixed Resolution: Enhancing VLLMs with Adaptive Input Scaling",
        "link_suffix": "/forum?id=gc70LAWjwe",
        "link": "https://openreview.net/forum?id=gc70LAWjwe",
        "pdf_link": "https://openreview.net/pdf?id=gc70LAWjwe",
        "keywords": "Visual Large Language Model",
        "abstract": "Real-world vision-language applications demand varying levels of perceptual granularity. However, most existing visual large language models (VLLMs), such as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to subpar performance. To address this problem, we first conduct a comprehensive and pioneering investigation into the resolution preferences of different vision-language tasks, revealing a correlation between resolution preferences with 1.image complexity, and 2.uncertainty variance of the VLLM at different image input resolutions. Building on this insight, we propose an empirical formula to determine the optimal resolution for a given vision-language task, accounting for these two factors as the zeroth-order and first-order terms in the Taylor expansion on a given image input. Second, based on rigorous experiments, we propose a novel parameter-efficient fine-tuning technique to extend the visual input resolution of pre-trained VLLMs to the identified optimal resolution. Extensive experiments on various vision-language tasks validate the effectiveness of our method."
    },
    {
        "title": "Learning Color Equivariant Representations",
        "link_suffix": "/forum?id=IXyfbaGlps",
        "link": "https://openreview.net/forum?id=IXyfbaGlps",
        "pdf_link": "https://openreview.net/pdf?id=IXyfbaGlps",
        "keywords": "Equivariant Neural Network, Geometric Deep Learning, Group Convolution",
        "abstract": "In this paper, we introduce group convolutional neural networks (GCNNs) equivariant to color variation. GCNNs have been designed for a variety of geometric transformations from 2D and 3D rotation groups, to semi-groups such as scale. Despite the improved interpretability, accuracy and generalizability of these architectures, GCNNs have seen limited application in the context of perceptual quantities. Notably, the recent CEConv network uses a GCNN to achieve equivariance to hue transformations by convolving input images with a hue rotated RGB filter. However, this approach leads to invalid RGB values which break equivariance and degrade performance. We resolve these issues with a lifting layer that transforms the input image directly, thereby circumventing the issue of invalid RGB values and improving equivariance error by over three orders of magnitude. Moreover, we extend the notion of color equivariance to include equivariance to saturation shift. Our hue-, saturation-, and color-equivariant networks achieve strong generalization to out-of-distribution perceptual variations and improved sample efficiency over conventional architectures. We demonstrate the utility of our approach on synthetic and real world datasets where we consistently outperform competitive baselines."
    },
    {
        "title": "Improved Localized Machine Unlearning Through the Lens of Memorization",
        "link_suffix": "/forum?id=2m5XI3nM46",
        "link": "https://openreview.net/forum?id=2m5XI3nM46",
        "pdf_link": "https://openreview.net/pdf?id=2m5XI3nM46",
        "keywords": "Machine Unlearning, Memorization, Localized Unlearning",
        "abstract": "Machine unlearning refers to removing the influence of a specified subset of training data from a machine learning model, efficiently, after it has already been trained. This is important for key applications, including making the model more accurate by removing outdated, mislabeled, or poisoned data. In this work, we study localized unlearning, where the unlearning algorithm operates on a (small) identified subset of parameters. Drawing inspiration from the memorization literature, we propose an improved localization strategy that yields strong results when paired with existing unlearning algorithms. We also propose a new unlearning algorithm, Deletion by Example Localization (DEL), that resets the parameters deemed-to-be most critical according to our localization strategy, and then finetunes them. Our extensive experiments on different datasets, forget sets and metrics reveal that DEL sets a new state-of-the-art for unlearning metrics, against both localized and full-parameter methods, while modifying a small subset of parameters, and outperforms the state-of-the-art localized unlearning in terms of test accuracy too."
    },
    {
        "title": "A Large Deviation Theory Analysis on the Implicit Bias of SGD",
        "link_suffix": "/forum?id=BZz6Zb4bwa",
        "link": "https://openreview.net/forum?id=BZz6Zb4bwa",
        "pdf_link": "https://openreview.net/pdf?id=BZz6Zb4bwa",
        "keywords": "implicit bias, implicit regularization, optimization, stochastic gradient descent, large deviation theory",
        "abstract": "Stochastic Gradient Descent (SGD) plays a key role in training deep learning models, yet its ability to implicitly regularize and enhance generalization remains an open theoretical question. We apply Large Deviation Theory (LDT) to analyze why SGD selects models with strong generalization properties. We show that the generalization error jointly depends on the level of concentration of its empirical loss around its expected value and the \\textit{abnormality} of the random deviations stemming from the stochastic nature of the training data observation process. Our analysis reveals that SGD gradients are inherently biased toward models exhibiting more concentrated losses and less abnormal and smaller random deviations. These theoretical insights are empirically validated using deep convolutional neural networks, confirming that mini-batch training acts as a natural regularizer by preventing convergence to models with high generalization errors."
    },
    {
        "title": "Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models",
        "link_suffix": "/forum?id=yaqPf0KAlN",
        "link": "https://openreview.net/forum?id=yaqPf0KAlN",
        "pdf_link": "https://openreview.net/pdf?id=yaqPf0KAlN",
        "keywords": "Mathematical Benchmark, LLM Evaluation, Olympic",
        "abstract": "Recent advancements in large language models (LLMs) have led to significant breakthroughs in mathematical reasoning capabilities. \nHowever, existing benchmarks like GSM8K or MATH are now being solved with high accuracy (e.g., OpenAI o1 achieves 94.8% on MATH dataset), indicating their inadequacy for truly challenging these models. To bridge this gap, we propose a comprehensive and challenging benchmark specifically designed to assess LLMs' mathematical reasoning at the Olympiad level. Unlike existing Olympiad-related benchmarks, our dataset focuses exclusively on mathematics and comprises a vast collection of 4428 competition-level problems with rigorous human annotation. These problems are meticulously categorized into over 33 sub-domains and span more than 10 distinct difficulty levels, enabling a holistic assessment of model performance in Olympiad-mathematical reasoning. Furthermore, we conducted an in-depth analysis based on this benchmark. Our experimental results show that even the most advanced models, OpenAI o1-mini and OpenAI o1-preview, struggle with highly challenging Olympiad-level problems, with 60.54% and 52.55% accuracy, highlighting significant challenges in Olympiad-level mathematical reasoning."
    },
    {
        "title": "EnsemW2S: Can an Ensemble of LLMs be Leveraged to Obtain a Stronger LLM?",
        "link_suffix": "/forum?id=OIEczoib6t",
        "link": "https://openreview.net/forum?id=OIEczoib6t",
        "pdf_link": "https://openreview.net/pdf?id=OIEczoib6t",
        "keywords": "Weak-to-Strong generalization, Superalignment, Ensemble Learning, LLMs",
        "abstract": "How can we harness the collective capabilities of multiple Large Language Models (LLMs) to create an even more powerful model? This question forms the foundation of our research, where we propose an innovative approach to weak-to-strong (w2s) generalization—a critical problem in AI alignment. Our work introduces an easy-to-hard (e2h) framework for studying the feasibility of w2s generalization, where weak models trained on simpler tasks collaboratively supervise stronger models on more complex tasks. This setup mirrors real-world challenges, where direct human supervision is limited. To achieve this, we develop a novel AdaBoost-inspired ensemble method, demonstrating that an ensemble of weak supervisors can enhance the performance of stronger LLMs across classification and generative tasks on difficult QA datasets. In several cases, our ensemble approach matches the performance of models trained on ground-truth data, establishing a new benchmark for w2s generalization. We observe an improvement of upto 14% over existing baseline and an average improvement of 5% and 4% for binary classification and generation task respectively. This research points to a promising direction for enhancing AI through collective supervision, especially in scenarios where labeled data is sparse or insufficient."
    },
    {
        "title": "Generating Freeform Endoskeletal Robots",
        "link_suffix": "/forum?id=awvJBtB2op",
        "link": "https://openreview.net/forum?id=awvJBtB2op",
        "pdf_link": "https://openreview.net/pdf?id=awvJBtB2op",
        "keywords": "co-design, agent design, robots, morphology, evolution, locomotion",
        "abstract": "The automatic design of embodied agents (e.g. robots) has existed for 31 years and is experiencing a renaissance of interest in the literature. To date however, the field has remained narrowly focused on two kinds of anatomically simple robots: (1) fully rigid, jointed bodies; and (2) fully soft, jointless bodies. Here we bridge these two extremes with the open ended creation of terrestrial endoskeletal robots: deformable soft bodies that leverage jointed internal skeletons to move efficiently across land. Simultaneous de novo generation of external and internal structures is achieved by (i) modeling 3D endoskeletal body plans as integrated collections of elastic and rigid cells that directly attach to form soft tissues anchored to compound rigid bodies; (ii) encoding these discrete mechanical subsystems into a continuous yet coherent latent embedding; (iii) optimizing the sensorimotor coordination of each decoded design using model-free reinforcement learning; and (iv) navigating this smooth yet highly non-convex latent manifold using evolutionary strategies. This yields an endless stream of novel species of ``higher robots'' that, like all higher animals, harness the mechanical advantages of both elastic tissues and skeletal levers for terrestrial travel. It also provides a plug-and-play experimental platform for benchmarking evolutionary design and representation learning algorithms in complex hierarchical embodied systems."
    },
    {
        "title": "Latent Variable Identifiability in Nonlinear Causal Models with Single-domain Data under Minimality Condition",
        "link_suffix": "/forum?id=QSuOHV62IQ",
        "link": "https://openreview.net/forum?id=QSuOHV62IQ",
        "pdf_link": "https://openreview.net/pdf?id=QSuOHV62IQ",
        "keywords": "Identifiability theory, Disentangled representation learning, Structural causal model",
        "abstract": "The identifiability of latent variables given observational data is one of the core issues in the field of disentangled representation learning. Recent progresses have been made on establishing identifiablity theories for latent causal models. However with much restrictions or unrealistic assumptions, their practicality on real applications are limited. In this paper, we propose a novel identifiablity theory for learning latent variables in nonlinear causal models, requiring only single-domain data. We prove that all latent variables in a powerset bipartite graph can be identified up to an invertible transformation, if the generation process of observable data is globally invertible, latent variables are independent, and shared latent variables entail minimal information. Experiments on synthetic data support the conclusions of our theory."
    },
    {
        "title": "Hypercone Assisted Contour Generation for Out-of-Distribution Detection",
        "link_suffix": "/forum?id=xE5ZaZGqBW",
        "link": "https://openreview.net/forum?id=xE5ZaZGqBW",
        "pdf_link": "https://openreview.net/pdf?id=xE5ZaZGqBW",
        "keywords": "OOD detection, Out-of-distribution detection, Computer Vision, Deep Learning, Representation Learning",
        "abstract": "Recent advances in the field of out-of-distribution (OOD) detection have placed great emphasis on learning better representations suited to this task. While there have been distance-based approaches, distributional awareness has seldom been exploited for better performance. We present HACk-OOD, a novel OOD detection method that makes no distributional assumption about the data, but automatically adapts to its distribution. Specifically, HACk-OOD constructs a set of hypercones by maximizing the angular distance to neighbors in a given data-point's vicinity, to approximate the contour within which in-distribution (ID) data-points lie. Experimental results show state-of-the-art FPR@95 and AUROC performance on Near-OOD detection and on Far-OOD detection on the challenging CIFAR-100 benchmark without explicitly training for OOD performance."
    },
    {
        "title": "Semantic Skill Extraction via Vision-Language Model Guidance for Efficient Reinforcement Learning",
        "link_suffix": "/forum?id=zY37C8d6bS",
        "link": "https://openreview.net/forum?id=zY37C8d6bS",
        "pdf_link": "https://openreview.net/pdf?id=zY37C8d6bS",
        "keywords": "Reinforcement Learning; Vision-Language Models; Temporal Abstraction",
        "abstract": "Extracting temporally extended skills can significantly improve the efficiency of reinforcement learning (RL) by breaking down complex decision-making problems with sparse rewards into simpler subtasks and enabling more effective credit assignment. However, existing abstraction methods either discover skills in an unsupervised manner, which often lacks semantic information and leads to erroneous or scattered skill extraction results, or require substantial human intervention. In this work, we propose to leverage the extensive knowledge in pretrained Vision-Language Models (VLMs) to progressively guide the latent space after vector quantization to be more semantically meaningful through relabeling each skill. This approach, termedVision-language model guidedTemporalAbstraction (VanTA), facilitates the discovery of more interpretable and task-relevant temporal segmentations from offline data without the need for extensive manual intervention or heuristics. By leveraging the rich information in VLMs, our method can significantly outperform existing offline RL approaches that depend only on limited training data. From a theory perspective, we demonstrate that stronger internal sequential correlations within each sub-task, induced by VanTA, effectively reduces suboptimality in policy learning. We validate the effectiveness of our approach through extensive experiments on diverse environments, including Franka Kitchen, Minigrid, and Crafter. These experiments show that our method surpasses existing approaches in long-horizon offline reinforcement learning scenarios with both proprioceptive and visual observations."
    },
    {
        "title": "AVCAPS: AN AUDIO-VISUAL DATASET WITH MODALITY-SPECIFIC CAPTIONS",
        "link_suffix": "/forum?id=FFUmPQM8c5",
        "link": "https://openreview.net/forum?id=FFUmPQM8c5",
        "pdf_link": "https://openreview.net/pdf?id=FFUmPQM8c5",
        "keywords": "Audio-visual dataset, captioning dataset, Multimodal learning",
        "abstract": "In this paper, we introduce AVCaps, an audio-visual captioning dataset that contains separate textual captions for the audio, visual, and audio-visual contents of video clips. The dataset contains 2061 video clips constituting a total of 28.8 hours. We provide up to 5 captions for the audio, visual, and audio-visual content of each clip, crowdsourced separately. Existing datasets focus on a single modality or do not provide modality-specific captions, limiting the study of how each modality contributes to overall comprehension in multimodal settings. Our dataset addresses this critical gap in multimodal research by offering a resource for studying how audio and visual content are captioned individually, as well as how audio-visual content is captioned in relation to these individual modalities. To counter the bias observed in crowdsourced audio-visual captions, which often emphasize visual over audio content, we generated three audio-visual captions for each clip using our crowdsourced captions by leveraging existing large language models (LLMs). We present multimodal and crossmodal captioning and retrieval experiments to illustrate the effectiveness of modality-specific captions in evaluating model performance. Notably, we show that a model trained on LLM-generated audio-visual captions captures audio information more effectively, achieving 14% higher Sentence-BERT similarity on ground truth audio captions compared to a model trained on crowdsourced audio-visual captions. We also discuss the possibilities in multimodal representation learning, question answering, developing new video captioning metrics, and generative AI that this dataset unlocks. The dataset will be freely available online."
    },
    {
        "title": "Large Legislative Models: Towards Efficient AI Policymaking in Economic Simulations",
        "link_suffix": "/forum?id=hGcxiNUbjy",
        "link": "https://openreview.net/forum?id=hGcxiNUbjy",
        "pdf_link": "https://openreview.net/pdf?id=hGcxiNUbjy",
        "keywords": "large language models, reinforcement learning, policymaking",
        "abstract": "The improvement of economic policymaking presents an opportunity for broad societal benefit, a notion that has inspired research towards AI-driven policymaking tools. AI policymaking holds the potential to surpass human performance through the ability to process data quickly at scale. However, existing RL-based methods exhibit sample inefficiency, and are further limited by an inability to flexibly incorporate nuanced information into their decision-making processes. Thus, we propose a novel method in which we instead utilize pre-trained Large Language Models (LLMs), as sample-efficient policymakers in socially complex multi-agent reinforcement learning (MARL) scenarios. We demonstrate significant efficiency gains, outperforming existing methods across three environments."
    },
    {
        "title": "Linear Recurrent Neural Networks with a Feature-Sequence Twist",
        "link_suffix": "/forum?id=I1484gDBr4",
        "link": "https://openreview.net/forum?id=I1484gDBr4",
        "pdf_link": "https://openreview.net/pdf?id=I1484gDBr4",
        "keywords": "linear recurrent neural networks, RNNs, Sequence model, FST",
        "abstract": "The transformer network architecture has led to advances in artificial intelligence.\nConversational AI applications, such as ChatGPT, and protein folding predictions with AlphaFold are made possible by transformer architectures and the self-attention mechanism.\nHowever, advancing towards more general, flexible, and energy-efficient artificial intelligence may require exploring new architectures that differ significantly from those currently used.\nTransformer networks have largely replaced recurrent neural networks (RNNs) for state-of-the-art performance on sequence-based tasks. \nHowever, in recent years there has been some successful competition from linear recurrent neural networks (LRNNs) and state space models (SSMs).\nA core advantage of LRNNs and SSMs over traditional RNNs is that the hidden states can be calculated in parallel.\nTherefore, like the transformer, they can make efficient use of GPU computation.Unlike the transformer, computational costs of parallelized LRNNs and SSMs can scale sub-quadratically with sequence length.\nDespite these advantages, LRNNs and SSMs often struggle to generate the deep and rich representations that have contributed to the success of transformer architectures.\nWe introduce Feature-Sequence Twisting (FST), a novel technique that transposes the sequence and feature dimensions between LRNN blocks.\nThe purpose of FST is to generate deeper representations of the sequence in subsequent LRNN blocks. \nSince the computational cost of LRNNs scale sub-quadratically with sequence length, FST remains practical to compute even for large feature dimensions.\nOur experiments demonstrate that the FST architecture outperforms transformer networks on tasks such as Long ListOps, achieving performance competitive with state-of-the-art models."
    },
    {
        "title": "STL-Drive: Formal Verification Guided End-to-end Automated Driving",
        "link_suffix": "/forum?id=DCg9r2DKKe",
        "link": "https://openreview.net/forum?id=DCg9r2DKKe",
        "pdf_link": "https://openreview.net/pdf?id=DCg9r2DKKe",
        "keywords": "Formal Verification, Automated Driving, Imitation Learning, Robustness, Safety",
        "abstract": "End-to-end automated driving behavior models require extensive training data from machine or human driver experts or interacting with the environment to learn a driving policy. Not all human driver expert data represent safe driving that the end-to-end model is learning to imitate, and similarly, neither are some of the behaviors learned during exploration while learning by trial and error. However, the models should learn from such data without being negatively affected during the learning process. We aim to provide a learning framework to incorporate formal verification methods to improve the robustness and safety of the learned models in the presence of training data that contain unsafe behaviors, dubbed as STL-Drive. We are particularly interested in utilizing this framework to enhance the safety of end-to-end automated driving models. In this work, we incorporate Signal Temporal Logic (STL) as the formal method to impose safety constraints. In addition, we utilize the Responsibility-Sensitive Safety (RSS) framework to define the safety constraints. We designed a loss function that combines the task objectives and the STL robustness score to balance the learned policy's performance and safety. We demonstrate that encoding safety constraints using STL and utilizing the robustness score during training improves the performance and safety of the driving policy. We validate our framework using open-loop predictive simulator NAVSIM and real-world data from OpenScene. The results of this study suggest a promising research direction where formal methods can enhance the safety and resilience of deep learning models. Formal verification of safety constraints for automated driving will further increase the public's trust in automated vehicles."
    },
    {
        "title": "ScImage: How good are multimodal large language models at scientific text-to-image generation?",
        "link_suffix": "/forum?id=ugyqNEOjoU",
        "link": "https://openreview.net/forum?id=ugyqNEOjoU",
        "pdf_link": "https://openreview.net/pdf?id=ugyqNEOjoU",
        "keywords": "LLMs, multimodality, science, image generation",
        "abstract": "Multimodal large language models (LLMs) have demonstrated impressive capabilities in generating high-quality images from textual instructions. However, their performance in generating scientific images—a critical application for accelerating scientific progress—remains underexplored. In this work, we address this gap by introducing ScImage, a benchmark designed to evaluate the multimodal capabilities of LLMs in generating scientific images from textual descriptions. ScImage assesses three key dimensions of understanding: spatial, numeric, and attribute comprehension, as well as their combinations, focusing on the relationships between scientific objects (e.g., squares, circles). We evaluate five LLMs—GPT4-o, Llama, AutomaTikZ, Dall-E, and StableDiffusion—using two modes of output generation: code-based outputs (Python, TikZ) and direct raster image generation. Additionally, we examine four different input languages: English, German, Farsi, and Chinese. Our evaluation, conducted with 11 scientists across three criteria (correctness, relevance, and scientific accuracy), reveals that while GPT4-o produces outputs of decent quality for simpler prompts involving individual dimensions such as spatial, numeric, or attribute understanding in isolation, all models face challenges in this task, especially for more complex prompts."
    },
    {
        "title": "Investigating Language-Specific Calibration For Pruning Multilingual Large Language Models",
        "link_suffix": "/forum?id=a0ftEY6puc",
        "link": "https://openreview.net/forum?id=a0ftEY6puc",
        "pdf_link": "https://openreview.net/pdf?id=a0ftEY6puc",
        "keywords": "multilinguality, pruning, large-language models, interpretability",
        "abstract": "Recent advances in large language model (LLM) pruning have shown state-of-the-art (SotA) compression results in post-training and retraining-free settings while maintaining high predictive performance. However, previous research mainly considered calibrating based on English text, despite the multilingual nature of modern LLMs and their frequent use in non-English languages. In this paper, we set out to investigate calibrating the pruning of multilingual language models for monolingual applications. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse languages, tasks, models, and SotA pruning techniques. Our results offer practical suggestions, for example, calibrating in the target language can efficiently retain the language modeling capability but does not necessarily benefit downstream tasks. Through further analysis of latent subspaces, pruning masks, and individual neurons within pruned models, we find that while pruning generally preserves strong language-specific features, it may fail to retain language-specific neuron activation patterns and subtle, language-agnostic features associated with knowledge and reasoning that are needed for complex tasks."
    },
    {
        "title": "Scalable Message Passing Neural Networks: No Need for Attention in Large Graph Representation Learning",
        "link_suffix": "/forum?id=TCgcEQjaUQ",
        "link": "https://openreview.net/forum?id=TCgcEQjaUQ",
        "pdf_link": "https://openreview.net/pdf?id=TCgcEQjaUQ",
        "keywords": "Message Passing Neural Networks, Graph Representation Learning, Large Graphs",
        "abstract": "We propose Scalable Message Passing Neural Networks (SMPNNs) and demonstrate that, by integrating standard convolutional message passing into a Pre-Layer Normalization Transformer-style block instead of attention, we can produce high-performing deep message-passing-based Graph Neural Networks (GNNs). This modification yields state-of-the-art results in large graph transductive learning, outperforming the best Graph Transformers in the literature without requiring the otherwise computationally and memory-expensive attention. Our architecture not only scales to large graphs but also makes it possible to construct deep message-passing networks, unlike simple GNNs, which have traditionally been constrained to shallow architectures due to oversmoothing. Moreover, we provide a new theoretical analysis of oversmoothing based on universal approximation which we use to motivate SMPNNs."
    },
    {
        "title": "MOEfication by Experts as Masks",
        "link_suffix": "/forum?id=762u1p9dgg",
        "link": "https://openreview.net/forum?id=762u1p9dgg",
        "pdf_link": "https://openreview.net/pdf?id=762u1p9dgg",
        "keywords": "sparse activated, mixture-of-experts, L0 regularization",
        "abstract": "In this work, we investigate how to sparsify a pre-trained dense large language model into a mixture-of-experts (MoE) architecture for faster inference. Our approach applies mask matrix to the activations for each expert, constrained by $L_0$ regularization to minimize the number of activated parameters. Starting with all parameters active, the model is progressively sparsified during training, ensuring minimal performance loss. This approach proves more efficient than one-shot sparsification techniques~\\citep{zhang2022moefication}, which typically require significant resources for performance recovery. Moreover, our approach automatically identifies shared, token-specific, and inactive experts, allowing for more efficient allocation of computational resources. Through extensive experiments, we achieve up to 97% performance retention on downstream tasks with only 50% of the feed-forward parameters activated in dense models. Beyond enhancing inference efficiency, this strategy of sharing computational units among experts presents a valuable framework for designing more generalized and efficient MoE architectures, opening avenues for future advancements in expert-based models."
    },
    {
        "title": "Communication-Efficient Federated Learning via Model-Agnostic Projection Adaptation",
        "link_suffix": "/forum?id=rhfOzJzsKN",
        "link": "https://openreview.net/forum?id=rhfOzJzsKN",
        "pdf_link": "https://openreview.net/pdf?id=rhfOzJzsKN",
        "keywords": "Federated Learning, Low-Rank Adaptation, Communication Efficiency, Subspace Optimization",
        "abstract": "Federated learning (FL) enables collaborative model training across distributed clients without centralizing sensitive raw data while benefiting from diverse data sources. \nDespite recent advancements in FL, the communication overhead remains a significant challenge, especially for large-scale models.\nRecent low-rank adaptation (LoRA) techniques have shown promise in reducing these burdens in FL, but they are typically applied to each layer individually and depend on the model architecture, which limits their performance.\nTo address these shortcomings, we propose  Model-Agnostic Projection Adaptation (MAPA), a novel approach that applies factorization to the entire model parameter space, which we view as asingle vector, regardless of the number of layers and model architecture. \nMAPA factorizes the single-vector model update into a fixedreconstruction matrixand a trainableprojection vector, with the reconstruction matrix being randomly initialized using a shared seed at each round. \nThis ensures thatonlythe projection vectors need to be communicated to the server, thereby reducing the communication cost.\nFurthermore, MAPA's vector-based representation and relaxed rank constraints allow for a larger reconstruction matrix and smaller projection vector dimensions compared to LoRA, enhancing the expressiveness of model updates while significantly reducing communication overhead. \nExperimental results demonstrate that MAPA outperforms existing FL methods in both communication efficiency and model performance, effectively coupling optimization and communication efficiency in FL environments."
    },
    {
        "title": "Certified Robustness to Data Poisoning in Gradient-Based Training",
        "link_suffix": "/forum?id=ExUC9dQJhQ",
        "link": "https://openreview.net/forum?id=ExUC9dQJhQ",
        "pdf_link": "https://openreview.net/pdf?id=ExUC9dQJhQ",
        "keywords": "Data Poisoning, Certified Robustness, Neural Networks",
        "abstract": "Modern machine learning pipelines leverage large amounts of public data, making it infeasible to guarantee data quality and leaving models open to poisoning and backdoor attacks. Provably bounding model behavior under such attacks remains an open problem. In this work, we address this challenge by developing the first framework providing provable guarantees on the behavior of models trained with potentially manipulated data without modifying the model or learning algorithm. In particular, our framework certifies robustness against untargeted and targeted poisoning, as well as backdoor attacks, for bounded and unbounded manipulations of the training inputs and labels. Our method leverages convex relaxations to over-approximate the set of all possible parameter updates for a given poisoning threat model, allowing us to bound the set of all reachable parameters for any gradient-based learning algorithm. Given this set of parameters, we provide bounds on worst-case behavior, including model performance and backdoor success rate. We demonstrate our approach on multiple real-world datasets from applications including energy consumption, medical imaging, and autonomous driving."
    },
    {
        "title": "DECISION-FOCUSED UNCERTAINTY QUANTIFICATION",
        "link_suffix": "/forum?id=iOMnn1hSBO",
        "link": "https://openreview.net/forum?id=iOMnn1hSBO",
        "pdf_link": "https://openreview.net/pdf?id=iOMnn1hSBO",
        "keywords": "Decision-focused learning, decision making, uncertainty quantification, healthcare",
        "abstract": "There is increasing interest in ``decision-focused\" machine learning methods which train models to account for how their predictions are used in downstream optimization problems. Doing so can often improve performance on subsequent decision problems. However, current methods for uncertainty quantification do not incorporate any information at all about downstream decisions. We develop a framework based on conformal prediction to produce prediction sets that account for a downstream decision loss function, making them more appropriate to inform high-stakes decision-making. Our approach harnesses the strengths of conformal methods—modularity, model-agnosticism, and statistical coverage guarantees—while incorporating downstream decisions and user-specified utility functions. We prove that our methods retain standard coverage guarantees.  Empirical evaluation across a range of datasets and utility metrics demonstrates that our methods achieve significantly lower decision loss compared to standard conformal methods. Additionally, we present a real-world use case in healthcare diagnosis, where our method effectively incorporates the hierarchical structure of dermatological diseases. It successfully generates sets with coherent diagnostic meaning, aiding the triage process during dermatology diagnosis and illustrating how our method can ground high-stakes decision-making on external domain knowledge."
    },
    {
        "title": "Do graph neural network states contain graph properties?",
        "link_suffix": "/forum?id=RdTYx4jd7C",
        "link": "https://openreview.net/forum?id=RdTYx4jd7C",
        "pdf_link": "https://openreview.net/pdf?id=RdTYx4jd7C",
        "keywords": "Graph Neural Networks, explainability, interpretability, Diagnostic classifiers",
        "abstract": "Graph learning models achieve state of the art performance on many tasks, but do so at ever larger model sizes. Accordingly, the complexity of their representations increase. Explainability techniques (XAI) have made remarkable progress in the interpretability of ML models. However, the non-relational nature of Graph Neural Networks (GNNs) make it difficult to reuse already existing XAI methods. While other works have focused on instance-based explanation methods for GNNs, very few have investigated model-based methods and, to our knowledge, none have tried to probe the embedding of the GNNs for well-known structural graph properties. In this paper we present a model agnostic explainability pipeline for Graph Neural Networks (GNNs) employing diagnostic classifiers. This pipeline aims to probe and interpret the learned representations in GNNs across various architectures and datasets, refining our understanding and trust in these models."
    },
    {
        "title": "CtrlSynth: Controllable Image Text Synthesis for Data-Efficient Multimodal Learning",
        "link_suffix": "/forum?id=1X85iw7tqY",
        "link": "https://openreview.net/forum?id=1X85iw7tqY",
        "pdf_link": "https://openreview.net/pdf?id=1X85iw7tqY",
        "keywords": "clip, synthetic data, multimodal learning, longtail",
        "abstract": "Pretraining strong vision or multimodal foundation models like CLIP relies on large-scale datasets (e.g., image-text pairs) that may be noisy, potentially misaligned, and have long-tail distributions. Previous work has shown promising results in augmenting datasets by generating synthetic samples. However, they only support domain-specific ad hoc use cases (like for image or text alone) and are limited in data diversity due to a lack of fine-grained control over the synthesis process. \nWe design a controllable image-text synthesis pipeline called CtrlSynth to enable data-efficient multimodal learning and improve vision and multimodal models in various use cases. The key idea is to decompose the visual semantics of an image into basic elements, apply user-specified control policies (e.g. remove, add, replace operations), and recompose them to synthesize images or texts. The decompose and recompose feature in CtrlSynth allows users to control data synthesis in a fine-grained manner by defining customized control policies to manipulate the basic elements. CtrlSynth leverages the capabilities of pretrained foundation models such as large language models (LLMs) or diffusion models (DMs) to reason and recompose basic elements such that synthetic samples are natural and composed in diverse ways. CtrlSynth pipeline is training-free and has a modular design, making it easy to support different pretrained models. \nCtrlSynth pipeline is also closed-loop, meaning it can synthesize text data based on the image or vice versa. Our evaluation shows that CtrlSynth samples substantially improve zero-shot classification, image-text retrieval, and compositional reasoning performance of CLIP models. We will publicly release the code and pipeline for future research."
    },
    {
        "title": "MEMFREEZING: TOWARDS PRACTICAL ADVERSARIAL ATTACKS ON TEMPORAL GRAPH NEURAL NETWORKS",
        "link_suffix": "/forum?id=8sCjS69c81",
        "link": "https://openreview.net/forum?id=8sCjS69c81",
        "pdf_link": "https://openreview.net/pdf?id=8sCjS69c81",
        "keywords": "Graph Neural Networks, Dynamic Graph, Adversarial Attack, Temporal Graph Neural Network",
        "abstract": "Temporal graph neural networks (TGNN) have achieved significant momentum in many real-world dynamic graph tasks, making it urgent to study their robustness against adversarial attacks in real-world scenarios.\nExisting TGNN adversarial attacks assume that attackers have complete knowledge of the input graphs. However, this is unrealistic in real-world scenarios, where attackers can, at best, access information about existing nodes and edges but not future ones at the time of the attack. However, applying effective attacks with only up-to-attack knowledge is particularly challenging due to the dynamic nature of TGNN input graphs. On the one hand, graph changes after the attacks may diminish the impact of attacks on the affected nodes.\nOn the other hand, targeting nodes that are unseen at the attack time introduces significant challenges.\nTo address these challenges, we introduce a novel adversarial attack framework, MemFreezing, to yield long-lasting and spreading adversarial attacks on TGNNs without the necessity to know knowledge about the post-attack changes in the dynamic graphs.\nMemFreezing strategically introduces fake nodes or edges to induce nodes' memories into similar and stable states, which we call the `frozen state.' In this state, nodes can no longer sense graph changes or carry information, thereby disrupting predictions.\nIn subsequent updates, these affected nodes maintain and propagate their frozen state with support from their neighboring nodes. \nThe experimental results demonstrate that MemFreezing can persistently decrease the TGNN models' performances in various tasks, delivering more effective attacks under practical setups."
    },
    {
        "title": "Dynamic Reconstruction of Hand-Object Interaction with Distributed Force-aware Contact Representation",
        "link_suffix": "/forum?id=J4D5WVoc5g",
        "link": "https://openreview.net/forum?id=J4D5WVoc5g",
        "pdf_link": "https://openreview.net/pdf?id=J4D5WVoc5g",
        "keywords": "Tactile sensing, hand-object tracking and reconstruction",
        "abstract": "We present ViTaM-D, a novel visual-tactile framework for dynamic hand-object interaction reconstruction, integrating distributed tactile sensing for more accurate contact modeling. While existing methods focus primarily on visual inputs, they struggle with capturing detailed contact interactions such as object deformation. Our approach leverages distributed tactile sensors to address this limitation by introducing DF-Field. This distributed force-aware contact representation models both kinetic and potential energy in hand-object interaction.\nViTaM-D first reconstructs hand-object interactions using a visual-only network, VDT-Net, and then refines contact details through a force-aware optimization (FO) process, enhancing object deformation modeling. To benchmark our approach, we introduce the HOT dataset, which features 600 sequences of hand-object interactions, including deformable objects, built in a high-precision simulation environment.\nExtensive experiments on both the DexYCB and HOT datasets demonstrate significant improvements in accuracy over previous state-of-the-art methods such as gSDF and HOTrack. Our results highlight the superior performance of ViTaM-D in both rigid and deformable object reconstruction, as well as the effectiveness of DF-Field in refining hand poses. This work offers a comprehensive solution to dynamic hand-object interaction reconstruction by seamlessly integrating visual and tactile data. Codes, models, and datasets will be available."
    }
]