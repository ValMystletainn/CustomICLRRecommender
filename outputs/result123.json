[{"title": "Anti-Correlated Noise in Epoch-Based Stochastic Gradient Descent: Implications for Weight Variances", "link_suffix": "/forum?id=FV6rPMwmuG", "link": "https://openreview.net/forum?id=FV6rPMwmuG", "pdf_link": "https://openreview.net/pdf?id=FV6rPMwmuG", "keywords": "Stochastic Gradient Descent, Asymptotic Analysis, Discrete Time, Hessian", "abstract": "Stochastic Gradient Descent (SGD) has become a cornerstone of neural network optimization due to its computational efficiency and generalization capabilities. However, the noise introduced by SGD is often assumed to be uncorrelated over time, despite the common practice of epoch-based training where data is sampled without replacement. In this work, we challenge this assumption and investigate the effects of epoch-based noise correlations on the stationary distribution of discrete-time SGD with momentum. Our main contributions are twofold: First, we calculate the exact autocorrelation of the noise during epoch-based training under the assumption that the noise is independent of small fluctuations in the weight vector, revealing that SGD noise is inherently anti-correlated over time. Second, we explore the influence of these anti-correlations on the variance of weight fluctuations. We find that for directions with curvature of the loss greater than a hyperparameter-dependent crossover value, the conventional results for uncorrelated noise are recovered. However, for relatively flat directions, the weight variance is significantly reduced, leading to a considerable decrease in loss fluctuations compared to the constant weight variance assumption. Furthermore, we demonstrate that training with these anti-correlations enhances test performance, suggesting that the inherent noise structure induced by epoch-based training plays a crucial role in finding flatter minima that generalize better.", "title_embedding_index": 6100, "title_abs_embedding_index": 6125}, {"title": "Energy-Oriented Alignment for Large Language Models", "link_suffix": "/forum?id=O2DVmb0pwo", "link": "https://openreview.net/forum?id=O2DVmb0pwo", "pdf_link": "https://openreview.net/pdf?id=O2DVmb0pwo", "keywords": "datasets and benchmarks, jailbreak, energy attack, large language model, efficiency, denial-of-service", "abstract": "Large language models (LLMs) have showcased remarkable capabilities on a variety of natural language processing (NLP) tasks, powering various real-world applications.\nEnsuring the safe and effective deployment of LLMs requires careful alignment to mitigate risks associated with malicious inputs, which now mainly involve toxic content and misinformation.\nIn this study, we expand this focus by identifying and exploring a novel category of energy-oriented malicious instructions, akin to Denial-of-Service (DoS) attacks.\nThese instructions provoke LLMs to generate excessively lengthy responses through impractical tasks, resulting in high energy and computational resource consumption, and even risking system overload.\nTo address this gap, we curate EnergyAlign, the first energy-oriented malicious instruction dataset with 8 diverse categories.\nThen, we conduct a comprehensive evaluation of 5 advanced proprietary LLMs and 24 open-source LLMs.\nThe results reveal a notable disparity: while proprietary LLMs can refuse such malicious inputs, most open-source LLMs are extremely vulnerable with a failure rate of up to 96.8%.\nAdditionally, we assess the effectiveness of jailbreak techniques in bypassing the energy-related safety measures of proprietary models.\nLastly, we highlight the inadequacies of existing defense mechanisms and propose energy-oriented alignment data against EnergyAlign for future research.", "title_embedding_index": 6101, "title_abs_embedding_index": 6126}, {"title": "ShieldHead: Decoding-time Safeguard for Large Language Models", "link_suffix": "/forum?id=NHCkILEmWn", "link": "https://openreview.net/forum?id=NHCkILEmWn", "pdf_link": "https://openreview.net/pdf?id=NHCkILEmWn", "keywords": "Large Language Model, Safety Guard, Content Moderation", "abstract": "In light of the widespread deployment of Large Language Models (LLMs), the responsibility for safeguarding and regulating LLM-generated content has taken on heightened significance. Recent advancements in LLM-based moderation methods, e.g., LlamaGuard, have demonstrated remarkable promise in identifying safety risks associated with both inputs and outputs in human-AI interactions. However, integrating LLM-based safeguards into a chatbot system requires an additional inference stage involving a moderation LLM with billions of parameters, which significantly increases computational costs and reduces overall efficiency. In this paper, we demonstrate that simply learning a classification head on the last-layer hidden states of the dialogue model provides a strong capability to identify harmful contents. The classification head, referred to as ShieldHead, serves as an auxiliary branch paralleled with next-token-prediction LM head, enabling the detection of potential risks in past text sequences. Additionally, a label disambiguation technique is employed to supervise ShieldHead with both token-level and sentence-level labels, which further enhances its performance. ShieldHead exhibits remarkable efficiency during inference, providing real-time moderation results alongside token-wise streaming output during the chatbot system's decoding phase. Extensive experimental results demonstrate the superiority of the proposed framework: a state-of-the-art performance on the XSTest and SafeRLHF datasets while running at a speed about 300\u00d7 faster (<1ms) than previous LLM-based moderation models with \uff5e99% less parameters of LlamaGuard.", "title_embedding_index": 6102, "title_abs_embedding_index": 6127}, {"title": "Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts", "link_suffix": "/forum?id=JSB171dSUU", "link": "https://openreview.net/forum?id=JSB171dSUU", "pdf_link": "https://openreview.net/pdf?id=JSB171dSUU", "keywords": "Multilingual LLM, Medical LLM", "abstract": "Adapting medical Large  Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a \\textit{Spread Out in the End} information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of \\textit{language family} experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.", "title_embedding_index": 6103, "title_abs_embedding_index": 6128}, {"title": "ProPicker: Promptable Segmentation for Particle Picking in Cryogenic Electron Tomography", "link_suffix": "/forum?id=IWHvZE3V9L", "link": "https://openreview.net/forum?id=IWHvZE3V9L", "pdf_link": "https://openreview.net/pdf?id=IWHvZE3V9L", "keywords": "cryo-ET, cryo-EM, cryogenic electron tomography, cryogenic electron microscopy, particle, picking, particle picking, object detection", "abstract": "Cryogenic electron tomography (cryo-ET) can produce detailed 3D images called\ntomograms of cellular environments. An essential step of cryo-ET reconstruction and analysis is to find all instances of a protein in tomograms, a task known as particle picking. Due to the low signal-to-noise ratio, artifacts, and vast diversity in proteins, particle picking is a challenging 3D object detection problem. Existing approaches are either slow or limited to picking a few particles of interest, which requires large annotated and difficult to obtain training datasets. In this work, we propose ProPicker, a fast and universal particle picker that can detect particles beyond those in the training set. Our promptable design allows for selectively detecting a specific protein in the volume based on an input prompt. Our experiments demonstrate that through a favorable trade-off between performance and speed, ProPicker can achieve performance close to or on par with state-of-the-art universal pickers, while being up to an order of magnitude faster. Moreover, ProPicker can be efficiently adapted to new proteins through fine-tuning on few annotated samples.", "title_embedding_index": 6104, "title_abs_embedding_index": 6129}, {"title": "Multi-Resolution Decomposable Diffusion Model for Non-Stationary Time Series Anomaly Detection", "link_suffix": "/forum?id=eWocmTQn7H", "link": "https://openreview.net/forum?id=eWocmTQn7H", "pdf_link": "https://openreview.net/pdf?id=eWocmTQn7H", "keywords": "Diffusion Model, Non-Stationary Time Series, Anomaly Detection, Multi-Resolution", "abstract": "Recently, generative models have shown considerable promise in unsupervised time series anomaly detection. Nonetheless, the task of effectively capturing complex temporal patterns and minimizing false alarms becomes increasingly challenging when dealing with non-stationary time series, characterized by continuously fluctuating statistical attributes and joint distributions. To confront these challenges, we underscore the benefits of multi-resolution modeling, which improves the ability to distinguish between anomalies and non-stationary behaviors by leveraging correlations across various resolution scales. In response, we introduce aMulti-ResolutionDecomposable DiffusionModel (MODEM), which integrates a coarse-to-fine diffusion paradigm with a frequency-enhanced decomposable network to adeptly navigate the intricacies of non-stationarity. Technically, the coarse-to-fine diffusion model embeds cross-resolution correlations into the forward process to optimize diffusion transitions mathematically. It then innovatively employs low-resolution recovery to guide the reverse trajectories of high-resolution series in a coarse-to-fine manner, enhancing the model's ability to learn and elucidate underlying temporal patterns. Furthermore, the frequency-enhanced decomposable network operates in the frequency domain to extract globally shared time-invariant information and time-variant temporal dynamics for accurate series reconstruction. Extensive experiments conducted across five real-world datasets demonstrate that our proposed MODEM achieves state-of-the-art performance and can be generalized to other time series tasks. The code will be publicly available upon acceptance.", "title_embedding_index": 6105, "title_abs_embedding_index": 6130}, {"title": "True Counterfactual Generation from Language Models", "link_suffix": "/forum?id=TUC0ZT2zIQ", "link": "https://openreview.net/forum?id=TUC0ZT2zIQ", "pdf_link": "https://openreview.net/pdf?id=TUC0ZT2zIQ", "keywords": "Causality, language models, counterfactuals", "abstract": "Understanding and manipulating the causal mechanisms in language models is essential for controlling their behavior. Previous work has primarily relied on techniques such as representation surgery---e.g., model ablations or manipulation of linear subspaces tied to specific concepts---to \\emph{intervene} on these models. To understand the impact of interventions precisely, it is useful to examine \\emph{counterfactual} strings---e.g., how a given sentence would have appeared had it been generated by the model following a specific intervention. We highlight that counterfactual reasoning is conceptually distinct from interventions, as articulated in Pearl's causal hierarchy. Based on this observation, we propose a framework for generating true string counterfactuals by reformulating language models as Generalized Structural-equations Models (GSEMs) using the Gumbel-max trick. This allows us to model the joint distribution over original strings and their counterfactuals resulting from the same instantiation of the sampling noise. We develop an algorithm based on hindsight Gumbel sampling that allows us to infer the latent noise variables and generate counterfactuals of observed strings. Experiments demonstrate that our approach produces meaningful counterfactuals while at the same time showing that commonly used intervention techniques have considerable undesired side effects.", "title_embedding_index": 6106, "title_abs_embedding_index": 6131}, {"title": "Learning High-Order Substructure Association from Molecules with Transformers", "link_suffix": "/forum?id=B6B6EhC1bW", "link": "https://openreview.net/forum?id=B6B6EhC1bW", "pdf_link": "https://openreview.net/pdf?id=B6B6EhC1bW", "keywords": "Molecule, repesentation learning, drug discovery, ADMET, drug properties prediction", "abstract": "Molecular graphs are commonly represented using SMILES (Simplified Molecular Input Line Entry System) strings, enabling the transformation of molecular graphs into token sequences. While transformers\u2014powerful neural networks originally developed for natural language processing\u2014have been adapted for learning molecular representations from SMILES by predicting masked tokens, they have yet to achieve competitive performance on ADMET benchmark datasets crucial for assessing drug properties such as absorption, distribution, metabolism, excretion, and toxicity. This paper identifies the challenge that traditional random token masking in SMILES overlooks essential molecular substructures, leading transformers to focus on superficial correlations between individual tokens rather than their relationships within substructures. We propose a novel approach that enhances transformers' capability to recognize molecular substructures by introducing a substructure-aware masking strategy alongside a new learning objective. This method embeds substructure information directly into the masking and prediction process, allowing the model to predict specific subgraphs instead of random tokens. Our experiments demonstrate that transformers employing this dual innovation outperform those utilizing conventional random masking, resulting in improved predictions of drug-related properties on ADMET benchmarks. This work contributes to the ongoing advancement of transformer architectures in the field of molecular representation learning.", "title_embedding_index": 6107, "title_abs_embedding_index": 6132}, {"title": "GlocalCLIP: Object-agnostic Global-Local Prompt Learning for Zero-shot Anomaly Detection", "link_suffix": "/forum?id=mgjRSXGnF0", "link": "https://openreview.net/forum?id=mgjRSXGnF0", "pdf_link": "https://openreview.net/pdf?id=mgjRSXGnF0", "keywords": "Zero-shot anomaly detection, Prompt learning, Object-agnostic glocal semantic prompt design, Deep-text prompt tuning, Glocal contrastive learning", "abstract": "Zero-shot anomaly detection (ZSAD) is crucial for detecting abnormal patterns in target datasets without using training samples, specifically in scenarios where there are distributional differences between the target domain and training data or where data scarcity arises because of restricted access. Although recently pretrained vision-language models demonstrate strong zero-shot performance across various visual tasks, they focus on learning class semantics, which makes their direct application to ZSAD challenging. To address this scenario, we propose GlocalCLIP, which uniquely separates global and local prompts and jointly optimizes them. This approach enables the object-agnostic glocal semantic prompt design to effectively capture general normal and anomalous patterns without dependency on specific objects in the image. We refine the text prompts for more precise adjustments by utilizing deep-text prompt tuning in the text encoder. In the vision encoder, we apply a V-V attention layer to capture detailed local image features. Finally, we introduce glocal contrastive learning to improve the complementary learning of global and local prompts, effectively detecting abnormal patterns across various domains. The generalization performance of GlocalCLIP in ZSAD was demonstrated on 15 real-world datasets from both the industrial and medical domains, achieving superior performance compared to existing methods.", "title_embedding_index": 6108, "title_abs_embedding_index": 6133}, {"title": "Federated Maximum Likelihood Inverse Reinforcement Learning with Convergence Guarantee", "link_suffix": "/forum?id=hnpXIEaOrS", "link": "https://openreview.net/forum?id=hnpXIEaOrS", "pdf_link": "https://openreview.net/pdf?id=hnpXIEaOrS", "keywords": "Inverse Reinforcement  Learning, Decentralized learning.", "abstract": "Inverse Reinforcement Learning (IRL) aims to recover the latent reward function and corresponding optimal policy from observed demonstrations. Existing IRL research predominantly focuses on a centralized learning approach, not suitable for real-world problems with distributed data and privacy restrictions. To this end, this paper proposes a novel algorithm for federated maximum-likelihood IRL (F-ML-IRL) and provides a rigorous analysis of its convergence and time-complexity. The proposed F-ML-IRL leverages a dual-aggregation to update the shared global model and performs bi-level local updates -- an upper-level learning task to optimize the parameterized reward function by maximizing the discounted likelihood of observing expert trajectories under the current policy and a low-level learning task to find the optimal policy concerning the entropy-regularized discounted cumulative reward under the current reward function. We analyze the convergence and time-complexity of the proposed F-ML-IRL algorithm and show that the global model in F-ML-IRL converges to a stationary point for both the reward and policy parameters within finite time, i.e., the log-distance between the recovered policy and the optimal policy, as well as the gradient of the likelihood objective, converge to zero. Finally, evaluating our F-ML-IRL algorithm on high-dimensional robotic control tasks in MuJoCo, we show that it ensures convergences of the recovered reward in decentralized learning and even outperforms centralized baselines due to its ability to utilize distributed data.", "title_embedding_index": 6109, "title_abs_embedding_index": 6134}, {"title": "A Finite-Time Analysis of Distributed Q-Learning", "link_suffix": "/forum?id=Z3n2QauIIk", "link": "https://openreview.net/forum?id=Z3n2QauIIk", "pdf_link": "https://openreview.net/pdf?id=Z3n2QauIIk", "keywords": "reinforcement learning, multi-agent, distributed learning, q-learning", "abstract": "Multi-agent reinforcement learning (MARL) has witnessed a remarkable surge in interest, fueled by the empirical success achieved in applications of single-agent reinforcement learning (RL). In this study, we consider a distributed Q-learning scenario, wherein a number of agents cooperatively solve a sequential decision making problem without access to the central reward function which is an average of the local rewards. In particular, we study finite-time analysis of a distributed Q-learning algorithm, and provide a new sample complexity result of $\\tilde{\\mathcal{O}}\\left( \\max\\left\\{  \\frac{1}{\\epsilon^2}\\frac{t_{\\text{mix}}}{(1-\\gamma)^6 d_{\\min}^4 }  ,\\frac{1}{\\epsilon}\\frac{\\sqrt{|\\mathcal{S}||\\mathcal{A}|}}{(1-\\sigma_2(\\boldsymbol{W}))(1-\\gamma)^4 d_{\\min}^3}  \\right\\} \\right)$ under tabular lookup setting for Markovian observation model.", "title_embedding_index": 6110, "title_abs_embedding_index": 6135}, {"title": "Decoupled Subgraph Federated Learning", "link_suffix": "/forum?id=v1rFkElnIn", "link": "https://openreview.net/forum?id=v1rFkElnIn", "pdf_link": "https://openreview.net/pdf?id=v1rFkElnIn", "keywords": "Federated Learning, Subgraph Federated Learning, Inter-Connected Graphs, GNN, Decoupled GCN", "abstract": "We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.", "title_embedding_index": 6111, "title_abs_embedding_index": 6136}, {"title": "Harnessing Diversity for Important Data Selection in Pretraining Large Language Models", "link_suffix": "/forum?id=bMC1t7eLRc", "link": "https://openreview.net/forum?id=bMC1t7eLRc", "pdf_link": "https://openreview.net/pdf?id=bMC1t7eLRc", "keywords": "LLMs, data selection, influence function, diversity", "abstract": "Data selection is of great significance in  pretraining large language models, given the  variation in quality within the large-scale available training corpora. \nTo achieve this, researchers are currently investigating the use of data influence to measure the importance of data instances, $i.e.,$ a high influence score indicates that incorporating this instance to the training set is likely to enhance the model performance. Consequently, they select the top-$k$ instances with the highest scores.  However, this approach has several limitations. \n(1) Calculating the accurate influence of all available data is time-consuming.\n(2) The selected data instances are not diverse enough, which may hinder the pretrained model's ability to generalize effectively to various downstream tasks.\nIn this paper, we introduce $\\texttt{Quad}$, a data selection approach that considers both quality and diversity by using data influence to achieve state-of-the-art pretraining results.\nTo compute the influence ($i.e.,$ the quality) more accurately and efficiently, we incorporate the attention layers to capture more semantic details, which can be accelerated through the Kronecker product. \nFor the diversity, $\\texttt{Quad}$ clusters the dataset into similar data instances within each cluster and diverse instances across different clusters. For each cluster, if we opt to select data from it, we take some samples to evaluate the influence to prevent processing all instances. Overall, we favor clusters with highly influential instances (ensuring high quality) or clusters that have been selected less frequently (ensuring diversity), thereby well balancing between quality and diversity.  Experiments on Slimpajama demonstrate that $\\texttt{Quad}$ significantly outperforms other data selection methods with a low FLOPs consumption. Further analysis also validates the effectiveness of our influence calculation.", "title_embedding_index": 6112, "title_abs_embedding_index": 6137}, {"title": "PRUNING CNNS WITH GRAPH RANDOM WALK & RANDOM MATRIX THEORY", "link_suffix": "/forum?id=AvLFLLqG0b", "link": "https://openreview.net/forum?id=AvLFLLqG0b", "pdf_link": "https://openreview.net/pdf?id=AvLFLLqG0b", "keywords": "Model Compression, Graph Learning, Random Matrix Theory", "abstract": "To facilitate the deployment of convolutional neural networks on resource-limited devices, filter pruning has emerged as an effective strategy because of its enabled practical acceleration. Evaluating the importance of filters is a crucial challenge in this field. Most existing works on filter pruning assess the relationships of filters using pairwise measures such as Euclidean distance and cosine correlation, which may not capture the global information within the layer. In this paper, we propose a novel filter pruning method, which leverages a graph-based approach to model the relationships among filters in convolutional layers. Each filter is represented as a node in a directed graph, and the edges between nodes capture the linear dependencies between filters. This structure allows us to assess the relative importance of each filter by conducting a random walk on the graph. Filters that exhibit weaker connections to others are considered less important and are pruned with minimal impact on model performance. Furthermore, we examine the eigenvalue spectrum of the adjacency matrix and observe a distribution similar to that of the spiked models in random matrix theory. This suggests that the spiked eigenvalues could serve as a significant indicator of the importance of each convolutional layer. We conduct image classification on CIFAR-10 and ImageNet to demonstrate the superiority of our method over the state-of-the-arts.", "title_embedding_index": 6113, "title_abs_embedding_index": 6138}, {"title": "Evaluating the World Models Used by Pretrained Learners", "link_suffix": "/forum?id=QtKYYatG3Z", "link": "https://openreview.net/forum?id=QtKYYatG3Z", "pdf_link": "https://openreview.net/pdf?id=QtKYYatG3Z", "keywords": "large language models, world models, transfer learning, evaluation", "abstract": "A common approach for assessing whether large pretrained models develop world models is by studying the behavior of fixed models. However, many of the benefits of having a world model arise when transferring a model to new tasks (e.g. few- shot learning). In this paper, we ask: what does it mean to test if alearnerhas a world model embodied in it? We consider a simple definition of a true world model: a mapping from inputs to states. We introduce a procedure that assesses a learner\u2019s world model by measuring its inductive bias when transferring to new tasks. This inductive bias can be measured in two distinct dimensions: does a learner extrapolate to new data by building functions of state, and to what degree do these functions capture the full state? We use this procedure to study the degree to which pretrained models extrapolate to new tasks based on state. We find that models that perform very well on next-token prediction can extrapolate to new tasks with very little inductive bias toward state. We conclude by assessing the possibility that these models learn bundles of heuristics that enable them to perform well on next-token prediction despite preserving little of state.", "title_embedding_index": 6114, "title_abs_embedding_index": 6139}, {"title": "Effective Learning with Node Perturbation in Multi-Layer Neural Networks", "link_suffix": "/forum?id=4qh6nurdYt", "link": "https://openreview.net/forum?id=4qh6nurdYt", "pdf_link": "https://openreview.net/pdf?id=4qh6nurdYt", "keywords": "efficient machine learning, optimization", "abstract": "Backpropagation (BP) remains the dominant and most successful method for training parameters of deep neural network models.\nHowever, BP relies on two computationally distinct phases, does not provide a satisfactory explanation of biological learning, and can be challenging to apply for training of networks with discontinuities or noisy node dynamics.\nBy comparison, node perturbation (NP) proposes learning by the injection of noise into network activations, and subsequent measurement of the induced loss change. NP relies on two forward (inference) passes, does not make use of network derivatives, and has been proposed as a model for learning in biological systems.\nHowever, standard NP is highly data inefficient and unstable due to its unguided noise-based search process.\nIn this work, we investigate different formulations of NP and relate it to the concept of directional derivatives as well as combining it with a decorrelating mechanism for layer-wise inputs.\nWe find that a closer alignment with directional derivatives together with input decorrelation at every layer strongly enhances performance of NP learning with large improvements in parameter convergence and much higher performance on the test data, approaching that of BP.\nFurthermore, our novel formulation allows for application to noisy systems in which the noise process itself is inaccessible.", "title_embedding_index": 6115, "title_abs_embedding_index": 6140}, {"title": "Decoupled Finetuning for Domain Generalizable Semantic Segmentation", "link_suffix": "/forum?id=qZEdmyqCHF", "link": "https://openreview.net/forum?id=qZEdmyqCHF", "pdf_link": "https://openreview.net/pdf?id=qZEdmyqCHF", "keywords": "Domain Generalization, Decoupled Optimization, Domain Generalizable Semantic Segmentation, Robustness", "abstract": "Joint finetuning of a pretrained encoder and a randomly initialized decoder has been the de facto standard in semantic segmentation, but the vulnerability of this approach to domain shift has not been studied. We investigate the vulnerability issue of joint finetuning, and propose a novel finetuning framework called Decoupled FineTuning for domain generalization (DeFT) as a solution. DeFT operates in two stages. Its first stage warms up the decoder with the frozen, pretrained encoder so that the decoder learns task-relevant knowledge while the encoder preserves its generalizable features. In the second stage, it decouples finetuning of the encoder and decoder into two pathways, each of which concatenates a usual component (UC) and generalized component (GC); each of the encoder and decoder plays a different role between UC and GC in different pathways. UCs are updated by gradients of the loss on the source domain, while GCs are updated by exponential moving average biased toward their initialization to retain their generalization capability. By the two separate optimization pathways with opposite UC-GC configurations, DeFT reduces the number of learnable parameters virtually, and decreases the distance between learned parameters and their initialization, leading to improved generalization capability. DeFT significantly outperformed existing methods in various domain shift scenarios, and its performance could be further boosted by incorporating a simple distance regularization.", "title_embedding_index": 6116, "title_abs_embedding_index": 6141}, {"title": "SFW sampling for diffusion models via external conditioning", "link_suffix": "/forum?id=xETLME9sNq", "link": "https://openreview.net/forum?id=xETLME9sNq", "pdf_link": "https://openreview.net/pdf?id=xETLME9sNq", "keywords": "diffusion, score-based, safeness, alignment, guidance", "abstract": "Score-based generative models (SBM), also known as diffusion models, are the de facto state of the art for image synthesis. Despite their unparalleled performance, SBMs have recently been in the spotlight for being tricked into creating not-safe-for-work (NSFW) content, such as violent images and non-consensual nudity. This article proposes a safe-for-work (SFW) sampler for SBMs implementing a Conditional Trajectory Correction step that guides the samples away from undesired regions in the ambient space using external multimodal models as the source of conditioning. Furthermore, using Contrastive Language Image Pre-training (CLIP), our method admits user-defined NSFW classes, which can vary in different settings. Our experiments on the text-to-image SBM Stable Diffusion validate that the proposed SFW sampler effectively reduces the generation of explicit content, as assessed via independent NSFW detectors. Furthermore, the proposed correction comes at a minor cost in image quality and has an almost null effect on samples that do not need correction. Our study confirms the suitability of the SFW sampler towards aligned SBM models.", "title_embedding_index": 6117, "title_abs_embedding_index": 6142}, {"title": "EconAI: Preference-driven Agents Simulating Economic Activities via Large Language Model", "link_suffix": "/forum?id=HzG3A0VD1k", "link": "https://openreview.net/forum?id=HzG3A0VD1k", "pdf_link": "https://openreview.net/pdf?id=HzG3A0VD1k", "keywords": "Knowledge Agent, Economic  Simulating", "abstract": "The emergence of artificial intelligence has transformed the methodological frameworks in economic research by simulating intricate interactions among diverse agents. Despite the advantage of large language models (LLMs), they often struggle with occasions involving decision-making interactions with environments. This challenge stems from the fact that most LLMs are rationality-driven, seeking optimal economic benefits, while humans are preference-driven, pursuing the balance of personal goals (\\textit{e.g.,} income and health). These differences hinder the LLMs' ability to effectively understand economic activities across various contexts, leading to biases in economic simulations. To tackle this issue, we introduce \\textbf{EconAI}, a novel approach aimed at enhancing the preference learning capabilities of LLMs by incorporating human-like preferences and cognitive processes. Specifically, EconAI features a 'knowledge brain' constructed from historical data and learning algorithms, enabling memory and making decisions for sophisticated economic facts.  By integrating elements of self-learning, reflection, and experience updates, we refine decision-making processes, resulting in more accurate economic planning and mitigating planning bias in economic activities. Through the integration of real-time economic data and historical trends, EconAI offers a robust simulation platform that can adapt to market fluctuations and economic shocks. Our findings demonstrate that EconAI can model economic phenomena like inflation and employment with greater precision, showcase a notable ability to adjust to changing economic conditions, and surpass existing frameworks significantly.", "title_embedding_index": 6118, "title_abs_embedding_index": 6143}, {"title": "iFedDR: Auto-Tuning Local Computation with Inexact Douglas-Rachford Splitting in Federated Learning", "link_suffix": "/forum?id=XFpb3T5Zc9", "link": "https://openreview.net/forum?id=XFpb3T5Zc9", "pdf_link": "https://openreview.net/pdf?id=XFpb3T5Zc9", "keywords": "Federated learning, Douglas-Rachford splitting, monotone operators, relative error, inexact proximal point", "abstract": "Federated learning usually requires specifying the amount of local computation needed a priori. In this work, we instead propose a systematic scheme to automatically adjust and potentially reduce the local computations while preserving convergence guarantees. We focus on proximal-based methods, where we demonstrate that the proximal operator can be evaluated inexactly up to a relative error, rather than relying on a predefined sequence of vanishing errors. Our proposed method, iFedDR, is based on a novel error-corrected version of inexact Douglas-Rachford splitting. It mitigates the need for hyperparameter tuning the number of client steps, by triggering refinement on-demand. We derive iFedDR as an instance of a much more general construction, which allows us to handle minimax problem, and which is interesting in its own right. Several numerical experiments are carried out demonstrating the favorable convergence properties of iFedDR.", "title_embedding_index": 6119, "title_abs_embedding_index": 6144}, {"title": "HIVEX: A High-Impact Environment Suite for Multi-Agent Research", "link_suffix": "/forum?id=ySmovxuDMi", "link": "https://openreview.net/forum?id=ySmovxuDMi", "pdf_link": "https://openreview.net/pdf?id=ySmovxuDMi", "keywords": "Environment Benchmark, Multi-Agent Reinforcement Learning, Multi-Agent Systems, Critical Ecological Challenges", "abstract": "Games have been vital test beds for the rapid development of Agent-based research. Remarkable progress has been achieved in the past, but it is unclear if the findings equip for real-world problems. While pressure grows, some of the most critical ecological challenges can find mitigation and prevention solutions through technology and its applications. Most real-world domains include multi-agent scenarios and require machine-machine and human-machine collaboration. Open-source environments have not advanced and are often toy scenarios, too abstract or not suitable for multi-agent research. By mimicking real-world problems and increasing the complexity of environments, we hope to advance state-of-the-art multi-agent research and inspire researchers to work on immediate real-world problems. Here, we present HIVEX, an environment suite to benchmark multi-agent research focusing on ecological challenges. HIVEX includes the following environments: Wind Farm Control, Wildfire Resource Management, Drone-Based Reforestation, Ocean Plastic Collection, and Aerial Wildfire Suppression. We provide environments, training examples, and baselines for the main and sub-tasks.", "title_embedding_index": 6120, "title_abs_embedding_index": 6145}, {"title": "Magnetic Mirror Descent Self-play Preference Optimization", "link_suffix": "/forum?id=PDnEDS244P", "link": "https://openreview.net/forum?id=PDnEDS244P", "pdf_link": "https://openreview.net/pdf?id=PDnEDS244P", "keywords": "AI Alignment, Mirror Descent, Self-play, Nash Equilibrium", "abstract": "Standard Reinforcement Learning from Human Feedback (RLHF) methods mainly optimize preferences through the Bradley-Terry (BT) reward model, which may misalign with natural human preferences due to the strong transitivity assumption.\nRecent work has reframed the preference learning problem as a two-player constant-sum game, aiming to learn policies that better reflect human preferences by finding the Nash equilibrium (NE) of this game. \nHowever, existing methods under this framework either guarantee only average-iterate convergence or rely on strong first-order approximation assumptions. \nIn this paper, we propose Mirror Descent Self-play Preference Optimization (MDSPO), a novel approach based on Magnetic Mirror Descent (MMD). By introducing an additional magnetic term, MDSPO achieves linear convergence rate to the NE of the regularized game. Furthermore, we establish theoretical guarantees for the convergence of our algorithm to the NE of the original game by periodically updating the reference policy. This approach effectively guarantees that the final policy accurately reflects the true human preferences. To ensure our algorithm is both theoretically sound and practically viable, we provide a simple yet effective implementation that adapts the theoretical insights to the RLHF setting. We demonstrate its effectiveness on a variety of benchmarks.", "title_embedding_index": 6121, "title_abs_embedding_index": 6146}, {"title": "PharmaVQA: A Retrieval-Augmented Visual Question Answering Framework for Molecular Representation via Pharmacophore Guided Prompts", "link_suffix": "/forum?id=aOJh9JvGcI", "link": "https://openreview.net/forum?id=aOJh9JvGcI", "pdf_link": "https://openreview.net/pdf?id=aOJh9JvGcI", "keywords": "Pharmacophore, Visual Question Answering, Prompt Learning, Molecule Represenation Learning, Bilinear Attention Network, Multi-modal Retrieval", "abstract": "In drug discovery, molecular representation learning is vital for understanding and generating new drug-like molecules. The accurate representation of molecules facilitates drug candidate screening and the optimization of lead compounds. The vastness of chemical space challenges traditional drug design and relies on complex computations. The Pharmacophore is a functional group contained within a drug molecule, which binds to receptors or biological macromolecules to produce biological effects and reduce computations. Pharmacophore-guided representation of molecules, however, remains a significant challenge. To address this issue, we propose an improved deep learning-based model called PharmaVQA for retrieving pharmacophore-related information directly from molecule databases, allowing for a more targeted understanding of drug-like molecules. Through the use of Visual Question Answering (VQA) framework, PharmaVQA captures pharmacophore data, generates knowledge prompts, and enriches molecular representations. On 46 benchmark datasets, PharmaVQA has demonstrated superior performance in both molecular property prediction and drug-target interaction prediction. Additionally, the applicability of PharmaVQA in drug discovery has been validated on an FDA-approved molecule dataset, where the Top-20 predictions were analyzed in real-world studies, with the majority of them experimentally validated as potential ligands previously reported in the literature. Our assessment of PharmaVQA is that it is a powerful and useful tool for accelerating the development of AI-assisted drug discovery across a wide range of areas.", "title_embedding_index": 6122, "title_abs_embedding_index": 6147}, {"title": "Cached Multi-Lora Composition for Multi-Concept Image Generation", "link_suffix": "/forum?id=4iFSBgxvIO", "link": "https://openreview.net/forum?id=4iFSBgxvIO", "pdf_link": "https://openreview.net/pdf?id=4iFSBgxvIO", "keywords": "Low-Rank Adaptation (LoRA), Multi-LoRA composition, Text-to-image models, Computational efficiency", "abstract": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in text-to-image models, enabling precise rendering of multiple distinct elements, such as characters and styles, in multi-concept image generation. However, current approaches face significant challenges when composing these LoRAs for multi-concept image generation, particularly as the number of LoRAs increases, resulting in diminished generated image quality. \nIn this paper, we initially investigate the role of LoRAs in the denoising process through the lens of the Fourier frequency domain.\nBased on the hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\", we have conducted empirical experiments and find that certain LoRAs amplify high-frequency features such as edges and textures, whereas others mainly focus on low-frequency elements, including the overall structure and smooth color gradients.\nBuilding on these insights, we devise a frequency domain based sequencing strategy to determine the optimal order in which LoRAs should be integrated during inference. This strategy offers a methodical and generalizable solution compared to the naive integration commonly found in existing LoRA fusion techniques.\nTo fully leverage our proposed LoRA order sequence determination method in multi-LoRA composition tasks, we introduce a novel, training-free framework, Cached Multi-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while maintaining cohesive image generation.\nWith its flexible backbone for multi-LoRA fusion and a non-uniform caching strategy tailored to individual LoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA composition and improve computational efficiency.\nOur experimental evaluations demonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion methods by a significant margin -- it achieves an average improvement of $2.19$% in CLIPScore, and $11.25%$% in MLLM win rate compared to LoRAhub, LoRA Composite, and LoRA Switch.", "title_embedding_index": 6123, "title_abs_embedding_index": 6148}, {"title": "CNS-Bench: Benchmarking Model Robustness Under Continuous Nuisance Shifts", "link_suffix": "/forum?id=1STZCCI8mn", "link": "https://openreview.net/forum?id=1STZCCI8mn", "pdf_link": "https://openreview.net/pdf?id=1STZCCI8mn", "keywords": "Generative models, benchmarking, computer vision", "abstract": "One important challenge in evaluating the robustness of vision models is to control individual nuisance factors independently.\nWhile some simple synthetic corruptions are commonly applied to existing models, they do not fully capture all realistic distribution shifts of real-world images. Moreover, existing generative robustness benchmarks only perform manipulations on individual nuisance shifts in one step. We demonstrate the importance of gradual and continuous nuisance shifts, as they allow evaluating the sensitivity and failure points of vision models. In particular, we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark for image classifier robustness. CNS-Bench allows generating a wide range of individual nuisance shifts in continuous severities by applying LoRA adapters to diffusion models. We perform a comprehensive large-scale study to evaluate the robustness of classifiers under various nuisance shifts. Through carefully-designed comparisons and analyses, we reveal the following observations: 1) Evaluating the model performance on a continuous scale allows the identification of model failure points and a more nuanced understanding of model robustness. 2) Model rankings can change for varying severities of a shift, which is not captured when averaging the performance over all severities. 3) The architecture has a strong influence on the robustness and the failure points of a model. \nOverall, our work demonstrated the advantage of using generative models for benchmarking robustness across diverse continuous nuisance shifts in a controlled and scalable manner.", "title_embedding_index": 6124, "title_abs_embedding_index": 6149}]