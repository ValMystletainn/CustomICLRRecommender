[{"title": "BrainOOD: Out-of-distribution Generalizable Brain Network Analysis", "link_suffix": "/forum?id=3xqqYOKILp", "link": "https://openreview.net/forum?id=3xqqYOKILp", "pdf_link": "https://openreview.net/pdf?id=3xqqYOKILp", "keywords": "Out-of-distribution Generalization, Brain Network Analysis, Graph Representation Learning", "abstract": "In neuroscience, identifying distinct patterns linked to neurological disorders, such as Alzheimer\u2019s and Autism, is critical for early diagnosis and effective intervention. Graph Neural Networks (GNNs) have shown promising in analyzing brain networks, but there are two major challenges in using GNNs: (1) distribution shifts in multi-site brain network data, leading to poor Out-of-Distribution (OOD) generalization, and (2) limited interpretability in identifying key brain regions critical to neurological disorders. Existing graph OOD methods, while effective in other domains, struggle with the unique characteristics of brain networks. To bridge these gaps, we introduce \\textit{BrainOOD},  a novel framework tailored for brain networks that enhances GNNs\u2019 OOD generalization and interpretability. BrainOOD framework consists of a feature selector and a structure extractor, which incorporates various auxiliary losses including an improved Graph Information Bottleneck (GIB) objective to recover causal subgraphs. By aligning structure selection across brain networks and filtering noisy features, BrainOOD offers reliable interpretations of critical brain regions. Our approach outperforms 16 existing methods and improves generalization to OOD subjects by up to 8.5%. Case studies highlight the scientific validity of the patterns extracted, which aligns with the findings in known neuroscience literature. We also propose the first OOD brain network benchmark, which provides a foundation for future research in this field.", "title_embedding_index": 19350, "title_abs_embedding_index": 19375}, {"title": "PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views", "link_suffix": "/forum?id=sR0xz6ZaH7", "link": "https://openreview.net/forum?id=sR0xz6ZaH7", "pdf_link": "https://openreview.net/pdf?id=sR0xz6ZaH7", "keywords": "3D Gaussian splatting, 3D reconstruction, Novel view synthesis", "abstract": "We propose $\\textbf{PixelGaussian}$, an efficient feed-forward framework for learning generalizable 3D Gaussian reconstruction from arbitrary views. Most existing methods rely on uniform pixel-wise Gaussian representations, which learn a fixed number of 3D Gaussians for each view and cannot generalize well to more input views. Differently, our PixelGaussian dynamically adapts both the Gaussian distribution and quantity based on geometric complexity, leading to more efficient representations and significant improvements in reconstruction quality. Specifically, we introduce a Cascade Gaussian Adapter (CGA) to adjust Gaussian distribution according to local geometry complexity identified by a keypoint scorer. CGA leverages deformable attention in context-aware hypernetworks to guide Gaussian pruning and splitting, ensuring accurate representation in complex regions while reducing redundancy. Furthermore, we design a transformer-based Iterative Gaussian Refiner (IGR) module that refines Gaussian representations through direct image-Gaussian interactions. Our PixelGaussian can effectively reduce Gaussian redundancy as input views increase. We conduct extensive experiments on the large-scale ACID and RealEstate10K datasets, where our method achieves state-of-the-art performance with good generalization to various numbers of views.", "title_embedding_index": 19351, "title_abs_embedding_index": 19376}, {"title": "Consistent Neural Embeddings through Flow Matching on Attractor-like Neural Manifolds", "link_suffix": "/forum?id=F5lcN7329a", "link": "https://openreview.net/forum?id=F5lcN7329a", "pdf_link": "https://openreview.net/pdf?id=F5lcN7329a", "keywords": "Brain-Computer Interface, Neural Decoding, Flow Matching, Dynamical Stability", "abstract": "The primary objective of brain-computer interfaces (BCIs) is to establish a direct connection between neural activity and behavioral actions through neural decoders. \nConsistent neural representation is crucial for achieving high-performance behavioral decoding over time.\nDue to the stochastic variability in neural recordings, existing neural representation techniques yield dynamical instability, leading to the failure of behavioral decoders in few-trial scenarios.\nIn this work, we propose a novel Flow-Based Dynamical Alignment (FDA) framework that leverages attractor-like ensemble dynamics on stable neural manifolds, which facilitate a new source-free alignment through likelihood maximization.\nThe consistency of latent embeddings obtained through FDA was theoretically verified based on dynamical stability, allowing for rapid adaptation with few trials.\nFurther experiments on multiple motor cortex datasets validate the superior performance of FDA.\nThe FDA method establishes a novel framework for consistent neural latent embeddings with few trials. \nOur work offers insights into neural dynamical stability, potentially enhancing the chronic reliability of real-world BCIs.", "title_embedding_index": 19352, "title_abs_embedding_index": 19377}, {"title": "WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling", "link_suffix": "/forum?id=yBlVlS2Fd9", "link": "https://openreview.net/forum?id=yBlVlS2Fd9", "pdf_link": "https://openreview.net/pdf?id=yBlVlS2Fd9", "keywords": "speech representation, discrete codec, audio language model", "abstract": "Language models have been effectively applied to modeling natural signals, such as images, video, speech, and audio. A crucial component of these models is the codec tokenizer, which compresses high-dimensional natural signals into lower-dimensional discrete tokens. In this paper, we introduce WavTokenizer, which offers several advantages over previous SOTA acoustic codec models in the audio domain: 1) extreme compression. By compressing the layers of quantizers and the temporal dimension of the discrete codec, one-second audio of 24kHz sampling rate requires only a single quantizer with 40 or 75 tokens. 2) improved subjective quality. Despite the reduced number of tokens, WavTokenizer achieves state-of-the-art reconstruction quality with outstanding UTMOS scores and inherently contains richer semantic information. Specifically, we achieve these results by designing a broader VQ space, extended contextual windows, and improved attention networks, as well as introducing a powerful multi-scale discriminator and an inverse Fourier transform structure. We conducted extensive reconstruction experiments in the domains of speech, audio, and music. WavTokenizer exhibited strong performance across various objective and subjective metrics compared to state-of-the-art models. We also tested semantic information, VQ utilization, and adaptability to generative models. Comprehensive ablation studies confirm the necessity of each module in WavTokenizer. The demo is available athttps://wavtokenizer.github.io/.", "title_embedding_index": 19353, "title_abs_embedding_index": 19378}, {"title": "Multiple Descents in Unsupervised Auto-Encoders: The Role of Noise, Domain Shift and Anomalies", "link_suffix": "/forum?id=YBv9EExJPk", "link": "https://openreview.net/forum?id=YBv9EExJPk", "pdf_link": "https://openreview.net/pdf?id=YBv9EExJPk", "keywords": "double descent, unsupervised learning, autoencoders, domain adaptation, bias-variance curve, anomalies", "abstract": "The phenomenon of double descent has recently gained attention in supervised learning. It challenges the conventional wisdom of the bias-variance trade-off by showcasing a surprising behavior. As the complexity of the model increases, the test error initially decreases until reaching a certain point where the model starts to overfit the train set, causing the test error to rise. However, deviating from classical theory, the error exhibits another decline when exceeding a certain degree of over-parameterization. We study the presence of double descent in unsupervised learning, an area that has received little attention and is not yet fully understood. We conduct extensive experiments using under-complete auto-encoders (AEs) for various applications, such as dealing with noisy data, domain shifts, and anomalies. We use synthetic and real data and identify model-wise, epoch-wise, and sample-wise double descent for all the aforementioned applications. Finally, we assessed the usability of the AEs for detecting anomalies and mitigating the domain shift between datasets. Our findings indicate that over-parameterized models can improve performance not only in terms of reconstruction, but also in enhancing capabilities for the downstream task.", "title_embedding_index": 19354, "title_abs_embedding_index": 19379}, {"title": "Integrative Decoding: Improving Factuality via Implicit Self-consistency", "link_suffix": "/forum?id=gGWYecsK1U", "link": "https://openreview.net/forum?id=gGWYecsK1U", "pdf_link": "https://openreview.net/pdf?id=gGWYecsK1U", "keywords": "Large Language Models, Hallucination, Factuality, Decoding", "abstract": "Self-consistency-based approaches, which involve repeatedly sampling multiple outputs and selecting the most consistent one as the final response, prove to be remarkably effective in improving the factual accuracy of large language models. Nonetheless, existing methods usually have strict constraints on the task format, largely limiting their applicability. In this paper, we present Integrative Decoding (ID), to unlock the potential of self-consistency in open-ended generation tasks. ID operates by constructing a set of inputs, each prepended with a previously sampled response, and then processes them concurrently, with the next token being selected by aggregating of all their corresponding predictions at each decoding step. In essence, this simple approach implicitly incorporates self-consistency in the decoding objective. Extensive evaluation shows that ID consistently enhances factuality over a wide range of language models, with substantial improvements on the TruthfulQA (+11.2%), Biographies (+15.4%) and LongFact (+8.5%) benchmarks. The performance gains amplify progressively as the number of sampled responses increases, indicating the potential of ID to scale up with repeated sampling.", "title_embedding_index": 19355, "title_abs_embedding_index": 19380}, {"title": "Differential Transformer", "link_suffix": "/forum?id=OvoCm1gGhN", "link": "https://openreview.net/forum?id=OvoCm1gGhN", "pdf_link": "https://openreview.net/pdf?id=OvoCm1gGhN", "keywords": "sequence modeling, language models, model architecture, Transformer", "abstract": "Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture for large language models.", "title_embedding_index": 19356, "title_abs_embedding_index": 19381}, {"title": "Style-Coherent Multi-Modality Image Fusion", "link_suffix": "/forum?id=1waeKNeQzG", "link": "https://openreview.net/forum?id=1waeKNeQzG", "pdf_link": "https://openreview.net/pdf?id=1waeKNeQzG", "keywords": "Multi-modality, Image Fusion, Style-based Learning, Self-supervised Learning", "abstract": "Multi-modality image fusion (MMIF) integrates heterogeneous images from diverse sensors. However, existing MMIF methods often overlook significant style discrepancies, such as saturation and resolution differences between modalities, resulting in overly smooth features in certain modalities. This tendency causes models to misjudge and disregard potentially crucial content. To address this issue, this paper proposes a novel style-coherent multi-modality fusion model that adeptly merges heterogeneous styled features from various modalities. Specifically, the proposed style-normalized fusion module progressively supplements the complete content structure by merging style-normalized features during cross-modal feature extraction. Meanwhile, a style-alignment fusion module is developed to align different feature representations across modalities, ensuring consistency. Additionally, to better preserve information and emphasize critical patterns during fusion, an adaptive reconstruction loss is applied to multi-modal images transformed into a unified image domain, enforcing mapping to a consistent modality representation. Extensive experiments validate that our method outperforms existing approaches on multiple MMIF tasks and exhibits greater potential to facilitate downstream applications.", "title_embedding_index": 19357, "title_abs_embedding_index": 19382}, {"title": "ControlSpeech: Towards Simultaneous Zero-shot Speaker Cloning and Zero-shot Language Style Control", "link_suffix": "/forum?id=zAogQOIphH", "link": "https://openreview.net/forum?id=zAogQOIphH", "pdf_link": "https://openreview.net/pdf?id=zAogQOIphH", "keywords": "text-to-speech, style control, discrete codec model", "abstract": "In this paper, we present ControlSpeech, a text-to-speech (TTS) system capable of fully cloning the speaker's voice and enabling arbitrary control and adjustment of speaking style, merely based on a few seconds of audio prompt and a simple textual style description prompt. Prior zero-shot TTS models only mimic the speaker's voice without further control and adjustment capabilities while prior controllable TTS models cannot perform speaker-specific voice generation. Therefore, ControlSpeech focuses on a more challenging task\u2014a TTS system with controllable timbre, content, and style at the same time. ControlSpeech takes speech prompts, content prompts, and style prompts as inputs and utilizes bidirectional attention and mask-based parallel decoding to capture codec representations corresponding to timbre, content, and style in a discrete decoupling codec space. Moreover, we analyze the many-to-many issue in textual style control and propose the Style Mixture Semantic Density (SMSD) module, which is based on Gaussian mixture density networks, to resolve this problem. The SMSD module enhances the fine-grained partitioning and sampling capabilities of style semantic information and enables speech generation with more diverse styles. To facilitate empirical validations, we make available a controllable model toolkit called ControlToolkit, which includes all source code, a new style controllable dataset VccmDataset, and our replicated competitive baseline models. Our experimental results demonstrate that ControlSpeech exhibits comparable or state-of-the-art (SOTA) performance in terms of controllability, timbre similarity, audio quality, robustness, and generalizability. Ablation studies further validate the necessity of each component in ControlSpeech. Audio samples are available athttps://controlspeech.github.io/.", "title_embedding_index": 19358, "title_abs_embedding_index": 19383}, {"title": "Not Every Image is Worth a Thousand Words: Quantifying Originality  in Stable Diffusion", "link_suffix": "/forum?id=Y5mm3Yb36I", "link": "https://openreview.net/forum?id=Y5mm3Yb36I", "pdf_link": "https://openreview.net/pdf?id=Y5mm3Yb36I", "keywords": "Interpretability, Computer Vision, Text-to-Image(T2I) Models, Stable Diffusion, Generalization, Copyrights Infringement", "abstract": "This work addresses the challenge of quantifying originality in text-to-image (T2I) generative diffusion models, with a focus on copyright originality. We begin by evaluating T2I models' ability to innovate and generalize through controlled experiments, revealing that stable diffusion models can effectively recreate unseen elements with sufficiently diverse training data. Then, our key insight is that concepts and combinations of image elements the model is familiar with, and saw more during training, are more concisly represented in the model's latent space. We hence propose a method that leverages textual inversion to measure the originality of an image based on the number of tokens required for its reconstruction by the model. Our approach is inspired by legal definitions of originality and aims to assess whether a model can produce original content without relying on specific prompts or having the training data of the model. We demonstrate our method using both a pre-trained stable diffusion model and a synthetic dataset, showing a correlation between the number of tokens and image originality. This work contributes to the understanding of originality in generative models and has implications for copyright infringement cases.", "title_embedding_index": 19359, "title_abs_embedding_index": 19384}, {"title": "DIESEL - Dynamic Inference-Guidance via Evasion of Semantic Embeddings in LLMs", "link_suffix": "/forum?id=fmHS8aBfuH", "link": "https://openreview.net/forum?id=fmHS8aBfuH", "pdf_link": "https://openreview.net/pdf?id=fmHS8aBfuH", "keywords": "Large Language Models, Inference Guidance, Safety, Robustness", "abstract": "In recent years, conversational large language models (LLMs) have shown tremendous success in tasks such as casual conversation, question answering, and personalized dialogue, making significant advancements in domains like virtual assistance, social interaction, and online customer engagement. However, they often generate responses that are not aligned with human values (e.g., ethical standards, safety, or social norms), leading to potentially unsafe or inappropriate outputs. While several techniques have been proposed to address this problem, they come with a cost, requiring computationally expensive training or dramatically increasing the inference time. In this paper, we present DIESEL, a lightweight inference guidance technique that can be seamlessly integrated into any autoregressive LLM to semantically filter undesired concepts from the response. DIESEL can function either as a standalone safeguard or as an additional layer of defense, enhancing response safety by reranking the LLM's proposed tokens based on their similarity to predefined negative concepts in the latent space. This approach provides an efficient and effective solution for maintaining alignment with human values. Our evaluation demonstrates DIESEL's effectiveness on state-of-the-art conversational models (e.g., Llama 3), even in challenging jailbreaking scenarios that test the limits of response safety. We further show that DIESEL can be generalized to use cases other than safety, providing a versatile solution for general-purpose response filtering with minimal computational overhead.", "title_embedding_index": 19360, "title_abs_embedding_index": 19385}, {"title": "HYBRID MODEL COLLABORATION FOR SIGN LANGUAGE TRANSLATION WITH VQ-VAE AND RAG ENHANCED LLMS", "link_suffix": "/forum?id=7kRFnSFN89", "link": "https://openreview.net/forum?id=7kRFnSFN89", "pdf_link": "https://openreview.net/pdf?id=7kRFnSFN89", "keywords": "Sign language translation, VQ-VAE, Large language model, Hybrid collaboration", "abstract": "Data shortages and the phonetic disparity between sign and spoken languages have historically limited the quality of sign language translation.  On another front, endowed with substantial prior knowledge, large language models perform exceptionally well across diverse tasks, significantly diminishing the demand for domain-specific training data. Building on these foundation, this paper presents VRG-SLT, an innovative framework that translates sign language into spoken language, facilitating communication between signing and non-signing communities. In practice, VRG-SLT utilizes a hierarchical VQ-VAE to convert continuous sign sequences into discrete representations, referred as sign codes, which are subsequently aligned with text by a fine-tuned pre-trained language model. Additionally, retrieval-augmented generation (RAG) is employed to extend and enhance the language model, producing more semantically coherent and precise spoken text. Featuring a hierarchical VQ-VAE and pre-trained large language models, VRG-SLT demonstrates state-of-the-art performance. It excels on modish benchmarks like How2Sign and PHOENIX-2014T. Moreover, the incorporation of additional factual knowledge through RAG further improves the accuracy of the generated text.", "title_embedding_index": 19361, "title_abs_embedding_index": 19386}, {"title": "Random Logit Scaling: Defending Deep Neural Networks Against Black-Box Score-Based Adversarial Example Attacks", "link_suffix": "/forum?id=mJzOHRSpSa", "link": "https://openreview.net/forum?id=mJzOHRSpSa", "pdf_link": "https://openreview.net/pdf?id=mJzOHRSpSa", "keywords": "Adversarial examples, Query-based black-box attacks, Randomized defenses", "abstract": "Machine learning models are increasingly adapted in various domains, such as autonomous driving, facial recognition, and malware detection, achieving state-of-the-art results. However, adversarial example attacks pose a significant threat to the reliable deployment of machine learning models in such applications. In recent years, some powerful adversarial example attacks have been proposed for the fast and query-efficient generation of adversarial examples, even in black-box scenarios where attackers only have an oracle access to the target model, highlighting the need for scalable, low-cost, and powerful defenses. In this work, we propose Random Logit Scaling (RLS), a randomization-based defense against black-box score-based adversarial example attacks. RLS is a plug-and-play, post-processing defense that can be implemented on top of any existing ML model with minimal effort. The idea behind RLS is to confuse an attacker by outputting falsified scores resulting from randomly scaled logits while maintaining the model accuracy. We show that RLS significantly reduces the success rate of state-of-the-art black-box score-based attacks while preserving the accuracy and minimizing confidence score distortion compared to state-of-the-art randomization-based defenses.", "title_embedding_index": 19362, "title_abs_embedding_index": 19387}, {"title": "Balanced conic rectified flow", "link_suffix": "/forum?id=ctSjIlYN74", "link": "https://openreview.net/forum?id=ctSjIlYN74", "pdf_link": "https://openreview.net/pdf?id=ctSjIlYN74", "keywords": "Image generation, generative model, rectified flow, diffusion, optimal transport, curvature, ODE", "abstract": "Rectified flow is a generative model that learns smooth transport mappings between two distributions through an ordinary differential equation (ODE). Unlike diffusion-based generative models, which require costly numerical integration of a generative ODE to sample images with state-of-the-art quality, rectified flow uses an iterative process called reflow to learn smooth and straight ODE paths. This allows for relatively simple and efficient generation of high-quality images. However, rectified flow still faces several challenges. 1) The reflow process requires a large number of generative pairs to preserve the target distribution, leading to significant computational costs. 2) Since the model is typically trained using only generated image pairs, its performance heavily depends on the 1-rectified flow model, causing it to become biased towards the generated data.In this work, we experimentally expose the limitations of the original rectified flow and propose a novel approach that incorporates real images into the training process. By preserving the ODE paths for real images, our method effectively reduces reliance on large amounts of generated data. Instead, we demonstrate that the reflow process can be conducted efficiently using a much smaller set of generated and real images. In CIFAR-10, we achieved significantly better FID scores, not only in one-step generation but also in full-step simulations, while using only $7.2%$ of the generative pairs compared to the original method. Furthermore, our approach induces straighter paths and avoids saturation on generated images during reflow, leading to more robust ODE learning while preserving the distribution of real images.", "title_embedding_index": 19363, "title_abs_embedding_index": 19388}, {"title": "Distilling Non-Autoregressive Model Knowledge for Autoregressive De Novo Peptide Sequencing", "link_suffix": "/forum?id=I2ZYngkRW6", "link": "https://openreview.net/forum?id=I2ZYngkRW6", "pdf_link": "https://openreview.net/pdf?id=I2ZYngkRW6", "keywords": "De novo, Peptide Sequencing, autoregressive, non-autoregressive", "abstract": "Autoregressive (next-token-prediction) models excel in various language generation tasks compared to non-autoregressive (parallel prediction) models. However, their advantage diminishes in certain biology-related tasks like protein modeling and de novo peptide sequencing. Notably, previous studies show that Non-Autoregressive Transformers (NAT) can largely outperform Autoregressive Transformers (AT) in amino acid sequence prediction due to their bidirectional information flow. Despite their advantages, NATs struggle with generalizing to longer sequences, scaling to larger models, and facing extreme optimization difficulties compared to AT models. Motivated by this, we propose a novel framework for directly distilling knowledge from NATs, known for encoding superior protein representations, to enhance autoregressive generation. Our approach employs joint training with a shared encoder and a specially designed cross-decoder attention module. Additionally, we introduce a new training pipeline that uses importance annealing and cross-decoder gradient blocking to facilitate effective knowledge transfer. Evaluations on a widely used 9-species benchmark show that our proposed design achieves state-of-the-art performance. Specifically, AT and NAT baseline models each excel in different types of data prediction due to their unique inductive biases. Our model combines these advantages, achieving strong performance across all data types and outperforming baselines across all evaluation metrics. This work not only advances de novo peptide sequencing but also provides valuable insights into how autoregressive generation can benefit from non-autoregressive knowledge and how next-token prediction (GPT-style) can be enhanced through bidirectional learning (BERT-style).\nWe release our code for reproduction in the anonymous repository here:https://anonymous.4open.science/r/CrossNovo-E263.", "title_embedding_index": 19364, "title_abs_embedding_index": 19389}, {"title": "Instance-level Consistent Graph With Unsupervised Human Parts for Person Re-identification", "link_suffix": "/forum?id=F9MTOYTzEm", "link": "https://openreview.net/forum?id=F9MTOYTzEm", "pdf_link": "https://openreview.net/pdf?id=F9MTOYTzEm", "keywords": "Person re-identification, Instance-level consistency, Human parts clustering, Graph convolution network", "abstract": "The representation of human parts plays a crucial role in person re-identification (re-ID) by offering discriminative cues, yet it presents challenges such as misalignment, occlusion, and extreme illumination. Previous methods have primarily focused on achieving strict part-level consistency. However, individual part features change inevitably under harsh conditions, hindering consistent representation. In this article, we propose an Instance-level Consistent Graph (ICG) framework to address this issue, which extracts structural information by introducing graph modeling atop unsupervised human parts. Firstly, we introduce an attention-based foreground separation to suppress non-instance noise. Subsequently, an unsupervised clustering method is designed to segment pixel-wise human parts within the foreground, enabling fine-grained part representations. We propose a flexible structure graph that derives instance-level structure from part features, treating each part feature as a node in a graph convolutional network. In essence, ICG mitigates incompleteness through feature flow among nodes, broadening the matching condition from strict part-level consistency to robust instance-level consistency. Extensive experiments on three popular person re-ID datasets demonstrate that ICG surpasses most state-of-the-art methods, exhibiting remarkable improvements over the baseline.", "title_embedding_index": 19365, "title_abs_embedding_index": 19390}, {"title": "Decoupling Backdoors from Main Task: Toward the Effective and Durable Backdoors in Federated Learning", "link_suffix": "/forum?id=Mb5vJijcHn", "link": "https://openreview.net/forum?id=Mb5vJijcHn", "pdf_link": "https://openreview.net/pdf?id=Mb5vJijcHn", "keywords": "Backdoor Attack\uff0cfederated learning", "abstract": "Federated learning, as a distributed machine learning method, enables multiple participants to collaboratively train a central model without sharing their private data. However, this decentralized mechanism introduces new privacy and security concerns. Malicious attackers can embed backdoors into local models, which are inherited by the central global model through the federated aggregation process. While previous studies have demonstrated the effectiveness of backdoor attacks, the effectiveness and durability often rely on unrealistic assumptions, such as a large number of attackers and scaled malicious contributions. These assumptions arise because a sufficient number of attackers can neutralize the contributions of honest participants, allowing the backdoor to be successfully inherited by the central model. In this work, we attribute these backdoor limitations to the coupling between the main and backdoor tasks. To address these backdoor limitations, we propose a min-max backdoor attack framework that decouples backdoors from the main task, ensuring that these two tasks do not interfere with each other. The maximization phase employs the principle of universal adversarial perturbation to create triggers that amplify the performance disparity between poisoned and benign samples. These samples are then used to train a backdoor model in the minimization process. We evaluate the proposed framework in both image classification and semantic analysis tasks. Comparisons with four backdoor attack methods under five defense algorithms show that our method achieves good attack performance even if there is a small number of attackers and when the submitted model parameters are not scaled. In addition, even if attackers are completely removed in the training process, the implanted backdoors will not be dramatically weakened by the contributions of other honest participants.", "title_embedding_index": 19366, "title_abs_embedding_index": 19391}, {"title": "Knowledge Benchmark Graph: Assisting Large Language Models in Designing Models by Retrieving Benchmark Knowledge", "link_suffix": "/forum?id=49fIu0yDJ4", "link": "https://openreview.net/forum?id=49fIu0yDJ4", "pdf_link": "https://openreview.net/pdf?id=49fIu0yDJ4", "keywords": "Knowledge Graph, Auto Machine Learning", "abstract": "In recent years, the design and transfer of neural network models have been widely studied due to their exceptional performance and capabilities. However, the complex nature of datasets and the vast architecture space pose significant challenges for both manual and automated algorithms in creating high-performance models. Inspired by researchers who design, train, and document the performance of various models across different datasets, this paper introduces a novel schema that transforms the benchmark data into a Knowledge Benchmark Graph (KBG), which primarily stores the facts in the form of performance(data, model). Constructing the KBG facilitates the structured storage of design knowledge, aiding subsequent model design and transfer. However, it is a non-trivial task to retrieve or design suitable neural networks based on the KBG, as real-world data are often off the records. To tackle this challenge, we propose transferring existing models stored in KBG by establishing correlations between unseen and previously seen datasets. Given that measuring dataset similarity is a complex and open-ended issue, we explore the potential for evaluating the correctness of the similarity function. Then, we further integrate the KBG with Large Language Models (LLMs), assisting LLMs to think and retrieve existing model knowledge in a manner akin to humans when designing or transferring models. We demonstrate our method specifically in the context of Graph Neural Network (GNN) architecture design, constructing a KBG (with 26,206 models, 211,669 performance records, and 2,540,064 facts) and validating the effectiveness of leveraging the KBG to promote GNN architecture design.", "title_embedding_index": 19367, "title_abs_embedding_index": 19392}, {"title": "LEGO: Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion", "link_suffix": "/forum?id=SMZqIOSdlN", "link": "https://openreview.net/forum?id=SMZqIOSdlN", "pdf_link": "https://openreview.net/pdf?id=SMZqIOSdlN", "keywords": "feature fusion, video anomaly detection, graph learning, multi-modal learning", "abstract": "In computer vision tasks, features often come from diverse representations, domains (e.g., indoor and outdoor), and modalities (e.g., text, images, and videos). Effectively fusing these features is essential for robust performance, especially with the availability of powerful pre-trained models like vision-language models. However, common fusion methods, such as concatenation, element-wise operations, and non-linear techniques, often fail to capture structural relationships, deep feature interactions, and suffer from inefficiency or misalignment of features across domains or modalities. In this paper, we shift from high-dimensional feature space to a lower-dimensional, interpretable graph space by constructing relationship graphs that encode feature relationships at different levels, e.g., clip, frame, patch, token, etc. To capture deeper interactions, we use graph power expansions and introduce a learnable graph fusion operator to combine these graph powers for more effective fusion. Our approach is relationship-centric, operates in a homogeneous space, and is mathematically principled, resembling element-wise relationship score aggregation via multilinear polynomials. We demonstrate the effectiveness of our graph-based fusion method on video anomaly detection, showing strong performance across multi-representational, multi-modal, and multi-domain feature fusion tasks.", "title_embedding_index": 19368, "title_abs_embedding_index": 19393}, {"title": "Video Instruction Tuning with Synthetic Data", "link_suffix": "/forum?id=8Livf4oZxz", "link": "https://openreview.net/forum?id=8Livf4oZxz", "pdf_link": "https://openreview.net/pdf?id=8Livf4oZxz", "keywords": "Video instruction dataset, video-language model", "abstract": "The development of video large multimodal models (LMMs) has been hindered by the difficulty of curating large amounts of high-quality raw data from the web. To address this, we consider an alternative approach, creating a high-quality synthetic dataset specifically for video instruction-following, namely LLaVA-Video-178K. This dataset includes key tasks such as detailed captioning, open-ended question-answering (QA), and multiple-choice QA. By training on this proposed dataset, in combination with existing visual instruction tuning data, we introduce LLaVA-Video, a new video LMM. Our experiments demonstrate that LLaVA-Video achieves strong performance across various video benchmarks, highlighting the effectiveness of our dataset. We plan to release the dataset, its generation pipeline, and the model checkpoints.", "title_embedding_index": 19369, "title_abs_embedding_index": 19394}, {"title": "Advancing Supervised Local Learning Beyond Classification with Long-term Feature Bank", "link_suffix": "/forum?id=igZ5PlRB0t", "link": "https://openreview.net/forum?id=igZ5PlRB0t", "pdf_link": "https://openreview.net/pdf?id=igZ5PlRB0t", "keywords": "Local learning", "abstract": "Local learning offers an alternative to traditional end-to-end back-propagation in deep neural networks, significantly reducing GPU memory usage. While local learning has shown promise in image classification tasks, its application to other visual tasks remains limited. This limitation arises primarily from two factors: 1) architectures tailored for classification are often not transferable to other tasks, leading to a lack of reusability of task-specific knowledge; 2) the absence of cross-scale feature communication results in degraded performance in tasks such as object detection and super-resolution. To address these challenges, we propose the Memory-augmented Auxiliary Network (MAN), which introduces a simplified design principle and incorporates a feature bank to enhance cross-task adaptability and communication. This work represents the first successful application of local learning methods beyond classification, demonstrating that MAN not only conserves GPU memory but also achieves performance on par with end-to-end approaches across multiple datasets for various visual tasks.", "title_embedding_index": 19370, "title_abs_embedding_index": 19395}, {"title": "On the Relation Between Linear Diffusion and Power Iteration", "link_suffix": "/forum?id=mKM9uoKSBN", "link": "https://openreview.net/forum?id=mKM9uoKSBN", "pdf_link": "https://openreview.net/pdf?id=mKM9uoKSBN", "keywords": "Diffusion models", "abstract": "Recently, diffusion models have gained popularity due to their impressive generative abilities. These models learn the implicit distribution given by the training dataset, and sample new data by transforming random noise through the reverse process, which can be thought of as gradual denoising. In this work, we examine the generation process as a ``correlation machine'', where random noise is repeatedly enhanced in correlation with the implicit given distribution. \nTo this end, we explore the linear case, where the optimal denoiser is known to be the PCA projection. This enables us to connect the theory of diffusion models to the spiked covariance model, where the dependence of the denoiser on the noise level and the amount of training data can be expressed analytically, in the rank-1 case.\nIn a series of numerical experiments, we extend this result to general low rank data, and show that low frequencies emerge earlier in the generation process, where the denoising basis vectors are more aligned to the true data with a rate depending on their eigenvalues. This model allows us to show that the linear diffusion model converges in mean to the leading eigenvector of the underlying data, similarly to the prevalent Power Iteration method. \nFinally, we empirically demonstrate the applicability of our findings beyond the linear case, in the Jacobians of a deep, non-linear denoiser, used in general image generation tasks.", "title_embedding_index": 19371, "title_abs_embedding_index": 19396}, {"title": "Enhancing Multilingual Reasoning in LLMs: Insights from Cross-Linguistic Correlations and Optimal Data Proportions", "link_suffix": "/forum?id=S6cBH99BhB", "link": "https://openreview.net/forum?id=S6cBH99BhB", "pdf_link": "https://openreview.net/pdf?id=S6cBH99BhB", "keywords": "Large Language Models, Multilingual Reasoning, Fine-Tuning", "abstract": "Large language models (LLMs) typically rely on fine-tuning to enhance their reasoning capabilities across various languages. However, limited research has been conducted on the optimal balance of language proportions within multilingual reasoning datasets. To fill this gap, we performed a systematic study to examine how different proportions of language data in multilingual reasoning datasets influence fine-tuning performance. Our study revealed a clear relationship between language proportions in datasets and the fine-tuning performance of LLMs. By fine-tuning multiple LLMs using the appropriate language distributions and data volumes identified in our study, we achieved state-of-the-art performance in both multilingual mathematical reasoning and solving mathematical problems using Python code. Furthermore, our approach significantly reduced data volume requirements and translation costs compared to existing methods, providing a valuable reference for future research.", "title_embedding_index": 19372, "title_abs_embedding_index": 19397}, {"title": "Personalized Federated Learning via Variational Massage Passing", "link_suffix": "/forum?id=BJ9mzoSeu1", "link": "https://openreview.net/forum?id=BJ9mzoSeu1", "pdf_link": "https://openreview.net/pdf?id=BJ9mzoSeu1", "keywords": "Personalized federated learning, variational message passing, feature representation learning", "abstract": "Conventional federated learning (FL) aims to train a unified machine learning model that fits data distributed across various agents. However, statistical heterogeneity arising from diverse data resources renders the single global model trained by FL ineffective for all clients. Personalized federated learning (pFL) has been proposed to primarily address this challenge by tailoring individualized models to each client's specific dataset while integrating global information during feature aggregation. Achieving efficient pFL necessitates the accurate estimation of global feature information across all the training data. Nonetheless, balancing the personalization of individual models with the global consensus of feature information remains a significant challenge in existing approaches. \nIn this paper, we propose pFedVMP, a novel pFL approach that employs variational message passing (VMP) to design feature aggregation protocols. \nBy leveraging the first-order and second-order statistical information, pFedVMP yields more precise estimates of the distributions of model parameters and global feature centroids.\nAdditionally, pFedVMP is effective in boosting training accuracy and preventing overfitting by regularizing local training with global feature centroids.\nExtensive experiments on heterogeneous data conditions demonstrate that pFedVMP surpasses state-of-the-art methods in both effectiveness and fairness.", "title_embedding_index": 19373, "title_abs_embedding_index": 19398}, {"title": "Towards Realistic Long-tailed Semi-supervised Learning in an Open World", "link_suffix": "/forum?id=zLHP6QDWYp", "link": "https://openreview.net/forum?id=zLHP6QDWYp", "pdf_link": "https://openreview.net/pdf?id=zLHP6QDWYp", "keywords": "Open-world, Realistic long-tailed semi-supervised learning, Logit adjustment", "abstract": "Open-world long-tailed semi-supervised learning (OLSSL) has increasingly attracted attention. However, existing OLSSL algorithms generally assume that the distributions between known and novel categories are nearly identical. Against this backdrop, we construct a more Realistic Open-world Long-tailed Semi-supervised Learning (ROLSSL) setting where there is no premise on the distribution relationships between known and novel categories. Furthermore, even within the known categories, the number of labeled samples is significantly smaller than that of the unlabeled samples, as acquiring valid annotations is often prohibitively costly in the real world. Under the proposed ROLSSL setting, we propose a simple yet potentially effective solution called dual-stage post-hoc logit adjustments. The proposed approach revisits the logit adjustment strategy by considering the relationships among the frequency of samples, the total number of categories, and the overall size of data. Then, it estimates the distribution of unlabeled data for both known and novel categories to dynamically readjust the corresponding predictive probabilities, effectively mitigating category bias during the learning of known and novel classes with more selective utilization of imbalanced unlabeled data. Extensive experiments on datasets such as CIFAR100 and ImageNet100 have demonstrated performance improvements of up to 50.1%, validating the superiority of our proposed method and establishing a strong baseline for this task. For further researches, the experimental code will be open soon.", "title_embedding_index": 19374, "title_abs_embedding_index": 19399}]