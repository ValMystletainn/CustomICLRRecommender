[
    {
        "title": "SwitchLoRA: Switched Low-Rank Adaptation Can Learn Full-Rank Information",
        "link_suffix": "/forum?id=iEUZMISIKj",
        "link": "https://openreview.net/forum?id=iEUZMISIKj",
        "pdf_link": "https://openreview.net/pdf?id=iEUZMISIKj",
        "keywords": "pre-training, lora, training efficiency, large language models",
        "abstract": "In the training of large language models, parameter-efficient techniques such as LoRA optimize memory usage and reduce communication overhead during the fine-tuning phase. However, applying such techniques directly during the pre-training phase results in poor performance, primarily because the premature implementation of low-rank training significantly reduces model accuracy. Existing methods like ReLoRA and GaLore have attempted to address this challenge by updating the low-rank subspace. However, they still fall short of achieving the accuracy of full-rank training because they must limit the update frequency to maintain optimizer state consistency, hindering their ability to closely approximate full-rank training behavior. In this paper, we introduce SwitchLoRA, a parameter-efficient training technique that frequently and smoothly replaces the trainable parameters of LoRA adapters with alternative parameters. SwitchLoRA updates the low-rank subspace incrementally, targeting only a few dimensions at a time to minimize the impact on optimizer states. This allows a higher update frequency, thereby enhancing accuracy by enabling the updated parameters to more closely mimic full-rank behavior during the pre-training phase. Our results demonstrate that SwitchLoRA actually surpasses full-rank training, reducing perplexity from 15.23 to 15.01 on the LLaMA 1.3B model while reducing communication overhead by 54% on the LLaMA 1.3B model. Furthermore, after full fine-tuning the SwitchLoRA pre-trained model and the full-rank pre-trained model on the GLUE benchmark, the SwitchLoRA pre-trained model showed an average accuracy gain of about 1% over the full-rank pre-trained model. This demonstrates enhanced generalization and reasoning capabilities of SwitchLoRA."
    },
    {
        "title": "Runtime Learning Machine",
        "link_suffix": "/forum?id=KCTHM2Ffh3",
        "link": "https://openreview.net/forum?id=KCTHM2Ffh3",
        "pdf_link": "https://openreview.net/pdf?id=KCTHM2Ffh3",
        "keywords": "Runtime Learning, Deep Reinforcement Learning, Safety, Unknown Unknown, Autonomous Systems",
        "abstract": "This paper proposes the Runtime Learning Machine for safety-critical autonomous systems. The learning machine has three interactive components: a high-performance (HP)-Student, a high-assurance (HA)-Teacher, and a Coordinator. The HP-Student is a high-performance but not fully verified Phy-DRL (physics-regulated deep reinforcement learning) agent that performs safe runtime learning in real plants, using real-time sensor data from real-time physical environments. On the other hand, HA-Teacher is a verified but simplified design, focusing on safety-critical functions. As a complementary, HA-Teacher's novelty lies in real-time patch for two missions: i) correcting unsafe learning of HP-Student, and ii) backing up safety. The Coordinator manages the interaction between HP-Student and HA-Teacher. Powered by the three interactive components, the runtime learning machine notably features i) assuring lifetime safety (i.e., safety guarantee in any runtime learning stage, regardless of HP-Student's success), ii) tolerating unknown unknowns, iii) addressing Sim2Real gap, and iv) automatic hierarchy learning (i.e., safety-first learning, and then high-performance learning). Experimental results involving a cart-pole system and a real quadruped robot, as well as comparisons with state-of-the-art safe DRL, fault-tolerant DRL, and approaches for addressing Sim2Real gap, demonstrate the machine's effectiveness and unique features."
    },
    {
        "title": "Path Complex Message Passing for Molecular Property Prediction",
        "link_suffix": "/forum?id=3nwlXtQESj",
        "link": "https://openreview.net/forum?id=3nwlXtQESj",
        "pdf_link": "https://openreview.net/pdf?id=3nwlXtQESj",
        "keywords": "Molecular Property Prediction; Path Complex; Geometric Deep Learning; High-order Interaction; Low-order Interaction",
        "abstract": "Geometric deep learning (GDL) has demonstrated enormous power in molecular data analysis. However, GDL faces challenges in achieving high efficiency and expressivity in molecular representations when high-order terms of the atomic force fields are not sufficiently learned. In this work, we introduce message passing on path complexes, called the Path Complex Message Passing, for molecular prediction. Path complexes represent the geometry of paths and can model the chemical and non-chemical interactions of atoms in a molecule across various dimensions. Our model defines messages on path complexes and employs neural message passing to learn simplex features, enabling feature communication within and between different dimensions. Since messages on high-order and low-order path complexes reflect different aspects of molecular energy, they are updated sequentially according to their order. The higher the order of the path complex, the richer the information it contains, and the higher its priority during inference. It can thus characterize various types of molecular interactions specified in molecular dynamics (MD) force fields. Our model has been extensively validated on benchmark datasets and achieves state-of-the-art results.\nThe code is available at \\url{https://anonymous.4open.science/r/Path-Complex-Neural-Network-32D6}"
    },
    {
        "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
        "link_suffix": "/forum?id=n0OtGl6VGb",
        "link": "https://openreview.net/forum?id=n0OtGl6VGb",
        "pdf_link": "https://openreview.net/pdf?id=n0OtGl6VGb",
        "keywords": "Large Language Models; KV Cache Compression; KV Cache Pruning",
        "abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications. However, their increased computational and memory demands present significant challenges, especially when handling long sequences. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence length, we identify substantial redundancy in the channel dimension of the KV cache, as indicated by an uneven magnitude distribution and a low-rank structure in the attention weights. In response, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in KV cache memory costs by over 20% compared with vanilla KV cache eviction and quantization methods. For instance, ThinK integrated with KIVI can achieve a 2.8x reduction in peak memory usage while maintaining nearly the same quality, enabling up to a 5x increase in batch size when using a single GPU. Extensive evaluations on the LLaMA and Mistral models across various long-sequence datasets verified the efficiency of ThinK, establishing a new baseline algorithm for efficient LLM deployment without compromising performance."
    },
    {
        "title": "THE ROBUSTNESS OF DIFFERENTIABLE CAUSAL DISCOVERY IN MISSPECIFIED SCENARIOS",
        "link_suffix": "/forum?id=iaP7yHRq1l",
        "link": "https://openreview.net/forum?id=iaP7yHRq1l",
        "pdf_link": "https://openreview.net/pdf?id=iaP7yHRq1l",
        "keywords": "Differentiable causal discovery, model assumption violations, benchmark",
        "abstract": "Causal discovery aims to learn causal relationships between variables from targeted data, making it a fundamental task in machine learning. However, causal discovery algorithms often rely on unverifiable causal assumptions, which are usually difficult to satisfy in real-world data, thereby limiting the broad application of causal discovery in practical scenarios. Inspired by these considerations, this work extensively benchmarks the empirical performance of various mainstream causal discovery algorithms, which assume i.i.d. data, under eight model assumption violations. Our experimental results show that differentiable causal discovery methods exhibit counter-intuitive robustness under the metrics of Structural Hamming Distance and Structural Intervention Distance of the inferred graphs in challenging scenarios, except for scale variation. We also provide the theoretical explanations for the performance of differentiable causal discovery methods. Finally, our work aims to comprehensively benchmark the performance of recent differentiable causal discovery methods under model assumption violations, and provide the standard for reasonable evaluation of causal discovery, as well as to further promote its application in real-world scenarios."
    },
    {
        "title": "A Sanity Check for AI-generated Image Detection",
        "link_suffix": "/forum?id=ODRHZrkOQM",
        "link": "https://openreview.net/forum?id=ODRHZrkOQM",
        "pdf_link": "https://openreview.net/pdf?id=ODRHZrkOQM",
        "keywords": "AI-generated image detection, Chameleon dataset, mixture-of-experts",
        "abstract": "With the rapid development of generative models, discerning AI-generated content has evoked increasing attention from both industry and academia. In this paper, we conduct a sanity check on \"whether the task of AI-generated image detection has been solved\". To start with, we present Chameleon dataset, consisting AI-generated images that are genuinely challenging for human perception. To quantify the generalization of existing methods, we evaluate 9 off-the-shelf AI-generated image detectors on Chameleon dataset. Upon analysis, almost all models classify AI-generated images as real ones. Later, we propose AIDE (AI-generated Image DEtector with Hybrid Features), which leverages multiple experts to simultaneously extract visual artifacts and noise patterns. Specifically, to capture the high-level semantics, we utilize CLIP to compute the visual embedding. This effectively enables the model to discern AI-generated images based on semantics or contextual information; Secondly, we select the highest frequency patches and the lowest frequency patches in the image, and compute the low-level patchwise features, aiming to detect AI-generated images by low-level artifacts, for example, noise pattern, anti-aliasing, etc. While evaluating on existing benchmarks, for example, AIGCDetectBenchmark and GenImage, AIDE achieves +3.5% and +4.6% improvements to state-of-the-art methods, and on our proposed challenging Chameleon benchmarks, it also achieves the promising results, despite this problem for detecting AI-generated images is far from being solved. The dataset, codes, and pre-train models will be made publicly available."
    },
    {
        "title": "Neural Phylogeny: Fine-Tuning Relationship Detection among Neural Networks",
        "link_suffix": "/forum?id=jv2zHOalpL",
        "link": "https://openreview.net/forum?id=jv2zHOalpL",
        "pdf_link": "https://openreview.net/pdf?id=jv2zHOalpL",
        "keywords": "Neural Phylogeny, Finetuning",
        "abstract": "Given a collection of neural networks, can we determine which are parent models and which are child models fine-tuned from the parents?\nIn this work, we strive to answer this question\nvia introducing a new task termed as neural phylogeny detection, aimed at identifying the existence and direction of the fine-tuning relationship. Specifically, neural phylogeny detection attempts to identify all parent-child model pairs and determine, within each pair, which model is the parent and which is the child.\nWe present two approaches for neural phylogeny detection: a learning-free method and a learning-based method. First, we propose a metric that leverages the distance from network parameters to a fake initialization to infer fine-tuning directions. By integrating this metric with traditional clustering algorithms, we propose a series of efficient, learning-free neural phylogeny detection methods. Second, we introduce a transformer-based neural phylogeny detector, which significantly enhances detection accuracy through a learning-based manner. Extensive experiments, ranging from shallow fully-connected networks to open-sourced Stable Diffusion and LLaMA models, progressively validate the effectiveness of both methods. The results demonstrate the reliability of both the learning-free and the learning-based approaches across various learning tasks and network architectures, as well as their ability to detect cross-generational phylogeny between ancestor models and their fine-tuned descendants."
    },
    {
        "title": "Practical\u03f5-Exploring Thompson Sampling for Reinforcement Learning with Continuous Controls",
        "link_suffix": "/forum?id=W5S1DEjN8x",
        "link": "https://openreview.net/forum?id=W5S1DEjN8x",
        "pdf_link": "https://openreview.net/pdf?id=W5S1DEjN8x",
        "keywords": "Reinforcement Learning, Exploration, Thompson Sampling",
        "abstract": "Balancing exploration and exploitation is crucial in reinforcement learning (RL). While Thompson Sampling (TS) is a sound and effective exploration strategy, its application to RL with high-dimensional continuous controls remains challenging. We propose Practical $\\epsilon$-Exploring Thompson Sampling (PETS), a practical approach that addresses these challenges. Since the posterior over the parameters of the action-value function is intractable, we leverage Langevin Monte Carlo (LMC) for sampling. We propose an approach which maintains $n$ parallel Markov chains to mitigate the issues of nai\"{ve} application of LMC. The next step following the posterior sampling in TS involves finding the optimal action under the sampled model of the action-value function. We explore both gradient-based and gradient-free approaches to approximate the optimal action, with extensive experiments. Furthermore, to justify the use of gradient-based optimization to approximate the optimal action, we analyze the regret for TS in the RL setting with continuous controls and show that it achieves the best-known bound previously established for the discrete setting. Our empirical results demonstrate that PETS, as an exploration strategy, can be integrated with leading RL algorithms, enhancing their performance and stability on benchmark continuous control tasks."
    },
    {
        "title": "Mitigating Time Discretization Challenges with WeatherODE: A Sandwich Physics-Driven Neural ODE for Weather Forecasting",
        "link_suffix": "/forum?id=UFzE9njwMG",
        "link": "https://openreview.net/forum?id=UFzE9njwMG",
        "pdf_link": "https://openreview.net/pdf?id=UFzE9njwMG",
        "keywords": "Weather Forecasting, NeuralODE, Time discretization",
        "abstract": "In the field of weather forecasting, traditional models often grapple with discretization errors and time-dependent source discrepancies, which limit their predictive performance. In this paper, we present WeatherODE, a novel one-stage, physics-driven ordinary differential equation (ODE) model designed to enhance weather forecasting accuracy. By leveraging wave equation theory and integrating a time-dependent source model, WeatherODE effectively addresses the challenges associated with time-discretization error and dynamic atmospheric processes. Moreover, we design a CNN-ViT-CNN sandwich structure, facilitating efficient learning dynamics tailored for distinct yet interrelated tasks with varying optimization biases in advection equation estimation. Through rigorous experiments, WeatherODE demonstrates superior performance in both global and regional weather forecasting tasks, outperforming recent state-of-the-art approaches by significant margins of over 40.0% and 31.8% in root mean square error (RMSE), respectively. The source code is available athttps://anonymous.4open.science/r/WeatherODE-5C13/."
    },
    {
        "title": "A Revisit of Total Correlation in Disentangled Variational Auto-Encoder with Partial Disentanglement",
        "link_suffix": "/forum?id=rdE9MCcNCz",
        "link": "https://openreview.net/forum?id=rdE9MCcNCz",
        "pdf_link": "https://openreview.net/pdf?id=rdE9MCcNCz",
        "keywords": "disentangling variational auto-encoder, independent component analysis, neural subspace, neuroscience",
        "abstract": "A fully disentangled variational auto-encoder (VAE) aims to identify disentangled latent components from observations. However, enforcing full independence between all latent components may be too strict for certain datasets. In some cases, multiple factors may be entangled together in a non-separable manner, or a single independent semantic meaning could be represented by multiple latent components within a higher-dimensional manifold. To address such scenarios with greater flexibility, we propose the Partially Disentangled VAE (PDisVAE), which generalizes the total correlation (TC) term in fully disentangled VAEs to a partial correlation (PC) term. This framework can handle group-wise independence and can naturally reduce to either the standard VAE or the fully disentangled VAE. Validation through three synthetic experiments demonstrates the correctness and practicality of PDisVAE. When applied to real-world datasets, PDisVAE discovers valuable information that is difficult to find using fully disentangled VAEs, implying its versatility and effectiveness."
    },
    {
        "title": "Enhancing Time-Series Forecasting with Iterative Decomposition and Separable Training",
        "link_suffix": "/forum?id=gEEJBXktQM",
        "link": "https://openreview.net/forum?id=gEEJBXktQM",
        "pdf_link": "https://openreview.net/pdf?id=gEEJBXktQM",
        "keywords": "time series forecasting, training method",
        "abstract": "Time series data, crucial for decision-making in fields like finance and healthcare, often presents challenges due to its inherent complexity, exacerbating the bias-variance tradeoff and leading to overfitting and underfitting in conventional forecasting models. While promising, state-of-the-art models like PatchTST, iTransformer, and DLinear are hindered by this tradeoff, limiting their ability to separate predictable patterns from noise. To resolve this, we propose the IDEAS framework, which reduces the bias-variance tradeoff to help models achieve optimal performance. IDEAS combines iterative residual decomposition, which reduces bias by extracting predictable patterns, and separable training, which reduces variance by independently optimizing each component. We provide theoretical proof and demonstrate through experiments that IDEAS significantly improves performance across four state-of-the-art models on nine complex benchmark datasets, offering a more robust solution for complex time series forecasting."
    },
    {
        "title": "Breaking thelog\u2061(1/\u03942)Barrier: Better Batched Best Arm Identification with Adaptive Grids",
        "link_suffix": "/forum?id=buxFBI6GG4",
        "link": "https://openreview.net/forum?id=buxFBI6GG4",
        "pdf_link": "https://openreview.net/pdf?id=buxFBI6GG4",
        "keywords": "Bandits",
        "abstract": "We investigate the problem of batched best arm identification in multi-armed bandits, where we want to find the best arm from a set of $n$ arms while minimizing both the number of samples and batches. We introduce an algorithm that achieves near-optimal sample complexity and features an instance-sensitive batch complexity, which breaks the $\\log(1/\\Delta_2)$ barrier. The main contribution of our algorithm is a novel sample allocation scheme that effectively balances exploration and exploitation for batch sizes. Experimental results indicate that our approach is more batch-efficient across various setups. We also extend this framework to the problem of batched best arm identification in linear bandits and achieve similar improvements."
    },
    {
        "title": "Visual Agents as Fast and Slow Thinkers",
        "link_suffix": "/forum?id=ncCuiD3KJQ",
        "link": "https://openreview.net/forum?id=ncCuiD3KJQ",
        "pdf_link": "https://openreview.net/pdf?id=ncCuiD3KJQ",
        "keywords": "Multimodal Large Language Model, System2 Thinking, Language Agent",
        "abstract": "Achieving human-level intelligence requires refining cognitive distinctions between \\textit{System 1} and \\textit{System 2} thinking. While contemporary AI, driven by large language models, demonstrates human-like traits, it falls short of genuine cognition. Transitioning from structured benchmarks to real-world scenarios presents challenges for visual agents, often leading to inaccurate and overly confident responses. To address the challenge, we introduce \\textbf{\\textsc{FaST}}, which incorporates the \\textbf{Fa}st and \\textbf{S}low \\textbf{T}hinking mechanism into visual agents. \\textsc{FaST} employs a switch adapter to dynamically select between \\textit{System 1/2} modes, tailoring the problem-solving approach to different task complexity. It tackles uncertain and unseen objects by adjusting model confidence and integrating new contextual data. With this novel design, we advocate a \\textit{flexible system}, \\textit{hierarchical reasoning} capabilities, and a \\textit{transparent decision-making} pipeline, all of which contribute to its ability to emulate human-like cognitive processes in visual intelligence. Empirical results demonstrate that \\textsc{FaST} outperforms various well-known baselines, achieving 80.8% accuracy over $VQA^{v2}$ for visual question answering and 48.7% $GIoU$ score over ReasonSeg for reasoning segmentation, demonstrate \\textsc{FaST}'s superior performance. Extensive testing validates the efficacy and robustness of \\textsc{FaST}'s core components, showcasing its potential to advance the development of cognitive visual agents in AI systems."
    },
    {
        "title": "Discovering the Gems in Early Layers: Accelerating Long-Context LLMs with 1000x Input Token Reduction",
        "link_suffix": "/forum?id=9iN8p1Xwtg",
        "link": "https://openreview.net/forum?id=9iN8p1Xwtg",
        "pdf_link": "https://openreview.net/pdf?id=9iN8p1Xwtg",
        "keywords": "Large Language Models, Long Context, Inference Acceleration",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in handling long context inputs, but this comes at the cost of increased computational resources and latency. Our research introduces a novel approach for the long context bottleneck to accelerate LLM inference and reduce GPU memory consumption. Our research demonstrates that LLMs can identify relevant tokens in the early layers before generating answers to a query. Leveraging this insight, we propose an algorithm that uses early layers of an LLM as filters to select and compress input tokens, significantly reducing the context length for subsequent processing.\nOur method, GemFilter, demonstrates substantial improvements in both speed and memory efficiency compared to existing techniques, such as standard attention and SnapKV/H2O. Notably, it achieves a 2.4$\\times$ speedup and 30% reduction in GPU memory usage compared to SOTA methods. Evaluation on the Needle in a Haystack task shows that GemFilter significantly outperforms standard attention, SnapKV and demonstrates comparable performance on the LongBench challenge.\nGemFilter is simple, training-free, and broadly applicable across different LLMs. Crucially, it provides interpretability by allowing humans to inspect the selected input sequence. These findings not only offer practical benefits for LLM deployment, but also enhance our understanding of LLM internal mechanisms, paving the way for further optimizations in LLM design and inference."
    },
    {
        "title": "Mamba-HMIL: Hierarchical Multiple Instance Learning via State Space Model for Whole Slide Image Diagnosis",
        "link_suffix": "/forum?id=0yVP49SDg0",
        "link": "https://openreview.net/forum?id=0yVP49SDg0",
        "pdf_link": "https://openreview.net/pdf?id=0yVP49SDg0",
        "keywords": "Whole Slide Images, Hierarchical Multiple Instance Learning, State Space Model.",
        "abstract": "Multiple instance learning (MIL) has been widely employed for gigapixel whole slide image (WSI) classification. Existing MIL methods, however, are found wanting to align with the clinical practice of pathologists, who typically scrutinize WSIs at varied scales and compare the local regions in a global perspective. Given that WSIs usually boast immense dimensions peppered with large regions not pertinent to diagnosis, we propose a novel hierarchical multiple instance learning method based on the state space model, called Mamba-HMIL, for WSI classification. Mamba-HMIL consists of three primary modules to enhance the performance of MIL. First, the hierarchical feature extractor harvests features across diverse scales. Second, for capturing the correlation among patches, the state space model demonstrates robust modeling capabilities. A Mixture of Experts (MoE) module is for stable SSM training. Third, the adaptive selection model strives to reduce redundancies by focusing on disease-positive regions. We evaluate Mamba-HMIL on two WSI subtype datasets (TCGA-NSCLC and TCGA-RCC) and two WSI survival datasets (TCGA-BRCA and TCGA-BLCA). Our results suggest that Mamba-HMIL outperforms existing MIL methods on both WSI tasks. Our code will be made publicly available."
    },
    {
        "title": "FoGE: Fock Space inspired encoding for graph prompting",
        "link_suffix": "/forum?id=z7QAz5y8Uz",
        "link": "https://openreview.net/forum?id=z7QAz5y8Uz",
        "pdf_link": "https://openreview.net/pdf?id=z7QAz5y8Uz",
        "keywords": "llm, prefix tuning, graph, graph encoding, geometric algebra, Fock space",
        "abstract": "Recent results show that modern Large Language Models (LLM) are indeed capable of understanding and answering questions about structured data such as graphs. Existing proposals often use some description of the graph to create an ``augmented'' prompt fed to the LLM. For a chosen class of graphs, if a well-tailored graph encoder is deployed to play together with a pre-trained LLM, the model can answer graph-related questions well. Existing solutions to graph-based prompts range from graph serialization to graph transformers. In this work, we show that the use of a parameter-free graph encoder based on Fock space representations, a concept borrowed from mathematical physics, is remarkably versatile in this problem setting. The simple construction, inherited directly from the theory with a few small adjustments, can provide rich and informative graph encodings, for a wide range of different graphs. We investigate the use of this idea for prefix-tuned prompts leveraging the capabilities of a pre-trained, frozen LLM. The modifications lead to a model that can answer graph-related questions -- from simple graphs to proteins to hypergraphs -- effectively and with minimal, if any, adjustments to the architecture. Our work significantly simplifies existing solutions and generalizes well to multiple different graph-based structures effortlessly."
    },
    {
        "title": "Geometric Median (GM) Matching for Robust Data Pruning",
        "link_suffix": "/forum?id=e2F0mJJeN0",
        "link": "https://openreview.net/forum?id=e2F0mJJeN0",
        "pdf_link": "https://openreview.net/pdf?id=e2F0mJJeN0",
        "keywords": "data pruning, robust, data selection",
        "abstract": "Data pruning, the combinatorial task of selecting a small and informative subset from a large dataset, is crucial for mitigating the enormous computational costs associated with training data-hungry modern deep learning models at scale. Since large-scale data collections are invariably noisy, developing data pruning strategies that remain robust even in the presence of corruption is critical in practice. \nIn response, we propose $\\gmm$ -- a herding~\\citep{welling2009herding} style greedy algorithm -- that {\\em yields a $k$-subset such that the mean of the subset approximates the geometric median of the (potentially) noisy dataset}. Theoretically, we show that $\\gm$ Matching enjoys an improved $\\gO(1/k)$ scaling over $\\gO(1/\\sqrt{k})$ scaling of uniform sampling; while achieving the optimal breakdown point of 1/2 even under arbitrary corruption. Extensive experiments across popular deep learning benchmarks indicate that $\\gm$ Matching consistently outperforms prior state-of-the-art; the gains become more profound at high rates of corruption and aggressive pruning rates; making it a strong baseline for robust data pruning."
    },
    {
        "title": "Fourier Sliced-Wasserstein Embedding for Multisets and Measures",
        "link_suffix": "/forum?id=BcYt84rcKq",
        "link": "https://openreview.net/forum?id=BcYt84rcKq",
        "pdf_link": "https://openreview.net/pdf?id=BcYt84rcKq",
        "keywords": "Sliced Wasserstein distance, Euclidean embedding, bi-Lipschitz, permutation invariance, multisets, optimal transport",
        "abstract": "We present theFourier Sliced Wasserstein (FSW) embedding\u2014a novel method to embed multisets and measures over $\\mathbb{R}^d$ into Euclidean space.Our proposed embedding approximately preserves the sliced Wasserstein distance on distributions, thereby yielding geometrically meaningful representations that better capture the structure of the input. Moreover, it is injective on measures andbi-Lipschitzon multisets\u2014a significant advantage over prevalent embedding methods based on sum- or max-pooling, which are provably not bi-Lipschitz, and in many cases, not even injective.\nThe required output dimension for these guarantees is near optimal: roughly $2 n d$, where $n$ is the maximal number of support points in the input.Conversely, we prove that it isimpossibleto embed distributions over $\\mathbb{R}^d$ into Euclidean space in a bi-Lipschitz manner. Thus, the metric properties of our embedding are, in a sense, the best achievable.Through numerical experiments, we demonstrate that our method yields superior representations of input multisets and offers practical advantage for learning on multiset data. Specifically, we show that (a) the FSW embedding induces significantly lower distortion on the space of multisets, compared to the leading method for computing sliced-Wasserstein-preserving embeddings; and (b) a simple combination of the FSW embedding and an MLP achieves state-of-the-art performance in learning the (non-sliced) Wasserstein distance."
    },
    {
        "title": "Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data",
        "link_suffix": "/forum?id=fgUFZAxywx",
        "link": "https://openreview.net/forum?id=fgUFZAxywx",
        "pdf_link": "https://openreview.net/pdf?id=fgUFZAxywx",
        "keywords": "Optimal Transport, Spherical Data Analysis",
        "abstract": "Efficient comparison of spherical probability distributions becomes important in fields such as computer vision, geosciences, and medicine. Sliced optimal transport distances, such as spherical and stereographic spherical sliced Wasserstein distances, have recently been developed to address this need. These methods reduce the computational burden of optimal transport by slicing hyperspheres into one-dimensional projections, i.e., lines or circles. Concurrently, linear optimal transport has been proposed to embed distributions into $L^2$ spaces, where the $L^2$ distance approximates the optimal transport distance, thereby simplifying comparisons across multiple distributions. In this work, we introduce the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which utilizes slicing to embed spherical distributions into $L^2$ spaces while preserving their intrinsic geometry, offering a computationally efficient metric for spherical probability measures. We establish the metricity of LSSOT and demonstrate its superior computational efficiency in applications such as cortical surface registration, 3D point cloud interpolation via gradient flow, and shape embedding. Our results demonstrate the significant computational benefits and high accuracy of LSSOT in these applications."
    },
    {
        "title": "\u2200uto\u2203\u2228\u2227L: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks",
        "link_suffix": "/forum?id=iv1TpRCJeK",
        "link": "https://openreview.net/forum?id=iv1TpRCJeK",
        "pdf_link": "https://openreview.net/pdf?id=iv1TpRCJeK",
        "keywords": "Large Language Models, Logical Reasoning, Autoformalization, Informalization, Formal Translation, Truth Maintenance",
        "abstract": "This paper presents $\\forall$uto$\\exists$$\\lor\\!\\land$L, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning.  $\\forall$uto$\\exists$$\\lor\\!\\land$L is the first benchmarking paradigm that offers several key advantages necessary for scaling objective evaluation of LLMs without human labeling: (a) ability to  evaluate LLMs of increasing sophistication by auto-generating tasks at different levels of difficulty; (b) auto-generation of ground truth that eliminates dependence on expensive and time consuming human annotation; (c) the use of automatically generated, randomized datasets that mitigate the ability of successive LLMs to overfit to static datasets used in many contemporary benchmarks. Empirical analysis shows that an LLM's performance on $\\forall$uto$\\exists$$\\lor\\!\\land$L is highly indicative of its performance on a diverse array of other benchmarks focusing on translation and reasoning tasks, making it a valuable autonomous evaluation paradigm in settings where hand-curated datasets can be hard to obtain and/or update."
    },
    {
        "title": "Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs",
        "link_suffix": "/forum?id=AgMpK7z4bz",
        "link": "https://openreview.net/forum?id=AgMpK7z4bz",
        "pdf_link": "https://openreview.net/pdf?id=AgMpK7z4bz",
        "keywords": "Reinforcement learning, action constraints",
        "abstract": "Action-constrained reinforcement learning (ACRL) is a generic framework for learning control policies with zero action constraint violation, which is required by various safety-critical and resource-constrained applications. The existing ACRL methods can typically achieve favorable constraint satisfaction but at the cost of either high computational burden incurred by the quadratic programs (QP) or increased architectural complexity due to the use of sophisticated generative models. In this paper, we propose a generic and computationally efficient framework that can adapt a standard unconstrained RL method to ACRL through two modifications: (i) To enforce the action constraints, we leverage the classic acceptance-rejection method, where we treat the unconstrained policy as the proposal distribution and derive a modified policy with feasible actions. (ii) To improve the acceptance rate of the proposal distribution, we construct an augmented two-objective Markov decision process (MDP), which include additional self-loop state transitions and a penalty signal for the rejected actions. This augmented MDP incentives the learned policy to stay close to the feasible action sets. Through extensive experiments in both robot control and resource allocation domains, we demonstrate that the proposed framework enjoys faster training progress, better constraint satisfaction, and a lower action inference time simultaneously than the state-of-the-art ACRL methods."
    },
    {
        "title": "Clustering and Entity Matching via Language Model Community Detection",
        "link_suffix": "/forum?id=NgMbGDCmAM",
        "link": "https://openreview.net/forum?id=NgMbGDCmAM",
        "pdf_link": "https://openreview.net/pdf?id=NgMbGDCmAM",
        "keywords": "language models, LLMs, embeddings, entity matching, entity resolution, clustering, community detection, knowledge graphs, vector databases",
        "abstract": "We introduce LMCD, a novel framework for semantic clustering and multi-set entity matching problems, in which we employ graph community detection algorithms to prune spurious edges from match graphs constructed using embedding and language models. We construct these match graphs by retrieving nearest embedding neighbors for each entity, then querying a language model to remove false positive pairs. Across a variety of cluster size distributions, and for tasks ranging from sentiment and topic categorization to deduplication of product databases, our approach outperforms existing methods without requiring any finetuning or labeled data beyond few-shot examples, and without needing to select the desired number of clusters in advance. Our embedding and inference stages are fully parallelizable, with query and computational costs which scale near-linearly in the number of entities. Our post-processing stage is bottlenecked only by the runtime of community detection algorithms on discrete graphs, which are often near-linear, with no explicit dependence on embedding dimension or numbers of clusters. This is in stark contrast to existing methods relying on high-dimensional clustering algorithms that are difficult to apply at scale; for entity matching our approach also ensures consistency constraints across matches regardless of group sizes, a desirable practical feature which is absent from all prior approaches other than vector clustering. Our improvements over previous techniques are most stark when clusters are numerous and heterogenously-sized, a regime which captures many clustering and matching problems of widespread practical importance."
    },
    {
        "title": "Understanding Complexity in VideoQA via Visual Program Generation",
        "link_suffix": "/forum?id=0yXqV8VJKi",
        "link": "https://openreview.net/forum?id=0yXqV8VJKi",
        "pdf_link": "https://openreview.net/pdf?id=0yXqV8VJKi",
        "keywords": "video understanding, codegen",
        "abstract": "We propose a data-driven approach to analyzing query complexity in Video Question Answering (VideoQA). Previous efforts in benchmark design have largely relied on human expertise to construct challenging samples. In this work, we experimentally demonstrate that humans struggle to accurately estimate which questions are hard to answer for machine learning models. \n    Our alternative, automated approach takes advantage of recent advances in code generation for visual question answering. In particular, we use generated code complexity as a proxy for the question complexity and demonstrate that it indeed shows a much stronger correlation with the models' performance, compared to human estimates. We then present a novel algorithm for estimating question complexity from code. It identifies fine-grained primitives which correlate with the hardest questions. These human-interpretable results lead to a number of discoveries about the key sources of complexity for VideoQA models. Finally, we extend our approach to generate complex questions for a given set of videos. This allows us to automatically construct a new benchmark, which is 1.9 times harder for VideoQA methods than existing manually designed datasets."
    },
    {
        "title": "Uncertainty Prioritized Experience Replay",
        "link_suffix": "/forum?id=aAxzDb0nlO",
        "link": "https://openreview.net/forum?id=aAxzDb0nlO",
        "pdf_link": "https://openreview.net/pdf?id=aAxzDb0nlO",
        "keywords": "reinforcement learning, replay, uncertainty",
        "abstract": "Prioritized experience replay, which improves sample efficiency by selecting relevant transitions to update parameter estimates, is a crucial component of contemporary deep reinforcement learning models. Typically, transitions are prioritized based on their temporal difference error. However, this approach is prone to favoring noisy transitions, even when the value estimation closely approximates the target mean. This phenomenon resembles thenoisy TVproblem postulated in the exploration literature, in which exploration-guided agents get stuck by mistaking noise for novelty. To mitigate the disruptive effects of noise in value estimation, we propose using epistemic uncertainty to guide the prioritization of transitions from the replay buffer. Epistemic uncertainty quantifies the uncertainty that can be reduced by learning, hence reducing transitions sampled from the buffer generated by unpredictable random processes. We first illustrate the benefits of epistemic uncertainty prioritized replay in two tabular toy models: a simple multi-arm bandit task, and a noisy gridworld. Subsequently, we evaluate our prioritization scheme on the Atari suite, outperforming quantile regression deep Q-learning benchmarks; thus forging a path for the use of epistemic uncertainty prioritized replay in reinforcement learning agents."
    },
    {
        "title": "A Q-learning approach to the Lowest Unique Positive Integer game",
        "link_suffix": "/forum?id=bdFzyzf4Qx",
        "link": "https://openreview.net/forum?id=bdFzyzf4Qx",
        "pdf_link": "https://openreview.net/pdf?id=bdFzyzf4Qx",
        "keywords": "Q-learning, Lowest Unique Positive Integer Game, Nash Equilibrium, Poisson-Nash equilibrium, real-time bidding, Swedish Limbo Lottery, multi-agent reinforcement learning, normal-form game, reverse auction, Poisson distribution, game theory.  TL;DR: This paper introduces a Q-learning-based approach to solve the Lowest Unique Positive Integer game, outperforming traditional Poisson-based methods and demonstrating real-world applications such as in reverse auctions and real-time bidding systems",
        "abstract": "The Lowest Unique Positive Integer (LUPI) game is a multiplayer game where participants attempt to choose the smallest number that no one else selects. While previous studies model LUPI using Poisson--Nash equilibrium assumptions, our work introduces a novel Q-learning-based approach to achieve equilibrium without the need for specific distribution assumptions, such as Poisson. We demonstrate that our Q-learning model successfully emulates the Nash equilibrium while allowing flexibility in the number of players, providing a more robust and practical solution for real-world applications like real-time bidding (RTB) systems. We compare our model's performance against existing Poisson-based strategies, showcasing improved accuracy and adaptability. Furthermore, we apply our model to the Swedish Limbo lottery data and observe significant deviations from theoretical predictions, highlighting the strength of learning-based approaches in dynamic, real-world scenarios."
    }
]