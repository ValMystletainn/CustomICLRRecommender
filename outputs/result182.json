[{"title": "Early Fusion Helps Vision Language Action Models Generalize Better", "link_suffix": "/forum?id=KBSHR4h8XV", "link": "https://openreview.net/forum?id=KBSHR4h8XV", "pdf_link": "https://openreview.net/pdf?id=KBSHR4h8XV", "keywords": "vision language action model; robot foundation model", "abstract": "Recent advances in Vision-Language-Action (VLA) models can enable robots to perform a wide range of tasks based on language or goal-based instructions. These VLA models typically encode text and images into disjoint tokens, generating actions that align with the given instructions. This requires the VLA models to simultaneously perform vision-language understanding and precise closed-loop control, resulting in significant challenges for them to generalize to new environments. However, contrastive pre-trained VLMs, such as CLIP, already possess vision-language alignment capabilities, which are underutilized by current VLA models. In this paper, we propose Early Fusion VLA (EF-VLA), a novel VLA architecture that exploits CLIP\u2019s vision-language understanding by performing early fusion, extracting fine-grained vision-language tokens relevant to the task instructions before passing them to the transformer policy. EF-VLA keeps the\nVLM frozen, allowing it to effectively perform unseen tasks without requiring fine-tuning, which often reduces generalization capabilities. Simulation and real-world experiments suggest that EF-VLA outperforms state-of-the-art VLA models on diverse tasks, with significant generalization capabilities in unseen environments.", "title_embedding_index": 9050, "title_abs_embedding_index": 9075}, {"title": "Physics-enhanced Neural Operator: An Application in Simulating Turbulent Transport", "link_suffix": "/forum?id=5LvTfc4fBz", "link": "https://openreview.net/forum?id=5LvTfc4fBz", "pdf_link": "https://openreview.net/pdf?id=5LvTfc4fBz", "keywords": "turbulent flow, neural operator, knowledge-guided machine learning, sequential simulation", "abstract": "The precise simulation of turbulent flows is of immense importance in a variety of scientific and engineering fields, including climate science, freshwater science, and the development of energy-efficient manufacturing processes. Within the realm of turbulent flow simulation, direct numerical simulation (DNS) is widely considered to be the most reliable approach, but it is prohibitively expensive for long-term simulation at fine spatial scales. Given the pressing need for efficient simulation, there is an increasing interest in building machine learning models for turbulence, either by reconstructing DNS from alternative low-fidelity simulations or by predicting DNS based on the patterns learned from historical data. However, standard machine learning techniques remain limited in capturing complex spatio-temporal characteristics of turbulent flows, resulting in limited performance and generalizability. This paper presents a novel physics-enhanced neural operator (PENO) that incorporates physical knowledge of partial differential equations (PDEs) to accurately model flow dynamics. The model is further refined by a self-augmentation mechanism to reduce the accumulated error in long-term simulations. The proposed method is evaluated through its performance on two distinct sets of 3D turbulent flow data, showcasing the model's capability to reconstruct high-resolution DNS data, maintain the inherent physical properties of flow transport, and generate flow simulations across various resolutions. Additionally, experimental results on multiple 2D vorticity flow series, generated by different PDEs, highlight the transferability and generalizability of the proposed method. This confirms its applicability to a wide range of real-world scenarios in which extensive simulations are needed under diverse settings.", "title_embedding_index": 9051, "title_abs_embedding_index": 9076}, {"title": "Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts", "link_suffix": "/forum?id=IDJUscOjM3", "link": "https://openreview.net/forum?id=IDJUscOjM3", "pdf_link": "https://openreview.net/pdf?id=IDJUscOjM3", "keywords": "Efficient Specialization of LLMs, Self-Improving, Mixture of Experts", "abstract": "We present Self-MoE, an approach that transforms a monolithic LLM into a compositional, modular system of self-specialized experts, named MiXSE (MiXture of Self-specialized Experts). Our approach leverages self-specialization, which constructs expert modules using self-generated synthetic data, each equipping a shared base LLM with distinct domain-specific capabilities, activated via self-optimized routing. This allows for dynamic and capability-specific handling of various target tasks, enhancing overall capabilities, without extensive human-labeled data and added parameters. Our empirical results reveal that specializing LLMs may exhibit potential trade-offs in performances on non-specialized tasks. On the other hand, our Self-MoE demonstrates substantial improvements (6.5%p on average) over the base LLM across diverse benchmarks such as knowledge, reasoning, math, and coding. It also consistently outperforms other methods, including instance merging and weight merging, while offering better flexibility and interpretability by design with semantic experts and routing. Our findings highlight the critical role of modularity, the applicability of Self-MoE to multiple base LLMs, and the potential of self-improvement in achieving efficient, scalable, and adaptable systems.", "title_embedding_index": 9052, "title_abs_embedding_index": 9077}, {"title": "AssembleFlow: Rigid Flow Matching with Inertial Frames for Molecular Assembly", "link_suffix": "/forum?id=jckKNzYYA6", "link": "https://openreview.net/forum?id=jckKNzYYA6", "pdf_link": "https://openreview.net/pdf?id=jckKNzYYA6", "keywords": "rigid flow matching, inertial frame, quaternion representation, material, molecular assembly, molecular crystallization", "abstract": "Molecular assembly, where a cluster of rigid molecules aggregated into strongly correlated forms, is fundamental to determining the properties of materials. However, traditional numerical methods for simulating this process are computationally expensive, and existing generative models on material generation overlook the rigidity inherent in molecular structures, leading to unwanted distortions and invalid internal structures in molecules. To address this, we introduce AssembleFlow. AssembleFlow leverages inertial frames to establish reference coordinate systems at the molecular level for tracking the orientation and motion of molecules within the cluster. It further decomposes molecular $\\text{SE}(3)$ transformations into translations in $\\mathbb{R}^3$ and rotations in $\\text{SO}(3)$, enabling explicit enforcement of both translational and rotational rigidity during each generation step within the flow matching framework. This decomposition also empowers distinct probability paths for each transformation group, effectively allowing for the separate learning of their velocity functions: the former, moving in Euclidean space, uses linear interpolation (LERP), while the latter, evolving in spherical space, employs spherical linear interpolation (SLERP) with a closed-form solution. Empirical validation on the benchmarking data COD-Cluster17 shows that AssembleFlow significantly outperforms six competitive deep learning baselines by at least 45% in assembly matching scores while maintaining 100% molecular integrity. Also, it matches the assembly performance of a widely used domain-specific simulation tool while reducing computational cost by 25-fold.", "title_embedding_index": 9053, "title_abs_embedding_index": 9078}, {"title": "Controlled Generation of Natural Adversarial Documents for Stealthy Retrieval Poisoning", "link_suffix": "/forum?id=0rmOx0Ifbf", "link": "https://openreview.net/forum?id=0rmOx0Ifbf", "pdf_link": "https://openreview.net/pdf?id=0rmOx0Ifbf", "keywords": "Dense Retrieval, Corpus Poisoning, Adversarial Attack", "abstract": "Recent work showed that retrieval based on embedding similarity (e.g., for retrieval-augmented generation) is vulnerable to poisoning: an adversary can craft malicious documents that are retrieved in response to broad classes of queries.  We demonstrate that previous, HotFlip-based techniques produce documents that are very easy to detect using perplexity filtering.  Even if generation is constrained to produce low-perplexity text, the resulting documents are recognized as unnatural by LLMs and can be automatically filtered from the retrieval corpus.\nWe design, implement, and evaluate a new controlled generation technique that combines an adversarial objective (embedding similarity) with a \"naturalness\" objective based on soft scores computed using an open-source, surrogate LLM.  The resulting adversarial documents (1) cannot be automatically detected using perplexity filtering and/or other LLMs, except at the cost of significant false positives in the retrieval corpus, yet (2) achieve similar poisoning efficacy to easily-detectable documents generated using HotFlip, and (3) are significantly more effective than prior methods for energy-guided generation, such as COLD.", "title_embedding_index": 9054, "title_abs_embedding_index": 9079}, {"title": "Language Models Implicitly Learn a Unified Representation Space", "link_suffix": "/forum?id=FrFQpAgnGE", "link": "https://openreview.net/forum?id=FrFQpAgnGE", "pdf_link": "https://openreview.net/pdf?id=FrFQpAgnGE", "keywords": "interpretability, representation, multilingual, multimodal", "abstract": "Modern language and multimodal models can process a wide variety of inputs across different languages and modalities. We hypothesize that models acquire this capability through learning aunified representation spaceacross heterogeneous data types. We first show that model representations for semantically equivalent inputs in different languages are similar in the intermediate layers, and that this space can further be interpreted using the model\u2019s dominant pretraining language (when it has one) via the logit lens. We also find that models show a similar tendency when processing other kinds of data, including code and visual/audio inputs. Interventions in the unified representation space further affect model outputs in expected ways: for example, replacing the image representations in a vision-language model with language token representations leads to output changes consistent with the language token semantics, suggesting that the unified representations space is not simply a byproduct of large-scale training on broad data, but something that is actively utilized by the model during input processing.", "title_embedding_index": 9055, "title_abs_embedding_index": 9080}, {"title": "Enhancing Cost Efficiency in Active Learning with Candidate Set Query", "link_suffix": "/forum?id=qmqRdxQcMA", "link": "https://openreview.net/forum?id=qmqRdxQcMA", "pdf_link": "https://openreview.net/pdf?id=qmqRdxQcMA", "keywords": "Active Learning, Conformal Prediction, Label efficient learning", "abstract": "This paper introduces a cost-efficient active learning (AL) framework for classification, featuring a novel query design called candidate set query. Unlike traditional AL queries requiring the oracle to examine all possible classes, our method narrows down the set of candidate classes likely to include the ground-truth class, significantly reducing the search space and labeling cost. Moreover, we leverage conformal prediction to dynamically generate small yet reliable candidate sets, adapting to model enhancement over successive AL rounds. To this end, we introduce an acquisition function designed to prioritize data points that offer high information gain at lower cost. Empirical evaluations on CIFAR-10, CIFAR-100, and ImageNet64x64 demonstrate the effectiveness and scalability of our framework. Notably, it reduces labeling cost by 42% on ImageNet64x64.", "title_embedding_index": 9056, "title_abs_embedding_index": 9081}, {"title": "PWM: Policy Learning with Multi-Task World Models", "link_suffix": "/forum?id=hOELrZfg0J", "link": "https://openreview.net/forum?id=hOELrZfg0J", "pdf_link": "https://openreview.net/pdf?id=hOELrZfg0J", "keywords": "reinforcement learning, model-based reinforcement learning, continuous control, world models", "abstract": "Reinforcement Learning (RL) has made significant strides in complex tasks but struggles in multi-task settings with different embodiments. World models methods offer scalability by learning a simulation of the environment, but often rely on inefficient gradient-free optimization methods for policy extraction. In contrast, gradient-based methods exhibit lower variance but fail to handle discontinuities. Our work reveals that well-regularized world models can generate smoother optimization landscapes than the actual dynamics, facilitating more effective first-order optimization. We introduce Policy learning with multi-task World Models (PWM), a novel model-based RL algorithm for continuous control. Initially, the world model is pre-trained on offline data, and then policies are extracted from it using first-order optimization in less than 10 minutes per task. PWM effectively solves tasks with up to 152 action dimensions and outperforms methods that use ground-truth dynamics. Additionally, PWM scales to an 80-task setting, achieving up to 27% higher rewards than existing baselines, without relying on costly online planning. Visualizations and code available athttps://policy-world-model.github.io/.", "title_embedding_index": 9057, "title_abs_embedding_index": 9082}, {"title": "Improved Diffusion-based Generative Model with Better Adversarial Robustness", "link_suffix": "/forum?id=1DVgysiIt7", "link": "https://openreview.net/forum?id=1DVgysiIt7", "pdf_link": "https://openreview.net/pdf?id=1DVgysiIt7", "keywords": "Generative Model; Adversarial Robustness; Diffusion Model; Distributional Robustness Optimization", "abstract": "Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation. However, its training and sampling processes are confronted with the problem of distribution mismatch. During the denoising process, the input data distributions of the model are different during the training and inference stages, which makes the model potentially generate inaccurate data. To obviate this, we conduct an analysis of the training objective of DPM, and theoretically prove that the mismatch can be mitigated by Distributionally Robust Optimization (DRO), which is equivalent to conducting robustness-driven Adversarial Training (AT) on the DPM. Furthermore, for the recently proposed consistency model (CM), which distills the inference process of the DPM, we prove that its training objective similarly faces the mismatch issue. Fortunately, such a problem is also mitigated by AT. Thereafter, we propose to conduct efficient AT on both DPM and CM. Finally, a series of empirical studies verify the effectiveness of AT in diffusion-based models.", "title_embedding_index": 9058, "title_abs_embedding_index": 9083}, {"title": "Self-Improving Robust Preference Optimization", "link_suffix": "/forum?id=ZSdubdbOoi", "link": "https://openreview.net/forum?id=ZSdubdbOoi", "pdf_link": "https://openreview.net/pdf?id=ZSdubdbOoi", "keywords": "Preference optimization, direct alignment, reinforcement learning from human feedback, Self-refinement", "abstract": "Both online and offline RLHF methods such as PPO and DPO have been extremely successful in aligning AI with human preferences. Despite their success, the existing methods suffer from some fundamental limitations: prominent among those limitations are\n (a) models trained with RLHF  can learn from mistakes  or negative examples through RL mechanism or contrastive loss at the time of training. However at the time of inference they are not equipped with an innate mechanism to correct mistakes by self-improvement. \n (b) The optimal solution of existing methods is highly task-dependent and thus it is difficult for them to generalize to new tasks.   Here we propose Self-Improving Robust Preference Optimization (SRPO), a practical and mathematically principled offline RLHF framework that address both these challenges. The key idea of SRPO is to cast the problem of learning from human preferences as a self-improvement process, which can be mathematically expressed in terms of a min-max objective that aims at joint optimization of self-improvement policy and the generative policy in an adversarial fashion. The solution for this optimization problem is independent of the training task and thus it is robust to its changes.\n We then show that this objective can be re-expressed in the form of a non-adversarial offline loss which can be optimized using standard supervised optimization techniques at scale without any need for reward model and online inference. \nWe show the effectiveness of SRPO in terms of  AI Win-Rate (WR) against human (GOLD) completions. In particular,  when \\srpo is evaluated on the XSUM dataset, it outperforms the celebrated DPO by a clear margin of $\\mathbf{15%}$ after $5$ self-revisions, achieving  WR of $\\mathbf{90}%$.", "title_embedding_index": 9059, "title_abs_embedding_index": 9084}, {"title": "Risk-Aware Distributional Intervention Policies for Language Models", "link_suffix": "/forum?id=tLPHgQMw08", "link": "https://openreview.net/forum?id=tLPHgQMw08", "pdf_link": "https://openreview.net/pdf?id=tLPHgQMw08", "keywords": "Language Models, Activations Steering, AI safety", "abstract": "Language models are prone to occasionally undesirable generations, such as harmful or toxic content, despite their impressive capability to produce texts that appear accurate and coherent. In this paper, we present a new two-stage approach to detect and mitigate undesirable content generations by rectifying activations. First, we train an ensemble of layer-wise classifiers to detect undesirable content using activations by minimizing a smooth surrogate of the risk-aware score. Then, for contents that are detected as undesirable, we propose layer-wise distributional intervention policies that perturb the attention heads minimally while guaranteeing probabilistically the effectiveness of the intervention. Benchmarks on several language models and datasets show that our method outperforms baselines in reducing the generation of undesirable output.", "title_embedding_index": 9060, "title_abs_embedding_index": 9085}, {"title": "LINA: An LLM-driven Neuro-Symbolic Approach for Faithful Logical Reasoning", "link_suffix": "/forum?id=3BoCwZFRJX", "link": "https://openreview.net/forum?id=3BoCwZFRJX", "pdf_link": "https://openreview.net/pdf?id=3BoCwZFRJX", "keywords": "Large Language Models, Logical Reasoning, Neuro-Symbolic Approach, Hypothetical-Deductive Reasoning", "abstract": "Large Language Models (LLMs) have exhibited remarkable potential across a wide array of reasoning tasks, including logical reasoning. Although massive efforts have been made to empower the logical reasoning ability of LLMs via external logical symbolic solvers, crucial challenges of the poor generalization ability to questions with different features and inevitable question information loss of symbolic solver-driven approaches remain unresolved. To mitigate these issues, we introduceLINA, a LLM-driven neuro-symbolic approach for faithful logical reasoning. By enabling an LLM to autonomously perform the transition from propositional logic extraction to sophisticated logical reasoning, LINA not only bolsters the resilience of the reasoning process but also eliminates the dependency on external solvers. Additionally, through its adoption of a hypothetical-deductive reasoning paradigm, LINA effectively circumvents the expansive search space challenge that plagues traditional forward reasoning methods. Empirical evaluations demonstrate that LINA substantially outperforms both established propositional logic frameworks and conventional prompting techniques across a spectrum of five logical reasoning tasks. Specifically, LINA achieves an improvement of 24.34% over LINC on the FOLIO dataset, while also surpassing prompting strategies like CoT and CoT-SC by up to 24.02%. Our code is available athttps://anonymous.4open.science/r/nshy-4148/.", "title_embedding_index": 9061, "title_abs_embedding_index": 9086}, {"title": "A graph-based global optimization framework for problems with nonconvex norm constraints and penalty functions", "link_suffix": "/forum?id=uZVDJfV2Ex", "link": "https://openreview.net/forum?id=uZVDJfV2Ex", "pdf_link": "https://openreview.net/pdf?id=uZVDJfV2Ex", "keywords": "Norm Constraints, Sparse Parameter Estimation, Nonconvex Regularization, Global Optimization, Mixed-Integer Nonlinear Programs, Decision Diagrams", "abstract": "Optimization problems with norm-bounding constraints appear in various applications, from portfolio optimization to machine learning, feature selection, and beyond. A widely used variant of these problems relaxes the norm-bounding constraint through Lagrangian relaxation and moves it to the objective function as a form of penalty or regularization term. A challenging class of these models uses the zero-norm function to induce sparsity in statistical parameter estimation models. Most existing exact solution methods for these problems use additional binary variables together with artificial bounds on variables to formulate them as a mixed-integer program in a higher dimension, which is then solved by off-the-shelf solvers. Other exact methods utilize specific structural properties of the objective function to solve certain variants of these problems, making them non-generalizable to other problems with different structures. An alternative approach employs nonconvex penalties with desirable statistical properties, which are solved using heuristic or local methods due to the structural complexity of those terms. In this paper, we develop a novel graph-based method to globally solve optimization problems that contain a generalization of norm-bounding constraints. This includes standard $\\ell_p$-norms for $p \\in [0, \\infty)$ as well as nonconvex penalty terms, such as SCAD and MCP, as special cases. Our method uses decision diagrams to build strong convex relaxations for these constraints in the original space of variables without the need to introduce additional auxiliary variables or impose artificial variable bounds. We show that the resulting convexification method, when incorporated into a spatial branch-and-cut framework, converges to the global optimal value of the problem under mild conditions. To demonstrate the capabilities of the proposed framework, we conduct preliminary computational experiments on benchmark sparse linear regression problems with complex nonconvex penalty terms that existing global solvers cannot model or solve. This establishes our framework as the first algorithm capable of globally solving such challenging mixed-integer nonlinear programs.", "title_embedding_index": 9062, "title_abs_embedding_index": 9087}, {"title": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing", "link_suffix": "/forum?id=PxlfzEePC0", "link": "https://openreview.net/forum?id=PxlfzEePC0", "pdf_link": "https://openreview.net/pdf?id=PxlfzEePC0", "keywords": "Diffusion transformer, text-to-image synthesis, Mixture-of-Experts", "abstract": "Diffusion transformers have been widely adopted for text-to-image synthesis. While scaling these models up to billions of parameters shows promise, the effectiveness of scaling beyond current sizes remains underexplored and challenging. By explicitly exploiting the computational heterogeneity of image generations, we develop a new family of Mixture-of-Experts (MoE) models (EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns to adaptively optimize the compute allocated to understand the input texts and generate the respective image patches, enabling heterogeneous computation aligned with varying text-image complexities. This heterogeneity provides an efficient way of scaling EC-DIT up to 97 billion parameters and achieving significant improvements in training convergence, text-to-image alignment, and overall generation quality over dense models and conventional MoE models. Through extensive ablations, we show that EC-DIT demonstrates superior scalability and adaptive compute allocation by recognizing varying textual importance through end-to-end training. Notably, in text-to-image alignment evaluation, our largest models achieve a state-of-the-art GenEval score of 71.68% and still maintain competitive inference speed with intuitive interpretability.", "title_embedding_index": 9063, "title_abs_embedding_index": 9088}, {"title": "Let\u2019s Stop Bleeding! Precise Bleeding Data Estimation & Visualization Methods for Laparoscopic Surgeries", "link_suffix": "/forum?id=XsXHqEVtiB", "link": "https://openreview.net/forum?id=XsXHqEVtiB", "pdf_link": "https://openreview.net/pdf?id=XsXHqEVtiB", "keywords": "Medical imaging, surgical image, GAN, image to image translation, segmentation", "abstract": "Intraoperative bleeding remains a significant challenge in modern surgery, necessitating rapid and accurate localization of bleeding sources to ensure effective hemostasis. Proactive detection and timely intervention are critical for minimizing blood loss, reducing operative time, preventing complications, and decreasing the need for intensive postoperative care. In this research, we introduce Selective Bleeding Alert Map (SBAM), a novel GAN-based framework designed for precise real-time detection of bleeding origins during surgery. Building upon our earlier BAM framework, SBAM shifts from broad, area-wide alerts to a focused approach that highlights only the exact bleeding areas, enhancing visual accuracy and potentially improving surgeon focus and visibility\u2014particularly beneficial in cases of minor bleeding where excessive alerts could interfere with the surgical process. To achieve this, we developed advanced image-to-image translation and segmentation models, custom thresholding techniques, and trajectory detection algorithms to pinpoint bleeding sources with high precision. Utilizing our developed mimic organ system for ethically sourced, realistic datasets\u2014alongside synthetic data generated from the orGAN system and Large Mask Inpainting (LaMa)\u2014we created a dedicated dataset specifically for SBAM training, including over 1,000 manually annotated images capturing both bleeding and non-bleeding regions within marked bleeding areas. Our instance segmentation model achieved a precision of 92.5%, an accuracy of 98% and a mask mean Average Precision of 85% at an IoU threshold of 0.5 (mAP@50). Additionally, the SBAM model demonstrated high accuracy in detecting bleeding points within real surgical videos from the Hamlyn dataset, underscoring its potential for practical surgical applications.Powered by core algorithms and uniquely developed datasets, SBAM represents a pivotal advancement in AI-assisted surgery, demonstrating superior performance in detecting bleeding regions with high precision during critical scenarios.", "title_embedding_index": 9064, "title_abs_embedding_index": 9089}, {"title": "R2-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning", "link_suffix": "/forum?id=CkgKSqZbuC", "link": "https://openreview.net/forum?id=CkgKSqZbuC", "pdf_link": "https://openreview.net/pdf?id=CkgKSqZbuC", "keywords": "LLM guardrail model, content moderation", "abstract": "As large language models (LLMs) become increasingly prevalent across various applications, it is critical to establish safety guardrails to moderate input/output content of LLMs and ensure compliance with safety policies. Existing guardrail models, such as OpenAI Mod and LlamaGuard, treat various safety categories (e.g., self-harm, self-harm/instructions) independently and fail to explicitly capture the intercorrelations among them. This has led to limitations such as ineffectiveness due to inadequate training on long-tail data from correlated safety categories, susceptibility to jailbreaking attacks, and inflexibility regarding new safety categories.\nTo address these limitations, we propose $R^2$-Guard, a robust reasoning enabled LLM guardrail via knowledge-enhanced logical reasoning. Specifically, $R^2$-Guard comprises two parts: data-driven guardrail models and reasoning components. The data-driven guardrail models provide unsafety probabilities of moderated content on different safety categories.\nWe then encode safety knowledge among different categories as first-order logical rules and embed them into a probabilistic graphic model (PGM) based reasoning component. The unsafety probabilities of different categories from data-driven guardrail models are sent to the reasoning component for final inference. We employ two types of PGMs: Markov logic networks (MLNs) and probabilistic circuits (PCs), and optimize PCs to achieve precision-efficiency balance via improved graph structure. We also propose different methods to optimize the weights of knowledge. To further perform stress tests for guardrail models, we employ a pairwise construction method to construct a new safety benchmark TwinSafety, which features principled categories and presents new challenges for moderation. We show that $R^2$-Guard is effective even given unrepresentative categories or challenging jailbreaking prompts. We demonstrate the effectiveness of $R^2$-Guard by comparisons with eight strong guardrail models on six standard moderation datasets, and demonstrate the robustness of $R^2$-Guard against four SOTA jailbreaking attacks. $R^2$-Guard significantly surpasses SOTA method LlamaGuard by 12.6% on standard moderation datasets and by 59.9% against jailbreaking attacks.\nWe further reveal that $R^2$-Guard can effectively adapt to safety category updates by simply editing the PGM reasoning graph.", "title_embedding_index": 9065, "title_abs_embedding_index": 9090}, {"title": "Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design", "link_suffix": "/forum?id=G328D1xt4W", "link": "https://openreview.net/forum?id=G328D1xt4W", "pdf_link": "https://openreview.net/pdf?id=G328D1xt4W", "keywords": "Discrete Diffusion Models, Reward Optimization, Fine-Tuning, AI for science, Reinforcement learning", "abstract": "Recent studies have demonstrated the strong empirical performance of diffusion models on discrete sequences (i.e., discrete diffusion models) across domains such as natural language and biological sequence generation. For example, in the protein inverse folding task, where the goal is to generate a protein sequence from a given backbone structure, conditional diffusion models have achieved impressive results in generating \"natural\" sequences that fold back into the original structure. However, practical design tasks often require not only modeling a conditional distribution but also optimizing specific task objectives. For instance, in the inverse folding task, we may prefer proteins with high stability. To address this, we consider the scenario where we have pre-trained discrete diffusion models that can generate \"natural\" sequences, as well as reward models that map sequences to task objectives. We then formulate the reward maximization problem within discrete diffusion models, analogous to reinforcement learning (RL), while minimizing the KL divergence against pre-trained diffusion models to preserve naturalness. To solve this RL problem, we propose a novel algorithm that enables direct backpropagation of rewards through entire trajectories generated by diffusion models, by making the originally non-differentiable trajectories differentiable using the Gumbel-Softmax trick. Our theoretical analysis indicates that our approach can generate sequences that are both \"natural\" (i.e., have a high probability under a pre-trained model) and yield high rewards. While similar tasks have been recently explored in diffusion models for continuous domains, our work addresses unique algorithmic and theoretical challenges specific to discrete diffusion models, which arise from their foundation in continuous-time Markov chains rather than Brownian motion. Finally, we demonstrate the effectiveness of our algorithm in generating DNA and protein sequences that optimize enhancer activity and protein stability, respectively.", "title_embedding_index": 9066, "title_abs_embedding_index": 9091}, {"title": "BAMDP Shaping: a Unified Theoretical Framework for Intrinsic Motivation and Reward Shaping", "link_suffix": "/forum?id=tijmpS9Vy2", "link": "https://openreview.net/forum?id=tijmpS9Vy2", "pdf_link": "https://openreview.net/pdf?id=tijmpS9Vy2", "keywords": "Reinforcement Learning Theory, Bayesian Reinforcement Learning, Intrinsic Motivation, Reward Shaping", "abstract": "Intrinsic motivation (IM) and reward shaping guide reinforcement learning (RL) agents by adding pseudo-rewards, which can lead to useful emergent behaviors. However, they can also exhibit unanticipated side effects -- leading to reward hacking or fixation with noisy TVs. Here we provide a theoretical model which anticipates these behaviors, and provides broad criteria under which unanticipated side effects can be bounded.\nWe characterize all pseudo-rewards as reward shaping in Bayes-Adaptive Markov Decision Processes (BAMDPs), which formulates the problem of learning in MDPs as a meta-level MDP over the agent's knowledge about the environment. \nWe can understand pseudo-rewards as guiding exploration by incentivizing RL agents to go to states with higher BAMDP value, which comprises the value of information gathered and the prior value of the physical state, while they mislead exploration when they align poorly with this value.\nWe extend potential-based shaping theory to prove that an RL algorithm which is approximately optimal for a shaped BAMDP is only guaranteed to remain so for the underlying problem when pseudo-rewards are BAMDP Potential-based shaping Functions (BAMPFs). \nWe also prove that for a BAMPF which settles---i.e., its potential eventually stops changing over time---no RL agent can reward-hack to find a policy maximizing shaped rewards without also maximizing real rewards. We show that it is straightforward to retrofit or design new pseudo-reward terms in this form to avoid unintended side effects.", "title_embedding_index": 9067, "title_abs_embedding_index": 9092}, {"title": "TUBench: Benchmarking Large Vision-Language Models on Trustworthiness with Unanswerable Questions", "link_suffix": "/forum?id=UHHOAe1uIS", "link": "https://openreview.net/forum?id=UHHOAe1uIS", "pdf_link": "https://openreview.net/pdf?id=UHHOAe1uIS", "keywords": "Vision-Language Models, Unanswerable Questions, Benchmark, Hallucination, Trustworthiness", "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable progress on visual perception and linguistic interpretation. Despite their impressive capabilities across various tasks, LVLMs still suffer from the issue of hallucination, which involves generating content that is incorrect or unfaithful to the visual or textual inputs. Traditional benchmarks, such as MME and POPE, evaluate hallucination in LVLMs within the scope of visual question answering (VQA) using answerable questions. However, some questions are unanswerable due to insufficient information in the images, and the performance of LVLMs on such unanswerable questions remains underexplored. To fill in this research blank, we propose TUBench, a benchmark specifically designed to evaluate the reliability of LVLMs using unanswerable questions. TUBench comprises an extensive collection of high-quality, unanswerable questions that are meticulously crafted using ten distinct strategies. To thoroughly evaluate LVLMs, the unanswerable questions in TUBench use images from four diverse domains as visual contexts: screenshots of code snippets, natural images, geometry diagrams, and screenshots of statistical tables. These unanswerable questions are tailored to test LVLMs' trustworthiness in code reasoning, commonsense reasoning, geometric reasoning, and mathematical reasoning related to tables, respectively. We conducted a comprehensive quantitative evaluation of 28 leading foundational models on TUBench, with Gemini-1.5-Pro, the top-performing model, achieving an average accuracy of 69.2%, and GPT-4o, the third-ranked model, reaching 66.7% average accuracy, in determining whether questions are answerable. Furthermore, our manual analysis of the model outputs reveals that: (1) Gemini-1.5-Pro provides both correct answers and explanations in only 41% of cases, and (2) hallucinations are the primary cause of error, accounting for 58.5% of the incorrect explanations generated by Gemini-1.5-Pro. These findings highlight that TUBench presents a significant challenge to current LVLMs, and offers a new perspective for evaluating hallucinations and trustworthiness through the lens of unanswerable questions.", "title_embedding_index": 9068, "title_abs_embedding_index": 9093}, {"title": "Initializing the Layer-wise Learning Rate", "link_suffix": "/forum?id=ATCanNIk1H", "link": "https://openreview.net/forum?id=ATCanNIk1H", "pdf_link": "https://openreview.net/pdf?id=ATCanNIk1H", "keywords": "learning rate, exploding gradient, vanishing gradient, initialization", "abstract": "Weight initialization schemes have been devised with heavy emphasis in the initial training dynamics, assuming the optimizer automatically handles appropriate step sizes in prolonged training. The optimizer typically calculates the step sizes using a single, global learning rate across all parameters, focusing exclusively on the (exponentially averaged) in-training time gradient. Motivated from hierarchical structure inherent in deep networks, this work explores assigning non-adaptive layer-wise learning rates based on the differences in gradient magnitude at initialization as a practical and effective optimization strategy. The gradient magnitude used to preset the layer-wise learning rates is measured at fan-in initialization, as stable activation variance is considered a desirable property during training, and so is assumed to largely hold true in prolonged training. Experiments on convolutional and transformer architectures show the proposed layer-wise learning rate can improve training stability and convergence in image classification and autoregressive language modeling", "title_embedding_index": 9069, "title_abs_embedding_index": 9094}, {"title": "A Brain-Inspired Regularizer for Adversarial Robustness", "link_suffix": "/forum?id=bBUhlynfRX", "link": "https://openreview.net/forum?id=bBUhlynfRX", "pdf_link": "https://openreview.net/pdf?id=bBUhlynfRX", "keywords": "Neuroscience, Machine Learning, CNN, Adversarial Attacks, Image Classification, Brain-Inspired", "abstract": "Convolutional Neural Networks (CNNs) excel in many visual tasks, but they tend to be sensitive to slight input perturbations that are imperceptible to the human eye, often resulting in task failures. Recent studies indicate that training CNNs with regularizers that promote brain-like representations, using neural recordings, can improve model robustness. However, the requirement to use neural data severely restricts the utility of these methods. Is it possible to develop regularizers that mimic the computational function of neural regularizers without the need for neural recordings, thereby expanding the usability and effectiveness of these techniques? In this work, we inspect a neural regularizer introduced in Li et al. to extract its underlying strength. The regularizer uses neural representational similarities, which we find also correlate with pixel similarities. Motivated by this finding, we introduce a new regularizer that retains the essence of the original but is computed using image pixel similarities, eliminating the need for neural recordings.  We show that our regularization method 1) significantly increases model robustness against a variety of black box attacks, 2) relies only on original, unaugmented datasets and 3) is computationally inexpensive. Our work explores how biologically motivated loss functions can be used to drive the performance of artificial neural networks.", "title_embedding_index": 9070, "title_abs_embedding_index": 9095}, {"title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents", "link_suffix": "/forum?id=sGfVBi15uY", "link": "https://openreview.net/forum?id=sGfVBi15uY", "pdf_link": "https://openreview.net/pdf?id=sGfVBi15uY", "keywords": "Large Language Models (LLMs), In-Context Decision-Making, Dueling Bandits", "abstract": "In-context decision-making is an important capability of artificial general intelligence, which Large Language Models (LLMs) have effectively demonstrated in various scenarios. However, LLMs often face challenges when dealing with numerical contexts, and limited attention has been paid to evaluating their performance through preference feedback generated by the environment. This paper is the first to investigate the performance of LLMs as decision-makers in the context of Dueling Bandits (DB). We compare GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-preview against eight well-established DB algorithms. Our results reveal that LLMs, particularly GPT-4 Turbo, quickly identify the Condorcet winner, thus outperforming existing state-of-the-art algorithms in terms of weak regret. Nevertheless, LLMs struggle to converge even when explicitly prompted to do so and are sensitive to prompt variations. To overcome these issues, we introduce a hybrid algorithm: LLM-Enhanced Adaptive Dueling (LEAD), which takes advantage of both in-context decision-making capabilities of LLMs and theoretical guarantees inherited from classic DB algorithms. We show that LEAD has theoretical guarantees on both weak and strong regret and validate its robustness even with noisy and adversarial prompts. The design of such an algorithm sheds light on how to enhance trustworthiness for LLMs used in decision-making tasks where performance robustness matters.", "title_embedding_index": 9071, "title_abs_embedding_index": 9096}, {"title": "Kernel-based Optimally Weighted Conformal Time-Series Prediction", "link_suffix": "/forum?id=oP7arLOWix", "link": "https://openreview.net/forum?id=oP7arLOWix", "pdf_link": "https://openreview.net/pdf?id=oP7arLOWix", "keywords": "Conformal prediction, Nonparametric kernel regression, Time series", "abstract": "Conformal prediction has been a popular distribution-free framework for uncertainty quantification. In this work, we present a novel conformal prediction method for time-series, which we call Kernel-based Optimally Weighted Conformal Prediction Intervals ($\\texttt{KOWCPI}$). Specifically, $\\texttt{KOWCPI}$ adapts the classic Reweighted Nadaraya-Watson (RNW) estimator for quantile regression on dependent data and learns optimal data-adaptive weights. Theoretically, we tackle the challenge of establishing a conditional coverage guarantee for non-exchangeable data under strong mixing conditions on the non-conformity scores. We demonstrate the superior performance of $\\texttt{KOWCPI}$ on real time-series against state-of-the-art methods, where $\\texttt{KOWCPI}$ achieves narrower confidence intervals without losing coverage.", "title_embedding_index": 9072, "title_abs_embedding_index": 9097}, {"title": "Solving hyperbolic conservation laws with characteristic based neural network", "link_suffix": "/forum?id=HDmmwwTIlf", "link": "https://openreview.net/forum?id=HDmmwwTIlf", "pdf_link": "https://openreview.net/pdf?id=HDmmwwTIlf", "keywords": "neural network, hyperbolic conservation laws, characteristics", "abstract": "Neural network PDE solvers have recently gained popularity.However, it faces difficulty to deal with sharp discontinuity like shock waves in hyperbolic conservation laws.In this paper we propose a characteristic-based neural network to solve one dimension hyperbolic laws.The smooth solution can be derived by equation of characteristic lines ,and shock waves are decided by simple ODE solver.This method achieves a high accuracy with high efficiency. In the future it is hopeful to apply this method to higher dimension problems.", "title_embedding_index": 9073, "title_abs_embedding_index": 9098}, {"title": "Federated Learning Nodes Can Reconstruct Peers' Image Data", "link_suffix": "/forum?id=5dttvRONu0", "link": "https://openreview.net/forum?id=5dttvRONu0", "pdf_link": "https://openreview.net/pdf?id=5dttvRONu0", "keywords": "Federated Learning, Data Reconstruction Attacks", "abstract": "Federated learning (FL) is a privacy-preserving machine learning framework that enables multiple nodes to train models on their local data and periodically average weight updates to benefit from other nodes' training. Each node's goal is to collaborate with other nodes to improve the model's performance while keeping its training data private. However, this framework does not guarantee data privacy. Prior work has shown that the gradient-sharing steps in FL can be vulnerable to data reconstruction attacks from a honest-but-curious central server. In this work, we show that a honest-but-curious node/client can also launch attacks to reconstruct peers' image data in a centralized system, presenting a severe privacy risk. We demonstrate that a single client can silently reconstruct other clients' private images using diluted information available within consecutive updates. We leverage state-of-the-art diffusion models to enhance the perceptual quality and recognizability of the reconstructed images, further demonstrating the risk of information leakage at a semantic level. This highlights the need for more robust privacy-preserving mechanisms that protect against silent client-side attacks during federated training. \nThe source code will be available as a link on the discussion forum once it is open.", "title_embedding_index": 9074, "title_abs_embedding_index": 9099}]