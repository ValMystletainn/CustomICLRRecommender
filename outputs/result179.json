[{"title": "Decentralized primal-dual actor-critic with entropy regularization for safe multi-agent reinforcement learning", "link_suffix": "/forum?id=luS9zeDpeO", "link": "https://openreview.net/forum?id=luS9zeDpeO", "pdf_link": "https://openreview.net/pdf?id=luS9zeDpeO", "keywords": "decentralized multi-agent reinforcement learning, safe multi-agent reinforcement learning, entropy regularization, deep reinforcement learning", "abstract": "We investigate the decentralized safe multi-agent reinforcement learning (MARL) problem based on homogeneous multi-agent systems, where agents aim to maximize the team-average return and the joint policy's entropy, while satisfying safety constraints associated to the cumulative team-average cost. A mathematical model referred to as a homogeneous constrained Markov game is formally characterized, based on which policy sharing provably preserves the optimality of our safe MARL problem. An on-policy decentralized primal-dual actor-critic algorithm is then proposed, where agents utilize both local gradient updates and consensus updates to learn local policies, without the requirement for a centralized trainer. Asymptotic convergence is proven using multi-timescale stochastic approximation theory under standard assumptions. Thereafter, a practical off-policy version of the proposed algorithm is developed based on the deep reinforcement learning training architecture. The effectiveness of our practical algorithm is demonstrated through comparisons with solid baselines on three safety-aware multi-robot coordination tasks in continuous action spaces.", "title_embedding_index": 8900, "title_abs_embedding_index": 8925}, {"title": "LLM4DV: Using Large Language Models for Hardware Test Stimuli Generation", "link_suffix": "/forum?id=Srfi0a7vB3", "link": "https://openreview.net/forum?id=Srfi0a7vB3", "pdf_link": "https://openreview.net/pdf?id=Srfi0a7vB3", "keywords": "Large Language Models, Automated Chip Design, Design Verification", "abstract": "Hardware design verification (DV) is a process that checks the functional equivalence of a hardware design against its specifications, improving hardware reliability and robustness. A key task in the DV process is the test stimuli generation, which creates a set of conditions or inputs for testing. A major challenge is that existing approaches to test stimuli generation require human effort due to the complexity and specificity of the test conditions required for an arbitrary hardware design. We seek an efficient and automated solution that takes advantage of large language models (LLMs). LLMs have shown promising results for improving hardware design automation, but remain under-explored for hardware DV. In this paper, we propose an open-source benchmarking framework called LLM4DV that efficiently orchestrates LLMs for automated hardware test stimuli generation. Our analysis evaluates five LLMs using six prompting improvements over eight hardware designs and provides insight for future work on LLMs for efficient DV.", "title_embedding_index": 8901, "title_abs_embedding_index": 8926}, {"title": "MVFL: Multivariate Vertical Federated Learning for Time-Series Forecasting", "link_suffix": "/forum?id=eP5ICc0584", "link": "https://openreview.net/forum?id=eP5ICc0584", "pdf_link": "https://openreview.net/pdf?id=eP5ICc0584", "keywords": "vertical federated learning, multivariate time series forecasting, resource-limited devices", "abstract": "Extending multivariate time series forecasting to resource-limited devices is a critical demand for real applications, especially with the advancements in IoT technologies. A common scenario is where the variates are distributed vertically on different devices and each device needs to do local forecasting. This paper studies a resource-efficient solution for this scenario based on vertical federated learning (VFL). Prior VFL frameworks are designed for situations where only one party holds the labels and would struggle to meet the demand of the targeted scenario, as storage resources usage would increase dramatically with the number of devices. Going beyond VFL, we design multivariate vertical federated learning (MVFL) as a novel federated learning framework, where we separate communication features and local features in an embedded feature space. This design enables MVFL to utilize storage and communication resources more efficiently by eliminating the redundant models. MVFL outperforms VFL approaches in both efficiency and accuracy. On four real-world benchmarks, compared to VFL, when the storage resources are equally utilized, MVFL yields a 14% relative improvement on loss with a 43% relative improvement on communication resources usage. Even when both MVFL and VFL employ the same main model size, MVFL achieves a 75% reduction in storage resources compared to VFL, albeit with a slight compromise in terms of loss.", "title_embedding_index": 8902, "title_abs_embedding_index": 8927}, {"title": "Improving the Language Understanding Capabilities of Large Language Models Using Reinforcement Learning", "link_suffix": "/forum?id=ZK1NnjpjEs", "link": "https://openreview.net/forum?id=ZK1NnjpjEs", "pdf_link": "https://openreview.net/pdf?id=ZK1NnjpjEs", "keywords": "Large Language Models, Natural Language Understanding, Proximal Policy Optimization, Fine-tuning, GLUE, SuperGLUE", "abstract": "Large language models (LLMs), primarily built on decoder-only transformer architectures, excel in natural language generation tasks and have shown promise in adapting to diverse downstream tasks using zero-shot and few-shot prompting techniques. However, these prompting methods often fall short on natural language understanding (NLU) tasks, where smaller encoder-only models like BERT-base consistently outperform LLMs on benchmarks such as GLUE and SuperGLUE. In this paper, we explore two approaches\u2014supervised fine-tuning and proximal policy optimization (PPO)\u2014to enhance the NLU capabilities of LLMs. To reduce the computational cost of full-model fine-tuning, we integrate low-rank adaptation (LoRA) layers, restricting updates to these layers during both supervised fine-tuning and PPO stages. In the supervised fine-tuning approach, task-specific prompts are concatenated with input queries and ground-truth labels from the NLU training corpus, optimizing the model using the next-token prediction objective. Despite this, LLMs still underperform compared to encoder-only models like BERT-base on several NLU tasks. To address this gap, we employ PPO, a reinforcement learning technique that treats each token generation as an action and evaluates the sequence of generated tokens using a reward function based on their alignment with ground-truth answers. PPO then updates the model to maximize these rewards, effectively aligning its outputs with the correct labels. Our experiments with the LLAMA2-7B model demonstrate that PPO-based fine-tuning significantly improves performance, delivering an average gain of 6.3 points over supervised fine-tuning on the GLUE benchmark. PPO surpasses zero-shot prompting by 38.7 points and few-shot prompting by 26.1 points on GLUE, while also outperforming these baselines by 28.8 and 28.5 points on SuperGLUE. Additionally, PPO exceeds the performance of BERT-large, a strong baseline, with an average improvement of 2.7 points on GLUE and 9.3 points on SuperGLUE. These improvements are consistent across models such as Qwen2.5-7B and MPT-7B, highlighting PPO\u2019s robustness and effectiveness in enhancing the NLU capabilities of LLMs.", "title_embedding_index": 8903, "title_abs_embedding_index": 8928}, {"title": "ReNovo: Retrieval-Based \\emph{De Novo} Mass Spectrometry Peptide Sequencing", "link_suffix": "/forum?id=uQnvYP7yX9", "link": "https://openreview.net/forum?id=uQnvYP7yX9", "pdf_link": "https://openreview.net/pdf?id=uQnvYP7yX9", "keywords": "Peptide Sequencing", "abstract": "Proteomics is the large-scale study of proteins. Tandem mass spectrometry, as the only high-throughput technique for protein sequence identification, plays a pivotal role in proteomics research. One of the long-standing challenges in this field is peptide identification, which entails determining the specific peptide (sequence of amino acids) that corresponds to each observed mass spectrum. The conventional approach involves database searching, wherein the observed mass spectrum is scored against a pre-constructed peptide database. However, the reliance on pre-existing databases limits applicability in scenarios where the peptide is absent from existing databases. Such circumstances necessitate \\emph{de novo} peptide sequencing, which derives peptide sequence solely from input mass spectrum, independent of any peptide database. Despite ongoing advancements in \\emph{de novo} peptide sequencing, its performance still has considerable room for improvement, which limits its application in large-scale experiments. In this study, we introduce a novel Retrieval-based \\emph{De Novo} peptide sequencing methodology, termed ReNovo, which draws inspiration from database search methods. Specifically, by constructing a datastore from training data, ReNovo can retrieve information from the datastore during the inference stage to conduct retrieval-based inference, thereby achieving improved performance. This innovative approach enables ReNovo to effectively combine the strengths of both methods: utilizing the assistance of the datastore while also being capable of predicting novel peptides that are not present in pre-existing databases. A series of experiments have confirmed that ReNovo outperforms state-of-the-art models across multiple widely-used datasets, incurring only minor storage and time consumption, representing a significant advancement in proteomics. Supplementary materials include the code.", "title_embedding_index": 8904, "title_abs_embedding_index": 8929}, {"title": "Multiway Multislice PHATE: Visualizing Hidden Dynamics of RNNs through Training", "link_suffix": "/forum?id=Y4GCrfAidr", "link": "https://openreview.net/forum?id=Y4GCrfAidr", "pdf_link": "https://openreview.net/pdf?id=Y4GCrfAidr", "keywords": "RNNs, Dimensionality Reduction, Hidden State, Visualization, Hidden Dynamics, Deep Learning", "abstract": "Recurrent neural networks (RNNs) are a widely used tool for sequential data analysis, however, they are still often seen as black boxes of computation. Understanding the functional principles of these networks is key to developing ideal model architectures and optimization strategies. Previous studies often only emphasize the networks' representation post-training, overlooking their evolution process. Here, we present Multiway Multislice PHATE (MM-PHATE), a novel method for visualizing the evolution of RNNs' hidden states. MM-PHATE is a graph-based embedding using structured kernels across the multiple dimensions spanned by RNNs: time, training epoch, and units. We demonstrate on multiple datasets that MM-PHATE uniquely preserves hidden representation community structure among units and identifies information processing and compression phases during training. The embedding allows users to look under the hood of RNNs across training and provides an intuitive and comprehensive strategy to understanding the network's internal dynamics and draw conclusions, e.g., on why and how one model outperforms another or how a specific architecture might impact an RNN's learning ability.", "title_embedding_index": 8905, "title_abs_embedding_index": 8930}, {"title": "Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive Extension", "link_suffix": "/forum?id=jqmptcSNVG", "link": "https://openreview.net/forum?id=jqmptcSNVG", "pdf_link": "https://openreview.net/pdf?id=jqmptcSNVG", "keywords": "AI for Science; Protein Design; Generative Models;", "abstract": "Peptides, short chains of amino acids, interact with target proteins, making them a unique class of protein-based therapeutics for treating human diseases. Recently, deep generative models have shown great promise in peptide generation. However, several challenges remain in designing effective peptide binders. First, not all residues contribute equally to peptide-target interactions. Second, the generated peptides must adopt correct geometries due to the constraints of peptide bonds. Third, realistic benchmarks for peptide drug development are still lacking.\nTo address these challenges, we introducePepHAR, a hot-spot-driven autoregressive generative model for designing peptides targeting specific proteins. Building on the observation that certain hot spot residues have higher interaction potentials, we first use an energy-based density model to fit and sample these key residues. Next, to ensure proper peptide geometry, we autoregressively extend peptide fragments by estimating dihedral angles between residue frames. Finally, we apply an optimization process to iteratively refine fragment assembly, ensuring valid peptide structures.\nBy combining hot spot sampling with fragment-based extension, our approach enables de novo peptide design tailored to a target protein and allows the incorporation of key hot spot residues into peptide scaffolds. Extensive experiments, including peptide design and peptide scaffold generation, demonstrate the strong potential ofPepHARin computational peptide design.", "title_embedding_index": 8906, "title_abs_embedding_index": 8931}, {"title": "Transformers are Efficient Compilers, Provably", "link_suffix": "/forum?id=sprjE7BTZR", "link": "https://openreview.net/forum?id=sprjE7BTZR", "pdf_link": "https://openreview.net/pdf?id=sprjE7BTZR", "keywords": "Transformers, Expressive Power, Programming Language, Attention Mechanism, Compiler", "abstract": "Transformer-based large language models (LLMs) have demonstrated surprisingly robust performance across a wide range of language-related tasks, including programming language understanding and generation. In this paper, we take the first steps towards a formal investigation of using transformers as compilers from an expressive power perspective. To this end, we introduce a representative programming language,mini-husky, which encapsulates key features of modern C-like languages. We show that if the input code sequence has a bounded depth in both the Abstract Syntax Tree (AST) and type inference (reasonable assumptions based on the clean code principle), then the number of parameters required by transformers depends only on the logarithm of the input sequence length to handle compilation tasks, such as AST construction, symbol resolution, and type analysis. A significant technical challenge stems from the fact that transformers operate at a low level, where each layer processes the input sequence as raw vectors without explicitly associating them with predefined structure or meaning. In contrast, high-level compiler tasks necessitate managing intricate relationships and structured program information. Our primary technical contribution is the development of a domain-specific language,Cybertron, which generates formal proofs of the transformer\u2019s expressive power, scaling to address compiler tasks. We further establish that recurrent neural networks (RNNs) require at least a linear number of parameters relative to the input sequence, leading to an exponential separation between transformers and RNNs. Finally, we empirically validate our theoretical results by comparing transformers and RNNs on compiler tasks withinmini-husky.", "title_embedding_index": 8907, "title_abs_embedding_index": 8932}, {"title": "Learning How Hard to Think: Input-Adaptive Allocation of LM Computation", "link_suffix": "/forum?id=6qUUgw9bAZ", "link": "https://openreview.net/forum?id=6qUUgw9bAZ", "pdf_link": "https://openreview.net/pdf?id=6qUUgw9bAZ", "keywords": "LLM, inference, scaling, test-time compute", "abstract": "Computationally intensive decoding procedures---including search, reranking, and self-critique---can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog.\nExisting work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-$k$ procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50% at no cost to quality.", "title_embedding_index": 8908, "title_abs_embedding_index": 8933}, {"title": "PDE-constrained Learning with Multi-time-stepping for Accelerated Fluid Simulation", "link_suffix": "/forum?id=stcN89QGfL", "link": "https://openreview.net/forum?id=stcN89QGfL", "pdf_link": "https://openreview.net/pdf?id=stcN89QGfL", "keywords": "physics-informed learning, multiscale time stepping, spatiotemporal dynamics prediction", "abstract": "Solving partial differential equations (PDEs) by numerical methods meet computational cost challenge for getting the accurate solution since fine grids and small timesteps are required. Machine learning methods can accelerate this process, but struggle with weak generalizability, interpretability, and data dependency, as well as suffer in long-term prediction. To tackle these challenges, we propose a PDE-constrained network with multiscale time stepping (MultiPDENet), which fuses the scheme of numerical methods and machine learning, for accelerated simulation of fluid flows. In particular, we design a convolutional filter based on the structure of finite difference stencils with a small number of parameters to optimize, enabling accurate estimation of spatial derivatives on coarse grids. A physics block with a 4th-order Runge-Kutta integrator at the fine time scale is established that incorporates the structure of PDEs to guide the prediction. To alleviate the curse of temporal error accumulation in long-term prediction, we introduce a multiscale time integration approach, where a neural network is used to correct the prediction error at a coarse time scale. Experiments across various PDE systems, including the Navier-Stokes equations, demonstrate that MultiPDENet can accurately predict long-term spatiotemporal dynamics, even given small and incomplete training data, e.g., spatiotemporally down-sampled datasets. MultiPDENet achieves the state-of-the-art performance compared with other baseline models, with up to 53$\\times$ speedup compared to classical numerical methods.", "title_embedding_index": 8909, "title_abs_embedding_index": 8934}, {"title": "Stable batched bandit:  Optimal regret with free inference", "link_suffix": "/forum?id=t5kThOYtxn", "link": "https://openreview.net/forum?id=t5kThOYtxn", "pdf_link": "https://openreview.net/pdf?id=t5kThOYtxn", "keywords": "Batched Bandit, Inference in Bandits", "abstract": "In this paper, we discuss statistical inference when using a sequential strategy to collect data. While inferential tasks become challenging with sequentially collected data, we argue that this problem can be alleviated when the sequential algorithm satisfies certain stability properties; we call such algorithms stable bandit algorithms. Focusing on batched bandit problems, we first demonstrate that popular algorithms including the greedy-UCB algorithm and $\\epsilon$-greedy ETC algorithms are not stable, complicating downstream inferential tasks. Our main result shows that a form of elimination algorithm is stable in the batched bandit setup, and we characterize the asymptotic distribution of the sample means. This result allows us to construct asymptotically exact confidence intervals for arm-means which are sharper than existing concentration-based bounds. As a byproduct of our main results, we propose an Explore and Commit (ETC) strategy, which is stable --- thus allowing easy statistical inference--- and also attains optimal regret up to a factor of 4.Our work connects two historically conflicting paradigms in sequential learning environments: regret minimization and statistical inference. Ultimately, we demonstrate that it is possible to minimize regret without sacrificing the ease of performing statistical inference, bridging the gap between these two important aspects of sequential decision-making.", "title_embedding_index": 8910, "title_abs_embedding_index": 8935}, {"title": "DynST: Large-Scale Spatial-Temporal Dataset for Transferable Traffic Forecasting with Dynamic Road Networks", "link_suffix": "/forum?id=vXSCD3ToCS", "link": "https://openreview.net/forum?id=vXSCD3ToCS", "pdf_link": "https://openreview.net/pdf?id=vXSCD3ToCS", "keywords": "Traffic Forecasting; Transfer Learning; Spatial-Temporal Data Mining; Dataset;", "abstract": "In real-world traffic networks, it is common to encounter a shortage of historical data in the target region. Researchers often address this issue through transfer learning. However, transfer learning tasks in traffic prediction currently lack dedicated datasets and instead rely on datasets designed for non-transfer prediction tasks. The major drawback of these existing datasets is the adoption of a fixed network topology to model the real world's road networks. This does not align with reality and limits the model's transferability. To tackle this issue, we propose DynST, a dataset specifically designed for transfer learning tasks in traffic prediction, with a massive data volume of 20.35 billion, spanning 20 years and 9 regions. The key feature of DynST is evolving dynamic road network topology, which reflects the evolution of real road networks. Moreover, to address the shortcomings of the distance-based adjacency generation algorithm, we introduce a novel tree-based algorithm. Extensive experiments demonstrate that the adoption of DynST as the source dataset can significantly enhance the performance of the target region. The comparative experiment also validates that our adjacency matrix generation algorithm can lead to improved prediction accuracy. We believe that DynST, with rich spatial variation information, will facilitate research in the field of transfer traffic prediction.", "title_embedding_index": 8911, "title_abs_embedding_index": 8936}, {"title": "Examining Alignment of Large Language Models through Representative Heuristics: the case of political stereotypes", "link_suffix": "/forum?id=7LGmXXZXtP", "link": "https://openreview.net/forum?id=7LGmXXZXtP", "pdf_link": "https://openreview.net/pdf?id=7LGmXXZXtP", "keywords": "safety of LLMs, political stereotypes, representative heuristics, cognitive bias", "abstract": "Examining the alignment of large language models (LLMs) has become increasingly important, particularly when these systems fail to operate as intended. This study explores the challenge of aligning LLMs with human intentions and values, with specific focus on their political inclinations. Previous research has highlighted LLMs' propensity to display political leanings, and their ability to mimic certain political parties' stances on various issues. However, the $\\textit{extent}$ and $\\textit{conditions}$ under which LLMs deviate from empirical positions have not been thoroughly examined. To address this gap, our study systematically investigates the factors contributing to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them.Drawing on cognitive science findings related to representativeness heuristics -where individuals readily recall the representative attribute of a target group in a way that leads to exaggerated beliefs- we scrutinize LLM responses through this heuristics lens. We conduct experiments to determine how LLMs exhibit stereotypes by inflating judgments in favor of specific political parties. Our results indicate that while LLMs can $\\textit{mimic}$ certain political parties' positions, they often $\\textit{exaggerate}$ these positions more than human respondents do. Notably, LLMs tend to overemphasize representativeness to a greater extent than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggeseting potential vulnerabilities to political stereotypes. We propose prompt-based mitigation strategies that demonstrate effectiveness in reducing the influence of representativeness in LLM responses.", "title_embedding_index": 8912, "title_abs_embedding_index": 8937}, {"title": "Learning to Rewrite: Generalized Detection of LLM-Generated Text", "link_suffix": "/forum?id=wojnTvBXqt", "link": "https://openreview.net/forum?id=wojnTvBXqt", "pdf_link": "https://openreview.net/pdf?id=wojnTvBXqt", "keywords": "LLM-generated text detection, AIGC detection", "abstract": "Large language models (LLMs) present significant risks when used to generate non-factual content and spread disinformation at scale. Detecting such LLM-generated content is crucial, yet current detectors often struggle to generalize in open-world contexts. We introduceLearning2Rewrite, a novel framework for detecting AI-generated text with exceptional generalization to unseen domains. Our method leverages the insight that LLMs inherently modify AI-generated content less than human-written text when tasked with rewriting. By training LLMs to minimize alterations on AI-generated inputs, we amplify this disparity, yielding a more distinguishable and generalizable edit distance across diverse text distributions. Extensive experiments on data from 21 independent domains and four major LLMs (GPT-3.5, GPT-4, Gemini, and Llama-3) demonstrate that our detector outperforms state-of-the-art detection methods by up to 23.04% in AUROC for in-distribution tests, 37.26% for out-of-distribution tests, and 48.66% under adversarial attacks. Our unique training objective ensures better generalizability compared to directly training for classification, when leveraging the same amount of learned parameters. Our findings suggest that reinforcing LLMs\u2019 inherent rewriting tendencies offers a robust and scalable solution for detecting AI-generated text.", "title_embedding_index": 8913, "title_abs_embedding_index": 8938}, {"title": "Learning-Augmented Learning of Gaussian Mixture Models", "link_suffix": "/forum?id=GeyZGQ8SSY", "link": "https://openreview.net/forum?id=GeyZGQ8SSY", "pdf_link": "https://openreview.net/pdf?id=GeyZGQ8SSY", "keywords": "learning-augmented algorithms, Gaussian mixture models", "abstract": "Gaussian mixture models (GMMs) is one of the most fundamental methods to identify and extract latent structure in complex datasets. Unfortunately, well-known hardness results require that any algorithm for learning a mixture of $k$ multivariate Gaussian distributions in $d$-dimensional space requires both runtime and sample complexity exponential in $d$, even if the Gaussians are reasonably separated. To overcome this barrier, we consider settings where algorithms are augmented with possibly erroneous ``advice'' to help learn the underlying GMMs. In particular, we consider a natural predictor that can be easily trained through machine learning models. Specifically, our predictor outputs a list of $\\beta$ possible labels for each sample from the mixture such that, with probability at least $1-\\alpha$, one of the labels in the list is the true label, for a fixed constant $\\alpha$. We show that to estimate the mixture up to total variation distance $\\tilde{\\mathcal{O}}(\\varepsilon)$, we can use $k\\cdot\\text{poly}\\left(d,\\log k,\\frac{1}{\\varepsilon}\\right)$ samples from the GMM, provided that $\\beta$ is upper bounded by any fixed constant. Moreover, our algorithm uses polynomial time, thus breaking known computational limitations of algorithms that do not have access to such advice.", "title_embedding_index": 8914, "title_abs_embedding_index": 8939}, {"title": "FDN: Interpretable Spatiotemporal Forecasting with Future Decomposition Networks", "link_suffix": "/forum?id=mUDazL3mTJ", "link": "https://openreview.net/forum?id=mUDazL3mTJ", "pdf_link": "https://openreview.net/pdf?id=mUDazL3mTJ", "keywords": "Spatiotemporal, Forecast, Graph, Hydrology, Traffic, Energy", "abstract": "Spatiotemporal systems comprise a collection of spatially distributed yet interdependent entities each generating unique dynamic signals. \nHighly sophisticated methods have been proposed in recent years delivering state-of-the-art (SOTA) forecasts but few have focused on interpretability. \nTo address this, we propose the Future Decomposition Network (FDN), a novel forecast model capable of (a) providing interpretable predictions through classification (b) revealing latent activity patterns in the target time-series and (c) delivering forecasts competitive with SOTA methods at a fraction of their memory and runtime cost. \nWe conduct comprehensive analyses on FDN for multiple datasets from hydrologic, traffic, and energy systems demonstrating its improved accuracy and interpretability.", "title_embedding_index": 8915, "title_abs_embedding_index": 8940}, {"title": "Exploring Weak-to-Strong Generalization for CLIP-based Classification", "link_suffix": "/forum?id=FwkYeLovHk", "link": "https://openreview.net/forum?id=FwkYeLovHk", "pdf_link": "https://openreview.net/pdf?id=FwkYeLovHk", "keywords": "Vision-Language Models, Weak-to-Strong Generalization", "abstract": "Aligning large-scale commercial models with user intent is crucial to preventing harmful outputs. Current methods rely on human supervision but become impractical as model complexity increases. When models surpass human knowledge, providing accurate feedback becomes challenging and inefficient.\nA novel solution proposed recently is using a weaker model to supervise a stronger model. This concept leverages the ability of weaker models to perform evaluations, thereby reducing the workload on human supervisors. \nPrevious work has shown the effectiveness of weak-to-strong generalization in the context of language-only models. Extending this concept to vision-language models leverages these insights, adapting the proven benefits to a multi-modal context.\nIn our study, we explore weak-to-strong generalization for CLIP-based classification. We propose a method, \\emph{class prototype learning} (CPL), which aims to enhance the classification capabilities of the CLIP model, by learning more representative prototypes for each category.\nOur findings indicate that despite the simple loss function under weak supervision, CPL yields robust results.\nOur experiments are conducted on challenging datasets to evaluate our method. Extensive experiments show that our method is effective, achieving a 3.67% improvement over baseline methods.", "title_embedding_index": 8916, "title_abs_embedding_index": 8941}, {"title": "EVO-RDesign: Leveraging Evolutionary Priors for Structure-Based RNA Design", "link_suffix": "/forum?id=U6gYBJ5vpg", "link": "https://openreview.net/forum?id=U6gYBJ5vpg", "pdf_link": "https://openreview.net/pdf?id=U6gYBJ5vpg", "keywords": "RNA Design, RNA Tertiary Structures, Drug Development, Evolutionary Priors, RNA Language Model", "abstract": "Designing RNA sequences based on RNA tertiary structures is a crucial aspect of future RNA design with significant potential to aid drug development. Recently, deep learning-based methods have made progress in this area; however, these methods are constrained by the limited availability of RNA structural data, making it challenging to achieve optimal performance. In this paper, we propose EVO-RDesign, which leverages the evolutionary priors embedded in extensive sequence data to facilitate better RNA sequence design. Specifically, RNA language models have recently been demonstrated to learn the evolutionary information of RNA. Therefore, we consider RNA language models as repositories of evolutionary priors and design a series of adaptors that enable EVO-RDesign to retrieve these priors conditioned on the input RNA structural information. To achieve better performance, the adaptor innovatively inputs RNA structural information and outputs from existing RNA design methods into the language model. Experiments demonstrate that EVO-RDesign outperforms RDesign, achieving a 3.5% increase in sequence recovery on RNAsolo. It also exhibits zero-shot generalization, with gains of 5.1% and 4.1% in sequence recovery on RNA-Puzzles and Rfam, respectively. We also apply in-silico folding to validate whether the generated sequences can fold into the specified 3D RNA backbones.", "title_embedding_index": 8917, "title_abs_embedding_index": 8942}, {"title": "TeamCraft: A Benchmark for Embodied Multi-Agent Systems in Minecraft", "link_suffix": "/forum?id=nE3flbe88p", "link": "https://openreview.net/forum?id=nE3flbe88p", "pdf_link": "https://openreview.net/pdf?id=nE3flbe88p", "keywords": "Multi-agent system, embodied AI", "abstract": "Complex 3D environments replete with dynamic interactions among multiple agents and objects are essential for the development of embodied intelligent agents. To facilitate research on Multi-Agent (MA) systems, we introduce \\benchmark, a challenging MA benchmark based on the Minecraft game. Instead of the abstract vector inputs commonly provided to agents in MA systems research, \\benchmark provides agents with multi-modal task specifications and observations. Given the three-orthographic-view graph of the environment along with language instructions, the agents must efficiently collaborate to complete assigned tasks. Such multi-modal inputs pose a higher level of difficulty, since agents must generalize across diverse object and background imagery, different numbers of agents, a wide range of tasks, etc. Our planner-generated dataset includes various tasks, such as building construction, smelting, and farming, with a total of 70,000 procedurally-generated demonstrations that feature over 50 objects across a wide variety of scenes.  We test the generalization abilities of several baseline Vision-Language Model (VLM) multi-agent control strategies in centralized and decentralized settings.", "title_embedding_index": 8918, "title_abs_embedding_index": 8943}, {"title": "OMS: One More Step Noise Searching to Enhance Membership Inference Attacks for Diffusion Models", "link_suffix": "/forum?id=IRCo9mHScB", "link": "https://openreview.net/forum?id=IRCo9mHScB", "pdf_link": "https://openreview.net/pdf?id=IRCo9mHScB", "keywords": "Membership Inference Attack, Diffusion Models, Data Privacy", "abstract": "The data-intensive nature of Diffusion models amplifies the risks of privacy infringements and copyright disputes, particularly when training on extensive unauthorized data scraped from the Internet. Membership Inference Attacks (MIA) aim to determine whether a data sample has been utilized by the target model during training, thereby serving as a pivotal tool for privacy preservation. Current MIA employs the prediction loss to distinguish between training member samples and non-members. \nThese methods assume that, compared to non-members, members, having been encountered by the model during training result in a smaller prediction loss. However, this assumption proves ineffective in diffusion models due to the randomly noise sampled during the training process. Rather than estimating the loss, our approach examines this random noise and reformulate the MIA as a noise search problem, assuming that members are more feasible to find the noise used in the training process.\nWe formulate this noise search process as an optimization problem and employ the fixed-point iteration to solve it. We analyze current MIA methods through the lens of the noise search framework and reveal that they rely on the first residual as the discriminative metric to differentiate members and non-members. Inspired by this observation, we introduce \\textbf{OMS}, which augments existing MIA methods by iterating  \\textbf{O}ne \\textbf{M}ore fixed-point \\textbf{S}tep to include a further residual, i.e., the second residual.We integrate our method into various MIA methods across different diffusion models. The experimental results validate the efficacy of our proposed approach.", "title_embedding_index": 8919, "title_abs_embedding_index": 8944}, {"title": "DCA-Bench: A Benchmark for Dataset Curation Agents", "link_suffix": "/forum?id=a4sknPttwV", "link": "https://openreview.net/forum?id=a4sknPttwV", "pdf_link": "https://openreview.net/pdf?id=a4sknPttwV", "keywords": "Dataset Curation, LLM Agent, Automatic Evaluation", "abstract": "The quality of datasets plays an increasingly crucial role in the research and development of modern artificial intelligence (AI). Despite the proliferation of open dataset platforms nowadays, data quality issues, such as incomplete documentation, inaccurate labels, ethical concerns, and outdated information, remain common in widely used datasets. Furthermore, these issues are often subtle and difficult to be detected by rule-based scripts, therefore requiring identification and verification by dataset users or maintainers--a process that is both time-consuming and prone to human mistakes. With the surging ability of large language models (LLM), it\u2019s promising to streamline the discovery of hidden dataset issues with LLM agents. To achieve this, one significant challenge is enabling LLM agents to detect issues in the wild rather than simply fixing known ones. In this work, we establish a benchmark to measure LLM agent\u2019s ability to tackle this challenge. We carefully curate 221 representative test cases from eight popular dataset platforms and propose an automatic evaluation framework using GPT-4. Our proposed framework shows strong empirical alignment with expert evaluations, validated through extensive comparisons with human annotations. Without any hints, a baseline GPT-4 agent can only reveal 11% of the data quality issues in the proposed dataset, highlighting the complexity of this task and indicating that applying LLM agents to real-world dataset curation still requires further in-depth exploration and innovation.", "title_embedding_index": 8920, "title_abs_embedding_index": 8945}, {"title": "Combining Denoised Neural Network and Genetic Symbolic Regression for Memory Behavior Modeling via Dynamic Asynchronous Optimization", "link_suffix": "/forum?id=DpOQwOzTc2", "link": "https://openreview.net/forum?id=DpOQwOzTc2", "pdf_link": "https://openreview.net/pdf?id=DpOQwOzTc2", "keywords": "Memory behavior, asynchronous optimization, neural networks, genetic symbolic regression", "abstract": "Memory behavior modeling is a key topic in cognitive psychology and education. Traditional psychology describes the dynamic nature of memory through memory equations derived from experimental data, but these models often lack accuracy and their forms are subject to considerable debate. In recent years, data-driven modeling approaches have improved the predictive accuracy of models but often lack interpretability, making it difficult to provide deeper cognitive insights. Although knowledge-driven neural network models have achieved remarkable success in fields such as physics, their application in behavior modeling has been limited. To address this, this paper proposes a Self-evolving Psychological knowledge informed neural network (SPsyINN), which leverages genetic symbolic regression to improve classical memory equations and integrates a neural network model to enhance both prediction accuracy and interpretability. Specifically, this work first constructs a framework that integrates genetic symbolic regression with neural networks and establishes an asynchronous dynamic loss update strategy between them. Secondly, it constructs an interaction mechanism mediated by proxy data, promoting effective communication between genetic symbolic regression and the neural network to achieve efficient joint optimization of the model. Lastly, a denoising module is introduced into the neural network model to improve its robustness and generalization ability to noisy data. SPsyINN outperforms state-of-the-art methods across all key performance metrics on four large-scale real-world memory datasets. Additional experiments demonstrate that the proposed dynamic asynchronous optimization strategy is key to achieving joint model optimization. Moreover, the findings suggest that the method can effectively improve or discover memory equations, highlighting its potential application in psychological research. Our code is released at:https://anonymous.4open.science/status/SPsyINN-3F18", "title_embedding_index": 8921, "title_abs_embedding_index": 8946}, {"title": "Decoupling Variable and Temporal Dependencies: A Novel Approach for Multivariate Time Series Forecasting", "link_suffix": "/forum?id=tYuVjFgEIK", "link": "https://openreview.net/forum?id=tYuVjFgEIK", "pdf_link": "https://openreview.net/pdf?id=tYuVjFgEIK", "keywords": "Time Series Forecasting, transformer, multivariate time series forecasting", "abstract": "In multivariate time series forecasting using the Transformer architecture, capturing temporal dependencies and modeling inter-variable relationships are crucial for improving performance. However, overemphasizing temporal dependencies can destabilize the model, increasing its sensitivity to noise, overfitting, and weakening its ability to capture inter-variable relationships. We propose a new approach called the Temporal-Variable Decoupling Network (TVDN) to address this challenge. This method decouples the modeling of variable dependencies from temporal dependencies and further separates temporal dependencies into historical and predictive sequence dependencies, allowing for a more effective capture of both. Specifically, the simultaneous learning of time-related and variable-related patterns can lead to harmful interference between the two. TVDN first extracts variable dependencies from historical data through a permutation-invariant model and then captures temporal dependencies using a permutation-equivariant model. By decoupling variable and temporal dependencies and historical and predictive sequence dependencies, this approach minimizes interference and allows for complementary extraction of both. Our method provides a concise and innovative approach to enhancing the utilization of temporal features. Experiments on multiple real-world datasets demonstrate that TVDN achieves state-of-the-art performance.", "title_embedding_index": 8922, "title_abs_embedding_index": 8947}, {"title": "Adaptive Graduated Non-Convexity for Point Cloud Registration", "link_suffix": "/forum?id=cIKQp84vqN", "link": "https://openreview.net/forum?id=cIKQp84vqN", "pdf_link": "https://openreview.net/pdf?id=cIKQp84vqN", "keywords": "Non-Convexity; Point Cloud; Registration", "abstract": "Point cloud registration is a critical and challenging task in computer vision. It is difficult to avoid poor local minima since the cost function is significantly non-convex. Correspondences tainted by significant or unknown outliers may cause the probability of finding a close-to-true transformation to drop rapidly, leading to point cloud registration failure. Many registration methods avoid local minima by updating the scale parameter of the cost function using graduated non-convexity (GNC). However, the update is usually performed in a fixed manner, resulting in limited accuracy and robustness of registration, and failure to reliably converge to the global minimum. Therefore, we present a novel method to robust point cloud registration based on Adaptive Graduated Non-Convexity (AGNC). By monitoring the positive definiteness of the Hessian of the cost function, the scale in graduated non-convexity is adaptively reduced without the need for a fixed optimization schedule. In addition, a multi-task knowledge sharing mechanism is used to achieve collaborative optimization of non-convex cost functions at different levels to further improve the success rate of point cloud registration under challenging high outlier conditions. Experimental results on simulated and real point cloud registration datasets show that AGNC far outperforms state-of-the-art methods in terms of robustness and accuracy, and can accurately obtain registration results even in the case of extreme 99% outlier rates. To the best of our knowledge, this is the first study that explores point cloud registration considering adaptive graduated non-convexity.", "title_embedding_index": 8923, "title_abs_embedding_index": 8948}, {"title": "Learning-Augmented Search Data Structures", "link_suffix": "/forum?id=N4rYbQowE3", "link": "https://openreview.net/forum?id=N4rYbQowE3", "pdf_link": "https://openreview.net/pdf?id=N4rYbQowE3", "keywords": "learning-augmented algorithms, data structures", "abstract": "We study the integration of machine learning advice to improve upon traditional data structure designed for efficient search queries. Although there has been recent effort in improving the performance of binary search trees using machine learning advice, e.g., Lin et. al.  (ICML 2022), the resulting constructions nevertheless suffer from inherent weaknesses of binary search trees, such as complexity of maintaining balance across multiple updates and the inability to handle partially-ordered or high-dimensional datasets. For these reasons, we focus on skip lists and KD trees in this work. Given access to a possibly erroneous oracle that outputs estimated fractional frequencies for search queries on a set of items, we construct skip lists and KD trees that provably provides the optimal expected search time, within nearly a factor of two. In fact, our learning-augmented skip lists and KD trees are still optimal up to a constant factor, even if the oracle is only accurate within a constant factor. We show that if the search queries follow the ubiquitous Zipfian distribution, then the expected search time for an item by our data structures is only a constant, independent of the total number $n$ of items, i.e., $\\mathcal{O}(1)$, whereas a traditional skip list or KD tree will have an expected search time of $\\mathcal{O}(\\log n)$. We also demonstrate robustness by showing that our data structures achieves an expected search time that is within a constant factor of an oblivious skip list/KD tree construction even when the predictions are arbitrarily incorrect. Finally, we empirically show that our learning-augmented search data structures outperforms their corresponding traditional analogs on both synthetic and real-world datasets.", "title_embedding_index": 8924, "title_abs_embedding_index": 8949}]